xist and make it valid.
            //

            MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);
        }

        if (PointerPte->u.Long == 0) {

            if (PointerPte <= CommitLimitPte) {

                //
                // This page is implicitly committed.
                //

                QuotaFree += 1;

            }

            //
            // Increment the count of non-zero page table entries
            // for this page table and the number of private pages
            // for the process.
            //

            Va = MiGetVirtualAddressMappedByPte (PointerPte);
            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (Va);

            MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

            MI_WRITE_INVALID_PTE (PointerPte, TempPte);
        }
        else {
            if (PointerPte->u.Long == DecommittedPte.u.Long) {

                //
                // Only commit the page if it is already decommitted.
                //

                MI_WRITE_INVALID_PTE (PointerPte, TempPte);
            }
            else {
                QuotaFree += 1;

                //
                // Make sure the protection for the page is right.
                //

                if (!ChangeProtection &&
                    (Protect != MiGetPageProtection (PointerPte,
                                                     Process,
                                                     FALSE))) {
                    ChangeProtection = TRUE;
                }
            }
        }
        PointerPte += 1;
    }

    UNLOCK_WS_UNSAFE (Process);

#if defined(_MIALT4K_)

    if (WowProcess != NULL) {

        StartingAddress = (PVOID) PAGE_4K_ALIGN(OriginalBase);

        EndingAddress = (PVOID)(((ULONG_PTR)OriginalBase +
                                OriginalRegionSize - 1) | (PAGE_4K - 1));

        CapturedRegionSize = (ULONG_PTR)EndingAddress -
                                  (ULONG_PTR)StartingAddress + 1L;

        //
        // Update the alternate permission table.
        //

        MiProtectFor4kPage (StartingAddress,
                            CapturedRegionSize,
                            OriginalProtectionMask,
                            ALT_COMMIT,
                            Process);
    }
#endif

    if ((ChargedExactQuota == FALSE) && (QuotaFree != 0)) {

        FoundVad->u.VadFlags.CommitCharge -= QuotaFree;
        ASSERT ((LONG_PTR)FoundVad->u.VadFlags.CommitCharge >= 0);
        Process->CommitCharge -= QuotaFree;
        UNLOCK_ADDRESS_SPACE (Process);

        MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - QuotaFree);

        MiReturnCommitment (QuotaFree);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_ALLOCVM2, QuotaFree);

        PsReturnProcessPageFileQuota (Process, QuotaFree);
        if (ChargedJobCommit) {
            PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES, -(SSIZE_T)QuotaFree);
        }
    }
    else {
        UNLOCK_ADDRESS_SPACE (Process);
    }

    //
    // Previously reserved pages have been committed or an error occurred.
    // Detach, dereference process and return status.
    //

done:

    if (ChangeProtection) {
        PVOID Start;
        SIZE_T Size;
        ULONG LastProtect;

        Start = StartingAddress;
        Size = CapturedRegionSize;
        MiProtectVirtualMemory (Process,
                                &Start,
                                &Size,
                                Protect,
                                &LastProtect);
    }

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }

    if (ProcessHandle != NtCurrentProcess()) {
        ObDereferenceObject (Process);
    }

    //
    // Establish an exception handler and write the size and base
    // address.
    //

    try {

        *RegionSize = CapturedRegionSize;
        *BaseAddress = StartingAddress;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        return GetExceptionCode();
    }

    return Status;

ErrorReturn:
        UNLOCK_WS_UNSAFE (Process);

ErrorReturn0:
        UNLOCK_ADDRESS_SPACE (Process);

ErrorReturn1:
        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }
        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (Process);
        }
        return Status;
}

NTSTATUS
MmCommitSessionMappedView (
    IN PVOID MappedAddress,
    IN SIZE_T ViewSize
    )

/*++

Routine Description:

    This function commits a region of pages within the session mapped
    view virtual address space.

Arguments:

    MappedAddress - Supplies the non-NULL address within a session mapped view
                    to begin committing pages at.  Note the backing section
                    must be pagefile backed.

    ViewSize - Supplies the actual size in bytes to be committed.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PSUBSECTION Subsection;
    ULONG_PTR Base16;
    ULONG Hash;
    ULONG Size;
    ULONG count;
    PMMSESSION Session;
    PVOID ViewBaseAddress;
    PVOID StartingAddress;
    PVOID EndingAddress;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE StartingPte;
    MMPTE TempPte;
    SIZE_T QuotaCharge;
    SIZE_T QuotaFree;
    LOGICAL ChargedExactQuota;
    PCONTROL_AREA ControlArea;
    PSEGMENT Segment;

    PAGED_CODE();

    //
    // Make sure the specified starting and ending addresses are
    // within the session view portion of the virtual address space.
    //

    if (((ULONG_PTR)MappedAddress < MiSessionViewStart) ||
        ((ULONG_PTR)MappedAddress >= MiSessionViewStart + MmSessionViewSize)) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER_1;
    }

    if ((ULONG_PTR)MiSessionViewStart + MmSessionViewSize - (ULONG_PTR)MappedAddress <
        ViewSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER_2;
    }

    ASSERT (ViewSize != 0);

    if ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) {
        return STATUS_NOT_MAPPED_VIEW;
    }

    //
    // Commit previously reserved pages.
    //

    StartingAddress = (PVOID)PAGE_ALIGN (MappedAddress);

    EndingAddress = (PVOID)(((ULONG_PTR)MappedAddress +
                            ViewSize - 1) | (PAGE_SIZE - 1));

    ViewSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1;

    ASSERT (MmIsAddressValid (MmSessionSpace) == TRUE);

    Session = &MmSessionSpace->Session;

    ChargedExactQuota = FALSE;

    QuotaCharge = (MiGetPteAddress (EndingAddress) - MiGetPteAddress (StartingAddress) + 1);

    //
    // Get the session view mutex to prevent win32k referencing bugs where
    // they might be trying to delete the view at the same time in another
    // thread.  This also blocks APCs so an APC which takes a page
    // fault does not corrupt various structures.
    //

    count = 0;

    Base16 = (ULONG_PTR)StartingAddress >> 16;

    LOCK_SYSTEM_VIEW_SPACE (Session);

    Hash = (ULONG)(Base16 % Session->SystemSpaceHashKey);

    do {
            
        ViewBaseAddress = (PVOID)(Session->SystemSpaceViewTable[Hash].Entry & ~0xFFFF);

        Size = (ULONG) ((Session->SystemSpaceViewTable[Hash].Entry & 0xFFFF) * X64K);

        if ((StartingAddress >= ViewBaseAddress) &&
            (EndingAddress < (PVOID)((PCHAR)ViewBaseAddress + Size))) {

            break;
        }

        Hash += 1;
        if (Hash >= Session->SystemSpaceHashSize) {
            Hash = 0;
            count += 1;
            if (count == 2) {
                KeBugCheckEx (DRIVER_UNMAPPING_INVALID_VIEW,
                              (ULONG_PTR)StartingAddress,
                              2,
                              0,
                              0);
            }
        }
    } while (TRUE);

    ControlArea = Session->SystemSpaceViewTable[Hash].ControlArea;

    if (ControlArea->FilePointer != NULL) {

        //
        // Only page file backed sections can be committed.
        //

        UNLOCK_SYSTEM_VIEW_SPACE (Session);
        return STATUS_ALREADY_COMMITTED;
    }

    //
    // Session views always start at the beginning of the file which makes
    // calculating the corresponding prototype PTE here straightforward.
    //

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    StartingPte = Subsection->SubsectionBase;

    StartingPte += (((ULONG_PTR) StartingAddress - (ULONG_PTR) ViewBaseAddress) >> PAGE_SHIFT);

    LastPte = StartingPte + QuotaCharge;

    if (LastPte >= Subsection->SubsectionBase + Subsection->PtesInSubsection) {
        UNLOCK_SYSTEM_VIEW_SPACE (Session);
        return STATUS_INVALID_PARAMETER_2;
    }

    //
    // Charge commitment for the range.
    //

    PointerPte = StartingPte;

    do {
        if (MiChargeCommitment (QuotaCharge, NULL) == TRUE) {
            break;
        }

        //
        // Reduce the charge we are asking for if possible.
        //

        if (ChargedExactQuota == TRUE) {

            //
            // We have already tried for the precise charge,
            // so just return an error.
            //

            KeReleaseGuardedMutexUnsafe (&MmSectionCommitMutex);

            UNLOCK_SYSTEM_VIEW_SPACE (Session);
            return STATUS_COMMITMENT_LIMIT;
        }

        //
        // The commitment charging of quota failed, calculate the
        // exact quota taking into account pages that may already be
        // committed and retry the operation.
        //

        KeAcquireGuardedMutexUnsafe (&MmSectionCommitMutex);

        while (PointerPte < LastPte) {

            //
            // Check to see if the prototype PTE is committed.
            // Note that prototype PTEs cannot be decommitted so
            // PTEs only need to be checked for zeroes.
            //

            if (PointerPte->u.Long != 0) {
                QuotaCharge -= 1;
            }
            PointerPte += 1;
        }

        PointerPte = StartingPte;

        ChargedExactQuota = TRUE;

        //
        // If the entire range is committed then there's nothing to charge.
        //

        if (QuotaCharge == 0) {
            KeReleaseGuardedMutexUnsafe (&MmSectionCommitMutex);

            UNLOCK_SYSTEM_VIEW_SPACE (Session);
            return STATUS_SUCCESS;
        }

    } while (TRUE);

    if (ChargedExactQuota == FALSE) {
        KeAcquireGuardedMutexUnsafe (&MmSectionCommitMutex);
    }

    //
    // Commit all the pages.
    //

    Segment = ControlArea->Segment;
    TempPte = Segment->SegmentPteTemplate;
    ASSERT (TempPte.u.Long != 0);

    QuotaFree = 0;

    while (PointerPte < LastPte) {

        if (PointerPte->u.Long != 0) {

            //
            // Page is already committed, back out commitment.
            //

            QuotaFree += 1;
        }
        else {
            MI_WRITE_INVALID_PTE (PointerPte, TempPte);
        }
        PointerPte += 1;
    }

    //
    // Subtract out any excess, then update the segment charges.
    // Note only segment commit is excess - process commit must
    // remain fully charged.
    //

    if (ChargedExactQuota == FALSE) {
        ASSERT (QuotaCharge >= QuotaFree);
        QuotaCharge -= QuotaFree;

        //
        // Return the QuotaFree excess commitment after the
        // mutexes are released to remove needless contention.
        //
    }
    else {

        //
        // Exact quota was charged so zero this to signify
        // there is no excess to return.
        //

        QuotaFree = 0;
    }

    if (QuotaCharge != 0) {
        Segment->NumberOfCommittedPages += QuotaCharge;
        InterlockedExchangeAddSizeT (&MmSharedCommit, QuotaCharge);

        MM_TRACK_COMMIT (MM_DBG_COMMIT_ALLOCVM_SEGMENT, QuotaCharge);
    }

    KeReleaseGuardedMutexUnsafe (&MmSectionCommitMutex);

    //
    // Update the per-process charges.
    //

    UNLOCK_SYSTEM_VIEW_SPACE (Session);

    //
    // Return any excess segment commit that may have been charged.
    //

    if (QuotaFree != 0) {
        MiReturnCommitment (QuotaFree);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_ALLOCVM_SEGMENT, QuotaFree);
    }

    return STATUS_SUCCESS;
}

NTSTATUS
MiResetVirtualMemory (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    )

/*++

Routine Description:


Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

    Vad - Supplies the relevant VAD for the range.

    Process - Supplies the current process.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, APCs disabled, AddressCreation mutex held.

--*/

{
    PVOID TempVa;
    PMMPTE PointerPte;
    PMMPTE ProtoPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE LastPte;
    MMPTE PteContents;
    ULONG Waited;
    ULONG First;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PMMCLONE_BLOCK CloneBlock;
#if DBG
    PMMCLONE_DESCRIPTOR CloneDescriptor;
#endif
    MMPTE_FLUSH_LIST PteFlushList;
#if defined(_X86_) || defined(_AMD64_)
    WSLE_NUMBER WsPfnIndex;
    WSLE_NUMBER WorkingSetIndex;
#endif

    if (Vad->u.VadFlags.PrivateMemory == 0) {

        if (Vad->ControlArea->FilePointer != NULL) {

            //
            // Only page file backed sections can be reset.
            //

            return STATUS_USER_MAPPED_FILE;
        }
    }

    OldIrql = MM_NOIRQL;

    First = TRUE;
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    PteFlushList.Count = 0;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    //
    // Examine all the PTEs in the range.
    //

    LOCK_WS_UNSAFE (Process);

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary (PointerPte) || (First)) {

            if (PteFlushList.Count != 0) {
                MiFlushPteList (&PteFlushList, FALSE);
                PteFlushList.Count = 0;
            }

            if (MiIsPteOnPpeBoundary (PointerPte) || (First)) {

                if (MiIsPteOnPxeBoundary (PointerPte) || (First)) {

                    PointerPxe = MiGetPpeAddress (PointerPte);

                    if (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                                     Process,
                                                     OldIrql,
                                                     &Waited)) {

                        //
                        // This extended page directory parent entry is empty,
                        // go to the next one.
                        //

                        PointerPxe += 1;
                        PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                        continue;
                    }
                }

                PointerPpe = MiGetPdeAddress (PointerPte);

                if (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                                 Process,
                                                 OldIrql,
                                                 &Waited)) {

                    //
                    // This page directory parent entry is empty,
                    // go to the next one.
                    //

                    PointerPpe += 1;
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    continue;
                }
            }

            //
            // Pointing to the next page table page, make
            // a page table page exist and make it valid.
            //

            First = FALSE;
            PointerPde = MiGetPteAddress (PointerPte);

            if (!MiDoesPdeExistAndMakeValid (PointerPde,
                                             Process,
                                             OldIrql,
                                             &Waited)) {

                //
                // This page directory entry is empty, go to the next one.
                //

                PointerPde += 1;
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                continue;
            }
        }

        PteContents = *PointerPte;
        ProtoPte = NULL;

        if ((PteContents.u.Hard.Valid == 0) &&
            (PteContents.u.Soft.Prototype == 1))  {

            //
            // This is a prototype PTE, evaluate the prototype PTE.  Note that
            // the fact it is a prototype PTE does not guarantee that this is a
            // regular or long VAD - it may be a short VAD in a forked process,
            // so check PrivateMemory before referencing the FirstPrototypePte
            // field.
            //

            if ((Vad->u.VadFlags.PrivateMemory == 0) &&
                (Vad->FirstPrototypePte != NULL)) {

                ProtoPte = MiGetProtoPteAddress (Vad,
                                        MI_VA_TO_VPN (
                                        MiGetVirtualAddressMappedByPte (PointerPte)));
            }
            else {
                CloneBlock = (PMMCLONE_BLOCK) MiPteToProto (PointerPte);
                ProtoPte = (PMMPTE) CloneBlock;
#if DBG
                CloneDescriptor = MiLocateCloneAddress (Process, (PVOID)CloneBlock);
                ASSERT (CloneDescriptor != NULL);
#endif
            }

            if (OldIrql == MM_NOIRQL) {
                ASSERT (PteFlushList.Count == 0);
                LOCK_PFN (OldIrql);
                ASSERT (OldIrql != MM_NOIRQL);
            }

            //
            // The working set mutex may be released in order to make the
            // prototype PTE which resides in paged pool resident.  If this
            // occurs, the page directory and/or page table of the original
            // user address may get trimmed.  Account for that here.
            //

            if (MiGetPteAddress (ProtoPte)->u.Hard.Valid == 0) {

                if (PteFlushList.Count != 0) {
                    MiFlushPteList (&PteFlushList, FALSE);
                    PteFlushList.Count = 0;
                }

                if (MiMakeSystemAddressValidPfnWs (ProtoPte, Process, OldIrql) != 0) {

                    //
                    // Working set mutex was released and PFN lock were
                    // released & reacquired, restart from the top.
                    //

                    First = TRUE;
                    continue;
                }
            }

            PteContents = *ProtoPte;
        }

        if (PteContents.u.Hard.Valid == 1) {

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

#if defined(_X86_) || defined(_AMD64_)

            if (!ProtoPte) {

                //
                // The access bit is set (and TB inserted) automatically by the
                // processor if the valid bit is set so clear it here in both
                // the PTE and the WSLE so we know it's more worthwhile to trim
                // should we need the memory.  If the access bit is already
                // clear then just skip the WSLE search under the premise
                // that it is already getting aged.
                //

                if (MI_GET_ACCESSED_IN_PTE (&PteContents) == 1) {

                    MI_SET_ACCESSED_IN_PTE (PointerPte, 0);

                    WsPfnIndex = Pfn1->u1.WsIndex;
    
                    TempVa = MiGetVirtualAddressMappedByPte (PointerPte);

                    WorkingSetIndex = MiLocateWsle (TempVa,
                                                    MmWorkingSetList,
                                                    WsPfnIndex);
  
                    ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);
    
                    MmWsle[WorkingSetIndex].u1.e1.Age = 3;
                }
            }

#endif

            if (OldIrql == MM_NOIRQL) {
                ASSERT (PteFlushList.Count == 0);
                LOCK_PFN (OldIrql);
                ASSERT (OldIrql != MM_NOIRQL);
                continue;
            }

            if (Pfn1->u3.e2.ReferenceCount == 1) {

                //
                // Only this process has the page mapped.
                //

                MI_SET_MODIFIED (Pfn1, 0, 0x20);
                MiReleasePageFileSpace (Pfn1->OriginalPte);
                Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
            }

            if (!ProtoPte) {

                if (MI_IS_PTE_DIRTY (PteContents)) {

                    //
                    // Clear the dirty bit and flush TB since it
                    // is NOT a prototype PTE.
                    //

                    MI_SET_ACCESSED_IN_PTE (&PteContents, 0);
                    MI_SET_PTE_CLEAN (PteContents);

                    MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, PteContents);

                    if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
                        TempVa = MiGetVirtualAddressMappedByPte (PointerPte);
                        PteFlushList.FlushVa[PteFlushList.Count] = TempVa;
                        PteFlushList.Count += 1;
                    }
                }
            }
        }
        else if (PteContents.u.Soft.Transition == 1) {

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

            if (OldIrql == MM_NOIRQL) {

                //
                // This must be a private page (because the PFN lock is not
                // held).  If the page is clean, just march on to the next one.
                //

                ASSERT (!ProtoPte);
                ASSERT (PteFlushList.Count == 0);

                if (Pfn1->u3.e1.PageLocation == StandbyPageList) {
                    PointerPte += 1;
                    continue;
                }

                LOCK_PFN (OldIrql);
                ASSERT (OldIrql != MM_NOIRQL);
                continue;
            }
            if ((Pfn1->u3.e1.PageLocation == ModifiedPageList) &&
                (Pfn1->u3.e2.ReferenceCount == 0)) {

                //
                // Remove from the modified list, release the page
                // file space and insert on the standby list.
                //

                MI_SET_MODIFIED (Pfn1, 0, 0x21);
                MiUnlinkPageFromList (Pfn1);
                MiReleasePageFileSpace (Pfn1->OriginalPte);
                Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
                MiInsertPageInList (&MmStandbyPageListHead,
                                    MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents));
            }
        }
        else {
            if (PteContents.u.Soft.PageFileHigh != 0) {

                if (OldIrql == MM_NOIRQL) {

                    //
                    // This must be a private page (because the PFN
                    // lock is not held).
                    //

                    ASSERT (!ProtoPte);
                    ASSERT (PteFlushList.Count == 0);

                    LOCK_PFN (OldIrql);
                    ASSERT (OldIrql != MM_NOIRQL);
                }

                MiReleasePageFileSpace (PteContents);

                if (PteFlushList.Count != 0) {
                    MiFlushPteList (&PteFlushList, FALSE);
                    PteFlushList.Count = 0;
                }

                if (ProtoPte) {
                    ProtoPte->u.Soft.PageFileHigh = 0;
                }

                UNLOCK_PFN (OldIrql);
                OldIrql = MM_NOIRQL;

                if (!ProtoPte) {
                    PointerPte->u.Soft.PageFileHigh = 0;
                }
            }
            else {
                if (OldIrql != MM_NOIRQL) {

                    if (PteFlushList.Count != 0) {
                        MiFlushPteList (&PteFlushList, FALSE);
                        PteFlushList.Count = 0;
                    }

                    UNLOCK_PFN (OldIrql);
                    OldIrql = MM_NOIRQL;
                }
            }
        }
        PointerPte += 1;
    }
    if (OldIrql != MM_NOIRQL) {
        if (PteFlushList.Count != 0) {
            MiFlushPteList (&PteFlushList, FALSE);
        }

        UNLOCK_PFN (OldIrql);
        OldIrql = MM_NOIRQL;
    }
    else {
        ASSERT (PteFlushList.Count == 0);
    }

    UNLOCK_WS_UNSAFE (Process);

    MmUnlockPagableImageSection (ExPageLockHandle);

    return STATUS_SUCCESS;
}

LOGICAL
MiCreatePageTablesForPhysicalRange (
    IN PEPROCESS Process,
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    )

/*++

Routine Description:

    This routine initializes page directory and page table pages for a
    user-controlled physical range of pages.

Arguments:

    Process - Supplies the current process.

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

Return Value:

    TRUE if the page tables were created, FALSE if not.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    MMPTE PteContents;
    PMMPTE LastPte;
    PMMPTE LastPde;
    PMMPTE LastPpe;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PVOID UsedPageTableHandle;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PFN_NUMBER PagesNeeded;

    PointerPpe = MiGetPpeAddress (StartingAddress);
    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPpe = MiGetPpeAddress (EndingAddress);
    LastPde = MiGetPdeAddress (EndingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    //
    // Charge resident available pages for all of the page directory and table
    // pages as they will not be paged until the VAD is freed.
    //

    if (LastPte != PointerPte) {
        PagesNeeded = MI_COMPUTE_PAGES_SPANNED (PointerPte,
                                                LastPte - PointerPte);

#if (_MI_PAGING_LEVELS >= 3)
        if (LastPde != PointerPde) {
            PagesNeeded += MI_COMPUTE_PAGES_SPANNED (PointerPde,
                                                     LastPde - PointerPde);
#if (_MI_PAGING_LEVELS >= 4)
            if (LastPpe != PointerPpe) {
                PagesNeeded += MI_COMPUTE_PAGES_SPANNED (PointerPpe,
                                                         LastPpe - PointerPpe);
            }
#endif
        }
#endif
    }
    else {
        PagesNeeded = 1;
#if (_MI_PAGING_LEVELS >= 3)
        PagesNeeded += 1;
#endif
#if (_MI_PAGING_LEVELS >= 4)
        PagesNeeded += 1;
#endif
    }

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    if ((SPFN_NUMBER)PagesNeeded > MI_NONPAGABLE_MEMORY_AVAILABLE() - 20) {
        UNLOCK_PFN (OldIrql);
        MmUnlockPagableImageSection (ExPageLockHandle);
        return FALSE;
    }

    MI_DECREMENT_RESIDENT_AVAILABLE (PagesNeeded, MM_RESAVAIL_ALLOCATE_USER_PAGE_TABLE);

    UNLOCK_PFN (OldIrql);

    UsedPageTableHandle = NULL;

    //
    // Fill in all the page table pages with the zero PTE.
    //

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary (PointerPte) || UsedPageTableHandle == NULL) {

            PointerPde = MiGetPteAddress (PointerPte);

            //
            // Pointing to the next page table page, make
            // a page table page exist and make it valid.
            //
            // Note this ripples sharecounts through the paging hierarchy so
            // there is no need to up sharecounts to prevent trimming of the
            // page directory (and parent) page as making the page table
            // valid below does this automatically.
            //

            MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);

            //
            // Up the sharecount so the page table page will not get
            // trimmed even if it has no currently valid entries.
            //

            PteContents = *PointerPde;
            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
            LOCK_PFN (OldIrql);
            Pfn1->u2.ShareCount += 1;
            UNLOCK_PFN (OldIrql);

            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (StartingAddress);
        }

        ASSERT (PointerPte->u.Long == 0);

        //
        // Increment the count of non-zero page table entries
        // for this page table - even though this entry is still zero,
        // this is a special case.
        //

        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

        PointerPte += 1;
        StartingAddress = (PVOID)((PUCHAR)StartingAddress + PAGE_SIZE);
    }
    MmUnlockPagableImageSection (ExPageLockHandle);
    return TRUE;
}

VOID
MiDeletePageTablesForPhysicalRange (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    )

/*++

Routine Description:

    This routine deletes page directory and page table pages for a
    user-controlled physical range of pages.

    Even though PTEs may be zero in this range, UsedPageTable counts were
    incremented for these special ranges and must be decremented now.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    PVOID TempVa;
    MMPTE PteContents;
    PMMPTE LastPte;
    PMMPTE LastPde;
    PMMPTE LastPpe;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PFN_NUMBER PagesNeeded;
    PEPROCESS CurrentProcess;
    PVOID UsedPageTableHandle;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PMMPTE PointerPpe;
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
#endif

    CurrentProcess = PsGetCurrentProcess();

    PointerPpe = MiGetPpeAddress (StartingAddress);
    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPpe = MiGetPpeAddress (EndingAddress);
    LastPde = MiGetPdeAddress (EndingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (StartingAddress);

    //
    // Each PTE is already zeroed - just delete the containing pages.
    //
    // Restore resident available pages for all of the page directory and table
    // pages as they can now be paged again.
    //

    if (LastPte != PointerPte) {
        PagesNeeded = MI_COMPUTE_PAGES_SPANNED (PointerPte,
                                                LastPte - PointerPte);
#if (_MI_PAGING_LEVELS >= 3)
        if (LastPde != PointerPde) {
            PagesNeeded += MI_COMPUTE_PAGES_SPANNED (PointerPde,
                                                     LastPde - PointerPde);
#if (_MI_PAGING_LEVELS >= 4)
            if (LastPpe != PointerPpe) {
                PagesNeeded += MI_COMPUTE_PAGES_SPANNED (PointerPpe,
                                                         LastPpe - PointerPpe);
            }
#endif
        }
#endif
    }
    else {
        PagesNeeded = 1;
#if (_MI_PAGING_LEVELS >= 3)
        PagesNeeded += 1;
#endif
#if (_MI_PAGING_LEVELS >= 4)
        PagesNeeded += 1;
#endif
    }

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    while (PointerPte <= LastPte) {

        ASSERT (PointerPte->u.Long == 0);

        PointerPte += 1;

        MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

        if ((MiIsPteOnPdeBoundary(PointerPte)) || (PointerPte > LastPte)) {

            //
            // The virtual address is on a page directory boundary or it is
            // the last address in the entire range.
            //
            // If all the entries have been eliminated from the previous
            // page table page, delete the page table page itself.
            //

            PointerPde = MiGetPteAddress (PointerPte - 1);
            ASSERT (PointerPde->u.Hard.Valid == 1);

            //
            // Down the sharecount on the finished page table page.
            //

            PteContents = *PointerPde;
            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
            ASSERT (Pfn1->u2.ShareCount > 1);
            Pfn1->u2.ShareCount -= 1;

            //
            // If all the entries have been eliminated from the previous
            // page table page, delete the page table page itself.
            //

            if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                ASSERT (PointerPde->u.Long != 0);

#if (_MI_PAGING_LEVELS >= 3)
                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPte - 1);
                MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
#endif

                TempVa = MiGetVirtualAddressMappedByPte(PointerPde);
                MiDeletePte (PointerPde,
                             TempVa,
                             FALSE,
                             CurrentProcess,
                             NULL,
                             NULL,
                             OldIrql);

#if (_MI_PAGING_LEVELS >= 3)
                if ((MiIsPteOnPpeBoundary(PointerPte)) || (PointerPte > LastPte)) {
    
                    PointerPpe = MiGetPteAddress (PointerPde);
                    ASSERT (PointerPpe->u.Hard.Valid == 1);
    
                    //
                    // If all the entries have been eliminated from the previous
                    // page directory page, delete the page directory page too.
                    //
    
                    if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                        ASSERT (PointerPpe->u.Long != 0);

#if (_MI_PAGING_LEVELS >= 4)
                        UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPde);
                        MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
#endif

                        TempVa = MiGetVirtualAddressMappedByPte(PointerPpe);
                        MiDeletePte (PointerPpe,
                                     TempVa,
                                     FALSE,
                                     CurrentProcess,
                                     NULL,
                                     NULL,
                                     OldIrql);

#if (_MI_PAGING_LEVELS >= 4)
                        if ((MiIsPteOnPxeBoundary(PointerPte)) || (PointerPte > LastPte)) {
                            PointerPxe = MiGetPdeAddress (PointerPde);
                            if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                                ASSERT (PointerPxe->u.Long != 0);
                                TempVa = MiGetVirtualAddressMappedByPte(PointerPxe);
                                MiDeletePte (PointerPxe,
                                             TempVa,
                                             FALSE,
                                             CurrentProcess,
                                             NULL,
                                             NULL,
                                             OldIrql);
                            }
                        }
#endif    
                    }
                }
#endif
            }

            if (PointerPte > LastPte) {
                break;
            }

            //
            // Release the PFN lock.  This prevents a single thread
            // from forcing other high priority threads from being
            // blocked while a large address range is deleted.
            //

            UNLOCK_PFN (OldIrql);
            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE ((PVOID)((PUCHAR)StartingAddress + PAGE_SIZE));
            LOCK_PFN (OldIrql);
        }

        StartingAddress = (PVOID)((PUCHAR)StartingAddress + PAGE_SIZE);
    }

    UNLOCK_PFN (OldIrql);

    MI_INCREMENT_RESIDENT_AVAILABLE (PagesNeeded, MM_RESAVAIL_FREE_USER_PAGE_TABLE);

    MmUnlockPagableImageSection (ExPageLockHandle);

    //
    // All done, return.
    //

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\checkpfn.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   checkpfn.c

Abstract:

    This module contains routines for sanity checking the PFN database.

Author:

    Lou Perazzoli (loup) 25-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#if DBG

PRTL_BITMAP CheckPfnBitMap;


VOID
MiCheckPfn (
            )

/*++

Routine Description:

    This routine checks each physical page in the PFN database to ensure
    it is in the proper state.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PEPROCESS Process;
    PMMPFN Pfn1;
    PFN_NUMBER Link, Previous;
    ULONG i;
    PMMPTE PointerPte;
    KIRQL OldIrql;
    USHORT ValidCheck[4];
    USHORT ValidPage[4];
    PMMPFN PfnX;

    ValidCheck[0] = ValidCheck[1] = ValidCheck[2] = ValidCheck[3] = 0;
    ValidPage[0] = ValidPage[1] = ValidPage[2] = ValidPage[3] = 0;

    if (CheckPfnBitMap == NULL) {
        MiCreateBitMap ( &CheckPfnBitMap, MmNumberOfPhysicalPages, NonPagedPool);
    }
    RtlClearAllBits (CheckPfnBitMap);

    Process = PsGetCurrentProcess ();

    //
    // Walk free list.
    //

    LOCK_PFN (OldIrql);

    Previous = MM_EMPTY_LIST;
    Link = MmFreePageListHead.Flink;
    for (i=0; i < MmFreePageListHead.Total; i++) {
        if (Link == MM_EMPTY_LIST) {
            DbgPrint("free list total count wrong\n");
            UNLOCK_PFN (OldIrql);
            return;
        }
        RtlSetBits (CheckPfnBitMap, (ULONG)Link, 1L);
        Pfn1 = MI_PFN_ELEMENT(Link);
        if (Pfn1->u3.e2.ReferenceCount != 0) {
            DbgPrint("non zero reference count on free list\n");
            MiFormatPfn(Pfn1);

        }
        if (Pfn1->u3.e1.PageLocation != FreePageList) {
            DbgPrint("page location not freelist\n");
            MiFormatPfn(Pfn1);
        }
        if (Pfn1->u2.Blink != Previous) {
            DbgPrint("bad blink on free list\n");
            MiFormatPfn(Pfn1);
        }
        Previous = Link;
        Link = Pfn1->u1.Flink;

    }
    if (Link != MM_EMPTY_LIST) {
            DbgPrint("free list total count wrong\n");
            Pfn1 = MI_PFN_ELEMENT(Link);
            MiFormatPfn(Pfn1);
    }

    //
    // Walk zeroed list.
    //

    Previous = MM_EMPTY_LIST;
    Link = MmZeroedPageListHead.Flink;
    for (i=0; i < MmZeroedPageListHead.Total; i++) {
        if (Link == MM_EMPTY_LIST) {
            DbgPrint("zero list total count wrong\n");
            UNLOCK_PFN (OldIrql);
            return;
        }
        RtlSetBits (CheckPfnBitMap, (ULONG)Link, 1L);
        Pfn1 = MI_PFN_ELEMENT(Link);
        if (Pfn1->u3.e2.ReferenceCount != 0) {
            DbgPrint("non zero reference count on zero list\n");
            MiFormatPfn(Pfn1);

        }
        if (Pfn1->u3.e1.PageLocation != ZeroedPageList) {
            DbgPrint("page location not zerolist\n");
            MiFormatPfn(Pfn1);
        }
        if (Pfn1->u2.Blink != Previous) {
            DbgPrint("bad blink on zero list\n");
            MiFormatPfn(Pfn1);
        }
        Previous = Link;
        Link = Pfn1->u1.Flink;

    }
    if (Link != MM_EMPTY_LIST) {
            DbgPrint("zero list total count wrong\n");
            Pfn1 = MI_PFN_ELEMENT(Link);
            MiFormatPfn(Pfn1);
    }

    //
    // Walk Bad list.
    //
    Previous = MM_EMPTY_LIST;
    Link = MmBadPageListHead.Flink;
    for (i=0; i < MmBadPageListHead.Total; i++) {
        if (Link == MM_EMPTY_LIST) {
            DbgPrint("Bad list total count wrong\n");
            UNLOCK_PFN (OldIrql);
            return;
        }
        RtlSetBits (CheckPfnBitMap, (ULONG)Link, 1L);
        Pfn1 = MI_PFN_ELEMENT(Link);
        if (Pfn1->u3.e2.ReferenceCount != 0) {
            DbgPrint("non zero reference count on Bad list\n");
            MiFormatPfn(Pfn1);

        }
        if (Pfn1->u3.e1.PageLocation != BadPageList) {
            DbgPrint("page location not Badlist\n");
            MiFormatPfn(Pfn1);
        }
        if (Pfn1->u2.Blink != Previous) {
            DbgPrint("bad blink on Bad list\n");
            MiFormatPfn(Pfn1);
        }
        Previous = Link;
        Link = Pfn1->u1.Flink;

    }
    if (Link != MM_EMPTY_LIST) {
            DbgPrint("Bad list total count wrong\n");
            Pfn1 = MI_PFN_ELEMENT(Link);
            MiFormatPfn(Pfn1);
    }

    //
    // Walk Standby list.
    //

    Previous = MM_EMPTY_LIST;
    Link = MmStandbyPageListHead.Flink;
    for (i=0; i < MmStandbyPageListHead.Total; i++) {
        if (Link == MM_EMPTY_LIST) {
            DbgPrint("Standby list total count wrong\n");
            UNLOCK_PFN (OldIrql);
            return;
        }
        RtlSetBits (CheckPfnBitMap, (ULONG)Link, 1L);
        Pfn1 = MI_PFN_ELEMENT(Link);
        if (Pfn1->u3.e2.ReferenceCount != 0) {
            DbgPrint("non zero reference count on Standby list\n");
            MiFormatPfn(Pfn1);

        }
        if (Pfn1->u3.e1.PageLocation != StandbyPageList) {
            DbgPrint("page location not Standbylist\n");
            MiFormatPfn(Pfn1);
        }
        if (Pfn1->u2.Blink != Previous) {
            DbgPrint("bad blink on Standby list\n");
            MiFormatPfn(Pfn1);
        }

        //
        // Check to see if referenced PTE is okay.
        //
        if (MI_IS_PFN_DELETED (Pfn1)) {
            DbgPrint("Invalid pteaddress in standby list\n");
            MiFormatPfn(Pfn1);

        } else {

            OldIrql = 99;
            if ((Pfn1->u3.e1.PrototypePte == 1) &&
                            (MmIsAddressValid (Pfn1->PteAddress))) {
                PointerPte = Pfn1->PteAddress;
            } else {
                PointerPte = MiMapPageInHyperSpace (Process,
                                                    Pfn1->u4.PteFrame,
                                                    &OldIrql);
                PointerPte = (PMMPTE)((ULONG_PTR)PointerPte +
                                    MiGetByteOffset(Pfn1->PteAddress));
            }
            if (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) != Link) {
                DbgPrint("Invalid PFN - PTE address is wrong in standby list\n");
                MiFormatPfn(Pfn1);
                MiFormatPte(PointerPte);
            }
            if (PointerPte->u.Soft.Transition == 0) {
                DbgPrint("Pte not in transition for page on standby list\n");
                MiFormatPfn(Pfn1);
                MiFormatPte(PointerPte);
            }
            if (OldIrql != 99) {
                MiUnmapPageInHyperSpace (Process, PointerPte, OldIrql);
                OldIrql = 99;
            }

        }

        Previous = Link;
        Link = Pfn1->u1.Flink;

    }
    if (Link != MM_EMPTY_LIST) {
            DbgPrint("Standby list total count wrong\n");
            Pfn1 = MI_PFN_ELEMENT(Link);
            MiFormatPfn(Pfn1);
    }

    //
    // Walk Modified list.
    //

    Previous = MM_EMPTY_LIST;
    Link = MmModifiedPageListHead.Flink;
    for (i=0; i < MmModifiedPageListHead.Total; i++) {
        if (Link == MM_EMPTY_LIST) {
            DbgPrint("Modified list total count wrong\n");
            UNLOCK_PFN (OldIrql);
            return;
        }
        RtlSetBits (CheckPfnBitMap, (ULONG)Link, 1L);
        Pfn1 = MI_PFN_ELEMENT(Link);
        if (Pfn1->u3.e2.ReferenceCount != 0) {
            DbgPrint("non zero reference count on Modified list\n");
            MiFormatPfn(Pfn1);

        }
        if (Pfn1->u3.e1.PageLocation != ModifiedPageList) {
            DbgPrint("page location not Modifiedlist\n");
            MiFormatPfn(Pfn1);
        }
        if (Pfn1->u2.Blink != Previous) {
            DbgPrint("bad blink on Modified list\n");
            MiFormatPfn(Pfn1);
        }
        //
        // Check to see if referenced PTE is okay.
        //
        if (MI_IS_PFN_DELETED (Pfn1)) {
            DbgPrint("Invalid pteaddress in modified list\n");
            MiFormatPfn(Pfn1);

        } else {

            if ((Pfn1->u3.e1.PrototypePte == 1) &&
                            (MmIsAddressValid (Pfn1->PteAddress))) {
                PointerPte = Pfn1->PteAddress;
            } else {
                PointerPte = MiMapPageInHyperSpace (Process,
                                                    Pfn1->u4.PteFrame,
                                                    &OldIrql);

                PointerPte = (PMMPTE)((ULONG_PTR)PointerPte +
                                    MiGetByteOffset(Pfn1->PteAddress));
            }

            if (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte) != Link) {
                DbgPrint("Invalid PFN - PTE address is wrong in modified list\n");
                MiFormatPfn(Pfn1);
                MiFormatPte(PointerPte);
            }
            if (PointerPte->u.Soft.Transition == 0) {
                DbgPrint("Pte not in transition for page on modified list\n");
                MiFormatPfn(Pfn1);
                MiFormatPte(PointerPte);
            }

            if (OldIrql != 99) {
                MiUnmapPageInHyperSpace (Process, PointerPte, OldIrql);
                OldIrql = 99;
            }
        }

        Previous = Link;
        Link = Pfn1->u1.Flink;

    }
    if (Link != MM_EMPTY_LIST) {
            DbgPrint("Modified list total count wrong\n");
            Pfn1 = MI_PFN_ELEMENT(Link);
            MiFormatPfn(Pfn1);
    }
    //
    // All non active pages have been scanned.  Locate the
    // active pages and make sure they are consistent.
    //

    //
    // set bit zero as page zero is reserved for now
    //

    RtlSetBits (CheckPfnBitMap, 0L, 1L);

    Link = RtlFindClearBitsAndSet (CheckPfnBitMap, 1L, 0);
    while (Link != 0xFFFFFFFF) {
        Pfn1 = MI_PFN_ELEMENT (Link);

        //
        // Make sure the PTE address is okay
        //

        if ((Pfn1->PteAddress >= (PMMPTE)HYPER_SPACE)
                && (Pfn1->u3.e1.PrototypePte == 0)) {
            DbgPrint("Pfn with illegal PTE address\n");
            MiFormatPfn(Pfn1);
            break;
        }

        if (Pfn1->PteAddress < (PMMPTE)PTE_BASE) {
            DbgPrint("Pfn with illegal PTE address\n");
            MiFormatPfn(Pfn1);
            break;
        }

#if defined(_IA64_)

        //
        // ignore PTEs mapped to IA64 kernel BAT.
        //

        if (MI_IS_PHYSICAL_ADDRESS(MiGetVirtualAddressMappedByPte(Pfn1->PteAddress))) {

            goto NoCheck;
        }

#endif // _IA64_

        //
        // Check to make sure the referenced PTE is for this page.
        //

        if ((Pfn1->u3.e1.PrototypePte == 1) &&
                            (MmIsAddressValid (Pfn1->PteAddress))) {
            PointerPte = Pfn1->PteAddress;
        } else {
            PointerPte = MiMapPageInHyperSpace (Process,
                                                Pfn1->u4.PteFrame,
                                                &OldIrql);

            PointerPte = (PMMPTE)((ULONG_PTR)PointerPte +
                                    MiGetByteOffset(Pfn1->PteAddress));
        }

        if (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte) != Link) {
            DbgPrint("Invalid PFN - PTE address is wrong in active list\n");
            MiFormatPfn(Pfn1);
            MiFormatPte(PointerPte);
        }
        if (PointerPte->u.Hard.Valid == 0) {
            //
            // if the page is a page table page it could be out of
            // the working set yet a transition page is keeping it
            // around in memory (ups the share count).
            //

            if ((Pfn1->PteAddress < (PMMPTE)PDE_BASE) ||
                (Pfn1->PteAddress > (PMMPTE)PDE_TOP)) {

                DbgPrint("Pte not valid for page on active list\n");
                MiFormatPfn(Pfn1);
                MiFormatPte(PointerPte);
            }
        }

        if (Pfn1->u3.e2.ReferenceCount != 1) {
            DbgPrint("refcount not 1\n");
            MiFormatPfn(Pfn1);
        }


        //
        // Check to make sure the PTE count for the frame is okay.
        //

        if (Pfn1->u3.e1.PrototypePte == 1) {
            PfnX = MI_PFN_ELEMENT(Pfn1->u4.PteFrame);
            for (i = 0; i < 4; i++) {
                if (ValidPage[i] == 0) {
                    ValidPage[i] = (USHORT)Pfn1->u4.PteFrame;
                }
                if (ValidPage[i] == (USHORT)Pfn1->u4.PteFrame) {
                    ValidCheck[i] += 1;
                    break;
                }
            }
        }
        if (OldIrql != 99) {
            MiUnmapPageInHyperSpace (Process, PointerPte, OldIrql);
            OldIrql = 99;
        }

#if defined(_IA64_)

NoCheck:

#endif

        Link = RtlFindClearBitsAndSet (CheckPfnBitMap, 1L, 0);

    }

    for (i = 0; i < 4; i++) {
        if (ValidPage[i] == 0) {
            break;
        }
        PfnX = MI_PFN_ELEMENT(ValidPage[i]);
    }

    UNLOCK_PFN (OldIrql);
    return;

}

VOID
MiDumpPfn ( )

{
    ULONG i;
    PMMPFN Pfn1;

    Pfn1 = MI_PFN_ELEMENT (MmLowestPhysicalPage);

    for (i=0; i < MmNumberOfPhysicalPages; i++) {
        MiFormatPfn (Pfn1);
        Pfn1++;
    }
    return;
}

VOID
MiFormatPfn (
    IN PMMPFN PointerPfn
    )

{

    MMPFN Pfn;
    PFN_NUMBER i;

    Pfn = *PointerPfn;
    i = MI_PFN_ELEMENT_TO_INDEX (PointerPfn);

    DbgPrint("***PFN %lx  flink %p  blink %p  ptecount-refcnt %lx\n",
        i,
        Pfn.u1.Flink,
        Pfn.u2.Blink,
        Pfn.u3.e2.ReferenceCount);

    DbgPrint("   pteaddr %p  originalPTE %p  flags %lx \n",
        Pfn.PteAddress,
        Pfn.OriginalPte,
        Pfn.u3.e2.ShortFlags);

    return;

}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\checkpte.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   checkpte.c

Abstract:

    This module contains routines for sanity checking the page directory.

Author:

    Lou Perazzoli (loup) 25-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#if DBG

VOID
CheckValidPte (
    IN PMMPTE PointerPte
    );


VOID
MiCheckPte (
    VOID
    )

/*++

Routine Description:

    This routine checks each page table page in an address space to
    ensure it is in the proper state.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    ULONG i;
    ULONG j;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPFN Pfn1;
    ULONG ValidCount;
    ULONG TransitionCount;
    KIRQL OldIrql;
    PEPROCESS TargetProcess;
    USHORT UsedPages;

    TargetProcess = PsGetCurrentProcess ();

    PointerPde = MiGetPdeAddress (NULL);

    UsedPages = 0;

    LOCK_WS (TargetProcess);
    LOCK_PFN (OldIrql);

    for (i = 0; i < PDE_PER_PAGE; i += 1) {

        if (PointerPde->u.Hard.Valid) {
            ValidCount = 0;
            TransitionCount = 0;
            CheckValidPte (PointerPde);

            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

            for (j = 0; j < PTE_PER_PAGE; j += 1) {

                if ((PointerPte >= MiGetPteAddress(HYPER_SPACE)) &&
                    (PointerPte < MiGetPteAddress(WORKING_SET_LIST))) {

                    goto endloop;
                }

                if (PointerPte->u.Hard.Valid) {
                    ValidCount += 1;
                    CheckValidPte (PointerPte);
                }
                else {

                    if ((PointerPte->u.Soft.Transition == 1) &&
                        (PointerPte->u.Soft.Prototype == 0)) {

                        //
                        // Transition PTE, up the transition count.
                        //

                        TransitionCount += 1;
                    }
                }

                if (PointerPte->u.Long != 0) {
                    UsedPages += 1;
                }
endloop:
                PointerPte += 1;
            }
            if ((i < 512) || (i == 896)) {
#if !defined (_WIN64)
                if (MmWorkingSetList->UsedPageTableEntries[i] != UsedPages) {
                   DbgPrint("used pages and page table used not equal %lx %lx %lx\n",
                    i,MmWorkingSetList->UsedPageTableEntries[i], UsedPages);
                }
#endif
            }

            //
            // Check the share count for the page table page.
            //
            if ((i < 511) || (i == 896)) {
                Pfn1 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
                if (Pfn1->u2.ShareCount != ((ULONG)1+ValidCount+TransitionCount)) {
                    DbgPrint("share count for page table page bad - %lx %lx %lx\n",
                        i,ValidCount, TransitionCount);
                    MiFormatPfn(Pfn1);
                }
            }
        }
        PointerPde += 1;
        UsedPages = 0;
    }

    UNLOCK_PFN (OldIrql);
    UNLOCK_WS (TargetProcess);

    return;
}

VOID
CheckValidPte (
    IN PMMPTE PointerPte
    )

{
    PMMPFN Pfn1;
    PMMPTE PointerPde;

    if (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte) > MmNumberOfPhysicalPages) {
        return;
    }

    Pfn1 = MI_PFN_ELEMENT(PointerPte->u.Hard.PageFrameNumber);

#if 0
    if (PointerPte->u.Hard.PageFrameNumber == 0) {
        DbgPrint("physical page zero mapped\n");
        MiFormatPte(PointerPte);
        MiFormatPfn(Pfn1);
    }
#endif

    if (Pfn1->u3.e1.PageLocation != ActiveAndValid) {
        DbgPrint("valid PTE with page frame not active and valid\n");
        MiFormatPfn(Pfn1);
        MiFormatPte(PointerPte);
    }

    if (Pfn1->u3.e1.PrototypePte == 0) {

        //
        // This is not a prototype PTE.
        //

        if (Pfn1->PteAddress != PointerPte) {
            DbgPrint("checkpte - Pfn PTE address and PTE address not equal\n");
            MiFormatPte(PointerPte);
            MiFormatPfn(Pfn1);
            return;
        }
    }

    if (!MmIsAddressValid(Pfn1->PteAddress)) {
        return;
    }

    PointerPde = MiGetPteAddress (Pfn1->PteAddress);

    if (PointerPde->u.Hard.Valid == 1) {
        if (MI_GET_PAGE_FRAME_FROM_PTE (PointerPde) != Pfn1->u4.PteFrame) {
            DbgPrint("checkpte - pteframe not right\n");
            MiFormatPfn(Pfn1);
            MiFormatPte(PointerPte);
            MiFormatPte(PointerPde);
        }
    }

    return;
}

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\buildmdl.c ===
/*++

Copyright (c) 1999 Microsoft Corporation

Module Name:

    buildmdl.c

Abstract:

    This module contains the Mm support routines for the cache manager to
    prefetching groups of pages from secondary storage using logical file
    offets instead of virtual addresses.  This saves the cache manager from
    having to map pages unnecessarily.

    The caller builds a list of various file objects and logical block offsets,
    passing them to MmPrefetchPagesIntoLockedMdl.  The code here then examines
    the internal pages, reading in those that are not already valid or in
    transition.  These pages are read with a single read, using a dummy page
    to bridge gaps of pages that were valid or transition prior to the I/O
    being issued.

    Upon conclusion of the I/O, control is returned to the calling thread.
    All pages are referenced counted as though they were probed and locked,
    regardless of whether they are currently valid or transition.

Author:

    Landy Wang (landyw) 12-Feb-2001

Revision History:

--*/

#include "mi.h"

#if DBG

ULONG MiCcDebug;

#define MI_CC_FORCE_PREFETCH    0x1     // Trim all user pages to force prefetch
#define MI_CC_DELAY             0x2     // Delay hoping to trigger collisions

#endif

typedef struct _MI_READ_INFO {

    PCONTROL_AREA ControlArea;
    PFILE_OBJECT FileObject;
    LARGE_INTEGER FileOffset;
    PMMINPAGE_SUPPORT InPageSupport;
    PMDL IoMdl;
    PMDL ApiMdl;
    PMMPFN DummyPagePfn;
    PSUBSECTION FirstReferencedSubsection;
    PSUBSECTION LastReferencedSubsection;
    SIZE_T LengthInBytes;

} MI_READ_INFO, *PMI_READ_INFO;

VOID
MiCcReleasePrefetchResources (
    IN PMI_READ_INFO MiReadInfo,
    IN NTSTATUS Status
    );

NTSTATUS
MiCcPrepareReadInfo (
    IN PMI_READ_INFO MiReadInfo
    );

NTSTATUS
MiCcPutPagesInTransition (
    IN PMI_READ_INFO MiReadInfo
    );

NTSTATUS
MiCcCompletePrefetchIos (
    PMI_READ_INFO MiReadInfo
    );

VOID
MiRemoveUserPages (
    VOID
    );

VOID
MiPfFreeDummyPage (
    IN PMMPFN DummyPagePfn
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text (PAGE, MmPrefetchPagesIntoLockedMdl)
#pragma alloc_text (PAGE, MiCcPrepareReadInfo)
#pragma alloc_text (PAGE, MiCcReleasePrefetchResources)
#endif


NTSTATUS
MmPrefetchPagesIntoLockedMdl (
    IN PFILE_OBJECT FileObject,
    IN PLARGE_INTEGER FileOffset,
    IN SIZE_T Length,
    OUT PMDL *MdlOut
    )

/*++

Routine Description:

    This routine fills an MDL with pages described by the file object's
    offset and length.

    This routine is for cache manager usage only.

Arguments:

    FileObject - Supplies a pointer to the file object for a file which was
                 opened with NO_INTERMEDIATE_BUFFERING clear, i.e., for
                 which CcInitializeCacheMap was called by the file system.

    FileOffset - Supplies the byte offset in the file for the desired data.

    Length - Supplies the length of the desired data in bytes.

    MdlOut - On output it returns a pointer to an Mdl describing
             the desired data.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode. PASSIVE_LEVEL.

--*/

{
    MI_READ_INFO MiReadInfo;
    NTSTATUS status;
    LOGICAL ApcNeeded;
    PETHREAD CurrentThread;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    RtlZeroMemory (&MiReadInfo, sizeof(MiReadInfo));

    MiReadInfo.FileObject = FileObject;
    MiReadInfo.FileOffset = *FileOffset;
    MiReadInfo.LengthInBytes = Length;

    //
    // Prepare for the impending read : allocate MDLs, inpage blocks,
    // reference count subsections, etc.
    //

    status = MiCcPrepareReadInfo (&MiReadInfo);

    if (!NT_SUCCESS (status)) {
        MiCcReleasePrefetchResources (&MiReadInfo, status);
        return status;
    }

    ASSERT (MiReadInfo.InPageSupport != NULL);

    //
    // APCs must be disabled once we put a page in transition.  Otherwise
    // a thread suspend will stop us from issuing the I/O - this will hang
    // any other threads that need the same page.
    //

    CurrentThread = PsGetCurrentThread();
    ApcNeeded = FALSE;

    KeEnterCriticalRegionThread (&CurrentThread->Tcb);

    //
    // The nested fault count protects this thread from deadlocks where a
    // special kernel APC fires and references the same user page(s) we are
    // putting in transition.
    //

    KeEnterGuardedRegionThread (&CurrentThread->Tcb);
    ASSERT (CurrentThread->NestedFaultCount == 0);
    CurrentThread->NestedFaultCount += 1;
    KeLeaveGuardedRegionThread (&CurrentThread->Tcb);

    //
    // Allocate physical memory, lock down all the pages and issue any
    // I/O that may be needed.  When MiCcPutPagesInTransition returns
    // STATUS_SUCCESS or STATUS_ISSUE_PAGING_IO, it guarantees that the
    // ApiMdl contains reference-counted (locked-down) pages.
    //

    status = MiCcPutPagesInTransition (&MiReadInfo);

    if (NT_SUCCESS (status)) {

        //
        // No I/O was issued because all the pages were already resident and
        // have now been locked down.
        //

        ASSERT (MiReadInfo.ApiMdl != NULL);
    }
    else if (status == STATUS_ISSUE_PAGING_IO) {

        //
        // Wait for the I/O to complete.  Note APCs must remain disabled.
        //

        ASSERT (MiReadInfo.InPageSupport != NULL);
    
        status = MiCcCompletePrefetchIos (&MiReadInfo);
    }
    else {

        //
        // Some error occurred (like insufficient memory, etc) so fail
        // the request by falling through.
        //
    }

    //
    // Release acquired resources like pool, subsections, etc.
    //

    MiCcReleasePrefetchResources (&MiReadInfo, status);

    //
    // Only now that the I/O have been completed (not just issued) can
    // APCs be re-enabled.  This prevents a user-issued suspend APC from
    // keeping a shared page in transition forever.
    //

    KeEnterGuardedRegionThread (&CurrentThread->Tcb);

    ASSERT (CurrentThread->NestedFaultCount == 1);

    CurrentThread->NestedFaultCount -= 1;

    if (CurrentThread->ApcNeeded == 1) {
        ApcNeeded = TRUE;
        CurrentThread->ApcNeeded = 0;
    }

    KeLeaveGuardedRegionThread (&CurrentThread->Tcb);

    KeLeaveCriticalRegionThread (&CurrentThread->Tcb);

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);
    ASSERT (CurrentThread->NestedFaultCount == 0);
    ASSERT (CurrentThread->ApcNeeded == 0);

    if (ApcNeeded == TRUE) {
        IoRetryIrpCompletions ();
    }

    *MdlOut = MiReadInfo.ApiMdl;

    return status;
}

VOID
MiCcReleasePrefetchResources (
    IN PMI_READ_INFO MiReadInfo,
    IN NTSTATUS Status
    )

/*++

Routine Description:

    This routine releases all resources consumed to handle a system cache
    logical offset based prefetch.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    PSUBSECTION FirstReferencedSubsection;
    PSUBSECTION LastReferencedSubsection;

    //
    // Release all subsection prototype PTE references. 
    //

    FirstReferencedSubsection = MiReadInfo->FirstReferencedSubsection;
    LastReferencedSubsection = MiReadInfo->LastReferencedSubsection;

    while (FirstReferencedSubsection != LastReferencedSubsection) {
        MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION) FirstReferencedSubsection,
                                         FirstReferencedSubsection->PtesInSubsection);
        FirstReferencedSubsection = FirstReferencedSubsection->NextSubsection;
    }

    if (MiReadInfo->IoMdl != NULL) {
        ExFreePool (MiReadInfo->IoMdl);
    }

    //
    // Note successful returns yield the ApiMdl so don't free it here.
    //

    if (!NT_SUCCESS (Status)) {
        if (MiReadInfo->ApiMdl != NULL) {
            ExFreePool (MiReadInfo->ApiMdl);
        }
    }

    if (MiReadInfo->InPageSupport != NULL) {

#if DBG
        MiReadInfo->InPageSupport->ListEntry.Next = NULL;
#endif

        MiFreeInPageSupportBlock (MiReadInfo->InPageSupport);
    }

    //
    // Put DummyPage back on the free list.
    //

    if (MiReadInfo->DummyPagePfn != NULL) {
        MiPfFreeDummyPage (MiReadInfo->DummyPagePfn);
    }
}


NTSTATUS
MiCcPrepareReadInfo (
    IN PMI_READ_INFO MiReadInfo
    )

/*++

Routine Description:

    This routine constructs MDLs that describe the pages in the argument
    read-list. The caller will then issue the I/O on return.

Arguments:

    MiReadInfo - Supplies a pointer to the read-list.

Return Value:

    Various NTSTATUS codes.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    UINT64 PteOffset;
    NTSTATUS Status;
    PMMPTE ProtoPte;
    PMMPTE LastProto;
    PMMPTE *ProtoPteArray;
    PCONTROL_AREA ControlArea;
    PSUBSECTION Subsection;
    PMMINPAGE_SUPPORT InPageSupport;
    PMDL Mdl;
    PMDL IoMdl;
    PMDL ApiMdl;
    ULONG i;
    PFN_NUMBER NumberOfPages;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (MiReadInfo->FileOffset.LowPart, MiReadInfo->LengthInBytes);

    //
    // Translate the section object into the relevant control area.
    //

    ControlArea = (PCONTROL_AREA)MiReadInfo->FileObject->SectionObjectPointer->DataSectionObject;

    //
    // If the section is backed by a ROM, then there's no need to prefetch
    // anything as it would waste RAM.
    //

    if (ControlArea->u.Flags.Rom == 1) {
		ASSERT (XIPConfigured == TRUE);
        return STATUS_NOT_SUPPORTED;
    }

    //
    // Initialize the internal Mi readlist.
    //

    MiReadInfo->ControlArea = ControlArea;

    //
    // Allocate and initialize an inpage support block for this run.
    //

    InPageSupport = MiGetInPageSupportBlock (MM_NOIRQL, &Status);
    
    if (InPageSupport == NULL) {
        ASSERT (!NT_SUCCESS (Status));
        return Status;
    }
    
    MiReadInfo->InPageSupport = InPageSupport;

    //
    // Allocate and initialize an MDL to return to our caller.  The actual
    // frame numbers are filled in when all the pages are reference counted.
    //

    ApiMdl = MmCreateMdl (NULL, NULL, NumberOfPages << PAGE_SHIFT);

    if (ApiMdl == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    ApiMdl->MdlFlags |= MDL_PAGES_LOCKED;

    MiReadInfo->ApiMdl = ApiMdl;

    //
    // Allocate and initialize an MDL to use for the actual transfer (if any).
    //

    IoMdl = MmCreateMdl (NULL, NULL, NumberOfPages << PAGE_SHIFT);

    if (IoMdl == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    MiReadInfo->IoMdl = IoMdl;
    Mdl = IoMdl;

    //
    // Make sure the section is really prefetchable - physical and
    // pagefile-backed sections are not.
    //

    if ((ControlArea->u.Flags.PhysicalMemory) ||
        (ControlArea->u.Flags.Image == 1) ||
        (ControlArea->FilePointer == NULL)) {

        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // Start the read at the proper file offset.
    //

    InPageSupport->ReadOffset = MiReadInfo->FileOffset;
    ASSERT (BYTE_OFFSET (InPageSupport->ReadOffset.LowPart) == 0);
    InPageSupport->FilePointer = MiReadInfo->FileObject;

    //
    // Stash a pointer to the start of the prototype PTE array (the values
    // in the array are not contiguous as they may cross subsections)
    // in the inpage block so we can walk it quickly later when the pages
    // are put into transition.
    //

    ProtoPteArray = (PMMPTE *)(Mdl + 1);

    InPageSupport->BasePte = (PMMPTE) ProtoPteArray;

    //
    // Data (but not image) reads use the whole page and the filesystems
    // zero fill any remainder beyond valid data length so we don't
    // bother to handle this here.  It is important to specify the
    // entire page where possible so the filesystem won't post this
    // which will hurt perf.  LWFIX: must use CcZero to make this true.
    //

    ASSERT (((ULONG_PTR)Mdl & (sizeof(QUAD) - 1)) == 0);
    InPageSupport->u1.e1.PrefetchMdlHighBits = ((ULONG_PTR)Mdl >> 3);

    //
    // Initialize the prototype PTE pointers.
    //

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    if (ControlArea->u.Flags.Rom == 0) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

#if DBG
    if (MiCcDebug & MI_CC_FORCE_PREFETCH) {
        MiRemoveUserPages ();
    }
#endif

    //
    // Calculate the first prototype PTE address.
    //

    PteOffset = (UINT64)(MiReadInfo->FileOffset.QuadPart >> PAGE_SHIFT);

    //
    // Make sure the PTEs are not in the extended part of the segment.
    //

    while (TRUE) {
            
        //
        // A memory barrier is needed to read the subsection chains
        // in order to ensure the writes to the actual individual
        // subsection data structure fields are visible in correct
        // order.  This avoids the need to acquire any stronger
        // synchronization (ie: PFN lock), thus yielding better
        // performance and pagability.
        //

        KeMemoryBarrier ();

        if (PteOffset < (UINT64) Subsection->PtesInSubsection) {
            break;
        }

        PteOffset -= Subsection->PtesInSubsection;
        Subsection = Subsection->NextSubsection;
    }

    Status = MiAddViewsForSectionWithPfn ((PMSUBSECTION) Subsection,
                                          Subsection->PtesInSubsection);

    if (!NT_SUCCESS (Status)) {
        return Status;
    }

    MiReadInfo->FirstReferencedSubsection = Subsection;
    MiReadInfo->LastReferencedSubsection = Subsection;

    ProtoPte = &Subsection->SubsectionBase[PteOffset];
    LastProto = &Subsection->SubsectionBase[Subsection->PtesInSubsection];

    for (i = 0; i < NumberOfPages; i += 1) {

        //
        // Calculate which PTE maps the given logical block offset.
        //
        // Always look forwards (as an optimization) in the subsection chain.
        //
        // A quick check is made first to avoid recalculations and loops where
        // possible.
        //
    
        if (ProtoPte >= LastProto) {

            //
            // Handle extended subsections.  Increment the view count for
            // every subsection spanned by this request, creating prototype
            // PTEs if needed.
            //

            ASSERT (i != 0);

            Subsection = Subsection->NextSubsection;

            Status = MiAddViewsForSectionWithPfn ((PMSUBSECTION) Subsection,
                                                  Subsection->PtesInSubsection);

            if (!NT_SUCCESS (Status)) {
                return Status;
            }

            MiReadInfo->LastReferencedSubsection = Subsection;

            ProtoPte = Subsection->SubsectionBase;

            LastProto = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
        }

        *ProtoPteArray = ProtoPte;
        ProtoPteArray += 1;

        ProtoPte += 1;
    }

    return STATUS_SUCCESS;
}

NTSTATUS
MiCcPutPagesInTransition (
    IN PMI_READ_INFO MiReadInfo
    )

/*++

Routine Description:

    This routine allocates physical memory for the specified read-list and
    puts all the pages in transition (so collided faults from other threads
    for these same pages remain coherent).  I/O for any pages not already
    resident are issued here.  The caller must wait for their completion.

Arguments:

    MiReadInfo - Supplies a pointer to the read-list.

Return Value:

    STATUS_SUCCESS - all the pages were already resident, reference counts
                     have been applied and no I/O needs to be waited for.

    STATUS_ISSUE_PAGING_IO - the I/O has been issued and the caller must wait.

    Various other failure status values indicate the operation failed.

Environment:

    Kernel mode. PASSIVE_LEVEL.

--*/

{
    NTSTATUS status;
    PMMPTE LocalPrototypePte;
    PVOID StartingVa;
    PFN_NUMBER MdlPages;
    KIRQL OldIrql;
    MMPTE PteContents;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER ResidentAvailableCharge;
    PPFN_NUMBER IoPage;
    PPFN_NUMBER ApiPage;
    PPFN_NUMBER Page;
    PPFN_NUMBER DestinationPage;
    ULONG PageColor;
    PMMPTE PointerPte;
    PMMPTE *ProtoPteArray;
    PMMPTE *EndProtoPteArray;
    PFN_NUMBER DummyPage;
    PMDL Mdl;
    PMDL FreeMdl;
    PMMPFN PfnProto;
    PMMPFN Pfn1;
    PMMPFN DummyPfn1;
    ULONG i;
    PFN_NUMBER DummyTrim;
    ULONG NumberOfPagesNeedingIo;
    MMPTE TempPte;
    PMMPTE PointerPde;
    PEPROCESS CurrentProcess;
    PMMINPAGE_SUPPORT InPageSupport;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    MiReadInfo->DummyPagePfn = NULL;

    FreeMdl = NULL;
    CurrentProcess = PsGetCurrentProcess();

    PfnProto = NULL;
    PointerPde = NULL;

    InPageSupport = MiReadInfo->InPageSupport;
    
    Mdl = MI_EXTRACT_PREFETCH_MDL (InPageSupport);
    ASSERT (Mdl == MiReadInfo->IoMdl);

    IoPage = (PPFN_NUMBER)(Mdl + 1);
    ApiPage = (PPFN_NUMBER)(MiReadInfo->ApiMdl + 1);

    StartingVa = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);
    
    MdlPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                               Mdl->ByteCount);

    if (MdlPages + 1 > MAXUSHORT) {

        //
        // The PFN ReferenceCount for the dummy page could wrap, refuse the
        // request.
        //

        return STATUS_INSUFFICIENT_RESOURCES;
    }

    NumberOfPagesNeedingIo = 0;

    ProtoPteArray = (PMMPTE *)InPageSupport->BasePte;
    EndProtoPteArray = ProtoPteArray + MdlPages;

    ASSERT (*ProtoPteArray != NULL);

    LOCK_PFN (OldIrql);

    //
    // Ensure sufficient pages exist for the transfer plus the dummy page.
    //

    if (((SPFN_NUMBER)MdlPages > (SPFN_NUMBER)(MmAvailablePages - MM_HIGH_LIMIT)) ||
        (MI_NONPAGABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER)MdlPages)) {

        UNLOCK_PFN (OldIrql);

        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Charge resident available immediately as the PFN lock may get released
    // and reacquired below before all the pages have been locked down.
    // Note the dummy page is immediately charged separately.
    //

    MI_DECREMENT_RESIDENT_AVAILABLE (MdlPages, MM_RESAVAIL_ALLOCATE_BUILDMDL);

    ResidentAvailableCharge = MdlPages;

    //
    // Allocate a dummy page to map discarded pages that aren't skipped.
    //

    DummyPage = MiRemoveAnyPage (0);
    Pfn1 = MI_PFN_ELEMENT (DummyPage);

    ASSERT (Pfn1->u2.ShareCount == 0);
    ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

    MiInitializePfnForOtherProcess (DummyPage, MI_PF_DUMMY_PAGE_PTE, 0);

    //
    // Always bias the reference count by 1 and charge for this locked page
    // up front so the myriad increments and decrements don't get slowed
    // down with needless checking.
    //

    Pfn1->u3.e1.PrototypePte = 0;
    MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, TRUE, 42);
    Pfn1->u3.e2.ReferenceCount += 1;

    Pfn1->u3.e1.ReadInProgress = 1;

    MiReadInfo->DummyPagePfn = Pfn1;

    DummyPfn1 = Pfn1;

    DummyPfn1->u3.e2.ReferenceCount =
        (USHORT)(DummyPfn1->u3.e2.ReferenceCount + MdlPages);

    //
    // Properly initialize the inpage support block fields we overloaded.
    //

    InPageSupport->BasePte = *ProtoPteArray;

    //
    // Build the proper InPageSupport and MDL to describe this run.
    //

    for (; ProtoPteArray < EndProtoPteArray; ProtoPteArray += 1, IoPage += 1, ApiPage += 1) {
    
        //
        // Fill the MDL entry for this RLE.
        //
    
        PointerPte = *ProtoPteArray;

        ASSERT (PointerPte != NULL);

        //
        // The PointerPte better be inside a prototype PTE allocation
        // so that subsequent page trims update the correct PTEs.
        //

        ASSERT (((PointerPte >= (PMMPTE)MmPagedPoolStart) &&
                (PointerPte <= (PMMPTE)MmPagedPoolEnd)) ||
                ((PointerPte >= (PMMPTE)MmSpecialPoolStart) && (PointerPte <= (PMMPTE)MmSpecialPoolEnd)));

        //
        // Check the state of this prototype PTE now that the PFN lock is held.
        // If the page is not resident, the PTE must be put in transition with
        // read in progress before the PFN lock is released.
        //

        //
        // Lock page containing prototype PTEs in memory by
        // incrementing the reference count for the page.
        // Unlock any page locked earlier containing prototype PTEs if
        // the containing page is not the same for both.
        //

        if (PfnProto != NULL) {

            if (PointerPde != MiGetPteAddress (PointerPte)) {

                ASSERT (PfnProto->u3.e2.ReferenceCount > 1);
                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnProto, 43);
                PfnProto = NULL;
            }
        }

        if (PfnProto == NULL) {

            ASSERT (!MI_IS_PHYSICAL_ADDRESS (PointerPte));
   
            PointerPde = MiGetPteAddress (PointerPte);
 
            if (PointerPde->u.Hard.Valid == 0) {
                MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
            }

            PfnProto = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
            MI_ADD_LOCKED_PAGE_CHARGE(PfnProto, TRUE, 44);
            PfnProto->u3.e2.ReferenceCount += 1;
            ASSERT (PfnProto->u3.e2.ReferenceCount > 1);
        }

recheck:
        PteContents = *PointerPte;

        // LWFIX: are zero or dzero ptes possible here ?
        ASSERT (PteContents.u.Long != ZeroKernelPte.u.Long);

        if (PteContents.u.Hard.Valid == 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn1->u3.e1.PrototypePte == 1);
            MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, TRUE, 45);
            Pfn1->u3.e2.ReferenceCount += 1;
            *ApiPage = PageFrameIndex;
            *IoPage = DummyPage;
            continue;
        }

        if ((PteContents.u.Soft.Prototype == 0) &&
            (PteContents.u.Soft.Transition == 1)) {

            //
            // The page is in transition.  If there is an inpage still in
            // progress, wait for it to complete.  Reference the PFN and
            // then march on.
            //

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn1->u3.e1.PrototypePte == 1);

            if (Pfn1->u4.InPageError) {

                //
                // There was an in-page read error and there are other
                // threads colliding for this page, delay to let the
                // other threads complete and then retry.
                //

                UNLOCK_PFN (OldIrql);
                KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
                LOCK_PFN (OldIrql);
                goto recheck;
            }

            if (Pfn1->u3.e1.ReadInProgress) {
                    // LWFIX - start with temp\aw.c
            }

            //
            // PTE refers to a normal transition PTE.
            //

            ASSERT ((SPFN_NUMBER)MmAvailablePages >= 0);

            if (MmAvailablePages == 0) {

                //
                // This can only happen if the system is utilizing a hardware
                // compression cache.  This ensures that only a safe amount
                // of the compressed virtual cache is directly mapped so that
                // if the hardware gets into trouble, we can bail it out.
                //

                UNLOCK_PFN (OldIrql);
                KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
                LOCK_PFN (OldIrql);
                goto recheck;
            }

            //
            // The PFN reference count will be 1 already here if the
            // modified writer has begun a write of this page.  Otherwise
            // it's ordinarily 0.
            //

            MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (Pfn1, TRUE, 46);

            Pfn1->u3.e2.ReferenceCount += 1;

            *IoPage = DummyPage;
            *ApiPage = PageFrameIndex;
            continue;
        }

        ASSERT (PteContents.u.Soft.Prototype == 1);

        if ((MmAvailablePages < MM_HIGH_LIMIT) &&
            (MiEnsureAvailablePageOrWait (NULL, NULL, OldIrql))) {

            //
            // Had to wait so recheck all state.
            //

            goto recheck;
        }

        NumberOfPagesNeedingIo += 1;

        //
        // Allocate a physical page.
        //

        PageColor = MI_PAGE_COLOR_VA_PROCESS (
                        MiGetVirtualAddressMappedByPte (PointerPte),
                        &CurrentProcess->NextPageColor);

        PageFrameIndex = MiRemoveAnyPage (PageColor);

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (PointerPte->u.Hard.Valid == 0);

        //
        // Initialize read-in-progress PFN.
        //
    
        MiInitializePfn (PageFrameIndex, PointerPte, 0);

        //
        // These pieces of MiInitializePfn initialization are overridden
        // here as these pages are only going into prototype
        // transition and not into any page tables.
        //

        Pfn1->u3.e1.PrototypePte = 1;
        MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, TRUE, 47);
        Pfn1->u2.ShareCount -= 1;
        Pfn1->u3.e1.PageLocation = ZeroedPageList;

        //
        // Initialize the I/O specific fields.
        //
    
        Pfn1->u1.Event = &InPageSupport->Event;
        Pfn1->u3.e1.ReadInProgress = 1;
        ASSERT (Pfn1->u4.InPageError == 0);

        //
        // Increment the PFN reference count in the control area for
        // the subsection.
        //

        MiReadInfo->ControlArea->NumberOfPfnReferences += 1;
    
        //
        // Put the prototype PTE into the transition state.
        //

        MI_MAKE_TRANSITION_PTE (TempPte,
                                PageFrameIndex,
                                PointerPte->u.Soft.Protection,
                                PointerPte);

        MI_WRITE_INVALID_PTE (PointerPte, TempPte);

        *IoPage = PageFrameIndex;
        *ApiPage = PageFrameIndex;
    }
    
    //
    // If all the pages were resident, dereference the dummy page references
    // now and notify our caller that I/O is not necessary.
    //
    
    if (NumberOfPagesNeedingIo == 0) {
        ASSERT (DummyPfn1->u3.e2.ReferenceCount > MdlPages);
        DummyPfn1->u3.e2.ReferenceCount =
            (USHORT)(DummyPfn1->u3.e2.ReferenceCount - MdlPages);

        //
        // Unlock page containing prototype PTEs.
        //

        if (PfnProto != NULL) {
            ASSERT (PfnProto->u3.e2.ReferenceCount > 1);
            MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnProto, 48);
        }

        UNLOCK_PFN (OldIrql);

        //
        // Return the upfront resident available charge as the
        // individual charges have all been made at this point.
        //

        MI_INCREMENT_RESIDENT_AVAILABLE (ResidentAvailableCharge,
                                         MM_RESAVAIL_FREE_BUILDMDL_EXCESS);

        return STATUS_SUCCESS;
    }

    //
    // Carefully trim leading dummy pages.
    //

    Page = (PPFN_NUMBER)(Mdl + 1);

    DummyTrim = 0;
    for (i = 0; i < MdlPages - 1; i += 1) {
        if (*Page == DummyPage) {
            DummyTrim += 1;
            Page += 1;
        }
        else {
            break;
        }
    }

    if (DummyTrim != 0) {

        Mdl->Size = (USHORT)(Mdl->Size - (DummyTrim * sizeof(PFN_NUMBER)));
        Mdl->ByteCount -= (ULONG)(DummyTrim * PAGE_SIZE);
        ASSERT (Mdl->ByteCount != 0);
        InPageSupport->ReadOffset.QuadPart += (DummyTrim * PAGE_SIZE);
        DummyPfn1->u3.e2.ReferenceCount =
                (USHORT)(DummyPfn1->u3.e2.ReferenceCount - DummyTrim);

        //
        // Shuffle down the PFNs in the MDL.
        // Recalculate BasePte to adjust for the shuffle.
        //

        Pfn1 = MI_PFN_ELEMENT (*Page);

        ASSERT (Pfn1->PteAddress->u.Hard.Valid == 0);
        ASSERT ((Pfn1->PteAddress->u.Soft.Prototype == 0) &&
                 (Pfn1->PteAddress->u.Soft.Transition == 1));

        InPageSupport->BasePte = Pfn1->PteAddress;

        DestinationPage = (PPFN_NUMBER)(Mdl + 1);

        do {
            *DestinationPage = *Page;
            DestinationPage += 1;
            Page += 1;
            i += 1;
        } while (i < MdlPages);

        MdlPages -= DummyTrim;
    }

    //
    // Carefully trim trailing dummy pages.
    //

    ASSERT (MdlPages != 0);

    Page = (PPFN_NUMBER)(Mdl + 1) + MdlPages - 1;

    if (*Page == DummyPage) {

        ASSERT (MdlPages >= 2);

        //
        // Trim the last page specially as it may be a partial page.
        //

        Mdl->Size -= sizeof(PFN_NUMBER);
        if (BYTE_OFFSET(Mdl->ByteCount) != 0) {
            Mdl->ByteCount &= ~(PAGE_SIZE - 1);
        }
        else {
            Mdl->ByteCount -= PAGE_SIZE;
        }
        ASSERT (Mdl->ByteCount != 0);
        DummyPfn1->u3.e2.ReferenceCount -= 1;

        //
        // Now trim any other trailing pages.
        //

        Page -= 1;
        DummyTrim = 0;
        while (Page != ((PPFN_NUMBER)(Mdl + 1))) {
            if (*Page != DummyPage) {
                break;
            }
            DummyTrim += 1;
            Page -= 1;
        }
        if (DummyTrim != 0) {
            ASSERT (Mdl->Size > (USHORT)(DummyTrim * sizeof(PFN_NUMBER)));
            Mdl->Size = (USHORT)(Mdl->Size - (DummyTrim * sizeof(PFN_NUMBER)));
            Mdl->ByteCount -= (ULONG)(DummyTrim * PAGE_SIZE);
            DummyPfn1->u3.e2.ReferenceCount =
                (USHORT)(DummyPfn1->u3.e2.ReferenceCount - DummyTrim);
        }

        ASSERT (MdlPages > DummyTrim + 1);
        MdlPages -= (DummyTrim + 1);

#if DBG
        StartingVa = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);
    
        ASSERT (MdlPages == ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                                               Mdl->ByteCount));
#endif
    }

    //
    // If the MDL is not already embedded in the inpage block, see if its
    // final size qualifies it - if so, embed it now.
    //

    if ((Mdl != &InPageSupport->Mdl) &&
        (Mdl->ByteCount <= (MM_MAXIMUM_READ_CLUSTER_SIZE + 1) * PAGE_SIZE)){

#if DBG
        RtlFillMemoryUlong (&InPageSupport->Page[0],
                            (MM_MAXIMUM_READ_CLUSTER_SIZE+1) * sizeof (PFN_NUMBER),
                            0xf1f1f1f1);
#endif

        RtlCopyMemory (&InPageSupport->Mdl, Mdl, Mdl->Size);

        FreeMdl = Mdl;

        Mdl = &InPageSupport->Mdl;

        ASSERT (((ULONG_PTR)Mdl & (sizeof(QUAD) - 1)) == 0);
        InPageSupport->u1.e1.PrefetchMdlHighBits = ((ULONG_PTR)Mdl >> 3);
    }

    ASSERT (MdlPages != 0);

    ASSERT (Mdl->Size - sizeof(MDL) == BYTES_TO_PAGES(Mdl->ByteCount) * sizeof(PFN_NUMBER));

    DummyPfn1->u3.e2.ReferenceCount =
        (USHORT)(DummyPfn1->u3.e2.ReferenceCount - NumberOfPagesNeedingIo);
    
    //
    // Unlock page containing prototype PTEs.
    //

    if (PfnProto != NULL) {
        ASSERT (PfnProto->u3.e2.ReferenceCount > 1);
        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnProto, 49);
    }

    UNLOCK_PFN (OldIrql);

    InterlockedIncrement ((PLONG) &MmInfoCounters.PageReadIoCount);

    InterlockedExchangeAdd ((PLONG) &MmInfoCounters.PageReadCount,
                            (LONG) NumberOfPagesNeedingIo);

    //
    // Return the upfront resident available charge as the
    // individual charges have all been made at this point.
    //

    MI_INCREMENT_RESIDENT_AVAILABLE (ResidentAvailableCharge,
                                     MM_RESAVAIL_FREE_BUILDMDL_EXCESS);

    if (FreeMdl != NULL) {
        ASSERT (MiReadInfo->IoMdl == FreeMdl);
        MiReadInfo->IoMdl = NULL;
        ExFreePool (FreeMdl);
    }

#if DBG

    if (MiCcDebug & MI_CC_DELAY) {

        //
        // This delay provides a window to increase the chance of collided 
        // faults.
        //

        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
    }

#endif

    //
    // Finish initialization of the prefetch MDL (and the API MDL).
    //
    
    ASSERT ((Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0);
    Mdl->MdlFlags |= (MDL_PAGES_LOCKED | MDL_IO_PAGE_READ);

    ASSERT (InPageSupport->u1.e1.Completed == 0);
    ASSERT (InPageSupport->Thread == PsGetCurrentThread());
    ASSERT64 (InPageSupport->UsedPageTableEntries == 0);
    ASSERT (InPageSupport->WaitCount >= 1);
    ASSERT (InPageSupport->u1.e1.PrefetchMdlHighBits != 0);

    //
    // The API caller expects an MDL containing all the locked pages so
    // it can be used for a transfer.
    //
    // Note that an extra reference count is not taken on each page -
    // rather when the Io MDL completes, its reference counts are not
    // decremented (except for the dummy page).  This combined with the
    // reference count already taken on the resident pages keeps the
    // accounting correct.  Only if an error occurs will the Io MDL
    // completion decrement the reference counts.
    //

    //
    // Initialize the inpage support block Pfn field.
    //

    LocalPrototypePte = InPageSupport->BasePte;

    ASSERT (LocalPrototypePte->u.Hard.Valid == 0);
    ASSERT ((LocalPrototypePte->u.Soft.Prototype == 0) &&
             (LocalPrototypePte->u.Soft.Transition == 1));

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(LocalPrototypePte);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    InPageSupport->Pfn = Pfn1;

    //
    // Issue the paging I/O.
    //

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    status = IoAsynchronousPageRead (InPageSupport->FilePointer,
                                     Mdl,
                                     &InPageSupport->ReadOffset,
                                     &InPageSupport->Event,
                                     &InPageSupport->IoStatus);

    if (!NT_SUCCESS (status)) {

        //
        // Set the event as the I/O system doesn't set it on errors.
        // This way our caller will automatically unroll the PFN reference
        // counts, etc, when the MiWaitForInPageComplete returns this status.
        //

        InPageSupport->IoStatus.Status = status;
        InPageSupport->IoStatus.Information = 0;
        KeSetEvent (&InPageSupport->Event, 0, FALSE);
    }

#if DBG

    if (MiCcDebug & MI_CC_DELAY) {

        //
        // This delay provides a window to increase the chance of collided 
        // faults.
        //

        KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
    }

#endif

    return STATUS_ISSUE_PAGING_IO;
}


NTSTATUS
MiCcCompletePrefetchIos (
    IN PMI_READ_INFO MiReadInfo
    )

/*++

Routine Description:

    This routine waits for a series of page reads to complete
    and completes the requests.

Arguments:

    MiReadInfo - Pointer to the read-list.

Return Value:

    NTSTATUS of the I/O request.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    PMDL Mdl;
    PMMPFN Pfn1;
    PMMPFN PfnClusterPage;
    PPFN_NUMBER Page;
    NTSTATUS status;
    LONG NumberOfBytes;
    PMMINPAGE_SUPPORT InPageSupport;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    InPageSupport = MiReadInfo->InPageSupport;

    ASSERT (InPageSupport->Pfn != 0);

    Pfn1 = InPageSupport->Pfn;
    Mdl = MI_EXTRACT_PREFETCH_MDL (InPageSupport);
    Page = (PPFN_NUMBER)(Mdl + 1);

    status = MiWaitForInPageComplete (InPageSupport->Pfn,
                                      InPageSupport->BasePte,
                                      NULL,
                                      InPageSupport->BasePte,
                                      InPageSupport,
                                      PREFETCH_PROCESS);

    //
    // MiWaitForInPageComplete RETURNS WITH THE PFN LOCK HELD!!!
    //

    NumberOfBytes = (LONG)Mdl->ByteCount;

    while (NumberOfBytes > 0) {

        //
        // Only decrement reference counts if an error occurred.
        //

        PfnClusterPage = MI_PFN_ELEMENT (*Page);

#if DBG
        if (PfnClusterPage->u4.InPageError) {

            //
            // If the page is marked with an error, then the whole transfer
            // must be marked as not successful as well.  The only exception
            // is the prefetch dummy page which is used in multiple
            // transfers concurrently and thus may have the inpage error
            // bit set at any time (due to another transaction besides
            // the current one).
            //

            ASSERT ((status != STATUS_SUCCESS) ||
                    (PfnClusterPage->PteAddress == MI_PF_DUMMY_PAGE_PTE));
        }
#endif
        if (PfnClusterPage->u3.e1.ReadInProgress != 0) {

            ASSERT (PfnClusterPage->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
            PfnClusterPage->u3.e1.ReadInProgress = 0;

            if (PfnClusterPage->u4.InPageError == 0) {
                PfnClusterPage->u1.Event = NULL;
            }
        }

        //
        // Note the reference count for each page is NOT decremented unless
        // the I/O failed, in which case it is done below.  This allows the
        // MmPrefetchPagesIntoLockedMdl API to return a locked page MDL.
        //

        Page += 1;
        NumberOfBytes -= PAGE_SIZE;
    }

    if (status != STATUS_SUCCESS) {

        //
        // An I/O error occurred during the page read
        // operation.  All the pages which were just
        // put into transition must be put onto the
        // free list if InPageError is set, and their
        // PTEs restored to the proper contents.
        //

        Page = (PPFN_NUMBER)(Mdl + 1);
        NumberOfBytes = (LONG)Mdl->ByteCount;

        while (NumberOfBytes > 0) {

            PfnClusterPage = MI_PFN_ELEMENT (*Page);

            MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(PfnClusterPage, 50);

            if (PfnClusterPage->u4.InPageError == 1) {

                if (PfnClusterPage->u3.e2.ReferenceCount == 0) {

                    ASSERT (PfnClusterPage->u3.e1.PageLocation ==
                                                    StandbyPageList);

                    MiUnlinkPageFromList (PfnClusterPage);
                    ASSERT (PfnClusterPage->u3.e2.ReferenceCount == 0);
                    MiRestoreTransitionPte (PfnClusterPage);
                    MiInsertPageInFreeList (*Page);
                }
            }
            Page += 1;
            NumberOfBytes -= PAGE_SIZE;
        }
    }

    //
    // All the relevant prototype PTEs should be in the transition or
    // valid states and all page frames should be referenced.
    // LWFIX: add code to checked build to verify this.
    //

    ASSERT (InPageSupport->WaitCount >= 1);
    UNLOCK_PFN (PASSIVE_LEVEL);

#if DBG
    InPageSupport->ListEntry.Next = NULL;
#endif

    MiFreeInPageSupportBlock (InPageSupport);
    MiReadInfo->InPageSupport = NULL;

    return status;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\compress.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    compress.c

Abstract:

    This module contains the routines to support allow hardware to
    transparently compress physical memory.

Author:

    Landy Wang (landyw) 21-Oct-2000

Revision History:

--*/

#include "mi.h"

#if defined (_MI_COMPRESSION)

Enable the #if 0 code in cmdat3.c to allow Ratio specification.

//
// Compression public interface.
//

#define MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION      0x1

typedef 
NTSTATUS
(*PMM_SET_COMPRESSION_THRESHOLD) (
    IN ULONGLONG CompressionByteThreshold
    );

typedef struct _MM_COMPRESSION_CONTEXT {
    ULONG Version;
    ULONG SizeInBytes;
    ULONGLONG ReservedBytes;
    PMM_SET_COMPRESSION_THRESHOLD SetCompressionThreshold;
} MM_COMPRESSION_CONTEXT, *PMM_COMPRESSION_CONTEXT;

#define MM_COMPRESSION_VERSION_INITIAL  1
#define MM_COMPRESSION_VERSION_CURRENT  1

NTSTATUS
MmRegisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    );

NTSTATUS
MmDeregisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    );

//
// This defaults to 75% but can be overridden in the registry.  At this
// percentage of *real* physical memory in use, an interrupt is generated so
// that memory management can zero pages to make more memory available.
//

#define MI_DEFAULT_COMPRESSION_THRESHOLD    75

ULONG MmCompressionThresholdRatio;

PFN_NUMBER MiNumberOfCompressionPages;

PMM_SET_COMPRESSION_THRESHOLD MiSetCompressionThreshold;

#if DBG
KIRQL MiCompressionIrql;
#endif

//
// Note there is also code in dynmem.c that is dependent on this #define.
//

#if defined (_MI_COMPRESSION_SUPPORTED_)

typedef struct _MI_COMPRESSION_INFO {
    ULONG IsrPageProcessed;
    ULONG DpcPageProcessed;
    ULONG IsrForcedDpc;
    ULONG IsrFailedDpc;

    ULONG IsrRan;
    ULONG DpcRan;
    ULONG DpcsFired;
    ULONG IsrSkippedZeroedPage;

    ULONG DpcSkippedZeroedPage;
    ULONG PfnForcedDpcInsert;
    ULONG PfnFailedDpcInsert;

} MI_COMPRESSION_INFO, *PMI_COMPRESSION_INFO;

MI_COMPRESSION_INFO MiCompressionInfo;      // LWFIX - temp remove.

PFN_NUMBER MiCompressionOverHeadInPages;

PKDPC MiCompressionDpcArray;
CCHAR MiCompressionProcessors;

VOID
MiCompressionDispatch (
    IN PKDPC Dpc,
    IN PVOID DeferredContext,
    IN PVOID SystemArgument1,
    IN PVOID SystemArgument2
    );

PVOID
MiMapCompressionInHyperSpace (
    IN PFN_NUMBER PageFrameIndex
    );

VOID
MiUnmapCompressionInHyperSpace (
    VOID
    );

SIZE_T
MiMakeCompressibleMemoryAtDispatch (
    IN SIZE_T NumberOfBytes OPTIONAL
    );


NTSTATUS
MmRegisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    )

/*++

Routine Description:

    This routine notifies memory management that compression hardware exists
    in the system.  Memory management responds by initializing compression
    support here.

Arguments:

    Context - Supplies the compression context pointer.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    KIRQL OldIrql;
    PFN_NUMBER OverHeadInPages;
    CCHAR Processor;
    CCHAR NumberProcessors;
    PKDPC CompressionDpcArray;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    if (Context->Version != MM_COMPRESSION_VERSION_CURRENT) {
        return STATUS_INVALID_PARAMETER_1;
    }

    if (Context->SizeInBytes < sizeof (MM_COMPRESSION_CONTEXT)) {
        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // If the subsequent hot-add cannot succeed then fail this API now.
    //

    if (MmDynamicPfn == 0) {
        return STATUS_NOT_SUPPORTED;
    }

    //
    // Hardware that can't generate a configurable interrupt is not supported.
    //

    if (Context->SetCompressionThreshold == NULL) {
        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // ReservedBytes indicates the number of reserved bytes required by the
    // underlying hardware.  For example, some hardware might have:
    //                    
    //  1.  translation tables which are 1/64 of the fictional RAM total.
    //
    //  2.  the first MB of memory is never compressed.
    //
    //  3.  an L3 which is never compressed.
    //
    //  etc.
    //
    //  ReservedBytes would be the sum of all of these types of ranges.
    //

    OverHeadInPages = (PFN_COUNT)(Context->ReservedBytes / PAGE_SIZE);

    if (MmResidentAvailablePages < (SPFN_NUMBER) OverHeadInPages) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if (MmAvailablePages < OverHeadInPages) {
        MmEmptyAllWorkingSets ();
        if (MmAvailablePages < OverHeadInPages) {
            return STATUS_INSUFFICIENT_RESOURCES;
        }
    }

    //
    // Create a DPC for every processor in the system as servicing the
    // compression interrupt is critical.
    //

    NumberProcessors = KeNumberProcessors;

    CompressionDpcArray = ExAllocatePoolWithTag (NonPagedPool,
                                             NumberProcessors * sizeof (KDPC),
                                             'pDmM');

    if (CompressionDpcArray == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    for (Processor = 0; Processor < NumberProcessors; Processor += 1) {

        KeInitializeDpc (CompressionDpcArray + Processor, MiCompressionDispatch, NULL);

        //
        // Set importance so this DPC always gets queued at the head.
        //

        KeSetImportanceDpc (CompressionDpcArray + Processor, HighImportance);

        KeSetTargetProcessorDpc (CompressionDpcArray + Processor, Processor);
    }

    LOCK_PFN (OldIrql);

    if (MmCompressionThresholdRatio == 0) {
        MmCompressionThresholdRatio = MI_DEFAULT_COMPRESSION_THRESHOLD;
    }
    else if (MmCompressionThresholdRatio > 100) {
        MmCompressionThresholdRatio = 100;
    }

    if ((MmResidentAvailablePages < (SPFN_NUMBER) OverHeadInPages) ||
        (MmAvailablePages < OverHeadInPages)) {

        UNLOCK_PFN (OldIrql);
        ExFreePool (CompressionDpcArray);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    MI_DECREMENT_RESIDENT_AVAILABLE (OverHeadInPages,
                                     MM_RESAVAIL_ALLOCATE_COMPRESSION);

    MmAvailablePages -= (PFN_COUNT) OverHeadInPages;

    //
    // Signal applications if allocating these pages caused a threshold cross.
    //

    MiNotifyMemoryEvents ();

    //
    // Snap our own copy to prevent busted drivers from causing overcommits
    // if they deregister improperly.
    //

    MiCompressionOverHeadInPages += OverHeadInPages;

    ASSERT (MiNumberOfCompressionPages == 0);

    ASSERT (MiSetCompressionThreshold == NULL);
    MiSetCompressionThreshold = Context->SetCompressionThreshold;

    if (MiCompressionDpcArray == NULL) {
        MiCompressionDpcArray = CompressionDpcArray;
        CompressionDpcArray = NULL;
        MiCompressionProcessors = NumberProcessors;
    }

    UNLOCK_PFN (OldIrql);

    if (CompressionDpcArray != NULL) {
        ExFreePool (CompressionDpcArray);
    }

    return STATUS_SUCCESS;
}


NTSTATUS
MiArmCompressionInterrupt (
    VOID
    )

/*++

Routine Description:

    This routine arms the hardware-generated compression interrupt.

Arguments:

    None.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, PFN lock held.

--*/

{
    NTSTATUS Status;
    PFN_NUMBER RealPages;
    ULONGLONG ByteThreshold;

    MM_PFN_LOCK_ASSERT();

    if (MiSetCompressionThreshold == NULL) {
        return STATUS_SUCCESS;
    }

    RealPages = MmNumberOfPhysicalPages - MiNumberOfCompressionPages - MiCompressionOverHeadInPages;

    ByteThreshold = (RealPages * MmCompressionThresholdRatio) / 100;
    ByteThreshold *= PAGE_SIZE;

    //
    // Note this callout is made with the PFN lock held !
    //

    Status = (*MiSetCompressionThreshold) (ByteThreshold);

    if (!NT_SUCCESS (Status)) {

        //
        // If the hardware fails, all is lost.
        //

        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x61941, 
                      MmNumberOfPhysicalPages,
                      RealPages,
                      MmCompressionThresholdRatio);
    }

    return Status;
}


NTSTATUS
MmDeregisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    )

/*++

Routine Description:

    This routine notifies memory management that compression hardware is
    being removed.  Note the compression driver must have already SUCCESSFULLY
    called MmRemovePhysicalMemoryEx.

Arguments:

    Context - Supplies the compression context pointer.

Return Value:

    STATUS_SUCCESS if compression support is initialized properly.

Environment:

    Kernel mode, PASSIVE_LEVEL.

--*/

{
    KIRQL OldIrql;
    PFN_COUNT OverHeadInPages;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    OverHeadInPages = (PFN_COUNT)(Context->ReservedBytes / PAGE_SIZE);

    LOCK_PFN (OldIrql);

    if (OverHeadInPages > MiCompressionOverHeadInPages) {
        UNLOCK_PFN (OldIrql);
        return STATUS_INVALID_PARAMETER;
    }

    MmAvailablePages += OverHeadInPages;

    //
    // Signal applications if allocating these pages caused a threshold cross.
    //

    MiNotifyMemoryEvents ();

    ASSERT (MiCompressionOverHeadInPages == OverHeadInPages);

    MiCompressionOverHeadInPages -= OverHeadInPages;

    MiSetCompressionThreshold = NULL;

    UNLOCK_PFN (OldIrql);

    MI_INCREMENT_RESIDENT_AVAILABLE (OverHeadInPages, 
                                     MM_RESAVAIL_FREE_COMPRESSION);

    return STATUS_SUCCESS;
}

VOID
MiCompressionDispatch (
    IN PKDPC Dpc,
    IN PVOID DeferredContext,
    IN PVOID SystemArgument1,
    IN PVOID SystemArgument2
    )
/*++

Routine Description:

    Called to make memory compressible if the PFN lock could not be
    acquired during the original device interrupt.

Arguments:

    Dpc - Supplies a pointer to a control object of type DPC.

    SystemArgument1 - Supplies the number of bytes to make compressible.

Return Value:

    None.

Environment:

    Kernel mode.  DISPATCH_LEVEL.

--*/
{
    SIZE_T NumberOfBytes;

    UNREFERENCED_PARAMETER (Dpc);
    UNREFERENCED_PARAMETER (DeferredContext);
    UNREFERENCED_PARAMETER (SystemArgument2);

    NumberOfBytes = (SIZE_T) SystemArgument1;

    MiCompressionInfo.DpcsFired += 1;

    MiMakeCompressibleMemoryAtDispatch (NumberOfBytes);
}

SIZE_T
MmMakeCompressibleMemory (
    IN SIZE_T NumberOfBytes OPTIONAL
    )

/*++

Routine Description:

    This routine attempts to move pages from transition to zero so that
    hardware compression can reclaim the physical memory.

Arguments:

    NumberOfBytes - Supplies the number of bytes to make compressible.
                    Zero indicates as much as possible.

Return Value:

    Returns the number of bytes made compressible.

Environment:

    Kernel mode.  Any IRQL as this is called from device interrupt service
    routines.

--*/

{
    KIRQL OldIrql;
    BOOLEAN Queued;
#if !defined(NT_UP)
    PFN_NUMBER PageFrameIndex;
    MMLISTS MemoryList;
    PMMPFNLIST ListHead;
    PMMPFN Pfn1;
    CCHAR Processor;
    PFN_NUMBER Total;
    PVOID ZeroBase;
    PKPRCB Prcb;
    PFN_NUMBER RequestedPages;
    PFN_NUMBER ActualPages;
    PKSPIN_LOCK_QUEUE LockQueuePfn;
#endif

    //
    // LWFIX: interlocked add in the request size above so overlapping
    // requests can be processed.
    //

    OldIrql = KeGetCurrentIrql();

    if (OldIrql <= DISPATCH_LEVEL) {
        return MiMakeCompressibleMemoryAtDispatch (NumberOfBytes);
    }

#if defined(NT_UP)

    //
    // In uniprocessor configurations, there is no indication as to the PFN lock
    // is owned because the uniprocessor kernel macros these into merely IRQL
    // raises.  Therefore this routine must be conservative when called above
    // DISPATCH_LEVEL and assume the lock is owned and just always queue
    // a DPC in these cases.
    //

    Queued = KeInsertQueueDpc (MiCompressionDpcArray,
                               (PVOID) NumberOfBytes,
                               NULL);

    if (Queued == TRUE) {
        MiCompressionInfo.PfnForcedDpcInsert += 1;
    }
    else {
        MiCompressionInfo.PfnFailedDpcInsert += 1;
    }

    return 0;

#else

#if DBG
    //
    // Make sure this interrupt always comes in at the same device IRQL.
    //

    ASSERT ((MiCompressionIrql == 0) || (OldIrql == MiCompressionIrql));
    MiCompressionIrql = OldIrql;
#endif

    Prcb = KeGetCurrentPrcb();

    RequestedPages = NumberOfBytes >> PAGE_SHIFT;
    ActualPages = 0;

    MemoryList = FreePageList;

    ListHead = MmPageLocationList[MemoryList];

    LockQueuePfn = &Prcb->LockQueue[LockQueuePfnLock];

    if (KeTryToAcquireQueuedSpinLockAtRaisedIrql (LockQueuePfn) == FALSE) {

        //
        // Unable to acquire the spinlock, queue a DPC to pick it up instead.
        //

        for (Processor = 0; Processor < MiCompressionProcessors; Processor += 1) {

            Queued = KeInsertQueueDpc (MiCompressionDpcArray + Processor,
                                       (PVOID) NumberOfBytes,
                                       NULL);
            if (Queued == TRUE) {
                MiCompressionInfo.PfnForcedDpcInsert += 1;
            }
            else {
                MiCompressionInfo.PfnFailedDpcInsert += 1;
            }
        }
        return 0;
    }

    MiCompressionInfo.IsrRan += 1;

    //
    // Run the free and transition list and zero the pages.
    //

    while (MemoryList <= StandbyPageList) {

        Total = ListHead->Total;

        PageFrameIndex = ListHead->Flink;

        while (Total != 0) {

            //
            // Transition pages may need restoration which requires a
            // hyperspace mapping plus control area deletion actions all of
            // which occur at DISPATCH_LEVEL.  So if we're at device IRQL,
            // only do the minimum and queue the rest.
            //

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            if ((Pfn1->u3.e1.InPageError == 1) &&
                (Pfn1->u3.e1.ReadInProgress == 1)) {

                //
                // This page is already zeroed so skip it.
                //

                MiCompressionInfo.IsrSkippedZeroedPage += 1;
            }
            else {

                //
                // Zero the page directly now instead of waiting for the low
                // priority zeropage thread to get a slice.  Note that the
                // slower mapping and zeroing routines are used here because
                // the faster ones are for the zeropage thread only.
                // Maybe we should change this someday.
                //

                ZeroBase = MiMapCompressionInHyperSpace (PageFrameIndex);

                KeZeroPages (ZeroBase, PAGE_SIZE);

                MiUnmapCompressionInHyperSpace ();

                ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

                //
                // Overload ReadInProgress to signify that collided faults that
                // occur before the PTE is completely restored will know to
                // delay and retry until the page (and PTE) are updated.
                //

                Pfn1->u3.e1.InPageError = 1;
                ASSERT (Pfn1->u3.e1.ReadInProgress == 0);
                Pfn1->u3.e1.ReadInProgress = 1;

                ActualPages += 1;

                if (ActualPages == RequestedPages) {
                    MemoryList = StandbyPageList;
                    ListHead = MmPageLocationList[MemoryList];
                    break;
                }
            }

            Total -= 1;
            PageFrameIndex = Pfn1->u1.Flink;
        }
        MemoryList += 1;
        ListHead += 1;
    }

    if (ActualPages != 0) {

        //
        // Rearm the interrupt as pages have now been zeroed.
        //

        MiArmCompressionInterrupt ();
    }

    KeReleaseQueuedSpinLockFromDpcLevel (LockQueuePfn);

    if (ActualPages != 0) {

        //
        // Pages were zeroed - queue a DPC to the current processor to
        // move them to the zero list.  Note this is not critical path so
        // don't bother sending a DPC to every processor for this case.
        //

        MiCompressionInfo.IsrPageProcessed += (ULONG)ActualPages;

        Processor = (CCHAR) KeGetCurrentProcessorNumber ();

        //
        // Ensure a hot-added processor scenario just works.
        //

        if (Processor >= MiCompressionProcessors) {
            Processor = MiCompressionProcessors;
        }

        Queued = KeInsertQueueDpc (MiCompressionDpcArray + Processor,
                                   (PVOID) NumberOfBytes,
                                   NULL);
        if (Queued == TRUE) {
            MiCompressionInfo.IsrForcedDpc += 1;
        }
        else {
            MiCompressionInfo.IsrFailedDpc += 1;
        }
    }

    return (ActualPages << PAGE_SHIFT);
#endif
}

SIZE_T
MiMakeCompressibleMemoryAtDispatch (
    IN SIZE_T NumberOfBytes OPTIONAL
    )

/*++

Routine Description:

    This routine attempts to move pages from transition to zero so that
    hardware compression can reclaim the physical memory.

Arguments:

    NumberOfBytes - Supplies the number of bytes to make compressible.
                    Zero indicates as much as possible.

Return Value:

    Returns the number of bytes made compressible.

Environment:

    Kernel mode.  DISPATCH_LEVEL.

--*/

{
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageFrameIndex2;
    PVOID ZeroBase;
    PMMPFN Pfn1;
    MMLISTS MemoryList;
    PMMPFNLIST ListHead;
    PFN_NUMBER RequestedPages;
    PFN_NUMBER ActualPages;
    LOGICAL NeedToZero;

    ASSERT (KeGetCurrentIrql () == DISPATCH_LEVEL);

    RequestedPages = NumberOfBytes >> PAGE_SHIFT;
    ActualPages = 0;

    MemoryList = FreePageList;
    ListHead = MmPageLocationList[MemoryList];

    MiCompressionInfo.DpcRan += 1;

    LOCK_PFN2 (OldIrql);

    //
    // Run the free and transition list and zero the pages.
    //

    while (MemoryList <= StandbyPageList) {

        while (ListHead->Total != 0) {

            //
            // Before removing the page from the head of the list (which will
            // zero the flag bits), snap whether it's been zeroed by our ISR
            // or whether we need to zero it here.
            //

            PageFrameIndex = ListHead->Flink;
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            NeedToZero = TRUE;
            if ((Pfn1->u3.e1.InPageError == 1) && (Pfn1->u3.e1.ReadInProgress == 1)) {
                MiCompressionInfo.DpcSkippedZeroedPage += 1;
                NeedToZero = FALSE;
            }

            //
            // Transition pages may need restoration which requires a
            // hyperspace mapping plus control area deletion actions all of
            // which occur at DISPATCH_LEVEL.  Since we're at DISPATCH_LEVEL
            // now, go ahead and do it.
            //

            PageFrameIndex2 = MiRemovePageFromList (ListHead);
            ASSERT (PageFrameIndex == PageFrameIndex2);

            //
            // Zero the page directly now instead of waiting for the low
            // priority zeropage thread to get a slice.  Note that the
            // slower mapping and zeroing routines are used here because
            // the faster ones are for the zeropage thread only.
            // Maybe we should change this someday.
            //

            if (NeedToZero == TRUE) {
                ZeroBase = MiMapCompressionInHyperSpace (PageFrameIndex);

                KeZeroPages (ZeroBase, PAGE_SIZE);

                MiUnmapCompressionInHyperSpace ();
            }

            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

            MiInsertPageInList (&MmZeroedPageListHead, PageFrameIndex);

            //
            // We have changed (zeroed) the contents of this page.
            // If memory mirroring is in progress, the bitmap must be updated.
            //

            if (MiMirroringActive == TRUE) {
                RtlSetBit (MiMirrorBitMap2, (ULONG)PageFrameIndex);
            }

            MiCompressionInfo.DpcPageProcessed += 1;
            ActualPages += 1;

            if (ActualPages == RequestedPages) {
                MemoryList = StandbyPageList;
                ListHead = MmPageLocationList[MemoryList];
                break;
            }
        }
        MemoryList += 1;
        ListHead += 1;
    }

    //
    // Rearm the interrupt as pages have now been zeroed.
    //

    MiArmCompressionInterrupt ();

    UNLOCK_PFN2 (OldIrql);

    return (ActualPages << PAGE_SHIFT);
}

PVOID
MiMapCompressionInHyperSpace (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure maps the specified physical page into the
    PTE within hyper space reserved explicitly for compression page
    mapping.

    The PTE is guaranteed to always be available since the PFN lock is held.

Arguments:

    PageFrameIndex - Supplies the physical page number to map.

Return Value:

    Returns the virtual address where the specified physical page was
    mapped.

Environment:

    Kernel mode, PFN lock held, any IRQL.

--*/

{
    MMPTE TempPte;
    PMMPTE PointerPte;
    PVOID FlushVaPointer;

    ASSERT (PageFrameIndex != 0);

    TempPte = ValidPtePte;
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    FlushVaPointer = (PVOID) (ULONG_PTR) COMPRESSION_MAPPING_PTE;

    //
    // Ensure both modified and accessed bits are set so the hardware doesn't
    // ever write this PTE.
    //

    ASSERT (TempPte.u.Hard.Dirty == 1);
    ASSERT (TempPte.u.Hard.Accessed == 1);

    PointerPte = MiGetPteAddress (COMPRESSION_MAPPING_PTE);
    ASSERT (PointerPte->u.Long == 0);

    //
    // Only flush the TB on the current processor as no context switch can
    // occur while using this mapping.
    //

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    KeFlushSingleTb (FlushVaPointer, FALSE);

    return (PVOID) MiGetVirtualAddressMappedByPte (PointerPte);
}

__forceinline
VOID
MiUnmapCompressionInHyperSpace (
    VOID
    )

/*++

Routine Description:

    This procedure unmaps the PTE reserved for mapping the compression page.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, PFN lock held, any IRQL.

--*/

{
    PMMPTE PointerPte;

    PointerPte = MiGetPteAddress (COMPRESSION_MAPPING_PTE);

    //
    // Capture the number of waiters.
    //

    ASSERT (PointerPte->u.Long != 0);

    MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

    return;
}
#else
NTSTATUS
MmRegisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    )
{
    UNREFERENCED_PARAMETER (Context);

    return STATUS_NOT_SUPPORTED;
}

NTSTATUS
MmDeregisterCompressionDevice (
    IN PMM_COMPRESSION_CONTEXT Context
    )
{
    UNREFERENCED_PARAMETER (Context);

    return STATUS_NOT_SUPPORTED;
}
SIZE_T
MmMakeCompressibleMemory (
    IN SIZE_T NumberOfBytes OPTIONAL
    )
{
    UNREFERENCED_PARAMETER (NumberOfBytes);

    return 0;
}
NTSTATUS
MiArmCompressionInterrupt (
    VOID
    )
{
    return STATUS_NOT_SUPPORTED;
}
#endif

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\deleteva.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   deleteva.c

Abstract:

    This module contains the routines for deleting virtual address space.

Author:

    Lou Perazzoli (loup) 11-May-1989
    Landy Wang (landyw) 02-June-1997

--*/

#include "mi.h"

#if defined (_WIN64) && defined (DBG_VERBOSE)

typedef struct _MI_TRACK_USE {

    PFN_NUMBER Pfn;
    PVOID Va;
    ULONG Id;
    ULONG PfnUse;
    ULONG PfnUseCounted;
    ULONG TickCount;
    PKTHREAD Thread;
    PEPROCESS Process;
} MI_TRACK_USE, *PMI_TRACK_USE;

ULONG MiTrackUseSize = 8192;
PMI_TRACK_USE MiTrackUse;
LONG MiTrackUseIndex;

VOID
MiInitUseCounts (
    VOID
    )
{
    MiTrackUse = ExAllocatePoolWithTag (NonPagedPool,
                                        MiTrackUseSize * sizeof (MI_TRACK_USE),
                                        'lqrI');
    ASSERT (MiTrackUse != NULL);
}


VOID
MiCheckUseCounts (
    PVOID TempHandle,
    PVOID Va,
    ULONG Id
    )

/*++

Routine Description:

    This routine ensures that all the counters are correct.

Arguments:

    TempHandle - Supplies the handle for used page table counts.

    Va - Supplies the virtual address.

    Id - Supplies the ID.

Return Value:

    None.

Environment:

    Kernel mode, called with APCs disabled, working set mutex and PFN lock
    held.

--*/

{
    LOGICAL LogIt;
    ULONG i;
    ULONG TempHandleCount;
    ULONG TempCounted;
    PMMPTE TempPage;
    KIRQL OldIrql;
    ULONG Index;
    PFN_NUMBER PageFrameIndex;
    PMI_TRACK_USE Information;
    LARGE_INTEGER TimeStamp;
    PMMPFN Pfn1;
    PEPROCESS Process;

    Process = PsGetCurrentProcess ();

    //
    // TempHandle is really the PMMPFN containing the UsedPageTableEntries.
    //

    Pfn1 = (PMMPFN)TempHandle;
    PageFrameIndex = MI_PFN_ELEMENT_TO_INDEX (Pfn1);

    TempHandleCount = MI_GET_USED_PTES_FROM_HANDLE (TempHandle);

    if (Id & 0x80000000) {
        ASSERT (TempHandleCount != 0);
    }

    TempPage = (PMMPTE) MiMapPageInHyperSpace (Process, PageFrameIndex, &OldIrql);

    TempCounted = 0;

    for (i = 0; i < PTE_PER_PAGE; i += 1) {
        if (TempPage->u.Long != 0) {
            TempCounted += 1;
        }
        TempPage += 1;
    }

#if 0
    if (zz & 0x4) {
        LogIt = FALSE;
        if (Pfn1->PteFrame == PageFrameIndex) {
            // TopLevel parent page, not interesting to us.
        }
        else {
            PMMPFN Pfn2;

            Pfn2 = MI_PFN_ELEMENT (Pfn1->PteFrame);
            if (Pfn2->PteFrame == Pfn1->PteFrame) {
                // our parent is the toplevel, so very interesting.
                LogIt = TRUE;
            }
        }
    }
    else {
        LogIt = TRUE;
    }
#else
    LogIt = TRUE;
#endif

    if (LogIt == TRUE) {

        //
        // Capture information
        //

        Index = InterlockedExchangeAdd(&MiTrackUseIndex, 1);                    
        Index &= (MiTrackUseSize - 1);

        Information = &(MiTrackUse[Index]);

        Information->Thread = KeGetCurrentThread();
        Information->Process = (PEPROCESS)((ULONG_PTR)PsGetCurrentProcess () + KeGetCurrentProcessorNumber ());
        Information->Va = Va;
        Information->Id = Id;
        KeQueryTickCount(&TimeStamp);
        Information->TickCount = TimeStamp.LowPart;
        Information->Pfn = PageFrameIndex;
        Information->PfnUse = TempHandleCount;
        Information->PfnUseCounted = TempCounted;

        if (TempCounted != TempHandleCount) {
            DbgPrint ("MiCheckUseCounts %p %x %x %x %x\n", Va, Id, PageFrameIndex, TempHandleCount, TempCounted);
            DbgBreakPoint ();
        }
    }

    MiUnmapPageInHyperSpace (Process, TempPage, OldIrql);
    return;
}
#endif


VOID
MiDeleteVirtualAddresses (
    IN PUCHAR Va,
    IN PUCHAR EndingAddress,
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This routine deletes the specified virtual address range within
    the current process.

Arguments:

    Va - Supplies the first virtual address to delete.

    EndingAddress - Supplies the last address to delete.

    Vad - Supplies the virtual address descriptor which maps this range
          or NULL if we are not concerned about views.  From the Vad the
          range of prototype PTEs is determined and this information is
          used to uncover if the PTE refers to a prototype PTE or a fork PTE.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL, called with address space and working set mutexes
    held.  The working set mutex may be released and reacquired to fault
    pages in.

--*/

{
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    WSLE_NUMBER WsPfnIndex;
    WSLE_NUMBER WorkingSetIndex;
    MMWSLENTRY Locked;
    WSLE_NUMBER Entry;
    ULONG InvalidPtes;
    LOGICAL PfnHeld;
    PVOID TempVa;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE ProtoPte;
    PMMPTE LastProtoPte;
    PMMPTE LastPte;
    PMMPTE LastPteThisPage;
    MMPTE TempPte;
    PEPROCESS CurrentProcess;
    PSUBSECTION Subsection;
    PVOID UsedPageTableHandle;
    KIRQL OldIrql;
    MMPTE_FLUSH_LIST FlushList;
    ULONG Waited;
    LOGICAL Skipped;
    LOGICAL AddressSpaceDeletion;
#if DBG
    PMMPTE ProtoPte2;
    PMMPTE LastProtoPte2;
    PMMCLONE_BLOCK CloneBlock;
    PMMCLONE_DESCRIPTOR CloneDescriptor;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PVOID UsedPageDirectoryHandle;
    PVOID TempHandle;
#endif

    FlushList.Count = 0;

    CurrentProcess = PsGetCurrentProcess ();

    PointerPpe = MiGetPpeAddress (Va);
    PointerPde = MiGetPdeAddress (Va);
    PointerPte = MiGetPteAddress (Va);
    PointerPxe = MiGetPxeAddress (Va);
    LastPte = MiGetPteAddress (EndingAddress);
    PfnHeld = FALSE;
    OldIrql = MM_NOIRQL;

    SATISFY_OVERZEALOUS_COMPILER (Subsection = NULL);

    if ((Vad == NULL) ||
        (Vad->u.VadFlags.PrivateMemory) ||
        (Vad->FirstPrototypePte == NULL)) {

        ProtoPte = NULL;
        LastProtoPte = NULL;
    }
    else {
        ProtoPte = Vad->FirstPrototypePte;
        LastProtoPte = (PMMPTE) 4;
    }

    if (CurrentProcess->CloneRoot == NULL) {
        AddressSpaceDeletion = TRUE;
    }
    else {
        AddressSpaceDeletion = FALSE;
    }

    do {

        //
        // Attempt to leap forward skipping over empty page directories
        // and page tables where possible.
        //

#if (_MI_PAGING_LEVELS >= 3)
restart:
#endif

        Skipped = FALSE;

        while (MiDoesPxeExistAndMakeValid (PointerPxe,
                                           CurrentProcess,
                                           MM_NOIRQL,
                                           &Waited) == FALSE) {
    
            //
            // This extended page directory parent entry is empty,
            // go to the next one.
            //
    
            Skipped = TRUE;
            PointerPxe += 1;
            PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);
    
            if (Va > EndingAddress) {
    
                //
                // All done, return.
                //
    
                return;
            }
        }
    
        while (MiDoesPpeExistAndMakeValid (PointerPpe,
                                           CurrentProcess,
                                           MM_NOIRQL,
                                           &Waited) == FALSE) {
    
            //
            // This page directory parent entry is empty, go to the next one.
            //
    
            Skipped = TRUE;
            PointerPpe += 1;
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);
    
            if (Va > EndingAddress) {
    
                //
                // All done, return.
                //
    
                return;
            }
#if (_MI_PAGING_LEVELS >= 4)
            if (MiIsPteOnPdeBoundary (PointerPpe)) {
                PointerPxe += 1;
                ASSERT (PointerPxe == MiGetPteAddress (PointerPpe));
                goto restart;
            }
#endif
        }

#if (_MI_PAGING_LEVELS >= 3) && defined (DBG)
        MI_CHECK_USED_PTES_HANDLE (PointerPte);
        TempHandle = MI_GET_USED_PTES_HANDLE (PointerPte);
        ASSERT ((MI_GET_USED_PTES_FROM_HANDLE (TempHandle) != 0) ||
                (PointerPde->u.Long == 0));
#endif

        while (MiDoesPdeExistAndMakeValid (PointerPde,
                                           CurrentProcess,
                                           MM_NOIRQL,
                                           &Waited) == FALSE) {
    
            //
            // This page directory entry is empty, go to the next one.
            //
    
            Skipped = TRUE;
            PointerPde += 1;
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);
    
            if (Va > EndingAddress) {
    
                //
                // All done, return.
                //
    
                return;
            }
    
#if (_MI_PAGING_LEVELS >= 3)
            if (MiIsPteOnPdeBoundary (PointerPde)) {
                PointerPpe += 1;
                ASSERT (PointerPpe == MiGetPteAddress (PointerPde));
                PointerPxe = MiGetPteAddress (PointerPpe);
                goto restart;
            }
#endif

#if DBG
            if ((LastProtoPte != NULL)  &&
                (Vad->u2.VadFlags2.ExtendableFile == 0)) {

                ProtoPte2 = MiGetProtoPteAddress (Vad, MI_VA_TO_VPN (Va));
                Subsection = MiLocateSubsection (Vad,MI_VA_TO_VPN (Va));
                LastProtoPte2 = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
                if (Vad->u.VadFlags.ImageMap != 1) {
                    if ((ProtoPte2 < Subsection->SubsectionBase) ||
                        (ProtoPte2 >= LastProtoPte2)) {
                        DbgPrint ("bad proto PTE %p va %p Vad %p sub %p\n",
                            ProtoPte2,Va,Vad,Subsection);
                        DbgBreakPoint();
                    }
                }
            }
#endif
        }
    
        //
        // The PPE and PDE are now valid, get the page table use address
        // as it changes whenever the PDE does.
        //

#if (_MI_PAGING_LEVELS >= 4)
        ASSERT64 (PointerPxe->u.Hard.Valid == 1);
#endif
        ASSERT64 (PointerPpe->u.Hard.Valid == 1);
        ASSERT (PointerPde->u.Hard.Valid == 1);
        ASSERT (Va <= EndingAddress);

        MI_CHECK_USED_PTES_HANDLE (Va);
        UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (Va);

#if (_MI_PAGING_LEVELS >= 3) && defined (DBG)
        ASSERT ((MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) != 0) ||
                (PointerPte->u.Long == 0));
#endif

        //
        // If we skipped chunks of address space, the prototype PTE pointer
        // must be updated now so VADs that span multiple subsections
        // are handled properly.
        //

        if ((Skipped == TRUE) && (LastProtoPte != NULL)) {

            ProtoPte = MiGetProtoPteAddress (Vad, MI_VA_TO_VPN(Va));
            Subsection = MiLocateSubsection (Vad, MI_VA_TO_VPN(Va));

            if (Subsection != NULL) {
                LastProtoPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
#if DBG
                if (Vad->u.VadFlags.ImageMap != 1) {
                    if ((ProtoPte < Subsection->SubsectionBase) ||
                        (ProtoPte >= LastProtoPte)) {
                        DbgPrint ("bad proto PTE %p va %p Vad %p sub %p\n",
                                    ProtoPte,Va,Vad,Subsection);
                        DbgBreakPoint();
                    }
                }
#endif
            }
            else {

                //
                // The Vad span is larger than the section being mapped.
                // Null the proto PTE local as no more proto PTEs will
                // need to be deleted at this point.
                //

                LastProtoPte = NULL;
            }
        }

        //
        // A valid address has been located, examine and delete each PTE.
        //

        InvalidPtes = 0;

        if (AddressSpaceDeletion == TRUE) {

            //
            // The working set mutex is held so no valid PTEs can be trimmed.
            // Take advantage of this fact and remove the WSLEs for all valid
            // PTEs now since the PFN lock is not held.  This is only done
            // for non-forked processes as deletion of forked PTEs below may
            // drop the working set mutex which would introduce races wth
            // WSLE deletion being done here.
            //
            // The deleting of the PTEs will require the PFN lock.
            //
    
            ASSERT (CurrentProcess->CloneRoot == NULL);

            LastPteThisPage = (PMMPTE)(((ULONG_PTR)PointerPte | (PAGE_SIZE - 1)) + 1) - 1;
            if (LastPteThisPage > LastPte) {
                LastPteThisPage = LastPte;
            }
    
            TempVa = MiGetVirtualAddressMappedByPte (LastPteThisPage);
    
            do {
                TempPte = *LastPteThisPage;
        
                if (TempPte.u.Hard.Valid != 0) {
#ifdef _X86_
#if DBG
#if !defined(NT_UP)
    
                    if (TempPte.u.Hard.Writable == 1) {
                        ASSERT (TempPte.u.Hard.Dirty == 1);
                    }
#endif //NTUP
#endif //DBG
#endif //X86
    
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&TempPte);
                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                    WsPfnIndex = Pfn1->u1.WsIndex;
    
                    //
                    // PTE is valid - find the WSLE for this page and eliminate it.
                    //
    
                    WorkingSetIndex = MiLocateWsle (TempVa,
                                                    MmWorkingSetList,
                                                    WsPfnIndex);
    
                    ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);
    
                    //
                    // Check to see if this entry is locked in the working set
                    // or locked in memory.
                    //
    
                    Locked = MmWsle[WorkingSetIndex].u1.e1;
    
                    MiRemoveWsle (WorkingSetIndex, MmWorkingSetList);
    
                    //
                    // Add this entry to the list of free working set entries
                    // and adjust the working set count.
                    //
    
                    MiReleaseWsle (WorkingSetIndex, &CurrentProcess->Vm);
    
                    if ((Locked.LockedInWs == 1) || (Locked.LockedInMemory == 1)) {
    
                        //
                        // This entry is locked.
                        //
    
                        ASSERT (WorkingSetIndex < MmWorkingSetList->FirstDynamic);
                        MmWorkingSetList->FirstDynamic -= 1;
    
                        if (WorkingSetIndex != MmWorkingSetList->FirstDynamic) {
    
                            Entry = MmWorkingSetList->FirstDynamic;
                            ASSERT (MmWsle[Entry].u1.e1.Valid);
    
                            MiSwapWslEntries (Entry,
                                              WorkingSetIndex,
                                              &CurrentProcess->Vm,
                                              FALSE);
                        }
                    }
                    else {
                        ASSERT (WorkingSetIndex >= MmWorkingSetList->FirstDynamic);
                    }
                }
        
                LastPteThisPage -= 1;
                TempVa = (PVOID)((ULONG_PTR)TempVa - PAGE_SIZE);
    
            } while (LastPteThisPage >= PointerPte);
        }

        do {
    
            TempPte = *PointerPte;
    
            if (TempPte.u.Long != 0) {
    
                //
                // One less used page table entry in this page table page.
                //
    
                MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
    
                if (IS_PTE_NOT_DEMAND_ZERO (TempPte)) {
    
                    if (LastProtoPte != NULL) {
                        if (ProtoPte >= LastProtoPte) {
                            ProtoPte = MiGetProtoPteAddress (Vad, MI_VA_TO_VPN(Va));
                            Subsection = MiLocateSubsection (Vad, MI_VA_TO_VPN(Va));
    
                            //
                            // Subsection may be NULL if this PTE contains the
                            // "search the VAD tree" encoding and the
                            // corresponding prototype PTE is in the unused
                            // PTE range of the segment - ie: a thread tried
                            // to reach beyond the end of his section,
                            // we encode the PTE this way initially during
                            // the fault processing and then later during
                            // the fault give the thread the AV - we don't
                            // clear the PTE encoding then, so we have to
                            // handle it now.
                            //
    
                            if (Subsection != NULL) {
                                LastProtoPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
                            }
                            else {
    
                                //
                                // No more prototype PTEs need to be deleted
                                // as we've passed the end of the valid portion
                                // of the segment, so clear LastProtoPte.
                                //
    
                                LastProtoPte = NULL;
                            }
                        }
#if DBG
                        if ((Vad->u.VadFlags.ImageMap != 1) && (LastProtoPte != NULL)) {
                            if ((ProtoPte < Subsection->SubsectionBase) ||
                                (ProtoPte >= LastProtoPte)) {
                                DbgPrint ("bad proto PTE %p va %p Vad %p sub %p\n",
                                            ProtoPte,Va,Vad,Subsection);
                                DbgBreakPoint();
                            }
                        }
#endif
                    }
    
                    if ((TempPte.u.Hard.Valid == 0) &&
                        (TempPte.u.Soft.Prototype == 1) &&
                        (AddressSpaceDeletion == TRUE)) {
    
                        //
                        // Pure (ie: not forked) prototype PTEs don't need PFN
                        // protection to be deleted.
                        //
    
                        ASSERT (CurrentProcess->CloneRoot == NULL);

#if DBG
                        if ((PointerPte <= MiHighestUserPte) &&
                            (TempPte.u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED) &&
                            (ProtoPte != MiPteToProto (PointerPte))) {
    
                            //
                            // Make sure the prototype PTE is a fork
                            // prototype PTE.
                            //
    
                            CloneBlock = (PMMCLONE_BLOCK)MiPteToProto (PointerPte);
                            CloneDescriptor = MiLocateCloneAddress (CurrentProcess, (PVOID)CloneBlock);
    
                            if (CloneDescriptor == NULL) {
                                DbgPrint("0PrototypePte %p Clone desc %p\n",
                                    PrototypePte, CloneDescriptor);
                                MiFormatPte (PointerPte);
                                ASSERT (FALSE);
                            }
                        }
#endif
    
                        InvalidPtes += 1;
                        MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
                    }
                    else {
                        if (PfnHeld == FALSE) {
                            PfnHeld = TRUE;
                            LOCK_PFN (OldIrql);
                        }
        
                        Waited = MiDeletePte (PointerPte,
                                              (PVOID)Va,
                                              AddressSpaceDeletion,
                                              CurrentProcess,
                                              ProtoPte,
                                              &FlushList,
                                              OldIrql);
                
#if (_MI_PAGING_LEVELS >= 3)
        
                        //
                        // This must be recalculated here if MiDeletePte
                        // dropped the PFN lock (this can happen when
                        // dealing with POSIX forked pages).  Since the
                        // used PTE count is kept in the PFN entry
                        // which could have been paged out and replaced
                        // during this window, recomputation of its
                        // address (not the contents) is necessary.
                        //
        
                        if (Waited != 0) {
                            MI_CHECK_USED_PTES_HANDLE (Va);
                            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (Va);
                        }
#endif
        
                        InvalidPtes = 0;
                    }
                }
                else {
                    InvalidPtes += 1;
                    MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
                }
            }
            else {
                InvalidPtes += 1;
            }
    
            if (InvalidPtes == 16) {
    
                if (PfnHeld == TRUE) {
    
                    if (FlushList.Count != 0) {
                        MiFlushPteList (&FlushList, FALSE);
                    }
    
                    ASSERT (OldIrql != MM_NOIRQL);
                    UNLOCK_PFN (OldIrql);
                    PfnHeld = FALSE;
                }
                else {
                    ASSERT (FlushList.Count == 0);
                }
    
                InvalidPtes = 0;
            }
    
            Va += PAGE_SIZE;
            PointerPte += 1;
            ProtoPte += 1;
    
            ASSERT64 (PointerPpe->u.Hard.Valid == 1);
            ASSERT (PointerPde->u.Hard.Valid == 1);
    
            //
            // If not at the end of a page table and still within the specified
            // range, then just march directly on to the next PTE.
            //
    
        }
        while ((!MiIsVirtualAddressOnPdeBoundary(Va)) && (Va <= EndingAddress));

        //
        // The virtual address is on a page directory boundary:
        //
        // 1. Flush the PTEs for the previous page table page.
        // 2. Delete the previous page directory & page table if appropriate.
        // 3. Attempt to leap forward skipping over empty page directories
        //    and page tables where possible.
        //

        //
        // If all the entries have been eliminated from the previous
        // page table page, delete the page table page itself.
        //
    
        if (PfnHeld == TRUE) {
            if (FlushList.Count != 0) {
                MiFlushPteList (&FlushList, FALSE);
            }
        }
        else {
            ASSERT (FlushList.Count == 0);
        }

        //
        // If all the entries have been eliminated from the previous
        // page table page, delete the page table page itself.
        //

        ASSERT64 (PointerPpe->u.Hard.Valid == 1);
        ASSERT (PointerPde->u.Hard.Valid == 1);

#if (_MI_PAGING_LEVELS >= 3)
        MI_CHECK_USED_PTES_HANDLE (PointerPte - 1);
#endif

        if ((MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) &&
            (PointerPde->u.Long != 0)) {

            if (PfnHeld == FALSE) {
                PfnHeld = TRUE;
                LOCK_PFN (OldIrql);
            }

#if (_MI_PAGING_LEVELS >= 3)
            UsedPageDirectoryHandle = MI_GET_USED_PTES_HANDLE (PointerPte - 1);
            MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryHandle);
#endif

            TempVa = MiGetVirtualAddressMappedByPte(PointerPde);
            MiDeletePte (PointerPde,
                         TempVa,
                         FALSE,
                         CurrentProcess,
                         NULL,
                         NULL,
                         OldIrql);

#if (_MI_PAGING_LEVELS >= 3)
            if ((MI_GET_USED_PTES_FROM_HANDLE (UsedPageDirectoryHandle) == 0) &&
                (PointerPpe->u.Long != 0)) {
    
#if (_MI_PAGING_LEVELS >= 4)
                UsedPageDirectoryHandle = MI_GET_USED_PTES_HANDLE (PointerPde);
                MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryHandle);
#endif

                TempVa = MiGetVirtualAddressMappedByPte(PointerPpe);
                MiDeletePte (PointerPpe,
                             TempVa,
                             FALSE,
                             CurrentProcess,
                             NULL,
                             NULL,
                             OldIrql);

#if (_MI_PAGING_LEVELS >= 4)
                if ((MI_GET_USED_PTES_FROM_HANDLE (UsedPageDirectoryHandle) == 0) &&
                    (PointerPxe->u.Long != 0)) {

                    TempVa = MiGetVirtualAddressMappedByPte(PointerPxe);
                    MiDeletePte (PointerPxe,
                                 TempVa,
                                 FALSE,
                                 CurrentProcess,
                                 NULL,
                                 NULL,
                                 OldIrql);
                }
#endif
    
            }
#endif
            ASSERT (OldIrql != MM_NOIRQL);
            UNLOCK_PFN (OldIrql);
            PfnHeld = FALSE;
        }

        if (PfnHeld == TRUE) {
            ASSERT (OldIrql != MM_NOIRQL);
            UNLOCK_PFN (OldIrql);
            PfnHeld = FALSE;
        }

        if (Va > EndingAddress) {
        
            //
            // All done, return.
            //
        
            return;
        }

        PointerPde = MiGetPdeAddress (Va);
        PointerPpe = MiGetPpeAddress (Va);
        PointerPxe = MiGetPxeAddress (Va);

    } while (TRUE);
}


ULONG
MiDeletePte (
    IN PMMPTE PointerPte,
    IN PVOID VirtualAddress,
    IN ULONG AddressSpaceDeletion,
    IN PEPROCESS CurrentProcess,
    IN PMMPTE PrototypePte,
    IN PMMPTE_FLUSH_LIST PteFlushList OPTIONAL,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine deletes the contents of the specified PTE.  The PTE
    can be in one of the following states:

        - active and valid
        - transition
        - in paging file
        - in prototype PTE format

Arguments:

    PointerPte - Supplies a pointer to the PTE to delete.

    VirtualAddress - Supplies the virtual address which corresponds to
                     the PTE.  This is used to locate the working set entry
                     to eliminate it.

    AddressSpaceDeletion - Supplies TRUE if the address space is being
                           deleted, FALSE otherwise.  If TRUE is specified
                           the TB is not flushed and valid addresses are
                           not removed from the working set.


    CurrentProcess - Supplies a pointer to the current process.

    PrototypePte - Supplies a pointer to the prototype PTE which currently
                   or originally mapped this page.  This is used to determine
                   if the PTE is a fork PTE and should have its reference block
                   decremented.

    PteFlushList - Supplies a flush list to use if the TB flush can be
                   deferred to the caller.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

Return Value:

    Nonzero if this routine released mutexes and locks, FALSE if not.

Environment:

    Kernel mode, APCs disabled, PFN lock and working set mutex held.

--*/

{
    PMMPTE PointerPde;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    MMPTE PteContents;
    WSLE_NUMBER WorkingSetIndex;
    WSLE_NUMBER Entry;
    MMWSLENTRY Locked;
    WSLE_NUMBER WsPfnIndex;
    PMMCLONE_BLOCK CloneBlock;
    PMMCLONE_DESCRIPTOR CloneDescriptor;
    ULONG DroppedLocks;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;

    MM_PFN_LOCK_ASSERT();

    DroppedLocks = 0;

#if DBG
    if (MmDebug & MM_DBG_PTE_UPDATE) {
        DbgPrint("deleting PTE\n");
        MiFormatPte (PointerPte);
    }
#endif

    PteContents = *PointerPte;

    if (PteContents.u.Hard.Valid == 1) {

#ifdef _X86_
#if DBG
#if !defined(NT_UP)

        if (PteContents.u.Hard.Writable == 1) {
            ASSERT (PteContents.u.Hard.Dirty == 1);
        }
#endif //NTUP
#endif //DBG
#endif //X86

        //
        // PTE is valid.  Check PFN database to see if this is a prototype PTE.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE(&PteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        WsPfnIndex = Pfn1->u1.WsIndex;

#if DBG
        if (MmDebug & MM_DBG_PTE_UPDATE) {
            MiFormatPfn(Pfn1);
        }
#endif

        CloneDescriptor = NULL;

        if (Pfn1->u3.e1.PrototypePte == 1) {

            CloneBlock = (PMMCLONE_BLOCK)Pfn1->PteAddress;

            //
            // Capture the state of the modified bit for this PTE.
            //

            MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);

            //
            // Decrement the share and valid counts of the page table
            // page which maps this PTE.
            //

            PointerPde = MiGetPteAddress (PointerPte);
            if (PointerPde->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
                if (!NT_SUCCESS(MiCheckPdeForPagedPool (PointerPte))) {
#endif
                    KeBugCheckEx (MEMORY_MANAGEMENT,
                                  0x61940, 
                                  (ULONG_PTR) PointerPte,
                                  (ULONG_PTR) PointerPde->u.Long,
                                  (ULONG_PTR) VirtualAddress);
#if (_MI_PAGING_LEVELS < 3)
                }
#endif
            }

            PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE(PointerPde);
            Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

            //
            // Decrement the share count for the physical page.
            //

            MiDecrementShareCountInline (Pfn1, PageFrameIndex);

            //
            // Check to see if this is a fork prototype PTE and if so
            // update the clone descriptor address.
            //

            if (PointerPte <= MiHighestUserPte) {

                if (PrototypePte != Pfn1->PteAddress) {

                    //
                    // Locate the clone descriptor within the clone tree.
                    //

                    CloneDescriptor = MiLocateCloneAddress (CurrentProcess, (PVOID)CloneBlock);

                    if (CloneDescriptor == NULL) {
                        KeBugCheckEx (MEMORY_MANAGEMENT,
                                      0x400, 
                                      (ULONG_PTR) PointerPte,
                                      (ULONG_PTR) PrototypePte,
                                      (ULONG_PTR) Pfn1->PteAddress);
                    }
                }
            }
        }
        else {

            if ((PMMPTE)((ULONG_PTR)Pfn1->PteAddress & ~0x1) != PointerPte) {
                KeBugCheckEx (MEMORY_MANAGEMENT,
                              0x401, 
                              (ULONG_PTR) PointerPte,
                              (ULONG_PTR) PointerPte->u.Long,
                              (ULONG_PTR) Pfn1->PteAddress);
            }

            //
            // Initializing CloneBlock & PointerPde is not needed for
            // correctness but without it the compiler cannot compile this code
            // W4 to check for use of uninitialized variables.
            //

            CloneBlock = NULL;
            PointerPde = NULL;

            ASSERT (Pfn1->u2.ShareCount == 1);

            //
            // This PTE is NOT a prototype PTE, delete the physical page.
            //
            // Decrement the share and valid counts of the page table
            // page which maps this PTE.
            //

            MiDecrementShareCountInline (MI_PFN_ELEMENT(Pfn1->u4.PteFrame),
                                         Pfn1->u4.PteFrame);

            MI_SET_PFN_DELETED (Pfn1);

            //
            // Decrement the share count for the physical page.  As the page
            // is private it will be put on the free list.
            //

            MiDecrementShareCount (Pfn1, PageFrameIndex);

            //
            // Decrement the count for the number of private pages.
            //

            CurrentProcess->NumberOfPrivatePages -= 1;
        }

        //
        // Find the WSLE for this page and eliminate it.
        //
        // If we are deleting the system portion of the address space, do
        // not remove WSLEs or flush translation buffers as there can be
        // no other usage of this address space.
        //

        if (AddressSpaceDeletion == FALSE) {

            WorkingSetIndex = MiLocateWsle (VirtualAddress,
                                            MmWorkingSetList,
                                            WsPfnIndex);

            ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);

            //
            // Check to see if this entry is locked in the working set
            // or locked in memory.
            //

            Locked = MmWsle[WorkingSetIndex].u1.e1;

            MiRemoveWsle (WorkingSetIndex, MmWorkingSetList);

            //
            // Add this entry to the list of free working set entries
            // and adjust the working set count.
            //

            MiReleaseWsle (WorkingSetIndex, &CurrentProcess->Vm);

            if ((Locked.LockedInWs == 1) || (Locked.LockedInMemory == 1)) {

                //
                // This entry is locked.
                //

                ASSERT (WorkingSetIndex < MmWorkingSetList->FirstDynamic);
                MmWorkingSetList->FirstDynamic -= 1;

                if (WorkingSetIndex != MmWorkingSetList->FirstDynamic) {

                    Entry = MmWorkingSetList->FirstDynamic;
                    ASSERT (MmWsle[Entry].u1.e1.Valid);

                    MiSwapWslEntries (Entry,
                                      WorkingSetIndex,
                                      &CurrentProcess->Vm,
                                      FALSE);
                }
            }
            else {
                ASSERT (WorkingSetIndex >= MmWorkingSetList->FirstDynamic);
            }
        }

        MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

        //
        // Flush the entry out of the TB.
        //

        if (!ARGUMENT_PRESENT (PteFlushList)) {
            KeFlushSingleTb (VirtualAddress, FALSE);
        }
        else {
            if (PteFlushList->Count != MM_MAXIMUM_FLUSH_COUNT) {
                PteFlushList->FlushVa[PteFlushList->Count] = VirtualAddress;
                PteFlushList->Count += 1;
            }
        }

        if (AddressSpaceDeletion == FALSE) {

            if (CloneDescriptor != NULL) {

                //
                // Flush TBs as this clone path could release the PFN lock.
                //

                if (ARGUMENT_PRESENT (PteFlushList)) {
                    MiFlushPteList (PteFlushList, FALSE);
                }

                //
                // Decrement the reference count for the clone block,
                // note that this could release and reacquire
                // the mutexes hence cannot be done until after the
                // working set entry has been removed.
                //

                if (MiDecrementCloneBlockReference (CloneDescriptor,
                                                    CloneBlock,
                                                    CurrentProcess,
                                                    OldIrql)) {

                    //
                    // The working set mutex was released, so the current
                    // current page directory & table page may have been
                    // paged out (note they cannot be deleted because the
                    // address space mutex is always held throughout).
                    //

                    DroppedLocks = 1;

                    //
                    // Ensure the PDE (and any table above it) are still
                    // resident.
                    //

                    MiMakePdeExistAndMakeValid (PointerPde,
                                                CurrentProcess,
                                                OldIrql);
                }
            }
        }
    }
    else if (PteContents.u.Soft.Prototype == 1) {

        //
        // This is a prototype PTE, if it is a fork PTE clean up the
        // fork structures.
        //

        if ((PteContents.u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED) &&
            (PointerPte <= MiHighestUserPte) &&
            (PrototypePte != MiPteToProto (PointerPte))) {

            CloneBlock = (PMMCLONE_BLOCK) MiPteToProto (PointerPte);
            CloneDescriptor = MiLocateCloneAddress (CurrentProcess,
                                                    (PVOID) CloneBlock);


            if (CloneDescriptor == NULL) {
                KeBugCheckEx (MEMORY_MANAGEMENT,
                              0x403, 
                              (ULONG_PTR) PointerPte,
                              (ULONG_PTR) PrototypePte,
                              (ULONG_PTR) PteContents.u.Long);
            }

            //
            // Decrement the reference count for the clone block,
            // note that this could release and reacquire
            // the mutexes.
            //

            MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

            if (ARGUMENT_PRESENT (PteFlushList)) {
                MiFlushPteList (PteFlushList, FALSE);
            }

            if (MiDecrementCloneBlockReference (CloneDescriptor,
                                                CloneBlock,
                                                CurrentProcess,
                                                OldIrql)) {

                //
                // The working set mutex was released, so the current
                // current page directory & table page may have been
                // paged out (note they cannot be deleted because the
                // address space mutex is always held throughout).
                //

                DroppedLocks = 1;

                //
                // Ensure the PDE (and any table above it) are still
                // resident.
                //

                PointerPde = MiGetPteAddress (PointerPte);

                MiMakePdeExistAndMakeValid (PointerPde,
                                            CurrentProcess,
                                            OldIrql);
            }
        }
    }
    else if (PteContents.u.Soft.Transition == 1) {

        //
        // This is a transition PTE. (Page is private)
        //

        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

        if ((PMMPTE)((ULONG_PTR)Pfn1->PteAddress & ~0x1) != PointerPte) {
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x402, 
                          (ULONG_PTR) PointerPte,
                          (ULONG_PTR) PointerPte->u.Long,
                          (ULONG_PTR) Pfn1->PteAddress);
        }

        MI_SET_PFN_DELETED (Pfn1);

        PageTableFrameIndex = Pfn1->u4.PteFrame;
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

        //
        // Check the reference count for the page, if the reference
        // count is zero, move the page to the free list, if the reference
        // count is not zero, ignore this page.  When the reference count
        // goes to zero, it will be placed on the free list.
        //

        if (Pfn1->u3.e2.ReferenceCount == 0) {
            MiUnlinkPageFromList (Pfn1);
            MiReleasePageFileSpace (Pfn1->OriginalPte);
            MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents));
        }

        //
        // Decrement the count for the number of private pages.
        //

        CurrentProcess->NumberOfPrivatePages -= 1;

    }
    else {

        //
        // Must be page file space.
        //

        if (PteContents.u.Soft.PageFileHigh != MI_PTE_LOOKUP_NEEDED) {

            if (MiReleasePageFileSpace (PteContents)) {

                //
                // Decrement the count for the number of private pages.
                //

                CurrentProcess->NumberOfPrivatePages -= 1;
            }
        }
    }

    //
    // Zero the PTE contents.
    //

    MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

    return DroppedLocks;
}


VOID
MiDeleteValidSystemPte (
    IN PMMPTE PointerPte,
    IN PVOID VirtualAddress,
    IN PMMSUPPORT WsInfo,
    IN PMMPTE_FLUSH_LIST PteFlushList OPTIONAL
    )

/*++

Routine Description:

    This routine deletes the contents of the specified valid system or
    session PTE.  The PTE *MUST* be valid and private (ie: not a prototype).

Arguments:

    PointerPte - Supplies a pointer to the PTE to delete.

    VirtualAddress - Supplies the virtual address which corresponds to
                     the PTE.  This is used to locate the working set entry
                     to eliminate it.

    WsInfo - Supplies the working set structure to update.

    PteFlushList - Supplies a flush list to use if the TB flush can be
                   deferred to the caller.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, PFN lock and working set mutex held.

--*/

{
    PMMPFN Pfn1;
    MMPTE PteContents;
    WSLE_NUMBER WorkingSetIndex;
    WSLE_NUMBER Entry;
    MMWSLENTRY Locked;
    WSLE_NUMBER WsPfnIndex;
    PFN_NUMBER PageFrameIndex;
    PMMWSL WorkingSetList;
    PMMWSLE Wsle;

    MM_PFN_LOCK_ASSERT();

    PteContents = *PointerPte;

    ASSERT (PteContents.u.Hard.Valid == 1);

#ifdef _X86_
#if DBG
#if !defined(NT_UP)

    if (PteContents.u.Hard.Writable == 1) {
        ASSERT (PteContents.u.Hard.Dirty == 1);
    }
#endif //NTUP
#endif //DBG
#endif //X86

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE(&PteContents);
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    WsPfnIndex = Pfn1->u1.WsIndex;

    ASSERT (Pfn1->u3.e1.PrototypePte == 0);

    if ((PMMPTE)((ULONG_PTR)Pfn1->PteAddress & ~0x1) != PointerPte) {
        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x401, 
                      (ULONG_PTR) PointerPte,
                      (ULONG_PTR) PointerPte->u.Long,
                      (ULONG_PTR) Pfn1->PteAddress);
    }

    ASSERT (Pfn1->u2.ShareCount == 1);

    //
    // This PTE is NOT a prototype PTE, delete the physical page.
    //
    // Decrement the share and valid counts of the page table
    // page which maps this PTE.
    //

    MiDecrementShareCountInline (MI_PFN_ELEMENT(Pfn1->u4.PteFrame),
                                 Pfn1->u4.PteFrame);

    MI_SET_PFN_DELETED (Pfn1);

    //
    // Decrement the share count for the physical page.  As the page
    // is private it will be put on the free list.
    //

    MiDecrementShareCount (Pfn1, PageFrameIndex);

    //
    // Find the WSLE for this page and eliminate it.
    //
    // If we are deleting the system portion of the address space, do
    // not remove WSLEs or flush translation buffers as there can be
    // no other usage of this address space.
    //

    WorkingSetList = WsInfo->VmWorkingSetList;

    WorkingSetIndex = MiLocateWsle (VirtualAddress,
                                    WorkingSetList,
                                    WsPfnIndex);

    ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);

    //
    // Check to see if this entry is locked in the working set
    // or locked in memory.
    //

    Wsle = WorkingSetList->Wsle;

    Locked = Wsle[WorkingSetIndex].u1.e1;

    MiRemoveWsle (WorkingSetIndex, WorkingSetList);

    //
    // Add this entry to the list of free working set entries
    // and adjust the working set count.
    //

    MiReleaseWsle (WorkingSetIndex, WsInfo);

    if ((Locked.LockedInWs == 1) || (Locked.LockedInMemory == 1)) {

        //
        // This entry is locked.
        //

        ASSERT (WorkingSetIndex < WorkingSetList->FirstDynamic);
        WorkingSetList->FirstDynamic -= 1;

        if (WorkingSetIndex != WorkingSetList->FirstDynamic) {

            Entry = WorkingSetList->FirstDynamic;
            ASSERT (Wsle[Entry].u1.e1.Valid);

            MiSwapWslEntries (Entry, WorkingSetIndex, WsInfo, FALSE);
        }
    }
    else {
        ASSERT (WorkingSetIndex >= WorkingSetList->FirstDynamic);
    }

    //
    // Zero the PTE contents.
    //

    MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

    //
    // Flush the entry out of the TB.
    //

    if (!ARGUMENT_PRESENT (PteFlushList)) {
        if (WsInfo == &MmSystemCacheWs) {
            KeFlushSingleTb (VirtualAddress, TRUE);
        }
        else {
            MI_FLUSH_SINGLE_SESSION_TB (VirtualAddress);
        }
    }
    else {
        if (PteFlushList->Count != MM_MAXIMUM_FLUSH_COUNT) {
            PteFlushList->FlushVa[PteFlushList->Count] = VirtualAddress;
            PteFlushList->Count += 1;
        }
    }

    if (WsInfo->Flags.SessionSpace == 1) {
        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_HASH_SHRINK, 1);
        InterlockedExchangeAddSizeT (&MmSessionSpace->NonPagablePages, -1);
        MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_WS_HASHPAGE_FREE, 1);
        InterlockedExchangeAddSizeT (&MmSessionSpace->CommittedPages, -1);
    }

    return;
}


ULONG
FASTCALL
MiReleasePageFileSpace (
    IN MMPTE PteContents
    )

/*++

Routine Description:

    This routine frees the paging file allocated to the specified PTE.

Arguments:

    PteContents - Supplies the PTE which is in page file format.

Return Value:

    Returns TRUE if any paging file space was deallocated.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    ULONG FreeBit;
    ULONG PageFileNumber;
    PMMPAGING_FILE PageFile;

    MM_PFN_LOCK_ASSERT();

    if (PteContents.u.Soft.Prototype == 1) {

        //
        // Not in page file format.
        //

        return FALSE;
    }

    FreeBit = GET_PAGING_FILE_OFFSET (PteContents);

    if ((FreeBit == 0) || (FreeBit == MI_PTE_LOOKUP_NEEDED)) {

        //
        // Page is not in a paging file, just return.
        //

        return FALSE;
    }

    PageFileNumber = GET_PAGING_FILE_NUMBER (PteContents);

    ASSERT (RtlCheckBit( MmPagingFile[PageFileNumber]->Bitmap, FreeBit) == 1);

#if DBG
    if ((FreeBit < 8192) && (PageFileNumber == 0)) {
        ASSERT ((MmPagingFileDebug[FreeBit] & 1) != 0);
        MmPagingFileDebug[FreeBit] ^= 1;
    }
#endif

    PageFile = MmPagingFile[PageFileNumber];
    MI_CLEAR_BIT (PageFile->Bitmap->Buffer, FreeBit);

    PageFile->FreeSpace += 1;
    PageFile->CurrentUsage -= 1;

    //
    // Check to see if we should move some MDL entries for the
    // modified page writer now that more free space is available.
    //

    if ((MmNumberOfActiveMdlEntries == 0) ||
        (PageFile->FreeSpace == MM_USABLE_PAGES_FREE)) {

        MiUpdateModifiedWriterMdls (PageFileNumber);
    }

    return TRUE;
}


VOID
FASTCALL
MiUpdateModifiedWriterMdls (
    IN ULONG PageFileNumber
    )

/*++

Routine Description:

    This routine ensures the MDLs for the specified paging file
    are in the proper state such that paging i/o can continue.

Arguments:

    PageFileNumber - Supplies the page file number to check the MDLs for.

Return Value:

    None.

Environment:

    Kernel mode, PFN lock held.

--*/

{
    ULONG i;
    PMMMOD_WRITER_MDL_ENTRY WriterEntry;

    //
    // Put the MDL entries into the active list.
    //

    for (i = 0; i < MM_PAGING_FILE_MDLS; i += 1) {

        if (MmPagingFile[PageFileNumber]->Entry[i]->CurrentList ==
            &MmFreePagingSpaceLow) {

            ASSERT (MmPagingFile[PageFileNumber]->Entry[i]->Links.Flink !=
                                                    MM_IO_IN_PROGRESS);

            //
            // Remove this entry and put it on the active list.
            //

            WriterEntry = MmPagingFile[PageFileNumber]->Entry[i];
            RemoveEntryList (&WriterEntry->Links);
            WriterEntry->CurrentList = &MmPagingFileHeader.ListHead;

            KeSetEvent (&WriterEntry->PagingListHead->Event, 0, FALSE);

            InsertTailList (&WriterEntry->PagingListHead->ListHead,
                            &WriterEntry->Links);
            MmNumberOfActiveMdlEntries += 1;
        }
    }

    return;
}

VOID
MiFlushPteList (
    IN PMMPTE_FLUSH_LIST PteFlushList,
    IN ULONG AllProcessors
    )

/*++

Routine Description:

    This routine flushes all the PTEs in the PTE flush list.
    If the list has overflowed, the entire TB is flushed.

Arguments:

    PteFlushList - Supplies a pointer to the list to be flushed.

    AllProcessors - Supplies TRUE if the flush occurs on all processors.

Return Value:

    None.

Environment:

    Kernel mode, PFN or a pre-process AWE lock may optionally be held.

--*/

{
    ULONG count;

    ASSERT (ARGUMENT_PRESENT (PteFlushList));

    count = PteFlushList->Count;

    if (count != 0) {
        if (count != 1) {
            if (count < MM_MAXIMUM_FLUSH_COUNT) {
                KeFlushMultipleTb (count,
                                   &PteFlushList->FlushVa[0],
                                   (BOOLEAN)AllProcessors);
            }
            else {

                //
                // Array has overflowed, flush the entire TB.
                //

                if (AllProcessors != FALSE) {
                    KeFlushEntireTb (TRUE, TRUE);
                }
                else {
                    KeFlushProcessTb (FALSE);
                }
            }
        }
        else {
            KeFlushSingleTb (PteFlushList->FlushVa[0],
                             (BOOLEAN)AllProcessors);
        }
        PteFlushList->Count = 0;
    }
    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\creasect.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   creasect.c

Abstract:

    This module contains the routines which implement the
    NtCreateSection and NtOpenSection.

Author:

    Lou Perazzoli (loup) 22-May-1989
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/

#include "mi.h"

const ULONG MMCONTROL = 'aCmM';
const ULONG MMTEMPORARY = 'xxmM';

#define MM_SIZE_OF_LARGEST_IMAGE ((ULONG)0x77000000)

#define MM_MAXIMUM_IMAGE_HEADER (2 * PAGE_SIZE)

extern SIZE_T MmAllocationFragment;

//
// The maximum number of image object (object table entries) is
// the number which will fit into the MM_MAXIMUM_IMAGE_HEADER with
// the start of the PE image header in the last word of the first.
//

#define MM_MAXIMUM_IMAGE_SECTIONS                       \
     ((MM_MAXIMUM_IMAGE_HEADER - (PAGE_SIZE + sizeof(IMAGE_NT_HEADERS))) /  \
            sizeof(IMAGE_SECTION_HEADER))

#if DBG
ULONG MiMakeImageFloppy[2];
ULONG_PTR MiMatchSectionBase;
#endif

extern POBJECT_TYPE IoFileObjectType;

CCHAR MmImageProtectionArray[16] = {
                                    MM_NOACCESS,
                                    MM_EXECUTE,
                                    MM_READONLY,
                                    MM_EXECUTE_READ,
                                    MM_WRITECOPY,
                                    MM_EXECUTE_WRITECOPY,
                                    MM_WRITECOPY,
                                    MM_EXECUTE_WRITECOPY,
                                    MM_NOACCESS,
                                    MM_EXECUTE,
                                    MM_READONLY,
                                    MM_EXECUTE_READ,
                                    MM_READWRITE,
                                    MM_EXECUTE_READWRITE,
                                    MM_READWRITE,
                                    MM_EXECUTE_READWRITE };


VOID
MiUpdateImageHeaderPage (
    IN PMMPTE PointerPte,
    IN PFN_NUMBER PageFrameNumber,
    IN PCONTROL_AREA ControlArea,
    IN LOGICAL MarkModified
    );

CCHAR
MiGetImageProtection (
    IN ULONG SectionCharacteristics
    );

NTSTATUS
MiVerifyImageHeader (
    IN PIMAGE_NT_HEADERS NtHeader,
    IN PIMAGE_DOS_HEADER DosHeader,
    IN ULONG NtHeaderSize
    );

LOGICAL
MiCheckDosCalls (
    IN PIMAGE_OS2_HEADER Os2Header,
    IN ULONG HeaderSize
    );

PCONTROL_AREA
MiFindImageSectionObject (
    IN PFILE_OBJECT File,
    IN PLOGICAL GlobalNeeded
    );

VOID
MiInsertImageSectionObject (
    IN PFILE_OBJECT File,
    IN PCONTROL_AREA ControlArea
    );

LOGICAL
MiFlushDataSection (
    IN PFILE_OBJECT File
    );

PVOID
MiCopyHeaderIfResident (
    IN PFILE_OBJECT File,
    IN PFN_NUMBER ImagePageFrameNumber
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MiCreateImageFileMap)
#pragma alloc_text(PAGE,NtCreateSection)
#pragma alloc_text(PAGE,NtOpenSection)
#pragma alloc_text(PAGE,MiGetImageProtection)
#pragma alloc_text(PAGE,MiVerifyImageHeader)
#pragma alloc_text(PAGE,MiCheckDosCalls)
#pragma alloc_text(PAGE,MiCreatePagingFileMap)
#pragma alloc_text(PAGE,MiCreateDataFileMap)
#endif

#pragma pack (1)
typedef struct _PHARLAP_CONFIG {
    UCHAR  uchCopyRight[0x32];
    USHORT usType;
    USHORT usRsv1;
    USHORT usRsv2;
    USHORT usSign;
} CONFIGPHARLAP, *PCONFIGPHARLAP;
#pragma pack ()


NTSTATUS
NtCreateSection (
    OUT PHANDLE SectionHandle,
    IN ACCESS_MASK DesiredAccess,
    IN POBJECT_ATTRIBUTES ObjectAttributes OPTIONAL,
    IN PLARGE_INTEGER MaximumSize OPTIONAL,
    IN ULONG SectionPageProtection,
    IN ULONG AllocationAttributes,
    IN HANDLE FileHandle OPTIONAL
     )

/*++

Routine Description:

    This function creates a section object and opens a handle to the object
    with the specified desired access.

Arguments:

    SectionHandle - A pointer to a variable that will
        receive the section object handle value.

    DesiredAccess - The desired types of access for the section.

    DesiredAccess Flags

         EXECUTE - Execute access to the section is
              desired.

         READ - Read access to the section is desired.

         WRITE - Write access to the section is desired.


    ObjectAttributes - Supplies a pointer to an object attributes structure.

    MaximumSize - Supplies the maximum size of the section in bytes.
         This value is rounded up to the host page size and
         specifies the size of the section (page file
         backed section) or the maximum size to which a
         file can be extended or mapped (file backed
         section).

    SectionPageProtection - Supplies the protection to place on each page
         in the section.  One of PAGE_READ, PAGE_READWRITE, PAGE_EXECUTE,
         or PAGE_WRITECOPY and, optionally, PAGE_NOCACHE may be specified.

    AllocationAttributes - Supplies a set of flags that describe the
         allocation attributes of the section.

        AllocationAttributes Flags

        SEC_BASED - The section is a based section and will be
            allocated at the same virtual address in each process
            address space that receives the section.  This does not
            imply that addresses are reserved for based sections.
            Rather if the section cannot be mapped at the based address
            an error is returned.


        SEC_RESERVE - All pages of the section are set to the
            reserved state.

        SEC_COMMIT - All pages of the section are set to the commit
            state.

        SEC_IMAGE - The file specified by the file handle is an
                    executable image file.

        SEC_FILE - The file specified by the file handle is a mapped
                   file.  If a file handle is supplied and neither
                   SEC_IMAGE or SEC_FILE is supplied, SEC_FILE is
                   assumed.

        SEC_NO_CHANGE - Once the file is mapped, the protection cannot
                        be changed nor can the view be unmapped.  The
                        view is unmapped when the process is deleted.
                        Cannot be used with SEC_IMAGE.

    FileHandle - Supplies an optional handle of an open file object.
         If the value of this handle is null, then the
         section will be backed by a paging file. Otherwise
         the section is backed by the specified data file.

Return Value:

    NTSTATUS.

--*/

{
    NTSTATUS Status;
    PVOID Section;
    HANDLE Handle;
    LARGE_INTEGER LargeSize;
    LARGE_INTEGER CapturedSize;
    ULONG RetryCount;
    PCONTROL_AREA ControlArea;

    if ((AllocationAttributes & ~(SEC_COMMIT | SEC_RESERVE | SEC_BASED |
            SEC_IMAGE | SEC_NOCACHE | SEC_NO_CHANGE)) != 0) {
        return STATUS_INVALID_PARAMETER_6;
    }

    if ((AllocationAttributes & (SEC_COMMIT | SEC_RESERVE | SEC_IMAGE)) == 0) {
        return STATUS_INVALID_PARAMETER_6;
    }

    if ((AllocationAttributes & SEC_IMAGE) &&
            (AllocationAttributes & (SEC_COMMIT | SEC_RESERVE |
                            SEC_NOCACHE | SEC_NO_CHANGE))) {

        return STATUS_INVALID_PARAMETER_6;
    }

    if ((AllocationAttributes & SEC_COMMIT) &&
            (AllocationAttributes & SEC_RESERVE)) {
        return STATUS_INVALID_PARAMETER_6;
    }

    //
    // Check the SectionProtection Flag.
    //

    if ((SectionPageProtection & PAGE_NOCACHE) ||
        (SectionPageProtection & PAGE_GUARD) ||
        (SectionPageProtection & PAGE_NOACCESS)) {

        //
        // No cache is only specified through SEC_NOCACHE option in the
        // allocation attributes.
        //

        return STATUS_INVALID_PAGE_PROTECTION;
    }


    if (KeGetPreviousMode() != KernelMode) {
        try {
            ProbeForWriteHandle(SectionHandle);

            if (ARGUMENT_PRESENT (MaximumSize)) {

#if !defined (_WIN64)

                //
                // Note we only probe for byte alignment because prior to 2195,
                // we never probed at all!  We don't want to break user apps
                // that had bad alignment if they worked before.
                //

                ProbeForReadSmallStructure(MaximumSize, sizeof(LARGE_INTEGER), sizeof(UCHAR));
#else
                ProbeForReadSmallStructure(MaximumSize, sizeof(LARGE_INTEGER), sizeof(LARGE_INTEGER));
#endif
                LargeSize = *MaximumSize;
            }
            else {
                ZERO_LARGE (LargeSize);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {
            return GetExceptionCode();
        }
    }
    else {
        if (ARGUMENT_PRESENT (MaximumSize)) {
            LargeSize = *MaximumSize;
        }
        else {
            ZERO_LARGE (LargeSize);
        }
    }

    RetryCount = 0;

retry:

    CapturedSize = LargeSize;

    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);
    Status = MmCreateSection (&Section,
                              DesiredAccess,
                              ObjectAttributes,
                              &CapturedSize,
                              SectionPageProtection,
                              AllocationAttributes,
                              FileHandle,
                              NULL);


    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);
    if (!NT_SUCCESS(Status)) {
        if ((Status == STATUS_FILE_LOCK_CONFLICT) &&
            (RetryCount < 3)) {

            //
            // The file system may have prevented this from working
            // due to log file flushing.  Delay and try again.
            //

            RetryCount += 1;

            KeDelayExecutionThread (KernelMode,
                                    FALSE,
                                    (PLARGE_INTEGER)&MmHalfSecond);

            goto retry;

        }
        return Status;
    }

    ControlArea = ((PSECTION)Section)->Segment->ControlArea;

#if DBG
    if (MmDebug & MM_DBG_SECTIONS) {
        DbgPrint ("inserting section %p control %p\n", Section, ControlArea);
    }
#endif

    if ((ControlArea != NULL) && (ControlArea->FilePointer != NULL)) {
        CcZeroEndOfLastPage (ControlArea->FilePointer);
    }

    //
    // Note if the insertion fails, Ob will dereference the object for us.
    //

    Status = ObInsertObject (Section,
                             NULL,
                             DesiredAccess,
                             0,
                             (PVOID *)NULL,
                             &Handle);

    if (NT_SUCCESS(Status)) {
        try {
            *SectionHandle = Handle;
        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If the write attempt fails, then do not report an error.
            // When the caller attempts to access the handle value,
            // an access violation will occur.
            //
        }
    }

    return Status;
}

NTSTATUS
MmCreateSection (
    OUT PVOID *SectionObject,
    IN ACCESS_MASK DesiredAccess,
    IN POBJECT_ATTRIBUTES ObjectAttributes OPTIONAL,
    IN PLARGE_INTEGER InputMaximumSize,
    IN ULONG SectionPageProtection,
    IN ULONG AllocationAttributes,
    IN HANDLE FileHandle OPTIONAL,
    IN PFILE_OBJECT FileObject OPTIONAL
    )

/*++

Routine Description:

    This function creates a section object and opens a handle to the object
    with the specified desired access.

Arguments:

    Section - A pointer to a variable that will
              receive the section object address.

    DesiredAccess - The desired types of access for the section.

    DesiredAccess Flags

         EXECUTE - Execute access to the section is desired.

         READ - Read access to the section is desired.

         WRITE - Write access to the section is desired.

    ObjectAttributes - Supplies a pointer to an object attributes structure.

    InputMaximumSize - Supplies the maximum size of the section in bytes.
                       This value is rounded up to the host page size and
                       specifies the size of the section (page file
                       backed section) or the maximum size to which a
                       file can be extended or mapped (file backed
                       section).

    SectionPageProtection - Supplies the protection to place on each page
                            in the section.  One of PAGE_READ, PAGE_READWRITE,
                            PAGE_EXECUTE, or PAGE_WRITECOPY and, optionally,
                            PAGE_NOCACHE may be specified.

    AllocationAttributes - Supplies a set of flags that describe the
                           allocation attributes of the section.

        AllocationAttributes Flags

        SEC_BASED - The section is a based section and will be
                    allocated at the same virtual address in each process
                    address space that receives the section.  This does not
                    imply that addresses are reserved for based sections.
                    Rather if the section cannot be mapped at the based address
                    an error is returned.

        SEC_RESERVE - All pages of the section are set to the
                      reserved state.

        SEC_COMMIT - All pages of the section are set to the commit state.

        SEC_IMAGE - The file specified by the file handle is an
                    executable image file.

        SEC_FILE - The file specified by the file handle is a mapped
                   file.  If a file handle is supplied and neither
                   SEC_IMAGE or SEC_FILE is supplied, SEC_FILE is
                   assumed.

    FileHandle - Supplies an optional handle of an open file object.
                 If the value of this handle is null, then the
                 section will be backed by a paging file. Otherwise
                 the section is backed by the specified data file.

    FileObject - Supplies an optional pointer to the file object.  If this
                 value is NULL and the FileHandle is NULL, then there is
                 no file to map (image or mapped file).  If this value
                 is specified, then the File is to be mapped as a MAPPED FILE
                 and NO file size checking will be performed.

                 ONLY THE SYSTEM CACHE SHOULD PROVIDE A FILE OBJECT WITH THE
                 CALL!! as this is optimized to not check the size, only do
                 data mapping, no protection check, etc.

    Note - Only one of FileHandle or File should be specified!

Return Value:

    Returns the relevant NTSTATUS code.

--*/

{
    SECTION Section;
    PSECTION NewSection;
    PSUBSECTION Subsection;
    ULONG SubsectionSize;
    PSEGMENT Segment;
    PSEGMENT NewSegment;
    KPROCESSOR_MODE PreviousMode;
    KIRQL OldIrql;
    NTSTATUS Status;
    NTSTATUS Status2;
    PCONTROL_AREA ControlArea;
    PCONTROL_AREA NewControlArea;
    PCONTROL_AREA SegmentControlArea;
    ACCESS_MASK FileDesiredAccess;
    PFILE_OBJECT File;
    PEVENT_COUNTER Event;
    ULONG IgnoreFileSizing;
    ULONG ProtectionMask;
    ULONG ProtectMaskForAccess;
    ULONG FileAcquired;
    PEVENT_COUNTER SegmentEvent;
    LOGICAL FileSizeChecked;
    LARGE_INTEGER TempSectionSize;
    UINT64 EndOfFile;
    ULONG IncrementedRefCount;
    SIZE_T ControlAreaSize;
    PUINT64 MaximumSize;
    PMM_AVL_TABLE SectionBasedRoot;
    LOGICAL GlobalNeeded;
    PFILE_OBJECT ChangeFileReference;
    SIZE_T SizeOfSection;
    ULONG PagedPoolCharge;
    ULONG NonPagedPoolCharge;
#if DBG
    PVOID PreviousSectionPointer;

    PreviousSectionPointer = (PVOID)-1;
#endif

    NewControlArea = (PCONTROL_AREA)-1;

    UNREFERENCED_PARAMETER (DesiredAccess);

    IgnoreFileSizing = FALSE;
    FileAcquired = FALSE;
    FileSizeChecked = FALSE;
    IncrementedRefCount = FALSE;

    MaximumSize = (PUINT64) InputMaximumSize;

    //
    // Check allocation attributes flags.
    //

    File = (PFILE_OBJECT)NULL;

    ASSERT ((AllocationAttributes & ~(SEC_COMMIT | SEC_RESERVE | SEC_BASED |
            SEC_IMAGE | SEC_NOCACHE | SEC_NO_CHANGE)) == 0);

    ASSERT ((AllocationAttributes & (SEC_COMMIT | SEC_RESERVE | SEC_IMAGE)) != 0);

    ASSERT (!((AllocationAttributes & SEC_IMAGE) &&
            (AllocationAttributes & (SEC_COMMIT | SEC_RESERVE |
                            SEC_NOCACHE | SEC_NO_CHANGE))));

    ASSERT (!((AllocationAttributes & SEC_COMMIT) &&
            (AllocationAttributes & SEC_RESERVE)));

    ASSERT (!((SectionPageProtection & PAGE_NOCACHE) ||
        (SectionPageProtection & PAGE_GUARD) ||
        (SectionPageProtection & PAGE_NOACCESS)));

    if (AllocationAttributes & SEC_NOCACHE) {
        SectionPageProtection |= PAGE_NOCACHE;
    }

    //
    // Check the protection field.
    //

    ProtectionMask = MiMakeProtectionMask (SectionPageProtection);
    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return STATUS_INVALID_PAGE_PROTECTION;
    }

    ProtectMaskForAccess = ProtectionMask & 0x7;

    FileDesiredAccess = MmMakeFileAccess[ProtectMaskForAccess];

    //
    // Get previous processor mode and probe output arguments if necessary.
    //

    PreviousMode = KeGetPreviousMode();

    Section.InitialPageProtection = SectionPageProtection;
    Section.Segment = (PSEGMENT)NULL;

    //
    // Initializing Segment is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    Segment = (PSEGMENT)-1;

    if (ARGUMENT_PRESENT(FileHandle) || ARGUMENT_PRESENT(FileObject)) {

        //
        // Only one of FileHandle or FileObject should be supplied.
        // If a FileObject is supplied, this must be from the
        // file system and therefore the file's size should not
        // be checked.
        //

        if (ARGUMENT_PRESENT(FileObject)) {
            IgnoreFileSizing = TRUE;
            File = FileObject;

            //
            // Quick check to see if a control area already exists.
            //

            if (File->SectionObjectPointer->DataSectionObject) {

                ChangeFileReference = NULL;

                LOCK_PFN (OldIrql);
                ControlArea =
                    (PCONTROL_AREA)(File->SectionObjectPointer->DataSectionObject);

                if ((ControlArea != NULL) &&
                    (!ControlArea->u.Flags.BeingDeleted) &&
                    (!ControlArea->u.Flags.BeingCreated)) {

                    //
                    // Control area exists and is not being deleted,
                    // reference it.
                    //

                    NewSegment = ControlArea->Segment;
                    if ((ControlArea->NumberOfSectionReferences == 0) &&
                        (ControlArea->NumberOfMappedViews == 0) &&
                        (ControlArea->ModifiedWriteCount == 0)) {

                        //
                        // Dereference the current file object (after releasing
                        // the PFN lock) and reference this one.
                        //

                        ASSERT (ControlArea->FilePointer != NULL);
                        ChangeFileReference = ControlArea->FilePointer;
                        ControlArea->FilePointer = FileObject;
                    }
                    ControlArea->u.Flags.Accessed = 1;
                    ControlArea->NumberOfSectionReferences += 1;
                    if (ControlArea->DereferenceList.Flink != NULL) {

                        //
                        // Remove this from the list of unused segments.
                        //

                        RemoveEntryList (&ControlArea->DereferenceList);

                        MI_UNUSED_SEGMENTS_REMOVE_CHARGE (ControlArea);

                        ControlArea->DereferenceList.Flink = NULL;
                        ControlArea->DereferenceList.Blink = NULL;
                    }
                    UNLOCK_PFN (OldIrql);

                    //
                    // Inform the object manager to defer this deletion by
                    // queueing it to another thread to eliminate deadlocks
                    // with the redirector.
                    //

                    if (ChangeFileReference != NULL) {
                        ObDereferenceObjectDeferDelete (ChangeFileReference);
                        ObReferenceObject (FileObject);
                    }

                    IncrementedRefCount = TRUE;
                    Section.SizeOfSection.QuadPart = (LONGLONG)*MaximumSize;

                    goto ReferenceObject;
                }
                UNLOCK_PFN (OldIrql);
            }

            ObReferenceObject (FileObject);

        }
        else {

            Status = ObReferenceObjectByHandle (FileHandle,
                                                FileDesiredAccess,
                                                IoFileObjectType,
                                                PreviousMode,
                                                (PVOID *)&File,
                                                NULL);
            if (!NT_SUCCESS(Status)) {
                return Status;
            }

            //
            // If this file doesn't have a section object pointer,
            // return an error.
            //

            if (File->SectionObjectPointer == NULL) {
                ObDereferenceObject (File);
                return STATUS_INVALID_FILE_FOR_SECTION;
            }
        }

        //
        // Check to see if the specified file already has a section.
        // If not, indicate in the file object's pointer to an FCB that
        // a section is being built.  This synchronizes segment creation
        // for the file.
        //

        if (AllocationAttributes & SEC_IMAGE) {

            //
            // This control area is always just a place holder - the real one
            // is allocated in MiCreateImageFileMap and will be allocated
            // with the correct size and this one freed in a short while.
            //
            // This place holder must always be allocated as a large control
            // area so that it can be chained for the per-session case.
            //

            ControlAreaSize = sizeof(LARGE_CONTROL_AREA) + sizeof(SUBSECTION);

            //
            // For image sections, make sure that Cc has released all references
            // to the section due to a previous data section mapping.  This will
            // decrease the chance that the image will need to be backed by 
            // the pagefile because writeable data sections remain on the file.
            //
            
            CcWaitForUninitializeCacheMap (File);
        }
        else {

            //
            // Data files are mapped with larger subsections than images or
            // pagefile-backed shared memory.  Factor that in here.
            //

            ControlAreaSize = sizeof(CONTROL_AREA) + sizeof(MSUBSECTION);
        }

        NewControlArea = ExAllocatePoolWithTag (NonPagedPool,
                                                ControlAreaSize,
                                                MMCONTROL);

        if (NewControlArea == NULL) {
            ObDereferenceObject (File);
            return STATUS_INSUFFICIENT_RESOURCES;
        }

        RtlZeroMemory (NewControlArea, ControlAreaSize);

        NewSegment = NULL;

        //
        // We only need the file resource if this was a user request, i.e. not
        // a call from the cache manager or file system.
        //

        if (ARGUMENT_PRESENT(FileHandle)) {

            PIRP tempIrp = (PIRP)FSRTL_FSP_TOP_LEVEL_IRP;

            Status = FsRtlAcquireToCreateMappedSection (File, SectionPageProtection);

            if (!NT_SUCCESS(Status)) {
                ExFreePool (NewControlArea);
                ObDereferenceObject (File);
                return Status;
            }

            IoSetTopLevelIrp(tempIrp);
            FileAcquired = TRUE;
        }

        //
        // Initializing GlobalNeeded is not needed for correctness, but
        // without it the compiler cannot compile this code W4 to check
        // for use of uninitialized variables.
        //

        GlobalNeeded = FALSE;

        //
        // Allocate an event to wait on in case the segment is in the
        // process of being deleted.  This event cannot be allocated
        // with the PFN database locked as pool expansion would deadlock.
        //

ReallocateandcheckSegment:

        SegmentEvent = MiGetEventCounter ();

        if (SegmentEvent == NULL) {
            if (FileAcquired) {
                IoSetTopLevelIrp (NULL);
                FsRtlReleaseFile (File);
            }
            ExFreePool (NewControlArea);
            ObDereferenceObject (File);
            return STATUS_INSUFFICIENT_RESOURCES;
        }

RecheckSegment:

        LOCK_PFN (OldIrql);

        if (AllocationAttributes & SEC_IMAGE) {

            ControlArea = MiFindImageSectionObject (File, &GlobalNeeded);

        }
        else {
            ControlArea =
               (PCONTROL_AREA)(File->SectionObjectPointer->DataSectionObject);
        }

        if (ControlArea != NULL) {

            //
            // A segment already exists for this file.  Make sure that it
            // is not in the process of being deleted, or being created.
            //

            if ((ControlArea->u.Flags.BeingDeleted) ||
                (ControlArea->u.Flags.BeingCreated)) {

                //
                // The segment object is in the process of being deleted or
                // created.
                // Check to see if another thread is waiting for the deletion,
                // otherwise create an event object to wait upon.
                //

                if (ControlArea->WaitingForDeletion == NULL) {

                    //
                    // Initialize an event and put its address in the control area.
                    //

                    ControlArea->WaitingForDeletion = SegmentEvent;
                    Event = SegmentEvent;
                    SegmentEvent = NULL;
                }
                else {
                    Event = ControlArea->WaitingForDeletion;

                    //
                    // No interlock is needed for the RefCount increment as
                    // no thread can be decrementing it since it is still
                    // pointed to by the control area.
                    //

                    Event->RefCount += 1;
                }

                //
                // Release the PFN lock, the file lock, and wait for the event.
                //

                UNLOCK_PFN (OldIrql);
                if (FileAcquired) {
                    IoSetTopLevelIrp (NULL);
                    FsRtlReleaseFile (File);
                }

                KeWaitForSingleObject (&Event->Event,
                                       WrVirtualMemory,
                                       KernelMode,
                                       FALSE,
                                       (PLARGE_INTEGER)NULL);

                //
                // Before this event can be set, the control area
                // WaitingForDeletion field must be cleared (and may be
                // reinitialized to something else), but cannot be reset
                // to our local event.  This allows us to dereference the
                // event count lock free.
                //

#if 0
                //
                // Note that the control area cannot be referenced at this
                // point because it may have been freed.
                //

                ASSERT (Event != ControlArea->WaitingForDeletion);
#endif

                if (FileAcquired) {
                    Status = FsRtlAcquireToCreateMappedSection (File, SectionPageProtection);

                    if (NT_SUCCESS(Status)) {
                        PIRP tempIrp = (PIRP)FSRTL_FSP_TOP_LEVEL_IRP;
                        IoSetTopLevelIrp (tempIrp);
                    }
                    else {
                        ExFreePool (NewControlArea);
                        ObDereferenceObject (File);
                        return Status;
                    }
                }

                MiFreeEventCounter (Event);

                if (SegmentEvent == NULL) {

                    //
                    // The event was freed from pool, allocate another
                    // event in case we have to synchronize one more time.
                    //

                    goto ReallocateandcheckSegment;
                }

                goto RecheckSegment;
            }

            //
            // There is already a segment for this file, have
            // this section refer to that segment.
            // No need to reference the file object any more.
            //

            if ((ControlArea->u.Flags.ImageMappedInSystemSpace) &&
                (AllocationAttributes & SEC_IMAGE) &&
                (KeGetPreviousMode () != KernelMode)) {

                UNLOCK_PFN (OldIrql);

                MiFreeEventCounter (SegmentEvent);
                if (FileAcquired) {
                    IoSetTopLevelIrp (NULL);
                    FsRtlReleaseFile (File);
                }
                ExFreePool (NewControlArea);
                ObDereferenceObject (File);
                return STATUS_CONFLICTING_ADDRESSES;
            }

            NewSegment = ControlArea->Segment;
            ControlArea->u.Flags.Accessed = 1;
            ControlArea->NumberOfSectionReferences += 1;
            if (ControlArea->DereferenceList.Flink != NULL) {

                //
                // Remove this from the list of unused segments.
                //

                RemoveEntryList (&ControlArea->DereferenceList);

                MI_UNUSED_SEGMENTS_REMOVE_CHARGE (ControlArea);

                ControlArea->DereferenceList.Flink = NULL;
                ControlArea->DereferenceList.Blink = NULL;
            }
            IncrementedRefCount = TRUE;

            //
            // If this reference was not from the cache manager
            // up the count of user references.
            //

            if (IgnoreFileSizing == FALSE) {
                ControlArea->NumberOfUserReferences += 1;
            }
        }
        else {

            //
            // There is no segment associated with this file object.
            // Set the file object to refer to the new control area.
            //

            ControlArea = NewControlArea;
            ControlArea->u.Flags.BeingCreated = 1;

            if (AllocationAttributes & SEC_IMAGE) {
                if (GlobalNeeded == TRUE) {
                    ControlArea->u.Flags.GlobalOnlyPerSession = 1;
                }

                MiInsertImageSectionObject (File, ControlArea);
            }
            else {
#if DBG
                PreviousSectionPointer = File->SectionObjectPointer;
#endif
                File->SectionObjectPointer->DataSectionObject = (PVOID) ControlArea;
            }
        }

        UNLOCK_PFN (OldIrql);

        if (SegmentEvent != NULL) {
            MiFreeEventCounter (SegmentEvent);
        }

        if (NewSegment != NULL) {

            //
            // A segment already exists for this file object.
            // If we're creating an imagemap, flush the data section
            // if there is one.
            //

            if (AllocationAttributes & SEC_IMAGE) {
                MiFlushDataSection (File);
            }

            //
            // Deallocate the new control area as it won't be needed.
            // Dereference the file object later when we're done with it.
            //

            ExFreePool (NewControlArea);

            //
            // The section is in paged pool, this can't be set until
            // the PFN mutex has been released.
            //

            if ((!IgnoreFileSizing) && (ControlArea->u.Flags.Image == 0)) {

                //
                // The file size in the segment may not match the current
                // file size, query the file system and get the file
                // size.
                //

                Status = FsRtlGetFileSize (File, (PLARGE_INTEGER)&EndOfFile);

                if (!NT_SUCCESS (Status)) {

                    if (FileAcquired) {
                        IoSetTopLevelIrp(NULL);
                        FsRtlReleaseFile (File);
                        FileAcquired = FALSE;
                    }

                    ObDereferenceObject (File);
                    goto UnrefAndReturn;
                }

                if (EndOfFile == 0 && *MaximumSize == 0) {

                    //
                    // Can't map a zero length without specifying the maximum
                    // size as non-zero.
                    //

                    Status = STATUS_MAPPED_FILE_SIZE_ZERO;

                    if (FileAcquired) {
                        IoSetTopLevelIrp(NULL);
                        FsRtlReleaseFile (File);
                        FileAcquired = FALSE;
                    }

                    ObDereferenceObject (File);
                    goto UnrefAndReturn;
                }
            }
            else {

                //
                // The size is okay in the segment.
                //

                EndOfFile = (UINT64) NewSegment->SizeOfSegment;
            }

            if (FileAcquired) {
                IoSetTopLevelIrp(NULL);
                FsRtlReleaseFile (File);
                FileAcquired = FALSE;
            }

            ObDereferenceObject (File);

            if (*MaximumSize == 0) {

                Section.SizeOfSection.QuadPart = (LONGLONG)EndOfFile;
                FileSizeChecked = TRUE;
            }
            else if (EndOfFile >= *MaximumSize) {

                //
                // EndOfFile is greater than the MaximumSize,
                // use the specified maximum size.
                //

                Section.SizeOfSection.QuadPart = (LONGLONG)*MaximumSize;
                FileSizeChecked = TRUE;
            }
            else {

                //
                // Need to extend the section, make sure the file was
                // opened for write access.
                //

                if (((SectionPageProtection & PAGE_READWRITE) |
                    (SectionPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {

                    Status = STATUS_SECTION_TOO_BIG;
                    goto UnrefAndReturn;
                }
                Section.SizeOfSection.QuadPart = (LONGLONG)*MaximumSize;
            }
        }
        else {

            //
            // The file does not have an associated segment, create a segment
            // object.
            //

            PERFINFO_SECTION_CREATE1(File);

            if (AllocationAttributes & SEC_IMAGE) {

                Status = MiCreateImageFileMap (File, &Segment);

            }
            else {

                Status = MiCreateDataFileMap (File,
                                              &Segment,
                                              MaximumSize,
                                              SectionPageProtection,
                                              AllocationAttributes,
                                              IgnoreFileSizing);
                ASSERT (PreviousSectionPointer == File->SectionObjectPointer);
            }

            if (!NT_SUCCESS(Status)) {

                //
                // Lock the PFN database and check to see if another thread has
                // tried to create a segment to the file object at the same
                // time.
                //

                LOCK_PFN (OldIrql);

                Event = ControlArea->WaitingForDeletion;
                ControlArea->WaitingForDeletion = NULL;
                ASSERT (ControlArea->u.Flags.FilePointerNull == 0);
                ControlArea->u.Flags.FilePointerNull = 1;

                if (AllocationAttributes & SEC_IMAGE) {
                    MiRemoveImageSectionObject (File, ControlArea);
                }
                else {
                    File->SectionObjectPointer->DataSectionObject = NULL;
                }
                ControlArea->u.Flags.BeingCreated = 0;

                UNLOCK_PFN (OldIrql);

                if (FileAcquired) {
                    IoSetTopLevelIrp(NULL);
                    FsRtlReleaseFile (File);
                }

                ExFreePool (NewControlArea);

                ObDereferenceObject (File);

                if (Event != NULL) {

                    //
                    // Signal any waiters that the segment structure exists.
                    //

                    KeSetEvent (&Event->Event, 0, FALSE);
                }

                return Status;
            }

            //
            // If the size was specified as zero, set the section size
            // from the created segment size.  This solves problems with
            // race conditions when multiple sections
            // are created for the same mapped file with varying sizes.
            //

            if (*MaximumSize == 0) {
                Section.SizeOfSection.QuadPart = (LONGLONG)Segment->SizeOfSegment;
            }
            else {
                Section.SizeOfSection.QuadPart = (LONGLONG)*MaximumSize;
            }
        }

    }
    else {

        //
        // No file handle exists, this is a page file backed section.
        //

        if (AllocationAttributes & SEC_IMAGE) {
            return STATUS_INVALID_FILE_FOR_SECTION;
        }

        Status = MiCreatePagingFileMap (&NewSegment,
                                        MaximumSize,
                                        ProtectionMask,
                                        AllocationAttributes);

        if (!NT_SUCCESS(Status)) {
            return Status;
        }

        //
        // Set the section size from the created segment size.  This
        // solves problems with race conditions when multiple sections
        // are created for the same mapped file with varying sizes.
        //

        Section.SizeOfSection.QuadPart = (LONGLONG)NewSegment->SizeOfSegment;
        ControlArea = NewSegment->ControlArea;

        //
        // Set IncrementedRefCount so any failures from this point before the
        // object is created will result in the control area & segment getting
        // torn down - otherwise these could leak.  This is because pagefile
        // backed sections are not (and should not be) added to the
        // dereference segment cache.
        //

        IncrementedRefCount = 1;
    }

    if (NewSegment == NULL) {

        //
        // A new segment had to be created.  Lock the PFN database and
        // check to see if any other threads also tried to create a new segment
        // for this file object at the same time.
        //

        NewSegment = Segment;

        SegmentControlArea = Segment->ControlArea;

        ASSERT (File != NULL);

        LOCK_PFN (OldIrql);

        Event = ControlArea->WaitingForDeletion;
        ControlArea->WaitingForDeletion = NULL;

        if (AllocationAttributes & SEC_IMAGE) {

            //
            // Change the control area in the file object pointer.
            //

            MiRemoveImageSectionObject (File, NewControlArea);
            MiInsertImageSectionObject (File, SegmentControlArea);

            ControlArea = SegmentControlArea;
        }
        else if (SegmentControlArea->u.Flags.Rom == 1) {
            ASSERT (File->SectionObjectPointer->DataSectionObject == NewControlArea);
            File->SectionObjectPointer->DataSectionObject = SegmentControlArea;

            ControlArea = SegmentControlArea;
        }

        ControlArea->u.Flags.BeingCreated = 0;

        UNLOCK_PFN (OldIrql);

        if ((AllocationAttributes & SEC_IMAGE) ||
            (SegmentControlArea->u.Flags.Rom == 1)) {

            //
            // Deallocate the pool used for the original control area.
            //

            ExFreePool (NewControlArea);
        }

        if (Event != NULL) {

            //
            // Signal any waiters that the segment structure exists.
            //

            KeSetEvent (&Event->Event, 0, FALSE);
        }

        PERFINFO_SECTION_CREATE(ControlArea);
    }

    //
    // Being created has now been cleared allowing other threads
    // to reference the segment.  Release the resource on the file.
    //

    if (FileAcquired) {
        IoSetTopLevelIrp(NULL);
        FsRtlReleaseFile (File);
        FileAcquired = FALSE;
    }

ReferenceObject:

    //
    // Now that the segment object is created, make the section object
    // refer to the segment object.
    //

    Section.Segment = NewSegment;
    Section.u.LongFlags = ControlArea->u.LongFlags;

    //
    // Update the count of writable user sections so the transaction semantics
    // can be supported.  Note that no lock synchronization is needed here as
    // the transaction manager must already check for any open writable handles
    // to the file - and no writable sections can be created without a writable
    // file handle.  So all that needs to be provided is a way for the
    // transaction manager to know that there are lingering user views or
    // created sections still open that have write access.
    //
    // This must be done before creating the object so a rogue user program
    // that suspends this thread cannot subvert a transaction.
    //

    if ((FileObject == NULL) &&
        (SectionPageProtection & (PAGE_READWRITE|PAGE_EXECUTE_READWRITE)) &&
        (ControlArea->u.Flags.Image == 0) &&
        (ControlArea->FilePointer != NULL)) {

        Section.u.Flags.UserWritable = 1;

        InterlockedIncrement ((PLONG)&ControlArea->Segment->WritableUserReferences);
    }

    //
    // Create the section object now.  The section object is created
    // now so that the error handling when the section object cannot
    // be created is simplified.
    //

    if ((ControlArea->u.Flags.Image == 1) || (ControlArea->FilePointer == NULL)) {
        PagedPoolCharge = sizeof (SECTION) + NewSegment->TotalNumberOfPtes * sizeof(MMPTE);
        SubsectionSize = sizeof (SUBSECTION);
    }
    else {
        PagedPoolCharge = 0;
        SubsectionSize = sizeof (MSUBSECTION);
    }

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {

        NonPagedPoolCharge = sizeof (CONTROL_AREA);
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        NonPagedPoolCharge = sizeof(LARGE_CONTROL_AREA);
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    do {
        NonPagedPoolCharge += SubsectionSize;
        Subsection = Subsection->NextSubsection;
    } while (Subsection != NULL);

    Status = ObCreateObject (PreviousMode,
                             MmSectionObjectType,
                             ObjectAttributes,
                             PreviousMode,
                             NULL,
                             sizeof(SECTION),
                             PagedPoolCharge,
                             NonPagedPoolCharge,
                             (PVOID *)&NewSection);

    if (!NT_SUCCESS(Status)) {

        if ((FileObject == NULL) &&
            (SectionPageProtection & (PAGE_READWRITE|PAGE_EXECUTE_READWRITE)) &&
            (ControlArea->u.Flags.Image == 0) &&
            (ControlArea->FilePointer != NULL)) {

            ASSERT (Section.u.Flags.UserWritable == 1);

            InterlockedDecrement ((PLONG)&ControlArea->Segment->WritableUserReferences);
        }

        goto UnrefAndReturn;
    }

    RtlCopyMemory (NewSection, &Section, sizeof(SECTION));
    NewSection->Address.StartingVpn = 0;

    if (!IgnoreFileSizing) {

        //
        // Indicate that the cache manager is not the owner of this
        // section.
        //

        NewSection->u.Flags.UserReference = 1;

        if (AllocationAttributes & SEC_NO_CHANGE) {

            //
            // Indicate that once the section is mapped, no protection
            // changes or freeing the mapping is allowed.
            //

            NewSection->u.Flags.NoChange = 1;
        }

        if (((SectionPageProtection & PAGE_READWRITE) |
            (SectionPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {

            //
            // This section does not support WRITE access, indicate
            // that changing the protection to WRITE results in COPY_ON_WRITE.
            //

            NewSection->u.Flags.CopyOnWrite = 1;
        }

        if (AllocationAttributes & SEC_BASED) {

            NewSection->u.Flags.Based = 1;

            SectionBasedRoot = &MmSectionBasedRoot;

            //
            // This section is based at a unique address system wide.
            // Ensure it does not wrap the virtual address space as the
            // SECTION structure would have to widen to accomodate this and
            // it's not worth the performance penalty for the very few isolated
            // cases that would want this.  Note that sections larger than the
            // address space can easily be created - it's just that beyond a
            // certain point you shouldn't specify SEC_BASED (anything this big
            // couldn't use a SEC_BASED section for anything anyway).
            //

            if ((UINT64)NewSection->SizeOfSection.QuadPart > (UINT64)MmHighSectionBase) {
                ObDereferenceObject (NewSection);
                return STATUS_NO_MEMORY;
            }

#if defined(_WIN64)
            SizeOfSection = NewSection->SizeOfSection.QuadPart;
#else
            SizeOfSection = NewSection->SizeOfSection.LowPart;
#endif

            //
            // Get the allocation base mutex.
            //

            KeAcquireGuardedMutex (&MmSectionBasedMutex);

            Status2 = MiFindEmptyAddressRangeDownBasedTree (
                                            SizeOfSection,
                                            MmHighSectionBase,
                                            X64K,
                                            SectionBasedRoot,
                                            (PVOID *)&NewSection->Address.StartingVpn);
            if (!NT_SUCCESS(Status2)) {
                KeReleaseGuardedMutex (&MmSectionBasedMutex);
                ObDereferenceObject (NewSection);
                return Status2;
            }

            NewSection->Address.EndingVpn = NewSection->Address.StartingVpn +
                                                SizeOfSection - 1;

            MiInsertBasedSection (NewSection);
            KeReleaseGuardedMutex (&MmSectionBasedMutex);
        }
    }

    //
    // If the cache manager is creating the section, set the was
    // purged flag as the file size can change.
    //

    ControlArea->u.Flags.WasPurged |= IgnoreFileSizing;

    //
    // Check to see if the section is for a data file and the size
    // of the section is greater than the current size of the
    // segment.
    //

    if (((ControlArea->u.Flags.WasPurged == 1) && (!IgnoreFileSizing)) &&
                      (!FileSizeChecked)
                            ||
        ((UINT64)NewSection->SizeOfSection.QuadPart >
                                NewSection->Segment->SizeOfSegment)) {

        TempSectionSize = NewSection->SizeOfSection;

        NewSection->SizeOfSection.QuadPart = (LONGLONG)NewSection->Segment->SizeOfSegment;

        //
        // Even if the caller didn't specify extension rights, we enable it here
        // temporarily to make the section correct.  Use a temporary section
        // instead of temporarily editing the real section to avoid opening
        // a security window that other concurrent threads could exploit.
        //

        if (((NewSection->InitialPageProtection & PAGE_READWRITE) |
            (NewSection->InitialPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {
                SECTION     WritableSection;

                *(PSECTION)&WritableSection = *NewSection;

                Status = MmExtendSection (&WritableSection,
                                          &TempSectionSize,
                                          IgnoreFileSizing);

                NewSection->SizeOfSection = WritableSection.SizeOfSection;
        }
        else {
            Status = MmExtendSection (NewSection,
                                      &TempSectionSize,
                                      IgnoreFileSizing);
        }

        if (!NT_SUCCESS(Status)) {
            ObDereferenceObject (NewSection);
            return Status;
        }
    }

    *SectionObject = (PVOID)NewSection;

    return Status;

UnrefAndReturn:

    //
    // Unreference the control area, if it was referenced and return
    // the error status.
    //

    if (FileAcquired) {
        IoSetTopLevelIrp(NULL);
        FsRtlReleaseFile (File);
    }

    if (IncrementedRefCount) {
        LOCK_PFN (OldIrql);
        ControlArea->NumberOfSectionReferences -= 1;
        if (!IgnoreFileSizing) {
            ASSERT ((LONG)ControlArea->NumberOfUserReferences > 0);
            ControlArea->NumberOfUserReferences -= 1;
        }
        MiCheckControlArea (ControlArea, NULL, OldIrql);
    }
    return Status;
}

LOGICAL
MiMakeControlAreaRom (
    IN PFILE_OBJECT File,
    IN PLARGE_CONTROL_AREA ControlArea,
    IN PFN_NUMBER PageFrameNumber
    )

/*++

Routine Description:

    This function marks the control area as ROM-backed.  It can fail if the
    parallel control area (image vs data) is currently active as ROM-backed
    as the same PFNs cannot be used for both control areas simultaneously.

Arguments:

    ControlArea - Supplies the relevant control area.

    PageFrameNumber - Supplies the starting physical page frame number.

Return Value:

    TRUE if the control area was marked as ROM-backed, FALSE if not.

--*/

{
    LOGICAL ControlAreaMarked;
    PCONTROL_AREA OtherControlArea;
    KIRQL OldIrql;

    ControlAreaMarked = FALSE;

    LOCK_PFN (OldIrql);

    if (ControlArea->u.Flags.Image == 1) {
        OtherControlArea = (PCONTROL_AREA) File->SectionObjectPointer->DataSectionObject;
    }
    else {
        OtherControlArea = (PCONTROL_AREA) File->SectionObjectPointer->ImageSectionObject;
    }

    //
    // This could be made smarter (ie: throw away the other control area if it's
    // not in use) but for now, keep it simple.
    //

    if ((OtherControlArea == NULL) || (OtherControlArea->u.Flags.Rom == 0)) {
        ControlArea->u.Flags.Rom = 1;
        ControlArea->StartingFrame = PageFrameNumber;
        ControlAreaMarked = TRUE;
    }

    UNLOCK_PFN (OldIrql);

    return ControlAreaMarked;
}

ULONG MiImageFailure;

#define MI_BAD_IMAGE(x)     MiImageFailure = x;


NTSTATUS
MiCreateImageFileMap (
    IN PFILE_OBJECT File,
    OUT PSEGMENT *Segment
    )

/*++

Routine Description:

    This function creates the necessary structures to allow the mapping
    of an image file.

    The image file is opened and verified for correctness, a segment
    object is created and initialized based on data in the image
    header.

Arguments:

    File - Supplies the file object for the image file.

    Segment - Returns the segment object.

Return Value:

    NTSTATUS.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    LOGICAL CheckSplitPages;
    LOGICAL MarkModified;
    LOGICAL SingleSubsection;
    NTSTATUS Status;
    ULONG_PTR EndingAddress;
    PFN_NUMBER NumberOfPtes;
    SIZE_T SizeOfSegment;
    ULONG SectionVirtualSize;
    ULONG SizeOfRawData;
    ULONG i;
    ULONG j;
    PCONTROL_AREA ControlArea;
    PSUBSECTION Subsection;
    PMMPTE PointerPte;
    MMPTE TempPte;
    MMPTE TempPteDemandZero;
    PVOID Base;
    PIMAGE_DOS_HEADER DosHeader;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_FILE_HEADER FileHeader;
    ULONG SizeOfImage;
    ULONG SizeOfHeaders;
#if defined (_WIN64)
    PIMAGE_NT_HEADERS32 NtHeader32;
#endif
    PIMAGE_DATA_DIRECTORY ComPlusDirectoryEntry;
    PIMAGE_SECTION_HEADER SectionTableEntry;
    PSEGMENT NewSegment;
    ULONG SectorOffset;
    ULONG NumberOfSubsections;
    PFN_NUMBER PageFrameNumber;
    PFN_NUMBER XipFrameNumber;
    LOGICAL XipFile;
    LOGICAL GlobalPerSession;
    LARGE_INTEGER StartingOffset;
    PCHAR ExtendedHeader;
    PPFN_NUMBER Page;
    ULONG_PTR PreferredImageBase;
    ULONG_PTR NextVa;
    ULONG_PTR ImageBase;
    KEVENT InPageEvent;
    PMDL Mdl;
    ULONG ImageFileSize;
    ULONG OffsetToSectionTable;
    ULONG ImageAlignment;
    ULONG RoundingAlignment;
    ULONG FileAlignment;
    LOGICAL ImageCommit;
    LOGICAL SectionCommit;
    IO_STATUS_BLOCK IoStatus;
    LARGE_INTEGER EndOfFile;
    ULONG NtHeaderSize;
    ULONG SubsectionsAllocated;
    PLARGE_CONTROL_AREA LargeControlArea;
    PSUBSECTION NewSubsection;
    ULONG OriginalProtection;
    ULONG LoaderFlags;
    ULONG TempNumberOfSubsections;
    PIMAGE_SECTION_HEADER TempSectionTableEntry;
    ULONG AdditionalSubsections;
    ULONG AdditionalPtes;
    ULONG AdditionalBasePtes;
    ULONG NewSubsectionsAllocated;
    PSEGMENT OldSegment;
    PMMPTE NewPointerPte;
    PMMPTE OldPointerPte;
    PFN_NUMBER OrigNumberOfPtes;
    PCONTROL_AREA NewControlArea;
    SIZE_T CommitCharged;
    PHYSICAL_ADDRESS PhysicalAddress;
    LOGICAL ActiveDataReferences;
    PFN_NUMBER StackMdl[(sizeof(MDL)/sizeof(PFN_NUMBER)) + MM_MAXIMUM_IMAGE_HEADER / PAGE_SIZE];
    PMMPFN ImagePages;
#if defined (_MIALT4K_)
    ULONG ReadCount;
    ULONG RawDataSize;
    ULONG ReadSize;
    PVOID HalfPage;
    PHYSICAL_ADDRESS HalfPagePhysicalAddress;
    PVOID StraddleVa;
    MMPTE PteContents;
    PMMPTE PreviousPte;
    PMMPTE ReadPte;
    PFN_NUMBER HalfPageFrameNumber;
    PFN_NUMBER StraddleFrameNumber;
    PSUBSECTION ReadSubsection;
    ULONG PreviousSectionCharacteristics;
    LARGE_INTEGER TempOffset;
#endif

#if defined (_IA64_)
    LOGICAL InvalidAlignmentAllowed;

    InvalidAlignmentAllowed = FALSE;
#else
#define InvalidAlignmentAllowed FALSE
#endif

    PAGED_CODE ();

    Status = FsRtlGetFileSize (File, &EndOfFile);

    if (!NT_SUCCESS (Status)) {

        MI_BAD_IMAGE (1);
        if (Status == STATUS_FILE_IS_A_DIRECTORY) {

            //
            // Can't map a directory as a section - return an error.
            //

            return STATUS_INVALID_FILE_FOR_SECTION;
        }

        return Status;
    }

    if (EndOfFile.HighPart != 0) {

        //
        // File too big. Return error.
        //

        return STATUS_INVALID_FILE_FOR_SECTION;
    }

    CommitCharged = 0;
    ExtendedHeader = NULL;
    ControlArea = NULL;
    NewSegment = NULL;
    Base = NULL;

    //
    // Initializing IoStatus.Information is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    IoStatus.Information = 0;

    Mdl = (PMDL) &StackMdl;

    //
    // Read in the file header.
    //
    // Create an event and MDL for the read operation.
    //

    KeInitializeEvent (&InPageEvent, NotificationEvent, FALSE);

    MmInitializeMdl (Mdl, NULL, PAGE_SIZE);

    Mdl->MdlFlags |= MDL_PAGES_LOCKED;

    PageFrameNumber = MiGetPageForHeader (TRUE);

    ASSERT (PageFrameNumber != 0);

    Pfn1 = MI_PFN_ELEMENT (PageFrameNumber);

    ASSERT (Pfn1->u1.Flink == 0);
    ImagePages = Pfn1;

    Page = (PPFN_NUMBER)(Mdl + 1);
    *Page = PageFrameNumber;

    ZERO_LARGE (StartingOffset);

    CcZeroEndOfLastPage (File);

    //
    // Flush the data section if there is one.
    //
    // At the same time, capture whether there are any references to the
    // data control area.  If so, flip the pages from the image control
    // area into pagefile backings to prevent anyone else's data writes
    // from changing the file after it is validated (this can happen if the
    // pages from the image control area need to be re-paged in later).
    //

    ActiveDataReferences = MiFlushDataSection (File);

    Base = MiCopyHeaderIfResident (File, PageFrameNumber);

    if (Base == NULL) {

        ASSERT (Mdl->MdlFlags & MDL_PAGES_LOCKED);

        Status = IoPageRead (File,
                             Mdl,
                             &StartingOffset,
                             &InPageEvent,
                             &IoStatus);

        if (Status == STATUS_PENDING) {
            KeWaitForSingleObject (&InPageEvent,
                                   WrPageIn,
                                   KernelMode,
                                   FALSE,
                                   NULL);

            Status = IoStatus.Status;
        }

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
        }

        if (!NT_SUCCESS(Status)) {
            MI_BAD_IMAGE (2);
            if ((Status != STATUS_FILE_LOCK_CONFLICT) && (Status != STATUS_FILE_IS_OFFLINE)) {
                Status = STATUS_INVALID_FILE_FOR_SECTION;
            }
            goto BadPeImageSegment;
        }

        Base = MiMapImageHeaderInHyperSpace (PageFrameNumber);

        if (IoStatus.Information != PAGE_SIZE) {

            //
            // A full page was not read from the file, zero any remaining
            // bytes.
            //

            RtlZeroMemory ((PVOID)((PCHAR)Base + IoStatus.Information),
                           PAGE_SIZE - IoStatus.Information);
        }
    }

    DosHeader = (PIMAGE_DOS_HEADER) Base;

    //
    // Check to determine if this is an NT image (PE format) or
    // a DOS image, Win-16 image, or OS/2 image.  If the image is
    // not NT format, return an error indicating which image it
    // appears to be.
    //

    if (DosHeader->e_magic != IMAGE_DOS_SIGNATURE) {

        Status = STATUS_INVALID_IMAGE_NOT_MZ;
#if 0
        MI_BAD_IMAGE (3);       // Don't log this as it happens all the time.
#endif
        goto BadPeImageSegment;
    }

#ifndef i386
    if (((ULONG)DosHeader->e_lfanew & 3) != 0) {

        //
        // The image header is not aligned on a longword boundary.
        // Report this as an invalid protected mode image.
        //

        Status = STATUS_INVALID_IMAGE_PROTECT;
        MI_BAD_IMAGE (4);
        goto BadPeImageSegment;
    }
#endif

    if ((ULONG)DosHeader->e_lfanew > EndOfFile.LowPart) {
        Status = STATUS_INVALID_IMAGE_PROTECT;
        MI_BAD_IMAGE (5);
        goto BadPeImageSegment;
    }

    if (((ULONG)DosHeader->e_lfanew +
            sizeof(IMAGE_NT_HEADERS) +
            (16 * sizeof(IMAGE_SECTION_HEADER))) <= (ULONG)DosHeader->e_lfanew) {
        Status = STATUS_INVALID_IMAGE_PROTECT;
        MI_BAD_IMAGE (6);
        goto BadPeImageSegment;
    }

    if (((ULONG)DosHeader->e_lfanew +
            sizeof(IMAGE_NT_HEADERS) +
            (16 * sizeof(IMAGE_SECTION_HEADER))) > PAGE_SIZE) {

        //
        // The PE header is not within the page already read or the
        // objects are in another page.
        // Build another MDL and read an additional 8k.
        //

        ExtendedHeader = ExAllocatePoolWithTag (NonPagedPool,
                                                MM_MAXIMUM_IMAGE_HEADER,
                                                MMTEMPORARY);
        if (ExtendedHeader == NULL) {
            Status = STATUS_INSUFFICIENT_RESOURCES;
            MI_BAD_IMAGE (7);
            goto BadPeImageSegment;
        }

        //
        // Build an MDL for the operation.
        //

        MmInitializeMdl (Mdl, ExtendedHeader, MM_MAXIMUM_IMAGE_HEADER);

        MmBuildMdlForNonPagedPool (Mdl);

        StartingOffset.LowPart = PtrToUlong(PAGE_ALIGN ((ULONG)DosHeader->e_lfanew));

        KeClearEvent (&InPageEvent);

        Status = IoPageRead (File,
                             Mdl,
                             &StartingOffset,
                             &InPageEvent,
                             &IoStatus);

        if (Status == STATUS_PENDING) {

            KeWaitForSingleObject (&InPageEvent,
                                   WrPageIn,
                                   KernelMode,
                                   FALSE,
                                   NULL);

            Status = IoStatus.Status;
        }

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
        }

        if (!NT_SUCCESS(Status)) {
            MI_BAD_IMAGE (8);
            if ((Status != STATUS_FILE_LOCK_CONFLICT) && (Status != STATUS_FILE_IS_OFFLINE)) {
                Status = STATUS_INVALID_FILE_FOR_SECTION;
            }
            goto BadPeImageSegment;
        }

        NtHeader = (PIMAGE_NT_HEADERS)((PCHAR)ExtendedHeader +
                          BYTE_OFFSET((ULONG)DosHeader->e_lfanew));

        NtHeaderSize = MM_MAXIMUM_IMAGE_HEADER -
                            (ULONG)(BYTE_OFFSET((ULONG)DosHeader->e_lfanew));
    }
    else {
        NtHeader = (PIMAGE_NT_HEADERS)((PCHAR)DosHeader +
                                          (ULONG)DosHeader->e_lfanew);
        NtHeaderSize = PAGE_SIZE - (ULONG)DosHeader->e_lfanew;
    }

    FileHeader = &NtHeader->FileHeader;

    //
    // Check to see if this is an NT image or a DOS or OS/2 image.
    //

    Status = MiVerifyImageHeader (NtHeader, DosHeader, NtHeaderSize);
    if (Status != STATUS_SUCCESS) {
        MI_BAD_IMAGE (9);
        goto BadPeImageSegment;
    }

    CheckSplitPages = FALSE;

#if defined(_WIN64)

    if (NtHeader->OptionalHeader.Magic != IMAGE_NT_OPTIONAL_HDR32_MAGIC) {
        NtHeader32 = NULL;

#endif
        ImageAlignment = NtHeader->OptionalHeader.SectionAlignment;
        FileAlignment = NtHeader->OptionalHeader.FileAlignment - 1;
        SizeOfImage = NtHeader->OptionalHeader.SizeOfImage;
        LoaderFlags = NtHeader->OptionalHeader.LoaderFlags;
        ImageBase = NtHeader->OptionalHeader.ImageBase;
        SizeOfHeaders = NtHeader->OptionalHeader.SizeOfHeaders;

        //
        // Read in the COM+ directory entry.
        //

        ComPlusDirectoryEntry = &NtHeader->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_COM_DESCRIPTOR];

#if defined (_WIN64)
    }
    else {

        //
        // The image is 32-bit.  All code below this point must check
        // if NtHeader is NULL.  If it is, then the image is PE32 and
        // NtHeader32 must be used.
        //

        NtHeader32 = (PIMAGE_NT_HEADERS32) NtHeader;
        NtHeader = NULL;

        ImageAlignment = NtHeader32->OptionalHeader.SectionAlignment;
        FileAlignment = NtHeader32->OptionalHeader.FileAlignment - 1;
        SizeOfImage = NtHeader32->OptionalHeader.SizeOfImage;
        LoaderFlags = NtHeader32->OptionalHeader.LoaderFlags;
        ImageBase = NtHeader32->OptionalHeader.ImageBase;
        SizeOfHeaders = NtHeader32->OptionalHeader.SizeOfHeaders;

        //
        // Read in the COM+ directory entry.
        //

        ComPlusDirectoryEntry = &NtHeader32->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_COM_DESCRIPTOR];

#if defined (_MIALT4K_)

        if ((ExtendedHeader == NULL) && (ImageAlignment == PAGE_4K)) {

            //
            // The DOS header, NT header and the section headers all fit in
            // the first page.  Check all the section headers now and if none
            // of them indicate unaligned global subsections then map this
            // image with individual subsections, splitting page permissions
            // and boundaries as needed.
            //

            OffsetToSectionTable = sizeof(ULONG) +
                                      sizeof(IMAGE_FILE_HEADER) +
                                      FileHeader->SizeOfOptionalHeader;

            if (BYTE_OFFSET (NtHeader32) + OffsetToSectionTable + 
                (16 * sizeof (IMAGE_SECTION_HEADER)) <= PAGE_SIZE) {

                SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader32 +
                                    OffsetToSectionTable);

                NumberOfSubsections = FileHeader->NumberOfSections;

                //
                // If any global subsection is unaligned then map the image
                // with a single subsection.  We could get clever here if/when
                // we encounter an unaligned global subsection, by seeing if
                // following (n consecutive) subsections are also global and
                // the last one ends on an aligned boundary - and if so,
                // still run the image natively, but it's probably unlikely
                // that any image has more than one global subsection.
                //

                CheckSplitPages = TRUE;
                while (NumberOfSubsections > 0) {

                    if ((SectionTableEntry->Characteristics & IMAGE_SCN_MEM_SHARED) &&
                        ((BYTE_OFFSET (SectionTableEntry->VirtualAddress) != 0) ||
                         (BYTE_OFFSET (SectionTableEntry->VirtualAddress + SectionTableEntry->Misc.VirtualSize) <= PAGE_4K))) {

                        CheckSplitPages = FALSE;
                        break;
                    }

                    SectionTableEntry += 1;
                    NumberOfSubsections -= 1;
                }
            }
        }

#endif

    }
#endif

    NumberOfPtes = BYTES_TO_PAGES (SizeOfImage);

    if (NumberOfPtes == 0) {
        Status = STATUS_INVALID_IMAGE_FORMAT;
        MI_BAD_IMAGE (0xA);
        goto BadPeImageSegment;
    }

#if defined (_WIN64)
    if (NumberOfPtes >= _4gb) {
        Status = STATUS_INVALID_IMAGE_FORMAT;
        MI_BAD_IMAGE (0xB);
        goto BadPeImageSegment;
    }
#endif

    //
    // Set the appropriate bit for .NET images inside the image's
    // section loader flags.
    //

    if ((ComPlusDirectoryEntry->VirtualAddress != 0) &&
        (ComPlusDirectoryEntry->Size != 0)) {

        LoaderFlags |= IMAGE_LOADER_FLAGS_COMPLUS;
    }

    RoundingAlignment = ImageAlignment;
    NumberOfSubsections = FileHeader->NumberOfSections;

    if ((ImageAlignment >= PAGE_SIZE) || (CheckSplitPages == TRUE)) {

        //
        // Allocate a control area and a subsection for each section
        // header plus one for the image header which has no section.
        //

        SubsectionsAllocated = NumberOfSubsections + 1;

        ControlArea = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(CONTROL_AREA) +
                                                    (sizeof(SUBSECTION) *
                                                    SubsectionsAllocated),
                                            'iCmM');
        SingleSubsection = FALSE;
    }
    else {

        //
        // The image alignment is less than the page size,
        // map the image with a single subsection.
        //

        ControlArea = ExAllocatePoolWithTag (NonPagedPool,
                      sizeof(CONTROL_AREA) + sizeof(SUBSECTION),
                      MMCONTROL);

        SubsectionsAllocated = 1;
        SingleSubsection = TRUE;
    }

    if (ControlArea == NULL) {

        //
        // The requested pool could not be allocated.
        //

        Status = STATUS_INSUFFICIENT_RESOURCES;
        MI_BAD_IMAGE (0xC);
        goto BadPeImageSegment;
    }

    //
    // Zero the control area and the FIRST subsection.
    //

    RtlZeroMemory (ControlArea, sizeof(CONTROL_AREA) + sizeof(SUBSECTION));

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    Subsection = (PSUBSECTION)(ControlArea + 1);

#if defined (_IA64_)
    if (ImageAlignment < PAGE_SIZE) {

        if ((FileHeader->Machine < USER_SHARED_DATA->ImageNumberLow) ||
            (FileHeader->Machine > USER_SHARED_DATA->ImageNumberHigh)) {

            if (CheckSplitPages == FALSE) {

                if (KeGetPreviousMode() != KernelMode) {
                    InvalidAlignmentAllowed = TRUE;
                }
                else {

                    //
                    // Don't allow kernel prefetch of wow images on IA64
                    // because these images might have shared subsections
                    // which are not handled properly.  The right fix for
                    // this is to combine image section creation for both
                    // prefetch (kernel) and non-prefetch (user) callers.
                    // However, we only found this just before shipping so
                    // this is risky as test coverage is light.  The worst
                    // implication of this disabling is just lower performance
                    // but is not a correctness issue.
                    //

                    Status = STATUS_INVALID_IMAGE_PROTECT;
                    MI_BAD_IMAGE (0x25);
                    goto BadPeImageSegment;
                }
            }
        }
    }
    OrigNumberOfPtes = NumberOfPtes;
#endif

    SizeOfSegment = sizeof(SEGMENT) + (sizeof(MMPTE) * ((ULONG)NumberOfPtes - 1)) +
                    sizeof(SECTION_IMAGE_INFORMATION);

    NewSegment = ExAllocatePoolWithTag (PagedPool | POOL_MM_ALLOCATION,
                                        SizeOfSegment,
                                        MMSECT);

    if (NewSegment == NULL) {

        //
        // The requested pool could not be allocated.
        //

        MI_BAD_IMAGE (0xD);
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto BadPeImageSegment;
    }

    *Segment = NewSegment;

    RtlZeroMemory (NewSegment, sizeof(SEGMENT));

    NewSegment->PrototypePte = &NewSegment->ThePtes[0];

    PointerPte = &NewSegment->ThePtes[0];

    Pfn1->u2.Blink = (PFN_NUMBER) PointerPte;

    NewSegment->ControlArea = ControlArea;
    NewSegment->u2.ImageInformation =
        (PSECTION_IMAGE_INFORMATION)((PCHAR)NewSegment + sizeof(SEGMENT) +
                                       (sizeof(MMPTE) * (NumberOfPtes - 1)));
    NewSegment->TotalNumberOfPtes = (ULONG) NumberOfPtes;
    NewSegment->NonExtendedPtes = (ULONG) NumberOfPtes;
    NewSegment->SizeOfSegment = NumberOfPtes * PAGE_SIZE;

    RtlZeroMemory (NewSegment->u2.ImageInformation,
                   sizeof (SECTION_IMAGE_INFORMATION));

#if DBG

    //
    // Zero fill the prototype PTEs so they can be checked in the error
    // path to make sure no valid entries got left behind.
    //

    if (NumberOfPtes != 0) {
        MiZeroMemoryPte (PointerPte, NumberOfPtes);
    }

#endif

//
// This code is built twice on the Win64 build - once for PE32+ and once for
// PE32 images.
//
#define INIT_IMAGE_INFORMATION(OptHdr) {                            \
    NewSegment->u2.ImageInformation->TransferAddress =              \
                    (PVOID)((ULONG_PTR)((OptHdr).ImageBase) +       \
                            (OptHdr).AddressOfEntryPoint);          \
    NewSegment->u2.ImageInformation->MaximumStackSize =             \
                            (OptHdr).SizeOfStackReserve;            \
    NewSegment->u2.ImageInformation->CommittedStackSize =           \
                            (OptHdr).SizeOfStackCommit;             \
    NewSegment->u2.ImageInformation->SubSystemType =                \
                            (OptHdr).Subsystem;                     \
    NewSegment->u2.ImageInformation->SubSystemMajorVersion = (USHORT)((OptHdr).MajorSubsystemVersion); \
    NewSegment->u2.ImageInformation->SubSystemMinorVersion = (USHORT)((OptHdr).MinorSubsystemVersion); \
    NewSegment->u2.ImageInformation->DllCharacteristics =           \
                            (OptHdr).DllCharacteristics;            \
    NewSegment->u2.ImageInformation->ImageContainsCode =            \
                            (BOOLEAN)(((OptHdr).SizeOfCode != 0) || \
                                      ((OptHdr).AddressOfEntryPoint != 0)); \
    }

#if defined (_WIN64)
    if (NtHeader) {
#endif
        INIT_IMAGE_INFORMATION(NtHeader->OptionalHeader);
#if defined (_WIN64)
    }
    else {

        //
        // The image is 32-bit so use the 32-bit header
        //

        INIT_IMAGE_INFORMATION(NtHeader32->OptionalHeader);
    }
#endif
    #undef INIT_IMAGE_INFORMATION

    NewSegment->u2.ImageInformation->ImageCharacteristics =
                            FileHeader->Characteristics;
    NewSegment->u2.ImageInformation->Machine = FileHeader->Machine;
    NewSegment->u2.ImageInformation->LoaderFlags = LoaderFlags;

    ControlArea->Segment = NewSegment;
    ControlArea->NumberOfSectionReferences = 1;
    ControlArea->NumberOfUserReferences = 1;
    ControlArea->u.Flags.BeingCreated = 1;
    ControlArea->u.Flags.Image = 1;
    ControlArea->u.Flags.File = 1;

    if ((ActiveDataReferences == TRUE) ||
        (IoIsDeviceEjectable(File->DeviceObject)) ||
        ((FileHeader->Characteristics &
                IMAGE_FILE_REMOVABLE_RUN_FROM_SWAP) &&
         (FILE_REMOVABLE_MEDIA & File->DeviceObject->Characteristics)) ||
        ((FileHeader->Characteristics &
                IMAGE_FILE_NET_RUN_FROM_SWAP) &&
         (FILE_REMOTE_DEVICE & File->DeviceObject->Characteristics))) {

        //
        // This file resides on a floppy disk or a removable media or
        // network with flags set indicating it should be copied
        // to the paging file.
        //

        ControlArea->u.Flags.FloppyMedia = 1;
    }

#if DBG
    if (MiMakeImageFloppy[0] & 0x1) {
        MiMakeImageFloppy[1] += 1;
        ControlArea->u.Flags.FloppyMedia = 1;
    }
#endif

    if (FILE_REMOTE_DEVICE & File->DeviceObject->Characteristics) {

        //
        // This file resides on a redirected drive.
        //

        ControlArea->u.Flags.Networked = 1;
    }

    ControlArea->FilePointer = File;

    //
    // Build the subsection and prototype PTEs for the image header.
    //

    Subsection->ControlArea = ControlArea;
    NextVa = ImageBase;

#if defined (_WIN64)

    //
    // Don't let bogus headers cause system alignment faults.
    //

    if (FileHeader->SizeOfOptionalHeader & (sizeof (ULONG_PTR) - 1)) {
        Status = STATUS_INVALID_IMAGE_FORMAT;
        MI_BAD_IMAGE (0xE);
        goto BadPeImageSegment;
    }
#endif

    if ((NextVa & (X64K - 1)) != 0) {

        //
        // Image header is not aligned on a 64k boundary.
        //

        Status = STATUS_INVALID_IMAGE_FORMAT;
        MI_BAD_IMAGE (0xF);
        goto BadPeImageSegment;
    }

    NewSegment->BasedAddress = (PVOID) NextVa;

#if DBG
    if (NextVa != 0 && NextVa == MiMatchSectionBase) {
        DbgPrint ("MM: NewSegment %p being created\n", NewSegment);
        DbgBreakPoint ();
    }
#endif

    if (SizeOfHeaders >= SizeOfImage) {
        Status = STATUS_INVALID_IMAGE_FORMAT;
        MI_BAD_IMAGE (0x10);
        goto BadPeImageSegment;
    }

    if (CheckSplitPages == FALSE) {
        Subsection->PtesInSubsection = MI_ROUND_TO_SIZE (SizeOfHeaders, ImageAlignment) >> PAGE_SHIFT;
    }
    else {
        Subsection->PtesInSubsection =
            (ULONG) MI_COMPUTE_PAGES_SPANNED (0,
                        MI_ROUND_TO_SIZE (SizeOfHeaders, ImageAlignment));
    }

    PointerPte = NewSegment->PrototypePte;
    Subsection->SubsectionBase = PointerPte;

    TempPte.u.Long = MiGetSubsectionAddressForPte (Subsection);
    TempPte.u.Soft.Prototype = 1;

    NewSegment->SegmentPteTemplate = TempPte;
    SectorOffset = 0;

    if (SingleSubsection == TRUE) {

        //
        // The image is aligned on less than a page size boundary.
        // Initialize the single subsection to refer to the image.
        //

        PointerPte = NewSegment->PrototypePte;

        Subsection->PtesInSubsection = (ULONG) NumberOfPtes;

#if !defined (_WIN64)

        //
        // Note this only needs to be checked for 32-bit systems as NT64
        // does a much more extensive reallocation of images that are built
        // with alignment less than PAGE_SIZE.
        //

        if ((UINT64)SizeOfImage < (UINT64)EndOfFile.QuadPart) {

            //
            // Images that have a size of image (according to the header) that's
            // smaller than the actual file only get that many prototype PTEs.
            // Initialize the subsection properly so no one can read off the
            // end as that would corrupt the PFN database element's original
            // PTE entry.
            //

            Subsection->NumberOfFullSectors = (SizeOfImage >> MMSECTOR_SHIFT);

            Subsection->u.SubsectionFlags.SectorEndOffset =
                                  SizeOfImage & MMSECTOR_MASK;
        }
        else {
#endif
            Subsection->NumberOfFullSectors =
                                (ULONG)(EndOfFile.QuadPart >> MMSECTOR_SHIFT);

            ASSERT ((ULONG)(EndOfFile.HighPart & 0xFFFFF000) == 0);

            Subsection->u.SubsectionFlags.SectorEndOffset =
                                  EndOfFile.LowPart & MMSECTOR_MASK;
#if !defined (_WIN64)
        }
#endif

        Subsection->u.SubsectionFlags.Protection = MM_EXECUTE_WRITECOPY;

        //
        // Set all the PTEs to the execute-read-write protection.
        // The section will control access to these and the segment
        // must provide a method to allow other users to map the file
        // for various protections.
        //

        TempPte.u.Soft.Protection = MM_EXECUTE_WRITECOPY;

        NewSegment->SegmentPteTemplate = TempPte;

        //
        // Invalid image alignments are supported for cross platform
        // emulation.  Only IA64 requires extra handling because
        // the native page size is larger than x86.
        //

        if (InvalidAlignmentAllowed == TRUE) {

            TempPteDemandZero.u.Long = 0;
            TempPteDemandZero.u.Soft.Protection = MM_EXECUTE_WRITECOPY;
            SectorOffset = 0;

            for (i = 0; i < NumberOfPtes; i += 1) {

                //
                // Set prototype PTEs.
                //

                if (SectorOffset < EndOfFile.LowPart) {

                    //
                    // Data resides on the disk, refer to the control section.
                    //

                    MI_WRITE_INVALID_PTE (PointerPte, TempPte);

                }
                else {

                    //
                    // Data does not reside on the disk, use Demand zero pages.
                    //

                    MI_WRITE_INVALID_PTE (PointerPte, TempPteDemandZero);
                }

                SectorOffset += PAGE_SIZE;
                PointerPte += 1;
            }
        }
        else {

            //
            // Set all the prototype PTEs to refer to the control section.
            //

            MiFillMemoryPte (PointerPte, NumberOfPtes, TempPte.u.Long);

            PointerPte += NumberOfPtes;
        }

        NewSegment->u1.ImageCommitment = NumberOfPtes;
    }
    else {

        //
        // Alignment is PAGE_SIZE or greater (or the image is a 32-bit
        // 4K aligned image being run with ALTPTE support).
        //

        if (Subsection->PtesInSubsection > NumberOfPtes) {

            //
            // Inconsistent image, size does not agree with header.
            //

            Status = STATUS_INVALID_IMAGE_FORMAT;
            MI_BAD_IMAGE (0x11);
            goto BadPeImageSegment;
        }

        NumberOfPtes -= Subsection->PtesInSubsection;

        Subsection->NumberOfFullSectors = SizeOfHeaders >> MMSECTOR_SHIFT;

        Subsection->u.SubsectionFlags.SectorEndOffset =
            SizeOfHeaders & MMSECTOR_MASK;

        Subsection->u.SubsectionFlags.ReadOnly = 1;
        Subsection->u.SubsectionFlags.Protection = MM_READONLY;

        TempPte.u.Soft.Protection = MM_READONLY;
        NewSegment->SegmentPteTemplate = TempPte;

        for (i = 0; i < Subsection->PtesInSubsection; i += 1) {

            //
            // Set all the prototype PTEs to refer to the control section.
            //

            if (SectorOffset < SizeOfHeaders) {
                MI_WRITE_INVALID_PTE (PointerPte, TempPte);
            }
            else {
                MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
            }
            SectorOffset += PAGE_SIZE;
            PointerPte += 1;
        }

        if (CheckSplitPages == TRUE) {
#if defined (_MIALT4K_)
            NextVa += (MI_ROUND_TO_SIZE (SizeOfHeaders, PAGE_4K));
#endif
        }
        else {
            NextVa += (i * PAGE_SIZE);
        }
    }

    //
    // Build the additional subsections.
    //

    PreferredImageBase = ImageBase;

    //
    // At this point the object table is read in (if it was not
    // already read in) and may displace the image header.
    //

    SectionTableEntry = NULL;
    OffsetToSectionTable = sizeof(ULONG) +
                              sizeof(IMAGE_FILE_HEADER) +
                              FileHeader->SizeOfOptionalHeader;

    if ((BYTE_OFFSET(NtHeader) + OffsetToSectionTable +
#if defined (_WIN64)
                BYTE_OFFSET(NtHeader32) +
#endif
                ((NumberOfSubsections + 1) *
                sizeof (IMAGE_SECTION_HEADER))) <= PAGE_SIZE) {

        //
        // Section tables are within the header which was read.
        //

#if defined(_WIN64)
        if (NtHeader32) {
            SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader32 +
                                    OffsetToSectionTable);
        }
        else
#endif
        {
            SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                                    OffsetToSectionTable);
        }

    }
    else {

        //
        // Has an extended header been read in and are the object
        // tables resident?
        //

        if (ExtendedHeader != NULL) {

#if defined(_WIN64)
            if (NtHeader32) {
                SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader32 +
                                        OffsetToSectionTable);
            }
            else
#endif
            {
                SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                                        OffsetToSectionTable);
            }

            //
            // Is the whole range of object tables mapped by the
            // extended header?
            //

            if ((((PCHAR)SectionTableEntry +
                 ((NumberOfSubsections + 1) *
                    sizeof (IMAGE_SECTION_HEADER))) -
                         (PCHAR)ExtendedHeader) >
                                            MM_MAXIMUM_IMAGE_HEADER) {
                SectionTableEntry = NULL;

            }
        }
    }

    if (SectionTableEntry == NULL) {

        //
        // The section table entries are not in the same
        // pages as the other data already read in.  Read in
        // the object table entries.
        //

        if (ExtendedHeader == NULL) {
            ExtendedHeader = ExAllocatePoolWithTag (NonPagedPool,
                                                    MM_MAXIMUM_IMAGE_HEADER,
                                                    MMTEMPORARY);
            if (ExtendedHeader == NULL) {
                Status = STATUS_INSUFFICIENT_RESOURCES;
                MI_BAD_IMAGE (0x12);
                goto BadPeImageSegment;
            }

            //
            // Build an MDL for the operation.
            //

            MmInitializeMdl (Mdl, ExtendedHeader, MM_MAXIMUM_IMAGE_HEADER);

            MmBuildMdlForNonPagedPool (Mdl);
        }

        StartingOffset.LowPart = PtrToUlong(PAGE_ALIGN (
                                    (ULONG)DosHeader->e_lfanew +
                                    OffsetToSectionTable));

        SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)ExtendedHeader +
                                BYTE_OFFSET((ULONG)DosHeader->e_lfanew +
                                OffsetToSectionTable));

        KeClearEvent (&InPageEvent);
        Status = IoPageRead (File,
                             Mdl,
                             &StartingOffset,
                             &InPageEvent,
                             &IoStatus);

        if (Status == STATUS_PENDING) {
            KeWaitForSingleObject (&InPageEvent,
                                   WrPageIn,
                                   KernelMode,
                                   FALSE,
                                   (PLARGE_INTEGER)NULL);
            Status = IoStatus.Status;
        }

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
        }

        if (!NT_SUCCESS(Status)) {
            MI_BAD_IMAGE (0x13);
            if ((Status != STATUS_FILE_LOCK_CONFLICT) && (Status != STATUS_FILE_IS_OFFLINE)) {
                Status = STATUS_INVALID_FILE_FOR_SECTION;
            }
            goto BadPeImageSegment;
        }

        //
        // From this point on NtHeader is only valid if it
        // was in the first page of the image, otherwise reading in
        // the object tables wiped it out.
        //
    }

    if ((SingleSubsection == TRUE) && (InvalidAlignmentAllowed == FALSE)) {

        //
        // The image header is no longer valid.
        //
        // Loop through all sections and make sure there is no
        // uninitialized data.
        //

        Status = STATUS_SUCCESS;

        while (NumberOfSubsections > 0) {
            if (SectionTableEntry->Misc.VirtualSize == 0) {
                SectionVirtualSize = SectionTableEntry->SizeOfRawData;
            }
            else {
                SectionVirtualSize = SectionTableEntry->Misc.VirtualSize;
            }

            //
            // If the raw pointer + raw size overflows a long word,
            // return an error.
            //

            if (SectionTableEntry->PointerToRawData +
                        SectionTableEntry->SizeOfRawData <
                SectionTableEntry->PointerToRawData) {

                KdPrint(("MMCREASECT: invalid section/file size %Z\n",
                    &File->FileName));

                Status = STATUS_INVALID_IMAGE_FORMAT;
                MI_BAD_IMAGE (0x14);
                break;
            }

            //
            // If the virtual size and address does not match the rawdata
            // and invalid alignments not allowed return an error.
            //

            if (((SectionTableEntry->PointerToRawData !=
                  SectionTableEntry->VirtualAddress))
                            ||
                   (SectionVirtualSize > SectionTableEntry->SizeOfRawData)) {

                KdPrint(("MMCREASECT: invalid BSS/Trailingzero %Z\n",
                        &File->FileName));

                Status = STATUS_INVALID_IMAGE_FORMAT;
                MI_BAD_IMAGE (0x15);
                break;
            }

            SectionTableEntry += 1;
            NumberOfSubsections -= 1;
        }


        if (!NT_SUCCESS(Status)) {
            goto BadPeImageSegment;
        }

        goto PeReturnSuccess;
    }

    if ((SingleSubsection == TRUE) && (InvalidAlignmentAllowed == TRUE)) {

        TempNumberOfSubsections = NumberOfSubsections;
        TempSectionTableEntry = SectionTableEntry;

        //
        // The image header is no longer valid.
        //
        // Loop through all sections and make sure there is no
        // uninitialized data.
        //
        // Also determine if there are shared data sections and
        // the number of extra PTEs they require.
        //

        AdditionalSubsections = 0;
        AdditionalPtes = 0;
        AdditionalBasePtes = 0;
        RoundingAlignment = PAGE_SIZE;

        while (TempNumberOfSubsections > 0) {
            ULONG EndOfSection;
            ULONG ExtraPages;

            if (TempSectionTableEntry->Misc.VirtualSize == 0) {
                SectionVirtualSize = TempSectionTableEntry->SizeOfRawData;
            }
            else {
                SectionVirtualSize = TempSectionTableEntry->Misc.VirtualSize;
            }

            EndOfSection = TempSectionTableEntry->PointerToRawData +
                           TempSectionTableEntry->SizeOfRawData;

            //
            // If the raw pointer + raw size overflows a long word, return an error.
            //

            if (EndOfSection < TempSectionTableEntry->PointerToRawData) {

                KdPrint(("MMCREASECT: invalid section/file size %Z\n",
                    &File->FileName));

                Status = STATUS_INVALID_IMAGE_FORMAT;
                MI_BAD_IMAGE (0x16);

                goto BadPeImageSegment;
            }

            //
            // If the section goes past SizeOfImage then allocate
            // additional PTEs.  On x86 this is handled by the subsection
            // mapping.  Note the additional data must be in memory so
            // it can be shuffled around later.
            //

            if ((EndOfSection <= EndOfFile.LowPart) &&
                (EndOfSection > SizeOfImage)) {

                //
                // Allocate enough PTEs to cover the end of this section.
                // Maximize with any other sections that extend beyond SizeOfImage
                //

                ExtraPages = MI_ROUND_TO_SIZE (EndOfSection, RoundingAlignment) >> PAGE_SHIFT;
                if ((ExtraPages > OrigNumberOfPtes) &&
                    (ExtraPages - OrigNumberOfPtes > AdditionalBasePtes)) {

                    AdditionalBasePtes = ExtraPages - (ULONG) OrigNumberOfPtes;
                }
            }

            //
            // Count number of shared data sections and additional PTEs needed.
            //

            if ((TempSectionTableEntry->Characteristics & IMAGE_SCN_MEM_SHARED) &&
                (!(TempSectionTableEntry->Characteristics & IMAGE_SCN_MEM_EXECUTE) ||
                 (TempSectionTableEntry->Characteristics & IMAGE_SCN_MEM_WRITE))) {
                AdditionalPtes +=
                    MI_ROUND_TO_SIZE (SectionVirtualSize, RoundingAlignment) >>
                                                                PAGE_SHIFT;
                AdditionalSubsections += 1;
            }

            TempSectionTableEntry += 1;
            TempNumberOfSubsections -= 1;
        }

        if (AdditionalBasePtes == 0 && (AdditionalSubsections == 0 || AdditionalPtes == 0)) {

            //
            // No shared data sections.
            //

            goto PeReturnSuccess;
        }

        //
        // There are additional base PTEs or shared data sections.
        // For shared sections, allocate new PTEs for these sections
        // at the end of the image.  The usermode wow loader will change
        // fixups to point to the new pages.
        //
        // First reallocate the control area.
        //

        NewSubsectionsAllocated = SubsectionsAllocated + AdditionalSubsections;

        NewControlArea = ExAllocatePoolWithTag(NonPagedPool,
                                    (ULONG) (sizeof(CONTROL_AREA) +
                                            (sizeof(SUBSECTION) *
                                                NewSubsectionsAllocated)),
                                                'iCmM');
        if (NewControlArea == NULL) {
            MI_BAD_IMAGE (0x17);
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto BadPeImageSegment;
        }

        //
        // Copy the old control area to the new one, modify some fields.
        //

        RtlCopyMemory (NewControlArea, ControlArea,
                        sizeof(CONTROL_AREA) +
                            sizeof(SUBSECTION) * SubsectionsAllocated);

        //
        // Now allocate a new segment that has the newly calculated number
        // of PTEs, initialize it from the previously allocated new segment,
        // and overwrite the fields that should be changed.
        //

        OldSegment = NewSegment;


        OrigNumberOfPtes += AdditionalBasePtes;
        PointerPte += AdditionalBasePtes;

        SizeOfSegment = sizeof(SEGMENT) +
                     (sizeof(MMPTE) * (OrigNumberOfPtes + AdditionalPtes - 1)) +
                        sizeof(SECTION_IMAGE_INFORMATION);

        NewSegment = ExAllocatePoolWithTag (PagedPool | POOL_MM_ALLOCATION,
                                            SizeOfSegment,
                                            MMSECT);

        if (NewSegment == NULL) {

            //
            // The requested pool could not be allocated.
            //

            MI_BAD_IMAGE (0x18);
            ExFreePool (NewControlArea);
            ExFreePool (OldSegment);
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto BadPeImageSegment;
        }

        *Segment = NewSegment;
        RtlCopyMemory (NewSegment, OldSegment, sizeof(SEGMENT));

        //
        // Align the prototype PTEs on the proper boundary.
        //

        NewPointerPte = &NewSegment->ThePtes[0];
        NewSegment->PrototypePte = &NewSegment->ThePtes[0];

        PointerPte = NewSegment->PrototypePte +
                                       (PointerPte - OldSegment->PrototypePte);

        NewSegment->ControlArea = NewControlArea;
        NewSegment->SegmentFlags.ExtraSharedWowSubsections = 1;
        NewSegment->u2.ImageInformation =
            (PSECTION_IMAGE_INFORMATION)((PCHAR)NewSegment + sizeof(SEGMENT) +
                     (sizeof(MMPTE) * (OrigNumberOfPtes + AdditionalPtes - 1)));
        NewSegment->TotalNumberOfPtes = (ULONG)(OrigNumberOfPtes + AdditionalPtes);
        NewSegment->NonExtendedPtes = (ULONG)(OrigNumberOfPtes + AdditionalPtes);
        NewSegment->SizeOfSegment = (UINT64)(OrigNumberOfPtes + AdditionalPtes) * PAGE_SIZE;

        RtlCopyMemory (NewSegment->u2.ImageInformation,
                       OldSegment->u2.ImageInformation,
                       sizeof (SECTION_IMAGE_INFORMATION));

        //
        // Adjust the prototype PTE pointer of the PE header page to point
        // at the new segment.
        //

        ASSERT (Pfn1->u2.Blink == (PFN_NUMBER) OldSegment->PrototypePte);
        Pfn1->u2.Blink = (PFN_NUMBER) NewSegment->PrototypePte;

        //
        // Now change the fields in the subsections to account for the new
        // control area and the new segment. Also change the PTEs in the
        // newly allocated segment to point to the new subsections.
        //

        NewControlArea->Segment = NewSegment;

        Subsection = (PSUBSECTION)(ControlArea + 1);
        NewSubsection = (PSUBSECTION)(NewControlArea + 1);
        NewSubsection->PtesInSubsection += AdditionalBasePtes;

        for (i = 0; i < SubsectionsAllocated; i += 1) {

            //
            // Note: SubsectionsAllocated is always 1 (for wx86), so this loop
            // is executed only once.
            //

            NewSubsection->ControlArea = (PCONTROL_AREA) NewControlArea;

            NewSubsection->SubsectionBase = NewSegment->PrototypePte +
                    (Subsection->SubsectionBase - OldSegment->PrototypePte);

            NewPointerPte = NewSegment->PrototypePte;
            OldPointerPte = OldSegment->PrototypePte;

            TempPte.u.Long = MiGetSubsectionAddressForPte (NewSubsection);
            TempPte.u.Soft.Prototype = 1;

            for (j = 0; j < OldSegment->TotalNumberOfPtes+AdditionalBasePtes; j += 1) {

                if ((OldPointerPte->u.Soft.Prototype == 1) &&
                    (MiGetSubsectionAddress (OldPointerPte) == Subsection)) {
                    OriginalProtection = MI_GET_PROTECTION_FROM_SOFT_PTE (OldPointerPte);
                    TempPte.u.Soft.Protection = OriginalProtection;
                    MI_WRITE_INVALID_PTE (NewPointerPte, TempPte);
                }
                else if (i == 0) {

                    //
                    // Since the outer for loop is executed only once, there
                    // is no need for the i == 0 above, but it is safer to
                    // have it. If the code changes later and other sections
                    // are added, their PTEs will get initialized here as
                    // DemandZero and if they are not DemandZero, they will be
                    // overwritten in a later iteration of the outer loop.
                    // For now, this else if clause will be executed only
                    // for DemandZero PTEs.
                    //

                    OriginalProtection = MI_GET_PROTECTION_FROM_SOFT_PTE (OldPointerPte);
                    TempPteDemandZero.u.Long = 0;
                    TempPteDemandZero.u.Soft.Protection = OriginalProtection;
                    MI_WRITE_INVALID_PTE (NewPointerPte, TempPteDemandZero);
                }

                NewPointerPte += 1;

                //
                // Stop incrementing the OldPointerPte at the last entry
                // and use it for the additional base PTEs.
                //

                if (j < OldSegment->TotalNumberOfPtes - 1) {
                    OldPointerPte += 1;
                }
            }

            Subsection += 1;
            NewSubsection += 1;
        }


        RtlZeroMemory (NewSubsection,
                            sizeof(SUBSECTION) * AdditionalSubsections);

        ExFreePool (OldSegment);
        ExFreePool (ControlArea);
        ControlArea = (PCONTROL_AREA) NewControlArea;

        //
        // Adjust some variables that are used below.
        // PointerPte has already been set above before OldSegment was freed.
        //

        SubsectionsAllocated = NewSubsectionsAllocated;
        Subsection = NewSubsection - 1; // Points to last used subsection.
        NumberOfPtes = AdditionalPtes;  // # PTEs that haven't yet been used in
                                        // previous subsections.

        //
        // Additional base PTEs have been added.  Only continue if there are
        // additional subsections to process.
        //

        if (AdditionalSubsections == 0 || AdditionalPtes == 0) {
            // no shared data sections
            goto PeReturnSuccess;
        }
    }

    ImageFileSize = EndOfFile.LowPart + 1;

#if defined (_MIALT4K_)
    if (CheckSplitPages == TRUE) {
        RoundingAlignment = PAGE_SIZE;
    }
    PreviousSectionCharacteristics = IMAGE_SCN_MEM_READ;
#endif

    while (NumberOfSubsections > 0) {

        if ((InvalidAlignmentAllowed == FALSE) ||
            ((SectionTableEntry->Characteristics & IMAGE_SCN_MEM_SHARED) &&
             (!(SectionTableEntry->Characteristics & IMAGE_SCN_MEM_EXECUTE) ||
              (SectionTableEntry->Characteristics & IMAGE_SCN_MEM_WRITE)))) {

            //
            // Handle case where virtual size is 0.
            //

            if (SectionTableEntry->Misc.VirtualSize == 0) {
                SectionVirtualSize = SectionTableEntry->SizeOfRawData;
            }
            else {
                SectionVirtualSize = SectionTableEntry->Misc.VirtualSize;
            }

            //
            // Fix for Borland linker problem.  The SizeOfRawData can
            // be a zero, but the PointerToRawData is not zero.
            // Set it to zero.
            //

            if (SectionTableEntry->SizeOfRawData == 0) {
                SectionTableEntry->PointerToRawData = 0;
            }

            //
            // If the section information wraps return an error.
            //

            if (SectionTableEntry->PointerToRawData +
                        SectionTableEntry->SizeOfRawData <
                SectionTableEntry->PointerToRawData) {

                MI_BAD_IMAGE (0x19);
                Status = STATUS_INVALID_IMAGE_FORMAT;
                goto BadPeImageSegment;
            }

            Subsection->NextSubsection = (Subsection + 1);

            Subsection += 1;
            Subsection->ControlArea = ControlArea;
            Subsection->NextSubsection = NULL;
            Subsection->UnusedPtes = 0;

            if (((NextVa != (PreferredImageBase + SectionTableEntry->VirtualAddress)) && (InvalidAlignmentAllowed == FALSE)) ||
                (SectionVirtualSize == 0)) {

                //
                // The specified virtual address does not align
                // with the next prototype PTE.
                //

                MI_BAD_IMAGE (0x1A);
                Status = STATUS_INVALID_IMAGE_FORMAT;
                goto BadPeImageSegment;
            }

            Subsection->PtesInSubsection =
                MI_ROUND_TO_SIZE (SectionVirtualSize, RoundingAlignment)
                                                                >> PAGE_SHIFT;

#if defined (_MIALT4K_)
            if (CheckSplitPages == TRUE) {
                Subsection->PtesInSubsection =
                    (ULONG) MI_COMPUTE_PAGES_SPANNED (NextVa,
                                                      SectionVirtualSize);
            }
#endif

            if (Subsection->PtesInSubsection > NumberOfPtes) {

                LOGICAL ImageOk;

                ImageOk = FALSE;

#if defined (_MIALT4K_)

                //
                // If this is a split wow binary then the PtesInSubsection
                // calculation may not have factored in that we may shortly
                // slide (or completely combine) this subsection into
                // the previous one.
                //

                if ((CheckSplitPages == TRUE) &&
                    (BYTE_OFFSET (SectionTableEntry->VirtualAddress) != 0)) {

                    if (SectionVirtualSize > PAGE_4K) {

                        //
                        // This subsection will slide below, check it
                        // accordingly.
                        //

                        if ((ULONG) MI_COMPUTE_PAGES_SPANNED (NextVa + PAGE_4K,
                               SectionVirtualSize - PAGE_4K) == NumberOfPtes) {

                            //
                            // The slide will make things right so
                            // this image is valid after all.  March on.
                            // Note this may result in the NumberOfPtes local
                            // temporarily going negative, but that's ok as it
                            // will get fixed at the time of the slide.
                            //

                            ImageOk = TRUE;
                        }
                    }
                    else {

                        //
                        // This subsection will be combined below so we know
                        // it's ok to keep going.
                        //
                        // Note this may result in the NumberOfPtes local
                        // temporarily going negative, but that's ok as it
                        // will get fixed at the time of the slide.
                        //

                        ImageOk = TRUE;
                    }
                }
#endif

                if (ImageOk == FALSE) {

                    //
                    // Inconsistent image, size does not agree with
                    // object tables.
                    //

                    MI_BAD_IMAGE (0x1B);
                    Status = STATUS_INVALID_IMAGE_FORMAT;
                    goto BadPeImageSegment;
                }
            }

            NumberOfPtes -= Subsection->PtesInSubsection;

            Subsection->u.LongFlags = 0;
            Subsection->StartingSector =
                          SectionTableEntry->PointerToRawData >> MMSECTOR_SHIFT;

            //
            // Align ending sector on file align boundary.
            //

            EndingAddress = (SectionTableEntry->PointerToRawData +
                                         SectionTableEntry->SizeOfRawData +
                                         FileAlignment) & ~FileAlignment;

            Subsection->NumberOfFullSectors = (ULONG)
                             ((EndingAddress >> MMSECTOR_SHIFT) -
                             Subsection->StartingSector);

            Subsection->u.SubsectionFlags.SectorEndOffset =
                                        (ULONG) EndingAddress & MMSECTOR_MASK;

            Subsection->SubsectionBase = PointerPte;

            //
            // Build both a demand zero PTE and a PTE pointing to the
            // subsection.
            //

            TempPte.u.Long = 0;
            TempPteDemandZero.u.Long = 0;

            TempPte.u.Long = MiGetSubsectionAddressForPte (Subsection);
            TempPte.u.Soft.Prototype = 1;
            ImageFileSize = SectionTableEntry->PointerToRawData +
                                        SectionTableEntry->SizeOfRawData;
            TempPte.u.Soft.Protection =
                     MiGetImageProtection (SectionTableEntry->Characteristics);
            TempPteDemandZero.u.Soft.Protection = TempPte.u.Soft.Protection;

            if (SectionTableEntry->PointerToRawData == 0) {
                TempPte = TempPteDemandZero;
            }

            Subsection->u.SubsectionFlags.ReadOnly = 1;
            Subsection->u.SubsectionFlags.Protection = MI_GET_PROTECTION_FROM_SOFT_PTE (&TempPte);

            //
            // Assume the subsection will be unwritable and therefore
            // won't be charged for any commitment.
            //

            SectionCommit = FALSE;
            ImageCommit = FALSE;

            if (TempPte.u.Soft.Protection & MM_PROTECTION_WRITE_MASK) {
                if ((TempPte.u.Soft.Protection & MM_COPY_ON_WRITE_MASK)
                                                == MM_COPY_ON_WRITE_MASK) {

                    //
                    // This page is copy on write, charge ImageCommitment
                    // for all pages in this subsection.
                    //

                    ImageCommit = TRUE;
                }
                else {

                    //
                    // This page is write shared, charge commitment when
                    // the mapping completes.
                    //

                    SectionCommit = TRUE;
                    Subsection->u.SubsectionFlags.GlobalMemory = 1;
                    ControlArea->u.Flags.GlobalMemory = 1;
                }
            }

            NewSegment->SegmentPteTemplate = TempPte;
            SectorOffset = 0;
            SizeOfRawData = SectionTableEntry->SizeOfRawData;

#if defined (_MIALT4K_)

            //
            // Check whether split permissions need to be applied to straddlers.
            //

            if ((CheckSplitPages == TRUE) &&
                (BYTE_OFFSET (SectionTableEntry->VirtualAddress) != 0)) {

                ASSERT (BYTE_OFFSET (SectionTableEntry->VirtualAddress) == PAGE_4K);
                PreviousPte = PointerPte - 1;

                ASSERT ((PreviousPte >= NewSegment->PrototypePte) &&
                        (PreviousPte < NewSegment->PrototypePte + NewSegment->TotalNumberOfPtes));
                PteContents.u.Long = PreviousPte->u.Long;
                ASSERT (PteContents.u.Hard.Valid == 0);

                //
                // Read in (if it's filesystem backed instead of demand zero or
                // no access) 4K of the previous page.  Combine it with the
                // first 4K contents of this page.
                //

                if (PreviousPte == NewSegment->PrototypePte) {
                    StraddleFrameNumber = PageFrameNumber;
                    PageFrameNumber = 0;
                }
                else {
                    StraddleFrameNumber = MiGetPageForHeader (TRUE);

                    ASSERT (StraddleFrameNumber != 0);

                    Pfn1 = MI_PFN_ELEMENT (StraddleFrameNumber);

                    ASSERT (Pfn1->u1.Flink == 0);
                    Pfn1->u1.Flink = (PFN_NUMBER) ImagePages;
                    Pfn1->u2.Blink = (PFN_NUMBER) PreviousPte;

                    ImagePages = Pfn1;
                }

                //
                // Either the current 4k page or the previous 4k page (or both)
                // may need to be read from the filesystem (vs being BSS).
                //

                StraddleVa = MiMapSinglePage (NULL,
                                              StraddleFrameNumber,
                                              MmCached,
                                              HighPagePriority);

                if (StraddleVa == NULL) {
                    MI_BAD_IMAGE (0x1C);
                    Status = STATUS_INSUFFICIENT_RESOURCES;
                    goto BadPeImageSegment;
                }

                //
                // Create an event and MDL for the read operation.
                //

                HalfPage = ExAllocatePoolWithTag (NonPagedPool,
                                                  PAGE_SIZE,
                                                  MMTEMPORARY);

                if (HalfPage == NULL) {
                    MiUnmapSinglePage (StraddleVa);
                    MI_BAD_IMAGE (0x1D);
                    Status = STATUS_INSUFFICIENT_RESOURCES;
                    goto BadPeImageSegment;
                }

                HalfPagePhysicalAddress = MmGetPhysicalAddress (HalfPage);

                ASSERT (HalfPagePhysicalAddress.QuadPart != 0);

                HalfPageFrameNumber = (HalfPagePhysicalAddress.QuadPart >> PAGE_SHIFT);

                ReadCount = 0;

                if (PteContents.u.Soft.Prototype == 1) {

                    ReadSubsection = Subsection - 1;
                    ReadPte = PreviousPte;
TwoReads:

                    IoStatus.Information = 0;
    
                    ASSERT (Mdl == (PMDL) &StackMdl);
    
                    KeInitializeEvent (&InPageEvent, NotificationEvent, FALSE);
    
                    MmInitializeMdl (Mdl, NULL, PAGE_4K);

                    Mdl->MdlFlags |= MDL_PAGES_LOCKED;
    
                    Page = (PPFN_NUMBER)(Mdl + 1);
                    *Page = HalfPageFrameNumber;

                    StartingOffset.QuadPart = MI_STARTING_OFFSET (ReadSubsection,
                                                                  ReadPte);
    
                    TempOffset = MiEndingOffset (ReadSubsection);

                    ASSERT (StartingOffset.QuadPart < TempOffset.QuadPart);

                    if (((UINT64)StartingOffset.QuadPart + PAGE_4K) > (UINT64)TempOffset.QuadPart) {

                        ASSERT ((ULONG)(TempOffset.QuadPart - StartingOffset.QuadPart)
                                > 0);

                        ReadSize = (ULONG)(TempOffset.QuadPart - StartingOffset.QuadPart);

                        //
                        // Round the offset to a 512-byte offset as this
                        // will help filesystems optimize the transfer.
                        // Note that filesystems will always zero fill
                        // the remainder between VDL and the next 512-byte
                        // multiple and we have already zeroed the whole page.
                        //

                        ReadSize = ((ReadSize + MMSECTOR_MASK) & ~MMSECTOR_MASK);
                        Mdl->ByteCount = ReadSize;
                    }

                    Status = IoPageRead (File,
                                         Mdl,
                                         &StartingOffset,
                                         &InPageEvent,
                                         &IoStatus);
    
                    if (Status == STATUS_PENDING) {
                        KeWaitForSingleObject (&InPageEvent,
                                               WrPageIn,
                                               KernelMode,
                                               FALSE,
                                               NULL);
    
                        Status = IoStatus.Status;
                    }
    
                    if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                        MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
                    }
    
                    if (!NT_SUCCESS (Status)) {
                        MI_BAD_IMAGE (0x1E);
                        ExFreePool (HalfPage);
                        MiUnmapSinglePage (StraddleVa);
                        goto BadPeImageSegment;
                    }
    
                    //
                    // Less than a full page may be read from the file,
                    // only copy the amount that was read (the rest of
                    // the target page is zero already).
                    //

                    ASSERT (IoStatus.Information <= PAGE_4K);

                    if ((ReadCount == 0) &&
                        (PreviousPte == NewSegment->PrototypePte)) {

                        ASSERT (PageFrameNumber == 0);
                    }
                    else {

                        RtlCopyMemory ((PVOID) ((PCHAR) StraddleVa + ReadCount * PAGE_4K),
                                       HalfPage,
                                       IoStatus.Information);
                    }
                }

                ReadCount += 1;

                //
                // Now fill the second 4K of this native page.  Either the
                // existing zero fill is sufficient or it must be
                // retrieved from the filesystem.
                //

                if ((ReadCount == 1) && (SectionVirtualSize != 0)) {

                    if ((SectionTableEntry->SizeOfRawData != 0) &&
                        (SectionTableEntry->PointerToRawData != 0)) {

                        //
                        // Data must be retrieved from the filesystem.
                        //

                        ReadSubsection = Subsection;
                        ReadPte = PointerPte;
                        goto TwoReads;
                    }
                    else {
                        RtlZeroMemory ((PVOID) ((PCHAR) StraddleVa + PAGE_4K),
                                       PAGE_4K);
                    }
                }

                if ((ReadCount == 2) &&
                    (SectionTableEntry->SizeOfRawData < PAGE_4K)) {

                    //
                    // The second 4K of this native page contains a
                    // mixture of initialized data and BSS.  We already
                    // read the whole 4K so zero any BSS now.
                    //

                    RawDataSize = SectionTableEntry->SizeOfRawData;
                    RawDataSize &= ~(sizeof (ULONGLONG) - 1);

                    RtlZeroMemory ((PVOID) ((PCHAR) StraddleVa + PAGE_4K + 
                        RawDataSize),
                        PAGE_4K - RawDataSize);
                }

                MiUnmapSinglePage (StraddleVa);

                ExFreePool (HalfPage);

                //
                // Update the last prototype PTE of the previous subsection
                // to indicate the permissions may need to be split.  This
                // ensures the last PTE of the image header (for example)
                // gets marked correctly.
                //

                PteContents.u.Soft.SplitPermissions = 1;

                //
                // Store the protection for the last half of the page in
                // the subsection itself.  Later when the image is mapped we
                // must retrieve it from here - you would think you could just
                // look forward to the next subsection and get it from there -
                // but you can't for the scenario where we merge the last
                // subsection into the prior one and thus would have no
                // place to get the protection for the last half of the split
                // page in this case.
                //

                (Subsection - 1)->LastSplitPageProtection = Subsection->u.SubsectionFlags.Protection;

                //
                // The native PTE gets the most relaxed permissions of the
                // 2 split pages.  Each alternate PTE gets the exact permission
                // granted as the alternate PTE permission always overrides
                // that of the native PTE.
                //

                ASSERT ((PreviousSectionCharacteristics & IMAGE_SCN_MEM_SHARED) == 0);
                ASSERT ((SectionTableEntry->Characteristics & IMAGE_SCN_MEM_SHARED) == 0);
                PteContents.u.Soft.Protection = 
                    MiGetImageProtection (SectionTableEntry->Characteristics |
                                          PreviousSectionCharacteristics);

                MI_WRITE_INVALID_PTE (PreviousPte, PteContents);

                //
                // Start the prototype PTEs for the current subsection
                // so they don't include any starting split portion because
                // that must always be tacked onto the end of the previous
                // subsection to ensure that there is only a single
                // prototype PTE for any given physical page.  This
                // may result in no prototype PTE at all for this subsection
                // if the virtual size for it doesn't cross over into an
                // additional page.
                //

                NumberOfPtes += 1;
                NextVa += PAGE_4K;

                if (SectionVirtualSize > PAGE_4K) {

                    //
                    // Slide the current subsection forward to account
                    // for the beginning that was "moved" into the previous
                    // subsection.
                    //

                    SectorOffset = (Subsection->NumberOfFullSectors << MMSECTOR_SHIFT);
                    if (Subsection->u.SubsectionFlags.SectorEndOffset != 0) {
                        SectorOffset += MMSECTOR_SHIFT;
                    }

                    if (SectorOffset > PAGE_4K) {
                        SectorOffset = PAGE_4K;
                        Subsection->StartingSector += (SectorOffset >> MMSECTOR_SHIFT);
                        Subsection->NumberOfFullSectors -= (SectorOffset >> MMSECTOR_SHIFT);
                    }
                    else if (Subsection->u.SubsectionFlags.SectorEndOffset != 0) {
                        Subsection->StartingSector += (SectorOffset >> MMSECTOR_SHIFT);
                        Subsection->u.SubsectionFlags.SectorEndOffset = 0;
                        Subsection->NumberOfFullSectors -= ((SectorOffset - 1) >> MMSECTOR_SHIFT);
                    }

                    Subsection->PtesInSubsection = (ULONG)
                        MI_COMPUTE_PAGES_SPANNED (NextVa,
                                                  SectionVirtualSize - PAGE_4K);

                    //
                    // Update sector offset and size of raw data to contain
                    // the alignment of the start of this subsection and its
                    // new size so BSS calculations below will be correct.
                    // ie - we always align each subsection at zero (pushing
                    // its first 4K page into the previous subsection if
                    // necessary).
                    //

                    SectorOffset = 0;
                    if (SizeOfRawData >= PAGE_4K) {
                        SizeOfRawData -= PAGE_4K;
                    }
                    else {
                        SizeOfRawData = 0;
                    }
                }
                else {

                    //
                    // This straddling subsection fits entirely in the
                    // previous subsection so there is no additional
                    // subsection to use for it - eliminate the current
                    // subsection now.
                    //

                    RtlZeroMemory (Subsection, sizeof (SUBSECTION));
                    Subsection -= 1;
                    Subsection->NextSubsection = NULL;

                    PreviousSectionCharacteristics = SectionTableEntry->Characteristics;

                    SectionTableEntry += 1;
                    NumberOfSubsections -= 1;

                    //
                    // Reduce the allocated subsection count as we have merged
                    // this subsection.  This is critical if there are any
                    // shared subsections in the image because we use this
                    // count to decide how much to allocate later as well as
                    // how many subsections will get non-NULL NextSubsection
                    // fields - so if it is too high we will end up with
                    // corrupt subsections on the end of the control area.
                    //

                    SubsectionsAllocated -= 1;

                    continue;
                }
            }

            PreviousSectionCharacteristics = SectionTableEntry->Characteristics;

#endif

            for (i = 0; i < Subsection->PtesInSubsection; i += 1) {

                //
                // Set all the prototype PTEs to refer to the control section.
                //

#if defined (_MIALT4K_)

                //
                // We are native page aligned on entry here because we
                // adjusted non-native alignment above.  However, within a
                // single subsection there can be both data and BSS/noaccess
                // ranges which can occur on a 4k boundary.  Thus, this must
                // be explicitly checked for and handled here.
                //

#endif

                if (SectorOffset < SectionVirtualSize) {

                    //
                    // Make PTE accessible.
                    //

                    if (SectionCommit) {
                        NewSegment->NumberOfCommittedPages += 1;
                    }
                    if (ImageCommit) {
                        NewSegment->u1.ImageCommitment += 1;
                    }

                    if (SectorOffset < SizeOfRawData) {

                        //
                        // Data resides on the disk, use the subsection
                        // format PTE.
                        //

                        MI_WRITE_INVALID_PTE (PointerPte, TempPte);
                    }
                    else {

                        //
                        // Demand zero pages.
                        //

                        MI_WRITE_INVALID_PTE (PointerPte, TempPteDemandZero);

#if defined (_MIALT4K_)

                        //
                        // Check the previous PTE for straddling.
                        //

                        if ((CheckSplitPages == TRUE) &&
                            (i != 0) &&
                            (SectorOffset - PAGE_SIZE < SizeOfRawData) &&
                            (BYTE_OFFSET (SizeOfRawData) != 0) &&
                            (BYTE_OFFSET (SizeOfRawData) <= PAGE_4K)) {

                            //
                            // The last half of the previous PTE is actually
                            // the straddler - ie: the previous PTE's first half
                            // needs to be filled from the filesystem and the
                            // second half must be zero-filled.
                            //
                            // Note the previous PTE does NOT need to be marked
                            // for split permissions because the permissions
                            // are actually the same.  The page is only 
                            // materialized so it is filled properly and this
                            // only needs to happen this one time.
                            //

                            PreviousPte = PointerPte - 1;
                            ASSERT ((PreviousPte >= NewSegment->PrototypePte) &&
                                    (PreviousPte < NewSegment->PrototypePte + NewSegment->TotalNumberOfPtes));
                            PteContents.u.Long = PreviousPte->u.Long;
                            ASSERT (PteContents.u.Hard.Valid == 0);
                            ASSERT (PteContents.u.Soft.Prototype == 1);

                            StraddleFrameNumber = MiGetPageForHeader (TRUE);

                            ASSERT (StraddleFrameNumber != 0);

                            Pfn1 = MI_PFN_ELEMENT (StraddleFrameNumber);

                            ASSERT (Pfn1->u1.Flink == 0);

                            Pfn1->u1.Flink = (PFN_NUMBER) ImagePages;
                            Pfn1->u2.Blink = (PFN_NUMBER) PreviousPte;

                            ImagePages = Pfn1;

                            StraddleVa = MiMapSinglePage (NULL,
                                                          StraddleFrameNumber,
                                                          MmCached,
                                                          HighPagePriority);

                            if (StraddleVa == NULL) {
                                MI_BAD_IMAGE (0x1F);
                                Status = STATUS_INSUFFICIENT_RESOURCES;
                                goto BadPeImageSegment;
                            }

                            //
                            // Create an event and MDL for the read operation.
                            //

                            HalfPage = ExAllocatePoolWithTag (NonPagedPool,
                                                              PAGE_SIZE,
                                                              MMTEMPORARY);

                            if (HalfPage == NULL) {
                                MiUnmapSinglePage (StraddleVa);
                                goto BadPeImageSegment;
                            }

                            HalfPagePhysicalAddress = MmGetPhysicalAddress (HalfPage);

                            ASSERT (HalfPagePhysicalAddress.QuadPart != 0);

                            HalfPageFrameNumber = (HalfPagePhysicalAddress.QuadPart >> PAGE_SHIFT);

                            IoStatus.Information = 0;
            
                            ASSERT (Mdl == (PMDL) &StackMdl);
            
                            KeInitializeEvent (&InPageEvent, NotificationEvent, FALSE);
            
                            MmInitializeMdl (Mdl, NULL, PAGE_4K);

                            Mdl->MdlFlags |= MDL_PAGES_LOCKED;
            
                            Page = (PPFN_NUMBER)(Mdl + 1);
                            *Page = HalfPageFrameNumber;

                            StartingOffset.QuadPart = MI_STARTING_OFFSET (Subsection,
                                                                          PreviousPte);
            
                            TempOffset = MiEndingOffset (Subsection);

                            ASSERT (StartingOffset.QuadPart < TempOffset.QuadPart);

                            if (((UINT64)StartingOffset.QuadPart + PAGE_4K) > (UINT64)TempOffset.QuadPart) {

                                ASSERT ((ULONG)(TempOffset.QuadPart - StartingOffset.QuadPart)
                                        > 0);

                                ReadSize = (ULONG)(TempOffset.QuadPart - StartingOffset.QuadPart);

                                //
                                // Round the offset to a 512-byte offset as this
                                // will help filesystems optimize the transfer.
                                // Note that filesystems will always zero fill
                                // the remainder between VDL and the next 512-byte
                                // multiple and we have already zeroed the whole page.
                                //

                                ReadSize = ((ReadSize + MMSECTOR_MASK) & ~MMSECTOR_MASK);
                                Mdl->ByteCount = ReadSize;
                            }

                            Status = IoPageRead (File,
                                                 Mdl,
                                                 &StartingOffset,
                                                 &InPageEvent,
                                                 &IoStatus);
            
                            if (Status == STATUS_PENDING) {
                                KeWaitForSingleObject (&InPageEvent,
                                                       WrPageIn,
                                                       KernelMode,
                                                       FALSE,
                                                       NULL);
            
                                Status = IoStatus.Status;
                            }
            
                            if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                                MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
                            }
            
                            if (!NT_SUCCESS (Status)) {
                                MI_BAD_IMAGE (0x20);
                                ExFreePool (HalfPage);
                                MiUnmapSinglePage (StraddleVa);
                                goto BadPeImageSegment;
                            }
            
                            //
                            // Less than a full page may be read from the file,
                            // only copy the amount that was read (the rest of
                            // the target page is zero already).
                            //

                            ASSERT (IoStatus.Information <= PAGE_4K);

                            RtlCopyMemory (StraddleVa,
                                           HalfPage,
                                           IoStatus.Information);

                            //
                            // The second 4K of this native page is already
                            // correctly zero as we allocated it that way.
                            //

                            MiUnmapSinglePage (StraddleVa);

                            ExFreePool (HalfPage);
                        }

#endif

                    }
                }
                else {

                    //
                    // No access pages.
                    // This can only occur for images with section alignment
                    // greater than the native PAGE_SIZE.
                    //

                    ASSERT (CheckSplitPages == FALSE);
                    MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);
                }

                SectorOffset += PAGE_SIZE;
                PointerPte += 1;
                NextVa += PAGE_SIZE;
            }

#if defined (_MIALT4K_)

            //
            // Ensure NextVa is bumped to the exact 4K (not native page size)
            // boundary.
            //

            if (CheckSplitPages == TRUE) {

                if (MI_ROUND_TO_SIZE (SectionTableEntry->VirtualAddress + SectionVirtualSize, PAGE_4K) != MI_ROUND_TO_SIZE (SectionTableEntry->VirtualAddress + SectionVirtualSize, PAGE_SIZE)) {

                    NextVa -= PAGE_4K;
                }
            }

#endif

        }

        SectionTableEntry += 1;
        NumberOfSubsections -= 1;
    }

    ASSERT ((PointerPte > NewSegment->PrototypePte) &&
            (PointerPte <= NewSegment->PrototypePte + NewSegment->TotalNumberOfPtes));

#if defined (_MIALT4K_)

    if (CheckSplitPages == TRUE) {

        if (BYTE_OFFSET (NextVa) == PAGE_4K) {

            //
            // The NextVa for the final subsection ended in the middle of a
            // native page.  Mark the prototype PTE as split and put the
            // correct (no access) permission for the 2nd page into the
            // subsection so it can be retrieved if/when the page is accessed.
            //

            //
            // If the final subsection page was BSS, then create the page
            // now and pagefile-back it so it can left as a prototype PTE.
            // Otherwise on the first fault, we'll notice it is demand zero
            // and allocate it that way with the writable bit clear in the
            // native PTE (see the IA64 MI_MAKE_VALID_PTE macro for split
            // pages).  Unfortunately the alternate table PTE entry will
            // already have been encoded copyonwrite, so the alternate fault
            // handler will call the native fault code trying to write it,
            // and get rejected with an AV since the native PTE will not be
            // writable.
            //
            // By allocating a straddler page here, we'll keep the page as
            // prototype-backed (instead of demand-zero), so the native fault
            // code will handle it properly.  Note this doesn't need to be done
            // if the final subsection page was data (or text) instead of BSS,
            // because then it is already going to remain prototype-backed
            // during the fault processing.
            //

            ASSERT (PointerPte - 1 != NewSegment->PrototypePte);
            ASSERT ((PointerPte - 1)->u.Hard.Valid == 0);
            ASSERT ((PointerPte - 1)->u.Soft.SplitPermissions == 0);

            if (FileHeader->NumberOfSections != 0) {
                TempPteDemandZero.u.Long = 0;
                TempPteDemandZero.u.Soft.Protection =
                     MiGetImageProtection ((SectionTableEntry - 1)->Characteristics);
                if ((PointerPte - 1)->u.Long == TempPteDemandZero.u.Long) {

                    //
                    // Allocate a page of zeroes and place it into the
                    // last prototype PTE.
                    //

                    StraddleFrameNumber = MiGetPageForHeader (TRUE);

                    ASSERT (StraddleFrameNumber != 0);

                    Pfn1 = MI_PFN_ELEMENT (StraddleFrameNumber);

                    ASSERT (Pfn1->u1.Flink == 0);
                    Pfn1->u1.Flink = (PFN_NUMBER) ImagePages;
                    Pfn1->u2.Blink = (PFN_NUMBER) (PointerPte - 1);

                    ImagePages = Pfn1;
                }
            }

            (PointerPte - 1)->u.Soft.SplitPermissions = 1;
            Subsection->LastSplitPageProtection = MM_NOACCESS;
        }
    }

#endif

    if (InvalidAlignmentAllowed == FALSE) {

        //
        // Account for the number of subsections that really are mapped.
        //

        ASSERT ((ImageAlignment >= PAGE_SIZE) || (CheckSplitPages == TRUE));

        //
        // If the file size is not as big as the image claimed to be,
        // return an error.
        //

        if (ImageFileSize > EndOfFile.LowPart) {

            //
            // Invalid image size.
            //

            KdPrint(("MMCREASECT: invalid image size - file size %lx - image size %lx\n %Z\n",
                EndOfFile.LowPart, ImageFileSize, &File->FileName));
            Status = STATUS_INVALID_IMAGE_FORMAT;
            MI_BAD_IMAGE (0x21);
            goto BadPeImageSegment;
        }

        //
        // The total number of PTEs was decremented as sections were built,
        // make sure that there are less than 64k's worth at this point.
        //

        if (NumberOfPtes >= (ImageAlignment >> PAGE_SHIFT)) {

            if ((CheckSplitPages == TRUE) && (NumberOfPtes == 0)) {
                NOTHING;
            }
            else {

                //
                // Inconsistent image, size does not agree with object tables.
                //

                KdPrint(("MMCREASECT: invalid image - PTE left %lx, image name %Z\n",
                    NumberOfPtes, &File->FileName));

                Status = STATUS_INVALID_IMAGE_FORMAT;
                MI_BAD_IMAGE (0x22);
                goto BadPeImageSegment;
            }
        }

        //
        // Set any remaining PTEs to no access.
        //

        if (NumberOfPtes != 0) {
            MiZeroMemoryPte (PointerPte, NumberOfPtes);
        }

        if ((ExtendedHeader == NULL) &&
            (SizeOfHeaders < PAGE_SIZE) &&
            (CheckSplitPages == FALSE)) {

            //
            // Zero remaining portion of header.
            //

            RtlZeroMemory ((PVOID)((PCHAR)Base +
                           SizeOfHeaders),
                           PAGE_SIZE - SizeOfHeaders);
        }
    }

    CommitCharged = NewSegment->NumberOfCommittedPages;

#if defined (_MIALT4K_)

    //
    // Charge commitment for all the straddler pages too.  It is possible
    // that some of them have been charged already depending on the types
    // of each straddle, but there are so few of these it's not worth
    // distinguishing this case.
    //

    Pfn1 = ImagePages;

    do {

        Pfn1 = (PMMPFN) Pfn1->u1.Flink;
        CommitCharged += 1;

    } while (Pfn1 != NULL);

    NewSegment->NumberOfCommittedPages = CommitCharged;

#endif

    if (CommitCharged != 0) {

        //
        // Commit the pages for the image section.
        //

        if (MiChargeCommitment (CommitCharged, NULL) == FALSE) {
            MI_BAD_IMAGE (0x23);
            Status = STATUS_COMMITMENT_LIMIT;
            CommitCharged = 0;
            goto BadPeImageSegment;
        }

        MM_TRACK_COMMIT (MM_DBG_COMMIT_IMAGE, CommitCharged);
        Status = STATUS_SUCCESS;

        InterlockedExchangeAddSizeT (&MmSharedCommit, CommitCharged);
    }

PeReturnSuccess:

    //
    // Only images that are linked with subsections aligned to the native
    // page size can be directly executed from ROM.
    //

    XipFile = FALSE;
    XipFrameNumber = 0;

    if ((FileAlignment == PAGE_SIZE - 1) && (XIPConfigured == TRUE)) {

        Status = XIPLocatePages (File, &PhysicalAddress);

        if (NT_SUCCESS(Status)) {

            XipFrameNumber = (PFN_NUMBER) (PhysicalAddress.QuadPart >> PAGE_SHIFT);
            //
            // The small control area will need to be reallocated as a large
            // one so the starting frame number can be inserted.  Set XipFile
            // to denote this.
            //

            XipFile = TRUE;
        }
    }

    //
    // If this image is global per session (or is going to be executed directly
    // from ROM), then allocate a large control area.  Note this doesn't need
    // to be done for systemwide global control areas or non-global control
    // areas.
    //

    GlobalPerSession = FALSE;
    if ((ControlArea->u.Flags.GlobalMemory) &&
        ((LoaderFlags & IMAGE_LOADER_FLAGS_SYSTEM_GLOBAL) == 0)) {

        GlobalPerSession = TRUE;
    }

    if ((XipFile == TRUE) || (GlobalPerSession == TRUE)) {

        LargeControlArea = ExAllocatePoolWithTag (NonPagedPool,
                                            (ULONG)(sizeof(LARGE_CONTROL_AREA) +
                                                    (sizeof(SUBSECTION) *
                                                    SubsectionsAllocated)),
                                                    'iCmM');
        if (LargeControlArea == NULL) {

            //
            // The requested pool could not be allocated.  If the image is
            // execute-in-place only (ie: not global per session), then just
            // execute it normally instead of inplace (to avoid not executing
            // it at all).
            //

            if ((XipFile == TRUE) && (GlobalPerSession == FALSE)) {
                goto SkipLargeControlArea;
            }

            MI_BAD_IMAGE (0x24);
            Status = STATUS_INSUFFICIENT_RESOURCES;

            goto BadPeImageSegment;
        }

        //
        // Copy the normal control area into our larger one, fix the linkages,
        // Fill in the additional fields in the new one and free the old one.
        //

        RtlCopyMemory (LargeControlArea, ControlArea, sizeof(CONTROL_AREA));

        ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

        if (XipFile == TRUE) {

            //
            // Mark the large control area accordingly.  If we can't, then 
            // throw it away and use the small control area and execute from
            // RAM instead.
            //

            if (MiMakeControlAreaRom (File, LargeControlArea, XipFrameNumber) == FALSE) {
                if (GlobalPerSession == FALSE) {
                    ExFreePool (LargeControlArea);
                    goto SkipLargeControlArea;
                }
            }
        }

        Subsection = (PSUBSECTION)(ControlArea + 1);
        NewSubsection = (PSUBSECTION)(LargeControlArea + 1);

        for (i = 0; i < SubsectionsAllocated; i += 1) {
            RtlCopyMemory (NewSubsection, Subsection, sizeof(SUBSECTION));
            NewSubsection->ControlArea = (PCONTROL_AREA) LargeControlArea;
            NewSubsection->NextSubsection = (NewSubsection + 1);

            PointerPte = NewSegment->PrototypePte;

            TempPte.u.Long = MiGetSubsectionAddressForPte (NewSubsection);
            TempPte.u.Soft.Prototype = 1;

            for (j = 0; j < NewSegment->TotalNumberOfPtes; j += 1) {

                ASSERT (PointerPte->u.Hard.Valid == 0);

                if ((PointerPte->u.Soft.Prototype == 1) &&
                    (MiGetSubsectionAddress (PointerPte) == Subsection)) {

                    OriginalProtection = MI_GET_PROTECTION_FROM_SOFT_PTE (PointerPte);
#if defined (_MIALT4K_)
                    TempPte.u.Soft.SplitPermissions =
                                PointerPte->u.Soft.SplitPermissions;
#endif

                    TempPte.u.Soft.Protection = OriginalProtection;
                    MI_WRITE_INVALID_PTE (PointerPte, TempPte);
                }

                PointerPte += 1;
            }

            Subsection += 1;
            NewSubsection += 1;
        }

        (NewSubsection - 1)->NextSubsection = NULL;

        NewSegment->ControlArea = (PCONTROL_AREA) LargeControlArea;

        if (GlobalPerSession == TRUE) {
            LargeControlArea->u.Flags.GlobalOnlyPerSession = 1;

            LargeControlArea->SessionId = 0;
            InitializeListHead (&LargeControlArea->UserGlobalList);
        }

        ExFreePool (ControlArea);

        ControlArea = (PCONTROL_AREA) LargeControlArea;
    }

SkipLargeControlArea:

    MiUnmapImageHeaderInHyperSpace ();

    //
    // Turn the image header and any straddler pages into transition pages
    // within the prototype PTEs.
    //
    // Note this could not be done earlier because otherwise these pages
    // could have been trimmed (and reused) before we even finished
    // initializing the segment.  This would be especially bad for the page
    // that contains the PE header because we'd still have it mapped and
    // be looking at its section header entries, etc while the page
    // was being reused !
    //

    PointerPte = NewSegment->PrototypePte;

    Pfn1 = ImagePages;

    do {

        Pfn2 = (PMMPFN) Pfn1->u1.Flink;

        PointerPte = (PMMPTE) Pfn1->u2.Blink;

        //
        // Note straddler pages must always get marked modified.  The PE
        // header only gets marked modified if it was shared with a straddler.
        //

        MarkModified = TRUE;

        //
        // The list is reversed, so the last entry (Pfn2 == NULL) is
        // the PE header.
        //

        if (Pfn2 == NULL) {
                        
            ASSERT (PointerPte == NewSegment->PrototypePte);

            if (PageFrameNumber != 0) {

                //
                // The PE header was not shared with a straddler.
                //

                MarkModified = FALSE;
            }
        }

        ASSERT ((PointerPte >= NewSegment->PrototypePte) &&
                (PointerPte < NewSegment->PrototypePte + NewSegment->TotalNumberOfPtes));

        //
        // Mark straddler pages as modified so they will be written
        // and retrieved from the pagefile (and never from the
        // filesystem again).  Put the straddler page on the modified
        // list and set the transition bit in the prototype PTE.
        //

        MiUpdateImageHeaderPage (PointerPte,
                                 Pfn1 - MmPfnDatabase,
                                 ControlArea,
                                 MarkModified);

        MarkModified = TRUE;

        Pfn1 = Pfn2;

    } while (Pfn1 != NULL);

    if (ExtendedHeader != NULL) {
        ExFreePool (ExtendedHeader);
    }

    return STATUS_SUCCESS;


    //
    // Error returns from image verification.
    //

BadPeImageSegment:

    ASSERT (!NT_SUCCESS (Status));

    ASSERT ((ControlArea == NULL) || (ControlArea->NumberOfPfnReferences == 0));

    if (NewSegment != NULL) {

        if (CommitCharged != 0) {

            ASSERT (CommitCharged == NewSegment->NumberOfCommittedPages);

            MiReturnCommitment (CommitCharged);
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_IMAGE_NO_LARGE_CA, CommitCharged);
            InterlockedExchangeAddSizeT (&MmSharedCommit, 0 - CommitCharged);
        }

#if DBG

        PointerPte = NewSegment->PrototypePte;

        for (i = 0; i < NewSegment->TotalNumberOfPtes; i += 1) {

            TempPte.u.Long = PointerPte->u.Long;

            ASSERT ((TempPte.u.Hard.Valid == 0) &&
                    ((TempPte.u.Soft.Prototype == 1) ||
                     (TempPte.u.Soft.Transition == 0)));

            PointerPte += 1;
        }

#endif

    }

    ASSERT ((ControlArea == NULL) || (ControlArea->NumberOfPfnReferences == 0));

    if (Base != NULL) {
        MiUnmapImageHeaderInHyperSpace ();
    }

    Pfn1 = ImagePages;

    do {
        Pfn2 = (PMMPFN) Pfn1->u1.Flink;

        Pfn1->u2.Blink = 0;     // Clear prototype PTE pointer before freeing

        MiRemoveImageHeaderPage (Pfn1 - MmPfnDatabase);

        Pfn1 = Pfn2;

    } while (Pfn1 != NULL);

    ASSERT ((ControlArea == NULL) || (ControlArea->NumberOfPfnReferences == 0));

    if (NewSegment != NULL) {
        ExFreePool (NewSegment);
    }

    if (ControlArea != NULL) {
        ExFreePool (ControlArea);
    }

    if (ExtendedHeader != NULL) {
        ExFreePool (ExtendedHeader);
    }

    return Status;
}


LOGICAL
MiCheckDosCalls (
    IN PIMAGE_OS2_HEADER Os2Header,
    IN ULONG HeaderSize
    )

/*++

Routine Description:

    This routine checks for DOS calls.

Arguments:

    Os2Header - Supplies a kernelmode pointer to the OS2 header.

    HeaderSize - Supplies the length in bytes of the mapped header.

Return Value:

    Returns TRUE if this is a Win-16 image, FALSE if not.

--*/

{
    PUCHAR ImportTable;
    UCHAR EntrySize;
    USHORT ModuleCount;
    USHORT ModuleSize;
    USHORT i;
    PUSHORT ModuleTable;

    PAGED_CODE();

    //
    // If there are no modules to check return immediately.
    //

    ModuleCount = Os2Header->ne_cmod;

    if (ModuleCount == 0) {
        return FALSE;
    }

    //
    // exe headers are notorious for having junk values for offsets
    // in the import table and module table so the header must be very
    // carefully validated.
    //

    //
    // Find out where the Module ref table is. Mod table has two byte
    // for each entry in import table. These two bytes tell the offset
    // in the import table for that entry.
    //

    ModuleTable = (PUSHORT)((PCHAR)Os2Header + (ULONG)Os2Header->ne_modtab);

    //
    // Make sure that the module table fits within the passed-in header.
    // Note that each module table entry is 2 bytes long.
    //

    if (((ULONG)Os2Header->ne_modtab + (ModuleCount * 2)) > HeaderSize) {
        return FALSE;
    }

    //
    // Now search individual entries for DOSCALLs.
    //

    for (i = 0; i < ModuleCount; i += 1) {

        ModuleSize = *((UNALIGNED USHORT *)ModuleTable);

        //
        // Import table has count byte followed by the string where count
        // is the string length.
        //

        ImportTable = (PUCHAR)((PCHAR)Os2Header +
                      (ULONG)Os2Header->ne_imptab + (ULONG)ModuleSize);

        //
        // Make sure the offset is within the valid range.
        //

        if (((ULONG)Os2Header->ne_imptab + (ULONG)ModuleSize) >= HeaderSize) {
            return FALSE;
        }

        EntrySize = *ImportTable++;

        //
        // 0 is a bad size, bail out.
        //

        if (EntrySize == 0) {
            return FALSE;
        }

        //
        // Make sure the offset is within the valid range.
        // The sizeof(UCHAR) is included in the check because ImportTable
        // was incremented above and is used in the RtlEqualMemory
        // comparison below.
        //

        if (((ULONG)Os2Header->ne_imptab + (ULONG)ModuleSize +
                        (ULONG)EntrySize + sizeof(UCHAR)) > HeaderSize) {
            return FALSE;
        }

        //
        // If size matches compare DOSCALLS.
        //

        if (EntrySize == 8) {
            if (RtlEqualMemory (ImportTable, "DOSCALLS", 8)) {
                return TRUE;
            }
        }

        //
        // Move on to the next module table entry.  Each entry is 2 bytes.
        //

        ModuleTable = (PUSHORT)((PCHAR)ModuleTable + 2);
    }

    return FALSE;
}


NTSTATUS
MiVerifyImageHeader (
    IN PIMAGE_NT_HEADERS NtHeader,
    IN PIMAGE_DOS_HEADER DosHeader,
    IN ULONG NtHeaderSize
    )

/*++

Routine Description:

    This function checks for various inconsistencies in the image header.

Arguments:

    NtHeader - Supplies a pointer to the NT header of the image.

    DosHeader - Supplies a pointer to the DOS header of the image.

    NtHeaderSize - Supplies the size in bytes of the NT header.

Return Value:

    NTSTATUS.

--*/

{
    PCONFIGPHARLAP PharLapConfigured;
    PUCHAR         pb;
    LONG           pResTableAddress;

    PAGED_CODE();

    if (NtHeader->Signature != IMAGE_NT_SIGNATURE) {
        if ((USHORT)NtHeader->Signature == (USHORT)IMAGE_OS2_SIGNATURE) {

            //
            // Check to see if this is a win-16 image.
            //

            if ((!MiCheckDosCalls ((PIMAGE_OS2_HEADER)NtHeader, NtHeaderSize)) &&
                ((((PIMAGE_OS2_HEADER)NtHeader)->ne_exetyp == 2)
                                ||
                ((((PIMAGE_OS2_HEADER)NtHeader)->ne_exetyp == 0)  &&
                  (((((PIMAGE_OS2_HEADER)NtHeader)->ne_expver & 0xff00) ==
                        0x200)  ||
                ((((PIMAGE_OS2_HEADER)NtHeader)->ne_expver & 0xff00) ==
                        0x300))))) {

                //
                // This is a win-16 image.
                //

                return STATUS_INVALID_IMAGE_WIN_16;
            }

            // The following OS/2 headers types go to NTDOS
            //
            // - exetype == 5 means binary is for Dos 4.0.
            //                e.g Borland Dos extender type
            //
            // - OS/2 apps which have no import table entries
            //   cannot be meant for the OS/2 ss.
            //                e.g. QuickC for dos binaries
            //
            //  - "old" Borland Dosx BC++ 3.x, Paradox 4.x
            //     exe type == 1
            //     DosHeader->e_cs * 16 + DosHeader->e_ip + 0x200 - 10
            //     contains the string " mode EXE$"
            //     but import table is empty, so we don't make special check
            //

            if (((PIMAGE_OS2_HEADER)NtHeader)->ne_exetyp == 5  ||
                ((PIMAGE_OS2_HEADER)NtHeader)->ne_enttab ==
                  ((PIMAGE_OS2_HEADER)NtHeader)->ne_imptab) {

                return STATUS_INVALID_IMAGE_PROTECT;
            }


            //
            // Borland Dosx types: exe type 1
            //
            //  - "new" Borland Dosx BP7.0
            //     exe type == 1
            //     DosHeader + 0x200 contains the string "16STUB"
            //     0x200 happens to be e_parhdr*16
            //

            if (((PIMAGE_OS2_HEADER)NtHeader)->ne_exetyp == 1 &&
                RtlEqualMemory((PUCHAR)DosHeader + 0x200, "16STUB", 6)) {

                return STATUS_INVALID_IMAGE_PROTECT;
            }

            //
            // Check for PharLap extended header which we run as a dos app.
            // The PharLap config block is pointed to by the SizeofHeader
            // field in the DosHdr.
            // The following algorithm for detecting a pharlap exe
            // was recommended by PharLap Software Inc.
            //

            PharLapConfigured =(PCONFIGPHARLAP) ((PCHAR)DosHeader +
                                      ((ULONG)DosHeader->e_cparhdr << 4));

            if ((PCHAR)PharLapConfigured <
                       (PCHAR)DosHeader + PAGE_SIZE - sizeof(CONFIGPHARLAP)) {
                if (RtlEqualMemory(&PharLapConfigured->uchCopyRight[0x18],
                                   "Phar Lap Software, Inc.", 24) &&
                    (PharLapConfigured->usSign == 0x4b50 ||  // stub loader type 2
                     PharLapConfigured->usSign == 0x4f50 ||  // bindable 286|DosExtender
                     PharLapConfigured->usSign == 0x5650  )) // bindable 286|DosExtender (Adv)
                  {
                    return STATUS_INVALID_IMAGE_PROTECT;
                }
            }



            //
            // Check for Rational extended header which we run as a dos app.
            // We look for the rational copyright at:
            //     wCopyRight = *(DosHeader->e_cparhdr*16 + 30h)
            //     pCopyRight = wCopyRight + DosHeader->e_cparhdr*16
            //     "Copyright (C) Rational Systems, Inc."
            //

            pb = ((PUCHAR)DosHeader + ((ULONG)DosHeader->e_cparhdr << 4));

            if ((ULONG_PTR)pb < (ULONG_PTR)DosHeader + PAGE_SIZE - 0x30 - sizeof(USHORT)) {
                pb += *(PUSHORT)(pb + 0x30);
                if ((ULONG_PTR)pb < (ULONG_PTR)DosHeader + PAGE_SIZE - 36 &&
                     RtlEqualMemory(pb,
                                    "Copyright (C) Rational Systems, Inc.",
                                    36))
                   {
                    return STATUS_INVALID_IMAGE_PROTECT;
                }
            }

            //
            // Check for lotus 123 family of applications. Starting
            // with 123 3.0 (till recently shipped 123 3.4), every
            // exe header is bound but is meant for DOS. This can
            // be checked via, a string signature in the extended
            // header. <len byte>"1-2-3 Preloader" is the string
            // at ne_nrestab offset.
            //

            pResTableAddress = ((PIMAGE_OS2_HEADER)NtHeader)->ne_nrestab;
            if (pResTableAddress > DosHeader->e_lfanew &&
                ((ULONG)((pResTableAddress+16) - DosHeader->e_lfanew) <
                            NtHeaderSize) &&
                RtlEqualMemory(
                    ((PUCHAR)NtHeader + 1 +
                             (ULONG)(pResTableAddress - DosHeader->e_lfanew)),
                    "1-2-3 Preloader",
                    15)) {
                    return STATUS_INVALID_IMAGE_PROTECT;
            }

            return STATUS_INVALID_IMAGE_NE_FORMAT;
        }

        if ((USHORT)NtHeader->Signature == (USHORT)IMAGE_OS2_SIGNATURE_LE) {

            //
            // This is a LE (OS/2) image. We don't support it, so give it to
            // DOS subsystem. There are cases (Rbase.exe) which have a LE
            // header but actually it is suppose to run under DOS. When we
            // do support LE format, some work needs to be done here to
            // decide whether to give it to VDM or OS/2.
            //

            return STATUS_INVALID_IMAGE_PROTECT;
        }
        return STATUS_INVALID_IMAGE_PROTECT;
    }

    if ((NtHeader->FileHeader.Machine == 0) &&
        (NtHeader->FileHeader.SizeOfOptionalHeader == 0)) {

        //
        // This is a bogus DOS app which has a 32-bit portion
        // masquerading as a PE image.
        //

        return STATUS_INVALID_IMAGE_PROTECT;
    }

    if (!(NtHeader->FileHeader.Characteristics & IMAGE_FILE_EXECUTABLE_IMAGE)) {
        return STATUS_INVALID_IMAGE_FORMAT;
    }

#ifdef i386

    //
    // Make sure the image header is aligned on a Long word boundary.
    //

    if (((ULONG_PTR)NtHeader & 3) != 0) {
        return STATUS_INVALID_IMAGE_FORMAT;
    }
#endif

#define VALIDATE_NTHEADER(Hdr) {                                    \
    /* File alignment must be multiple of 512 and power of 2. */    \
    if (((((Hdr)->OptionalHeader).FileAlignment & 511) != 0) &&     \
        (((Hdr)->OptionalHeader).FileAlignment !=                   \
         ((Hdr)->OptionalHeader).SectionAlignment)) {               \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((Hdr)->OptionalHeader).FileAlignment == 0) {               \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((((Hdr)->OptionalHeader).FileAlignment - 1) &              \
          ((Hdr)->OptionalHeader).FileAlignment) != 0) {            \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((Hdr)->OptionalHeader).SectionAlignment < ((Hdr)->OptionalHeader).FileAlignment) { \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((Hdr)->OptionalHeader).SizeOfImage > MM_SIZE_OF_LARGEST_IMAGE) { \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if ((Hdr)->FileHeader.NumberOfSections > MM_MAXIMUM_IMAGE_SECTIONS) { \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((Hdr)->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR32_MAGIC) && \
        !((Hdr)->FileHeader.Machine == IMAGE_FILE_MACHINE_I386))  { \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
                                                                    \
    if (((Hdr)->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR64_MAGIC) && \
        !(((Hdr)->FileHeader.Machine == IMAGE_FILE_MACHINE_IA64) || \
          ((Hdr)->FileHeader.Machine == IMAGE_FILE_MACHINE_AMD64))) { \
        return STATUS_INVALID_IMAGE_FORMAT;                         \
    }                                                               \
}

    if (NtHeader->OptionalHeader.Magic != IMAGE_NT_OPTIONAL_HDR_MAGIC) {

        //
        // Image doesn't have the right magic value in its optional header.
        //

#if defined (_WIN64)
        if (NtHeader->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR32_MAGIC) {

            //
            // PE32 image.  Validate it as such.
            //

            PIMAGE_NT_HEADERS32 NtHeader32 = (PIMAGE_NT_HEADERS32)NtHeader;

            VALIDATE_NTHEADER(NtHeader32);
            return STATUS_SUCCESS;
        }
#else /* !defined(_WIN64) */
        if (NtHeader->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR64_MAGIC) {

            //
            // 64bit image on a 32bit machine.
            //
            return STATUS_INVALID_IMAGE_WIN_64;
        }
#endif
        return STATUS_INVALID_IMAGE_FORMAT;
    }

    VALIDATE_NTHEADER(NtHeader);
    #undef VALIDATE_NTHEADER

    return STATUS_SUCCESS;
}

NTSTATUS
MiCreateDataFileMap (
    IN PFILE_OBJECT File,
    OUT PSEGMENT *Segment,
    IN PUINT64 MaximumSize,
    IN ULONG SectionPageProtection,
    IN ULONG AllocationAttributes,
    IN ULONG IgnoreFileSizing
    )

/*++

Routine Description:

    This function creates the necessary structures to allow the mapping
    of a data file.

    The data file is accessed to verify desired access, a segment
    object is created and initialized.

Arguments:

    File - Supplies the file object for the image file.

    Segment - Returns the segment object.

    MaximumSize - Supplies the maximum size for the mapping.

    SectionPageProtection - Supplies the initial page protection.

    AllocationAttributes - Supplies the allocation attributes for the mapping.

    IgnoreFileSizing - Supplies TRUE if the cache manager is specifying the
                       file size and so it does not need to be validated.

Return Value:

    NTSTATUS.

--*/

{
    NTSTATUS Status;
    ULONG j;
    ULONG Size;
    UINT64 PartialSize;
    PCONTROL_AREA ControlArea;
    PLARGE_CONTROL_AREA LargeControlArea;
    PMAPPED_FILE_SEGMENT NewSegment;
    PMSUBSECTION Subsection;
    PMSUBSECTION ExtendedSubsection;
    PMSUBSECTION LargeExtendedSubsection;
    MMPTE TempPte;
    UINT64 EndOfFile;
    UINT64 LastFileChunk;
    UINT64 FileOffset;
    UINT64 NumberOfPtesForEntireFile;
    ULONG ExtendedSubsections;
    PMSUBSECTION Last;
    ULONG NumberOfNewSubsections;
    SIZE_T AllocationFragment;
    PHYSICAL_ADDRESS PhysicalAddress;
    PFN_NUMBER PageFrameNumber;

    PAGED_CODE();

    ExtendedSubsections = 0;

    // *************************************************************
    // Create mapped file section.
    // *************************************************************

    if (!IgnoreFileSizing) {

        Status = FsRtlGetFileSize (File, (PLARGE_INTEGER)&EndOfFile);

        if (Status == STATUS_FILE_IS_A_DIRECTORY) {

            //
            // Can't map a directory as a section. Return error.
            //

            return STATUS_INVALID_FILE_FOR_SECTION;
        }

        if (!NT_SUCCESS (Status)) {
            return Status;
        }

        if (EndOfFile == 0 && *MaximumSize == 0) {

            //
            // Can't map a zero length without specifying the maximum
            // size as non-zero.
            //

            return STATUS_MAPPED_FILE_SIZE_ZERO;
        }

        //
        // Make sure this file is big enough for the section.
        //

        if (*MaximumSize > EndOfFile) {

            //
            // If the maximum size is greater than the end-of-file,
            // and the user did not request page_write or page_execute_readwrite
            // to the section, reject the request.
            //

            if (((SectionPageProtection & PAGE_READWRITE) |
                (SectionPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {

                return STATUS_SECTION_TOO_BIG;
            }

            //
            // Check to make sure that the allocation size large enough
            // to contain all the data, if not set a new allocation size.
            //

            EndOfFile = *MaximumSize;

            Status = FsRtlSetFileSize (File, (PLARGE_INTEGER)&EndOfFile);

            if (!NT_SUCCESS (Status)) {
                return Status;
            }
        }
    }
    else {

        //
        // Ignore the file size, this call is from the cache manager.
        //

        EndOfFile = *MaximumSize;
    }

    //
    // Calculate the number of prototype PTE chunks to build for this section.
    // A subsection is also allocated for each chunk as all the prototype PTEs
    // in any given chunk are initially encoded to point at the same subsection.
    //
    // The maximum total section size is 16PB (2^54).  This is because the
    // StartingSector4132 field in each subsection, ie: 2^42-1 bits of file
    // offset where the offset is in 4K (not pagesize) units.  Thus, a
    // subsection may describe a *BYTE* file start offset of maximum
    // 2^54 - 4K.
    //
    // Each subsection can span at most 16TB - 64K.  This is because the
    // NumberOfFullSectors and various other fields in the subsection are
    // ULONGs.  In reality, this is a nonissue as far as maximum section size
    // is concerned because any number of subsections can be chained together
    // and in fact, subsections are allocated to span less to facilitate
    // efficient dynamic prototype PTE trimming and reconstruction.
    //

    if (EndOfFile > MI_MAXIMUM_SECTION_SIZE) {
        return STATUS_SECTION_TOO_BIG;
    }

    NumberOfPtesForEntireFile = (EndOfFile + PAGE_SIZE - 1) >> PAGE_SHIFT;

    NewSegment = ExAllocatePoolWithTag (PagedPool,
                                        sizeof(MAPPED_FILE_SEGMENT),
                                        'mSmM');

    if (NewSegment == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Allocate the subsection memory in smaller sizes so the corresponding
    // prototype PTEs can be trimmed later if paged pool virtual address
    // space becomes scarce.  Note the size is snapped locally so it can
    // be changed dynamically without locking.
    //

    AllocationFragment = MmAllocationFragment;

    ASSERT (MiGetByteOffset (AllocationFragment) == 0);
    ASSERT (AllocationFragment >= PAGE_SIZE);
    ASSERT64 (AllocationFragment < _4gb);

    Size = (ULONG) AllocationFragment;
    PartialSize = NumberOfPtesForEntireFile * sizeof(MMPTE);

    NumberOfNewSubsections = 0;
    ExtendedSubsection = NULL;

    //
    // Initializing Last is not needed for correctness, but without it the
    // compiler cannot compile this code W4 to check for use of uninitialized
    // variables.
    //

    Last = NULL;

    ControlArea = (PCONTROL_AREA)File->SectionObjectPointer->DataSectionObject;

    do {

        if (PartialSize < (UINT64) AllocationFragment) {
            PartialSize = (UINT64) ROUND_TO_PAGES (PartialSize);
            Size = (ULONG) PartialSize;
        }

        if (ExtendedSubsection == NULL) {
            ExtendedSubsection = (PMSUBSECTION)(ControlArea + 1);

            //
            // Control area and first subsection were zeroed at allocation time.
            //
        }
        else {

            ExtendedSubsection = ExAllocatePoolWithTag (NonPagedPool,
                                                        sizeof(MSUBSECTION),
                                                        'cSmM');

            if (ExtendedSubsection == NULL) {
                ExFreePool (NewSegment);

                //
                // Free all the previous allocations and return an error.
                //

                ExtendedSubsection = (PMSUBSECTION)(ControlArea + 1);
                ExtendedSubsection = (PMSUBSECTION) ExtendedSubsection->NextSubsection;
                while (ExtendedSubsection != NULL) {
                    Last = (PMSUBSECTION) ExtendedSubsection->NextSubsection;
                    ExFreePool (ExtendedSubsection);
                    ExtendedSubsection = Last;
                }
                return STATUS_INSUFFICIENT_RESOURCES;
            }

            RtlZeroMemory (ExtendedSubsection, sizeof(MSUBSECTION));
            Last->NextSubsection = (PSUBSECTION) ExtendedSubsection;
        }

        NumberOfNewSubsections += 1;

        ExtendedSubsection->PtesInSubsection = Size / sizeof(MMPTE);

        Last = ExtendedSubsection;
        PartialSize -= Size;
    } while (PartialSize != 0);

    *Segment = (PSEGMENT) NewSegment;
    RtlZeroMemory (NewSegment, sizeof(MAPPED_FILE_SEGMENT));

    NewSegment->LastSubsectionHint = ExtendedSubsection;

    //
    // Control area and first subsection were zeroed at allocation time.
    //

    ControlArea->Segment = (PSEGMENT) NewSegment;
    ControlArea->NumberOfSectionReferences = 1;

    if (IgnoreFileSizing == FALSE) {

        //
        // This reference is not from the cache manager.
        //

        ControlArea->NumberOfUserReferences = 1;
    }
    else {

        //
        // Set the was purged flag to indicate that the
        // file size was not explicitly set.
        //

        ControlArea->u.Flags.WasPurged = 1;
    }

    ControlArea->u.Flags.BeingCreated = 1;
    ControlArea->u.Flags.File = 1;

    if (FILE_REMOTE_DEVICE & File->DeviceObject->Characteristics) {

        //
        // This file resides on a redirected drive.
        //

        ControlArea->u.Flags.Networked = 1;
    }

    if (AllocationAttributes & SEC_NOCACHE) {
        ControlArea->u.Flags.NoCache = 1;
    }

    ControlArea->FilePointer = File;

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    Subsection = (PMSUBSECTION)(ControlArea + 1);

    //
    // Loop through all the subsections and fill in the PTEs.
    //

    TempPte.u.Long = MiGetSubsectionAddressForPte (Subsection);
    TempPte.u.Soft.Prototype = 1;

    //
    // Set all the PTEs to the execute-read-write protection.
    // The section will control access to these and the segment
    // must provide a method to allow other users to map the file
    // for various protections.
    //

    TempPte.u.Soft.Protection = MM_EXECUTE_READWRITE;

    NewSegment->ControlArea = ControlArea;
    NewSegment->SizeOfSegment = EndOfFile;
    NewSegment->TotalNumberOfPtes = (ULONG) NumberOfPtesForEntireFile;
    if (NumberOfPtesForEntireFile >= 0x100000000) {
        NewSegment->SegmentFlags.TotalNumberOfPtes4132 = (ULONG_PTR)(NumberOfPtesForEntireFile >> 32);
    }

    NewSegment->SegmentPteTemplate = TempPte;

    if (Subsection->NextSubsection != NULL) {

        //
        // Multiple segments and subsections.
        // Align first so it is a multiple of the allocation size.
        //

        NewSegment->NonExtendedPtes =
          (Subsection->PtesInSubsection & ~(((ULONG)AllocationFragment >> PAGE_SHIFT) - 1));
    }
    else {
        NewSegment->NonExtendedPtes = NewSegment->TotalNumberOfPtes;
    }

    Subsection->PtesInSubsection = NewSegment->NonExtendedPtes;

    FileOffset = 0;

    do {

        //
        // Loop through all the subsections to initialize them.
        //

        Subsection->ControlArea = ControlArea;

        Mi4KStartForSubsection(&FileOffset, Subsection);

        Subsection->u.SubsectionFlags.Protection = MM_EXECUTE_READWRITE;

        if (Subsection->NextSubsection == NULL) {

            LastFileChunk = (EndOfFile >> MM4K_SHIFT) - FileOffset;

            //
            // Note this next line restricts the number of bytes mapped by
            // a single subsection to 16TB-4K.  Multiple subsections can always
            // be chained together to support an overall file of size 16K TB.
            //

            Subsection->NumberOfFullSectors = (ULONG)LastFileChunk;

            Subsection->u.SubsectionFlags.SectorEndOffset =
                                 (ULONG) EndOfFile & MM4K_MASK;

            j = Subsection->PtesInSubsection;

            Subsection->PtesInSubsection = (ULONG)(
                NumberOfPtesForEntireFile -
                                (FileOffset >> (PAGE_SHIFT - MM4K_SHIFT)));

            MI_CHECK_SUBSECTION (Subsection);

            Subsection->UnusedPtes = j - Subsection->PtesInSubsection;
        }
        else {
            Subsection->NumberOfFullSectors =
                Subsection->PtesInSubsection << (PAGE_SHIFT - MM4K_SHIFT);

            MI_CHECK_SUBSECTION (Subsection);
        }

        FileOffset += (((UINT64)Subsection->PtesInSubsection) <<
                                        (PAGE_SHIFT - MM4K_SHIFT));
        Subsection = (PMSUBSECTION) Subsection->NextSubsection;
    } while (Subsection != NULL);

    if (XIPConfigured == TRUE) {

        Status = XIPLocatePages (File, &PhysicalAddress);

        if (NT_SUCCESS(Status)) {

            PageFrameNumber = (PFN_NUMBER) (PhysicalAddress.QuadPart >> PAGE_SHIFT);
            //
            // Allocate a large control area (so the starting frame number
            // can be saved) and repoint all the created subsections to it.
            //

            LargeControlArea = ExAllocatePoolWithTag (NonPagedPool,
                                            (ULONG)(sizeof(LARGE_CONTROL_AREA) +
                                                    sizeof(MSUBSECTION)),
                                                    MMCONTROL);

            if (LargeControlArea != NULL) {

                *(PCONTROL_AREA) LargeControlArea = *ControlArea;

                if (MiMakeControlAreaRom (File, LargeControlArea, PageFrameNumber) == TRUE) {

                    LargeExtendedSubsection = (PMSUBSECTION)(LargeControlArea + 1);
                    ExtendedSubsection = (PMSUBSECTION)(ControlArea + 1);

                    *LargeExtendedSubsection = *ExtendedSubsection;
                    LargeExtendedSubsection->ControlArea = (PCONTROL_AREA) LargeControlArea;

                    //
                    // Only the first subsection needed to be directly modified
                    // as above because it is allocated in a single chunk with
                    // the control area.  Any additional subsections below
                    // just need their control area pointers updated.
                    //

                    ASSERT (NumberOfNewSubsections >= 1);
                    j = NumberOfNewSubsections - 1;

                    while (j != 0) {

                        ExtendedSubsection = (PMSUBSECTION) ExtendedSubsection->NextSubsection;
                        ExtendedSubsection->ControlArea = (PCONTROL_AREA) LargeControlArea;
                        j -= 1;
                    }

                    NewSegment->ControlArea = (PCONTROL_AREA) LargeControlArea;
                }
                else {
                    ExFreePool (LargeControlArea);
                }
            }
        }
    }

    return STATUS_SUCCESS;
}

NTSTATUS
MiCreatePagingFileMap (
    OUT PSEGMENT *Segment,
    IN PUINT64 MaximumSize,
    IN ULONG ProtectionMask,
    IN ULONG AllocationAttributes
    )

/*++

Routine Description:

    This function creates the necessary structures to allow the mapping
    of a paging file.

Arguments:

    Segment - Returns the segment object.

    MaximumSize - Supplies the maximum size for the mapping.

    ProtectionMask - Supplies the initial page protection.

    AllocationAttributes - Supplies the allocation attributes for the
                           mapping.

Return Value:

    NTSTATUS.

--*/


{
    UINT64 MaximumFileSize;
    PFN_NUMBER NumberOfPtes;
    SIZE_T SizeOfSegment;
    PCONTROL_AREA ControlArea;
    PSEGMENT NewSegment;
    PMMPTE PointerPte;
    PSUBSECTION Subsection;
    MMPTE TempPte;

    PAGED_CODE();

    //*******************************************************************
    // Create a section backed by the paging file.
    //*******************************************************************

    if (*MaximumSize == 0) {
        return STATUS_INVALID_PARAMETER_4;
    }

    //
    // Limit page file backed sections to the amount of pool that could
    // possibly be allocated to hold the prototype PTEs.  Note this may
    // be larger than the size of any *single* pagefile.
    //

#if defined (_WIN64)

    //
    // Limit the maximum size to the number of PTEs that can be stored in
    // the NonExtendedPtes field in the segment so that segment deletion
    // which uses this will use the proper value.
    //

    MaximumFileSize = ((UINT64)1 << (32 + PAGE_SHIFT)) - sizeof (ULONG_PTR) - sizeof(SEGMENT);
#else
    MaximumFileSize = MAXULONG_PTR - sizeof(SEGMENT);
#endif

    MaximumFileSize /= sizeof(MMPTE);
    MaximumFileSize <<= PAGE_SHIFT;

    if (*MaximumSize > MaximumFileSize) {
        return STATUS_SECTION_TOO_BIG;
    }

    //
    // Create the segment object.
    //
    // Calculate the number of prototype PTEs to build for this segment.
    //

    NumberOfPtes = (PFN_NUMBER) ((*MaximumSize + PAGE_SIZE - 1) >> PAGE_SHIFT);

    if (AllocationAttributes & SEC_COMMIT) {

        //
        // Commit the pages for the section.
        //

        ASSERT (ProtectionMask != 0);

        if (MiChargeCommitment (NumberOfPtes, NULL) == FALSE) {
            return STATUS_COMMITMENT_LIMIT;
        }
    }

    SizeOfSegment = sizeof(SEGMENT) + sizeof(MMPTE) * (NumberOfPtes - 1);

    NewSegment = ExAllocatePoolWithTag (PagedPool | POOL_MM_ALLOCATION,
                                        SizeOfSegment,
                                        MMSECT);

    if (NewSegment == NULL) {

        //
        // The requested pool could not be allocated.
        //

        if (AllocationAttributes & SEC_COMMIT) {
            MiReturnCommitment (NumberOfPtes);
        }
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    *Segment = NewSegment;

    ControlArea = ExAllocatePoolWithTag (NonPagedPool,
                                 (ULONG)sizeof(CONTROL_AREA) +
                                                (ULONG)sizeof(SUBSECTION),
                                         MMCONTROL);

    if (ControlArea == NULL) {

        //
        // The requested pool could not be allocated.
        //

        ExFreePool (NewSegment);

        if (AllocationAttributes & SEC_COMMIT) {
            MiReturnCommitment (NumberOfPtes);
        }
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Zero control area and first subsection.
    //

    RtlZeroMemory (ControlArea, sizeof(CONTROL_AREA) + sizeof(SUBSECTION));

    ControlArea->Segment = NewSegment;
    ControlArea->NumberOfSectionReferences = 1;
    ControlArea->NumberOfUserReferences = 1;

    if (AllocationAttributes & SEC_BASED) {
        ControlArea->u.Flags.Based = 1;
    }

    if (AllocationAttributes & SEC_RESERVE) {
        ControlArea->u.Flags.Reserve = 1;
    }

    if (AllocationAttributes & SEC_COMMIT) {
        ControlArea->u.Flags.Commit = 1;
    }

    Subsection = (PSUBSECTION)(ControlArea + 1);

    Subsection->ControlArea = ControlArea;
    Subsection->PtesInSubsection = (ULONG)NumberOfPtes;
    Subsection->u.SubsectionFlags.Protection = ProtectionMask;

    //
    // Align the prototype PTEs on the proper boundary.
    //

    PointerPte = &NewSegment->ThePtes[0];

    //
    // Zero the segment header.
    //

    RtlZeroMemory (NewSegment, sizeof(SEGMENT));

    NewSegment->PrototypePte = &NewSegment->ThePtes[0];

    NewSegment->ControlArea = ControlArea;

    //
    // Record the process that created this segment for the performance
    // analysis tools.
    //

    NewSegment->u1.CreatingProcess = PsGetCurrentProcess ();

    NewSegment->SizeOfSegment = (UINT64)NumberOfPtes * PAGE_SIZE;
    NewSegment->TotalNumberOfPtes = (ULONG)NumberOfPtes;
    NewSegment->NonExtendedPtes = (ULONG)NumberOfPtes;

    PointerPte = NewSegment->PrototypePte;
    Subsection->SubsectionBase = PointerPte;
    TempPte = ZeroPte;

    if (AllocationAttributes & SEC_COMMIT) {
        TempPte.u.Soft.Protection = ProtectionMask;

        //
        // Record commitment charging.
        //

        MM_TRACK_COMMIT (MM_DBG_COMMIT_PAGEFILE_BACKED_SHMEM, NumberOfPtes);

        NewSegment->NumberOfCommittedPages = NumberOfPtes;

        InterlockedExchangeAddSizeT (&MmSharedCommit, NumberOfPtes);
    }

    NewSegment->SegmentPteTemplate.u.Soft.Protection = ProtectionMask;

    //
    // Set all the prototype PTEs to either no access or demand zero
    // depending on the commit flag.
    //

    MiFillMemoryPte (PointerPte, NumberOfPtes, TempPte.u.Long);

    return STATUS_SUCCESS;
}


NTSTATUS
NtOpenSection (
    OUT PHANDLE SectionHandle,
    IN ACCESS_MASK DesiredAccess,
    IN POBJECT_ATTRIBUTES ObjectAttributes
    )

/*++

Routine Description:

    This function opens a handle to a section object with the specified
    desired access.

Arguments:


    Sectionhandle - Supplies a pointer to a variable that will
                    receive the section object handle value.

    DesiredAccess - Supplies the desired types of access  for the
                    section.

    DesiredAccess Flags

         EXECUTE - Execute access to the section is desired.

         READ - Read access to the section is desired.

         WRITE - Write access to the section is desired.

    ObjectAttributes - Supplies a pointer to an object attributes structure.

Return Value:

    NTSTATUS.

--*/

{
    HANDLE Handle;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;

    PAGED_CODE();
    //
    // Get previous processor mode and probe output arguments if necessary.
    //

    PreviousMode = KeGetPreviousMode();
    if (PreviousMode != KernelMode) {
        try {
            ProbeForWriteHandle(SectionHandle);
        } except (EXCEPTION_EXECUTE_HANDLER) {
            return GetExceptionCode();
        }
    }

    //
    // Open handle to the section object with the specified desired
    // access.
    //

    Status = ObOpenObjectByName (ObjectAttributes,
                                 MmSectionObjectType,
                                 PreviousMode,
                                 NULL,
                                 DesiredAccess,
                                 NULL,
                                 &Handle);

    try {
        *SectionHandle = Handle;
    } except (EXCEPTION_EXECUTE_HANDLER) {
        return Status;
    }

    return Status;
}

CCHAR
MiGetImageProtection (
    IN ULONG SectionCharacteristics
    )

/*++

Routine Description:

    This function takes a section characteristic mask from the
    image and converts it to an PTE protection mask.

Arguments:

    SectionCharacteristics - Supplies the characteristics mask from the
                             image.

Return Value:

    Returns the protection mask for the PTE.

--*/

{
    ULONG Index;

    PAGED_CODE();

    Index = 0;
    if (SectionCharacteristics & IMAGE_SCN_MEM_EXECUTE) {
        Index |= 1;
    }
    if (SectionCharacteristics & IMAGE_SCN_MEM_READ) {
        Index |= 2;
    }
    if (SectionCharacteristics & IMAGE_SCN_MEM_WRITE) {
        Index |= 4;
    }
    if (SectionCharacteristics & IMAGE_SCN_MEM_SHARED) {
        Index |= 8;
    }

    return MmImageProtectionArray[Index];
}

PFN_NUMBER
MiGetPageForHeader (
    LOGICAL ZeroPage
    )

/*++

Routine Description:

    This non-pagable function acquires the PFN lock, removes
    a page and updates the PFN database as though the page was
    ready to be deleted if the reference count is decremented.

Arguments:

    ZeroPage - Supplies TRUE if the caller requires a zero-filled page.

Return Value:

    Returns the physical page frame number.

--*/

{
    KIRQL OldIrql;
    PFN_NUMBER PageFrameNumber;
    PMMPFN Pfn1;
    PEPROCESS Process;
    ULONG PageColor;

    Process = PsGetCurrentProcess ();

    PageColor = MI_PAGE_COLOR_VA_PROCESS ((PVOID)X64K,
                                          &Process->NextPageColor);

    //
    // Lock the PFN database and get a page.
    //

    LOCK_PFN (OldIrql);

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, NULL, OldIrql);
    }

    //
    // Remove page for 64k alignment.
    //

    if (ZeroPage) {
        PageFrameNumber = MiRemoveZeroPage (PageColor);
    }
    else {
        PageFrameNumber = MiRemoveAnyPage (PageColor);
    }

    //
    // Increment the reference count for the page so the
    // paging I/O will work, and so this page cannot be stolen from us.
    //

    Pfn1 = MI_PFN_ELEMENT (PageFrameNumber);
    Pfn1->u3.e2.ReferenceCount += 1;

    //
    // Don't need the PFN lock for the fields below...
    //

    UNLOCK_PFN (OldIrql);

    ASSERT (Pfn1->u1.Flink == 0);
    ASSERT (Pfn1->u2.Blink == 0);

    Pfn1->OriginalPte = ZeroPte;
    Pfn1->PteAddress = (PVOID) (ULONG_PTR)X64K;
    MI_SET_PFN_DELETED (Pfn1);

    return PageFrameNumber;
}

VOID
MiUpdateImageHeaderPage (
    IN PMMPTE PointerPte,
    IN PFN_NUMBER PageFrameNumber,
    IN PCONTROL_AREA ControlArea,
    IN LOGICAL MarkModified
    )

/*++

Routine Description:

    This non-pagable function acquires the PFN lock, and
    turns the specified prototype PTE into a transition PTE
    referring to the specified physical page.  It then
    decrements the reference count causing the page to
    be placed on the standby or modified list.

Arguments:

    PointerPte - Supplies the PTE to set into the transition state.

    PageFrameNumber - Supplies the physical page.

    ControlArea - Supplies the control area for the prototype PTEs.

    MarkModified - Supplies TRUE if the PFN should be marked modified (this
                   will give it pagefile-backing on trim vs filesystem backing).

Return Value:

    None.

--*/

{
    PMMPTE PointerPde;
    PMMPFN Pfn1;
    KIRQL OldIrql;

    Pfn1 = MI_PFN_ELEMENT (PageFrameNumber);

    PointerPde = MiGetPteAddress (PointerPte);
    LOCK_PFN (OldIrql);

    if (PointerPde->u.Hard.Valid == 0) {
        MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
    }

    MiInitializeTransitionPfn (PageFrameNumber, PointerPte);

    if (MarkModified == TRUE) {
        MI_SET_MODIFIED (Pfn1, 1, 0x19);
    }

    if (Pfn1->OriginalPte.u.Soft.Prototype == 1) {
        ControlArea->NumberOfPfnReferences += 1;
    }

    //
    // Add the page to the standby list.
    //

    MiDecrementReferenceCount (Pfn1, PageFrameNumber);

    UNLOCK_PFN (OldIrql);
    return;
}

VOID
MiRemoveImageHeaderPage (
    IN PFN_NUMBER PageFrameNumber
    )

/*++

Routine Description:

    This non-pagable function acquires the PFN lock, and decrements
    the reference count thereby causing the physical page to
    be deleted.

Arguments:

    PageFrameNumber - Supplies the PFN to decrement.

Return Value:

    None.

--*/
{
    PMMPFN Pfn1;
    KIRQL OldIrql;

    Pfn1 = MI_PFN_ELEMENT (PageFrameNumber);

    LOCK_PFN (OldIrql);
    MiDecrementReferenceCount (Pfn1, PageFrameNumber);
    UNLOCK_PFN (OldIrql);
    return;
}

PCONTROL_AREA
MiFindImageSectionObject (
    IN PFILE_OBJECT File,
    OUT PLOGICAL GlobalNeeded
    )

/*++

Routine Description:

    This function searches the control area chains (if any) for an existing
    cache of the specified image file.  For non-global control areas, there is
    no chain and the control area is shared for all callers and sessions.
    Likewise for systemwide global control areas.

    However, for global PER-SESSION control areas, we must do the walk.

Arguments:

    File - Supplies the file object for the image file.

    GlobalNeeded - Supplies a pointer to store whether a global control area is
                   required as a placeholder.  This can only be set when there
                   is already some global control area in the list - ie: our
                   caller should only rely on this when this function returns
                   NULL so the caller knows what kind of control area to
                   insert.

Return Value:

    Returns the matching control area or NULL if one does not exist.

Environment:

    Must be holding the PFN lock.

--*/

{
    PCONTROL_AREA ControlArea;
    PLARGE_CONTROL_AREA LargeControlArea;
    PLIST_ENTRY Head, Next;
    ULONG SessionId;

    MM_PFN_LOCK_ASSERT();

    *GlobalNeeded = FALSE;

    //
    // Get first (if any) control area pointer.
    //

    ControlArea = (PCONTROL_AREA)(File->SectionObjectPointer->ImageSectionObject);

    //
    // If no control area, or the control area is not session global,
    // then our job is easy.  Note, however, that they each require different
    // return values as they represent different states.
    //

    if (ControlArea == NULL) {
        return NULL;
    }

    if (ControlArea->u.Flags.GlobalOnlyPerSession == 0) {
        return ControlArea;
    }

    LargeControlArea = (PLARGE_CONTROL_AREA) ControlArea;

    //
    // Get the current session ID and search for a matching control area.
    //

    SessionId = MmGetSessionId (PsGetCurrentProcess());

    if (LargeControlArea->SessionId == SessionId) {
        return (PCONTROL_AREA) LargeControlArea;
    }

    //
    // Must search the control area list for a matching session ID.
    //

    Head = &LargeControlArea->UserGlobalList;

    for (Next = Head->Flink; Next != Head; Next = Next->Flink) {

        LargeControlArea = CONTAINING_RECORD (Next, LARGE_CONTROL_AREA, UserGlobalList);

        ASSERT (LargeControlArea->u.Flags.GlobalOnlyPerSession == 1);

        if (LargeControlArea->SessionId == SessionId) {
            return (PCONTROL_AREA) LargeControlArea;
        }
    }

    //
    // No match, so tell our caller to create a new global control area.
    //

    *GlobalNeeded = TRUE;

    return NULL;
}

VOID
MiInsertImageSectionObject(
    IN PFILE_OBJECT File,
    IN PCONTROL_AREA InputControlArea
    )

/*++

Routine Description:

    This function inserts the control area into the file's section object
    pointers.  For non-global control areas, there is no chain and the
    control area is shared for all callers and sessions.
    Likewise for systemwide global control areas.

    However, for global PER-SESSION control areas, we must do a list insertion.

Arguments:

    File - Supplies the file object for the image file.

    InputControlArea - Supplies the control area to insert.

Return Value:

    None.

Environment:

    Must be holding the PFN lock.

--*/

{
    PLIST_ENTRY Head;
    PLARGE_CONTROL_AREA ControlArea;
    PLARGE_CONTROL_AREA FirstControlArea;
#if DBG
    PLIST_ENTRY Next;
    PLARGE_CONTROL_AREA NextControlArea;
#endif

    MM_PFN_LOCK_ASSERT();

    ControlArea = (PLARGE_CONTROL_AREA) InputControlArea;

    //
    // If this is not a global-per-session control area or just a placeholder
    // control area (with no chain already in place) then just put it in.
    //

    FirstControlArea = (PLARGE_CONTROL_AREA)(File->SectionObjectPointer->ImageSectionObject);

    if (FirstControlArea == NULL) {
        if (ControlArea->u.Flags.GlobalOnlyPerSession == 0) {
            File->SectionObjectPointer->ImageSectionObject = (PVOID)ControlArea;
            return;
        }
    }

    //
    // A per-session control area needs to be inserted...
    //

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 1);

    ControlArea->SessionId = MmGetSessionId (PsGetCurrentProcess());

    //
    // If the control area list is empty, just initialize links for this entry.
    //

    if (File->SectionObjectPointer->ImageSectionObject == NULL) {
        InitializeListHead (&ControlArea->UserGlobalList);
    }
    else {

        //
        // Insert new entry before the current first entry.  The control area
        // must be in the midst of creation/deletion or have a valid session
        // ID to be inserted.
        //

        ASSERT (ControlArea->u.Flags.BeingDeleted ||
                ControlArea->u.Flags.BeingCreated ||
                ControlArea->SessionId != (ULONG)-1);

        FirstControlArea = (PLARGE_CONTROL_AREA)(File->SectionObjectPointer->ImageSectionObject);

        Head = &FirstControlArea->UserGlobalList;

#if DBG
        //
        // Ensure no duplicate session IDs exist in the list.
        //

        for (Next = Head->Flink; Next != Head; Next = Next->Flink) {
            NextControlArea = CONTAINING_RECORD (Next, LARGE_CONTROL_AREA, UserGlobalList);
            ASSERT (NextControlArea->SessionId != (ULONG)-1 &&
                    NextControlArea->SessionId != ControlArea->SessionId);
        }
#endif

        InsertTailList (Head, &ControlArea->UserGlobalList);
    }

    //
    // Update first control area pointer.
    //

    File->SectionObjectPointer->ImageSectionObject = (PVOID) ControlArea;
}

VOID
MiRemoveImageSectionObject(
    IN PFILE_OBJECT File,
    IN PCONTROL_AREA InputControlArea
    )

/*++

Routine Description:

    This function searches the control area chains (if any) for an existing
    cache of the specified image file.  For non-global control areas, there is
    no chain and the control area is shared for all callers and sessions.
    Likewise for systemwide global control areas.

    However, for global PER-SESSION control areas, we must do the walk.

    Upon finding the specified control area, we unlink it.

Arguments:

    File - Supplies the file object for the image file.

    InputControlArea - Supplies the control area to remove.

Return Value:

    None.

Environment:

    Must be holding the PFN lock.

--*/

{
#if DBG
    PLIST_ENTRY Head;
#endif
    PLIST_ENTRY Next;
    PLARGE_CONTROL_AREA ControlArea;
    PLARGE_CONTROL_AREA FirstControlArea;
    PLARGE_CONTROL_AREA NextControlArea;

    MM_PFN_LOCK_ASSERT();

    ControlArea = (PLARGE_CONTROL_AREA) InputControlArea;

    FirstControlArea = (PLARGE_CONTROL_AREA)(File->SectionObjectPointer->ImageSectionObject);

    //
    // Get a pointer to the first control area.  If this is not a
    // global-per-session control area, then there is no list, so we're done.
    //

    if (FirstControlArea->u.Flags.GlobalOnlyPerSession == 0) {
        ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

        File->SectionObjectPointer->ImageSectionObject = NULL;
        return;
    }

    //
    // A list may exist.  Walk it as necessary and delete the requested entry.
    //

    if (FirstControlArea == ControlArea) {

        //
        // The first entry is the one to remove.  If it is the only entry
        // in the list, then the new first entry pointer will be NULL.
        // Otherwise, get a pointer to the next entry and unlink the current.
        //

        if (IsListEmpty (&FirstControlArea->UserGlobalList)) {
            NextControlArea = NULL;
        }
        else {
            Next = FirstControlArea->UserGlobalList.Flink;
            RemoveEntryList (&FirstControlArea->UserGlobalList);
            NextControlArea = CONTAINING_RECORD (Next,
                                                 LARGE_CONTROL_AREA,
                                                 UserGlobalList);

            ASSERT (NextControlArea->u.Flags.GlobalOnlyPerSession == 1);
        }

        File->SectionObjectPointer->ImageSectionObject = (PVOID)NextControlArea;
        return;
    }

    //
    // Remove the entry, note that the ImageSectionObject need not be updated
    // as the entry is not at the head.
    //

#if DBG
    Head = &FirstControlArea->UserGlobalList;

    for (Next = Head->Flink; Next != Head; Next = Next->Flink) {

        NextControlArea = CONTAINING_RECORD (Next,
                                             LARGE_CONTROL_AREA,
                                             UserGlobalList);

        ASSERT (NextControlArea->u.Flags.GlobalOnlyPerSession == 1);

        if (NextControlArea == ControlArea) {
            break;
        }
    }
    ASSERT (Next != Head);
#endif

    RemoveEntryList (&ControlArea->UserGlobalList);
}

LOGICAL
MiFlushDataSection (
    IN PFILE_OBJECT File
    )

/*++

Routine Description:

    This routine flushes the data section if there is one.

Arguments:

    File - Supplies the file object.

Return Value:

    TRUE if there is a data section that may be in use, FALSE if not.

Environment:

    Kernel mode, APC_LEVEL and below.

--*/

{
    KIRQL OldIrql;
    IO_STATUS_BLOCK IoStatus;
    PCONTROL_AREA ControlArea;
    LOGICAL DataInUse;

    DataInUse = FALSE;

    LOCK_PFN (OldIrql);

    ControlArea = (PCONTROL_AREA) File->SectionObjectPointer->DataSectionObject;

    if (ControlArea) {

        if ((ControlArea->NumberOfSectionReferences != 0) ||
            (ControlArea->NumberOfMappedViews != 0)) {

            DataInUse = TRUE;
        }

        if (ControlArea->NumberOfSystemCacheViews) {
            UNLOCK_PFN (OldIrql);
            CcFlushCache (File->SectionObjectPointer,
                          NULL,
                          0,
                          &IoStatus);

        }
        else {
            UNLOCK_PFN (OldIrql);
            MmFlushSection (File->SectionObjectPointer,
                            NULL,
                            0,
                            &IoStatus,
                            TRUE);
        }
    }
    else {
        UNLOCK_PFN (OldIrql);
    }

    return DataInUse;
}


PVOID
MiCopyHeaderIfResident (
    IN PFILE_OBJECT File,
    IN PFN_NUMBER ImagePageFrameNumber
    )

/*++

Routine Description:

    This routine copies the image header from the data section if there is
    one and the page is already resident or in transition.

Arguments:

    File - Supplies the file object.

    ImagePageFrameNumber - Supplies the image frame to copy the data into.

Return Value:

    Virtual address of the image page frame number if successful, NULL if not.

Environment:

    Kernel mode, APC_LEVEL and below.

--*/

{
    PMMPFN Pfn1;
    PVOID DataPage;
    PVOID ImagePage;
    KIRQL OldIrql;
    PCONTROL_AREA ControlArea;
    PMMPTE PointerPte;
    MMPTE PteContents;
    PFN_NUMBER PageFrameIndex;
    PEPROCESS Process;
    PSUBSECTION Subsection;
    PSECTION_OBJECT_POINTERS SectionObjectPointer;

    //
    // Take a quick (safely unsynchronized) look to see whether to bother
    // mapping the image header page at all - if there's no data section
    // object, then skip it and just return.
    //

    SectionObjectPointer = File->SectionObjectPointer;
    if (SectionObjectPointer == NULL) {
        return NULL;
    }

    ControlArea = (PCONTROL_AREA) SectionObjectPointer->DataSectionObject;

    if (ControlArea == NULL) {
        return NULL;
    }

    //
    // There's a data section, so map the target page.
    //

    ImagePage = MiMapImageHeaderInHyperSpace (ImagePageFrameNumber);

    LOCK_PFN (OldIrql);

    //
    // Now that we are synchronized via the PFN lock, take a safe look.
    //

    SectionObjectPointer = File->SectionObjectPointer;
    if (SectionObjectPointer == NULL) {
        UNLOCK_PFN (OldIrql);
        MiUnmapImageHeaderInHyperSpace ();
        return NULL;
    }

    ControlArea = (PCONTROL_AREA) SectionObjectPointer->DataSectionObject;

    if (ControlArea == NULL) {
        UNLOCK_PFN (OldIrql);
        MiUnmapImageHeaderInHyperSpace ();
        return NULL;
    }

    if ((ControlArea->u.Flags.BeingCreated) ||
        (ControlArea->u.Flags.BeingDeleted)) {

        UNLOCK_PFN (OldIrql);
        MiUnmapImageHeaderInHyperSpace ();
        return NULL;
    }

    if (ControlArea->u.Flags.Rom == 0) {
        Subsection = (PSUBSECTION) (ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    //
    // If the prototype PTEs have been tossed (or never created) then we
    // don't have any data to copy.
    //

    PointerPte = Subsection->SubsectionBase;

    if (PointerPte == NULL) {
        UNLOCK_PFN (OldIrql);
        MiUnmapImageHeaderInHyperSpace ();
        return NULL;
    }

    if (MiGetPteAddress (PointerPte)->u.Hard.Valid == 0) {

        //
        // We have no reference to the data section so if we can't do this
        // without relinquishing the PFN lock, then don't bother.
        // ie: the entire control area and everything can be freed
        // while a call to MiMakeSystemAddressValidPfn releases the lock.
        //

        UNLOCK_PFN (OldIrql);
        MiUnmapImageHeaderInHyperSpace ();
        return NULL;
    }

    PteContents = *PointerPte;

    if ((PteContents.u.Hard.Valid == 1) ||
       ((PteContents.u.Soft.Prototype == 0) &&
         (PteContents.u.Soft.Transition == 1))) {

        if (PteContents.u.Hard.Valid == 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
        }
        else {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            if (Pfn1->u3.e1.ReadInProgress != 0) {
                UNLOCK_PFN (OldIrql);
                MiUnmapImageHeaderInHyperSpace ();
                return NULL;
            }
        }

        Process = PsGetCurrentProcess ();

        DataPage = MiMapPageInHyperSpaceAtDpc (Process, PageFrameIndex);

        RtlCopyMemory (ImagePage, DataPage, PAGE_SIZE);

        MiUnmapPageInHyperSpaceFromDpc (Process, DataPage);

        UNLOCK_PFN (OldIrql);

        return ImagePage;
    }

    //
    // The data page is not resident, so return NULL and the caller will take
    // the long way.
    //

    UNLOCK_PFN (OldIrql);
    MiUnmapImageHeaderInHyperSpace ();
    return NULL;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\crashdmp.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   crashdmp.c

Abstract:

    This module contains routines which provide support for writing out
    a crashdump on system failure.

Author:

    Landy Wang (landyw) 04-Oct-2000

Revision History:

--*/

#include "mi.h"

LOGICAL
MiIsAddressRangeValid (
    IN PVOID VirtualAddress,
    IN SIZE_T Length
    )
{
    PUCHAR Va;
    PUCHAR EndVa;
    ULONG Pages;
    
    Va = PAGE_ALIGN (VirtualAddress);
    Pages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (VirtualAddress, Length);
    EndVa = Va + (Pages << PAGE_SHIFT);
    
    while (Va < EndVa) {

        if (!MiIsAddressValid (Va, TRUE)) {
            return FALSE;
        }

        Va += PAGE_SIZE;
    }

    return TRUE;
}

VOID
MiRemoveFreePoolMemoryFromDump (
    IN PMM_KERNEL_DUMP_CONTEXT Context
    )

/*++

Routine Description:

    Removes all memory from the nonpaged pool free page lists to reduce the size
    of a kernel memory dump.

    Because the entries in these structures are destroyed by errant drivers
    that modify pool after freeing it, the entries are carefully
    validated prior to any dereferences.

Arguments:

    Context - Supplies the dump context pointer that must be passed to
              IoFreeDumpRange.

Return Value:

    None.

Environment:

    Kernel-mode, post-bugcheck.

    For use by crashdump routines ONLY.

--*/
{
    PLIST_ENTRY Entry;
    PLIST_ENTRY List;
    PLIST_ENTRY ListEnd;
    PMMFREE_POOL_ENTRY PoolEntry;
    ULONG LargePageMapped;

    List = &MmNonPagedPoolFreeListHead[0];
    ListEnd = List + MI_MAX_FREE_LIST_HEADS;

    for ( ; List < ListEnd; List += 1) {

        for (Entry = List->Flink; Entry != List; Entry = Entry->Flink) {

            PoolEntry = CONTAINING_RECORD (Entry,
                                           MMFREE_POOL_ENTRY,
                                           List);

            //
            // Check for corrupted values.
            //
            
            if (BYTE_OFFSET(PoolEntry) != 0) {
                break;
            }

            //
            // Check that the entry has not been corrupted.
            //
            
            if (MiIsAddressRangeValid (PoolEntry, sizeof (MMFREE_POOL_ENTRY)) == FALSE) {
                break;
            }

            if (PoolEntry->Size == 0) {
                break;
            }

            //
            // Signature is only maintained in checked builds.
            //
            
            ASSERT (PoolEntry->Signature == MM_FREE_POOL_SIGNATURE);

            //
            // Verify that the element's flinks and blinks are valid.
            //

            if ((!MiIsAddressRangeValid (Entry->Flink, sizeof (LIST_ENTRY))) ||
                (!MiIsAddressRangeValid (Entry->Blink, sizeof (LIST_ENTRY))) ||
                (Entry->Blink->Flink != Entry) ||
                (Entry->Flink->Blink != Entry)) {

                break;
            }

            //
            // The list entry is valid, remove it from the dump.
            //
        
            if (MI_IS_PHYSICAL_ADDRESS (PoolEntry)) {
                LargePageMapped = 1;
            }
            else {
                LargePageMapped = 0;
            }

            Context->FreeDumpRange (Context,
                                    PoolEntry,
                                    PoolEntry->Size,
                                    LargePageMapped);
        }
    }

}

VOID
MiAddPagesWithNoMappings (
    IN PMM_KERNEL_DUMP_CONTEXT Context
    )
/*++

Routine Description:

    Add pages to a kernel memory crashdump that do not have a
    virtual mapping in this process context.

    This includes entries that are wired directly into the TB.

Arguments:

    Context - Crashdump context pointer.

Return Value:

    None.

Environment:

    Kernel-mode, post-bugcheck.

    For use by crash dump routines ONLY.
    
--*/

{
#if defined (_X86_)

    ULONG LargePageMapped;
    PVOID Va;
    PHYSICAL_ADDRESS DirBase;

    //
    // Add the current page directory table page - don't use the directory
    // table base for the crashing process as we have switched cr3 on
    // stack overflow crashes, etc.
    //

    _asm {
        mov     eax, cr3
        mov     DirBase.LowPart, eax
    }

    //
    // cr3 is always located below 4gb physical.
    //

    DirBase.HighPart = 0;

    Va = MmGetVirtualForPhysical (DirBase);

    if (MI_IS_PHYSICAL_ADDRESS (Va)) {
        LargePageMapped = 1;
    }
    else {
        LargePageMapped = 0;
    }

    Context->SetDumpRange (Context,
                           Va,
                           1,
                           LargePageMapped);

#elif defined(_AMD64_)

    ULONG LargePageMapped;
    PVOID Va;
    PHYSICAL_ADDRESS DirBase;

    //
    // Add the current page directory table page - don't use the directory
    // table base for the crashing process as we have switched cr3 on
    // stack overflow crashes, etc.
    //

    DirBase.QuadPart = ReadCR3 ();

    Va = MmGetVirtualForPhysical (DirBase);

    if (MI_IS_PHYSICAL_ADDRESS (Va)) {
        LargePageMapped = 1;
    }
    else {
        LargePageMapped = 0;
    }

    Context->SetDumpRange (Context,
                           Va,
                           1,
                           LargePageMapped);

#elif defined(_IA64_)

    if (MiKseg0Mapping == TRUE) {
        Context->SetDumpRange (
                        Context,
                        MiKseg0Start,
                        (((ULONG_PTR)MiKseg0End - (ULONG_PTR)MiKseg0Start) >> PAGE_SHIFT) + 1,
                        1);
    }

#endif
}


LOGICAL
MiAddRangeToCrashDump (
    IN PMM_KERNEL_DUMP_CONTEXT Context,
    IN PVOID Va,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    Adds the specified range of memory to the crashdump.

Arguments:

    Context - Supplies the crashdump context pointer.

    Va - Supplies the starting virtual address.

    NumberOfBytes - Supplies the number of bytes to dump.  Note that for IA64,
                    this must not cause the range to cross a region boundary.

Return Value:

    TRUE if all valid pages were added to the crashdump, FALSE otherwise.

Environment:

    Kernel mode, post-bugcheck.

    For use by crash dump routines ONLY.

--*/

{
    LOGICAL Status;
    LOGICAL AddThisPage;
    ULONG Hint;
    PVOID EndingAddress;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NumberOfPages;
    
    Hint = 0;
    Status = TRUE;

    EndingAddress = (PVOID)((ULONG_PTR)Va + NumberOfBytes - 1);

#if defined(_IA64_)

    //
    // IA64 has a separate page directory parent for each region and
    // unimplemented address bits are ignored by the processor (as
    // long as they are canonical), but we must watch for them
    // here so the incrementing PPE walk doesn't go off the end.
    // This is done by truncating any given region request so it does
    // not go past the end of the specified region.  Note this
    // automatically will include the page maps which are sign extended
    // because the PPEs would just wrap anyway.
    //

    if (((ULONG_PTR)EndingAddress & ~VRN_MASK) >= MM_VA_MAPPED_BY_PPE * PDE_PER_PAGE) {
        EndingAddress = (PVOID)(((ULONG_PTR)EndingAddress & VRN_MASK) |
                         ((MM_VA_MAPPED_BY_PPE * PDE_PER_PAGE) - 1));
    }

#endif

    Va = PAGE_ALIGN (Va);

    PointerPxe = MiGetPxeAddress (Va);
    PointerPpe = MiGetPpeAddress (Va);
    PointerPde = MiGetPdeAddress (Va);
    PointerPte = MiGetPteAddress (Va);

    do {

#if (_MI_PAGING_LEVELS >= 3)
restart:
#endif

        KdCheckForDebugBreak ();

#if (_MI_PAGING_LEVELS >= 4)
        while (PointerPxe->u.Hard.Valid == 0) {

            //
            // This extended page directory parent entry is empty,
            // go to the next one.
            //

            PointerPxe += 1;
            PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            if ((Va > EndingAddress) || (Va == NULL)) {

                //
                // All done, return.
                //

                return Status;
            }
        }
#endif

        ASSERT (MiGetPpeAddress(Va) == PointerPpe);

#if (_MI_PAGING_LEVELS >= 3)
        while (PointerPpe->u.Hard.Valid == 0) {

            //
            // This page directory parent entry is empty, go to the next one.
            //

            PointerPpe += 1;
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            if ((Va > EndingAddress) || (Va == NULL)) {

                //
                // All done, return.
                //

                return Status;
            }
#if (_MI_PAGING_LEVELS >= 4)
            if (MiIsPteOnPdeBoundary (PointerPpe)) {
                PointerPxe += 1;
                ASSERT (PointerPxe == MiGetPteAddress (PointerPpe));
                goto restart;
            }
#endif

        }
#endif

        while (PointerPde->u.Hard.Valid == 0) {

            //
            // This page directory entry is empty, go to the next one.
            //

            PointerPde += 1;
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            if ((Va > EndingAddress) || (Va == NULL)) {

                //
                // All done, return.
                //

                return Status;
            }

#if (_MI_PAGING_LEVELS >= 3)
            if (MiIsPteOnPdeBoundary (PointerPde)) {
                PointerPpe += 1;
                ASSERT (PointerPpe == MiGetPteAddress (PointerPde));
                PointerPxe = MiGetPteAddress (PointerPpe);
                goto restart;
            }
#endif
        }

        //
        // A valid PDE has been located, examine each PTE.
        //

        ASSERT64 (PointerPpe->u.Hard.Valid == 1);
        ASSERT (PointerPde->u.Hard.Valid == 1);
        ASSERT (Va <= EndingAddress);

        if (MI_PDE_MAPS_LARGE_PAGE (PointerPde)) {

            //
            // This is a large page mapping - if the first page is backed
            // by RAM, then they all must be, so add the entire range
            // to the dump.
            //
                
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);

            if (MI_IS_PFN (PageFrameIndex)) {

                NumberOfPages = (((ULONG_PTR)MiGetVirtualAddressMappedByPde (PointerPde + 1) - (ULONG_PTR)Va) / PAGE_SIZE);

                Status = Context->SetDumpRange (Context,
                                                Va,
                                                NumberOfPages,
                                                1);

                if (!NT_SUCCESS (Status)) {
#if DBG
                    DbgPrint ("Adding large VA %p to crashdump failed\n", Va);
                    DbgBreakPoint ();
#endif
                    Status = FALSE;
                }
            }

            PointerPde += 1;
            Va = MiGetVirtualAddressMappedByPde (PointerPde);

            if ((Va > EndingAddress) || (Va == NULL)) {
                return Status;
            }

            PointerPte = MiGetPteAddress (Va);
            PointerPpe = MiGetPpeAddress (Va);
            PointerPxe = MiGetPxeAddress (Va);

            //
            // March on to the next page directory.
            //

            continue;
        }

        //
        // Exclude memory that is mapped in the system cache.
        // Note the system cache starts and ends on page directory boundaries
        // and is never mapped with large pages.
        //
        
        if (MI_IS_SYSTEM_CACHE_ADDRESS (Va)) {
            PointerPde += 1;
            Va = MiGetVirtualAddressMappedByPde (PointerPde);

            if ((Va > EndingAddress) || (Va == NULL)) {
                return Status;
            }

            PointerPte = MiGetPteAddress (Va);
            PointerPpe = MiGetPpeAddress (Va);
            PointerPxe = MiGetPxeAddress (Va);

            //
            // March on to the next page directory.
            //

            continue;
        }

        do {

            AddThisPage = FALSE;
            PageFrameIndex = 0;

            if (PointerPte->u.Hard.Valid == 1) {

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
                AddThisPage = TRUE;
            }
            else if ((PointerPte->u.Soft.Prototype == 0) &&
                     (PointerPte->u.Soft.Transition == 1)) {

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerPte);
                AddThisPage = TRUE;
            }

            if (AddThisPage == TRUE) {

                //
                // Include only addresses that are backed by RAM, not mapped to
                // I/O space.
                //
                
                AddThisPage = MI_IS_PFN (PageFrameIndex);

                if (AddThisPage == TRUE) {

                    //
                    // Add this page to the dump.
                    //
        
                    Status = Context->SetDumpRange (Context,
                                                    (PVOID) PageFrameIndex,
                                                    1,
                                                    2);

                    if (!NT_SUCCESS (Status)) {
#if DBG
                        DbgPrint ("Adding VA %p to crashdump failed\n", Va);
                        DbgBreakPoint ();
#endif
                        Status = FALSE;
                    }
                }
            }

            Va = (PVOID)((ULONG_PTR)Va + PAGE_SIZE);
            PointerPte += 1;

            ASSERT64 (PointerPpe->u.Hard.Valid == 1);
            ASSERT (PointerPde->u.Hard.Valid == 1);

            if ((Va > EndingAddress) || (Va == NULL)) {
                return Status;
            }

            //
            // If not at the end of a page table and still within the specified
            // range, just march directly on to the next PTE.
            //
            // Otherwise, if the virtual address is on a page directory boundary
            // then attempt to leap forward skipping over empty mappings
            // where possible.
            //

        } while (!MiIsVirtualAddressOnPdeBoundary(Va));

        ASSERT (PointerPte == MiGetPteAddress (Va));
        PointerPde = MiGetPdeAddress (Va);
        PointerPpe = MiGetPpeAddress (Va);
        PointerPxe = MiGetPxeAddress (Va);

    } while (TRUE);

    // NEVER REACHED
}


VOID
MiAddActivePageDirectories (
    IN PMM_KERNEL_DUMP_CONTEXT Context
    )
{
    UCHAR i;
    PKPRCB Prcb;
    PKPROCESS Process;
    PFN_NUMBER PageFrameIndex;

#if defined (_X86PAE_)
    PMMPTE PointerPte;
    ULONG j;
#endif

    for (i = 0; i < KeNumberProcessors; i += 1) {

        Prcb = KiProcessorBlock[i];

        Process = Prcb->CurrentThread->ApcState.Process;

#if defined (_X86PAE_)

        //
        // Add the 4 top level page directory pages to the dump.
        //

        PointerPte = (PMMPTE) ((PEPROCESS)Process)->PaeTop;

        for (j = 0; j < PD_PER_SYSTEM; j += 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE(PointerPte);
            PointerPte += 1;
            Context->SetDumpRange (Context, (PVOID) PageFrameIndex, 1, 2);
        }

        //
        // Add the real cr3 page to the dump, note that the value stored in the
        // directory table base is really a physical address (not a frame).
        //

        PageFrameIndex = Process->DirectoryTableBase[0];
        PageFrameIndex = (PageFrameIndex >> PAGE_SHIFT);

#else

        PageFrameIndex =
            MI_GET_DIRECTORY_FRAME_FROM_PROCESS ((PEPROCESS)(Process));

#endif

        //
        // Add this physical page to the dump.
        //

        Context->SetDumpRange (Context, (PVOID) PageFrameIndex, 1, 2);
    }

#if defined(_IA64_)

    //
    // The first processor's PCR is mapped in region 4 which is not (and cannot)
    // be scanned later, so explicitly add it to the dump here.
    //

    Prcb = KiProcessorBlock[0];

    Context->SetDumpRange (Context, (PVOID) Prcb->PcrPage, 1, 2);
#endif
}


VOID
MmGetKernelDumpRange (
    IN PMM_KERNEL_DUMP_CONTEXT Context
    )

/*++

Routine Description:

    Add (and subtract) ranges of system memory to the crashdump.

Arguments:

    Context - Crashdump context pointer.

Return Value:

    None.

Environment:

    Kernel mode, post-bugcheck.

    For use by crash dump routines ONLY.

--*/

{
    PVOID Va;
    SIZE_T NumberOfBytes;
    
    ASSERT ((Context != NULL) &&
            (Context->SetDumpRange != NULL) &&
            (Context->FreeDumpRange != NULL));
            
    MiAddActivePageDirectories (Context);

#if defined(_IA64_)

    //
    // Note each IA64 region must be passed separately to MiAddRange...
    //

    Va = (PVOID) ALT4KB_PERMISSION_TABLE_START;
    NumberOfBytes = PDE_UTBASE + PAGE_SIZE - (ULONG_PTR) Va;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

    Va = (PVOID) MM_SESSION_SPACE_DEFAULT;
    NumberOfBytes = PDE_STBASE + PAGE_SIZE - (ULONG_PTR) Va;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

    Va = (PVOID) KADDRESS_BASE;
    NumberOfBytes = PDE_KTBASE + PAGE_SIZE - (ULONG_PTR) Va;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

#elif defined(_AMD64_)

    Va = (PVOID) MM_SYSTEM_RANGE_START;
    NumberOfBytes = MM_KSEG0_BASE - (ULONG_PTR) Va;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

    Va = (PVOID) MM_KSEG2_BASE;
    NumberOfBytes = MM_SYSTEM_SPACE_START - (ULONG_PTR) Va;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

    Va = (PVOID) MM_PAGED_POOL_START;
    NumberOfBytes = MM_SYSTEM_SPACE_END - (ULONG_PTR) Va + 1;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

#else

    Va = MmSystemRangeStart;
    NumberOfBytes = MM_SYSTEM_SPACE_END - (ULONG_PTR) Va + 1;
    MiAddRangeToCrashDump (Context, Va, NumberOfBytes);

#endif

    //
    // Add any memory that is a part of the kernel space, but does not
    // have a virtual mapping (hence was not collected above).
    //
    
    MiAddPagesWithNoMappings (Context);

    //
    // Remove nonpaged pool that is not in use.
    //

    MiRemoveFreePoolMemoryFromDump (Context);
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\dmpaddr.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    dmpaddr.c

Abstract:

    Routines to examine pages and addresses.

Author:

    Lou Perazzoli (loup) 20-Mar-1989
    Landy Wang (landyw) 02-Jun-1997

Environment:

    Kernel Mode.

Revision History:

--*/

#include "mi.h"

#if DBG

LOGICAL
MiFlushUnusedSectionInternal (
    IN PCONTROL_AREA ControlArea
    );

#endif

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MmPerfSnapShotValidPhysicalMemory)
#endif

extern PFN_NUMBER MiStartOfInitialPoolFrame;
extern PFN_NUMBER MiEndOfInitialPoolFrame;
extern PMMPTE MmSystemPtesEnd[MaximumPtePoolTypes];

#if DBG
PFN_NUMBER MiIdentifyFrame = (PFN_NUMBER)-1;
ULONG MiIdentifyCounters[64];

#define MI_INCREMENT_IDENTIFY_COUNTER(x) {  \
            ASSERT (x < 64);                \
            MiIdentifyCounters[x] += 1;     \
        }
#else
#define MI_INCREMENT_IDENTIFY_COUNTER(x)
#endif


#if DBG
VOID
MiDumpValidAddresses (
    )
{
    ULONG_PTR va;
    ULONG i;
    ULONG j;
    PMMPTE PointerPde;
    PMMPTE PointerPte;

    va = 0;
    PointerPde = MiGetPdeAddress ((PVOID)va);

    for (i = 0; i < PDE_PER_PAGE; i += 1) {
        if (PointerPde->u.Hard.Valid) {
            DbgPrint("  **valid PDE, element %ld  %lx %lx\n",i,i,
                          PointerPde->u.Long);
            PointerPte = MiGetPteAddress ((PVOID)va);

            for (j = 0 ; j < PTE_PER_PAGE; j += 1) {
                if (PointerPte->u.Hard.Valid) {
                    DbgPrint("Valid address at %p PTE %p\n", (ULONG)va,
                          PointerPte->u.Long);
                }
                va += PAGE_SIZE;
                PointerPte += 1;
            }
        }
        else {
            va += (ULONG_PTR)PDE_PER_PAGE * (ULONG_PTR)PAGE_SIZE;
        }

        PointerPde += 1;
    }

    return;

}

VOID
MiFormatPte (
    IN PMMPTE PointerPte
    )
{

    PMMPTE proto_pte;
    PSUBSECTION subsect;

    if (MmIsAddressValid (PointerPte) == FALSE) {
        DbgPrint("   cannot dump PTE %p - it's not valid\n\n",
                 PointerPte);
        return;
    }

    DbgPrint("***DumpPTE at %p contains %p\n",
             PointerPte,
             PointerPte->u.Long);

    proto_pte = MiPteToProto(PointerPte);
    subsect = MiGetSubsectionAddress(PointerPte);

    DbgPrint("   protoaddr %p subsectaddr %p\n\n",
             proto_pte,
             (ULONG_PTR)subsect);

    return;
}

VOID
MiDumpWsl (
    VOID
    )
{
    ULONG i;
    PMMWSLE wsle;
    PEPROCESS CurrentProcess;

    CurrentProcess = PsGetCurrentProcess();

    DbgPrint("***WSLE cursize %lx frstfree %lx  Min %lx  Max %lx\n",
        CurrentProcess->Vm.WorkingSetSize,
        MmWorkingSetList->FirstFree,
        CurrentProcess->Vm.MinimumWorkingSetSize,
        CurrentProcess->Vm.MaximumWorkingSetSize);

    DbgPrint("   firstdyn %lx  last ent %lx  next slot %lx\n",
        MmWorkingSetList->FirstDynamic,
        MmWorkingSetList->LastEntry,
        MmWorkingSetList->NextSlot);

    wsle = MmWsle;

    for (i = 0; i < MmWorkingSetList->LastEntry; i += 1) {
        DbgPrint(" index %lx  %p\n",i,wsle->u1.Long);
        wsle += 1;
    }
    return;

}

#define ALLOC_SIZE ((ULONG)8*1024)
#define MM_SAVED_CONTROL 64

//
// Note these are deliberately sign-extended so they will always be greater
// than the highest user address.
//

#define MM_NONPAGED_POOL_MARK           ((PUCHAR)(LONG_PTR)0xfffff123)
#define MM_PAGED_POOL_MARK              ((PUCHAR)(LONG_PTR)0xfffff124)
#define MM_KERNEL_STACK_MARK            ((PUCHAR)(LONG_PTR)0xfffff125)
#define MM_PAGEFILE_BACKED_SHMEM_MARK   ((PUCHAR)(LONG_PTR)0xfffff126)

#define MM_DUMP_ONLY_VALID_PAGES    1

typedef struct _KERN_MAP {
    PVOID StartVa;
    PVOID EndVa;
    PKLDR_DATA_TABLE_ENTRY Entry;
} KERN_MAP, *PKERN_MAP;

ULONG
MiBuildKernelMap (
    OUT PKERN_MAP *KernelMapOut
    );

LOGICAL
MiIsAddressRangeValid (
    IN PVOID VirtualAddress,
    IN SIZE_T Length
    );

NTSTATUS
MmMemoryUsage (
    IN PVOID Buffer,
    IN ULONG Size,
    IN ULONG Type,
    OUT PULONG OutLength
    )

/*++

Routine Description:

    This routine (debugging only) dumps the current memory usage by
    walking the PFN database.

Arguments:

    Buffer - Supplies a *USER SPACE* buffer in which to copy the data.

    Size - Supplies the size of the buffer.

    Type - Supplies a value of 0 to dump everything,
           a value of 1 to dump only valid pages.

    OutLength - Returns how much data was written into the buffer.

Return Value:

    NTSTATUS.

--*/

{
    ULONG i;
    MMPFN_IDENTITY PfnId;
    PMMPFN LastPfn;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PSYSTEM_MEMORY_INFORMATION MemInfo;
    PSYSTEM_MEMORY_INFO Info;
    PSYSTEM_MEMORY_INFO InfoStart;
    PSYSTEM_MEMORY_INFO InfoEnd;
    PUCHAR String;
    PUCHAR Master;
    PCONTROL_AREA ControlArea;
    NTSTATUS status;
    ULONG Length;
    PEPROCESS Process;
    PUCHAR End;
    PCONTROL_AREA SavedControl[MM_SAVED_CONTROL];
    PSYSTEM_MEMORY_INFO  SavedInfo[MM_SAVED_CONTROL];
    ULONG j;
    ULONG ControlCount;
    UCHAR PageFileMappedString[] = "PageFile Mapped";
    UCHAR MetaFileString[] =       "Fs Meta File";
    UCHAR NoNameString[] =         "No File Name";
    UCHAR NonPagedPoolString[] =   "NonPagedPool";
    UCHAR PagedPoolString[] =      "PagedPool";
    UCHAR KernelStackString[] =    "Kernel Stack";
    PUCHAR NameString;
    PKERN_MAP KernMap;
    ULONG KernSize;
    PVOID VirtualAddress;
    PSUBSECTION Subsection;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;

    String = NULL;
    ControlCount = 0;
    Master = NULL;
    status = STATUS_SUCCESS;

    KernSize = MiBuildKernelMap (&KernMap);
    if (KernSize == 0) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    MemInfo = ExAllocatePoolWithTag (NonPagedPool, (SIZE_T) Size, 'lMmM');

    if (MemInfo == NULL) {
        ExFreePool (KernMap);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    InfoStart = &MemInfo->Memory[0];
    InfoEnd = InfoStart;
    End = (PUCHAR)MemInfo + Size;

    //
    // Walk through the ranges identifying pages.
    //

    LOCK_PFN (OldIrql);

    for (i = 0; i < MmPhysicalMemoryBlock->NumberOfRuns; i += 1) {

        Pfn1 = MI_PFN_ELEMENT (MmPhysicalMemoryBlock->Run[i].BasePage);
        LastPfn = Pfn1 + MmPhysicalMemoryBlock->Run[i].PageCount;

        for ( ; Pfn1 < LastPfn; Pfn1 += 1) {

            RtlZeroMemory (&PfnId, sizeof(PfnId));

            MiIdentifyPfn (Pfn1, &PfnId);

            if ((PfnId.u1.e1.ListDescription == FreePageList) ||
                (PfnId.u1.e1.ListDescription == ZeroedPageList) ||
                (PfnId.u1.e1.ListDescription == BadPageList) ||
                (PfnId.u1.e1.ListDescription == TransitionPage)) {
                    continue;
            }

            if (PfnId.u1.e1.ListDescription != ActiveAndValid) {
                if (Type == MM_DUMP_ONLY_VALID_PAGES) {
                    continue;
                }
            }

            if (PfnId.u1.e1.UseDescription == MMPFNUSE_PAGEFILEMAPPED) {

                //
                // This page belongs to a pagefile-backed shared memory section.
                //

                Master = MM_PAGEFILE_BACKED_SHMEM_MARK;
            }
            else if ((PfnId.u1.e1.UseDescription == MMPFNUSE_FILE) ||
                     (PfnId.u1.e1.UseDescription == MMPFNUSE_METAFILE)) {

                //
                // This shared page maps a file or file metadata.
                //

                Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
                ControlArea = Subsection->ControlArea;
                Master = (PUCHAR) ControlArea;
            }
            else if (PfnId.u1.e1.UseDescription == MMPFNUSE_NONPAGEDPOOL) {

                //
                // This is nonpaged pool, put it in the nonpaged pool cell.
                //

                Master = MM_NONPAGED_POOL_MARK;
            }
            else if (PfnId.u1.e1.UseDescription == MMPFNUSE_PAGEDPOOL) {

                //
                // This is paged pool, put it in the paged pool cell.
                //

                Master = MM_PAGED_POOL_MARK;
            }
            else if (PfnId.u1.e1.UseDescription == MMPFNUSE_SESSIONPRIVATE) {

                //
                // Call this paged pool for now.
                //

                Master = MM_PAGED_POOL_MARK;
            }
            else if (PfnId.u1.e1.UseDescription == MMPFNUSE_DRIVERLOCKPAGE) {

                //
                // Call this nonpaged pool for now.
                //

                Master = MM_NONPAGED_POOL_MARK;
            }
            else if (PfnId.u1.e1.UseDescription == MMPFNUSE_AWEPAGE) {

                //
                // Call this nonpaged pool for now.
                //

                Master = MM_NONPAGED_POOL_MARK;
            }
            else {

                //
                // See if the page is part of the kernel or a driver image.
                // If not but it's in system PTEs, call it a kernel thread
                // stack.
                //
                // If neither of the above, then see if the page belongs to
                // a user address or a session pagetable page.
                //

                VirtualAddress = PfnId.u2.VirtualAddress;

                for (j = 0; j < KernSize; j += 1) {
                    if ((VirtualAddress >= KernMap[j].StartVa) &&
                        (VirtualAddress < KernMap[j].EndVa)) {
                        Master = (PUCHAR)&KernMap[j];
                        break;
                    }
                }

                if (j == KernSize) {
                    if (PfnId.u1.e1.UseDescription == MMPFNUSE_SYSTEMPTE) {
                        Master = MM_KERNEL_STACK_MARK;
                    }
                    else if (MI_IS_SESSION_PTE (VirtualAddress)) {
                        Master = MM_NONPAGED_POOL_MARK;
                    }
                    else {

                        ASSERT ((PfnId.u1.e1.UseDescription == MMPFNUSE_PROCESSPRIVATE) || (PfnId.u1.e1.UseDescription == MMPFNUSE_PAGETABLE));

                        Master = (PUCHAR) (ULONG_PTR) PfnId.u1.e3.PageDirectoryBase;
                    }
                }
            }

            //
            // The page has been identified.
            // See if there is already a bucket allocated for it.
            //

            for (Info = InfoStart; Info < InfoEnd; Info += 1) {
                if (Info->StringOffset == Master) {
                    break;
                }
            }

            if (Info == InfoEnd) {

                InfoEnd += 1;
                if ((PUCHAR)InfoEnd > End) {
                    status = STATUS_DATA_OVERRUN;
                    goto Done;
                }

                RtlZeroMemory (Info, sizeof(*Info));
                Info->StringOffset = Master;
            }

            if (PfnId.u1.e1.ListDescription == ActiveAndValid) {
                Info->ValidCount += 1;
            }
            else if ((PfnId.u1.e1.ListDescription == StandbyPageList) ||
                     (PfnId.u1.e1.ListDescription == TransitionPage)) {

                Info->TransitionCount += 1;
            }
            else if ((PfnId.u1.e1.ListDescription == ModifiedPageList) ||
                     (PfnId.u1.e1.ListDescription == ModifiedNoWritePageList)) {
                Info->ModifiedCount += 1;
            }

            if (PfnId.u1.e1.UseDescription == MMPFNUSE_PAGETABLE) {
                Info->PageTableCount += 1;
            }
        }
    }

    MemInfo->StringStart = (ULONG_PTR)Buffer + (ULONG_PTR)InfoEnd - (ULONG_PTR)MemInfo;
    String = (PUCHAR)InfoEnd;

    //
    // Process the buckets ...
    //

    for (Info = InfoStart; Info < InfoEnd; Info += 1) {

        ControlArea = NULL;

        if (Info->StringOffset == MM_PAGEFILE_BACKED_SHMEM_MARK) {
            Length = 16;
            NameString = PageFileMappedString;
        }
        else if (Info->StringOffset == MM_NONPAGED_POOL_MARK) {
            Length = 14;
            NameString = NonPagedPoolString;
        }
        else if (Info->StringOffset == MM_PAGED_POOL_MARK) {
            Length = 14;
            NameString = PagedPoolString;
        }
        else if (Info->StringOffset == MM_KERNEL_STACK_MARK) {
            Length = 14;
            NameString = KernelStackString;
        }
        else if (((PUCHAR)Info->StringOffset >= (PUCHAR)&KernMap[0]) &&
                   ((PUCHAR)Info->StringOffset <= (PUCHAR)&KernMap[KernSize])) {

            DataTableEntry = ((PKERN_MAP)Info->StringOffset)->Entry;
            NameString = (PUCHAR)DataTableEntry->BaseDllName.Buffer;
            Length = DataTableEntry->BaseDllName.Length;
        }
        else if (Info->StringOffset > (PUCHAR)MM_HIGHEST_USER_ADDRESS) {

            //
            // This points to a control area - get the file name.
            //

            ControlArea = (PCONTROL_AREA)(Info->StringOffset);
            NameString = (PUCHAR)&ControlArea->FilePointer->FileName.Buffer[0];

            Length = ControlArea->FilePointer->FileName.Length;
            if (Length == 0) {
                if (ControlArea->u.Flags.NoModifiedWriting) {
                    NameString = MetaFileString;
                    Length = 14;
                }
                else if (ControlArea->u.Flags.File == 0) {
                    NameString = PageFileMappedString;
                    Length = 16;
                }
                else {
                    NameString = NoNameString;
                    Length = 14;
                }
            }
        }
        else {

            //
            // This is a process (or session) top-level page directory.
            //

            Pfn1 = MI_PFN_ELEMENT (PtrToUlong(Info->StringOffset));
            ASSERT (Pfn1->u4.PteFrame == MI_PFN_ELEMENT_TO_INDEX (Pfn1));

            Process = (PEPROCESS)Pfn1->u1.Event;

            NameString = &Process->ImageFileName[0];
            Length = 16;
        }

        if ((String+Length+2) >= End) {
            status = STATUS_DATA_OVERRUN;
            Info->StringOffset = NULL;
            goto Done;
        }

        if ((ControlArea == NULL) ||
            (MiIsAddressRangeValid (NameString, Length))) {

            RtlCopyMemory (String, NameString, Length);
            Info->StringOffset = (PUCHAR)Buffer + ((PUCHAR)String - (PUCHAR)MemInfo);
            String[Length] = 0;
            String[Length + 1] = 0;
            String += Length + 2;
        }
        else {
            if (!(ControlArea->u.Flags.BeingCreated ||
                  ControlArea->u.Flags.BeingDeleted) &&
                  (ControlCount < MM_SAVED_CONTROL)) {

                SavedControl[ControlCount] = ControlArea;
                SavedInfo[ControlCount] = Info;
                ControlArea->NumberOfSectionReferences += 1;
                ControlCount += 1;
            }
            Info->StringOffset = NULL;
        }
    }

Done:
    UNLOCK_PFN (OldIrql);
    ExFreePool (KernMap);

    while (ControlCount != 0) {

        //
        // Process all the pagable name strings.
        //

        ControlCount -= 1;
        ControlArea = SavedControl[ControlCount];
        Info = SavedInfo[ControlCount];
        NameString = (PUCHAR)&ControlArea->FilePointer->FileName.Buffer[0];
        Length = ControlArea->FilePointer->FileName.Length;
        if (Length == 0) {
            if (ControlArea->u.Flags.NoModifiedWriting) {
                Length = 12;
                NameString = MetaFileString;
            }
            else if (ControlArea->u.Flags.File == 0) {
                NameString = PageFileMappedString;
                Length = 16;

            }
            else {
                NameString = NoNameString;
                Length = 12;
            }
        }
        if ((String+Length+2) >= End) {
            status = STATUS_DATA_OVERRUN;
        }
        if (status != STATUS_DATA_OVERRUN) {
            RtlCopyMemory (String, NameString, Length);
            Info->StringOffset = (PUCHAR)Buffer + ((PUCHAR)String - (PUCHAR)MemInfo);
            String[Length] = 0;
            String[Length + 1] = 0;
            String += Length + 2;
        }

        LOCK_PFN (OldIrql);
        ControlArea->NumberOfSectionReferences -= 1;
        MiCheckForControlAreaDeletion (ControlArea);
        UNLOCK_PFN (OldIrql);
    }
    *OutLength = (ULONG)((PUCHAR)String - (PUCHAR)MemInfo);

    //
    // Carefully copy the results to the user buffer.
    //

    try {
        RtlCopyMemory (Buffer, MemInfo, (ULONG_PTR)String - (ULONG_PTR)MemInfo);
    } except (EXCEPTION_EXECUTE_HANDLER) {
        status = GetExceptionCode();
    }

    ExFreePool (MemInfo);

    return status;
}

ULONG
MiBuildKernelMap (
    OUT PKERN_MAP *KernelMapOut
    )
{
    PKTHREAD CurrentThread;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PKERN_MAP KernelMap;
    ULONG i;

    i = 0;
    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceShared (&PsLoadedModuleResource, TRUE);

    //
    // The caller wants us to allocate the return result buffer.  Size it
    // by allocating the maximum possibly needed as this should not be
    // very big (relatively).  It is the caller's responsibility to free
    // this.  Obviously this option can only be requested after pool has
    // been initialized.
    //

    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {
        i += 1;
        NextEntry = NextEntry->Flink;
    }

    KernelMap = ExAllocatePoolWithTag (NonPagedPool,
                                       i * sizeof(KERN_MAP),
                                       'lMmM');

    if (KernelMap == NULL) {
        return 0;
    }

    *KernelMapOut = KernelMap;

    i = 0;
    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {
        DataTableEntry = CONTAINING_RECORD (NextEntry,
                                            KLDR_DATA_TABLE_ENTRY,
                                            InLoadOrderLinks);
        KernelMap[i].Entry = DataTableEntry;
        KernelMap[i].StartVa = DataTableEntry->DllBase;
        KernelMap[i].EndVa = (PVOID)((ULONG_PTR)KernelMap[i].StartVa +
                                         DataTableEntry->SizeOfImage);
        i += 1;
        NextEntry = NextEntry->Flink;
    }

    ExReleaseResourceLite(&PsLoadedModuleResource);
    KeLeaveCriticalRegionThread (CurrentThread);

    return i;
}

VOID
MiDumpReferencedPages (
    VOID
    )

/*++

Routine Description:

    This routine (debugging only) dumps all PFN entries which appear
    to be locked in memory for i/o.

Arguments:

    None.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PMMPFN PfnLast;

    LOCK_PFN (OldIrql);

    Pfn1 = MI_PFN_ELEMENT (MmLowestPhysicalPage);
    PfnLast = MI_PFN_ELEMENT (MmHighestPhysicalPage);

    while (Pfn1 <= PfnLast) {

        if (MI_IS_PFN (MI_PFN_ELEMENT_TO_INDEX (Pfn1))) {

            if ((Pfn1->u2.ShareCount == 0) &&
                (Pfn1->u3.e2.ReferenceCount != 0)) {

                MiFormatPfn (Pfn1);
            }

            if (Pfn1->u3.e2.ReferenceCount > 1) {
                MiFormatPfn (Pfn1);
            }
        }

        Pfn1 += 1;
    }

    UNLOCK_PFN (OldIrql);
    return;
}

#else //DBG

NTSTATUS
MmMemoryUsage (
    IN PVOID Buffer,
    IN ULONG Size,
    IN ULONG Type,
    OUT PULONG OutLength
    )
{
    UNREFERENCED_PARAMETER (Buffer);
    UNREFERENCED_PARAMETER (Size);
    UNREFERENCED_PARAMETER (Type);
    UNREFERENCED_PARAMETER (OutLength);

    return STATUS_NOT_IMPLEMENTED;
}

#endif //DBG

//
// One benefit of using run length maximums of less than 4GB is that even
// frame numbers above 4GB are handled properly despite the 32-bit limitations
// of the bitmap routines.
//

#define MI_MAXIMUM_PFNID_RUN    4096


NTSTATUS
MmPerfSnapShotValidPhysicalMemory (
    VOID
    )

/*++

Routine Description:

    This routine logs the PFN numbers of all ActiveAndValid pages.

Arguments:

    None.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    ULONG i;
    PFN_NUMBER StartPage;
    PFN_NUMBER EndPage;
    ULONG_PTR MemSnapLocal[(sizeof(MMPFN_MEMSNAP_INFORMATION)/sizeof(ULONG_PTR)) + (MI_MAXIMUM_PFNID_RUN / (8*sizeof(ULONG_PTR))) ];
    PMMPFN_MEMSNAP_INFORMATION MemSnap;
    PMMPFN Pfn1;
    PMMPFN FirstPfn;
    PMMPFN LastPfn;
    PMMPFN MaxPfn;
    PMMPFN InitialPfn;
    RTL_BITMAP BitMap;
    PULONG ActualBits;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ASSERT ((MI_MAXIMUM_PFNID_RUN % (8 * sizeof(ULONG_PTR))) == 0);

    MemSnap = (PMMPFN_MEMSNAP_INFORMATION) MemSnapLocal;

    ActualBits = (PULONG)(MemSnap + 1);

    RtlInitializeBitMap (&BitMap, ActualBits, MI_MAXIMUM_PFNID_RUN);

    MemSnap->Count = 0;
    RtlClearAllBits (&BitMap);

    KeAcquireGuardedMutex (&MmDynamicMemoryMutex);

    for (i = 0; i < MmPhysicalMemoryBlock->NumberOfRuns; i += 1) {

        StartPage = MmPhysicalMemoryBlock->Run[i].BasePage;
        EndPage = StartPage + MmPhysicalMemoryBlock->Run[i].PageCount;
        FirstPfn = MI_PFN_ELEMENT (StartPage);
        LastPfn = MI_PFN_ELEMENT (EndPage);

        //
        // Find the first valid PFN and start the run there.
        //

        for (Pfn1 = FirstPfn; Pfn1 < LastPfn; Pfn1 += 1) {
            if (Pfn1->u3.e1.PageLocation == ActiveAndValid) {
                break;
            }
        }

        if (Pfn1 == LastPfn) {

            //
            // No valid PFNs in this block, move on to the next block.
            //

            continue;
        }

        MaxPfn = LastPfn;
        InitialPfn = NULL;

        do {
            if (Pfn1->u3.e1.PageLocation == ActiveAndValid) {
                if (InitialPfn == NULL) { 
                    MemSnap->InitialPageFrameIndex = MI_PFN_ELEMENT_TO_INDEX (Pfn1);
                    InitialPfn = Pfn1;
                    MaxPfn = InitialPfn + MI_MAXIMUM_PFNID_RUN;
                }
                RtlSetBit (&BitMap, (ULONG) (Pfn1 - InitialPfn));
            }

            Pfn1 += 1;

            if ((Pfn1 >= MaxPfn) && (InitialPfn != NULL)) {

                //
                // Log the bitmap as we're at then end of it.
                //

                ASSERT ((Pfn1 - InitialPfn) == MI_MAXIMUM_PFNID_RUN);
                MemSnap->Count = MI_MAXIMUM_PFNID_RUN;
                PerfInfoLogBytes (PERFINFO_LOG_TYPE_MEMORYSNAPLITE,
                                  MemSnap,
                                  sizeof(MemSnapLocal));

                InitialPfn = NULL;
                MaxPfn = LastPfn;
                RtlClearAllBits (&BitMap);
            }
        } while (Pfn1 < LastPfn);

        //
        // Dump any straggling bitmap entries now as this range is finished.
        //

        if (InitialPfn != NULL) {

            ASSERT (Pfn1 == LastPfn);
            ASSERT (Pfn1 < MaxPfn);
            ASSERT (Pfn1 > InitialPfn);

            MemSnap->Count = Pfn1 - InitialPfn;
            PerfInfoLogBytes (PERFINFO_LOG_TYPE_MEMORYSNAPLITE,
                              MemSnap,
                              sizeof(MMPFN_MEMSNAP_INFORMATION) +
                                  (ULONG) ((MemSnap->Count + 8) / 8));

            RtlClearAllBits (&BitMap);
        }
    }

    KeReleaseGuardedMutex (&MmDynamicMemoryMutex);

    return STATUS_SUCCESS;
}

#define PFN_ID_BUFFERS    128


NTSTATUS
MmIdentifyPhysicalMemory (
    VOID
    )

/*++

Routine Description:

    This routine calls the pfn id code for each page.  Because
    the logging can't handle very large amounts of data in a burst
    (limited buffering), the data is broken into page size chunks.

Arguments:

    None.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PMMPFN EndPfn;
    PFN_NUMBER PageFrameIndex;
    MMPFN_IDENTITY PfnIdBuffer[PFN_ID_BUFFERS];
    PMMPFN_IDENTITY BufferPointer;
    PMMPFN_IDENTITY BufferLast;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    BufferPointer = &PfnIdBuffer[0];
    BufferLast = BufferPointer + PFN_ID_BUFFERS;
    RtlZeroMemory (PfnIdBuffer, sizeof(PfnIdBuffer));

    KeAcquireGuardedMutex (&MmDynamicMemoryMutex);

    //
    // Walk through the ranges and identify pages until
    // the buffer is full or we've run out of pages.
    //

    for (i = 0; i < MmPhysicalMemoryBlock->NumberOfRuns; i += 1) {

        PageFrameIndex = MmPhysicalMemoryBlock->Run[i].BasePage;
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        EndPfn = Pfn1 + MmPhysicalMemoryBlock->Run[i].PageCount;

        LOCK_PFN (OldIrql);

        while (Pfn1 < EndPfn) {

            MiIdentifyPfn (Pfn1, BufferPointer);

            BufferPointer += 1;

            if (BufferPointer == BufferLast) {

                //
                // Release and reacquire the PFN lock so it's not held so long.
                //

                UNLOCK_PFN (OldIrql);

                //
                // Log the buffered entries.
                //

                BufferPointer = &PfnIdBuffer[0];
                do {

                    PerfInfoLogBytes (PERFINFO_LOG_TYPE_PAGEINMEMORY,
                                      BufferPointer,
                                      sizeof(PfnIdBuffer[0]));

                    BufferPointer += 1;

                } while (BufferPointer < BufferLast);

                //
                // Reset the buffer to the beginning and zero it.
                //

                BufferPointer = &PfnIdBuffer[0];
                RtlZeroMemory (PfnIdBuffer, sizeof(PfnIdBuffer));

                LOCK_PFN (OldIrql);
            }
            Pfn1 += 1;
        }

        UNLOCK_PFN (OldIrql);
    }

    //
    // Note that releasing this mutex here means the last entry can be
    // inserted out of order if we are preempted and another thread starts
    // the same operation (or if we're on an MP machine).  The PERF module
    // must handle this properly as any synchronization provided by this
    // routine is purely a side effect not deliberate.
    //

    KeReleaseGuardedMutex (&MmDynamicMemoryMutex);

    if (BufferPointer != &PfnIdBuffer[0]) {

        BufferLast = BufferPointer;
        BufferPointer = &PfnIdBuffer[0];

        do {

            PerfInfoLogBytes (PERFINFO_LOG_TYPE_PAGEINMEMORY,
                              BufferPointer,
                              sizeof(PfnIdBuffer[0]));

            BufferPointer += 1;

        } while (BufferPointer < BufferLast);
    }

    return STATUS_SUCCESS;
}

VOID
FASTCALL
MiIdentifyPfn (
    IN PMMPFN Pfn1,
    OUT PMMPFN_IDENTITY PfnIdentity
    )

/*++

Routine Description:

    This routine captures relevant information for the argument page frame.

Arguments:

    Pfn1 - Supplies the PFN element of the page frame number being queried.

    PfnIdentity - Receives the structure to fill in with the information.

Return Value:

    None.

Environment:

    Kernel mode.  PFN lock held.

--*/

{
    ULONG i;
    PMMPTE PteAddress;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PVOID VirtualAddress;
    PFILE_OBJECT FilePointer;
    PFN_NUMBER PageFrameIndex;

    MI_INCREMENT_IDENTIFY_COUNTER (8);

    ASSERT (PfnIdentity->u2.VirtualAddress == 0);
    ASSERT (PfnIdentity->u1.e1.ListDescription == 0);
    ASSERT (PfnIdentity->u1.e1.UseDescription == 0);
    ASSERT (PfnIdentity->u1.e1.Pinned == 0);
    ASSERT (PfnIdentity->u1.e2.Offset == 0);

    MM_PFN_LOCK_ASSERT();

    PageFrameIndex = MI_PFN_ELEMENT_TO_INDEX (Pfn1);
    PfnIdentity->PageFrameIndex = PageFrameIndex;
    PfnIdentity->u1.e1.ListDescription = Pfn1->u3.e1.PageLocation;

#if DBG
    if (PageFrameIndex == MiIdentifyFrame) {
        DbgPrint ("MmIdentifyPfn: requested PFN %p\n", PageFrameIndex);
        DbgBreakPoint ();
    }
#endif

    MI_INCREMENT_IDENTIFY_COUNTER (Pfn1->u3.e1.PageLocation);

    switch (Pfn1->u3.e1.PageLocation) {

        case ZeroedPageList:
        case FreePageList:
        case BadPageList:
                return;

        case ActiveAndValid:

                //
                // It's too much work to determine if the page is locked
                // in a working set due to cross-process WSL references, etc.
                // So don't bother for now.
                //

                ASSERT (PfnIdentity->u1.e1.ListDescription == MMPFNLIST_ACTIVE);

                if (Pfn1->u1.WsIndex == 0) {
                    MI_INCREMENT_IDENTIFY_COUNTER (9);
                    PfnIdentity->u1.e1.Pinned = 1;
                }
                else if (Pfn1->u3.e2.ReferenceCount > 1) {

                    //
                    // This page is pinned, presumably for an ongoing I/O.
                    //

                    PfnIdentity->u1.e1.Pinned = 1;
                    MI_INCREMENT_IDENTIFY_COUNTER (10);
                }
                break;

        case StandbyPageList:
        case ModifiedPageList:
        case ModifiedNoWritePageList:
                if (Pfn1->u3.e2.ReferenceCount >= 1) {

                    //
                    // This page is pinned, presumably for an ongoing I/O.
                    //

                    PfnIdentity->u1.e1.Pinned = 1;
                    MI_INCREMENT_IDENTIFY_COUNTER (11);
                }

                if ((Pfn1->u3.e1.PageLocation == ModifiedPageList) &&
                    (MI_IS_PFN_DELETED (Pfn1)) &&
                    (Pfn1->u2.ShareCount == 0)) {

                    //
                    // This page may be a modified write completing in the
                    // context of the modified writer thread.  If the
                    // address space was deleted while the I/O was in
                    // progress, the frame will be released now.  More
                    // importantly, the frame's containing frame is
                    // meaningless as it may have already been freed
                    // and reused.
                    //
                    // We can't tell what this page was being used for
                    // since its address space is gone, so just call it
                    // process private for now.
                    //

                    MI_INCREMENT_IDENTIFY_COUNTER (40);
                    PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PROCESSPRIVATE;

                    return;
                }

                break;

        case TransitionPage:

                //
                // This page is pinned due to a straggling I/O - the virtual
                // address has been deleted but an I/O referencing it has not
                // completed.
                //

                PfnIdentity->u1.e1.Pinned = 1;
                MI_INCREMENT_IDENTIFY_COUNTER (11);
                PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PROCESSPRIVATE;
                return;

        default:
#if DBG
                DbgPrint ("MmIdentifyPfn: unknown PFN %p %x\n",
                            Pfn1, Pfn1->u3.e1.PageLocation);
                DbgBreakPoint ();
#endif
                break;

    }

    //
    // Capture differing information based on the type of page being examined.
    //

    //
    // General purpose stress shows 40% of the pages are prototypes so
    // for speed, check for these first.
    //

    if (Pfn1->u3.e1.PrototypePte == 1) {

        MI_INCREMENT_IDENTIFY_COUNTER (12);

        if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {

            //
            // Demand zero or (equivalently) pagefile backed.
            //
            // There are some hard problems here preventing more indepth
            // identification of these pages:
            //
            // 1.  The PFN contains a backpointer to the prototype PTE - but
            //     there is no definitive way to get to the SEGMENT or
            //     CONTROL_AREA from this.
            //
            // 2.  The prototype PTE pointer itself may be paged out and
            //     the PFN lock is held right now.
            //

            MI_INCREMENT_IDENTIFY_COUNTER (13);

#if 0
            PfnIdentity->u2.FileObject = (PVOID) ControlArea->Segment->u1.CreatingProcess;

            PfnIdentity->u1.e2.Offset = (((ULONG_PTR)ControlArea->Segment->u2.FirstMappedVa) >> MMSECTOR_SHIFT);
#endif

            PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PAGEFILEMAPPED;
            return;
        }

        MI_INCREMENT_IDENTIFY_COUNTER (14);

        //
        // Backed by a mapped file.
        //

        Subsection = MiGetSubsectionAddress (&Pfn1->OriginalPte);
        ControlArea = Subsection->ControlArea;
        ASSERT (ControlArea->u.Flags.File == 1);
        FilePointer = ControlArea->FilePointer;
        ASSERT (FilePointer != NULL);

        PfnIdentity->u2.FileObject = FilePointer;

        if (Subsection->SubsectionBase != NULL) {
            PfnIdentity->u1.e2.Offset = (MiStartingOffset (Subsection, Pfn1->PteAddress) >> MMSECTOR_SHIFT);
        }
        else {

            //
            // The only time we should be here (a valid PFN with no subsection)
            // is if we are the segment dereference thread putting pages into
            // the freelist.  At this point the PFN lock is held and the
            // control area/subsection/PFN structures are not yet consistent
            // so just treat this as an offset of 0 as it should be rare.
            //

            ASSERT (PsGetCurrentThread()->StartAddress == (PVOID)(ULONG_PTR)MiDereferenceSegmentThread);
        }

        //
        // Check for nomodwrite sections - typically this is filesystem
        // metadata although it could also be registry data (which is named).
        //

        if (ControlArea->u.Flags.NoModifiedWriting) {
            MI_INCREMENT_IDENTIFY_COUNTER (15);
            PfnIdentity->u1.e1.UseDescription = MMPFNUSE_METAFILE;
            return;
        }

        if (FilePointer->FileName.Length != 0) {

            //
            // This mapped file has a name.
            //

            MI_INCREMENT_IDENTIFY_COUNTER (16);
            PfnIdentity->u1.e1.UseDescription = MMPFNUSE_FILE;
            return;
        }

        //
        // No name - this file must be in the midst of a purge, but it
        // still *was* a mapped file of some sort.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (17);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_FILE;
        return;
    }

    if ((PageFrameIndex >= MiStartOfInitialPoolFrame) &&
        (PageFrameIndex <= MiEndOfInitialPoolFrame)) {

        //
        // This is initial nonpaged pool.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (18);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_NONPAGEDPOOL;
        VirtualAddress = (PVOID)((ULONG_PTR)MmNonPagedPoolStart +
                                ((PageFrameIndex - MiStartOfInitialPoolFrame) << PAGE_SHIFT));
        PfnIdentity->u2.VirtualAddress = VirtualAddress;
        return;
    }

    PteAddress = Pfn1->PteAddress;
    VirtualAddress = MiGetVirtualAddressMappedByPte (PteAddress);
    PfnIdentity->u2.VirtualAddress = VirtualAddress;

    if (MI_IS_SESSION_ADDRESS(VirtualAddress)) {

        //
        // Note session addresses that map images (or views) that haven't
        // undergone a copy-on-write split were already treated as prototype
        // PTEs above.  This clause handles session pool and copy-on-written
        // pages.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (19);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_SESSIONPRIVATE;
        return;
    }

    if ((VirtualAddress >= MmPagedPoolStart) &&
        (VirtualAddress <= MmPagedPoolEnd)) {

        //
        // This is paged pool.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (20);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PAGEDPOOL;
        return;

    }

    if ((VirtualAddress >= MmNonPagedPoolExpansionStart) &&
        (VirtualAddress < MmNonPagedPoolEnd)) {

        //
        // This is expansion nonpaged pool.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (21);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_NONPAGEDPOOL;
        return;
    }

    if ((VirtualAddress >= MmNonPagedSystemStart) &&
        (PteAddress <= MmSystemPtesEnd[SystemPteSpace])) {

        //
        // This is driver space, kernel stack, special pool or other
        // system PTE mappings.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (22);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_SYSTEMPTE;
        return;
    }

#if defined (_X86_)

    //
    // 2 other ranges of system PTEs can exist on x86.
    //

    if (((MiNumberOfExtraSystemPdes != 0) &&
         (VirtualAddress >= (PVOID)MiExtraResourceStart) &&
         (VirtualAddress < (PVOID)MiExtraResourceEnd)) ||

        ((MiUseMaximumSystemSpace != 0) &&
         (VirtualAddress >= (PVOID)MiUseMaximumSystemSpace) &&
         (VirtualAddress < (PVOID)MiUseMaximumSystemSpaceEnd)))
    {
        //
        // This is driver space, kernel stack, special pool or other
        // system PTE mappings.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (23);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_SYSTEMPTE;
        return;
    }

#endif

    if (Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) {

        MI_INCREMENT_IDENTIFY_COUNTER (24);

        //
        // Carefully check here as this could be a legitimate frame as well.
        //

        if ((Pfn1->u3.e1.StartOfAllocation == 1) &&
            (Pfn1->u3.e1.EndOfAllocation == 1) &&
            (Pfn1->u3.e1.PageLocation == ActiveAndValid)) {
                if (MI_IS_PFN_DELETED (Pfn1)) {
                    MI_INCREMENT_IDENTIFY_COUNTER (25);
                    PfnIdentity->u1.e1.UseDescription = MMPFNUSE_DRIVERLOCKPAGE;
                }
                else {
                    MI_INCREMENT_IDENTIFY_COUNTER (26);
                    PfnIdentity->u1.e1.UseDescription = MMPFNUSE_AWEPAGE;
                }
                return;
        }
    }

#if DBG

    //
    // In checked kernels, AWE frames get their containing frame decremented
    // when the AWE frame is freed.
    //

    if (Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME - 1) {

        MI_INCREMENT_IDENTIFY_COUNTER (24);

        //
        // Carefully check here as this could be a legitimate frame as well.
        //

        if ((Pfn1->u3.e1.StartOfAllocation == 0) &&
            (Pfn1->u3.e1.EndOfAllocation == 0) &&
            (Pfn1->u3.e1.PageLocation == StandbyPageList)) {

            MI_INCREMENT_IDENTIFY_COUNTER (26);
            PfnIdentity->u1.e1.UseDescription = MMPFNUSE_AWEPAGE;
            return;
        }
    }

#endif

    //
    // Check the PFN working set index carefully here.  This must be done
    // before walking back through the containing frames because if this page
    // is not in a working set, the containing frame may not be meaningful and
    // dereferencing it can crash the system and/or yield incorrect walks.
    // This is because if a page will never be trimmable there is no need to
    // have a containing frame initialized.  This also covers the case of
    // data pages mapped via large page directory entries as these have no
    // containing page table frame.
    //

    if (Pfn1->u3.e1.PageLocation == ActiveAndValid) {

        if (Pfn1->u1.WsIndex == 0) {

            //
            // Default to calling these allocations nonpaged pool because even
            // when they technically are not, from a usage standpoint they are.
            // Note the default is overridden for specific cases where the usage
            // is not in fact nonpaged.
            //

            PfnIdentity->u1.e1.UseDescription = MMPFNUSE_NONPAGEDPOOL;
            ASSERT (PfnIdentity->u1.e1.Pinned == 1);
            MI_INCREMENT_IDENTIFY_COUNTER (27);
            return;
        }
    }


    //
    // Must be a process private page
    //
    // OR
    //
    // a page table, page directory, parent or extended parent.
    //

    i = 0;
    while (Pfn1->u4.PteFrame != PageFrameIndex) {

        //
        // The only way the PTE address will go out of bounds is if this is
        // a top level page directory page for a process that has been
        // swapped out but is still waiting for the transition/modified
        // page table pages to be reclaimed.  ie: until that happens, the
        // page directory is marked Active, but the PteAddress & containing
        // page are pointing at the EPROCESS pool page.
        //

#if defined(_IA64_)

        if (((Pfn1->PteAddress >= (PMMPTE) PTE_BASE) &&
            (Pfn1->PteAddress <= (PMMPTE) PTE_TOP)) ||

            ((Pfn1->PteAddress >= (PMMPTE) PTE_KBASE) &&
            (Pfn1->PteAddress <= (PMMPTE) PTE_KTOP)) ||

            ((Pfn1->PteAddress >= (PMMPTE) PTE_SBASE) &&
            (Pfn1->PteAddress <= (PMMPTE) PTE_STOP)) ||
        
            ((Pfn1->PteAddress >= (PMMPTE) PDE_BASE) &&
            (Pfn1->PteAddress <= (PMMPTE) PDE_TOP)) ||

            ((Pfn1->PteAddress >= (PMMPTE) PDE_KBASE) &&
            (Pfn1->PteAddress <= (PMMPTE) PDE_KTOP)) ||

            ((Pfn1->PteAddress >= (PMMPTE) PDE_SBASE) &&
            (Pfn1->PteAddress <= (PMMPTE) PDE_STOP)) ||
        
            ((Pfn1->PteAddress >= (PMMPTE) PDE_TBASE) &&
            (Pfn1->PteAddress <= (PMMPTE) ((ULONG_PTR)PDE_TBASE + PAGE_SIZE -1))) ||
        
            ((Pfn1->PteAddress >= (PMMPTE) PDE_KTBASE) &&
            (Pfn1->PteAddress <= (PMMPTE) ((ULONG_PTR)PDE_KTBASE + PAGE_SIZE -1))) ||
        
            ((Pfn1->PteAddress >= (PMMPTE) PDE_STBASE) &&
            (Pfn1->PteAddress <= (PMMPTE) ((ULONG_PTR)PDE_STBASE + PAGE_SIZE -1)))
        )

#else

        if ((Pfn1->PteAddress >= (PMMPTE) PTE_BASE) &&
            (Pfn1->PteAddress <= (PMMPTE) PTE_TOP))

#endif

        {
            PageFrameIndex = Pfn1->u4.PteFrame;
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            i += 1;
        }
        else {
            MI_INCREMENT_IDENTIFY_COUNTER (41);
            break;
        }
    }

    MI_INCREMENT_IDENTIFY_COUNTER (31+i);

    PfnIdentity->u1.e3.PageDirectoryBase = PageFrameIndex;

#if defined(_X86PAE_)

    //
    // PAE is unique because the 3rd level is not defined as only a mini
    // 4 entry 3rd level is in use.  Check for that explicitly, noting that
    // it takes one extra walk to get to the top.  Top level PAE pages (the
    // ones that contain only the 4 PDPTE pointers) are treated above as
    // active pinned pages, not as pagetable pages because each one is shared
    // across 127 processes and resides in the system global space.
    //

    if (i == _MI_PAGING_LEVELS + 1) {

        //
        // Had to walk all the way to the top.  Must be a data page.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (29);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PROCESSPRIVATE;
        return;
    }

#else

    if (i == _MI_PAGING_LEVELS) {

        //
        // Had to walk all the way to the top.  Must be a data page.
        //

        MI_INCREMENT_IDENTIFY_COUNTER (29);
        PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PROCESSPRIVATE;
        return;
    }

#endif

    //
    // Must have been a page in the hierarchy (not a data page) as we arrived
    // at the top early.
    //

    MI_INCREMENT_IDENTIFY_COUNTER (30);
    PfnIdentity->u1.e1.UseDescription = MMPFNUSE_PAGETABLE;

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\dynmem.c ===
/*++

Copyright (c) 1999  Microsoft Corporation

Module Name:

    dynmem.c

Abstract:

    This module contains the routines which implement dynamically adding
    and removing physical memory from the system.

Author:

    Landy Wang (landyw) 05-Feb-1999

Revision History:

--*/

#include "mi.h"

KGUARDED_MUTEX MmDynamicMemoryMutex;

LOGICAL MiTrimRemovalPagesOnly = FALSE;

#if DBG
ULONG MiShowStuckPages;
ULONG MiDynmemData[9];
#endif

#if defined (_MI_COMPRESSION)
extern PMM_SET_COMPRESSION_THRESHOLD MiSetCompressionThreshold;
#endif

//
// Leave the low 3 bits clear as this will be inserted into the PFN PteAddress.
//

#define PFN_REMOVED     ((PMMPTE)(INT_PTR)(int)0x99887768)

PFN_COUNT
MiRemovePhysicalPages (
    IN PFN_NUMBER StartPage,
    IN PFN_NUMBER EndPage
    );

NTSTATUS
MiRemovePhysicalMemory (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes,
    IN LOGICAL PermanentRemoval,
    IN ULONG Flags
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MmRemovePhysicalMemory)
#pragma alloc_text(PAGE,MmMarkPhysicalMemoryAsBad)
#pragma alloc_text(PAGELK,MmAddPhysicalMemory)
#pragma alloc_text(PAGELK,MmAddPhysicalMemoryEx)
#pragma alloc_text(PAGELK,MiRemovePhysicalMemory)
#pragma alloc_text(PAGELK,MmMarkPhysicalMemoryAsGood)
#pragma alloc_text(PAGELK,MmGetPhysicalMemoryRanges)
#pragma alloc_text(PAGELK,MiRemovePhysicalPages)
#endif


NTSTATUS
MmAddPhysicalMemory (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes
    )

/*++

Routine Description:

    A wrapper for MmAddPhysicalMemoryEx.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being added.
                     If any bytes were added (ie: STATUS_SUCCESS is being
                     returned), the actual amount is returned here.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    return MmAddPhysicalMemoryEx (StartAddress, NumberOfBytes, 0);
}


NTSTATUS
MmAddPhysicalMemoryEx (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes,
    IN ULONG Flags
    )

/*++

Routine Description:

    This routine adds the specified physical address range to the system.
    This includes initializing PFN database entries and adding it to the
    freelists.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being added.
                     If any bytes were added (ie: STATUS_SUCCESS is being
                     returned), the actual amount is returned here.

    Flags  - Supplies relevant flags describing the memory range.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    ULONG i;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    LOGICAL Inserted;
    LOGICAL Updated;
    MMPTE TempPte;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PFN_NUMBER PagesToReturn;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER start;
    PFN_NUMBER count;
    PFN_NUMBER StartPage;
    PFN_NUMBER EndPage;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER Page;
    PFN_NUMBER LastPage;
    PFN_NUMBER TotalPagesAllowed;
    PFN_COUNT PagesNeeded;
    PPHYSICAL_MEMORY_DESCRIPTOR OldPhysicalMemoryBlock;
    PPHYSICAL_MEMORY_DESCRIPTOR NewPhysicalMemoryBlock;
    PPHYSICAL_MEMORY_RUN NewRun;
    LOGICAL PfnDatabaseIsPhysical;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    if (BYTE_OFFSET(StartAddress->LowPart) != 0) {
        return STATUS_INVALID_PARAMETER_1;
    }

    if (BYTE_OFFSET(NumberOfBytes->LowPart) != 0) {
        return STATUS_INVALID_PARAMETER_2;
    }

#if defined (_MI_COMPRESSION)
    if (Flags & ~MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
        return STATUS_INVALID_PARAMETER_3;
    }
#else
    if (Flags != 0) {
        return STATUS_INVALID_PARAMETER_3;
    }
#endif

    //
    // The system must be configured for dynamic memory addition.  This is
    // critical as only then is the database guaranteed to be non-sparse.
    //
    
    if (MmDynamicPfn == 0) {
        return STATUS_NOT_SUPPORTED;
    }

    if (MI_IS_PHYSICAL_ADDRESS(MmPfnDatabase)) {
        PfnDatabaseIsPhysical = TRUE;
    }
    else {
        PfnDatabaseIsPhysical = FALSE;
    }

    StartPage = (PFN_NUMBER)(StartAddress->QuadPart >> PAGE_SHIFT);
    NumberOfPages = (PFN_NUMBER)(NumberOfBytes->QuadPart >> PAGE_SHIFT);

    EndPage = StartPage + NumberOfPages;

    if (StartPage >= EndPage) {
        return STATUS_INVALID_PARAMETER_1;
    }

    if (EndPage - 1 > MmHighestPossiblePhysicalPage) {

        //
        // Truncate the request into something that can be mapped by the PFN
        // database.
        //

        EndPage = MmHighestPossiblePhysicalPage + 1;
        NumberOfPages = EndPage - StartPage;
    }

    if (StartPage >= EndPage) {
        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // Ensure that the memory being added does not exceed the license
    // restrictions.
    //

    if (ExVerifySuite(DataCenter) == TRUE) {
        TotalPagesAllowed = MI_DTC_MAX_PAGES;
    }
    else if ((MmProductType != 0x00690057) &&
             (ExVerifySuite(Enterprise) == TRUE)) {

        TotalPagesAllowed = MI_ADS_MAX_PAGES;
    }
    else {
        TotalPagesAllowed = MI_DEFAULT_MAX_PAGES;
    }

    if (MmNumberOfPhysicalPages + NumberOfPages > TotalPagesAllowed) {

        //
        // Truncate the request appropriately.
        //

        NumberOfPages = TotalPagesAllowed - MmNumberOfPhysicalPages;
        EndPage = StartPage + NumberOfPages;
    }

    //
    // The range cannot wrap.
    //

    if (StartPage >= EndPage) {
        return STATUS_INVALID_PARAMETER_1;
    }

    KeAcquireGuardedMutex (&MmDynamicMemoryMutex);

    OldPhysicalMemoryBlock = MmPhysicalMemoryBlock;

    i = (sizeof(PHYSICAL_MEMORY_DESCRIPTOR) +
         (sizeof(PHYSICAL_MEMORY_RUN) * (MmPhysicalMemoryBlock->NumberOfRuns + 1)));

    NewPhysicalMemoryBlock = ExAllocatePoolWithTag (NonPagedPool,
                                                    i,
                                                    '  mM');

    if (NewPhysicalMemoryBlock == NULL) {
        KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // The range cannot overlap any ranges that are already present.
    //

    start = 0;
    TempPte = ValidKernelPte;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

#if defined (_MI_COMPRESSION)

    //
    // Adding compression-generated ranges can only be done if the hardware
    // has already successfully announced itself.
    //

    if (Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
        if (MiSetCompressionThreshold == NULL) {
            UNLOCK_PFN (OldIrql);
            MmUnlockPagableImageSection(ExPageLockHandle);
            KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
            ExFreePool (NewPhysicalMemoryBlock);
            return STATUS_NOT_SUPPORTED;
        }
    }
#endif

    do {

        count = MmPhysicalMemoryBlock->Run[start].PageCount;
        Page = MmPhysicalMemoryBlock->Run[start].BasePage;

        if (count != 0) {

            LastPage = Page + count;

            if ((StartPage < Page) && (EndPage > Page)) {
                UNLOCK_PFN (OldIrql);
                MmUnlockPagableImageSection(ExPageLockHandle);
                KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
                ExFreePool (NewPhysicalMemoryBlock);
                return STATUS_CONFLICTING_ADDRESSES;
            }

            if ((StartPage >= Page) && (StartPage < LastPage)) {
                UNLOCK_PFN (OldIrql);
                MmUnlockPagableImageSection(ExPageLockHandle);
                KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
                ExFreePool (NewPhysicalMemoryBlock);
                return STATUS_CONFLICTING_ADDRESSES;
            }
        }

        start += 1;

    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    //
    // Fill any gaps in the (sparse) PFN database needed for these pages,
    // unless the PFN database was physically allocated and completely
    // committed up front.
    //

    PagesNeeded = 0;

    if (PfnDatabaseIsPhysical == FALSE) {
        PointerPte = MiGetPteAddress (MI_PFN_ELEMENT(StartPage));
        LastPte = MiGetPteAddress ((PCHAR)(MI_PFN_ELEMENT(EndPage)) - 1);
    
        while (PointerPte <= LastPte) {
            if (PointerPte->u.Hard.Valid == 0) {
                PagesNeeded += 1;
            }
            PointerPte += 1;
        }
    
        if (MmAvailablePages < PagesNeeded) {
            UNLOCK_PFN (OldIrql);
            MmUnlockPagableImageSection(ExPageLockHandle);
            KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
            ExFreePool (NewPhysicalMemoryBlock);
            return STATUS_INSUFFICIENT_RESOURCES;
        }
    
        PointerPte = MiGetPteAddress (MI_PFN_ELEMENT(StartPage));
    
        while (PointerPte <= LastPte) {
            if (PointerPte->u.Hard.Valid == 0) {
    
                PageFrameIndex = MiRemoveZeroPage(MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));
    
                MiInitializePfn (PageFrameIndex, PointerPte, 0);
    
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

                MI_WRITE_VALID_PTE (PointerPte, TempPte);
            }
            PointerPte += 1;
        }
        MI_DECREMENT_RESIDENT_AVAILABLE (PagesNeeded, MM_RESAVAIL_ALLOCATE_HOTADD_PFNDB);
    }

    //
    // If the new range is adjacent to an existing range, just merge it into
    // the old block.  Otherwise use the new block as a new entry will have to
    // be used.
    //

    NewPhysicalMemoryBlock->NumberOfRuns = MmPhysicalMemoryBlock->NumberOfRuns + 1;
    NewPhysicalMemoryBlock->NumberOfPages = MmPhysicalMemoryBlock->NumberOfPages + NumberOfPages;

    NewRun = &NewPhysicalMemoryBlock->Run[0];
    start = 0;
    Inserted = FALSE;
    Updated = FALSE;

    do {

        Page = MmPhysicalMemoryBlock->Run[start].BasePage;
        count = MmPhysicalMemoryBlock->Run[start].PageCount;

        if (Inserted == FALSE) {

            //
            // Note overlaps into adjacent ranges were already checked above.
            //

            if (StartPage == Page + count) {
                MmPhysicalMemoryBlock->Run[start].PageCount += NumberOfPages;
                OldPhysicalMemoryBlock = NewPhysicalMemoryBlock;
                MmPhysicalMemoryBlock->NumberOfPages += NumberOfPages;

                //
                // Coalesce below and above to avoid leaving zero length gaps
                // as these gaps would prevent callers from removing ranges
                // the span them.
                //

                if (start + 1 < MmPhysicalMemoryBlock->NumberOfRuns) {

                    start += 1;
                    Page = MmPhysicalMemoryBlock->Run[start].BasePage;
                    count = MmPhysicalMemoryBlock->Run[start].PageCount;

                    if (StartPage + NumberOfPages == Page) {
                        MmPhysicalMemoryBlock->Run[start - 1].PageCount +=
                            count;
                        MmPhysicalMemoryBlock->NumberOfRuns -= 1;

                        //
                        // Copy any remaining entries.
                        //
    
                        if (start != MmPhysicalMemoryBlock->NumberOfRuns) {
                            RtlMoveMemory (&MmPhysicalMemoryBlock->Run[start],
                                           &MmPhysicalMemoryBlock->Run[start + 1],
                                           (MmPhysicalMemoryBlock->NumberOfRuns - start) * sizeof (PHYSICAL_MEMORY_RUN));
                        }
                    }
                }
                Updated = TRUE;
                break;
            }

            if (StartPage + NumberOfPages == Page) {
                MmPhysicalMemoryBlock->Run[start].BasePage = StartPage;
                MmPhysicalMemoryBlock->Run[start].PageCount += NumberOfPages;
                OldPhysicalMemoryBlock = NewPhysicalMemoryBlock;
                MmPhysicalMemoryBlock->NumberOfPages += NumberOfPages;
                Updated = TRUE;
                break;
            }

            if (StartPage + NumberOfPages <= Page) {

                if (start + 1 < MmPhysicalMemoryBlock->NumberOfRuns) {

                    if (StartPage + NumberOfPages <= MmPhysicalMemoryBlock->Run[start + 1].BasePage) {
                        //
                        // Don't insert here - the new entry really belongs
                        // (at least) one entry further down.
                        //

                        continue;
                    }
                }

                NewRun->BasePage = StartPage;
                NewRun->PageCount = NumberOfPages;
                NewRun += 1;
                Inserted = TRUE;
                Updated = TRUE;
            }
        }

        *NewRun = MmPhysicalMemoryBlock->Run[start];
        NewRun += 1;

        start += 1;

    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    //
    // If the memory block has not been updated, then the new entry must
    // be added at the very end.
    //

    if (Updated == FALSE) {
        ASSERT (Inserted == FALSE);
        NewRun->BasePage = StartPage;
        NewRun->PageCount = NumberOfPages;
        Inserted = TRUE;
    }

    //
    // Repoint the MmPhysicalMemoryBlock at the new chunk, free the old after
    // releasing the PFN lock.
    //

    if (Inserted == TRUE) {
        OldPhysicalMemoryBlock = MmPhysicalMemoryBlock;
        MmPhysicalMemoryBlock = NewPhysicalMemoryBlock;
    }

    //
    // Note that the page directory (page parent entries on Win64) must be
    // filled in at system boot so that already-created processes do not fault
    // when referencing the new PFNs.
    //

    //
    // Walk through the memory descriptors and add pages to the
    // free list in the PFN database.
    //

    PageFrameIndex = StartPage;
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    if (EndPage - 1 > MmHighestPhysicalPage) {
        MmHighestPhysicalPage = EndPage - 1;
    }

    while (PageFrameIndex < EndPage) {

        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u3.e2.ShortFlags == 0);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT64 (Pfn1->UsedPageTableEntries == 0);
        ASSERT (Pfn1->OriginalPte.u.Long == ZeroKernelPte.u.Long);
        ASSERT (Pfn1->u4.PteFrame == 0);
        ASSERT ((Pfn1->PteAddress == PFN_REMOVED) ||
                (Pfn1->PteAddress == (PMMPTE)(UINT_PTR)0));

        //
        // Initialize the color for NUMA purposes.
        //

        MiDetermineNode (PageFrameIndex, Pfn1);

        //
        // Set the PTE address to the physical page for
        // virtual address alignment checking.
        //

        Pfn1->PteAddress = (PMMPTE)(PageFrameIndex << PTE_SHIFT);

        MiInsertPageInFreeList (PageFrameIndex);

        PageFrameIndex += 1;
        
        Pfn1 += 1;
    }

    MmNumberOfPhysicalPages += (PFN_COUNT)NumberOfPages;

    //
    // Only non-compression ranges get to contribute to ResidentAvailable as
    // adding compression ranges to this could crash the system.
    //
    // For the same reason, compression range additions also need to subtract
    // from AvailablePages the amount the above MiInsertPageInFreeList added.
    //

    PagesToReturn = NumberOfPages;

#if defined (_MI_COMPRESSION)
    if (Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
        MmAvailablePages -= (PFN_COUNT) NumberOfPages;

        //
        // Signal applications if allocating these pages caused a threshold cross.
        //

        MiNotifyMemoryEvents ();

        MiNumberOfCompressionPages += NumberOfPages;
        PagesToReturn = 0;
    }
    else {

        //
        // Since real (noncompression-generated) physical memory was added,
        // rearm the interrupt to occur at a higher threshold.
        //

        MiArmCompressionInterrupt ();
    }
#endif

    RtlSetBits (&MiPfnBitMap, (ULONG) StartPage, (ULONG) (EndPage - StartPage));

    UNLOCK_PFN (OldIrql);

    MI_INCREMENT_RESIDENT_AVAILABLE (PagesToReturn,
                                     MM_RESAVAIL_FREE_HOTADD_MEMORY);

    InterlockedExchangeAdd ((PLONG)&SharedUserData->NumberOfPhysicalPages,
                            (LONG) NumberOfPages);

    //
    // Carefully increase all commit limits to reflect the additional memory -
    // notice the current usage must be done first so no one else cuts the
    // line.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommittedPages, PagesNeeded);

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, NumberOfPages);

    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, NumberOfPages);

    MmUnlockPagableImageSection(ExPageLockHandle);

    KeReleaseGuardedMutex (&MmDynamicMemoryMutex);

    ExFreePool (OldPhysicalMemoryBlock);

    //
    // Indicate number of bytes actually added to our caller.
    //

    NumberOfBytes->QuadPart = (ULONGLONG)NumberOfPages * PAGE_SIZE;

    return STATUS_SUCCESS;
}


NTSTATUS
MiRemovePhysicalMemory (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes,
    IN LOGICAL PermanentRemoval,
    IN ULONG Flags
    )

/*++

Routine Description:

    This routine attempts to remove the specified physical address range
    from the system.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being removed.

    PermanentRemoval  - Supplies TRUE if the memory is being permanently
                        (ie: physically) removed.  FALSE if not (ie: just a
                        bad page detected via ECC which is being marked
                        "don't-use".

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    ULONG i;
    ULONG Additional;
    PFN_NUMBER Page;
    PFN_NUMBER LastPage;
    PFN_NUMBER OriginalLastPage;
    PFN_NUMBER start;
    PFN_NUMBER PagesReleased;
    PFN_NUMBER ResAvailPagesReleased;
    PMMPFN Pfn1;
    PMMPFN StartPfn;
    PMMPFN EndPfn;
    KIRQL OldIrql;
    PFN_NUMBER StartPage;
    PFN_NUMBER EndPage;
    PFN_COUNT NumberOfPages;
    PFN_COUNT ParityPages;
    SPFN_NUMBER MaxPages;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER RemovedPages;
    PFN_NUMBER RemovedPagesThisPass;
    LOGICAL Inserted;
    NTSTATUS Status;
    PMMPTE PointerPte;
    PMMPTE EndPte;
    PVOID VirtualAddress;
    PPHYSICAL_MEMORY_DESCRIPTOR OldPhysicalMemoryBlock;
    PPHYSICAL_MEMORY_DESCRIPTOR NewPhysicalMemoryBlock;
    PPHYSICAL_MEMORY_RUN NewRun;
    LOGICAL PfnDatabaseIsPhysical;
    PFN_NUMBER HighestPossiblePhysicalPage;
    PFN_COUNT FluidPages;
    MMPTE_FLUSH_LIST PteFlushList;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ASSERT (BYTE_OFFSET(NumberOfBytes->LowPart) == 0);
    ASSERT (BYTE_OFFSET(StartAddress->LowPart) == 0);

    if (MI_IS_PHYSICAL_ADDRESS(MmPfnDatabase)) {

        if (PermanentRemoval == TRUE) {

            //
            // The system must be configured for dynamic memory addition.  This
            // is not strictly required to remove the memory, but it's better
            // to check for it now under the assumption that the administrator
            // is probably going to want to add this range of memory back in -
            // better to give the error now and refuse the removal than to
            // refuse the addition later.
            //
        
            if (MmDynamicPfn == 0) {
                return STATUS_NOT_SUPPORTED;
            }
        }
    
        PfnDatabaseIsPhysical = TRUE;
    }
    else {
        PfnDatabaseIsPhysical = FALSE;
    }

    if (PermanentRemoval == TRUE) {
        HighestPossiblePhysicalPage = MmHighestPossiblePhysicalPage;
        FluidPages = 100;
    }
    else {
        HighestPossiblePhysicalPage = MmHighestPhysicalPage;
        FluidPages = 0;
    }

    StartPage = (PFN_NUMBER)(StartAddress->QuadPart >> PAGE_SHIFT);
    NumberOfPages = (PFN_COUNT)(NumberOfBytes->QuadPart >> PAGE_SHIFT);

    EndPage = StartPage + NumberOfPages;

    if (StartPage >= EndPage) {
        return STATUS_INVALID_PARAMETER_1;
    }

    if (EndPage - 1 > HighestPossiblePhysicalPage) {

        //
        // Truncate the request into something that can be mapped by the PFN
        // database.
        //

        EndPage = MmHighestPossiblePhysicalPage + 1;
        NumberOfPages = (PFN_COUNT)(EndPage - StartPage);
    }

    if (StartPage >= EndPage) {
        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // The range cannot wrap.
    //

    if (StartPage >= EndPage) {
        return STATUS_INVALID_PARAMETER_1;
    }

#if !defined (_MI_COMPRESSION)
    if (Flags != 0) {
        return STATUS_INVALID_PARAMETER_4;
    }
#endif

    StartPfn = MI_PFN_ELEMENT (StartPage);
    EndPfn = MI_PFN_ELEMENT (EndPage);

    KeAcquireGuardedMutex (&MmDynamicMemoryMutex);

    //
    // Make sure the caller is freeing real memory (ie: PFN-backed).
    //

    if (RtlAreBitsSet (&MiPfnBitMap,
                       (PFN_COUNT) StartPage,
                       (PFN_COUNT) (EndPage - StartPage)) == FALSE) {

        KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
        return STATUS_INVALID_PARAMETER_1;
    }

#if DBG
    MiDynmemData[0] += 1;
#endif

    //
    // Attempt to decrease all commit limits to reflect the removed memory.
    //

    if (MiChargeTemporaryCommitmentForReduction (NumberOfPages + FluidPages) == FALSE) {
#if DBG
        MiDynmemData[1] += 1;
#endif
        KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    //
    // Reduce the systemwide commit limit - note this is carefully done
    // *PRIOR* to returning this commitment so no one else (including a DPC
    // in this very thread) can consume past the limit.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, 0 - (PFN_NUMBER)NumberOfPages);

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, 0 - (PFN_NUMBER)NumberOfPages);

    //
    // Now that the systemwide commit limit has been lowered, the amount
    // we have removed can be safely returned.
    //

    MiReturnCommitment (NumberOfPages + FluidPages);

    MmLockPagableSectionByHandle (ExPageLockHandle);

    //
    // Check for outstanding promises that cannot be broken.
    //

    LOCK_PFN (OldIrql);

    if (PermanentRemoval == FALSE) {

        //
        // If it's just the removal of ECC-marked bad pages, then don't
        // allow the caller to remove any pages that have already been
        // ECC-removed.  This is to prevent recursive erroneous charges.
        //

        for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
            if (Pfn1->u3.e1.ParityError == 1) {
                UNLOCK_PFN (OldIrql);
                Status = STATUS_INVALID_PARAMETER_2;
                goto giveup2;
            }
        }
    }

    MaxPages = MI_NONPAGABLE_MEMORY_AVAILABLE() - FluidPages;

    if ((SPFN_NUMBER)NumberOfPages > MaxPages) {
#if DBG
        MiDynmemData[2] += 1;
#endif
        UNLOCK_PFN (OldIrql);
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto giveup2;
    }

    //
    // The range must be contained in a single entry.  It is
    // permissible for it to be part of a single entry, but it
    // must not cross multiple entries.
    //

    Additional = (ULONG)-2;

    start = 0;
    do {

        Page = MmPhysicalMemoryBlock->Run[start].BasePage;
        LastPage = Page + MmPhysicalMemoryBlock->Run[start].PageCount;

        if ((StartPage >= Page) && (EndPage <= LastPage)) {
            if ((StartPage == Page) && (EndPage == LastPage)) {
                Additional = (ULONG)-1;
            }
            else if ((StartPage == Page) || (EndPage == LastPage)) {
                Additional = 0;
            }
            else {
                Additional = 1;
            }
            break;
        }

        start += 1;

    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    if (Additional == (ULONG)-2) {
#if DBG
        MiDynmemData[3] += 1;
#endif
        UNLOCK_PFN (OldIrql);
        Status = STATUS_CONFLICTING_ADDRESSES;
        goto giveup2;
    }

    for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
        Pfn1->u3.e1.RemovalRequested = 1;
    }

    if (PermanentRemoval == TRUE) {
        MmNumberOfPhysicalPages -= NumberOfPages;

        InterlockedExchangeAdd ((PLONG)&SharedUserData->NumberOfPhysicalPages,
                                0 - NumberOfPages);
    }

#if defined (_MI_COMPRESSION)

    //
    // Only removal of non-compression ranges decrement ResidentAvailable as
    // only those ranges actually incremented this when they were added.
    //

    if ((Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) == 0) {
        MI_DECREMENT_RESIDENT_AVAILABLE (NumberOfPages, MM_RESAVAIL_ALLOCATE_HOTREMOVE_MEMORY);

        //
        // Since real (noncompression-generated) physical memory is being
        // removed, rearm the interrupt to occur at a lower threshold.
        //

        if (PermanentRemoval == TRUE) {
            MiArmCompressionInterrupt ();
        }
    }
#else
    MI_DECREMENT_RESIDENT_AVAILABLE (NumberOfPages, MM_RESAVAIL_ALLOCATE_HOTREMOVE_MEMORY);
#endif

    //
    // The free and zero lists must be pruned now before releasing the PFN
    // lock otherwise if another thread allocates the page from these lists,
    // the allocation will clear the RemovalRequested flag forever.
    //

    RemovedPages = MiRemovePhysicalPages (StartPage, EndPage);

#if defined (_MI_COMPRESSION)

    //
    // Compression range removals add back into AvailablePages the same
    // amount that MiUnlinkPageFromList removes (as the original addition
    // of these ranges never bumps this counter).
    //

    if (Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
        MmAvailablePages += (PFN_COUNT) RemovedPages;

        //
        // Signal applications if allocating these pages caused a threshold cross.
        //

        MiNotifyMemoryEvents ();

        MiNumberOfCompressionPages -= RemovedPages;
    }
#endif

    if (RemovedPages != NumberOfPages) {

#if DBG
retry:
#endif
    
        Pfn1 = StartPfn;
    
        InterlockedIncrement (&MiDelayPageFaults);
    
        for (i = 0; i < 5; i += 1) {
    
            UNLOCK_PFN (OldIrql);
    
            //
            // Attempt to move pages to the standby list.  Note that only the
            // pages with RemovalRequested set are moved.
            //
    
            MiTrimRemovalPagesOnly = TRUE;
    
            MmEmptyAllWorkingSets ();
    
            MiTrimRemovalPagesOnly = FALSE;
    
            MiFlushAllPages ();
    
            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmHalfSecond);
    
            if (i >= 2) {

                //
                // Purge the transition list as transition pages keep
                // page tables from being taken and we need to try harder.
                //

                MiPurgeTransitionList ();
            }

            LOCK_PFN (OldIrql);
    
            RemovedPagesThisPass = MiRemovePhysicalPages (StartPage, EndPage);

            RemovedPages += RemovedPagesThisPass;
    
#if defined (_MI_COMPRESSION)

            //
            // Compression range removals add back into AvailablePages the same
            // amount that MiUnlinkPageFromList removes (as the original
            // addition of these ranges never bumps this counter).
            //

            if (Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
                MmAvailablePages += (PFN_COUNT) RemovedPagesThisPass;

                //
                // Signal applications if allocating these pages
                // caused a threshold cross.
                //

                MiNotifyMemoryEvents ();

                MiNumberOfCompressionPages -= RemovedPagesThisPass;
            }

#endif

            if (RemovedPages == NumberOfPages) {
                break;
            }
    
            //
            // RemovedPages doesn't include pages that were freed directly
            // to the bad page list via MiDecrementReferenceCount or by
            // ECC marking.  So use the above check purely as an optimization -
            // and walk here before ever giving up.
            //

            for ( ; Pfn1 < EndPfn; Pfn1 += 1) {
                if (Pfn1->u3.e1.PageLocation != BadPageList) {
                    break;
                }
            }

            if (Pfn1 == EndPfn) {
                RemovedPages = NumberOfPages;
                break;
            }
        }

        InterlockedDecrement (&MiDelayPageFaults);
    }

    if (RemovedPages != NumberOfPages) {
#if DBG
        MiDynmemData[4] += 1;
        if (MiShowStuckPages != 0) {

            RemovedPages = 0;
            for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
                if (Pfn1->u3.e1.PageLocation != BadPageList) {
                    RemovedPages += 1;
                }
            }

            ASSERT (RemovedPages != 0);

            DbgPrint("MiRemovePhysicalMemory : could not get %d of %d pages\n",
                RemovedPages, NumberOfPages);

            if (MiShowStuckPages & 0x2) {

                ULONG PfnsPrinted;
                ULONG EnoughShown;
                PMMPFN FirstPfn;
                PFN_COUNT PfnCount;

                PfnCount = 0;
                PfnsPrinted = 0;
                EnoughShown = 100;
    
                //
                // Initializing FirstPfn is not needed for correctness
                // but without it the compiler cannot compile this code
                // W4 to check for use of uninitialized variables.
                //

                FirstPfn = NULL;

                if (MiShowStuckPages & 0x4) {
                    EnoughShown = (ULONG)-1;
                }
    
                DbgPrint("Stuck PFN list: ");
                for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
                    if (Pfn1->u3.e1.PageLocation != BadPageList) {
                        if (PfnCount == 0) {
                            FirstPfn = Pfn1;
                        }
                        PfnCount += 1;
                    }
                    else {
                        if (PfnCount != 0) {
                            DbgPrint("%x -> %x ; ", MI_PFN_ELEMENT_TO_INDEX (FirstPfn),
                                                    MI_PFN_ELEMENT_TO_INDEX (FirstPfn + PfnCount - 1));
                            PfnsPrinted += 1;
                            if (PfnsPrinted == EnoughShown) {
                                break;
                            }
                            PfnCount = 0;
                        }
                    }
                }
                if (PfnCount != 0) {
                    DbgPrint("%x -> %x ; ", MI_PFN_ELEMENT_TO_INDEX (FirstPfn),
                                            MI_PFN_ELEMENT_TO_INDEX (FirstPfn + PfnCount - 1));
                }
                DbgPrint("\n");
            }
            if (MiShowStuckPages & 0x8) {
                DbgBreakPoint ();
            }
            if (MiShowStuckPages & 0x10) {
                goto retry;
            }
        }
#endif
        UNLOCK_PFN (OldIrql);
        Status = STATUS_NO_MEMORY;
        goto giveup;
    }

#if DBG
    for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
        ASSERT (Pfn1->u3.e1.PageLocation == BadPageList);
    }
#endif

    //
    // All the pages in the range have been removed.
    //

    if (PermanentRemoval == FALSE) {

        //
        // If it's just the removal of ECC-marked bad pages, then no
        // adjustment to the physical memory block ranges or PFN database
        // trimming is needed.  Exit now.
        //

        for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {
            ASSERT (Pfn1->u3.e1.ParityError == 0);
            Pfn1->u3.e1.ParityError = 1;
        }

        UNLOCK_PFN (OldIrql);

        MmUnlockPagableImageSection(ExPageLockHandle);
    
        KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
    
        NumberOfBytes->QuadPart = (ULONGLONG)NumberOfPages * PAGE_SIZE;
    
        return STATUS_SUCCESS;
    }

    //
    // Update the physical memory blocks and other associated housekeeping.
    //

    if (Additional == 0) {

        //
        // The range can be split off from an end of an existing chunk so no
        // pool growth or shrinkage is required.
        //

        NewPhysicalMemoryBlock = MmPhysicalMemoryBlock;
        OldPhysicalMemoryBlock = NULL;
    }
    else {

        //
        // The range cannot be split off from an end of an existing chunk so
        // pool growth or shrinkage is required.
        //

        UNLOCK_PFN (OldIrql);

        i = (sizeof(PHYSICAL_MEMORY_DESCRIPTOR) +
             (sizeof(PHYSICAL_MEMORY_RUN) * (MmPhysicalMemoryBlock->NumberOfRuns + Additional)));

        NewPhysicalMemoryBlock = ExAllocatePoolWithTag (NonPagedPool,
                                                        i,
                                                        '  mM');

        if (NewPhysicalMemoryBlock == NULL) {
            Status = STATUS_INSUFFICIENT_RESOURCES;
#if DBG
            MiDynmemData[5] += 1;
#endif
            goto giveup;
        }

        OldPhysicalMemoryBlock = MmPhysicalMemoryBlock;
        RtlZeroMemory (NewPhysicalMemoryBlock, i);

        LOCK_PFN (OldIrql);
    }

    //
    // Remove or split the requested range from the existing memory block.
    //

    NewPhysicalMemoryBlock->NumberOfRuns = MmPhysicalMemoryBlock->NumberOfRuns + Additional;
    NewPhysicalMemoryBlock->NumberOfPages = MmPhysicalMemoryBlock->NumberOfPages - NumberOfPages;

    NewRun = &NewPhysicalMemoryBlock->Run[0];
    start = 0;
    Inserted = FALSE;

    do {

        Page = MmPhysicalMemoryBlock->Run[start].BasePage;
        LastPage = Page + MmPhysicalMemoryBlock->Run[start].PageCount;

        if (Inserted == FALSE) {

            if ((StartPage >= Page) && (EndPage <= LastPage)) {

                if ((StartPage == Page) && (EndPage == LastPage)) {
                    ASSERT (Additional == -1);
                    start += 1;
                    continue;
                }
                else if ((StartPage == Page) || (EndPage == LastPage)) {
                    ASSERT (Additional == 0);
                    if (StartPage == Page) {
                        MmPhysicalMemoryBlock->Run[start].BasePage += NumberOfPages;
                    }
                    MmPhysicalMemoryBlock->Run[start].PageCount -= NumberOfPages;
                }
                else {
                    ASSERT (Additional == 1);

                    OriginalLastPage = LastPage;

                    MmPhysicalMemoryBlock->Run[start].PageCount =
                        StartPage - MmPhysicalMemoryBlock->Run[start].BasePage;

                    *NewRun = MmPhysicalMemoryBlock->Run[start];
                    NewRun += 1;

                    NewRun->BasePage = EndPage;
                    NewRun->PageCount = OriginalLastPage - EndPage;
                    NewRun += 1;

                    start += 1;
                    continue;
                }

                Inserted = TRUE;
            }
        }

        *NewRun = MmPhysicalMemoryBlock->Run[start];
        NewRun += 1;
        start += 1;

    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    //
    // Repoint the MmPhysicalMemoryBlock at the new chunk.
    // Free the old block after releasing the PFN lock.
    //

    MmPhysicalMemoryBlock = NewPhysicalMemoryBlock;

    if (EndPage - 1 == MmHighestPhysicalPage) {
        MmHighestPhysicalPage = StartPage - 1;
    }

    //
    // Throw away all the removed pages that are currently enqueued.
    //

    ParityPages = 0;
    for (Pfn1 = StartPfn; Pfn1 < EndPfn; Pfn1 += 1) {

        ASSERT (Pfn1->u3.e1.PageLocation == BadPageList);
        ASSERT (Pfn1->u3.e1.RemovalRequested == 1);

        //
        // Some pages may have already been ECC-removed.  For these pages,
        // the commit limits and resident available pages have already been
        // adjusted - tally them here so we can undo the extraneous charge
        // just applied.
        //
    
        if (Pfn1->u3.e1.ParityError == 1) {
            ParityPages += 1;
        }

        MiUnlinkPageFromList (Pfn1);

        ASSERT (Pfn1->u1.Flink == 0);
        ASSERT (Pfn1->u2.Blink == 0);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
        ASSERT64 (Pfn1->UsedPageTableEntries == 0);

        Pfn1->PteAddress = PFN_REMOVED;

        //
        // Note this clears ParityError among other flags...
        //

        Pfn1->u3.e2.ShortFlags = 0;
        Pfn1->OriginalPte.u.Long = ZeroKernelPte.u.Long;
        Pfn1->u4.PteFrame = 0;
    }

    //
    // Now that the removed pages have been discarded, eliminate the PFN
    // entries that mapped them.  Straddling entries left over from an
    // adjacent earlier removal are not collapsed at this point.
    //
    //

    PagesReleased = 0;
    PteFlushList.Count = 0;

    if (PfnDatabaseIsPhysical == FALSE) {

        VirtualAddress = (PVOID)ROUND_TO_PAGES(MI_PFN_ELEMENT(StartPage));
        PointerPte = MiGetPteAddress (VirtualAddress);
        EndPte = MiGetPteAddress (PAGE_ALIGN(MI_PFN_ELEMENT(EndPage)));

        while (PointerPte < EndPte) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn1->u2.ShareCount == 1);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
            Pfn1->u2.ShareCount = 0;
            MI_SET_PFN_DELETED (Pfn1);
#if DBG
            Pfn1->u3.e1.PageLocation = StandbyPageList;
#endif //DBG
            MiDecrementReferenceCount (Pfn1, PageFrameIndex);
    
            MI_WRITE_INVALID_PTE (PointerPte, ZeroKernelPte);

            if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
                PteFlushList.FlushVa[PteFlushList.Count] = VirtualAddress;
                PteFlushList.Count += 1;
            }

            PagesReleased += 1;
            PointerPte += 1;
            VirtualAddress = (PVOID)((PCHAR)VirtualAddress + PAGE_SIZE);
        }
    }

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList, TRUE);
    }

#if DBG
    MiDynmemData[6] += 1;
#endif

    RtlClearBits (&MiPfnBitMap,
                  (ULONG) MI_PFN_ELEMENT_TO_INDEX (StartPfn),
                  (ULONG) (EndPfn - StartPfn));

    UNLOCK_PFN (OldIrql);

    //
    // Give back anything that has been double-charged.
    //

    ResAvailPagesReleased = PagesReleased;

    if (ParityPages != 0) {
        ResAvailPagesReleased += ParityPages;
    }

    if (ResAvailPagesReleased != 0) {
        MI_INCREMENT_RESIDENT_AVAILABLE (ResAvailPagesReleased,
                                         MM_RESAVAIL_FREE_HOTREMOVE_MEMORY1);
    }

    //
    // Give back anything that has been double-charged.
    //

    if (ParityPages != 0) {
        InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, ParityPages);
        InterlockedExchangeAddSizeT (&MmTotalCommitLimit, ParityPages);
    }

    if (PagesReleased != 0) {
        MiReturnCommitment (PagesReleased);
    }

    MmUnlockPagableImageSection(ExPageLockHandle);

    KeReleaseGuardedMutex (&MmDynamicMemoryMutex);

    if (OldPhysicalMemoryBlock != NULL) {
        ExFreePool (OldPhysicalMemoryBlock);
    }

    NumberOfBytes->QuadPart = (ULONGLONG)NumberOfPages * PAGE_SIZE;

    return STATUS_SUCCESS;

giveup:

    //
    // All the pages in the range were not obtained.  Back everything out.
    //

    PageFrameIndex = StartPage;
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    LOCK_PFN (OldIrql);

    while (PageFrameIndex < EndPage) {

        ASSERT (Pfn1->u3.e1.RemovalRequested == 1);

        Pfn1->u3.e1.RemovalRequested = 0;

        if (Pfn1->u3.e1.PageLocation == BadPageList) {
            MiUnlinkPageFromList (Pfn1);
            MiInsertPageInFreeList (PageFrameIndex);
        }

        Pfn1 += 1;
        PageFrameIndex += 1;
    }

    ResAvailPagesReleased = NumberOfPages;

#if defined (_MI_COMPRESSION)

    //
    // Only removal of non-compression ranges decrement ResidentAvailable as
    // only those ranges actually incremented this when they were added.
    //

    if (Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {

        //
        // Compression range removals add back into AvailablePages the same
        // amount that MiUnlinkPageFromList removes (as the original
        // addition of these ranges never bumps this counter).
        //

        ResAvailPagesReleased = 0;
        MmAvailablePages -= (PFN_COUNT) RemovedPages;

        //
        // Signal applications if allocating these pages caused a threshold cross.
        //

        MiNotifyMemoryEvents ();

        MiNumberOfCompressionPages += RemovedPages;
    }
#endif

    if (PermanentRemoval == TRUE) {
        MmNumberOfPhysicalPages += NumberOfPages;

        InterlockedExchangeAdd ((PLONG)&SharedUserData->NumberOfPhysicalPages,
                                NumberOfPages);

#if defined (_MI_COMPRESSION)

        //
        // Rearm the interrupt to occur at the original threshold.
        //

        if ((Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) == 0) {
            MiArmCompressionInterrupt ();
        }
#endif
    }

    UNLOCK_PFN (OldIrql);

    MI_INCREMENT_RESIDENT_AVAILABLE (ResAvailPagesReleased, MM_RESAVAIL_FREE_HOTREMOVE_FAILED);

giveup2:

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, NumberOfPages);
    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, NumberOfPages);

    MmUnlockPagableImageSection(ExPageLockHandle);
    KeReleaseGuardedMutex (&MmDynamicMemoryMutex);

    return Status;
}


NTSTATUS
MmRemovePhysicalMemory (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes
    )

/*++

Routine Description:

    A wrapper for MmRemovePhysicalMemoryEx.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being removed.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    return MmRemovePhysicalMemoryEx (StartAddress, NumberOfBytes, 0);
}

NTSTATUS
MmRemovePhysicalMemoryEx (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes,
    IN ULONG Flags
    )

/*++

Routine Description:

    This routine attempts to remove the specified physical address range
    from the system.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being removed.

    Flags  - Supplies relevant flags describing the memory range.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    NTSTATUS Status;
#if defined (_X86_) || defined (_AMD64_)
    BOOLEAN CachesFlushed;
#endif
#if defined(_IA64_)
    PVOID VirtualAddress;
    PVOID SingleVirtualAddress;
    SIZE_T SizeInBytes;
    SIZE_T MapSizeInBytes;
    PFN_COUNT NumberOfPages;
    PFN_COUNT i;
    PFN_NUMBER StartPage;
#endif

    PAGED_CODE();

#if defined (_MI_COMPRESSION_SUPPORTED_)
    if (Flags & MM_PHYSICAL_MEMORY_PRODUCED_VIA_COMPRESSION) {
        return STATUS_NOT_SUPPORTED;
    }
#else
    if (Flags != 0) {
        return STATUS_INVALID_PARAMETER_3;
    }
#endif

#if defined (_X86_) || defined (_AMD64_)

    //
    // Issue a cache invalidation here just as a test to make sure the
    // machine can support it.  If not, then don't bother trying to remove
    // any memory.
    //

    CachesFlushed = KeInvalidateAllCaches ();
    if (CachesFlushed == FALSE) {
        return STATUS_NOT_SUPPORTED;
    }
#endif

#if defined(_IA64_)

    //
    // Pick up at least a single PTE mapping now as we do not want to fail this
    // call if no PTEs are available after a successful remove.  Resorting to
    // actually using this PTE should be a very rare case indeed.
    //

    SingleVirtualAddress = (PMMPTE)MiMapSinglePage (NULL,
                                                    0,
                                                    MmCached,
                                                    HighPagePriority);

    if (SingleVirtualAddress == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

#endif

    Status = MiRemovePhysicalMemory (StartAddress, NumberOfBytes, TRUE, Flags);

    if (NT_SUCCESS (Status)) {

#if defined (_X86_) || defined (_AMD64_)
        CachesFlushed = KeInvalidateAllCaches ();
        ASSERT (CachesFlushed == TRUE);
#endif

#if defined(_IA64_)
        SizeInBytes = (SIZE_T)NumberOfBytes->QuadPart;

        //
        // Flush the entire TB to remove any KSEG translations that may map the
        // pages being removed.  Otherwise hardware or software speculation
        // can reference the memory speculatively which would crash the machine.
        //

        KeFlushEntireTb (TRUE, TRUE);

        //
        // Establish an uncached mapping to the pages being removed.
        //

        MapSizeInBytes = SizeInBytes;

        //
        // Initializing VirtualAddress is not needed for correctness
        // but without it the compiler cannot compile this code
        // W4 to check for use of uninitialized variables.
        //

        VirtualAddress = NULL;

        while (MapSizeInBytes > PAGE_SIZE) {

            VirtualAddress = MmMapIoSpace (*StartAddress,
                                           MapSizeInBytes,
                                           MmNonCached);

            if (VirtualAddress != NULL) {
                break;
            }

            MapSizeInBytes = MapSizeInBytes >> 1;
        }

        if (MapSizeInBytes <= PAGE_SIZE) {

            StartPage = (PFN_NUMBER)(StartAddress->QuadPart >> PAGE_SHIFT);

            NumberOfPages = (PFN_COUNT)(NumberOfBytes->QuadPart >> PAGE_SHIFT);

            for (i = 0; i < NumberOfPages; i += 1) {

                SingleVirtualAddress = (PMMPTE)MiMapSinglePage (SingleVirtualAddress,
                                                                StartPage,
                                                                MmCached,
                                                                HighPagePriority);

                KeSweepCacheRangeWithDrain (TRUE,
                                            SingleVirtualAddress,
                                            PAGE_SIZE);

                StartPage += 1;
            }
        }
        else {

            //
            // Drain all pending transactions and prefetches and perform cache
            // evictions.  Only drain 4gb max at a time as this API takes a
            // ULONG.
            //

            while (SizeInBytes > _4gb) {
                KeSweepCacheRangeWithDrain (TRUE, VirtualAddress, _4gb - 1);
                SizeInBytes -= (_4gb - 1);
            }

            KeSweepCacheRangeWithDrain (TRUE,
                                        VirtualAddress,
                                        (ULONG)SizeInBytes);

            MmUnmapIoSpace (VirtualAddress, NumberOfBytes->QuadPart);
        }
#endif
    }

#if defined(_IA64_)
    MiUnmapSinglePage (SingleVirtualAddress);
#endif

    return Status;
}

NTSTATUS
MmMarkPhysicalMemoryAsBad (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes
    )

/*++

Routine Description:

    This routine attempts to mark the specified physical address range
    as bad so the system will not use it.  This is generally done for pages
    which contain ECC errors.

    Note that this is different from removing pages permanently (ie: physically
    removing the memory board) which should be done via the
    MmRemovePhysicalMemory API.

    The caller is responsible for maintaining a global table so that subsequent
    boots can examine it and remove the ECC pages before loading the kernel.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being removed.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    PAGED_CODE();

    return MiRemovePhysicalMemory (StartAddress, NumberOfBytes, FALSE, 0);
}

NTSTATUS
MmMarkPhysicalMemoryAsGood (
    IN PPHYSICAL_ADDRESS StartAddress,
    IN OUT PLARGE_INTEGER NumberOfBytes
    )

/*++

Routine Description:

    This routine attempts to mark the specified physical address range
    as good so the system will use it.  This is generally done for pages
    which used to (but presumably no longer do) contain ECC errors.

    Note that this is different from adding pages permanently (ie: physically
    inserting a new memory board) which should be done via the
    MmAddPhysicalMemory API.

    The caller is responsible for removing these entries from a global table
    so that subsequent boots will use the pages.

Arguments:

    StartAddress  - Supplies the starting physical address.

    NumberOfBytes  - Supplies a pointer to the number of bytes being removed.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER start;
    PFN_NUMBER count;
    PFN_NUMBER StartPage;
    PFN_NUMBER EndPage;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER Page;
    PFN_NUMBER LastPage;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ASSERT (BYTE_OFFSET(NumberOfBytes->LowPart) == 0);
    ASSERT (BYTE_OFFSET(StartAddress->LowPart) == 0);

    StartPage = (PFN_NUMBER)(StartAddress->QuadPart >> PAGE_SHIFT);
    NumberOfPages = (PFN_NUMBER)(NumberOfBytes->QuadPart >> PAGE_SHIFT);

    EndPage = StartPage + NumberOfPages;

    KeAcquireGuardedMutex (&MmDynamicMemoryMutex);

    if (EndPage - 1 > MmHighestPhysicalPage) {

        //
        // Truncate the request into something that can be mapped by the PFN
        // database.
        //

        EndPage = MmHighestPhysicalPage + 1;
        NumberOfPages = EndPage - StartPage;
    }

    //
    // The range cannot wrap.
    //

    if (StartPage >= EndPage) {
        KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
        return STATUS_INVALID_PARAMETER_1;
    }

    //
    // The request must lie within an already present range.
    //

    start = 0;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    do {

        count = MmPhysicalMemoryBlock->Run[start].PageCount;
        Page = MmPhysicalMemoryBlock->Run[start].BasePage;

        if (count != 0) {

            LastPage = Page + count;

            if ((StartPage >= Page) && (EndPage <= LastPage)) {
                break;
            }
        }

        start += 1;

    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    if (start == MmPhysicalMemoryBlock->NumberOfRuns) {
        UNLOCK_PFN (OldIrql);
        MmUnlockPagableImageSection(ExPageLockHandle);
        KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
        return STATUS_CONFLICTING_ADDRESSES;
    }

    //
    // Walk through the range and add only pages previously removed to the
    // free list in the PFN database.
    //

    PageFrameIndex = StartPage;
    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    NumberOfPages = 0;

    while (PageFrameIndex < EndPage) {

        if ((Pfn1->u3.e1.ParityError == 1) &&
            (Pfn1->u3.e1.RemovalRequested == 1) &&
            (Pfn1->u3.e1.PageLocation == BadPageList)) {

            Pfn1->u3.e1.ParityError = 0;
            Pfn1->u3.e1.RemovalRequested = 0;
            MiUnlinkPageFromList (Pfn1);
            MiInsertPageInFreeList (PageFrameIndex);
            NumberOfPages += 1;
        }

        Pfn1 += 1;
        PageFrameIndex += 1;
    }

    UNLOCK_PFN (OldIrql);

    MI_INCREMENT_RESIDENT_AVAILABLE (NumberOfPages, MM_RESAVAIL_FREE_HOTADD_ECC);

    //
    // Increase all commit limits to reflect the additional memory.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, NumberOfPages);

    InterlockedExchangeAddSizeT (&MmTotalCommitLimit, NumberOfPages);

    MmUnlockPagableImageSection(ExPageLockHandle);

    KeReleaseGuardedMutex (&MmDynamicMemoryMutex);

    //
    // Indicate number of bytes actually added to our caller.
    //

    NumberOfBytes->QuadPart = (ULONGLONG)NumberOfPages * PAGE_SIZE;

    return STATUS_SUCCESS;
}

PPHYSICAL_MEMORY_RANGE
MmGetPhysicalMemoryRanges (
    VOID
    )

/*++

Routine Description:

    This routine returns the virtual address of a nonpaged pool block which
    contains the physical memory ranges in the system.

    The returned block contains physical address and page count pairs.
    The last entry contains zero for both.

    The caller must understand that this block can change at any point before
    or after this snapshot.

    It is the caller's responsibility to free this block.

Arguments:

    None.

Return Value:

    NULL on failure.

Environment:

    Kernel mode.  PASSIVE level.  No locks held.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    PPHYSICAL_MEMORY_RANGE p;
    PPHYSICAL_MEMORY_RANGE PhysicalMemoryBlock;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    KeAcquireGuardedMutex (&MmDynamicMemoryMutex);

    i = sizeof(PHYSICAL_MEMORY_RANGE) * (MmPhysicalMemoryBlock->NumberOfRuns + 1);

    PhysicalMemoryBlock = ExAllocatePoolWithTag (NonPagedPool,
                                                 i,
                                                 'hPmM');

    if (PhysicalMemoryBlock == NULL) {
        KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
        return NULL;
    }

    p = PhysicalMemoryBlock;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    ASSERT (i == (sizeof(PHYSICAL_MEMORY_RANGE) * (MmPhysicalMemoryBlock->NumberOfRuns + 1)));

    for (i = 0; i < MmPhysicalMemoryBlock->NumberOfRuns; i += 1) {
        p->BaseAddress.QuadPart = (LONGLONG)MmPhysicalMemoryBlock->Run[i].BasePage * PAGE_SIZE;
        p->NumberOfBytes.QuadPart = (LONGLONG)MmPhysicalMemoryBlock->Run[i].PageCount * PAGE_SIZE;
        p += 1;
    }

    p->BaseAddress.QuadPart = 0;
    p->NumberOfBytes.QuadPart = 0;

    UNLOCK_PFN (OldIrql);

    MmUnlockPagableImageSection(ExPageLockHandle);

    KeReleaseGuardedMutex (&MmDynamicMemoryMutex);

    return PhysicalMemoryBlock;
}

PFN_COUNT
MiRemovePhysicalPages (
    IN PFN_NUMBER StartPage,
    IN PFN_NUMBER EndPage
    )

/*++

Routine Description:

    This routine searches the PFN database for free, zeroed or standby pages
    that are marked for removal.

Arguments:

    StartPage - Supplies the low physical frame number to remove.

    EndPage - Supplies the last physical frame number to remove.

Return Value:

    Returns the number of pages removed from the free, zeroed and standby lists.

Environment:

    Kernel mode, PFN lock held.  Since this routine is PAGELK, the caller is
    responsible for locking it down and unlocking it on return.

--*/

{
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPFN PfnNextColored;
    PMMPFN PfnNextFlink;
    PMMPFN PfnLastColored;
    PFN_NUMBER Page;
    LOGICAL RemovePage;
    ULONG Color;
    PMMCOLOR_TABLES ColorHead;
    PFN_NUMBER MovedPage;
    MMLISTS MemoryList;
    PFN_NUMBER PageNextColored;
    PFN_NUMBER PageNextFlink;
    PFN_NUMBER PageLastColored;
    PFN_COUNT NumberOfPages;
    PMMPFNLIST ListHead;
    LOGICAL RescanNeeded;

    MM_PFN_LOCK_ASSERT();

    NumberOfPages = 0;

rescan:

    //
    // Grab all zeroed (and then free) pages first directly from the
    // colored lists to avoid multiple walks down these singly linked lists.
    // Handle transition pages last.
    //

    for (MemoryList = ZeroedPageList; MemoryList <= FreePageList; MemoryList += 1) {

        ListHead = MmPageLocationList[MemoryList];

        for (Color = 0; Color < MmSecondaryColors; Color += 1) {
            ColorHead = &MmFreePagesByColor[MemoryList][Color];

            MovedPage = (PFN_NUMBER) MM_EMPTY_LIST;

            while (ColorHead->Flink != MM_EMPTY_LIST) {

                Page = ColorHead->Flink;
    
                Pfn1 = MI_PFN_ELEMENT(Page);

                ASSERT ((MMLISTS)Pfn1->u3.e1.PageLocation == MemoryList);

                // 
                // The Flink and Blink must be nonzero here for the page
                // to be on the listhead.  Only code that scans the
                // MmPhysicalMemoryBlock has to check for the zero case.
                //

                ASSERT (Pfn1->u1.Flink != 0);
                ASSERT (Pfn1->u2.Blink != 0);

                //
                // See if the page is desired by the caller.
                //
                // Systems utilizing memory compression may have more
                // pages on the zero, free and standby lists than we
                // want to give out.  Explicitly check MmAvailablePages
                // instead (and recheck whenever the PFN lock is
                // released and reacquired).
                //

                if ((Pfn1->u3.e1.RemovalRequested == 1) &&
                    (MmAvailablePages != 0)) {

                    ASSERT (Pfn1->u3.e1.ReadInProgress == 0);
    
                    MiUnlinkFreeOrZeroedPage (Pfn1);
    
                    MiInsertPageInList (&MmBadPageListHead, Page);

                    NumberOfPages += 1;
                }
                else {

                    //
                    // Unwanted so put the page on the end of list.
                    // If first time, save pfn.
                    //

                    if (MovedPage == MM_EMPTY_LIST) {
                        MovedPage = Page;
                    }
                    else if (Page == MovedPage) {

                        //
                        // No more pages available in this colored chain.
                        //

                        break;
                    }

                    //
                    // If the colored chain has more than one entry then
                    // put this page on the end.
                    //

                    PageNextColored = (PFN_NUMBER)Pfn1->OriginalPte.u.Long;

                    if (PageNextColored == MM_EMPTY_LIST) {

                        //
                        // No more pages available in this colored chain.
                        //

                        break;
                    }

                    ASSERT (Pfn1->u1.Flink != 0);
                    ASSERT (Pfn1->u1.Flink != MM_EMPTY_LIST);
                    ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                    PfnNextColored = MI_PFN_ELEMENT(PageNextColored);
                    ASSERT ((MMLISTS)PfnNextColored->u3.e1.PageLocation == MemoryList);
                    ASSERT (PfnNextColored->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                    //
                    // Adjust the free page list so Page
                    // follows PageNextFlink.
                    //

                    PageNextFlink = Pfn1->u1.Flink;
                    PfnNextFlink = MI_PFN_ELEMENT(PageNextFlink);

                    ASSERT ((MMLISTS)PfnNextFlink->u3.e1.PageLocation == MemoryList);
                    ASSERT (PfnNextFlink->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                    PfnLastColored = ColorHead->Blink;
                    ASSERT (PfnLastColored != (PMMPFN)MM_EMPTY_LIST);
                    ASSERT (PfnLastColored->OriginalPte.u.Long == MM_EMPTY_LIST);
                    ASSERT (PfnLastColored->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                    ASSERT (PfnLastColored->u2.Blink != MM_EMPTY_LIST);

                    ASSERT ((MMLISTS)PfnLastColored->u3.e1.PageLocation == MemoryList);
                    PageLastColored = MI_PFN_ELEMENT_TO_INDEX (PfnLastColored);

                    if (ListHead->Flink == Page) {

                        ASSERT (Pfn1->u2.Blink == MM_EMPTY_LIST);
                        ASSERT (ListHead->Blink != Page);

                        ListHead->Flink = PageNextFlink;

                        PfnNextFlink->u2.Blink = MM_EMPTY_LIST;
                    }
                    else {

                        ASSERT (Pfn1->u2.Blink != MM_EMPTY_LIST);
                        ASSERT ((MMLISTS)(MI_PFN_ELEMENT((MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink)))->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                        ASSERT ((MMLISTS)(MI_PFN_ELEMENT((MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink)))->u3.e1.PageLocation == MemoryList);

                        MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink = PageNextFlink;
                        PfnNextFlink->u2.Blink = Pfn1->u2.Blink;
                    }

#if DBG
                    if (PfnLastColored->u1.Flink == MM_EMPTY_LIST) {
                        ASSERT (ListHead->Blink == PageLastColored);
                    }
#endif

                    Pfn1->u1.Flink = PfnLastColored->u1.Flink;
                    Pfn1->u2.Blink = PageLastColored;

                    if (ListHead->Blink == PageLastColored) {
                        ListHead->Blink = Page;
                    }

                    //
                    // Adjust the colored chains.
                    //

                    if (PfnLastColored->u1.Flink != MM_EMPTY_LIST) {
                        ASSERT (MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                        ASSERT ((MMLISTS)(MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u3.e1.PageLocation) == MemoryList);
                        MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u2.Blink = Page;
                    }

                    ASSERT (PfnLastColored->OriginalPte.u.Long == MM_EMPTY_LIST);
                    PfnLastColored->u1.Flink = Page;

                    ColorHead->Flink = PageNextColored;
                    PfnNextColored->u4.PteFrame = MM_EMPTY_LIST;

                    Pfn1->OriginalPte.u.Long = MM_EMPTY_LIST;
                    Pfn1->u4.PteFrame = PageLastColored;

                    PfnLastColored->OriginalPte.u.Long = Page;
                    ColorHead->Blink = Pfn1;
                }
            }
        }
    }

    RescanNeeded = FALSE;
    Pfn1 = MI_PFN_ELEMENT (StartPage);

    do {

        if ((Pfn1->u3.e1.PageLocation == StandbyPageList) &&
            (Pfn1->u1.Flink != 0) &&
            (Pfn1->u2.Blink != 0) &&
            (Pfn1->u3.e2.ReferenceCount == 0) &&
            (MmAvailablePages != 0)) {

            //
            // Systems utilizing memory compression may have more
            // pages on the zero, free and standby lists than we
            // want to give out.  Explicitly check MmAvailablePages
            // above instead (and recheck whenever the PFN lock is
            // released and reacquired).
            //

            ASSERT (Pfn1->u3.e1.ReadInProgress == 0);

            RemovePage = TRUE;

            if (Pfn1->u3.e1.RemovalRequested == 0) {

                //
                // This page is not directly needed for a hot remove - but if
                // it contains a chunk of prototype PTEs (and this chunk is
                // in a page that needs to be removed), then any pages
                // referenced by transition prototype PTEs must also be removed
                // before the desired page can be removed.
                //
                // The same analogy holds for page table, directory, parent
                // and extended parent pages.
                //

                Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);
                if (Pfn2->u3.e1.RemovalRequested == 0) {
#if (_MI_PAGING_LEVELS >= 3)
                    Pfn2 = MI_PFN_ELEMENT (Pfn2->u4.PteFrame);
                    if (Pfn2->u3.e1.RemovalRequested == 0) {
                        RemovePage = FALSE;
                    }
                    else if (Pfn2->u2.ShareCount == 1) {
                        RescanNeeded = TRUE;
                    }
#if (_MI_PAGING_LEVELS >= 4)
                    Pfn2 = MI_PFN_ELEMENT (Pfn2->u4.PteFrame);
                    if (Pfn2->u3.e1.RemovalRequested == 0) {
                        RemovePage = FALSE;
                    }
                    else if (Pfn2->u2.ShareCount == 1) {
                        RescanNeeded = TRUE;
                    }
#endif
#else
                    RemovePage = FALSE;
#endif
                }
                else if (Pfn2->u2.ShareCount == 1) {
                    RescanNeeded = TRUE;
                }
            }
    
            if (RemovePage == TRUE) {

                //
                // This page is in the desired range - grab it.
                //
    
                MiUnlinkPageFromList (Pfn1);
                MiRestoreTransitionPte (Pfn1);
                MiInsertPageInList (&MmBadPageListHead, StartPage);
                NumberOfPages += 1;
            }
        }

        StartPage += 1;
        Pfn1 += 1;

    } while (StartPage < EndPage);

    if (RescanNeeded == TRUE) {

        //
        // A page table, directory or parent was freed by removing a transition
        // page from the cache.  Rescan from the top to pick it up.
        //

#if DBG
        MiDynmemData[7] += 1;
#endif

        goto rescan;
    }
#if DBG
    else {
        MiDynmemData[8] += 1;
    }
#endif

    return NumberOfPages;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\extsect.c ===
/*++

Copyright (c) 1990  Microsoft Corporation

Module Name:

   extsect.c

Abstract:

    This module contains the routines which implement the
    NtExtendSection service.

Author:

    Lou Perazzoli (loup) 8-May-1990
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtExtendSection)
#pragma alloc_text(PAGE,MmExtendSection)
#endif

extern SIZE_T MmAllocationFragment;

ULONG MiExtendedSubsectionsConvertedToDynamic;

#if DBG
VOID
MiSubsectionConsistent (
    IN PSUBSECTION Subsection
    )
/*++

Routine Description:

    This function checks to ensure the subsection is consistent.

Arguments:

    Subsection - Supplies a pointer to the subsection to be checked.

Return Value:

    None.

--*/

{
    ULONG   Sectors;
    ULONG   FullPtes;

    //
    // Compare the disk sectors (4K units) to the PTE allocation.
    //

    Sectors = Subsection->NumberOfFullSectors;
    if (Subsection->u.SubsectionFlags.SectorEndOffset) {
        Sectors += 1;
    }

    //
    // Calculate how many PTEs are needed to map this number of sectors.
    //

    FullPtes = Sectors >> (PAGE_SHIFT - MM4K_SHIFT);

    if (Sectors & ((1 << (PAGE_SHIFT - MM4K_SHIFT)) - 1)) {
        FullPtes += 1;
    }

    if (FullPtes != Subsection->PtesInSubsection) {
        DbgPrint("Mm: Subsection inconsistent (%x vs %x)\n",
            FullPtes,
            Subsection->PtesInSubsection);
        DbgBreakPoint();
    }
}
#endif


NTSTATUS
NtExtendSection (
    IN HANDLE SectionHandle,
    IN OUT PLARGE_INTEGER NewSectionSize
    )

/*++

Routine Description:

    This function extends the size of the specified section.  If
    the current size of the section is greater than or equal to the
    specified section size, the size is not updated.

Arguments:

    SectionHandle - Supplies an open handle to a section object.

    NewSectionSize - Supplies the new size for the section object.

Return Value:

    NTSTATUS.

--*/

{
    KPROCESSOR_MODE PreviousMode;
    PVOID Section;
    NTSTATUS Status;
    LARGE_INTEGER CapturedNewSectionSize;

    PAGED_CODE();

    //
    // Check to make sure the new section size is accessible.
    //

    PreviousMode = KeGetPreviousMode();

    if (PreviousMode != KernelMode) {

        try {

            ProbeForWriteSmallStructure (NewSectionSize,
                                         sizeof(LARGE_INTEGER),
                                         PROBE_ALIGNMENT (LARGE_INTEGER));

            CapturedNewSectionSize = *NewSectionSize;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode ();
        }

    }
    else {

        CapturedNewSectionSize = *NewSectionSize;
    }

    //
    // Reference the section object.
    //

    Status = ObReferenceObjectByHandle (SectionHandle,
                                        SECTION_EXTEND_SIZE,
                                        MmSectionObjectType,
                                        PreviousMode,
                                        (PVOID *)&Section,
                                        NULL);

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    Status = MmExtendSection (Section, &CapturedNewSectionSize, FALSE);

    ObDereferenceObject (Section);

    //
    // Update the NewSectionSize field.
    //

    try {

        //
        // Return the captured section size.
        //

        *NewSectionSize = CapturedNewSectionSize;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        NOTHING;
    }

    return Status;
}


LOGICAL
MiAppendSubsectionChain (
    IN PMSUBSECTION LastSubsection,
    IN PMSUBSECTION ExtendedSubsectionHead
    )

/*++

Routine Description:

    This nonpagable wrapper function extends the specified subsection chain.

Arguments:

    LastSubsection - Supplies the last subsection in the existing control area.

    ExtendedSubsectionHead - Supplies an anchor pointing to the first
                             subsection in the chain to append.

Return Value:

    TRUE if the chain was appended, FALSE if not.

--*/

{
    KIRQL OldIrql;
    PMSUBSECTION NewSubsection;

    ASSERT (ExtendedSubsectionHead->NextSubsection != NULL);

    ASSERT (ExtendedSubsectionHead->u.SubsectionFlags.SectorEndOffset == 0);

    NewSubsection = (PMSUBSECTION) ExtendedSubsectionHead->NextSubsection;

    LOCK_PFN (OldIrql);

    //
    // This subsection may be extending a range that is already
    // mapped by a VAD(s).  There is no way to tell how many VADs
    // already map it so if any do, just leave all the new subsections
    // marked as not reclaimable until the control area is deleted.
    //
    // If however, there are no *USER* references to this control area,
    // then the subsections can be marked as dynamic now.  Note other
    // portions of code (currently only prefetch) that issue "dereference
    // from this subsection to the end of file" are safe because these
    // portions create user sections first and so the first check below
    // will be FALSE.
    //

    if (LastSubsection->ControlArea->NumberOfUserReferences != 0) {

        //
        // The caller has not allocated prototype PTEs and they are required.
        // Return so the caller can allocate and retry.
        //

        if (NewSubsection->SubsectionBase == NULL) {
            ASSERT (NewSubsection->u.SubsectionFlags.SubsectionStatic == 0);
            UNLOCK_PFN (OldIrql);
            return FALSE;
        }
#if DBG
        do {
            ASSERT (NewSubsection->u.SubsectionFlags.SubsectionStatic == 1);
            NewSubsection = (PMSUBSECTION) NewSubsection->NextSubsection;
        } while (NewSubsection != NULL);
#endif
    }
    else if (NewSubsection->SubsectionBase != NULL) {

        //
        // The prototype PTEs are no longer required (user views went away)
        // even though they were when the caller first built the subsections.
        // Mark the subsections as dynamic now.
        //

        do {
            ASSERT (NewSubsection->u.SubsectionFlags.SubsectionStatic == 1);

            MI_SNAP_SUB (NewSubsection, 0x1);

            NewSubsection->u.SubsectionFlags.SubsectionStatic = 0;
            NewSubsection->u2.SubsectionFlags2.SubsectionConverted = 1;
            NewSubsection->NumberOfMappedViews = 1;

            MiRemoveViewsFromSection (NewSubsection, 
                                      NewSubsection->PtesInSubsection);

            MiExtendedSubsectionsConvertedToDynamic += 1;

            MI_SNAP_SUB (NewSubsection, 0x2);
            NewSubsection = (PMSUBSECTION) NewSubsection->NextSubsection;
        } while (NewSubsection != NULL);
    }

    LastSubsection->u.SubsectionFlags.SectorEndOffset = 0;

    LastSubsection->NumberOfFullSectors = ExtendedSubsectionHead->NumberOfFullSectors;

    //
    // A memory barrier is needed to ensure the writes initializing the
    // subsection fields are visible prior to linking the subsection into
    // the chain.  This is because some reads from these fields are done
    // lock free for improved performance.
    //

    KeMemoryBarrier ();

    LastSubsection->NextSubsection = ExtendedSubsectionHead->NextSubsection;

    UNLOCK_PFN (OldIrql);

    return TRUE;
}


NTSTATUS
MmExtendSection (
    IN PVOID SectionToExtend,
    IN OUT PLARGE_INTEGER NewSectionSize,
    IN ULONG IgnoreFileSizeChecking
    )

/*++

Routine Description:

    This function extends the size of the specified section.  If
    the current size of the section is greater than or equal to the
    specified section size, the size is not updated.

Arguments:

    Section - Supplies a pointer to a referenced section object.

    NewSectionSize - Supplies the new size for the section object.

    IgnoreFileSizeChecking -  Supplies the value TRUE is file size
                              checking should be ignored (i.e., it
                              is being called from a file system which
                              has already done the checks).  FALSE
                              if the checks still need to be made.

Return Value:

    NTSTATUS.

--*/

{
    LOGICAL Appended;
    PMMPTE ProtoPtes;
    MMPTE TempPte;
    PCONTROL_AREA ControlArea;
    PSEGMENT Segment;
    PSECTION Section;
    PSUBSECTION LastSubsection;
    PSUBSECTION Subsection;
    PMSUBSECTION ExtendedSubsection;
    MSUBSECTION ExtendedSubsectionHead;
    PMSUBSECTION LastExtendedSubsection;
    UINT64 RequiredPtes;
    UINT64 NumberOfPtes;
    UINT64 TotalNumberOfPtes;
    ULONG PtesUsed;
    ULONG UnusedPtes;
    UINT64 AllocationSize;
    UINT64 RunningSize;
    UINT64 EndOfFile;
    NTSTATUS Status;
    LARGE_INTEGER NumberOf4KsForEntireFile;
    LARGE_INTEGER Starting4K;
    LARGE_INTEGER NextSubsection4KStart;
    LARGE_INTEGER Last4KChunk;
    ULONG PartialSize;
    SIZE_T AllocationFragment;
    PKTHREAD CurrentThread;

    PAGED_CODE();

    Section = (PSECTION)SectionToExtend;

    //
    // Make sure the section is really extendable - physical and
    // image sections are not.
    //

    Segment = Section->Segment;
    ControlArea = Segment->ControlArea;

    if ((ControlArea->u.Flags.PhysicalMemory || ControlArea->u.Flags.Image) ||
         (ControlArea->FilePointer == NULL)) {
        return STATUS_SECTION_NOT_EXTENDED;
    }

    //
    // Acquire the section extension mutex, this blocks other threads from
    // updating the size at the same time.
    //

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceExclusiveLite (&MmSectionExtendResource, TRUE);

    //
    // Calculate the number of prototype PTE chunks to build for this section.
    // A subsection is also allocated for each chunk as all the prototype PTEs
    // in any given chunk are initially encoded to point at the same subsection.
    //
    // The maximum total section size is 16PB (2^54).  This is because the
    // StartingSector4132 field in each subsection, ie: 2^42-1 bits of file
    // offset where the offset is in 4K (not pagesize) units.  Thus, a
    // subsection may describe a *BYTE* file start offset of maximum
    // 2^54 - 4K.
    //
    // Each subsection can span at most 16TB - 64K.  This is because the
    // NumberOfFullSectors and various other fields in the subsection are
    // ULONGs.  In reality, this is a nonissue as far as maximum section size
    // is concerned because any number of subsections can be chained together
    // and in fact, subsections are allocated to span less to facilitate
    // efficient dynamic prototype PTE trimming and reconstruction.
    //

    if (NewSectionSize->QuadPart > MI_MAXIMUM_SECTION_SIZE) {
        Status = STATUS_SECTION_TOO_BIG;
        goto ReleaseAndReturn;
    }

    NumberOfPtes = (NewSectionSize->QuadPart + PAGE_SIZE - 1) >> PAGE_SHIFT;

    if (ControlArea->u.Flags.WasPurged == 0) {

        if ((UINT64)NewSectionSize->QuadPart <= (UINT64)Section->SizeOfSection.QuadPart) {
            *NewSectionSize = Section->SizeOfSection;
            goto ReleaseAndReturnSuccess;
        }
    }

    //
    // If a file handle was specified, set the allocation size of the file.
    //

    if (IgnoreFileSizeChecking == FALSE) {

        //
        // Release the resource so we don't deadlock with the file
        // system trying to extend this section at the same time.
        //

        ExReleaseResourceLite (&MmSectionExtendResource);

        //
        // Get a different resource to single thread query/set operations.
        //

        ExAcquireResourceExclusiveLite (&MmSectionExtendSetResource, TRUE);

        //
        // Query the file size to see if this file really needs extending.
        //
        // If the specified size is less than the current size, return
        // the current size.
        //

        Status = FsRtlGetFileSize (ControlArea->FilePointer,
                                   (PLARGE_INTEGER)&EndOfFile);

        if (!NT_SUCCESS (Status)) {
            ExReleaseResourceLite (&MmSectionExtendSetResource);
            KeLeaveCriticalRegionThread (CurrentThread);
            return Status;
        }

        if ((UINT64)NewSectionSize->QuadPart > EndOfFile) {

            //
            // Don't allow section extension unless the section was originally
            // created with write access.  The check couldn't be done at create
            // time without breaking existing binaries, so the caller gets the
            // error at this point instead.
            //

            if (((Section->InitialPageProtection & PAGE_READWRITE) |
                (Section->InitialPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {
#if DBG
                    DbgPrint("Section extension failed %x\n", Section);
#endif
                    ExReleaseResourceLite (&MmSectionExtendSetResource);
                    KeLeaveCriticalRegionThread (CurrentThread);
                    return STATUS_SECTION_NOT_EXTENDED;
            }

            //
            // Current file is smaller, attempt to set a new end of file.
            //

            EndOfFile = *(PUINT64)NewSectionSize;

            Status = FsRtlSetFileSize (ControlArea->FilePointer,
                                       (PLARGE_INTEGER)&EndOfFile);

            if (!NT_SUCCESS (Status)) {
                ExReleaseResourceLite (&MmSectionExtendSetResource);
                KeLeaveCriticalRegionThread (CurrentThread);
                return Status;
            }
        }

        if (Segment->ExtendInfo) {
            KeAcquireGuardedMutex (&MmSectionBasedMutex);
            if (Segment->ExtendInfo) {
                Segment->ExtendInfo->CommittedSize = EndOfFile;
            }
            KeReleaseGuardedMutex (&MmSectionBasedMutex);
        }

        //
        // Release the query/set resource and reacquire the extend section
        // resource.
        //

        ExReleaseResourceLite (&MmSectionExtendSetResource);
        ExAcquireResourceExclusiveLite (&MmSectionExtendResource, TRUE);
    }

    //
    // Find the last subsection.
    //

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    if (((PMAPPED_FILE_SEGMENT)Segment)->LastSubsectionHint != NULL) {
        LastSubsection = (PSUBSECTION) ((PMAPPED_FILE_SEGMENT)Segment)->LastSubsectionHint;
    }
    else {
        if (ControlArea->u.Flags.Rom == 1) {
            LastSubsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
        }
        else {
            LastSubsection = (PSUBSECTION)(ControlArea + 1);
        }
    }

    while (LastSubsection->NextSubsection != NULL) {
        ASSERT (LastSubsection->UnusedPtes == 0);
        LastSubsection = LastSubsection->NextSubsection;
    }

    MI_CHECK_SUBSECTION (LastSubsection);

    //
    // Does the structure need extending?
    //

    TotalNumberOfPtes = (((UINT64)Segment->SegmentFlags.TotalNumberOfPtes4132) << 32) | Segment->TotalNumberOfPtes;

    if (NumberOfPtes <= TotalNumberOfPtes) {

        //
        // The segment is already large enough, just update
        // the section size and return.
        //

        Section->SizeOfSection = *NewSectionSize;
        if (Segment->SizeOfSegment < (UINT64)NewSectionSize->QuadPart) {

            //
            // Only update if it is really bigger.
            //

            Segment->SizeOfSegment = *(PUINT64)NewSectionSize;

            Mi4KStartFromSubsection(&Starting4K, LastSubsection);

            Last4KChunk.QuadPart = (NewSectionSize->QuadPart >> MM4K_SHIFT) - Starting4K.QuadPart;

            ASSERT (Last4KChunk.HighPart == 0);

            LastSubsection->NumberOfFullSectors = Last4KChunk.LowPart;
            LastSubsection->u.SubsectionFlags.SectorEndOffset =
                                        NewSectionSize->LowPart & MM4K_MASK;
            MI_CHECK_SUBSECTION (LastSubsection);
        }
        goto ReleaseAndReturnSuccess;
    }

    //
    // Add new structures to the section - locate the last subsection
    // and add there.
    //

    RequiredPtes = NumberOfPtes - TotalNumberOfPtes;
    PtesUsed = 0;

    if (RequiredPtes < LastSubsection->UnusedPtes) {

        //
        // There are ample PTEs to extend the section already allocated.
        //

        PtesUsed = (ULONG) RequiredPtes;
        RequiredPtes = 0;

    }
    else {
        PtesUsed = LastSubsection->UnusedPtes;
        RequiredPtes -= PtesUsed;
    }

    LastSubsection->PtesInSubsection += PtesUsed;
    LastSubsection->UnusedPtes -= PtesUsed;
    Segment->SizeOfSegment += (UINT64)PtesUsed * PAGE_SIZE;

    TotalNumberOfPtes += PtesUsed;

    Segment->TotalNumberOfPtes = (ULONG) TotalNumberOfPtes;
    if (TotalNumberOfPtes >= 0x100000000) {
        Segment->SegmentFlags.TotalNumberOfPtes4132 = (ULONG_PTR)(TotalNumberOfPtes >> 32);
    }

    if (RequiredPtes == 0) {

        //
        // There is no extension necessary, update the high VBN.
        //

        Mi4KStartFromSubsection(&Starting4K, LastSubsection);

        Last4KChunk.QuadPart = (NewSectionSize->QuadPart >> MM4K_SHIFT) - Starting4K.QuadPart;

        ASSERT (Last4KChunk.HighPart == 0);

        LastSubsection->NumberOfFullSectors = Last4KChunk.LowPart;

        LastSubsection->u.SubsectionFlags.SectorEndOffset =
                                    NewSectionSize->LowPart & MM4K_MASK;
        MI_CHECK_SUBSECTION (LastSubsection);
    }
    else {

        //
        // An extension is required.
        //
        // Allocate the subsection(s) now.
        //
        // If there are any user views, then also allocate the prototype
        // PTEs now (because a user view may be for an extended VAD which
        // would already be encompassing this new extension).  If there
        // are no user views, then don't allocate the prototype PTEs until
        // the views are actually mapped (system views never pre-extend past
        // the end of the file).
        //

        NumberOf4KsForEntireFile.QuadPart = Segment->SizeOfSegment >> MM4K_SHIFT;
        AllocationSize = MI_ROUND_TO_SIZE (RequiredPtes * sizeof(MMPTE), PAGE_SIZE);

        AllocationFragment = MmAllocationFragment;

        RunningSize = 0;

        ExtendedSubsectionHead = *(PMSUBSECTION)LastSubsection;

        LastExtendedSubsection = &ExtendedSubsectionHead;

        ASSERT (LastExtendedSubsection->NextSubsection == NULL);

        SATISFY_OVERZEALOUS_COMPILER (NextSubsection4KStart.QuadPart = 0);

        do {

            //
            // Bound the size of each prototype PTE allocation so both :
            //  1. it can succeed even in cases where the pool is fragmented.
            //  2. later on last control area dereference, each subsection
            //     is converted to dynamic and can be pruned/recreated
            //     individually without losing (or requiring) contiguous pool.
            //

            if (AllocationSize - RunningSize > AllocationFragment) {
                PartialSize = (ULONG) AllocationFragment;
            }
            else {
                PartialSize = (ULONG) (AllocationSize - RunningSize);
            }

            //
            // Allocate an extended subsection.
            //

            ExtendedSubsection = (PMSUBSECTION) ExAllocatePoolWithTag (NonPagedPool,
                                                            sizeof(MSUBSECTION),
                                                            'dSmM');
            if (ExtendedSubsection == NULL) {
                goto ExtensionFailed;
            }

            ExtendedSubsection->SubsectionBase = NULL;
            ExtendedSubsection->NextSubsection = NULL;

            LastExtendedSubsection->NextSubsection = (PSUBSECTION) ExtendedSubsection;

            ASSERT (ControlArea->FilePointer != NULL);

            ExtendedSubsection->u.LongFlags = 0;

            ExtendedSubsection->ControlArea = ControlArea;

            ExtendedSubsection->PtesInSubsection = PartialSize / sizeof(MMPTE);
            ExtendedSubsection->UnusedPtes = 0;

            RunningSize += PartialSize;

            if (RunningSize > (RequiredPtes * sizeof(MMPTE))) {
                UnusedPtes = (ULONG)(RunningSize / sizeof(MMPTE) - RequiredPtes);
                ExtendedSubsection->PtesInSubsection -= UnusedPtes;
                ExtendedSubsection->UnusedPtes = UnusedPtes;
            }

            ExtendedSubsection->u.SubsectionFlags.Protection =
                    (unsigned) Segment->SegmentPteTemplate.u.Soft.Protection;

            ExtendedSubsection->DereferenceList.Flink = NULL;
            ExtendedSubsection->DereferenceList.Blink = NULL;
            ExtendedSubsection->NumberOfMappedViews = 0;
            ExtendedSubsection->u2.LongFlags2 = 0;


            //
            // Adjust the previous subsection to account for the new length.
            // Note that since the next allocation in this loop may fail,
            // the very first previous subsection changes are not rippled
            // to the chained subsection until the loop completes successfully.
            //

            if (LastExtendedSubsection == &ExtendedSubsectionHead) {

                Mi4KStartFromSubsection (&Starting4K, LastExtendedSubsection);

                Last4KChunk.QuadPart = NumberOf4KsForEntireFile.QuadPart -
                                            Starting4K.QuadPart;

                if (LastExtendedSubsection->u.SubsectionFlags.SectorEndOffset) {
                    Last4KChunk.QuadPart += 1;
                }

                ASSERT (Last4KChunk.HighPart == 0);

                LastExtendedSubsection->NumberOfFullSectors = Last4KChunk.LowPart;
                LastExtendedSubsection->u.SubsectionFlags.SectorEndOffset = 0;

                //
                // If the number of sectors doesn't completely fill the PTEs
                // (only possible when the page size is not MM4K), then
                // fill it now.
                //

                if (LastExtendedSubsection->NumberOfFullSectors & ((1 << (PAGE_SHIFT - MM4K_SHIFT)) - 1)) {
                    LastExtendedSubsection->NumberOfFullSectors += 1;
                }

                MI_CHECK_SUBSECTION (LastExtendedSubsection);

                Starting4K.QuadPart += LastExtendedSubsection->NumberOfFullSectors;
                NextSubsection4KStart.QuadPart = Starting4K.QuadPart;
            }
            else {
                NextSubsection4KStart.QuadPart += LastExtendedSubsection->NumberOfFullSectors;
            }

            //
            // Initialize the newly allocated subsection.
            //

            Mi4KStartForSubsection (&NextSubsection4KStart, ExtendedSubsection);

            if (RunningSize < AllocationSize) {

                //
                // Not the final subsection so all quantities are full pages.
                //

                ExtendedSubsection->NumberOfFullSectors =
                        (PartialSize / sizeof (MMPTE)) << (PAGE_SHIFT - MM4K_SHIFT);
                ExtendedSubsection->u.SubsectionFlags.SectorEndOffset = 0;
            }
            else {

                //
                // The final subsection so quantities are not always full pages.
                //

                Last4KChunk.QuadPart =
                    (NewSectionSize->QuadPart >> MM4K_SHIFT) - NextSubsection4KStart.QuadPart;

                ASSERT (Last4KChunk.HighPart == 0);

                ExtendedSubsection->NumberOfFullSectors = Last4KChunk.LowPart;

                ExtendedSubsection->u.SubsectionFlags.SectorEndOffset =
                                    NewSectionSize->LowPart & MM4K_MASK;
            }

            MI_CHECK_SUBSECTION (ExtendedSubsection);

            //
            // This subsection may be extending a range that is already
            // mapped by a VAD(s).  There is no way to tell how many VADs
            // already map it so just mark the entire subsection as not
            // reclaimable until the control area is deleted.
            //
            // This also saves other portions of code that issue "dereference
            // from this subsection to the end of file" as these subsections are
            // marked as static not dynamic (at least until segment dereference
            // time).
            //
            // When this chain is appended to the control area at the end of
            // this routine an attempt is made to convert the subsection chain
            // to dynamic if no user mapped views are active.
            //

            LastExtendedSubsection = ExtendedSubsection;

        } while (RunningSize < AllocationSize);

        if (ControlArea->NumberOfUserReferences == 0) {
            ASSERT (IgnoreFileSizeChecking == TRUE);
        }

        //
        // All the subsections have been allocated, try to append the
        // subsection chain without allocating prototype PTEs.  If user
        // views are present, the append will fail, at which point we will
        // attempt to allocate prototype PTEs and retry.
        //

        Appended = MiAppendSubsectionChain ((PMSUBSECTION)LastSubsection,
                                            &ExtendedSubsectionHead);

        if (Appended == FALSE) {

            RunningSize = 0;
    
            Subsection = (PSUBSECTION) &ExtendedSubsectionHead;
    
            do {
    
                if (AllocationSize - RunningSize > AllocationFragment) {
                    PartialSize = (ULONG) AllocationFragment;
                }
                else {
                    PartialSize = (ULONG) (AllocationSize - RunningSize);
                }
    
                RunningSize += PartialSize;
    
                ProtoPtes = (PMMPTE)ExAllocatePoolWithTag (PagedPool | POOL_MM_ALLOCATION,
                                                           PartialSize,
                                                           MMSECT);
    
                if (ProtoPtes == NULL) {
                    goto ExtensionFailed;
                }
    
                Subsection = Subsection->NextSubsection;
    
                Subsection->SubsectionBase = ProtoPtes;
                Subsection->u.SubsectionFlags.SubsectionStatic = 1;
    
                //
                // Fill in the prototype PTEs for this subsection.
                //
                // Set all the PTEs to the initial execute-read-write protection.
                // The section will control access to these and the segment
                // must provide a method to allow other users to map the file
                // for various protections.
                //
    
                TempPte.u.Long = MiGetSubsectionAddressForPte (Subsection);
                TempPte.u.Soft.Prototype = 1;
    
                TempPte.u.Soft.Protection = Segment->SegmentPteTemplate.u.Soft.Protection;
    
                MiFillMemoryPte (ProtoPtes, PartialSize / sizeof (MMPTE), TempPte.u.Long);
    
            } while (RunningSize < AllocationSize);
    
            ASSERT (ControlArea->DereferenceList.Flink == NULL);
    
            //
            // Link the newly created subsection chain into the existing list.
            // Note that any adjustments (NumberOfFullSectors, etc) made to
            // the temp copy of the last subsection in the existing control
            // area must be *CAREFULLY* copied to the real copy in the chain (the
            // entire structure cannot just be block copied) as other fields
            // in the real copy (ie: NumberOfMappedViews may be changed in
            // parallel by another thread).
            //
    
            Appended = MiAppendSubsectionChain ((PMSUBSECTION)LastSubsection,
                                                &ExtendedSubsectionHead);

            ASSERT (Appended == TRUE);
        }

        TotalNumberOfPtes += RequiredPtes;

        Segment->TotalNumberOfPtes = (ULONG) TotalNumberOfPtes;
        if (TotalNumberOfPtes >= 0x100000000) {
            Segment->SegmentFlags.TotalNumberOfPtes4132 = (ULONG_PTR)(TotalNumberOfPtes >> 32);
        }

        if (LastExtendedSubsection != &ExtendedSubsectionHead) {
            ((PMAPPED_FILE_SEGMENT)Segment)->LastSubsectionHint =
                    LastExtendedSubsection;
        }
    }

    Segment->SizeOfSegment = *(PUINT64)NewSectionSize;
    Section->SizeOfSection = *NewSectionSize;

ReleaseAndReturnSuccess:

    Status = STATUS_SUCCESS;

ReleaseAndReturn:

    ExReleaseResourceLite (&MmSectionExtendResource);
    KeLeaveCriticalRegionThread (CurrentThread);

    return Status;

ExtensionFailed:

    //
    // Required pool to extend the section could not be allocated.
    // Reset the subsection and control area fields to their
    // original values.
    //

    LastSubsection->PtesInSubsection -= PtesUsed;
    LastSubsection->UnusedPtes += PtesUsed;

    TotalNumberOfPtes -= PtesUsed;
    Segment->SegmentFlags.TotalNumberOfPtes4132 = 0;

    Segment->TotalNumberOfPtes = (ULONG) TotalNumberOfPtes;
    if (TotalNumberOfPtes >= 0x100000000) {
        Segment->SegmentFlags.TotalNumberOfPtes4132 = (ULONG_PTR)(TotalNumberOfPtes >> 32);
    }

    Segment->SizeOfSegment -= ((UINT64)PtesUsed * PAGE_SIZE);

    //
    // Free all the previous allocations and return an error.
    //

    LastSubsection = ExtendedSubsectionHead.NextSubsection;

    while (LastSubsection != NULL) {
        Subsection = LastSubsection->NextSubsection;
        if (LastSubsection->SubsectionBase != NULL) {
            ExFreePool (LastSubsection->SubsectionBase);
        }
        ExFreePool (LastSubsection);
        LastSubsection = Subsection;
    }

    Status = STATUS_INSUFFICIENT_RESOURCES;
    goto ReleaseAndReturn;
}

PMMPTE
FASTCALL
MiGetProtoPteAddressExtended (
    IN PMMVAD Vad,
    IN ULONG_PTR Vpn
    )

/*++

Routine Description:

    This function calculates the address of the prototype PTE
    for the corresponding virtual address.

Arguments:

    Vad - Supplies a pointer to the virtual address descriptor which
          encompasses the virtual address.

    Vpn - Supplies the virtual page number to locate a prototype PTE for.

Return Value:

    The corresponding prototype PTE address.

--*/

{
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    ULONG PteOffset;

    ControlArea = Vad->ControlArea;

    if (ControlArea->u.Flags.GlobalOnlyPerSession == 0) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    //
    // Locate the subsection which contains the First Prototype PTE
    // for this VAD.
    //

    while ((Subsection->SubsectionBase == NULL) ||
           (Vad->FirstPrototypePte < Subsection->SubsectionBase) ||
           (Vad->FirstPrototypePte >=
               &Subsection->SubsectionBase[Subsection->PtesInSubsection])) {

        //
        // Get the next subsection.
        //

        Subsection = Subsection->NextSubsection;
        if (Subsection == NULL) {
            return NULL;
        }
    }

    ASSERT (Subsection->SubsectionBase != NULL);

    //
    // How many PTEs beyond this subsection must we go?
    //

    PteOffset = (ULONG) (((Vpn - Vad->StartingVpn) +
                 (ULONG)(Vad->FirstPrototypePte - Subsection->SubsectionBase)) -
                 Subsection->PtesInSubsection);

    ASSERT (PteOffset < 0xF0000000);

    PteOffset += Subsection->PtesInSubsection;

    //
    // Locate the subsection which contains the prototype PTEs.
    //

    while (PteOffset >= Subsection->PtesInSubsection) {
        PteOffset -= Subsection->PtesInSubsection;
        Subsection = Subsection->NextSubsection;
        if (Subsection == NULL) {
            return NULL;
        }
    }

    //
    // The PTEs are in this subsection.
    //

    ASSERT (Subsection->SubsectionBase != NULL);

    ASSERT (PteOffset < Subsection->PtesInSubsection);

    return &Subsection->SubsectionBase[PteOffset];

}

PSUBSECTION
FASTCALL
MiLocateSubsection (
    IN PMMVAD Vad,
    IN ULONG_PTR Vpn
    )

/*++

Routine Description:

    This function calculates the address of the subsection
    for the corresponding virtual address.

    This function only works for mapped files NOT mapped images.

Arguments:

    Vad - Supplies a pointer to the virtual address descriptor which
          encompasses the virtual address.

    Vpn - Supplies the virtual page number to locate a prototype PTE for.

Return Value:

    The corresponding prototype subsection.

--*/

{
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    ULONG PteOffset;

    ControlArea = Vad->ControlArea;

    if (ControlArea->u.Flags.Rom == 0) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    if (ControlArea->u.Flags.Image) {

        if (ControlArea->u.Flags.GlobalOnlyPerSession == 1) {
            Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
        }

        //
        // There is only one subsection, don't look any further.
        //

        return Subsection;
    }

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    //
    // Locate the subsection which contains the First Prototype PTE
    // for this VAD.  Note all the SubsectionBase values must be non-NULL
    // for the subsection range spanned by the VAD because the VAD still
    // exists.  Carefully skip over preceding subsections not mapped by
    // this VAD because if no other VADs map them either, their base
    // can be NULL.
    //

    while ((Subsection->SubsectionBase == NULL) ||
           (Vad->FirstPrototypePte < Subsection->SubsectionBase) ||
           (Vad->FirstPrototypePte >=
               &Subsection->SubsectionBase[Subsection->PtesInSubsection])) {

        //
        // Get the next subsection.
        //

        Subsection = Subsection->NextSubsection;
        if (Subsection == NULL) {
            return NULL;
        }
    }

    ASSERT (Subsection->SubsectionBase != NULL);

    //
    // How many PTEs beyond this subsection must we go?
    //

    PteOffset = (ULONG)((Vpn - Vad->StartingVpn) +
         (ULONG)(Vad->FirstPrototypePte - Subsection->SubsectionBase));

    ASSERT (PteOffset < 0xF0000000);

    //
    // Locate the subsection which contains the prototype PTEs.
    //

    while (PteOffset >= Subsection->PtesInSubsection) {
        PteOffset -= Subsection->PtesInSubsection;
        Subsection = Subsection->NextSubsection;
        if (Subsection == NULL) {
            return NULL;
        }
        ASSERT (Subsection->SubsectionBase != NULL);
    }

    //
    // The PTEs are in this subsection.
    //

    ASSERT (Subsection->SubsectionBase != NULL);

    return Subsection;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\debugsup.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   debugsup.c

Abstract:

    This module contains routines which provide support for the
    kernel debugger.

Author:

    Lou Perazzoli (loup) 02-Aug-1990
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#include <kdp.h>

ULONG MmPoisonedTb;
LONG MiInDebugger;


PVOID
MiDbgWriteCheck (
    IN PVOID VirtualAddress,
    IN PHARDWARE_PTE Opaque,
    IN LOGICAL ForceWritableIfPossible
    )

/*++

Routine Description:

    This routine checks the specified virtual address and if it is
    valid and writable, it returns that virtual address, otherwise
    it returns NULL.

Arguments:

    VirtualAddress - Supplies the virtual address to check.

    Opaque - Supplies an opaque pointer.

Return Value:

    Returns NULL if the address is not valid or writable, otherwise
    returns the virtual address.

Environment:

    Kernel mode IRQL at DISPATCH_LEVEL or greater.

--*/

{
    MMPTE PteContents;
    PMMPTE InputPte;
    PMMPTE PointerPte;
    ULONG_PTR IsPhysical;

    InputPte = (PMMPTE)Opaque;

    InputPte->u.Long = 0;

    if (!MmIsAddressValid (VirtualAddress)) {
        return NULL;
    }

#if defined(_IA64_)

    //
    // There are regions mapped by TRs (PALcode, PCR, etc) that are
    // not part of the MI_IS_PHYSICAL_ADDRESS macro.
    //

    IsPhysical = MiIsVirtualAddressMappedByTr (VirtualAddress);
    if (IsPhysical == FALSE) {
        IsPhysical = MI_IS_PHYSICAL_ADDRESS (VirtualAddress);
    }
#else
    IsPhysical = MI_IS_PHYSICAL_ADDRESS (VirtualAddress);
#endif

    if (IsPhysical) {

        //
        // All superpage mappings must be read-write and never generate
        // faults so nothing needs to be done for this case.
        //

        return VirtualAddress;
    }

    PointerPte = MiGetPteAddress (VirtualAddress);

    PteContents = *PointerPte;
    
#if defined(_IA64_)

    //
    // IA64 does not set the dirty bit in the processor microcode so
    // check for it here.  Note the access bit was already set by the
    // caller if it wasn't initially on.
    //

    if ((PteContents.u.Hard.Write == 0) || (PteContents.u.Hard.Dirty == 0))

#elif defined(NT_UP)
    if (PteContents.u.Hard.Write == 0)
#else
    if (PteContents.u.Hard.Writable == 0)
#endif
    {
        if (ForceWritableIfPossible == FALSE) {
            return NULL;
        }

        //
        // PTE is not writable, make it so.
        //

        *InputPte = PteContents;
    
        //
        // Carefully modify the PTE to ensure write permissions,
        // preserving the page's cache attributes to keep the TB
        // coherent.
        //
    
#if defined(NT_UP) || defined(_IA64_)
        PteContents.u.Hard.Write = 1;
#else
        PteContents.u.Hard.Writable = 1;
#endif
        MI_SET_PTE_DIRTY (PteContents);
        MI_SET_ACCESSED_IN_PTE (&PteContents, 1);
    
        MI_DEBUGGER_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, PteContents);
    
        //
        // Note KeFillEntryTb does not IPI the other processors. This is
        // required as the other processors are frozen in the debugger
        // and we will deadlock if we try and IPI them.
        // Just flush the current processor instead.
        //

        KeFillEntryTb (VirtualAddress);
    }

    return VirtualAddress;
}

VOID
MiDbgReleaseAddress (
    IN PVOID VirtualAddress,
    IN PHARDWARE_PTE Opaque
    )

/*++

Routine Description:

    This routine resets the specified virtual address access permissions
    to its original state.

Arguments:

    VirtualAddress - Supplies the virtual address to check.

    Opaque - Supplies an opaque pointer.

Return Value:

    None.

Environment:

    Kernel mode IRQL at DISPATCH_LEVEL or greater.

--*/

{
    MMPTE TempPte;
    PMMPTE PointerPte;
    PMMPTE InputPte;

    InputPte = (PMMPTE)Opaque;

    ASSERT (MmIsAddressValid (VirtualAddress));

    if (InputPte->u.Long != 0) {

        PointerPte = MiGetPteAddress (VirtualAddress);

        TempPte = *InputPte;
        TempPte.u.Hard.Dirty = 1;
    
        MI_DEBUGGER_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, TempPte);
    
        KeFillEntryTb (VirtualAddress);
    }

    return;
}

PVOID64
MiDbgTranslatePhysicalAddress (
    IN PHYSICAL_ADDRESS PhysicalAddress,
    IN ULONG Flags
    )

/*++

Routine Description:

    This routine maps the specified physical address and returns
    the virtual address which maps the physical address.

    The next call to MiDbgTranslatePhysicalAddress removes the
    previous physical address translation, hence only a single
    physical address can be examined at a time (can't cross page
    boundaries).

Arguments:

    PhysicalAddress - Supplies the physical address to map and translate.

    Flags -

        MMDBG_COPY_WRITE - Ignored.

        MMDBG_COPY_PHYSICAL - Ignored.

        MMDBG_COPY_UNSAFE - Ignored.

        MMDBG_COPY_CACHED - Use a PTE with the cached attribute for the
                            mapping to ensure TB coherence.

        MMDBG_COPY_UNCACHED - Use a PTE with the uncached attribute for the
                              mapping to ensure TB coherence.

        MMDBG_COPY_WRITE_COMBINED - Use a PTE with the writecombined attribute
                                    for the mapping to ensure TB coherence.

        Note the cached/uncached/write combined attribute requested by the
        caller is ignored if Mm can internally determine the proper attribute.

Return Value:

    The virtual address which corresponds to the physical address.

Environment:

    Kernel mode IRQL at DISPATCH_LEVEL or greater.

--*/

{
    MMPTE TempPte;
    PVOID BaseAddress;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    MMPTE OriginalPte;
    PMMIO_TRACKER Tracker;
    PLIST_ENTRY NextEntry;
    LOGICAL AttributeConfirmed;

    //
    // The debugger can call this before Mm has even initialized in Phase 0 !
    // MmDebugPte cannot be referenced before Mm has initialized without
    // causing an infinite loop wedging the machine.
    //

    if (MmPhysicalMemoryBlock == NULL) {
        return NULL;
    }

    BaseAddress = MiGetVirtualAddressMappedByPte (MmDebugPte);

    TempPte = ValidKernelPte;

    PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);

    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    if (MI_IS_PFN (PageFrameIndex)) {

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        switch (Pfn1->u3.e1.CacheAttribute) {

            case MiCached:
            case MiNotMapped:
            default:
                break;
            case MiNonCached:
                MI_DISABLE_CACHING (TempPte);
                break;
            case MiWriteCombined:
                MI_SET_PTE_WRITE_COMBINE (TempPte);
                break;
        }
    }
    else {

        AttributeConfirmed = FALSE;

        NextEntry = MmIoHeader.Flink;

        while (NextEntry != &MmIoHeader) {

            Tracker = (PMMIO_TRACKER) CONTAINING_RECORD (NextEntry,
                                                         MMIO_TRACKER,
                                                         ListEntry.Flink);

            if ((PageFrameIndex >= Tracker->PageFrameIndex) &&
                (PageFrameIndex < Tracker->PageFrameIndex + Tracker->NumberOfPages)) {
                Flags &= ~(MMDBG_COPY_CACHED | MMDBG_COPY_UNCACHED | MMDBG_COPY_WRITE_COMBINED);
                switch (Tracker->CacheAttribute) {
                    case MiNonCached :
                        Flags |= MMDBG_COPY_UNCACHED;
                        break;
                    case MiWriteCombined :
                        Flags |= MMDBG_COPY_WRITE_COMBINED;
                        break;
                    case MiCached :
                    default:
                        Flags |= MMDBG_COPY_CACHED;
                        break;
                }
                AttributeConfirmed = TRUE;
                break;
            }

            NextEntry = Tracker->ListEntry.Flink;
        }

        if (Flags & MMDBG_COPY_CACHED) {
            NOTHING;
        }
        else if (Flags & MMDBG_COPY_UNCACHED) {

            //
            // Just flush the entire TB on this processor but not the others
            // as an IPI may not be safe depending on when/why we broke into
            // the debugger.
            //
            // If IPIs were safe, we would have used
            // MI_PREPARE_FOR_NONCACHED (MiNonCached) instead.
            //

            KeFlushCurrentTb ();

            MI_DISABLE_CACHING (TempPte);
        }
        else if (Flags & MMDBG_COPY_WRITE_COMBINED) {

            //
            // Just flush the entire TB on this processor but not the others
            // as an IPI may not be safe depending on when/why we broke into
            // the debugger.
            //
            // If IPIs were safe, we would have used
            // MI_PREPARE_FOR_NONCACHED (MiWriteCombined) instead.
            //

            KeFlushCurrentTb ();

            MI_SET_PTE_WRITE_COMBINE (TempPte);
        }
        else {

            //
            // This is an access to I/O space and we don't know the correct
            // attribute type.  Only proceed if the caller explicitly specified
            // an attribute and hope he didn't get it wrong.  If no attribute
            // is specified then just return failure.
            //

            return NULL;
        }

        //
        // Since we really don't know if the caller got the attribute right,
        // set the flag below so (assuming the machine doesn't hard hang) we
        // can at least tell in the crash that he may have whacked the TB.
        //

        if (AttributeConfirmed == FALSE) {
            MmPoisonedTb += 1;
        }
    }

    MI_SET_ACCESSED_IN_PTE (&TempPte, 1);

    OriginalPte.u.Long = 0;

    OriginalPte.u.Long = InterlockedCompareExchangePte (MmDebugPte,
                                                        TempPte.u.Long,
                                                        OriginalPte.u.Long);
                                                         
    if (OriginalPte.u.Long != 0) {

        //
        // Someone else is using the debug PTE.  Inform our caller it is not
        // available.
        //

        return NULL;
    }

    //
    // Just flush (no sweep) the TB entry on this processor as an IPI
    // may not be safe depending on when/why we broke into the debugger.
    // Note that if we are in kd, then all the processors are frozen and
    // this thread can't migrate so the local TB flush is enough.  For
    // the localkd case, our caller has raised to DISPATCH_LEVEL thereby
    // ensuring this thread can't migrate even though the other processors
    // are not frozen.
    //

    KiFlushSingleTb (BaseAddress);

    return (PVOID64)((ULONG_PTR)BaseAddress + BYTE_OFFSET(PhysicalAddress.LowPart));
}

VOID
MiDbgUnTranslatePhysicalAddress (
    VOID
    )

/*++

Routine Description:

    This routine unmaps the virtual address currently mapped by the debug PTE.

    This is needed so that stale PTE mappings are not left in the debug PTE
    as if the page attribute subsequently changes, a stale mapping would
    cause TB incoherency.

    This can only be called if the previous MiDbgTranslatePhysicalAddress
    succeeded.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode IRQL at DISPATCH_LEVEL or greater.

--*/

{
    PVOID BaseAddress;

    BaseAddress = MiGetVirtualAddressMappedByPte (MmDebugPte);

    ASSERT (MmIsAddressValid (BaseAddress));

    InterlockedExchangePte (MmDebugPte, ZeroPte.u.Long);

    KiFlushSingleTb (BaseAddress);

    return;
}
 
NTSTATUS
MmDbgCopyMemory (
    IN ULONG64 UntrustedAddress,
    IN PVOID Buffer,
    IN ULONG Size,
    IN ULONG Flags
    )

/*++

Routine Description:

    Transfers a single chunk of memory between a buffer and a system
    address.  The transfer can be a read or write with a virtual or
    physical address.

    The chunk size must be 1, 2, 4 or 8 bytes and the address
    must be appropriately aligned for the size.

Arguments:

    UntrustedAddress - Supplies the system address being read from or written
                       into.  The address is translated appropriately and
                       validated before being used.  This address must not
                       cross a page boundary.

    Buffer - Supplies the buffer to read into or write from.  It is the caller's
             responsibility to ensure this buffer address is nonpaged and valid
             (ie: will not generate any faults including access bit faults)
             throughout the duration of this call.  This routine (not the
             caller) will handle copying into this buffer as the buffer
             address may not be aligned properly for the requested transfer.

             Typically this buffer points to a kd circular buffer or an
             ExLockUserBuffer'd address.  Note this buffer can cross page
             boundaries.

    Size - Supplies the size of the transfer.  This may be 1, 2, 4 or 8 bytes.

    Flags -

        MMDBG_COPY_WRITE - Write from the buffer to the address.
                           If this is not set a read is done.

        MMDBG_COPY_PHYSICAL - The address is a physical address and by default
                              a PTE with a cached attribute will be used to
                              map it to retrieve (or set) the specified data.
                              If this is not set the address is virtual.

        MMDBG_COPY_UNSAFE - No locks are taken during operation.  It
                            is the caller's responsibility to ensure
                            stability of the system during the call.

        MMDBG_COPY_CACHED - If MMDBG_COPY_PHYSICAL is specified, then use
                            a PTE with the cached attribute for the mapping
                            to ensure TB coherence.

        MMDBG_COPY_UNCACHED - If MMDBG_COPY_PHYSICAL is specified, then use
                              a PTE with the uncached attribute for the mapping
                              to ensure TB coherence.

        MMDBG_COPY_WRITE_COMBINED - If MMDBG_COPY_PHYSICAL is specified, then
                                    use a PTE with the writecombined attribute
                                    for the mapping to ensure TB coherence.

Return Value:

    NTSTATUS.

--*/

{
    LOGICAL ForceWritableIfPossible;
    PMMSUPPORT Ws;
    ULONG i;
    KIRQL OldIrql;
    KIRQL PfnIrql;
    PVOID VirtualAddress;
    HARDWARE_PTE Opaque;
    CHAR TempBuffer[8];
    PCHAR SourceBuffer;
    PCHAR TargetBuffer;
    PHYSICAL_ADDRESS PhysicalAddress;
    PETHREAD Thread;
    LOGICAL PfnHeld;
    LOGICAL IoHeld;

    switch (Size) {
        case 1:
            break;
        case 2:
            break;
        case 4:
            break;
        case 8:
            break;
        default:
            return STATUS_INVALID_PARAMETER_3;
    }

    if (UntrustedAddress & (Size - 1)) {

        //
        // The untrusted address is not properly aligned with the requested
        // transfer size.  This is a caller error.
        //

        return STATUS_INVALID_PARAMETER_3;
    }

    if (((ULONG)UntrustedAddress & ~(Size - 1)) !=
        (((ULONG)UntrustedAddress + Size - 1) & ~(Size - 1))) {

        //
        // The range spanned by the untrusted address crosses a page boundary.
        // Straddling pages is not allowed.  This is a caller error.
        //

        return STATUS_INVALID_PARAMETER_3;
    }

    PfnHeld = FALSE;
    IoHeld = FALSE;
    Ws = NULL;

    //
    // Initializing PfnIrql and OldIrql are not needed for
    // correctness but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    PfnIrql = PASSIVE_LEVEL;
    OldIrql = PASSIVE_LEVEL;
    ForceWritableIfPossible = TRUE;

    if ((Flags & MMDBG_COPY_PHYSICAL) == 0) {

        //
        // If the caller has not frozen the machine (ie: this is localkd or the
        // equivalent), then acquire the PFN lock.  This keeps the address
        // valid even after the return from the MmIsAddressValid call.  Note
        // that for system (or session) addresses, the relevant working set
        // mutex is acquired to prevent the page from getting trimmed or the
        // PTE access bit from getting cleared.  For user space addresses,
        // no mutex is needed because the access is performed using the user
        // virtual address inside an exception handler.
        //

        if ((Flags & MMDBG_COPY_UNSAFE) == 0) {

            if (KeGetCurrentIrql () > APC_LEVEL) {
                return STATUS_INVALID_PARAMETER_4;
            }

            //
            // Note that for safe copy mode (ie: the system is live), the
            // address must not be made writable if it is not already because
            // other threads might concurrently access it this way and losing
            // copy-on-write semantics, etc would be very bad.
            //

            ForceWritableIfPossible = FALSE;

            if ((PVOID) (ULONG_PTR) UntrustedAddress >= MmSystemRangeStart) {

                Thread = PsGetCurrentThread ();

                if (MmIsSessionAddress ((PVOID)(ULONG_PTR)UntrustedAddress)) {

                    PEPROCESS Process;

                    Process = PsGetCurrentProcessByThread (Thread);

                    if ((Process->Vm.Flags.SessionLeader == 1) ||
                        (Process->Session == NULL)) {

                        //
                        // smss may transiently have a session space but
                        // that's of no interest to our caller.  The system
                        // and idle process never have a session at all.
                        //

                        return STATUS_INVALID_PARAMETER_1;
                    }

                    Ws = &MmSessionSpace->GlobalVirtualAddress->Vm;
                }
                else {
                    Ws = &MmSystemCacheWs;
                }

                if (KeGetOwnerGuardedMutex (&Ws->WorkingSetMutex) == KeGetCurrentThread ()) {
                    return STATUS_INVALID_PARAMETER_4;
                }

                PfnHeld = TRUE;

                LOCK_WORKING_SET (Ws);

                LOCK_PFN (PfnIrql);
            }
            else {

                //
                // The caller specified a user address.
                //

                if (MI_WS_OWNER (PsGetCurrentProcess ())) {
                    return STATUS_INVALID_PARAMETER_4;
                }

                //
                // Probe and access the address carefully inside an
                // exception handler.
                //

                try {
                    if (Flags & MMDBG_COPY_WRITE) {
                        ProbeForWrite ((PVOID)(ULONG_PTR)UntrustedAddress, Size, Size);
                    }
                    else {
                        ProbeForRead ((PVOID)(ULONG_PTR)UntrustedAddress, Size, Size);
                    }
                } except(EXCEPTION_EXECUTE_HANDLER) {
                    return GetExceptionCode ();
                }

                VirtualAddress = (PVOID) (ULONG_PTR) UntrustedAddress;

                if (Flags & MMDBG_COPY_WRITE) {
                    goto WriteData;
                }
                else {
                    goto ReadData;
                }
            }
        }

        if (MmIsAddressValid ((PVOID) (ULONG_PTR) UntrustedAddress) == FALSE) {

            if (PfnHeld == TRUE) {
                UNLOCK_PFN (PfnIrql);
            }
            if (Ws != NULL) {
                UNLOCK_WORKING_SET (Ws);
            }

            return STATUS_INVALID_PARAMETER_1;
        }

#if defined(_IA64_) && defined (_MIALT4K_)

        //
        // Split PTEs (for emulated processes) may generate a fault,
        // depending on their access bits, the ALTPTEs, etc.
        //
        // They share the same encoding as large pages so make sure it's
        // really a PTE before checking the bit.
        //
        // Also make sure that it even has a PTE (large pages don't) !
        //

        if (((PVOID) (ULONG_PTR) UntrustedAddress < MmSystemRangeStart) &&
            (Flags & MMDBG_COPY_UNSAFE) &&
            (PsGetCurrentProcess()->Wow64Process != NULL) &&
            (!MI_PDE_MAPS_LARGE_PAGE (MiGetPdeAddress ((PVOID)UntrustedAddress))) &&
            (MI_PDE_MAPS_LARGE_PAGE (MiGetPteAddress ((PVOID)UntrustedAddress))) &&
            (MiGetPteAddress ((PVOID)UntrustedAddress)->u.Hard.Cache == MM_PTE_CACHE_RESERVED) &&
            ((KeGetCurrentIrql () > APC_LEVEL) ||
             (KeGetCurrentThread () == KeGetOwnerGuardedMutex (&PsGetCurrentProcess()->Wow64Process->AlternateTableLock)) ||
             (MI_WS_OWNER (PsGetCurrentProcess ())))) {

            if (PfnHeld == TRUE) {
                UNLOCK_PFN (PfnIrql);
            }
            if (Ws != NULL) {
                UNLOCK_WORKING_SET (Ws);
            }

            return STATUS_INVALID_PARAMETER_1;
        }

#endif

        VirtualAddress = (PVOID) (ULONG_PTR) UntrustedAddress;
    }
    else {

        PhysicalAddress.QuadPart = UntrustedAddress;

        //
        // If the caller has not frozen the machine (ie: this is localkd or the
        // equivalent), then acquire the PFN lock.  This prevents
        // MmPhysicalMemoryBlock from changing inside the debug PTE routines
        // and also blocks APCs so malicious callers cannot suspend us
        // while we hold the debug PTE.
        //

        if ((Flags & MMDBG_COPY_UNSAFE) == 0) {

            if (KeGetCurrentIrql () > APC_LEVEL) {
                return STATUS_INVALID_PARAMETER_4;
            }

            IoHeld = TRUE;
            PfnHeld = TRUE;

            ExAcquireSpinLock (&MmIoTrackerLock, &OldIrql);

            LOCK_PFN (PfnIrql);
        }

        VirtualAddress = (PVOID) (ULONG_PTR) MiDbgTranslatePhysicalAddress (PhysicalAddress, Flags);

        if (VirtualAddress == NULL) {
            if (PfnHeld == TRUE) {
                UNLOCK_PFN (PfnIrql);
            }
            if (IoHeld == TRUE) {
                ExReleaseSpinLock (&MmIoTrackerLock, OldIrql);
            }
            return STATUS_UNSUCCESSFUL;
        }
    }

    if (Flags & MMDBG_COPY_WRITE) {
        VirtualAddress = MiDbgWriteCheck (VirtualAddress, &Opaque, ForceWritableIfPossible);

        if (VirtualAddress == NULL) {
            if (PfnHeld == TRUE) {
                UNLOCK_PFN (PfnIrql);
            }
            if (IoHeld == TRUE) {
                ExReleaseSpinLock (&MmIoTrackerLock, OldIrql);
            }
            if (Ws != NULL) {
                UNLOCK_WORKING_SET (Ws);
            }
            return STATUS_INVALID_PARAMETER_1;
        }

WriteData:

        //
        // Carefully capture the source buffer into a local *aligned* buffer
        // as the write to the target must be done using the desired operation
        // size specified by the caller.  This is because the target may be
        // a memory mapped device which requires specific transfer sizes.
        //

        SourceBuffer = (PCHAR) Buffer;

        try {
            for (i = 0; i < Size; i += 1) {
                TempBuffer[i] = *SourceBuffer;
                SourceBuffer += 1;
            }
        } except(EXCEPTION_EXECUTE_HANDLER) {
            ASSERT (Ws == NULL);
            ASSERT (PfnHeld == FALSE);
            ASSERT (IoHeld == FALSE);
            return GetExceptionCode();
        }

        switch (Size) {
            case 1:
                *(PCHAR) VirtualAddress = *(PCHAR) TempBuffer;
                break;
            case 2:
                *(PSHORT) VirtualAddress = *(PSHORT) TempBuffer;
                break;
            case 4:
                *(PULONG) VirtualAddress = *(PULONG) TempBuffer;
                break;
            case 8:
                *(PULONGLONG) VirtualAddress = *(PULONGLONG) TempBuffer;
                break;
            default:
                break;
        }

        if ((PVOID) (ULONG_PTR) UntrustedAddress >= MmSystemRangeStart) {
            MiDbgReleaseAddress (VirtualAddress, &Opaque);
        }
    }
    else {

ReadData:

        try {
            switch (Size) {
                case 1:
                    *(PCHAR) TempBuffer = *(PCHAR) VirtualAddress;
                    break;
                case 2:
                    *(PSHORT) TempBuffer = *(PSHORT) VirtualAddress;
                    break;
                case 4:
                    *(PULONG) TempBuffer = *(PULONG) VirtualAddress;
                    break;
                case 8:
                    *(PULONGLONG) TempBuffer = *(PULONGLONG) VirtualAddress;
                    break;
                default:
                    break;
            }
        } except(EXCEPTION_EXECUTE_HANDLER) {
            ASSERT (Ws == NULL);
            ASSERT (PfnHeld == FALSE);
            ASSERT (IoHeld == FALSE);
            return GetExceptionCode();
        }

        //
        // The buffer to fill may not be aligned so do it one character at
        // a time.
        //

        TargetBuffer = (PCHAR) Buffer;

        for (i = 0; i < Size; i += 1) {
            *TargetBuffer = TempBuffer[i];
            TargetBuffer += 1;
        }
    }

    if (Flags & MMDBG_COPY_PHYSICAL) {
        MiDbgUnTranslatePhysicalAddress ();
    }

    if (PfnHeld == TRUE) {
        UNLOCK_PFN (PfnIrql);
    }

    if (IoHeld == TRUE) {
        ExReleaseSpinLock (&MmIoTrackerLock, OldIrql);
    }

    if (Ws != NULL) {
        UNLOCK_WORKING_SET (Ws);
    }

    return STATUS_SUCCESS;
}

LOGICAL
MmDbgIsLowMemOk (
    IN PFN_NUMBER PageFrameIndex,
    OUT PPFN_NUMBER NextPageFrameIndex,
    IN OUT PULONG CorruptionOffset
    )

/*++

Routine Description:

    This is a special function called only from the kernel debugger
    to check that the physical memory below 4Gb removed with /NOLOWMEM
    contains the expected fill patterns.  If not, there is a high
    probability that a driver which cannot handle physical addresses greater
    than 32 bits corrupted the memory.

Arguments:

    PageFrameIndex - Supplies the physical page number to check.

    NextPageFrameIndex - Supplies the next physical page number the caller
                         should check or 0 if the search is complete.

    CorruptionOffset - If corruption is found, the byte offset
                       of the corruption start is returned here.

Return Value:

    TRUE if the page was removed and the fill pattern is correct, or
    if the page was never removed.  FALSE if corruption was detected
    in the page.

Environment:

    This routine is for use of the kernel debugger ONLY, specifically
    the !chklowmem command.

    The debugger's PTE will be repointed.

--*/

{
#if defined (_MI_MORE_THAN_4GB_)

    PULONG Va;
    ULONG Index;
    PHYSICAL_ADDRESS Pa;
#if DBG
    PMMPFN Pfn;
#endif

    if (MiNoLowMemory == 0) {
        *NextPageFrameIndex = 0;
        return TRUE;
    }

    if (MiLowMemoryBitMap == NULL) {
        *NextPageFrameIndex = 0;
        return TRUE;
    }

    if (PageFrameIndex >= MiNoLowMemory - 1) {
        *NextPageFrameIndex = 0;
    }
    else {
        *NextPageFrameIndex = PageFrameIndex + 1;
    }

    //
    // Verify that the page to be verified is one of the reclaimed
    // pages.
    //

    if ((PageFrameIndex >= MiLowMemoryBitMap->SizeOfBitMap) ||
        (RtlCheckBit (MiLowMemoryBitMap, PageFrameIndex) == 0)) {

        return TRUE;
    }

    //
    // At this point we have a low page that is not in active use.
    // The fill pattern must match.
    //

#if DBG
    Pfn = MI_PFN_ELEMENT (PageFrameIndex);
    ASSERT (Pfn->u4.PteFrame == MI_MAGIC_4GB_RECLAIM);
    ASSERT (Pfn->u3.e1.PageLocation == ActiveAndValid);
#endif

    //
    // Map the physical page using the debug PTE so the
    // fill pattern can be validated.
    //
    // The debugger cannot be using this virtual address on entry or exit.
    //

    Pa.QuadPart = ((ULONGLONG)PageFrameIndex) << PAGE_SHIFT;

    Va = (PULONG) MiDbgTranslatePhysicalAddress (Pa, 0);

    if (Va == NULL) {
        return TRUE;
    }

    for (Index = 0; Index < PAGE_SIZE / sizeof(ULONG); Index += 1) {

        if (*Va != (PageFrameIndex | MI_LOWMEM_MAGIC_BIT)) {

            if (CorruptionOffset != NULL) {
                *CorruptionOffset = Index * sizeof(ULONG);
            }

            MiDbgUnTranslatePhysicalAddress ();
            return FALSE;
        }

        Va += 1;
    }
    MiDbgUnTranslatePhysicalAddress ();
#else
    UNREFERENCED_PARAMETER (PageFrameIndex);
    UNREFERENCED_PARAMETER (CorruptionOffset);

    *NextPageFrameIndex = 0;
#endif

    return TRUE;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\flushbuf.c ===
/*++

Copyright (c) 1991  Microsoft Corporation

Module Name:

   flushbuf.c

Abstract:

    This module contains the code to flush the write buffer or otherwise
    synchronize writes on the host processor.  Also, contains code
    to flush the instruction cache of specified process.

Author:

    David N. Cutler 24-Apr-1991

Revision History:

--*/

#include "mi.h"

ULONG
MiFlushRangeFilter (
    IN PEXCEPTION_POINTERS ExceptionPointers,
    IN PVOID *BaseAddress,
    IN PSIZE_T Length,
    IN PLOGICAL Retry
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtFlushWriteBuffer)
#pragma alloc_text(PAGE,NtFlushInstructionCache)
#pragma alloc_text(PAGE,MiFlushRangeFilter)
#endif


NTSTATUS
NtFlushWriteBuffer (
   VOID
   )

/*++

Routine Description:

    This function flushes the write buffer on the current processor.

Arguments:

    None.

Return Value:

    STATUS_SUCCESS.

--*/

{
    PAGED_CODE();

    KeFlushWriteBuffer();
    return STATUS_SUCCESS;
}

ULONG
MiFlushRangeFilter (
    IN PEXCEPTION_POINTERS ExceptionPointers,
    IN PVOID *BaseAddress,
    IN PSIZE_T Length,
    IN PLOGICAL Retry
    )

/*++

Routine Description:

    This is the exception handler used by NtFlushInstructionCache to protect
    against bad virtual addresses passed to KeSweepIcacheRange.  If an
    access violation occurs, this routine causes NtFlushInstructionCache to
    restart the sweep at the page following the failing page.

Arguments:

    ExceptionPointers - Supplies exception information.

    BaseAddress - Supplies a pointer to address the base of the region being
                  flushed.  If the failing address is not in the last page of
                  the region, this routine updates BaseAddress to point to the
                  next page of the region.

    Length - Supplies a pointer the length of the region being flushed.
             If the failing address is not in the last page of the region,
             this routine updates Length to reflect restarting the flush at
             the next page of the region.

    Retry - Supplies a pointer to a LOGICAL that the caller has initialized
            to FALSE.  This routine sets this LOGICAL to TRUE if an access
            violation occurs in a page before the last page of the flush region.

Return Value:

    EXCEPTION_EXECUTE_HANDLER.

--*/

{
    PEXCEPTION_RECORD ExceptionRecord;
    ULONG_PTR BadVa;
    ULONG_PTR NextVa;
    ULONG_PTR EndVa;

    ExceptionRecord = ExceptionPointers->ExceptionRecord;

    //
    // If the exception was an access violation, skip the current page of the
    // region and move to the next page.
    //

    if ( ExceptionRecord->ExceptionCode == STATUS_ACCESS_VIOLATION ) {

        //
        // Get the failing address, calculate the base address of the next page,
        // and calculate the address at the end of the region.
        //

        BadVa = ExceptionRecord->ExceptionInformation[1];
        NextVa = ROUND_TO_PAGES( BadVa + 1 );
        EndVa = *(PULONG_PTR)BaseAddress + *Length;

        //
        // If the next page didn't wrap, and the next page is below the end of
        // the region, update Length and BaseAddress appropriately and set Retry
        // to TRUE to indicate to NtFlushInstructionCache that it should call
        // KeSweepIcacheRange again.
        //

        if ( (NextVa > BadVa) && (NextVa < EndVa) ) {
            *Length = (ULONG) (EndVa - NextVa);
            *BaseAddress = (PVOID)NextVa;
            *Retry = TRUE;
        }
    }

    return EXCEPTION_EXECUTE_HANDLER;
}

NTSTATUS
NtFlushInstructionCache (
    IN HANDLE ProcessHandle,
    IN PVOID BaseAddress OPTIONAL,
    IN SIZE_T Length
    )

/*++

Routine Description:

    This function flushes the instruction cache for the specified process.

Arguments:

    ProcessHandle - Supplies a handle to the process in which the instruction
                    cache is to be flushed. Must have PROCESS_VM_WRITE access
                    to the specified process.

    BaseAddress - Supplies an optional pointer to base of the region that
                  is flushed.

    Length - Supplies the length of the region that is flushed if the base
             address is specified.

Return Value:

    STATUS_SUCCESS.

--*/

{
    KAPC_STATE ApcState;
    KPROCESSOR_MODE PreviousMode;
    PEPROCESS Process;
    NTSTATUS Status;
    LOGICAL Retry;
    PVOID RangeBase;
    SIZE_T RangeLength;

    PAGED_CODE();

    PreviousMode = KeGetPreviousMode();

    //
    // If the base address is not specified, or the base address is specified
    // and the length is not zero, then flush the specified instruction cache
    // range.
    //

    if ((ARGUMENT_PRESENT(BaseAddress) == FALSE) || (Length != 0)) {

        //
        // If previous mode is user and the range specified falls in kernel
        // address space, return an error.
        //

        if ((ARGUMENT_PRESENT(BaseAddress) != FALSE) &&
            (PreviousMode != KernelMode)) {
            try {
                ProbeForRead(BaseAddress, Length, sizeof(UCHAR));
            } except(EXCEPTION_EXECUTE_HANDLER) {
                return GetExceptionCode();
            }
        }

        //
        // If the specified process is not the current process, then
        // the process must be attached to during the flush.
        //

        Process = NULL;
        if (ProcessHandle != NtCurrentProcess()) {

            //
            // Reference the specified process checking for PROCESS_VM_WRITE
            // access.
            //

            Status = ObReferenceObjectByHandle(ProcessHandle,
                                               PROCESS_VM_WRITE,
                                               PsProcessType,
                                               PreviousMode,
                                               (PVOID *)&Process,
                                               NULL);

            if (!NT_SUCCESS(Status)) {
                return Status;
            }

            //
            // Attach to the process.
            //

            KeStackAttachProcess (&Process->Pcb, &ApcState);
        }

        //
        // If the base address is not specified, sweep the entire instruction
        // cache.  If the base address is specified, flush the specified range.
        //

        if (ARGUMENT_PRESENT(BaseAddress) == FALSE) {
            KeSweepIcache(FALSE);

        } else {

            //
            // Parts of the specified range may be invalid.  An exception
            // handler is used to skip over those parts.  Before calling
            // KeSweepIcacheRange, we set Retry to FALSE.  If an access
            // violation occurs in KeSweepIcacheRange, the MiFlushRangeFilter
            // exception filter is called.  It updates RangeBase and
            // RangeLength to skip over the failing page, and sets Retry to
            // TRUE.  As long as Retry is TRUE, we continue to call
            // KeSweepIcacheRange.
            //

            RangeBase = BaseAddress;
            RangeLength = Length;

            do {
                Retry = FALSE;

                try {

                    KeSweepIcacheRange(FALSE, RangeBase, RangeLength);

                } except(MiFlushRangeFilter(GetExceptionInformation(),
                                            &RangeBase,
                                            &RangeLength,
                                            &Retry)) {
                    NOTHING;
                }
            } while (Retry != FALSE);
        }

        //
        // If the specified process is not the current process, then
        // detach from it and dereference it.
        //

        if (Process != NULL) {
            KeUnstackDetachProcess (&ApcState);
            ObDereferenceObject(Process);
        }
    }

    return STATUS_SUCCESS;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\forksup.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   forksup.c

Abstract:

    This module contains the routines which support the POSIX fork operation.

Author:

    Lou Perazzoli (loup) 22-Jul-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

VOID
MiUpControlAreaRefs (
    IN PMMVAD Vad
    );

ULONG
MiDoneWithThisPageGetAnother (
    IN PPFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PEPROCESS CurrentProcess
    );

ULONG
MiLeaveThisPageGetAnother (
    OUT PPFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PEPROCESS CurrentProcess
    );

VOID
MiUpForkPageShareCount (
    IN PMMPFN PfnForkPtePage
    );

ULONG
MiHandleForkTransitionPte (
    IN PMMPTE PointerPte,
    IN PMMPTE PointerNewPte,
    IN PMMCLONE_BLOCK ForkProtoPte
    );

VOID
MiDownShareCountFlushEntireTb (
    IN PFN_NUMBER PageFrameIndex
    );

VOID
MiBuildForkPageTable (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PMMPTE PointerNewPde,
    IN PFN_NUMBER PdePhysicalPage,
    IN PMMPFN PfnPdPage,
    IN LOGICAL MakeValid
    );

#define MM_FORK_SUCCEEDED 0
#define MM_FORK_FAILED 1

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,MiCloneProcessAddressSpace)
#endif


NTSTATUS
MiCloneProcessAddressSpace (
    IN PEPROCESS ProcessToClone,
    IN PEPROCESS ProcessToInitialize
    )

/*++

Routine Description:

    This routine stands on its head to produce a copy of the specified
    process's address space in the process to initialize.  This
    is done by examining each virtual address descriptor's inherit
    attributes.  If the pages described by the VAD should be inherited,
    each PTE is examined and copied into the new address space.

    For private pages, fork prototype PTEs are constructed and the pages
    become shared, copy-on-write, between the two processes.


Arguments:

    ProcessToClone - Supplies the process whose address space should be
                     cloned.

    ProcessToInitialize - Supplies the process whose address space is to
                          be created.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled.  No VADs exist for the new process on entry
    and the new process is not on the working set expansion links list so
    it cannot be trimmed (or outswapped).  Hence none of the pages in the
    new process need to be locked down.

--*/

{
    LOGICAL ChargedClonePoolQuota;
    PAWEINFO AweInfo;
    PVOID RestartKey;
    PFN_NUMBER PdePhysicalPage;
    PEPROCESS CurrentProcess;
    PMMPTE PdeBase;
    PMMCLONE_HEADER CloneHeader;
    PMMCLONE_BLOCK CloneProtos;
    PMMCLONE_DESCRIPTOR CloneDescriptor;
    PMM_AVL_TABLE CloneRoot;
    PMM_AVL_TABLE TargetCloneRoot;
    PFN_NUMBER RootPhysicalPage;
    PMMVAD NewVad;
    PMMVAD Vad;
    PMMVAD NextVad;
    PMMVAD *VadList;
    PMMVAD FirstNewVad;
    PMMCLONE_DESCRIPTOR *CloneList;
    PMMCLONE_DESCRIPTOR FirstNewClone;
    PMMCLONE_DESCRIPTOR Clone;
    PMMCLONE_DESCRIPTOR NextClone;
    PMMCLONE_DESCRIPTOR NewClone;
    ULONG Attached;
    ULONG CloneFailed;
    ULONG VadInsertFailed;
    WSLE_NUMBER WorkingSetIndex;
    PVOID VirtualAddress;
    NTSTATUS status;
    PMMPFN Pfn2;
    PMMPFN PfnPdPage;
    MMPTE TempPte;
    MMPTE PteContents;
    KAPC_STATE ApcState;
    ULONG i;
#if defined (_X86PAE_)
    PMDL MdlPageDirectory;
    PFN_NUMBER PageDirectoryFrames[PD_PER_SYSTEM];
    PFN_NUMBER MdlHackPageDirectory[(sizeof(MDL)/sizeof(PFN_NUMBER)) + PD_PER_SYSTEM];
#else
    PFN_NUMBER MdlDirPage;
#endif
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE LastPte;
    PMMPTE LastPde;
    PMMPTE PointerNewPte;
    PMMPTE NewPteMappedAddress;
    PMMPTE PointerNewPde;
    PMI_PHYSICAL_VIEW PhysicalView;
    PMI_PHYSICAL_VIEW NextPhysicalView;
    PMI_PHYSICAL_VIEW PhysicalViewList;
    ULONG PhysicalViewCount;
    PFN_NUMBER PageFrameIndex;
    PMMCLONE_BLOCK ForkProtoPte;
    PMMCLONE_BLOCK CloneProto;
    PMMCLONE_BLOCK LockedForkPte;
    PMMPTE ContainingPte;
    ULONG NumberOfForkPtes;
    PFN_NUMBER NumberOfPrivatePages;
    SIZE_T TotalPagedPoolCharge;
    SIZE_T TotalNonPagedPoolCharge;
    PMMPFN PfnForkPtePage;
    PVOID UsedPageTableEntries;
    ULONG ReleasedWorkingSetMutex;
    ULONG FirstTime;
    ULONG Waited;
    ULONG PpePdeOffset;
    PFN_NUMBER HyperPhysicalPage;
#if defined (_MIALT4K_)
    PVOID TempAliasInformation;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PointerPpeLast;
    PFN_NUMBER PageDirFrameIndex;
    PVOID UsedPageDirectoryEntries;
    PMMPTE PointerNewPpe;
    PMMPTE PpeBase;
    PMMPFN PfnPpPage;
#if (_MI_PAGING_LEVELS >= 4)
    PVOID UsedPageDirectoryParentEntries;
    PFN_NUMBER PpePhysicalPage;
    PFN_NUMBER PageParentFrameIndex;
    PMMPTE PointerNewPxe;
    PMMPTE PxeBase;
    PMMPFN PfnPxPage;
    PFN_NUMBER MdlDirParentPage;
#endif

#else
    PMMWSL HyperBase = NULL;
    PMMWSL HyperWsl;
#endif

    PAGED_CODE();

    HyperPhysicalPage = ProcessToInitialize->WorkingSetPage;
    NumberOfForkPtes = 0;
    Attached = FALSE;
    PageFrameIndex = (PFN_NUMBER)-1;
    PhysicalViewList = NULL;
    PhysicalViewCount = 0;
    FirstNewVad = NULL;

    CloneHeader = NULL;
    CloneProtos = NULL;
    CloneDescriptor = NULL;
    CloneRoot = NULL;
    TargetCloneRoot = NULL;
    ChargedClonePoolQuota = FALSE;

    if (ProcessToClone != PsGetCurrentProcess()) {
        Attached = TRUE;
        KeStackAttachProcess (&ProcessToClone->Pcb, &ApcState);
    }

#if defined (_X86PAE_)
    PointerPte = (PMMPTE) ProcessToInitialize->PaeTop;

    for (i = 0; i < PD_PER_SYSTEM; i += 1) {
        PageDirectoryFrames[i] = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        PointerPte += 1;
    }

    RootPhysicalPage = PageDirectoryFrames[PD_PER_SYSTEM - 1];
#else
    RootPhysicalPage = ProcessToInitialize->Pcb.DirectoryTableBase[0] >> PAGE_SHIFT;
#endif

    CurrentProcess = ProcessToClone;

    //
    // Get the working set mutex and the address creation mutex
    // of the process to clone.  This prevents page faults while we
    // are examining the address map and prevents virtual address space
    // from being created or deleted.
    //

    LOCK_ADDRESS_SPACE (CurrentProcess);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (CurrentProcess->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn1;
    }

    //
    // Check for AWE, write watch and large page regions as they are not
    // duplicated so fork is not allowed.  Note that since this is a
    // readonly list walk, the address space mutex is sufficient to
    // synchronize properly.
    //

    if (CurrentProcess->PhysicalVadRoot != NULL) {

        RestartKey = NULL;

#if 1

        //
        // Don't allow cloning of any processes with physical VADs of
        // any sort.  This is to prevent drivers that map nonpaged pool
        // into user space from creating security/corruption holes if
        // the application clones because drivers wouldn't know how
        // to clean up (destroy) all the views prior to freeing the pool.
        //

        if (MiEnumerateGenericTableWithoutSplayingAvl (CurrentProcess->PhysicalVadRoot, &RestartKey) != NULL) {
            status = STATUS_INVALID_PAGE_PROTECTION;
            goto ErrorReturn1;
        }

#else

        do {

            PhysicalView = (PMI_PHYSICAL_VIEW) MiEnumerateGenericTableWithoutSplayingAvl (CurrentProcess->PhysicalVadRoot, &RestartKey);

            if (PhysicalView == NULL) {
                break;
            }

            if (PhysicalView->u.LongFlags != MI_PHYSICAL_VIEW_PHYS) {
                status = STATUS_INVALID_PAGE_PROTECTION;
                goto ErrorReturn1;
            }

            if ((PhysicalView->Vad->u.VadFlags.PrivateMemory == 1) ||
                (PhysicalView->Vad->u2.VadFlags2.Inherit == MM_VIEW_SHARE)) {

                PhysicalViewCount += 1;
            }

        } while (TRUE);

#endif

    }

    AweInfo = (PAWEINFO) CurrentProcess->AweInfo;

    if (AweInfo != NULL) {
        RestartKey = NULL;

        do {

            PhysicalView = (PMI_PHYSICAL_VIEW) MiEnumerateGenericTableWithoutSplayingAvl (&AweInfo->AweVadRoot, &RestartKey);

            if (PhysicalView == NULL) {
                break;
            }

            if (PhysicalView->u.LongFlags != MI_PHYSICAL_VIEW_PHYS) {
                status = STATUS_INVALID_PAGE_PROTECTION;
                goto ErrorReturn1;
            }

        } while (TRUE);
    }

    //
    // Attempt to acquire the needed pool before starting the
    // clone operation, this allows an easier failure path in
    // the case of insufficient system resources.  The working set mutex
    // must be acquired (and held throughout) to protect against modifications
    // to the NumberOfPrivatePages field in the EPROCESS.
    //

#if defined (_MIALT4K_)
    if (CurrentProcess->Wow64Process != NULL) {
        LOCK_ALTERNATE_TABLE_UNSAFE (CurrentProcess->Wow64Process);
    }
#endif

    LOCK_WS (CurrentProcess);

    ASSERT (CurrentProcess->ForkInProgress == NULL);

    //
    // Indicate to the pager that the current process is being
    // forked.  This blocks other threads in that process from
    // modifying clone block counts and contents as well as alternate PTEs.
    //

    CurrentProcess->ForkInProgress = PsGetCurrentThread ();

#if defined (_MIALT4K_)
    if (CurrentProcess->Wow64Process != NULL) {
        UNLOCK_ALTERNATE_TABLE_UNSAFE (CurrentProcess->Wow64Process);
    }
#endif

    NumberOfPrivatePages = CurrentProcess->NumberOfPrivatePages;

    TargetCloneRoot = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(MM_AVL_TABLE),
                                             'rCmM');

    if (TargetCloneRoot == NULL) {
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn2;
    }

    RtlZeroMemory (TargetCloneRoot, sizeof(MM_AVL_TABLE));
    TargetCloneRoot->BalancedRoot.u1.Parent = MI_MAKE_PARENT (&TargetCloneRoot->BalancedRoot, 0);

    //
    // Only allocate a clone root for the calling process if he doesn't
    // already have one (ie: this is his first fork call).  Note that if
    // the allocation succeeds, the root table remains until the process
    // exits regardless of whether the fork succeeds.
    //

    if (CurrentProcess->CloneRoot == NULL) {
        CloneRoot = ExAllocatePoolWithTag (NonPagedPool,
                                           sizeof(MM_AVL_TABLE),
                                           'rCmM');

        if (CloneRoot == NULL) {
            status = STATUS_INSUFFICIENT_RESOURCES;
            goto ErrorReturn2;
        }

        RtlZeroMemory (CloneRoot, sizeof(MM_AVL_TABLE));
        CloneRoot->BalancedRoot.u1.Parent = MI_MAKE_PARENT (&CloneRoot->BalancedRoot, 0);
        CurrentProcess->CloneRoot = CloneRoot;
    }

    CloneProtos = ExAllocatePoolWithTag (PagedPool, sizeof(MMCLONE_BLOCK) *
                                                NumberOfPrivatePages,
                                                'lCmM');
    if (CloneProtos == NULL) {
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn2;
    }

    CloneHeader = ExAllocatePoolWithTag (NonPagedPool,
                                         sizeof(MMCLONE_HEADER),
                                         'hCmM');
    if (CloneHeader == NULL) {
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn2;
    }

    CloneDescriptor = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(MMCLONE_DESCRIPTOR),
                                             'dCmM');
    if (CloneDescriptor == NULL) {
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn2;
    }

    //
    // Charge the current process the quota for the paged and nonpaged
    // global structures.  This consists of the array of clone blocks
    // in paged pool and the clone header in non-paged pool.
    //

    status = PsChargeProcessPagedPoolQuota (CurrentProcess,
                                            sizeof(MMCLONE_BLOCK) * NumberOfPrivatePages);

    if (!NT_SUCCESS(status)) {

        //
        // Unable to charge quota for the clone blocks.
        //

        goto ErrorReturn2;
    }

    status = PsChargeProcessNonPagedPoolQuota (CurrentProcess,
                                               sizeof(MMCLONE_HEADER));

    if (!NT_SUCCESS(status)) {

        //
        // Unable to charge quota for the clone header.
        //

        PsReturnProcessPagedPoolQuota (CurrentProcess,
                                       sizeof(MMCLONE_BLOCK) * NumberOfPrivatePages);
        goto ErrorReturn2;
    }

    ChargedClonePoolQuota = TRUE;

    Vad = MiGetFirstVad (CurrentProcess);
    VadList = &FirstNewVad;

    if (PhysicalViewCount != 0) {

        PMM_AVL_TABLE PhysicalVadRoot;

        PhysicalVadRoot = ProcessToInitialize->PhysicalVadRoot;

        //
        // The address space mutex synchronizes the allocation of the
        // EPROCESS PhysicalVadRoot but is not needed here because no one
        // can be manipulating the target process except this thread.
        // This table root is not deleted until the process exits.
        //

        if (PhysicalVadRoot == NULL) {

            PhysicalVadRoot = (PMM_AVL_TABLE) ExAllocatePoolWithTag (
                                                        NonPagedPool,
                                                        sizeof (MM_AVL_TABLE),
                                                        MI_PHYSICAL_VIEW_ROOT_KEY);

            if (PhysicalVadRoot == NULL) {
                status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn2;
            }

            RtlZeroMemory (PhysicalVadRoot, sizeof (MM_AVL_TABLE));
            ASSERT (PhysicalVadRoot->NumberGenericTableElements == 0);
            PhysicalVadRoot->BalancedRoot.u1.Parent = &PhysicalVadRoot->BalancedRoot;

            MiInsertPhysicalVadRoot (ProcessToInitialize, PhysicalVadRoot);
        }

        i = PhysicalViewCount;
        do {

            PhysicalView = (PMI_PHYSICAL_VIEW) ExAllocatePoolWithTag (
                                                   NonPagedPool,
                                                   sizeof(MI_PHYSICAL_VIEW),
                                                   MI_PHYSICAL_VIEW_KEY);

            if (PhysicalView == NULL) {
                status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn2;
            }

            PhysicalView->u1.Parent = (PMMADDRESS_NODE) PhysicalViewList;
            PhysicalViewList = PhysicalView;
            i -= 1;
        } while (i != 0);
    }

    while (Vad != NULL) {

        //
        // If the VAD does not go to the child, ignore it.
        //

        if ((Vad->u.VadFlags.UserPhysicalPages == 0) &&
            (Vad->u.VadFlags.LargePages == 0) &&

            ((Vad->u.VadFlags.PrivateMemory == 1) ||
            (Vad->u2.VadFlags2.Inherit == MM_VIEW_SHARE))) {

            NewVad = ExAllocatePoolWithTag (NonPagedPool, sizeof(MMVAD_LONG), 'ldaV');

            if (NewVad == NULL) {

                //
                // Unable to allocate pool for all the VADs.  Deallocate
                // all VADs and other pool obtained so far.
                //

                *VadList = NULL;
                status = STATUS_INSUFFICIENT_RESOURCES;
                goto ErrorReturn2;
            }

            RtlZeroMemory (NewVad, sizeof(MMVAD_LONG));

#if defined (_MIALT4K_)
            if (((Vad->u.VadFlags.PrivateMemory) && (Vad->u.VadFlags.NoChange == 0)) 
                ||
                (Vad->u2.VadFlags2.LongVad == 0)) {

                NOTHING;
            }
            else if (((PMMVAD_LONG)Vad)->AliasInformation != NULL) {

                //
                // This VAD has aliased VADs which are going to be duplicated
                // into the clone's address space, but the alias list must
                // be explicitly copied.
                //

                ((PMMVAD_LONG)NewVad)->AliasInformation = MiDuplicateAliasVadList (Vad);

                if (((PMMVAD_LONG)NewVad)->AliasInformation == NULL) {
                    ExFreePool (NewVad);
                    *VadList = NULL;
                    status = STATUS_INSUFFICIENT_RESOURCES;
                    goto ErrorReturn2;
                }
            }
#endif

            *VadList = NewVad;
            VadList = &NewVad->u1.Parent;
        }
        Vad = MiGetNextVad (Vad);
    }

    //
    // Terminate list of VADs for new process.
    //

    *VadList = NULL;

    //
    // Initializing UsedPageTableEntries is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    UsedPageTableEntries = NULL;
    PdeBase = NULL;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Initializing these is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    PageDirFrameIndex = 0;
    UsedPageDirectoryEntries = NULL;
    PpeBase = NULL;

#if (_MI_PAGING_LEVELS >= 4)
    PxeBase = NULL;
    PageParentFrameIndex = 0;
    UsedPageDirectoryParentEntries = NULL;
#endif

    //
    // Map the (extended) page directory parent page into the system address
    // space.
    //

    PpeBase = (PMMPTE) MiMapSinglePage (NULL,
                                        RootPhysicalPage,
                                        MmCached,
                                        HighPagePriority);

    if (PpeBase == NULL) {
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn2;
    }

    PfnPpPage = MI_PFN_ELEMENT (RootPhysicalPage);

#if (_MI_PAGING_LEVELS >= 4)

    //
    // The PxeBase is going to map the real top-level.  For 4 level
    // architectures, the PpeBase above is mapping the wrong page, but
    // it doesn't matter because the initial value is never used - it
    // just serves as a way to obtain a mapping PTE and will be repointed
    // correctly before it is ever used.
    //

    PxeBase = (PMMPTE) MiMapSinglePage (NULL,
                                        RootPhysicalPage,
                                        MmCached,
                                        HighPagePriority);

    if (PxeBase == NULL) {
        MiUnmapSinglePage (PpeBase);
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn2;
    }

    PfnPxPage = MI_PFN_ELEMENT (RootPhysicalPage);

    MdlDirParentPage = RootPhysicalPage;

#endif

#endif

    //
    // Initialize a page directory map so it can be
    // unlocked in the loop and the end of the loop without
    // any testing to see if has a valid value the first time through.
    // Note this is a dummy map for 64-bit systems and a real one for 32-bit.
    //

#if !defined (_X86PAE_)

    MdlDirPage = RootPhysicalPage;

    PdePhysicalPage = RootPhysicalPage;

    PdeBase = (PMMPTE) MiMapSinglePage (NULL,
                                        MdlDirPage,
                                        MmCached,
                                        HighPagePriority);

#else

    //
    // All 4 page directory pages need to be mapped for PAE so the heavyweight
    // mapping must be used.
    //

    MdlPageDirectory = (PMDL)&MdlHackPageDirectory[0];

    MmInitializeMdl (MdlPageDirectory,
                     (PVOID)PDE_BASE,
                     PD_PER_SYSTEM * PAGE_SIZE);

    MdlPageDirectory->MdlFlags |= MDL_PAGES_LOCKED;

    RtlCopyMemory (MdlPageDirectory + 1,
                   PageDirectoryFrames,
                   PD_PER_SYSTEM * sizeof (PFN_NUMBER));

    PdePhysicalPage = RootPhysicalPage;

    PdeBase = (PMMPTE) MmMapLockedPagesSpecifyCache (MdlPageDirectory,
                                                     KernelMode,
                                                     MmCached,
                                                     NULL,
                                                     FALSE,
                                                     HighPagePriority);

#endif

    if (PdeBase == NULL) {
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn3;
    }

    PfnPdPage = MI_PFN_ELEMENT (RootPhysicalPage);

#if (_MI_PAGING_LEVELS < 3)

    //
    // Map hyperspace so target UsedPageTable entries can be incremented.
    //

    HyperBase = (PMMWSL)MiMapSinglePage (NULL,
                                         HyperPhysicalPage,
                                         MmCached,
                                         HighPagePriority);

    if (HyperBase == NULL) {
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn3;
    }

    //
    // MmWorkingSetList is not page aligned when booted /3GB so account
    // for that here when established the used page table entry pointer.
    //

    HyperWsl = (PMMWSL) ((PCHAR)HyperBase + BYTE_OFFSET(MmWorkingSetList));

#endif

    //
    // Map the hyperspace page (any cached page would serve equally well)
    // to acquire a mapping PTE now (prior to starting through the loop) -
    // this way we guarantee forward progress regardless of resources once
    // the loop is entered.
    //

    NewPteMappedAddress = (PMMPTE) MiMapSinglePage (NULL,
                                                    HyperPhysicalPage,
                                                    MmCached,
                                                    HighPagePriority);

    if (NewPteMappedAddress == NULL) {
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn3;
    }

    PointerNewPte = NewPteMappedAddress;

    //
    // Build a new clone prototype PTE block and descriptor, note that
    // each prototype PTE has a reference count following it.
    //

    ForkProtoPte = CloneProtos;

    LockedForkPte = ForkProtoPte;
    MiLockPagedAddress (LockedForkPte);

    CloneHeader->NumberOfPtes = (ULONG)NumberOfPrivatePages;
    CloneHeader->NumberOfProcessReferences = 1;
    CloneHeader->ClonePtes = CloneProtos;

    CloneDescriptor->StartingVpn = (ULONG_PTR)CloneProtos;
    CloneDescriptor->EndingVpn = (ULONG_PTR)((ULONG_PTR)CloneProtos +
                            NumberOfPrivatePages *
                              sizeof(MMCLONE_BLOCK));
    CloneDescriptor->EndingVpn -= 1;
    CloneDescriptor->NumberOfReferences = 0;
    CloneDescriptor->FinalNumberOfReferences = 0;
    CloneDescriptor->NumberOfPtes = (ULONG)NumberOfPrivatePages;
    CloneDescriptor->CloneHeader = CloneHeader;
    CloneDescriptor->PagedPoolQuotaCharge = sizeof(MMCLONE_BLOCK) *
                                NumberOfPrivatePages;

    //
    // Insert the clone descriptor for this fork operation into the
    // process which was cloned.
    //

    MiInsertClone (CurrentProcess, CloneDescriptor);

    //
    // Examine each virtual address descriptor and create the
    // proper structures for the new process.
    //

    Vad = MiGetFirstVad (CurrentProcess);
    NewVad = FirstNewVad;

    while (Vad != NULL) {

        //
        // Examine the VAD to determine its type and inheritance
        // attribute.
        //

        if ((Vad->u.VadFlags.UserPhysicalPages == 0) &&
            (Vad->u.VadFlags.LargePages == 0) &&

            ((Vad->u.VadFlags.PrivateMemory == 1) ||
            (Vad->u2.VadFlags2.Inherit == MM_VIEW_SHARE))) {

            //
            // The virtual address descriptor should be shared in the
            // forked process.
            //
            // Make a copy of the VAD for the new process, the new VADs
            // are preallocated and linked together through the parent
            // field.
            //

            NextVad = NewVad->u1.Parent;

            if (Vad->u.VadFlags.PrivateMemory == 1) {
                *(PMMVAD_SHORT)NewVad = *(PMMVAD_SHORT)Vad;
                NewVad->u.VadFlags.NoChange = 0;
            }
            else {
                if (Vad->u2.VadFlags2.LongVad == 0) {
                    *NewVad = *Vad;
                }
                else {

#if defined (_MIALT4K_)

                    //
                    // The VADs duplication earlier in this routine keeps both
                    // the current process' VAD tree and the new process' VAD
                    // list ordered.  ASSERT on this below.
                    //

#if DBG
                    if (((PMMVAD_LONG)Vad)->AliasInformation == NULL) {
                        ASSERT (((PMMVAD_LONG)NewVad)->AliasInformation == NULL);
                    }
                    else {
                        ASSERT (((PMMVAD_LONG)NewVad)->AliasInformation != NULL);
                    }
#endif

                    TempAliasInformation = ((PMMVAD_LONG)NewVad)->AliasInformation;
#endif

                    *(PMMVAD_LONG)NewVad = *(PMMVAD_LONG)Vad;

#if defined (_MIALT4K_)
                    ((PMMVAD_LONG)NewVad)->AliasInformation = TempAliasInformation;
#endif

                    if (Vad->u2.VadFlags2.ExtendableFile == 1) {
                        KeAcquireGuardedMutexUnsafe (&MmSectionBasedMutex);
                        ASSERT (Vad->ControlArea->Segment->ExtendInfo != NULL);
                        Vad->ControlArea->Segment->ExtendInfo->ReferenceCount += 1;
                        KeReleaseGuardedMutexUnsafe (&MmSectionBasedMutex);
                    }
                }
            }

            NewVad->u2.VadFlags2.LongVad = 1;

            if (NewVad->u.VadFlags.NoChange) {
                if ((NewVad->u2.VadFlags2.OneSecured) ||
                    (NewVad->u2.VadFlags2.MultipleSecured)) {

                    //
                    // Eliminate these as the memory was secured
                    // only in this process, not in the new one.
                    //

                    NewVad->u2.VadFlags2.OneSecured = 0;
                    NewVad->u2.VadFlags2.MultipleSecured = 0;
                    ((PMMVAD_LONG) NewVad)->u3.List.Flink = NULL;
                    ((PMMVAD_LONG) NewVad)->u3.List.Blink = NULL;
                }
                if (NewVad->u2.VadFlags2.SecNoChange == 0) {
                    NewVad->u.VadFlags.NoChange = 0;
                }
            }
            NewVad->u1.Parent = NextVad;

            //
            // If the VAD refers to a section, up the view count for that
            // section.  This requires the PFN lock to be held.
            //

            if ((Vad->u.VadFlags.PrivateMemory == 0) &&
                (Vad->ControlArea != NULL)) {

                if ((Vad->u.VadFlags.Protection & MM_READWRITE) &&
                    (Vad->ControlArea->FilePointer != NULL) &&
                    (Vad->ControlArea->u.Flags.Image == 0)) {

                    InterlockedIncrement ((PLONG)&Vad->ControlArea->Segment->WritableUserReferences);
                }

                //
                // Increment the count of the number of views for the
                // section object.  This requires the PFN lock to be held.
                //

                MiUpControlAreaRefs (Vad);
            }

            if (Vad->u.VadFlags.PhysicalMapping == 1) {
                PhysicalView = PhysicalViewList;
                ASSERT (PhysicalViewCount != 0);
                ASSERT (PhysicalView != NULL);
                PhysicalViewCount -= 1;
                PhysicalViewList = (PMI_PHYSICAL_VIEW) PhysicalView->u1.Parent;

                PhysicalView->Vad = NewVad;
                PhysicalView->u.LongFlags = MI_PHYSICAL_VIEW_PHYS;

                PhysicalView->StartingVpn = Vad->StartingVpn;
                PhysicalView->EndingVpn = Vad->EndingVpn;

                MiPhysicalViewInserter (ProcessToInitialize, PhysicalView);
            }

            //
            // Examine each PTE and create the appropriate PTE for the
            // new process.
            //

            PointerPde = NULL;      // Needless, but keeps W4 happy
            PointerPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->StartingVpn));
            LastPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->EndingVpn));
            FirstTime = TRUE;

            do {

                //
                // For each PTE contained in the VAD check the page table
                // page, and if non-zero, make the appropriate modifications
                // to copy the PTE to the new process.
                //

                if ((FirstTime) || MiIsPteOnPdeBoundary (PointerPte)) {

                    PointerPxe = MiGetPpeAddress (PointerPte);
                    PointerPpe = MiGetPdeAddress (PointerPte);
                    PointerPde = MiGetPteAddress (PointerPte);

                    do {

#if (_MI_PAGING_LEVELS >= 4)
                        while (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                                            CurrentProcess,
                                                            MM_NOIRQL,
                                                            &Waited)) {
    
                            //
                            // Extended page directory parent is empty,
                            // go to the next one.
                            //
    
                            PointerPxe += 1;
                            PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                            if (PointerPte > LastPte) {
    
                                //
                                // All done with this VAD, exit loop.
                                //
    
                                goto AllDone;
                            }
                        }
    
#endif
                        Waited = 0;

                        while (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                                            CurrentProcess,
                                                            MM_NOIRQL,
                                                            &Waited)) {
    
                            //
                            // Page directory parent is empty, go to the next one.
                            //
    
                            PointerPpe += 1;
                            if (MiIsPteOnPdeBoundary (PointerPpe)) {
                                PointerPxe = MiGetPteAddress (PointerPpe);
                                Waited = 1;
                                break;
                            }
                            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                            if (PointerPte > LastPte) {
    
                                //
                                // All done with this VAD, exit loop.
                                //
    
                                goto AllDone;
                            }
                        }
    
                        if (Waited != 0) {
                            continue;
                        }
    
                        while (!MiDoesPdeExistAndMakeValid (PointerPde,
                                                            CurrentProcess,
                                                            MM_NOIRQL,
                                                            &Waited)) {
    
                            //
                            // This page directory is empty, go to the next one.
                            //
    
                            PointerPde += 1;
                            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                            if (PointerPte > LastPte) {
    
                                //
                                // All done with this VAD, exit loop.
                                //
    
                                goto AllDone;
                            }
#if (_MI_PAGING_LEVELS >= 3)
                            if (MiIsPteOnPdeBoundary (PointerPde)) {
                                PointerPpe = MiGetPteAddress (PointerPde);
                                PointerPxe = MiGetPdeAddress (PointerPde);
                                Waited = 1;
                                break;
                            }
#endif
                        }
    
                    } while (Waited != 0);

                    FirstTime = FALSE;

#if (_MI_PAGING_LEVELS >= 4)
                    //
                    // Calculate the address of the PXE in the new process's
                    // extended page directory parent page.
                    //

                    PointerNewPxe = &PxeBase[MiGetPpeOffset(PointerPte)];

                    if (PointerNewPxe->u.Long == 0) {

                        //
                        // No physical page has been allocated yet, get a page
                        // and map it in as a valid page.  This will become
                        // a page directory parent page for the new process.
                        //
                        // Note that unlike page table pages which are left
                        // in transition, page directory parent pages (and page
                        // directory pages) are left valid and hence
                        // no share count decrement is done.
                        //

                        ReleasedWorkingSetMutex =
                                MiLeaveThisPageGetAnother (&PageParentFrameIndex,
                                                           PointerPxe,
                                                           CurrentProcess);

                        MI_ZERO_USED_PAGETABLE_ENTRIES (MI_PFN_ELEMENT(PageParentFrameIndex));

                        if (ReleasedWorkingSetMutex) {

                            //
                            // Ensure the PDE (and any table above it) are still
                            // resident.
                            //

                            MiMakePdeExistAndMakeValid (PointerPde,
                                                        CurrentProcess,
                                                        MM_NOIRQL);
                        }

                        //
                        // Hand initialize this PFN as normal initialization
                        // would do it for the process whose context we are
                        // attached to.
                        //
                        // The PFN lock must be held while initializing the
                        // frame to prevent those scanning the database for
                        // free frames from taking it after we fill in the
                        // u2 field.
                        //

                        MiBuildForkPageTable (PageParentFrameIndex,
                                              PointerPxe,
                                              PointerNewPxe,
                                              RootPhysicalPage,
                                              PfnPxPage,
                                              TRUE);

                        //
                        // Map the new page directory page into the system
                        // portion of the address space.  Note that hyperspace
                        // cannot be used as other operations (allocating
                        // nonpaged pool at DPC level) could cause the
                        // hyperspace page being used to be reused.
                        //

                        MdlDirParentPage = PageParentFrameIndex;

                        ASSERT (PpeBase != NULL);

                        PpeBase = (PMMPTE) MiMapSinglePage (PpeBase,
                                                            MdlDirParentPage,
                                                            MmCached,
                                                            HighPagePriority);

                        PpePhysicalPage = PageParentFrameIndex;

                        PfnPpPage = MI_PFN_ELEMENT (PpePhysicalPage);
    
                        UsedPageDirectoryParentEntries = (PVOID) PfnPpPage;
                    }
                    else {

                        ASSERT (PointerNewPxe->u.Hard.Valid == 1);

                        PpePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerNewPxe);

                        //
                        // If we are switching from one page directory parent
                        // frame to another and the last one is one that we
                        // freshly allocated, the last one's reference count
                        // must be decremented now that we're done with it.
                        //
                        // Note that at least one target PXE has already been
                        // initialized for this codepath to execute.
                        //

                        ASSERT (PageParentFrameIndex == MdlDirParentPage);

                        if (MdlDirParentPage != PpePhysicalPage) {
                            ASSERT (MdlDirParentPage != (PFN_NUMBER)-1);
                            PageParentFrameIndex = PpePhysicalPage;
                            MdlDirParentPage = PageParentFrameIndex;

                            ASSERT (PpeBase != NULL);
    
                            PpeBase = (PMMPTE) MiMapSinglePage (PpeBase,
                                                                MdlDirParentPage,
                                                                MmCached,
                                                                HighPagePriority);
    
                            PointerNewPpe = PpeBase;

                            PfnPpPage = MI_PFN_ELEMENT (PpePhysicalPage);
        
                            UsedPageDirectoryParentEntries = (PVOID)PfnPpPage;
                        }
                    }
#endif

#if (_MI_PAGING_LEVELS >= 3)

                    //
                    // Calculate the address of the PPE in the new process's
                    // page directory parent page.
                    //

                    PointerNewPpe = &PpeBase[MiGetPdeOffset(PointerPte)];

                    if (PointerNewPpe->u.Long == 0) {

                        //
                        // No physical page has been allocated yet, get a page
                        // and map it in as a valid page.  This will
                        // become a page directory page for the new process.
                        //
                        // Note that unlike page table pages which are left
                        // in transition, page directory pages are left valid
                        // and hence no share count decrement is done.
                        //

                        ReleasedWorkingSetMutex =
                                MiLeaveThisPageGetAnother (&PageDirFrameIndex,
                                                           PointerPpe,
                                                           CurrentProcess);

                        MI_ZERO_USED_PAGETABLE_ENTRIES (MI_PFN_ELEMENT(PageDirFrameIndex));

                        if (ReleasedWorkingSetMutex) {

                            //
                            // Ensure the PDE (and any table above it) are still
                            // resident.
                            //

                            MiMakePdeExistAndMakeValid (PointerPde,
                                                        CurrentProcess,
                                                        MM_NOIRQL);
                        }

                        //
                        // Hand initialize this PFN as normal initialization
                        // would do it for the process whose context we are
                        // attached to.
                        //
                        // The PFN lock must be held while initializing the
                        // frame to prevent those scanning the database for
                        // free frames from taking it after we fill in the
                        // u2 field.
                        //

                        MiBuildForkPageTable (PageDirFrameIndex,
                                              PointerPpe,
                                              PointerNewPpe,
#if (_MI_PAGING_LEVELS >= 4)
                                              PpePhysicalPage,
#else
                                              RootPhysicalPage,
#endif
                                              PfnPpPage,
                                              TRUE);

#if (_MI_PAGING_LEVELS >= 4)
                        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryParentEntries);
#endif
                        //
                        // Map the new page directory page into the system
                        // portion of the address space.  Note that hyperspace
                        // cannot be used as other operations (allocating
                        // nonpaged pool at DPC level) could cause the
                        // hyperspace page being used to be reused.
                        //

                        MdlDirPage = PageDirFrameIndex;

                        ASSERT (PdeBase != NULL);

                        PdeBase = (PMMPTE) MiMapSinglePage (PdeBase,
                                                            MdlDirPage,
                                                            MmCached,
                                                            HighPagePriority);

                        PointerNewPde = PdeBase;
                        PdePhysicalPage = PageDirFrameIndex;

                        PfnPdPage = MI_PFN_ELEMENT (PdePhysicalPage);
    
                        UsedPageDirectoryEntries = (PVOID)PfnPdPage;
                    }
                    else {
                        ASSERT (PointerNewPpe->u.Hard.Valid == 1 ||
                                PointerNewPpe->u.Soft.Transition == 1);

                        if (PointerNewPpe->u.Hard.Valid == 1) {
                            PdePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerNewPpe);
                        }
                        else {
                            PdePhysicalPage = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (PointerNewPpe);
                        }

                        //
                        // If we are switching from one page directory frame to
                        // another and the last one is one that we freshly
                        // allocated, the last one's reference count must be
                        // decremented now that we're done with it.
                        //
                        // Note that at least one target PPE has already been
                        // initialized for this codepath to execute.
                        //

                        ASSERT (PageDirFrameIndex == MdlDirPage);

                        if (MdlDirPage != PdePhysicalPage) {
                            ASSERT (MdlDirPage != (PFN_NUMBER)-1);
                            PageDirFrameIndex = PdePhysicalPage;
                            MdlDirPage = PageDirFrameIndex;

                            ASSERT (PdeBase != NULL);
    
                            PdeBase = (PMMPTE) MiMapSinglePage (PdeBase,
                                                                MdlDirPage,
                                                                MmCached,
                                                                HighPagePriority);
    
                            PointerNewPde = PdeBase;

                            PfnPdPage = MI_PFN_ELEMENT (PdePhysicalPage);
        
                            UsedPageDirectoryEntries = (PVOID)PfnPdPage;
                        }
                    }
#endif

                    //
                    // Calculate the address of the PDE in the new process's
                    // page directory page.
                    //

#if defined (_X86PAE_)
                    //
                    // All four PAE page directory frames are mapped virtually
                    // contiguous and so the PpePdeOffset can (and must) be
                    // safely used here.
                    //
                    PpePdeOffset = MiGetPdeIndex(MiGetVirtualAddressMappedByPte(PointerPte));
#else
                    PpePdeOffset = MiGetPdeOffset(MiGetVirtualAddressMappedByPte(PointerPte));
#endif

                    PointerNewPde = &PdeBase[PpePdeOffset];

                    if (PointerNewPde->u.Long == 0) {

                        //
                        // No physical page has been allocated yet, get a page
                        // and map it in as a transition page.  This will
                        // become a page table page for the new process.
                        //

                        ReleasedWorkingSetMutex =
                                MiDoneWithThisPageGetAnother (&PageFrameIndex,
                                                              PointerPde,
                                                              CurrentProcess);

#if (_MI_PAGING_LEVELS >= 3)
                        MI_ZERO_USED_PAGETABLE_ENTRIES (MI_PFN_ELEMENT(PageFrameIndex));
#endif
                        if (ReleasedWorkingSetMutex) {

                            //
                            // Ensure the PDE (and any table above it) are still
                            // resident.
                            //

                            MiMakePdeExistAndMakeValid (PointerPde,
                                                        CurrentProcess,
                                                        MM_NOIRQL);
                        }

                        //
                        // Hand initialize this PFN as normal initialization
                        // would do it for the process whose context we are
                        // attached to.
                        //
                        // The PFN lock must be held while initializing the
                        // frame to prevent those scanning the database for
                        // free frames from taking it after we fill in the
                        // u2 field.
                        //

#if defined (_X86PAE_)
                        PdePhysicalPage = PageDirectoryFrames[MiGetPdPteOffset(MiGetVirtualAddressMappedByPte(PointerPte))];
                        PfnPdPage = MI_PFN_ELEMENT (PdePhysicalPage);
#endif

                        MiBuildForkPageTable (PageFrameIndex,
                                              PointerPde,
                                              PointerNewPde,
                                              PdePhysicalPage,
                                              PfnPdPage,
                                              FALSE);

#if (_MI_PAGING_LEVELS >= 3)
                        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageDirectoryEntries);
#endif

                        //
                        // Map the new page table page into the system portion
                        // of the address space.  Note that hyperspace
                        // cannot be used as other operations (allocating
                        // nonpaged pool at DPC level) could cause the
                        // hyperspace page being used to be reused.
                        //

                        ASSERT (NewPteMappedAddress != NULL);

                        PointerNewPte = (PMMPTE) MiMapSinglePage (NewPteMappedAddress,
                                                                  PageFrameIndex,
                                                                  MmCached,
                                                                  HighPagePriority);
                    
                        ASSERT (PointerNewPte != NULL);
                    }

                    //
                    // Calculate the address of the new PTE to build.
                    // Note that FirstTime could be true, yet the page
                    // table page already built.
                    //

                    PointerNewPte = (PMMPTE)((ULONG_PTR)PAGE_ALIGN(PointerNewPte) |
                                            BYTE_OFFSET (PointerPte));

#if (_MI_PAGING_LEVELS >= 3)
                    UsedPageTableEntries = (PVOID)MI_PFN_ELEMENT((PFN_NUMBER)PointerNewPde->u.Hard.PageFrameNumber);
#else
#if !defined (_X86PAE_)
                    UsedPageTableEntries = (PVOID)&HyperWsl->UsedPageTableEntries
                                                [MiGetPteOffset( PointerPte )];
#else
                    UsedPageTableEntries = (PVOID)&HyperWsl->UsedPageTableEntries
                                                [MiGetPdeIndex(MiGetVirtualAddressMappedByPte(PointerPte))];
#endif
#endif

                }

                //
                // Make the fork prototype PTE location resident.
                //

                if (PAGE_ALIGN (ForkProtoPte) != PAGE_ALIGN (LockedForkPte)) {
                    MiUnlockPagedAddress (LockedForkPte, FALSE);
                    LockedForkPte = ForkProtoPte;
                    MiLockPagedAddress (LockedForkPte);
                }

                MiMakeSystemAddressValid (PointerPte, CurrentProcess);

                PteContents = *PointerPte;

                //
                // Check each PTE.
                //

                if (PteContents.u.Long == 0) {
                    NOTHING;
                }
                else if (PteContents.u.Hard.Valid == 1) {

                    //
                    // Valid.
                    //

                    if (Vad->u.VadFlags.PhysicalMapping == 1) {

                        //
                        // A PTE just went from not present, not transition to
                        // present.  The share count and valid count must be
                        // updated in the new page table page which contains
                        // this PTE.
                        //

                        ASSERT (PageFrameIndex != (PFN_NUMBER)-1);
                        Pfn2 = MI_PFN_ELEMENT (PageFrameIndex);
                        Pfn2->u2.ShareCount += 1;

                        //
                        // Another zeroed PTE is being made non-zero.
                        //

                        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableEntries);

                        PointerPte += 1;
                        PointerNewPte += 1;
                        continue;
                    }

                    Pfn2 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);
                    VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                    WorkingSetIndex = MiLocateWsle (VirtualAddress,
                                                    MmWorkingSetList,
                                                    Pfn2->u1.WsIndex);

                    ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);

                    if (Pfn2->u3.e1.PrototypePte == 1) {

                        //
                        // This is a prototype PTE.  The PFN database does
                        // not contain the contents of this PTE it contains
                        // the contents of the prototype PTE.  This PTE must
                        // be reconstructed to contain a pointer to the
                        // prototype PTE.
                        //
                        // The working set list entry contains information about
                        // how to reconstruct the PTE.
                        //

                        if (MmWsle[WorkingSetIndex].u1.e1.SameProtectAsProto
                                                                        == 0) {

                            //
                            // The protection for the prototype PTE is in the
                            // WSLE.
                            //

                            TempPte.u.Long = 0;
                            TempPte.u.Soft.Protection =
                                MI_GET_PROTECTION_FROM_WSLE(&MmWsle[WorkingSetIndex]);
                            TempPte.u.Soft.PageFileHigh = MI_PTE_LOOKUP_NEEDED;

                        }
                        else {

                            //
                            // The protection is in the prototype PTE.
                            //

                            TempPte.u.Long = MiProtoAddressForPte (
                                                            Pfn2->PteAddress);
                        }

                        TempPte.u.Proto.Prototype = 1;
                        MI_WRITE_INVALID_PTE (PointerNewPte, TempPte);

                        //
                        // A PTE is now non-zero, increment the used page
                        // table entries counter.
                        //

                        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableEntries);

                        //
                        // Check to see if this is a fork prototype PTE,
                        // and if it is increment the reference count
                        // which is in the longword following the PTE.
                        //

                        if (MiLocateCloneAddress (CurrentProcess, (PVOID)Pfn2->PteAddress) !=
                                    NULL) {

                            //
                            // The reference count field, or the prototype PTE
                            // for that matter may not be in the working set.
                            //

                            CloneProto = (PMMCLONE_BLOCK)Pfn2->PteAddress;

                            ASSERT (CloneProto->CloneRefCount >= 1);
                            InterlockedIncrement (&CloneProto->CloneRefCount);

                            if (PAGE_ALIGN (ForkProtoPte) !=
                                                    PAGE_ALIGN (LockedForkPte)) {
                                MiUnlockPagedAddress (LockedForkPte, FALSE);
                                LockedForkPte = ForkProtoPte;
                                MiLockPagedAddress (LockedForkPte);
                            }

                            MiMakeSystemAddressValid (PointerPte,
                                                      CurrentProcess);
                        }
                    }
                    else {

                        //
                        // This is a private page, create a fork prototype PTE
                        // which becomes the "prototype" PTE for this page.
                        // The protection is the same as that in the prototype
                        // PTE so the WSLE does not need to be updated.
                        //

                        MI_MAKE_VALID_PTE_WRITE_COPY (PointerPte);

                        KeFlushSingleTb (VirtualAddress, FALSE);

                        ForkProtoPte->ProtoPte = *PointerPte;
                        ForkProtoPte->CloneRefCount = 2;

                        //
                        // Transform the PFN element to reference this new fork
                        // prototype PTE.
                        //

                        Pfn2->PteAddress = &ForkProtoPte->ProtoPte;
                        Pfn2->u3.e1.PrototypePte = 1;

                        ContainingPte = MiGetPteAddress(&ForkProtoPte->ProtoPte);
                        if (ContainingPte->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
                            if (!NT_SUCCESS(MiCheckPdeForPagedPool (&ForkProtoPte->ProtoPte))) {
#endif
                                KeBugCheckEx (MEMORY_MANAGEMENT,
                                              0x61940, 
                                              (ULONG_PTR)&ForkProtoPte->ProtoPte,
                                              (ULONG_PTR)ContainingPte->u.Long,
                                              (ULONG_PTR)MiGetVirtualAddressMappedByPte(&ForkProtoPte->ProtoPte));
#if (_MI_PAGING_LEVELS < 3)
                            }
#endif
                        }
                        Pfn2->u4.PteFrame = MI_GET_PAGE_FRAME_FROM_PTE (ContainingPte);


                        //
                        // Increment the share count for the page containing the
                        // fork prototype PTEs as we have just placed a valid
                        // PTE into the page.
                        //

                        PfnForkPtePage = MI_PFN_ELEMENT (
                                            ContainingPte->u.Hard.PageFrameNumber );

                        MiUpForkPageShareCount (PfnForkPtePage);

                        //
                        // Change the protection in the PFN database to COPY
                        // on write, if writable.
                        //

                        MI_MAKE_PROTECT_WRITE_COPY (Pfn2->OriginalPte);

                        //
                        // Put the protection into the WSLE and mark the WSLE
                        // to indicate that the protection field for the PTE
                        // is the same as the prototype PTE.
                        //

                        MmWsle[WorkingSetIndex].u1.e1.Protection =
                            MI_GET_PROTECTION_FROM_SOFT_PTE(&Pfn2->OriginalPte);

                        MmWsle[WorkingSetIndex].u1.e1.SameProtectAsProto = 1;

                        TempPte.u.Long = MiProtoAddressForPte (Pfn2->PteAddress);
                        TempPte.u.Proto.Prototype = 1;
                        MI_WRITE_INVALID_PTE (PointerNewPte, TempPte);

                        //
                        // A PTE is now non-zero, increment the used page
                        // table entries counter.
                        //

                        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableEntries);

                        //
                        // One less private page (it's now shared).
                        //

                        CurrentProcess->NumberOfPrivatePages -= 1;

                        ForkProtoPte += 1;
                        NumberOfForkPtes += 1;
                    }
                }
                else if (PteContents.u.Soft.Prototype == 1) {

                    //
                    // Prototype PTE, check to see if this is a fork
                    // prototype PTE already.  Note that if COW is set,
                    // the PTE can just be copied (fork compatible format).
                    //

                    MI_WRITE_INVALID_PTE (PointerNewPte, PteContents);

                    //
                    // A PTE is now non-zero, increment the used page
                    // table entries counter.
                    //

                    MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableEntries);

                    //
                    // Check to see if this is a fork prototype PTE,
                    // and if it is increment the reference count
                    // which is in the longword following the PTE.
                    //

                    CloneProto = (PMMCLONE_BLOCK)(ULONG_PTR)MiPteToProto(PointerPte);

                    if (MiLocateCloneAddress (CurrentProcess, (PVOID)CloneProto) != NULL) {

                        //
                        // The reference count field, or the prototype PTE
                        // for that matter may not be in the working set.
                        //

                        ASSERT (CloneProto->CloneRefCount >= 1);
                        InterlockedIncrement (&CloneProto->CloneRefCount);

                        if (PAGE_ALIGN (ForkProtoPte) !=
                                                PAGE_ALIGN (LockedForkPte)) {
                            MiUnlockPagedAddress (LockedForkPte, FALSE);
                            LockedForkPte = ForkProtoPte;
                            MiLockPagedAddress (LockedForkPte);
                        }

                        MiMakeSystemAddressValid (PointerPte, CurrentProcess);
                    }
                }
                else if (PteContents.u.Soft.Transition == 1) {

                    //
                    // Transition.
                    //

                    if (MiHandleForkTransitionPte (PointerPte,
                                                   PointerNewPte,
                                                   ForkProtoPte)) {
                        //
                        // PTE is no longer transition, try again.
                        //

                        continue;
                    }

                    //
                    // A PTE is now non-zero, increment the used page
                    // table entries counter.
                    //

                    MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableEntries);

                    //
                    // One less private page (it's now shared).
                    //

                    CurrentProcess->NumberOfPrivatePages -= 1;

                    ForkProtoPte += 1;
                    NumberOfForkPtes += 1;
                }
                else {

                    //
                    // Page file format (may be demand zero).
                    //

                    if (IS_PTE_NOT_DEMAND_ZERO (PteContents)) {

                        if (PteContents.u.Soft.Protection == MM_DECOMMIT) {

                            //
                            // This is a decommitted PTE, just move it
                            // over to the new process.  Don't increment
                            // the count of private pages.
                            //

                            MI_WRITE_INVALID_PTE (PointerNewPte, PteContents);
                        }
                        else {

                            //
                            // The PTE is not demand zero, move the PTE to
                            // a fork prototype PTE and make this PTE and
                            // the new processes PTE refer to the fork
                            // prototype PTE.
                            //

                            ForkProtoPte->ProtoPte = PteContents;

                            //
                            // Make the protection write-copy if writable.
                            //

                            MI_MAKE_PROTECT_WRITE_COPY (ForkProtoPte->ProtoPte);

                            ForkProtoPte->CloneRefCount = 2;

                            TempPte.u.Long =
                                 MiProtoAddressForPte (&ForkProtoPte->ProtoPte);

                            TempPte.u.Proto.Prototype = 1;

                            MI_WRITE_INVALID_PTE (PointerPte, TempPte);
                            MI_WRITE_INVALID_PTE (PointerNewPte, TempPte);

                            //
                            // One less private page (it's now shared).
                            //

                            CurrentProcess->NumberOfPrivatePages -= 1;

                            ForkProtoPte += 1;
                            NumberOfForkPtes += 1;
                        }
                    }
                    else {

                        //
                        // The page is demand zero, make the new process's
                        // page demand zero.
                        //

                        MI_WRITE_INVALID_PTE (PointerNewPte, PteContents);
                    }

                    //
                    // A PTE is now non-zero, increment the used page
                    // table entries counter.
                    //

                    MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableEntries);
                }

                PointerPte += 1;
                PointerNewPte += 1;

            } while (PointerPte <= LastPte);
AllDone:
            NewVad = NewVad->u1.Parent;
        }
        Vad = MiGetNextVad (Vad);

    } // end while for VADs

    ASSERT (PhysicalViewCount == 0);

    //
    // Unlock paged pool page.
    //

    MiUnlockPagedAddress (LockedForkPte, FALSE);

    //
    // Unmap the PD Page and hyper space page.
    //

#if (_MI_PAGING_LEVELS >= 4)
    MiUnmapSinglePage (PxeBase);
#endif

#if (_MI_PAGING_LEVELS >= 3)
    MiUnmapSinglePage (PpeBase);
#endif

#if !defined (_X86PAE_)
    MiUnmapSinglePage (PdeBase);
#else
    MmUnmapLockedPages (PdeBase, MdlPageDirectory);
#endif

#if (_MI_PAGING_LEVELS < 3)
    MiUnmapSinglePage (HyperBase);
#endif

    MiUnmapSinglePage (NewPteMappedAddress);

    //
    // Make the count of private pages match between the two processes.
    //

    ASSERT ((SPFN_NUMBER)CurrentProcess->NumberOfPrivatePages >= 0);

    ProcessToInitialize->NumberOfPrivatePages =
                                          CurrentProcess->NumberOfPrivatePages;

    ASSERT (NumberOfForkPtes <= CloneDescriptor->NumberOfPtes);

    if (NumberOfForkPtes != 0) {

        //
        // The number of fork PTEs is non-zero, set the values
        // into the structures.
        //

        CloneHeader->NumberOfPtes = NumberOfForkPtes;
        CloneDescriptor->NumberOfReferences = NumberOfForkPtes;
        CloneDescriptor->FinalNumberOfReferences = NumberOfForkPtes;
        CloneDescriptor->NumberOfPtes = NumberOfForkPtes;
    }
    else {

        //
        // There were no fork PTEs created.  Remove the clone descriptor
        // from this process and clean up the related structures.
        //

        MiRemoveClone (CurrentProcess, CloneDescriptor);

        UNLOCK_WS (CurrentProcess);

        ExFreePool (CloneDescriptor->CloneHeader->ClonePtes);

        ExFreePool (CloneDescriptor->CloneHeader);

        //
        // Return the pool for the global structures referenced by the
        // clone descriptor.
        //

        PsReturnProcessPagedPoolQuota (CurrentProcess,
                                       CloneDescriptor->PagedPoolQuotaCharge);

        PsReturnProcessNonPagedPoolQuota (CurrentProcess, sizeof(MMCLONE_HEADER));
        ExFreePool (CloneDescriptor);

        LOCK_WS (CurrentProcess);
    }

    //
    // As we have updated many PTEs to clear dirty bits, flush the
    // TB cache.  Note that this was not done every time we changed
    // a valid PTE so other threads could be modifying the address
    // space without causing copy on writes. Too bad, because an app
    // that is not synchronizing itself is going to have coherency problems
    // anyway.  Note that this cannot cause any system page corruption because
    // the address space mutex was (and is) still held throughout and is
    // not released until after we flush the TB now.
    //

    MiDownShareCountFlushEntireTb (PageFrameIndex);

    PageFrameIndex = (PFN_NUMBER)-1;

    //
    // Copy the clone descriptors from this process to the new process.
    //

    Clone = MiGetFirstClone (CurrentProcess);
    CloneList = &FirstNewClone;
    CloneFailed = FALSE;

    while (Clone != NULL) {

        //
        // Increment the count of processes referencing this clone block.
        //

        ASSERT (Clone->CloneHeader->NumberOfProcessReferences >= 1);

        InterlockedIncrement (&Clone->CloneHeader->NumberOfProcessReferences);

        do {
            NewClone = ExAllocatePoolWithTag (NonPagedPool,
                                              sizeof( MMCLONE_DESCRIPTOR),
                                              'dCmM');

            if (NewClone != NULL) {
                break;
            }

            //
            // There are insufficient resources to continue this operation,
            // however, to properly clean up at this point, all the
            // clone headers must be allocated, so when the cloned process
            // is deleted, the clone headers will be found.  If the pool
            // is not readily available, loop periodically trying for it.
            // Force the clone operation to fail so the pool will soon be
            // released.
            //
            // Release the working set mutex so this process can be trimmed
            // and reacquire after the delay.
            //

            UNLOCK_WS (CurrentProcess);

            CloneFailed = TRUE;
            status = STATUS_INSUFFICIENT_RESOURCES;

            KeDelayExecutionThread (KernelMode,
                                    FALSE,
                                    (PLARGE_INTEGER)&MmShortTime);

            LOCK_WS (CurrentProcess);

        } while (TRUE);

        *NewClone = *Clone;

        //
        // Carefully update the FinalReferenceCount as this forking thread
        // may have begun while a faulting thread is waiting in
        // MiDecrementCloneBlockReference for the clone PTEs to inpage.
        // In this case, the ReferenceCount has been decremented but the
        // FinalReferenceCount hasn't been yet.  When the faulter awakes, he
        // will automatically take care of this process, but we must fix up
        // the child process now.  Otherwise the clone descriptor, clone header
        // and clone PTE pool allocations will leak and so will the charged
        // quota.
        //

        if (NewClone->FinalNumberOfReferences > NewClone->NumberOfReferences) {
            NewClone->FinalNumberOfReferences = NewClone->NumberOfReferences;
        }

        *CloneList = NewClone;

        CloneList = (PMMCLONE_DESCRIPTOR *)&NewClone->u1.Parent;
        Clone = MiGetNextClone (Clone);
    }

    *CloneList = NULL;

#if defined (_MIALT4K_)

    if (CurrentProcess->Wow64Process != NULL) {

        //
        // Copy the alternate table entries now.
        //

        MiDuplicateAlternateTable (CurrentProcess, ProcessToInitialize);
    }

#endif

    ASSERT (CurrentProcess->ForkInProgress == PsGetCurrentThread ());
    CurrentProcess->ForkInProgress = NULL;

    //
    // Release the working set mutex and the address creation mutex from
    // the current process as all the necessary information is now
    // captured.
    //

    UNLOCK_WS (CurrentProcess);

    UNLOCK_ADDRESS_SPACE (CurrentProcess);

    //
    // Attach to the process to initialize and insert the vad and clone
    // descriptors into the tree.
    //

    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
        Attached = FALSE;
    }

    if (PsGetCurrentProcess() != ProcessToInitialize) {
        Attached = TRUE;
        KeStackAttachProcess (&ProcessToInitialize->Pcb, &ApcState);
    }

    CurrentProcess = ProcessToInitialize;

    //
    // We are now in the context of the new process, build the
    // VAD list and the clone list.
    //

    Vad = FirstNewVad;
    VadInsertFailed = FALSE;

    LOCK_WS (CurrentProcess);

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Update the WSLEs for the page directories that were added.
    //

    PointerPpe = MiGetPpeAddress (0);
    PointerPpeLast = MiGetPpeAddress (MM_HIGHEST_USER_ADDRESS);
    PointerPxe = MiGetPxeAddress (0);

    while (PointerPpe <= PointerPpeLast) {

#if (_MI_PAGING_LEVELS >= 4)
        while (PointerPxe->u.Long == 0) {
            PointerPxe += 1;
            PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
            if (PointerPpe > PointerPpeLast) {
                goto WslesFinished;
            }
        }

        //
        // Update the WSLE for this page directory parent page (if we haven't
        // already done it in this loop).
        //

        ASSERT (PointerPxe->u.Hard.Valid == 1);

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPxe);

        PfnPdPage = MI_PFN_ELEMENT (PageFrameIndex);
    
        if (PfnPdPage->u1.Event == NULL) {
    
            do {
                WorkingSetIndex = MiAddValidPageToWorkingSet (PointerPpe,
                                                              PointerPxe,
                                                              PfnPdPage,
                                                              0);

                if (WorkingSetIndex != 0) {
                    break;
                }

                KeDelayExecutionThread (KernelMode,
                                        FALSE,
                                        (PLARGE_INTEGER)&Mm30Milliseconds);

            } while (TRUE);
        }

#endif

        if (PointerPpe->u.Long != 0) {

            ASSERT (PointerPpe->u.Hard.Valid == 1);

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPpe);

            PfnPdPage = MI_PFN_ELEMENT (PageFrameIndex);
        
            ASSERT (PfnPdPage->u1.Event == NULL);
        
            do {
                WorkingSetIndex = MiAddValidPageToWorkingSet (MiGetVirtualAddressMappedByPte (PointerPpe),
                                                              PointerPpe,
                                                              PfnPdPage,
                                                              0);
                if (WorkingSetIndex != 0) {
                    break;
                }

                KeDelayExecutionThread (KernelMode,
                                        FALSE,
                                        (PLARGE_INTEGER)&Mm30Milliseconds);

            } while (TRUE);
        }

        PointerPpe += 1;
#if (_MI_PAGING_LEVELS >= 4)
        if (MiIsPteOnPdeBoundary (PointerPpe)) {
            PointerPxe += 1;
            ASSERT (PointerPxe == MiGetPteAddress (PointerPpe));
        }
#endif
    }

#if (_MI_PAGING_LEVELS >= 4)
WslesFinished:
#endif
#endif

    if (CurrentProcess->PhysicalVadRoot != NULL) {

        RestartKey = NULL;

        do {

            PhysicalView = (PMI_PHYSICAL_VIEW) MiEnumerateGenericTableWithoutSplayingAvl (CurrentProcess->PhysicalVadRoot, &RestartKey);

            if (PhysicalView == NULL) {
                break;
            }

            ASSERT (PhysicalView->u.LongFlags == MI_PHYSICAL_VIEW_PHYS);

            PointerPde = MiGetPdeAddress (MI_VPN_TO_VA (PhysicalView->StartingVpn));
            LastPde = MiGetPdeAddress (MI_VPN_TO_VA (PhysicalView->EndingVpn));

            do {

                //
                // The PDE entry must still be in transition (they all are),
                // but we guaranteed that the page table page itself is still
                // resident because we've incremented the sharecount by the
                // number of physical mappings the page table page contains.
                //
                // The only reason the PDE would be valid here is if we've
                // already processed a different physical view that shares
                // the same page table page.
                //
                // Increment it once more here (to account for itself), make
                // the PDE valid and insert it into the working set list.
                //

                if (PointerPde->u.Hard.Valid == 0) {

                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
    
                    Pfn2 = MI_PFN_ELEMENT (PageFrameIndex);
    
                    //
                    // There can be no I/O in progress on this page, it cannot
                    // be trimmed and no one else can be accessing it, so the
                    // PFN lock is not needed to increment the share count here.
                    //
    
                    Pfn2->u2.ShareCount += 1;
    
                    MI_MAKE_TRANSITION_PTE_VALID (TempPte, PointerPde);
                    MI_SET_PTE_DIRTY (TempPte);
                    MI_WRITE_VALID_PTE (PointerPde, TempPte);
    
                    ASSERT (Pfn2->u1.Event == NULL);
                
                    do {
                        WorkingSetIndex = MiAddValidPageToWorkingSet (MiGetVirtualAddressMappedByPte (PointerPde),
                                                                      PointerPde,
                                                                      Pfn2,
                                                                      0);
                        if (WorkingSetIndex != 0) {
                            break;
                        }
        
                        KeDelayExecutionThread (KernelMode,
                                                FALSE,
                                                (PLARGE_INTEGER)&Mm30Milliseconds);
        
                    } while (TRUE);
                }

                PointerPde += 1;

            } while (PointerPde <= LastPde);

        } while (TRUE);
    }

    while (Vad != NULL) {

        NextVad = Vad->u1.Parent;

        if (VadInsertFailed) {
            Vad->u.VadFlags.CommitCharge = MM_MAX_COMMIT;
        }

        status = MiInsertVad (Vad);

        if (!NT_SUCCESS(status)) {

            //
            // Charging quota for the VAD failed, set the
            // remaining quota fields in this VAD and all
            // subsequent VADs to zero so the VADs can be
            // inserted and later deleted.
            //

            VadInsertFailed = TRUE;

            //
            // Do the loop again for this VAD.
            //

            continue;
        }

        //
        // Update the current virtual size.
        //

        CurrentProcess->VirtualSize += PAGE_SIZE +
                            ((Vad->EndingVpn - Vad->StartingVpn) >> PAGE_SHIFT);

        Vad = NextVad;
    }

    UNLOCK_WS (CurrentProcess);

    //
    // Update the peak virtual size.
    //

    CurrentProcess->PeakVirtualSize = CurrentProcess->VirtualSize;

    CurrentProcess->CloneRoot = TargetCloneRoot;

    Clone = FirstNewClone;
    TotalPagedPoolCharge = 0;
    TotalNonPagedPoolCharge = 0;

    while (Clone != NULL) {

        NextClone = (PMMCLONE_DESCRIPTOR) Clone->u1.Parent;
        MiInsertClone (CurrentProcess, Clone);

        //
        // Calculate the paged pool and non-paged pool to charge for these
        // operations.
        //

        TotalPagedPoolCharge += Clone->PagedPoolQuotaCharge;
        TotalNonPagedPoolCharge += sizeof(MMCLONE_HEADER);

        Clone = NextClone;
    }

    if (CloneFailed || VadInsertFailed) {

        PS_SET_BITS (&CurrentProcess->Flags, PS_PROCESS_FLAGS_FORK_FAILED);

        if (Attached) {
            KeUnstackDetachProcess (&ApcState);
        }

        return status;
    }

    status = PsChargeProcessPagedPoolQuota (CurrentProcess,
                                            TotalPagedPoolCharge);

    if (!NT_SUCCESS(status)) {

        PS_SET_BITS (&CurrentProcess->Flags, PS_PROCESS_FLAGS_FORK_FAILED);

        if (Attached) {
            KeUnstackDetachProcess (&ApcState);
        }
        return status;
    }

    status = PsChargeProcessNonPagedPoolQuota (CurrentProcess,
                                               TotalNonPagedPoolCharge);

    if (!NT_SUCCESS(status)) {

        PsReturnProcessPagedPoolQuota (CurrentProcess, TotalPagedPoolCharge);

        PS_SET_BITS (&CurrentProcess->Flags, PS_PROCESS_FLAGS_FORK_FAILED);

        if (Attached) {
            KeUnstackDetachProcess (&ApcState);
        }
        return status;
    }

    ASSERT ((ProcessToClone->Flags & PS_PROCESS_FLAGS_FORK_FAILED) == 0);
    ASSERT ((CurrentProcess->Flags & PS_PROCESS_FLAGS_FORK_FAILED) == 0);

    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
    }

    return STATUS_SUCCESS;

    //
    // Error returns.
    //

ErrorReturn3:

#if (_MI_PAGING_LEVELS >= 4)
        if (PxeBase != NULL) {
            MiUnmapSinglePage (PxeBase);
        }
#endif

#if (_MI_PAGING_LEVELS >= 3)
        if (PpeBase != NULL) {
            MiUnmapSinglePage (PpeBase);
        }
        if (PdeBase != NULL) {
            MiUnmapSinglePage (PdeBase);
        }
#else
        if (HyperBase != NULL) {
            MiUnmapSinglePage (HyperBase);
        }
        if (PdeBase != NULL) {
#if !defined (_X86PAE_)
            MiUnmapSinglePage (PdeBase);
#else
            MmUnmapLockedPages (PdeBase, MdlPageDirectory);
#endif
        }

#endif

ErrorReturn2:
        CurrentProcess->ForkInProgress = NULL;
        UNLOCK_WS (CurrentProcess);

        if (ChargedClonePoolQuota == TRUE) {
            PsReturnProcessPagedPoolQuota (CurrentProcess, sizeof(MMCLONE_BLOCK) *
                                           NumberOfPrivatePages);
            PsReturnProcessNonPagedPoolQuota (CurrentProcess, sizeof(MMCLONE_HEADER));
        }

        PhysicalView = PhysicalViewList;
        while (PhysicalView != NULL) {
            NextPhysicalView = (PMI_PHYSICAL_VIEW) PhysicalView->u1.Parent;
            ExFreePool (PhysicalView);
            PhysicalView = NextPhysicalView;
        }

        NewVad = FirstNewVad;
        while (NewVad != NULL) {
            Vad = NewVad->u1.Parent;
            ExFreePool (NewVad);
            NewVad = Vad;
        }

        if (CloneDescriptor != NULL) {
            ExFreePool (CloneDescriptor);
        }

        if (CloneHeader != NULL) {
            ExFreePool (CloneHeader);
        }

        if (CloneProtos != NULL) {
            ExFreePool (CloneProtos);
        }
ErrorReturn1:
        UNLOCK_ADDRESS_SPACE (CurrentProcess);
        if (TargetCloneRoot != NULL) {
            ExFreePool (TargetCloneRoot);
        }
        ASSERT ((CurrentProcess->Flags & PS_PROCESS_FLAGS_FORK_FAILED) == 0);
        if (Attached) {
            KeUnstackDetachProcess (&ApcState);
        }
        return status;
}

ULONG
MiDecrementCloneBlockReference (
    IN PMMCLONE_DESCRIPTOR CloneDescriptor,
    IN PMMCLONE_BLOCK CloneBlock,
    IN PEPROCESS CurrentProcess,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine decrements the reference count field of a "fork prototype
    PTE" (clone-block).  If the reference count becomes zero, the reference
    count for the clone-descriptor is decremented and if that becomes zero,
    it is deallocated and the number of processes count for the clone header is
    decremented.  If the number of processes count becomes zero, the clone
    header is deallocated.

Arguments:

    CloneDescriptor - Supplies the clone descriptor which describes the
                      clone block.

    CloneBlock - Supplies the clone block to decrement the reference count of.

    CurrentProcess - Supplies the current process.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at.

Return Value:

    TRUE if the working set mutex was released, FALSE if it was not.

Environment:

    Kernel mode, APCs disabled, address creation mutex, working set mutex
    and PFN lock held.

--*/

{
    PMMCLONE_HEADER CloneHeader;
    ULONG MutexReleased;
    MMPTE CloneContents;
    PMMPFN Pfn3;
    PMMPFN Pfn4;
    LONG NewCount;
    LOGICAL WsHeldSafe;

    ASSERT (CurrentProcess == PsGetCurrentProcess ());

    MutexReleased = FALSE;

    //
    // Note carefully : the clone descriptor count is decremented *BEFORE*
    // dereferencing the pagable clone PTEs.  This is because the working
    // set mutex is released and reacquired if the clone PTEs need to be made
    // resident for the dereference.  And this opens a window where a fork
    // could begin.  This thread will wait for the fork to finish, but the
    // fork will copy the clone descriptors (including this one) and get a
    // stale descriptor reference count (too high by one) as our decrement
    // will only occur in our descriptor and not the forked one.
    //
    // Decrementing the clone descriptor count *BEFORE* potentially
    // releasing the working set mutex solves this entire problem.
    //
    // Note that after the decrement, the clone descriptor can
    // only be referenced here if the count dropped to exactly zero.  (If it
    // was nonzero, some other thread may drive it to zero and free it in the
    // gap where we release locks to inpage the clone block).
    //

    CloneDescriptor->NumberOfReferences -= 1;

    ASSERT (CloneDescriptor->NumberOfReferences >= 0);

    if (CloneDescriptor->NumberOfReferences == 0) {

        //
        // There are no longer any PTEs in this process which refer
        // to the fork prototype PTEs for this clone descriptor.
        // Remove the CloneDescriptor now so a fork won't see it either.
        //

        UNLOCK_PFN (OldIrql);

        //
        // MiRemoveClone and its callees are pagable so release the PFN
        // lock now.
        //

        MiRemoveClone (CurrentProcess, CloneDescriptor);
        MutexReleased = TRUE;

        LOCK_PFN (OldIrql);
    }

    //
    // Now process the clone PTE block and any other descriptor cleanup that
    // may be needed.
    //

    MutexReleased = MiMakeSystemAddressValidPfnWs (CloneBlock, CurrentProcess, OldIrql);

    while ((CurrentProcess->ForkInProgress != NULL) &&
           (CurrentProcess->ForkInProgress != PsGetCurrentThread ())) {

        UNLOCK_PFN (OldIrql);

        MiWaitForForkToComplete (CurrentProcess);

        LOCK_PFN (OldIrql);

        MiMakeSystemAddressValidPfnWs (CloneBlock, CurrentProcess, OldIrql);

        MutexReleased = TRUE;
    }

    NewCount = InterlockedDecrement (&CloneBlock->CloneRefCount);

    ASSERT (NewCount >= 0);

    if (NewCount == 0) {

        CloneContents = CloneBlock->ProtoPte;

        if (CloneContents.u.Long != 0) {

            //
            // The last reference to a fork prototype PTE has been removed.
            // Deallocate any page file space and the transition page, if any.
            //

            ASSERT (CloneContents.u.Hard.Valid == 0);

            //
            // Assert that the PTE is not in subsection format (doesn't point
            // to a file).
            //

            ASSERT (CloneContents.u.Soft.Prototype == 0);

            if (CloneContents.u.Soft.Transition == 1) {

                //
                // Prototype PTE in transition, put the page on the free list.
                //

                Pfn3 = MI_PFN_ELEMENT (CloneContents.u.Trans.PageFrameNumber);
                Pfn4 = MI_PFN_ELEMENT (Pfn3->u4.PteFrame);

                MI_SET_PFN_DELETED (Pfn3);

                MiDecrementShareCount (Pfn4, Pfn3->u4.PteFrame);

                //
                // Check the reference count for the page, if the reference
                // count is zero and the page is not on the freelist,
                // move the page to the free list, if the reference
                // count is not zero, ignore this page.
                // When the reference count goes to zero, it will be placed
                // on the free list.
                //

                if ((Pfn3->u3.e2.ReferenceCount == 0) &&
                    (Pfn3->u3.e1.PageLocation != FreePageList)) {

                    MiUnlinkPageFromList (Pfn3);
                    MiReleasePageFileSpace (Pfn3->OriginalPte);
                    MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&CloneContents));
                }
            }
            else {

                if (IS_PTE_NOT_DEMAND_ZERO (CloneContents)) {
                    MiReleasePageFileSpace (CloneContents);
                }
            }
        }
    }

    //
    // Decrement the final number of references to the clone descriptor.  The
    // decrement of the NumberOfReferences above serves to decide
    // whether to remove the clone descriptor from the process tree so that
    // a wait on a paged out clone PTE block doesn't let a fork copy the
    // descriptor while it is half-changed.
    //
    // The FinalNumberOfReferences serves as a way to distinguish which
    // thread (multiple threads may have collided waiting for the inpage
    // of the clone PTE block) is the last one to wake up from the wait as
    // only this one (it may not be the same one who drove NumberOfReferences
    // to zero) can finally free the pool safely.
    //

    CloneDescriptor->FinalNumberOfReferences -= 1;

    ASSERT (CloneDescriptor->FinalNumberOfReferences >= 0);

    if (CloneDescriptor->FinalNumberOfReferences == 0) {

        UNLOCK_PFN (OldIrql);

        //
        // There are no longer any PTEs in this process which refer
        // to the fork prototype PTEs for this clone descriptor.
        // Decrement the process reference count in the CloneHeader.
        //

        //
        // The working set lock may have been acquired safely or unsafely
        // by our caller.  Handle both cases here and below.
        //

        UNLOCK_WS_REGARDLESS (CurrentProcess, WsHeldSafe);

        MutexReleased = TRUE;

        CloneHeader = CloneDescriptor->CloneHeader;

        NewCount = InterlockedDecrement (&CloneHeader->NumberOfProcessReferences);
        ASSERT (NewCount >= 0);

        //
        // If the count is zero, there are no more processes pointing
        // to this fork header so blow it away.
        //

        if (NewCount == 0) {

#if DBG
            ULONG i;

            CloneBlock = CloneHeader->ClonePtes;
            for (i = 0; i < CloneHeader->NumberOfPtes; i += 1) {
                if (CloneBlock->CloneRefCount != 0) {
                    DbgBreakPoint ();
                }
                CloneBlock += 1;
            }
#endif

            ExFreePool (CloneHeader->ClonePtes);

            ExFreePool (CloneHeader);
        }

        //
        // Return the pool for the global structures referenced by the
        // clone descriptor.
        //

        if ((CurrentProcess->Flags & PS_PROCESS_FLAGS_FORK_FAILED) == 0) {

            //
            // Fork succeeded so return quota that was taken out earlier.
            //

            PsReturnProcessPagedPoolQuota (CurrentProcess,
                                           CloneDescriptor->PagedPoolQuotaCharge);

            PsReturnProcessNonPagedPoolQuota (CurrentProcess,
                                              sizeof(MMCLONE_HEADER));
        }

        ExFreePool (CloneDescriptor);

        //
        // The working set lock may have been acquired safely or unsafely
        // by our caller.  Reacquire it in the same manner our caller did.
        //

        LOCK_WS_REGARDLESS (CurrentProcess, WsHeldSafe);

        LOCK_PFN (OldIrql);
    }

    return MutexReleased;
}

LOGICAL
MiWaitForForkToComplete (
    IN PEPROCESS CurrentProcess
    )

/*++

Routine Description:

    This routine waits for the current process to complete a fork operation.

Arguments:

    CurrentProcess - Supplies the current process value.

Return Value:

    TRUE if the mutex was released and reacquired to wait.  FALSE if not.

Environment:

    Kernel mode, APCs disabled, working set mutex held.

--*/

{
    LOGICAL WsHeldSafe;

    //
    // A fork operation is in progress and the count of clone-blocks
    // and other structures may not be changed.  Release the working
    // set mutex and wait for the address creation mutex which governs the
    // fork operation.
    //

    if (CurrentProcess->ForkInProgress == PsGetCurrentThread()) {
        return FALSE;
    }

    //
    // The working set mutex may have been acquired safely or unsafely
    // by our caller.  Handle both cases here and below, carefully making sure
    // that the OldIrql left in the WS mutex on return is the same as on entry.
    //
    // Note it is ok to drop to PASSIVE or APC level here as long as it is
    // not lower than our caller was at.  Using the WorkingSetMutex whose irql
    // field was initialized by our caller ensures that the proper irql
    // environment is maintained (ie: the caller may be blocking APCs
    // deliberately).
    //

    UNLOCK_WS_REGARDLESS (CurrentProcess, WsHeldSafe);

    //
    // Acquire the address creation mutex as this can only succeed when the
    // forking thread is done in MiCloneProcessAddressSpace.  Thus, acquiring
    // this mutex doesn't stop another thread from starting another fork, but
    // it does serve as a way to know the current fork is done (enough).
    //

    LOCK_ADDRESS_SPACE (CurrentProcess);

    UNLOCK_ADDRESS_SPACE (CurrentProcess);

    //
    // The working set lock may have been acquired safely or unsafely
    // by our caller.  Reacquire it in the same manner our caller did.
    //

    LOCK_WS_REGARDLESS (CurrentProcess, WsHeldSafe);

    return TRUE;
}

VOID
MiUpControlAreaRefs (
    IN PMMVAD Vad
    )
{
    KIRQL OldIrql;
    PCONTROL_AREA ControlArea;
    PSUBSECTION FirstSubsection;
    PSUBSECTION LastSubsection;

    ControlArea = Vad->ControlArea;

    LOCK_PFN (OldIrql);

    ControlArea->NumberOfMappedViews += 1;
    ControlArea->NumberOfUserReferences += 1;

    if ((ControlArea->u.Flags.Image == 0) &&
        (ControlArea->FilePointer != NULL) &&
        (ControlArea->u.Flags.PhysicalMemory == 0)) {

        FirstSubsection = MiLocateSubsection (Vad, Vad->StartingVpn);

        //
        // Note LastSubsection may be NULL for extendable VADs when
        // the EndingVpn is past the end of the section.  In this
        // case, all the subsections can be safely incremented.
        //
        // Note also that the reference must succeed because each
        // subsection's prototype PTEs are guaranteed to already 
        // exist by virtue of the fact that the creating process
        // already has this VAD currently mapping them.
        //

        LastSubsection = MiLocateSubsection (Vad, Vad->EndingVpn);

        while (FirstSubsection != LastSubsection) {
            MiReferenceSubsection ((PMSUBSECTION) FirstSubsection);
            FirstSubsection = FirstSubsection->NextSubsection;
        }

        if (LastSubsection != NULL) {
            MiReferenceSubsection ((PMSUBSECTION) LastSubsection);
        }
    }

    UNLOCK_PFN (OldIrql);
    return;
}


ULONG
MiDoneWithThisPageGetAnother (
    IN PPFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PEPROCESS CurrentProcess
    )

{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    ULONG ReleasedMutex;

    UNREFERENCED_PARAMETER (PointerPde);

    ReleasedMutex = FALSE;

    if (*PageFrameIndex != (PFN_NUMBER)-1) {

        //
        // Decrement the share count of the last page which
        // we operated on.
        //

        Pfn1 = MI_PFN_ELEMENT (*PageFrameIndex);

        LOCK_PFN (OldIrql);

        MiDecrementShareCount (Pfn1, *PageFrameIndex);
    }
    else {
        LOCK_PFN (OldIrql);
    }

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        ReleasedMutex = MiEnsureAvailablePageOrWait (CurrentProcess, NULL, OldIrql);
    }

    *PageFrameIndex = MiRemoveZeroPage (
                   MI_PAGE_COLOR_PTE_PROCESS (PointerPde,
                                              &CurrentProcess->NextPageColor));

    //
    // Temporarily mark the page as bad so that contiguous memory allocators
    // won't steal it when we release the PFN lock below.  This also prevents
    // the MiIdentifyPfn code from trying to identify it as we haven't filled
    // in all the fields yet.
    //

    Pfn1 = MI_PFN_ELEMENT (*PageFrameIndex);
    Pfn1->u3.e1.PageLocation = BadPageList;

    UNLOCK_PFN (OldIrql);
    return ReleasedMutex;
}

ULONG
MiLeaveThisPageGetAnother (
    OUT PPFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PEPROCESS CurrentProcess
    )

{
    KIRQL OldIrql;
    ULONG ReleasedMutex;
    PMMPFN Pfn1;

    UNREFERENCED_PARAMETER (PointerPde);

    ReleasedMutex = FALSE;

    LOCK_PFN (OldIrql);

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        ReleasedMutex = MiEnsureAvailablePageOrWait (CurrentProcess, NULL, OldIrql);
    }

    *PageFrameIndex = MiRemoveZeroPage (
                   MI_PAGE_COLOR_PTE_PROCESS (PointerPde,
                                              &CurrentProcess->NextPageColor));

    //
    // Temporarily mark the page as bad so that contiguous memory allocators
    // won't steal it when we release the PFN lock below.  This also prevents
    // the MiIdentifyPfn code from trying to identify it as we haven't filled
    // in all the fields yet.
    //

    Pfn1 = MI_PFN_ELEMENT (*PageFrameIndex);
    Pfn1->u3.e1.PageLocation = BadPageList;

    UNLOCK_PFN (OldIrql);
    return ReleasedMutex;
}

ULONG
MiHandleForkTransitionPte (
    IN PMMPTE PointerPte,
    IN PMMPTE PointerNewPte,
    IN PMMCLONE_BLOCK ForkProtoPte
    )
{
    KIRQL OldIrql;
    PMMPFN Pfn2;
    PMMPFN Pfn3;
    MMPTE PteContents;
    PMMPTE ContainingPte;
    PFN_NUMBER PageTablePage;
    MMPTE TempPte;
    PMMPFN PfnForkPtePage;

    LOCK_PFN (OldIrql);

    //
    // Now that we have the PFN lock which prevents pages from
    // leaving the transition state, examine the PTE again to
    // ensure that it is still transition.
    //

    PteContents = *PointerPte;

    if ((PteContents.u.Soft.Transition == 0) ||
        (PteContents.u.Soft.Prototype == 1)) {

        //
        // The PTE is no longer in transition... do this loop again.
        //

        UNLOCK_PFN (OldIrql);
        return TRUE;
    }

    //
    // The PTE is still in transition, handle like a valid PTE.
    //

    Pfn2 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

    //
    // Assertion that PTE is not in prototype PTE format.
    //

    ASSERT (Pfn2->u3.e1.PrototypePte != 1);

    //
    // This is a private page in transition state,
    // create a fork prototype PTE
    // which becomes the "prototype" PTE for this page.
    //

    ForkProtoPte->ProtoPte = PteContents;

    //
    // Make the protection write-copy if writable.
    //

    MI_MAKE_PROTECT_WRITE_COPY (ForkProtoPte->ProtoPte);

    ForkProtoPte->CloneRefCount = 2;

    //
    // Transform the PFN element to reference this new fork
    // prototype PTE.
    //

    //
    // Decrement the share count for the page table
    // page which contains the PTE as it is no longer
    // valid or in transition.
    //

    Pfn2->PteAddress = &ForkProtoPte->ProtoPte;
    Pfn2->u3.e1.PrototypePte = 1;

    //
    // Make original PTE copy on write.
    //

    MI_MAKE_PROTECT_WRITE_COPY (Pfn2->OriginalPte);

    ContainingPte = MiGetPteAddress (&ForkProtoPte->ProtoPte);

    if (ContainingPte->u.Hard.Valid == 0) {
#if (_MI_PAGING_LEVELS < 3)
        if (!NT_SUCCESS(MiCheckPdeForPagedPool (&ForkProtoPte->ProtoPte))) {
#endif
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x61940, 
                          (ULONG_PTR)&ForkProtoPte->ProtoPte,
                          (ULONG_PTR)ContainingPte->u.Long,
                          (ULONG_PTR)MiGetVirtualAddressMappedByPte(&ForkProtoPte->ProtoPte));
#if (_MI_PAGING_LEVELS < 3)
        }
#endif
    }

    PageTablePage = Pfn2->u4.PteFrame;

    Pfn2->u4.PteFrame = MI_GET_PAGE_FRAME_FROM_PTE (ContainingPte);

    //
    // Increment the share count for the page containing
    // the fork prototype PTEs as we have just placed
    // a transition PTE into the page.
    //

    PfnForkPtePage = MI_PFN_ELEMENT (ContainingPte->u.Hard.PageFrameNumber);

    PfnForkPtePage->u2.ShareCount += 1;

    TempPte.u.Long = MiProtoAddressForPte (Pfn2->PteAddress);
    TempPte.u.Proto.Prototype = 1;

#if 0

    //
    // Note that NOACCESS transition PTEs must be treated specially -
    // they cannot just be made prototype PTEs with the same protection
    // as the prototype PTE in the clone block - rather they must explicitly
    // put NO_ACCESS into the hardware PTE because faults always treat
    // prototype PTEs as having accessible permissions (because they are
    // typically shared amongst various processes with differing process
    // protections).
    //

    //
    // This cannot be enabled unless fault lookups walk the entire clone
    // tree looking for an address match (it's not sorted like VAD tree).
    //

    if (PteContents.u.Soft.Protection & MM_GUARD_PAGE) {
        TempPte.u.Long = 0;
        TempPte.u.Soft.Protection = PteContents.u.Soft.Protection;
        TempPte.u.Soft.PageFileHigh = MI_PTE_CLONE_LOOKUP_NEEDED;
    }

#endif

    MI_WRITE_INVALID_PTE (PointerPte, TempPte);
    MI_WRITE_INVALID_PTE (PointerNewPte, TempPte);

    //
    // Decrement the share count for the page table
    // page which contains the PTE as it is no longer
    // valid or in transition.
    //

    Pfn3 = MI_PFN_ELEMENT (PageTablePage);

    MiDecrementShareCount (Pfn3, PageTablePage);

    UNLOCK_PFN (OldIrql);
    return FALSE;
}

VOID
MiDownShareCountFlushEntireTb (
    IN PFN_NUMBER PageFrameIndex
    )

{
    PMMPFN Pfn1;
    KIRQL OldIrql;

    if (PageFrameIndex != (PFN_NUMBER)-1) {

        //
        // Decrement the share count of the last page which
        // we operated on.
        //

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        LOCK_PFN (OldIrql);

        MiDecrementShareCount (Pfn1, PageFrameIndex);
    }
    else {
        LOCK_PFN (OldIrql);
    }

    KeFlushProcessTb (FALSE);

    UNLOCK_PFN (OldIrql);
    return;
}

VOID
MiUpForkPageShareCount (
    IN PMMPFN PfnForkPtePage
    )
{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);
    PfnForkPtePage->u2.ShareCount += 1;

    UNLOCK_PFN (OldIrql);
    return;
}

VOID
MiBuildForkPageTable (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PMMPTE PointerNewPde,
    IN PFN_NUMBER PdePhysicalPage,
    IN PMMPFN PfnPdPage,
    IN LOGICAL MakeValid
    )
{
    KIRQL OldIrql;
    PMMPFN Pfn1;
#if (_MI_PAGING_LEVELS >= 3)
    MMPTE TempPpe;
#endif

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    //
    // The PFN lock must be held while initializing the
    // frame to prevent those scanning the database for free
    // frames from taking it after we fill in the u2 field.
    //

    LOCK_PFN (OldIrql);

    Pfn1->OriginalPte = DemandZeroPde;
    Pfn1->u2.ShareCount = 1;
    Pfn1->u3.e2.ReferenceCount = 1;
    Pfn1->PteAddress = PointerPde;
    MI_SET_MODIFIED (Pfn1, 1, 0x10);
    Pfn1->u3.e1.PageLocation = ActiveAndValid;
    Pfn1->u3.e1.CacheAttribute = MiCached;
    Pfn1->u4.PteFrame = PdePhysicalPage;

    //
    // Increment the share count for the page containing
    // this PTE as the PTE is in transition.
    //

    PfnPdPage->u2.ShareCount += 1;

    UNLOCK_PFN (OldIrql);

    if (MakeValid == TRUE) {

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Put the PPE into the valid state as it will point at a page
        // directory page that is valid (not transition) when the fork is
        // complete.  All the page table pages will be in transition, but
        // the page directories cannot be as they contain the PTEs for the
        // page tables.
        //
    
        TempPpe = ValidPdePde;

        MI_MAKE_VALID_PTE (TempPpe,
                           PageFrameIndex,
                           MM_READWRITE,
                           PointerPde);
    
        MI_SET_PTE_DIRTY (TempPpe);

        //
        // Make the PTE owned by user mode.
        //
    
        MI_SET_OWNER_IN_PTE (PointerNewPde, UserMode);

        MI_WRITE_VALID_PTE (PointerNewPde, TempPpe);
#endif
    }
    else {

        //
        // Put the PDE into the transition state as it is not
        // really mapped and decrement share count does not
        // put private pages into transition, only prototypes.
        //
    
        MI_WRITE_INVALID_PTE (PointerNewPde, TransitionPde);

        //
        // Make the PTE owned by user mode.
        //
    
        MI_SET_OWNER_IN_PTE (PointerNewPde, UserMode);

        PointerNewPde->u.Trans.PageFrameNumber = PageFrameIndex;
    }

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\freevm.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   freevm.c

Abstract:

    This module contains the routines which implement the
    NtFreeVirtualMemory service.

Author:

    Lou Perazzoli (loup) 22-May-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#define MEM_CHECK_COMMIT_STATE 0x400000

#define MM_VALID_PTE_SIZE (256)


MMPTE MmDecommittedPte = {MM_DECOMMIT << MM_PROTECT_FIELD_SHIFT};

#if DBG
extern PEPROCESS MmWatchProcess;
#endif // DBG


#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtFreeVirtualMemory)
#pragma alloc_text(PAGE,MiIsEntireRangeCommitted)
#endif

VOID
MiProcessValidPteList (
    IN PMMPTE *PteList,
    IN ULONG Count
    );

ULONG
MiDecommitPages (
    IN PVOID StartingAddress,
    IN PMMPTE EndingPte,
    IN PEPROCESS Process,
    IN PMMVAD_SHORT Vad
    );


NTSTATUS
NtFreeVirtualMemory(
    IN HANDLE ProcessHandle,
    IN OUT PVOID *BaseAddress,
    IN OUT PSIZE_T RegionSize,
    IN ULONG FreeType
     )

/*++

Routine Description:

    This function deletes a region of pages within the virtual address
    space of a subject process.

Arguments:

    ProcessHandle - An open handle to a process object.

    BaseAddress - The base address of the region of pages
                  to be freed. This value is rounded down to the
                  next host page address boundary.

    RegionSize - A pointer to a variable that will receive
                 the actual size in bytes of the freed region of
                 pages. The initial value of this argument is
                 rounded up to the next host page size boundary.

    FreeType - A set of flags that describe the type of
               free that is to be performed for the specified
               region of pages.

       FreeType Flags

        MEM_DECOMMIT - The specified region of pages is to be decommitted.

        MEM_RELEASE - The specified region of pages is to be released.

Return Value:

    NTSTATUS.

--*/

{
    KAPC_STATE ApcState;
    PMMVAD_SHORT Vad;
    PMMVAD_SHORT NewVad;
    PMMVAD PreviousVad;
    PMMVAD NextVad;
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    PVOID StartingAddress;
    PVOID EndingAddress;
    NTSTATUS Status;
    LOGICAL Attached;
    SIZE_T CapturedRegionSize;
    PVOID CapturedBase;
    PMMPTE StartingPte;
    PMMPTE EndingPte;
    SIZE_T OldQuota;
    SIZE_T QuotaCharge;
    SIZE_T CommitReduction;
    ULONG_PTR OldEnd;
    LOGICAL UserPhysicalPages;
#if defined(_MIALT4K_)
    PVOID StartingAddress4k;
    PVOID EndingAddress4k;
    PVOID Wow64Process;
#endif
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    PAGED_CODE();

    //
    // Check to make sure FreeType is good.
    //

    if ((FreeType & ~(MEM_DECOMMIT | MEM_RELEASE)) != 0) {
        return STATUS_INVALID_PARAMETER_4;
    }

    //
    // One of MEM_DECOMMIT or MEM_RELEASE must be specified, but not both.
    //

    if (((FreeType & (MEM_DECOMMIT | MEM_RELEASE)) == 0) ||
        ((FreeType & (MEM_DECOMMIT | MEM_RELEASE)) ==
                            (MEM_DECOMMIT | MEM_RELEASE))) {
        return STATUS_INVALID_PARAMETER_4;
    }
    CurrentThread = PsGetCurrentThread ();

    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

    //
    // Establish an exception handler, probe the specified addresses
    // for write access and capture the initial values.
    //

    try {

        if (PreviousMode != KernelMode) {

            ProbeForWritePointer (BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
        }

        //
        // Capture the base address.
        //

        CapturedBase = *BaseAddress;

        //
        // Capture the region size.
        //

        CapturedRegionSize = *RegionSize;

    } except (ExSystemExceptionFilter()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode();
    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_USER_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER_2;
    }

    if ((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (ULONG_PTR)CapturedBase <
                                                        CapturedRegionSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER_3;

    }

    EndingAddress = (PVOID)(((LONG_PTR)CapturedBase + CapturedRegionSize - 1) |
                        (PAGE_SIZE - 1));

    StartingAddress = PAGE_ALIGN(CapturedBase);

    Attached = FALSE;

    if (ProcessHandle == NtCurrentProcess()) {
        Process = CurrentProcess;
    }
    else {

        //
        // Reference the specified process handle for VM_OPERATION access.
        //

        Status = ObReferenceObjectByHandle ( ProcessHandle,
                                             PROCESS_VM_OPERATION,
                                             PsProcessType,
                                             PreviousMode,
                                             (PVOID *)&Process,
                                             NULL );

        if (!NT_SUCCESS(Status)) {
            return Status;
        }

        //
        // If the specified process is not the current process, attach
        // to the specified process.
        //

        if (CurrentProcess != Process) {
            KeStackAttachProcess (&Process->Pcb, &ApcState);
            Attached = TRUE;
        }
    }

    CommitReduction = 0;

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be inserted and walked.  Block APCs to prevent page faults while
    // we own the working set mutex.
    //

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }

#if defined(_MIALT4K_)

    Wow64Process = Process->Wow64Process;

    //
    // Initializing these is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    StartingAddress4k = NULL;
    EndingAddress4k = NULL;

    if (CapturedRegionSize != 0) {
        
        if (Wow64Process != NULL) {

            //
            // Adjust Starting/EndingAddress for the native page size.
            //
            // StartingAddress: if this happened to be 4k aligned, but not
            // native aligned, then look at the previous 4k page and if it's
            // allocated then align the starting page to the next native
            // page, otherwise align it to the current one.
            //
            // EndingAddress: if this happened to be 4k aligned but not
            // native aligned, then look at the next 4k page and if it's
            // allocated, then make the ending address the previous
            // native page, otherwise make it the current.
            //
            // This is to ensure VADs are not leaked inside
            // the process when releasing partial allocations.
            //
            
            ASSERT (StartingAddress == PAGE_ALIGN(StartingAddress));

            StartingAddress4k = (PVOID)PAGE_4K_ALIGN(CapturedBase);

            if (StartingAddress4k >= MmWorkingSetList->HighestUserAddress) {

                //
                // The caller's address is not in the WOW64 area, pass it
                // through as a native request.
                //

                Wow64Process = NULL;
                goto NativeRequest;
            }

            EndingAddress4k = (PVOID)(((LONG_PTR)CapturedBase + CapturedRegionSize - 1) |
                                (PAGE_4K - 1));
    
            if (BYTE_OFFSET (StartingAddress4k) != 0) {

                if (MiArePreceding4kPagesAllocated (StartingAddress4k) == TRUE) {
                    StartingAddress = PAGE_NEXT_ALIGN (StartingAddress4k);
                }
            }

            if (EndingAddress4k >= MmWorkingSetList->HighestUserAddress) {

                //
                // The caller's address is not in the WOW64 area, pass it
                // through as a native request.
                //

                Wow64Process = NULL;
                goto NativeRequest;
            }

            if (BYTE_OFFSET (EndingAddress4k) != PAGE_SIZE - 1) {

                if (MiAreFollowing4kPagesAllocated (EndingAddress4k) == TRUE) {
                    EndingAddress = (PVOID)((ULONG_PTR)PAGE_ALIGN(EndingAddress4k) - 1);
                }
            }

            if (StartingAddress > EndingAddress) {

                //
                // There is no need to free native pages.
                //

                Vad = NULL;
                goto FreeAltPages;
            }
        }
    }
        
NativeRequest:

#endif

    Vad = (PMMVAD_SHORT)MiLocateAddress (StartingAddress);

    if (Vad == NULL) {

        //
        // No Virtual Address Descriptor located for Base Address.
        //

        Status = STATUS_MEMORY_NOT_ALLOCATED;
        goto ErrorReturn;
    }

    //
    // Found the associated Virtual Address Descriptor.
    //

    if (Vad->EndingVpn < MI_VA_TO_VPN (EndingAddress)) {

        //
        // The entire range to delete is not contained within a single
        // virtual address descriptor.  Return an error.
        //

        Status = STATUS_UNABLE_TO_FREE_VM;
        goto ErrorReturn;
    }

    //
    // Check to ensure this Vad is deletable.  Delete is required
    // for both decommit and release.
    //

    if ((Vad->u.VadFlags.PrivateMemory == 0) ||
        (Vad->u.VadFlags.PhysicalMapping == 1)) {
        Status = STATUS_UNABLE_TO_DELETE_SECTION;
        goto ErrorReturn;
    }

    if (Vad->u.VadFlags.NoChange == 1) {

        //
        // An attempt is being made to delete a secured VAD, check
        // to see if this deletion is allowed.
        //

        if (FreeType & MEM_RELEASE) {

            //
            // Specify the whole range, this solves the problem with
            // splitting the VAD and trying to decide where the various
            // secure ranges need to go.
            //

            Status = MiCheckSecuredVad ((PMMVAD)Vad,
                                        MI_VPN_TO_VA (Vad->StartingVpn),
                        ((Vad->EndingVpn - Vad->StartingVpn) << PAGE_SHIFT) +
                                (PAGE_SIZE - 1),
                                        MM_SECURE_DELETE_CHECK);

        }
        else {
            Status = MiCheckSecuredVad ((PMMVAD)Vad,
                                        CapturedBase,
                                        CapturedRegionSize,
                                        MM_SECURE_DELETE_CHECK);
        }
        if (!NT_SUCCESS (Status)) {
            goto ErrorReturn;
        }
    }

    UserPhysicalPages = FALSE;

    PreviousVad = MiGetPreviousVad (Vad);
    NextVad = MiGetNextVad (Vad);
    if (FreeType & MEM_RELEASE) {

        //
        // *****************************************************************
        // MEM_RELEASE was specified.
        // *****************************************************************
        //

        //
        // The descriptor for the address range is deletable.  Remove or split
        // the descriptor.
        //

        //
        // If the region size is zero, remove the whole VAD.
        //

        if (CapturedRegionSize == 0) {

            //
            // If the region size is specified as 0, the base address
            // must be the starting address for the region.
            //

            if (MI_VA_TO_VPN (CapturedBase) != Vad->StartingVpn) {
                Status = STATUS_FREE_VM_NOT_AT_BASE;
                goto ErrorReturn;
            }

            //
            // This Virtual Address Descriptor has been deleted.
            //

            StartingAddress = MI_VPN_TO_VA (Vad->StartingVpn);
            EndingAddress = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);

#if defined(_MIALT4K_)
            StartingAddress4k = StartingAddress;
            EndingAddress4k  = EndingAddress;
#endif

            //
            // Free all the physical pages that this VAD might be mapping.
            // Since only the AWE lock synchronizes the remap API, carefully
            // remove this VAD from the list first.
            //

            LOCK_WS_UNSAFE (Process);

            if (Vad->u.VadFlags.LargePages == 1) {
                MiAweViewRemover (Process, (PMMVAD)Vad);
                MiFreeLargePages (MI_VPN_TO_VA (Vad->StartingVpn),
                                  MI_VPN_TO_VA_ENDING (Vad->EndingVpn));
            }
            else if (Vad->u.VadFlags.UserPhysicalPages == 1) {
                MiAweViewRemover (Process, (PMMVAD)Vad);
                MiRemoveUserPhysicalPagesVad (Vad);
                UserPhysicalPages = TRUE;
            }
            else if (Vad->u.VadFlags.WriteWatch == 1) {
                MiPhysicalViewRemover (Process, (PMMVAD)Vad);
            }

            MiRemoveVad ((PMMVAD)Vad);

            //
            // Free the VAD pool after releasing our mutexes
            // to reduce contention.
            //

        }
        else {

            //
            // Region's size was not specified as zero, delete the
            // whole VAD or split the VAD.
            //

            if (MI_VA_TO_VPN (StartingAddress) == Vad->StartingVpn) {
                if (MI_VA_TO_VPN (EndingAddress) == Vad->EndingVpn) {

                    //
                    // This Virtual Address Descriptor has been deleted.
                    //

                    //
                    // Free all the physical pages that this VAD might be
                    // mapping.  Since only the AWE lock synchronizes the
                    // remap API, carefully remove this VAD from the list first.
                    //
        
                    LOCK_WS_UNSAFE (Process);

                    if (Vad->u.VadFlags.LargePages == 1) {
                        MiAweViewRemover (Process, (PMMVAD)Vad);
                        MiFreeLargePages (MI_VPN_TO_VA (Vad->StartingVpn),
                                          MI_VPN_TO_VA_ENDING (Vad->EndingVpn));
                    }
                    else if (Vad->u.VadFlags.UserPhysicalPages == 1) {
                        MiAweViewRemover (Process, (PMMVAD)Vad);
                        MiRemoveUserPhysicalPagesVad (Vad);
                        UserPhysicalPages = TRUE;
                    }
                    else if (Vad->u.VadFlags.WriteWatch == 1) {
                        MiPhysicalViewRemover (Process, (PMMVAD)Vad);
                    }

                    MiRemoveVad ((PMMVAD)Vad);

                    //
                    // Free the VAD pool after releasing our mutexes
                    // to reduce contention.
                    //

                }
                else {

                    if ((Vad->u.VadFlags.UserPhysicalPages == 1) ||
                        (Vad->u.VadFlags.LargePages == 1) ||
                        (Vad->u.VadFlags.WriteWatch == 1)) {

                        //
                        // Splitting or chopping a physical VAD, large page VAD
                        // or a write-watch VAD is not allowed.
                        //

                        Status = STATUS_FREE_VM_NOT_AT_BASE;
                        goto ErrorReturn;
                    }

                    LOCK_WS_UNSAFE (Process);

                    //
                    // This Virtual Address Descriptor has a new starting
                    // address.
                    //

                    CommitReduction = MiCalculatePageCommitment (
                                                            StartingAddress,
                                                            EndingAddress,
                                                            (PMMVAD)Vad,
                                                            Process);

                    Vad->StartingVpn = MI_VA_TO_VPN ((PCHAR)EndingAddress + 1);
                    Vad->u.VadFlags.CommitCharge -= CommitReduction;
                    ASSERT ((SSIZE_T)Vad->u.VadFlags.CommitCharge >= 0);
                    NextVad = (PMMVAD)Vad;
                    Vad = NULL;
                }
            }
            else {

                if ((Vad->u.VadFlags.UserPhysicalPages == 1) ||
                    (Vad->u.VadFlags.LargePages == 1) ||
                    (Vad->u.VadFlags.WriteWatch == 1)) {

                    //
                    // Splitting or chopping a physical VAD, large page VAD
                    // or a write-watch VAD is not allowed.
                    //

                    Status = STATUS_FREE_VM_NOT_AT_BASE;
                    goto ErrorReturn;
                }

                //
                // Starting address is greater than start of VAD.
                //

                if (MI_VA_TO_VPN (EndingAddress) == Vad->EndingVpn) {

                    //
                    // Change the ending address of the VAD.
                    //

                    LOCK_WS_UNSAFE (Process);

                    CommitReduction = MiCalculatePageCommitment (
                                                            StartingAddress,
                                                            EndingAddress,
                                                            (PMMVAD)Vad,
                                                            Process);

                    Vad->u.VadFlags.CommitCharge -= CommitReduction;

                    Vad->EndingVpn = MI_VA_TO_VPN ((PCHAR)StartingAddress - 1);
                    PreviousVad = (PMMVAD)Vad;
                }
                else {

                    //
                    // Split this VAD as the address range is within the VAD.
                    //

                    NewVad = ExAllocatePoolWithTag (NonPagedPool,
                                                    sizeof(MMVAD_SHORT),
                                                    'FdaV');

                    if (NewVad == NULL) {
                        Status = STATUS_INSUFFICIENT_RESOURCES;
                        goto ErrorReturn;
                    }

                    *NewVad = *Vad;

                    NewVad->StartingVpn = MI_VA_TO_VPN ((PCHAR)EndingAddress + 1);
                    //
                    // Set the commit charge to zero so MiInsertVad will
                    // not charge commitment for splitting the VAD.
                    //

                    NewVad->u.VadFlags.CommitCharge = 0;

                    OldEnd = Vad->EndingVpn;

                    LOCK_WS_UNSAFE (Process);

                    CommitReduction = MiCalculatePageCommitment (
                                                            StartingAddress,
                                                            EndingAddress,
                                                            (PMMVAD)Vad,
                                                            Process);

                    OldQuota = Vad->u.VadFlags.CommitCharge - CommitReduction;

                    Vad->EndingVpn = MI_VA_TO_VPN ((PCHAR)StartingAddress - 1);

                    //
                    // Insert the VAD, this could fail due to quota charges.
                    //

                    Status = MiInsertVad ((PMMVAD)NewVad);

                    if (!NT_SUCCESS(Status)) {

                        //
                        // Inserting the Vad failed, reset the original
                        // Vad, free the new Vad and return an error.
                        //

                        Vad->EndingVpn = OldEnd;
                        UNLOCK_WS_AND_ADDRESS_SPACE (Process);
                        ExFreePool (NewVad);
                        goto ErrorReturn2;
                    }

                    //
                    // As we have split the original VAD into 2 separate VADs
                    // there is no way of knowing what the commit charge
                    // is for each VAD.  Calculate the charge and reset
                    // each VAD.  Note that we also use the previous value
                    // to make sure the books stay balanced.
                    //

                    QuotaCharge = MiCalculatePageCommitment (MI_VPN_TO_VA (Vad->StartingVpn),
                                                             (PCHAR)StartingAddress - 1,
                                                             (PMMVAD)Vad,
                                                             Process);

                    Vad->u.VadFlags.CommitCharge = QuotaCharge;

                    //
                    // Give the remaining charge to the new VAD.
                    //

                    NewVad->u.VadFlags.CommitCharge = OldQuota - QuotaCharge;
                    PreviousVad = (PMMVAD)Vad;
                    NextVad = (PMMVAD)NewVad;
                }
                Vad = NULL;
            }
        }

        //
        // Return commitment for page table pages if possible.
        //

        MiReturnPageTablePageCommitment (StartingAddress,
                                         EndingAddress,
                                         Process,
                                         PreviousVad,
                                         NextVad);

        if (UserPhysicalPages == TRUE) {
            MiDeletePageTablesForPhysicalRange (StartingAddress, EndingAddress);
        }
        else {

            MiDeleteVirtualAddresses (StartingAddress,
                                      EndingAddress,
                                      NULL);
        }

        UNLOCK_WS_UNSAFE (Process);

        CapturedRegionSize = 1 + (PCHAR)EndingAddress - (PCHAR)StartingAddress;

        //
        // Update the virtual size in the process header.
        //

        Process->VirtualSize -= CapturedRegionSize;

#if defined(_MIALT4K_)
        if (Wow64Process != NULL) {
            goto FreeAltPages;
        }
#endif

        Process->CommitCharge -= CommitReduction;

        UNLOCK_ADDRESS_SPACE (Process);

        if (CommitReduction != 0) {

            MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - CommitReduction);

            ASSERT (Vad == NULL);
            PsReturnProcessPageFileQuota (Process, CommitReduction);
            MiReturnCommitment (CommitReduction);

            if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
                PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES, -(SSIZE_T)CommitReduction);
            }

            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_NTFREEVM1, CommitReduction);
        }
        else if (Vad != NULL) {
            ExFreePool (Vad);
        }

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }

        if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (Process);
        }
        //
        // Establish an exception handler and write the size and base
        // address.
        //

        try {

            *RegionSize = CapturedRegionSize;
            *BaseAddress = StartingAddress;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // An exception occurred, don't take any action (just handle
            // the exception and return success.

        }

        return STATUS_SUCCESS;
    }

    //
    // **************************************************************
    //
    // MEM_DECOMMIT was specified.
    //
    // **************************************************************
    //

    if (Vad->u.VadFlags.UserPhysicalPages == 1) {

        //
        // Pages from a physical VAD must be released via
        // NtFreeUserPhysicalPages, not this routine.
        //

        Status = STATUS_MEMORY_NOT_ALLOCATED;
        goto ErrorReturn;
    }

    if (Vad->u.VadFlags.LargePages == 1) {

        //
        // Pages from a large page VAD must be released -
        // they cannot be merely decommitted.
        //

        Status = STATUS_MEMORY_NOT_ALLOCATED;
        goto ErrorReturn;
    }

    //
    // Check to ensure the complete range of pages is already committed.
    //

    if (CapturedRegionSize == 0) {

        if (MI_VA_TO_VPN (CapturedBase) != Vad->StartingVpn) {
            Status = STATUS_FREE_VM_NOT_AT_BASE;
            goto ErrorReturn;
        }
        EndingAddress = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);

#if defined(_MIALT4K_)
        StartingAddress4k = StartingAddress;
        EndingAddress4k  = EndingAddress;
#endif
    }

#if 0
    if (FreeType & MEM_CHECK_COMMIT_STATE) {
        if ( !MiIsEntireRangeCommitted(StartingAddress,
                                       EndingAddress,
                                       Vad,
                                       Process)) {

            //
            // The entire range to be decommitted is not committed,
            // return an error.
            //

            Status = STATUS_UNABLE_TO_DECOMMIT_VM;
            goto ErrorReturn;
        }
    }
#endif //0

    //
    // The address range is entirely committed, decommit it now.
    //

    //
    // Calculate the initial quotas and commit charges for this VAD.
    //

    StartingPte = MiGetPteAddress (StartingAddress);
    EndingPte = MiGetPteAddress (EndingAddress);

    CommitReduction = 1 + EndingPte - StartingPte;

    LOCK_WS_UNSAFE (Process);

    //
    // Check to see if the entire range can be decommitted by
    // just updating the virtual address descriptor.
    //

    CommitReduction -= MiDecommitPages (StartingAddress,
                                        EndingPte,
                                        Process,
                                        Vad);

    UNLOCK_WS_UNSAFE (Process);

    //
    // Adjust the quota charges.
    //

    ASSERT ((LONG)CommitReduction >= 0);

    Vad->u.VadFlags.CommitCharge -= CommitReduction;
    ASSERT ((LONG)Vad->u.VadFlags.CommitCharge >= 0);
    Vad = NULL;

#if defined(_MIALT4K_)

FreeAltPages:

    if (Wow64Process != NULL) {

        if (FreeType & MEM_RELEASE) {
            MiReleaseFor4kPage (StartingAddress4k, 
                                EndingAddress4k, 
                                Process);
        }
        else {
            MiDecommitFor4kPage (StartingAddress4k, 
                                 EndingAddress4k, 
                                 Process);
        }

        StartingAddress = StartingAddress4k;
        EndingAddress = EndingAddress4k;
    }

#endif

    Process->CommitCharge -= CommitReduction;

    UNLOCK_ADDRESS_SPACE (Process);

    if (CommitReduction != 0) {

        MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - CommitReduction);

        PsReturnProcessPageFileQuota (Process, CommitReduction);
        MiReturnCommitment (CommitReduction);

        if (Process->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
            PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES, -(SSIZE_T)CommitReduction);
        }

        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_NTFREEVM2, CommitReduction);
    }
    else if (Vad != NULL) {
        ExFreePool (Vad);
    }

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }

    if (ProcessHandle != NtCurrentProcess()) {
        ObDereferenceObject (Process);
    }

    //
    // Establish an exception handler and write the size and base address.
    //

    try {

        *RegionSize = 1 + (PCHAR)EndingAddress - (PCHAR)StartingAddress;
        *BaseAddress = StartingAddress;

    } except (EXCEPTION_EXECUTE_HANDLER) {
        NOTHING;
    }

    return STATUS_SUCCESS;

ErrorReturn:
       UNLOCK_ADDRESS_SPACE (Process);

ErrorReturn2:
       if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
       }

       if (ProcessHandle != NtCurrentProcess()) {
            ObDereferenceObject (Process);
       }
       return Status;
}

ULONG
MiIsEntireRangeCommitted (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine examines the range of pages from the starting address
    up to and including the ending address and returns TRUE if every
    page in the range is committed, FALSE otherwise.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

    Vad - Supplies the virtual address descriptor which describes the range.

    Process - Supplies the current process.

Return Value:

    TRUE if the entire range is committed.
    FALSE if any page within the range is not committed.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    ULONG FirstTime;
    ULONG Waited;
    PVOID Va;

    PAGED_CODE();

    FirstTime = TRUE;

    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    //
    // Set the Va to the starting address + 8, this solves problems
    // associated with address 0 (NULL) being used as a valid virtual
    // address and NULL in the VAD commitment field indicating no pages
    // are committed.
    //

    Va = (PVOID)((PCHAR)StartingAddress + 8);

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary(PointerPte) || (FirstTime)) {

            //
            // This may be a PXE/PPE/PDE boundary, check to see if all the
            // PXE/PPE/PDE pages exist.
            //

            FirstTime = FALSE;
            PointerPde = MiGetPteAddress (PointerPte);
            PointerPpe = MiGetPteAddress (PointerPde);
            PointerPxe = MiGetPteAddress (PointerPpe);

            do {

#if (_MI_PAGING_LEVELS >= 4)
retry:
#endif

                while (!MiDoesPxeExistAndMakeValid (PointerPxe, Process, MM_NOIRQL, &Waited)) {

                    //
                    // No PPE exists for the starting address, check the VAD
                    // to see if the pages are committed.
                    //

                    PointerPxe += 1;

                    PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    Va = MiGetVirtualAddressMappedByPte (PointerPte);

                    if (PointerPte > LastPte) {

                        //
                        // Make sure the entire range is committed.
                        //

                        if (Vad->u.VadFlags.MemCommit == 0) {

                            //
                            // The entire range to be decommitted is not
                            // committed, return an error.
                            //

                            return FALSE;
                        }
                        return TRUE;
                    }

                    //
                    // Make sure the range thus far is committed.
                    //

                    if (Vad->u.VadFlags.MemCommit == 0) {

                        //
                        // The entire range to be decommitted is not committed,
                        // return an error.
                        //

                        return FALSE;
                    }
                }

                while (!MiDoesPpeExistAndMakeValid (PointerPpe, Process, MM_NOIRQL, &Waited)) {

                    //
                    // No PDE exists for the starting address, check the VAD
                    // to see if the pages are committed.
                    //

                    PointerPpe += 1;
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    Va = MiGetVirtualAddressMappedByPte (PointerPte);

                    if (PointerPte > LastPte) {

                        //
                        // Make sure the entire range is committed.
                        //

                        if (Vad->u.VadFlags.MemCommit == 0) {

                            //
                            // The entire range to be decommitted is not
                            // committed, return an error.
                            //

                            return FALSE;
                        }
                        return TRUE;
                    }

                    //
                    // Make sure the range thus far is committed.
                    //

                    if (Vad->u.VadFlags.MemCommit == 0) {

                        //
                        // The entire range to be decommitted is not committed,
                        // return an error.
                        //

                        return FALSE;
                    }
#if (_MI_PAGING_LEVELS >= 4)
                    if (MiIsPteOnPdeBoundary (PointerPpe)) {
                        PointerPxe = MiGetPteAddress (PointerPpe);
                        goto retry;
                    }
#endif
                }

                Waited = 0;

                while (!MiDoesPdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL, &Waited)) {

                    //
                    // No PDE exists for the starting address, check the VAD
                    // to see if the pages are committed.
                    //

                    PointerPde += 1;
                    PointerPpe = MiGetPteAddress (PointerPde);
                    PointerPxe = MiGetPdeAddress (PointerPde);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    Va = MiGetVirtualAddressMappedByPte (PointerPte);

                    if (PointerPte > LastPte) {

                        //
                        // Make sure the entire range is committed.
                        //

                        if (Vad->u.VadFlags.MemCommit == 0) {

                            //
                            // The entire range to be decommitted is not committed,
                            // return an error.
                            //

                            return FALSE;
                        }
                        return TRUE;
                    }

                    //
                    // Make sure the range thus far is committed.
                    //

                    if (Vad->u.VadFlags.MemCommit == 0) {

                        //
                        // The entire range to be decommitted is not committed,
                        // return an error.
                        //

                        return FALSE;
                    }
#if (_MI_PAGING_LEVELS >= 3)
                    if (MiIsPteOnPdeBoundary (PointerPde)) {
                        PointerPpe = MiGetPteAddress (PointerPde);
#if (_MI_PAGING_LEVELS >= 4)
                        if (MiIsPteOnPpeBoundary (PointerPde)) {
                            PointerPxe = MiGetPdeAddress (PointerPde);
                            Waited = 1;
                            break;
                        }
#endif
                        Waited = 1;
                        break;
                    }
#endif
                }
            } while (Waited != 0);
        }

        //
        // The page table page exists, check each PTE for commitment.
        //

        if (PointerPte->u.Long == 0) {

            //
            // This page has not been committed, check the VAD.
            //

            if (Vad->u.VadFlags.MemCommit == 0) {

                //
                // The entire range to be decommitted is not committed,
                // return an error.
                //

                return FALSE;
            }
        }
        else {

            //
            // Has this page been explicitly decommitted?
            //

            if (MiIsPteDecommittedPage (PointerPte)) {

                //
                // This page has been explicitly decommitted, return an error.
                //

                return FALSE;
            }
        }
        PointerPte += 1;
        Va = (PVOID)((PCHAR)(Va) + PAGE_SIZE);
    }
    return TRUE;
}

ULONG
MiDecommitPages (
    IN PVOID StartingAddress,
    IN PMMPTE EndingPte,
    IN PEPROCESS Process,
    IN PMMVAD_SHORT Vad
    )

/*++

Routine Description:

    This routine decommits the specified range of pages.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingPte - Supplies the ending PTE of the range.

    Process - Supplies the current process.

    Vad - Supplies the virtual address descriptor which describes the range.

Return Value:

    Value to reduce commitment by for the VAD.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PVOID Va;
    ULONG CommitReduction;
    PMMPTE CommitLimitPte;
    KIRQL OldIrql;
    PMMPTE ValidPteList[MM_VALID_PTE_SIZE];
    ULONG count;
    WSLE_NUMBER WorkingSetIndex;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    WSLE_NUMBER Entry;
    MMWSLENTRY Locked;
    MMPTE PteContents;
    PFN_NUMBER PageTableFrameIndex;
    PVOID UsedPageTableHandle;

    count = 0;
    CommitReduction = 0;

    if (Vad->u.VadFlags.MemCommit) {
        CommitLimitPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->EndingVpn));
    }
    else {
        CommitLimitPte = NULL;
    }

    //
    // Decommit each page by setting the PTE to be explicitly
    // decommitted.  The PTEs cannot be deleted all at once as
    // this would set the PTEs to zero which would auto-evaluate
    // as committed if referenced by another thread when a page
    // table page is being in-paged.
    //

    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    Va = StartingAddress;

    //
    // Loop through all the PDEs which map this region and ensure that
    // they exist.  If they don't exist create them by touching a
    // PTE mapped by the PDE.
    //

    MiMakePdeExistAndMakeValid(PointerPde, Process, MM_NOIRQL);

    while (PointerPte <= EndingPte) {

        if (MiIsPteOnPdeBoundary (PointerPte)) {

            PointerPde = MiGetPdeAddress (Va);
            if (count != 0) {
                MiProcessValidPteList (&ValidPteList[0], count);
                count = 0;
            }

            MiMakePdeExistAndMakeValid(PointerPde, Process, MM_NOIRQL);
        }

        //
        // The working set lock is held.  No PTEs can go from
        // invalid to valid or valid to invalid.  Transition
        // PTEs can go from transition to pagefile.
        //

        PteContents = *PointerPte;

        if (PteContents.u.Long != 0) {

            if (PointerPte->u.Long == MmDecommittedPte.u.Long) {

                //
                // This PTE is already decommitted.
                //

                CommitReduction += 1;
            }
            else {

                Process->NumberOfPrivatePages -= 1;

                if (PteContents.u.Hard.Valid == 1) {

                    //
                    // Make sure this is not a forked PTE.
                    //

                    Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

                    if (Pfn1->u3.e1.PrototypePte) {

                        LOCK_PFN (OldIrql);

                        MiDeletePte (PointerPte,
                                     Va,
                                     FALSE,
                                     Process,
                                     NULL,
                                     NULL,
                                     OldIrql);

                        UNLOCK_PFN (OldIrql);

                        Process->NumberOfPrivatePages += 1;
                        MI_WRITE_INVALID_PTE (PointerPte, MmDecommittedPte);
                    }
                    else {

                        //
                        // PTE is valid, process later when PFN lock is held.
                        //

                        if (count == MM_VALID_PTE_SIZE) {
                            MiProcessValidPteList (&ValidPteList[0], count);
                            count = 0;
                        }
                        ValidPteList[count] = PointerPte;
                        count += 1;

                        //
                        // Remove address from working set list.
                        //

                        WorkingSetIndex = Pfn1->u1.WsIndex;

                        ASSERT (PAGE_ALIGN(MmWsle[WorkingSetIndex].u1.Long) ==
                                                                           Va);
                        //
                        // Check to see if this entry is locked in the
                        // working set or locked in memory.
                        //

                        Locked = MmWsle[WorkingSetIndex].u1.e1;

                        MiRemoveWsle (WorkingSetIndex, MmWorkingSetList);

                        //
                        // Add this entry to the list of free working set
                        // entries and adjust the working set count.
                        //

                        MiReleaseWsle (WorkingSetIndex, &Process->Vm);

                        if ((Locked.LockedInWs == 1) || (Locked.LockedInMemory == 1)) {

                            //
                            // This entry is locked.
                            //

                            MmWorkingSetList->FirstDynamic -= 1;

                            if (WorkingSetIndex != MmWorkingSetList->FirstDynamic) {
                                Entry = MmWorkingSetList->FirstDynamic;
                                ASSERT (MmWsle[Entry].u1.e1.Valid);

                                MiSwapWslEntries (Entry,
                                                  WorkingSetIndex,
                                                  &Process->Vm,
                                                  FALSE);
                            }
                        }
                        MI_SET_PTE_IN_WORKING_SET (PointerPte, 0);
                    }
                }
                else if (PteContents.u.Soft.Prototype) {

                    //
                    // This is a forked PTE, just delete it.
                    //

                    LOCK_PFN (OldIrql);

                    MiDeletePte (PointerPte,
                                 Va,
                                 FALSE,
                                 Process,
                                 NULL,
                                 NULL,
                                 OldIrql);

                    UNLOCK_PFN (OldIrql);

                    Process->NumberOfPrivatePages += 1;
                    MI_WRITE_INVALID_PTE (PointerPte, MmDecommittedPte);
                }
                else if (PteContents.u.Soft.Transition == 1) {

                    //
                    // Transition PTE, get the PFN database lock
                    // and reprocess this one.
                    //

                    LOCK_PFN (OldIrql);
                    PteContents = *PointerPte;

                    if (PteContents.u.Soft.Transition == 1) {

                        //
                        // PTE is still in transition, delete it.
                        //

                        Pfn1 = MI_PFN_ELEMENT (PteContents.u.Trans.PageFrameNumber);

                        MI_SET_PFN_DELETED (Pfn1);

                        PageTableFrameIndex = Pfn1->u4.PteFrame;
                        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);
        
                        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                        //
                        // Check the reference count for the page, if the
                        // reference count is zero, move the page to the
                        // free list, if the reference count is not zero,
                        // ignore this page.  When the reference count
                        // goes to zero, it will be placed on the free list.
                        //

                        if (Pfn1->u3.e2.ReferenceCount == 0) {
                            MiUnlinkPageFromList (Pfn1);
                            MiReleasePageFileSpace (Pfn1->OriginalPte);
                            MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents));
                        }

                    }
                    else {

                        //
                        // Page MUST be in page file format!
                        //

                        ASSERT (PteContents.u.Soft.Valid == 0);
                        ASSERT (PteContents.u.Soft.Prototype == 0);
                        ASSERT (PteContents.u.Soft.PageFileHigh != 0);
                        MiReleasePageFileSpace (PteContents);
                    }
                    MI_WRITE_INVALID_PTE (PointerPte, MmDecommittedPte);
                    UNLOCK_PFN (OldIrql);
                }
                else {

                    //
                    // Must be demand zero or paging file format.
                    //

                    if (PteContents.u.Soft.PageFileHigh != 0) {
                        LOCK_PFN (OldIrql);
                        MiReleasePageFileSpace (PteContents);
                        UNLOCK_PFN (OldIrql);
                    }
                    else {

                        //
                        // Don't subtract out the private page count for
                        // a demand zero page.
                        //

                        Process->NumberOfPrivatePages += 1;
                    }

                    MI_WRITE_INVALID_PTE (PointerPte, MmDecommittedPte);
                }
            }
        }
        else {

            //
            // The PTE is already zero.
            //

            //
            // Increment the count of non-zero page table entries for this
            // page table and the number of private pages for the process.
            //

            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (Va);

            MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

            if (PointerPte > CommitLimitPte) {

                //
                // PTE is not committed.
                //

                CommitReduction += 1;
            }
            MI_WRITE_INVALID_PTE (PointerPte, MmDecommittedPte);
        }

        PointerPte += 1;
        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
    }
    if (count != 0) {
        MiProcessValidPteList (&ValidPteList[0], count);
    }

    return CommitReduction;
}


VOID
MiProcessValidPteList (
    IN PMMPTE *ValidPteList,
    IN ULONG Count
    )

/*++

Routine Description:

    This routine flushes the specified range of valid PTEs.

Arguments:

    ValidPteList - Supplies a pointer to an array of PTEs to flush.

    Count - Supplies the count of the number of elements in the array.

Return Value:

    none.

Environment:

    Kernel mode, APCs disabled, WorkingSetMutex and AddressCreation mutexes
    held.

--*/

{
    ULONG i;
    MMPTE_FLUSH_LIST PteFlushList;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    KIRQL OldIrql;

    i = 0;
    PteFlushList.Count = Count;

    if (Count < MM_MAXIMUM_FLUSH_COUNT) {

        do {
            PteFlushList.FlushVa[i] =
                    MiGetVirtualAddressMappedByPte (ValidPteList[i]);
            i += 1;
        } while (i != Count);
        i = 0;
    }

    LOCK_PFN (OldIrql);

    do {
        PteContents = *ValidPteList[i];
        ASSERT (PteContents.u.Hard.Valid == 1);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE(&PteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // Decrement the share and valid counts of the page table
        // page which maps this PTE.
        //

        PageTableFrameIndex = Pfn1->u4.PteFrame;
        Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

        MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

        MI_SET_PFN_DELETED (Pfn1);

        //
        // Decrement the share count for the physical page.  As the page
        // is private it will be put on the free list.
        //

        MiDecrementShareCount (Pfn1, PageFrameIndex);

        *ValidPteList[i] = MmDecommittedPte;
        i += 1;

    } while (i != Count);

    MiFlushPteList (&PteFlushList, FALSE);

    UNLOCK_PFN (OldIrql);

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\flushsec.c ===
/*++

Copyright (c) 1990  Microsoft Corporation

Module Name:

   flushsec.c

Abstract:

    This module contains the routines which implement the
    NtFlushVirtualMemory service.

Author:

    Lou Perazzoli (loup) 8-May-1990
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

PSUBSECTION
MiGetSystemCacheSubsection (
    IN PVOID BaseAddress,
    OUT PMMPTE *ProtoPte
    );

VOID
MiFlushDirtyBitsToPfn (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte,
    IN PEPROCESS Process,
    IN BOOLEAN SystemCache
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtFlushVirtualMemory)
#pragma alloc_text(PAGE,MmFlushVirtualMemory)
#endif

extern POBJECT_TYPE IoFileObjectType;

NTSTATUS
NtFlushVirtualMemory (
    IN HANDLE ProcessHandle,
    IN OUT PVOID *BaseAddress,
    IN OUT PSIZE_T RegionSize,
    OUT PIO_STATUS_BLOCK IoStatus
    )

/*++

Routine Description:

    This function flushes a range of virtual address which map
    a data file back into the data file if they have been modified.

Arguments:

    ProcessHandle - Supplies an open handle to a process object.

    BaseAddress - Supplies a pointer to a variable that will receive
         the base address the flushed region.  The initial value
         of this argument is the base address of the region of the
         pages to flush.

    RegionSize - Supplies a pointer to a variable that will receive
         the actual size in bytes of the flushed region of pages.
         The initial value of this argument is rounded up to the
         next host-page-size boundary.

         If this value is specified as zero, the mapped range from
         the base address to the end of the range is flushed.

    IoStatus - Returns the value of the IoStatus for the last attempted
         I/O operation.

Return Value:

    Returns the status

    TBS


--*/

{
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;
    PVOID CapturedBase;
    SIZE_T CapturedRegionSize;
    IO_STATUS_BLOCK TemporaryIoStatus;

    PAGED_CODE();

    PreviousMode = KeGetPreviousMode();
    if (PreviousMode != KernelMode) {

        //
        // Establish an exception handler, probe the specified addresses
        // for write access and capture the initial values.
        //

        try {

            ProbeForWritePointer (BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
            ProbeForWriteIoStatus (IoStatus);

            //
            // Capture the base address.
            //

            CapturedBase = *BaseAddress;

            //
            // Capture the region size.
            //

            CapturedRegionSize = *RegionSize;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            //
            // If an exception occurs during the probe or capture
            // of the initial values, then handle the exception and
            // return the exception code as the status value.
            //

            return GetExceptionCode();
        }

    }
    else {

        //
        // Capture the base address.
        //

        CapturedBase = *BaseAddress;

        //
        // Capture the region size.
        //

        CapturedRegionSize = *RegionSize;

    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_USER_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER_2;
    }

    if (((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (ULONG_PTR)CapturedBase) <
                                                        CapturedRegionSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER_2;

    }

    Status = ObReferenceObjectByHandle ( ProcessHandle,
                                         PROCESS_VM_OPERATION,
                                         PsProcessType,
                                         PreviousMode,
                                         (PVOID *)&Process,
                                         NULL );
    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    Status = MmFlushVirtualMemory (Process,
                                   &CapturedBase,
                                   &CapturedRegionSize,
                                   &TemporaryIoStatus);

    ObDereferenceObject (Process);

    //
    // Establish an exception handler and write the size and base
    // address.
    //

    try {

        *RegionSize = CapturedRegionSize;
        *BaseAddress = PAGE_ALIGN (CapturedBase);
        *IoStatus = TemporaryIoStatus;

    } except (EXCEPTION_EXECUTE_HANDLER) {
    }

    return Status;

}


VOID
MiFlushAcquire (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This is a helper routine to reference count the control area if needed
    during a flush section call to prevent the section object from being
    deleted while the flush is ongoing.

Arguments:

    ControlArea - Supplies a pointer to the control area.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    ASSERT ((LONG)ControlArea->NumberOfMappedViews >= 1);
    ControlArea->NumberOfMappedViews += 1;

    UNLOCK_PFN (OldIrql);
}


VOID
MiFlushRelease (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This is a helper routine to release the control area reference needed
    during a flush section call.

Arguments:

    ControlArea - Supplies a pointer to the control area.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);

    ASSERT ((LONG)ControlArea->NumberOfMappedViews >= 1);
    ControlArea->NumberOfMappedViews -= 1;

    //
    // Check to see if the control area should be deleted.  This
    // will release the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);
}


NTSTATUS
MmFlushVirtualMemory (
    IN PEPROCESS Process,
    IN OUT PVOID *BaseAddress,
    IN OUT PSIZE_T RegionSize,
    OUT PIO_STATUS_BLOCK IoStatus
    )

/*++

Routine Description:

    This function flushes a range of virtual address which map
    a data file back into the data file if they have been modified.

    Note that the modification is this process's view of the pages,
    on certain implementations (like the Intel 386), the modify
    bit is captured in the PTE and not forced to the PFN database
    until the page is removed from the working set.  This means
    that pages which have been modified by another process will
    not be flushed to the data file.

Arguments:

    Process - Supplies a pointer to a process object.

    BaseAddress - Supplies a pointer to a variable that will receive
                  the base address of the flushed region.  The initial value
                  of this argument is the base address of the region of the
                  pages to flush.

    RegionSize - Supplies a pointer to a variable that will receive
                 the actual size in bytes of the flushed region of pages.
                 The initial value of this argument is rounded up to the
                 next host-page-size boundary.

                 If this value is specified as zero, the mapped range from
                 the base address to the end of the range is flushed.

    IoStatus - Returns the value of the IoStatus for the last attempted
               I/O operation.

Return Value:

    NTSTATUS.

--*/

{
    PMMVAD Vad;
    PVOID EndingAddress;
    PVOID Va;
    PEPROCESS CurrentProcess;
    BOOLEAN SystemCache;
    PCONTROL_AREA ControlArea;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PMMPTE LastPte;
    PMMPTE FinalPte;
    PSUBSECTION Subsection;
    PSUBSECTION LastSubsection;
    NTSTATUS Status;
    ULONG ConsecutiveFileLockFailures;
    ULONG Waited;
    LOGICAL EntireRestOfVad;
    LOGICAL Attached;
    KAPC_STATE ApcState;

    PAGED_CODE();

    Attached = FALSE;

    //
    // Determine if the specified base address is within the system
    // cache and if so, don't attach, the working set mutex is still
    // required to "lock" paged pool pages (proto PTEs) into the
    // working set.
    //

    EndingAddress = (PVOID)(((ULONG_PTR)*BaseAddress + *RegionSize - 1) |
                                                            (PAGE_SIZE - 1));
    *BaseAddress = PAGE_ALIGN (*BaseAddress);

    if (MI_IS_SESSION_ADDRESS (*BaseAddress)) {

        //
        // Nothing in session space needs flushing.
        //

        return STATUS_NOT_MAPPED_VIEW;
    }

    CurrentProcess = PsGetCurrentProcess ();

    if (!MI_IS_SYSTEM_CACHE_ADDRESS(*BaseAddress)) {

        SystemCache = FALSE;

        //
        // Attach to the specified process.
        //

        if (CurrentProcess != Process) {
            KeStackAttachProcess (&Process->Pcb, &ApcState);
            Attached = TRUE;
        }

        LOCK_ADDRESS_SPACE (Process);

        //
        // Make sure the address space was not deleted, if so, return an error.
        //

        if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
            Status = STATUS_PROCESS_IS_TERMINATING;
            goto ErrorReturn;
        }

        Vad = MiLocateAddress (*BaseAddress);

        if (Vad == NULL) {

            //
            // No Virtual Address Descriptor located for Base Address.
            //

            Status = STATUS_NOT_MAPPED_VIEW;
            goto ErrorReturn;
        }

        if (*RegionSize == 0) {
            EndingAddress = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);
            EntireRestOfVad = TRUE;
        }
        else {
            EntireRestOfVad = FALSE;
        }

        if ((Vad->u.VadFlags.PrivateMemory == 1) ||
            (MI_VA_TO_VPN (EndingAddress) > Vad->EndingVpn)) {

            //
            // This virtual address descriptor does not refer to a Segment
            // object.
            //

            Status = STATUS_NOT_MAPPED_VIEW;
            goto ErrorReturn;
        }

        //
        // Make sure this VAD maps a data file (not an image file).
        //

        ControlArea = Vad->ControlArea;

        if ((ControlArea->FilePointer == NULL) ||
             (Vad->u.VadFlags.ImageMap == 1)) {

            //
            // This virtual address descriptor does not refer to a Segment
            // object.
            //

            Status = STATUS_NOT_MAPPED_DATA;
            goto ErrorReturn;
        }

        LOCK_WS_UNSAFE (Process);
    }
    else {

        SATISFY_OVERZEALOUS_COMPILER (Vad = NULL);
        SATISFY_OVERZEALOUS_COMPILER (ControlArea = NULL);
        SATISFY_OVERZEALOUS_COMPILER (EntireRestOfVad = FALSE);

        SystemCache = TRUE;
        Process = CurrentProcess;
        LOCK_WS (Process);
    }

    PointerPxe = MiGetPxeAddress (*BaseAddress);
    PointerPpe = MiGetPpeAddress (*BaseAddress);
    PointerPde = MiGetPdeAddress (*BaseAddress);
    PointerPte = MiGetPteAddress (*BaseAddress);
    LastPte = MiGetPteAddress (EndingAddress);
    *RegionSize = (PCHAR)EndingAddress - (PCHAR)*BaseAddress + 1;

retry:

    while (!MiDoesPxeExistAndMakeValid (PointerPxe, Process, MM_NOIRQL, &Waited)) {

        //
        // This page directory parent entry is empty, go to the next one.
        //

        PointerPxe += 1;
        PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
        Va = MiGetVirtualAddressMappedByPte (PointerPte);

        if (PointerPte > LastPte) {
            break;
        }
    }

    while (!MiDoesPpeExistAndMakeValid (PointerPpe, Process, MM_NOIRQL, &Waited)) {

        //
        // This page directory parent entry is empty, go to the next one.
        //

        PointerPpe += 1;
        PointerPxe = MiGetPteAddress (PointerPpe);
        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
        Va = MiGetVirtualAddressMappedByPte (PointerPte);

        if (PointerPte > LastPte) {
            break;
        }
#if (_MI_PAGING_LEVELS >= 4)
        if (MiIsPteOnPdeBoundary (PointerPpe)) {
            goto retry;
        }
#endif
    }

    Waited = 0;

    if (PointerPte <= LastPte) {
        while (!MiDoesPdeExistAndMakeValid(PointerPde, Process, MM_NOIRQL, &Waited)) {

            //
            // No page table page exists for this address.
            //

            PointerPde += 1;

            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

            if (PointerPte > LastPte) {
                break;
            }

#if (_MI_PAGING_LEVELS >= 3)
            if (MiIsPteOnPdeBoundary (PointerPde)) {

                if (MiIsPteOnPpeBoundary (PointerPde)) {
                    PointerPxe = MiGetPdeAddress (PointerPde);
                }
                PointerPpe = MiGetPteAddress (PointerPde);
                goto retry;
            }
#endif

            Va = MiGetVirtualAddressMappedByPte (PointerPte);
        }

        //
        // If the PFN lock (and accordingly the WS mutex) was
        // released and reacquired we must retry the operation.
        //

        if ((PointerPte <= LastPte) && (Waited != 0)) {
            goto retry;
        }
    }

    MiFlushDirtyBitsToPfn (PointerPte, LastPte, Process, SystemCache);

    if (SystemCache) {

        //
        // No VADs exist for the system cache.
        //

        UNLOCK_WS (Process);

        Subsection = MiGetSystemCacheSubsection (*BaseAddress, &PointerPte);

        LastSubsection = MiGetSystemCacheSubsection (EndingAddress, &FinalPte);

        //
        // Flush the PTEs from the specified section.
        //

        Status = MiFlushSectionInternal (PointerPte,
                                         FinalPte,
                                         Subsection,
                                         LastSubsection,
                                         FALSE,
                                         TRUE,
                                         IoStatus);
    }
    else {

        //
        // Protect against the section being prematurely deleted.
        //

        MiFlushAcquire (ControlArea);

        PointerPte = MiGetProtoPteAddress (Vad, MI_VA_TO_VPN (*BaseAddress));
        Subsection = MiLocateSubsection (Vad, MI_VA_TO_VPN(*BaseAddress));
        LastSubsection = MiLocateSubsection (Vad, MI_VA_TO_VPN(EndingAddress));

        //
        // The last subsection is NULL if the section is not fully 
        // committed.  Only allow the flush if the caller said do the whole
        // thing, otherwise it's an error.
        //

        if (LastSubsection == NULL) {

            if (EntireRestOfVad == FALSE) {

                //
                // Caller can only specify the range that is committed or zero
                // to indicate the entire range.
                //

                UNLOCK_WS_AND_ADDRESS_SPACE (Process);
                if (Attached == TRUE) {
                    KeUnstackDetachProcess (&ApcState);
                }
                MiFlushRelease (ControlArea);
                return STATUS_NOT_MAPPED_VIEW;
            }

            LastSubsection = Subsection;
            while (LastSubsection->NextSubsection) {
                LastSubsection = LastSubsection->NextSubsection;
            }

            //
            // A memory barrier is needed to read the subsection chains
            // in order to ensure the writes to the actual individual
            // subsection data structure fields are visible in correct
            // order.  This avoids the need to acquire any stronger
            // synchronization (ie: PFN lock), thus yielding better
            // performance and pagability.
            //

            KeMemoryBarrier ();

            FinalPte = LastSubsection->SubsectionBase + LastSubsection->PtesInSubsection - 1;
        }
        else {
            FinalPte = MiGetProtoPteAddress (Vad, MI_VA_TO_VPN (EndingAddress));
        }

        UNLOCK_WS_AND_ADDRESS_SPACE (Process);
        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }

        //
        // Preacquire the file to synchronize the flush.
        //

        ConsecutiveFileLockFailures = 0;

        do {

            Status = FsRtlAcquireFileForCcFlushEx (ControlArea->FilePointer);

            if (!NT_SUCCESS(Status)) {
                break;
            }

            //
            // Flush the PTEs from the specified section.
            //

            Status = MiFlushSectionInternal (PointerPte,
                                             FinalPte,
                                             Subsection,
                                             LastSubsection,
                                             TRUE,
                                             TRUE,
                                             IoStatus);

            //
            // Release the file we acquired.
            //

            FsRtlReleaseFileForCcFlush (ControlArea->FilePointer);

            //
            // Only try the request more than once if the filesystem told us
            // it had a deadlock.
            //

            if (Status != STATUS_FILE_LOCK_CONFLICT) {
                break;
            }

            ConsecutiveFileLockFailures += 1;
            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);

        } while (ConsecutiveFileLockFailures < 5);

        MiFlushRelease (ControlArea);
    }

    return Status;

ErrorReturn:

    ASSERT (SystemCache == FALSE);

    UNLOCK_ADDRESS_SPACE (Process);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }
    return Status;

}

NTSTATUS
MmFlushSection (
    IN PSECTION_OBJECT_POINTERS SectionObjectPointer,
    IN PLARGE_INTEGER Offset,
    IN SIZE_T RegionSize,
    OUT PIO_STATUS_BLOCK IoStatus,
    IN ULONG AcquireFile
    )

/*++

Routine Description:

    This function flushes to the backing file any modified pages within
    the specified range of the section.

Arguments:

    SectionObjectPointer - Supplies a pointer to the section objects.

    Offset - Supplies the offset into the section in which to begin
             flushing pages.  If this argument is not present, then the
             whole section is flushed without regard to the region size
             argument.

    RegionSize - Supplies the size in bytes to flush.  This is rounded
                 to a page multiple.

    IoStatus - Returns the value of the IoStatus for the last attempted
               I/O operation.

    AcquireFile - Nonzero if the callback should be used to acquire the file.

Return Value:

    Returns status of the operation.

--*/

{
    PCONTROL_AREA ControlArea;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    KIRQL OldIrql;
    UINT64 PteOffset;
    UINT64 LastPteOffset;
    PSUBSECTION Subsection;
    PSUBSECTION TempSubsection;
    PSUBSECTION LastSubsection;
    PSUBSECTION LastSubsectionWithProtos;
    PMAPPED_FILE_SEGMENT Segment;
    PETHREAD CurrentThread;
    NTSTATUS status;
    BOOLEAN OldClusterState;
    ULONG ConsecutiveFileLockFailures;

    //
    // Initialize IoStatus for success, in case we take an early exit.
    //

    IoStatus->Status = STATUS_SUCCESS;
    IoStatus->Information = RegionSize;

    LOCK_PFN (OldIrql);

    ControlArea = ((PCONTROL_AREA)(SectionObjectPointer->DataSectionObject));

    ASSERT ((ControlArea == NULL) || (ControlArea->u.Flags.Image == 0));

    if ((ControlArea == NULL) ||
        (ControlArea->u.Flags.BeingDeleted) ||
        (ControlArea->u.Flags.BeingCreated) ||
        (ControlArea->u.Flags.Rom) ||
        (ControlArea->NumberOfPfnReferences == 0)) {

        //
        // This file no longer has an associated segment or is in the
        // process of coming or going.
        // If the number of PFN references is zero, then this control
        // area does not have any valid or transition pages that need
        // to be flushed.
        //

        UNLOCK_PFN (OldIrql);
        return STATUS_SUCCESS;
    }

    //
    // Locate the subsection.
    //

    ASSERT (ControlArea->u.Flags.Image == 0);
    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);
    ASSERT (ControlArea->u.Flags.PhysicalMemory == 0);

    Subsection = (PSUBSECTION)(ControlArea + 1);

    if (!ARGUMENT_PRESENT (Offset)) {

        //
        // If the offset is not specified, flush the complete file ignoring
        // the region size.
        //

        ASSERT (ControlArea->FilePointer != NULL);

        PteOffset = 0;

        LastSubsection = Subsection;

        Segment = (PMAPPED_FILE_SEGMENT) ControlArea->Segment;

        if (MiIsAddressValid (Segment, TRUE)) {
            if (Segment->LastSubsectionHint != NULL) {
                LastSubsection = (PSUBSECTION) Segment->LastSubsectionHint;
            }
        }

        while (LastSubsection->NextSubsection != NULL) {
            LastSubsection = LastSubsection->NextSubsection;
        }

        LastPteOffset = LastSubsection->PtesInSubsection - 1;
    }
    else {

        PteOffset = (UINT64)(Offset->QuadPart >> PAGE_SHIFT);

        //
        // Make sure the PTEs are not in the extended part of the segment.
        //

        while (PteOffset >= (UINT64) Subsection->PtesInSubsection) {
            PteOffset -= Subsection->PtesInSubsection;
            if (Subsection->NextSubsection == NULL) {

                //
                // Past end of mapping, just return success.
                //

                UNLOCK_PFN (OldIrql);
                return STATUS_SUCCESS;
            }
            Subsection = Subsection->NextSubsection;
        }

        ASSERT (PteOffset < (UINT64) Subsection->PtesInSubsection);

        //
        // Locate the address of the last prototype PTE to be flushed.
        //

        LastPteOffset = PteOffset + (((RegionSize + BYTE_OFFSET(Offset->LowPart)) - 1) >> PAGE_SHIFT);

        LastSubsection = Subsection;

        while (LastPteOffset >= (UINT64) LastSubsection->PtesInSubsection) {
            LastPteOffset -= LastSubsection->PtesInSubsection;
            if (LastSubsection->NextSubsection == NULL) {
                LastPteOffset = LastSubsection->PtesInSubsection - 1;
                break;
            }
            LastSubsection = LastSubsection->NextSubsection;
        }

        ASSERT (LastPteOffset < LastSubsection->PtesInSubsection);
    }

    //
    // Try for the fast reference on the first and last subsection.
    // If that cannot be gotten, then there are no prototype PTEs for this
    // subsection, therefore there is nothing in it to flush so leap forwards.
    //
    // Note that subsections in between do not need referencing as
    // MiFlushSectionInternal is smart enough to skip them if they're
    // nonresident.
    //

    if (MiReferenceSubsection ((PMSUBSECTION)Subsection) == FALSE) {
        do {
            //
            // If this increment would put us past the end offset, then nothing
            // to flush, just return success.
            //

            if (Subsection == LastSubsection) {
                UNLOCK_PFN (OldIrql);
                return STATUS_SUCCESS;
            }
            Subsection = Subsection->NextSubsection;

            //
            // If this increment put us past the end of section, then nothing
            // to flush, just return success.
            //

            if (Subsection == NULL) {
                UNLOCK_PFN (OldIrql);
                return STATUS_SUCCESS;
            }

            if ((PMSUBSECTION)Subsection->SubsectionBase == NULL) {
                continue;
            }

            if (MiReferenceSubsection ((PMSUBSECTION)Subsection) == FALSE) {
                continue;
            }

            //
            // Start the flush at this subsection which is now referenced.
            //

            PointerPte = &Subsection->SubsectionBase[0];
            break;

        } while (TRUE);
    }
    else {
        PointerPte = &Subsection->SubsectionBase[PteOffset];
    }

    ASSERT (Subsection->SubsectionBase != NULL);

    //
    // The first subsection is referenced, now reference count the last one.
    // If the first is the last, just double reference it anyway as it
    // simplifies cleanup later.
    //

    if (MiReferenceSubsection ((PMSUBSECTION)LastSubsection) == FALSE) {

        ASSERT (Subsection != LastSubsection);

        TempSubsection = Subsection->NextSubsection;
        LastSubsectionWithProtos = NULL;

        while (TempSubsection != LastSubsection) {

            //
            // If this increment put us past the end of section, then nothing
            // to flush, just return success.
            //

            ASSERT (TempSubsection != NULL);

            if ((PMSUBSECTION)TempSubsection->SubsectionBase != NULL) {
                LastSubsectionWithProtos = TempSubsection;
            }

            TempSubsection = TempSubsection->NextSubsection;
        }

        //
        // End the flush at this subsection and reference it.
        //

        if (LastSubsectionWithProtos == NULL) {
            ASSERT (Subsection != NULL);
            ASSERT (Subsection->SubsectionBase != NULL);
            TempSubsection = Subsection;
        }
        else {
            TempSubsection = LastSubsectionWithProtos;
        }

        if (MiReferenceSubsection ((PMSUBSECTION)TempSubsection) == FALSE) {
            ASSERT (FALSE);
        }

        ASSERT (TempSubsection->SubsectionBase != NULL);

        LastSubsection = TempSubsection;
        LastPteOffset = LastSubsection->PtesInSubsection - 1;
    }

    //
    // Up the map view count so the control area cannot be deleted
    // out from under the call.
    //

    ControlArea->NumberOfMappedViews += 1;

    UNLOCK_PFN (OldIrql);

    //
    // End the flush at this subsection which is now referenced.
    //

    LastPte = &LastSubsection->SubsectionBase[LastPteOffset];

    CurrentThread = PsGetCurrentThread();

    //
    // Indicate that disk verify errors should be returned as exceptions.
    //

    OldClusterState = CurrentThread->ForwardClusterOnly;
    CurrentThread->ForwardClusterOnly = TRUE;

    //
    // Preacquire the file if we are going to synchronize the flush.
    //

    if (AcquireFile == 0) {

        //
        // Flush the PTEs from the specified section.
        //

        status = MiFlushSectionInternal (PointerPte,
                                         LastPte,
                                         Subsection,
                                         LastSubsection,
                                         TRUE,
                                         TRUE,
                                         IoStatus);
    }
    else {

        ConsecutiveFileLockFailures = 0;

        do {

            status = FsRtlAcquireFileForCcFlushEx (ControlArea->FilePointer);

            if (!NT_SUCCESS(status)) {
                break;
            }

            //
            // Flush the PTEs from the specified section.
            //

            status = MiFlushSectionInternal (PointerPte,
                                             LastPte,
                                             Subsection,
                                             LastSubsection,
                                             TRUE,
                                             TRUE,
                                             IoStatus);

            //
            // Release the file we acquired.
            //

            FsRtlReleaseFileForCcFlush (ControlArea->FilePointer);

            //
            // Only try the request more than once if the filesystem told us
            // it had a deadlock.
            //

            if (status != STATUS_FILE_LOCK_CONFLICT) {
                break;
            }

            ConsecutiveFileLockFailures += 1;
            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&MmShortTime);

        } while (ConsecutiveFileLockFailures < 5);
    }

    CurrentThread->ForwardClusterOnly = OldClusterState;

    LOCK_PFN (OldIrql);

    MiDecrementSubsections (Subsection, Subsection);
    MiDecrementSubsections (LastSubsection, LastSubsection);

    ASSERT ((LONG)ControlArea->NumberOfMappedViews >= 1);
    ControlArea->NumberOfMappedViews -= 1;

    //
    // Check to see if the control area should be deleted.  This
    // will release the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);

    return status;
}


LONGLONG
MiStartingOffset(
    IN PSUBSECTION Subsection,
    IN PMMPTE PteAddress
    )

/*++

Routine Description:

    This function calculates the file offset given a subsection and a PTE
    offset.  Note that images are stored in 512-byte units whereas data is
    stored in 4K units.

    When this is all debugged, this should be made into a macro.

Arguments:

    Subsection - Supplies a subsection to reference for the file address.

    PteAddress - Supplies a PTE within the subsection

Return Value:

    Returns the file offset to obtain the backing data from.

--*/

{
    LONGLONG PteByteOffset;
    LARGE_INTEGER StartAddress;

    if (Subsection->ControlArea->u.Flags.Image == 1) {
            return MI_STARTING_OFFSET ( Subsection,
                                        PteAddress);
    }

    ASSERT (Subsection->SubsectionBase != NULL);

    PteByteOffset = (LONGLONG)((PteAddress - Subsection->SubsectionBase))
                            << PAGE_SHIFT;

    Mi4KStartFromSubsection (&StartAddress, Subsection);

    StartAddress.QuadPart = StartAddress.QuadPart << MM4K_SHIFT;

    PteByteOffset += StartAddress.QuadPart;

    return PteByteOffset;
}

LARGE_INTEGER
MiEndingOffset(
    IN PSUBSECTION Subsection
    )

/*++

Routine Description:

    This function calculates the last valid file offset in a given subsection.
    offset.  Note that images are stored in 512-byte units whereas data is
    stored in 4K units.

    When this is all debugged, this should be made into a macro.

Arguments:

    Subsection - Supplies a subsection to reference for the file address.

    PteAddress - Supplies a PTE within the subsection

Return Value:

    Returns the file offset to obtain the backing data from.

--*/

{
    LARGE_INTEGER FileByteOffset;

    if (Subsection->ControlArea->u.Flags.Image == 1) {
        FileByteOffset.QuadPart =
            ((UINT64)Subsection->StartingSector + (UINT64)Subsection->NumberOfFullSectors) <<
                MMSECTOR_SHIFT;
    }
    else {
        Mi4KStartFromSubsection (&FileByteOffset, Subsection);

        FileByteOffset.QuadPart += Subsection->NumberOfFullSectors;

        FileByteOffset.QuadPart = FileByteOffset.QuadPart << MM4K_SHIFT;
    }

    FileByteOffset.QuadPart += Subsection->u.SubsectionFlags.SectorEndOffset;

    return FileByteOffset;
}


NTSTATUS
MiFlushSectionInternal (
    IN PMMPTE StartingPte,
    IN PMMPTE FinalPte,
    IN PSUBSECTION FirstSubsection,
    IN PSUBSECTION LastSubsection,
    IN ULONG Synchronize,
    IN LOGICAL WriteInProgressOk,
    OUT PIO_STATUS_BLOCK IoStatus
    )

/*++

Routine Description:

    This function flushes to the backing file any modified pages within
    the specified range of the section.  The parameters describe the
    section's prototype PTEs (start and end) and the subsections
    which correspond to the starting and ending PTE.

    Each PTE in the subsection between the specified start and end
    is examined and if the page is either valid or transition AND
    the page has been modified, the modify bit is cleared in the PFN
    database and the page is flushed to its backing file.

Arguments:

    StartingPte - Supplies a pointer to the first prototype PTE to
                  be examined for flushing.

    FinalPte - Supplies a pointer to the last prototype PTE to be
               examined for flushing.

    FirstSubsection - Supplies the subsection that contains the
                      StartingPte.

    LastSubsection - Supplies the subsection that contains the
                     FinalPte.

    Synchronize - Supplies TRUE if synchronization with all threads
                  doing flush operations to this section should occur.

    WriteInProgressOk - Supplies TRUE if the caller can tolerate a write
                        already in progress for any dirty pages.

    IoStatus - Returns the value of the IoStatus for the last attempted
               I/O operation.

Return Value:

    Returns status of the operation.

--*/

{
    LOGICAL DroppedPfnLock;
    PCONTROL_AREA ControlArea;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE LastWritten;
    PMMPTE FirstWritten;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    PMDL Mdl;
    KEVENT IoEvent;
    PSUBSECTION Subsection;
    PMSUBSECTION MappedSubsection;
    PPFN_NUMBER Page;
    PFN_NUMBER PageFrameIndex;
    PPFN_NUMBER LastPage;
    NTSTATUS Status;
    UINT64 StartingOffset;
    UINT64 TempOffset;
    LOGICAL WriteNow;
    LOGICAL Bail;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + (MM_MAXIMUM_DISK_IO_SIZE / PAGE_SIZE) + 1];
    ULONG ReflushCount;
    ULONG MaxClusterSize;
    PFILE_OBJECT FilePointer;
    LOGICAL CurrentThreadIsDereferenceThread;

    //
    // WriteInProgressOk is only FALSE when the segment dereference thread is
    // doing a top-level flush just prior to cleaning the section or subsection.
    // Note that this flag may be TRUE even for the dereference thread because
    // the dereference thread calls filesystems who may then issue a flush.
    //

    if (WriteInProgressOk == FALSE) {
        CurrentThreadIsDereferenceThread = TRUE;
        ASSERT (PsGetCurrentThread()->StartAddress == (PVOID)(ULONG_PTR)MiDereferenceSegmentThread);
    }
    else {
        CurrentThreadIsDereferenceThread = FALSE;

        //
        // This may actually be the dereference thread as the segment deletion
        // dereferences the file object potentially calling the filesystem which
        // may then issue a CcFlushCache/MmFlushSection.  For our purposes,
        // lower level flushes in this context are treated as though they
        // came from a different thread.
        //
    }

    WriteNow = FALSE;
    Bail = FALSE;

    IoStatus->Status = STATUS_SUCCESS;
    IoStatus->Information = 0;
    Mdl = (PMDL)&MdlHack[0];

    KeInitializeEvent (&IoEvent, NotificationEvent, FALSE);

    FinalPte += 1;  // Point to 1 past the last one.

    FirstWritten = NULL;
    LastWritten = NULL;
    LastPage = 0;
    Subsection = FirstSubsection;
    PointerPte = StartingPte;
    ControlArea = FirstSubsection->ControlArea;
    FilePointer = ControlArea->FilePointer;

    ASSERT ((ControlArea->u.Flags.Image == 0) &&
            (FilePointer != NULL) &&
            (ControlArea->u.Flags.PhysicalMemory == 0));

    //
    // Initializing these is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    MappedSubsection = NULL;
    StartingOffset = 0;

    //
    // Try to cluster pages as long as the storage stack can handle it.
    //

    MaxClusterSize = MmModifiedWriteClusterSize;

    LOCK_PFN (OldIrql);

    ASSERT (ControlArea->u.Flags.Image == 0);

    if (ControlArea->NumberOfPfnReferences == 0) {

        //
        // No transition or valid prototype PTEs present, hence
        // no need to flush anything.
        //

        UNLOCK_PFN (OldIrql);
        return STATUS_SUCCESS;
    }

    while ((Synchronize) && (ControlArea->FlushInProgressCount != 0)) {

        //
        // Another thread is currently performing a flush operation on
        // this file.  Wait for that flush to complete.
        //

        ControlArea->u.Flags.CollidedFlush = 1;

        //
        // Keep APCs blocked so no special APCs can be delivered in KeWait
        // which would cause the dispatcher lock to be released opening a
        // window where this thread could miss a pulse.
        //

        UNLOCK_PFN_AND_THEN_WAIT (APC_LEVEL);

        KeWaitForSingleObject (&MmCollidedFlushEvent,
                               WrPageOut,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)&MmOneSecond);
        KeLowerIrql (OldIrql);
        LOCK_PFN (OldIrql);
    }

    ControlArea->FlushInProgressCount += 1;

    //
    // Clear the deferred entry list as pages from it may get marked modified
    // during the processing.  Note that any transition page which is currently
    // clean but has a nonzero reference count may get marked modified if
    // there is a pending transaction and note well that this transaction may
    // complete at any time !  Thus, this case must be carefully handled.
    //

#if !defined(MI_MULTINODE)
    if (MmPfnDeferredList != NULL) {
        MiDeferredUnlockPages (MI_DEFER_PFN_HELD);
    }
#else
    //
    // Each and every node's deferred list would have to be checked so
    // we might as well go the long way and just call.
    //

    MiDeferredUnlockPages (MI_DEFER_PFN_HELD);
#endif

    for (;;) {

        if (LastSubsection != Subsection) {

            //
            // Flush to the last PTE in this subsection.
            //

            LastPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
        }
        else {

            //
            // Flush to the end of the range.
            //

            LastPte = FinalPte;
        }

        if (Subsection->SubsectionBase == NULL) {

            //
            // The prototype PTEs for this subsection have either never been
            // created or have been tossed due to memory pressure.  Either
            // way, this range can be skipped as there are obviously no
            // dirty pages in it.  If there are other dirty pages
            // to be written, write them now as we are skipping over PTEs.
            //

            if (LastWritten != NULL) {
                ASSERT (MappedSubsection != NULL);
                WriteNow = TRUE;
                goto CheckForWrite;
            }
            if (LastSubsection == Subsection) {
                break;
            }
            Subsection = Subsection->NextSubsection;
            PointerPte = Subsection->SubsectionBase;
            continue;
        }

        //
        // Up the number of mapped views to prevent other threads
        // from freeing this to the unused subsection list while we're
        // operating on it.
        //

        MappedSubsection = (PMSUBSECTION) Subsection;
        MappedSubsection->NumberOfMappedViews += 1;

        if (MappedSubsection->DereferenceList.Flink != NULL) {

            //
            // Remove this from the list of unused subsections.
            //

            RemoveEntryList (&MappedSubsection->DereferenceList);

            MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);

            MappedSubsection->DereferenceList.Flink = NULL;
        }

        if (CurrentThreadIsDereferenceThread == FALSE) {

            //
            // Set the access bit so an already ongoing trim won't blindly
            // delete the prototype PTEs on completion of a mapped write.
            // This can happen if the current thread dirties some pages and
            // then deletes the view before the trim write finishes - this
            // bit informs the trimming thread that a rescan is needed so
            // that writes are not lost.
            //

            MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;
        }

        //
        // If the prototype PTEs are paged out or have a share count
        // of 1, they cannot contain any transition or valid PTEs.
        //

        if (!MiCheckProtoPtePageState(PointerPte, OldIrql, &DroppedPfnLock)) {
            PointerPte = (PMMPTE)(((ULONG_PTR)PointerPte | (PAGE_SIZE - 1)) + 1);
        }

        while (PointerPte < LastPte) {

            if (MiIsPteOnPdeBoundary(PointerPte)) {

                //
                // We are on a page boundary, make sure this PTE is resident.
                //

                if (!MiCheckProtoPtePageState(PointerPte, OldIrql, &DroppedPfnLock)) {
                    PointerPte = (PMMPTE)((PCHAR)PointerPte + PAGE_SIZE);

                    //
                    // If there are dirty pages to be written, write them
                    // now as we are skipping over PTEs.
                    //

                    if (LastWritten != NULL) {
                        WriteNow = TRUE;
                        goto CheckForWrite;
                    }
                    continue;
                }
            }

            PteContents = *PointerPte;

            if ((PteContents.u.Hard.Valid == 1) ||
                   ((PteContents.u.Soft.Prototype == 0) &&
                     (PteContents.u.Soft.Transition == 1))) {

                //
                // Prototype PTE in transition, there are 3 possible cases:
                //  1. The page is part of an image which is sharable and
                //     refers to the paging file - dereference page file
                //     space and free the physical page.
                //  2. The page refers to the segment but is not modified -
                //     free the physical page.
                //  3. The page refers to the segment and is modified -
                //     write the page to the file and free the physical page.
                //

                if (PteContents.u.Hard.Valid == 1) {
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
                }
                else {
                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
                }

                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                ASSERT (Pfn1->OriginalPte.u.Soft.Prototype == 1);
                ASSERT (Pfn1->OriginalPte.u.Hard.Valid == 0);

                //
                // Note that any transition page which is currently clean but
                // has a nonzero reference count may get marked modified if
                // there is a pending transaction and note well that this
                // transaction may complete at any time !  Thus, this case
                // must be carefully handled since the segment dereference
                // thread must be given a collision error for this one as it
                // requires that no pages be dirtied after a successful return.
                //

                if ((CurrentThreadIsDereferenceThread == TRUE) &&
                    (Pfn1->u3.e2.ReferenceCount != 0)) {

#if DBG
                    if ((PteContents.u.Hard.Valid != 0) &&
                        (MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed == 0) &&
                        (ControlArea->u.Flags.Accessed == 0)) {

                        DbgPrint ("MM: flushing valid proto, %p %p\n",
                                        Pfn1, PointerPte);
                        DbgBreakPoint ();
                    }
#endif

                    PointerPte = LastPte;
                    Bail = TRUE;

                    if (LastWritten != NULL) {
                        WriteNow = TRUE;
                    }
                    goto CheckForWrite;
                }

                //
                // If the page is modified OR a write is in progress
                // flush it.  The write in progress case catches problems
                // where the modified page write continually writes a
                // page and gets errors writing it, by writing pages
                // in this state, the error will be propagated back to
                // the caller.
                //

                if ((Pfn1->u3.e1.Modified == 1) ||
                    (Pfn1->u3.e1.WriteInProgress)) {

                    if ((WriteInProgressOk == FALSE) &&
                        (Pfn1->u3.e1.WriteInProgress)) {

                            PointerPte = LastPte;
                            Bail = TRUE;

                            if (LastWritten != NULL) {
                                WriteNow = TRUE;
                            }
                            goto CheckForWrite;
                    }

                    if (LastWritten == NULL) {

                        //
                        // This is the first page of a cluster, initialize
                        // the MDL, etc.
                        //

                        LastPage = (PPFN_NUMBER)(Mdl + 1);

                        //
                        // Calculate the offset to read into the file.
                        //  offset = base + ((thispte - basepte) << PAGE_SHIFT)
                        //

                        StartingOffset = (UINT64) MiStartingOffset (
                                                             Subsection,
                                                             Pfn1->PteAddress);

                        MI_INITIALIZE_ZERO_MDL (Mdl);

                        Mdl->MdlFlags |= MDL_PAGES_LOCKED;
                        Mdl->StartVa =
                                  (PVOID)ULongToPtr(Pfn1->u3.e1.PageColor << PAGE_SHIFT);
                        Mdl->Size = (CSHORT)(sizeof(MDL) +
                                   (sizeof(PFN_NUMBER) * MaxClusterSize));
                        FirstWritten = PointerPte;
                    }

                    LastWritten = PointerPte;
                    Mdl->ByteCount += PAGE_SIZE;
                    if (Mdl->ByteCount == (PAGE_SIZE * MaxClusterSize)) {
                        WriteNow = TRUE;
                    }

                    if (PteContents.u.Hard.Valid == 0) {

                        //
                        // The page is in transition.
                        //

                        MiUnlinkPageFromList (Pfn1);
                        MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE(Pfn1, TRUE, 18);
                    }
                    else {
                        MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, TRUE, 20);
                    }

                    //
                    // Clear the modified bit for this page.
                    //

                    MI_SET_MODIFIED (Pfn1, 0, 0x22);

                    //
                    // Up the reference count for the physical page as there
                    // is I/O in progress.
                    //

                    Pfn1->u3.e2.ReferenceCount += 1;

                    *LastPage = PageFrameIndex;
                    LastPage += 1;
                }
                else {

                    //
                    // This page was not modified and therefore ends the
                    // current write cluster if any.  Set WriteNow to TRUE
                    // if there is a cluster being built.
                    //

                    if (LastWritten != NULL) {
                        WriteNow = TRUE;
                    }
                }
            }
            else {

                //
                // This page was not modified and therefore ends the
                // current write cluster if any.  Set WriteNow to TRUE
                // if there is a cluster being built.
                //

                if (LastWritten != NULL) {
                    WriteNow = TRUE;
                }
            }

            PointerPte += 1;

CheckForWrite:

            //
            // Write the current cluster if it is complete,
            // full, or the loop is now complete.
            //

            if ((WriteNow) ||
                ((PointerPte == LastPte) && (LastWritten != NULL))) {

                LARGE_INTEGER EndOfFile;

                //
                // Issue the write request.
                //

                UNLOCK_PFN (OldIrql);

                WriteNow = FALSE;

                //
                // Make sure the write does not go past the
                // end of file. (segment size).
                //

                EndOfFile = MiEndingOffset(Subsection);
                TempOffset = (UINT64) EndOfFile.QuadPart;

                if (StartingOffset + Mdl->ByteCount > TempOffset) {

                    ASSERT ((ULONG_PTR)(TempOffset - StartingOffset) >
                             (Mdl->ByteCount - PAGE_SIZE));

                    Mdl->ByteCount = (ULONG)(TempOffset - StartingOffset);
                }

                ReflushCount = 0;
                
                while (TRUE) {

                    KeClearEvent (&IoEvent);

                    Status = IoSynchronousPageWrite (FilePointer,
                                                     Mdl,
                                                     (PLARGE_INTEGER)&StartingOffset,
                                                     &IoEvent,
                                                     IoStatus);

                    if (NT_SUCCESS(Status)) {

                        //
                        // Success was returned, so wait for the i/o event.
                        //

                        KeWaitForSingleObject (&IoEvent,
                                               WrPageOut,
                                               KernelMode,
                                               FALSE,
                                               NULL);
                    }
                    else {

                        //
                        // Copy the error to the IoStatus, for error
                        // handling below.
                        //
    
                        IoStatus->Status = Status;
                    }

                    if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                        MmUnmapLockedPages (Mdl->MappedSystemVa, Mdl);
                    }

                    if (MmIsRetryIoStatus(IoStatus->Status)) {
                        
                        ReflushCount -= 1;
                        if (ReflushCount & MiIoRetryMask) {
                            KeDelayExecutionThread (KernelMode, FALSE, (PLARGE_INTEGER)&Mm30Milliseconds);
                            continue;
                        }
                    }

                    break;
                }

                Page = (PPFN_NUMBER)(Mdl + 1);

                LOCK_PFN (OldIrql);

                if (MiIsPteOnPdeBoundary(PointerPte) == 0) {

                    //
                    // The next PTE is not in a different page, make
                    // sure the PTE for the prototype PTE page was not
                    // put in transition while the I/O was in progress.
                    // Note the prototype PTE page itself cannot be reused
                    // as each outstanding page has a sharecount on it - but
                    // the PTE mapping it can be put in transition regardless
                    // of sharecount because it is a system page.
                    //

                    if (MiGetPteAddress (PointerPte)->u.Hard.Valid == 0) {
                        MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
                    }
                }

                if (NT_SUCCESS(IoStatus->Status)) {

                    //
                    // The I/O completed successfully, unlock the pages.
                    //

                    while (Page < LastPage) {

                        Pfn2 = MI_PFN_ELEMENT (*Page);
                        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn2, 19);
                        Page += 1;
                    }
                }
                else {

                    //
                    // Don't count on the file system to convey
                    // anything in the information field on errors.
                    //

                    IoStatus->Information = 0;

                    //
                    // The I/O completed unsuccessfully, unlock the pages
                    // and return an error status.
                    //

                    while (Page < LastPage) {

                        Pfn2 = MI_PFN_ELEMENT (*Page);

                        //
                        // Mark the page dirty again so it can be rewritten.
                        //

                        MI_SET_MODIFIED (Pfn2, 1, 0x1);

                        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn2, 21);

                        Page += 1;
                    }

                    if ((MmIsRetryIoStatus (IoStatus->Status)) &&
                        (MaxClusterSize != 1) &&
                        (Mdl->ByteCount > PAGE_SIZE)) {

                        //
                        // Retries of a cluster have failed, reissue
                        // the cluster one page at a time as the
                        // storage stack should always be able to
                        // make forward progress this way.
                        //

                        ASSERT (FirstWritten != NULL);
                        ASSERT (LastWritten != NULL);
                        ASSERT (FirstWritten != LastWritten);

                        PointerPte = FirstWritten;
                        if (MiGetPteAddress (PointerPte)->u.Hard.Valid == 0) {
                            MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
                        }
                        MaxClusterSize = 1;
                    }
                    else {
    
                        //
                        // Calculate how much was written thus far
                        // and add that to the information field
                        // of the IOSB.
                        //
    
                        IoStatus->Information +=
                            (((LastWritten - StartingPte) << PAGE_SHIFT) -
                                                            Mdl->ByteCount);
                        LastWritten = NULL;
    
                        //
                        // Set this to force termination of the outermost loop.
                        //
    
                        Subsection = LastSubsection;
                        break;
                    }

                } // end if error on i/o

                //
                // As the PFN lock has been released and
                // reacquired, do this loop again as the
                // PTE may have changed state.
                //

                LastWritten = NULL;
            } // end if chunk to write

        } //end while

        ASSERT (MappedSubsection->DereferenceList.Flink == NULL);
        ASSERT (((LONG_PTR)MappedSubsection->NumberOfMappedViews >= 1) ||
                (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1));

        MappedSubsection->NumberOfMappedViews -= 1;

        if ((MappedSubsection->NumberOfMappedViews == 0) &&
            (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0)) {

            //
            // Insert this subsection into the unused subsection list.
            //

            InsertTailList (&MmUnusedSubsectionList,
                            &MappedSubsection->DereferenceList);
            MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
        }

        if ((Bail == TRUE) || (Subsection == LastSubsection)) {

            //
            // The last range has been flushed or we have collided with the
            // mapped page writer.  Regardless, exit the top FOR loop
            // and return.
            //

            break;
        }

        Subsection = Subsection->NextSubsection;
        PointerPte = Subsection->SubsectionBase;

    }  //end for

    ASSERT (LastWritten == NULL);

    ControlArea->FlushInProgressCount -= 1;
    if ((ControlArea->u.Flags.CollidedFlush == 1) &&
        (ControlArea->FlushInProgressCount == 0)) {
        ControlArea->u.Flags.CollidedFlush = 0;
        KePulseEvent (&MmCollidedFlushEvent, 0, FALSE);
    }
    UNLOCK_PFN (OldIrql);

    if (Bail == TRUE) {

        //
        // This routine collided with the mapped page writer and the caller
        // expects an error for this.  Give it to him.
        //

        return STATUS_MAPPED_WRITER_COLLISION;
    }

    return IoStatus->Status;
}

BOOLEAN
MmPurgeSection (
    IN PSECTION_OBJECT_POINTERS SectionObjectPointer,
    IN PLARGE_INTEGER Offset,
    IN SIZE_T RegionSize,
    IN ULONG IgnoreCacheViews
    )

/*++

Routine Description:

    This function determines if any views of the specified section
    are mapped, and if not, purges valid pages (even modified ones)
    from the specified section and returns any used pages to the free
    list.  This is accomplished by examining the prototype PTEs
    from the specified offset to the end of the section, and if
    any prototype PTEs are in the transition state, putting the
    prototype PTE back into its original state and putting the
    physical page on the free list.

    NOTE:

    If there is an I/O operation ongoing for one of the pages,
    that page is eliminated from the segment and allowed to "float"
    until the i/o is complete.  Once the share count goes to zero
    the page will be added to the free page list.

Arguments:

    SectionObjectPointer - Supplies a pointer to the section objects.

    Offset - Supplies the offset into the section in which to begin
             purging pages.  If this argument is not present, then the
             whole section is purged without regard to the region size
             argument.


    RegionSize - Supplies the size of the region to purge.  If this
                 is specified as zero and Offset is specified, the
                 region from Offset to the end of the file is purged.

                 Note: The largest value acceptable for RegionSize is
                 0xFFFF0000;

    IgnoreCacheViews - Supplies FALSE if mapped views in the system
                 cache should cause the function to return FALSE.
                 This is the normal case.
                 Supplies TRUE if mapped views should be ignored
                 and the flush should occur.  NOTE THAT IF TRUE
                 IS SPECIFIED AND ANY DATA PURGED IS CURRENTLY MAPPED
                 AND VALID A BUGCHECK WILL OCCUR!!

Return Value:

    Returns TRUE if either no section exists for the file object or
    the section is not mapped and the purge was done, FALSE otherwise.

    Note that FALSE is returned if during the purge operation, a page
    could not be purged due to a non-zero reference count.

--*/

{
    LOGICAL DroppedPfnLock;
    PCONTROL_AREA ControlArea;
    PMAPPED_FILE_SEGMENT Segment;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE FinalPte;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    UINT64 PteOffset;
    UINT64 LastPteOffset;
    PMSUBSECTION MappedSubsection;
    PSUBSECTION Subsection;
    PSUBSECTION FirstSubsection;
    PSUBSECTION LastSubsection;
    PSUBSECTION TempSubsection;
    PSUBSECTION LastSubsectionWithProtos;
    LARGE_INTEGER LocalOffset;
    BOOLEAN ReturnValue;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
#if DBG
    PFN_NUMBER LastLocked = 0;
#endif

    //
    // This is needed in case a page is on the mapped page writer list -
    // the PFN lock will need to be released and APCs disabled.
    //

    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    //
    //  Capture caller's file size, since we may modify it.
    //

    if (ARGUMENT_PRESENT(Offset)) {

        LocalOffset = *Offset;
        Offset = &LocalOffset;
    }

    //
    //  See if we can truncate this file to where the caller wants
    //  us to.
    //

    if (!MiCanFileBeTruncatedInternal(SectionObjectPointer, Offset, TRUE, &OldIrql)) {
        return FALSE;
    }

    //
    // PFN LOCK IS NOW HELD!
    //

    ControlArea = (PCONTROL_AREA)(SectionObjectPointer->DataSectionObject);
    if ((ControlArea == NULL) || (ControlArea->u.Flags.Rom)) {
        UNLOCK_PFN (OldIrql);
        return TRUE;
    }

    //
    //  Even though MiCanFileBeTruncatedInternal returned TRUE, there could
    //  still be a system cache mapped view.  We cannot truncate if
    //  the Cache Manager has a view mapped.
    //

    if ((IgnoreCacheViews == FALSE) &&
        (ControlArea->NumberOfSystemCacheViews != 0)) {

        UNLOCK_PFN (OldIrql);
        return FALSE;
    }

#if 0

    //
    // Prevent races when the control area is being deleted as the clean
    // path releases the PFN lock midway through.  File objects may still have
    // section object pointers and data section objects that point at this
    // control area, hence the purge can be issued.
    //
    // Check for this and fail the purge as the control area (and the section
    // object pointers/data section objects) will be going away momentarily.
    // Note that even though drivers have these data section objects, no one
    // currently has an open section for this control area and no one is
    // allowed to open one until the clean path finishes.
    //

    if (ControlArea->u.Flags.BeingDeleted == 1) {
        UNLOCK_PFN (OldIrql);
        return FALSE;
    }

#else

    //
    // The above check can be removed as MiCanFileBeTruncatedInternal does
    // the same check, so just assert it below.
    //

    ASSERT (ControlArea->u.Flags.BeingDeleted == 0);

#endif

    //
    // Purge the section - locate the subsection which
    // contains the PTEs.
    //

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    Subsection = (PSUBSECTION)(ControlArea + 1);

    if (!ARGUMENT_PRESENT (Offset)) {

        //
        // If the offset is not specified, flush the complete file ignoring
        // the region size.
        //

        PteOffset = 0;
        RegionSize = 0;

    }
    else {

        PteOffset = (UINT64)(Offset->QuadPart >> PAGE_SHIFT);

        //
        // Make sure the PTEs are not in the extended part of the segment.
        //

        while (PteOffset >= (UINT64) Subsection->PtesInSubsection) {
            PteOffset -= Subsection->PtesInSubsection;
            Subsection = Subsection->NextSubsection;
            if (Subsection == NULL) {

                //
                // The offset must be equal to the size of
                // the section, don't purge anything just return.
                //

                UNLOCK_PFN (OldIrql);
                return TRUE;
            }
        }

        ASSERT (PteOffset < (UINT64) Subsection->PtesInSubsection);
    }

    //
    // Locate the address of the last prototype PTE to be flushed.
    //

    if (RegionSize == 0) {

        //
        // Flush to end of section.
        //

        LastSubsection = Subsection;

        Segment = (PMAPPED_FILE_SEGMENT) ControlArea->Segment;

        if (MiIsAddressValid (Segment, TRUE)) {
            if (Segment->LastSubsectionHint != NULL) {
                LastSubsection = (PSUBSECTION) Segment->LastSubsectionHint;
            }
        }

        while (LastSubsection->NextSubsection != NULL) {
            LastSubsection = LastSubsection->NextSubsection;
        }

        LastPteOffset = LastSubsection->PtesInSubsection - 1;
    }
    else {

        //
        // Calculate the end of the region.
        //

        LastPteOffset = PteOffset +
            (((RegionSize + BYTE_OFFSET(Offset->LowPart)) - 1) >> PAGE_SHIFT);

        LastSubsection = Subsection;

        while (LastPteOffset >= (UINT64) LastSubsection->PtesInSubsection) {
            LastPteOffset -= LastSubsection->PtesInSubsection;
            if (LastSubsection->NextSubsection == NULL) {
                LastPteOffset = LastSubsection->PtesInSubsection - 1;
                break;
            }
            LastSubsection = LastSubsection->NextSubsection;
        }

        ASSERT (LastPteOffset < (UINT64) LastSubsection->PtesInSubsection);
    }

    //
    // Try for the fast reference on the first and last subsection.
    // If that cannot be gotten, then there are no prototype PTEs for this
    // subsection, therefore there is nothing in it to flush so leap forwards.
    //
    // Note that subsections in between do not need referencing as
    // the purge is smart enough to skip them if they're nonresident.
    //

    if (MiReferenceSubsection ((PMSUBSECTION)Subsection) == FALSE) {
        do {
            //
            // If this increment would put us past the end offset, then nothing
            // to flush, just return success.
            //

            if (Subsection == LastSubsection) {
                UNLOCK_PFN (OldIrql);
                return TRUE;
            }
            Subsection = Subsection->NextSubsection;

            //
            // If this increment put us past the end of section, then nothing
            // to flush, just return success.
            //

            if (Subsection == NULL) {
                UNLOCK_PFN (OldIrql);
                return TRUE;
            }

            if (MiReferenceSubsection ((PMSUBSECTION)Subsection) == FALSE) {
                continue;
            }

            //
            // Start the flush at this subsection which is now referenced.
            //

            PointerPte = &Subsection->SubsectionBase[0];
            break;

        } while (TRUE);
    }
    else {
        PointerPte = &Subsection->SubsectionBase[PteOffset];
    }

    FirstSubsection = Subsection;
    ASSERT (Subsection->SubsectionBase != NULL);

    //
    // The first subsection is referenced, now reference count the last one.
    // If the first is the last, just double reference it anyway as it
    // simplifies cleanup later.
    //

    if (MiReferenceSubsection ((PMSUBSECTION)LastSubsection) == FALSE) {

        ASSERT (Subsection != LastSubsection);

        TempSubsection = Subsection->NextSubsection;
        LastSubsectionWithProtos = NULL;

        while (TempSubsection != LastSubsection) {

            //
            // If this increment put us past the end of section, then nothing
            // to flush, just return success.
            //

            ASSERT (TempSubsection != NULL);

            if ((PMSUBSECTION)TempSubsection->SubsectionBase != NULL) {
                LastSubsectionWithProtos = TempSubsection;
            }

            TempSubsection = TempSubsection->NextSubsection;
        }

        //
        // End the flush at this subsection and reference it.
        //

        if (LastSubsectionWithProtos == NULL) {
            ASSERT (Subsection != NULL);
            ASSERT (Subsection->SubsectionBase != NULL);
            TempSubsection = Subsection;
        }
        else {
            TempSubsection = LastSubsectionWithProtos;
        }

        if (MiReferenceSubsection ((PMSUBSECTION)TempSubsection) == FALSE) {
            ASSERT (FALSE);
        }

        ASSERT (TempSubsection->SubsectionBase != NULL);

        LastSubsection = TempSubsection;
        LastPteOffset = LastSubsection->PtesInSubsection - 1;
    }

    //
    // End the flush at this subsection which is now referenced.
    //
    // Point final PTE to 1 beyond the end.
    //

    FinalPte = &LastSubsection->SubsectionBase[LastPteOffset + 1];

    //
    // Increment the number of mapped views to
    // prevent the section from being deleted while the purge is
    // in progress.
    //

    ControlArea->NumberOfMappedViews += 1;

    //
    // Set being purged so no one can map a view
    // while the purge is going on.
    //

    ControlArea->u.Flags.BeingPurged = 1;
    ControlArea->u.Flags.WasPurged = 1;

    ReturnValue = TRUE;

    for (;;) {

        if (OldIrql == MM_NOIRQL) {
            LOCK_PFN (OldIrql);
        }

        if (LastSubsection != Subsection) {

            //
            // Flush to the last PTE in this subsection.
            //

            LastPte = &Subsection->SubsectionBase[Subsection->PtesInSubsection];
        }
        else {

            //
            // Flush to the end of the range.
            //

            LastPte = FinalPte;
        }

        if (Subsection->SubsectionBase == NULL) {

            //
            // The prototype PTEs for this subsection have either never been
            // created or have been tossed due to memory pressure.  Either
            // way, this range can be skipped as there are obviously no
            // pages to purge in this range.
            //

            ASSERT (OldIrql != MM_NOIRQL);
            UNLOCK_PFN (OldIrql);
            OldIrql = MM_NOIRQL;
            goto nextrange;
        }

        //
        // Up the number of mapped views to prevent other threads
        // from freeing this to the unused subsection list while we're
        // operating on it.
        //

        MappedSubsection = (PMSUBSECTION) Subsection;
        MappedSubsection->NumberOfMappedViews += 1;

        if (MappedSubsection->DereferenceList.Flink != NULL) {

            //
            // Remove this from the list of unused subsections.
            //

            RemoveEntryList (&MappedSubsection->DereferenceList);

            MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);

            MappedSubsection->DereferenceList.Flink = NULL;
        }

        //
        // Set the access bit so an already ongoing trim won't blindly
        // delete the prototype PTEs on completion of a mapped write.
        // This can happen if the current thread dirties some pages and
        // then deletes the view before the trim write finishes - this
        // bit informs the trimming thread that a rescan is needed so
        // that writes are not lost.
        //

        MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;

        //
        // If the page table page containing the PTEs is not
        // resident, then no PTEs can be in the valid or transition
        // state!  Skip over the PTEs.
        //

        if (!MiCheckProtoPtePageState(PointerPte, OldIrql, &DroppedPfnLock)) {
            PointerPte = (PMMPTE)(((ULONG_PTR)PointerPte | (PAGE_SIZE - 1)) + 1);
        }

        while (PointerPte < LastPte) {

            //
            // If the page table page containing the PTEs is not
            // resident, then no PTEs can be in the valid or transition
            // state!  Skip over the PTEs.
            //

            if (MiIsPteOnPdeBoundary(PointerPte)) {
                if (!MiCheckProtoPtePageState(PointerPte, OldIrql, &DroppedPfnLock)) {
                    PointerPte = (PMMPTE)((PCHAR)PointerPte + PAGE_SIZE);
                    continue;
                }
            }

            PteContents = *PointerPte;

            if (PteContents.u.Hard.Valid == 1) {

                //
                // A valid PTE was found, it must be mapped in the
                // system cache.  Just exit the loop and return FALSE
                // and let the caller fix this.
                //

                ReturnValue = FALSE;
                break;
            }

            if ((PteContents.u.Soft.Prototype == 0) &&
                     (PteContents.u.Soft.Transition == 1)) {

                if (OldIrql == MM_NOIRQL) {
                    PointerPde = MiGetPteAddress (PointerPte);
                    LOCK_PFN (OldIrql);
                    if (PointerPde->u.Hard.Valid == 0) {
                        MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
                    }
                    continue;
                }

                PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents);
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                if ((Pfn1->OriginalPte.u.Soft.Prototype != 1) ||
                    (Pfn1->OriginalPte.u.Hard.Valid != 0) ||
                    (Pfn1->PteAddress != PointerPte)) {

                    //
                    // The pool containing the prototype PTEs has been
                    // corrupted.  Pool corruption like this is fatal.
                    //

                    KeBugCheckEx (POOL_CORRUPTION_IN_FILE_AREA,
                                  0x2,
                                  (ULONG_PTR)PointerPte,
                                  (ULONG_PTR)Pfn1->PteAddress,
                                  (ULONG_PTR)PteContents.u.Long);
                }

#if DBG
                if ((Pfn1->u3.e2.ReferenceCount != 0) &&
                    (Pfn1->u3.e1.WriteInProgress == 0)) {

                    //
                    // There must be an I/O in progress on this page.
                    //

                    if (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE(&PteContents) != LastLocked) {
                        UNLOCK_PFN (OldIrql);

                        LastLocked = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
                        PointerPde = MiGetPteAddress (PointerPte);
                        LOCK_PFN (OldIrql);
                        if (PointerPde->u.Hard.Valid == 0) {
                            MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
                        }
                        continue;
                    }
                }
#endif //DBG

                //
                // If the modified page writer has page locked for I/O
                // wait for the I/O's to be completed and the pages
                // to be unlocked.  The eliminates a race condition
                // when the modified page writer locks the pages, then
                // a purge occurs and completes before the mapped
                // writer thread runs.
                //

                if (Pfn1->u3.e1.WriteInProgress == 1) {

                    //
                    // A 3 or more thread deadlock can occur where:
                    //
                    // 1.  The mapped page writer thread has issued a write
                    //     and is in the filesystem code waiting for a resource.
                    //
                    // 2.  Thread 2 owns the resource above but is waiting for
                    //     the filesystem's quota mutex.
                    //
                    // 3.  Thread 3 owns the quota mutex and is right here
                    //     doing a purge from the cache manager when he notices
                    //     the page to be purged is either already being written
                    //     or is in the mapped page writer list.  If it is
                    //     already being written everything will unjam.  If it
                    //     is still on the mapped page writer list awaiting
                    //     processing, then it must be cancelled - otherwise
                    //     if this thread were to wait, deadlock can occur.
                    //
                    // The alternative to all this is for the filesystems to
                    // always release the quota mutex before purging but the
                    // filesystem overhead to do this is substantial.
                    //

                    if (MiCancelWriteOfMappedPfn (PageFrameIndex, OldIrql) == TRUE) {

                        //
                        // Stopping any failed writes (even deliberately
                        // cancelled ones) automatically cause a delay.  A
                        // successful stop also results in the PFN lock
                        // being released and reacquired.  So loop back to
                        // the top now as the world may have changed.
                        //

                        if (MiGetPteAddress (PointerPte)->u.Hard.Valid == 0) {
                            MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
                        }
                        continue;
                    }

                    ASSERT (ControlArea->ModifiedWriteCount != 0);
                    ASSERT (Pfn1->u3.e2.ReferenceCount != 0);

                    ControlArea->u.Flags.SetMappedFileIoComplete = 1;

                    //
                    // Keep APCs blocked so no special APCs can be delivered
                    // in KeWait which would cause the dispatcher lock to be
                    // released opening a window where this thread could miss
                    // a pulse.
                    //

                    UNLOCK_PFN_AND_THEN_WAIT (APC_LEVEL);

                    KeWaitForSingleObject (&MmMappedFileIoComplete,
                                           WrPageOut,
                                           KernelMode,
                                           FALSE,
                                           NULL);
                    KeLowerIrql (OldIrql);
                    PointerPde = MiGetPteAddress (PointerPte);
                    LOCK_PFN (OldIrql);
                    if (PointerPde->u.Hard.Valid == 0) {
                        MiMakeSystemAddressValidPfn (PointerPte, OldIrql);
                    }
                    continue;
                }

                if (Pfn1->u3.e1.ReadInProgress == 1) {

                    //
                    // The page currently is being read in from the
                    // disk.  Treat this just like a valid PTE and
                    // return false.
                    //

                    ReturnValue = FALSE;
                    break;
                }

                ASSERT (!((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                    (Pfn1->OriginalPte.u.Soft.Transition == 1)));

                MI_WRITE_INVALID_PTE (PointerPte, Pfn1->OriginalPte);

                ASSERT (Pfn1->OriginalPte.u.Hard.Valid == 0);

                ControlArea->NumberOfPfnReferences -= 1;
                ASSERT ((LONG)ControlArea->NumberOfPfnReferences >= 0);

                MiUnlinkPageFromList (Pfn1);

                MI_SET_PFN_DELETED (Pfn1);

                PageTableFrameIndex = Pfn1->u4.PteFrame;
                Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

                MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

                //
                // If the reference count for the page is zero, insert
                // it into the free page list, otherwise leave it alone
                // and when the reference count is decremented to zero
                // the page will go to the free list.
                //

                if (Pfn1->u3.e2.ReferenceCount == 0) {
                    MiReleasePageFileSpace (Pfn1->OriginalPte);
                    MiInsertPageInFreeList (MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents));
                }
            }
            PointerPte += 1;

            if ((MiIsPteOnPdeBoundary(PointerPte)) && (OldIrql != MM_NOIRQL)) {

                //
                // Unlock PFN so large requests will not block other
                // threads on MP systems.
                //

                UNLOCK_PFN (OldIrql);
                OldIrql = MM_NOIRQL;
            }
        }

        if (OldIrql == MM_NOIRQL) {
            LOCK_PFN (OldIrql);
        }

        ASSERT (MappedSubsection->DereferenceList.Flink == NULL);
        ASSERT (((LONG_PTR)MappedSubsection->NumberOfMappedViews >= 1) ||
                (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1));

        MappedSubsection->NumberOfMappedViews -= 1;

        if ((MappedSubsection->NumberOfMappedViews == 0) &&
            (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0)) {

            //
            // Insert this subsection into the unused subsection list.
            //

            InsertTailList (&MmUnusedSubsectionList,
                            &MappedSubsection->DereferenceList);
            MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
        }

        ASSERT (OldIrql != MM_NOIRQL);

        UNLOCK_PFN (OldIrql);
        OldIrql = MM_NOIRQL;

nextrange:

        if ((LastSubsection != Subsection) && (ReturnValue)) {

            //
            // Get the next subsection in the list.
            //

            Subsection = Subsection->NextSubsection;
            PointerPte = Subsection->SubsectionBase;
        }
        else {

            //
            // The last range has been flushed, exit the top FOR loop
            // and return.
            //

            break;
        }
    }

    if (OldIrql == MM_NOIRQL) {
        LOCK_PFN (OldIrql);
    }

    MiDecrementSubsections (FirstSubsection, FirstSubsection);
    MiDecrementSubsections (LastSubsection, LastSubsection);

    ASSERT ((LONG)ControlArea->NumberOfMappedViews >= 1);
    ControlArea->NumberOfMappedViews -= 1;

    ControlArea->u.Flags.BeingPurged = 0;

    //
    // Check to see if the control area should be deleted.  This
    // will release the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);
    return ReturnValue;
}

BOOLEAN
MmFlushImageSection (
    IN PSECTION_OBJECT_POINTERS SectionPointer,
    IN MMFLUSH_TYPE FlushType
    )

/*++

Routine Description:

    This function determines if any views of the specified image section
    are mapped, and if not, flushes valid pages (even modified ones)
    from the specified section and returns any used pages to the free
    list.  This is accomplished by examining the prototype PTEs
    from the specified offset to the end of the section, and if
    any prototype PTEs are in the transition state, putting the
    prototype PTE back into its original state and putting the
    physical page on the free list.

Arguments:

    SectionPointer - Supplies a pointer to a section object pointers
                     within the FCB.

    FlushType - Supplies the type of flush to check for.  One of
                MmFlushForDelete or MmFlushForWrite.

Return Value:

    Returns TRUE if either no section exists for the file object or
    the section is not mapped and the purge was done, FALSE otherwise.

--*/

{
    PLIST_ENTRY Next;
    PCONTROL_AREA ControlArea;
    PLARGE_CONTROL_AREA LargeControlArea;
    KIRQL OldIrql;
    LOGICAL state;

    if (FlushType == MmFlushForDelete) {

        //
        // Do a quick check to see if there are any mapped views for
        // the data section.  If so, just return FALSE.
        //

        LOCK_PFN (OldIrql);
        ControlArea = (PCONTROL_AREA)(SectionPointer->DataSectionObject);
        if (ControlArea != NULL) {
            if ((ControlArea->NumberOfUserReferences != 0) ||
                (ControlArea->u.Flags.BeingCreated)) {
                UNLOCK_PFN (OldIrql);
                return FALSE;
            }
        }
        UNLOCK_PFN (OldIrql);
    }

    //
    // Check the status of the control area.  If the control area is in use
    // or the control area is being deleted, this operation cannot continue.
    //

    state = MiCheckControlAreaStatus (CheckImageSection,
                                      SectionPointer,
                                      FALSE,
                                      &ControlArea,
                                      &OldIrql);

    if (ControlArea == NULL) {
        return (BOOLEAN) state;
    }

    //
    // PFN LOCK IS NOW HELD!
    //

    //
    // Repeat until there are no more control areas - multiple control areas
    // for the same image section occur to support user global DLLs - these DLLs
    // require data that is shared within a session but not across sessions.
    // Note this can only happen for Hydra.
    //

    do {

        //
        // Set the being deleted flag and up the number of mapped views
        // for the segment.  Upping the number of mapped views prevents
        // the segment from being deleted and passed to the deletion thread
        // while we are forcing a delete.
        //

        ControlArea->u.Flags.BeingDeleted = 1;
        ControlArea->NumberOfMappedViews = 1;
        LargeControlArea = NULL;

        if (ControlArea->u.Flags.GlobalOnlyPerSession == 0) {
            NOTHING;
        }
        else if (IsListEmpty(&((PLARGE_CONTROL_AREA)ControlArea)->UserGlobalList)) {
            ASSERT (ControlArea ==
                    (PCONTROL_AREA)SectionPointer->ImageSectionObject);
        }
        else {

            //
            // Check if there's only one image section in this control area, so
            // we don't reference the section object pointers as the
            // MiCleanSection call may result in its deletion.
            //

            //
            // There are multiple control areas, bump the reference count
            // on one of them (doesn't matter which one) so that it can't
            // go away.  This ensures the section object pointers will stick
            // around even after the calls below so we can safely reloop to
            // flush any other remaining control areas.
            //

            ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 1);

            Next = ((PLARGE_CONTROL_AREA)ControlArea)->UserGlobalList.Flink;

            LargeControlArea = CONTAINING_RECORD (Next,
                                                  LARGE_CONTROL_AREA,
                                                  UserGlobalList);
        
            ASSERT (LargeControlArea->u.Flags.GlobalOnlyPerSession == 1);

            LargeControlArea->NumberOfSectionReferences += 1;
        }

        //
        // This is a page file backed or image segment.  The segment is being
        // deleted, remove all references to the paging file and physical
        // memory.
        //

        UNLOCK_PFN (OldIrql);

        MiCleanSection (ControlArea, TRUE);

        //
        // Get the next Hydra control area.
        //

        if (LargeControlArea != NULL) {
            state = MiCheckControlAreaStatus (CheckImageSection,
                                              SectionPointer,
                                              FALSE,
                                              &ControlArea,
                                              &OldIrql);
            if (!ControlArea) {
                LOCK_PFN (OldIrql);
                LargeControlArea->NumberOfSectionReferences -= 1;
                MiCheckControlArea ((PCONTROL_AREA)LargeControlArea,
                                    NULL,
                                    OldIrql);
            }
            else {
                LargeControlArea->NumberOfSectionReferences -= 1;
                MiCheckControlArea ((PCONTROL_AREA)LargeControlArea,
                                    NULL,
                                    OldIrql);
                LOCK_PFN (OldIrql);
            }
        }
        else {
            state = TRUE;
            break;
        }

    } while (ControlArea);

    return (BOOLEAN) state;
}

VOID
MiFlushDirtyBitsToPfn (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte,
    IN PEPROCESS Process,
    IN BOOLEAN SystemCache
    )

{
    KIRQL OldIrql;
    MMPTE PteContents;
    PMMPFN Pfn1;
    PVOID Va;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    ULONG Waited;
    MMPTE_FLUSH_LIST PteFlushList;

    PteFlushList.Count = 0;

    Va = MiGetVirtualAddressMappedByPte (PointerPte);

    LOCK_PFN (OldIrql);

    while (PointerPte <= LastPte) {

        PteContents = *PointerPte;

        if ((PteContents.u.Hard.Valid == 1) &&
            (MI_IS_PTE_DIRTY (PteContents))) {

            //
            // Flush the modify bit to the PFN database.
            //

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

            MI_SET_MODIFIED (Pfn1, 1, 0x2);

            MI_SET_PTE_CLEAN (PteContents);

            //
            // No need to capture the PTE contents as we are going to
            // write the page anyway and the Modify bit will be cleared
            // before the write is done.
            //

            MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, PteContents);

            if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
                PteFlushList.FlushVa[PteFlushList.Count] = Va;
                PteFlushList.Count += 1;
            }
        }

        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
        PointerPte += 1;

        if (MiIsPteOnPdeBoundary (PointerPte)) {

            if (PteFlushList.Count != 0) {
                MiFlushPteList (&PteFlushList, SystemCache);
                PteFlushList.Count = 0;
            }

            PointerPde = MiGetPteAddress (PointerPte);

            while (PointerPte <= LastPte) {

                PointerPxe = MiGetPdeAddress (PointerPde);
                PointerPpe = MiGetPteAddress (PointerPde);

                if (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                                 Process,
                                                 OldIrql,
                                                 &Waited)) {

                    //
                    // No page directory parent page exists for this address.
                    //

                    PointerPxe += 1;
                    PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                }
                else if (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                                      Process,
                                                      OldIrql,
                                                      &Waited)) {

                    //
                    // No page directory page exists for this address.
                    //

                    PointerPpe += 1;
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                }
                else {

                    Waited = 0;

                    if (!MiDoesPdeExistAndMakeValid (PointerPde,
                                                     Process,
                                                     OldIrql,
                                                     &Waited)) {

                        //
                        // No page table page exists for this address.
                        //

                        PointerPde += 1;

                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                    }
                    else {

                        //
                        // If the PFN lock (and accordingly the WS mutex) was
                        // released and reacquired we must retry the operation.
                        //

                        if (Waited != 0) {
                            continue;
                        }

                        //
                        // The PFN lock has been held since we acquired the
                        // page directory parent, ie: this PTE we can operate on
                        // immediately.
                        //

                        break;
                    }
                }
            }

            Va = MiGetVirtualAddressMappedByPte (PointerPte);
        }
    }

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList, SystemCache);
    }

    UNLOCK_PFN (OldIrql);
    return;
}

PSUBSECTION
MiGetSystemCacheSubsection (
    IN PVOID BaseAddress,
    OUT PMMPTE *ProtoPte
    )

{
    PMMPTE PointerPte;
    PSUBSECTION Subsection;

    PointerPte = MiGetPteAddress (BaseAddress);

    Subsection = MiGetSubsectionAndProtoFromPte (PointerPte, ProtoPte);

    return Subsection;
}


LOGICAL
MiCheckProtoPtePageState (
    IN PMMPTE PrototypePte,
    IN KIRQL OldIrql,
    OUT PLOGICAL DroppedPfnLock
    )

/*++

Routine Description:

    Checks the state of the page containing the specified prototype PTE.

    If the page is valid or transition and has transition or valid prototype
    PTEs contained with it, TRUE is returned and the page is made valid
    (if transition).  Otherwise return FALSE indicating no prototype
    PTEs within this page are of interest.

Arguments:

    PrototypePte - Supplies a pointer to a prototype PTE within the page.

    OldIrql - Supplies the IRQL the caller acquired the PFN lock at or MM_NOIRQL
              if the caller does not hold the PFN lock.

    DroppedPfnLock - Supplies a pointer to a logical this routine sets to
                     TRUE if the PFN lock is released & reacquired.

Return Value:

    TRUE if the page containing the proto PTE was made resident.
    FALSE otherwise.

--*/

{
    PMMPTE PointerPte;
    MMPTE PteContents;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn;

    *DroppedPfnLock = FALSE;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // First check whether the page directory page is present.  Since there
    // is no lazy loading of PPEs, the validity check alone is sufficient.
    //

    PointerPte = MiGetPdeAddress (PrototypePte);
    PteContents = *PointerPte;

    if (PteContents.u.Hard.Valid == 0) {
        return FALSE;
    }

#endif

    PointerPte = MiGetPteAddress (PrototypePte);

#if (_MI_PAGING_LEVELS < 3)

    if (PointerPte->u.Hard.Valid == 0) {
        MiCheckPdeForPagedPool (PrototypePte);
    }

#endif

    PteContents = *PointerPte;

    if (PteContents.u.Hard.Valid == 1) {
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
        Pfn = MI_PFN_ELEMENT (PageFrameIndex);
        if (Pfn->u2.ShareCount != 1) {
            return TRUE;
        }
    }
    else if ((PteContents.u.Soft.Prototype == 0) &&
               (PteContents.u.Soft.Transition == 1)) {

        //
        // Transition, if on standby or modified, return FALSE.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
        Pfn = MI_PFN_ELEMENT (PageFrameIndex);
        if (Pfn->u3.e1.PageLocation >= ActiveAndValid) {
            if (OldIrql != MM_NOIRQL) {
                MiMakeSystemAddressValidPfn (PrototypePte, OldIrql);
                *DroppedPfnLock = TRUE;
            }
            return TRUE;
        }
    }

    //
    // Page is not resident or is on standby / modified list.
    //

    return FALSE;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\hypermap.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   hypermap.c

Abstract:

    This module contains the routines which map physical pages into
    reserved PTEs within hyper space.

Author:

    Lou Perazzoli (loup) 5-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

PMMPTE MiFirstReservedZeroingPte;

KEVENT MiImageMappingPteEvent;

#pragma alloc_text(PAGE,MiMapImageHeaderInHyperSpace)
#pragma alloc_text(PAGE,MiUnmapImageHeaderInHyperSpace)


PVOID
MiMapPageInHyperSpace (
    IN PEPROCESS Process,
    IN PFN_NUMBER PageFrameIndex,
    IN PKIRQL OldIrql
    )

/*++

Routine Description:

    This procedure maps the specified physical page into hyper space
    and returns the virtual address which maps the page.

    ************************************
    *                                  *
    * Returns with a spin lock held!!! *
    *                                  *
    ************************************

Arguments:

    Process - Supplies the current process.

    PageFrameIndex - Supplies the physical page number to map.

    OldIrql - Supplies a pointer in which to return the entry IRQL.

Return Value:

    Returns the address where the requested page was mapped.

    RETURNS WITH THE HYPERSPACE SPIN LOCK HELD!!!!

    The routine MiUnmapHyperSpaceMap MUST be called to release the lock!!!!

Environment:

    Kernel mode.

--*/

{
    MMPTE TempPte;
    PMMPTE PointerPte;
    PFN_NUMBER offset;

    ASSERT (PageFrameIndex != 0);

    PointerPte = MmFirstReservedMappingPte;

#if defined(NT_UP)
    UNREFERENCED_PARAMETER (Process);
#endif

    LOCK_HYPERSPACE (Process, OldIrql);

    //
    // Get offset to first free PTE.
    //

    offset = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    if (offset == 0) {

        //
        // All the reserved PTEs have been used, make them all invalid.
        //

        MI_MAKING_MULTIPLE_PTES_INVALID (FALSE);

#if DBG
        {
        PMMPTE LastPte;

        LastPte = PointerPte + NUMBER_OF_MAPPING_PTES;

        do {
            ASSERT (LastPte->u.Long == 0);
            LastPte -= 1;
        } while (LastPte > PointerPte);
        }
#endif

        //
        // Use the page frame number field of the first PTE as an
        // offset into the available mapping PTEs.
        //

        offset = NUMBER_OF_MAPPING_PTES;

        //
        // Flush entire TB only on processors executing this process.
        //

        KeFlushProcessTb (FALSE);
    }

    //
    // Change offset for next time through.
    //

    PointerPte->u.Hard.PageFrameNumber = offset - 1;

    //
    // Point to free entry and make it valid.
    //

    PointerPte += offset;
    ASSERT (PointerPte->u.Hard.Valid == 0);


    TempPte = ValidPtePte;
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    //
    // Return the VA that maps the page.
    //

    return MiGetVirtualAddressMappedByPte (PointerPte);
}

PVOID
MiMapPageInHyperSpaceAtDpc (
    IN PEPROCESS Process,
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure maps the specified physical page into hyper space
    and returns the virtual address which maps the page.

    ************************************
    *                                  *
    * Returns with a spin lock held!!! *
    *                                  *
    ************************************

Arguments:

    Process - Supplies the current process.

    PageFrameIndex - Supplies the physical page number to map.

Return Value:

    Returns the address where the requested page was mapped.

    RETURNS WITH THE HYPERSPACE SPIN LOCK HELD!!!!

    The routine MiUnmapHyperSpaceMap MUST be called to release the lock!!!!

Environment:

    Kernel mode, DISPATCH_LEVEL on entry.

--*/

{

    MMPTE TempPte;
    PMMPTE PointerPte;
    PFN_NUMBER offset;

#if defined(NT_UP)
    UNREFERENCED_PARAMETER (Process);
#endif

    ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL);
    ASSERT (PageFrameIndex != 0);

    LOCK_HYPERSPACE_AT_DPC (Process);

    //
    // Get offset to first free PTE.
    //

    PointerPte = MmFirstReservedMappingPte;

    offset = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    if (offset == 0) {

        //
        // All the reserved PTEs have been used, make them all invalid.
        //

        MI_MAKING_MULTIPLE_PTES_INVALID (FALSE);

#if DBG
        {
        PMMPTE LastPte;

        LastPte = PointerPte + NUMBER_OF_MAPPING_PTES;

        do {
            ASSERT (LastPte->u.Long == 0);
            LastPte -= 1;
        } while (LastPte > PointerPte);
        }
#endif

        //
        // Use the page frame number field of the first PTE as an
        // offset into the available mapping PTEs.
        //

        offset = NUMBER_OF_MAPPING_PTES;

        //
        // Flush entire TB only on processors executing this process.
        //

        KeFlushProcessTb (FALSE);
    }

    //
    // Change offset for next time through.
    //

    PointerPte->u.Hard.PageFrameNumber = offset - 1;

    //
    // Point to free entry and make it valid.
    //

    PointerPte += offset;
    ASSERT (PointerPte->u.Hard.Valid == 0);


    TempPte = ValidPtePte;
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    //
    // Return the VA that maps the page.
    //

    return MiGetVirtualAddressMappedByPte (PointerPte);
}

PVOID
MiMapImageHeaderInHyperSpace (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    This procedure maps the specified physical page into the
    PTE within hyper space reserved explicitly for image page
    header mapping.  By reserving an explicit PTE for mapping
    the PTE, page faults can occur while the PTE is mapped within
    hyperspace and no other hyperspace maps will affect this PTE.

    Note that if another thread attempts to map an image at the
    same time, it will be forced into a wait state until the
    header is "unmapped".

Arguments:

    PageFrameIndex - Supplies the physical page number to map.

Return Value:

    Returns the virtual address where the specified physical page was
    mapped.

Environment:

    Kernel mode.

--*/

{
    MMPTE TempPte;
    MMPTE OriginalPte;
    PMMPTE PointerPte;

    ASSERT (PageFrameIndex != 0);

    TempPte = ValidPtePte;
    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    //
    // Ensure both modified and accessed bits are set so the hardware doesn't
    // ever write this PTE.
    //

    ASSERT (TempPte.u.Hard.Dirty == 1);
    ASSERT (TempPte.u.Hard.Accessed == 1);

    PointerPte = MiGetPteAddress (IMAGE_MAPPING_PTE);

    do {
        OriginalPte.u.Long = 0;

        OriginalPte.u.Long = InterlockedCompareExchangePte (
                                PointerPte,
                                TempPte.u.Long,
                                OriginalPte.u.Long);
                                                             
        if (OriginalPte.u.Long == 0) {
            break;
        }

        //
        // Another thread modified the PTE just before us or the PTE was
        // already in use.  This should be very rare - go the long way.
        //

        InterlockedIncrement ((PLONG)&MmWorkingSetList->NumberOfImageWaiters);

        //
        // Deliberately wait with a timeout since the PTE release runs
        // without lock synchronization so there is the extremely rare
        // race window which the timeout saves us from.
        //

        KeWaitForSingleObject (&MiImageMappingPteEvent,
                               Executive,
                               KernelMode,
                               FALSE,
                               (PLARGE_INTEGER)&MmOneSecond);

        InterlockedDecrement ((PLONG)&MmWorkingSetList->NumberOfImageWaiters);

    } while (TRUE);

    //
    // Flush the specified TB entry without writing the PTE as we
    // always want to do interlocked writes to this PTE and this is
    // being done above.
    //
    // Note the flush must be made across all processors as this thread
    // may migrate.  Also this must be done here instead of in the unmap
    // in order to support lock-free operation.
    //

    KeFlushSingleTb (IMAGE_MAPPING_PTE, TRUE);

    return (PVOID) IMAGE_MAPPING_PTE;
}

VOID
MiUnmapImageHeaderInHyperSpace (
    VOID
    )

/*++

Routine Description:

    This procedure unmaps the PTE reserved for mapping the image
    header, flushes the TB, and, if the WaitingForImageMapping field
    is not NULL, sets the specified event.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PMMPTE PointerPte;

    PointerPte = MiGetPteAddress (IMAGE_MAPPING_PTE);

    //
    // Capture the number of waiters.
    //

    ASSERT (PointerPte->u.Long != 0);

    InterlockedExchangePte (PointerPte, ZeroPte.u.Long);

    if (MmWorkingSetList->NumberOfImageWaiters != 0) {

        //
        // If there are any threads waiting, wake them all now.  Note this
        // will wake threads in other processes as well, but it is very
        // rare that there are any waiters in the entire system period.
        //

        KePulseEvent (&MiImageMappingPteEvent, 0, FALSE);
    }

    return;
}

PVOID
MiMapPagesToZeroInHyperSpace (
    IN PMMPFN Pfn1,
    IN PFN_COUNT NumberOfPages
    )

/*++

Routine Description:

    This procedure maps the specified physical pages for the zero page thread
    and returns the virtual address which maps them.

    This is ONLY to be used by THE zeroing page thread.

Arguments:

    Pfn1 - Supplies the pointer to the physical page numbers to map.

    NumberOfPages - Supplies the number of pages to map.

Return Value:

    Returns the virtual address where the specified physical pages were
    mapped.

Environment:

    PASSIVE_LEVEL.

--*/

{
    PFN_NUMBER offset;
    MMPTE TempPte;
    PMMPTE PointerPte;
    PFN_NUMBER PageFrameIndex;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ASSERT (NumberOfPages != 0);
    ASSERT (NumberOfPages <= NUMBER_OF_ZEROING_PTES);

    PointerPte = MiFirstReservedZeroingPte;

    //
    // Get offset to first free PTE.
    //

    offset = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    if (NumberOfPages > offset) {

        //
        // Not enough unused PTEs left, make them all invalid.
        //

        MI_MAKING_MULTIPLE_PTES_INVALID (FALSE);

#if DBG
        {
        PMMPTE LastPte;

        LastPte = PointerPte + NUMBER_OF_ZEROING_PTES;

        do {
            ASSERT (LastPte->u.Long == 0);
            LastPte -= 1;
        } while (LastPte > PointerPte);
        }
#endif

        //
        // Use the page frame number field of the first PTE as an
        // offset into the available zeroing PTEs.
        //

        offset = NUMBER_OF_ZEROING_PTES;
        PointerPte->u.Hard.PageFrameNumber = offset;

        //
        // Flush entire TB only on processors executing this process as this
        // thread may migrate there at any time.
        //

        KeFlushProcessTb (FALSE);
    }

    //
    // Change offset for next time through.
    //

    PointerPte->u.Hard.PageFrameNumber = offset - NumberOfPages;

    //
    // Point to free entries and make them valid.  Note that the frames
    // are mapped in reverse order but our caller doesn't care anyway.
    //

    PointerPte += (offset + 1);

    TempPte = ValidPtePte;

    ASSERT (Pfn1 != (PMMPFN) MM_EMPTY_LIST);

    do {

        PageFrameIndex = MI_PFN_ELEMENT_TO_INDEX (Pfn1);

        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

        PointerPte -= 1;

        ASSERT (PointerPte->u.Hard.Valid == 0);

        MI_WRITE_VALID_PTE (PointerPte, TempPte);

        Pfn1 = (PMMPFN) Pfn1->u1.Flink;

    } while (Pfn1 != (PMMPFN) MM_EMPTY_LIST);

    //
    // Return the VA that maps the page.
    //

    return MiGetVirtualAddressMappedByPte (PointerPte);
}

VOID
MiUnmapPagesInZeroSpace (
    IN PVOID VirtualAddress,
    IN PFN_COUNT NumberOfPages
    )

/*++

Routine Description:

    This procedure unmaps the specified physical pages for the zero page thread.

    This is ONLY to be used by THE zeroing page thread.

Arguments:

    VirtualAddress - Supplies the pointer to the physical page numbers to unmap.

    NumberOfPages - Supplies the number of pages to unmap.

Return Value:

    None.

Environment:

    PASSIVE_LEVEL.

--*/

{
    PMMPTE PointerPte;

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    ASSERT (NumberOfPages != 0);
    ASSERT (NumberOfPages <= NUMBER_OF_ZEROING_PTES);

    PointerPte = MiGetPteAddress (VirtualAddress);

    MiZeroMemoryPte (PointerPte, NumberOfPages);

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\lockvm.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   lockvm.c

Abstract:

    This module contains the routines which implement the
    NtLockVirtualMemory service.

Author:

    Lou Perazzoli (loup) 20-August-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtLockVirtualMemory)
#pragma alloc_text(PAGE,NtUnlockVirtualMemory)
#endif


NTSTATUS
NtLockVirtualMemory (
    IN HANDLE ProcessHandle,
    IN OUT PVOID *BaseAddress,
    IN OUT PSIZE_T RegionSize,
    IN ULONG MapType
     )

/*++

Routine Description:

    This function locks a region of pages within the working set list
    of a subject process.

    The caller of this function must have PROCESS_VM_OPERATION access
    to the target process.  The caller must also have SeLockMemoryPrivilege.

Arguments:

   ProcessHandle - Supplies an open handle to a process object.

   BaseAddress - The base address of the region of pages
                 to be locked. This value is rounded down to the
                 next host page address boundary.

   RegionSize - A pointer to a variable that will receive
                the actual size in bytes of the locked region of
                pages. The initial value of this argument is
                rounded up to the next host page size boundary.

   MapType - A set of flags that describe the type of locking to
             perform.  One of MAP_PROCESS or MAP_SYSTEM.

Return Value:

    NTSTATUS.

    STATUS_PRIVILEGE_NOT_HELD - The caller did not have sufficient
                                privilege to perform the requested operation.

--*/

{
    PVOID Va;
    PVOID StartingVa;
    PVOID EndingAddress;
    KAPC_STATE ApcState;
    PMMPTE PointerPte;
    PMMPTE PointerPte1;
    PMMPFN Pfn1;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    ULONG_PTR CapturedRegionSize;
    PVOID CapturedBase;
    PEPROCESS TargetProcess;
    NTSTATUS Status;
    LOGICAL WasLocked;
    KPROCESSOR_MODE PreviousMode;
    WSLE_NUMBER Entry;
    WSLE_NUMBER SwapEntry;
    SIZE_T NumberOfAlreadyLocked;
    SIZE_T NumberToLock;
    WSLE_NUMBER WorkingSetIndex;
    PMMVAD Vad;
    PVOID LastVa;
    LOGICAL Attached;
    PETHREAD Thread;
#if defined(_MIALT4K_)
    PVOID Wow64Process;
#endif

    PAGED_CODE();

    WasLocked = FALSE;
    LastVa = NULL;

    //
    // Validate the flags in MapType.
    //

    if ((MapType & ~(MAP_PROCESS | MAP_SYSTEM)) != 0) {
        return STATUS_INVALID_PARAMETER;
    }

    if ((MapType & (MAP_PROCESS | MAP_SYSTEM)) == 0) {
        return STATUS_INVALID_PARAMETER;
    }

    Thread = PsGetCurrentThread ();

    PreviousMode = KeGetPreviousModeByThread (&Thread->Tcb);

    try {

        if (PreviousMode != KernelMode) {

            ProbeForWritePointer ((PULONG)BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
        }

        //
        // Capture the base address.
        //

        CapturedBase = *BaseAddress;

        //
        // Capture the region size.
        //

        CapturedRegionSize = *RegionSize;

    } except (ExSystemExceptionFilter ()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode ();
    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_USER_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER;
    }

    if ((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (ULONG_PTR)CapturedBase <
                                                        CapturedRegionSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER;

    }

    if (CapturedRegionSize == 0) {
        return STATUS_INVALID_PARAMETER;
    }

    //
    // Reference the specified process.
    //

    Status = ObReferenceObjectByHandle (ProcessHandle,
                                        PROCESS_VM_OPERATION,
                                        PsProcessType,
                                        PreviousMode,
                                        (PVOID *)&TargetProcess,
                                        NULL);

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    if ((MapType & MAP_SYSTEM) != 0) {

        //
        // In addition to PROCESS_VM_OPERATION access to the target
        // process, the caller must have SE_LOCK_MEMORY_PRIVILEGE.
        //

        if (!SeSinglePrivilegeCheck (SeLockMemoryPrivilege, PreviousMode)) {

            ObDereferenceObject (TargetProcess);
            return STATUS_PRIVILEGE_NOT_HELD;
        }
    }

    //
    // Attach to the specified process.
    //

    if (ProcessHandle != NtCurrentProcess ()) {
        KeStackAttachProcess (&TargetProcess->Pcb, &ApcState);
        Attached = TRUE;
    }
    else {
        Attached = FALSE;
    }

    //
    // Get address creation mutex, this prevents the
    // address range from being modified while it is examined.  Raise
    // to APC level to prevent an APC routine from acquiring the
    // address creation mutex.  Get the working set mutex so the
    // number of already locked pages in the request can be determined.
    //

#if defined(_MIALT4K_)

    //
    // Changing to 4k aligned should not change the correctness.
    //

    EndingAddress = PAGE_4K_ALIGN((PCHAR)CapturedBase + CapturedRegionSize - 1);
#else
    EndingAddress = PAGE_ALIGN((PCHAR)CapturedBase + CapturedRegionSize - 1);
#endif

    Va = PAGE_ALIGN (CapturedBase);
    NumberOfAlreadyLocked = 0;
    NumberToLock = ((ULONG_PTR)EndingAddress - (ULONG_PTR)Va) >> PAGE_SHIFT;

    LOCK_ADDRESS_SPACE (TargetProcess);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (TargetProcess->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn1;
    }

    if (NumberToLock + MM_FLUID_WORKING_SET >
                                    TargetProcess->Vm.MinimumWorkingSetSize) {
        Status = STATUS_WORKING_SET_QUOTA;
        goto ErrorReturn1;
    }

    //
    // Note the working set mutex must be held throughout the loop below to
    // prevent other threads from locking or unlocking WSL entries.
    //

    LOCK_WS_UNSAFE (TargetProcess);

    while (Va <= EndingAddress) {

        if (Va > LastVa) {

            //
            // Don't lock physically mapped views.
            //

            Vad = MiLocateAddress (Va);

            if (Vad == NULL) {
                Status = STATUS_ACCESS_VIOLATION;
                goto ErrorReturn;
            }

            if ((Vad->u.VadFlags.PhysicalMapping == 1) ||
                (Vad->u.VadFlags.LargePages == 1) ||
                (Vad->u.VadFlags.UserPhysicalPages == 1)) {
                Status = STATUS_INCOMPATIBLE_FILE_MAP;
                goto ErrorReturn;
            }
            LastVa = MI_VPN_TO_VA (Vad->EndingVpn);
        }

        if (MiIsAddressValid (Va, TRUE)) {

            //
            // The page is valid, therefore it is in the working set.
            // Locate the WSLE for the page and see if it is locked.
            //

            PointerPte1 = MiGetPteAddress (Va);
            Pfn1 = MI_PFN_ELEMENT (PointerPte1->u.Hard.PageFrameNumber);

            WorkingSetIndex = MiLocateWsle (Va,
                                            MmWorkingSetList,
                                            Pfn1->u1.WsIndex);

            ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);

            if (WorkingSetIndex < MmWorkingSetList->FirstDynamic) {

                //
                // This page is locked in the working set.
                //

                NumberOfAlreadyLocked += 1;

                //
                // Check to see if the WAS_LOCKED status should be returned.
                //

                if ((MapType & MAP_PROCESS) &&
                        (MmWsle[WorkingSetIndex].u1.e1.LockedInWs == 1)) {
                    WasLocked = TRUE;
                }

                if ((MapType & MAP_SYSTEM) &&
                        (MmWsle[WorkingSetIndex].u1.e1.LockedInMemory == 1)) {
                    WasLocked = TRUE;
                }
            }
        }
        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
    }

    UNLOCK_WS_UNSAFE (TargetProcess);

    //
    // Check to ensure the working set list is still fluid after
    // the requested number of pages are locked.
    //

    if (TargetProcess->Vm.MinimumWorkingSetSize <
          ((MmWorkingSetList->FirstDynamic + NumberToLock +
                      MM_FLUID_WORKING_SET) - NumberOfAlreadyLocked)) {

        Status = STATUS_WORKING_SET_QUOTA;
        goto ErrorReturn1;
    }

    Va = PAGE_ALIGN (CapturedBase);

#if defined(_MIALT4K_)

    Wow64Process = TargetProcess->Wow64Process;

    if (Wow64Process != NULL) {
        Va = PAGE_4K_ALIGN (CapturedBase);
    }

#endif

    //
    // Set up an exception handler and touch each page in the specified
    // range.  Mark this thread as the address space mutex owner so it cannot
    // sneak its stack in as the argument region and trick us into trying to
    // grow it if the reference faults (as this would cause a deadlock since
    // this thread already owns the address space mutex).  Note this would have
    // the side effect of not allowing this thread to fault on guard pages in
    // other data regions while the accesses below are ongoing - but that could
    // only happen in an APC and those are blocked right now anyway.
    //

    ASSERT (KeAreAllApcsDisabled () == TRUE);
    ASSERT (Thread->AddressSpaceOwner == 0);
    Thread->AddressSpaceOwner = 1;

    try {

        while (Va <= EndingAddress) {
            *(volatile ULONG *)Va;
            Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
        }

    } except (EXCEPTION_EXECUTE_HANDLER) {
        Status = GetExceptionCode();
        ASSERT (KeAreAllApcsDisabled () == TRUE);
        ASSERT (Thread->AddressSpaceOwner == 1);
        Thread->AddressSpaceOwner = 0;
        goto ErrorReturn1;
    }

    ASSERT (KeAreAllApcsDisabled () == TRUE);
    ASSERT (Thread->AddressSpaceOwner == 1);
    Thread->AddressSpaceOwner = 0;

    //
    // The complete address range is accessible, lock the pages into
    // the working set.
    //

    PointerPte = MiGetPteAddress (CapturedBase);
    Va = PAGE_ALIGN (CapturedBase);

#if defined(_MIALT4K_)

    if (Wow64Process != NULL) {
        Va = PAGE_4K_ALIGN (CapturedBase);
    }

#endif

    StartingVa = Va;

    //
    // Acquire the working set mutex, no page faults are allowed.
    //

    LOCK_WS_UNSAFE (TargetProcess);

    while (Va <= EndingAddress) {

        //
        // Make sure the PDE is valid.
        //

        PointerPde = MiGetPdeAddress (Va);
        PointerPpe = MiGetPpeAddress (Va);
        PointerPxe = MiGetPxeAddress (Va);

        //
        // Ensure the PDE (and any table above it) are still
        // resident.
        //

        MiMakePdeExistAndMakeValid (PointerPde,
                                    TargetProcess,
                                    MM_NOIRQL);

        //
        // Make sure the page is in the working set.
        //

        while (PointerPte->u.Hard.Valid == 0) {

            //
            // Release the working set mutex and fault in the page.
            //

            UNLOCK_WS_UNSAFE (TargetProcess);

            //
            // Page in the PDE and make the PTE valid.
            //

            try {
                *(volatile ULONG *)Va;
            } except (EXCEPTION_EXECUTE_HANDLER) {

                //
                // Since all the pages were accessed above with the address
                // space mutex held and it is still held now, the only way
                // an exception could occur would be due to a device error,
                // ie: hardware malfunction, net cable disconnection, CD
                // being removed, etc.
                //
                // Recompute EndingAddress so the actual number of pages locked
                // is written back to the user now.  If this is the very first
                // page then return a failure status.
                //

                EndingAddress = PAGE_ALIGN (Va);

#if defined(_MIALT4K_)
                if (Wow64Process != NULL) {
                    EndingAddress = PAGE_4K_ALIGN (Va);
                }
#endif

                if (EndingAddress == StartingVa) {
                    Status = GetExceptionCode ();
                    goto ErrorReturn1;
                }

                ASSERT (NT_SUCCESS (Status));
                EndingAddress = (PVOID)((ULONG_PTR)EndingAddress - 1);
#if defined(_MIALT4K_)
                if (Wow64Process != NULL) {
                    CapturedRegionSize = (ULONG_PTR)EndingAddress - (ULONG_PTR)CapturedBase;
                }
#endif
                goto SuccessReturn1;
            }

            //
            // Reacquire the working set mutex.
            //

            LOCK_WS_UNSAFE (TargetProcess);

            //
            // Ensure the PDE (and any table above it) are still resident.
            // They could have been trimmed from the working set before the
            // working set lock was reacquired above.
            //

            MiMakePdeExistAndMakeValid (PointerPde,
                                        TargetProcess,
                                        MM_NOIRQL);
        }

        //
        // The page is now in the working set, lock the page into
        // the working set.
        //

        PointerPte1 = MiGetPteAddress (Va);
        Pfn1 = MI_PFN_ELEMENT (PointerPte1->u.Hard.PageFrameNumber);

        Entry = MiLocateWsle (Va, MmWorkingSetList, Pfn1->u1.WsIndex);

        if (Entry >= MmWorkingSetList->FirstDynamic) {

            SwapEntry = MmWorkingSetList->FirstDynamic;

            if (Entry != MmWorkingSetList->FirstDynamic) {

                //
                // Swap this entry with the one at first dynamic.
                //

                MiSwapWslEntries (Entry, SwapEntry, &TargetProcess->Vm, FALSE);
            }

            MmWorkingSetList->FirstDynamic += 1;
        }
        else {
            SwapEntry = Entry;
        }

        //
        // Indicate that the page is locked.
        //

        if (MapType & MAP_PROCESS) {
            MmWsle[SwapEntry].u1.e1.LockedInWs = 1;
        }

        if (MapType & MAP_SYSTEM) {
            MmWsle[SwapEntry].u1.e1.LockedInMemory = 1;
        }

        //
        // Increment to the next Va and PTE.
        //

        PointerPte += 1;
        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
    }

    UNLOCK_WS_UNSAFE (TargetProcess);

SuccessReturn1:

#if (defined(_MIALT4K_))

    if (Wow64Process != NULL) {
        MiLockFor4kPage (CapturedBase, CapturedRegionSize, TargetProcess);
    }

#endif

    UNLOCK_ADDRESS_SPACE (TargetProcess);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }
    ObDereferenceObject (TargetProcess);

    //
    // Update return arguments.
    //

    //
    // Establish an exception handler and write the size and base
    // address.
    //

    try {

#if defined(_MIALT4K_)

        if (Wow64Process != NULL) { 

            *RegionSize = ((PCHAR)EndingAddress -
                        (PCHAR)PAGE_4K_ALIGN(CapturedBase)) + PAGE_4K;

            *BaseAddress = PAGE_4K_ALIGN(CapturedBase);


        }
        else {    

#endif
        *RegionSize = ((PCHAR)EndingAddress - (PCHAR)PAGE_ALIGN(CapturedBase)) +
                                                                    PAGE_SIZE;
        *BaseAddress = PAGE_ALIGN(CapturedBase);

#if defined(_MIALT4K_)
        }
#endif

    } except (EXCEPTION_EXECUTE_HANDLER) {
        return GetExceptionCode();
    }

    if (WasLocked) {
        return STATUS_WAS_LOCKED;
    }

    return STATUS_SUCCESS;

ErrorReturn:
        UNLOCK_WS_UNSAFE (TargetProcess);
ErrorReturn1:
        UNLOCK_ADDRESS_SPACE (TargetProcess);
        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }
        ObDereferenceObject (TargetProcess);
        return Status;
}

NTSTATUS
NtUnlockVirtualMemory (
    IN HANDLE ProcessHandle,
    IN OUT PVOID *BaseAddress,
    IN OUT PSIZE_T RegionSize,
    IN ULONG MapType
    )

/*++

Routine Description:

    This function unlocks a region of pages within the working set list
    of a subject process.

    As a side effect, any pages which are not locked and are in the
    process's working set are removed from the process's working set.
    This allows NtUnlockVirtualMemory to remove a range of pages
    from the working set.

    The caller of this function must have PROCESS_VM_OPERATION access
    to the target process.

    The caller must also have SeLockMemoryPrivilege for MAP_SYSTEM.

Arguments:

   ProcessHandle - Supplies an open handle to a process object.

   BaseAddress - The base address of the region of pages
                 to be unlocked. This value is rounded down to the
                 next host page address boundary.

   RegionSize - A pointer to a variable that will receive
                the actual size in bytes of the unlocked region of
                pages. The initial value of this argument is
                rounded up to the next host page size boundary.

   MapType - A set of flags that describe the type of unlocking to
             perform.  One of MAP_PROCESS or MAP_SYSTEM.

Return Value:

    NTSTATUS.

--*/

{
    PVOID Va;
    PVOID EndingAddress;
    SIZE_T CapturedRegionSize;
    PVOID CapturedBase;
    PEPROCESS TargetProcess;
    NTSTATUS Status;
    KPROCESSOR_MODE PreviousMode;
    WSLE_NUMBER Entry;
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    PMMVAD Vad;
    PVOID LastVa;
    LOGICAL Attached;
    KAPC_STATE ApcState;
#if defined(_MIALT4K_)
    PVOID Wow64Process;
#endif

    PAGED_CODE();

    LastVa = NULL;

    //
    // Validate the flags in MapType.
    //

    if ((MapType & ~(MAP_PROCESS | MAP_SYSTEM)) != 0) {
        return STATUS_INVALID_PARAMETER;
    }

    if ((MapType & (MAP_PROCESS | MAP_SYSTEM)) == 0) {
        return STATUS_INVALID_PARAMETER;
    }

    PreviousMode = KeGetPreviousMode();

    try {

        if (PreviousMode != KernelMode) {

            ProbeForWritePointer (BaseAddress);
            ProbeForWriteUlong_ptr (RegionSize);
        }

        //
        // Capture the base address.
        //

        CapturedBase = *BaseAddress;

        //
        // Capture the region size.
        //

        CapturedRegionSize = *RegionSize;

    } except (ExSystemExceptionFilter ()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode ();
    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_USER_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER;
    }

    if ((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (ULONG_PTR)CapturedBase <
                                                        CapturedRegionSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER;

    }

    if (CapturedRegionSize == 0) {
        return STATUS_INVALID_PARAMETER;
    }

    Status = ObReferenceObjectByHandle (ProcessHandle,
                                        PROCESS_VM_OPERATION,
                                        PsProcessType,
                                        PreviousMode,
                                        (PVOID *)&TargetProcess,
                                        NULL);

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

#if defined(_MIALT4K_)
    Wow64Process = TargetProcess->Wow64Process;
#endif

    if ((MapType & MAP_SYSTEM) != 0) {

        //
        // In addition to PROCESS_VM_OPERATION access to the target
        // process, the caller must have SE_LOCK_MEMORY_PRIVILEGE.
        //

        if (!SeSinglePrivilegeCheck(
                           SeLockMemoryPrivilege,
                           PreviousMode
                           )) {

            ObDereferenceObject (TargetProcess);
            return STATUS_PRIVILEGE_NOT_HELD;
        }
    }

    //
    // Attach to the specified process.
    //

    if (ProcessHandle != NtCurrentProcess()) {
        KeStackAttachProcess (&TargetProcess->Pcb, &ApcState);
        Attached = TRUE;
    }
    else {
        Attached = FALSE;
    }

    EndingAddress = PAGE_ALIGN((PCHAR)CapturedBase + CapturedRegionSize - 1);

    Va = PAGE_ALIGN (CapturedBase);

    //
    // Get address creation mutex, this prevents the
    // address range from being modified while it is examined.
    // Block APCs so an APC routine can't get a page fault and
    // corrupt the working set list, etc.
    //

    LOCK_ADDRESS_SPACE (TargetProcess);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (TargetProcess->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn1;
    }

    LOCK_WS_UNSAFE (TargetProcess);

    while (Va <= EndingAddress) {

        //
        // Check to ensure all the specified pages are locked.
        //

        if (Va > LastVa) {
            Vad = MiLocateAddress (Va);
            if (Vad == NULL) {
                Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
                Status = STATUS_NOT_LOCKED;
                break;
            }

            //
            // Don't unlock physically mapped views.
            //

            if ((Vad->u.VadFlags.PhysicalMapping == 1) ||
                (Vad->u.VadFlags.LargePages == 1) ||
                (Vad->u.VadFlags.UserPhysicalPages == 1)) {
                Va = MI_VPN_TO_VA (Vad->EndingVpn);
                break;
            }
            LastVa = MI_VPN_TO_VA (Vad->EndingVpn);
        }

        if (!MiIsAddressValid (Va, TRUE)) {

            //
            // This page is not valid, therefore not in working set.
            //

            Status = STATUS_NOT_LOCKED;
        }
        else {

            PointerPte = MiGetPteAddress (Va);
            ASSERT (PointerPte->u.Hard.Valid != 0);
            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
            Entry = MiLocateWsle (Va, MmWorkingSetList, Pfn1->u1.WsIndex);
            ASSERT (Entry != WSLE_NULL_INDEX);

            if ((MmWsle[Entry].u1.e1.LockedInWs == 0) &&
                (MmWsle[Entry].u1.e1.LockedInMemory == 0)) {

                //
                // Not locked in memory or system, remove from working
                // set.
                //

                PERFINFO_PAGE_INFO_DECL();

                PERFINFO_GET_PAGE_INFO(PointerPte);

                if (MiFreeWsle (Entry, &TargetProcess->Vm, PointerPte)) {
                    PERFINFO_LOG_WS_REMOVAL(PERFINFO_LOG_TYPE_OUTWS_EMPTYQ, &TargetProcess->Vm);
                }

                Status = STATUS_NOT_LOCKED;

            }
            else if (MapType & MAP_PROCESS) {
                if (MmWsle[Entry].u1.e1.LockedInWs == 0)  {

                    //
                    // This page is not locked.
                    //

                    Status = STATUS_NOT_LOCKED;
                }
            }
            else {
                if (MmWsle[Entry].u1.e1.LockedInMemory == 0)  {

                    //
                    // This page is not locked.
                    //

                    Status = STATUS_NOT_LOCKED;
                }
            }
        }
        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
    }

#if defined(_MIALT4K_)

    if (Wow64Process != NULL) {

        //
        // This call may release and reacquire the working set mutex !!!
        //
        // Therefore the loop following must handle PTEs which have been
        // trimmed during this window.
        //

        Status = MiUnlockFor4kPage (CapturedBase,
                                    CapturedRegionSize,
                                    TargetProcess);
    }

#endif

    if (Status == STATUS_NOT_LOCKED) {
        goto ErrorReturn;
    }

    //
    // The complete address range is locked, unlock them.
    //

    Va = PAGE_ALIGN (CapturedBase);
    LastVa = NULL;

    while (Va <= EndingAddress) {

#if defined(_MIALT4K_)

        if (Wow64Process != NULL) {

            //
            // This call may release and reacquire the working set mutex !!!
            //
            // Therefore the code below must handle PTEs which have been
            // trimmed during this window.
            //

            if (!MiShouldBeUnlockedFor4kPage(Va, TargetProcess)) {

                //
                // The other 4k pages in the native page still hold
                // the page lock.  Should skip unlocking.
                //

                Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
                continue;
            }
        }

#endif
        //
        // Don't unlock physically mapped views.
        //

        if (Va > LastVa) {
            Vad = MiLocateAddress (Va);
            ASSERT (Vad != NULL);

            if ((Vad->u.VadFlags.PhysicalMapping == 1) ||
                (Vad->u.VadFlags.LargePages == 1) ||
                (Vad->u.VadFlags.UserPhysicalPages == 1)) {
                Va = MI_VPN_TO_VA (Vad->EndingVpn);
                break;
            }
            LastVa = MI_VPN_TO_VA (Vad->EndingVpn);
        }

#if defined(_MIALT4K_)
        if (!MiIsAddressValid (Va, TRUE)) {

            //
            // The page or any mapping table page may have been trimmed when
            // MiUnlockFor4kPage or MiShouldBeUnlockedFor4kPage released the
            // working set mutex.  If this has occurred, then clearly the
            // address is no longer locked so just skip it.
            //

            Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
            continue;
        }
#endif

        PointerPte = MiGetPteAddress (Va);
        ASSERT (PointerPte->u.Hard.Valid == 1);
        Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);
        Entry = MiLocateWsle (Va, MmWorkingSetList, Pfn1->u1.WsIndex);

        if (MapType & MAP_PROCESS) {
            MmWsle[Entry].u1.e1.LockedInWs = 0;
        }

        if (MapType & MAP_SYSTEM) {
            MmWsle[Entry].u1.e1.LockedInMemory = 0;
        }

        if ((MmWsle[Entry].u1.e1.LockedInMemory == 0) &&
             MmWsle[Entry].u1.e1.LockedInWs == 0) {

            //
            // The page is no longer should be locked, move
            // it to the dynamic part of the working set.
            //

            MmWorkingSetList->FirstDynamic -= 1;

            if (Entry != MmWorkingSetList->FirstDynamic) {

                //
                // Swap this element with the last locked page, making
                // this element the new first dynamic entry.
                //

                MiSwapWslEntries (Entry,
                                  MmWorkingSetList->FirstDynamic,
                                  &TargetProcess->Vm,
                                  FALSE);
            }
        }

        Va = (PVOID)((PCHAR)Va + PAGE_SIZE);
    }

    UNLOCK_WS_AND_ADDRESS_SPACE (TargetProcess);

    if (Attached == TRUE) {
        KeUnstackDetachProcess (&ApcState);
    }
    ObDereferenceObject (TargetProcess);

    //
    // Update return arguments.
    //
    // Establish an exception handler and write the size and base
    // address.
    //

    try {

#if defined(_MIALT4K_)

        if (Wow64Process != NULL) { 

            *RegionSize = ((PCHAR)EndingAddress -
                        (PCHAR)PAGE_4K_ALIGN(CapturedBase)) + PAGE_4K;

            *BaseAddress = PAGE_4K_ALIGN(CapturedBase);


        }
        else {    

#endif
        *RegionSize = ((PCHAR)EndingAddress -
                        (PCHAR)PAGE_ALIGN(CapturedBase)) + PAGE_SIZE;

        *BaseAddress = PAGE_ALIGN(CapturedBase);

#if defined(_MIALT4K_)
        }
#endif

    } except (EXCEPTION_EXECUTE_HANDLER) {
        return GetExceptionCode ();
    }

    return STATUS_SUCCESS;

ErrorReturn:

    UNLOCK_WS_UNSAFE (TargetProcess);

ErrorReturn1:

    UNLOCK_ADDRESS_SPACE (TargetProcess);

        if (Attached == TRUE) {
            KeUnstackDetachProcess (&ApcState);
        }
        ObDereferenceObject (TargetProcess);
        return Status;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\mapcache.c ===
/*++

Copyright (c) 1990  Microsoft Corporation

Module Name:

    mapcache.c

Abstract:

    This module contains the routines which implement mapping views
    of sections into the system-wide cache.

Author:

    Lou Perazzoli (loup) 22-May-1990
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/


#include "mi.h"

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiInitializeSystemCache)
#pragma alloc_text(PAGE,MiAddMappedPtes)
#endif

extern ULONG MmFrontOfList;

#define X256K 0x40000

PMMPTE MmFirstFreeSystemCache;

PMMPTE MmLastFreeSystemCache;

PMMPTE MmSystemCachePteBase;

ULONG MiMapCacheFailures;

LONG
MiMapCacheExceptionFilter (
    IN PNTSTATUS Status,
    IN PEXCEPTION_POINTERS ExceptionPointer
    );

NTSTATUS
MmMapViewInSystemCache (
    IN PVOID SectionToMap,
    OUT PVOID *CapturedBase,
    IN OUT PLARGE_INTEGER SectionOffset,
    IN OUT PULONG CapturedViewSize
    )

/*++

Routine Description:

    This function maps a view in the specified subject process to
    the section object.  The page protection is identical to that
    of the prototype PTE.

    This function is a kernel mode interface to allow LPC to map
    a section given the section pointer to map.

    This routine assumes all arguments have been probed and captured.

Arguments:

    SectionToMap - Supplies a pointer to the section object.

    BaseAddress - Supplies a pointer to a variable that will receive
                  the base address of the view. If the initial value
                  of this argument is not null, then the view will
                  be allocated starting at the specified virtual
                  address rounded down to the next 64kb address
                  boundary. If the initial value of this argument is
                  null, then the operating system will determine
                  where to allocate the view using the information
                  specified by the ZeroBits argument value and the
                  section allocation attributes (i.e. based and tiled).

    SectionOffset - Supplies the offset from the beginning of the section
                    to the view in bytes. This value must be a multiple
                    of 256k.

    ViewSize - Supplies a pointer to a variable that will receive
               the actual size in bytes of the view.  The initial
               values of this argument specifies the size of the view
               in bytes and is rounded up to the next host page size
               boundary and must be less than or equal to 256k.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    PSECTION Section;
    UINT64 PteOffset;
    UINT64 LastPteOffset;
    KIRQL OldIrql;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE ProtoPte;
    PMMPTE LastProto;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    NTSTATUS Status;
    ULONG Waited;
    MMPTE PteContents;
    PFN_NUMBER NumberOfPages;
#if DBG
    PMMPTE PointerPte2;
#endif

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    Section = SectionToMap;

    //
    // Assert the view size is less than 256k and the section offset
    // is aligned on a 256k boundary.
    //

    ASSERT (*CapturedViewSize <= X256K);
    ASSERT ((SectionOffset->LowPart & (X256K - 1)) == 0);

    //
    // Make sure the section is not an image section or a page file
    // backed section.
    //

    if (Section->u.Flags.Image) {
        return STATUS_NOT_MAPPED_DATA;
    }

    ControlArea = Section->Segment->ControlArea;

    ASSERT (*CapturedViewSize != 0);

    NumberOfPages = BYTES_TO_PAGES (*CapturedViewSize);

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    if (ControlArea->u.Flags.Rom == 0) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    //
    // Calculate the first prototype PTE address.
    //

    PteOffset = (UINT64)(SectionOffset->QuadPart >> PAGE_SHIFT);
    LastPteOffset = PteOffset + NumberOfPages;

    //
    // Make sure the PTEs are not in the extended part of the segment.
    //

    while (PteOffset >= (UINT64) Subsection->PtesInSubsection) {
        PteOffset -= Subsection->PtesInSubsection;
        LastPteOffset -= Subsection->PtesInSubsection;
        Subsection = Subsection->NextSubsection;
    }

    LOCK_PFN (OldIrql);

    ASSERT (ControlArea->u.Flags.BeingCreated == 0);
    ASSERT (ControlArea->u.Flags.BeingDeleted == 0);
    ASSERT (ControlArea->u.Flags.BeingPurged == 0);

    //
    // Find a free 256k base in the cache.
    //

    if (MmFirstFreeSystemCache == (PMMPTE)MM_EMPTY_LIST) {
        UNLOCK_PFN (OldIrql);
        return STATUS_NO_MEMORY;
    }

    PointerPte = MmFirstFreeSystemCache;

    //
    // Update next free entry.
    //

    ASSERT (PointerPte->u.Hard.Valid == 0);

    MmFirstFreeSystemCache = MmSystemCachePteBase + PointerPte->u.List.NextEntry;
    ASSERT (MmFirstFreeSystemCache <= MiGetPteAddress (MmSystemCacheEnd));

    //
    // Increment the count of the number of views for the
    // section object.  This requires the PFN lock to be held.
    //

    ControlArea->NumberOfMappedViews += 1;
    ControlArea->NumberOfSystemCacheViews += 1;
    ASSERT (ControlArea->NumberOfSectionReferences != 0);

    //
    // An unoccupied address range has been found, put the PTEs in
    // the range into prototype PTEs.
    //

    if (ControlArea->FilePointer != NULL) {
    
        //
        // Increment the view count for every subsection spanned by this view,
        // creating prototype PTEs if needed.
        //
        // N.B. This call always returns with the PFN lock released !
        //

        Status = MiAddViewsForSection ((PMSUBSECTION)Subsection,
                                       LastPteOffset,
                                       OldIrql,
                                       &Waited);

        ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

        if (!NT_SUCCESS (Status)) {

            //
            // Zero both the next and TB flush stamp PTEs before unmapping so
            // the unmap won't hit entries it can't decode.
            //

            MiMapCacheFailures += 1;
            PointerPte->u.List.NextEntry = 0;
            (PointerPte + 1)->u.List.NextEntry = 0;

            MmUnmapViewInSystemCache (MiGetVirtualAddressMappedByPte (PointerPte),
                                      SectionToMap,
                                      FALSE);
            return Status;
        }
    }
    else {
        UNLOCK_PFN (OldIrql);
    }

    if (PointerPte->u.List.NextEntry == MM_EMPTY_PTE_LIST) {
        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x778,
                      (ULONG_PTR)PointerPte,
                      0,
                      0);
    }

    //
    // Check to see if the TB needs to be flushed.  Note that due to natural
    // TB traffic and the number of system cache views, this is an extremely
    // rare operation.
    //

    if ((PointerPte + 1)->u.List.NextEntry == (KeReadTbFlushTimeStamp() & MM_FLUSH_COUNTER_MASK)) {
        KeFlushEntireTb (TRUE, TRUE);
    }

    //
    // Zero this explicitly now since the number of pages may be only 1.
    //

    (PointerPte + 1)->u.List.NextEntry = 0;

    *CapturedBase = MiGetVirtualAddressMappedByPte (PointerPte);

    ProtoPte = &Subsection->SubsectionBase[PteOffset];

    LastProto = &Subsection->SubsectionBase[Subsection->PtesInSubsection];

    LastPte = PointerPte + NumberOfPages;

#if DBG

    for (PointerPte2 = PointerPte + 2; PointerPte2 < LastPte; PointerPte2 += 1) {
        ASSERT (PointerPte2->u.Long == ZeroKernelPte.u.Long);
    }

#endif

    while (PointerPte < LastPte) {

        if (ProtoPte >= LastProto) {

            //
            // Handle extended subsections.
            //

            Subsection = Subsection->NextSubsection;
            ProtoPte = Subsection->SubsectionBase;
            LastProto = &Subsection->SubsectionBase[
                                        Subsection->PtesInSubsection];
        }
        PteContents.u.Long = MiProtoAddressForKernelPte (ProtoPte);
        MI_WRITE_INVALID_PTE (PointerPte, PteContents);

        ASSERT (((ULONG_PTR)PointerPte & (MM_COLOR_MASK << PTE_SHIFT)) ==
                 (((ULONG_PTR)ProtoPte & (MM_COLOR_MASK << PTE_SHIFT))));

        PointerPte += 1;
        ProtoPte += 1;
    }

    return STATUS_SUCCESS;
}

NTSTATUS
MiAddMappedPtes (
    IN PMMPTE FirstPte,
    IN PFN_NUMBER NumberOfPtes,
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This function maps a view in the current address space to the
    specified control area.  The page protection is identical to that
    of the prototype PTE.

    This routine assumes the caller has called MiCheckPurgeAndUpMapCount,
    hence the PFN lock is not needed here.

Arguments:

    FirstPte - Supplies a pointer to the first PTE of the current address
               space to initialize.

    NumberOfPtes - Supplies the number of PTEs to initialize.

    ControlArea - Supplies the control area to point the PTEs at.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.
    
--*/

{
    MMPTE PteContents;
    PMMPTE PointerPte;
    PMMPTE ProtoPte;
    PMMPTE LastProto;
    PMMPTE LastPte;
    PSUBSECTION Subsection;
    NTSTATUS Status;

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    PointerPte = FirstPte;
    ASSERT (NumberOfPtes != 0);
    LastPte = FirstPte + NumberOfPtes;

    ASSERT (ControlArea->NumberOfMappedViews >= 1);
    ASSERT (ControlArea->NumberOfUserReferences >= 1);
    ASSERT (ControlArea->u.Flags.HadUserReference == 1);
    ASSERT (ControlArea->NumberOfSectionReferences != 0);

    ASSERT (ControlArea->u.Flags.BeingCreated == 0);
    ASSERT (ControlArea->u.Flags.BeingDeleted == 0);
    ASSERT (ControlArea->u.Flags.BeingPurged == 0);

    if ((ControlArea->FilePointer != NULL) &&
        (ControlArea->u.Flags.Image == 0) &&
        (ControlArea->u.Flags.PhysicalMemory == 0)) {

        //
        // Increment the view count for every subsection spanned by this view.
        //

        Status = MiAddViewsForSectionWithPfn ((PMSUBSECTION)Subsection,
                                              NumberOfPtes);

        if (!NT_SUCCESS (Status)) {
            return Status;
        }
    }

    ProtoPte = Subsection->SubsectionBase;

    LastProto = &Subsection->SubsectionBase[Subsection->PtesInSubsection];

    while (PointerPte < LastPte) {

        if (ProtoPte >= LastProto) {

            //
            // Handle extended subsections.
            //

            Subsection = Subsection->NextSubsection;
            ProtoPte = Subsection->SubsectionBase;
            LastProto = &Subsection->SubsectionBase[
                                        Subsection->PtesInSubsection];
        }
        ASSERT (PointerPte->u.Long == ZeroKernelPte.u.Long);
        PteContents.u.Long = MiProtoAddressForKernelPte (ProtoPte);
        MI_WRITE_INVALID_PTE (PointerPte, PteContents);

        ASSERT (((ULONG_PTR)PointerPte & (MM_COLOR_MASK << PTE_SHIFT)) ==
                 (((ULONG_PTR)ProtoPte  & (MM_COLOR_MASK << PTE_SHIFT))));

        PointerPte += 1;
        ProtoPte += 1;
    }

    return STATUS_SUCCESS;
}

VOID
MmUnmapViewInSystemCache (
    IN PVOID BaseAddress,
    IN PVOID SectionToUnmap,
    IN ULONG AddToFront
    )

/*++

Routine Description:

    This function unmaps a view from the system cache.

    NOTE: When this function is called, no pages may be locked in
    the cache for the specified view.

Arguments:

    BaseAddress - Supplies the base address of the section in the
                  system cache.

    SectionToUnmap - Supplies a pointer to the section which the
                     base address maps.

    AddToFront - Supplies TRUE if the unmapped pages should be
                 added to the front of the standby list (i.e., their
                 value in the cache is low).  FALSE otherwise.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PMMPTE FirstPte;
    PMMPTE ProtoPte;
    PMMPTE PointerPde;
    MMPTE ProtoPteContents;
    MMPTE PteContents;
    KIRQL OldIrql;
    WSLE_NUMBER WorkingSetIndex;
    PCONTROL_AREA ControlArea;
    ULONG WsHeld;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER PageTableFrameIndex;
    PMSUBSECTION MappedSubsection;
    PMSUBSECTION LastSubsection;
    PETHREAD CurrentThread;
#if DBG
    PFN_NUMBER i;
    PFN_NUMBER j;
    PMSUBSECTION SubsectionArray[X256K / PAGE_SIZE];
    PMMPTE PteArray[X256K / PAGE_SIZE];

    i = 0;
    RtlZeroMemory (SubsectionArray, sizeof(SubsectionArray));

    RtlCopyMemory (PteArray, MiGetPteAddress (BaseAddress), sizeof (PteArray));
#endif

    WsHeld = FALSE;

    CurrentThread = PsGetCurrentThread ();

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    PointerPte = MiGetPteAddress (BaseAddress);
    LastPte = PointerPte + (X256K / PAGE_SIZE);

    FirstPte = PointerPte;
    PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress (PointerPte));
    Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

    //
    // Get the control area for the segment which is mapped here.
    //

    ControlArea = ((PSECTION)SectionToUnmap)->Segment->ControlArea;
    LastSubsection = NULL;

    ASSERT ((ControlArea->u.Flags.Image == 0) &&
            (ControlArea->u.Flags.PhysicalMemory == 0) &&
            (ControlArea->u.Flags.GlobalOnlyPerSession == 0));

    do {

        //
        // The cache is organized in chunks of 256k bytes, clear
        // the first chunk then check to see if this is the last chunk.
        //
        // The page table page is always resident for the system cache.
        // Check each PTE: it is in one of three states, either valid or
        // prototype PTE format or zero.
        //

        PteContents = *PointerPte;

        if (PteContents.u.Hard.Valid == 1) {

            //
            // The PTE is valid.
            //

            if (!WsHeld) {
                WsHeld = TRUE;
                LOCK_SYSTEM_WS (CurrentThread);
                continue;
            }

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            WorkingSetIndex = MiLocateWsle (BaseAddress,
                                            MmSystemCacheWorkingSetList,
                                            Pfn1->u1.WsIndex);

            MiRemoveWsle (WorkingSetIndex, MmSystemCacheWorkingSetList);

            MiReleaseWsle (WorkingSetIndex, &MmSystemCacheWs);

            MI_SET_PTE_IN_WORKING_SET (PointerPte, 0);

            //
            // Decrement the view count for every subsection this view spans.
            // But make sure it's only done once per subsection in a given view.
            //
            // The subsections can only be decremented after all the
            // PTEs have been cleared and PFN sharecounts decremented so no
            // prototype PTEs will be valid if it is indeed the final subsection
            // dereference.  This is critical so the dereference segment
            // thread doesn't free pool containing valid prototype PTEs.
            //

            if (ControlArea->FilePointer != NULL) {

                ASSERT (Pfn1->u3.e1.PrototypePte);
                ASSERT (Pfn1->OriginalPte.u.Soft.Prototype);

                if ((LastSubsection != NULL) &&
                    (Pfn1->PteAddress >= LastSubsection->SubsectionBase) &&
                    (Pfn1->PteAddress < LastSubsection->SubsectionBase + LastSubsection->PtesInSubsection)) {

                    NOTHING;
                }
                else {

                    MappedSubsection = (PMSUBSECTION)MiGetSubsectionAddress (&Pfn1->OriginalPte);
                    if (MappedSubsection->ControlArea != ControlArea) {
                        KeBugCheckEx (MEMORY_MANAGEMENT,
                                      0x780,
                                      (ULONG_PTR) PointerPte,
                                      (ULONG_PTR) Pfn1,
                                      (ULONG_PTR) Pfn1->OriginalPte.u.Long);
                    }

                    ASSERT ((MappedSubsection->NumberOfMappedViews >= 1) ||
                            (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1));

                    if (LastSubsection != MappedSubsection) {

                        if (LastSubsection != NULL) {
#if DBG
                            for (j = 0; j < i; j += 1) {
                                ASSERT (SubsectionArray[j] != MappedSubsection);
                            }
                            SubsectionArray[i] = MappedSubsection;
#endif
                            LOCK_PFN (OldIrql);
                            MiRemoveViewsFromSection (LastSubsection,
                                                      LastSubsection->PtesInSubsection);
                            UNLOCK_PFN (OldIrql);
                        }
                        LastSubsection = MappedSubsection;
                    }
                }
            }

            LOCK_PFN (OldIrql);

            //
            // Capture the state of the modified bit for this PTE.
            //

            MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);

            //
            // Decrement the share and valid counts of the page table
            // page which maps this PTE.
            //

            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

            //
            // Decrement the share count for the physical page.
            //

#if DBG
            if (ControlArea->NumberOfMappedViews == 1) {
                ASSERT (Pfn1->u2.ShareCount == 1);
            }
#endif

            MmFrontOfList = AddToFront;
            MiDecrementShareCountInline (Pfn1, PageFrameIndex);
            MmFrontOfList = FALSE;

            UNLOCK_PFN (OldIrql);
        }
        else {

            ASSERT ((PteContents.u.Long == ZeroKernelPte.u.Long) ||
                    (PteContents.u.Soft.Prototype == 1));

            if (PteContents.u.Soft.Prototype == 1) {

                //
                // Decrement the view count for every subsection this view
                // spans.  But make sure it's only done once per subsection
                // in a given view.
                //
    
                if (ControlArea->FilePointer != NULL) {

                    ProtoPte = MiPteToProto (&PteContents);

                    if ((LastSubsection != NULL) &&
                        (ProtoPte >= LastSubsection->SubsectionBase) &&
                        (ProtoPte < LastSubsection->SubsectionBase + LastSubsection->PtesInSubsection)) {

                        NOTHING;
                    }
                    else {

                        PointerPde = MiGetPteAddress (ProtoPte);
                        LOCK_PFN (OldIrql);

                        //
                        // PTE is not valid, check the state of
                        // the prototype PTE.
                        //

                        if (PointerPde->u.Hard.Valid == 0) {
                            if (WsHeld) {
                                MiMakeSystemAddressValidPfnSystemWs (ProtoPte,
                                                                     OldIrql);
                            }
                            else {
                                MiMakeSystemAddressValidPfn (ProtoPte, OldIrql);
                            }

                            //
                            // Page fault occurred, recheck state
                            // of original PTE.
                            //

                            UNLOCK_PFN (OldIrql);
                            continue;
                        }

                        ProtoPteContents = *ProtoPte;

                        if (ProtoPteContents.u.Hard.Valid == 1) {
                            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&ProtoPteContents);
                            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                            ProtoPte = &Pfn1->OriginalPte;
                        }
                        else if ((ProtoPteContents.u.Soft.Transition == 1) &&
                                 (ProtoPteContents.u.Soft.Prototype == 0)) {
                            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&ProtoPteContents);
                            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                            ProtoPte = &Pfn1->OriginalPte;
                        }
                        else {
                            Pfn1 = NULL;
                            ASSERT (ProtoPteContents.u.Soft.Prototype == 1);
                        }

                        MappedSubsection = (PMSUBSECTION)MiGetSubsectionAddress (ProtoPte);
                        if (MappedSubsection->ControlArea != ControlArea) {
                            KeBugCheckEx (MEMORY_MANAGEMENT,
                                          0x781,
                                          (ULONG_PTR) PointerPte,
                                          (ULONG_PTR) Pfn1,
                                          (ULONG_PTR) ProtoPte);
                        }

                        ASSERT ((MappedSubsection->NumberOfMappedViews >= 1) ||
                                (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1));

                        if (LastSubsection != MappedSubsection) {
    
                            if (LastSubsection != NULL) {
#if DBG
                                for (j = 0; j < i; j += 1) {
                                    ASSERT (SubsectionArray[j] != MappedSubsection);
                                }
                                SubsectionArray[i] = MappedSubsection;
#endif
                                MiRemoveViewsFromSection (LastSubsection,
                                                          LastSubsection->PtesInSubsection);
                            }
                            LastSubsection = MappedSubsection;
                        }

                        UNLOCK_PFN (OldIrql);
                    }
                }
            }

            if (WsHeld) {
                UNLOCK_SYSTEM_WS ();
                WsHeld = FALSE;
            }
        }
        MI_WRITE_INVALID_PTE (PointerPte, ZeroKernelPte);

        PointerPte += 1;
        BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
#if DBG
        i += 1;
#endif
    } while (PointerPte < LastPte);

    if (WsHeld) {
        UNLOCK_SYSTEM_WS ();
    }

    FirstPte->u.List.NextEntry = MM_EMPTY_PTE_LIST;

    (FirstPte+1)->u.List.NextEntry = (KeReadTbFlushTimeStamp() & MM_FLUSH_COUNTER_MASK);

    LOCK_PFN (OldIrql);

    //
    // Free this entry to the end of the list.
    //

    MmLastFreeSystemCache->u.List.NextEntry = FirstPte - MmSystemCachePteBase;
    MmLastFreeSystemCache = FirstPte;

    if (LastSubsection != NULL) {
        MiRemoveViewsFromSection (LastSubsection,
                                  LastSubsection->PtesInSubsection);
    }

    //
    // Decrement the number of mapped views for the segment
    // and check to see if the segment should be deleted.
    //

    ControlArea->NumberOfMappedViews -= 1;
    ControlArea->NumberOfSystemCacheViews -= 1;

    //
    // Check to see if the control area (segment) should be deleted.
    // This routine releases the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);

    return;
}


VOID
MiRemoveMappedPtes (
    IN PVOID BaseAddress,
    IN ULONG NumberOfPtes,
    IN PCONTROL_AREA ControlArea,
    IN PMMSUPPORT Ws
    )

/*++

Routine Description:

    This function unmaps a view from the system or session view space.

    NOTE: When this function is called, no pages may be locked in
    the space for the specified view.

Arguments:

    BaseAddress - Supplies the base address of the section in the
                  system or session view space.

    NumberOfPtes - Supplies the number of PTEs to unmap.

    ControlArea - Supplies the control area mapping the view.

    Ws - Supplies the charged working set structures.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPFN Pfn1;
    PMMPTE FirstPte;
    PMMPTE ProtoPte;
    MMPTE PteContents;
    KIRQL OldIrql;
    WSLE_NUMBER WorkingSetIndex;
    ULONG DereferenceSegment;
    MMPTE_FLUSH_LIST PteFlushList;
    MMPTE ProtoPteContents;
    PFN_NUMBER PageFrameIndex;
    ULONG WsHeld;
    PMMPFN Pfn2;
    PFN_NUMBER PageTableFrameIndex;
    PMSUBSECTION MappedSubsection;
    PMSUBSECTION LastSubsection;
    PETHREAD CurrentThread;

    CurrentThread = PsGetCurrentThread ();
    DereferenceSegment = FALSE;
    WsHeld = FALSE;
    LastSubsection = NULL;

    PteFlushList.Count = 0;
    PointerPte = MiGetPteAddress (BaseAddress);
    FirstPte = PointerPte;

    //
    // Get the control area for the segment which is mapped here.
    //

    while (NumberOfPtes) {

        //
        // The page table page is always resident for the system space (and
        // for a session space) map.
        //
        // Check each PTE, it is in one of two states, either valid or
        // prototype PTE format.
        //

        PteContents = *PointerPte;
        if (PteContents.u.Hard.Valid == 1) {

            //
            // Lock the working set to prevent races with the trimmer,
            // then re-examine the PTE.
            //

            if (!WsHeld) {
                WsHeld = TRUE;
                LOCK_WORKING_SET (Ws);
                continue;
            }

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            WorkingSetIndex = MiLocateWsle (BaseAddress,
                                            Ws->VmWorkingSetList,
                                            Pfn1->u1.WsIndex);

            ASSERT (WorkingSetIndex != WSLE_NULL_INDEX);

            MiRemoveWsle (WorkingSetIndex,
                          Ws->VmWorkingSetList);

            MiReleaseWsle (WorkingSetIndex, Ws);

            MI_SET_PTE_IN_WORKING_SET (PointerPte, 0);

            PointerPde = MiGetPteAddress (PointerPte);

            LOCK_PFN (OldIrql);

            //
            // The PTE is valid.
            //

            //
            // Decrement the view count for every subsection this view spans.
            // But make sure it's only done once per subsection in a given view.
            //
            // The subsections can only be decremented after all the
            // PTEs have been cleared and PFN sharecounts decremented so no
            // prototype PTEs will be valid if it is indeed the final subsection
            // dereference.  This is critical so the dereference segment
            // thread doesn't free pool containing valid prototype PTEs.
            //

            if ((Pfn1->u3.e1.PrototypePte) &&
                (Pfn1->OriginalPte.u.Soft.Prototype)) {

                if ((LastSubsection != NULL) &&
                    (Pfn1->PteAddress >= LastSubsection->SubsectionBase) &&
                    (Pfn1->PteAddress < LastSubsection->SubsectionBase + LastSubsection->PtesInSubsection)) {

                    NOTHING;
                }
                else {

                    MappedSubsection = (PMSUBSECTION)MiGetSubsectionAddress (&Pfn1->OriginalPte);
                    if (LastSubsection != MappedSubsection) {

                        ASSERT (ControlArea == MappedSubsection->ControlArea);

                        if ((ControlArea->FilePointer != NULL) &&
                            (ControlArea->u.Flags.Image == 0) &&
                            (ControlArea->u.Flags.PhysicalMemory == 0)) {

                            if (LastSubsection != NULL) {
                                MiRemoveViewsFromSection (LastSubsection,
                                                          LastSubsection->PtesInSubsection);
                            }
                            LastSubsection = MappedSubsection;
                        }
                    }
                }
            }

            //
            // Capture the state of the modified bit for this PTE.
            //

            MI_CAPTURE_DIRTY_BIT_TO_PFN (PointerPte, Pfn1);

            //
            // Flush the TB for this page.
            //

            if (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT) {
                PteFlushList.FlushVa[PteFlushList.Count] = BaseAddress;
                PteFlushList.Count += 1;
            }

#if (_MI_PAGING_LEVELS < 3)

            //
            // The PDE must be carefully checked against the master table
            // because the PDEs are all zeroed in process creation.  If this
            // process has never faulted on any address in this range (all
            // references prior and above were filled directly by the TB as
            // the PTEs are global on non-Hydra), then the PDE reference
            // below to determine the page table frame will be zero.
            //
            // Note this cannot happen on NT64 as no master table is used.
            //

            if (PointerPde->u.Long == 0) {

                PMMPTE MasterPde;

                MasterPde = &MmSystemPagePtes [((ULONG_PTR)PointerPde &
                             (PD_PER_SYSTEM * (sizeof(MMPTE) * PDE_PER_PAGE) - 1)) / sizeof(MMPTE)];

                ASSERT (MasterPde->u.Hard.Valid == 1);
                MI_WRITE_VALID_PTE (PointerPde, *MasterPde);
            }
#endif

            //
            // Decrement the share and valid counts of the page table
            // page which maps this PTE.
            //

            PageTableFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
            Pfn2 = MI_PFN_ELEMENT (PageTableFrameIndex);

            MiDecrementShareCountInline (Pfn2, PageTableFrameIndex);

            //
            // Decrement the share count for the physical page.
            //

            MiDecrementShareCount (Pfn1, PageFrameIndex);
            UNLOCK_PFN (OldIrql);
        }
        else {

            ASSERT ((PteContents.u.Long == ZeroKernelPte.u.Long) ||
                    (PteContents.u.Soft.Prototype == 1));

            if (PteContents.u.Soft.Prototype == 1) {

                //
                // Decrement the view count for every subsection this view
                // spans.  But make sure it's only done once per subsection
                // in a given view.
                //
    
                ProtoPte = MiPteToProto (&PteContents);

                if ((LastSubsection != NULL) &&
                    (ProtoPte >= LastSubsection->SubsectionBase) &&
                    (ProtoPte < LastSubsection->SubsectionBase + LastSubsection->PtesInSubsection)) {

                    NOTHING;
                }
                else {

                    if (WsHeld) {
                        UNLOCK_WORKING_SET (Ws);
                        WsHeld = FALSE;
                    }

                    //
                    // PTE is not valid, check the state of the prototype PTE.
                    //

                    PointerPde = MiGetPteAddress (ProtoPte);
                    LOCK_PFN (OldIrql);

                    if (PointerPde->u.Hard.Valid == 0) {
                        MiMakeSystemAddressValidPfn (ProtoPte, OldIrql);

                        //
                        // Page fault occurred, recheck state of original PTE.
                        //

                        UNLOCK_PFN (OldIrql);
                        continue;
                    }

                    ProtoPteContents = *ProtoPte;

                    if (ProtoPteContents.u.Hard.Valid == 1) {
                        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&ProtoPteContents);
                        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                        ProtoPte = &Pfn1->OriginalPte;
                        if (ProtoPte->u.Soft.Prototype == 0) {
                            ProtoPte = NULL;
                        }
                    }
                    else if ((ProtoPteContents.u.Soft.Transition == 1) &&
                             (ProtoPteContents.u.Soft.Prototype == 0)) {
                        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&ProtoPteContents);
                        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                        ProtoPte = &Pfn1->OriginalPte;
                        if (ProtoPte->u.Soft.Prototype == 0) {
                            ProtoPte = NULL;
                        }
                    }
                    else if (ProtoPteContents.u.Soft.Prototype == 1) {
                        NOTHING;
                    }
                    else {

                        //
                        // Could be a zero PTE or a demand zero PTE.
                        // Neither belong to a mapped file.
                        //

                        ProtoPte = NULL;
                    }

                    if (ProtoPte != NULL) {

                        MappedSubsection = (PMSUBSECTION)MiGetSubsectionAddress (ProtoPte);
                        if (LastSubsection != MappedSubsection) {

                            ASSERT (ControlArea == MappedSubsection->ControlArea);

                            if ((ControlArea->FilePointer != NULL) &&
                                (ControlArea->u.Flags.Image == 0) &&
                                (ControlArea->u.Flags.PhysicalMemory == 0)) {

                                if (LastSubsection != NULL) {
                                    MiRemoveViewsFromSection (LastSubsection,
                                                              LastSubsection->PtesInSubsection);
                                }
                                LastSubsection = MappedSubsection;
                            }
                        }
                    }
                    UNLOCK_PFN (OldIrql);
                }
            }
        }
        MI_WRITE_INVALID_PTE (PointerPte, ZeroKernelPte);

        PointerPte += 1;
        BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
        NumberOfPtes -= 1;
    }

    if (WsHeld) {
        UNLOCK_WORKING_SET (Ws);
    }

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList, TRUE);
    }

    if (Ws != &MmSystemCacheWs) {

        //
        // Session space has no ASN - flush the entire TB.
        //
    
        MI_FLUSH_ENTIRE_SESSION_TB (TRUE, TRUE);
    }

    LOCK_PFN (OldIrql);

    if (LastSubsection != NULL) {
        MiRemoveViewsFromSection (LastSubsection,
                                  LastSubsection->PtesInSubsection);
    }

    //
    // Decrement the number of user references as the caller upped them
    // via MiCheckPurgeAndUpMapCount when this was originally mapped.
    //

    ControlArea->NumberOfUserReferences -= 1;

    //
    // Decrement the number of mapped views for the segment
    // and check to see if the segment should be deleted.
    //

    ControlArea->NumberOfMappedViews -= 1;

    //
    // Check to see if the control area (segment) should be deleted.
    // This routine releases the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);
}

VOID
MiInitializeSystemCache (
    IN ULONG MinimumWorkingSet,
    IN ULONG MaximumWorkingSet
    )

/*++

Routine Description:

    This routine initializes the system cache working set and
    data management structures.

Arguments:

    MinimumWorkingSet - Supplies the minimum working set for the system
                        cache.

    MaximumWorkingSet - Supplies the maximum working set size for the
                        system cache.

Return Value:

    None.

Environment:

    Kernel mode, called only at phase 0 initialization.

--*/

{
    ULONG Color;
    ULONG_PTR SizeOfSystemCacheInPages;
    ULONG_PTR HunksOf256KInCache;
    PMMWSLE WslEntry;
    ULONG NumberOfEntriesMapped;
    PFN_NUMBER i;
    MMPTE PteContents;
    PMMPTE PointerPte;
    KIRQL OldIrql;

    PointerPte = MiGetPteAddress (MmSystemCacheWorkingSetList);

    PteContents = ValidKernelPte;

    Color = MI_GET_PAGE_COLOR_FROM_PTE (PointerPte);

    LOCK_PFN (OldIrql);

    i = MiRemoveZeroPage (Color);

    PteContents.u.Hard.PageFrameNumber = i;

    MI_WRITE_VALID_PTE (PointerPte, PteContents);

    MiInitializePfn (i, PointerPte, 1L);

    MmResidentAvailablePages -= 1;

    UNLOCK_PFN (OldIrql);

#if defined (_WIN64)
    MmSystemCacheWsle = (PMMWSLE)(MmSystemCacheWorkingSetList + 1);
#else
    MmSystemCacheWsle =
            (PMMWSLE)(&MmSystemCacheWorkingSetList->UsedPageTableEntries[0]);
#endif

    MmSystemCacheWs.VmWorkingSetList = MmSystemCacheWorkingSetList;
    MmSystemCacheWs.WorkingSetSize = 0;

    //
    // Don't use entry 0 as an index of zero in the PFN database
    // means that the page can be assigned to a slot.  This is not
    // a problem for process working sets as page 0 is private.
    //

#if defined (_MI_DEBUG_WSLE)
    MmSystemCacheWorkingSetList->Quota = 0;
#endif
    MmSystemCacheWorkingSetList->FirstFree = 1;
    MmSystemCacheWorkingSetList->FirstDynamic = 1;
    MmSystemCacheWorkingSetList->NextSlot = 1;
    MmSystemCacheWorkingSetList->HashTable = NULL;
    MmSystemCacheWorkingSetList->HashTableSize = 0;
    MmSystemCacheWorkingSetList->Wsle = MmSystemCacheWsle;

    MmSystemCacheWorkingSetList->HashTableStart = 
       (PVOID)((PCHAR)PAGE_ALIGN (&MmSystemCacheWorkingSetList->Wsle[MM_MAXIMUM_WORKING_SET]) + PAGE_SIZE);

    MmSystemCacheWorkingSetList->HighestPermittedHashAddress = MmSystemCacheStart;

    NumberOfEntriesMapped = (ULONG)(((PMMWSLE)((PCHAR)MmSystemCacheWorkingSetList +
                                PAGE_SIZE)) - MmSystemCacheWsle);

    MinimumWorkingSet = NumberOfEntriesMapped - 1;

    MmSystemCacheWs.MinimumWorkingSetSize = MinimumWorkingSet;
    MmSystemCacheWorkingSetList->LastEntry = MinimumWorkingSet;

    if (MaximumWorkingSet <= MinimumWorkingSet) {
        MaximumWorkingSet = MinimumWorkingSet + (PAGE_SIZE / sizeof (MMWSLE));
    }

    MmSystemCacheWs.MaximumWorkingSetSize = MaximumWorkingSet;

    //
    // Initialize the following slots as free.
    //

    WslEntry = MmSystemCacheWsle + 1;

    for (i = 1; i < NumberOfEntriesMapped; i++) {

        //
        // Build the free list, note that the first working
        // set entries (CurrentEntry) are not on the free list.
        // These entries are reserved for the pages which
        // map the working set and the page which contains the PDE.
        //

        WslEntry->u1.Long = (i + 1) << MM_FREE_WSLE_SHIFT;
        WslEntry += 1;
    }

    WslEntry -= 1;
    WslEntry->u1.Long = WSLE_NULL_INDEX << MM_FREE_WSLE_SHIFT;  // End of list.

    MmSystemCacheWorkingSetList->LastInitializedWsle = NumberOfEntriesMapped - 1;

    //
    // Build a free list structure in the PTEs for the system cache.
    //

    MmSystemCachePteBase = MI_PTE_BASE_FOR_LOWEST_KERNEL_ADDRESS;

    SizeOfSystemCacheInPages = MI_COMPUTE_PAGES_SPANNED (MmSystemCacheStart,
                                (PCHAR)MmSystemCacheEnd - (PCHAR)MmSystemCacheStart + 1);

    HunksOf256KInCache = SizeOfSystemCacheInPages / (X256K / PAGE_SIZE);

    PointerPte = MiGetPteAddress (MmSystemCacheStart);

    MmFirstFreeSystemCache = PointerPte;

    for (i = 0; i < HunksOf256KInCache; i += 1) {
        PointerPte->u.List.NextEntry = (PointerPte + (X256K / PAGE_SIZE)) - MmSystemCachePteBase;
        PointerPte += X256K / PAGE_SIZE;
    }

    PointerPte -= X256K / PAGE_SIZE;

#if defined(_X86_)

    //
    // Add any extended ranges.
    //

    if (MiSystemCacheEndExtra != MmSystemCacheEnd) {

        SizeOfSystemCacheInPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (MiSystemCacheStartExtra,
                                    (PCHAR)MiSystemCacheEndExtra - (PCHAR)MiSystemCacheStartExtra + 1);
    
        HunksOf256KInCache = SizeOfSystemCacheInPages / (X256K / PAGE_SIZE);
    
        if (HunksOf256KInCache) {

            PMMPTE PointerPteExtended;
    
            PointerPteExtended = MiGetPteAddress (MiSystemCacheStartExtra);
            PointerPte->u.List.NextEntry = PointerPteExtended - MmSystemCachePteBase;
            PointerPte = PointerPteExtended;

            for (i = 0; i < HunksOf256KInCache; i += 1) {
                PointerPte->u.List.NextEntry = (PointerPte + (X256K / PAGE_SIZE)) - MmSystemCachePteBase;
                PointerPte += X256K / PAGE_SIZE;
            }
    
            PointerPte -= X256K / PAGE_SIZE;
        }
    }
#endif

    PointerPte->u.List.NextEntry = MM_EMPTY_PTE_LIST;
    MmLastFreeSystemCache = PointerPte;

    MiAllowWorkingSetExpansion (&MmSystemCacheWs);
}

BOOLEAN
MmCheckCachedPageState (
    IN PVOID SystemCacheAddress,
    IN BOOLEAN SetToZero
    )

/*++

Routine Description:

    This routine checks the state of the specified page that is mapped in
    the system cache.  If the specified virtual address can be made valid
    (i.e., the page is already in memory), it is made valid and the value
    TRUE is returned.

    If the page is not in memory, and SetToZero is FALSE, the
    value FALSE is returned.  However, if SetToZero is TRUE, a page of
    zeroes is materialized for the specified virtual address and the address
    is made valid and the value TRUE is returned.

    This routine is for usage by the cache manager.

Arguments:

    SystemCacheAddress - Supplies the address of a page mapped in the
                         system cache.

    SetToZero - Supplies TRUE if a page of zeroes should be created in the
                case where no page is already mapped.

Return Value:

    FALSE if touching this page would cause a page fault resulting
          in a page read.

    TRUE if there is a physical page in memory for this address.

Environment:

    Kernel mode.

--*/

{
    PETHREAD Thread;
    MMWSLE WsleMask;
    ULONG Flags;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE ProtoPte;
    PFN_NUMBER PageFrameIndex;
    WSLE_NUMBER WorkingSetIndex;
    MMPTE TempPte;
    MMPTE ProtoPteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    LOGICAL BarrierNeeded;
    ULONG BarrierStamp;
    PSUBSECTION Subsection;
    PFILE_OBJECT FileObject;
    LONGLONG FileOffset;

    PointerPte = MiGetPteAddress (SystemCacheAddress);

    //
    // Make the PTE valid if possible.
    //

    if (PointerPte->u.Hard.Valid == 1) {
        return TRUE;
    }

    BarrierNeeded = FALSE;

    Thread = PsGetCurrentThread ();

    LOCK_SYSTEM_WS (Thread);

    if (PointerPte->u.Hard.Valid == 1) {
        UNLOCK_SYSTEM_WS ();
        return TRUE;
    }

    ASSERT (PointerPte->u.Soft.Prototype == 1);
    ProtoPte = MiPteToProto (PointerPte);
    PointerPde = MiGetPteAddress (ProtoPte);

    LOCK_PFN (OldIrql);

    ASSERT (PointerPte->u.Hard.Valid == 0);
    ASSERT (PointerPte->u.Soft.Prototype == 1);

    //
    // PTE is not valid, check the state of the prototype PTE.
    //

    if (PointerPde->u.Hard.Valid == 0) {

        MiMakeSystemAddressValidPfnSystemWs (ProtoPte, OldIrql);

        //
        // Page fault occurred, recheck state of original PTE.
        //

        if (PointerPte->u.Hard.Valid == 1) {
            goto UnlockAndReturnTrue;
        }
    }

    ProtoPteContents = *ProtoPte;

    if (ProtoPteContents.u.Hard.Valid == 1) {

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&ProtoPteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // The prototype PTE is valid, make the cache PTE
        // valid and add it to the working set.
        //

        TempPte = ProtoPteContents;

    }
    else if ((ProtoPteContents.u.Soft.Transition == 1) &&
               (ProtoPteContents.u.Soft.Prototype == 0)) {

        //
        // Prototype PTE is in the transition state.  Remove the page
        // from the page list and make it valid.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&ProtoPteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        if ((Pfn1->u3.e1.ReadInProgress) || (Pfn1->u4.InPageError)) {

            //
            // Collided page fault, return.
            //

            goto UnlockAndReturnTrue;
        }

        if (MmAvailablePages < MM_HIGH_LIMIT) {

            //
            // This can only happen if the system is utilizing
            // a hardware compression cache.  This ensures that
            // only a safe amount of the compressed virtual cache
            // is directly mapped so that if the hardware gets
            // into trouble, we can bail it out.
            //
            // The same is true when machines are low on memory - we don't
            // want this thread to gobble up the pages from every modified
            // write that completes because that would starve waiting threads.
            //
            // Just unlock everything here to give the compression
            // reaper a chance to ravage pages and then retry.
            //

            if ((PsGetCurrentThread()->MemoryMaker == 0) ||
                (MmAvailablePages == 0)) {

                goto UnlockAndReturnTrue;
            }
        }

        MiUnlinkPageFromList (Pfn1);

        Pfn1->u3.e2.ReferenceCount += 1;
        Pfn1->u3.e1.PageLocation = ActiveAndValid;
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiCached);

        MI_SNAP_DATA (Pfn1, ProtoPte, 1);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           Pfn1->OriginalPte.u.Soft.Protection,
                           NULL );

        MI_WRITE_VALID_PTE (ProtoPte, TempPte);

        //
        // Increment the valid PTE count for the page containing
        // the prototype PTE.
        //

        Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);
    }
    else {

        //
        // Page is not in memory, if a page of zeroes is requested,
        // get a page of zeroes and make it valid.
        //

        if ((SetToZero == FALSE) || (MmAvailablePages < MM_HIGH_LIMIT)) {
            UNLOCK_PFN (OldIrql);
            UNLOCK_SYSTEM_WS ();

            //
            // Fault the page into memory.
            //

            MmAccessFault (FALSE, SystemCacheAddress, KernelMode, NULL);
            return FALSE;
        }

        //
        // Increment the count of Pfn references for the control area
        // corresponding to this file.
        //

        MiGetSubsectionAddress (
                    ProtoPte)->ControlArea->NumberOfPfnReferences += 1;

        PageFrameIndex = MiRemoveZeroPage(MI_GET_PAGE_COLOR_FROM_PTE (ProtoPte));

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // This barrier check is needed after zeroing the page and
        // before setting the PTE (not the prototype PTE) valid.
        // Capture it now, check it at the last possible moment.
        //

        BarrierNeeded = TRUE;
        BarrierStamp = (ULONG)Pfn1->u4.PteFrame;

        MiInitializePfn (PageFrameIndex, ProtoPte, 1);
        Pfn1->u2.ShareCount = 0;
        Pfn1->u3.e1.PrototypePte = 1;

        MI_SNAP_DATA (Pfn1, ProtoPte, 2);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           Pfn1->OriginalPte.u.Soft.Protection,
                           NULL );

        MI_WRITE_VALID_PTE (ProtoPte, TempPte);
    }

    //
    // Increment the share count since the page is being put into a working
    // set.
    //

    Pfn1->u2.ShareCount += 1;

    //
    // Increment the reference count of the page table
    // page for this PTE.
    //

    PointerPde = MiGetPteAddress (PointerPte);
    Pfn2 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);

    Pfn2->u2.ShareCount += 1;

    MI_SET_GLOBAL_STATE (TempPte, 1);

    TempPte.u.Hard.Owner = MI_PTE_OWNER_KERNEL;

    if (BarrierNeeded) {
        MI_BARRIER_SYNCHRONIZE (BarrierStamp);
    }

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    //
    // Capture the original PTE as it is needed for prefetch fault information.
    //

    TempPte = Pfn1->OriginalPte;

    UNLOCK_PFN (OldIrql);

    WsleMask.u1.Long = 0;
    WsleMask.u1.e1.SameProtectAsProto = 1;

    WorkingSetIndex = MiAllocateWsle (&MmSystemCacheWs,
                                      PointerPte,
                                      Pfn1,
                                      WsleMask.u1.Long);

    if (WorkingSetIndex == 0) {

        //
        // No working set entry was available so just trim the page.
        // Note another thread may be writing too so the page must be
        // trimmed instead of just tossed.
        //
        // The protection is in the prototype PTE.
        //

        ASSERT (Pfn1->u3.e1.PrototypePte == 1);
        ASSERT (ProtoPte == Pfn1->PteAddress);
        TempPte.u.Long = MiProtoAddressForPte (ProtoPte);

        MiTrimPte (SystemCacheAddress, PointerPte, Pfn1, NULL, TempPte);
    }

    UNLOCK_SYSTEM_WS ();

    if ((WorkingSetIndex != 0) &&
        (CCPF_IS_PREFETCHER_ACTIVE()) &&
        (TempPte.u.Soft.Prototype == 1)) {

        Subsection = MiGetSubsectionAddress (&TempPte);

        //
        // Log prefetch fault information now that the PFN lock has been
        // released and the PTE has been made valid.  This minimizes PFN
        // lock contention, allows CcPfLogPageFault to allocate (and fault
        // on) pool, and allows other threads in this process to execute
        // without faulting on this address.
        //

        FileObject = Subsection->ControlArea->FilePointer;
        FileOffset = MiStartingOffset (Subsection, ProtoPte);

        Flags = 0;

        ASSERT (Subsection->ControlArea->u.Flags.Image == 0);

        if (Subsection->ControlArea->u.Flags.Rom) {
            Flags |= CCPF_TYPE_ROM;
        }

        CcPfLogPageFault (FileObject, FileOffset, Flags);
    }

    return TRUE;

UnlockAndReturnTrue:

    UNLOCK_PFN (OldIrql);
    UNLOCK_SYSTEM_WS ();

    return TRUE;
}

NTSTATUS
MmCopyToCachedPage (
    IN PVOID SystemCacheAddress,
    IN PVOID UserBuffer,
    IN ULONG Offset,
    IN SIZE_T CountInBytes,
    IN BOOLEAN DontZero
    )

/*++

Routine Description:

    This routine checks the state of the specified page that is mapped in
    the system cache.  If the specified virtual address can be made valid
    (i.e., the page is already in memory), it is made valid and success
    is returned.

    This routine is for usage by the cache manager.

Arguments:

    SystemCacheAddress - Supplies the address of a page mapped in the system
                         cache.  This MUST be a page aligned address!

    UserBuffer - Supplies the address of a user buffer to copy into the
                 system cache at the specified address + offset.

    Offset - Supplies the offset into the UserBuffer to copy the data.

    CountInBytes - Supplies the byte count to copy from the user buffer.

    DontZero - Supplies TRUE if the buffer should not be zeroed (the
               caller will track zeroing).  FALSE if it should be zeroed.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, <= APC_LEVEL.

--*/

{
    PMMPTE CopyPte;
    PVOID CopyAddress;
    MMWSLE WsleMask;
    ULONG Flags;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE ProtoPte;
    PFN_NUMBER PageFrameIndex;
    WSLE_NUMBER WorkingSetIndex;
    MMPTE TempPte;
    MMPTE TempPte2;
    MMPTE ProtoPteContents;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    KIRQL OldIrql;
    SIZE_T EndFill;
    PVOID Buffer;
    NTSTATUS Status;
    NTSTATUS ExceptionStatus;
    PCONTROL_AREA ControlArea;
    PETHREAD Thread;
    ULONG SavedState;
    PSUBSECTION Subsection;
    PFILE_OBJECT FileObject;
    LONGLONG FileOffset;
    LOGICAL NewPage;

    UNREFERENCED_PARAMETER (DontZero);

    NewPage = FALSE;
    WsleMask.u1.Long = 0;
    Status = STATUS_SUCCESS;
    Pfn1 = NULL;

    Thread = PsGetCurrentThread ();

    SATISFY_OVERZEALOUS_COMPILER (TempPte2.u.Soft.Prototype = 0);
    SATISFY_OVERZEALOUS_COMPILER (ProtoPte = NULL);
    SATISFY_OVERZEALOUS_COMPILER (TempPte.u.Long = 0);
    SATISFY_OVERZEALOUS_COMPILER (Pfn1 = NULL);
    SATISFY_OVERZEALOUS_COMPILER (Pfn2 = NULL);
    SATISFY_OVERZEALOUS_COMPILER (PageFrameIndex = 0);

    ASSERT (((ULONG_PTR)SystemCacheAddress & (PAGE_SIZE - 1)) == 0);
    ASSERT ((CountInBytes + Offset) <= PAGE_SIZE);
    ASSERT (KeGetCurrentIrql() < DISPATCH_LEVEL);

    PointerPte = MiGetPteAddress (SystemCacheAddress);

    if (PointerPte->u.Hard.Valid == 1) {
        goto Copy;
    }

    //
    // Acquire the working set mutex now as it is highly likely we will
    // be inserting this system cache address into the working set list.
    // This allows us to safely recover if no WSLEs are available because
    // it prevents any other threads from locking down the address until
    // we are done here.
    //

    LOCK_SYSTEM_WS (Thread);

    //
    // Note the world may change while we waited for the working set mutex.
    //

    if (PointerPte->u.Hard.Valid == 1) {
        UNLOCK_SYSTEM_WS ();
        goto Copy;
    }

    ASSERT (PointerPte->u.Soft.Prototype == 1);
    ProtoPte = MiPteToProto (PointerPte);
    PointerPde = MiGetPteAddress (ProtoPte);

    LOCK_PFN (OldIrql);

    ASSERT (PointerPte->u.Hard.Valid == 0);

Recheck:

    if (PointerPte->u.Hard.Valid == 1) {

        if (Pfn1 != NULL) {

            //
            // Toss the page as we won't be needing it after all, another
            // thread has won the race.
            //

            PageFrameIndex = Pfn1 - MmPfnDatabase;
            MiInsertPageInFreeList (PageFrameIndex);
        }
        UNLOCK_PFN (OldIrql);
        UNLOCK_SYSTEM_WS ();
        goto Copy;
    }

    //
    // Make the PTE valid if possible.
    //

    ASSERT (PointerPte->u.Soft.Prototype == 1);

    //
    // PTE is not valid, check the state of the prototype PTE.
    //

    if (PointerPde->u.Hard.Valid == 0) {

        MiMakeSystemAddressValidPfnSystemWs (ProtoPte, OldIrql);

        //
        // Page fault occurred, recheck state of original PTE.
        //

        if (PointerPte->u.Hard.Valid == 1) {

            if (Pfn1 != NULL) {

                //
                // Toss the page as we won't be needing it after all, another
                // thread has won the race.
                //

                PageFrameIndex = Pfn1 - MmPfnDatabase;
                MiInsertPageInFreeList (PageFrameIndex);
            }
            UNLOCK_PFN (OldIrql);
            UNLOCK_SYSTEM_WS ();
            goto Copy;
        }
    }

    ProtoPteContents = *ProtoPte;

    if (ProtoPteContents.u.Hard.Valid == 1) {

        if (Pfn1 != NULL) {

            //
            // Toss the page as we won't be needing it after all, another
            // thread has won the race.
            //

            PageFrameIndex = Pfn1 - MmPfnDatabase;
            MiInsertPageInFreeList (PageFrameIndex);
        }

        //
        // The prototype PTE is valid, make the cache PTE
        // valid and add it to the working set.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&ProtoPteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        //
        // Increment the share count as this prototype PTE will be
        // mapped into the system cache shortly.
        //

        Pfn1->u2.ShareCount += 1;

        TempPte = ProtoPteContents;

        ASSERT (Pfn1->u1.Event != NULL);
    }
    else if ((ProtoPteContents.u.Soft.Transition == 1) &&
             (ProtoPteContents.u.Soft.Prototype == 0)) {

        if (Pfn1 != NULL) {

            //
            // Toss the page as we won't be needing it after all, another
            // thread has won the race.
            //

            PageFrameIndex = Pfn1 - MmPfnDatabase;
            MiInsertPageInFreeList (PageFrameIndex);
        }

        //
        // Prototype PTE is in the transition state.  Remove the page
        // from the page list and make it valid.
        //

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&ProtoPteContents);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        if ((Pfn1->u3.e1.ReadInProgress) || (Pfn1->u4.InPageError)) {

            //
            // Collided page fault or in page error, try the copy
            // operation incurring a page fault.
            //

            UNLOCK_PFN (OldIrql);
            UNLOCK_SYSTEM_WS ();
            goto Copy;
        }

        ASSERT ((SPFN_NUMBER)MmAvailablePages >= 0);

        if (MmAvailablePages < MM_LOW_LIMIT) {

            //
            // This can only happen if the system is utilizing a hardware
            // compression cache.  This ensures that only a safe amount
            // of the compressed virtual cache is directly mapped so that
            // if the hardware gets into trouble, we can bail it out.
            //
            // The same is true when machines are low on memory - we don't
            // want this thread to gobble up the pages from every modified
            // write that completes because that would starve waiting threads.
            //

            if (MiEnsureAvailablePageOrWait (NULL, SystemCacheAddress, OldIrql)) {

                //
                // A wait operation occurred which could have changed the
                // state of the PTE.  Recheck the PTE state.
                //

                Pfn1 = NULL;
                goto Recheck;
            }
        }

        MiUnlinkPageFromList (Pfn1);

        Pfn1->u3.e2.ReferenceCount += 1;
        Pfn1->u3.e1.PageLocation = ActiveAndValid;

        MI_SET_MODIFIED (Pfn1, 1, 0x6);

        ASSERT (Pfn1->u2.ShareCount == 0);
        Pfn1->u2.ShareCount += 1;

        MI_SNAP_DATA (Pfn1, ProtoPte, 3);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           Pfn1->OriginalPte.u.Soft.Protection,
                           NULL);

        MI_SET_PTE_DIRTY (TempPte);

        MI_WRITE_VALID_PTE (ProtoPte, TempPte);

        //
        // Do NOT increment the share count for the page containing
        // the prototype PTE because it is already correct (the share
        // count is for both transition & valid PTE entries and this one
        // was transition before we just made it valid).
        //
    }
    else {

        if (Pfn1 == NULL) {

            //
            // Page is not in memory, if a page of zeroes is requested,
            // get a page of zeroes and make it valid.
            //
    
            if ((MmAvailablePages < MM_HIGH_LIMIT) &&
                (MiEnsureAvailablePageOrWait (NULL, SystemCacheAddress, OldIrql))) {
    
                //
                // A wait operation occurred which could have changed the
                // state of the PTE.  Recheck the PTE state.
                //
    
                goto Recheck;
            }
    
            //
            // Remove any page from the list in preparation for receiving
            // the user data.
            //
    
            PageFrameIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_PTE (ProtoPte));
    
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    
            ASSERT (Pfn1->u2.ShareCount == 0);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
    
            //
            // Temporarily mark the page as bad so that contiguous
            // memory allocators won't steal it when we release
            // the PFN lock below.  This also prevents the
            // MiIdentifyPfn code from trying to identify it as
            // we haven't filled in all the fields yet.
            //
    
            Pfn1->u3.e1.PageLocation = BadPageList;
    
            //
            // Map the page with a system PTE and do the copy into the page
            // directly.  Then retry the whole operation in case another racing
            // syscache-address-accessing thread has raced ahead of us for the
            // same address.
            //
    
            UNLOCK_PFN (OldIrql);
            UNLOCK_SYSTEM_WS ();
    
            CopyPte = MiReserveSystemPtes (1, SystemPteSpace);
    
            if (CopyPte == NULL) {

                //
                // No PTEs available for us to take the fast path, the cache
                // manager will have to copy the data directly.
                //
    
                LOCK_PFN (OldIrql);
                MiInsertPageInFreeList (PageFrameIndex);
                UNLOCK_PFN (OldIrql);
    
                return STATUS_INSUFFICIENT_RESOURCES;
            }
    
            MI_MAKE_VALID_PTE (TempPte,
                               PageFrameIndex,
                               MM_READWRITE,
                               CopyPte);
    
            MI_SET_PTE_DIRTY (TempPte);
    
            MI_WRITE_VALID_PTE (CopyPte, TempPte);
    
            CopyAddress = MiGetVirtualAddressMappedByPte (CopyPte);
    
            //
            // Zero the memory outside the range we're going to copy.
            //
    
            if (Offset != 0) {
                RtlZeroMemory (CopyAddress, Offset);
            }
    
            Buffer = (PVOID)((PCHAR) CopyAddress + Offset);
    
            EndFill = PAGE_SIZE - (Offset + CountInBytes);

            if (EndFill != 0) {
                RtlZeroMemory ((PVOID)((PCHAR)Buffer + CountInBytes),
                               EndFill);
            }
    
            //
            // Perform the copy of the user buffer into the page under
            // an exception handler.
            //
    
            MmSavePageFaultReadAhead (Thread, &SavedState);
            MmSetPageFaultReadAhead (Thread, 0);
    
            ExceptionStatus = STATUS_SUCCESS;
    
            try {
    
                RtlCopyBytes (Buffer, UserBuffer, CountInBytes);
    
            } except (MiMapCacheExceptionFilter (&ExceptionStatus, GetExceptionInformation())) {
    
                ASSERT (ExceptionStatus != STATUS_MULTIPLE_FAULT_VIOLATION);
    
                Status = ExceptionStatus;
            }
    
            MmResetPageFaultReadAhead (Thread, SavedState);
    
            MiReleaseSystemPtes (CopyPte, 1, SystemPteSpace);
    
            if (!NT_SUCCESS (Status)) {

                LOCK_PFN (OldIrql);
                MiInsertPageInFreeList (PageFrameIndex);
                UNLOCK_PFN (OldIrql);

                return Status;
            }
    
            //
            // Recheck everything as the world may have changed while we
            // released our locks.  Loop up and see if another thread has
            // already changed things (free our page if so), otherwise
            // we'll use this page the next time through.
            //

            LOCK_SYSTEM_WS (Thread);
            LOCK_PFN (OldIrql);

            goto Recheck;
        }

        PageFrameIndex = Pfn1 - MmPfnDatabase;

        ASSERT (Pfn1->u3.e1.PageLocation == BadPageList);
        ASSERT (Pfn1->u2.ShareCount == 0);
        ASSERT (Pfn1->u3.e2.ReferenceCount == 0);

        //
        // Increment the valid PTE count for the page containing
        // the prototype PTE.
        //

        MiInitializePfn (PageFrameIndex, ProtoPte, 1);

        ASSERT (Pfn1->u3.e1.ReadInProgress == 0);

        Pfn1->u3.e1.PrototypePte = 1;

        Pfn1->u1.Event = NULL;

        //
        // Increment the count of PFN references for the control area
        // corresponding to this file.
        //

        ControlArea = MiGetSubsectionAddress (ProtoPte)->ControlArea;

        ControlArea->NumberOfPfnReferences += 1;

        NewPage = TRUE;

        MI_SNAP_DATA (Pfn1, ProtoPte, 4);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           Pfn1->OriginalPte.u.Soft.Protection,
                           NULL);

        MI_SET_PTE_DIRTY (TempPte);

        MI_SET_GLOBAL_STATE (TempPte, 0);

        MI_WRITE_VALID_PTE (ProtoPte, TempPte);
    }

    //
    // Capture prefetch fault information.
    //

    TempPte2 = Pfn1->OriginalPte;

    //
    // Increment the share count of the page table page for this PTE.
    //

    PointerPde = MiGetPteAddress (PointerPte);
    Pfn2 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);

    Pfn2->u2.ShareCount += 1;

    MI_SET_GLOBAL_STATE (TempPte, 1);

    TempPte.u.Hard.Owner = MI_PTE_OWNER_KERNEL;

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
    ASSERT (Pfn1->PteAddress == ProtoPte);

    UNLOCK_PFN (OldIrql);

    WsleMask.u1.e1.SameProtectAsProto = 1;

    WorkingSetIndex = MiAllocateWsle (&MmSystemCacheWs,
                                      PointerPte,
                                      Pfn1,
                                      WsleMask.u1.Long);

    if (WorkingSetIndex == 0) {

        //
        // No working set entry was available so just trim the page.
        // Note another thread may be writing too so the page must be
        // trimmed instead of just tossed.
        //
        // The protection is in the prototype PTE.
        //

        ASSERT (Pfn1->u3.e1.PrototypePte == 1);
        ASSERT (ProtoPte == Pfn1->PteAddress);
        TempPte.u.Long = MiProtoAddressForPte (ProtoPte);

        MiTrimPte (SystemCacheAddress, PointerPte, Pfn1, NULL, TempPte);
    }

    UNLOCK_SYSTEM_WS ();

Copy:

    if (NewPage == FALSE) {

        //
        // Perform the copy since it hasn't been done already.
        //
    
        MmSavePageFaultReadAhead (Thread, &SavedState);
        MmSetPageFaultReadAhead (Thread, 0);
    
        //
        // Copy the user buffer into the cache under an exception handler.
        //
    
        ExceptionStatus = STATUS_SUCCESS;
    
        Buffer = (PVOID)((PCHAR) SystemCacheAddress + Offset);
    
        try {
    
            RtlCopyBytes (Buffer, UserBuffer, CountInBytes);
    
        } except (MiMapCacheExceptionFilter (&ExceptionStatus, GetExceptionInformation())) {
    
            ASSERT (ExceptionStatus != STATUS_MULTIPLE_FAULT_VIOLATION);
    
            Status = ExceptionStatus;
        }
    
        MmResetPageFaultReadAhead (Thread, SavedState);
    }

    //
    // If a virtual address was made directly present (ie: not via the normal
    // fault mechanisms), then log prefetch fault information now that the
    // PFN lock has been released and the PTE has been made valid.  This
    // minimizes PFN lock contention, allows CcPfLogPageFault to allocate
    // (and fault on) pool, and allows other threads in this process to
    // execute without faulting on this address.
    //

    if ((WsleMask.u1.e1.SameProtectAsProto == 1) &&
        (TempPte2.u.Soft.Prototype == 1)) {

        Subsection = MiGetSubsectionAddress (&TempPte2);

        FileObject = Subsection->ControlArea->FilePointer;
        FileOffset = MiStartingOffset (Subsection, ProtoPte);

        Flags = 0;

        ASSERT (Subsection->ControlArea->u.Flags.Image == 0);

        if (Subsection->ControlArea->u.Flags.Rom) {
            Flags |= CCPF_TYPE_ROM;
        }

        CcPfLogPageFault (FileObject, FileOffset, Flags);
    }

    return Status;
}

LONG
MiMapCacheExceptionFilter (
    IN PNTSTATUS Status,
    IN PEXCEPTION_POINTERS ExceptionPointer
    )

/*++

Routine Description:

    This routine is a filter for exceptions during copying data
    from the user buffer to the system cache.  It stores the
    status code from the exception record into the status argument.
    In the case of an in page i/o error it returns the actual
    error code and in the case of an access violation it returns
    STATUS_INVALID_USER_BUFFER.

Arguments:

    Status - Returns the status from the exception record.

    ExceptionCode - Supplies the exception code to being checked.

Return Value:

    ULONG - returns EXCEPTION_EXECUTE_HANDLER

--*/

{
    NTSTATUS local;

    local = ExceptionPointer->ExceptionRecord->ExceptionCode;

    //
    // If the exception is STATUS_IN_PAGE_ERROR, get the I/O error code
    // from the exception record.
    //

    if (local == STATUS_IN_PAGE_ERROR) {
        if (ExceptionPointer->ExceptionRecord->NumberParameters >= 3) {
            local = (NTSTATUS)ExceptionPointer->ExceptionRecord->ExceptionInformation[2];
        }
    }

    if (local == STATUS_ACCESS_VIOLATION) {
        local = STATUS_INVALID_USER_BUFFER;
    }

    *Status = local;
    return EXCEPTION_EXECUTE_HANDLER;
}


VOID
MmUnlockCachedPage (
    IN PVOID AddressInCache
    )

/*++

Routine Description:

    This routine unlocks a previous locked cached page.

Arguments:

    AddressInCache - Supplies the address where the page was locked
                     in the system cache.  This must be the same
                     address that MmCopyToCachedPage was called with.

Return Value:

    None.

--*/

{
    PMMPTE PointerPte;
    PMMPFN Pfn1;
    KIRQL OldIrql;

    PointerPte = MiGetPteAddress (AddressInCache);

    ASSERT (PointerPte->u.Hard.Valid == 1);
    Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

    LOCK_PFN (OldIrql);

    if (Pfn1->u3.e2.ReferenceCount <= 1) {
        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x777,
                      (ULONG_PTR)PointerPte->u.Hard.PageFrameNumber,
                      Pfn1->u3.e2.ReferenceCount,
                      (ULONG_PTR)AddressInCache);
        return;
    }

    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 25);

    UNLOCK_PFN (OldIrql);
    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\iosup.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   iosup.c

Abstract:

    This module contains routines which provide support for the I/O system.

Author:

    Lou Perazzoli (loup) 25-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#undef MmIsRecursiveIoFault

ULONG MiCacheOverride[4];

#if DBG
ULONG MmShowMapOverlaps;
#endif

extern LONG MmTotalSystemDriverPages;

BOOLEAN
MmIsRecursiveIoFault (
    VOID
    );

PVOID
MiAllocateContiguousMemory (
    IN SIZE_T NumberOfBytes,
    IN PFN_NUMBER LowestAcceptablePfn,
    IN PFN_NUMBER HighestAcceptablePfn,
    IN PFN_NUMBER BoundaryPfn,
    IN MEMORY_CACHING_TYPE CacheType,
    PVOID CallingAddress
    );

PVOID
MiMapLockedPagesInUserSpace (
     IN PMDL MemoryDescriptorList,
     IN PVOID StartingVa,
     IN MEMORY_CACHING_TYPE CacheType,
     IN PVOID BaseVa
     );

VOID
MiUnmapLockedPagesInUserSpace (
     IN PVOID BaseAddress,
     IN PMDL MemoryDescriptorList
     );

VOID
MiAddMdlTracker (
    IN PMDL MemoryDescriptorList,
    IN PVOID CallingAddress,
    IN PVOID CallersCaller,
    IN PFN_NUMBER NumberOfPagesToLock,
    IN ULONG Who
    );

KSPIN_LOCK MmIoTrackerLock;
LIST_ENTRY MmIoHeader;

#if DBG
PFN_NUMBER MmIoHeaderCount;
ULONG MmIoHeaderNumberOfEntries;
ULONG MmIoHeaderNumberOfEntriesPeak;
#endif

PCHAR MiCacheStrings[] = {
    "noncached",
    "cached",
    "writecombined",
    "None"
};

typedef struct _PTE_TRACKER {
    LIST_ENTRY ListEntry;
    PMDL Mdl;
    PFN_NUMBER Count;
    PVOID SystemVa;
    PVOID StartVa;
    ULONG Offset;
    ULONG Length;
    ULONG_PTR Page;
    PVOID CallingAddress;
    PVOID CallersCaller;
    BOOLEAN IoMapping;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;
} PTE_TRACKER, *PPTE_TRACKER;

typedef struct _SYSPTES_HEADER {
    LIST_ENTRY ListHead;
    PFN_NUMBER Count;
    PFN_NUMBER NumberOfEntries;
    PFN_NUMBER NumberOfEntriesPeak;
} SYSPTES_HEADER, *PSYSPTES_HEADER;

ULONG MmTrackPtes = 0;
BOOLEAN MiTrackPtesAborted = FALSE;
SYSPTES_HEADER MiPteHeader;
SLIST_HEADER MiDeadPteTrackerSListHead;
KSPIN_LOCK MiPteTrackerLock;

KSPIN_LOCK MiTrackIoLock;

#if (_MI_PAGING_LEVELS>=3)
KSPIN_LOCK MiLargePageLock;
RTL_BITMAP MiLargeVaBitMap;
#endif

ULONG MiNonCachedCollisions;

#if DBG
PFN_NUMBER MiCurrentAdvancedPages;
PFN_NUMBER MiAdvancesGiven;
PFN_NUMBER MiAdvancesFreed;
#endif

VOID
MiInsertPteTracker (
    IN PMDL MemoryDescriptorList,
    IN ULONG Flags,
    IN LOGICAL IoMapping,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute,
    IN PVOID MyCaller,
    IN PVOID MyCallersCaller
    );

VOID
MiRemovePteTracker (
    IN PMDL MemoryDescriptorList OPTIONAL,
    IN PVOID VirtualAddress,
    IN PFN_NUMBER NumberOfPtes
    );

LOGICAL
MiReferenceIoSpace (
    IN PMDL MemoryDescriptorList,
    IN PPFN_NUMBER Page
    );

LOGICAL
MiDereferenceIoSpace (
    IN PMDL MemoryDescriptorList
    );

VOID
MiPhysicalViewInserter (
    IN PEPROCESS Process,
    IN PMI_PHYSICAL_VIEW PhysicalView
    );

VOID
MiZeroAwePageWorker (
    IN PVOID Context
    );

#if DBG
ULONG MiPrintLockedPages;

VOID
MiVerifyLockedPageCharges (
    VOID
    );
#endif

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT, MmSetPageProtection)
#pragma alloc_text(INIT, MiInitializeIoTrackers)
#pragma alloc_text(INIT, MiInitializeLargePageSupport)

#pragma alloc_text(PAGE, MmAllocateIndependentPages)
#pragma alloc_text(PAGE, MmFreeIndependentPages)
#pragma alloc_text(PAGE, MmLockPagableDataSection)
#pragma alloc_text(PAGE, MiLookupDataTableEntry)
#pragma alloc_text(PAGE, MmSetBankedSection)
#pragma alloc_text(PAGE, MmProbeAndLockProcessPages)
#pragma alloc_text(PAGE, MmProbeAndLockSelectedPages)
#pragma alloc_text(PAGE, MmMapVideoDisplay)
#pragma alloc_text(PAGE, MmUnmapVideoDisplay)
#pragma alloc_text(PAGE, MmGetSectionRange)
#pragma alloc_text(PAGE, MiMapSinglePage)
#pragma alloc_text(PAGE, MiUnmapSinglePage)
#pragma alloc_text(PAGE, MmAllocateMappingAddress)
#pragma alloc_text(PAGE, MmFreeMappingAddress)
#pragma alloc_text(PAGE, MmAllocateNonCachedMemory)
#pragma alloc_text(PAGE, MmFreeNonCachedMemory)
#pragma alloc_text(PAGE, MmLockPagedPool)
#pragma alloc_text(PAGE, MmLockPagableSectionByHandle)
#pragma alloc_text(PAGE, MiZeroAwePageWorker)

#pragma alloc_text(PAGELK, MmEnablePAT)
#pragma alloc_text(PAGELK, MiUnmapLockedPagesInUserSpace)
#pragma alloc_text(PAGELK, MmAllocatePagesForMdl)
#pragma alloc_text(PAGELK, MiZeroInParallel)
#pragma alloc_text(PAGELK, MmFreePagesFromMdl)
#pragma alloc_text(PAGELK, MmUnlockPagedPool)
#pragma alloc_text(PAGELK, MmGatherMemoryForHibernate)
#pragma alloc_text(PAGELK, MmReturnMemoryForHibernate)
#pragma alloc_text(PAGELK, MmReleaseDumpAddresses)
#pragma alloc_text(PAGELK, MmMapUserAddressesToPage)
#pragma alloc_text(PAGELK, MiPhysicalViewInserter)
#pragma alloc_text(PAGELK, MiPhysicalViewAdjuster)

#pragma alloc_text(PAGEVRFY, MmIsSystemAddressLocked)
#pragma alloc_text(PAGEVRFY, MmAreMdlPagesLocked)
#endif

extern POOL_DESCRIPTOR NonPagedPoolDescriptor;

PFN_NUMBER MmMdlPagesAllocated;

KEVENT MmCollidedLockEvent;
LONG MmCollidedLockWait;

BOOLEAN MiWriteCombiningPtes = FALSE;

#if DBG
ULONG MiPrintAwe;
ULONG MmStopOnBadProbe = 1;
#endif

#define MI_PROBE_RAISE_SIZE 16

ULONG MiProbeRaises[MI_PROBE_RAISE_SIZE];

#define MI_INSTRUMENT_PROBE_RAISES(i)       \
        ASSERT (i < MI_PROBE_RAISE_SIZE);   \
        MiProbeRaises[i] += 1;

//
//  Note: this should be > 2041 to account for the cache manager's
//  aggressive zeroing logic.
//

ULONG MmReferenceCountCheck = MAXUSHORT / 2;

ULONG MiMdlsAdjusted = FALSE;


VOID
MmProbeAndLockPages (
     IN OUT PMDL MemoryDescriptorList,
     IN KPROCESSOR_MODE AccessMode,
     IN LOCK_OPERATION Operation
     )

/*++

Routine Description:

    This routine probes the specified pages, makes the pages resident and
    locks the physical pages mapped by the virtual pages in memory.  The
    Memory descriptor list is updated to describe the physical pages.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List
                            (MDL). The supplied MDL must supply a virtual
                            address, byte offset and length field.  The
                            physical page portion of the MDL is updated when
                            the pages are locked in memory.

    AccessMode - Supplies the access mode in which to probe the arguments.
                 One of KernelMode or UserMode.

    Operation - Supplies the operation type.  One of IoReadAccess, IoWriteAccess
                or IoModifyAccess.

Return Value:

    None - exceptions are raised.

Environment:

    Kernel mode.  APC_LEVEL and below for pagable addresses,
                  DISPATCH_LEVEL and below for non-pagable addresses.

--*/

{
    ULONG Processor;
    PPFN_NUMBER Page;
    MMPTE PteContents;
    PMMPTE LastPte;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    PVOID Va;
    PVOID EndVa;
    PVOID AlignedVa;
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER LastPageFrameIndex;
    PEPROCESS CurrentProcess;
    KIRQL OldIrql;
    PFN_NUMBER NumberOfPagesToLock;
    PFN_NUMBER NumberOfPagesSpanned;
    NTSTATUS status;
    NTSTATUS ProbeStatus;
    PETHREAD Thread;
    ULONG SavedState;
    PMI_PHYSICAL_VIEW PhysicalView;
    PCHAR StartVa;
    PVOID CallingAddress;
    PVOID CallersCaller;
    PAWEINFO AweInfo;
    PEX_PUSH_LOCK PushLock;
    TABLE_SEARCH_RESULT SearchResult;
#if defined (_MIALT4K_)
    MMPTE AltPteContents;
    PMMPTE PointerAltPte;
    PMMPTE LastPointerAltPte;
    PMMPTE AltPointerPte;
    PMMPTE AltPointerPde;
    PMMPTE AltPointerPpe;
    PMMPTE AltPointerPxe;
#endif


    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT (((ULONG)MemoryDescriptorList->ByteOffset & ~(PAGE_SIZE - 1)) == 0);

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    ASSERT (((ULONG_PTR)MemoryDescriptorList->StartVa & (PAGE_SIZE - 1)) == 0);
    AlignedVa = (PVOID)MemoryDescriptorList->StartVa;

    ASSERT ((MemoryDescriptorList->MdlFlags & (
                    MDL_PAGES_LOCKED |
                    MDL_MAPPED_TO_SYSTEM_VA |
                    MDL_SOURCE_IS_NONPAGED_POOL |
                    MDL_PARTIAL |
                    MDL_IO_SPACE)) == 0);

    Va = (PCHAR)AlignedVa + MemoryDescriptorList->ByteOffset;
    StartVa = Va;

    //
    // Endva is one byte past the end of the buffer, if ACCESS_MODE is not
    // kernel, make sure the EndVa is in user space AND the byte count
    // does not cause it to wrap.
    //

    EndVa = (PVOID)((PCHAR)Va + MemoryDescriptorList->ByteCount);

    if ((AccessMode != KernelMode) &&
        ((EndVa > (PVOID)MM_USER_PROBE_ADDRESS) || (Va >= EndVa))) {
        *Page = MM_EMPTY_LIST;
        MI_INSTRUMENT_PROBE_RAISES(0);
        ExRaiseStatus (STATUS_ACCESS_VIOLATION);
        return;
    }

    //
    // You would think there is an optimization which could be performed here:
    // if the operation is for WriteAccess and the complete page is
    // being modified, we can remove the current page, if it is not
    // resident, and substitute a demand zero page.
    // Note, that after analysis by marking the thread and then
    // noting if a page read was done, this rarely occurs.
    //

    Thread = PsGetCurrentThread ();

    NumberOfPagesToLock = ADDRESS_AND_SIZE_TO_SPAN_PAGES (Va,
                                   MemoryDescriptorList->ByteCount);

    ASSERT (NumberOfPagesToLock != 0);

    if (Va <= MM_HIGHEST_USER_ADDRESS) {

        CurrentProcess = PsGetCurrentProcessByThread (Thread);

        if (CurrentProcess->AweInfo != NULL) {

            AweInfo = CurrentProcess->AweInfo;                
        
            //
            // Block APCs to prevent recursive pushlock scenarios as
            // this is not supported.
            //

            KeEnterGuardedRegionThread (&Thread->Tcb);

            PushLock = ExAcquireCacheAwarePushLockShared (AweInfo->PushLock);

            //
            // Provide a fast path for transfers that are within
            // a single AWE region.
            //

            Processor = KeGetCurrentProcessorNumber ();
            PhysicalView = AweInfo->PhysicalViewHint[Processor];

            if ((PhysicalView != NULL) &&
                ((PVOID)StartVa >= MI_VPN_TO_VA (PhysicalView->StartingVpn)) &&
                ((PVOID)((PCHAR)EndVa - 1) <= MI_VPN_TO_VA_ENDING (PhysicalView->EndingVpn))) {
                NOTHING;
            }
            else {

                //
                // Lookup the element and save the result.
                //

                SearchResult = MiFindNodeOrParent (&AweInfo->AweVadRoot,
                                                   MI_VA_TO_VPN (StartVa),
                                                   (PMMADDRESS_NODE *) &PhysicalView);
                if ((SearchResult == TableFoundNode) &&
                    ((PVOID)StartVa >= MI_VPN_TO_VA (PhysicalView->StartingVpn)) &&
                    ((PVOID)((PCHAR)EndVa - 1) <= MI_VPN_TO_VA_ENDING (PhysicalView->EndingVpn))) {
                    AweInfo->PhysicalViewHint[Processor] = PhysicalView;
                }
                else {
                    ExReleaseCacheAwarePushLockShared (PushLock);
                    KeLeaveGuardedRegionThread (&Thread->Tcb);
                    goto DefaultProbeAndLock;
                }
            }
            
            MemoryDescriptorList->Process = CurrentProcess;

            MemoryDescriptorList->MdlFlags |= (MDL_PAGES_LOCKED | MDL_DESCRIBES_AWE);

            if (PhysicalView->u.LongFlags & MI_PHYSICAL_VIEW_AWE) {

                PointerPte = MiGetPteAddress (StartVa);
                LastPte = MiGetPteAddress ((PCHAR)EndVa - 1);

                do {
                    PteContents = *PointerPte;

                    if (PteContents.u.Hard.Valid == 0) {

                        ExReleaseCacheAwarePushLockShared (PushLock);
                        KeLeaveGuardedRegionThread (&Thread->Tcb);

                        *Page = MM_EMPTY_LIST;
                        MI_INSTRUMENT_PROBE_RAISES(9);
                        status = STATUS_ACCESS_VIOLATION;
                        goto failure2;
                    }

                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

                    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                    if (Pfn1->AweReferenceCount >= (LONG)MmReferenceCountCheck) {

                        ASSERT (FALSE);
                        ExReleaseCacheAwarePushLockShared (PushLock);
                        KeLeaveGuardedRegionThread (&Thread->Tcb);

                        *Page = MM_EMPTY_LIST;
                        status = STATUS_WORKING_SET_QUOTA;
                        goto failure2;
                    }

                    InterlockedIncrement (&Pfn1->AweReferenceCount);

                    *Page = PageFrameIndex;

                    Page += 1;
                    PointerPte += 1;
                } while (PointerPte <= LastPte);


                ExReleaseCacheAwarePushLockShared (PushLock);
                KeLeaveGuardedRegionThread (&Thread->Tcb);

                return;
            }

            if (PhysicalView->u.LongFlags & MI_PHYSICAL_VIEW_LARGE) {

                //
                // The PTE cannot be referenced (it doesn't exist), but it
                // serves the useful purpose of identifying when we cross
                // PDEs and therefore must recompute the base PFN.
                //

                PointerPte = MiGetPteAddress (StartVa);
                PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (StartVa);
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

                do {

                    if (Pfn1->AweReferenceCount >= (LONG)MmReferenceCountCheck) {
                        ASSERT (FALSE);
                        ExReleaseCacheAwarePushLockShared (PushLock);
                        KeLeaveGuardedRegionThread (&Thread->Tcb);

                        *Page = MM_EMPTY_LIST;
                        status = STATUS_WORKING_SET_QUOTA;
                        goto failure2;
                    }

                    InterlockedIncrement (&Pfn1->AweReferenceCount);

                    *Page = PageFrameIndex;

                    NumberOfPagesToLock -= 1;

                    if (NumberOfPagesToLock == 0) {
                        break;
                    }

                    Page += 1;

                    PointerPte += 1;

                    if (!MiIsPteOnPdeBoundary (PointerPte)) {
                        PageFrameIndex += 1;
                        Pfn1 += 1;
                    }
                    else {
                        StartVa = MiGetVirtualAddressMappedByPte (PointerPte);
                        PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (StartVa);
                        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                    }

                } while (TRUE);

                ExReleaseCacheAwarePushLockShared (PushLock);
                KeLeaveGuardedRegionThread (&Thread->Tcb);

                return;
            }
        }
    }

DefaultProbeAndLock:

    NumberOfPagesSpanned = NumberOfPagesToLock;

    if (!MI_IS_PHYSICAL_ADDRESS(Va)) {

        ProbeStatus = STATUS_SUCCESS;

        MmSavePageFaultReadAhead (Thread, &SavedState);
        MmSetPageFaultReadAhead (Thread, (ULONG)(NumberOfPagesToLock - 1));

        try {

            do {

                *Page = MM_EMPTY_LIST;

                //
                // Make sure the page is resident.
                //

                *(volatile CHAR *)Va;

                if ((Operation != IoReadAccess) &&
                    (Va <= MM_HIGHEST_USER_ADDRESS)) {

                    //
                    // Probe for write access as well.
                    //

                    ProbeForWriteChar ((PCHAR)Va);
                }

                NumberOfPagesToLock -= 1;

                MmSetPageFaultReadAhead (Thread, (ULONG)(NumberOfPagesToLock - 1));
                Va = (PVOID) (((ULONG_PTR)Va + PAGE_SIZE) & ~(PAGE_SIZE - 1));
                Page += 1;
            } while (Va < EndVa);

            ASSERT (NumberOfPagesToLock == 0);
            Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        } except (EXCEPTION_EXECUTE_HANDLER) {
            ProbeStatus = GetExceptionCode();
        }

        //
        // We may still fault again below but it's generally rare.
        // Restore this thread's normal fault behavior now.
        //

        MmResetPageFaultReadAhead (Thread, SavedState);

        if (ProbeStatus != STATUS_SUCCESS) {
            MI_INSTRUMENT_PROBE_RAISES(1);
            MemoryDescriptorList->Process = NULL;
            ExRaiseStatus (ProbeStatus);
            return;
        }

        PointerPte = MiGetPteAddress (StartVa);
    }
    else {

        //
        // Set PointerPte to NULL to indicate this is a physical address range.
        //

        if (Va <= MM_HIGHEST_USER_ADDRESS) {
            PointerPte = MiGetPteAddress (StartVa);
        }
        else {
            PointerPte = NULL;
        }

        *Page = MM_EMPTY_LIST;
    }

    PointerPxe = MiGetPxeAddress (StartVa);
    PointerPpe = MiGetPpeAddress (StartVa);
    PointerPde = MiGetPdeAddress (StartVa);

    Va = AlignedVa;
    ASSERT (Page == (PPFN_NUMBER)(MemoryDescriptorList + 1));

    //
    // Indicate whether this is a read or write operation.
    //

    if (Operation != IoReadAccess) {
        MemoryDescriptorList->MdlFlags |= MDL_WRITE_OPERATION;
    }
    else {
        MemoryDescriptorList->MdlFlags &= ~(MDL_WRITE_OPERATION);
    }

    //
    // Initialize MdlFlags (assume the probe will succeed).
    //

    MemoryDescriptorList->MdlFlags |= MDL_PAGES_LOCKED;

    if (Va <= MM_HIGHEST_USER_ADDRESS) {

        //
        // These are user space addresses, check them carefully.
        //

        ASSERT (NumberOfPagesSpanned != 0);

        CurrentProcess = PsGetCurrentProcess ();

        //
        // Initialize the MDL process field (assume the probe will succeed).
        //

        MemoryDescriptorList->Process = CurrentProcess;

        LastPte = MiGetPteAddress ((PCHAR)EndVa - 1);

        InterlockedExchangeAddSizeT (&CurrentProcess->NumberOfLockedPages,
                                     NumberOfPagesSpanned);
    }
    else {

        CurrentProcess = NULL;

        MemoryDescriptorList->Process = NULL;

        Va = (PCHAR)Va + MemoryDescriptorList->ByteOffset;

        NumberOfPagesToLock = ADDRESS_AND_SIZE_TO_SPAN_PAGES (Va,
                                    MemoryDescriptorList->ByteCount);

        if (PointerPte == NULL) {

            //
            // On certain architectures, virtual addresses
            // may be physical and hence have no corresponding PTE.
            //

            PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (Va);
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            LastPageFrameIndex = PageFrameIndex + NumberOfPagesToLock;

            //
            // Acquire the PFN database lock.
            //
    
            LOCK_PFN2 (OldIrql);

            ASSERT ((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0);

            //
            // Ensure the systemwide locked pages count remains fluid.
            //
    
            if (MI_NONPAGABLE_MEMORY_AVAILABLE() <= (SPFN_NUMBER) NumberOfPagesToLock) {
    
                //
                // This page is for nonpaged privileged code or data and must
                // already be resident so continue onwards.
                //
    
                MI_INSTRUMENT_PROBE_RAISES(8);
            }
    
            do {
    
                //
                // Check to make sure each page is not locked down an unusually
                // high number of times.
                //
    
                ASSERT (MI_IS_PFN (PageFrameIndex));
                ASSERT (PageFrameIndex <= MmHighestPhysicalPage);

                if (Pfn1->u3.e2.ReferenceCount >= MmReferenceCountCheck) {
                    UNLOCK_PFN2 (OldIrql);
                    ASSERT (FALSE);
                    status = STATUS_WORKING_SET_QUOTA;
                    goto failure;
                }
    
                MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, TRUE, 0);

                if (MemoryDescriptorList->MdlFlags & MDL_WRITE_OPERATION) {
                    MI_SNAP_DIRTY (Pfn1, 1, 0x99);
                }

                Pfn1->u3.e2.ReferenceCount += 1;

                *Page = PageFrameIndex;

                Page += 1;
                PageFrameIndex += 1;
                Pfn1 += 1;

            } while (PageFrameIndex < LastPageFrameIndex);

            UNLOCK_PFN2 (OldIrql);
            return;
        }

        //
        // Since this operation is to a system address, no need to check for
        // PTE write access below so mark the access as a read so only the
        // operation type (and not where the Va is) needs to be checked in the
        // subsequent loop.
        //

        Operation = IoReadAccess;

        LastPte = MiGetPteAddress ((PCHAR)EndVa - 1);
    }

    LOCK_PFN2 (OldIrql);

    do {

        while (
#if (_MI_PAGING_LEVELS>=4)
               (PointerPxe->u.Hard.Valid == 0) ||
#endif
#if (_MI_PAGING_LEVELS>=3)
               (PointerPpe->u.Hard.Valid == 0) ||
#endif
               ((PointerPde->u.Hard.Valid == 0) ||
                (((MI_PDE_MAPS_LARGE_PAGE (PointerPde)) == 0) &&
                 (PointerPte->u.Hard.Valid == 0)))) {

            //
            // The VA is not resident, release the PFN lock and access the page
            // to make it appear.
            //

            UNLOCK_PFN2 (OldIrql);

            MmSavePageFaultReadAhead (Thread, &SavedState);
            MmSetPageFaultReadAhead (Thread, 0);

            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            status = MmAccessFault (FALSE, Va, KernelMode, NULL);

            MmResetPageFaultReadAhead (Thread, SavedState);

            if (!NT_SUCCESS(status)) {
                goto failure;
            }

            LOCK_PFN2 (OldIrql);
        }

        if (MI_PDE_MAPS_LARGE_PAGE (PointerPde)) {

            Va = MiGetVirtualAddressMappedByPte (PointerPte);

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde) + MiGetPteOffset (Va);
        }
        else {

            PteContents = *PointerPte;

            //
            // There is a subtle race here where the PTE contents can get zeroed
            // by a thread running on another processor.  This can only happen
            // for an AWE address space because these ranges (deliberately for
            // performance reasons) do not acquire the PFN lock during remap
            // operations.  In this case, one of 2 scenarios is possible -
            // either the old PTE is read or the new.  The new may be a zero
            // PTE if the map request was to invalidate *or* non-zero (and
            // valid) if the map request was inserting a new entry.  For the
            // latter, we don't care if we lock the old or new frame here as
            // it's an application bug to provoke this behavior - and
            // regardless of which is used, no corruption can occur because
            // the PFN lock is acquired during an NtFreeUserPhysicalPages.
            // But the former must be checked for explicitly here.  As a
            // separate note, the PXE/PPE/PDE accesses above are always safe
            // even for the AWE deletion race because these tables
            // are never lazy-allocated for AWE ranges.
            //

            if (PteContents.u.Hard.Valid == 0) {
                ASSERT (PteContents.u.Long == 0);
                ASSERT (PsGetCurrentProcess ()->AweInfo != NULL);
                UNLOCK_PFN2 (OldIrql);
                status = STATUS_ACCESS_VIOLATION;
                goto failure;
            }

#if defined (_MIALT4K_)
        
            if (PteContents.u.Hard.Cache == MM_PTE_CACHE_RESERVED) {
                           
                //
                // This is a wow64 split page - ie: the individual 4k
                // pages have different permissions, so each 4k page within
                // this native page must be probed individually.
                //
                // Note split pages are generally rare.
                //
    
                ASSERT (PsGetCurrentProcess()->Wow64Process != NULL);
                ASSERT (EndVa <= MmWorkingSetList->HighestUserAddress);
    
                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                PointerAltPte = MiGetAltPteAddress (Va);
                LastPointerAltPte = PointerAltPte + (PAGE_SIZE / PAGE_4K) - 1;
    
                AltPointerPxe = MiGetPxeAddress (PointerAltPte);
                AltPointerPpe = MiGetPpeAddress (PointerAltPte);
                AltPointerPde = MiGetPdeAddress (PointerAltPte);
                AltPointerPte = MiGetPteAddress (PointerAltPte);
    
#if (_MI_PAGING_LEVELS==4)
                while ((AltPointerPxe->u.Hard.Valid == 0) ||
                       (AltPointerPpe->u.Hard.Valid == 0) ||
                       (AltPointerPde->u.Hard.Valid == 0) ||
                       (AltPointerPte->u.Hard.Valid == 0))
#elif (_MI_PAGING_LEVELS==3)
                while ((AltPointerPpe->u.Hard.Valid == 0) ||
                       (AltPointerPde->u.Hard.Valid == 0) ||
                       (AltPointerPte->u.Hard.Valid == 0))
#else
                while ((AltPointerPde->u.Hard.Valid == 0) ||
                       (AltPointerPte->u.Hard.Valid == 0))
#endif
                {
    
                    //
                    // The ALTPTEs are not resident, release the PFN lock and
                    // access it to make it appear.  Then restart the entire
                    // operation as the PFN lock was released so anything
                    // could have happened to the address space.
                    //
    
                    UNLOCK_PFN2 (OldIrql);
    
                    MmSavePageFaultReadAhead (Thread, &SavedState);
                    MmSetPageFaultReadAhead (Thread, 0);
    
                    status = MmAccessFault (FALSE, PointerAltPte, KernelMode, NULL);
    
                    MmResetPageFaultReadAhead (Thread, SavedState);
    
                    if (!NT_SUCCESS(status)) {
                        goto failure;
                    }
    
                    LOCK_PFN2 (OldIrql);
    
                    continue;
                }

                //
                // The ALTPTEs are now present and the PFN lock is held again.  
                // Examine the individual 4k page states in the ALTPTEs.
                //
                // Note that only the relevant 4k pages can be examined - ie:
                // if the transfer starts in the 2nd 4k of a native page,
                // then don't examine the 1st 4k.  If the transfer ends in
                // the first half of a native page, then don't examine the
                // 2nd 4k.
                //
                
                ASSERT (PAGE_SIZE == 2 * PAGE_4K);

                if (PAGE_ALIGN (StartVa) == PAGE_ALIGN (Va)) {

                    //
                    // We are in the first page, see if we need to round up.
                    //

                    if (BYTE_OFFSET (StartVa) >= PAGE_4K) {
                        PointerAltPte += 1;
                        Va = (PVOID)((ULONG_PTR)Va + PAGE_4K);
                    }
                }

                if (PAGE_ALIGN ((PCHAR)EndVa - 1) == PAGE_ALIGN (Va)) {

                    //
                    // We are in the last page, see if we need to round down.
                    //

                    if (BYTE_OFFSET ((PCHAR)EndVa - 1) < PAGE_4K) {
                        LastPointerAltPte -= 1;
                    }
                }

                //
                // We better not have rounded up and down in the same page !
                //

                ASSERT (PointerAltPte <= LastPointerAltPte);
    
                ASSERT (PointerAltPte != NULL);
    
                do {
    
                    //
                    //  If the sub 4k page is :
                    //
                    //  1 - No access or
                    //  2 - This is a private not-committed page or
                    //  3 - This is write operation and the page is read only
                    //
                    // then return an access violation.
                    //
    
                    AltPteContents = *PointerAltPte;

                    if (AltPteContents.u.Alt.NoAccess != 0) {
                        status = STATUS_ACCESS_VIOLATION;
                        UNLOCK_PFN2 (OldIrql);
                        goto failure;
                    }

                    if ((AltPteContents.u.Alt.Commit == 0) && (AltPteContents.u.Alt.Private != 0)) {
                        status = STATUS_ACCESS_VIOLATION;
                        UNLOCK_PFN2 (OldIrql);
                        goto failure;
                    }

                    if (Operation != IoReadAccess) {

                        //
                        // If the caller is writing and the ALTPTE indicates
                        // it's not writable or copy on write, then AV.
                        //
                        // If it's copy on write, then fall through for further
                        // interrogation.
                        //

                        if ((AltPteContents.u.Alt.Write == 0) &&
                            (AltPteContents.u.Alt.CopyOnWrite == 0)) {
    
                            status = STATUS_ACCESS_VIOLATION;
                            UNLOCK_PFN2 (OldIrql);
                            goto failure;
                        }
                    }
    
                    //
                    //  If the sub 4k page is :
                    //
                    //  1 - has not been accessed yet or
                    //  2 - demand-fill zero or
                    //  3 - copy-on-write, and this is a write operation
                    //
                    //  then go the long way and see if it can be paged in.
                    //
    
                    if ((AltPteContents.u.Alt.Accessed == 0) ||
                        (AltPteContents.u.Alt.FillZero != 0) ||
                        ((Operation != IoReadAccess) && (AltPteContents.u.Alt.CopyOnWrite == 1))) {
    
                        UNLOCK_PFN2 (OldIrql);
    
                        MmSavePageFaultReadAhead (Thread, &SavedState);
                        MmSetPageFaultReadAhead (Thread, 0);
    
                        status = MmX86Fault (FALSE, Va, KernelMode, NULL);
    
                        MmResetPageFaultReadAhead (Thread, SavedState);
    
                        if (!NT_SUCCESS(status)) {
                            goto failure;
                        }
    
                        //
                        // Clear PointerAltPte to signify a restart is needed
                        // (because the PFN lock was released so the address
                        // space may have changed).
                        //

                        PointerAltPte = NULL;

                        LOCK_PFN2 (OldIrql);
                        
                        break;
                    } 
                    
                    PointerAltPte += 1;
                    Va = (PVOID)((ULONG_PTR)Va + PAGE_4K);
    
                } while (PointerAltPte <= LastPointerAltPte);
    
                if (PointerAltPte == NULL) {
                    continue;
                }
            }
#endif

            if (Operation != IoReadAccess) {

                if ((PteContents.u.Long & MM_PTE_WRITE_MASK) == 0) {

                    if (PteContents.u.Long & MM_PTE_COPY_ON_WRITE_MASK) {

                        //
                        // The protection has changed from writable to copy on
                        // write.  This can happen if a fork is in progress for
                        // example.  Restart the operation at the top.
                        //

                        Va = MiGetVirtualAddressMappedByPte (PointerPte);

                        if (Va <= MM_HIGHEST_USER_ADDRESS) {
                            UNLOCK_PFN2 (OldIrql);

                            MmSavePageFaultReadAhead (Thread, &SavedState);
                            MmSetPageFaultReadAhead (Thread, 0);

                            status = MmAccessFault (TRUE, Va, KernelMode, NULL);

                            MmResetPageFaultReadAhead (Thread, SavedState);

                            if (!NT_SUCCESS(status)) {
                                goto failure;
                            }

                            LOCK_PFN2 (OldIrql);

                            continue;
                        }
                    }

                    //
                    // The caller has made the page protection more
                    // restrictive, this should never be done once the
                    // request has been issued !  Rather than wading
                    // through the PFN database entry to see if it
                    // could possibly work out, give the caller an
                    // access violation.
                    //

#if DBG
                    DbgPrint ("MmProbeAndLockPages: PTE %p %p changed\n",
                        PointerPte,
                        PteContents.u.Long);

                    if (MmStopOnBadProbe) {
                        DbgBreakPoint ();
                    }
#endif

                    UNLOCK_PFN2 (OldIrql);
                    status = STATUS_ACCESS_VIOLATION;
                    goto failure;
                }
            }

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
        }

        if (MI_IS_PFN (PageFrameIndex)) {

            if (MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) {

                //
                // MDLs cannot be filled with a mixture of real and I/O
                // space page frame numbers.
                //

                MI_INSTRUMENT_PROBE_RAISES(6);
                UNLOCK_PFN2 (OldIrql);
                status = STATUS_ACCESS_VIOLATION;
                goto failure;
            }

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            //
            // Check to make sure this page is not locked down an unusually
            // high number of times.
            //
    
            if (Pfn1->u3.e2.ReferenceCount >= MmReferenceCountCheck) {
                MI_INSTRUMENT_PROBE_RAISES(3);
                UNLOCK_PFN2 (OldIrql);
                ASSERT (FALSE);
                status = STATUS_WORKING_SET_QUOTA;
                goto failure;
            }

            //
            // Ensure the systemwide locked pages count is fluid.
            //
    
            if (MI_NONPAGABLE_MEMORY_AVAILABLE() <= 0) {

                //
                // If this page is for privileged code/data,
                // then force it in regardless.
                //

                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                if ((Va < MM_HIGHEST_USER_ADDRESS) ||
                    (MI_IS_SYSTEM_CACHE_ADDRESS(Va)) ||
                    ((Va >= MmPagedPoolStart) && (Va <= MmPagedPoolEnd))) {

                    MI_INSTRUMENT_PROBE_RAISES(5);
                    UNLOCK_PFN2 (OldIrql);
                    status = STATUS_WORKING_SET_QUOTA;
                    goto failure;
                }

                MI_INSTRUMENT_PROBE_RAISES(12);
            }
    
            if (MemoryDescriptorList->MdlFlags & MDL_WRITE_OPERATION) {
                MI_SNAP_DIRTY (Pfn1, 1, 0x98);
            }

            if (MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, FALSE, 0) == FALSE) {

                //
                // If this page is for privileged code/data,
                // then force it in regardless.
                //
    
                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                if ((Va < MM_HIGHEST_USER_ADDRESS) ||
                    (MI_IS_SYSTEM_CACHE_ADDRESS(Va)) ||
                    ((Va >= MmPagedPoolStart) && (Va <= MmPagedPoolEnd))) {

                    UNLOCK_PFN2 (OldIrql);
                    MI_INSTRUMENT_PROBE_RAISES(10);
                    status = STATUS_WORKING_SET_QUOTA;
                    goto failure;
                }
                MI_ADD_LOCKED_PAGE_CHARGE(Pfn1, TRUE, 0);
            }

            Pfn1->u3.e2.ReferenceCount += 1;
        }
        else {

            //
            // This is an I/O space address - there is no PFN database entry
            // for it, so no reference counts may be modified for these pages.
            //
            // Don't charge page locking for this transfer as it is all
            // physical, just add it to the MDL.
            //

            if (CurrentProcess != NULL) {

                //
                // The VA better be within a \Device\PhysicalMemory VAD.
                //

                if (CurrentProcess->PhysicalVadRoot == NULL) {
#if DBG
                    DbgPrint ("MmProbeAndLockPages: Physical VA0 %p not found\n", Va);
                    DbgBreakPoint ();
#endif
                    UNLOCK_PFN2 (OldIrql);
                    MI_INSTRUMENT_PROBE_RAISES (2);
                    status = STATUS_ACCESS_VIOLATION;
                    goto failure;
                }

                Va = MiGetVirtualAddressMappedByPte (PointerPte);

                SearchResult = MiFindNodeOrParent (CurrentProcess->PhysicalVadRoot,
                                                   MI_VA_TO_VPN (Va),
                                                   (PMMADDRESS_NODE *) &PhysicalView);
                if ((SearchResult == TableFoundNode) &&
                    (PhysicalView->u.LongFlags & (MI_PHYSICAL_VIEW_PHYS))) {

                    ASSERT (PhysicalView->Vad->u.VadFlags.PhysicalMapping == 1);

                    //
                    // The range lies within a physical VAD.
                    //
    
                    if (Operation != IoReadAccess) {
    
                        //
                        // Ensure the VAD is writable.  Changing individual PTE
                        // protections in a physical VAD is not allowed.
                        //
    
                        if ((PhysicalView->Vad->u.VadFlags.Protection & MM_READWRITE) == 0) {
                            MI_INSTRUMENT_PROBE_RAISES(4);
                            UNLOCK_PFN2 (OldIrql);
                            status = STATUS_ACCESS_VIOLATION;
                            goto failure;
                        }
                    }
    
                    if (((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0) &&
                        (Page != (PPFN_NUMBER)(MemoryDescriptorList + 1))) {

                        //
                        // MDLs cannot be filled with a mixture of real and I/O
                        // space page frame numbers.
                        //

                        MI_INSTRUMENT_PROBE_RAISES(7);
                        UNLOCK_PFN2 (OldIrql);
                        status = STATUS_ACCESS_VIOLATION;
                        goto failure;
                    }
                }
                else {
#if DBG
                    DbgPrint ("MmProbeAndLockPages: Physical VA1 %p not found\n", Va);
                    DbgBreakPoint ();
#endif
                    UNLOCK_PFN2 (OldIrql);
                    MI_INSTRUMENT_PROBE_RAISES (11);
                    status = STATUS_ACCESS_VIOLATION;
                    goto failure;
                }
            }
#if DBG

            //
            // This page is in I/O space, therefore all the argument pages
            // better be.
            //

            if (Page != (PPFN_NUMBER)(MemoryDescriptorList + 1)) {
                ASSERT (!MI_IS_PFN (*(PPFN_NUMBER)(MemoryDescriptorList + 1)));
            }
#endif

            if (((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0) &&
                (CurrentProcess != NULL)) {

                InterlockedExchangeAddSizeT (&CurrentProcess->NumberOfLockedPages,
                                             0 - NumberOfPagesSpanned);
            }
            MemoryDescriptorList->MdlFlags |= MDL_IO_SPACE;
        }

        *Page = PageFrameIndex;

        Page += 1;

        PointerPte += 1;

        if (MiIsPteOnPdeBoundary (PointerPte)) {

            PointerPde += 1;

            if (MiIsPteOnPpeBoundary (PointerPte)) {
                PointerPpe += 1;
                if (MiIsPteOnPxeBoundary (PointerPte)) {
                    PointerPxe += 1;
                }
            }
        }

    } while (PointerPte <= LastPte);

    UNLOCK_PFN2 (OldIrql);

    if (AlignedVa <= MM_HIGHEST_USER_ADDRESS) {

        //
        // User space buffers that reside in I/O space need to be reference
        // counted because SANs will want to reuse the physical space but cannot
        // do this unless it is guaranteed there are no more pending I/Os
        // going from/to it.
        //

        if (MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) {
            if (MiReferenceIoSpace (MemoryDescriptorList, Page) == FALSE) {
                status = STATUS_INSUFFICIENT_RESOURCES;
                goto failure;
            }
        }

        if (MmTrackLockedPages == TRUE) {

            ASSERT (NumberOfPagesSpanned != 0);

            RtlGetCallersAddress (&CallingAddress, &CallersCaller);

            MiAddMdlTracker (MemoryDescriptorList,
                             CallingAddress,
                             CallersCaller,
                             NumberOfPagesSpanned,
                             1);
        }
    }

    return;

failure:

    //
    // An exception occurred.  Unlock the pages locked so far.
    //

    if (MmTrackLockedPages == TRUE) {

        //
        // Adjust the MDL length so that MmUnlockPages only
        // processes the part that was completed.
        //

        ULONG PagesLocked;

        PagesLocked = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartVa,
                              MemoryDescriptorList->ByteCount);

        RtlGetCallersAddress (&CallingAddress, &CallersCaller);

        MiAddMdlTracker (MemoryDescriptorList,
                         CallingAddress,
                         CallersCaller,
                         PagesLocked,
                         0);
    }

failure2:

    MmUnlockPages (MemoryDescriptorList);

    //
    // Raise an exception of access violation to the caller.
    //

    MI_INSTRUMENT_PROBE_RAISES(13);
    ExRaiseStatus (status);
    return;
}

NTKERNELAPI
VOID
MmProbeAndLockProcessPages (
    IN OUT PMDL MemoryDescriptorList,
    IN PEPROCESS Process,
    IN KPROCESSOR_MODE AccessMode,
    IN LOCK_OPERATION Operation
    )

/*++

Routine Description:

    This routine probes and locks the address range specified by
    the MemoryDescriptorList in the specified Process for the AccessMode
    and Operation.

Arguments:

    MemoryDescriptorList - Supplies a pre-initialized MDL that describes the
                           address range to be probed and locked.

    Process - Specifies the address of the process whose address range is
              to be locked.

    AccessMode - The mode for which the probe should check access to the range.

    Operation - Supplies the type of access which for which to check the range.

Return Value:

    None.

--*/

{
    KAPC_STATE ApcState;
    LOGICAL Attached;
    NTSTATUS Status;

    Attached = FALSE;
    Status = STATUS_SUCCESS;

    if (Process != PsGetCurrentProcess ()) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    try {

        MmProbeAndLockPages (MemoryDescriptorList,
                             AccessMode,
                             Operation);

    } except (EXCEPTION_EXECUTE_HANDLER) {
        Status = GetExceptionCode();
    }

    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
    }

    if (Status != STATUS_SUCCESS) {
        ExRaiseStatus (Status);
    }
    return;
}

VOID
MiAddMdlTracker (
    IN PMDL MemoryDescriptorList,
    IN PVOID CallingAddress,
    IN PVOID CallersCaller,
    IN PFN_NUMBER NumberOfPagesToLock,
    IN ULONG Who
    )

/*++

Routine Description:

    This routine adds an MDL to the specified process' chain.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List
                           (MDL). The MDL must supply the length. The
                           physical page portion of the MDL is updated when
                           the pages are locked in memory.

    CallingAddress - Supplies the address of the caller of our caller.

    CallersCaller - Supplies the address of the caller of CallingAddress.

    NumberOfPagesToLock - Specifies the number of pages to lock.

    Who - Specifies which routine is adding the entry.

Return Value:

    None - exceptions are raised.

Environment:

    Kernel mode.  APC_LEVEL and below.

--*/

{
    PEPROCESS Process;
    PLOCK_HEADER LockedPagesHeader;
    PLOCK_TRACKER Tracker;
    PLOCK_TRACKER P;
    PLIST_ENTRY NextEntry;
    KLOCK_QUEUE_HANDLE LockHandle;

    ASSERT (MmTrackLockedPages == TRUE);

    Process = MemoryDescriptorList->Process;

    if (Process == NULL) {
        return;
    }

    LockedPagesHeader = Process->LockedPagesList;

    if (LockedPagesHeader == NULL) {
        return;
    }

    //
    // It's ok to check unsynchronized for aborted tracking as the worst case
    // is just that one more entry gets added which will be freed later anyway.
    // The main purpose behind aborted tracking is that frees and exits don't
    // mistakenly bugcheck when an entry cannot be found.
    //

    if (LockedPagesHeader->Valid == FALSE) {
        return;
    }

    Tracker = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof (LOCK_TRACKER),
                                     'kLmM');

    if (Tracker == NULL) {

        //
        // It's ok to set this without synchronization as the worst case
        // is just that a few more entries gets added which will be freed
        // later anyway.  The main purpose behind aborted tracking is that
        // frees and exits don't mistakenly bugcheck when an entry cannot
        // be found.
        //
    
        LockedPagesHeader->Valid = FALSE;

        return;
    }

    Tracker->Mdl = MemoryDescriptorList;
    Tracker->Count = NumberOfPagesToLock;
    Tracker->StartVa = MemoryDescriptorList->StartVa;
    Tracker->Offset = MemoryDescriptorList->ByteOffset;
    Tracker->Length = MemoryDescriptorList->ByteCount;
    Tracker->Page = *(PPFN_NUMBER)(MemoryDescriptorList + 1);

    Tracker->CallingAddress = CallingAddress;
    Tracker->CallersCaller = CallersCaller;

    Tracker->Who = Who;
    Tracker->Process = Process;

    //
    // Update the list for this process.  First make sure it's not already
    // inserted.
    //

    KeAcquireInStackQueuedSpinLock (&LockedPagesHeader->Lock, &LockHandle);

    NextEntry = LockedPagesHeader->ListHead.Flink;

    while (NextEntry != &LockedPagesHeader->ListHead) {

        P = CONTAINING_RECORD (NextEntry,
                               LOCK_TRACKER,
                               ListEntry);

        if (P->Mdl == MemoryDescriptorList) {
            KeBugCheckEx (LOCKED_PAGES_TRACKER_CORRUPTION,
                          0x1,
                          (ULONG_PTR) P,
                          (ULONG_PTR) MemoryDescriptorList,
                          (ULONG_PTR) LockedPagesHeader->Count);
        }
        NextEntry = NextEntry->Flink;
    }

    InsertTailList (&LockedPagesHeader->ListHead, &Tracker->ListEntry);

    LockedPagesHeader->Count += NumberOfPagesToLock;

    KeReleaseInStackQueuedSpinLock (&LockHandle);
}

LOGICAL
MiFreeMdlTracker (
    IN OUT PMDL MemoryDescriptorList,
    IN PFN_NUMBER NumberOfPages
    )

/*++

Routine Description:

    This deletes an MDL from the specified process' chain.  Used specifically
    by MmProbeAndLockSelectedPages () because it builds an MDL in its local
    stack and then copies the requested pages into the real MDL.  This lets
    us track these pages.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List
                           (MDL). The MDL must supply the length.

    NumberOfPages - Supplies the number of pages to be freed.

Return Value:

    TRUE.

Environment:

    Kernel mode.  APC_LEVEL and below.

--*/
{
    KLOCK_QUEUE_HANDLE LockHandle;
    PLOCK_TRACKER Tracker;
    PLIST_ENTRY NextEntry;
    PLOCK_HEADER LockedPagesHeader;
    PPFN_NUMBER Page;
    PVOID PoolToFree;

    ASSERT (MemoryDescriptorList->Process != NULL);

    LockedPagesHeader = (PLOCK_HEADER)MemoryDescriptorList->Process->LockedPagesList;

    if (LockedPagesHeader == NULL) {
        return TRUE;
    }

    PoolToFree = NULL;

    Page = (PPFN_NUMBER) (MemoryDescriptorList + 1);

    KeAcquireInStackQueuedSpinLock (&LockedPagesHeader->Lock, &LockHandle);

    NextEntry = LockedPagesHeader->ListHead.Flink;
    while (NextEntry != &LockedPagesHeader->ListHead) {

        Tracker = CONTAINING_RECORD (NextEntry,
                                     LOCK_TRACKER,
                                     ListEntry);

        if (MemoryDescriptorList == Tracker->Mdl) {

            if (PoolToFree != NULL) {
                KeBugCheckEx (LOCKED_PAGES_TRACKER_CORRUPTION,
                              0x3,
                              (ULONG_PTR) PoolToFree,
                              (ULONG_PTR) Tracker,
                              (ULONG_PTR) MemoryDescriptorList);
            }

            ASSERT (Tracker->Page == *Page);
            ASSERT (Tracker->Count == NumberOfPages);

            RemoveEntryList (NextEntry);
            LockedPagesHeader->Count -= NumberOfPages;

            PoolToFree = (PVOID) Tracker;
        }
        NextEntry = Tracker->ListEntry.Flink;
    }

    KeReleaseInStackQueuedSpinLock (&LockHandle);

    if (PoolToFree == NULL) {

        //
        // A driver is trying to unlock pages that aren't locked.
        //

        if (LockedPagesHeader->Valid == FALSE) {
            return TRUE;
        }

        KeBugCheckEx (PROCESS_HAS_LOCKED_PAGES,
                      1,
                      (ULONG_PTR)MemoryDescriptorList,
                      MemoryDescriptorList->Process->NumberOfLockedPages,
                      (ULONG_PTR)MemoryDescriptorList->Process->LockedPagesList);
    }

    ExFreePool (PoolToFree);

    return TRUE;
}


LOGICAL
MmUpdateMdlTracker (
    IN PMDL MemoryDescriptorList,
    IN PVOID CallingAddress,
    IN PVOID CallersCaller
    )

/*++

Routine Description:

    This updates an MDL in the specified process' chain.  Used by the I/O
    system so that proper driver identification can be done even when I/O
    is actually locking the pages on their behalf.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List.

    CallingAddress - Supplies the address of the caller of our caller.

    CallersCaller - Supplies the address of the caller of CallingAddress.

Return Value:

    TRUE if the MDL was found, FALSE if not.

Environment:

    Kernel mode.  APC_LEVEL and below.

--*/
{
    KLOCK_QUEUE_HANDLE LockHandle;
    PLOCK_TRACKER Tracker;
    PLIST_ENTRY NextEntry;
    PLOCK_HEADER LockedPagesHeader;
    PEPROCESS Process;

    ASSERT (MmTrackLockedPages == TRUE);

    Process = MemoryDescriptorList->Process;

    if (Process == NULL) {
        return FALSE;
    }

    LockedPagesHeader = (PLOCK_HEADER) Process->LockedPagesList;

    if (LockedPagesHeader == NULL) {
        return FALSE;
    }

    KeAcquireInStackQueuedSpinLock (&LockedPagesHeader->Lock, &LockHandle);

    //
    // Walk the list backwards as it's likely the MDL was
    // just recently inserted.
    //

    NextEntry = LockedPagesHeader->ListHead.Blink;
    while (NextEntry != &LockedPagesHeader->ListHead) {

        Tracker = CONTAINING_RECORD (NextEntry,
                                     LOCK_TRACKER,
                                     ListEntry);

        if (MemoryDescriptorList == Tracker->Mdl) {
            ASSERT (Tracker->Page == *(PPFN_NUMBER) (MemoryDescriptorList + 1));
            Tracker->CallingAddress = CallingAddress;
            Tracker->CallersCaller = CallersCaller;
            KeReleaseInStackQueuedSpinLock (&LockHandle);
            return TRUE;
        }
        NextEntry = Tracker->ListEntry.Blink;
    }

    KeReleaseInStackQueuedSpinLock (&LockHandle);

    //
    // The caller is trying to update an MDL that is no longer locked.
    //

    return FALSE;
}


LOGICAL
MiUpdateMdlTracker (
    IN PMDL MemoryDescriptorList,
    IN ULONG AdvancePages
    )

/*++

Routine Description:

    This updates an MDL in the specified process' chain.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List.

    AdvancePages - Supplies the number of pages being advanced.

Return Value:

    TRUE if the MDL was found, FALSE if not.

Environment:

    Kernel mode.  DISPATCH_LEVEL and below.

--*/
{
    KLOCK_QUEUE_HANDLE LockHandle;
    PPFN_NUMBER Page;
    PLOCK_TRACKER Tracker;
    PLIST_ENTRY NextEntry;
    PLOCK_HEADER LockedPagesHeader;
    PEPROCESS Process;

    ASSERT (MmTrackLockedPages == TRUE);

    Process = MemoryDescriptorList->Process;

    if (Process == NULL) {
        return FALSE;
    }

    LockedPagesHeader = (PLOCK_HEADER) Process->LockedPagesList;

    if (LockedPagesHeader == NULL) {
        return FALSE;
    }

    KeAcquireInStackQueuedSpinLock (&LockedPagesHeader->Lock, &LockHandle);

    //
    // Walk the list backwards as it's likely the MDL was
    // just recently inserted.
    //

    NextEntry = LockedPagesHeader->ListHead.Blink;
    while (NextEntry != &LockedPagesHeader->ListHead) {

        Tracker = CONTAINING_RECORD (NextEntry,
                                     LOCK_TRACKER,
                                     ListEntry);

        if (MemoryDescriptorList == Tracker->Mdl) {

            Page = (PPFN_NUMBER) (MemoryDescriptorList + 1);

            ASSERT (Tracker->Page == *Page);
            ASSERT (Tracker->Count > AdvancePages);

            Tracker->Page = *(Page + AdvancePages);
            Tracker->Count -= AdvancePages;

            KeReleaseInStackQueuedSpinLock (&LockHandle);
            return TRUE;
        }
        NextEntry = Tracker->ListEntry.Blink;
    }

    KeReleaseInStackQueuedSpinLock (&LockHandle);

    //
    // The caller is trying to update an MDL that is no longer locked.
    //

    return FALSE;
}


NTKERNELAPI
VOID
MmProbeAndLockSelectedPages (
    IN OUT PMDL MemoryDescriptorList,
    IN PFILE_SEGMENT_ELEMENT SegmentArray,
    IN KPROCESSOR_MODE AccessMode,
    IN LOCK_OPERATION Operation
    )

/*++

Routine Description:

    This routine probes the specified pages, makes the pages resident and
    locks the physical pages mapped by the virtual pages in memory.  The
    Memory descriptor list is updated to describe the physical pages.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List
                           (MDL). The MDL must supply the length. The
                           physical page portion of the MDL is updated when
                           the pages are locked in memory.

    SegmentArray - Supplies a pointer to a list of buffer segments to be
                   probed and locked.

    AccessMode - Supplies the access mode in which to probe the arguments.
                 One of KernelMode or UserMode.

    Operation - Supplies the operation type.  One of IoReadAccess, IoWriteAccess
                or IoModifyAccess.

Return Value:

    None - exceptions are raised.

Environment:

    Kernel mode.  APC_LEVEL and below.

--*/

{
    NTSTATUS Status;
    PMDL TempMdl;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + 1];
    PPFN_NUMBER Page;
    PFILE_SEGMENT_ELEMENT LastSegment;
    PVOID CallingAddress;
    PVOID CallersCaller;
    ULONG NumberOfPagesToLock;

    PAGED_CODE();

    NumberOfPagesToLock = 0;

    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT (((ULONG_PTR)MemoryDescriptorList->ByteOffset & ~(PAGE_SIZE - 1)) == 0);

    ASSERT ((MemoryDescriptorList->MdlFlags & (
                    MDL_PAGES_LOCKED |
                    MDL_MAPPED_TO_SYSTEM_VA |
                    MDL_SOURCE_IS_NONPAGED_POOL |
                    MDL_PARTIAL |
                    MDL_IO_SPACE)) == 0);

    //
    // Initialize TempMdl.
    //

    TempMdl = (PMDL) MdlHack;

    //
    // Even systems without 64 bit pointers are required to zero the
    // upper 32 bits of the segment address so use alignment rather
    // than the buffer pointer.
    //

    MmInitializeMdl (TempMdl, SegmentArray->Buffer, PAGE_SIZE);

    Page = (PPFN_NUMBER) (MemoryDescriptorList + 1);

    //
    // Calculate the end of the segment list.
    //

    LastSegment = SegmentArray +
                  BYTES_TO_PAGES (MemoryDescriptorList->ByteCount);

    ASSERT (SegmentArray < LastSegment);

    //
    // Build a small Mdl for each segment and call probe and lock pages.
    // Then copy the PFNs to the real mdl.  The first page is processed
    // outside of the try/finally to ensure that the flags and process
    // field are correctly set in case MmUnlockPages needs to be called.
    //
    // Note that if the MmProbeAndLockPages of the first page raises an
    // exception, it is not handled here, but instead handed directly to
    // our caller (who must be handling it).
    //

    MmProbeAndLockPages (TempMdl, AccessMode, Operation);

    if (MmTrackLockedPages == TRUE) {

        //
        // Since we move the page from the temp MDL to the real one below
        // and never free the temp one, fixup our accounting now.
        //

        if (MiFreeMdlTracker (TempMdl, 1) == TRUE) {
            NumberOfPagesToLock += 1;
        }
    }

    *Page = *((PPFN_NUMBER) (TempMdl + 1));
    Page += 1;

    //
    // Copy the flags and process fields.
    //

    MemoryDescriptorList->MdlFlags |= TempMdl->MdlFlags;
    MemoryDescriptorList->Process = TempMdl->Process;

    Status = STATUS_SUCCESS;
    SegmentArray += 1;

    try {

        while (SegmentArray < LastSegment) {

            //
            // Even systems without 64 bit pointers are required to zero the
            // upper 32 bits of the segment address so use alignment rather
            // than the buffer pointer.
            //

            TempMdl->StartVa = (PVOID)(ULONG_PTR)SegmentArray->Buffer;
            TempMdl->MdlFlags = 0;

            SegmentArray += 1;
            MmProbeAndLockPages (TempMdl, AccessMode, Operation);

            if (MmTrackLockedPages == TRUE) {

                //
                // Since we move the page from the temp MDL to the real one
                // below and never free the temp one, fixup our accounting now.
                //

                if (MiFreeMdlTracker (TempMdl, 1) == TRUE) {
                    NumberOfPagesToLock += 1;
                }
            }

            *Page = *((PPFN_NUMBER) (TempMdl + 1));
            Page += 1;
        }
    } except (EXCEPTION_EXECUTE_HANDLER) {
        Status = GetExceptionCode ();
        ASSERT (!NT_SUCCESS (Status));
    }

    if (!NT_SUCCESS (Status)) {

        //
        // Adjust the MDL length so that MmUnlockPages only processes
        // the part that was completed.
        //

        MemoryDescriptorList->ByteCount =
            (ULONG) (Page - (PPFN_NUMBER) (MemoryDescriptorList + 1)) << PAGE_SHIFT;

        if (MmTrackLockedPages == TRUE) {

            RtlGetCallersAddress (&CallingAddress, &CallersCaller);

            MiAddMdlTracker (MemoryDescriptorList,
                             CallingAddress,
                             CallersCaller,
                             NumberOfPagesToLock,
                             2);
        }

        MmUnlockPages (MemoryDescriptorList);
        ExRaiseStatus (Status);
    }

    if (MmTrackLockedPages == TRUE) {

        RtlGetCallersAddress (&CallingAddress, &CallersCaller);

        MiAddMdlTracker (MemoryDescriptorList,
                         CallingAddress,
                         CallersCaller,
                         NumberOfPagesToLock,
                         3);
    }

    return;
}

VOID
MiDecrementReferenceCountForAwePage (
    IN PMMPFN Pfn1,
    IN LOGICAL PfnHeld
    )

/*++

Routine Description:

    This routine decrements the reference count for an AWE-allocated page.
    Descriptor List.  If this decrements the count to zero, the page is
    put on the freelist and various resident available and commitment
    counters are updated.

Arguments:

    Pfn - Supplies a pointer to the PFN database element for the physical
          page to decrement the reference count for.

    PfnHeld - Supplies TRUE if the caller holds the PFN lock.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    KIRQL OldIrql;

    if (PfnHeld == FALSE) {
        LOCK_PFN2 (OldIrql);
    }
    else {
        OldIrql = PASSIVE_LEVEL;
    }

    ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
    ASSERT (Pfn1->AweReferenceCount == 0);
    ASSERT (Pfn1->u4.AweAllocation == 1);

    if (Pfn1->u3.e2.ReferenceCount >= 2) {
        Pfn1->u3.e2.ReferenceCount -= 1;
        if (PfnHeld == FALSE) {
            UNLOCK_PFN2 (OldIrql);
        }
    }
    else {

        //
        // This is the final dereference - the page was sitting in
        // limbo (not on any list) waiting for this last I/O to complete.
        //

        ASSERT (Pfn1->u3.e1.PageLocation != ActiveAndValid);
        ASSERT (Pfn1->u2.ShareCount == 0);
        MiDecrementReferenceCount (Pfn1, MI_PFN_ELEMENT_TO_INDEX (Pfn1));

        if (PfnHeld == FALSE) {
            UNLOCK_PFN2 (OldIrql);
        }
    }

    MI_INCREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_FREE_AWE);

    MiReturnCommitment (1);
    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_MDL_PAGES, 1);

    InterlockedExchangeAddSizeT (&MmMdlPagesAllocated, -1);

    return;
}

SINGLE_LIST_ENTRY MmLockedIoPagesHead;

LOGICAL
MiReferenceIoSpace (
    IN OUT PMDL MemoryDescriptorList,
    IN PPFN_NUMBER Page
    )

/*++

Routine Description:

    This routine reference counts physical pages which reside in I/O space
    when they are probed on behalf of a user-initiated transfer.  These counts
    cannot be kept inside PFN database entries because there are no PFN entries
    for I/O space.
    
    These counts are kept because SANs will want to reuse the physical space
    but cannot do this unless it is guaranteed there are no more pending I/Os
    going from/to it.  If the process has not exited, but the SAN driver has
    unmapped the views to its I/O space, it still needs this as a way to know
    that the application does not have a transfer in progress to the previously
    mapped space.

Arguments:

    MemoryDescriptorList - Supplies a pointer to the memory descriptor list.

    Page - Supplies a pointer to the PFN just after the end of the MDL.

Return Value:

    TRUE if the reference counts were updated, FALSE if not.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    PMDL Tracker;
    KIRQL OldIrql;
    SIZE_T MdlSize;

    MdlSize = (PCHAR)Page - (PCHAR)MemoryDescriptorList;

    Tracker = ExAllocatePoolWithTag (NonPagedPool, MdlSize, 'tImM');

    if (Tracker == NULL) {
        return FALSE;
    }

    RtlCopyMemory ((PVOID) Tracker,
                   (PVOID) MemoryDescriptorList,
                   MdlSize);

    Tracker->MappedSystemVa = (PVOID) MemoryDescriptorList;

    //
    // Add this transfer to the list.
    //

    ExAcquireSpinLock (&MiTrackIoLock, &OldIrql);

    PushEntryList (&MmLockedIoPagesHead, (PSINGLE_LIST_ENTRY) Tracker);

    ExReleaseSpinLock (&MiTrackIoLock, OldIrql);

    return TRUE;
}

NTKERNELAPI
LOGICAL
MiDereferenceIoSpace (
    IN OUT PMDL MemoryDescriptorList
    )

/*++

Routine Description:

    This routine decrements the reference counts on physical pages which
    reside in I/O space that were previously probed on behalf of a
    user-initiated transfer.  These counts cannot be kept inside PFN
    database entries because there are no PFN entries for I/O space.
    
    These counts are kept because SANs will want to reuse the physical space
    but cannot do this unless it is guaranteed there are no more pending I/Os
    going from/to it.  If the process has not exited, but the SAN driver has
    unmapped the views to its I/O space, it still needs this as a way to know
    that the application does not have a transfer in progress to the previously
    mapped space.

Arguments:

    MemoryDescriptorList - Supplies a pointer to the memory descriptor list.

Return Value:

    TRUE if the reference counts were updated, FALSE if not.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    PMDL PrevEntry;
    PMDL NextEntry;

    PrevEntry = NULL;

    ExAcquireSpinLock (&MiTrackIoLock, &OldIrql);

    NextEntry = (PMDL) MmLockedIoPagesHead.Next;

    while (NextEntry != NULL) {

        if (NextEntry->MappedSystemVa == (PVOID) MemoryDescriptorList) {

            if (PrevEntry != NULL) {
                PrevEntry->Next = NextEntry->Next;
            }
            else {
                MmLockedIoPagesHead.Next = (PSINGLE_LIST_ENTRY) NextEntry->Next;
            }

            ExReleaseSpinLock (&MiTrackIoLock, OldIrql);

            ExFreePool (NextEntry);

            return TRUE;
        }

        PrevEntry = NextEntry;
        NextEntry = NextEntry->Next;
    }

    ExReleaseSpinLock (&MiTrackIoLock, OldIrql);

    return FALSE;
}

LOGICAL
MmIsIoSpaceActive (
    IN PHYSICAL_ADDRESS StartAddress,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    This routine returns TRUE if any portion of the requested range still has
    an outstanding pending I/O.  It is the calling driver's responsibility to
    unmap all usermode mappings to the specified range (so another transfer
    cannot be initiated) prior to calling this API.
    
    These counts are kept because SANs will want to reuse the physical space
    but cannot do this unless it is guaranteed there are no more pending I/Os
    going from/to it.  If the process has not exited, but the SAN driver has
    unmapped the views to its I/O space, it still needs this as a way to know
    that the application does not have a transfer in progress to the previously
    mapped space.

Arguments:

    StartAddress - Supplies the physical address of the start of the I/O range.
                   This MUST NOT be within system DRAM as those pages are not
                   tracked by this structure.

    NumberOfBytes - Supplies the number of bytes in the range.

Return Value:

    TRUE if any page in the range is currently locked for I/O, FALSE if not.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    PFN_NUMBER NumberOfPages;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    PVOID StartingVa;
    PMDL MemoryDescriptorList;
    PFN_NUMBER StartPage;
    PFN_NUMBER EndPage;
    PHYSICAL_ADDRESS EndAddress;

    ASSERT (NumberOfBytes != 0);
    StartPage = (PFN_NUMBER) (StartAddress.QuadPart >> PAGE_SHIFT);
    EndAddress.QuadPart = StartAddress.QuadPart + NumberOfBytes - 1;
    EndPage = (PFN_NUMBER) (EndAddress.QuadPart >> PAGE_SHIFT);

#if DBG
    do {
        ASSERT (!MI_IS_PFN (StartPage));
        StartPage += 1;
    } while (StartPage <= EndPage);
    StartPage = (PFN_NUMBER) (StartAddress.QuadPart >> PAGE_SHIFT);
#endif

    ExAcquireSpinLock (&MiTrackIoLock, &OldIrql);

    MemoryDescriptorList = (PMDL) MmLockedIoPagesHead.Next;

    while (MemoryDescriptorList != NULL) {

        StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

        NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                              MemoryDescriptorList->ByteCount);

        ASSERT (NumberOfPages != 0);

        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        LastPage = Page + NumberOfPages;

        do {

            if (*Page == MM_EMPTY_LIST) {

                //
                // There are no more locked pages.
                //

                break;
            }

            if ((*Page >= StartPage) && (*Page <= EndPage)) {
                ExReleaseSpinLock (&MiTrackIoLock, OldIrql);
                return TRUE;
            }

            Page += 1;

        } while (Page < LastPage);

        MemoryDescriptorList = MemoryDescriptorList->Next;
    }

    ExReleaseSpinLock (&MiTrackIoLock, OldIrql);

    return FALSE;
}


VOID
MmUnlockPages (
     IN OUT PMDL MemoryDescriptorList
     )

/*++

Routine Description:

    This routine unlocks physical pages which are described by a Memory
    Descriptor List.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a memory descriptor list
                           (MDL). The supplied MDL must have been supplied
                           to MmLockPages to lock the pages down.  As the
                           pages are unlocked, the MDL is updated.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    LONG EntryCount;
    LONG OriginalCount;
    PVOID OldValue;
    PEPROCESS Process;
    PFN_NUMBER NumberOfPages;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    PVOID StartingVa;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    CSHORT MdlFlags;
    PSLIST_ENTRY SingleListEntry;
    PMI_PFN_DEREFERENCE_CHUNK DerefMdl;
    PSLIST_HEADER PfnDereferenceSListHead;
    PSLIST_ENTRY *PfnDeferredList;

    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PAGES_LOCKED) != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_SOURCE_IS_NONPAGED_POOL) == 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PARTIAL) == 0);
    ASSERT (MemoryDescriptorList->ByteCount != 0);

    Process = MemoryDescriptorList->Process;

    //
    // Carefully snap a copy of the MDL flags - realize that bits in it may
    // change due to some of the subroutines called below.  Only bits that
    // we know can't change are examined in this local copy.  This is done
    // to reduce the amount of processing while the PFN lock is held.
    //

    MdlFlags = MemoryDescriptorList->MdlFlags;

    if (MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {

        //
        // This MDL has been mapped into system space, unmap now.
        //

        MmUnmapLockedPages (MemoryDescriptorList->MappedSystemVa,
                            MemoryDescriptorList);
    }

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                              MemoryDescriptorList->ByteCount);

    ASSERT (NumberOfPages != 0);

    if (MdlFlags & MDL_DESCRIBES_AWE) {

        ASSERT (Process != NULL);
        ASSERT (Process->AweInfo != NULL);

        LastPage = Page + NumberOfPages;

        //
        // Note neither AWE nor PFN locks are needed for unlocking these MDLs
        // in all but the very rare cases (see below).
        //

        do {

            if (*Page == MM_EMPTY_LIST) {

                //
                // There are no more locked pages - if there were none at all
                // then we're done.
                //

                break;
            }

            ASSERT (MI_IS_PFN (*Page));
            ASSERT (*Page <= MmHighestPhysicalPage);
            Pfn1 = MI_PFN_ELEMENT (*Page);

            do {
                EntryCount = Pfn1->AweReferenceCount;

                ASSERT ((LONG)EntryCount > 0);
                ASSERT (Pfn1->u4.AweAllocation == 1);
                ASSERT (Pfn1->u3.e2.ReferenceCount != 0);

                OriginalCount = InterlockedCompareExchange (&Pfn1->AweReferenceCount,
                                                            EntryCount - 1,
                                                            EntryCount);

                if (OriginalCount == EntryCount) {

                    //
                    // This thread can be racing against other threads also
                    // calling MmUnlockPages and also a thread calling
                    // NtFreeUserPhysicalPages.  All threads can safely do
                    // interlocked decrements on the "AWE reference count".
                    // Whichever thread drives it to zero is responsible for
                    // decrementing the actual PFN reference count (which may
                    // be greater than 1 due to other non-AWE API calls being
                    // used on the same page).  The thread that drives this
                    // reference count to zero must put the page on the actual
                    // freelist at that time and decrement various resident
                    // available and commitment counters also.
                    //

                    if (OriginalCount == 1) {

                        //
                        // This thread has driven the AWE reference count to
                        // zero so it must initiate a decrement of the PFN
                        // reference count (while holding the PFN lock), etc.
                        //
                        // This path should be rare since typically I/Os
                        // complete before these types of pages are freed by
                        // the app.
                        //

                        MiDecrementReferenceCountForAwePage (Pfn1, FALSE);
                    }

                    break;
                }
            } while (TRUE);

            Page += 1;
        } while (Page < LastPage);

        MemoryDescriptorList->MdlFlags &= ~(MDL_PAGES_LOCKED | MDL_DESCRIBES_AWE);

        return;
    }

    if ((MmTrackLockedPages == TRUE) && (Process != NULL)) {
        MiFreeMdlTracker (MemoryDescriptorList, NumberOfPages);
    }

    //
    // Only unlock if not I/O space.
    //

    if ((MdlFlags & MDL_IO_SPACE) == 0) {

        if (Process != NULL) {
            ASSERT ((SPFN_NUMBER)Process->NumberOfLockedPages >= 0);
            InterlockedExchangeAddSizeT (&Process->NumberOfLockedPages,
                                         0 - NumberOfPages);
        }

        LastPage = Page + NumberOfPages;

        //
        // Calculate PFN addresses and termination without the PFN lock
        // (it's not needed for this) to reduce PFN lock contention.
        //

        ASSERT (sizeof(PFN_NUMBER) == sizeof(PMMPFN));

        do {

            if (*Page == MM_EMPTY_LIST) {

                //
                // There are no more locked pages - if there were none at all
                // then we're done.
                //

                if (Page == (PPFN_NUMBER)(MemoryDescriptorList + 1)) {
                    MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;
                    return;
                }

                LastPage = Page;
                break;
            }
            ASSERT (MI_IS_PFN (*Page));
            ASSERT (*Page <= MmHighestPhysicalPage);

            Pfn1 = MI_PFN_ELEMENT (*Page);
            *Page = (PFN_NUMBER) Pfn1;
            Page += 1;
        } while (Page < LastPage);

        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        //
        // If the MDL can be queued so the PFN acquisition/release can be
        // amortized then do so.
        //

        if (NumberOfPages <= MI_MAX_DEREFERENCE_CHUNK) {

#if defined(MI_MULTINODE)

            PKNODE Node = KeGetCurrentNode ();

            //
            // The node may change beneath us but that should be fairly
            // infrequent and not worth checking for.  Just make sure the
            // same node that gives us a free entry gets the deferred entry
            // back.
            //

            PfnDereferenceSListHead = &Node->PfnDereferenceSListHead;
#else
            PfnDereferenceSListHead = &MmPfnDereferenceSListHead;
#endif

            //
            // Pop an entry from the freelist.
            //

            SingleListEntry = InterlockedPopEntrySList (PfnDereferenceSListHead);

            if (SingleListEntry != NULL) {
                DerefMdl = CONTAINING_RECORD (SingleListEntry,
                                              MI_PFN_DEREFERENCE_CHUNK,
                                              ListEntry);

                DerefMdl->Flags = MdlFlags;
                DerefMdl->NumberOfPages = (USHORT) (LastPage - Page);

#if defined (_WIN64)

                //
                // Avoid the majority of the high cost of RtlCopyMemory on
                // 64 bit platforms.
                //

                if (DerefMdl->NumberOfPages == 1) {
                    DerefMdl->Pfns[0] = *Page;
                }
                else if (DerefMdl->NumberOfPages == 2) {
                    DerefMdl->Pfns[0] = *Page;
                    DerefMdl->Pfns[1] = *(Page + 1);
                }
                else
#endif
                RtlCopyMemory ((PVOID)(&DerefMdl->Pfns[0]),
                               (PVOID)Page,
                               (LastPage - Page) * sizeof (PFN_NUMBER));

                MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;

                //
                // Push this entry on the deferred list.
                //

#if defined(MI_MULTINODE)
                PfnDeferredList = &Node->PfnDeferredList;
#else
                PfnDeferredList = &MmPfnDeferredList;
#endif

                do {

                    OldValue = *PfnDeferredList;
                    SingleListEntry->Next = OldValue;

                } while (InterlockedCompareExchangePointer (
                                PfnDeferredList,
                                SingleListEntry,
                                OldValue) != OldValue);
                return;
            }
        }

        SingleListEntry = NULL;

        if (MdlFlags & MDL_WRITE_OPERATION) {

            LOCK_PFN2 (OldIrql);

            do {

                //
                // If this was a write operation set the modified bit in the
                // PFN database.
                //

                Pfn1 = (PMMPFN) (*Page);

                MI_SET_MODIFIED (Pfn1, 1, 0x3);

                if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                             (Pfn1->u3.e1.WriteInProgress == 0)) {

                    ULONG FreeBit;
                    FreeBit = GET_PAGING_FILE_OFFSET (Pfn1->OriginalPte);

                    if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
                        MiReleaseConfirmedPageFileSpace (Pfn1->OriginalPte);
                        Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
                    }
                }

                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 1);

                Page += 1;
            } while (Page < LastPage);
        }
        else {

            LOCK_PFN2 (OldIrql);

            do {

                Pfn1 = (PMMPFN) (*Page);

                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 1);

                Page += 1;
            } while (Page < LastPage);
        }

        if (NumberOfPages <= MI_MAX_DEREFERENCE_CHUNK) {

            //
            // The only reason this code path is being reached is because
            // a deferred entry was not available so clear the list now.
            //

            MiDeferredUnlockPages (MI_DEFER_PFN_HELD | MI_DEFER_DRAIN_LOCAL_ONLY);
        }

        UNLOCK_PFN2 (OldIrql);
    }
    else {
        MiDereferenceIoSpace (MemoryDescriptorList);
    }

    MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;

    return;
}


VOID
MiDeferredUnlockPages (
     ULONG Flags
     )

/*++

Routine Description:

    This routine unlocks physical pages which were previously described by
    a Memory Descriptor List.

Arguments:

    Flags - Supplies a bitfield of the caller's needs :

        MI_DEFER_PFN_HELD - Indicates the caller holds the PFN lock on entry.

        MI_DEFER_DRAIN_LOCAL_ONLY - Indicates the caller only wishes to drain
                                    the current processor's queue.  This only
                                    has meaning in NUMA systems.

Return Value:

    None.

Environment:

    Kernel mode, PFN database lock *MAY* be held on entry (see Flags).

--*/

{
    KIRQL OldIrql = 0;
    ULONG FreeBit;
    ULONG i;
    ULONG ListCount;
    ULONG TotalNodes;
    PFN_NUMBER NumberOfPages;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastPage;
    PMMPFN Pfn1;
    CSHORT MdlFlags;
    PSLIST_ENTRY SingleListEntry;
    PSLIST_ENTRY LastEntry;
    PSLIST_ENTRY FirstEntry;
    PSLIST_ENTRY NextEntry;
    PSLIST_ENTRY VeryLastEntry;
    PMI_PFN_DEREFERENCE_CHUNK DerefMdl;
    PSLIST_HEADER PfnDereferenceSListHead;
    PSLIST_ENTRY *PfnDeferredList;
#if defined(MI_MULTINODE)
    PKNODE Node;
#endif

    i = 0;
    ListCount = 0;
    TotalNodes = 1;

    if ((Flags & MI_DEFER_PFN_HELD) == 0) {
        LOCK_PFN2 (OldIrql);
    }

    MM_PFN_LOCK_ASSERT();

#if defined(MI_MULTINODE)
    if (Flags & MI_DEFER_DRAIN_LOCAL_ONLY) {
        Node = KeGetCurrentNode();
        PfnDeferredList = &Node->PfnDeferredList;
        PfnDereferenceSListHead = &Node->PfnDereferenceSListHead;
    }
    else {
        TotalNodes = KeNumberNodes;
        Node = KeNodeBlock[0];
        PfnDeferredList = &Node->PfnDeferredList;
        PfnDereferenceSListHead = &Node->PfnDereferenceSListHead;
    }
#else
    PfnDeferredList = &MmPfnDeferredList;
    PfnDereferenceSListHead = &MmPfnDereferenceSListHead;
#endif

    do {

        if (*PfnDeferredList == NULL) {

#if !defined(MI_MULTINODE)
            if ((Flags & MI_DEFER_PFN_HELD) == 0) {
                UNLOCK_PFN2 (OldIrql);
            }
            return;
#else
            i += 1;
            if (i < TotalNodes) {
                Node = KeNodeBlock[i];
                PfnDeferredList = &Node->PfnDeferredList;
                PfnDereferenceSListHead = &Node->PfnDereferenceSListHead;
                continue;
            }
            break;
#endif
        }

        //
        // Process each deferred unlock entry until they're all done.
        //

        LastEntry = NULL;
        VeryLastEntry = NULL;

        do {

            SingleListEntry = *PfnDeferredList;

            FirstEntry = SingleListEntry;

            do {

                NextEntry = SingleListEntry->Next;

                //
                // Process the deferred entry.
                //

                DerefMdl = CONTAINING_RECORD (SingleListEntry,
                                              MI_PFN_DEREFERENCE_CHUNK,
                                              ListEntry);

                MdlFlags = DerefMdl->Flags;
                NumberOfPages = (PFN_NUMBER) DerefMdl->NumberOfPages;
                ASSERT (NumberOfPages <= MI_MAX_DEREFERENCE_CHUNK);
                Page = &DerefMdl->Pfns[0];
                LastPage = Page + NumberOfPages;

#if DBG
                //
                // Mark the entry as processed so if it mistakenly gets
                // reprocessed, we will assert above.
                //

                DerefMdl->NumberOfPages |= 0x80;
#endif
                if (MdlFlags & MDL_WRITE_OPERATION) {

                    do {

                        //
                        // If this was a write operation set the modified bit
                        // in the PFN database.
                        //

                        Pfn1 = (PMMPFN) (*Page);

                        MI_SET_MODIFIED (Pfn1, 1, 0x4);

                        if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                            (Pfn1->u3.e1.WriteInProgress == 0)) {

                            FreeBit = GET_PAGING_FILE_OFFSET (Pfn1->OriginalPte);

                            if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
                                MiReleaseConfirmedPageFileSpace (Pfn1->OriginalPte);
                                Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
                            }
                        }

                        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 1);

                        Page += 1;
                    } while (Page < LastPage);
                }
                else {

                    do {

                        Pfn1 = (PMMPFN) (*Page);

                        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 1);

                        Page += 1;
                    } while (Page < LastPage);
                }

                ListCount += 1;

                //
                // March on to the next entry if there is one.
                //

                if (NextEntry == LastEntry) {
                    break;
                }

                SingleListEntry = NextEntry;

            } while (TRUE);

            if (VeryLastEntry == NULL) {
                VeryLastEntry = SingleListEntry;
            }

            if ((*PfnDeferredList == FirstEntry) &&
                (InterlockedCompareExchangePointer (PfnDeferredList,
                                                    NULL,
                                                    FirstEntry) == FirstEntry)) {
                ASSERT (*PfnDeferredList != FirstEntry);
                break;
            }
            ASSERT (*PfnDeferredList != FirstEntry);
            LastEntry = FirstEntry;

        } while (TRUE);

        //
        // Push the processed list chain on the freelist.
        //

        ASSERT (ListCount != 0);
        ASSERT (FirstEntry != NULL);
        ASSERT (VeryLastEntry != NULL);

#if defined(MI_MULTINODE)
        InterlockedPushListSList (PfnDereferenceSListHead,
                                  FirstEntry,
                                  VeryLastEntry,
                                  ListCount);

        i += 1;
        if (i < TotalNodes) {
            Node = KeNodeBlock[i];
            PfnDeferredList = &Node->PfnDeferredList;
            PfnDereferenceSListHead = &Node->PfnDereferenceSListHead;
            ListCount = 0;
        }
        else {
            break;
        }
    } while (TRUE);
#else
    } while (FALSE);
#endif

    if ((Flags & MI_DEFER_PFN_HELD) == 0) {
        UNLOCK_PFN2 (OldIrql);
    }

#if !defined(MI_MULTINODE)

    //
    // If possible, push the processed chain after releasing the PFN lock.
    //

    InterlockedPushListSList (PfnDereferenceSListHead,
                              FirstEntry,
                              VeryLastEntry,
                              ListCount);
#endif
}

VOID
MmBuildMdlForNonPagedPool (
    IN OUT PMDL MemoryDescriptorList
    )

/*++

Routine Description:

    This routine fills in the "pages" portion of the MDL using the PFN
    numbers corresponding to the buffers which reside in non-paged pool.

    Unlike MmProbeAndLockPages, there is no corresponding unlock as no
    reference counts are incremented as the buffers being in nonpaged
    pool are always resident.

Arguments:

    MemoryDescriptorList - Supplies a pointer to a Memory Descriptor List
                            (MDL). The supplied MDL must supply a virtual
                            address, byte offset and length field.  The
                            physical page portion of the MDL is updated when
                            the pages are locked in memory.  The virtual
                            address must be within the non-paged portion
                            of the system space.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    PPFN_NUMBER Page;
    PPFN_NUMBER EndPage;
    PMMPTE PointerPte;
    PVOID VirtualAddress;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NumberOfPages;

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & (
                    MDL_PAGES_LOCKED |
                    MDL_MAPPED_TO_SYSTEM_VA |
                    MDL_SOURCE_IS_NONPAGED_POOL |
                    MDL_PARTIAL)) == 0);

    MemoryDescriptorList->Process = NULL;

    //
    // Endva is last byte of the buffer.
    //

    MemoryDescriptorList->MdlFlags |= MDL_SOURCE_IS_NONPAGED_POOL;

    ASSERT (MmIsNonPagedSystemAddressValid (MemoryDescriptorList->StartVa));

    VirtualAddress = MemoryDescriptorList->StartVa;

    MemoryDescriptorList->MappedSystemVa =
            (PVOID)((PCHAR)VirtualAddress + MemoryDescriptorList->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (MemoryDescriptorList->MappedSystemVa,
                                           MemoryDescriptorList->ByteCount);

    ASSERT (NumberOfPages != 0);

    EndPage = Page + NumberOfPages;

    if (MI_IS_PHYSICAL_ADDRESS(VirtualAddress)) {

        PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (VirtualAddress);

        do {
            *Page = PageFrameIndex;
            Page += 1;
            PageFrameIndex += 1;
        } while (Page < EndPage);
    }
    else {

        PointerPte = MiGetPteAddress (VirtualAddress);

        do {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            *Page = PageFrameIndex;
            Page += 1;
            PointerPte += 1;
        } while (Page < EndPage);
    }

    //
    // Assume either all the frames are in the PFN database (ie: the MDL maps
    // pool) or none of them (the MDL maps dualport RAM) are.
    //

    if (!MI_IS_PFN (PageFrameIndex)) {
        MemoryDescriptorList->MdlFlags |= MDL_IO_SPACE;
    }

    return;
}

VOID
MiInitializeIoTrackers (
    VOID
    )
{
    InitializeSListHead (&MiDeadPteTrackerSListHead);
    KeInitializeSpinLock (&MiPteTrackerLock);
    InitializeListHead (&MiPteHeader.ListHead);

    KeInitializeSpinLock (&MmIoTrackerLock);
    InitializeListHead (&MmIoHeader);

    KeInitializeSpinLock (&MiTrackIoLock);
}

VOID
MiInsertPteTracker (
    IN PMDL MemoryDescriptorList,
    IN ULONG Flags,
    IN LOGICAL IoMapping,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute,
    IN PVOID MyCaller,
    IN PVOID MyCallersCaller
    )

/*++

Routine Description:

    This function inserts a PTE tracking block as the caller has just
    consumed system PTEs.

Arguments:

    MemoryDescriptorList - Supplies a valid Memory Descriptor List.

    Flags - Supplies the following values:

            0 - Indicates all the fields of the MDL are legitimate and can
                be snapped.

            1 - Indicates the caller is mapping physically contiguous memory.
                The only valid MDL fields are Page[0] & ByteCount.
                Page[0] contains the PFN start, ByteCount the byte count.

            2 - Indicates the caller is just reserving mapping PTEs.
                The only valid MDL fields are Page[0] & ByteCount.
                Page[0] contains the pool tag, ByteCount the byte count.

    MyCaller - Supplies the return address of the caller who consumed the
               system PTEs to map this MDL.

    MyCallersCaller - Supplies the return address of the caller of the caller
                      who consumed the system PTEs to map this MDL.

Return Value:

    None.

Environment:

    Kernel mode, DISPATCH_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    PVOID StartingVa;
    PPTE_TRACKER Tracker;
    PSLIST_ENTRY SingleListEntry;
    PSLIST_ENTRY NextSingleListEntry;
    PFN_NUMBER NumberOfPtes;

    ASSERT (KeGetCurrentIrql() <= DISPATCH_LEVEL);

    if (ExQueryDepthSList (&MiDeadPteTrackerSListHead) < 10) {
        Tracker = (PPTE_TRACKER) InterlockedPopEntrySList (&MiDeadPteTrackerSListHead);
    }
    else {
        SingleListEntry = ExInterlockedFlushSList (&MiDeadPteTrackerSListHead);

        Tracker = (PPTE_TRACKER) SingleListEntry;

        if (SingleListEntry != NULL) {

            SingleListEntry = SingleListEntry->Next;

            while (SingleListEntry != NULL) {

                NextSingleListEntry = SingleListEntry->Next;

                ExFreePool (SingleListEntry);

                SingleListEntry = NextSingleListEntry;
            }
        }
    }

    if (Tracker == NULL) {

        Tracker = ExAllocatePoolWithTag (NonPagedPool,
                                         sizeof (PTE_TRACKER),
                                         'ySmM');

        if (Tracker == NULL) {
            MiTrackPtesAborted = TRUE;
            return;
        }
    }

    switch (Flags) {

        case 0:

            //
            // Regular MDL mapping.
            //

            StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                            MemoryDescriptorList->ByteOffset);

            NumberOfPtes = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                                   MemoryDescriptorList->ByteCount);
            Tracker->Mdl = MemoryDescriptorList;

            Tracker->StartVa = MemoryDescriptorList->StartVa;
            Tracker->Offset = MemoryDescriptorList->ByteOffset;
            Tracker->Length = MemoryDescriptorList->ByteCount;

            break;

        case 1:

            //
            // MmMapIoSpace call (ie: physically contiguous mapping).
            //

            StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                            MemoryDescriptorList->ByteOffset);

            NumberOfPtes = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                                   MemoryDescriptorList->ByteCount);
            Tracker->Mdl = (PVOID)1;
            break;

        default:
            ASSERT (FALSE);
            // Fall through

        case 2:

            //
            // MmAllocateReservedMapping call (ie: currently maps nothing).
            //

            NumberOfPtes = (MemoryDescriptorList->ByteCount >> PAGE_SHIFT);
            Tracker->Mdl = NULL;
            break;
    }

    Tracker->Count = NumberOfPtes;

    Tracker->CallingAddress = MyCaller;
    Tracker->CallersCaller = MyCallersCaller;

    Tracker->SystemVa = MemoryDescriptorList->MappedSystemVa;
    Tracker->Page = *(PPFN_NUMBER)(MemoryDescriptorList + 1);

    Tracker->CacheAttribute = CacheAttribute;
    Tracker->IoMapping = (BOOLEAN) IoMapping;

    ExAcquireSpinLock (&MiPteTrackerLock, &OldIrql);

    InsertHeadList (&MiPteHeader.ListHead, &Tracker->ListEntry);

    MiPteHeader.Count += NumberOfPtes;
    MiPteHeader.NumberOfEntries += 1;

    if (MiPteHeader.NumberOfEntries > MiPteHeader.NumberOfEntriesPeak) {
        MiPteHeader.NumberOfEntriesPeak = MiPteHeader.NumberOfEntries;
    }

    ExReleaseSpinLock (&MiPteTrackerLock, OldIrql);
}

VOID
MiRemovePteTracker (
    IN PMDL MemoryDescriptorList OPTIONAL,
    IN PVOID VirtualAddress,
    IN PFN_NUMBER NumberOfPtes
    )

/*++

Routine Description:

    This function removes a PTE tracking block from the lists as the PTEs
    are being freed.

Arguments:

    MemoryDescriptorList - Supplies a valid Memory Descriptor List.

    PteAddress - Supplies the address the system PTEs were mapped to.

    NumberOfPtes - Supplies the number of system PTEs allocated.

Return Value:

    None.

Environment:

    Kernel mode, DISPATCH_LEVEL or below. Locks (including the PFN) may be held.

--*/

{
    KIRQL OldIrql;
    PPTE_TRACKER Tracker;
    PLIST_ENTRY LastFound;
    PLIST_ENTRY NextEntry;

    LastFound = NULL;

    VirtualAddress = PAGE_ALIGN (VirtualAddress);

    ExAcquireSpinLock (&MiPteTrackerLock, &OldIrql);

    NextEntry = MiPteHeader.ListHead.Flink;
    while (NextEntry != &MiPteHeader.ListHead) {

        Tracker = (PPTE_TRACKER) CONTAINING_RECORD (NextEntry,
                                                    PTE_TRACKER,
                                                    ListEntry.Flink);

        if (VirtualAddress == PAGE_ALIGN (Tracker->SystemVa)) {

            if (LastFound != NULL) {

                //
                // Duplicate map entry.
                //

                KeBugCheckEx (SYSTEM_PTE_MISUSE,
                              0x1,
                              (ULONG_PTR)Tracker,
                              (ULONG_PTR)MemoryDescriptorList,
                              (ULONG_PTR)LastFound);
            }

            if (Tracker->Count != NumberOfPtes) {

                //
                // Not unmapping the same of number of PTEs that were mapped.
                //

                KeBugCheckEx (SYSTEM_PTE_MISUSE,
                              0x2,
                              (ULONG_PTR)Tracker,
                              Tracker->Count,
                              NumberOfPtes);
            }

            if ((ARGUMENT_PRESENT (MemoryDescriptorList)) &&
                ((MemoryDescriptorList->MdlFlags & MDL_FREE_EXTRA_PTES) == 0) &&
                (MiMdlsAdjusted == FALSE)) {

                if (Tracker->SystemVa != MemoryDescriptorList->MappedSystemVa) {

                    //
                    // Not unmapping the same address that was mapped.
                    //

                    KeBugCheckEx (SYSTEM_PTE_MISUSE,
                                  0x3,
                                  (ULONG_PTR)Tracker,
                                  (ULONG_PTR)Tracker->SystemVa,
                                  (ULONG_PTR)MemoryDescriptorList->MappedSystemVa);
                }

                if (Tracker->Page != *(PPFN_NUMBER)(MemoryDescriptorList + 1)) {

                    //
                    // The first page in the MDL has changed since it was mapped.
                    //

                    KeBugCheckEx (SYSTEM_PTE_MISUSE,
                                  0x4,
                                  (ULONG_PTR)Tracker,
                                  (ULONG_PTR)Tracker->Page,
                                  (ULONG_PTR) *(PPFN_NUMBER)(MemoryDescriptorList + 1));
                }

                if (Tracker->StartVa != MemoryDescriptorList->StartVa) {

                    //
                    // Map and unmap don't match up.
                    //

                    KeBugCheckEx (SYSTEM_PTE_MISUSE,
                                  0x5,
                                  (ULONG_PTR)Tracker,
                                  (ULONG_PTR)Tracker->StartVa,
                                  (ULONG_PTR)MemoryDescriptorList->StartVa);
                }
            }

            RemoveEntryList (NextEntry);
            LastFound = NextEntry;
        }
        NextEntry = Tracker->ListEntry.Flink;
    }

    if ((LastFound == NULL) && (MiTrackPtesAborted == FALSE)) {

        //
        // Can't unmap something that was never (or isn't currently) mapped.
        //

        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x6,
                      (ULONG_PTR)MemoryDescriptorList,
                      (ULONG_PTR)VirtualAddress,
                      (ULONG_PTR)NumberOfPtes);
    }

    MiPteHeader.Count -= NumberOfPtes;
    MiPteHeader.NumberOfEntries -= 1;

    ExReleaseSpinLock (&MiPteTrackerLock, OldIrql);

    //
    // Insert the tracking block into the dead PTE list for later
    // release.  Locks (including the PFN lock) may be held on entry, thus the
    // block cannot be directly freed to pool at this time.
    //

    if (LastFound != NULL) {
        InterlockedPushEntrySList (&MiDeadPteTrackerSListHead,
                                   (PSLIST_ENTRY)LastFound);
    }

    return;
}

PVOID
MiGetHighestPteConsumer (
    OUT PULONG_PTR NumberOfPtes
    )

/*++

Routine Description:

    This function examines the PTE tracking blocks and returns the biggest
    consumer.

Arguments:

    None.

Return Value:

    The loaded module entry of the biggest consumer.

Environment:

    Kernel mode, called during bugcheck only.  Many locks may be held.

--*/

{
    PPTE_TRACKER Tracker;
    PVOID BaseAddress;
    PFN_NUMBER NumberOfPages;
    PLIST_ENTRY NextEntry;
    PLIST_ENTRY NextEntry2;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    ULONG_PTR Highest;
    ULONG_PTR PagesByThisModule;
    PKLDR_DATA_TABLE_ENTRY HighDataTableEntry;

    *NumberOfPtes = 0;

    //
    // No locks are acquired as this is only called during a bugcheck.
    //

    if ((MmTrackPtes & 0x1) == 0) {
        return NULL;
    }

    if (MiTrackPtesAborted == TRUE) {
        return NULL;
    }

    if (IsListEmpty(&MiPteHeader.ListHead)) {
        return NULL;
    }

    if (PsLoadedModuleList.Flink == NULL) {
        return NULL;
    }

    Highest = 0;
    HighDataTableEntry = NULL;

    NextEntry = PsLoadedModuleList.Flink;
    while (NextEntry != &PsLoadedModuleList) {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        PagesByThisModule = 0;

        //
        // Walk the PTE mapping list and update each driver's counts.
        //
    
        NextEntry2 = MiPteHeader.ListHead.Flink;
        while (NextEntry2 != &MiPteHeader.ListHead) {
    
            Tracker = (PPTE_TRACKER) CONTAINING_RECORD (NextEntry2,
                                                        PTE_TRACKER,
                                                        ListEntry.Flink);
    
            BaseAddress = Tracker->CallingAddress;
            NumberOfPages = Tracker->Count;
    
            if ((BaseAddress >= DataTableEntry->DllBase) &&
                (BaseAddress < (PVOID)((ULONG_PTR)(DataTableEntry->DllBase) + DataTableEntry->SizeOfImage))) {

                PagesByThisModule += NumberOfPages;
            }
        
            NextEntry2 = NextEntry2->Flink;
    
        }
    
        if (PagesByThisModule > Highest) {
            Highest = PagesByThisModule;
            HighDataTableEntry = DataTableEntry;
        }

        NextEntry = NextEntry->Flink;
    }

    *NumberOfPtes = Highest;

    return (PVOID)HighDataTableEntry;
}

MI_PFN_CACHE_ATTRIBUTE
MiInsertIoSpaceMap (
    IN PVOID BaseVa,
    IN PFN_NUMBER PageFrameIndex,
    IN PFN_NUMBER NumberOfPages,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute
    )

/*++

Routine Description:

    This function inserts an I/O space tracking block, returning the cache
    type the caller should use.  The cache type is different from the input
    cache type if an overlap collision is detected.

Arguments:

    BaseVa - Supplies the virtual address that will be used for the mapping.

    PageFrameIndex - Supplies the starting physical page number that will be
                     mapped.

    NumberOfPages - Supplies the number of pages to map.

    CacheAttribute - Supplies the caller's desired cache attribute.

Return Value:

    The cache attribute that is safe to use.

Environment:

    Kernel mode, DISPATCH_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    PMMIO_TRACKER Tracker;
    PMMIO_TRACKER Tracker2;
    PLIST_ENTRY NextEntry;
    ULONG Hash;

    ASSERT (KeGetCurrentIrql() <= DISPATCH_LEVEL);

    Tracker = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof (MMIO_TRACKER),
                                     'ySmM');

    if (Tracker == NULL) {
        return MiNotMapped;
    }

    Tracker->BaseVa = BaseVa;
    Tracker->PageFrameIndex = PageFrameIndex;
    Tracker->NumberOfPages = NumberOfPages;
    Tracker->CacheAttribute = CacheAttribute;

    RtlZeroMemory (&Tracker->StackTrace[0], MI_IO_BACKTRACE_LENGTH * sizeof(PVOID)); 

    RtlCaptureStackBackTrace (2, MI_IO_BACKTRACE_LENGTH, Tracker->StackTrace, &Hash);

    ASSERT (!MI_IS_PFN (PageFrameIndex));

    ExAcquireSpinLock (&MmIoTrackerLock, &OldIrql);

    //
    // Scan I/O space mappings for duplicate or overlapping entries.
    //

    NextEntry = MmIoHeader.Flink;
    while (NextEntry != &MmIoHeader) {

        Tracker2 = (PMMIO_TRACKER) CONTAINING_RECORD (NextEntry,
                                                      MMIO_TRACKER,
                                                      ListEntry.Flink);

        if ((Tracker->PageFrameIndex < Tracker2->PageFrameIndex + Tracker2->NumberOfPages) &&
            (Tracker->PageFrameIndex + Tracker->NumberOfPages > Tracker2->PageFrameIndex)) {

#if DBG
            if ((MmShowMapOverlaps & 0x1) ||

                ((Tracker->CacheAttribute != Tracker2->CacheAttribute) &&
                (MmShowMapOverlaps & 0x2))) {

                DbgPrint ("MM: Iospace mapping overlap %p %p\n",
                                Tracker,
                                Tracker2);

                DbgPrint ("Physical range 0x%p->%p first mapped %s at VA %p\n",
                                Tracker2->PageFrameIndex << PAGE_SHIFT,
                                (Tracker2->PageFrameIndex + Tracker2->NumberOfPages) << PAGE_SHIFT,
                                MiCacheStrings[Tracker2->CacheAttribute],
                                Tracker2->BaseVa);
                DbgPrint ("\tCall stack: %p %p %p %p %p %p\n",
                                Tracker2->StackTrace[0],
                                Tracker2->StackTrace[1],
                                Tracker2->StackTrace[2],
                                Tracker2->StackTrace[3],
                                Tracker2->StackTrace[4],
                                Tracker2->StackTrace[5]);

                DbgPrint ("Physical range 0x%p->%p now being mapped %s at VA %p\n",
                                Tracker->PageFrameIndex << PAGE_SHIFT,
                                (Tracker->PageFrameIndex + Tracker->NumberOfPages) << PAGE_SHIFT,
                                MiCacheStrings[Tracker->CacheAttribute],
                                Tracker->BaseVa);
                DbgPrint ("\tCall stack: %p %p %p %p %p %p\n",
                                Tracker->StackTrace[0],
                                Tracker->StackTrace[1],
                                Tracker->StackTrace[2],
                                Tracker->StackTrace[3],
                                Tracker->StackTrace[4],
                                Tracker->StackTrace[5]);

                if (MmShowMapOverlaps & 0x80000000) {
                    DbgBreakPoint ();
                }
            }
#endif

            if (Tracker->CacheAttribute != Tracker2->CacheAttribute) {
                MiCacheOverride[3] += 1;

                Tracker->CacheAttribute = Tracker2->CacheAttribute;
            }

            //
            // Don't bother checking for overlapping multiple entries.
            // This would be a very strange driver bug and is already
            // caught by the verifier anyway.
            //
        }

        NextEntry = Tracker2->ListEntry.Flink;
    }

    InsertHeadList (&MmIoHeader, &Tracker->ListEntry);

#if DBG
    MmIoHeaderCount += NumberOfPages;
    MmIoHeaderNumberOfEntries += 1;

    if (MmIoHeaderNumberOfEntries > MmIoHeaderNumberOfEntriesPeak) {
        MmIoHeaderNumberOfEntriesPeak = MmIoHeaderNumberOfEntries;
    }
#endif

    ExReleaseSpinLock (&MmIoTrackerLock, OldIrql);

    return Tracker->CacheAttribute;
}

VOID
MiRemoveIoSpaceMap (
    IN PVOID BaseVa,
    IN PFN_NUMBER NumberOfPages
    )

/*++

Routine Description:

    This function removes an I/O space tracking block from the lists.

Arguments:

    BaseVa - Supplies the virtual address that will be used for the unmapping.

    NumberOfPages - Supplies the number of pages to unmap.

Return Value:

    None.

Environment:

    Kernel mode, DISPATCH_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    PMMIO_TRACKER Tracker;
    PLIST_ENTRY NextEntry;
    PVOID AlignedVa;

    AlignedVa = PAGE_ALIGN (BaseVa);

    ExAcquireSpinLock (&MmIoTrackerLock, &OldIrql);

    NextEntry = MmIoHeader.Flink;
    while (NextEntry != &MmIoHeader) {

        Tracker = (PMMIO_TRACKER) CONTAINING_RECORD (NextEntry,
                                                     MMIO_TRACKER,
                                                     ListEntry.Flink);

        if ((PAGE_ALIGN (Tracker->BaseVa) == AlignedVa) &&
            (Tracker->NumberOfPages == NumberOfPages)) {

            RemoveEntryList (NextEntry);

#if DBG
            MmIoHeaderCount -= NumberOfPages;
            MmIoHeaderNumberOfEntries -= 1;
#endif

            ExReleaseSpinLock (&MmIoTrackerLock, OldIrql);

            ExFreePool (Tracker);

            return;
        }
        NextEntry = Tracker->ListEntry.Flink;
    }

    //
    // Can't unmap something that was never (or isn't currently) mapped.
    //

    KeBugCheckEx (SYSTEM_PTE_MISUSE,
                  0x400,
                  (ULONG_PTR)BaseVa,
                  (ULONG_PTR)NumberOfPages,
                  0);
}

PVOID
MiMapSinglePage (
     IN PVOID VirtualAddress OPTIONAL,
     IN PFN_NUMBER PageFrameIndex,
     IN MEMORY_CACHING_TYPE CacheType,
     IN MM_PAGE_PRIORITY Priority
     )

/*++

Routine Description:

    This function (re)maps a single system PTE to the specified physical page.

Arguments:

    VirtualAddress - Supplies the virtual address to map the page frame at.
                     NULL indicates a system PTE is needed.  Non-NULL supplies
                     the virtual address returned by an earlier
                     MiMapSinglePage call.

    PageFrameIndex - Supplies the page frame index to map.

    CacheType - Supplies the type of cache mapping to use for the MDL.
                MmCached indicates "normal" kernel or user mappings.

    Priority - Supplies an indication as to how important it is that this
               request succeed under low available PTE conditions.

Return Value:

    Returns the base address where the page is mapped, or NULL if the
    mapping failed.

Environment:

    Kernel mode.  APC_LEVEL or below.

--*/

{
    PMMPTE PointerPte;
    MMPTE TempPte;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    PAGED_CODE ();

    UNREFERENCED_PARAMETER (Priority);

    //
    // If this routine is ever changed to allow other than fully cachable
    // requests then checks must be added for large page TB overlaps which
    // can result in this function failing where it cannot today.
    //

    ASSERT (CacheType == MmCached);

    if (VirtualAddress == NULL) {

        PointerPte = MiReserveSystemPtes (1, SystemPteSpace);

        if (PointerPte == NULL) {
    
            //
            // Not enough system PTES are available.
            //
    
            return NULL;
        }

        ASSERT (PointerPte->u.Hard.Valid == 0);
        VirtualAddress = MiGetVirtualAddressMappedByPte (PointerPte);
    }
    else {
        ASSERT (MI_IS_PHYSICAL_ADDRESS (VirtualAddress) == 0);
        ASSERT (VirtualAddress >= MM_SYSTEM_RANGE_START);

        PointerPte = MiGetPteAddress (VirtualAddress);
        ASSERT (PointerPte->u.Hard.Valid == 1);

        MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

        KeFlushSingleTb (VirtualAddress, TRUE);
    }

    TempPte = ValidKernelPte;

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, 0);

    switch (CacheAttribute) {

        case MiNonCached:
            MI_DISABLE_CACHING (TempPte);
            break;

        case MiCached:
            break;

        case MiWriteCombined:
            MI_SET_PTE_WRITE_COMBINE (TempPte);
            break;

        default:
            ASSERT (FALSE);
            break;
    }

    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

    MI_PREPARE_FOR_NONCACHED (CacheAttribute);

    MI_WRITE_VALID_PTE (PointerPte, TempPte);

    MI_SWEEP_CACHE (CacheAttribute, VirtualAddress, PAGE_SIZE);

    return VirtualAddress;
}

PVOID
MmMapLockedPages (
     IN PMDL MemoryDescriptorList,
     IN KPROCESSOR_MODE AccessMode
     )

/*++

Routine Description:

    This function maps physical pages described by a memory descriptor
    list into the system virtual address space or the user portion of
    the virtual address space.

Arguments:

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                            been updated by MmProbeAndLockPages.


    AccessMode - Supplies an indicator of where to map the pages;
                 KernelMode indicates that the pages should be mapped in the
                 system part of the address space, UserMode indicates the
                 pages should be mapped in the user part of the address space.

Return Value:

    Returns the base address where the pages are mapped.  The base address
    has the same offset as the virtual address in the MDL.

    This routine will raise an exception if the processor mode is USER_MODE
    and quota limits or VM limits are exceeded.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below if access mode is KernelMode,
                  APC_LEVEL or below if access mode is UserMode.

--*/

{
    return MmMapLockedPagesSpecifyCache (MemoryDescriptorList,
                                         AccessMode,
                                         MmCached,
                                         NULL,
                                         TRUE,
                                         HighPagePriority);
}

VOID
MiUnmapSinglePage (
     IN PVOID VirtualAddress
     )

/*++

Routine Description:

    This routine unmaps a single locked page which was previously mapped via
    an MiMapSinglePage call.

Arguments:

    VirtualAddress - Supplies the virtual address used to map the page.

Return Value:

    None.

Environment:

    Kernel mode.  APC_LEVEL or below, base address is within system space.

--*/

{
    PMMPTE PointerPte;

    PAGED_CODE ();

    ASSERT (MI_IS_PHYSICAL_ADDRESS (VirtualAddress) == 0);
    ASSERT (VirtualAddress >= MM_SYSTEM_RANGE_START);

    PointerPte = MiGetPteAddress (VirtualAddress);

    MiReleaseSystemPtes (PointerPte, 1, SystemPteSpace);
    return;
}

PVOID
MmAllocateMappingAddress (
     IN SIZE_T NumberOfBytes,
     IN ULONG PoolTag
     )

/*++

Routine Description:

    This function allocates a system PTE mapping of the requested length
    that can be used later to map arbitrary addresses.

Arguments:

    NumberOfBytes - Supplies the maximum number of bytes the mapping can span.

    PoolTag - Supplies a pool tag to associate this mapping to the caller.

Return Value:

    Returns a virtual address where to use for later mappings.

Environment:

    Kernel mode.  PASSIVE_LEVEL.

--*/

{
    PPFN_NUMBER Page;
    PMMPTE PointerPte;
    PVOID BaseVa;
    PVOID CallingAddress;
    PVOID CallersCaller;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + 1];
    PMDL MemoryDescriptorList;
    PFN_NUMBER NumberOfPages;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);

    //
    // Make sure there are enough PTEs of the requested size.
    // Try to ensure available PTEs inline when we're rich.
    // Otherwise go the long way.
    //

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (0, NumberOfBytes);

    if (NumberOfPages == 0) {

        RtlGetCallersAddress (&CallingAddress, &CallersCaller);

        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x100,
                      NumberOfPages,
                      PoolTag,
                      (ULONG_PTR) CallingAddress);
    }

    //
    // Callers must identify themselves.
    //

    if (PoolTag == 0) {
        return NULL;
    }

    //
    // Leave space to stash the length and tag.
    //

    NumberOfPages += 2;

    PointerPte = MiReserveSystemPtes ((ULONG)NumberOfPages, SystemPteSpace);

    if (PointerPte == NULL) {

        //
        // Not enough system PTES are available.
        //

        return NULL;
    }

    //
    // Make sure the valid bit is always zero in the stash PTEs.
    //

    *(PULONG_PTR)PointerPte = (NumberOfPages << 1);
    PointerPte += 1;

    *(PULONG_PTR)PointerPte = (PoolTag & ~0x1);
    PointerPte += 1;

    BaseVa = MiGetVirtualAddressMappedByPte (PointerPte);

    if (MmTrackPtes & 0x1) {

        RtlGetCallersAddress (&CallingAddress, &CallersCaller);

        MemoryDescriptorList = (PMDL) MdlHack;

        MemoryDescriptorList->MappedSystemVa = BaseVa;
        MemoryDescriptorList->StartVa = (PVOID)(ULONG_PTR)PoolTag;
        MemoryDescriptorList->ByteOffset = 0;
        MemoryDescriptorList->ByteCount = (ULONG)((NumberOfPages - 2) * PAGE_SIZE);

        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
        *Page = 0;

        MiInsertPteTracker (MemoryDescriptorList,
                            2,
                            FALSE,
                            MiCached,
                            CallingAddress,
                            CallersCaller);
    }

    return BaseVa;
}

VOID
MmFreeMappingAddress (
     IN PVOID BaseAddress,
     IN ULONG PoolTag
     )

/*++

Routine Description:

    This routine unmaps a virtual address range previously reserved with
    MmAllocateMappingAddress.

Arguments:

    BaseAddress - Supplies the base address previously reserved.

    PoolTag - Supplies the caller's identifying tag.

Return Value:

    None.

Environment:

    Kernel mode.  PASSIVE_LEVEL.

--*/

{
    ULONG OriginalPoolTag;
    PFN_NUMBER NumberOfPages;
    PMMPTE PointerBase;
    PMMPTE PointerPte;
    PMMPTE LastPte;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);
    ASSERT (!MI_IS_PHYSICAL_ADDRESS (BaseAddress));
    ASSERT (BaseAddress > MM_HIGHEST_USER_ADDRESS);

    PointerPte = MiGetPteAddress (BaseAddress);
    PointerBase = PointerPte - 2;

    OriginalPoolTag = *(PULONG) (PointerPte - 1);
    ASSERT ((OriginalPoolTag & 0x1) == 0);

    if (OriginalPoolTag != (PoolTag & ~0x1)) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x101,
                      (ULONG_PTR)BaseAddress,
                      PoolTag,
                      OriginalPoolTag);
    }

    NumberOfPages = *(PULONG_PTR)PointerBase;
    ASSERT ((NumberOfPages & 0x1) == 0);
    NumberOfPages >>= 1;

    if (NumberOfPages <= 2) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x102,
                      (ULONG_PTR)BaseAddress,
                      PoolTag,
                      NumberOfPages);
    }

    NumberOfPages -= 2;
    LastPte = PointerPte + NumberOfPages;

    while (PointerPte < LastPte) {
        if (PointerPte->u.Long != 0) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x103,
                          (ULONG_PTR)BaseAddress,
                          PoolTag,
                          NumberOfPages);
        }
        PointerPte += 1;
    }

    if (MmTrackPtes & 0x1) {
        MiRemovePteTracker (NULL, BaseAddress, NumberOfPages);
    }

    //
    // Note the tag and size are nulled out when the PTEs are released below
    // so any drivers that try to use their mapping after freeing it get
    // caught immediately.
    //

    MiReleaseSystemPtes (PointerBase, (ULONG)NumberOfPages + 2, SystemPteSpace);
    return;
}

PVOID
MmMapLockedPagesWithReservedMapping (
    IN PVOID MappingAddress,
    IN ULONG PoolTag,
    IN PMDL MemoryDescriptorList,
    IN MEMORY_CACHING_TYPE CacheType
    )

/*++

Routine Description:

    This function maps physical pages described by a memory descriptor
    list into the system virtual address space.

Arguments:

    MappingAddress - Supplies a valid mapping address obtained earlier via
                     MmAllocateMappingAddress.

    PoolTag - Supplies the caller's identifying tag.

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

    CacheType - Supplies the type of cache mapping to use for the MDL.
                MmCached indicates "normal" kernel or user mappings.

Return Value:

    Returns the base address where the pages are mapped.  The base address
    has the same offset as the virtual address in the MDL.

    This routine will return NULL if the cache type requested is incompatible
    with the pages being mapped or if the caller tries to map an MDL that is
    larger than the virtual address range originally reserved.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below.  The caller must synchronize usage
    of the argument virtual address space.

--*/

{
    KIRQL OldIrql;
    CSHORT IoMapping;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER VaPageSpan;
    PFN_NUMBER SavedPageCount;
    PPFN_NUMBER Page;
    PMMPTE PointerBase;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    MMPTE TempPte;
    PVOID StartingVa;
    PFN_NUMBER NumberOfPtes;
    PFN_NUMBER PageFrameIndex;
    ULONG OriginalPoolTag;
    PMMPFN Pfn2;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    ASSERT (KeGetCurrentIrql () <= DISPATCH_LEVEL);

    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0);

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                           MemoryDescriptorList->ByteCount);

    PointerPte = MiGetPteAddress (MappingAddress);
    PointerBase = PointerPte - 2;

    OriginalPoolTag = *(PULONG) (PointerPte - 1);
    ASSERT ((OriginalPoolTag & 0x1) == 0);

    if (OriginalPoolTag != (PoolTag & ~0x1)) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x104,
                      (ULONG_PTR)MappingAddress,
                      PoolTag,
                      OriginalPoolTag);
    }

    VaPageSpan = *(PULONG_PTR)PointerBase;
    ASSERT ((VaPageSpan & 0x1) == 0);
    VaPageSpan >>= 1;

    if (VaPageSpan <= 2) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x105,
                      (ULONG_PTR)MappingAddress,
                      PoolTag,
                      VaPageSpan);
    }

    if (NumberOfPages > VaPageSpan - 2) {

        //
        // The caller is trying to map an MDL that spans a range larger than
        // the reserving mapping !  This is a driver bug.
        //

        ASSERT (FALSE);
        return NULL;
    }

    //
    // All the mapping PTEs must be zero.
    //

    LastPte = PointerPte + VaPageSpan - 2;

    while (PointerPte < LastPte) {

        if (PointerPte->u.Long != 0) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x107,
                          (ULONG_PTR)MappingAddress,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)LastPte);
        }

        PointerPte += 1;
    }

    PointerPte = PointerBase + 2;
    SavedPageCount = NumberOfPages;

    ASSERT ((MemoryDescriptorList->MdlFlags & (
                        MDL_MAPPED_TO_SYSTEM_VA |
                        MDL_SOURCE_IS_NONPAGED_POOL |
                        MDL_PARTIAL_HAS_BEEN_MAPPED)) == 0);

    ASSERT ((MemoryDescriptorList->MdlFlags & (
                        MDL_PAGES_LOCKED |
                        MDL_PARTIAL)) != 0);

    //
    // If a noncachable mapping is requested, none of the pages in the
    // requested MDL can reside in a large page.  Otherwise we would be
    // creating an incoherent overlapping TB entry as the same physical
    // page would be mapped by 2 different TB entries with different
    // cache attributes.
    //

    IoMapping = MemoryDescriptorList->MdlFlags & MDL_IO_SPACE;

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, IoMapping);

    if (CacheAttribute != MiCached) {

        LOCK_PFN2 (OldIrql);

        do {

            if (*Page == MM_EMPTY_LIST) {
                break;
            }

            PageFrameIndex = *Page;

            if (MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (PageFrameIndex)) {
                UNLOCK_PFN2 (OldIrql);
                MiNonCachedCollisions += 1;
                return NULL;
            }

            Page += 1;
            NumberOfPages -= 1;
        } while (NumberOfPages != 0);

        UNLOCK_PFN2 (OldIrql);

        NumberOfPages = SavedPageCount;
        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

        MI_PREPARE_FOR_NONCACHED (CacheAttribute);
    }

    NumberOfPtes = NumberOfPages;

    TempPte = ValidKernelPte;

    MI_ADD_EXECUTE_TO_VALID_PTE_IF_PAE (TempPte);

    switch (CacheAttribute) {

        case MiNonCached:
            MI_DISABLE_CACHING (TempPte);
            break;

        case MiCached:
            break;

        case MiWriteCombined:
            MI_SET_PTE_WRITE_COMBINE (TempPte);
            break;

        default:
            ASSERT (FALSE);
            break;
    }

    OldIrql = HIGH_LEVEL;

    do {

        if (*Page == MM_EMPTY_LIST) {
            break;
        }

        ASSERT (PointerPte->u.Hard.Valid == 0);

        if (IoMapping == 0) {

            Pfn2 = MI_PFN_ELEMENT (*Page);
            ASSERT (Pfn2->u3.e2.ReferenceCount != 0);
            TempPte = ValidKernelPte;

            switch (Pfn2->u3.e1.CacheAttribute) {

                case MiCached:
                    if (CacheAttribute != MiCached) {

                        //
                        // The caller asked for a noncached or writecombined
                        // mapping, but the page is already mapped cached by
                        // someone else.  Override the caller's request in
                        // order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[0] += 1;
                    }
                    break;

                case MiNonCached:
                    if (CacheAttribute != MiNonCached) {

                        //
                        // The caller asked for a cached or writecombined
                        // mapping, but the page is already mapped noncached
                        // by someone else.  Override the caller's request
                        // in order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[1] += 1;
                    }
                    MI_DISABLE_CACHING (TempPte);
                    break;

                case MiWriteCombined:
                    if (CacheAttribute != MiWriteCombined) {

                        //
                        // The caller asked for a cached or noncached
                        // mapping, but the page is already mapped
                        // writecombined by someone else.  Override the
                        // caller's request in order to keep the TB page
                        // attribute coherent.
                        //

                        MiCacheOverride[2] += 1;
                    }
                    MI_SET_PTE_WRITE_COMBINE (TempPte);
                    break;

                case MiNotMapped:

                    //
                    // This better be for a page allocated with
                    // MmAllocatePagesForMdl.  Otherwise it might be a
                    // page on the freelist which could subsequently be
                    // given out with a different attribute !
                    //

                    ASSERT ((Pfn2->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
                            (Pfn2->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1)));
                    if (OldIrql == HIGH_LEVEL) {
                        LOCK_PFN2 (OldIrql);
                    }

                    switch (CacheAttribute) {

                        case MiCached:
                            Pfn2->u3.e1.CacheAttribute = MiCached;
                            break;

                        case MiNonCached:
                            Pfn2->u3.e1.CacheAttribute = MiNonCached;
                            MI_DISABLE_CACHING (TempPte);
                            break;

                        case MiWriteCombined:
                            Pfn2->u3.e1.CacheAttribute = MiWriteCombined;
                            MI_SET_PTE_WRITE_COMBINE (TempPte);
                            break;

                        default:
                            ASSERT (FALSE);
                            break;
                    }
                    break;

                default:
                    ASSERT (FALSE);
                    break;
            }
        }

        TempPte.u.Hard.PageFrameNumber = *Page;
        MI_WRITE_VALID_PTE (PointerPte, TempPte);
        Page += 1;
        PointerPte += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    if (OldIrql != HIGH_LEVEL) {
        UNLOCK_PFN2 (OldIrql);
    }

    MI_SWEEP_CACHE (CacheAttribute, MappingAddress, SavedPageCount * PAGE_SIZE);

    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0);
    MemoryDescriptorList->MappedSystemVa = MappingAddress;

    MemoryDescriptorList->MdlFlags |= MDL_MAPPED_TO_SYSTEM_VA;

    if ((MemoryDescriptorList->MdlFlags & MDL_PARTIAL) != 0) {
        MemoryDescriptorList->MdlFlags |= MDL_PARTIAL_HAS_BEEN_MAPPED;
    }

    MappingAddress = (PVOID)((PCHAR)MappingAddress + MemoryDescriptorList->ByteOffset);

    return MappingAddress;
}

VOID
MmUnmapReservedMapping (
     IN PVOID BaseAddress,
     IN ULONG PoolTag,
     IN PMDL MemoryDescriptorList
     )

/*++

Routine Description:

    This routine unmaps locked pages which were previously mapped via
    a MmMapLockedPagesWithReservedMapping call.

Arguments:

    BaseAddress - Supplies the base address where the pages were previously
                  mapped.

    PoolTag - Supplies the caller's identifying tag.

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

Return Value:

    None.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below.  The caller must synchronize usage
    of the argument virtual address space.

--*/

{
    ULONG OriginalPoolTag;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER ExtraPages;
    PFN_NUMBER VaPageSpan;
    PMMPTE PointerBase;
    PMMPTE LastPte;
    PMMPTE LastMdlPte;
    PVOID StartingVa;
    PVOID VaFlushList[MM_MAXIMUM_FLUSH_COUNT];
    PMMPTE PointerPte;
    PFN_NUMBER i;
    PPFN_NUMBER Page;
    PPFN_NUMBER LastCurrentPage;

    ASSERT (KeGetCurrentIrql () <= DISPATCH_LEVEL);
    ASSERT (MemoryDescriptorList->ByteCount != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) != 0);

    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PARENT_MAPPED_SYSTEM_VA) == 0);
    ASSERT (!MI_IS_PHYSICAL_ADDRESS (BaseAddress));
    ASSERT (BaseAddress > MM_HIGHEST_USER_ADDRESS);

    PointerPte = MiGetPteAddress (BaseAddress);
    PointerBase = PointerPte - 2;

    OriginalPoolTag = *(PULONG) (PointerPte - 1);
    ASSERT ((OriginalPoolTag & 0x1) == 0);

    if (OriginalPoolTag != (PoolTag & ~0x1)) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x108,
                      (ULONG_PTR)BaseAddress,
                      PoolTag,
                      OriginalPoolTag);
    }

    VaPageSpan = *(PULONG_PTR)PointerBase;
    ASSERT ((VaPageSpan & 0x1) == 0);
    VaPageSpan >>= 1;

    if (VaPageSpan <= 2) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x109,
                      (ULONG_PTR)BaseAddress,
                      PoolTag,
                      VaPageSpan);
    }

    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                           MemoryDescriptorList->ByteCount);

    if (NumberOfPages > VaPageSpan - 2) {
        KeBugCheckEx (SYSTEM_PTE_MISUSE,
                      0x10A,
                      (ULONG_PTR)BaseAddress,
                      VaPageSpan,
                      NumberOfPages);
    }

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    LastCurrentPage = Page + NumberOfPages;

    if (MemoryDescriptorList->MdlFlags & MDL_FREE_EXTRA_PTES) {

        ExtraPages = *(Page + NumberOfPages);
        ASSERT (ExtraPages <= MiCurrentAdvancedPages);
        ASSERT (NumberOfPages + ExtraPages <= VaPageSpan - 2);
        NumberOfPages += ExtraPages;
#if DBG
        InterlockedExchangeAddSizeT (&MiCurrentAdvancedPages, 0 - ExtraPages);
        MiAdvancesFreed += ExtraPages;
#endif
    }

    LastMdlPte = PointerPte + NumberOfPages;
    LastPte = PointerPte + VaPageSpan - 2;

    //
    // The range described by the argument MDL must be mapped.
    //

    while (PointerPte < LastMdlPte) {
        if (PointerPte->u.Hard.Valid == 0) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x10B,
                          (ULONG_PTR)BaseAddress,
                          PoolTag,
                          NumberOfPages);
        }

#if DBG
        ASSERT ((*Page == MI_GET_PAGE_FRAME_FROM_PTE (PointerPte)) ||
                (MemoryDescriptorList->MdlFlags & MDL_FREE_EXTRA_PTES));

        if (((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0) &&
            (Page < LastCurrentPage)) {

            PMMPFN Pfn3;
            Pfn3 = MI_PFN_ELEMENT (*Page);
            ASSERT (Pfn3->u3.e2.ReferenceCount != 0);
        }

        Page += 1;
#endif

        PointerPte += 1;
    }

    //
    // The range past the argument MDL must be unmapped.
    //

    while (PointerPte < LastPte) {
        if (PointerPte->u.Long != 0) {
            KeBugCheckEx (SYSTEM_PTE_MISUSE,
                          0x10C,
                          (ULONG_PTR)BaseAddress,
                          PoolTag,
                          NumberOfPages);
        }
        PointerPte += 1;
    }

    MiZeroMemoryPte (PointerBase + 2, NumberOfPages);

    if (NumberOfPages == 1) {
        KeFlushSingleTb (BaseAddress, TRUE);
    }
    else if (NumberOfPages < MM_MAXIMUM_FLUSH_COUNT) {

        for (i = 0; i < NumberOfPages; i += 1) {
            VaFlushList[i] = BaseAddress;
            BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
        }

        KeFlushMultipleTb ((ULONG)NumberOfPages, &VaFlushList[0], TRUE);
    }
    else {
        KeFlushEntireTb (TRUE, TRUE);
    }

    MemoryDescriptorList->MdlFlags &= ~(MDL_MAPPED_TO_SYSTEM_VA |
                                        MDL_PARTIAL_HAS_BEEN_MAPPED);

    return;
}
NTKERNELAPI
NTSTATUS
MmAdvanceMdl (
    IN PMDL Mdl,
    IN ULONG NumberOfBytes
    )

/*++

Routine Description:

    This routine takes the specified MDL and "advances" it forward
    by the specified number of bytes.  If this causes the MDL to advance
    past the initial page, the pages that are advanced over are immediately
    unlocked and the system VA that maps the MDL is also adjusted (along
    with the user address).
    
    WARNING !  WARNING !  WARNING !

    This means the caller MUST BE AWARE that the "advanced" pages are
    immediately reused and therefore MUST NOT BE REFERENCED by the caller
    once this routine has been called.  Likewise the virtual address as
    that is also being adjusted here.

    Even if the caller has statically allocated this MDL on his local stack,
    he cannot use more than the space currently described by the MDL on return
    from this routine unless he first unmaps the MDL (if it was mapped).
    Otherwise the system PTE lists will be corrupted.

Arguments:

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

    NumberOfBytes - The number of bytes to advance the MDL by.

Return Value:

    NTSTATUS.

--*/

{
    ULONG i;
    ULONG PageCount;
    ULONG FreeBit;
    ULONG Slush;
    KIRQL OldIrql;
    PPFN_NUMBER Page;
    PPFN_NUMBER NewPage;
    ULONG OffsetPages;
    PEPROCESS Process;
    PMMPFN Pfn1;
    CSHORT MdlFlags;
    PVOID StartingVa;
    PFN_NUMBER NumberOfPages;

    ASSERT (KeGetCurrentIrql () <= DISPATCH_LEVEL);
    ASSERT (Mdl->MdlFlags & (MDL_PAGES_LOCKED | MDL_SOURCE_IS_NONPAGED_POOL));
    ASSERT (BYTE_OFFSET (Mdl->StartVa) == 0);

    //
    // Disallow advancement past the end of the MDL.
    //

    if (NumberOfBytes >= Mdl->ByteCount) {
        return STATUS_INVALID_PARAMETER_2;
    }

    PageCount = 0;

    MiMdlsAdjusted = TRUE;

    StartingVa = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa, Mdl->ByteCount);

    if (Mdl->ByteOffset != 0) {
        Slush = PAGE_SIZE - Mdl->ByteOffset;

        if (NumberOfBytes < Slush) {

            Mdl->ByteCount -= NumberOfBytes;
            Mdl->ByteOffset += NumberOfBytes;

            //
            // StartVa never includes the byte offset (it's always page-aligned)
            // so don't adjust it here.  MappedSystemVa does include byte
            // offsets so do adjust that.
            //

            if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
                Mdl->MappedSystemVa = (PVOID) ((PCHAR)Mdl->MappedSystemVa + NumberOfBytes);
            }

            return STATUS_SUCCESS;
        }

        NumberOfBytes -= Slush;

        Mdl->StartVa = (PVOID) ((PCHAR)Mdl->StartVa + PAGE_SIZE);
        Mdl->ByteOffset = 0;
        Mdl->ByteCount -= Slush;

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {
            Mdl->MappedSystemVa = (PVOID) ((PCHAR)Mdl->MappedSystemVa + Slush);
        }

        //
        // Up the number of pages (and addresses) that need to slide.
        //

        PageCount += 1;
    }

    //
    // The MDL start is now nicely page aligned.  Make sure there's still
    // data left in it (we may have finished it off above), then operate on it.
    //

    if (NumberOfBytes != 0) {

        Mdl->ByteCount -= NumberOfBytes;

        Mdl->ByteOffset = BYTE_OFFSET (NumberOfBytes);

        OffsetPages = NumberOfBytes >> PAGE_SHIFT;

        Mdl->StartVa = (PVOID) ((PCHAR)Mdl->StartVa + (OffsetPages << PAGE_SHIFT));
        PageCount += OffsetPages;

        if (Mdl->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {

            Mdl->MappedSystemVa = (PVOID) ((PCHAR)Mdl->MappedSystemVa +
                                           (OffsetPages << PAGE_SHIFT) +
                                           Mdl->ByteOffset);
        }
    }

    ASSERT (PageCount <= NumberOfPages);

    if (PageCount != 0) {

        //
        // Slide the page frame numbers forward decrementing reference counts
        // on the ones that are released.  Then adjust the mapped system VA
        // (if there is one) to reflect the current frame.  Note that the TB
        // does not need to be flushed due to the careful sliding and when
        // the MDL is finally completely unmapped, the extra information
        // added to the MDL here is used to free the entire original PTE
        // mapping range in one chunk so as not to fragment the PTE space.
        //

        Page = (PPFN_NUMBER)(Mdl + 1);
        NewPage = Page;

        Process = Mdl->Process;

        MdlFlags = Mdl->MdlFlags;

        if (Process != NULL) {

            if ((MdlFlags & MDL_PAGES_LOCKED) &&
                ((MdlFlags & MDL_IO_SPACE) == 0)) {

                ASSERT ((MdlFlags & MDL_SOURCE_IS_NONPAGED_POOL) == 0);
                ASSERT ((SPFN_NUMBER)Process->NumberOfLockedPages >= 0);

                InterlockedExchangeAddSizeT (&Process->NumberOfLockedPages,
                                             0 - PageCount);
            }

            if (MmTrackLockedPages == TRUE) {
                MiUpdateMdlTracker (Mdl, PageCount);
            }
        }

        LOCK_PFN2 (OldIrql);

        for (i = 0; i < PageCount; i += 1) {

            //
            // Decrement the stale page frames now, this will unlock them
            // resulting in them being immediately reused if necessary.
            //

            if ((MdlFlags & MDL_PAGES_LOCKED) &&
                ((MdlFlags & MDL_IO_SPACE) == 0)) {

                ASSERT ((MdlFlags & MDL_SOURCE_IS_NONPAGED_POOL) == 0);

                Pfn1 = MI_PFN_ELEMENT (*Page);

                if (MdlFlags & MDL_WRITE_OPERATION) {

                    //
                    // If this was a write operation set the modified bit
                    // in the PFN database.
                    //

                    MI_SET_MODIFIED (Pfn1, 1, 0x3);

                    if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                                 (Pfn1->u3.e1.WriteInProgress == 0)) {

                        FreeBit = GET_PAGING_FILE_OFFSET (Pfn1->OriginalPte);

                        if ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED)) {
                            MiReleaseConfirmedPageFileSpace (Pfn1->OriginalPte);
                            Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
                        }
                    }
                }
                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn1, 1);
            }
            Page += 1;
        }

        UNLOCK_PFN2 (OldIrql);

        //
        // Now ripple the remaining pages to the front of the MDL, effectively
        // purging the old ones which have just been released.
        //

        ASSERT (i < NumberOfPages);

        for ( ; i < NumberOfPages; i += 1) {

            if (*Page == MM_EMPTY_LIST) {
                break;
            }

            *NewPage = *Page;
            NewPage += 1;
            Page += 1;
        }

        //
        // If the MDL has been mapped, stash the number of pages advanced
        // at the end of the frame list inside the MDL and mark the MDL as
        // containing extra PTEs to free.  Thus when the MDL is finally
        // completely unmapped, this can be used so the entire original PTE
        // mapping range can be freed in one chunk so as not to fragment the
        // PTE space.
        //

        if (MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) {

#if DBG
            InterlockedExchangeAddSizeT (&MiCurrentAdvancedPages, PageCount);
            MiAdvancesGiven += PageCount;
#endif

            if (MdlFlags & MDL_FREE_EXTRA_PTES) {

                //
                // This MDL has already been advanced at least once.  Any
                // PTEs from those advancements need to be preserved now.
                //

                ASSERT (*Page <= MiCurrentAdvancedPages - PageCount);
                PageCount += *(PULONG)Page;
            }
            else {
                Mdl->MdlFlags |= MDL_FREE_EXTRA_PTES;
            }

            *NewPage = PageCount;
        }
    }

    return STATUS_SUCCESS;
}

NTKERNELAPI
NTSTATUS
MmProtectMdlSystemAddress (
    IN PMDL MemoryDescriptorList,
    IN ULONG NewProtect
    )

/*++

Routine Description:

    This function protects the system address range specified
    by the argument Memory Descriptor List.

    Note the caller must make this MDL mapping readwrite before finally
    freeing (or reusing) it.

Arguments:

    MemoryDescriptorList - Supplies the MDL describing the virtual range.

    NewProtect - Supplies the protection to set the pages to (PAGE_XX).

Return Value:

    NTSTATUS.

Environment:

    Kernel mode, IRQL DISPATCH_LEVEL or below.  The caller is responsible for
    synchronizing access to this routine.

--*/

{
    KIRQL OldIrql;
    PVOID BaseAddress;
    PVOID SystemVa;
    MMPTE PteContents;
    PMMPTE PointerPte;
    ULONG ProtectionMask;
#if DBG
    PMMPFN Pfn1;
    PPFN_NUMBER Page;
#endif
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NumberOfPages;
    MMPTE_FLUSH_LIST PteFlushList;
    MMPTE OriginalPte;
    LOGICAL WasValid;
    PMM_PTE_MAPPING Map;
    PMM_PTE_MAPPING MapEntry;
    PMM_PTE_MAPPING FoundMap;
    PLIST_ENTRY NextEntry;

    ASSERT (KeGetCurrentIrql () <= DISPATCH_LEVEL);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PAGES_LOCKED) != 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_SOURCE_IS_NONPAGED_POOL) == 0);
    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_PARTIAL) == 0);
    ASSERT (MemoryDescriptorList->ByteCount != 0);

    if ((MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA) == 0) {
        return STATUS_NOT_MAPPED_VIEW;
    }

    BaseAddress = MemoryDescriptorList->MappedSystemVa;

    ASSERT (BaseAddress > MM_HIGHEST_USER_ADDRESS);

    ASSERT (!MI_IS_PHYSICAL_ADDRESS (BaseAddress));

    ProtectionMask = MiMakeProtectionMask (NewProtect);

    //
    // No bogus or copy-on-write protections allowed for these.
    //

    if ((ProtectionMask == MM_INVALID_PROTECTION) ||
        (ProtectionMask == MM_GUARD_PAGE) ||
        (ProtectionMask == MM_DECOMMIT) ||
        (ProtectionMask == MM_NOCACHE) ||
        (ProtectionMask == MM_WRITECOPY) ||
        (ProtectionMask == MM_EXECUTE_WRITECOPY)) {

        return STATUS_INVALID_PAGE_PROTECTION;
    }

    PointerPte = MiGetPteAddress (BaseAddress);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (BaseAddress,
                                           MemoryDescriptorList->ByteCount);

    SystemVa = PAGE_ALIGN (BaseAddress);

    //
    // Initializing Map is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    Map = NULL;

    if (ProtectionMask != MM_READWRITE) {

        Map = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof(MM_PTE_MAPPING),
                                     'mPmM');

        if (Map == NULL) {
            return STATUS_INSUFFICIENT_RESOURCES;
        }

        Map->SystemVa = SystemVa;
        Map->SystemEndVa = (PVOID)((ULONG_PTR)SystemVa + (NumberOfPages << PAGE_SHIFT));
        Map->Protection = ProtectionMask;
    }

#if DBG
    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
#endif

    PteFlushList.Count = 0;

    while (NumberOfPages != 0) {

        PteContents = *PointerPte;

        if (PteContents.u.Hard.Valid == 1) {
            WasValid = TRUE;
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);
            OriginalPte = PteContents;
        }
        else if ((PteContents.u.Soft.Transition == 1) &&
                 (PteContents.u.Soft.Protection == MM_NOACCESS)) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);
            WasValid = FALSE;
#if defined(_IA64_)
            OriginalPte.u.Hard.Cache = PteContents.u.Trans.Rsvd0;
#else
            OriginalPte.u.Hard.WriteThrough = PteContents.u.Soft.PageFileLow;
            OriginalPte.u.Hard.CacheDisable = (PteContents.u.Soft.PageFileLow >> 1);
#endif

        }
        else {
            KeBugCheckEx (MEMORY_MANAGEMENT,
                          0x1235,
                          (ULONG_PTR)MemoryDescriptorList,
                          (ULONG_PTR)PointerPte,
                          (ULONG_PTR)PteContents.u.Long);
        }

#if DBG
        ASSERT (*Page == PageFrameIndex);

        if ((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0) {
            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
        }

        Page += 1;
#endif

        if (ProtectionMask == MM_NOACCESS) {

            //
            // To generate a bugcheck on bogus access: Prototype must stay
            // clear, transition must stay set, protection must stay NO_ACCESS.
            //

            MI_MAKE_VALID_PTE_TRANSITION (PteContents, MM_NOACCESS);

            //
            // Stash the cache attributes into the software PTE so they can
            // be restored later.
            //

#if defined(_IA64_)
            PteContents.u.Trans.Rsvd0 = OriginalPte.u.Hard.Cache;
#else
            PteContents.u.Soft.PageFileLow = OriginalPte.u.Hard.WriteThrough;
            PteContents.u.Soft.PageFileLow |= (OriginalPte.u.Hard.CacheDisable << 1);
#endif
            MI_WRITE_INVALID_PTE (PointerPte, PteContents);
        }
        else {
            MI_MAKE_VALID_PTE (PteContents,
                               PageFrameIndex,
                               ProtectionMask,
                               PointerPte);

            if (ProtectionMask & MM_READWRITE) {
                MI_SET_PTE_DIRTY (PteContents);
            }

            //
            // Extract cache type from the original PTE so it can be preserved.
            // Note that since we only allow protection changes (not caching
            // attribute changes), there is no need to flush or sweep TBs on
            // insertion below.
            //

#if defined(_IA64_)
            PteContents.u.Hard.Cache = OriginalPte.u.Hard.Cache;
#else
            PteContents.u.Hard.WriteThrough = OriginalPte.u.Hard.WriteThrough;
            PteContents.u.Hard.CacheDisable = OriginalPte.u.Hard.CacheDisable;
#endif
            if (WasValid == TRUE) {
                MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, PteContents);
            }
            else {
                MI_WRITE_VALID_PTE (PointerPte, PteContents);
            }
        }

        if ((WasValid == TRUE) &&
            (PteFlushList.Count != MM_MAXIMUM_FLUSH_COUNT)) {

            PteFlushList.FlushVa[PteFlushList.Count] = BaseAddress;
            PteFlushList.Count += 1;
        }

        BaseAddress = (PVOID)((ULONG_PTR)BaseAddress + PAGE_SIZE);
        PointerPte += 1;
        NumberOfPages -= 1;
    }

    //
    // Flush the TB entries for any relevant pages.
    //

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList, TRUE);
    }

    if (ProtectionMask != MM_READWRITE) {

        //
        // Insert (or update) the list entry describing this range.
        // Don't bother sorting the list as there will never be many entries.
        //

        FoundMap = NULL;

        OldIrql = KeAcquireSpinLockRaiseToSynch (&MmProtectedPteLock);

        NextEntry = MmProtectedPteList.Flink;

        while (NextEntry != &MmProtectedPteList) {

            MapEntry = CONTAINING_RECORD (NextEntry,
                                          MM_PTE_MAPPING,
                                          ListEntry);

            if (MapEntry->SystemVa == SystemVa) {
                ASSERT (MapEntry->SystemEndVa == Map->SystemEndVa);
                MapEntry->Protection = Map->Protection;
                FoundMap = MapEntry;
                break;
            }
            NextEntry = NextEntry->Flink;
        }

        if (FoundMap == NULL) {
            InsertHeadList (&MmProtectedPteList, &Map->ListEntry);
        }

        KeReleaseSpinLock (&MmProtectedPteLock, OldIrql);

        if (FoundMap != NULL) {
            ExFreePool (Map);
        }
    }
    else {

        //
        // If there is an existing list entry describing this range, remove it.
        //

        if (!IsListEmpty (&MmProtectedPteList)) {

            FoundMap = NULL;

            OldIrql = KeAcquireSpinLockRaiseToSynch (&MmProtectedPteLock);

            NextEntry = MmProtectedPteList.Flink;

            while (NextEntry != &MmProtectedPteList) {

                MapEntry = CONTAINING_RECORD (NextEntry,
                                              MM_PTE_MAPPING,
                                              ListEntry);

                if (MapEntry->SystemVa == SystemVa) {
                    RemoveEntryList (NextEntry);
                    FoundMap = MapEntry;
                    break;
                }
                NextEntry = NextEntry->Flink;
            }

            KeReleaseSpinLock (&MmProtectedPteLock, OldIrql);

            if (FoundMap != NULL) {
                ExFreePool (FoundMap);
            }
        }
    }

    ASSERT (MemoryDescriptorList->MdlFlags & MDL_MAPPED_TO_SYSTEM_VA);

    return STATUS_SUCCESS;
}

LOGICAL
MiCheckSystemPteProtection (
    IN ULONG_PTR StoreInstruction,
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This function determines whether the faulting virtual address lies
    within the non-writable alternate system PTE mappings.

Arguments:

    StoreInstruction - Supplies nonzero if the operation causes a write into
                       memory, zero if not.

    VirtualAddress - Supplies the virtual address which caused the fault.

Return Value:

    TRUE if the fault was handled by this code (and PTE updated), FALSE if not.

Environment:

    Kernel mode.  Called from the fault handler at any IRQL.

--*/

{
    KIRQL OldIrql;
    PMMPTE PointerPte;
    ULONG ProtectionCode;
    PLIST_ENTRY NextEntry;
    PMM_PTE_MAPPING MapEntry;

    //
    // If PTE mappings with various protections are active and the faulting
    // address lies within these mappings, resolve the fault with
    // the appropriate protections.
    //

    if (IsListEmpty (&MmProtectedPteList)) {
        return FALSE;
    }

    OldIrql = KeAcquireSpinLockRaiseToSynch (&MmProtectedPteLock);

    NextEntry = MmProtectedPteList.Flink;

    while (NextEntry != &MmProtectedPteList) {

        MapEntry = CONTAINING_RECORD (NextEntry,
                                      MM_PTE_MAPPING,
                                      ListEntry);

        if ((VirtualAddress >= MapEntry->SystemVa) &&
            (VirtualAddress < MapEntry->SystemEndVa)) {

            ProtectionCode = MapEntry->Protection;
            KeReleaseSpinLock (&MmProtectedPteLock, OldIrql);

            PointerPte = MiGetPteAddress (VirtualAddress);

            if (StoreInstruction != 0) {
                if ((ProtectionCode & MM_READWRITE) == 0) {

                    KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                                  (ULONG_PTR)VirtualAddress,
                                  (ULONG_PTR)PointerPte->u.Long,
                                  0,
                                  16);
                }
            }

            MI_NO_FAULT_FOUND (StoreInstruction,
                               PointerPte,
                               VirtualAddress,
                               FALSE);

            //
            // Fault was handled directly here, no need for the caller to
            // do anything.
            //

            return TRUE;
        }
        NextEntry = NextEntry->Flink;
    }

    KeReleaseSpinLock (&MmProtectedPteLock, OldIrql);

    return FALSE;
}

VOID
MiInsertPhysicalVadRoot (
    IN PEPROCESS Process,
    IN PMM_AVL_TABLE PhysicalVadRoot
    )

/*++

Routine Description:

    This function is a nonpaged wrapper which acquires the PFN lock to insert
    the physical VAD AVL root table into the specified process.

Arguments:

    Process - Supplies the process to add the physical VAD root to.

    PhysicalVadRoot - Supplies the physical vad root table to link in.

Return Value:

    None.

Environment:

    Kernel mode.  APC_LEVEL, address space (and optionally working set)
    mutex held.

--*/
{
    KIRQL OldIrql;

    ASSERT (KeGetOwnerGuardedMutex (&Process->AddressCreationLock) == KeGetCurrentThread ());

    //
    // Acquire the PFN lock to synchronize with concurrent threads calling
    // MmProbeAndLockPages which examines this table.
    //

    LOCK_PFN (OldIrql);

    ASSERT (Process->PhysicalVadRoot == NULL);

    Process->PhysicalVadRoot = PhysicalVadRoot;

    UNLOCK_PFN (OldIrql);

    return;
}

VOID
MiPhysicalViewInserter (
    IN PEPROCESS Process,
    IN PMI_PHYSICAL_VIEW PhysicalView
    )

/*++

Routine Description:

    This function is a nonpaged wrapper which acquires the PFN lock to insert
    a physical VAD into the process chain.

Arguments:

    Process - Supplies the process to add the physical VAD to.

    PhysicalView - Supplies the physical view data to link in.

Return Value:

    None.

Environment:

    Kernel mode.  APC_LEVEL, working set and address space mutexes held.

--*/
{
    KIRQL OldIrql;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    ASSERT (Process->PhysicalVadRoot != NULL);

    MiInsertNode ((PMMADDRESS_NODE)PhysicalView, Process->PhysicalVadRoot);

    UNLOCK_PFN (OldIrql);

    if (PhysicalView->Vad->u.VadFlags.WriteWatch == 1) {

        //
        // Mark this process as forever containing write-watch
        // address space(s).
        //

        PS_SET_BITS (&Process->Flags, PS_PROCESS_FLAGS_USING_WRITE_WATCH);
    }

    MmUnlockPagableImageSection (ExPageLockHandle);

    return;
}

VOID
MiPhysicalViewRemover (
    IN PEPROCESS Process,
    IN PMMVAD Vad
    )

/*++

Routine Description:

    This function is a nonpaged wrapper which acquires the PFN lock to remove
    a physical VAD from the process chain.

Arguments:

    Process - Supplies the process to remove the physical VAD from.

    Vad - Supplies the Vad to remove.

Return Value:

    None.

Environment:

    Kernel mode, APC_LEVEL, working set and address space mutexes held.

--*/
{
    KIRQL OldIrql;
    PRTL_BITMAP BitMap;
    PMI_PHYSICAL_VIEW PhysicalView;
    ULONG BitMapSize;
    TABLE_SEARCH_RESULT SearchResult;

    LOCK_PFN (OldIrql);

    //
    // Lookup the element and save the result.
    //

    ASSERT (Process->PhysicalVadRoot != NULL);

    SearchResult = MiFindNodeOrParent (Process->PhysicalVadRoot,
                                       Vad->StartingVpn,
                                       (PMMADDRESS_NODE *) &PhysicalView);

    ASSERT (SearchResult == TableFoundNode);
    ASSERT (PhysicalView->Vad == Vad);

    MiRemoveNode ((PMMADDRESS_NODE)PhysicalView, Process->PhysicalVadRoot);

    UNLOCK_PFN (OldIrql);

    if (Vad->u.VadFlags.WriteWatch == 1) {
        BitMap = PhysicalView->u.BitMap;
        BitMapSize = sizeof(RTL_BITMAP) + (ULONG)(((BitMap->SizeOfBitMap + 31) / 32) * 4);
        PsReturnProcessNonPagedPoolQuota (Process, BitMapSize);
        ExFreePool (BitMap);
    }

    ExFreePool (PhysicalView);

    return;
}

VOID
MiPhysicalViewAdjuster (
    IN PEPROCESS Process,
    IN PMMVAD OldVad,
    IN PMMVAD NewVad
    )

/*++

Routine Description:

    This function is a nonpaged wrapper which acquires the PFN lock to repoint
    a physical VAD in the process chain.

Arguments:

    Process - Supplies the process in which to adjust the physical VAD.

    Vad - Supplies the old Vad to replace.

    NewVad - Supplies the newVad to substitute.

Return Value:

    None.

Environment:

    Kernel mode, called with APCs disabled, working set mutex held.

--*/
{
    KIRQL OldIrql;
    PMI_PHYSICAL_VIEW PhysicalView;
    TABLE_SEARCH_RESULT SearchResult;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    //
    // Lookup the element and save the result.
    //

    ASSERT (Process->PhysicalVadRoot != NULL);

    SearchResult = MiFindNodeOrParent (Process->PhysicalVadRoot,
                                       OldVad->StartingVpn,
                                       (PMMADDRESS_NODE *) &PhysicalView);

    ASSERT (SearchResult == TableFoundNode);
    ASSERT (PhysicalView->Vad == OldVad);

    PhysicalView->Vad = NewVad;

    UNLOCK_PFN (OldIrql);

    MmUnlockPagableImageSection (ExPageLockHandle);

    return;
}

PVOID
MiMapLockedPagesInUserSpace (
     IN PMDL MemoryDescriptorList,
     IN PVOID StartingVa,
     IN MEMORY_CACHING_TYPE CacheType,
     IN PVOID BaseVa
     )

/*++

Routine Description:

    This function maps physical pages described by a memory descriptor
    list into the user portion of the virtual address space.

Arguments:

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.


    StartingVa - Supplies the starting address.

    CacheType - Supplies the type of cache mapping to use for the MDL.
                MmCached indicates "normal" user mappings.

    BaseVa - Supplies the base address of the view. If the initial
             value of this argument is not null, then the view will
             be allocated starting at the specified virtual
             address rounded down to the next 64kb address
             boundary. If the initial value of this argument is
             null, then the operating system will determine
             where to allocate the view.

Return Value:

    Returns the base address where the pages are mapped.  The base address
    has the same offset as the virtual address in the MDL.

    This routine will raise an exception if quota limits or VM limits are
    exceeded.

Environment:

    Kernel mode.  APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    CSHORT IoMapping;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER SavedPageCount;
    PFN_NUMBER PageFrameIndex;
    PPFN_NUMBER Page;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PCHAR Va;
    MMPTE TempPte;
    PVOID EndingAddress;
    PMMVAD_LONG Vad;
    PEPROCESS Process;
    PMMPFN Pfn2;
    PVOID UsedPageTableHandle;
    PMI_PHYSICAL_VIEW PhysicalView;
    NTSTATUS Status;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;
    PMM_AVL_TABLE PhysicalVadRoot;

    PAGED_CODE ();
    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                           MemoryDescriptorList->ByteCount);

    //
    // If a noncachable mapping is requested, none of the pages in the
    // requested MDL can reside in a large page.  Otherwise we would be
    // creating an incoherent overlapping TB entry as the same physical
    // page would be mapped by 2 different TB entries with different
    // cache attributes.
    //

    IoMapping = MemoryDescriptorList->MdlFlags & MDL_IO_SPACE;

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, IoMapping);

    if (CacheAttribute != MiCached) {

        SavedPageCount = NumberOfPages;

        LOCK_PFN (OldIrql);

        do {

            if (*Page == MM_EMPTY_LIST) {
                break;
            }
            PageFrameIndex = *Page;
            if (MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (PageFrameIndex)) {
                UNLOCK_PFN (OldIrql);
                MiNonCachedCollisions += 1;
                ExRaiseStatus (STATUS_INVALID_ADDRESS);
                return NULL;
            }

            Page += 1;
            NumberOfPages -= 1;
        } while (NumberOfPages != 0);
        UNLOCK_PFN (OldIrql);

        NumberOfPages = SavedPageCount;
        Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    }

    //
    // Map the pages into the user part of the address as user
    // read/write no-delete.
    //

    Vad = ExAllocatePoolWithTag (NonPagedPool, sizeof(MMVAD_LONG), 'ldaV');

    if (Vad == NULL) {
        ExRaiseStatus (STATUS_INSUFFICIENT_RESOURCES);
        return NULL;
    }

    PhysicalView = (PMI_PHYSICAL_VIEW)ExAllocatePoolWithTag (NonPagedPool,
                                                             sizeof(MI_PHYSICAL_VIEW),
                                                             MI_PHYSICAL_VIEW_KEY);
    if (PhysicalView == NULL) {
        ExFreePool (Vad);
        ExRaiseStatus (STATUS_INSUFFICIENT_RESOURCES);
        return NULL;
    }

    RtlZeroMemory (Vad, sizeof (MMVAD_LONG));

    ASSERT (Vad->ControlArea == NULL);
    ASSERT (Vad->FirstPrototypePte == NULL);
    ASSERT (Vad->u.LongFlags == 0);
    Vad->u.VadFlags.Protection = MM_READWRITE;
    Vad->u.VadFlags.PhysicalMapping = 1;
    Vad->u.VadFlags.PrivateMemory = 1;

    Vad->u2.VadFlags2.LongVad = 1;

    PhysicalView->Vad = (PMMVAD) Vad;
    PhysicalView->u.LongFlags = MI_PHYSICAL_VIEW_PHYS;

    Process = PsGetCurrentProcess ();

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (BaseVa != NULL) {

        if (BYTE_OFFSET (BaseVa) != 0) {

            //
            // Invalid base address.
            //

            Status = STATUS_INVALID_ADDRESS;
            goto ErrorReturn;
        }

        EndingAddress = (PVOID)((PCHAR)BaseVa + ((ULONG_PTR)NumberOfPages * PAGE_SIZE) - 1);

        if ((EndingAddress <= BaseVa) || (EndingAddress > MM_HIGHEST_VAD_ADDRESS)) {
            //
            // Invalid region size.
            //

            Status = STATUS_INVALID_ADDRESS;
            goto ErrorReturn;
        }

        LOCK_ADDRESS_SPACE (Process);

        //
        // Make sure the address space was not deleted, if so, return an error.
        //

        if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
            UNLOCK_ADDRESS_SPACE (Process);
            Status = STATUS_PROCESS_IS_TERMINATING;
            goto ErrorReturn;
        }

        //
        // Make sure the address space is not already in use.
        //

        if (MiCheckForConflictingVadExistence (Process, BaseVa, EndingAddress) == TRUE) {
            UNLOCK_ADDRESS_SPACE (Process);
            Status = STATUS_CONFLICTING_ADDRESSES;
            goto ErrorReturn;
        }
    }
    else {

        //
        // Get the address creation mutex.
        //

        LOCK_ADDRESS_SPACE (Process);

        //
        // Make sure the address space was not deleted, if so, return an error.
        //

        if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
            UNLOCK_ADDRESS_SPACE (Process);
            Status = STATUS_PROCESS_IS_TERMINATING;
            goto ErrorReturn;
        }

        Status = MiFindEmptyAddressRange ((ULONG_PTR)NumberOfPages * PAGE_SIZE,
                                          X64K,
                                          0,
                                          &BaseVa);

        if (!NT_SUCCESS (Status)) {
            UNLOCK_ADDRESS_SPACE (Process);
            goto ErrorReturn;
        }

        EndingAddress = (PVOID)((PCHAR)BaseVa + ((ULONG_PTR)NumberOfPages * PAGE_SIZE) - 1);
    }

    Vad->StartingVpn = MI_VA_TO_VPN (BaseVa);
    Vad->EndingVpn = MI_VA_TO_VPN (EndingAddress);

    PhysicalView->StartingVpn = Vad->StartingVpn;
    PhysicalView->EndingVpn = Vad->EndingVpn;

    PhysicalVadRoot = Process->PhysicalVadRoot;

    //
    // The address space mutex synchronizes the allocation of the
    // EPROCESS PhysicalVadRoot.  This table root is not deleted until
    // the process exits.
    //

    if (PhysicalVadRoot == NULL) {

        PhysicalVadRoot = (PMM_AVL_TABLE) ExAllocatePoolWithTag (
                                                    NonPagedPool,
                                                    sizeof (MM_AVL_TABLE),
                                                    MI_PHYSICAL_VIEW_ROOT_KEY);

        if (PhysicalVadRoot == NULL) {
            UNLOCK_ADDRESS_SPACE (Process);
            Status = STATUS_INSUFFICIENT_RESOURCES;
            goto ErrorReturn;
        }

        RtlZeroMemory (PhysicalVadRoot, sizeof (MM_AVL_TABLE));
        ASSERT (PhysicalVadRoot->NumberGenericTableElements == 0);
        PhysicalVadRoot->BalancedRoot.u1.Parent = &PhysicalVadRoot->BalancedRoot;

        MiInsertPhysicalVadRoot (Process, PhysicalVadRoot);
    }

    LOCK_WS_UNSAFE (Process);

    Status = MiInsertVad ((PMMVAD) Vad);

    if (!NT_SUCCESS(Status)) {
        UNLOCK_WS_AND_ADDRESS_SPACE (Process);
        goto ErrorReturn;
    }

    //
    // The VAD has been inserted, but the physical view descriptor cannot
    // be until the page table page hierarchy is in place.  This is to
    // prevent races with probes.
    //

    //
    // Create a page table and fill in the mappings for the Vad.
    //

    Va = BaseVa;
    PointerPte = MiGetPteAddress (BaseVa);

    MI_PREPARE_FOR_NONCACHED (CacheAttribute);

    do {

        if (*Page == MM_EMPTY_LIST) {
            break;
        }

        PointerPde = MiGetPteAddress (PointerPte);

        MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);

        ASSERT (PointerPte->u.Hard.Valid == 0);

        //
        // Another zeroed PTE is being made non-zero.
        //

        UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (Va);

        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

        TempPte = ValidUserPte;
        TempPte.u.Hard.PageFrameNumber = *Page;

        if (IoMapping == 0) {

            Pfn2 = MI_PFN_ELEMENT (*Page);
            ASSERT (Pfn2->u3.e2.ReferenceCount != 0);

            switch (Pfn2->u3.e1.CacheAttribute) {

                case MiCached:
                    if (CacheAttribute != MiCached) {
                        //
                        // The caller asked for a noncached or writecombined
                        // mapping, but the page is already mapped cached by
                        // someone else.  Override the caller's request in
                        // order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[0] += 1;
                    }
                    break;

                case MiNonCached:
                    if (CacheAttribute != MiNonCached) {

                        //
                        // The caller asked for a cached or writecombined
                        // mapping, but the page is already mapped noncached
                        // by someone else.  Override the caller's request
                        // in order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[1] += 1;
                    }
                    MI_DISABLE_CACHING (TempPte);
                    break;

                case MiWriteCombined:
                    if (CacheAttribute != MiWriteCombined) {

                        //
                        // The caller asked for a cached or noncached
                        // mapping, but the page is already mapped
                        // writecombined by someone else.  Override the
                        // caller's request in order to keep the TB page
                        // attribute coherent.
                        //

                        MiCacheOverride[2] += 1;
                    }
                    MI_SET_PTE_WRITE_COMBINE (TempPte);
                    break;

                case MiNotMapped:

                    //
                    // This better be for a page allocated with
                    // MmAllocatePagesForMdl.  Otherwise it might be a
                    // page on the freelist which could subsequently be
                    // given out with a different attribute !
                    //

                    ASSERT ((Pfn2->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
                            (Pfn2->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1)));
                    switch (CacheAttribute) {

                        case MiCached:
                            Pfn2->u3.e1.CacheAttribute = MiCached;
                            break;

                        case MiNonCached:
                            Pfn2->u3.e1.CacheAttribute = MiNonCached;
                            MI_DISABLE_CACHING (TempPte);
                            break;

                        case MiWriteCombined:
                            Pfn2->u3.e1.CacheAttribute = MiWriteCombined;
                            MI_SET_PTE_WRITE_COMBINE (TempPte);
                            break;

                        default:
                            ASSERT (FALSE);
                            break;
                    }
                    break;

                default:
                    ASSERT (FALSE);
                    break;
            }
        }
        else {
            switch (CacheAttribute) {

                case MiCached:
                    break;

                case MiNonCached:
                    MI_DISABLE_CACHING (TempPte);
                    break;

                case MiWriteCombined:
                    MI_SET_PTE_WRITE_COMBINE (TempPte);
                    break;

                default:
                    ASSERT (FALSE);
                    break;
            }
        }

        MI_WRITE_VALID_PTE (PointerPte, TempPte);

        //
        // A PTE just went from not present, not transition to
        // present.  The share count and valid count must be
        // updated in the page table page which contains this PTE.
        //

        Pfn2 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
        Pfn2->u2.ShareCount += 1;

        Page += 1;
        PointerPte += 1;
        NumberOfPages -= 1;
        Va += PAGE_SIZE;
    } while (NumberOfPages != 0);

    MI_SWEEP_CACHE (CacheAttribute, BaseVa, MemoryDescriptorList->ByteCount);

    //
    // Insert the physical view descriptor now that the page table page
    // hierarchy is in place.  Note probes can find this descriptor immediately.
    //

    MiPhysicalViewInserter (Process, PhysicalView);

    UNLOCK_WS_AND_ADDRESS_SPACE (Process);

    ASSERT (BaseVa != NULL);

    BaseVa = (PVOID)((PCHAR)BaseVa + MemoryDescriptorList->ByteOffset);

    return BaseVa;

ErrorReturn:

    ExFreePool (Vad);
    ExFreePool (PhysicalView);
    ExRaiseStatus (Status);
    return NULL;
}

VOID
MiUnmapLockedPagesInUserSpace (
     IN PVOID BaseAddress,
     IN PMDL MemoryDescriptorList
     )

/*++

Routine Description:

    This routine unmaps locked pages which were previously mapped via
    a MmMapLockedPages function.

Arguments:

    BaseAddress - Supplies the base address where the pages were previously
                  mapped.

    MemoryDescriptorList - Supplies a valid Memory Descriptor List which has
                           been updated by MmProbeAndLockPages.

Return Value:

    None.

Environment:

    Kernel mode.  DISPATCH_LEVEL or below if base address is within system
    space, APC_LEVEL or below if base address is in user space.

--*/

{
    PFN_NUMBER NumberOfPages;
    PPFN_NUMBER Page;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
#if (_MI_PAGING_LEVELS >= 3)
    PMMPTE PointerPpe;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
#endif
    PVOID StartingVa;
    PVOID EndingVa;
    KIRQL OldIrql;
    PMMVAD Vad;
    PMMVAD PreviousVad;
    PMMVAD NextVad;
    PVOID TempVa;
    PEPROCESS Process;
    PMMPFN PageTablePfn;
    PFN_NUMBER PageTablePage;
    PVOID UsedPageTableHandle;
    MMPTE_FLUSH_LIST PteFlushList;

    PteFlushList.Count = 0;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingVa,
                                           MemoryDescriptorList->ByteCount);

    ASSERT (NumberOfPages != 0);

    PointerPte = MiGetPteAddress (BaseAddress);
    PointerPde = MiGetPdeAddress (BaseAddress);

    //
    // This was mapped into the user portion of the address space and
    // the corresponding virtual address descriptor must be deleted.
    //

    //
    // Get the working set mutex and address creation mutex.
    //

    Process = PsGetCurrentProcess ();

    LOCK_ADDRESS_SPACE (Process);

    Vad = MiLocateAddress (BaseAddress);

    if ((Vad == NULL) || (Vad->u.VadFlags.PhysicalMapping == 0)) {
        UNLOCK_ADDRESS_SPACE (Process);
        MmUnlockPagableImageSection(ExPageLockHandle);
        return;
    }

    PreviousVad = MiGetPreviousVad (Vad);
    NextVad = MiGetNextVad (Vad);

    StartingVa = MI_VPN_TO_VA (Vad->StartingVpn);
    EndingVa = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);

    LOCK_WS_UNSAFE (Process);

    MiPhysicalViewRemover (Process, Vad);

    MiRemoveVad (Vad);

    //
    // Return commitment for page table pages if possible.
    //

    MiReturnPageTablePageCommitment (StartingVa,
                                     EndingVa,
                                     Process,
                                     PreviousVad,
                                     NextVad);

    UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (BaseAddress);
    PageTablePage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
    PageTablePfn = MI_PFN_ELEMENT (PageTablePage);

    //
    // Get the PFN lock so we can safely decrement share and valid
    // counts on page table pages.
    //

    LOCK_PFN (OldIrql);

    do {

        if (*Page == MM_EMPTY_LIST) {
            break;
        }

        ASSERT64 (MiGetPdeAddress(PointerPte)->u.Hard.Valid == 1);
        ASSERT (MiGetPteAddress(PointerPte)->u.Hard.Valid == 1);
        ASSERT (PointerPte->u.Hard.Valid == 1);

        //
        // Another PTE is being zeroed.
        //

        MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

        MI_WRITE_INVALID_PTE (PointerPte, ZeroPte);

        if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
            PteFlushList.FlushVa[PteFlushList.Count] = BaseAddress;
            PteFlushList.Count += 1;
        }

        MiDecrementShareCountInline (PageTablePfn, PageTablePage);

        PointerPte += 1;
        NumberOfPages -= 1;
        BaseAddress = (PVOID)((PCHAR)BaseAddress + PAGE_SIZE);
        Page += 1;

        if ((MiIsPteOnPdeBoundary(PointerPte)) || (NumberOfPages == 0)) {

            if (PteFlushList.Count != 0) {
                MiFlushPteList (&PteFlushList, FALSE);
                PteFlushList.Count = 0;
            }

            PointerPde = MiGetPteAddress(PointerPte - 1);
            ASSERT (PointerPde->u.Hard.Valid == 1);

            //
            // If all the entries have been eliminated from the previous
            // page table page, delete the page table page itself.  Likewise
            // with the page directory and parent pages.
            //

            if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                ASSERT (PointerPde->u.Long != 0);

#if (_MI_PAGING_LEVELS >= 3)
                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPte - 1);
                MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
#endif

                TempVa = MiGetVirtualAddressMappedByPte (PointerPde);
                MiDeletePte (PointerPde,
                             TempVa,
                             FALSE,
                             Process,
                             NULL,
                             NULL,
                             OldIrql);

#if (_MI_PAGING_LEVELS >= 3)
                if ((MiIsPteOnPpeBoundary(PointerPte)) || (NumberOfPages == 0)) {
    
                    PointerPpe = MiGetPteAddress (PointerPde);
                    ASSERT (PointerPpe->u.Hard.Valid == 1);
    
                    //
                    // If all the entries have been eliminated from the previous
                    // page directory page, delete the page directory page too.
                    //
    
                    if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                        ASSERT (PointerPpe->u.Long != 0);

#if (_MI_PAGING_LEVELS >= 4)
                        UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPde);
                        MI_DECREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
#endif

                        TempVa = MiGetVirtualAddressMappedByPte(PointerPpe);
                        MiDeletePte (PointerPpe,
                                     TempVa,
                                     FALSE,
                                     Process,
                                     NULL,
                                     NULL,
                                     OldIrql);

#if (_MI_PAGING_LEVELS >= 4)
                        if ((MiIsPteOnPxeBoundary(PointerPte)) || (NumberOfPages == 0)) {
                            PointerPxe = MiGetPdeAddress (PointerPde);
                            ASSERT (PointerPxe->u.Long != 0);
                            if (MI_GET_USED_PTES_FROM_HANDLE (UsedPageTableHandle) == 0) {
                                TempVa = MiGetVirtualAddressMappedByPte(PointerPxe);
                                MiDeletePte (PointerPxe,
                                             TempVa,
                                             FALSE,
                                             Process,
                                             NULL,
                                             NULL,
                                             OldIrql);
                            }
                        }
#endif    
                    }
                }
#endif
            }

            if (NumberOfPages == 0) {
                break;
            }

            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (BaseAddress);
            PointerPde += 1;
            PageTablePage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde);
            PageTablePfn = MI_PFN_ELEMENT (PageTablePage);
        }

    } while (NumberOfPages != 0);

    if (PteFlushList.Count != 0) {
        MiFlushPteList (&PteFlushList, FALSE);
    }

    UNLOCK_PFN (OldIrql);
    UNLOCK_WS_AND_ADDRESS_SPACE (Process);
    ExFreePool (Vad);
    MmUnlockPagableImageSection(ExPageLockHandle);
    return;
}

#define MI_LARGE_PAGE_VA_SPACE ((ULONG64)8 * 1024 * 1024 * 1024)  // Relatively arbitrary

#if (_MI_PAGING_LEVELS>=3)

PVOID MiLargeVaStart;
ULONG MiLargeVaInUse [(MI_LARGE_PAGE_VA_SPACE / MM_MINIMUM_VA_FOR_LARGE_PAGE) / 32];

#endif

VOID
MiInitializeLargePageSupport (
    VOID
    )

/*++

Routine Description:

    This function is called once at system initialization.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, INIT time.  Resident available pages are not yet initialized,
    but everything else is.

--*/

{

#if (_MI_PAGING_LEVELS>=3)

    ULONG PageColor;
    KIRQL OldIrql;
    MMPTE TempPte;
    PMMPTE PointerPpe;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NumberOfPages;

    MiLargeVaStart = (PVOID)-1;

    RtlInitializeBitMap (&MiLargeVaBitMap,
                         MiLargeVaInUse,
                         (ULONG) sizeof (MiLargeVaInUse) * 8);

    ASSERT (MmNonPagedPoolEnd != NULL);

    KeInitializeSpinLock (&MiLargePageLock);

#if (_MI_PAGING_LEVELS>=4)

    PointerPpe = MiGetPxeAddress (MmNonPagedPoolEnd) + 1;

    while (PointerPpe->u.Long != 0) {
        PointerPpe += 1;
    }

    //
    // Allocate the top level extended page directory parent.
    //

    if (MiChargeCommitment (1, NULL) == FALSE) {
        RtlSetAllBits (&MiLargeVaBitMap);
        return;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_LARGE_VA_PAGES, 1);

    ASSERT (PointerPpe->u.Long == 0);
    PointerPpe->u.Long = MM_DEMAND_ZERO_WRITE_PTE;

    LOCK_PFN (OldIrql);

    if (MmAvailablePages < MM_HIGH_LIMIT) {
        MiEnsureAvailablePageOrWait (NULL, NULL, OldIrql);
    }

    PageColor = MI_PAGE_COLOR_VA_PROCESS (VirtualAddress,
                                          &MI_SYSTEM_PAGE_COLOR);

    PageFrameIndex = MiRemoveZeroPage (PageColor);

    MiInitializePfn (PageFrameIndex, PointerPpe, 1);

    UNLOCK_PFN (OldIrql);

    MI_MAKE_VALID_PTE (TempPte, PageFrameIndex, MM_READWRITE, PointerPpe);

    MI_SET_PTE_DIRTY (TempPte);

    MI_WRITE_VALID_PTE (PointerPpe, TempPte);

    PointerPpe = MiGetVirtualAddressMappedByPte (PointerPpe);

#else

    PointerPpe = MiGetPpeAddress (MmNonPagedPoolEnd) + 1;

    while (PointerPpe->u.Long != 0) {
        PointerPpe += 1;
    }

#endif

    MiLargeVaStart = MiGetVirtualAddressMappedByPpe (PointerPpe);

    NumberOfPages = (MI_LARGE_PAGE_VA_SPACE / MM_VA_MAPPED_BY_PPE);

    ASSERT (NumberOfPages != 0);

    if (MiChargeCommitment (NumberOfPages, NULL) == FALSE) {
        RtlSetAllBits (&MiLargeVaBitMap);
        return;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_LARGE_VA_PAGES, NumberOfPages);

    do {

        ASSERT (PointerPpe->u.Long == 0);
        PointerPpe->u.Long = MM_DEMAND_ZERO_WRITE_PTE;

        LOCK_PFN (OldIrql);

        if (MmAvailablePages < MM_HIGH_LIMIT) {
            MiEnsureAvailablePageOrWait (NULL, NULL, OldIrql);
        }

        PageColor = MI_PAGE_COLOR_VA_PROCESS (VirtualAddress,
                                              &MI_SYSTEM_PAGE_COLOR);

        PageFrameIndex = MiRemoveZeroPage (PageColor);

        MiInitializePfn (PageFrameIndex, PointerPpe, 1);

        UNLOCK_PFN (OldIrql);

        MI_MAKE_VALID_PTE (TempPte, PageFrameIndex, MM_READWRITE, PointerPpe);

        MI_SET_PTE_DIRTY (TempPte);

        MI_WRITE_VALID_PTE (PointerPpe, TempPte);

        PointerPpe += 1;

        NumberOfPages -= 1;

    } while (NumberOfPages != 0);

    RtlClearAllBits (&MiLargeVaBitMap);

#else

    //
    // Initialize process tracking so that large page system PTE mappings
    // can be rippled during creation/deletion.
    //

    MiLargePageHyperPte = MiReserveSystemPtes (1, SystemPteSpace);

    if (MiLargePageHyperPte == NULL) {
        MiIssueNoPtesBugcheck (1, SystemPteSpace);
    }

    MiLargePageHyperPte->u.Long = 0;

    InitializeListHead (&MmProcessList);

    InsertTailList (&MmProcessList, &PsGetCurrentProcess()->MmProcessLinks);

#endif

    return;
}

#if !defined (_WIN64)
PMMPTE MiInitialSystemPageDirectory;
#endif


PVOID
MiMapWithLargePages (
    IN PFN_NUMBER PageFrameIndex,
    IN PFN_NUMBER NumberOfPages,
    IN ULONG Protection,
    IN MEMORY_CACHING_TYPE CacheType
    )

/*++

Routine Description:

    This function maps the specified physical address into the non-pagable
    portion of the system address space using large TB entries.  If the range
    cannot be mapped using large TB entries then NULL is returned and the
    caller will map it with small TB entries.

Arguments:

    PageFrameIndex - Supplies the starting page frame index to map.

    NumberOfPages - Supplies the number of pages to map.

    Protection - Supplies the number of pages to map.

    CacheType - Supplies MmNonCached if the physical address is to be mapped
                as non-cached, MmCached if the address should be cached, and
                MmWriteCombined if the address should be cached and
                write-combined as a frame buffer which is to be used only by
                the video port driver.  All other callers should use
                MmUSWCCached.  MmUSWCCached is available only if the PAT
                feature is present and available.

                For I/O device registers, this is usually specified
                as MmNonCached.

Return Value:

    Returns the virtual address which maps the specified physical addresses.
    The value NULL is returned if sufficient large virtual address space for
    the mapping could not be found.

Environment:

    Kernel mode, Should be IRQL of APC_LEVEL or below, but unfortunately
    callers are coming in at DISPATCH_LEVEL and it's too late to change the
    rules now.  This means you can never make this routine pagable.

--*/

{
    MMPTE TempPde;
    PMMPTE PointerPde;
    PMMPTE LastPde;
    PVOID BaseVa;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;
    KIRQL OldIrql;
    LOGICAL IoMapping;
#if defined(_WIN64)
    ULONG StartPosition;
    ULONG NumberOfBits;
#else
    PMMPTE TargetPde;
    PMMPTE TargetPdeBase;
    PMMPTE PointerPdeBase;
    PFN_NUMBER PageDirectoryIndex;
    PEPROCESS Process;
    PEPROCESS CurrentProcess;
    PLIST_ENTRY NextEntry;
#endif
#if defined (_X86PAE_)
    ULONG i;
    PMMPTE PaeTop;
#endif

    ASSERT ((NumberOfPages % (MM_MINIMUM_VA_FOR_LARGE_PAGE >> PAGE_SHIFT)) == 0);
    ASSERT ((PageFrameIndex % (MM_MINIMUM_VA_FOR_LARGE_PAGE >> PAGE_SHIFT)) == 0);

#ifdef _X86_
    if ((KeFeatureBits & KF_LARGE_PAGE) == 0) {
        return NULL;
    }
#endif

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, TRUE);

    IoMapping = !MI_IS_PFN (PageFrameIndex);

#if defined(_WIN64)

    NumberOfBits = (ULONG)(NumberOfPages / (MM_MINIMUM_VA_FOR_LARGE_PAGE >> PAGE_SHIFT));

    ExAcquireSpinLock (&MiLargePageLock, &OldIrql);

    StartPosition = RtlFindClearBitsAndSet (&MiLargeVaBitMap,
                                            NumberOfBits,
                                            0);

    ExReleaseSpinLock (&MiLargePageLock, OldIrql);

    if (StartPosition == NO_BITS_FOUND) {
        return NULL;
    }

    BaseVa = (PVOID)((PCHAR)MiLargeVaStart + (StartPosition * MM_MINIMUM_VA_FOR_LARGE_PAGE));

    if (IoMapping) {

        CacheAttribute = MiInsertIoSpaceMap (BaseVa,
                                             PageFrameIndex,
                                             NumberOfPages,
                                             CacheAttribute);

        if (CacheAttribute == MiNotMapped) { 
            ExAcquireSpinLock (&MiLargePageLock, &OldIrql);
            RtlClearBits (&MiLargeVaBitMap, StartPosition, NumberOfBits);
            ExReleaseSpinLock (&MiLargePageLock, OldIrql);
            return NULL;
        }
    }

    PointerPde = MiGetPdeAddress (BaseVa);

#else

    PointerPde = MiReserveAlignedSystemPtes ((ULONG)NumberOfPages,
                                             SystemPteSpace,
                                             MM_MINIMUM_VA_FOR_LARGE_PAGE);

    if (PointerPde == NULL) {
        return NULL;
    }

    ASSERT (BYTE_OFFSET (PointerPde) == 0);

    BaseVa = MiGetVirtualAddressMappedByPte (PointerPde);
    ASSERT (((ULONG_PTR)BaseVa & (MM_VA_MAPPED_BY_PDE - 1)) == 0);

    if (IoMapping) {

        CacheAttribute = MiInsertIoSpaceMap (BaseVa,
                                             PageFrameIndex,
                                             NumberOfPages,
                                             CacheAttribute);

        if (CacheAttribute == MiNotMapped) { 
            MiReleaseSystemPtes (PointerPde,
                                 NumberOfPages,
                                 SystemPteSpace);
            return NULL;
        }
    }

    PointerPde = MiGetPteAddress (PointerPde);

    PointerPdeBase = PointerPde;

#endif

    MI_MAKE_VALID_PTE (TempPde,
                       PageFrameIndex,
                       Protection,
                       PointerPde);

    MI_SET_PTE_DIRTY (TempPde);
    MI_SET_ACCESSED_IN_PTE (&TempPde, 1);

#if defined(_X86PAE_)

    if (MiUseGlobalBitInLargePdes == TRUE) {
        TempPde.u.Hard.Global = 1;
    }

#elif defined(_X86_) || defined (_AMD64_)

    if (ValidKernelPde.u.Long & MM_PTE_GLOBAL_MASK) {
        TempPde.u.Hard.Global = 1;
    }

#endif

    MI_MAKE_PDE_MAP_LARGE_PAGE (&TempPde);

    switch (CacheAttribute) {

        case MiNonCached:
            MI_DISABLE_LARGE_PTE_CACHING (TempPde);
            break;

        case MiCached:
            break;

        case MiWriteCombined:
            MI_SET_LARGE_PTE_WRITE_COMBINE (TempPde);
            break;

        default:
            ASSERT (FALSE);
            break;
    }

    LastPde = PointerPde + (NumberOfPages / (MM_VA_MAPPED_BY_PDE >> PAGE_SHIFT));

    MI_PREPARE_FOR_NONCACHED (CacheAttribute);

#if defined(_WIN64)

    while (PointerPde < LastPde) {

        ASSERT (PointerPde->u.Long == 0);

        MI_WRITE_VALID_PTE (PointerPde, TempPde);

        TempPde.u.Hard.PageFrameNumber += (MM_VA_MAPPED_BY_PDE >> PAGE_SHIFT);

        PointerPde += 1;
    }

#else

    CurrentProcess = PsGetCurrentProcess ();

    LOCK_EXPANSION2 (OldIrql);

    NextEntry = MmProcessList.Flink;

    while (NextEntry != &MmProcessList) {

        Process = CONTAINING_RECORD (NextEntry, EPROCESS, MmProcessLinks);

        // Two process states must be carefully handled here -
        //
        // 1.  Processes that are just being created where they are still
        //     initializing their page directory, etc.
        //
        // 2.  Processes that are outswapped.
        //

        if (Process->Flags & PS_PROCESS_FLAGS_PDE_UPDATE_NEEDED) {

            // 
            // The process is further along in process creation or is still
            // outswapped.  Either way, an update is already queued so our
            // current changes will be processed later anyway before the process
            // can run so no need to do anything here.
            // 

            NOTHING;
        }
        else if (Process->Pcb.DirectoryTableBase[0] == 0) {

            //
            // This process is being created and there is no way to tell where
            // during creation it is (ie: it may be filling PDEs right now!).
            // So just mark the process as needing a PDE update at the
            // beginning of MmInitializeProcessAddressSpace.
            //

            PS_SET_BITS (&Process->Flags, PS_PROCESS_FLAGS_PDE_UPDATE_NEEDED);
        }
        else if (Process->Flags & PS_PROCESS_FLAGS_OUTSWAPPED) {

            // 
            // This process is outswapped.  Even though the page directory
            // may still be in transition, the process must be inswapped
            // before it can run again, so just mark the process as needing
            // a PDE update at that time.
            // 

            PS_SET_BITS (&Process->Flags, PS_PROCESS_FLAGS_PDE_UPDATE_NEEDED);
        }
        else {

            //
            // This process is resident so update the relevant PDEs in its
            // address space right now.
            //

            PointerPde = PointerPdeBase;
            TempPde.u.Hard.PageFrameNumber = PageFrameIndex;

#if !defined (_X86PAE_)
            PageDirectoryIndex = Process->Pcb.DirectoryTableBase[0] >> PAGE_SHIFT;
#else
            //
            // The range cannot cross PAE PDPTE entries, but we do need to
            // locate which entry it does lie in.
            //

            PaeTop = Process->PaeTop;

            i = (((ULONG_PTR) PointerPde - PDE_BASE) >> PAGE_SHIFT);
            ASSERT ((PaeTop + i)->u.Hard.Valid == 1);
            PageDirectoryIndex = (PFN_NUMBER)((PaeTop + i)->u.Hard.PageFrameNumber);
#endif

            TargetPdeBase = (PMMPTE) MiMapPageInHyperSpaceAtDpc (
                                                    CurrentProcess,
                                                    PageDirectoryIndex);

            TargetPde = (PMMPTE)((PCHAR) TargetPdeBase + BYTE_OFFSET (PointerPde));

            while (PointerPde < LastPde) {

                ASSERT (TargetPde->u.Long != 0);
                ASSERT (TargetPde->u.Hard.Valid != 0);

                *TargetPde = TempPde;

                TempPde.u.Hard.PageFrameNumber += (MM_VA_MAPPED_BY_PDE >> PAGE_SHIFT);

                PointerPde += 1;
                TargetPde += 1;
            }

            MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, TargetPdeBase);
        }

        NextEntry = NextEntry->Flink;
    }

    UNLOCK_EXPANSION2 (OldIrql);

#endif

    MI_SWEEP_CACHE (CacheAttribute, BaseVa, NumberOfPages << PAGE_SHIFT);

    //
    // Force all processors to use the latest mappings.
    //

    KeFlushEntireTb (TRUE, TRUE);

    return BaseVa;
}


VOID
MiUnmapLargePages (
    IN PVOID BaseAddress,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    This function unmaps a range of physical addresses which were previously
    mapped via MiMapWithLargePages.

Arguments:

    BaseAddress - Supplies the base virtual address where the physical
                  address was previously mapped.

    NumberOfBytes - Supplies the number of bytes which were mapped.

Return Value:

    None.

Environment:

    Kernel mode, Should be IRQL of APC_LEVEL or below, but unfortunately
    callers are coming in at DISPATCH_LEVEL and it's too late to change the
    rules now.  This means you can never make this routine pagable.

--*/

{
    PMMPTE PointerPde;
    PMMPTE LastPde;
    KIRQL OldIrql;
#if defined(_WIN64)
    ULONG StartPosition;
    ULONG NumberOfBits;
#else
    PMMPTE RestorePde;
    PMMPTE TargetPde;
    PMMPTE TargetPdeBase;
    PMMPTE PointerPdeBase;
    PFN_NUMBER PageDirectoryIndex;
    PEPROCESS Process;
    PEPROCESS CurrentProcess;
    PLIST_ENTRY NextEntry;
#endif
#if defined (_X86PAE_)
    PMMPTE PaeTop;
    ULONG i;
#endif

    ASSERT (NumberOfBytes != 0);

    ASSERT (((ULONG_PTR)BaseAddress % MM_MINIMUM_VA_FOR_LARGE_PAGE) == 0);
    ASSERT ((NumberOfBytes % MM_MINIMUM_VA_FOR_LARGE_PAGE) == 0);

#if defined(_WIN64)
    NumberOfBits = (ULONG)(NumberOfBytes / MM_MINIMUM_VA_FOR_LARGE_PAGE);

    StartPosition = (ULONG)(((ULONG_PTR)BaseAddress - (ULONG_PTR)MiLargeVaStart) / MM_MINIMUM_VA_FOR_LARGE_PAGE);

    ASSERT (RtlAreBitsSet (&MiLargeVaBitMap, StartPosition, NumberOfBits) == TRUE);
#endif

    PointerPde = MiGetPdeAddress (BaseAddress);

    LastPde = PointerPde + (NumberOfBytes / MM_VA_MAPPED_BY_PDE);

#if defined(_WIN64)

    while (PointerPde < LastPde) {

        ASSERT (PointerPde->u.Hard.Valid != 0);
        ASSERT (PointerPde->u.Long != 0);
        ASSERT (MI_PDE_MAPS_LARGE_PAGE (PointerPde));

        MI_WRITE_INVALID_PTE (PointerPde, ZeroKernelPte);

        PointerPde += 1;
    }

#else

    PointerPdeBase = PointerPde;

    CurrentProcess = PsGetCurrentProcess ();

    LOCK_EXPANSION2 (OldIrql);

    NextEntry = MmProcessList.Flink;

    while (NextEntry != &MmProcessList) {

        Process = CONTAINING_RECORD (NextEntry, EPROCESS, MmProcessLinks);

        // Two process states must be carefully handled here -
        //
        // 1.  Processes that are just being created where they are still
        //     initializing their page directory, etc.
        //
        // 2.  Processes that are outswapped.
        //

        if (Process->Flags & PS_PROCESS_FLAGS_PDE_UPDATE_NEEDED) {

            // 
            // The process is further along in process creation or is still
            // outswapped.  Either way, an update is already queued so our
            // current changes will be processed later anyway before the process
            // can run so no need to do anything here.
            // 

            NOTHING;
        }
        else if (Process->Pcb.DirectoryTableBase[0] == 0) {

            //
            // This process is being created and there is no way to tell where
            // during creation it is (ie: it may be filling PDEs right now!).
            // So just mark the process as needing a PDE update at the
            // beginning of MmInitializeProcessAddressSpace.
            //

            PS_SET_BITS (&Process->Flags, PS_PROCESS_FLAGS_PDE_UPDATE_NEEDED);
        }
        else if (Process->Flags & PS_PROCESS_FLAGS_OUTSWAPPED) {

            // 
            // This process is outswapped.  Even though the page directory
            // may still be in transition, the process must be inswapped
            // before it can run again, so just mark the process as needing
            // a PDE update at that time.
            // 

            PS_SET_BITS (&Process->Flags, PS_PROCESS_FLAGS_PDE_UPDATE_NEEDED);
        }
        else {

            //
            // This process is resident so update the relevant PDEs in its
            // address space right now.
            //

            PointerPde = PointerPdeBase;

#if !defined (_X86PAE_)
            PageDirectoryIndex = Process->Pcb.DirectoryTableBase[0] >> PAGE_SHIFT;
#else
            //
            // The range cannot cross PAE PDPTE entries, but we do need to
            // locate which entry it does lie in.
            //

            PaeTop = Process->PaeTop;

            i = (((ULONG_PTR) PointerPde - PDE_BASE) >> PAGE_SHIFT);
            ASSERT ((PaeTop + i)->u.Hard.Valid == 1);
            PageDirectoryIndex = (PFN_NUMBER)((PaeTop + i)->u.Hard.PageFrameNumber);
#endif

            TargetPdeBase = (PMMPTE) MiMapPageInHyperSpaceAtDpc (
                                                    CurrentProcess,
                                                    PageDirectoryIndex);

            TargetPde = (PMMPTE)((PCHAR) TargetPdeBase + BYTE_OFFSET (PointerPde));

            RestorePde = MiInitialSystemPageDirectory + (PointerPde - (PMMPTE)PDE_BASE);

            while (PointerPde < LastPde) {

                ASSERT (TargetPde->u.Long != 0);
                ASSERT (TargetPde->u.Hard.Valid != 0);
                ASSERT (RestorePde->u.Long != 0);
                ASSERT (RestorePde->u.Hard.Valid != 0);

                *TargetPde = *RestorePde;

                PointerPde += 1;
                TargetPde += 1;
                RestorePde += 1;
            }

            MiUnmapPageInHyperSpaceFromDpc (CurrentProcess, TargetPdeBase);
        }

        NextEntry = NextEntry->Flink;
    }

    UNLOCK_EXPANSION2 (OldIrql);

#endif

    //
    // Force all processors to use the latest mappings.
    //

    KeFlushEntireTb (TRUE, TRUE);

#if defined(_WIN64)

    ExAcquireSpinLock (&MiLargePageLock, &OldIrql);

    RtlClearBits (&MiLargeVaBitMap, StartPosition, NumberOfBits);

    ExReleaseSpinLock (&MiLargePageLock, OldIrql);

#else

    PointerPde = MiGetVirtualAddressMappedByPte (PointerPdeBase);

    MiReleaseSystemPtes (PointerPde,
                         (ULONG)(NumberOfBytes >> PAGE_SHIFT),
                         SystemPteSpace);

#endif

    return;
}


PVOID
MmMapIoSpace (
    IN PHYSICAL_ADDRESS PhysicalAddress,
    IN SIZE_T NumberOfBytes,
    IN MEMORY_CACHING_TYPE CacheType
    )

/*++

Routine Description:

    This function maps the specified physical address into the non-pagable
    portion of the system address space.

Arguments:

    PhysicalAddress - Supplies the starting physical address to map.

    NumberOfBytes - Supplies the number of bytes to map.

    CacheType - Supplies MmNonCached if the physical address is to be mapped
                as non-cached, MmCached if the address should be cached, and
                MmWriteCombined if the address should be cached and
                write-combined as a frame buffer which is to be used only by
                the video port driver.  All other callers should use
                MmUSWCCached.  MmUSWCCached is available only if the PAT
                feature is present and available.

                For I/O device registers, this is usually specified
                as MmNonCached.

Return Value:

    Returns the virtual address which maps the specified physical addresses.
    The value NULL is returned if sufficient virtual address space for
    the mapping could not be found.

Environment:

    Kernel mode, Should be IRQL of APC_LEVEL or below, but unfortunately
    callers are coming in at DISPATCH_LEVEL and it's too late to change the
    rules now.  This means you can never make this routine pagable.

--*/

{
    KIRQL OldIrql;
    CSHORT IoMapping;
    PMMPFN Pfn1;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER LastPageFrameIndex;
    PMMPTE PointerPte;
    PVOID BaseVa;
    MMPTE TempPte;
    PMDL TempMdl;
    PFN_NUMBER MdlHack[(sizeof(MDL)/sizeof(PFN_NUMBER)) + 1];
    PVOID CallingAddress;
    PVOID CallersCaller;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    //
    // For compatibility for when CacheType used to be passed as a BOOLEAN
    // mask off the upper bits (TRUE == MmCached, FALSE == MmNonCached).
    //

    CacheType &= 0xFF;

    if (CacheType >= MmMaximumCacheType) {
        return NULL;
    }

#if !defined (_MI_MORE_THAN_4GB_)
    ASSERT (PhysicalAddress.HighPart == 0);
#endif

    ASSERT (NumberOfBytes != 0);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (PhysicalAddress.LowPart,
                                                    NumberOfBytes);

    //
    // See if the first frame is in the PFN database and if so, they all must
    // be.  Note since the caller is mapping the frames, they must already be
    // locked so the PFN lock is not needed to protect against a hot-remove.
    //

    PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);

    IoMapping = (CSHORT) (!MI_IS_PFN (PageFrameIndex));

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, IoMapping);

    if (IoMapping) {

        //
        // If the size and start address are an exact multiple of the
        // minimum large page size, try to use large pages to map the request.
        //

        if (((PhysicalAddress.LowPart % MM_MINIMUM_VA_FOR_LARGE_PAGE) == 0) &&
            ((NumberOfBytes % MM_MINIMUM_VA_FOR_LARGE_PAGE) == 0)) {

            BaseVa = MiMapWithLargePages (PageFrameIndex,
                                          NumberOfPages,
                                          MM_EXECUTE_READWRITE,
                                          CacheType);

            if (BaseVa != NULL) {
                goto Done;
            }
        }

        Pfn1 = NULL;
    }
    else {
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
    }

    PointerPte = MiReserveSystemPtes ((ULONG)NumberOfPages, SystemPteSpace);

    if (PointerPte == NULL) {
        return NULL;
    }

    BaseVa = (PVOID)MiGetVirtualAddressMappedByPte (PointerPte);

    PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);

    if (Pfn1 == NULL) {
        CacheAttribute = MiInsertIoSpaceMap (BaseVa,
                                             PageFrameIndex,
                                             NumberOfPages,
                                             CacheAttribute);

        if (CacheAttribute == MiNotMapped) { 
            MiReleaseSystemPtes (PointerPte, (ULONG) NumberOfPages, SystemPteSpace);
            return NULL;
        }
    }

    if (CacheAttribute != MiCached) {

        //
        // If a noncachable mapping is requested, none of the pages in the
        // requested MDL can reside in a large page.  Otherwise we would be
        // creating an incoherent overlapping TB entry as the same physical
        // page would be mapped by 2 different TB entries with different
        // cache attributes.
        //

        LastPageFrameIndex = PageFrameIndex + NumberOfPages;

        LOCK_PFN2 (OldIrql);

        do {

            if (MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (PageFrameIndex)) {
                UNLOCK_PFN2 (OldIrql);
                MiNonCachedCollisions += 1;
                if (Pfn1 == NULL) {
                    MiRemoveIoSpaceMap (BaseVa, NumberOfPages);
                }
                MiReleaseSystemPtes (PointerPte,
                                     (ULONG) NumberOfPages,
                                     SystemPteSpace);
                return NULL;
            }

            PageFrameIndex += 1;

        } while (PageFrameIndex < LastPageFrameIndex);

        UNLOCK_PFN2 (OldIrql);
    }

    BaseVa = (PVOID)((PCHAR)BaseVa + BYTE_OFFSET(PhysicalAddress.LowPart));

    TempPte = ValidKernelPte;

    switch (CacheAttribute) {

        case MiNonCached:
            MI_DISABLE_CACHING (TempPte);
            break;

        case MiCached:
            break;

        case MiWriteCombined:
            MI_SET_PTE_WRITE_COMBINE (TempPte);
            break;

        default:
            ASSERT (FALSE);
            break;
    }

#if defined(_X86_)

    //
    // Set the physical range to the proper caching type.  If the PAT feature
    // is supported, then we just use the caching type in the PTE.  Otherwise
    // modify the MTRRs if applicable.
    //
    // Note if the cache request is for cached or noncached, don't waste
    // an MTRR on this range because the PTEs can be encoded to provide
    // equivalent functionality.
    //

    if ((MiWriteCombiningPtes == FALSE) && (CacheAttribute == MiWriteCombined)) {

        //
        // If the address is an I/O space address, use MTRRs if possible.
        //

        NTSTATUS Status;

        //
        // If the address is a memory address, don't risk using MTRRs because
        // other pages in the range are likely mapped with differing attributes
        // in the TB and we must not add a conflicting range.
        //

        if (Pfn1 != NULL) {
            if (Pfn1 == NULL) {
                MiRemoveIoSpaceMap (BaseVa, NumberOfPages);
            }
            MiReleaseSystemPtes(PointerPte, NumberOfPages, SystemPteSpace);
            return NULL;
        }

        //
        // Since the attribute may have been overridden (due to a collision
        // with a prior exiting mapping), make sure the CacheType is also
        // consistent before editing the MTRRs.
        //

        CacheType = MmWriteCombined;

        Status = KeSetPhysicalCacheTypeRange (PhysicalAddress,
                                              NumberOfBytes,
                                              CacheType);

        if (!NT_SUCCESS(Status)) {

            //
            // There's still a problem, fail the request.
            //

            if (Pfn1 == NULL) {
                MiRemoveIoSpaceMap (BaseVa, NumberOfPages);
            }
            MiReleaseSystemPtes(PointerPte, NumberOfPages, SystemPteSpace);
            return NULL;
        }

        //
        // Override the write combine (weak UC) bits in the PTE and
        // instead use a cached attribute.  This is because the processor
        // will use the least cachable (ie: functionally safer) attribute
        // of the PTE & MTRR to use - so specifying fully cached for the PTE
        // ensures that the MTRR value will win out.
        //

        TempPte = ValidKernelPte;
    }
#endif

    MI_ADD_EXECUTE_TO_VALID_PTE_IF_PAE (TempPte);

    PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);
    ASSERT ((Pfn1 == MI_PFN_ELEMENT (PageFrameIndex)) || (Pfn1 == NULL));

    OldIrql = HIGH_LEVEL;

    MI_PREPARE_FOR_NONCACHED (CacheAttribute);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 0);
        if (Pfn1 != NULL) {

            ASSERT ((Pfn1->u3.e2.ReferenceCount != 0) ||
                    ((Pfn1->u3.e1.Rom == 1) && (CacheType == MmCached)));

            TempPte = ValidKernelPte;

            MI_ADD_EXECUTE_TO_VALID_PTE_IF_PAE (TempPte);

            switch (Pfn1->u3.e1.CacheAttribute) {

                case MiCached:
                    if (CacheAttribute != MiCached) {

                        //
                        // The caller asked for a noncached or writecombined
                        // mapping, but the page is already mapped cached by
                        // someone else.  Override the caller's request in
                        // order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[0] += 1;
                    }
                    break;

                case MiNonCached:
                    if (CacheAttribute != MiNonCached) {

                        //
                        // The caller asked for a cached or writecombined
                        // mapping, but the page is already mapped noncached
                        // by someone else.  Override the caller's request
                        // in order to keep the TB page attribute coherent.
                        //

                        MiCacheOverride[1] += 1;
                    }
                    MI_DISABLE_CACHING (TempPte);
                    break;

                case MiWriteCombined:
                    if (CacheAttribute != MiWriteCombined) {

                        //
                        // The caller asked for a cached or noncached
                        // mapping, but the page is already mapped
                        // writecombined by someone else.  Override the
                        // caller's request in order to keep the TB page
                        // attribute coherent.
                        //

                        MiCacheOverride[2] += 1;
                    }
                    MI_SET_PTE_WRITE_COMBINE (TempPte);
                    break;

                case MiNotMapped:

                    //
                    // This better be for a page allocated with
                    // MmAllocatePagesForMdl.  Otherwise it might be a
                    // page on the freelist which could subsequently be
                    // given out with a different attribute !
                    //

#if defined (_MI_MORE_THAN_4GB_)
                    ASSERT ((Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
                            (Pfn1->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1)) ||
                            (Pfn1->u4.PteFrame == MI_MAGIC_4GB_RECLAIM));
#else
                    ASSERT ((Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME) ||
                            (Pfn1->PteAddress == (PVOID) (ULONG_PTR)(X64K | 0x1)));
#endif
                    if (OldIrql == HIGH_LEVEL) {
                        LOCK_PFN2 (OldIrql);
                    }

                    switch (CacheAttribute) {

                        case MiCached:
                            Pfn1->u3.e1.CacheAttribute = MiCached;
                            break;

                        case MiNonCached:
                            Pfn1->u3.e1.CacheAttribute = MiNonCached;
                            MI_DISABLE_CACHING (TempPte);
                            break;

                        case MiWriteCombined:
                            Pfn1->u3.e1.CacheAttribute = MiWriteCombined;
                            MI_SET_PTE_WRITE_COMBINE (TempPte);
                            break;

                        default:
                            ASSERT (FALSE);
                            break;
                    }
                    break;

                default:
                    ASSERT (FALSE);
                    break;
            }
            Pfn1 += 1;
        }
        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
        MI_WRITE_VALID_PTE (PointerPte, TempPte);
        PointerPte += 1;
        PageFrameIndex += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    if (OldIrql != HIGH_LEVEL) {
        UNLOCK_PFN2 (OldIrql);
    }

    MI_SWEEP_CACHE (CacheAttribute, BaseVa, NumberOfBytes);

Done:

    if (MmTrackPtes & 0x1) {

        RtlGetCallersAddress (&CallingAddress, &CallersCaller);

        TempMdl = (PMDL) MdlHack;
        TempMdl->MappedSystemVa = BaseVa;

        PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);
        *(PPFN_NUMBER)(TempMdl + 1) = PageFrameIndex;

        TempMdl->StartVa = (PVOID)(PAGE_ALIGN((ULONG_PTR)PhysicalAddress.QuadPart));
        TempMdl->ByteOffset = BYTE_OFFSET(PhysicalAddress.LowPart);
        TempMdl->ByteCount = (ULONG)NumberOfBytes;
    
        CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, IoMapping);

        MiInsertPteTracker (TempMdl,
                            1,
                            IoMapping,
                            CacheAttribute,
                            CallingAddress,
                            CallersCaller);
    }
    
    return BaseVa;
}

VOID
MmUnmapIoSpace (
     IN PVOID BaseAddress,
     IN SIZE_T NumberOfBytes
     )

/*++

Routine Description:

    This function unmaps a range of physical addresses which were previously
    mapped via an MmMapIoSpace function call.

Arguments:

    BaseAddress - Supplies the base virtual address where the physical
                  address was previously mapped.

    NumberOfBytes - Supplies the number of bytes which were mapped.

Return Value:

    None.

Environment:

    Kernel mode, Should be IRQL of APC_LEVEL or below, but unfortunately
    callers are coming in at DISPATCH_LEVEL and it's too late to change the
    rules now.  This means you can never make this routine pagable.

--*/

{
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;

    ASSERT (NumberOfBytes != 0);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES (BaseAddress, NumberOfBytes);

    if (MmTrackPtes & 0x1) {
        MiRemovePteTracker (NULL, BaseAddress, NumberOfPages);
    }

    PointerPde = MiGetPdeAddress (BaseAddress);

    if (MI_PDE_MAPS_LARGE_PAGE (PointerPde) == 0) {

        PointerPte = MiGetPteAddress (BaseAddress);

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

        if (!MI_IS_PFN (PageFrameIndex)) {

            //
            // The PTEs must be invalidated and the TB flushed *BEFORE*
            // removing the I/O space map.  This is because another
            // thread can immediately map the same I/O space with another
            // set of PTEs (and conflicting TB attributes) before we
            // call MiReleaseSystemPtes.
            //

            MiZeroMemoryPte (PointerPte, NumberOfPages);

            if (NumberOfPages == 1) {
                KeFlushSingleTb (BaseAddress, TRUE);
            }
            else {
                KeFlushEntireTb (TRUE, TRUE);
            }

            MiRemoveIoSpaceMap (BaseAddress, NumberOfPages);
        }

        MiReleaseSystemPtes (PointerPte, (ULONG)NumberOfPages, SystemPteSpace);
    }
    else {

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPde) +
                                   MiGetPteOffset (BaseAddress);

        if (!MI_IS_PFN (PageFrameIndex)) {
            MiRemoveIoSpaceMap (BaseAddress, NumberOfPages);
        }

        //
        // There is a race here because the I/O space mapping entry has been
        // removed but the TB has not yet been flushed (nor have the PDEs
        // been invalidated).  Another thread could request a map in between
        // the above removal and the below invalidation.  If this map occurs
        // where a driver provides the wrong page attribute it will not be
        // detected.  This is not worth closing as it is a driver bug anyway
        // and you really can't always stop them from hurting themselves if
        // they are determined to do so.  Note the alternative of invalidating
        // first isn't attractive because then the same PTEs could be
        // immediately reallocated and the new owner might want to add an
        // I/O space entry before this thread got to remove his.  So additional
        // lock serialization would need to be added.  Not worth it.
        //

        MiUnmapLargePages (BaseAddress, NumberOfBytes);
    }

    return;
}

PVOID
MiAllocateContiguousMemory (
    IN SIZE_T NumberOfBytes,
    IN PFN_NUMBER LowestAcceptablePfn,
    IN PFN_NUMBER HighestAcceptablePfn,
    IN PFN_NUMBER BoundaryPfn,
    IN MEMORY_CACHING_TYPE CacheType,
    PVOID CallingAddress
    )

/*++

Routine Description:

    This function allocates a range of physically contiguous non-paged
    pool.  It relies on the fact that non-paged pool is built at
    system initialization time from a contiguous range of physical
    memory.  It allocates the specified size of non-paged pool and
    then checks to ensure it is contiguous as pool expansion does
    not maintain the contiguous nature of non-paged pool.

    This routine is designed to be used by a driver's initialization
    routine to allocate a contiguous block of physical memory for
    issuing DMA requests from.

Arguments:

    NumberOfBytes - Supplies the number of bytes to allocate.

    LowestAcceptablePfn - Supplies the lowest page frame number
                          which is valid for the allocation.

    HighestAcceptablePfn - Supplies the highest page frame number
                           which is valid for the allocation.

    BoundaryPfn - Supplies the page frame number multiple the allocation must
                  not cross.  0 indicates it can cross any boundary.

    CacheType - Supplies the type of cache mapping that will be used for the
                memory.

    CallingAddress - Supplies the calling address of the allocator.

Return Value:

    NULL - a contiguous range could not be found to satisfy the request.

    NON-NULL - Returns a pointer (virtual address in the nonpaged portion
               of the system) to the allocated physically contiguous
               memory.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    PVOID BaseAddress;
    PFN_NUMBER SizeInPages;
    PFN_NUMBER LowestPfn;
    PFN_NUMBER HighestPfn;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    ASSERT (NumberOfBytes != 0);

    LowestPfn = LowestAcceptablePfn;

#if defined (_MI_MORE_THAN_4GB_)
    if (MiNoLowMemory != 0) {
        if (HighestAcceptablePfn < MiNoLowMemory) {

            return MiAllocateLowMemory (NumberOfBytes,
                                        LowestAcceptablePfn,
                                        HighestAcceptablePfn,
                                        BoundaryPfn,
                                        CallingAddress,
                                        CacheType,
                                        'tnoC');
        }
        LowestPfn = MiNoLowMemory;
    }
#endif

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, 0);

    //
    // N.B. This setting of SizeInPages to exactly the request size
    // means the non-NULL return value from MiCheckForContiguousMemory
    // is guaranteed to be the BaseAddress.  If this size is ever
    // changed, then the non-NULL return value must be checked and
    // split/returned accordingly.
    //

    SizeInPages = BYTES_TO_PAGES (NumberOfBytes);
    HighestPfn = HighestAcceptablePfn;

    if (CacheAttribute == MiCached) {

        BaseAddress = ExAllocatePoolWithTag (NonPagedPoolCacheAligned,
                                             NumberOfBytes,
                                             'mCmM');

        if (BaseAddress != NULL) {

            if (MiCheckForContiguousMemory (BaseAddress,
                                            SizeInPages,
                                            SizeInPages,
                                            LowestPfn,
                                            HighestPfn,
                                            BoundaryPfn,
                                            CacheAttribute)) {

                return BaseAddress;
            }

            //
            // The allocation from pool does not meet the contiguous
            // requirements.  Free the allocation and see if any of
            // the free pool pages do.
            //

            ExFreePool (BaseAddress);
        }
    }

    if (KeGetCurrentIrql() > APC_LEVEL) {
        return NULL;
    }

    BaseAddress = MiFindContiguousMemory (LowestPfn,
                                          HighestPfn,
                                          BoundaryPfn,
                                          SizeInPages,
                                          CacheType,
                                          CallingAddress);

    return BaseAddress;
}


PVOID
MmAllocateContiguousMemorySpecifyCache (
    IN SIZE_T NumberOfBytes,
    IN PHYSICAL_ADDRESS LowestAcceptableAddress,
    IN PHYSICAL_ADDRESS HighestAcceptableAddress,
    IN PHYSICAL_ADDRESS BoundaryAddressMultiple OPTIONAL,
    IN MEMORY_CACHING_TYPE CacheType
    )

/*++

Routine Description:

    This function allocates a range of physically contiguous non-cached,
    non-paged memory.  This is accomplished by using MmAllocateContiguousMemory
    which uses nonpaged pool virtual addresses to map the found memory chunk.

    Then this function establishes another map to the same physical addresses,
    but this alternate map is initialized as non-cached.  All references by
    our caller will be done through this alternate map.

    This routine is designed to be used by a driver's initialization
    routine to allocate a contiguous block of noncached physical memory for
    things like the AGP GART.

Arguments:

    NumberOfBytes - Supplies the number of bytes to allocate.

    LowestAcceptableAddress - Supplies the lowest physical address
                              which is valid for the allocation.  For
                              example, if the device can only reference
                              physical memory in the 8M to 16MB range, this
                              value would be set to 0x800000 (8Mb).

    HighestAcceptableAddress - Supplies the highest physical address
                               which is valid for the allocation.  For
                               example, if the device can only reference
                               physical memory below 16MB, this
                               value would be set to 0xFFFFFF (16Mb - 1).

    BoundaryAddressMultiple - Supplies the physical address multiple this
                              allocation must not cross.

Return Value:

    NULL - a contiguous range could not be found to satisfy the request.

    NON-NULL - Returns a pointer (virtual address in the nonpaged portion
               of the system) to the allocated physically contiguous
               memory.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    PVOID BaseAddress;
    PFN_NUMBER LowestPfn;
    PFN_NUMBER HighestPfn;
    PFN_NUMBER BoundaryPfn;
    PVOID CallingAddress;
    PVOID CallersCaller;

    RtlGetCallersAddress (&CallingAddress, &CallersCaller);

    ASSERT (NumberOfBytes != 0);

    LowestPfn = (PFN_NUMBER)(LowestAcceptableAddress.QuadPart >> PAGE_SHIFT);
    if (BYTE_OFFSET(LowestAcceptableAddress.LowPart)) {
        LowestPfn += 1;
    }

    if (BYTE_OFFSET(BoundaryAddressMultiple.LowPart)) {
        return NULL;
    }

    BoundaryPfn = (PFN_NUMBER)(BoundaryAddressMultiple.QuadPart >> PAGE_SHIFT);

    HighestPfn = (PFN_NUMBER)(HighestAcceptableAddress.QuadPart >> PAGE_SHIFT);

    if (HighestPfn > MmHighestPossiblePhysicalPage) {
        HighestPfn = MmHighestPossiblePhysicalPage;
    }

    if (LowestPfn > HighestPfn) {

        //
        // The caller's range is beyond what physically exists, it cannot
        // succeed.  Bail now to avoid an expensive fruitless search.
        //

        return NULL;
    }

    BaseAddress = MiAllocateContiguousMemory (NumberOfBytes,
                                              LowestPfn,
                                              HighestPfn,
                                              BoundaryPfn,
                                              CacheType,
                                              CallingAddress);

    return BaseAddress;
}

PVOID
MmAllocateContiguousMemory (
    IN SIZE_T NumberOfBytes,
    IN PHYSICAL_ADDRESS HighestAcceptableAddress
    )

/*++

Routine Description:

    This function allocates a range of physically contiguous non-paged pool.

    This routine is designed to be used by a driver's initialization
    routine to allocate a contiguous block of physical memory for
    issuing DMA requests from.

Arguments:

    NumberOfBytes - Supplies the number of bytes to allocate.

    HighestAcceptableAddress - Supplies the highest physical address
                               which is valid for the allocation.  For
                               example, if the device can only reference
                               physical memory in the lower 16MB this
                               value would be set to 0xFFFFFF (16Mb - 1).

Return Value:

    NULL - a contiguous range could not be found to satisfy the request.

    NON-NULL - Returns a pointer (virtual address in the nonpaged portion
               of the system) to the allocated physically contiguous
               memory.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    PFN_NUMBER HighestPfn;
    PVOID CallingAddress;
    PVOID VirtualAddress;
    PVOID CallersCaller;

    RtlGetCallersAddress (&CallingAddress, &CallersCaller);

    HighestPfn = (PFN_NUMBER)(HighestAcceptableAddress.QuadPart >> PAGE_SHIFT);

    if (HighestPfn > MmHighestPossiblePhysicalPage) {
        HighestPfn = MmHighestPossiblePhysicalPage;
    }

    VirtualAddress = MiAllocateContiguousMemory (NumberOfBytes,
                                                 0,
                                                 HighestPfn,
                                                 0,
                                                 MmCached,
                                                 CallingAddress);
            
    return VirtualAddress;
}

#if defined (_WIN64)
#define SPECIAL_POOL_ADDRESS(p) \
        ((((p) >= MmSpecialPoolStart) && ((p) < MmSpecialPoolEnd)) || \
        (((p) >= MmSessionSpecialPoolStart) && ((p) < MmSessionSpecialPoolEnd)))
#else
#define SPECIAL_POOL_ADDRESS(p) \
        (((p) >= MmSpecialPoolStart) && ((p) < MmSpecialPoolEnd))
#endif


VOID
MmFreeContiguousMemory (
    IN PVOID BaseAddress
    )

/*++

Routine Description:

    This function deallocates a range of physically contiguous non-paged
    pool which was allocated with the MmAllocateContiguousMemory function.

Arguments:

    BaseAddress - Supplies the base virtual address where the physical
                  address was previously mapped.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    ULONG SizeInPages;
    PMMPTE PointerPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER LastPage;
    PMMPFN Pfn1;
    PMMPFN StartPfn;

    PAGED_CODE();

#if defined (_MI_MORE_THAN_4GB_)
    if (MiNoLowMemory != 0) {
        if (MiFreeLowMemory (BaseAddress, 'tnoC') == TRUE) {
            return;
        }
    }
#endif

    if (((BaseAddress >= MmNonPagedPoolStart) &&
        (BaseAddress < (PVOID)((ULONG_PTR)MmNonPagedPoolStart + MmSizeOfNonPagedPoolInBytes))) ||

        ((BaseAddress >= MmNonPagedPoolExpansionStart) &&
        (BaseAddress < MmNonPagedPoolEnd)) ||

        (SPECIAL_POOL_ADDRESS(BaseAddress))) {

        ExFreePool (BaseAddress);
    }
    else {

        //
        // The contiguous memory being freed may be the target of a delayed
        // unlock.  Since these pages may be immediately released, force
        // any pending delayed actions to occur now.
        //

        MiDeferredUnlockPages (0);

        PointerPte = MiGetPteAddress (BaseAddress);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        if (Pfn1->u3.e1.StartOfAllocation == 0) {
            KeBugCheckEx (BAD_POOL_CALLER,
                          0x60,
                          (ULONG_PTR)BaseAddress,
                          0,
                          0);
        }

        StartPfn = Pfn1;
        Pfn1->u3.e1.StartOfAllocation = 0;
        Pfn1 -= 1;

        do {
            Pfn1 += 1;
            ASSERT (Pfn1->u3.e2.ReferenceCount == 1);
            ASSERT (Pfn1->u2.ShareCount == 1);
            ASSERT (Pfn1->PteAddress == PointerPte);
            ASSERT (Pfn1->OriginalPte.u.Long == MM_DEMAND_ZERO_WRITE_PTE);
            ASSERT (Pfn1->u4.PteFrame == MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress(PointerPte)));
            ASSERT (Pfn1->u3.e1.PageLocation == ActiveAndValid);
            ASSERT (Pfn1->u4.VerifierAllocation == 0);
            ASSERT (Pfn1->u3.e1.LargeSessionAllocation == 0);
            ASSERT (Pfn1->u3.e1.PrototypePte == 0);
            MI_SET_PFN_DELETED(Pfn1);
            PointerPte += 1;

        } while (Pfn1->u3.e1.EndOfAllocation == 0);

        Pfn1->u3.e1.EndOfAllocation = 0;

        SizeInPages = (ULONG)(Pfn1 - StartPfn + 1);

        //
        // Notify deadlock verifier that a region that can contain locks
        // will become invalid.
        //

        if (MmVerifierData.Level & DRIVER_VERIFIER_DEADLOCK_DETECTION) {
            VerifierDeadlockFreePool (BaseAddress, SizeInPages << PAGE_SHIFT);
        }

        //
        // Release the mapping.
        //

        MmUnmapIoSpace (BaseAddress, SizeInPages << PAGE_SHIFT);

        //
        // Release the actual pages.
        //

        LastPage = PageFrameIndex + SizeInPages;

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        LOCK_PFN (OldIrql);

        do {
            MiDecrementShareCount (Pfn1, PageFrameIndex);
            PageFrameIndex += 1;
            Pfn1 += 1;
        } while (PageFrameIndex < LastPage);

        UNLOCK_PFN (OldIrql);

        MI_INCREMENT_RESIDENT_AVAILABLE (SizeInPages, MM_RESAVAIL_FREE_CONTIGUOUS2);

        MiReturnCommitment (SizeInPages);
    }
}


VOID
MmFreeContiguousMemorySpecifyCache (
    IN PVOID BaseAddress,
    IN SIZE_T NumberOfBytes,
    IN MEMORY_CACHING_TYPE CacheType
    )

/*++

Routine Description:

    This function deallocates a range of noncached memory in
    the non-paged portion of the system address space.

Arguments:

    BaseAddress - Supplies the base virtual address where the noncached

    NumberOfBytes - Supplies the number of bytes allocated to the request.
                    This must be the same number that was obtained with
                    the MmAllocateContiguousMemorySpecifyCache call.

    CacheType - Supplies the cachetype used when the caller made the
                MmAllocateContiguousMemorySpecifyCache call.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    UNREFERENCED_PARAMETER (NumberOfBytes);
    UNREFERENCED_PARAMETER (CacheType);

    MmFreeContiguousMemory (BaseAddress);
}



PVOID
MmAllocateIndependentPages (
    IN SIZE_T NumberOfBytes,
    IN ULONG Node
    )

/*++

Routine Description:

    This function allocates a range of virtually contiguous nonpaged pages
    without using superpages.  This allows the caller to apply independent
    page protections to each page.

Arguments:

    NumberOfBytes - Supplies the number of bytes to allocate.

    Node - Supplies the preferred node number for the backing physical pages.
           If pages on the preferred node are not available, any page will
           be used.  -1 indicates no preferred node.

Return Value:

    The virtual address of the memory or NULL if none could be allocated.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    ULONG PageColor;
    PFN_NUMBER NumberOfPages;
    PMMPTE PointerPte;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PVOID BaseAddress;
    KIRQL OldIrql;

    ASSERT ((Node == (ULONG)-1) || (Node < KeNumberNodes));

    NumberOfPages = BYTES_TO_PAGES (NumberOfBytes);

    PointerPte = MiReserveSystemPtes ((ULONG)NumberOfPages, SystemPteSpace);

    if (PointerPte == NULL) {
        return NULL;
    }

    if (MiChargeCommitment (NumberOfPages, NULL) == FALSE) {
        MiReleaseSystemPtes (PointerPte, (ULONG)NumberOfPages, SystemPteSpace);
        return NULL;
    }

    BaseAddress = (PVOID)MiGetVirtualAddressMappedByPte (PointerPte);

    LOCK_PFN (OldIrql);

    if ((SPFN_NUMBER)NumberOfPages > MI_NONPAGABLE_MEMORY_AVAILABLE()) {
        UNLOCK_PFN (OldIrql);
        MiReturnCommitment (NumberOfPages);
        MiReleaseSystemPtes (PointerPte, (ULONG)NumberOfPages, SystemPteSpace);
        return NULL;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_INDEPENDENT_PAGES, NumberOfPages);

    MI_DECREMENT_RESIDENT_AVAILABLE (NumberOfPages, MM_RESAVAIL_ALLOCATE_INDEPENDENT);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 0);

        if (MmAvailablePages < MM_HIGH_LIMIT) {
            MiEnsureAvailablePageOrWait (NULL, NULL, OldIrql);
        }

        if (Node == (ULONG)-1) {
            PageColor = MI_GET_PAGE_COLOR_FROM_PTE (PointerPte);
        }
        else {
            PageColor = (((MI_SYSTEM_PAGE_COLOR++) & MmSecondaryColorMask) |
                           (Node << MmSecondaryColorNodeShift));
        }

        PageFrameIndex = MiRemoveAnyPage (PageColor);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           MM_READWRITE,
                           PointerPte);

        MI_SET_PTE_DIRTY (TempPte);
        MI_WRITE_VALID_PTE (PointerPte, TempPte);
        MiInitializePfn (PageFrameIndex, PointerPte, 1);

        PointerPte += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    UNLOCK_PFN (OldIrql);

    NumberOfPages = BYTES_TO_PAGES (NumberOfBytes);

    return BaseAddress;
}

BOOLEAN
MmSetPageProtection (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes,
    IN ULONG NewProtect
    )

/*++

Routine Description:

    This function sets the specified virtual address range to the desired
    protection.  This assumes that the virtual addresses are backed by PTEs
    which can be set (ie: not in kseg0 or large pages).

Arguments:

    VirtualAddress - Supplies the start address to protect.

    NumberOfBytes - Supplies the number of bytes to set.

    NewProtect - Supplies the protection to set the pages to (PAGE_XX).

Return Value:

    TRUE if the protection was applied, FALSE if not.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PFN_NUMBER i;
    PFN_NUMBER NumberOfPages;
    PMMPTE PointerPte;
    MMPTE TempPte;
    MMPTE NewPteContents;
    KIRQL OldIrql;
    ULONG ProtectionMask;
    MMPTE_FLUSH_LIST PteFlushList;

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    if (MI_IS_PHYSICAL_ADDRESS(VirtualAddress)) {
        return FALSE;
    }

    ProtectionMask = MiMakeProtectionMask (NewProtect);
    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return FALSE;
    }

    PointerPte = MiGetPteAddress (VirtualAddress);
    NumberOfPages = BYTES_TO_PAGES (NumberOfBytes);
    ASSERT (NumberOfPages != 0);

    PteFlushList.Count = 0;

    LOCK_PFN (OldIrql);

    for (i = 0; i < NumberOfPages; i += 1) {

        TempPte = *PointerPte;

        MI_MAKE_VALID_PTE (NewPteContents,
                           TempPte.u.Hard.PageFrameNumber,
                           ProtectionMask,
                           PointerPte);

        NewPteContents.u.Hard.Dirty = TempPte.u.Hard.Dirty;

        MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, NewPteContents);

        if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
            PteFlushList.FlushVa[PteFlushList.Count] =
                (PVOID)((PUCHAR)VirtualAddress + (i << PAGE_SHIFT));
            PteFlushList.Count += 1;
        }

        PointerPte += 1;
    }

    ASSERT (PteFlushList.Count != 0);

    MiFlushPteList (&PteFlushList, TRUE);

    UNLOCK_PFN (OldIrql);

    return TRUE;
}

VOID
MmFreeIndependentPages (
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    Returns pages previously allocated with MmAllocateIndependentPages.

Arguments:

    VirtualAddress - Supplies the virtual address to free.

    NumberOfBytes - Supplies the number of bytes to free.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    MMPTE PteContents;
    PMMPTE PointerPte;
    PMMPTE BasePte;
    PMMPTE EndPte;
    PMMPFN Pfn1;
    PMMPFN Pfn2;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    NumberOfPages = BYTES_TO_PAGES (NumberOfBytes);

    PointerPte = MiGetPteAddress (VirtualAddress);
    BasePte = PointerPte;
    EndPte = PointerPte + NumberOfPages;

    LOCK_PFN (OldIrql);

    do {

        PteContents = *PointerPte;

        ASSERT (PteContents.u.Hard.Valid == 1);

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        Pfn2 = MI_PFN_ELEMENT (Pfn1->u4.PteFrame);

        MiDecrementShareCount (Pfn2, Pfn1->u4.PteFrame);

        MI_SET_PFN_DELETED (Pfn1);
        MiDecrementShareCount (Pfn1, PageFrameIndex);

        PointerPte += 1;

    } while (PointerPte < EndPte);

    //
    // Update the count of resident available pages.
    //

    UNLOCK_PFN (OldIrql);

    MI_INCREMENT_RESIDENT_AVAILABLE (NumberOfPages, MM_RESAVAIL_FREE_INDEPENDENT);

    //
    // Return PTEs and commitment.
    //

    MiReleaseSystemPtes (BasePte, (ULONG)NumberOfPages, SystemPteSpace);

    MiReturnCommitment (NumberOfPages);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_INDEPENDENT_PAGES, NumberOfPages);
}


VOID
MiZeroAwePageWorker (
    IN PVOID Context
    )

/*++

Routine Description:

    This routine is the worker routine executed by all processors to
    fan out page zeroing for AWE allocations.

Arguments:

    Context - Supplies a pointer to the workitem.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
#if defined(MI_MULTINODE) 
    LOGICAL SetIdeal;
    ULONG Processor;
    PEPROCESS DefaultProcess;
#endif
    PKTHREAD Thread;
    KPRIORITY OldPriority;
    PMMPFN Pfn1;
    SCHAR OldBasePriority;
    PMMPFN PfnNextColored;
    PCOLORED_PAGE_INFO ColoredPageInfo;
    MMPTE TempPte;
    PMMPTE BasePte;
    PMMPTE PointerPte;
    PVOID VirtualAddress;
    PFN_NUMBER PageFrameIndex;
    PFN_COUNT i;
    PFN_COUNT RequestedPtes;

    ColoredPageInfo = (PCOLORED_PAGE_INFO) Context;

    //
    // Use the initiating thread's priority instead of the default system
    // thread priority.
    //

    Thread = KeGetCurrentThread ();
    OldBasePriority = Thread->BasePriority;
    Thread->BasePriority = ColoredPageInfo->BasePriority;
    OldPriority = KeSetPriorityThread (Thread, Thread->BasePriority);

    //
    // Dispatch each worker thread to a processor local to the memory being
    // zeroed.
    //

#if defined(MI_MULTINODE) 
    Processor = 0;

    if (PsInitialSystemProcess != NULL) {
        DefaultProcess = PsInitialSystemProcess;
    }
    else {
        DefaultProcess = PsIdleProcess;
    }

    if (ColoredPageInfo->Affinity != DefaultProcess->Pcb.Affinity) {

        KeFindFirstSetLeftAffinity (ColoredPageInfo->Affinity, &Processor);
        Processor = (CCHAR) KeSetIdealProcessorThread (Thread,
                                                       (CCHAR) Processor);

        SetIdeal = TRUE;
    }
    else {
        SetIdeal = FALSE;
    }
#endif

    Pfn1 = ColoredPageInfo->PfnAllocation;

    ASSERT (Pfn1 != (PMMPFN) MM_EMPTY_LIST);
    ASSERT (ColoredPageInfo->PagesQueued != 0);

    //
    // Zero all argument pages.
    //

    do {

        ASSERT (ColoredPageInfo->PagesQueued != 0);

        RequestedPtes = ColoredPageInfo->PagesQueued;

#if !defined (_WIN64)

        //
        // NT64 has an abundance of PTEs so try to map the whole request with
        // a single call.  For NT32, this resource needs to be carefully shared.
        //

        if (RequestedPtes > (1024 * 1024) / PAGE_SIZE) {
            RequestedPtes = (1024 * 1024) / PAGE_SIZE;
        }
#endif

        do {
            BasePte = MiReserveSystemPtes (RequestedPtes, SystemPteSpace);

            if (BasePte != NULL) {
                break;
            }

            RequestedPtes >>= 1;

        } while (RequestedPtes != 0);

        if (BasePte != NULL) {

            //
            // Able to get a reasonable chunk, go for a big zero.
            //

            PointerPte = BasePte;

            MI_MAKE_VALID_PTE (TempPte,
                               0,
                               MM_READWRITE,
                               PointerPte);

            MI_SET_PTE_DIRTY (TempPte);

            for (i = 0; i < RequestedPtes; i += 1) {

                ASSERT (Pfn1 != (PMMPFN) MM_EMPTY_LIST);

                PageFrameIndex = MI_PFN_ELEMENT_TO_INDEX (Pfn1);

                ASSERT (PointerPte->u.Hard.Valid == 0);

                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

                MI_WRITE_VALID_PTE (PointerPte, TempPte);

                PfnNextColored = (PMMPFN) (ULONG_PTR) Pfn1->OriginalPte.u.Long;
                Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
                Pfn1 = PfnNextColored;

                PointerPte += 1;
            }

            ColoredPageInfo->PagesQueued -= RequestedPtes;

            VirtualAddress = MiGetVirtualAddressMappedByPte (BasePte);

            KeZeroPages (VirtualAddress, ((ULONG_PTR)RequestedPtes) << PAGE_SHIFT);

            MiReleaseSystemPtes (BasePte, RequestedPtes, SystemPteSpace);
        }
        else {

            //
            // No PTEs left, zero a single page at a time.
            //

            MiZeroPhysicalPage (MI_PFN_ELEMENT_TO_INDEX (Pfn1), 0);

            PfnNextColored = (PMMPFN) (ULONG_PTR) Pfn1->OriginalPte.u.Long;
            Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
            Pfn1 = PfnNextColored;

            ColoredPageInfo->PagesQueued -= 1;
        }

    } while (Pfn1 != (PMMPFN) MM_EMPTY_LIST);

    //
    // Let the initiator know we've zeroed our share.
    //

    KeSetEvent (&ColoredPageInfo->Event, 0, FALSE);

    //
    // Restore the entry thread priority and ideal processor - this is critical
    // if we were called directly from the initiator instead of in the
    // context of a system thread.
    //

#if defined(MI_MULTINODE) 
    if (SetIdeal == TRUE) {
        KeSetIdealProcessorThread (Thread, (CCHAR) Processor);
    }
#endif

    KeSetPriorityThread (Thread, OldPriority);
    Thread->BasePriority = OldBasePriority;

    return;
}

VOID
MiZeroInParallel (
    IN PCOLORED_PAGE_INFO ColoredPageInfoBase
    )

/*++

Routine Description:

    This routine zeroes all the free & standby pages, fanning out the work.
    This is done even on UP machines because the worker thread code maps
    large MDLs and is thus better performing than zeroing a single
    page at a time.

Arguments:

    ColoredPageInfoBase - Supplies the informational structure about which
                          pages to zero.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.  The caller must lock down the
    PAGELK section that this routine resides in.

--*/

{
#if defined(MI_MULTINODE) 
    ULONG i;
#endif
    ULONG WaitCount;
    PKEVENT WaitObjects[MAXIMUM_WAIT_OBJECTS];
    KWAIT_BLOCK WaitBlockArray[MAXIMUM_WAIT_OBJECTS];
    KPRIORITY OldPriority;
    SCHAR OldBasePriority;
    NTSTATUS WakeupStatus;
    PETHREAD EThread;
    PKTHREAD Thread;
    OBJECT_ATTRIBUTES ObjectAttributes;
    HANDLE ThreadHandle;
    SCHAR WorkerThreadPriority;
    PCOLORED_PAGE_INFO ColoredPageInfo;
    ULONG Color;
    PMMPFN Pfn1;
    NTSTATUS Status;

    WaitCount = 0;

    EThread = PsGetCurrentThread ();
    Thread = &EThread->Tcb;

    //
    // Raise our priority to the highest non-realtime priority.  This
    // usually allows us to spawn all our worker threads without
    // interruption from them, but also doesn't starve the modified or
    // mapped page writers because we are going to access pagable code
    // and data below and during the spawning.
    //

    OldBasePriority = Thread->BasePriority;
    ASSERT ((OldBasePriority >= 1) || (InitializationPhase == 0));
    Thread->BasePriority = LOW_REALTIME_PRIORITY - 1;
    OldPriority = KeSetPriorityThread (Thread, LOW_REALTIME_PRIORITY - 1);
    WorkerThreadPriority = OldBasePriority;

    for (Color = 0; Color < MmSecondaryColors; Color += 1) {

        ColoredPageInfo = &ColoredPageInfoBase[Color];

        Pfn1 = ColoredPageInfo->PfnAllocation;

        if (Pfn1 != (PMMPFN) MM_EMPTY_LIST) {

            //
            // Assume no memory penalty for non-local memory on this
            // machine so there's no need to run with restricted affinity.
            //

            ColoredPageInfo->BasePriority = WorkerThreadPriority;

#if defined(MI_MULTINODE) 

            if (PsInitialSystemProcess != NULL) {
                ColoredPageInfo->Affinity = PsInitialSystemProcess->Pcb.Affinity;
            }
            else {
                ColoredPageInfo->Affinity = PsIdleProcess->Pcb.Affinity;
            }

            for (i = 0; i < KeNumberNodes; i += 1) {

                if (KeNodeBlock[i]->Color == (Color >> MmSecondaryColorNodeShift)) {
                    ColoredPageInfo->Affinity = KeNodeBlock[i]->ProcessorMask;
                    break;
                }
            }

#endif

            KeInitializeEvent (&ColoredPageInfo->Event,
                               SynchronizationEvent,
                               FALSE);

            //
            // Don't spawn threads to zero the memory if we are a system
            // thread.  This is because this routine can be called by
            // drivers in the context of the segment dereference thread, so
            // referencing pagable memory here can cause a deadlock.
            //

            if ((IS_SYSTEM_THREAD (EThread)) || (InitializationPhase == 0)) {
                MiZeroAwePageWorker ((PVOID) ColoredPageInfo);
            }
            else {

                InitializeObjectAttributes (&ObjectAttributes,
                                            NULL,
                                            0,
                                            NULL,
                                            NULL);

                //
                // We are creating a system thread for each memory
                // node instead of using the executive worker thread
                // pool.   This is because we want to run the threads
                // at a lower priority to keep the machine responsive
                // during all this zeroing.  And doing this to a worker
                // thread can cause a deadlock as various other
                // components (registry, etc) expect worker threads to be
                // available at a higher priority right away.
                //

                Status = PsCreateSystemThread (&ThreadHandle,
                                               THREAD_ALL_ACCESS,
                                               &ObjectAttributes,
                                               0L,
                                               NULL,
                                               MiZeroAwePageWorker,
                                               (PVOID) ColoredPageInfo);

                if (NT_SUCCESS(Status)) {
                    ZwClose (ThreadHandle);
                }
                else {
                    MiZeroAwePageWorker ((PVOID) ColoredPageInfo);
                }
            }

            WaitObjects[WaitCount] = &ColoredPageInfo->Event;

            WaitCount += 1;

            if (WaitCount == MAXIMUM_WAIT_OBJECTS) {

                //
                // Done issuing the first round of workitems,
                // lower priority & wait.
                //

                KeSetPriorityThread (Thread, OldPriority);
                Thread->BasePriority = OldBasePriority;

                WakeupStatus = KeWaitForMultipleObjects (WaitCount,
                                                         &WaitObjects[0],
                                                         WaitAll,
                                                         Executive,
                                                         KernelMode,
                                                         FALSE,
                                                         NULL,
                                                         &WaitBlockArray[0]);
                ASSERT (WakeupStatus == STATUS_SUCCESS);

                WaitCount = 0;

                Thread->BasePriority = LOW_REALTIME_PRIORITY - 1;
                KeSetPriorityThread (Thread, LOW_REALTIME_PRIORITY - 1);
            }
        }
    }

    //
    // Done issuing all the workitems, lower priority & wait.
    //

    KeSetPriorityThread (Thread, OldPriority);
    Thread->BasePriority = OldBasePriority;

    if (WaitCount != 0) {

        WakeupStatus = KeWaitForMultipleObjects (WaitCount,
                                                 &WaitObjects[0],
                                                 WaitAll,
                                                 Executive,
                                                 KernelMode,
                                                 FALSE,
                                                 NULL,
                                                 &WaitBlockArray[0]);

        ASSERT (WakeupStatus == STATUS_SUCCESS);
    }

    return;
}


PMDL
MmAllocatePagesForMdl (
    IN PHYSICAL_ADDRESS LowAddress,
    IN PHYSICAL_ADDRESS HighAddress,
    IN PHYSICAL_ADDRESS SkipBytes,
    IN SIZE_T TotalBytes
    )

/*++

Routine Description:

    This routine searches the PFN database for free, zeroed or standby pages
    to satisfy the request.  This does not map the pages - it just allocates
    them and puts them into an MDL.  It is expected that our caller will
    map the MDL as needed.

    NOTE: this routine may return an MDL mapping a smaller number of bytes
    than the amount requested.  It is the caller's responsibility to check the
    MDL upon return for the size actually allocated.

    These pages comprise physical non-paged memory and are zero-filled.

    This routine is designed to be used by an AGP driver to obtain physical
    memory in a specified range since hardware may provide substantial
    performance wins depending on where the backing memory is allocated.

    Because the caller may use these pages for a noncached mapping, care is
    taken to never allocate any pages that reside in a large page (in order
    to prevent TB incoherency of the same page being mapped by multiple
    translations with different attributes).

Arguments:

    LowAddress - Supplies the low physical address of the first range that
                 the allocated pages can come from.

    HighAddress - Supplies the high physical address of the first range that
                  the allocated pages can come from.

    SkipBytes - Number of bytes to skip (from the Low Address) to get to the
                next physical address range that allocated pages can come from.

    TotalBytes - Supplies the number of bytes to allocate.

Return Value:

    MDL - An MDL mapping a range of pages in the specified range.
          This may map less memory than the caller requested if the full amount
          is not currently available.

    NULL - No pages in the specified range OR not enough virtually contiguous
           nonpaged pool for the MDL is available at this time.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMDL MemoryDescriptorList;
    PMDL MemoryDescriptorList2;
    PMMPFN Pfn1;
    PMMPFN PfnNextColored;
    PMMPFN PfnNextFlink;
    PMMPFN PfnLastColored;
    KIRQL OldIrql;
    PFN_NUMBER start;
    PFN_NUMBER Page;
    PFN_NUMBER NextPage;
    PFN_NUMBER found;
    PFN_NUMBER BasePage;
    PFN_NUMBER LowPage;
    PFN_NUMBER HighPage;
    PFN_NUMBER SizeInPages;
    PFN_NUMBER MdlPageSpan;
    PFN_NUMBER SkipPages;
    PFN_NUMBER MaxPages;
    PFN_NUMBER PagesExamined;
    PPFN_NUMBER MdlPage;
    ULONG Color;
    PMMCOLOR_TABLES ColorHead;
    MMLISTS MemoryList;
    PFN_NUMBER LowPage1;
    PFN_NUMBER HighPage1;
    LOGICAL PagePlacementOk;
    PFN_NUMBER PageNextColored;
    PFN_NUMBER PageNextFlink;
    PFN_NUMBER PageLastColored;
    PMMPFNLIST ListHead;
    PCOLORED_PAGE_INFO ColoredPageInfoBase;
    PCOLORED_PAGE_INFO ColoredPageInfo;
    ULONG ColorHeadsDrained;
    ULONG NodePassesLeft;
    ULONG ColorCount;
    ULONG BaseColor;
    PFN_NUMBER ZeroCount;
#if DBG
    PPFN_NUMBER LastMdlPage;
    ULONG FinishedCount;
    PEPROCESS Process;
#endif

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    //
    // The skip increment must be a page-size multiple.
    //

    if (BYTE_OFFSET(SkipBytes.LowPart)) {
        return NULL;
    }

    LowPage = (PFN_NUMBER)(LowAddress.QuadPart >> PAGE_SHIFT);
    HighPage = (PFN_NUMBER)(HighAddress.QuadPart >> PAGE_SHIFT);

    if (HighPage > MmHighestPossiblePhysicalPage) {
        HighPage = MmHighestPossiblePhysicalPage;
    }

    //
    // Maximum allocation size is constrained by the MDL ByteCount field.
    //

    if (TotalBytes > (SIZE_T)((ULONG)(MAXULONG - PAGE_SIZE))) {
        TotalBytes = (SIZE_T)((ULONG)(MAXULONG - PAGE_SIZE));
    }

    SizeInPages = (PFN_NUMBER)ADDRESS_AND_SIZE_TO_SPAN_PAGES(0, TotalBytes);

    SkipPages = (PFN_NUMBER)(SkipBytes.QuadPart >> PAGE_SHIFT);

    BasePage = LowPage;

    //
    // Check without the PFN lock as the actual number of pages to get will
    // be recalculated later while holding the lock.
    //

    MaxPages = MI_NONPAGABLE_MEMORY_AVAILABLE() - 1024;

    if ((SPFN_NUMBER)MaxPages <= 0) {
        SizeInPages = 0;
    }
    else if (SizeInPages > MaxPages) {
        SizeInPages = MaxPages;
    }

    if (SizeInPages == 0) {
        return NULL;
    }

#if DBG
    if (SizeInPages < (PFN_NUMBER)ADDRESS_AND_SIZE_TO_SPAN_PAGES(0, TotalBytes)) {
        if (MiPrintAwe != 0) {
            DbgPrint("MmAllocatePagesForMdl1: unable to get %p pages, trying for %p instead\n",
                ADDRESS_AND_SIZE_TO_SPAN_PAGES(0, TotalBytes),
                SizeInPages);
        }
    }
#endif

    //
    // Allocate an MDL to return the pages in.
    //

    do {
        MemoryDescriptorList = MmCreateMdl (NULL,
                                            NULL,
                                            SizeInPages << PAGE_SHIFT);
    
        if (MemoryDescriptorList != NULL) {
            break;
        }
        SizeInPages -= (SizeInPages >> 4);
    } while (SizeInPages != 0);

    if (MemoryDescriptorList == NULL) {
        return NULL;
    }

    //
    // Ensure there is enough commit prior to allocating the pages.
    //

    if (MiChargeCommitment (SizeInPages, NULL) == FALSE) {
        ExFreePool (MemoryDescriptorList);
        return NULL;
    }

    //
    // Allocate a list of colored anchors.
    //

    ColoredPageInfoBase = (PCOLORED_PAGE_INFO) ExAllocatePoolWithTag (
                                NonPagedPool,
                                MmSecondaryColors * sizeof (COLORED_PAGE_INFO),
                                'ldmM');

    if (ColoredPageInfoBase == NULL) {
        ExFreePool (MemoryDescriptorList);
        MiReturnCommitment (SizeInPages);
        return NULL;
    }

    for (Color = 0; Color < MmSecondaryColors; Color += 1) {
        ColoredPageInfoBase[Color].PfnAllocation = (PMMPFN) MM_EMPTY_LIST;
        ColoredPageInfoBase[Color].PagesQueued = 0;
    }

    MdlPageSpan = SizeInPages;

    //
    // Recalculate the total size while holding the PFN lock.
    //

    start = 0;
    found = 0;
    ZeroCount = 0;

    MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    MmLockPagableSectionByHandle (ExPageLockHandle);

    KeAcquireGuardedMutex (&MmDynamicMemoryMutex);

    LOCK_PFN (OldIrql);

    MiDeferredUnlockPages (MI_DEFER_PFN_HELD);

    MaxPages = MI_NONPAGABLE_MEMORY_AVAILABLE() - 1024;

    if ((SPFN_NUMBER)MaxPages <= 0) {
        SizeInPages = 0;
    }
    else if (SizeInPages > MaxPages) {
        SizeInPages = MaxPages;
    }

    //
    // Systems utilizing memory compression may have more pages on the zero,
    // free and standby lists than we want to give out.  Explicitly check
    // MmAvailablePages instead (and recheck whenever the PFN lock is released
    // and reacquired).
    //

    if ((SPFN_NUMBER)SizeInPages > (SPFN_NUMBER)(MmAvailablePages - MM_HIGH_LIMIT)) {
        if (MmAvailablePages > MM_HIGH_LIMIT) {
            SizeInPages = MmAvailablePages - MM_HIGH_LIMIT;
        }
        else {
            SizeInPages = 0;
        }
    }

    if (SizeInPages == 0) {
        UNLOCK_PFN (OldIrql);
        KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
        MmUnlockPagableImageSection (ExPageLockHandle);
        ExFreePool (MemoryDescriptorList);
        MiReturnCommitment (MdlPageSpan);
        ExFreePool (ColoredPageInfoBase);
        return NULL;
    }

    MM_TRACK_COMMIT (MM_DBG_COMMIT_MDL_PAGES, SizeInPages);

    //
    // Charge resident available pages now for all the pages so the PFN lock
    // can be released between the loops below.  Excess charging is returned
    // at the conclusion of the loops.
    //

    InterlockedExchangeAddSizeT (&MmMdlPagesAllocated, SizeInPages);

    MI_DECREMENT_RESIDENT_AVAILABLE (SizeInPages, MM_RESAVAIL_ALLOCATE_FOR_MDL);

    //
    // Grab all zeroed (and then free) pages first directly from the
    // colored lists to avoid multiple walks down these singly linked lists.
    // Then snatch transition pages as needed.  In addition to optimizing
    // the speed of the removals this also avoids cannibalizing the page
    // cache unless it's absolutely needed.
    //

    NodePassesLeft = 1;
    ColorCount = MmSecondaryColors;
    BaseColor = 0;

#if defined(MI_MULTINODE) 

    if (KeNumberNodes > 1) {

        PKNODE Node;

        Node = KeGetCurrentNode ();

        if ((Node->FreeCount[ZeroedPageList]) ||
            (Node->FreeCount[FreePageList])) {

            //
            // There are available pages on this node.  Restrict search.
            //

            NodePassesLeft = 2;
            ColorCount = MmSecondaryColorMask + 1;
            BaseColor = Node->MmShiftedColor;
            ASSERT(ColorCount == MmSecondaryColors / KeNumberNodes);
        }
    }

    do {

        //
        // Loop: 1st pass restricted to node, 2nd pass unrestricted.
        //

#endif

        MemoryList = ZeroedPageList;

        do {

            //
            // Scan the zero list and then the free list.
            //

            ASSERT (MemoryList <= FreePageList);

            ListHead = MmPageLocationList[MemoryList];

            //
            // Initialize the loop iteration controls.  Clearly pages
            // can be added or removed from the colored lists when we
            // deliberately drop the PFN lock below (just to be a good
            // citizen), but even if we never released the lock, we wouldn't
            // have scanned more than the colorhead count anyway, so
            // this is a much better way to go.
            //

            ColorHeadsDrained = 0;
            PagesExamined = 0;

            ColorHead = &MmFreePagesByColor[MemoryList][BaseColor];
            ColoredPageInfo = &ColoredPageInfoBase[BaseColor];
            for (Color = 0; Color < ColorCount; Color += 1) {
                ASSERT (ColorHead->Count <= MmNumberOfPhysicalPages);
                ColoredPageInfo->PagesLeftToScan = ColorHead->Count;
                if (ColorHead->Count == 0) {
                    ColorHeadsDrained += 1;
                }
                ColorHead += 1;
                ColoredPageInfo += 1;
            }

            Color = 0;

#if defined(MI_MULTINODE)

            Color = (Color & MmSecondaryColorMask) | BaseColor;

#endif

            ASSERT (Color < MmSecondaryColors);
            do {

                //
                // Scan the current list by color.
                //

                ColorHead = &MmFreePagesByColor[MemoryList][Color];
                ColoredPageInfo = &ColoredPageInfoBase[Color];

                if (NodePassesLeft == 1) {

                    //
                    // Unrestricted search across all colors.
                    //

                    Color += 1;
                    if (Color >= MmSecondaryColors) {
                        Color = 0;
                    }
                }

#if defined(MI_MULTINODE) 

                else {

                    //
                    // Restrict first pass searches to current node.
                    //

                    ASSERT (NodePassesLeft == 2);
                    Color = BaseColor | ((Color + 1) & MmSecondaryColorMask);
                }

#endif

                if (ColoredPageInfo->PagesLeftToScan == 0) {

                    //
                    // This colored list has already been completely
                    // searched.
                    //

                    continue;
                }

                if (ColorHead->Flink == MM_EMPTY_LIST) {

                    //
                    // This colored list is empty.
                    //

                    ColoredPageInfo->PagesLeftToScan = 0;
                    ColorHeadsDrained += 1;
                    continue;
                }

                while (ColorHead->Flink != MM_EMPTY_LIST) {

                    ASSERT (ColoredPageInfo->PagesLeftToScan != 0);

                    ColoredPageInfo->PagesLeftToScan -= 1;

                    if (ColoredPageInfo->PagesLeftToScan == 0) {
                        ColorHeadsDrained += 1;
                    }

                    PagesExamined += 1;

                    Page = ColorHead->Flink;
    
                    Pfn1 = MI_PFN_ELEMENT(Page);

                    ASSERT ((MMLISTS)Pfn1->u3.e1.PageLocation == MemoryList);

                    //
                    // See if the page is within the caller's page constraints.
                    //

                    PagePlacementOk = FALSE;

                    //
                    // Since the caller may do anything with these frames
                    // including mapping them uncached or write combined,
                    // don't give out frames that are being mapped
                    // by (cached) superpages.
                    //

                    if (Pfn1->u4.MustBeCached == 0) {

                        LowPage1 = LowPage;
                        HighPage1 = HighPage;

                        do {
                            if ((Page >= LowPage1) && (Page <= HighPage1)) {
                                PagePlacementOk = TRUE;
                                break;
                            }

                            if (SkipPages == 0) {
                                break;
                            }

                            LowPage1 += SkipPages;
                            HighPage1 += SkipPages;

                            if (HighPage1 > MmHighestPhysicalPage) {
                                HighPage1 = MmHighestPhysicalPage;
                            }

                        } while (LowPage1 <= MmHighestPhysicalPage);
                    }
            
                    // 
                    // The Flink and Blink must be nonzero here for the page
                    // to be on the listhead.  Only code that scans the
                    // MmPhysicalMemoryBlock has to check for the zero case.
                    //

                    ASSERT (Pfn1->u1.Flink != 0);
                    ASSERT (Pfn1->u2.Blink != 0);

                    if (PagePlacementOk == FALSE) {

                        if (ColoredPageInfo->PagesLeftToScan == 0) {

                            //
                            // No more pages to scan in this colored chain.
                            //

                            break;
                        }

                        //
                        // If the colored list has more than one entry then
                        // move this page to the end of this colored list.
                        //

                        PageNextColored = (PFN_NUMBER)Pfn1->OriginalPte.u.Long;

                        if (PageNextColored == MM_EMPTY_LIST) {

                            //
                            // No more pages in this colored chain.
                            //

                            ColoredPageInfo->PagesLeftToScan = 0;
                            ColorHeadsDrained += 1;
                            break;
                        }

                        ASSERT (Pfn1->u1.Flink != 0);
                        ASSERT (Pfn1->u1.Flink != MM_EMPTY_LIST);
                        ASSERT (Pfn1->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                        PfnNextColored = MI_PFN_ELEMENT(PageNextColored);
                        ASSERT ((MMLISTS)PfnNextColored->u3.e1.PageLocation == MemoryList);
                        ASSERT (PfnNextColored->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                        //
                        // Adjust the free page list so Page
                        // follows PageNextFlink.
                        //

                        PageNextFlink = Pfn1->u1.Flink;
                        PfnNextFlink = MI_PFN_ELEMENT(PageNextFlink);

                        ASSERT ((MMLISTS)PfnNextFlink->u3.e1.PageLocation == MemoryList);
                        ASSERT (PfnNextFlink->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);

                        PfnLastColored = ColorHead->Blink;
                        ASSERT (PfnLastColored != (PMMPFN)MM_EMPTY_LIST);
                        ASSERT (PfnLastColored->OriginalPte.u.Long == MM_EMPTY_LIST);
                        ASSERT (PfnLastColored->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                        ASSERT (PfnLastColored->u2.Blink != MM_EMPTY_LIST);

                        ASSERT ((MMLISTS)PfnLastColored->u3.e1.PageLocation == MemoryList);
                        PageLastColored = MI_PFN_ELEMENT_TO_INDEX (PfnLastColored);

                        if (ListHead->Flink == Page) {

                            ASSERT (Pfn1->u2.Blink == MM_EMPTY_LIST);
                            ASSERT (ListHead->Blink != Page);

                            ListHead->Flink = PageNextFlink;

                            PfnNextFlink->u2.Blink = MM_EMPTY_LIST;
                        }
                        else {

                            ASSERT (Pfn1->u2.Blink != MM_EMPTY_LIST);
                            ASSERT ((MMLISTS)(MI_PFN_ELEMENT((MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink)))->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                            ASSERT ((MMLISTS)(MI_PFN_ELEMENT((MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink)))->u3.e1.PageLocation == MemoryList);

                            MI_PFN_ELEMENT(Pfn1->u2.Blink)->u1.Flink = PageNextFlink;
                            PfnNextFlink->u2.Blink = Pfn1->u2.Blink;
                        }

#if DBG
                        if (PfnLastColored->u1.Flink == MM_EMPTY_LIST) {
                            ASSERT (ListHead->Blink == PageLastColored);
                        }
#endif

                        Pfn1->u1.Flink = PfnLastColored->u1.Flink;
                        Pfn1->u2.Blink = PageLastColored;

                        if (ListHead->Blink == PageLastColored) {
                            ListHead->Blink = Page;
                        }

                        //
                        // Adjust the colored chains.
                        //

                        if (PfnLastColored->u1.Flink != MM_EMPTY_LIST) {
                            ASSERT (MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u4.PteFrame != MI_MAGIC_AWE_PTEFRAME);
                            ASSERT ((MMLISTS)(MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u3.e1.PageLocation) == MemoryList);
                            MI_PFN_ELEMENT(PfnLastColored->u1.Flink)->u2.Blink = Page;
                        }

                        PfnLastColored->u1.Flink = Page;

                        ColorHead->Flink = PageNextColored;
                        PfnNextColored->u4.PteFrame = MM_EMPTY_LIST;

                        Pfn1->OriginalPte.u.Long = MM_EMPTY_LIST;
                        Pfn1->u4.PteFrame = PageLastColored;

                        ASSERT (PfnLastColored->OriginalPte.u.Long == MM_EMPTY_LIST);
                        PfnLastColored->OriginalPte.u.Long = Page;
                        ColorHead->Blink = Pfn1;

                        continue;
                    }

                    found += 1;
                    ASSERT (Pfn1->u3.e1.ReadInProgress == 0);
                    MiUnlinkFreeOrZeroedPage (Pfn1);

                    Pfn1->u3.e2.ReferenceCount = 1;
                    Pfn1->u2.ShareCount = 1;
                    MI_SET_PFN_DELETED(Pfn1);
                    Pfn1->u4.PteFrame = MI_MAGIC_AWE_PTEFRAME;
                    Pfn1->u3.e1.PageLocation = ActiveAndValid;
                    ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);

                    Pfn1->u3.e1.StartOfAllocation = 1;
                    Pfn1->u3.e1.EndOfAllocation = 1;
                    Pfn1->u4.VerifierAllocation = 0;
                    Pfn1->u3.e1.LargeSessionAllocation = 0;

                    //
                    // Add free pages to the list of pages to be
                    // zeroed before returning.
                    //

                    if (MemoryList == FreePageList) {
                        Pfn1->OriginalPte.u.Long = (ULONG_PTR) ColoredPageInfo->PfnAllocation;
                        ColoredPageInfo->PfnAllocation = Pfn1;
                        ColoredPageInfo->PagesQueued += 1;
                        ZeroCount += 1;
                    }
                    else {
                        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
                    }

                    *MdlPage = Page;
                    MdlPage += 1;

                    if (found == SizeInPages) {

                        //
                        // All the pages requested are available.
                        //

#if DBG
                        FinishedCount = 0;
                        for (Color = 0; Color < ColorCount; Color += 1) {
                            if (ColoredPageInfoBase[Color + BaseColor].PagesLeftToScan == 0) {
                                FinishedCount += 1;
                            }
                        }
                        ASSERT (FinishedCount == ColorHeadsDrained);
#endif

                        goto pass2_done;
                    }

                    //
                    // March on to the next colored chain so the overall
                    // allocation round-robins the page colors.
                    //

                    PagesExamined = PAGE_SIZE;
                    break;
                }

                //
                // If we've held the PFN lock for a while, release it to
                // give DPCs and other processors a chance to run.
                //

                if (PagesExamined >= PAGE_SIZE) {
                    UNLOCK_PFN (OldIrql);
                    PagesExamined = 0;
                    LOCK_PFN (OldIrql);
                }

                //
                // Systems utilizing memory compression may have more
                // pages on the zero, free and standby lists than we
                // want to give out.  The same is true when machines
                // are low on memory - we don't want this thread to gobble
                // up the pages from every modified write that completes
                // because that would starve waiting threads.
                //
                // Explicitly check MmAvailablePages instead (and recheck
                // whenever the PFN lock is released and reacquired).
                //

                if (MmAvailablePages < MM_HIGH_LIMIT) {
                    goto pass2_done;
                }

            } while (ColorHeadsDrained != ColorCount);

            //
            // Release the PFN lock to give DPCs and other processors
            // a chance to run.  Nothing magic about the instructions
            // between the unlock and the relock.
            //

            UNLOCK_PFN (OldIrql);

#if DBG
            FinishedCount = 0;
            for (Color = 0; Color < ColorCount; Color += 1) {
                if (ColoredPageInfoBase[Color + BaseColor].PagesLeftToScan == 0) {
                    FinishedCount += 1;
                }
            }
            ASSERT (FinishedCount == ColorHeadsDrained);
#endif

            MemoryList += 1;

            LOCK_PFN (OldIrql);

        } while (MemoryList <= FreePageList);

#if defined(MI_MULTINODE)

        //
        // Expand range to all colors for next pass.
        //

        ColorCount = MmSecondaryColors;
        BaseColor = 0;

        NodePassesLeft -= 1;

    } while (NodePassesLeft != 0);

#endif

    //
    // Briefly release the PFN lock to give DPCs and other processors
    // a time slice.
    //

    UNLOCK_PFN (OldIrql);

    LOCK_PFN (OldIrql);

    //
    // Walk the transition list looking for pages satisfying the
    // constraints as walking the physical memory block can be draining.
    //

    for (Page = MmStandbyPageListHead.Flink; Page != MM_EMPTY_LIST; Page = NextPage) {

        //
        // Systems utilizing memory compression may have more
        // pages on the zero, free and standby lists than we
        // want to give out.  The same is true when machines
        // are low on memory - we don't want this thread to gobble
        // up the pages from every modified write that completes
        // because that would starve waiting threads.
        //
        // Explicitly check MmAvailablePages instead (and recheck whenever
        // the PFN lock is released and reacquired).
        //

        if (MmAvailablePages < MM_HIGH_LIMIT) {
            break;
        }

        Pfn1 = MI_PFN_ELEMENT (Page);
        NextPage = Pfn1->u1.Flink;

        //
        // Since the caller may do anything with these frames including
        // mapping them uncached or write combined, don't give out frames
        // that are being mapped by (cached) superpages.
        //

        if (Pfn1->u4.MustBeCached == 1) {
            continue;
        }

        LowPage1 = LowPage;
        HighPage1 = HighPage;
        PagePlacementOk = FALSE;

        do {
            if ((Page >= LowPage1) && (Page <= HighPage1)) {
                PagePlacementOk = TRUE;
                break;
            }

            if (SkipPages == 0) {
                break;
            }

            LowPage1 += SkipPages;
            HighPage1 += SkipPages;

            if (HighPage1 > MmHighestPhysicalPage) {
                HighPage1 = MmHighestPhysicalPage;
            }

        } while (LowPage1 <= MmHighestPhysicalPage);

        if (PagePlacementOk == TRUE) {

            ASSERT (Pfn1->u3.e1.ReadInProgress == 0);

            found += 1;

            //
            // This page is in the desired range - grab it.
            //

            MiUnlinkPageFromList (Pfn1);
            ASSERT (Pfn1->u3.e2.ReferenceCount == 0);
            MiRestoreTransitionPte (Pfn1);

            Pfn1->u3.e2.ReferenceCount = 1;
            Pfn1->u2.ShareCount = 1;
            MI_SET_PFN_DELETED(Pfn1);
            Pfn1->u4.PteFrame = MI_MAGIC_AWE_PTEFRAME;
            Pfn1->u3.e1.PageLocation = ActiveAndValid;
            ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
            Pfn1->u3.e1.StartOfAllocation = 1;
            Pfn1->u3.e1.EndOfAllocation = 1;
            Pfn1->u4.VerifierAllocation = 0;
            Pfn1->u3.e1.LargeSessionAllocation = 0;

            //
            // Add standby pages to the list of pages to be
            // zeroed before returning.
            //

            Color = MI_GET_COLOR_FROM_LIST_ENTRY (Page, Pfn1);

            ColoredPageInfo = &ColoredPageInfoBase[Color];
            Pfn1->OriginalPte.u.Long = (ULONG_PTR) ColoredPageInfo->PfnAllocation;
            ColoredPageInfo->PfnAllocation = Pfn1;
            ColoredPageInfo->PagesQueued += 1;
            ZeroCount += 1;

            *MdlPage = Page;
            MdlPage += 1;

            if (found == SizeInPages) {

                //
                // All the pages requested are available.
                //

                break;
            }
        }
    }

pass2_done:

    //
    // The full amount was charged up front - remove any excess now.
    //

    UNLOCK_PFN (OldIrql);

    MI_INCREMENT_RESIDENT_AVAILABLE (SizeInPages - found, MM_RESAVAIL_FREE_FOR_MDL_EXCESS);

    InterlockedExchangeAddSizeT (&MmMdlPagesAllocated, 0 - (SizeInPages - found));

    KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
    MmUnlockPagableImageSection (ExPageLockHandle);

    if (found != MdlPageSpan) {
        ASSERT (found < MdlPageSpan);
        MiReturnCommitment (MdlPageSpan - found);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_AWE_EXCESS, MdlPageSpan - found);
    }

    if (found == 0) {
        ExFreePool (ColoredPageInfoBase);
        ExFreePool (MemoryDescriptorList);
        return NULL;
    }

    if (ZeroCount != 0) {

        //
        // Zero all the free & standby pages, fanning out the work.  This
        // is done even on UP machines because the worker thread code maps
        // large MDLs and is thus better performing than zeroing a single
        // page at a time.
        //

        MiZeroInParallel (ColoredPageInfoBase);

        //
        // Denote that no pages are left to be zeroed because in addition
        // to zeroing them, we have reset all their OriginalPte fields
        // to demand zero so they cannot be walked by the zeroing loop
        // below.
        //

        ZeroCount = 0;
    }

    ExFreePool (ColoredPageInfoBase);

    MemoryDescriptorList->ByteCount = (ULONG)(found << PAGE_SHIFT);

    if (found != SizeInPages) {
        *MdlPage = MM_EMPTY_LIST;
    }

    //
    // If the number of pages allocated was substantially less than the
    // initial request amount, attempt to allocate a smaller MDL to save
    // pool.
    //

    if ((MdlPageSpan - found) > ((4 * PAGE_SIZE) / sizeof (PFN_NUMBER))) {

        MemoryDescriptorList2 = MmCreateMdl ((PMDL)0,
                                             (PVOID)0,
                                             found << PAGE_SHIFT);
    
        if (MemoryDescriptorList2 != NULL) {

            RtlCopyMemory ((PVOID)(MemoryDescriptorList2 + 1),
                           (PVOID)(MemoryDescriptorList + 1),
                           found * sizeof (PFN_NUMBER));

            ExFreePool (MemoryDescriptorList);
            MemoryDescriptorList = MemoryDescriptorList2;
        }
    }

#if DBG
    //
    // Ensure all pages are within the caller's page constraints.
    //

    MdlPage = (PPFN_NUMBER)(MemoryDescriptorList + 1);
    LastMdlPage = MdlPage + found;

    LowPage = (PFN_NUMBER)(LowAddress.QuadPart >> PAGE_SHIFT);
    HighPage = (PFN_NUMBER)(HighAddress.QuadPart >> PAGE_SHIFT);
    Process = PsGetCurrentProcess ();

    MmLockPagableSectionByHandle (ExPageLockHandle);

    while (MdlPage < LastMdlPage) {
        Page = *MdlPage;
        PagePlacementOk = FALSE;
        LowPage1 = LowPage;
        HighPage1 = HighPage;

        do {
            if ((Page >= LowPage1) && (Page <= HighPage1)) {
                PagePlacementOk = TRUE;
                break;
            }

            if (SkipPages == 0) {
                break;
            }

            LowPage1 += SkipPages;
            HighPage1 += SkipPages;

            if (LowPage1 > MmHighestPhysicalPage) {
                break;
            }
            if (HighPage1 > MmHighestPhysicalPage) {
                HighPage1 = MmHighestPhysicalPage;
            }
        } while (TRUE);

#if 0

        // 
        // Make sure page really is zero.
        //

        VirtualAddress = MiMapPageInHyperSpace (Process, Page, &OldIrql);

        ASSERT (RtlCompareMemoryUlong (VirtualAddress, PAGE_SIZE, 0) == PAGE_SIZE);

        MiUnmapPageInHyperSpace (Process, VirtualAddress, OldIrql);

#endif
        ASSERT (PagePlacementOk == TRUE);
        Pfn1 = MI_PFN_ELEMENT(*MdlPage);
        ASSERT (Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME);
        MdlPage += 1;
    }

    MmUnlockPagableImageSection (ExPageLockHandle);

#endif

    //
    // Mark the MDL's pages as locked so the the kernelmode caller can
    // map the MDL using MmMapLockedPages* without asserting.
    //

    MemoryDescriptorList->MdlFlags |= MDL_PAGES_LOCKED;

    return MemoryDescriptorList;
}


VOID
MmFreePagesFromMdl (
    IN PMDL MemoryDescriptorList
    )

/*++

Routine Description:

    This routine walks the argument MDL freeing each physical page back to
    the PFN database.  This is designed to free pages acquired via
    MmAllocatePagesForMdl only.

Arguments:

    MemoryDescriptorList - Supplies an MDL which contains the pages to be freed.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/
{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PVOID StartingAddress;
    PVOID AlignedVa;
    PPFN_NUMBER Page;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER TotalPages;
    PFN_NUMBER DeltaPages;
    LONG EntryCount;
    LONG OriginalCount;

    ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

    DeltaPages = 0;

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    ASSERT ((MemoryDescriptorList->MdlFlags & MDL_IO_SPACE) == 0);

    ASSERT (((ULONG_PTR)MemoryDescriptorList->StartVa & (PAGE_SIZE - 1)) == 0);
    AlignedVa = (PVOID)MemoryDescriptorList->StartVa;

    StartingAddress = (PVOID)((PCHAR)AlignedVa +
                    MemoryDescriptorList->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingAddress,
                                              MemoryDescriptorList->ByteCount);

    TotalPages = NumberOfPages;

    MI_MAKING_MULTIPLE_PTES_INVALID (TRUE);

    MmLockPagableSectionByHandle (ExPageLockHandle);

    LOCK_PFN (OldIrql);

    do {

        if (*Page == MM_EMPTY_LIST) {

            //
            // There are no more locked pages.
            //

            break;
        }

        ASSERT (MI_IS_PFN (*Page));
        ASSERT (*Page <= MmHighestPhysicalPage);

        Pfn1 = MI_PFN_ELEMENT (*Page);
        ASSERT (Pfn1->u2.ShareCount == 1);
        ASSERT (MI_IS_PFN_DELETED (Pfn1) == TRUE);
        ASSERT (MI_PFN_IS_AWE (Pfn1) == TRUE);
        ASSERT (Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME);

        Pfn1->u3.e1.StartOfAllocation = 0;
        Pfn1->u3.e1.EndOfAllocation = 0;
        Pfn1->u2.ShareCount = 0;

#if DBG
        Pfn1->u4.PteFrame -= 1;
        Pfn1->u3.e1.PageLocation = StandbyPageList;
#endif

        if (Pfn1->u4.AweAllocation == 1) {

            do {

                EntryCount = Pfn1->AweReferenceCount;

                ASSERT ((LONG)EntryCount > 0);
                ASSERT (Pfn1->u3.e2.ReferenceCount != 0);

                OriginalCount = InterlockedCompareExchange (&Pfn1->AweReferenceCount,
                                                            EntryCount - 1,
                                                            EntryCount);

                if (OriginalCount == EntryCount) {

                    //
                    // This thread can be racing against other threads
                    // calling MmUnlockPages.  All threads can safely do
                    // interlocked decrements on the "AWE reference count".
                    // Whichever thread drives it to zero is responsible for
                    // decrementing the actual PFN reference count (which may
                    // be greater than 1 due to other non-AWE API calls being
                    // used on the same page).  The thread that drives this
                    // reference count to zero must put the page on the actual
                    // freelist at that time and decrement various resident
                    // available and commitment counters also.
                    //

                    if (OriginalCount == 1) {

                        //
                        // This thread has driven the AWE reference count to
                        // zero so it must initiate a decrement of the PFN
                        // reference count (while holding the PFN lock), etc.
                        //
                        // This path should be the frequent one since typically
                        // I/Os complete before these types of pages are
                        // freed by the app.
                        //

                        MiDecrementReferenceCountForAwePage (Pfn1, TRUE);
                    }

                    break;
                }
            } while (TRUE);
        }
        else {
            MiDecrementReferenceCountInline (Pfn1, *Page);
            DeltaPages += 1;
        }

        *Page++ = MM_EMPTY_LIST;

        //
        // Nothing magic about the divisor here - just releasing the PFN lock
        // periodically to allow other processors and DPCs a chance to execute.
        //

        if ((NumberOfPages & 0xF) == 0) {

            UNLOCK_PFN (OldIrql);

            LOCK_PFN (OldIrql);
        }

        NumberOfPages -= 1;

    } while (NumberOfPages != 0);

    UNLOCK_PFN (OldIrql);

    MmUnlockPagableImageSection (ExPageLockHandle);

    if (DeltaPages != 0) {
        MI_INCREMENT_RESIDENT_AVAILABLE (DeltaPages, MM_RESAVAIL_FREE_FROM_MDL);
        InterlockedExchangeAddSizeT (&MmMdlPagesAllocated, 0 - DeltaPages);
        MiReturnCommitment (DeltaPages);
        MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_MDL_PAGES, DeltaPages);
    }

    MemoryDescriptorList->MdlFlags &= ~MDL_PAGES_LOCKED;
}


NTSTATUS
MmMapUserAddressesToPage (
    IN PVOID BaseAddress,
    IN SIZE_T NumberOfBytes,
    IN PVOID PageAddress
    )

/*++

Routine Description:

    This function maps a range of addresses in a physical memory VAD to the
    specified page address.  This is typically used by a driver to nicely
    remove an application's access to things like video memory when the
    application is not responding to requests to relinquish it.

    Note the entire range must be currently mapped (ie, all the PTEs must
    be valid) by the caller.

Arguments:

    BaseAddress - Supplies the base virtual address where the physical
                  address is mapped.

    NumberOfBytes - Supplies the number of bytes to remap to the new address.

    PageAddress - Supplies the virtual address of the page this is remapped to.
                  This must be nonpaged memory.

Return Value:

    Various NTSTATUS codes.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMMVAD Vad;
    PMMPTE PointerPte;
    MMPTE PteContents;
    PMMPTE LastPte;
    PEPROCESS Process;
    NTSTATUS Status;
    PVOID EndingAddress;
    PFN_NUMBER PageFrameNumber;
    SIZE_T NumberOfPtes;
    PHYSICAL_ADDRESS PhysicalAddress;
    KIRQL OldIrql;

    PAGED_CODE();

    if (BaseAddress > MM_HIGHEST_USER_ADDRESS) {
        return STATUS_INVALID_PARAMETER_1;
    }

    if ((ULONG_PTR)BaseAddress + NumberOfBytes > (ULONG64)MM_HIGHEST_USER_ADDRESS) {
        return STATUS_INVALID_PARAMETER_2;
    }

    Process = PsGetCurrentProcess();

    EndingAddress = (PVOID)((PCHAR)BaseAddress + NumberOfBytes - 1);

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }

    Vad = (PMMVAD)MiLocateAddress (BaseAddress);

    if (Vad == NULL) {

        //
        // No virtual address descriptor located.
        //

        Status = STATUS_MEMORY_NOT_ALLOCATED;
        goto ErrorReturn;
    }

    if (NumberOfBytes == 0) {

        //
        // If the region size is specified as 0, the base address
        // must be the starting address for the region.  The entire VAD
        // will then be repointed.
        //

        if (MI_VA_TO_VPN (BaseAddress) != Vad->StartingVpn) {
            Status = STATUS_FREE_VM_NOT_AT_BASE;
            goto ErrorReturn;
        }

        BaseAddress = MI_VPN_TO_VA (Vad->StartingVpn);
        EndingAddress = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);
        NumberOfBytes = (PCHAR)EndingAddress - (PCHAR)BaseAddress + 1;
    }

    //
    // Found the associated virtual address descriptor.
    //

    if (Vad->EndingVpn < MI_VA_TO_VPN (EndingAddress)) {

        //
        // The entire range to remap is not contained within a single
        // virtual address descriptor.  Return an error.
        //

        Status = STATUS_INVALID_PARAMETER_2;
        goto ErrorReturn;
    }

    if (Vad->u.VadFlags.PhysicalMapping == 0) {

        //
        // The virtual address descriptor is not a physical mapping.
        //

        Status = STATUS_INVALID_ADDRESS;
        goto ErrorReturn;
    }

    PointerPte = MiGetPteAddress (BaseAddress);
    LastPte = MiGetPteAddress (EndingAddress);
    NumberOfPtes = LastPte - PointerPte + 1;

    //
    // Lock down because the PFN lock is going to be acquired shortly.
    //

    MmLockPagableSectionByHandle(ExPageLockHandle);

    LOCK_WS_UNSAFE (Process);

    PhysicalAddress = MmGetPhysicalAddress (PageAddress);
    PageFrameNumber = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);

    PteContents = *PointerPte;
    PteContents.u.Hard.PageFrameNumber = PageFrameNumber;

#if DBG

    //
    // All the PTEs must be valid or the filling will corrupt the
    // UsedPageTableCounts.
    //

    do {
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PointerPte += 1;
    } while (PointerPte < LastPte);
    PointerPte = MiGetPteAddress (BaseAddress);
#endif

    //
    // Fill the PTEs and flush at the end - no race here because it doesn't
    // matter whether the user app sees the old or the new data until we
    // return (writes going to either page is acceptable prior to return
    // from this function).  There is no race with I/O and ProbeAndLockPages
    // because the PFN lock is acquired here.
    //

    LOCK_PFN (OldIrql);

#if !defined (_X86PAE_)
    MiFillMemoryPte (PointerPte, NumberOfPtes, PteContents.u.Long);
#else

    //
    // Note that the PAE architecture must very carefully fill these PTEs.
    //

    do {
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PointerPte += 1;
        InterlockedExchangePte (PointerPte, PteContents.u.Long);
    } while (PointerPte < LastPte);
    PointerPte = MiGetPteAddress (BaseAddress);

#endif

    if (NumberOfPtes == 1) {
        KeFlushSingleTb (BaseAddress, FALSE);
    }
    else {
        KeFlushProcessTb (FALSE);
    }

    UNLOCK_PFN (OldIrql);

    UNLOCK_WS_UNSAFE (Process);

    MmUnlockPagableImageSection (ExPageLockHandle);

    Status = STATUS_SUCCESS;

ErrorReturn:

    UNLOCK_ADDRESS_SPACE (Process);

    return Status;
}


PHYSICAL_ADDRESS
MmGetPhysicalAddress (
     IN PVOID BaseAddress
     )

/*++

Routine Description:

    This function returns the corresponding physical address for a
    valid virtual address.

Arguments:

    BaseAddress - Supplies the virtual address for which to return the
                  physical address.

Return Value:

    Returns the corresponding physical address.

Environment:

    Kernel mode.  Any IRQL level.

--*/

{
    PMMPTE PointerPte;
    PHYSICAL_ADDRESS PhysicalAddress;

    if (MI_IS_PHYSICAL_ADDRESS(BaseAddress)) {
        PhysicalAddress.QuadPart = MI_CONVERT_PHYSICAL_TO_PFN (BaseAddress);
    }
    else {

#if (_MI_PAGING_LEVELS>=4)
        PointerPte = MiGetPxeAddress (BaseAddress);
        if (PointerPte->u.Hard.Valid == 0) {
            KdPrint(("MM:MmGetPhysicalAddressFailed base address was %p",
                      BaseAddress));
            ZERO_LARGE (PhysicalAddress);
            return PhysicalAddress;
        }
#endif

#if (_MI_PAGING_LEVELS>=3)
        PointerPte = MiGetPpeAddress (BaseAddress);
        if (PointerPte->u.Hard.Valid == 0) {
            KdPrint(("MM:MmGetPhysicalAddressFailed base address was %p",
                      BaseAddress));
            ZERO_LARGE (PhysicalAddress);
            return PhysicalAddress;
        }
#endif

        PointerPte = MiGetPdeAddress (BaseAddress);
        if (PointerPte->u.Hard.Valid == 0) {
            KdPrint(("MM:MmGetPhysicalAddressFailed base address was %p",
                      BaseAddress));
            ZERO_LARGE (PhysicalAddress);
            return PhysicalAddress;
        }

        if (MI_PDE_MAPS_LARGE_PAGE (PointerPte)) {
            PhysicalAddress.QuadPart = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte) +
                                           MiGetPteOffset (BaseAddress);
        }
        else {
            PointerPte = MiGetPteAddress (BaseAddress);

            if (PointerPte->u.Hard.Valid == 0) {
                KdPrint(("MM:MmGetPhysicalAddressFailed base address was %p",
                          BaseAddress));
                ZERO_LARGE (PhysicalAddress);
                return PhysicalAddress;
            }
            PhysicalAddress.QuadPart = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        }
    }

    PhysicalAddress.QuadPart = PhysicalAddress.QuadPart << PAGE_SHIFT;
    PhysicalAddress.LowPart += BYTE_OFFSET(BaseAddress);

    return PhysicalAddress;
}

PVOID
MmGetVirtualForPhysical (
    IN PHYSICAL_ADDRESS PhysicalAddress
     )

/*++

Routine Description:

    This function returns the corresponding virtual address for a physical
    address whose primary virtual address is in system space.

Arguments:

    PhysicalAddress - Supplies the physical address for which to return the
                      virtual address.

Return Value:

    Returns the corresponding virtual address.

Environment:

    Kernel mode.  Any IRQL level.

--*/

{
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn;

    PageFrameIndex = (PFN_NUMBER)(PhysicalAddress.QuadPart >> PAGE_SHIFT);

    Pfn = MI_PFN_ELEMENT (PageFrameIndex);

    return (PVOID)((PCHAR)MiGetVirtualAddressMappedByPte (Pfn->PteAddress) +
                    BYTE_OFFSET (PhysicalAddress.LowPart));
}

//
// Nonpaged helper routine.
//

VOID
MiMarkMdlPageAttributes (
    IN PMDL Mdl,
    IN PFN_NUMBER NumberOfPages,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute
    )
{
    PMMPFN Pfn1;
    PFN_NUMBER PageFrameIndex;
    PPFN_NUMBER Page;

    Page = (PPFN_NUMBER)(Mdl + 1);

    do {
        PageFrameIndex = *Page;

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
        ASSERT (Pfn1->u3.e1.CacheAttribute == MiNotMapped);
        Pfn1->u3.e1.CacheAttribute = CacheAttribute;

        Page += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);
}


PVOID
MmAllocateNonCachedMemory (
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    This function allocates a range of noncached memory in
    the non-paged portion of the system address space.

    This routine is designed to be used by a driver's initialization
    routine to allocate a noncached block of virtual memory for
    various device specific buffers.

Arguments:

    NumberOfBytes - Supplies the number of bytes to allocate.

Return Value:

    NON-NULL - Returns a pointer (virtual address in the nonpaged portion
               of the system) to the allocated physically contiguous
               memory.

    NULL - The specified request could not be satisfied.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PPFN_NUMBER Page;
    PMMPTE PointerPte;
    MMPTE TempPte;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;
    PMDL Mdl;
    PVOID BaseAddress;
    PHYSICAL_ADDRESS LowAddress;
    PHYSICAL_ADDRESS HighAddress;
    PHYSICAL_ADDRESS SkipBytes;
    PFN_NUMBER NumberOfPagesAllocated;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

    ASSERT (NumberOfBytes != 0);

#if defined (_WIN64)

    //
    // Maximum allocation size is constrained by the MDL ByteCount field.
    //

    if (NumberOfBytes >= _4gb) {
        return NULL;
    }

#endif

    NumberOfPages = BYTES_TO_PAGES(NumberOfBytes);

    //
    // Even though an MDL is not needed per se, it is much more convenient
    // to use the routine below because it checks for things like appropriate
    // cachability of the pages, etc.  Note that the MDL returned may map
    // fewer pages than requested - check for this and if so, return NULL.
    //

    LowAddress.QuadPart = 0;
    HighAddress.QuadPart = (ULONGLONG)-1;
    SkipBytes.QuadPart = 0;

    Mdl = MmAllocatePagesForMdl (LowAddress,
                                 HighAddress,
                                 SkipBytes,
                                 NumberOfBytes);
    if (Mdl == NULL) {
        return NULL;
    }

    BaseAddress = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);

    NumberOfPagesAllocated = ADDRESS_AND_SIZE_TO_SPAN_PAGES (BaseAddress, Mdl->ByteCount);

    if (NumberOfPages != NumberOfPagesAllocated) {
        ASSERT (NumberOfPages > NumberOfPagesAllocated);
        MmFreePagesFromMdl (Mdl);
        ExFreePool (Mdl);
        return NULL;
    }

    //
    // Obtain enough virtual space to map the pages.  Add an extra PTE so the
    // MDL can be stashed now and retrieved on release.
    //

    PointerPte = MiReserveSystemPtes ((ULONG)NumberOfPages + 1, SystemPteSpace);

    if (PointerPte == NULL) {
        MmFreePagesFromMdl (Mdl);
        ExFreePool (Mdl);
        return NULL;
    }

    *(PMDL *)PointerPte = Mdl;
    PointerPte += 1;

    BaseAddress = (PVOID)MiGetVirtualAddressMappedByPte (PointerPte);

    Page = (PPFN_NUMBER)(Mdl + 1);

    MI_MAKE_VALID_PTE (TempPte,
                       0,
                       MM_READWRITE,
                       PointerPte);

    MI_SET_PTE_DIRTY (TempPte);

    CacheAttribute = MI_TRANSLATE_CACHETYPE (MmNonCached, FALSE);

    switch (CacheAttribute) {

        case MiNonCached:
            MI_DISABLE_CACHING (TempPte);
            break;

        case MiCached:
            break;

        case MiWriteCombined:
            MI_SET_PTE_WRITE_COMBINE (TempPte);
            break;

        default:
            ASSERT (FALSE);
            break;
    }

    MI_PREPARE_FOR_NONCACHED (CacheAttribute);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 0);
        PageFrameIndex = *Page;

        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

        MI_WRITE_VALID_PTE (PointerPte, TempPte);

        Page += 1;
        PointerPte += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    MI_SWEEP_CACHE (CacheAttribute, BaseAddress, NumberOfBytes);

    MiMarkMdlPageAttributes (Mdl, NumberOfPagesAllocated, CacheAttribute);

    return BaseAddress;
}

VOID
MmFreeNonCachedMemory (
    IN PVOID BaseAddress,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    This function deallocates a range of noncached memory in
    the non-paged portion of the system address space.

Arguments:

    BaseAddress - Supplies the base virtual address where the noncached
                  memory resides.

    NumberOfBytes - Supplies the number of bytes allocated to the request.
                    This must be the same number that was obtained with
                    the MmAllocateNonCachedMemory call.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMDL Mdl;
    PMMPTE PointerPte;
    PFN_NUMBER NumberOfPages;
#if DBG
    PFN_NUMBER i;
    PVOID StartingAddress;
#endif

    ASSERT (NumberOfBytes != 0);
    ASSERT (PAGE_ALIGN (BaseAddress) == BaseAddress);

    MI_MAKING_MULTIPLE_PTES_INVALID (TRUE);

    NumberOfPages = BYTES_TO_PAGES(NumberOfBytes);

    PointerPte = MiGetPteAddress (BaseAddress);

    Mdl = *(PMDL *)(PointerPte - 1);

#if DBG
    StartingAddress = (PVOID)((PCHAR)Mdl->StartVa + Mdl->ByteOffset);

    i = ADDRESS_AND_SIZE_TO_SPAN_PAGES (StartingAddress, Mdl->ByteCount);

    ASSERT (NumberOfPages == i);
#endif

    MmFreePagesFromMdl (Mdl);

    ExFreePool (Mdl);

    MiReleaseSystemPtes (PointerPte - 1,
                         (ULONG)NumberOfPages + 1,
                         SystemPteSpace);

    return;
}

SIZE_T
MmSizeOfMdl (
    IN PVOID Base,
    IN SIZE_T Length
    )

/*++

Routine Description:

    This function returns the number of bytes required for an MDL for a
    given buffer and size.

Arguments:

    Base - Supplies the base virtual address for the buffer.

    Length - Supplies the size of the buffer in bytes.

Return Value:

    Returns the number of bytes required to contain the MDL.

Environment:

    Kernel mode.  Any IRQL level.

--*/

{
    return( sizeof( MDL ) +
                (ADDRESS_AND_SIZE_TO_SPAN_PAGES( Base, Length ) *
                 sizeof( PFN_NUMBER ))
          );
}


PMDL
MmCreateMdl (
    IN PMDL MemoryDescriptorList OPTIONAL,
    IN PVOID Base,
    IN SIZE_T Length
    )

/*++

Routine Description:

    This function optionally allocates and initializes an MDL.

Arguments:

    MemoryDescriptorList - Optionally supplies the address of the MDL
                           to initialize.  If this address is supplied as NULL
                           an MDL is allocated from non-paged pool and
                           initialized.

    Base - Supplies the base virtual address for the buffer.

    Length - Supplies the size of the buffer in bytes.

Return Value:

    Returns the address of the initialized MDL.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    SIZE_T MdlSize;

#if defined (_WIN64)
    //
    // Since the Length has to fit into the MDL's ByteCount field, ensure it
    // doesn't wrap on 64-bit systems.
    //

    if (Length >= _4gb) {
        return NULL;
    }
#endif

    MdlSize = MmSizeOfMdl (Base, Length);

    if (!ARGUMENT_PRESENT(MemoryDescriptorList)) {

        MemoryDescriptorList = (PMDL)ExAllocatePoolWithTag (NonPagedPool,
                                                            MdlSize,
                                                            'ldmM');
        if (MemoryDescriptorList == (PMDL)0) {
            return NULL;
        }
    }

    MmInitializeMdl (MemoryDescriptorList, Base, Length);
    return MemoryDescriptorList;
}

BOOLEAN
MmSetAddressRangeModified (
    IN PVOID Address,
    IN SIZE_T Length
    )

/*++

Routine Description:

    This routine sets the modified bit in the PFN database for the
    pages that correspond to the specified address range.

    Note that the dirty bit in the PTE is cleared by this operation.

Arguments:

    Address - Supplies the address of the start of the range.  This
              range must reside within the system cache.

    Length - Supplies the length of the range.

Return Value:

    TRUE if at least one PTE was dirty in the range, FALSE otherwise.

Environment:

    Kernel mode.  APC_LEVEL and below for pagable addresses,
                  DISPATCH_LEVEL and below for non-pagable addresses.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPFN Pfn1;
    MMPTE PteContents;
    KIRQL OldIrql;
    PVOID VaFlushList[MM_MAXIMUM_FLUSH_COUNT];
    ULONG Count;
    BOOLEAN Result;

    Count = 0;
    Result = FALSE;

    //
    // Loop on the copy on write case until the page is only
    // writable.
    //

    PointerPte = MiGetPteAddress (Address);
    LastPte = MiGetPteAddress ((PVOID)((PCHAR)Address + Length - 1));

    LOCK_PFN2 (OldIrql);

    do {

        PteContents = *PointerPte;

        if (PteContents.u.Hard.Valid == 1) {

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

            MI_SET_MODIFIED (Pfn1, 1, 0x5);

            if ((Pfn1->OriginalPte.u.Soft.Prototype == 0) &&
                         (Pfn1->u3.e1.WriteInProgress == 0)) {
                MiReleasePageFileSpace (Pfn1->OriginalPte);
                Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
            }

#ifdef NT_UP
            //
            // On uniprocessor systems no need to flush if this processor
            // doesn't think the PTE is dirty.
            //

            if (MI_IS_PTE_DIRTY (PteContents)) {
                Result = TRUE;
#else  //NT_UP
                Result |= (BOOLEAN)(MI_IS_PTE_DIRTY (PteContents));
#endif //NT_UP

                //
                // Clear the write bit in the PTE so new writes can be tracked.
                //

                MI_SET_PTE_CLEAN (PteContents);
                MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, PteContents);

                if (Count != MM_MAXIMUM_FLUSH_COUNT) {
                    VaFlushList[Count] = Address;
                    Count += 1;
                }
#ifdef NT_UP
            }
#endif //NT_UP
        }
        PointerPte += 1;
        Address = (PVOID)((PCHAR)Address + PAGE_SIZE);
    } while (PointerPte <= LastPte);

    if (Count != 0) {
        if (Count == 1) {
            KeFlushSingleTb (VaFlushList[0], TRUE);
        }
        else if (Count != MM_MAXIMUM_FLUSH_COUNT) {
            KeFlushMultipleTb (Count, &VaFlushList[0], TRUE);
        }
        else {
            KeFlushEntireTb (FALSE, TRUE);
        }
    }
    UNLOCK_PFN2 (OldIrql);
    return Result;
}


PVOID
MiCheckForContiguousMemory (
    IN PVOID BaseAddress,
    IN PFN_NUMBER BaseAddressPages,
    IN PFN_NUMBER SizeInPages,
    IN PFN_NUMBER LowestPfn,
    IN PFN_NUMBER HighestPfn,
    IN PFN_NUMBER BoundaryPfn,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute
    )

/*++

Routine Description:

    This routine checks to see if the physical memory mapped
    by the specified BaseAddress for the specified size is
    contiguous and that the first page is greater than or equal to
    the specified LowestPfn and that the last page of the physical memory is
    less than or equal to the specified HighestPfn.

Arguments:

    BaseAddress - Supplies the base address to start checking at.

    BaseAddressPages - Supplies the number of pages to scan from the
                       BaseAddress.

    SizeInPages - Supplies the number of pages in the range.

    LowestPfn - Supplies lowest PFN acceptable as a physical page.

    HighestPfn - Supplies the highest PFN acceptable as a physical page.

    BoundaryPfn - Supplies the PFN multiple the allocation must
                  not cross.  0 indicates it can cross any boundary.

    CacheAttribute - Supplies the type of cache mapping that will be used
                     for the memory.

Return Value:

    Returns the usable virtual address within the argument range that the
    caller should return to his caller.  NULL if there is no usable address.

Environment:

    Kernel mode, memory management internal.

--*/

{
    KIRQL OldIrql;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PFN_NUMBER PreviousPage;
    PFN_NUMBER Page;
    PFN_NUMBER HighestStartPage;
    PFN_NUMBER LastPage;
    PFN_NUMBER OriginalPage;
    PFN_NUMBER OriginalLastPage;
    PVOID BoundaryAllocation;
    PFN_NUMBER BoundaryMask;
    PFN_NUMBER PageCount;
    MMPTE PteContents;

    BoundaryMask = ~(BoundaryPfn - 1);

    if (LowestPfn > HighestPfn) {
        return NULL;
    }

    if (LowestPfn + SizeInPages <= LowestPfn) {
        return NULL;
    }

    if (LowestPfn + SizeInPages - 1 > HighestPfn) {
        return NULL;
    }

    if (BaseAddressPages < SizeInPages) {
        return NULL;
    }

    if (MI_IS_PHYSICAL_ADDRESS (BaseAddress)) {

        //
        // All physical addresses are by definition cached and therefore do
        // not qualify for our caller.
        //

        if (CacheAttribute != MiCached) {
            return NULL;
        }

        OriginalPage = MI_CONVERT_PHYSICAL_TO_PFN(BaseAddress);
        OriginalLastPage = OriginalPage + BaseAddressPages;

        Page = OriginalPage;
        LastPage = OriginalLastPage;

        //
        // Close the gaps, then examine the range for a fit.
        //

        if (Page < LowestPfn) {
            Page = LowestPfn;
        }

        if (LastPage > HighestPfn + 1) {
            LastPage = HighestPfn + 1;
        }

        HighestStartPage = LastPage - SizeInPages;

        if (Page > HighestStartPage) {
            return NULL;
        }

        if (BoundaryPfn != 0) {
            do {
                if (((Page ^ (Page + SizeInPages - 1)) & BoundaryMask) == 0) {

                    //
                    // This portion of the range meets the alignment
                    // requirements.
                    //

                    break;
                }
                Page |= (BoundaryPfn - 1);
                Page += 1;
            } while (Page <= HighestStartPage);

            if (Page > HighestStartPage) {
                return NULL;
            }
            BoundaryAllocation = (PVOID)((PCHAR)BaseAddress + ((Page - OriginalPage) << PAGE_SHIFT));

            //
            // The request can be satisfied.  Since specific alignment was
            // requested, return the fit now without getting fancy.
            //

            return BoundaryAllocation;
        }

        //
        // If possible return a chunk on the end to reduce fragmentation.
        //
    
        if (LastPage == OriginalLastPage) {
            return (PVOID)((PCHAR)BaseAddress + ((BaseAddressPages - SizeInPages) << PAGE_SHIFT));
        }
    
        //
        // The end chunk did not satisfy the requirements.  The next best option
        // is to return a chunk from the beginning.  Since that's where the search
        // began, just return the current chunk.
        //

        return (PVOID)((PCHAR)BaseAddress + ((Page - OriginalPage) << PAGE_SHIFT));
    }

    //
    // Check the virtual addresses for physical contiguity.
    //

    PointerPte = MiGetPteAddress (BaseAddress);
    LastPte = PointerPte + BaseAddressPages;

    HighestStartPage = HighestPfn + 1 - SizeInPages;
    PageCount = 0;

    //
    // Initializing PreviousPage is not needed for correctness
    // but without it the compiler cannot compile this code
    // W4 to check for use of uninitialized variables.
    //

    PreviousPage = 0;

    while (PointerPte < LastPte) {

        PteContents = *PointerPte;
        ASSERT (PteContents.u.Hard.Valid == 1);
        Page = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);

        //
        // Before starting a new run, ensure that it
        // can satisfy the location & boundary requirements (if any).
        //

        if (PageCount == 0) {

            if ((Page >= LowestPfn) &&
                (Page <= HighestStartPage) &&
                ((CacheAttribute == MiCached) || (MI_PFN_ELEMENT (Page)->u4.MustBeCached == 0))) {

                if (BoundaryPfn == 0) {
                    PageCount += 1;
                }
                else if (((Page ^ (Page + SizeInPages - 1)) & BoundaryMask) == 0) {
                    //
                    // This run's physical address meets the alignment
                    // requirement.
                    //

                    PageCount += 1;
                }
            }

            if (PageCount == SizeInPages) {

                if (CacheAttribute != MiCached) {

                    //
                    // Recheck the cachability while holding the PFN lock.
                    //

                    LOCK_PFN2 (OldIrql);
                
                    if (MI_PFN_ELEMENT (Page)->u4.MustBeCached == 0) {
                        PageCount = 1;
                    }
                    else {
                        PageCount = 0;
                    }

                    UNLOCK_PFN2 (OldIrql);
                }

                if (PageCount != 0) {

                    //
                    // Success - found a single page satifying the requirements.
                    //

                    BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte);
                    return BaseAddress;
                }
            }

            PreviousPage = Page;
            PointerPte += 1;
            continue;
        }

        if (Page != PreviousPage + 1) {

            //
            // This page is not physically contiguous.  Start over.
            //

            PageCount = 0;
            continue;
        }

        PageCount += 1;

        if (PageCount == SizeInPages) {

            if (CacheAttribute != MiCached) {

                LOCK_PFN2 (OldIrql);

                do {
                    if ((MI_PFN_ELEMENT (Page))->u4.MustBeCached == 1) {
                        break;
                    }

                    Page -= 1;
                    PageCount -= 1;

                } while (PageCount != 0);

                UNLOCK_PFN2 (OldIrql);

                if (PageCount != 0) {
                    PageCount = 0;
                    continue;
                }

                PageCount = SizeInPages;
            }

            //
            // Success - found a page range satifying the requirements.
            //

            BaseAddress = MiGetVirtualAddressMappedByPte (PointerPte - PageCount + 1);
            return BaseAddress;
        }

        PreviousPage = Page;
        PointerPte += 1;
    }

    return NULL;
}


VOID
MmLockPagableSectionByHandle (
    IN PVOID ImageSectionHandle
    )


/*++

Routine Description:

    This routine checks to see if the specified pages are resident in
    the process's working set and if so the reference count for the
    page is incremented.  The allows the virtual address to be accessed
    without getting a hard page fault (have to go to the disk... except
    for extremely rare case when the page table page is removed from the
    working set and migrates to the disk.

    If the virtual address is that of the system wide global "cache" the
    virtual address of the "locked" pages is always guaranteed to
    be valid.

    NOTE: This routine is not to be used for general locking of user
    addresses - use MmProbeAndLockPages.  This routine is intended for
    well behaved system code like the file system caches which allocates
    virtual addresses for mapping files AND guarantees that the mapping
    will not be modified (deleted or changed) while the pages are locked.

Arguments:

    ImageSectionHandle - Supplies the value returned by a previous call
                         to MmLockPagableDataSection.  This is a pointer to
                         the section header for the image.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    ULONG EntryCount;
    ULONG OriginalCount;
    PKTHREAD CurrentThread;
    PIMAGE_SECTION_HEADER NtSection;
    PVOID BaseAddress;
    ULONG SizeToLock;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PLONG SectionLockCountPointer;

    if (MI_IS_PHYSICAL_ADDRESS(ImageSectionHandle)) {

        //
        // No need to lock physical addresses.
        //

        return;
    }

    NtSection = (PIMAGE_SECTION_HEADER)ImageSectionHandle;

    BaseAddress = SECTION_BASE_ADDRESS(NtSection);
    SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (NtSection);

    ASSERT (!MI_IS_SYSTEM_CACHE_ADDRESS(BaseAddress));

    //
    // The address must be within the system space.
    //

    ASSERT (BaseAddress >= MmSystemRangeStart);

    SizeToLock = NtSection->SizeOfRawData;

    //
    // Generally, SizeOfRawData is larger than VirtualSize for each
    // section because it includes the padding to get to the subsection
    // alignment boundary.  However, if the image is linked with
    // subsection alignment == native page alignment, the linker will
    // have VirtualSize be much larger than SizeOfRawData because it
    // will account for all the bss.
    //

    if (SizeToLock < NtSection->Misc.VirtualSize) {
        SizeToLock = NtSection->Misc.VirtualSize;
    }

    PointerPte = MiGetPteAddress(BaseAddress);
    LastPte = MiGetPteAddress((PCHAR)BaseAddress + SizeToLock - 1);

    ASSERT (SizeToLock != 0);

    CurrentThread = KeGetCurrentThread ();

    KeEnterCriticalRegionThread (CurrentThread);

    //
    //  The lock count values have the following meanings :
    //
    //  Value of 0 means unlocked.
    //  Value of 1 means lock in progress by another thread.
    //  Value of 2 or more means locked.
    //
    //  If the value is 1, this thread must block until the other thread's
    //  lock operation is complete.
    //

    do {
        EntryCount = *SectionLockCountPointer;

        if (EntryCount != 1) {

            OriginalCount = InterlockedCompareExchange (SectionLockCountPointer,
                                                        EntryCount + 1,
                                                        EntryCount);

            if (OriginalCount == EntryCount) {

                //
                // Success - this is the first thread to update.
                //

                ASSERT (OriginalCount != 1);
                break;
            }

            //
            // Another thread updated the count before this thread's attempt
            // so it's time to start over.
            //
        }
        else {

            //
            // A lock is in progress, wait for it to finish.  This should be
            // generally rare, and even in this case, the pulse will usually
            // wake us.  A timeout is used so that the wait and the pulse do
            // not need to be interlocked.
            //

            InterlockedIncrement (&MmCollidedLockWait);

            KeWaitForSingleObject (&MmCollidedLockEvent,
                                   WrVirtualMemory,
                                   KernelMode,
                                   FALSE,
                                   (PLARGE_INTEGER)&MmShortTime);

            InterlockedDecrement (&MmCollidedLockWait);
        }

    } while (TRUE);

    if (OriginalCount >= 2) {

        //
        // Already locked, just return.
        //

        KeLeaveCriticalRegionThread (CurrentThread);
        return;
    }

    ASSERT (OriginalCount == 0);
    ASSERT (*SectionLockCountPointer == 1);

    //
    // Value was 0 when the lock was obtained.  It is now 1 indicating
    // a lock is in progress.
    //

    MiLockCode (PointerPte, LastPte, MM_LOCK_BY_REFCOUNT);

    //
    // Set lock count to 2 (it was 1 when this started) and check
    // to see if any other threads tried to lock while this was happening.
    //

    ASSERT (*SectionLockCountPointer == 1);
    OriginalCount = InterlockedIncrement (SectionLockCountPointer);
    ASSERT (OriginalCount >= 2);

    if (MmCollidedLockWait != 0) {
        KePulseEvent (&MmCollidedLockEvent, 0, FALSE);
    }

    //
    // Enable user APCs now that the pulse has occurred.  They had to be
    // blocked to prevent any suspensions of this thread as that would
    // stop all waiters indefinitely.
    //

    KeLeaveCriticalRegionThread (CurrentThread);

    return;
}


VOID
MiLockCode (
    IN PMMPTE FirstPte,
    IN PMMPTE LastPte,
    IN ULONG LockType
    )

/*++

Routine Description:

    This routine checks to see if the specified pages are resident in
    the process's working set and if so the reference count for the
    page is incremented.  This allows the virtual address to be accessed
    without getting a hard page fault (have to go to the disk...) except
    for the extremely rare case when the page table page is removed from the
    working set and migrates to the disk.

    If the virtual address is that of the system wide global "cache", the
    virtual address of the "locked" pages is always guaranteed to
    be valid.

    NOTE: This routine is not to be used for general locking of user
    addresses - use MmProbeAndLockPages.  This routine is intended for
    well behaved system code like the file system caches which allocates
    virtual addresses for mapping files AND guarantees that the mapping
    will not be modified (deleted or changed) while the pages are locked.

Arguments:

    FirstPte - Supplies the base address to begin locking.

    LastPte - The last PTE to lock.

    LockType - Supplies either MM_LOCK_BY_REFCOUNT or MM_LOCK_NONPAGE.
               LOCK_BY_REFCOUNT increments the reference count to keep
               the page in memory, LOCK_NONPAGE removes the page from
               the working set so it's locked just like nonpaged pool.

Return Value:

    None.

Environment:

    Kernel mode.

--*/

{
    PMMPFN Pfn1;
    PMMPTE PointerPte;
    MMPTE TempPte;
    MMPTE PteContents;
    WSLE_NUMBER WorkingSetIndex;
    WSLE_NUMBER SwapEntry;
    PFN_NUMBER PageFrameIndex;
    KIRQL OldIrql;
    LOGICAL SessionSpace;
    PMMWSL WorkingSetList;
    PMMSUPPORT Vm;
    PETHREAD CurrentThread;

    ASSERT (!MI_IS_PHYSICAL_ADDRESS(MiGetVirtualAddressMappedByPte(FirstPte)));
    PointerPte = FirstPte;

    CurrentThread = PsGetCurrentThread ();

    SessionSpace = MI_IS_SESSION_IMAGE_ADDRESS (MiGetVirtualAddressMappedByPte(FirstPte));

    if (SessionSpace == TRUE) {
        Vm = &MmSessionSpace->GlobalVirtualAddress->Vm;
        WorkingSetList = MmSessionSpace->Vm.VmWorkingSetList;

        //
        // Session space is never locked by refcount.
        //

        ASSERT (LockType != MM_LOCK_BY_REFCOUNT);
    }
    else {

        Vm = &MmSystemCacheWs;
        WorkingSetList = NULL;
    }

    LOCK_WORKING_SET (Vm);

    LOCK_PFN (OldIrql);

    do {

        PteContents = *PointerPte;
        ASSERT (PteContents.u.Long != ZeroKernelPte.u.Long);
        if (PteContents.u.Hard.Valid == 1) {

            //
            // This address is already in the system (or session) working set.
            //

            Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

            //
            // Up the reference count so the page cannot be released.
            //

            MI_ADD_LOCKED_PAGE_CHARGE (Pfn1, TRUE, 36);
            Pfn1->u3.e2.ReferenceCount += 1;

            if (LockType != MM_LOCK_BY_REFCOUNT) {

                //
                // If the page is in the system working set, remove it.
                // The system working set lock MUST be owned to check to
                // see if this page is in the working set or not.  This
                // is because the pager may have just released the PFN lock,
                // acquired the system lock and is now trying to add the
                // page to the system working set.
                //
                // If the page is in the SESSION working set, it cannot be
                // removed as all these pages are carefully accounted for.
                // Instead move it to the locked portion of the working set
                // if it is not there already.
                //

                if (Pfn1->u1.WsIndex != 0) {

                    UNLOCK_PFN (OldIrql);

                    if (SessionSpace == TRUE) {

                        WorkingSetIndex = MiLocateWsle (
                                    MiGetVirtualAddressMappedByPte(PointerPte),
                                    WorkingSetList,
                                    Pfn1->u1.WsIndex);

                        if (WorkingSetIndex >= WorkingSetList->FirstDynamic) {
                
                            SwapEntry = WorkingSetList->FirstDynamic;
                
                            if (WorkingSetIndex != WorkingSetList->FirstDynamic) {
                
                                //
                                // Swap this entry with the one at first
                                // dynamic.  Note that the working set index
                                // in the PTE is updated here as well.
                                //
                
                                MiSwapWslEntries (WorkingSetIndex,
                                                  SwapEntry,
                                                  Vm,
                                                  FALSE);
                            }
                
                            WorkingSetList->FirstDynamic += 1;

                            //
                            // Indicate that the page is now locked.
                            //
            
                            MmSessionSpace->Wsle[SwapEntry].u1.e1.LockedInWs = 1;
                            MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_LOCK_CODE2, 1);
                            InterlockedExchangeAddSizeT (&MmSessionSpace->NonPagablePages, 1);
                            LOCK_PFN (OldIrql);
                            Pfn1->u1.WsIndex = SwapEntry;

                            //
                            // Adjust available pages as this page is now not
                            // in any working set, just like a non-paged pool
                            // page.
                            //
            
                            MI_DECREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_ALLOCATE_LOCK_CODE1);

                            if (Pfn1->u3.e1.PrototypePte == 0) {
                                InterlockedDecrement (&MmTotalSystemDriverPages);
                            }
                        }
                        else {
                            ASSERT (MmSessionSpace->Wsle[WorkingSetIndex].u1.e1.LockedInWs == 1);
                            LOCK_PFN (OldIrql);
                        }
                    }
                    else {
                        MiRemoveWsle (Pfn1->u1.WsIndex, MmSystemCacheWorkingSetList);
                        MiReleaseWsle (Pfn1->u1.WsIndex, &MmSystemCacheWs);

                        MI_SET_PTE_IN_WORKING_SET (PointerPte, 0);
                        LOCK_PFN (OldIrql);
                        MI_ZERO_WSINDEX (Pfn1);

                        //
                        // Adjust available pages as this page is now not in any
                        // working set, just like a non-paged pool page.
                        //
        
                        MI_DECREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_ALLOCATE_LOCK_CODE2);

                        if (Pfn1->u3.e1.PrototypePte == 0) {
                            InterlockedDecrement (&MmTotalSystemDriverPages);
                        }
                    }

                }
                ASSERT (Pfn1->u3.e2.ReferenceCount > 1);
                MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1, 37);
            }
        }
        else if (PteContents.u.Soft.Prototype == 1) {

            //
            // Page is not in memory and it is a prototype.
            //

            MiMakeSystemAddressValidPfnSystemWs (
                    MiGetVirtualAddressMappedByPte(PointerPte), OldIrql);

            continue;
        }
        else if (PteContents.u.Soft.Transition == 1) {

            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_TRANSITION_PTE (&PteContents);

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            if ((Pfn1->u3.e1.ReadInProgress) ||
                (Pfn1->u4.InPageError)) {

                //
                // Page read is ongoing, force a collided fault.
                //

                MiMakeSystemAddressValidPfnSystemWs (
                        MiGetVirtualAddressMappedByPte(PointerPte), OldIrql);

                continue;
            }

            //
            // Paged pool is trimmed without regard to sharecounts.
            // This means a paged pool PTE can be in transition while
            // the page is still marked active.
            //

            if (Pfn1->u3.e1.PageLocation == ActiveAndValid) {

                ASSERT (((Pfn1->PteAddress >= MiGetPteAddress(MmPagedPoolStart)) &&
                        (Pfn1->PteAddress <= MiGetPteAddress(MmPagedPoolEnd))) ||
                        ((Pfn1->PteAddress >= MiGetPteAddress(MmSpecialPoolStart)) &&
                        (Pfn1->PteAddress <= MiGetPteAddress(MmSpecialPoolEnd))));

                //
                // Don't increment the valid PTE count for the
                // paged pool page.
                //

                ASSERT (Pfn1->u2.ShareCount != 0);
                ASSERT (Pfn1->u3.e2.ReferenceCount != 0);
                Pfn1->u2.ShareCount += 1;
            }
            else {

                if (MmAvailablePages == 0) {

                    //
                    // This can only happen if the system is utilizing
                    // a hardware compression cache.  This ensures that
                    // only a safe amount of the compressed virtual cache
                    // is directly mapped so that if the hardware gets
                    // into trouble, we can bail it out.
                    //
                    // Just unlock everything here to give the compression
                    // reaper a chance to ravage pages and then retry.
                    //

                    UNLOCK_PFN (OldIrql);

                    UNLOCK_WORKING_SET (Vm);

                    LOCK_WORKING_SET (Vm);

                    LOCK_PFN (OldIrql);

                    continue;
                }

                MiUnlinkPageFromList (Pfn1);

                //
                // Increment the reference count and set the share count to 1.
                // Note the reference count may be 1 already if a modified page
                // write is underway.  The systemwide locked page charges
                // are correct in either case and nothing needs to be done
                // just yet.
                //

                Pfn1->u3.e2.ReferenceCount += 1;
                Pfn1->u2.ShareCount = 1;
            }

            Pfn1->u3.e1.PageLocation = ActiveAndValid;
            Pfn1->u3.e1.CacheAttribute = MiCached;

            MI_MAKE_VALID_PTE (TempPte,
                               PageFrameIndex,
                               Pfn1->OriginalPte.u.Soft.Protection,
                               PointerPte);

            MI_WRITE_VALID_PTE (PointerPte, TempPte);

            //
            // Increment the reference count one for putting it the
            // working set list and one for locking it for I/O.
            //

            if (LockType == MM_LOCK_BY_REFCOUNT) {

                //
                // Lock the page in the working set by upping the
                // reference count.
                //

                MI_ADD_LOCKED_PAGE_CHARGE (Pfn1, TRUE, 34);
                Pfn1->u3.e2.ReferenceCount += 1;
                Pfn1->u1.Event = NULL;

                UNLOCK_PFN (OldIrql);

                WorkingSetIndex = MiAllocateWsle (Vm,
                                                  PointerPte,
                                                  Pfn1,
                                                  0);

                if (WorkingSetIndex == 0) {

                    //
                    // No working set entry was available.  Another (broken
                    // or malicious thread) may have already written to this
                    // page since the PTE was made valid.  So trim the
                    // page instead of discarding it.
                    //
                    // Note the page cannot be a prototype because the
                    // PTE was transition above.
                    //

                    ASSERT (Pfn1->u3.e1.PrototypePte == 0);

                    LOCK_PFN (OldIrql);

                    //
                    // Undo the reference count & locked page charge (if any).
                    //

                    MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1, 51);

                    UNLOCK_PFN (OldIrql);

                    MiTrimPte (MiGetVirtualAddressMappedByPte (PointerPte),
                               PointerPte,
                               Pfn1,
                               NULL,
                               ZeroPte);

                    //
                    // Release all locks so that other threads (like the
                    // working set trimmer) can try to freely make memory.
                    //

                    UNLOCK_WORKING_SET (Vm);

                    KeDelayExecutionThread (KernelMode,
                                            FALSE,
                                            (PLARGE_INTEGER)&Mm30Milliseconds);

                    LOCK_WORKING_SET (Vm);

                    LOCK_PFN (OldIrql);

                    //
                    // Retry the same page now.
                    //

                    continue;
                }

                LOCK_PFN (OldIrql);
            }
            else {

                //
                // The wsindex field must be zero because the
                // page is not in the system (or session) working set.
                //

                ASSERT (Pfn1->u1.WsIndex == 0);

                //
                // Adjust available pages as this page is now not in any
                // working set, just like a non-paged pool page.  On entry
                // this page was in transition so it was part of the
                // available pages by definition.
                //

                MI_DECREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_ALLOCATE_LOCK_CODE3);

                if (Pfn1->u3.e1.PrototypePte == 0) {
                    InterlockedDecrement (&MmTotalSystemDriverPages);
                }
                if (SessionSpace == TRUE) {
                    MM_BUMP_SESS_COUNTER (MM_DBG_SESSION_NP_LOCK_CODE1, 1);
                    InterlockedExchangeAddSizeT (&MmSessionSpace->NonPagablePages, 1);
                }
            }
        }
        else {

            //
            // Page is not in memory.
            //

            MiMakeSystemAddressValidPfnSystemWs (
                    MiGetVirtualAddressMappedByPte(PointerPte), OldIrql);

            continue;
        }

        PointerPte += 1;
    } while (PointerPte <= LastPte);

    UNLOCK_PFN (OldIrql);

    UNLOCK_WORKING_SET (Vm);

    return;
}


NTSTATUS
MmGetSectionRange (
    IN PVOID AddressWithinSection,
    OUT PVOID *StartingSectionAddress,
    OUT PULONG SizeofSection
    )
{
    ULONG Span;
    PKTHREAD CurrentThread;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    ULONG i;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER NtSection;
    NTSTATUS Status;
    ULONG_PTR Rva;

    PAGED_CODE();

    //
    // Search the loaded module list for the data table entry that describes
    // the DLL that was just unloaded. It is possible that an entry is not in
    // the list if a failure occurred at a point in loading the DLL just before
    // the data table entry was generated.
    //

    Status = STATUS_NOT_FOUND;

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceSharedLite (&PsLoadedModuleResource, TRUE);

    DataTableEntry = MiLookupDataTableEntry (AddressWithinSection, TRUE);
    if (DataTableEntry) {

        Rva = (ULONG_PTR)((PUCHAR)AddressWithinSection - (ULONG_PTR)DataTableEntry->DllBase);

        NtHeaders = (PIMAGE_NT_HEADERS) RtlImageNtHeader (DataTableEntry->DllBase);
        if (NtHeaders == NULL) {
            Status = STATUS_NOT_FOUND;
            goto Finished;
        }

        NtSection = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeaders +
                            sizeof(ULONG) +
                            sizeof(IMAGE_FILE_HEADER) +
                            NtHeaders->FileHeader.SizeOfOptionalHeader
                            );

        for (i = 0; i < NtHeaders->FileHeader.NumberOfSections; i += 1) {

            //
            // Generally, SizeOfRawData is larger than VirtualSize for each
            // section because it includes the padding to get to the subsection
            // alignment boundary.  However if the image is linked with
            // subsection alignment == native page alignment, the linker will
            // have VirtualSize be much larger than SizeOfRawData because it
            // will account for all the bss.
            //

            Span = NtSection->SizeOfRawData;

            if (Span < NtSection->Misc.VirtualSize) {
                Span = NtSection->Misc.VirtualSize;
            }

            if ((Rva >= NtSection->VirtualAddress) &&
                (Rva < NtSection->VirtualAddress + Span)) {

                //
                // Found it.
                //

                *StartingSectionAddress = (PVOID)
                    ((PCHAR) DataTableEntry->DllBase + NtSection->VirtualAddress);
                *SizeofSection = Span;
                Status = STATUS_SUCCESS;
                break;
            }

            NtSection += 1;
        }
    }

Finished:

    ExReleaseResourceLite (&PsLoadedModuleResource);
    KeLeaveCriticalRegionThread (CurrentThread);
    return Status;
}


PVOID
MmLockPagableDataSection (
    IN PVOID AddressWithinSection
    )

/*++

Routine Description:

    This functions locks the entire section that contains the specified
    section in memory.  This allows pagable code to be brought into
    memory and to be used as if the code was not really pagable.  This
    should not be done with a high degree of frequency.

Arguments:

    AddressWithinSection - Supplies the address of a function
        contained within a section that should be brought in and locked
        in memory.

Return Value:

    This function returns a value to be used in a subsequent call to
    MmUnlockPagableImageSection.

--*/

{
    ULONG Span;
    PLONG SectionLockCountPointer;
    PKTHREAD CurrentThread;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    ULONG i;
    PIMAGE_NT_HEADERS NtHeaders;
    PIMAGE_SECTION_HEADER NtSection;
    PIMAGE_SECTION_HEADER FoundSection;
    ULONG_PTR Rva;

    PAGED_CODE();

    if (MI_IS_PHYSICAL_ADDRESS(AddressWithinSection)) {

        //
        // Physical address, just return that as the handle.
        //

        return AddressWithinSection;
    }

    //
    // Search the loaded module list for the data table entry that describes
    // the DLL that was just unloaded. It is possible that an entry is not in
    // the list if a failure occurred at a point in loading the DLL just before
    // the data table entry was generated.
    //

    FoundSection = NULL;

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceSharedLite (&PsLoadedModuleResource, TRUE);

    DataTableEntry = MiLookupDataTableEntry (AddressWithinSection, TRUE);

    Rva = (ULONG_PTR)((PUCHAR)AddressWithinSection - (ULONG_PTR)DataTableEntry->DllBase);

    NtHeaders = (PIMAGE_NT_HEADERS) RtlImageNtHeader (DataTableEntry->DllBase);

    if (NtHeaders == NULL) {

        //
        // This is a firmware entry - no one should be trying to lock these.
        //

        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x1234,
                      (ULONG_PTR)AddressWithinSection,
                      1,
                      0);
    }

    NtSection = (PIMAGE_SECTION_HEADER)((ULONG_PTR)NtHeaders +
                        sizeof(ULONG) +
                        sizeof(IMAGE_FILE_HEADER) +
                        NtHeaders->FileHeader.SizeOfOptionalHeader
                        );

    for (i = 0; i < NtHeaders->FileHeader.NumberOfSections; i += 1) {

        //
        // Generally, SizeOfRawData is larger than VirtualSize for each
        // section because it includes the padding to get to the subsection
        // alignment boundary.  However, if the image is linked with
        // subsection alignment == native page alignment, the linker will
        // have VirtualSize be much larger than SizeOfRawData because it
        // will account for all the bss.
        //

        Span = NtSection->SizeOfRawData;

        if (Span < NtSection->Misc.VirtualSize) {
            Span = NtSection->Misc.VirtualSize;
        }

        if ((Rva >= NtSection->VirtualAddress) &&
            (Rva < NtSection->VirtualAddress + Span)) {

            FoundSection = NtSection;

            if (SECTION_BASE_ADDRESS(NtSection) != ((PUCHAR)DataTableEntry->DllBase +
                            NtSection->VirtualAddress)) {

                //
                // Overwrite the PointerToRelocations field (and on Win64, the
                // PointerToLinenumbers field also) so that it contains
                // the Va of this section.
                //
                // NumberOfRelocations & NumberOfLinenumbers contains
                // the Lock Count for the section.
                //

                SECTION_BASE_ADDRESS(NtSection) = ((PUCHAR)DataTableEntry->DllBase +
                                        NtSection->VirtualAddress);

                SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (NtSection);
                *SectionLockCountPointer = 0;
            }

            //
            // Now lock in the code.
            //

#if DBG
            if (MmDebug & MM_DBG_LOCK_CODE) {
                SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (NtSection);
                DbgPrint("MM Lock %wZ %8s %p -> %p : %p %3ld.\n",
                        &DataTableEntry->BaseDllName,
                        NtSection->Name,
                        AddressWithinSection,
                        NtSection,
                        SECTION_BASE_ADDRESS(NtSection),
                        *SectionLockCountPointer);
            }
#endif //DBG

            MmLockPagableSectionByHandle ((PVOID)NtSection);

            break;
        }
        NtSection += 1;
    }

    ExReleaseResourceLite (&PsLoadedModuleResource);
    KeLeaveCriticalRegionThread (CurrentThread);
    if (!FoundSection) {
        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x1234,
                      (ULONG_PTR)AddressWithinSection,
                      0,
                      0);
    }
    return (PVOID)FoundSection;
}


PKLDR_DATA_TABLE_ENTRY
MiLookupDataTableEntry (
    IN PVOID AddressWithinSection,
    IN ULONG ResourceHeld
    )

/*++

Routine Description:

    This functions locates the data table entry that maps the specified address.

Arguments:

    AddressWithinSection - Supplies the address of a function contained
                           within the desired module.

    ResourceHeld - Supplies TRUE if the loaded module resource is already held,
                   FALSE if not.

Return Value:

    The address of the loaded module list data table entry that maps the
    argument address.

--*/

{
    PKTHREAD CurrentThread;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PKLDR_DATA_TABLE_ENTRY FoundEntry;
    PLIST_ENTRY NextEntry;

    PAGED_CODE();

    FoundEntry = NULL;

    //
    // Search the loaded module list for the data table entry that describes
    // the DLL that was just unloaded. It is possible that an entry is not in
    // the list if a failure occurred at a point in loading the DLL just before
    // the data table entry was generated.
    //

    if (!ResourceHeld) {
        CurrentThread = KeGetCurrentThread ();
        KeEnterCriticalRegionThread (CurrentThread);
        ExAcquireResourceSharedLite (&PsLoadedModuleResource, TRUE);
    }
    else {
        CurrentThread = NULL;
    }

    NextEntry = PsLoadedModuleList.Flink;
    ASSERT (NextEntry != NULL);

    do {

        DataTableEntry = CONTAINING_RECORD(NextEntry,
                                           KLDR_DATA_TABLE_ENTRY,
                                           InLoadOrderLinks);

        //
        // Locate the loaded module that contains this address.
        //

        if ( AddressWithinSection >= DataTableEntry->DllBase &&
             AddressWithinSection < (PVOID)((PUCHAR)DataTableEntry->DllBase+DataTableEntry->SizeOfImage) ) {

            FoundEntry = DataTableEntry;
            break;
        }

        NextEntry = NextEntry->Flink;
    } while (NextEntry != &PsLoadedModuleList);

    if (CurrentThread != NULL) {
        ExReleaseResourceLite (&PsLoadedModuleResource);
        KeLeaveCriticalRegionThread (CurrentThread);
    }
    return FoundEntry;
}

VOID
MmUnlockPagableImageSection (
    IN PVOID ImageSectionHandle
    )

/*++

Routine Description:

    This function unlocks from memory, the pages locked by a preceding call to
    MmLockPagableDataSection.

Arguments:

    ImageSectionHandle - Supplies the value returned by a previous call
                         to MmLockPagableDataSection.

Return Value:

    None.

--*/

{
    PKTHREAD CurrentThread;
    PIMAGE_SECTION_HEADER NtSection;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PVOID BaseAddress;
    ULONG SizeToUnlock;
    ULONG Count;
    PLONG SectionLockCountPointer;

    if (MI_IS_PHYSICAL_ADDRESS(ImageSectionHandle)) {

        //
        // No need to unlock physical addresses.
        //

        return;
    }

    NtSection = (PIMAGE_SECTION_HEADER)ImageSectionHandle;

    //
    // Address must be in the system working set.
    //

    BaseAddress = SECTION_BASE_ADDRESS(NtSection);
    SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (NtSection);
    SizeToUnlock = NtSection->SizeOfRawData;

    //
    // Generally, SizeOfRawData is larger than VirtualSize for each
    // section because it includes the padding to get to the subsection
    // alignment boundary.  However, if the image is linked with
    // subsection alignment == native page alignment, the linker will
    // have VirtualSize be much larger than SizeOfRawData because it
    // will account for all the bss.
    //

    if (SizeToUnlock < NtSection->Misc.VirtualSize) {
        SizeToUnlock = NtSection->Misc.VirtualSize;
    }

    PointerPte = MiGetPteAddress(BaseAddress);
    LastPte = MiGetPteAddress((PCHAR)BaseAddress + SizeToUnlock - 1);

    CurrentThread = KeGetCurrentThread ();

    //
    // Block user APCs as the initial decrement below could push the count to 1.
    // This puts this thread into the critical path that must finish as all
    // other threads trying to lock the section will be waiting for this thread.
    // Entering a critical region here ensures that a suspend cannot stop us.
    //

    KeEnterCriticalRegionThread (CurrentThread);

    Count = InterlockedDecrement (SectionLockCountPointer);
    
    if (Count < 1) {
        KeBugCheckEx (MEMORY_MANAGEMENT,
                      0x1010,
                      (ULONG_PTR)BaseAddress,
                      (ULONG_PTR)NtSection,
                      *SectionLockCountPointer);
    }

    if (Count != 1) {
        KeLeaveCriticalRegionThread (CurrentThread);
        return;
    }

    LOCK_PFN2 (OldIrql);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 1);

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        ASSERT (Pfn1->u3.e2.ReferenceCount > 1);

        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1, 37);

        PointerPte += 1;

    } while (PointerPte <= LastPte);

    UNLOCK_PFN2 (OldIrql);

    ASSERT (*SectionLockCountPointer == 1);
    Count = InterlockedDecrement (SectionLockCountPointer);
    ASSERT (Count == 0);

    if (MmCollidedLockWait != 0) {
        KePulseEvent (&MmCollidedLockEvent, 0, FALSE);
    }

    //
    // Enable user APCs now that the pulse has occurred.  They had to be
    // blocked to prevent any suspensions of this thread as that would
    // stop all waiters indefinitely.
    //

    KeLeaveCriticalRegionThread (CurrentThread);

    return;
}


BOOLEAN
MmIsRecursiveIoFault (
    VOID
    )

/*++

Routine Description:

    This function examines the thread's page fault clustering information
    and determines if the current page fault is occurring during an I/O
    operation.

Arguments:

    None.

Return Value:

    Returns TRUE if the fault is occurring during an I/O operation,
    FALSE otherwise.

--*/

{
    PETHREAD Thread;

    Thread = PsGetCurrentThread ();

    return (BOOLEAN)(Thread->DisablePageFaultClustering |
                     Thread->ForwardClusterOnly);
}


VOID
MmMapMemoryDumpMdl (
    IN OUT PMDL MemoryDumpMdl
    )

/*++

Routine Description:

    For use by crash dump routine ONLY.  Maps an MDL into a fixed
    portion of the address space.  Only 1 MDL can be mapped at a
    time.

Arguments:

    MemoryDumpMdl - Supplies the MDL to map.

Return Value:

    None, fields in MDL updated.

--*/

{
    PFN_NUMBER NumberOfPages;
    PMMPTE PointerPte;
    PCHAR BaseVa;
    MMPTE TempPte;
    PMMPFN Pfn1;
    PPFN_NUMBER Page;

    NumberOfPages = BYTES_TO_PAGES (MemoryDumpMdl->ByteCount + MemoryDumpMdl->ByteOffset);

    ASSERT (NumberOfPages <= 16);

    PointerPte = MmCrashDumpPte;
    BaseVa = (PCHAR)MiGetVirtualAddressMappedByPte(PointerPte);
    MemoryDumpMdl->MappedSystemVa = (PCHAR)BaseVa + MemoryDumpMdl->ByteOffset;
    TempPte = ValidKernelPte;
    Page = (PPFN_NUMBER)(MemoryDumpMdl + 1);

    //
    // If the pages don't span the entire dump virtual address range,
    // build a barrier.  Otherwise use the default barrier provided at the
    // end of the dump virtual address range.
    //

    if (NumberOfPages < 16) {
        MI_WRITE_INVALID_PTE (PointerPte + NumberOfPages, ZeroPte);
        KiFlushSingleTb (BaseVa + (NumberOfPages << PAGE_SHIFT));
    }

    do {

        Pfn1 = MI_PFN_ELEMENT (*Page);
        TempPte = ValidKernelPte;

        switch (Pfn1->u3.e1.CacheAttribute) {
            case MiCached:
                break;

            case MiNonCached:
                MI_DISABLE_CACHING (TempPte);
                break;

            case MiWriteCombined:
                MI_SET_PTE_WRITE_COMBINE (TempPte);
                break;

            default:
                break;
        }

        TempPte.u.Hard.PageFrameNumber = *Page;

        //
        // Note this PTE may be valid or invalid prior to the overwriting here.
        //

        if (PointerPte->u.Hard.Valid == 1) {
            if (PointerPte->u.Long != TempPte.u.Long) {
                MI_WRITE_VALID_PTE_NEW_PAGE (PointerPte, TempPte);
                KiFlushSingleTb (BaseVa);
            }
        }
        else {
            MI_WRITE_VALID_PTE (PointerPte, TempPte);
        }

        Page += 1;
        PointerPte += 1;
        BaseVa += PAGE_SIZE;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    return;
}


VOID
MmReleaseDumpAddresses (
    IN PFN_NUMBER NumberOfPages
    )

/*++

Routine Description:

    For use by hibernate routine ONLY.  Puts zeros back into the
    used dump PTEs.

Arguments:

    NumberOfPages - Supplies the number of PTEs to zero.

Return Value:

    None.

--*/

{
    PVOID BaseVa;
    PMMPTE PointerPte;

    PointerPte = MmCrashDumpPte;
    BaseVa = MiGetVirtualAddressMappedByPte (PointerPte);

    MiZeroMemoryPte (MmCrashDumpPte, NumberOfPages);

    while (NumberOfPages != 0) {

        KiFlushSingleTb (BaseVa);

        BaseVa = (PVOID) ((PCHAR) BaseVa + PAGE_SIZE);
        NumberOfPages -= 1;
    }
}


NTSTATUS
MmSetBankedSection (
    IN HANDLE ProcessHandle,
    IN PVOID VirtualAddress,
    IN ULONG BankLength,
    IN BOOLEAN ReadWriteBank,
    IN PBANKED_SECTION_ROUTINE BankRoutine,
    IN PVOID Context
    )

/*++

Routine Description:

    This function declares a mapped video buffer as a banked
    section.  This allows banked video devices (i.e., even
    though the video controller has a megabyte or so of memory,
    only a small bank (like 64k) can be mapped at any one time.

    In order to overcome this problem, the pager handles faults
    to this memory, unmaps the current bank, calls off to the
    video driver and then maps in the new bank.

    This function creates the necessary structures to allow the
    video driver to be called from the pager.

 ********************* NOTE NOTE NOTE *************************
    At this time only read/write banks are supported!

Arguments:

    ProcessHandle - Supplies a handle to the process in which to
                    support the banked video function.

    VirtualAddress - Supplies the virtual address where the video
                     buffer is mapped in the specified process.

    BankLength - Supplies the size of the bank.

    ReadWriteBank - Supplies TRUE if the bank is read and write.

    BankRoutine - Supplies a pointer to the routine that should be
                  called by the pager.

    Context - Supplies a context to be passed by the pager to the
              BankRoutine.

Return Value:

    Returns the status of the function.

Environment:

    Kernel mode, APC_LEVEL or below.

--*/

{
    KAPC_STATE ApcState;
    NTSTATUS Status;
    PEPROCESS Process;
    PMMVAD Vad;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    MMPTE TempPte;
    ULONG_PTR size;
    LONG count;
    ULONG NumberOfPtes;
    PMMBANKED_SECTION Bank;

    PAGED_CODE ();

    UNREFERENCED_PARAMETER (ReadWriteBank);

    //
    // Reference the specified process handle for VM_OPERATION access.
    //

    Status = ObReferenceObjectByHandle ( ProcessHandle,
                                         PROCESS_VM_OPERATION,
                                         PsProcessType,
                                         KernelMode,
                                         (PVOID *)&Process,
                                         NULL );

    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    KeStackAttachProcess (&Process->Pcb, &ApcState);

    //
    // Get the address creation mutex to block multiple threads from
    // creating or deleting address space at the same time and
    // get the working set mutex so virtual address descriptors can
    // be inserted and walked.  Block APCs so an APC which takes a page
    // fault does not corrupt various structures.
    //

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        Status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }

    Vad = MiLocateAddress (VirtualAddress);

    if ((Vad == NULL) ||
        (Vad->StartingVpn != MI_VA_TO_VPN (VirtualAddress)) ||
        (Vad->u.VadFlags.PhysicalMapping == 0)) {
        Status = STATUS_NOT_MAPPED_DATA;
        goto ErrorReturn;
    }

    size = PAGE_SIZE + ((Vad->EndingVpn - Vad->StartingVpn) << PAGE_SHIFT);
    if ((size % BankLength) != 0) {
        Status = STATUS_INVALID_VIEW_SIZE;
        goto ErrorReturn;
    }

    count = -1;
    NumberOfPtes = BankLength;

    do {
        NumberOfPtes = NumberOfPtes >> 1;
        count += 1;
    } while (NumberOfPtes != 0);

    //
    // Turn VAD into Banked VAD
    //

    NumberOfPtes = BankLength >> PAGE_SHIFT;

    Bank = ExAllocatePoolWithTag (NonPagedPool,
                                    sizeof (MMBANKED_SECTION) +
                                       (NumberOfPtes - 1) * sizeof(MMPTE),
                                    'kBmM');
    if (Bank == NULL) {
        Status = STATUS_INSUFFICIENT_RESOURCES;
        goto ErrorReturn;
    }

    Bank->BankShift = PTE_SHIFT + count - PAGE_SHIFT;

    PointerPte = MiGetPteAddress(MI_VPN_TO_VA (Vad->StartingVpn));
    ASSERT (PointerPte->u.Hard.Valid == 1);

    Bank->BasePhysicalPage = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
    Bank->BasedPte = PointerPte;
    Bank->BankSize = BankLength;
    Bank->BankedRoutine = BankRoutine;
    Bank->Context = Context;
    Bank->CurrentMappedPte = PointerPte;

    //
    // Build the template PTEs structure.
    //

    count = 0;
    TempPte = ZeroPte;

    MI_MAKE_VALID_PTE (TempPte,
                       Bank->BasePhysicalPage,
                       MM_READWRITE,
                       PointerPte);

    if (TempPte.u.Hard.Write) {
        MI_SET_PTE_DIRTY (TempPte);
    }

    do {
        Bank->BankTemplate[count] = TempPte;
        TempPte.u.Hard.PageFrameNumber += 1;
        count += 1;
    } while ((ULONG)count < NumberOfPtes );

    LastPte = MiGetPteAddress (MI_VPN_TO_VA (Vad->EndingVpn));

    //
    // Set all PTEs within this range to zero.  Any faults within
    // this range will call the banked routine before making the
    // page valid.
    //

    LOCK_WS_UNSAFE (Process);

    ((PMMVAD_LONG) Vad)->u4.Banked = Bank;

    RtlFillMemory (PointerPte,
                   (size >> (PAGE_SHIFT - PTE_SHIFT)),
                   (UCHAR)ZeroPte.u.Long);

    KeFlushEntireTb (TRUE, TRUE);

    UNLOCK_WS_UNSAFE (Process);

    Status = STATUS_SUCCESS;

ErrorReturn:

    UNLOCK_ADDRESS_SPACE (Process);
    KeUnstackDetachProcess (&ApcState);
    ObDereferenceObject (Process);
    return Status;
}

PVOID
MmMapVideoDisplay (
     IN PHYSICAL_ADDRESS PhysicalAddress,
     IN SIZE_T NumberOfBytes,
     IN MEMORY_CACHING_TYPE CacheType
     )

/*++

Routine Description:

    This function maps the specified physical address into the non-pagable
    portion of the system address space.

Arguments:

    PhysicalAddress - Supplies the starting physical address to map.

    NumberOfBytes - Supplies the number of bytes to map.

    CacheType - Supplies MmNonCached if the physical address is to be mapped
                as non-cached, MmCached if the address should be cached, and
                MmWriteCombined if the address should be cached and
                write-combined as a frame buffer. For I/O device registers,
                this is usually specified as MmNonCached.

Return Value:

    Returns the virtual address which maps the specified physical addresses.
    The value NULL is returned if sufficient virtual address space for
    the mapping could not be found.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PAGED_CODE();

    return MmMapIoSpace (PhysicalAddress, NumberOfBytes, CacheType);
}

VOID
MmUnmapVideoDisplay (
     IN PVOID BaseAddress,
     IN SIZE_T NumberOfBytes
     )

/*++

Routine Description:

    This function unmaps a range of physical address which were previously
    mapped via an MmMapVideoDisplay function call.

Arguments:

    BaseAddress - Supplies the base virtual address where the physical
                  address was previously mapped.

    NumberOfBytes - Supplies the number of bytes which were mapped.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    MmUnmapIoSpace (BaseAddress, NumberOfBytes);
    return;
}


VOID
MmLockPagedPool (
    IN PVOID Address,
    IN SIZE_T SizeInBytes
    )

/*++

Routine Description:

    Locks the specified address (which MUST reside in paged pool) into
    memory until MmUnlockPagedPool is called.

Arguments:

    Address - Supplies the address in paged pool to lock.

    SizeInBytes - Supplies the size in bytes to lock.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;

    PointerPte = MiGetPteAddress (Address);
    LastPte = MiGetPteAddress ((PVOID)((PCHAR)Address + (SizeInBytes - 1)));

    MiLockCode (PointerPte, LastPte, MM_LOCK_BY_REFCOUNT);

    return;
}

NTKERNELAPI
VOID
MmUnlockPagedPool (
    IN PVOID Address,
    IN SIZE_T SizeInBytes
    )

/*++

Routine Description:

    Unlocks paged pool that was locked with MmLockPagedPool.

Arguments:

    Address - Supplies the address in paged pool to unlock.

    Size - Supplies the size to unlock.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    KIRQL OldIrql;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;

    MmLockPagableSectionByHandle(ExPageLockHandle);
    PointerPte = MiGetPteAddress (Address);
    LastPte = MiGetPteAddress ((PVOID)((PCHAR)Address + (SizeInBytes - 1)));
    LOCK_PFN (OldIrql);

    do {
        ASSERT (PointerPte->u.Hard.Valid == 1);

        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        ASSERT (Pfn1->u3.e2.ReferenceCount > 1);

        MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (Pfn1, 35);

        PointerPte += 1;
    } while (PointerPte <= LastPte);

    UNLOCK_PFN (OldIrql);
    MmUnlockPagableImageSection(ExPageLockHandle);
    return;
}

NTKERNELAPI
ULONG
MmGatherMemoryForHibernate (
    IN PMDL Mdl,
    IN BOOLEAN Wait
    )

/*++

Routine Description:

    Finds enough memory to fill in the pages of the MDL for power management
    hibernate function.

Arguments:

    Mdl - Supplies an MDL, the start VA field should be NULL.  The length
          field indicates how many pages to obtain.

    Wait - FALSE to fail immediately if the pages aren't available.

Return Value:

    TRUE if the MDL could be filled in, FALSE otherwise.

Environment:

    Kernel mode, IRQL of DISPATCH_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    PFN_NUMBER AvailablePages;
    PFN_NUMBER PagesNeeded;
    PPFN_NUMBER Pages;
    PFN_NUMBER i;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn1;
    ULONG status;
    PKTHREAD CurrentThread;

    status = FALSE;

    PagesNeeded = Mdl->ByteCount >> PAGE_SHIFT;
    Pages = (PPFN_NUMBER)(Mdl + 1);

    i = Wait ? 100 : 1;

    CurrentThread = KeGetCurrentThread ();

    KeEnterCriticalRegionThread (CurrentThread);

    InterlockedIncrement (&MiDelayPageFaults);

    do {

        LOCK_PFN2 (OldIrql);

        MiDeferredUnlockPages (MI_DEFER_PFN_HELD);

        //
        // Don't use MmAvailablePages here because if compression hardware is
        // being used we would bail prematurely.  Check the lists explicitly
        // in order to provide our caller with the maximum number of pages.
        //

        AvailablePages = MmZeroedPageListHead.Total +
                         MmFreePageListHead.Total +
                         MmStandbyPageListHead.Total;

        if (AvailablePages > PagesNeeded) {

            //
            // Fill in the MDL.
            //

            do {
                PageFrameIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_PTE (NULL));
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
#if DBG
                Pfn1->PteAddress = (PVOID) (ULONG_PTR)X64K;
#endif
                MI_SET_PFN_DELETED (Pfn1);
                Pfn1->u3.e2.ReferenceCount += 1;
                Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
                *Pages = PageFrameIndex;
                Pages += 1;
                PagesNeeded -= 1;
            } while (PagesNeeded);
            UNLOCK_PFN2 (OldIrql);
            Mdl->MdlFlags |= MDL_PAGES_LOCKED;
            status = TRUE;
            break;
        }

        UNLOCK_PFN2 (OldIrql);

        //
        // If we're being called at DISPATCH_LEVEL we cannot move pages to
        // the standby list because mutexes must be acquired to do so.
        //

        if (OldIrql > APC_LEVEL) {
            break;
        }

        if (!i) {
            break;
        }

        //
        // Attempt to move pages to the standby list.
        //

        MmEmptyAllWorkingSets ();
        MiFlushAllPages();

        KeDelayExecutionThread (KernelMode,
                                FALSE,
                                (PLARGE_INTEGER)&Mm30Milliseconds);
        i -= 1;

    } while (TRUE);

    InterlockedDecrement (&MiDelayPageFaults);

    KeLeaveCriticalRegionThread (CurrentThread);

    return status;
}

NTKERNELAPI
VOID
MmReturnMemoryForHibernate (
    IN PMDL Mdl
    )

/*++

Routine Description:

    Returns memory from MmGatherMemoryForHibername.

Arguments:

    Mdl - Supplies an MDL, the start VA field should be NULL.  The length
          field indicates how many pages to obtain.

Return Value:

    None.

Environment:

    Kernel mode, IRQL of APC_LEVEL or below.

--*/

{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PPFN_NUMBER Pages;
    PPFN_NUMBER LastPage;

    Pages = (PPFN_NUMBER)(Mdl + 1);
    LastPage = Pages + (Mdl->ByteCount >> PAGE_SHIFT);

    LOCK_PFN2 (OldIrql);

    do {
        Pfn1 = MI_PFN_ELEMENT (*Pages);
        MiDecrementReferenceCount (Pfn1, *Pages);
        Pages += 1;
    } while (Pages < LastPage);

    UNLOCK_PFN2 (OldIrql);
    return;
}


VOID
MmEnablePAT (
     VOID
     )

/*++

Routine Description:

    This routine enables the page attribute capability for individual PTE
    mappings.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode.

--*/
{
    MiWriteCombiningPtes = TRUE;
}

LOGICAL
MmIsSystemAddressLocked (
    IN PVOID VirtualAddress
    )

/*++

Routine Description:

    This routine determines whether the specified system address is currently
    locked.

    This routine should only be called for debugging purposes, as it is not
    guaranteed upon return to the caller that the address is still locked.
    (The address could easily have been trimmed prior to return).

Arguments:

    VirtualAddress - Supplies the virtual address to check.

Return Value:

    TRUE if the address is locked.  FALSE if not.

Environment:

    DISPATCH LEVEL or below.  No memory management locks may be held.

--*/
{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PMMPTE PointerPte;
    PFN_NUMBER PageFrameIndex;

    if (IS_SYSTEM_ADDRESS (VirtualAddress) == FALSE) {
        return FALSE;
    }

    if (MI_IS_PHYSICAL_ADDRESS (VirtualAddress)) {
        return TRUE;
    }

    //
    // Hyperspace and page maps are not treated as locked down.
    //

    if (MI_IS_PROCESS_SPACE_ADDRESS (VirtualAddress) == TRUE) {
        return FALSE;
    }

#if defined (_IA64_)
    if (MI_IS_KERNEL_PTE_ADDRESS (VirtualAddress) == TRUE) {
        return FALSE;
    }
#endif

    PointerPte = MiGetPteAddress (VirtualAddress);

    LOCK_PFN2 (OldIrql);

    if (MiIsAddressValid (VirtualAddress, TRUE) == FALSE) {
        UNLOCK_PFN2 (OldIrql);
        return FALSE;
    }

    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

    //
    // Note that the mapped page may not be in the PFN database.  Treat
    // this as locked.
    //

    if (!MI_IS_PFN (PageFrameIndex)) {
        UNLOCK_PFN2 (OldIrql);
        return TRUE;
    }

    Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

    //
    // Check for the page being locked by reference.
    //

    if (Pfn1->u3.e2.ReferenceCount > 1) {
        UNLOCK_PFN2 (OldIrql);
        return TRUE;
    }

    if (Pfn1->u3.e2.ReferenceCount > Pfn1->u2.ShareCount) {
        UNLOCK_PFN2 (OldIrql);
        return TRUE;
    }

    //
    // Check whether the page is locked into the working set.
    //

    if (Pfn1->u1.Event == NULL) {
        UNLOCK_PFN2 (OldIrql);
        return TRUE;
    }

    UNLOCK_PFN2 (OldIrql);

    return FALSE;
}

LOGICAL
MmAreMdlPagesLocked (
    IN PMDL MemoryDescriptorList
    )

/*++

Routine Description:

    This routine determines whether the pages described by the argument
    MDL are currently locked.

    This routine should only be called for debugging purposes, as it is not
    guaranteed upon return to the caller that the pages are still locked.

Arguments:

    MemoryDescriptorList - Supplies the memory descriptor list to check.

Return Value:

    TRUE if ALL the pages described by the argument MDL are locked.
    FALSE if not.

Environment:

    DISPATCH LEVEL or below.  No memory management locks may be held.

--*/
{
    PFN_NUMBER NumberOfPages;
    PPFN_NUMBER Page;
    PVOID StartingVa;
    PMMPFN Pfn1;
    KIRQL OldIrql;

    //
    // We'd like to assert that MDL_PAGES_LOCKED is set but can't because
    // some drivers have privately constructed MDLs and they never set the
    // bit properly.
    //

    if ((MemoryDescriptorList->MdlFlags & (MDL_IO_SPACE | MDL_SOURCE_IS_NONPAGED_POOL)) != 0) {
        return TRUE;
    }

    StartingVa = (PVOID)((PCHAR)MemoryDescriptorList->StartVa +
                    MemoryDescriptorList->ByteOffset);

    NumberOfPages = ADDRESS_AND_SIZE_TO_SPAN_PAGES(StartingVa,
                                              MemoryDescriptorList->ByteCount);

    Page = (PPFN_NUMBER)(MemoryDescriptorList + 1);

    LOCK_PFN2 (OldIrql);

    do {

        if (*Page == MM_EMPTY_LIST) {

            //
            // There are no more locked pages.
            //

            break;
        }

        //
        // Note that the mapped page may not be in the PFN database.  Treat
        // this as locked.
        //

        if (MI_IS_PFN (*Page)) {

            Pfn1 = MI_PFN_ELEMENT (*Page);

            //
            // Check for the page being locked by reference
            //
            // - or -
            //
            // whether the page is locked into the working set.
            //
        
            if ((Pfn1->u3.e2.ReferenceCount <= Pfn1->u2.ShareCount) &&
                (Pfn1->u3.e2.ReferenceCount <= 1) &&
                (Pfn1->u1.Event != NULL)) {

                //
                // The page is not locked by reference or in a working set.
                //
    
                UNLOCK_PFN2 (OldIrql);
            
                return FALSE;
            }
        }

        Page += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    UNLOCK_PFN2 (OldIrql);

    return TRUE;
}

#if DBG

VOID
MiVerifyLockedPageCharges (
    VOID
    )
{
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PFN_NUMBER start;
    PFN_NUMBER count;
    PFN_NUMBER Page;
    PFN_NUMBER LockCharged;

    if (MiPrintLockedPages == 0) {
        return;
    }

    if (KeGetCurrentIrql() > APC_LEVEL) {
        return;
    }

    start = 0;
    LockCharged = 0;

    KeAcquireGuardedMutex (&MmDynamicMemoryMutex);

    LOCK_PFN (OldIrql);

    do {

        count = MmPhysicalMemoryBlock->Run[start].PageCount;
        Page = MmPhysicalMemoryBlock->Run[start].BasePage;

        if (count != 0) {
            Pfn1 = MI_PFN_ELEMENT (Page);
            do {
                if (Pfn1->u4.LockCharged == 1) {
                    if (MiPrintLockedPages & 0x4) {
                        DbgPrint ("%x ", MI_PFN_ELEMENT_TO_INDEX (Pfn1));
                    }
                    LockCharged += 1;
                }
                count -= 1;
                Pfn1 += 1;
            } while (count != 0);
        }

        start += 1;
    } while (start != MmPhysicalMemoryBlock->NumberOfRuns);

    if (LockCharged != MmSystemLockPagesCount) {
        if (MiPrintLockedPages & 0x1) {
            DbgPrint ("MM: Locked pages MISMATCH %u %u\n",
                LockCharged, MmSystemLockPagesCount);
        }
    }
    else {
        if (MiPrintLockedPages & 0x2) {
            DbgPrint ("MM: Locked pages ok %u\n",
                LockCharged);
        }
    }

    UNLOCK_PFN (OldIrql);

    KeReleaseGuardedMutex (&MmDynamicMemoryMutex);

    return;
}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\mapview.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   mapview.c

Abstract:

    This module contains the routines which implement the
    NtMapViewOfSection service.

Author:

    Lou Perazzoli (loup) 22-May-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"
#if defined(_WIN64)
#include <wow64t.h>
#endif

const ULONG MMDB = 'bDmM';
extern const ULONG MMVADKEY;

#if DBG
#define MI_BP_BADMAPS() TRUE
#else
ULONG MiStopBadMaps;
#define MI_BP_BADMAPS() (MiStopBadMaps & 0x1)
#endif

NTSTATUS
MiSetPageModified (
    IN PMMVAD Vad,
    IN PVOID Address
    );

extern LIST_ENTRY MmLoadedUserImageList;

ULONG   MiSubsectionsConvertedToDynamic;

#define X256MEG (256*1024*1024)

#if DBG
extern PEPROCESS MmWatchProcess;
#endif // DBG

#define ROUND_TO_PAGES64(Size)  (((UINT64)(Size) + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1))

MMSESSION   MmSession;

NTSTATUS
MiMapViewOfImageSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN PSECTION Section,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG_PTR ZeroBits,
    IN SIZE_T ImageCommitment
    );

NTSTATUS
MiMapViewOfDataSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN PSECTION Section,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG ProtectionMask,
    IN SIZE_T CommitSize,
    IN ULONG_PTR ZeroBits,
    IN ULONG AllocationType
    );

VOID
MiRemoveMappedPtes (
    IN PVOID BaseAddress,
    IN ULONG NumberOfPtes,
    IN PCONTROL_AREA ControlArea,
    IN PMMSUPPORT WorkingSetInfo
    );

NTSTATUS
MiMapViewInSystemSpace (
    IN PVOID Section,
    IN PMMSESSION Session,
    OUT PVOID *MappedBase,
    IN OUT PSIZE_T ViewSize
    );

NTSTATUS
MiUnmapViewInSystemSpace (
    IN PMMSESSION Session,
    IN PVOID MappedBase
    );

VOID
MiFillSystemPageDirectory (
    PVOID Base,
    SIZE_T NumberOfBytes
    );

VOID
MiLoadUserSymbols (
    IN PCONTROL_AREA ControlArea,
    IN PVOID StartingAddress,
    IN PEPROCESS Process
    );

#if DBG
VOID
VadTreeWalk (
    VOID
    );

VOID
MiDumpConflictingVad(
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad
    );
#endif //DBG


PVOID
MiCacheImageSymbols(
    IN PVOID ImageBase
    );

PVOID
MiInsertInSystemSpace (
    IN PMMSESSION Session,
    IN ULONG SizeIn64k,
    IN PCONTROL_AREA ControlArea
    );

ULONG
MiRemoveFromSystemSpace (
    IN PMMSESSION Session,
    IN PVOID Base,
    OUT PCONTROL_AREA *ControlArea
    );

VOID
MiInsertPhysicalViewAndRefControlArea (
    IN PEPROCESS Process,
    IN PCONTROL_AREA ControlArea,
    IN PMI_PHYSICAL_VIEW PhysicalView
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(PAGE,NtMapViewOfSection)
#pragma alloc_text(PAGE,MmMapViewOfSection)
#pragma alloc_text(PAGE,MmSecureVirtualMemory)
#pragma alloc_text(PAGE,MiSecureVirtualMemory)
#pragma alloc_text(PAGE,MmUnsecureVirtualMemory)
#pragma alloc_text(PAGE,MiUnsecureVirtualMemory)
#pragma alloc_text(PAGE,MiCacheImageSymbols)
#pragma alloc_text(PAGE,NtAreMappedFilesTheSame)
#pragma alloc_text(PAGE,MiLoadUserSymbols)
#pragma alloc_text(PAGE,MiMapViewOfImageSection)
#pragma alloc_text(PAGE,MiMapViewOfDataSection)
#pragma alloc_text(PAGE,MiMapViewOfPhysicalSection)
#pragma alloc_text(PAGE,MiInsertInSystemSpace)
#pragma alloc_text(PAGE,MmMapViewInSystemSpace)
#pragma alloc_text(PAGE,MmMapViewInSessionSpace)
#pragma alloc_text(PAGE,MiUnmapViewInSystemSpace)
#pragma alloc_text(PAGE,MmUnmapViewInSystemSpace)
#pragma alloc_text(PAGE,MmUnmapViewInSessionSpace)
#pragma alloc_text(PAGE,MiMapViewInSystemSpace)
#pragma alloc_text(PAGE,MiRemoveFromSystemSpace)
#pragma alloc_text(PAGE,MiInitializeSystemSpaceMap)
#pragma alloc_text(PAGE,MiFreeSessionSpaceMap)
#pragma alloc_text(PAGE,MmGetSessionMappedViewInformation)

#pragma alloc_text(PAGELK,MiInsertPhysicalViewAndRefControlArea)
#if DBG
#pragma alloc_text(PAGE,MiDumpConflictingVad)
#endif
#endif


NTSTATUS
NtMapViewOfSection (
    IN HANDLE SectionHandle,
    IN HANDLE ProcessHandle,
    IN OUT PVOID *BaseAddress,
    IN ULONG_PTR ZeroBits,
    IN SIZE_T CommitSize,
    IN OUT PLARGE_INTEGER SectionOffset OPTIONAL,
    IN OUT PSIZE_T ViewSize,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG AllocationType,
    IN ULONG Protect
    )

/*++

Routine Description:

    This function maps a view in the specified subject process to
    the section object.

Arguments:

    SectionHandle - Supplies an open handle to a section object.

    ProcessHandle - Supplies an open handle to a process object.

    BaseAddress - Supplies a pointer to a variable that will receive
                  the base address of the view. If the initial value
                  of this argument is not null, then the view will
                  be allocated starting at the specified virtual
                  address rounded down to the next 64kb address
                  boundary. If the initial value of this argument is
                  null, then the operating system will determine
                  where to allocate the view using the information
                  specified by the ZeroBits argument value and the
                  section allocation attributes (i.e. based and
                  tiled).
        
    ZeroBits - Supplies the number of high order address bits that
               must be zero in the base address of the section
               view. The value of this argument must be less than
               or equal to the maximum number of zero bits and is only
               used when memory management determines where to allocate
               the view (i.e. when BaseAddress is null).
        
               If ZeroBits is zero, then no zero bit constraints are applied.

               If ZeroBits is greater than 0 and less than 32, then it is
               the number of leading zero bits from bit 31.  Bits 63:32 are
               also required to be zero.  This retains compatibility
               with 32-bit systems.
                
               If ZeroBits is greater than 32, then it is considered as
               a mask and the number of leading zeroes are counted out
               in the mask.  This then becomes the zero bits argument.

    CommitSize - Supplies the size of the initially committed region
                 of the view in bytes. This value is rounded up to
                 the next host page size boundary.

    SectionOffset - Supplies the offset from the beginning of the
                    section to the view in bytes. This value is
                    rounded down to the next host page size boundary.

    ViewSize - Supplies a pointer to a variable that will receive
               the actual size in bytes of the view. If the value
               of this argument is zero, then a view of the
               section will be mapped starting at the specified
               section offset and continuing to the end of the
               section. Otherwise the initial value of this
               argument specifies the size of the view in bytes
               and is rounded up to the next host page size
               boundary.
        
    InheritDisposition - Supplies a value that specifies how the
                         view is to be shared by a child process created
                         with a create process operation.

        InheritDisposition Values

         ViewShare - Inherit view and share a single copy
              of the committed pages with a child process
              using the current protection value.

         ViewUnmap - Do not map the view into a child process.

    AllocationType - Supplies the type of allocation.

         MEM_TOP_DOWN
         MEM_DOS_LIM
         MEM_LARGE_PAGES
         MEM_RESERVE - for file mapped sections only.

    Protect - Supplies the protection desired for the region of
              initially committed pages.

        Protect Values


         PAGE_NOACCESS - No access to the committed region
              of pages is allowed. An attempt to read,
              write, or execute the committed region
              results in an access violation (i.e. a GP
              fault).

         PAGE_EXECUTE - Execute access to the committed
              region of pages is allowed. An attempt to
              read or write the committed region results in
              an access violation.

         PAGE_READONLY - Read only and execute access to the
              committed region of pages is allowed. An
              attempt to write the committed region results
              in an access violation.

         PAGE_READWRITE - Read, write, and execute access to
              the region of committed pages is allowed. If
              write access to the underlying section is
              allowed, then a single copy of the pages are
              shared. Otherwise the pages are shared read
              only/copy on write.

Return Value:

    Various NTSTATUS codes.

--*/

{
    PSECTION Section;
    PEPROCESS Process;
    KPROCESSOR_MODE PreviousMode;
    NTSTATUS Status;
    PVOID CapturedBase;
    SIZE_T CapturedViewSize;
    LARGE_INTEGER TempViewSize;
    LARGE_INTEGER CapturedOffset;
    ULONGLONG HighestPhysicalAddressInPfnDatabase;
    ACCESS_MASK DesiredSectionAccess;
    ULONG ProtectMaskForAccess;
    LOGICAL WriteCombined;
    PETHREAD CurrentThread;
    PEPROCESS CurrentProcess;

    PAGED_CODE();

    //
    // Check the zero bits argument for correctness.
    //

#if defined (_WIN64)

    if (ZeroBits >= 32) {

        //
        // ZeroBits is a mask instead of a count.  Translate it to a count now.
        //

        ZeroBits = 64 - RtlFindMostSignificantBit (ZeroBits) - 1;
    }
    else if (ZeroBits) {
        ZeroBits += 32;
    }

#endif

    if (ZeroBits > MM_MAXIMUM_ZERO_BITS) {
        return STATUS_INVALID_PARAMETER_4;
    }

    //
    // Check the inherit disposition flags.
    //

    if ((InheritDisposition > ViewUnmap) ||
        (InheritDisposition < ViewShare)) {
        return STATUS_INVALID_PARAMETER_8;
    }

    //
    // Check the allocation type field.
    //

#ifdef i386

    //
    // Only allow DOS_LIM support for i386.  The MEM_DOS_LIM flag allows
    // map views of data sections to be done on 4k boundaries rather
    // than 64k boundaries.
    //

    if ((AllocationType & ~(MEM_TOP_DOWN | MEM_LARGE_PAGES | MEM_DOS_LIM |
           SEC_NO_CHANGE | MEM_RESERVE)) != 0) {
        return STATUS_INVALID_PARAMETER_9;
    }
#else
    if ((AllocationType & ~(MEM_TOP_DOWN | MEM_LARGE_PAGES |
           SEC_NO_CHANGE | MEM_RESERVE)) != 0) {
        return STATUS_INVALID_PARAMETER_9;
    }

#endif //i386

    //
    // Check the protection field.
    //

    if (Protect & PAGE_WRITECOMBINE) {
        Protect &= ~PAGE_WRITECOMBINE;
        WriteCombined = TRUE;
    }
    else {
        WriteCombined = FALSE;
    }

    ProtectMaskForAccess = MiMakeProtectionMask (Protect);
    if (ProtectMaskForAccess == MM_INVALID_PROTECTION) {
        return STATUS_INVALID_PAGE_PROTECTION;
    }

    ProtectMaskForAccess = ProtectMaskForAccess & 0x7;

    DesiredSectionAccess = MmMakeSectionAccess[ProtectMaskForAccess];

    CurrentThread = PsGetCurrentThread ();
    CurrentProcess = PsGetCurrentProcessByThread (CurrentThread);

    PreviousMode = KeGetPreviousModeByThread(&CurrentThread->Tcb);

    //
    // Establish an exception handler, probe the specified addresses
    // for write access and capture the initial values.
    //

    try {
        if (PreviousMode != KernelMode) {
            ProbeForWritePointer ((PULONG)BaseAddress);
            ProbeForWriteUlong_ptr (ViewSize);

        }

        if (ARGUMENT_PRESENT (SectionOffset)) {
            if (PreviousMode != KernelMode) {
                ProbeForWriteSmallStructure (SectionOffset,
                                             sizeof(LARGE_INTEGER),
                                             PROBE_ALIGNMENT (LARGE_INTEGER));
            }
            CapturedOffset = *SectionOffset;
        }
        else {
            ZERO_LARGE (CapturedOffset);
        }

        //
        // Capture the base address.
        //

        CapturedBase = *BaseAddress;

        //
        // Capture the region size.
        //

        CapturedViewSize = *ViewSize;

    } except (ExSystemExceptionFilter()) {

        //
        // If an exception occurs during the probe or capture
        // of the initial values, then handle the exception and
        // return the exception code as the status value.
        //

        return GetExceptionCode();
    }

    //
    // Make sure the specified starting and ending addresses are
    // within the user part of the virtual address space.
    //

    if (CapturedBase > MM_HIGHEST_VAD_ADDRESS) {

        //
        // Invalid base address.
        //

        return STATUS_INVALID_PARAMETER_3;
    }

    if (((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS - (ULONG_PTR)CapturedBase) <
                                                        CapturedViewSize) {

        //
        // Invalid region size;
        //

        return STATUS_INVALID_PARAMETER_3;

    }

    if (((ULONG_PTR)CapturedBase + CapturedViewSize) > ((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT >> ZeroBits)) {

        //
        // Desired Base and zero_bits conflict.
        //

        return STATUS_INVALID_PARAMETER_4;
    }

    Status = ObReferenceObjectByHandle ( ProcessHandle,
                                         PROCESS_VM_OPERATION,
                                         PsProcessType,
                                         PreviousMode,
                                         (PVOID *)&Process,
                                         NULL );
    if (!NT_SUCCESS(Status)) {
        return Status;
    }

    //
    // Reference the section object, if a view is mapped to the section
    // object, the object is not dereferenced as the virtual address
    // descriptor contains a pointer to the section object.
    //

    Status = ObReferenceObjectByHandle ( SectionHandle,
                                         DesiredSectionAccess,
                                         MmSectionObjectType,
                                         PreviousMode,
                                         (PVOID *)&Section,
                                         NULL );

    if (!NT_SUCCESS(Status)) {
        goto ErrorReturn1;
    }

    if (Section->u.Flags.Image == 0) {

        //
        // This is not an image section, make sure the section page
        // protection is compatible with the specified page protection.
        //

        if (!MiIsProtectionCompatible (Section->InitialPageProtection,
                                       Protect)) {
            Status = STATUS_SECTION_PROTECTION;
            goto ErrorReturn;
        }
    }

    //
    // Check to see if this the section backs physical memory, if
    // so DON'T align the offset on a 64K boundary, just a 4k boundary.
    //

    if (Section->Segment->ControlArea->u.Flags.PhysicalMemory) {
        HighestPhysicalAddressInPfnDatabase = (ULONGLONG)MmHighestPhysicalPage << PAGE_SHIFT;
        CapturedOffset.LowPart = CapturedOffset.LowPart & ~(PAGE_SIZE - 1);

        //
        // No usermode mappings past the end of the PFN database are allowed.
        // Address wrap is checked in the common path.
        //

        if (PreviousMode != KernelMode) {

            if ((ULONGLONG)(CapturedOffset.QuadPart + CapturedViewSize) > HighestPhysicalAddressInPfnDatabase) {
                Status = STATUS_INVALID_PARAMETER_6;
                goto ErrorReturn;
            }
        }

    }
    else {

        //
        // Make sure alignments are correct for specified address
        // and offset into the file.
        //

        if ((AllocationType & MEM_DOS_LIM) == 0) {
            if (((ULONG_PTR)CapturedBase & (X64K - 1)) != 0) {
                Status = STATUS_MAPPED_ALIGNMENT;
                goto ErrorReturn;
            }

            if ((ARGUMENT_PRESENT (SectionOffset)) &&
                ((CapturedOffset.LowPart & (X64K - 1)) != 0)) {
                Status = STATUS_MAPPED_ALIGNMENT;
                goto ErrorReturn;
            }
        }
    }

    //
    // Check to make sure the view size plus the offset is less
    // than the size of the section.
    //

    if ((ULONGLONG) (CapturedOffset.QuadPart + CapturedViewSize) <
        (ULONGLONG)CapturedOffset.QuadPart) {

        Status = STATUS_INVALID_VIEW_SIZE;
        goto ErrorReturn;
    }

    if (((ULONGLONG) (CapturedOffset.QuadPart + CapturedViewSize) >
                 (ULONGLONG)Section->SizeOfSection.QuadPart) &&
        ((AllocationType & MEM_RESERVE) == 0)) {

        Status = STATUS_INVALID_VIEW_SIZE;
        goto ErrorReturn;
    }

    if (CapturedViewSize == 0) {

        //
        // Set the view size to be size of the section less the offset.
        //

        TempViewSize.QuadPart = Section->SizeOfSection.QuadPart -
                                                CapturedOffset.QuadPart;

        CapturedViewSize = (SIZE_T)TempViewSize.QuadPart;

        if (

#if !defined(_WIN64)

            (TempViewSize.HighPart != 0) ||

#endif

            (((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS - (ULONG_PTR)CapturedBase) <
                                                        CapturedViewSize)) {

            //
            // Invalid region size;
            //

            Status = STATUS_INVALID_VIEW_SIZE;
            goto ErrorReturn;
        }
    }

    //
    // Check commit size.
    //

    if ((CommitSize > CapturedViewSize) &&
        ((AllocationType & MEM_RESERVE) == 0)) {
        Status = STATUS_INVALID_PARAMETER_5;
        goto ErrorReturn;
    }

    if (WriteCombined == TRUE) {
        Protect |= PAGE_WRITECOMBINE;
    }

    Status = MmMapViewOfSection ((PVOID)Section,
                                 Process,
                                 &CapturedBase,
                                 ZeroBits,
                                 CommitSize,
                                 &CapturedOffset,
                                 &CapturedViewSize,
                                 InheritDisposition,
                                 AllocationType,
                                 Protect);

    if (!NT_SUCCESS(Status) ) {

        if ((Status == STATUS_CONFLICTING_ADDRESSES) &&
            (Section->Segment->ControlArea->u.Flags.Image) &&
            (Process == CurrentProcess)) {

            DbgkMapViewOfSection (Section,
                                  CapturedBase,
                                  CapturedOffset.LowPart,
                                  CapturedViewSize);
        }
        goto ErrorReturn;
    }

    //
    // Any time the current process maps an image file,
    // a potential debug event occurs. DbgkMapViewOfSection
    // handles these events.
    //

    if ((Section->Segment->ControlArea->u.Flags.Image) &&
        (Process == CurrentProcess)) {

        if (Status != STATUS_IMAGE_NOT_AT_BASE) {
            DbgkMapViewOfSection (Section,
                                  CapturedBase,
                                  CapturedOffset.LowPart,
                                  CapturedViewSize);
        }
    }

    //
    // Establish an exception handler and write the size and base address.
    //

    try {

        *ViewSize = CapturedViewSize;
        *BaseAddress = CapturedBase;

        if (ARGUMENT_PRESENT(SectionOffset)) {
            *SectionOffset = CapturedOffset;
        }

    } except (EXCEPTION_EXECUTE_HANDLER) {
        NOTHING;
    }

ErrorReturn:
    ObDereferenceObject (Section);

ErrorReturn1:
    ObDereferenceObject (Process);
    return Status;
}

NTSTATUS
MmMapViewOfSection (
    IN PVOID SectionToMap,
    IN PEPROCESS Process,
    IN OUT PVOID *CapturedBase,
    IN ULONG_PTR ZeroBits,
    IN SIZE_T CommitSize,
    IN OUT PLARGE_INTEGER SectionOffset,
    IN OUT PSIZE_T CapturedViewSize,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG AllocationType,
    IN ULONG Protect
    )

/*++

Routine Description:

    This function maps a view in the specified subject process to
    the section object.

    This function is a kernel mode interface to allow LPC to map
    a section given the section pointer to map.

    This routine assumes all arguments have been probed and captured.

    ********************************************************************
    ********************************************************************
    ********************************************************************

    NOTE:

    CapturedViewSize, SectionOffset, and CapturedBase must be
    captured in non-paged system space (i.e., kernel stack).

    ********************************************************************
    ********************************************************************
    ********************************************************************

Arguments:

    SectionToMap - Supplies a pointer to the section object.

    Process - Supplies a pointer to the process object.

    BaseAddress - Supplies a pointer to a variable that will receive
                  the base address of the view. If the initial value
                  of this argument is not NULL, then the view will
                  be allocated starting at the specified virtual
                  address rounded down to the next 64kb address
                  boundary. If the initial value of this argument is
                  NULL, then the operating system will determine
                  where to allocate the view using the information
                  specified by the ZeroBits argument value and the
                  section allocation attributes (i.e. based and
                  tiled).

    ZeroBits - Supplies the number of high order address bits that
               must be zero in the base address of the section
               view. The value of this argument must be less than
               21 and is only used when the operating system
               determines where to allocate the view (i.e. when
               BaseAddress is NULL).

    CommitSize - Supplies the size of the initially committed region
                 of the view in bytes. This value is rounded up to
                 the next host page size boundary.

    SectionOffset - Supplies the offset from the beginning of the
                    section to the view in bytes. This value is
                    rounded down to the next host page size boundary.

    ViewSize - Supplies a pointer to a variable that will receive
               the actual size in bytes of the view. If the value
               of this argument is zero, then a view of the
               section will be mapped starting at the specified
               section offset and continuing to the end of the
               section. Otherwise the initial value of this
               argument specifies the size of the view in bytes
               and is rounded up to the next host page size boundary.

    InheritDisposition - Supplies a value that specifies how the
                         view is to be shared by a child process created
                         with a create process operation.

    AllocationType - Supplies the type of allocation.

    Protect - Supplies the protection desired for the region of
              initially committed pages.

Return Value:

    NTSTATUS.

--*/
{
    KAPC_STATE ApcState;
    LOGICAL Attached;
    PSECTION Section;
    PCONTROL_AREA ControlArea;
    ULONG ProtectionMask;
    NTSTATUS status;
    LOGICAL WriteCombined;
    SIZE_T ImageCommitment;
    ULONG ExecutePermission;

    PAGED_CODE();

    Attached = FALSE;

    Section = (PSECTION)SectionToMap;

    //
    // Check to make sure the section is not smaller than the view size.
    //

    if ((LONGLONG)*CapturedViewSize > Section->SizeOfSection.QuadPart) {
        if ((AllocationType & MEM_RESERVE) == 0) {
            return STATUS_INVALID_VIEW_SIZE;
        }
    }

    if (AllocationType & MEM_RESERVE) {
        if (((Section->InitialPageProtection & PAGE_READWRITE) |
            (Section->InitialPageProtection & PAGE_EXECUTE_READWRITE)) == 0) {

            return STATUS_SECTION_PROTECTION;
        }
    }

    if (Section->u.Flags.NoCache) {
        Protect |= PAGE_NOCACHE;
    }

    //
    // Note that write combining is only relevant to physical memory sections
    // because they are never trimmed - the write combining bits in a PTE entry
    // are not preserved across trims.
    //

    if (Protect & PAGE_WRITECOMBINE) {
        Protect &= ~PAGE_WRITECOMBINE;
        WriteCombined = TRUE;
    }
    else {
        WriteCombined = FALSE;
    }

    //
    // Check the protection field.
    //

    ProtectionMask = MiMakeProtectionMask (Protect);
    if (ProtectionMask == MM_INVALID_PROTECTION) {
        return STATUS_INVALID_PAGE_PROTECTION;
    }

    ControlArea = Section->Segment->ControlArea;

    //
    // If the specified process is not the current process, attach
    // to the specified process.
    //

    if (PsGetCurrentProcess() != Process) {
        KeStackAttachProcess (&Process->Pcb, &ApcState);
        Attached = TRUE;
    }

    //
    // Get the address creation mutex to block multiple threads
    // creating or deleting address space at the same time.
    //

    LOCK_ADDRESS_SPACE (Process);

    //
    // Make sure the address space was not deleted, if so, return an error.
    //

    if (Process->Flags & PS_PROCESS_FLAGS_VM_DELETED) {
        status = STATUS_PROCESS_IS_TERMINATING;
        goto ErrorReturn;
    }
    
    //
    // Map the view base on the type.
    //

    if (ControlArea->u.Flags.PhysicalMemory) {

        status = MiMapViewOfPhysicalSection (ControlArea,
                                             Process,
                                             CapturedBase,
                                             SectionOffset,
                                             CapturedViewSize,
                                             ProtectionMask,
                                             ZeroBits,
                                             AllocationType,
                                             WriteCombined);

    }
    else if (ControlArea->u.Flags.Image) {
        if (AllocationType & MEM_RESERVE) {
            status = STATUS_INVALID_PARAMETER_9;
        }
        else if (WriteCombined == TRUE) {
            status = STATUS_INVALID_PARAMETER_10;
        }
        else {

            ImageCommitment = Section->Segment->u1.ImageCommitment;

            status = MiMapViewOfImageSection (ControlArea,
                                              Process,
                                              CapturedBase,
                                              SectionOffset,
                                              CapturedViewSize,
                                              Section,
                                              InheritDisposition,
                                              ZeroBits,
                                              ImageCommitment);
        }

    }
    else {

        //
        // Not an image section, therefore it is a data section.
        //

        if (WriteCombined == TRUE) {
            status = STATUS_INVALID_PARAMETER_10;
        }
        else {
            
            //
            // Add execute permission if necessary.
            //

#if defined (_WIN64)
            if (Process->Wow64Process == NULL && Process->Peb != NULL)
#elif defined (_X86PAE_)
            if (Process->Peb != NULL)
#else
            if (FALSE)
#endif
            {

                ExecutePermission = 0;

                try {
                    ExecutePermission = Process->Peb->ExecuteOptions & MEM_EXECUTE_OPTION_DATA;
                } except (EXCEPTION_EXECUTE_HANDLER) {
                    status = GetExceptionCode();
                    goto ErrorReturn;
                }

                if (ExecutePermission != 0) {

                    switch (Protect & 0xF) {
                        case PAGE_READONLY:
                            Protect &= ~PAGE_READONLY;
                            Protect |= PAGE_EXECUTE_READ;
                            break;
                        case PAGE_READWRITE:
                            Protect &= ~PAGE_READWRITE;
                            Protect |= PAGE_EXECUTE_READWRITE;
                            break;
                        case PAGE_WRITECOPY:
                            Protect &= ~PAGE_WRITECOPY;
                            Protect |= PAGE_EXECUTE_WRITECOPY;
                            break;
                        default:
                            break;
                    }

                    //
                    // Recheck protection.
                    //

                    ProtectionMask = MiMakeProtectionMask (Protect);

                    if (ProtectionMask == MM_INVALID_PROTECTION) {
                        status = STATUS_INVALID_PAGE_PROTECTION;
                        goto ErrorReturn;
                    }
                }
            }

            status = MiMapViewOfDataSection (ControlArea,
                                             Process,
                                             CapturedBase,
                                             SectionOffset,
                                             CapturedViewSize,
                                             Section,
                                             InheritDisposition,
                                             ProtectionMask,
                                             CommitSize,
                                             ZeroBits,
                                             AllocationType);
        }
    }

ErrorReturn:
    UNLOCK_ADDRESS_SPACE (Process);

    if (Attached) {
        KeUnstackDetachProcess (&ApcState);
    }

    return status;
}

VOID
MiInsertPhysicalViewAndRefControlArea (
    IN PEPROCESS Process,
    IN PCONTROL_AREA ControlArea,
    IN PMI_PHYSICAL_VIEW PhysicalView
    )

/*++

Routine Description:

    This is a nonpaged helper routine to insert a physical view and reference
    the control area accordingly.

Arguments:

    Process - Supplies a pointer to the process.

    ControlArea - Supplies a pointer to the control area.

    PhysicalView - Supplies a pointer to the physical view.

Return Value:

    None.

Environment:

    Kernel Mode, address creation and working set mutex held.

--*/

{
    KIRQL OldIrql;

    MmLockPagableSectionByHandle (ExPageLockHandle);

    //
    // Increment the count of the number of views for the
    // section object.  This requires the PFN lock to be held.
    //

    LOCK_PFN (OldIrql);

    ASSERT (PhysicalView->Vad->u.VadFlags.PhysicalMapping == 1);

    ASSERT (Process->PhysicalVadRoot != NULL);

    MiInsertNode ((PMMADDRESS_NODE)PhysicalView, Process->PhysicalVadRoot);

    ControlArea->NumberOfMappedViews += 1;
    ControlArea->NumberOfUserReferences += 1;

    ASSERT (ControlArea->NumberOfSectionReferences != 0);

    UNLOCK_PFN (OldIrql);

    MmUnlockPagableImageSection (ExPageLockHandle);
}

//
// Nonpaged wrapper.
//
LOGICAL
MiCheckCacheAttributes (
    IN PFN_NUMBER PageFrameIndex,
    IN PFN_NUMBER NumberOfPages,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute
    )
{
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PFN_NUMBER BadFrameStart;
    PFN_NUMBER BadFrameEnd;
    LOGICAL AttemptedMapOfUnownedFrame;
#if DBG
#define BACKTRACE_LENGTH 8
    ULONG i;
    ULONG Hash;
    PVOID StackTrace [BACKTRACE_LENGTH];
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PUNICODE_STRING BadDriverName;
#endif

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    BadFrameStart = 0;
    BadFrameEnd = 0;
    AttemptedMapOfUnownedFrame = FALSE;

    LOCK_PFN (OldIrql);

    do {

        if (MI_IS_PFN (PageFrameIndex)) {

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

            switch (Pfn1->u3.e1.CacheAttribute) {

                case MiCached:
                    if (CacheAttribute != MiCached) {
                        UNLOCK_PFN (OldIrql);
                        return FALSE;
                    }
                    break;

                case MiNonCached:
                    if (CacheAttribute != MiNonCached) {
                        UNLOCK_PFN (OldIrql);
                        return FALSE;
                    }
                    break;

                case MiWriteCombined:
                    if (CacheAttribute != MiWriteCombined) {
                        UNLOCK_PFN (OldIrql);
                        return FALSE;
                    }
                    break;

                case MiNotMapped:
                    if (AttemptedMapOfUnownedFrame == FALSE) {
                        BadFrameStart = PageFrameIndex;
                        AttemptedMapOfUnownedFrame = TRUE;
                    }
                    BadFrameEnd = PageFrameIndex;
                    break;

                default:
                    ASSERT (FALSE);
                    break;
            }
        }
        PageFrameIndex += 1;
        NumberOfPages -= 1;
    } while (NumberOfPages != 0);

    UNLOCK_PFN (OldIrql);

    if (AttemptedMapOfUnownedFrame == TRUE) {

#if DBG

        BadDriverName = NULL;

        RtlZeroMemory (StackTrace, sizeof (StackTrace));

        RtlCaptureStackBackTrace (1, BACKTRACE_LENGTH, StackTrace, &Hash);

        for (i = 0; i < BACKTRACE_LENGTH; i += 1) {

            if (StackTrace[i] <= MM_HIGHEST_USER_ADDRESS) {
                break;
            }

            DataTableEntry = MiLookupDataTableEntry (StackTrace[i], FALSE);

            if ((DataTableEntry != NULL) && ((PVOID)DataTableEntry != (PVOID)PsLoadedModuleList.Flink)) {
                //
                // Found the bad caller.
                //

                BadDriverName = &DataTableEntry->FullDllName;
            }
        }

        if (BadDriverName != NULL) {
            DbgPrint ("*******************************************************************************\n"
                  "*\n"
                  "* %wZ is mapping physical memory %p->%p\n"
                  "* that it does not own.  This can cause internal CPU corruption.\n"
                  "*\n"
                  "*******************************************************************************\n",
                BadDriverName,
                BadFrameStart << PAGE_SHIFT,
                (BadFrameEnd << PAGE_SHIFT) | (PAGE_SIZE - 1));
        }
        else {
            DbgPrint ("*******************************************************************************\n"
                  "*\n"
                  "* A driver is mapping physical memory %p->%p\n"
                  "* that it does not own.  This can cause internal CPU corruption.\n"
                  "*\n"
                  "*******************************************************************************\n",
                BadFrameStart << PAGE_SHIFT,
                (BadFrameEnd << PAGE_SHIFT) | (PAGE_SIZE - 1));
        }

#else
        DbgPrint ("*******************************************************************************\n"
              "*\n"
              "* A driver is mapping physical memory %p->%p\n"
              "* that it does not own.  This can cause internal CPU corruption.\n"
              "* A checked build will stop in the kernel debugger\n"
              "* so this problem can be fully debugged.\n"
              "*\n"
              "*******************************************************************************\n",
            BadFrameStart << PAGE_SHIFT,
            (BadFrameEnd << PAGE_SHIFT) | (PAGE_SIZE - 1));
#endif

        if (MI_BP_BADMAPS()) {
            DbgBreakPoint ();
        }
    }

    return TRUE;
}

NTSTATUS
MiMapViewOfPhysicalSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN ULONG ProtectionMask,
    IN ULONG_PTR ZeroBits,
    IN ULONG AllocationType,
    IN LOGICAL WriteCombined
    )

/*++

Routine Description:

    This routine maps the specified physical section into the
    specified process's address space.

Arguments:

    see MmMapViewOfSection above...

    ControlArea - Supplies the control area for the section.

    Process - Supplies the process pointer which is receiving the section.

    ProtectionMask - Supplies the initial page protection-mask.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, address creation mutex held.

--*/

{
    PMMVAD_LONG Vad;
    CSHORT IoMapping;
    PVOID StartingAddress;
    PVOID EndingAddress;
    PVOID HighestUserAddress;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    MMPTE TempPte;
    PMMPFN Pfn2;
    SIZE_T PhysicalViewSize;
    ULONG_PTR Alignment;
    PVOID UsedPageTableHandle;
    PMI_PHYSICAL_VIEW PhysicalView;
    NTSTATUS Status;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER StartingPageFrameIndex;
    PFN_NUMBER NumberOfPages;
    MEMORY_CACHING_TYPE CacheType;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;
    PMM_AVL_TABLE PhysicalVadRoot;

    //
    // Physical memory section.
    //

    if (AllocationType & MEM_RESERVE) {
        return STATUS_INVALID_PARAMETER_9;
    }

    Alignment = X64K;

    if (*CapturedBase == NULL) {

        //
        // Attempt to locate address space starting on a 64k boundary.
        //

#if !defined (_MI_MORE_THAN_4GB_)
        ASSERT (SectionOffset->HighPart == 0);
#endif

        PhysicalViewSize = *CapturedViewSize +
                               (SectionOffset->LowPart & (X64K - 1));

        //
        // Check whether the registry indicates that all applications
        // should be given virtual address ranges from the highest
        // address downwards in order to test 3GB-aware apps on 32-bit
        // machines and 64-bit apps on NT64.
        //

        if ((AllocationType & MEM_TOP_DOWN) || (Process->VmTopDown == 1)) {

            if (ZeroBits != 0) {
                HighestUserAddress = (PVOID)((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT >> ZeroBits);
                if (HighestUserAddress > MM_HIGHEST_VAD_ADDRESS) {
                    HighestUserAddress = MM_HIGHEST_VAD_ADDRESS;
                }
            }
            else {
                HighestUserAddress = MM_HIGHEST_VAD_ADDRESS;
            }

            Status = MiFindEmptyAddressRangeDown (&Process->VadRoot,
                                                  PhysicalViewSize,
                                                  HighestUserAddress,
                                                  Alignment,
                                                  &StartingAddress);
        }
        else {

            Status = MiFindEmptyAddressRange (PhysicalViewSize,
                                              Alignment,
                                              (ULONG)ZeroBits,
                                              &StartingAddress);
        }

        if (!NT_SUCCESS (Status)) {
            return Status;
        }

        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                PhysicalViewSize - 1L) | (PAGE_SIZE - 1L));
        StartingAddress = (PVOID)((ULONG_PTR)StartingAddress +
                                     (SectionOffset->LowPart & (X64K - 1)));

        if (ZeroBits > 0) {
            if (EndingAddress > (PVOID)((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT >> ZeroBits)) {
                return STATUS_NO_MEMORY;
            }
        }

    }
    else {

        //
        // Check to make sure the specified base address to ending address
        // is currently unused.
        //

        StartingAddress = (PVOID)((ULONG_PTR)MI_64K_ALIGN(*CapturedBase) +
                                    (SectionOffset->LowPart & (X64K - 1)));
        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                *CapturedViewSize - 1L) | (PAGE_SIZE - 1L));

        if (MiCheckForConflictingVadExistence (Process, StartingAddress, EndingAddress) == TRUE) {
            return STATUS_CONFLICTING_ADDRESSES;
        }
    }

    //
    // If a noncachable mapping is requested, none of the physical pages in the
    // range can reside in a large page.  Otherwise we would be creating an
    // incoherent overlapping TB entry as the same physical
    // page would be mapped by 2 different TB entries with different
    // cache attributes.
    //

    StartingPageFrameIndex = (PFN_NUMBER) (SectionOffset->QuadPart >> PAGE_SHIFT);

    CacheType = MmCached;

    if (WriteCombined == TRUE) {
        CacheType = MmWriteCombined;
    }
    else if (ProtectionMask & MM_NOCACHE) {
        CacheType = MmNonCached;
    }

    IoMapping = 1;

    if (MI_IS_PFN (StartingPageFrameIndex)) {
        IoMapping = 0;
    }

    CacheAttribute = MI_TRANSLATE_CACHETYPE (CacheType, IoMapping);

    NumberOfPages = MiGetPteAddress (EndingAddress) - MiGetPteAddress (StartingAddress) + 1;

    if (CacheAttribute != MiCached) {
        PageFrameIndex = StartingPageFrameIndex;
        while (PageFrameIndex < StartingPageFrameIndex + NumberOfPages) {
            if (MI_PAGE_FRAME_INDEX_MUST_BE_CACHED (PageFrameIndex)) {
                MiNonCachedCollisions += 1;
                return STATUS_CONFLICTING_ADDRESSES;
            }
            PageFrameIndex += 1;
        }
    }

    //
    // An unoccupied address range has been found, build the virtual
    // address descriptor to describe this range.
    //

    PhysicalVadRoot = Process->PhysicalVadRoot;

    //
    // The address space mutex synchronizes the allocation of the
    // EPROCESS PhysicalVadRoot.  This table root is not deleted until
    // the process exits.
    //

    if (Process->PhysicalVadRoot == NULL) {

        PhysicalVadRoot = (PMM_AVL_TABLE) ExAllocatePoolWithTag (
                                                    NonPagedPool,
                                                    sizeof (MM_AVL_TABLE),
                                                    MI_PHYSICAL_VIEW_ROOT_KEY);

        if (PhysicalVadRoot == NULL) {
            return STATUS_INSUFFICIENT_RESOURCES;
        }

        RtlZeroMemory (PhysicalVadRoot, sizeof (MM_AVL_TABLE));
        ASSERT (PhysicalVadRoot->NumberGenericTableElements == 0);
        PhysicalVadRoot->BalancedRoot.u1.Parent = &PhysicalVadRoot->BalancedRoot;
        MiInsertPhysicalVadRoot (Process, PhysicalVadRoot);
    }

    //
    // Attempt to allocate the pool and charge quota during the Vad insertion.
    //

    PhysicalView = (PMI_PHYSICAL_VIEW)ExAllocatePoolWithTag (NonPagedPool,
                                                             sizeof(MI_PHYSICAL_VIEW),
                                                             MI_PHYSICAL_VIEW_KEY);
    if (PhysicalView == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    Vad = (PMMVAD_LONG)ExAllocatePoolWithTag (NonPagedPool,
                                              sizeof(MMVAD_LONG),
                                              'ldaV');
    if (Vad == NULL) {
        ExFreePool (PhysicalView);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    RtlZeroMemory (Vad, sizeof(MMVAD_LONG));
    Vad->StartingVpn = MI_VA_TO_VPN (StartingAddress);
    Vad->EndingVpn = MI_VA_TO_VPN (EndingAddress);
    Vad->ControlArea = ControlArea;
    Vad->u2.VadFlags2.Inherit = MM_VIEW_UNMAP;
    Vad->u2.VadFlags2.LongVad = 1;
    Vad->u.VadFlags.PhysicalMapping = 1;
    Vad->u.VadFlags.Protection = ProtectionMask;

    PhysicalView->Vad = (PMMVAD) Vad;
    PhysicalView->StartingVpn = Vad->StartingVpn;
    PhysicalView->EndingVpn = Vad->EndingVpn;
    PhysicalView->u.LongFlags = MI_PHYSICAL_VIEW_PHYS;

    //
    // Set the last contiguous PTE field in the Vad to the page frame
    // number of the starting physical page.
    //

    Vad->LastContiguousPte = (PMMPTE) StartingPageFrameIndex;

    Vad->FirstPrototypePte = (PMMPTE) StartingPageFrameIndex;

    ASSERT (Vad->FirstPrototypePte <= Vad->LastContiguousPte);

    //
    // Initialize the PTE templates as the working set mutex is not needed to
    // synchronize this (so avoid adding extra contention).
    //

    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);
    LastPte = MiGetPteAddress (EndingAddress);

    MI_MAKE_VALID_PTE (TempPte,
                       StartingPageFrameIndex,
                       ProtectionMask,
                       PointerPte);

    if (TempPte.u.Hard.Write) {
        MI_SET_PTE_DIRTY (TempPte);
    }

    if (CacheAttribute == MiWriteCombined) {
        MI_SET_PTE_WRITE_COMBINE (TempPte);
    }
    else if (CacheAttribute == MiNonCached) {
        MI_DISABLE_CACHING (TempPte);
    }

    MI_ADD_EXECUTE_TO_VALID_PTE_IF_PAE (TempPte);

    //
    // Ensure no page frame cache attributes conflict.
    //

    if (MiCheckCacheAttributes (StartingPageFrameIndex, NumberOfPages, CacheAttribute) == FALSE) {
        Status = STATUS_CONFLICTING_ADDRESSES;
        goto Failure1;
    }

    //
    // Insert the VAD.  This could fail due to quota charges.
    //

    LOCK_WS_UNSAFE (Process);

    Status = MiInsertVad ((PMMVAD) Vad);

    if (!NT_SUCCESS(Status)) {

        UNLOCK_WS_UNSAFE (Process);

Failure1:
        ExFreePool (PhysicalView);

        //
        // The pool allocation succeeded, but the quota charge
        // in InsertVad failed, deallocate the pool and return
        // an error.
        //

        ExFreePool (Vad);
        return Status;
    }

    //
    // Build the PTEs in the address space.
    //

    MI_PREPARE_FOR_NONCACHED (CacheAttribute);

    MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);

    Pfn2 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);

    UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (StartingAddress);

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary (PointerPte)) {

            PointerPde = MiGetPteAddress (PointerPte);

            MiMakePdeExistAndMakeValid (PointerPde, Process, MM_NOIRQL);

            Pfn2 = MI_PFN_ELEMENT (PointerPde->u.Hard.PageFrameNumber);
            UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (MiGetVirtualAddressMappedByPte (PointerPte));
        }

        //
        // Increment the count of non-zero page table entries for this
        // page table and the number of private pages for the process.
        //

        MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);

        ASSERT (PointerPte->u.Long == 0);

        MI_WRITE_VALID_PTE (PointerPte, TempPte);

        Pfn2->u2.ShareCount += 1;

        PointerPte += 1;
        TempPte.u.Hard.PageFrameNumber += 1;
    }

    MI_SWEEP_CACHE (CacheAttribute,
                    StartingAddress,
                    (PCHAR) EndingAddress - (PCHAR) StartingAddress);

    //
    // Increment the count of the number of views for the
    // section object.  This requires the PFN lock to be held.
    // At the same time, insert the physical view descriptor now that
    // the page table page hierarchy is in place.  Note probes can find
    // this descriptor immediately.
    //

    MiInsertPhysicalViewAndRefControlArea (Process, ControlArea, PhysicalView);

    UNLOCK_WS_UNSAFE (Process);

    //
    // Update the current virtual size in the process header.
    //

    *CapturedViewSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
    Process->VirtualSize += *CapturedViewSize;

    if (Process->VirtualSize > Process->PeakVirtualSize) {
        Process->PeakVirtualSize = Process->VirtualSize;
    }

    *CapturedBase = StartingAddress;

    return STATUS_SUCCESS;
}


VOID
MiSetControlAreaSymbolsLoaded (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This is a nonpaged helper routine to mark the specified control area as
    having its debug symbols loaded.

Arguments:

    ControlArea - Supplies a pointer to the control area.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);
    ControlArea->u.Flags.DebugSymbolsLoaded = 1;
    UNLOCK_PFN (OldIrql);
}

VOID
MiLoadUserSymbols (
    IN PCONTROL_AREA ControlArea,
    IN PVOID StartingAddress,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine loads symbols for user space executables and DLLs.

Arguments:

    ControlArea - Supplies the control area for the section.

    StartingAddress - Supplies the virtual address the section is mapped at.

    Process - Supplies the process pointer which is receiving the section.

Return Value:

    None.

Environment:

    Kernel Mode, address creation mutex held.

--*/

{
    PLIST_ENTRY Head;
    PLIST_ENTRY Next;
    PKLDR_DATA_TABLE_ENTRY Entry;
    PIMAGE_NT_HEADERS NtHeaders;
    PUNICODE_STRING FileName;
    ANSI_STRING AnsiName;
    NTSTATUS Status;
    PKTHREAD CurrentThread;

    //
    //  TEMP TEMP TEMP rip out ANSI conversion when debugger converted.
    //

    FileName = (PUNICODE_STRING)&ControlArea->FilePointer->FileName;

    if (FileName->Length == 0) {
        return;
    }

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);
    ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);

    Head = &MmLoadedUserImageList;
    Next = Head->Flink;

    while (Next != Head) {
        Entry = CONTAINING_RECORD (Next,
                                   KLDR_DATA_TABLE_ENTRY,
                                   InLoadOrderLinks);

        if (Entry->DllBase == StartingAddress) {
            Entry->LoadCount += 1;
            break;
        }
        Next = Next->Flink;
    }

    if (Next == Head) {
        Entry = ExAllocatePoolWithTag (NonPagedPool,
                                sizeof( *Entry ) +
                                    FileName->Length +
                                    sizeof( UNICODE_NULL ),
                                    MMDB);
        if (Entry != NULL) {

            RtlZeroMemory (Entry, sizeof(*Entry));

            try {
                NtHeaders = RtlImageNtHeader (StartingAddress);
                if (NtHeaders != NULL) {
#if defined(_WIN64)
                    if (NtHeaders->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR64_MAGIC) {
                        Entry->SizeOfImage = NtHeaders->OptionalHeader.SizeOfImage;
                        Entry->CheckSum = NtHeaders->OptionalHeader.CheckSum;
                    }
                    else {
                        PIMAGE_NT_HEADERS32 NtHeaders32 = (PIMAGE_NT_HEADERS32)NtHeaders;

                        Entry->SizeOfImage = NtHeaders32->OptionalHeader.SizeOfImage;
                        Entry->CheckSum = NtHeaders32->OptionalHeader.CheckSum;
                    }
#else
                    Entry->SizeOfImage = NtHeaders->OptionalHeader.SizeOfImage;
                    Entry->CheckSum = NtHeaders->OptionalHeader.CheckSum;
#endif
                }
            } except (EXCEPTION_EXECUTE_HANDLER) {
                NOTHING;
            }

            Entry->DllBase = StartingAddress;
            Entry->FullDllName.Buffer = (PWSTR)(Entry+1);
            Entry->FullDllName.Length = FileName->Length;
            Entry->FullDllName.MaximumLength = (USHORT)
                (Entry->FullDllName.Length + sizeof( UNICODE_NULL ));

            RtlCopyMemory (Entry->FullDllName.Buffer,
                           FileName->Buffer,
                           FileName->Length);

            Entry->FullDllName.Buffer[ Entry->FullDllName.Length / sizeof( WCHAR )] = UNICODE_NULL;
            Entry->LoadCount = 1;
            InsertTailList (&MmLoadedUserImageList,
                            &Entry->InLoadOrderLinks);

        }
    }

    ExReleaseResourceLite (&PsLoadedModuleResource);
    KeLeaveCriticalRegionThread (CurrentThread);

    Status = RtlUnicodeStringToAnsiString (&AnsiName,
                                           FileName,
                                           TRUE);

    if (NT_SUCCESS( Status)) {
        DbgLoadImageSymbols (&AnsiName,
                             StartingAddress,
                             (ULONG_PTR)Process);
        RtlFreeAnsiString (&AnsiName);
    }
    return;
}


VOID
MiDereferenceControlArea (
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This is a nonpaged helper routine to dereference the specified control area.

Arguments:

    ControlArea - Supplies a pointer to the control area.

Return Value:

    None.

--*/

{
    KIRQL OldIrql;

    LOCK_PFN (OldIrql);
    ControlArea->NumberOfMappedViews -= 1;
    ControlArea->NumberOfUserReferences -= 1;

    //
    // Check to see if the control area (segment) should be deleted.
    // This routine releases the PFN lock.
    //

    MiCheckControlArea (ControlArea, NULL, OldIrql);
}


NTSTATUS
MiMapViewOfImageSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN PSECTION Section,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG_PTR ZeroBits,
    IN SIZE_T ImageCommitment
    )

/*++

Routine Description:

    This routine maps the specified Image section into the
    specified process's address space.

Arguments:

    see MmMapViewOfSection above...

    ControlArea - Supplies the control area for the section.

    Process - Supplies the process pointer which is receiving the section.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, address creation mutex held.

--*/

{
    PMMVAD Vad;
    PMMVAD PreviousVad;
    PMMVAD NextVad;
    PVOID StartingAddress;
    PVOID OutputStartingAddress;
    PVOID EndingAddress;
    PVOID HighestUserAddress;
    LOGICAL Attached;
    PSUBSECTION Subsection;
    ULONG PteOffset;
    NTSTATUS Status;
    NTSTATUS ReturnedStatus;
    PMMPTE ProtoPte;
    PVOID BasedAddress;
    SIZE_T NeededViewSize;
    SIZE_T OutputViewSize;

    Attached = FALSE;

    //
    // Image file.
    //
    // Locate the first subsection (text) and create a virtual
    // address descriptor to map the entire image here.
    //

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    //
    // Check to see if a purge operation is in progress and if so, wait
    // for the purge to complete.  In addition, up the count of mapped
    // views for this control area.
    //

    Status = MiCheckPurgeAndUpMapCount (ControlArea, TRUE);
    
    if (!NT_SUCCESS (Status)) {
        return Status;
    }

    //
    // Capture the based address to the stack, to prevent page faults.
    //

    BasedAddress = ControlArea->Segment->BasedAddress;

    if (*CapturedViewSize == 0) {
        *CapturedViewSize =
            (ULONG_PTR)(Section->SizeOfSection.QuadPart - SectionOffset->QuadPart);
    }

    ReturnedStatus = STATUS_SUCCESS;

    //
    // Determine if a specific base was specified.
    //

    if (*CapturedBase != NULL) {

        //
        // Captured base is not NULL.
        //
        // Check to make sure the specified address range is currently unused
        // and within the user address space.
        //

        StartingAddress = MI_64K_ALIGN(*CapturedBase);
        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                       *CapturedViewSize - 1) | (PAGE_SIZE - 1));

        if ((StartingAddress <=  MM_HIGHEST_VAD_ADDRESS) &&
            (((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS + 1) -
                                (ULONG_PTR)StartingAddress >= *CapturedViewSize) &&

            (EndingAddress <= MM_HIGHEST_VAD_ADDRESS)) {

            if (MiCheckForConflictingVadExistence (Process, StartingAddress, EndingAddress) == TRUE) {
                MiDereferenceControlArea (ControlArea);
                return STATUS_CONFLICTING_ADDRESSES;
            }
        }
        else {
            MiDereferenceControlArea (ControlArea);
            return STATUS_CONFLICTING_ADDRESSES;
        }

        //
        // A conflicting VAD was not found and the specified address range is
        // within the user address space. If the image will not reside at its
        // base address, then set a special return status.
        //

        if (((ULONG_PTR)StartingAddress +
            (ULONG_PTR)MI_64K_ALIGN(SectionOffset->LowPart)) != (ULONG_PTR)BasedAddress) {
            ReturnedStatus = STATUS_IMAGE_NOT_AT_BASE;
        }
    }
    else {

        //
        // Captured base is NULL.
        //
        // If the captured view size is greater than the largest size that
        // can fit in the user address space, then it is not possible to map
        // the image.
        //

        if ((PVOID)*CapturedViewSize > MM_HIGHEST_VAD_ADDRESS) {
            MiDereferenceControlArea (ControlArea);
            return STATUS_NO_MEMORY;
        }

        //
        // Check to make sure the specified address range is currently unused
        // and within the user address space.
        //

        StartingAddress = (PVOID)((ULONG_PTR)BasedAddress +
                                    (ULONG_PTR)MI_64K_ALIGN(SectionOffset->LowPart));

        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                    *CapturedViewSize - 1) | (PAGE_SIZE - 1));

        Vad = (PMMVAD) TRUE;
        NeededViewSize = *CapturedViewSize;

        if ((StartingAddress >= MM_LOWEST_USER_ADDRESS) &&
            (StartingAddress <= MM_HIGHEST_VAD_ADDRESS) &&
            (((ULONG_PTR)MM_HIGHEST_VAD_ADDRESS + 1) -
                                (ULONG_PTR)StartingAddress >= *CapturedViewSize) &&

            (EndingAddress <= MM_HIGHEST_VAD_ADDRESS)) {

            Vad = (PMMVAD) (ULONG_PTR) MiCheckForConflictingVadExistence (Process, StartingAddress, EndingAddress);
        }

        //
        // If the VAD address is not NULL, then a conflict was discovered.
        // Attempt to select another address range in which to map the image.
        //

        if (Vad != NULL) {

            //
            // The image could not be mapped at its natural base address
            // try to find another place to map it.
            //
            // If the system has been biased to an alternate base address to
            // allow 3gb of user address space, then make sure the high order
            // address bit is zero.
            //

            if ((MmVirtualBias != 0) && (ZeroBits == 0)) {
                ZeroBits = 1;
            }

            ReturnedStatus = STATUS_IMAGE_NOT_AT_BASE;

            //
            // Find a starting address on a 64k boundary.
            //
            // Check whether the registry indicates that all applications
            // should be given virtual address ranges from the highest
            // address downwards in order to test 3GB-aware apps on 32-bit
            // machines and 64-bit apps on NT64.
            //

            if (Process->VmTopDown == 1) {

                if (ZeroBits != 0) {
                    HighestUserAddress = (PVOID)((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT >> ZeroBits);
                    if (HighestUserAddress > MM_HIGHEST_VAD_ADDRESS) {
                        HighestUserAddress = MM_HIGHEST_VAD_ADDRESS;
                    }
                }
                else {
                    HighestUserAddress = MM_HIGHEST_VAD_ADDRESS;
                }

                Status = MiFindEmptyAddressRangeDown (&Process->VadRoot,
                                                      NeededViewSize,
                                                      HighestUserAddress,
                                                      X64K,
                                                      &StartingAddress);
            }
            else {
                Status = MiFindEmptyAddressRange (NeededViewSize,
                                                  X64K,
                                                  (ULONG)ZeroBits,
                                                  &StartingAddress);
            }

            if (!NT_SUCCESS (Status)) {
                MiDereferenceControlArea (ControlArea);
                return Status;
            }

            EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                        *CapturedViewSize - 1) | (PAGE_SIZE - 1));
        }
    }

    //
    // Allocate and initialize a VAD for the specified address range.
    //

    Vad = ExAllocatePoolWithTag (NonPagedPool, sizeof(MMVAD), MMVADKEY);

    if (Vad == NULL) {
        MiDereferenceControlArea (ControlArea);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    RtlZeroMemory (Vad, sizeof(MMVAD));
    Vad->StartingVpn = MI_VA_TO_VPN (StartingAddress);
    Vad->EndingVpn = MI_VA_TO_VPN (EndingAddress);
    Vad->u2.VadFlags2.Inherit = (InheritDisposition == ViewShare);
    Vad->u.VadFlags.ImageMap = 1;

    //
    // Set the protection in the VAD as EXECUTE_WRITE_COPY.
    //

    Vad->u.VadFlags.Protection = MM_EXECUTE_WRITECOPY;
    Vad->ControlArea = ControlArea;

    //
    // Set the first prototype PTE field in the Vad.
    //

    SectionOffset->LowPart = SectionOffset->LowPart & ~(X64K - 1);
    PteOffset = (ULONG)(SectionOffset->QuadPart >> PAGE_SHIFT);

    Vad->FirstPrototypePte = &Subsection->SubsectionBase[PteOffset];
    Vad->LastContiguousPte = MM_ALLOCATION_FILLS_VAD;

    //
    // NOTE: the full commitment is charged even if a partial map of an
    // image is being done.  This saves from having to run through the
    // entire image (via prototype PTEs) and calculate the charge on
    // a per page basis for the partial map.
    //

    Vad->u.VadFlags.CommitCharge = (SIZE_T)ImageCommitment; // ****** temp

    ASSERT (Vad->FirstPrototypePte <= Vad->LastContiguousPte);

    LOCK_WS_UNSAFE (Process);

    Status = MiInsertVad (Vad);

    UNLOCK_WS_UNSAFE (Process);

    if (!NT_SUCCESS(Status)) {

        MiDereferenceControlArea (ControlArea);

        //
        // The quota charge in InsertVad failed,
        // deallocate the pool and return an error.
        //

        ExFreePool (Vad);
        return Status;
    }

    OutputStartingAddress = StartingAddress;
    OutputViewSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;

#if DBG
    if (MmDebug & MM_DBG_WALK_VAD_TREE) {
        DbgPrint("mapped image section vads\n");
        VadTreeWalk ();
    }
#endif

    //
    // Update the current virtual size in the process header.
    //

    Process->VirtualSize += OutputViewSize;

    if (Process->VirtualSize > Process->PeakVirtualSize) {
        Process->PeakVirtualSize = Process->VirtualSize;
    }

#if defined(_MIALT4K_)

    //
    // Clear the alternate PTE bitmap so that future faults in this image range
    // will result in MiSyncAltPte being called.  This prevents stale ALTPTEs
    // (filled if the user app previously referenced bogus addresses not in
    // any VAD from inside a try-except) from causing AVs on what will be
    // a now-legitimate image address.
    //

    if (Process->Wow64Process != NULL) {
        MiDeleteFor4kPage (StartingAddress, EndingAddress, Process);
    }

#endif

    if (ControlArea->u.Flags.FloppyMedia) {

        //
        // The image resides on a floppy disk, in-page all
        // pages from the floppy and mark them as modified so
        // they migrate to the paging file rather than reread
        // them from the floppy disk which may have been removed.
        //

        ProtoPte = Vad->FirstPrototypePte;

        //
        // This could get an in-page error from the floppy.
        //

        while (StartingAddress < EndingAddress) {

            //
            // If the prototype PTE is valid, transition or
            // in prototype PTE format, bring the page into
            // memory and set the modified bit.
            //

            if ((ProtoPte->u.Hard.Valid == 1) ||

                (((ProtoPte->u.Soft.Prototype == 1) ||
                 (ProtoPte->u.Soft.Transition == 1)) &&
                 (ProtoPte->u.Soft.Protection != MM_NOACCESS))
                
                ) {

                Status = MiSetPageModified (Vad, StartingAddress);

                if (!NT_SUCCESS (Status)) {

                    //
                    // An in page error must have occurred touching the image,
                    // Ignore the error and continue to the next page - unless
                    // it's being run over a network.  If it's being run over
                    // a net and the control area is marked as floppy, then
                    // the image must be marked NET_RUN_FROM_SWAP, so any
                    // inpage error must be treated as terminal now - so the app
                    // doesn't later spontaneously abort when referencing
                    // this page.  This provides app writers with a way to
                    // mark their app in a way which is robust regardless of
                    // network conditions.
                    //

                    if (ControlArea->u.Flags.Networked) {

                        //
                        // N.B.  There are no race conditions with the user
                        // deleting/substituting this mapping from another
                        // thread as the address space mutex is still held.
                        //

                        Process->VirtualSize -= OutputViewSize;

                        PreviousVad = MiGetPreviousVad (Vad);
                        NextVad = MiGetNextVad (Vad);
                    
                        StartingAddress = MI_VPN_TO_VA (Vad->StartingVpn);
                        EndingAddress = MI_VPN_TO_VA_ENDING (Vad->EndingVpn);

                        LOCK_WS_UNSAFE (Process);

                        MiRemoveVad (Vad);
                    
                        //
                        // Return commitment for page table pages.
                        //
                    
                        MiReturnPageTablePageCommitment (StartingAddress,
                                                         EndingAddress,
                                                         Process,
                                                         PreviousVad,
                                                         NextVad);
                    
                        MiRemoveMappedView (Process, Vad);
                    
                        UNLOCK_WS_UNSAFE (Process);

                        ExFreePool (Vad);

                        return Status;
                    }
                }
            }
            ProtoPte += 1;
            StartingAddress = (PVOID)((PCHAR)StartingAddress + PAGE_SIZE);
        }
    }

    *CapturedViewSize = OutputViewSize;
    *CapturedBase = OutputStartingAddress;

    ASSERT (NT_SUCCESS (ReturnedStatus));

    //
    // Check to see if this image is for the architecture of the current
    // machine.
    //

    if (ControlArea->Segment->u2.ImageInformation->ImageContainsCode &&
        ((ControlArea->Segment->u2.ImageInformation->Machine <
                                      USER_SHARED_DATA->ImageNumberLow) ||
         (ControlArea->Segment->u2.ImageInformation->Machine >
                                      USER_SHARED_DATA->ImageNumberHigh)
        )
       ) {
#if defined (_WIN64)

        //
        // If this is a wow64 process then allow i386 images.
        //

        if (!Process->Wow64Process ||
            ControlArea->Segment->u2.ImageInformation->Machine != IMAGE_FILE_MACHINE_I386) {
            return STATUS_IMAGE_MACHINE_TYPE_MISMATCH;
        }
#else   //!_WIN64
        return STATUS_IMAGE_MACHINE_TYPE_MISMATCH;
#endif
    }

    StartingAddress = MI_VPN_TO_VA (Vad->StartingVpn);

    if (PsImageNotifyEnabled) {

        IMAGE_INFO ImageInfo;

        if ((StartingAddress < MmHighestUserAddress) &&
            (Process->UniqueProcessId) &&
            (Process != PsInitialSystemProcess)) {

            ImageInfo.Properties = 0;
            ImageInfo.ImageAddressingMode = IMAGE_ADDRESSING_MODE_32BIT;
            ImageInfo.ImageBase = StartingAddress;
            ImageInfo.ImageSize = OutputViewSize;
            ImageInfo.ImageSelector = 0;
            ImageInfo.ImageSectionNumber = 0;

            PsCallImageNotifyRoutines (&ControlArea->FilePointer->FileName,
                                       Process->UniqueProcessId,
                                       &ImageInfo);
        }
    }

    ASSERT (ControlArea->u.Flags.Image);

    if ((NtGlobalFlag & FLG_ENABLE_KDEBUG_SYMBOL_LOAD) &&
        (ReturnedStatus != STATUS_IMAGE_NOT_AT_BASE) &&
        (ControlArea->u.Flags.DebugSymbolsLoaded == 0)) {

        if (MiCacheImageSymbols (StartingAddress)) {

            MiSetControlAreaSymbolsLoaded (ControlArea);

            MiLoadUserSymbols (ControlArea, StartingAddress, Process);
        }
    }

#if defined(_MIALT4K_)

    if (Process->Wow64Process != NULL) {
        MiProtectImageFileFor4kPage (StartingAddress, OutputViewSize);
    }

#endif

    return ReturnedStatus;

}

NTSTATUS
MiAddViewsForSectionWithPfn (
    IN PMSUBSECTION StartMappedSubsection,
    IN UINT64 LastPteOffset OPTIONAL
    )

/*++

Routine Description:

    This nonpagable wrapper routine maps the views into the specified section.

Arguments:

    StartMappedSubsection - Supplies the mapped subsection to start at.

    LastPteOffset - Supplies the last PTE offset to end the views at.
                    Supplies zero if views are desired from the supplied
                    subsection to the end of the file.

Return Value:

    NTSTATUS.

Environment:

    Kernel Mode, address creation mutex optionally held if called in process
    context.  APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;
    NTSTATUS Status;
    ULONG Waited;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    LOCK_PFN (OldIrql);

    //
    // This routine returns with the PFN lock released !
    //

    Status = MiAddViewsForSection (StartMappedSubsection,
                                   LastPteOffset,
                                   OldIrql,
                                   &Waited);

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    return Status;
}

LOGICAL
MiReferenceSubsection (
    IN PMSUBSECTION MappedSubsection
    )

/*++

Routine Description:

    This nonpagable routine reference counts the specified subsection if
    its prototype PTEs are already setup, otherwise it returns FALSE.

Arguments:

    MappedSubsection - Supplies the mapped subsection to start at.

Return Value:

    NTSTATUS.

Environment:

    Kernel Mode, PFN lock held.

--*/

{
    ASSERT ((MappedSubsection->ControlArea->u.Flags.Image == 0) &&
            (MappedSubsection->ControlArea->FilePointer != NULL) &&
            (MappedSubsection->ControlArea->u.Flags.PhysicalMemory == 0));

    MM_PFN_LOCK_ASSERT();

    //
    // Note the control area is not necessarily active at this point.
    //

    if (MappedSubsection->SubsectionBase == NULL) {

        //
        // No prototype PTEs exist, caller will have to go the long way.
        //

        return FALSE;
    }

    //
    // The mapping base exists so the number of mapped views can be
    // safely incremented.  This prevents a trim from starting after
    // we release the lock.
    //

    MappedSubsection->NumberOfMappedViews += 1;

    MI_SNAP_SUB (MappedSubsection, 0x4);

    if (MappedSubsection->DereferenceList.Flink != NULL) {

        //
        // Remove this from the list of unused subsections.
        //

        RemoveEntryList (&MappedSubsection->DereferenceList);

        MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);

        MappedSubsection->DereferenceList.Flink = NULL;
    }

    //
    // Set the access bit so an already ongoing trim won't blindly
    // delete the prototype PTEs on completion of a mapped write.
    // This can happen if the current thread dirties some pages and
    // then deletes the view before the trim write finishes - this
    // bit informs the trimming thread that a rescan is needed so
    // that writes are not lost.
    //

    MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;

    return TRUE;
}

NTSTATUS
MiAddViewsForSection (
    IN PMSUBSECTION StartMappedSubsection,
    IN UINT64 LastPteOffset OPTIONAL,
    IN KIRQL OldIrql,
    OUT PULONG Waited
    )

/*++

Routine Description:

    This nonpagable routine maps the views into the specified section.

    N.B. This routine may release and reacquire the PFN lock !

    N.B. This routine always returns with the PFN lock released !

    N.B. Callers may pass in view lengths that exceed the length of
         the section and this must succeed.  Thus MiAddViews must check
         for this and know to stop the references at the end.  More
         importantly, MiRemoveViews must also contain the same logic.

Arguments:

    StartMappedSubsection - Supplies the mapped subsection to start at.

    LastPteOffset - Supplies the number of PTEs (beginning at the supplied
                    subsection) to provide views for.  Supplies zero if
                    views are desired from the supplied subsection to the
                    end of the file.

    Waited - Supplies a pointer to a ULONG to increment if the PFN lock is
             released and reacquired.

Return Value:

    NTSTATUS, PFN lock released.

Environment:

    Kernel Mode, PFN lock held.

--*/

{
    MMPTE TempPte;
    ULONG Size;
    PMMPTE ProtoPtes;
    PMSUBSECTION MappedSubsection;

    *Waited = 0;

    MappedSubsection = StartMappedSubsection;

    ASSERT ((MappedSubsection->ControlArea->u.Flags.Image == 0) &&
            (MappedSubsection->ControlArea->FilePointer != NULL) &&
            (MappedSubsection->ControlArea->u.Flags.PhysicalMemory == 0));

    MM_PFN_LOCK_ASSERT();

    do {

        //
        // Note the control area must be active at this point.
        //

        ASSERT (MappedSubsection->ControlArea->DereferenceList.Flink == NULL);

        if (MappedSubsection->SubsectionBase != NULL) {

            //
            // The mapping base exists so the number of mapped views can be
            // safely incremented.  This prevents a trim from starting after
            // we release the lock.
            //

            MappedSubsection->NumberOfMappedViews += 1;

            MI_SNAP_SUB (MappedSubsection, 0x5);

            if (MappedSubsection->DereferenceList.Flink != NULL) {

                //
                // Remove this from the list of unused subsections.
                //

                RemoveEntryList (&MappedSubsection->DereferenceList);

                MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);

                MappedSubsection->DereferenceList.Flink = NULL;
            }

            //
            // Set the access bit so an already ongoing trim won't blindly
            // delete the prototype PTEs on completion of a mapped write.
            // This can happen if the current thread dirties some pages and
            // then deletes the view before the trim write finishes - this
            // bit informs the trimming thread that a rescan is needed so
            // that writes are not lost.
            //

            MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;
        }
        else {

            ASSERT (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0);
            ASSERT (MappedSubsection->NumberOfMappedViews == 0);

            MI_SNAP_SUB (MappedSubsection, 0x6);

            //
            // No prototype PTEs currently exist for this subsection.
            // Allocate and populate them properly now.
            //

            UNLOCK_PFN (OldIrql);
            *Waited = 1;

            Size = (MappedSubsection->PtesInSubsection + MappedSubsection->UnusedPtes) * sizeof(MMPTE);

            ASSERT (Size != 0);

            ProtoPtes = (PMMPTE)ExAllocatePoolWithTag (PagedPool | POOL_MM_ALLOCATION,
                                                       Size,
                                                       MMSECT);

            if (ProtoPtes == NULL) {
                MI_SNAP_SUB (MappedSubsection, 0x7);
                goto Failed;
            }

            //
            // Fill in the prototype PTEs for this subsection.
            //

            TempPte.u.Long = MiGetSubsectionAddressForPte (MappedSubsection);
            TempPte.u.Soft.Prototype = 1;

            //
            // Set all the PTEs to the initial execute-read-write protection.
            // The section will control access to these and the segment
            // must provide a method to allow other users to map the file
            // for various protections.
            //

            TempPte.u.Soft.Protection = MappedSubsection->ControlArea->Segment->SegmentPteTemplate.u.Soft.Protection;

            MiFillMemoryPte (ProtoPtes, Size / sizeof (MMPTE), TempPte.u.Long);

            LOCK_PFN (OldIrql);

            //
            // Now that the mapping base is guaranteed to be nonzero (shortly),
            // the number of mapped views can be safely incremented.  This
            // prevents a trim from starting after we release the lock.
            //

            MappedSubsection->NumberOfMappedViews += 1;

            MI_SNAP_SUB (MappedSubsection, 0x8);

            //
            // Set the access bit so an already ongoing trim won't blindly
            // delete the prototype PTEs on completion of a mapped write.
            // This can happen if the current thread dirties some pages and
            // then deletes the view before the trim write finishes - this
            // bit informs the trimming thread that a rescan is needed so
            // that writes are not lost.
            //

            MappedSubsection->u2.SubsectionFlags2.SubsectionAccessed = 1;

            //
            // Check to make sure another thread didn't do this already while
            // the lock was released.
            //

            if (MappedSubsection->SubsectionBase == NULL) {

                ASSERT (MappedSubsection->NumberOfMappedViews == 1);

                MappedSubsection->SubsectionBase = ProtoPtes;
            }
            else {
                if (MappedSubsection->DereferenceList.Flink != NULL) {
    
                    //
                    // Remove this from the list of unused subsections.
                    //
    
                    ASSERT (MappedSubsection->NumberOfMappedViews == 1);

                    RemoveEntryList (&MappedSubsection->DereferenceList);
    
                    MI_UNUSED_SUBSECTIONS_COUNT_REMOVE (MappedSubsection);
    
                    MappedSubsection->DereferenceList.Flink = NULL;
                }
                else {
                    ASSERT (MappedSubsection->NumberOfMappedViews > 1);
                }

                //
                // This unlock and release of pool could be postponed until
                // the end of this routine when the lock is released anyway
                // but this should be a rare case anyway so don't bother.
                //

                UNLOCK_PFN (OldIrql);
                ExFreePool (ProtoPtes);
                LOCK_PFN (OldIrql);
            }
            MI_SNAP_SUB (MappedSubsection, 0x9);
        }

        if (LastPteOffset != 0) {
            ASSERT ((LONG)MappedSubsection->PtesInSubsection > 0);
            ASSERT ((UINT64)LastPteOffset > 0);
            if (LastPteOffset <= (UINT64) MappedSubsection->PtesInSubsection) {
                break;
            }
            LastPteOffset -= MappedSubsection->PtesInSubsection;
        }

        MappedSubsection = (PMSUBSECTION) MappedSubsection->NextSubsection;

    } while (MappedSubsection != NULL);

    UNLOCK_PFN (OldIrql);

    return STATUS_SUCCESS;

Failed:

    LOCK_PFN (OldIrql);

    //
    // A prototype PTE pool allocation failed.  Carefully undo any allocations
    // and references done so far.
    //

    while (StartMappedSubsection != MappedSubsection) {
        ASSERT ((LONG_PTR)StartMappedSubsection->NumberOfMappedViews >= 1);
        StartMappedSubsection->NumberOfMappedViews -= 1;
        ASSERT (StartMappedSubsection->u.SubsectionFlags.SubsectionStatic == 0);
        ASSERT (StartMappedSubsection->DereferenceList.Flink == NULL);
        MI_SNAP_SUB (MappedSubsection, 0xA);
        if (StartMappedSubsection->NumberOfMappedViews == 0) {

            //
            // Insert this subsection into the unused subsection list.
            // Since it's not likely there are any resident protos at this
            // point, enqueue each subsection at the front.
            //

            InsertHeadList (&MmUnusedSubsectionList,
                            &StartMappedSubsection->DereferenceList);
            MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
        }
        StartMappedSubsection = (PMSUBSECTION) StartMappedSubsection->NextSubsection;
    }

    UNLOCK_PFN (OldIrql);

    return STATUS_INSUFFICIENT_RESOURCES;
}


VOID
MiRemoveViewsFromSection (
    IN PMSUBSECTION StartMappedSubsection,
    IN UINT64 LastPteOffset OPTIONAL
    )

/*++

Routine Description:

    This nonpagable routine removes the views from the specified section if
    the reference count reaches zero.

    N.B. Callers may pass in view lengths that exceed the length of
         the section and this must succeed.  Thus MiAddViews checks
         for this and knows to stop the references at the end.  More
         importantly, MiRemoveViews must also contain the same logic.

Arguments:

    StartMappedSubsection - Supplies the mapped subsection to start at.

    LastPteOffset - Supplies the number of PTEs (beginning at the supplied
                    subsection) to remove.  Supplies zero to remove views
                    from the supplied subsection to the end of the file.

Return Value:

    None.

Environment:

    Kernel Mode, PFN lock held.

--*/

{
    PMSUBSECTION MappedSubsection;

    MappedSubsection = StartMappedSubsection;

    ASSERT ((MappedSubsection->ControlArea->u.Flags.Image == 0) &&
            (MappedSubsection->ControlArea->FilePointer != NULL) &&
            (MappedSubsection->ControlArea->u.Flags.PhysicalMemory == 0));

    MM_PFN_LOCK_ASSERT();

    do {

        //
        // Note the control area must be active at this point.
        //

        ASSERT (MappedSubsection->ControlArea->DereferenceList.Flink == NULL);
        ASSERT (MappedSubsection->SubsectionBase != NULL);
        ASSERT (MappedSubsection->DereferenceList.Flink == NULL);

        ASSERT (((LONG_PTR)MappedSubsection->NumberOfMappedViews >= 1) ||
                (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1));

        MappedSubsection->NumberOfMappedViews -= 1;

        MI_SNAP_SUB (MappedSubsection, 0x3);

        if ((MappedSubsection->NumberOfMappedViews == 0) &&
            (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 0)) {

            //
            // Insert this subsection into the unused subsection list.
            //

            InsertTailList (&MmUnusedSubsectionList,
                            &MappedSubsection->DereferenceList);
            MI_UNUSED_SUBSECTIONS_COUNT_INSERT (MappedSubsection);
        }

        if (LastPteOffset != 0) {
            if (LastPteOffset <= (UINT64)MappedSubsection->PtesInSubsection) {
                break;
            }
            LastPteOffset -= MappedSubsection->PtesInSubsection;
        }

        MappedSubsection = (PMSUBSECTION) MappedSubsection->NextSubsection;

    } while (MappedSubsection != NULL);

    return;
}


VOID
MiRemoveViewsFromSectionWithPfn (
    IN PMSUBSECTION StartMappedSubsection,
    IN UINT64 LastPteOffset OPTIONAL
    )

/*++

Routine Description:

    This nonpagable routine removes the views from the specified section if
    the reference count reaches zero.

Arguments:

    StartMappedSubsection - Supplies the mapped subsection to start at.

    LastPteOffset - Supplies the number of PTEs (beginning at the supplied
                    subsection) to remove.  Supplies zero to remove views
                    from the supplied subsection to the end of the file.

Return Value:

    None.

Environment:

    Kernel Mode, address creation mutex optionally held if called in process
    context.  APC_LEVEL or below.

--*/

{
    KIRQL OldIrql;

    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    LOCK_PFN (OldIrql);

    MiRemoveViewsFromSection (StartMappedSubsection, LastPteOffset);

    UNLOCK_PFN (OldIrql);
}
#if DBG
extern PMSUBSECTION MiActiveSubsection;
#endif

VOID
MiConvertStaticSubsections (
    IN PCONTROL_AREA ControlArea
    )
{
    PMSUBSECTION MappedSubsection;

    ASSERT (ControlArea->u.Flags.Image == 0);
    ASSERT (ControlArea->FilePointer != NULL);
    ASSERT (ControlArea->u.Flags.PhysicalMemory == 0);

    if (ControlArea->u.Flags.Rom == 0) {
        MappedSubsection = (PMSUBSECTION)(ControlArea + 1);
    }
    else {
        MappedSubsection = (PMSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    do {
        MI_SNAP_SUB (MappedSubsection, 0xB);
        if (MappedSubsection->DereferenceList.Flink != NULL) {

            // 
            // This subsection is already on the dereference subsection list.
            // This is the expected case.
            // 

            NOTHING;
        }
        else if (MappedSubsection->u.SubsectionFlags.SubsectionStatic == 1) {

            // 
            // This subsection was not put on the dereference subsection list
            // because it was created as part of an extension or nowrite.
            // Since this is the last segment dereference, convert the
            // subsection and its prototype PTEs to dynamic now (iff nowrite
            // is clear).
            // 

            MappedSubsection->u.SubsectionFlags.SubsectionStatic = 0;
            MappedSubsection->u2.SubsectionFlags2.SubsectionConverted = 1;
            MappedSubsection->NumberOfMappedViews = 1;

            MiRemoveViewsFromSection (MappedSubsection, 
                                      MappedSubsection->PtesInSubsection);

            MiSubsectionsConvertedToDynamic += 1;
        }
        else if (MappedSubsection->SubsectionBase == NULL) {

            //
            // This subsection has already had its prototype PTEs reclaimed
            // (or never allocated), hence it is not on any reclaim lists.
            //

            NOTHING;
        }
        else {

            // 
            // This subsection is being processed by the dereference
            // segment thread right now !  The dereference thread sets the
            // mapped view count to 1 when it starts processing the subsection.
            // The subsequent flush then increases it to 2 while in progress.
            // So the count must be either 1 or 2 at this point.
            // 

            ASSERT (MappedSubsection == MiActiveSubsection);
            ASSERT ((MappedSubsection->NumberOfMappedViews == 1) ||
                    (MappedSubsection->NumberOfMappedViews == 2));
        }
        MappedSubsection = (PMSUBSECTION) MappedSubsection->NextSubsection;
    } while (MappedSubsection != NULL);
}

NTSTATUS
MiMapViewOfDataSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN PSECTION Section,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG ProtectionMask,
    IN SIZE_T CommitSize,
    IN ULONG_PTR ZeroBits,
    IN ULONG AllocationType
    )

/*++

Routine Description:

    This routine maps the specified mapped file or pagefile-backed section
    into the specified process's address space.

Arguments:

    see MmMapViewOfSection above...

    ControlArea - Supplies the control area for the section.

    Process - Supplies the process pointer which is receiving the section.

    ProtectionMask - Supplies the initial page protection-mask.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, address creation mutex held.

--*/

{
    PMMVAD Vad;
    SIZE_T VadSize;
    PVOID StartingAddress;
    PVOID EndingAddress;
    PSUBSECTION Subsection;
    UINT64 PteOffset;
    UINT64 LastPteOffset;
    UINT64 TotalNumberOfPtes;
    PVOID HighestUserAddress;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    MMPTE TempPte;
    ULONG_PTR Alignment;
    SIZE_T QuotaCharge;
    SIZE_T QuotaExcess;
    PMMPTE TheFirstPrototypePte;
    PVOID CapturedStartingVa;
    ULONG CapturedCopyOnWrite;
    NTSTATUS Status;
    PSEGMENT Segment;
    SIZE_T SizeOfSection;
#if defined(_MIALT4K_)
    SIZE_T ViewSizeFor4k;
    ULONG AltFlags;
#endif

    QuotaCharge = 0;
    Segment = ControlArea->Segment;

    if ((AllocationType & MEM_RESERVE) && (ControlArea->FilePointer == NULL)) {
        return STATUS_INVALID_PARAMETER_9;
    }

    //
    // Check to see if there is a purge operation ongoing for
    // this segment.
    //

    if ((AllocationType & MEM_DOS_LIM) != 0) {
        if ((*CapturedBase == NULL) ||
            (AllocationType & MEM_RESERVE)) {

            //
            // If MEM_DOS_LIM is specified, the address to map the
            // view MUST be specified as well.
            //

            return STATUS_INVALID_PARAMETER_3;
        }
        Alignment = PAGE_SIZE;
    }
    else {
        Alignment = X64K;
    }

    //
    // Check to see if a purge operation is in progress and if so, wait
    // for the purge to complete.  In addition, up the count of mapped
    // views for this control area.
    //

    Status = MiCheckPurgeAndUpMapCount (ControlArea, FALSE);
    
    if (!NT_SUCCESS (Status)) {
        return Status;
    }

    if (*CapturedViewSize == 0) {

        SectionOffset->LowPart &= ~((ULONG)Alignment - 1);
        *CapturedViewSize = (ULONG_PTR)(Section->SizeOfSection.QuadPart -
                                    SectionOffset->QuadPart);
    }
    else {
        *CapturedViewSize += SectionOffset->LowPart & (Alignment - 1);
        SectionOffset->LowPart &= ~((ULONG)Alignment - 1);
    }

    ASSERT ((SectionOffset->LowPart & ((ULONG)Alignment - 1)) == 0);

    if ((LONG_PTR)*CapturedViewSize <= 0) {

        //
        // Section offset or view size past size of section.
        //

        MiDereferenceControlArea (ControlArea);

        return STATUS_INVALID_VIEW_SIZE;
    }

    //
    // Calculate the first prototype PTE field so it can be stored in the Vad.
    //

    ASSERT (ControlArea->u.Flags.GlobalOnlyPerSession == 0);

    if (ControlArea->u.Flags.Rom == 0) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    PteOffset = (SectionOffset->QuadPart >> PAGE_SHIFT);

    //
    // Make sure the PTEs are not in the extended part of the segment.
    //

    TotalNumberOfPtes = (((UINT64)Segment->SegmentFlags.TotalNumberOfPtes4132) << 32) | Segment->TotalNumberOfPtes;

    if (PteOffset >= TotalNumberOfPtes) {
        MiDereferenceControlArea (ControlArea);
        return STATUS_INVALID_VIEW_SIZE;
    }

    LastPteOffset = ((SectionOffset->QuadPart + *CapturedViewSize + PAGE_SIZE - 1) >> PAGE_SHIFT);
    ASSERT (LastPteOffset >= PteOffset);

    while (PteOffset >= (UINT64)Subsection->PtesInSubsection) {
        PteOffset -= Subsection->PtesInSubsection;
        LastPteOffset -= Subsection->PtesInSubsection;
        Subsection = Subsection->NextSubsection;
        ASSERT (Subsection != NULL);
    }

    if (ControlArea->FilePointer != NULL) {

        //
        // Increment the view count for every subsection spanned by this view.
        //
        // N.B. Callers may pass in view lengths that exceed the length of
        // the section and this must succeed.  Thus MiAddViews must check
        // for this and know to stop the references at the end.  More
        // importantly, MiRemoveViews must also contain the same logic.
        //

        Status = MiAddViewsForSectionWithPfn ((PMSUBSECTION)Subsection,
                                              LastPteOffset);

        if (!NT_SUCCESS (Status)) {
            MiDereferenceControlArea (ControlArea);
            return Status;
        }
    }

    ASSERT (Subsection->SubsectionBase != NULL);
    TheFirstPrototypePte = &Subsection->SubsectionBase[PteOffset];

    //
    // Calculate the quota for the specified pages.
    //

    if ((ControlArea->FilePointer == NULL) &&
        (CommitSize != 0) &&
        (Segment->NumberOfCommittedPages < TotalNumberOfPtes)) {

        PointerPte = TheFirstPrototypePte;
        LastPte = PointerPte + BYTES_TO_PAGES(CommitSize);

        //
        // Charge quota for the entire requested range.  If the charge succeeds,
        // excess is returned when the PTEs are actually filled in.
        //

        QuotaCharge = LastPte - PointerPte;
    }

    CapturedStartingVa = (PVOID)Section->Address.StartingVpn;
    CapturedCopyOnWrite = Section->u.Flags.CopyOnWrite;

    if ((*CapturedBase == NULL) && (CapturedStartingVa == NULL)) {

        //
        // The section is not based,
        // find an empty range starting on a 64k boundary.
        //

        if ((AllocationType & MEM_TOP_DOWN) || (Process->VmTopDown == 1)) {

            if (ZeroBits != 0) {
                HighestUserAddress = (PVOID)((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT >> ZeroBits);
                if (HighestUserAddress > MM_HIGHEST_VAD_ADDRESS) {
                    HighestUserAddress = MM_HIGHEST_VAD_ADDRESS;
                }
            }
            else {
                HighestUserAddress = MM_HIGHEST_VAD_ADDRESS;
            }

            Status = MiFindEmptyAddressRangeDown (&Process->VadRoot,
                                                  *CapturedViewSize,
                                                  HighestUserAddress,
                                                  Alignment,
                                                  &StartingAddress);
        }
        else {
            Status = MiFindEmptyAddressRange (*CapturedViewSize,
                                              Alignment,
                                              (ULONG)ZeroBits,
                                              &StartingAddress);
        }

        if (!NT_SUCCESS (Status)) {
            if (ControlArea->FilePointer != NULL) {
                MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                                 LastPteOffset);
            }

            MiDereferenceControlArea (ControlArea);
            return Status;
        }

        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                    *CapturedViewSize - 1L) | (PAGE_SIZE - 1L));

        if (ZeroBits != 0) {
            if (EndingAddress > (PVOID)((ULONG_PTR)MM_USER_ADDRESS_RANGE_LIMIT >> ZeroBits)) {
                if (ControlArea->FilePointer != NULL) {
                    MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                                     LastPteOffset);
                }
                MiDereferenceControlArea (ControlArea);
                return STATUS_NO_MEMORY;
            }
        }
    }
    else {

        if (*CapturedBase == NULL) {

            //
            // The section is based.
            //

#if defined(_WIN64)
            SizeOfSection = SectionOffset->QuadPart;
#else
            SizeOfSection = SectionOffset->LowPart;
#endif

            StartingAddress = (PVOID)((PCHAR)CapturedStartingVa + SizeOfSection);
        }
        else {

            StartingAddress = MI_ALIGN_TO_SIZE (*CapturedBase, Alignment);

        }

        //
        // Check to make sure the specified base address to ending address
        // is currently unused.
        //

        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                   *CapturedViewSize - 1L) | (PAGE_SIZE - 1L));

        if (MiCheckForConflictingVadExistence (Process, StartingAddress, EndingAddress) == TRUE) {
            if (ControlArea->FilePointer != NULL) {
                MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                                 LastPteOffset);
            }
            MiDereferenceControlArea (ControlArea);

            return STATUS_CONFLICTING_ADDRESSES;
        }
    }

    //
    // An unoccupied address range has been found, build the virtual
    // address descriptor to describe this range.
    //

    if (AllocationType & MEM_RESERVE) {
        VadSize = sizeof (MMVAD_LONG);
        Vad = ExAllocatePoolWithTag (NonPagedPool, VadSize, 'ldaV');
    }
    else {
        VadSize = sizeof (MMVAD);
        Vad = ExAllocatePoolWithTag (NonPagedPool, VadSize, MMVADKEY);
    }

    if (Vad == NULL) {
        if (ControlArea->FilePointer != NULL) {
            MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                             LastPteOffset);
        }
        MiDereferenceControlArea (ControlArea);
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    RtlZeroMemory (Vad, VadSize);

    Vad->StartingVpn = MI_VA_TO_VPN (StartingAddress);
    Vad->EndingVpn = MI_VA_TO_VPN (EndingAddress);
    Vad->FirstPrototypePte = TheFirstPrototypePte;

    //
    // Set the protection in the PTE template field of the VAD.
    //

    Vad->ControlArea = ControlArea;

    Vad->u2.VadFlags2.Inherit = (InheritDisposition == ViewShare);
    Vad->u.VadFlags.Protection = ProtectionMask;
    Vad->u2.VadFlags2.CopyOnWrite = CapturedCopyOnWrite;

    //
    // Note that MEM_DOS_LIM significance is lost here, but those
    // files are not mapped MEM_RESERVE.
    //

    Vad->u2.VadFlags2.FileOffset = (ULONG)(SectionOffset->QuadPart >> 16);

    if ((AllocationType & SEC_NO_CHANGE) || (Section->u.Flags.NoChange)) {
        Vad->u.VadFlags.NoChange = 1;
        Vad->u2.VadFlags2.SecNoChange = 1;
    }

    if (AllocationType & MEM_RESERVE) {
        PMMEXTEND_INFO ExtendInfo;
        PMMVAD_LONG VadLong;

        Vad->u2.VadFlags2.LongVad = 1;

        KeAcquireGuardedMutexUnsafe (&MmSectionBasedMutex);
        ExtendInfo = Segment->ExtendInfo;
        if (ExtendInfo) {
            ExtendInfo->ReferenceCount += 1;
        }
        else {

            ExtendInfo = ExAllocatePoolWithTag (NonPagedPool,
                                                sizeof(MMEXTEND_INFO),
                                                'xCmM');
            if (ExtendInfo == NULL) {
                KeReleaseGuardedMutexUnsafe (&MmSectionBasedMutex);

                if (ControlArea->FilePointer != NULL) {
                    MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                                     LastPteOffset);
                }
                MiDereferenceControlArea (ControlArea);
        
                //
                // The pool allocation succeeded, but the quota charge
                // in InsertVad failed, deallocate the pool and return
                // an error.
                //
    
                ExFreePool (Vad);
                return STATUS_INSUFFICIENT_RESOURCES;
            }
            ExtendInfo->ReferenceCount = 1;
            ExtendInfo->CommittedSize = Segment->SizeOfSegment;
            Segment->ExtendInfo = ExtendInfo;
        }
        if (ExtendInfo->CommittedSize < (UINT64)Section->SizeOfSection.QuadPart) {
            ExtendInfo->CommittedSize = (UINT64)Section->SizeOfSection.QuadPart;
        }
        KeReleaseGuardedMutexUnsafe (&MmSectionBasedMutex);
        Vad->u2.VadFlags2.ExtendableFile = 1;

        VadLong = (PMMVAD_LONG) Vad;

        ASSERT (VadLong->u4.ExtendedInfo == NULL);
        VadLong->u4.ExtendedInfo = ExtendInfo;
    }

    //
    // If the page protection is write-copy or execute-write-copy
    // charge for each page in the view as it may become private.
    //

    if (MI_IS_PTE_PROTECTION_COPY_WRITE(ProtectionMask)) {
        Vad->u.VadFlags.CommitCharge = (BYTES_TO_PAGES ((PCHAR) EndingAddress -
                           (PCHAR) StartingAddress));
    }

    PteOffset += (Vad->EndingVpn - Vad->StartingVpn);

    if (PteOffset < Subsection->PtesInSubsection) {
        Vad->LastContiguousPte = &Subsection->SubsectionBase[PteOffset];

    }
    else {
        Vad->LastContiguousPte = &Subsection->SubsectionBase[
                                    (Subsection->PtesInSubsection - 1) +
                                    Subsection->UnusedPtes];
    }

    if (QuotaCharge != 0) {
        if (MiChargeCommitment (QuotaCharge, NULL) == FALSE) {
            if (ControlArea->FilePointer != NULL) {
                MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                                 LastPteOffset);
            }
            MiDereferenceControlArea (ControlArea);
    
            ExFreePool (Vad);
            return STATUS_COMMITMENT_LIMIT;
        }
    }

    ASSERT (Vad->FirstPrototypePte <= Vad->LastContiguousPte);

    LOCK_WS_UNSAFE (Process);

    Status = MiInsertVad (Vad);

    UNLOCK_WS_UNSAFE (Process);

    if (!NT_SUCCESS(Status)) {

        if (ControlArea->FilePointer != NULL) {
            MiRemoveViewsFromSectionWithPfn ((PMSUBSECTION)Subsection,
                                             LastPteOffset);
        }

        MiDereferenceControlArea (ControlArea);

        //
        // The quota charge in InsertVad failed, deallocate the pool and return
        // an error.
        //

        ExFreePool (Vad);
        if (QuotaCharge != 0) {
            MiReturnCommitment (QuotaCharge);
        }
        return Status;
    }

    //
    // Stash the first mapped address for the performance analysis tools.
    // Note this is not synchronized across multiple processes but that's ok.
    //

    if (ControlArea->FilePointer == NULL) {
        if (Segment->u2.FirstMappedVa == NULL) {
            Segment->u2.FirstMappedVa = StartingAddress;
        }
    }

#if DBG
    if (!(AllocationType & MEM_RESERVE)) {
        ASSERT(((ULONG_PTR)EndingAddress - (ULONG_PTR)StartingAddress) <=
                ROUND_TO_PAGES64(Segment->SizeOfSegment));
    }
#endif

    //
    // If a commit size was specified, make sure those pages are committed.
    //

    if (QuotaCharge != 0) {

        PointerPte = Vad->FirstPrototypePte;
        LastPte = PointerPte + BYTES_TO_PAGES(CommitSize);
        TempPte = Segment->SegmentPteTemplate;
        QuotaExcess = 0;

        KeAcquireGuardedMutexUnsafe (&MmSectionCommitMutex);

        while (PointerPte < LastPte) {

            if (PointerPte->u.Long == 0) {

                MI_WRITE_INVALID_PTE (PointerPte, TempPte);
            }
            else {
                QuotaExcess += 1;
            }
            PointerPte += 1;
        }

        ASSERT (QuotaCharge >= QuotaExcess);
        QuotaCharge -= QuotaExcess;

        MM_TRACK_COMMIT (MM_DBG_COMMIT_MAPVIEW_DATA, QuotaCharge);

        Segment->NumberOfCommittedPages += QuotaCharge;

        ASSERT (Segment->NumberOfCommittedPages <= TotalNumberOfPtes);

        KeReleaseGuardedMutexUnsafe (&MmSectionCommitMutex);

        InterlockedExchangeAddSizeT (&MmSharedCommit, QuotaCharge);

        if (QuotaExcess != 0) {
            MiReturnCommitment (QuotaExcess);
        }
    }

#if defined(_MIALT4K_)

    if (Process->Wow64Process != NULL) {


        EndingAddress = (PVOID)(((ULONG_PTR)StartingAddress +
                                 *CapturedViewSize - 1L) | (PAGE_4K - 1L));

        ViewSizeFor4k = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;

        if (ControlArea->FilePointer != NULL) {

            AltFlags = (AllocationType & MEM_RESERVE) ? 0 : ALT_COMMIT;

            MiProtectFor4kPage (StartingAddress,
                                ViewSizeFor4k,
                                ProtectionMask,
                                (ALT_ALLOCATE | AltFlags),
                                Process);
        }
        else {

            MiProtectMapFileFor4kPage (StartingAddress,
                                       ViewSizeFor4k,
                                       ProtectionMask, 
                                       CommitSize,
                                       Vad->FirstPrototypePte,
                                       Vad->LastContiguousPte,
                                       Process);
        }
    }
#endif

    //
    // Update the current virtual size in the process header.
    //

    *CapturedViewSize = (PCHAR)EndingAddress - (PCHAR)StartingAddress + 1L;
    Process->VirtualSize += *CapturedViewSize;

    if (Process->VirtualSize > Process->PeakVirtualSize) {
        Process->PeakVirtualSize = Process->VirtualSize;
    }

    *CapturedBase = StartingAddress;

    //
    // Update the count of writable user mappings so the transaction semantics
    // can be supported.  Note that no lock synchronization is needed here as
    // the transaction manager must already check for any open writable handles
    // to the file - and no writable sections can be created without a writable
    // file handle.  So all that needs to be provided is a way for the
    // transaction manager to know that there are lingering views or created
    // sections still open that have write access.
    //

    if ((ProtectionMask & MM_READWRITE) &&
        (ControlArea->FilePointer != NULL)) {

#if 0
        //
        // The section may no longer exist at this point so these ASSERTs
        // cannot be enabled.
        //

        ASSERT (Section->u.Flags.UserWritable == 1);
        ASSERT (Section->InitialPageProtection & (PAGE_READWRITE|PAGE_EXECUTE_READWRITE));
#endif

        InterlockedIncrement ((PLONG)&Segment->WritableUserReferences);
    }

    return STATUS_SUCCESS;
}

NTSTATUS
MiCheckPurgeAndUpMapCount (
    IN PCONTROL_AREA ControlArea,
    IN LOGICAL FailIfSystemViews
    )

/*++

Routine Description:

    This routine synchronizes with any on going purge operations
    on the same segment (identified via the control area).  If
    another purge operation is occurring, the function blocks until
    it is completed.

    When this function returns the MappedView and the NumberOfUserReferences
    count for the control area will be incremented thereby referencing
    the control area.

Arguments:

    ControlArea - Supplies the control area for the segment to be purged.

    FailIfSystemViews - Supplies TRUE if the request should fail when there
                        are active kernel mappings.  When the image is mapped
                        in system or session space as a driver, copy on write
                        does not work so user processes are not allowed to
                        map the image.

Return Value:

    STATUS_SUCCESS if the synchronization was successful.
    NTSTATUS if the synchronization did not occur due to low resources, etc.

Environment:

    Kernel Mode.

--*/

{
    NTSTATUS Status;
    KIRQL OldIrql;
    PEVENT_COUNTER PurgedEvent;
    PEVENT_COUNTER WaitEvent;
    ULONG OldRef;
    PKTHREAD CurrentThread;

    PurgedEvent = NULL;
    OldRef = 1;

    if (FailIfSystemViews == TRUE) {
        ASSERT (ControlArea->u.Flags.Image != 0);
    }

    LOCK_PFN (OldIrql);

    while (ControlArea->u.Flags.BeingPurged != 0) {

        //
        // A purge operation is in progress.
        //

        if (PurgedEvent == NULL) {

            //
            // Release the locks and allocate pool for the event.
            //

            UNLOCK_PFN (OldIrql);
            PurgedEvent = MiGetEventCounter ();
            if (PurgedEvent == NULL) {
                return STATUS_INSUFFICIENT_RESOURCES;
            }

            LOCK_PFN (OldIrql);
            continue;
        }

        if (ControlArea->WaitingForDeletion == NULL) {
            ControlArea->WaitingForDeletion = PurgedEvent;
            WaitEvent = PurgedEvent;
            PurgedEvent = NULL;
        }
        else {
            WaitEvent = ControlArea->WaitingForDeletion;

            //
            // No interlock is needed for the RefCount increment as
            // no thread can be decrementing it since it is still
            // pointed to by the control area.
            //

            WaitEvent->RefCount += 1;
        }

        //
        // Release the PFN lock and wait for the event.
        //

        CurrentThread = KeGetCurrentThread ();
        KeEnterCriticalRegionThread (CurrentThread);
        UNLOCK_PFN_AND_THEN_WAIT(OldIrql);

        KeWaitForSingleObject(&WaitEvent->Event,
                              WrVirtualMemory,
                              KernelMode,
                              FALSE,
                              (PLARGE_INTEGER)NULL);

        //
        // Before this event can be set, the control area WaitingForDeletion
        // field must be cleared (and may be reinitialized to something else),
        // but cannot be reset to our local event.  This allows us to
        // dereference the event count lock free.
        //

        ASSERT (WaitEvent != ControlArea->WaitingForDeletion);

        MiFreeEventCounter (WaitEvent);

        LOCK_PFN (OldIrql);
        KeLeaveCriticalRegionThread (CurrentThread);
    }

    if ((FailIfSystemViews == TRUE) &&
        (ControlArea->u.Flags.ImageMappedInSystemSpace) &&
        (KeGetPreviousMode() != KernelMode)) {

        UNLOCK_PFN (OldIrql);
        Status = STATUS_CONFLICTING_ADDRESSES;
    }
    else {

        //
        // Indicate another file is mapped for the segment.
        //

        ControlArea->NumberOfMappedViews += 1;
        ControlArea->NumberOfUserReferences += 1;
        ControlArea->u.Flags.HadUserReference = 1;
        ASSERT (ControlArea->NumberOfSectionReferences != 0);

        UNLOCK_PFN (OldIrql);
        Status = STATUS_SUCCESS;
    }

    if (PurgedEvent != NULL) {
        MiFreeEventCounter (PurgedEvent);
    }

    return Status;
}

typedef struct _NTSYM {
    struct _NTSYM *Next;
    PVOID SymbolTable;
    ULONG NumberOfSymbols;
    PVOID StringTable;
    USHORT Flags;
    USHORT EntrySize;
    ULONG MinimumVa;
    ULONG MaximumVa;
    PCHAR MapName;
    ULONG MapNameLen;
} NTSYM, *PNTSYM;

PVOID
MiCacheImageSymbols (
    IN PVOID ImageBase
    )
{
    ULONG DebugSize;
    PVOID DebugDirectory;

    PAGED_CODE ();

    DebugDirectory = NULL;

    try {

        DebugDirectory = RtlImageDirectoryEntryToData (ImageBase,
                                                       TRUE,
                                                       IMAGE_DIRECTORY_ENTRY_DEBUG,
                                                       &DebugSize);
        //
        // If using remote KD, ImageBase is what it wants to see.
        //

    } except (EXCEPTION_EXECUTE_HANDLER) {

        NOTHING;
    }

    return DebugDirectory;
}


NTSYSAPI
NTSTATUS
NTAPI
NtAreMappedFilesTheSame (
    IN PVOID File1MappedAsAnImage,
    IN PVOID File2MappedAsFile
    )

/*++

Routine Description:

    This routine compares the two files mapped at the specified
    addresses to see if they are both the same file.

Arguments:

    File1MappedAsAnImage - Supplies an address within the first file which
        is mapped as an image file.

    File2MappedAsFile - Supplies an address within the second file which
        is mapped as either an image file or a data file.

Return Value:


    STATUS_SUCCESS is returned if the two files are the same.

    STATUS_NOT_SAME_DEVICE is returned if the files are different.

    Other status values can be returned if the addresses are not mapped as
    files, etc.

Environment:

    User mode callable system service.

--*/

{
    PMMVAD FoundVad1;
    PMMVAD FoundVad2;
    NTSTATUS Status;
    PEPROCESS Process;

    Process = PsGetCurrentProcess();

    LOCK_ADDRESS_SPACE (Process);

    FoundVad1 = MiLocateAddress (File1MappedAsAnImage);
    FoundVad2 = MiLocateAddress (File2MappedAsFile);

    if ((FoundVad1 == NULL) || (FoundVad2 == NULL)) {

        //
        // No virtual address is allocated at the specified base address,
        // return an error.
        //

        Status = STATUS_INVALID_ADDRESS;
        goto ErrorReturn;
    }

    //
    // Check file names.
    //

    if ((FoundVad1->u.VadFlags.PrivateMemory == 1) ||
        (FoundVad2->u.VadFlags.PrivateMemory == 1)) {
        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorReturn;
    }

    if ((FoundVad1->ControlArea == NULL) ||
        (FoundVad2->ControlArea == NULL)) {
        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorReturn;
    }

    if ((FoundVad1->ControlArea->FilePointer == NULL) ||
        (FoundVad2->ControlArea->FilePointer == NULL)) {
        Status = STATUS_CONFLICTING_ADDRESSES;
        goto ErrorReturn;
    }

    Status = STATUS_NOT_SAME_DEVICE;

    if ((PVOID)FoundVad1->ControlArea ==
            FoundVad2->ControlArea->FilePointer->SectionObjectPointer->ImageSectionObject) {
        Status = STATUS_SUCCESS;
    }

ErrorReturn:

    UNLOCK_ADDRESS_SPACE (Process);
    return Status;
}



NTSTATUS
MiSetPageModified (
    IN PMMVAD Vad,
    IN PVOID Address
    )

/*++

Routine Description:

    This routine sets the modified bit in the PFN database for the
    pages that correspond to the specified address range.

    Note that the dirty bit in the PTE is cleared by this operation.

Arguments:

    Vad - Supplies the VAD to charge.

    Address - Supplies the user address to access.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  Address space mutex held.

--*/

{
    PMMPFN Pfn1;
    NTSTATUS Status;
    PEPROCESS CurrentProcess;
    SIZE_T RealCharge;
    MMPTE PteContents;
    KIRQL OldIrql;
    PMMPTE PointerPxe;
    PMMPTE PointerPpe;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    LOGICAL ReturnCommitment;
    LOGICAL ChargedJobCommit;

    //
    // Charge commitment up front even though we may not use it because
    // failing to get it later would make things messy.
    //

    RealCharge = 1;
    ReturnCommitment = FALSE;
    ChargedJobCommit = FALSE;

    CurrentProcess = PsGetCurrentProcess ();

    Status = PsChargeProcessPageFileQuota (CurrentProcess, RealCharge);

    if (!NT_SUCCESS (Status)) {
        return STATUS_COMMITMENT_LIMIT;
    }

    if (CurrentProcess->CommitChargeLimit) {
        if (CurrentProcess->CommitCharge + RealCharge > CurrentProcess->CommitChargeLimit) {
            if (CurrentProcess->Job) {
                PsReportProcessMemoryLimitViolation ();
            }
            PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
            return STATUS_COMMITMENT_LIMIT;
        }
    }

    if (CurrentProcess->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {

        if (PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES,
                                    RealCharge) == FALSE) {

            PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
            return STATUS_COMMITMENT_LIMIT;
        }
        ChargedJobCommit = TRUE;
    }

    if (MiChargeCommitment (RealCharge, NULL) == FALSE) {
        if (ChargedJobCommit == TRUE) {
            PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES,
                                    -(SSIZE_T)RealCharge);
        }
        PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
        return STATUS_COMMITMENT_LIMIT;
    }

    CurrentProcess->CommitCharge += RealCharge;

    if (CurrentProcess->CommitCharge > CurrentProcess->CommitChargePeak) {
        CurrentProcess->CommitChargePeak = CurrentProcess->CommitCharge;
    }

    MI_INCREMENT_TOTAL_PROCESS_COMMIT (RealCharge);
    MM_TRACK_COMMIT (MM_DBG_COMMIT_INSERT_VAD, RealCharge);

    PointerPte = MiGetPteAddress (Address);
    PointerPde = MiGetPdeAddress (Address);
    PointerPpe = MiGetPpeAddress (Address);
    PointerPxe = MiGetPxeAddress (Address);

    do {

        try {

            *(volatile CCHAR *)Address;

        } except (EXCEPTION_EXECUTE_HANDLER) {

            CurrentProcess->CommitCharge -= RealCharge;
            if (ChargedJobCommit == TRUE) {
                PsChangeJobMemoryUsage (PS_JOB_STATUS_REPORT_COMMIT_CHANGES,
                                        -(SSIZE_T)RealCharge);
            }
            MiReturnCommitment (RealCharge);
            PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
            return GetExceptionCode ();
        }

        LOCK_WS_UNSAFE (CurrentProcess);

#if (_MI_PAGING_LEVELS >= 4)
        if ((PointerPxe->u.Hard.Valid == 0) ||
            (PointerPpe->u.Hard.Valid == 0) ||
            (PointerPde->u.Hard.Valid == 0) ||
            (PointerPte->u.Hard.Valid == 0))
#elif (_MI_PAGING_LEVELS >= 3)
        if ((PointerPpe->u.Hard.Valid == 0) ||
            (PointerPde->u.Hard.Valid == 0) ||
            (PointerPte->u.Hard.Valid == 0))
#else
        if ((PointerPde->u.Hard.Valid == 0) ||
            (PointerPte->u.Hard.Valid == 0))
#endif
        {
            //
            // Page is no longer valid.
            //

            UNLOCK_WS_UNSAFE (CurrentProcess);

            continue;
        }

        break;

    } while (TRUE);

    PteContents = *PointerPte;

    ASSERT (PteContents.u.Hard.Valid == 1);

    Pfn1 = MI_PFN_ELEMENT (PteContents.u.Hard.PageFrameNumber);

    LOCK_PFN (OldIrql);

    MI_SET_MODIFIED (Pfn1, 1, 0x8);

    if (Pfn1->OriginalPte.u.Soft.Prototype == 0) {
        if (Pfn1->u3.e1.WriteInProgress == 0) {
            MiReleasePageFileSpace (Pfn1->OriginalPte);
            Pfn1->OriginalPte.u.Soft.PageFileHigh = 0;
        }

        //
        // We didn't need to (and shouldn't have) charged commitment for
        // this page as it was already pagefile backed (ie: someone else
        // already charged commitment for it earlier).
        //

        ReturnCommitment = TRUE;
    }

#ifdef NT_UP
    if (MI_IS_PTE_DIRTY (PteContents)) {
#endif //NT_UP
        MI_SET_PTE_CLEAN (PteContents);

        //
        // Clear the dirty bit in the PTE so new writes can be tracked.
        //

        MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, PteContents);

        KeFlushSingleTb (Address, TRUE);
#ifdef NT_UP
    }
#endif //NT_UP

    UNLOCK_PFN (OldIrql);

    UNLOCK_WS_UNSAFE (CurrentProcess);

    if (ReturnCommitment == TRUE) {
        CurrentProcess->CommitCharge -= RealCharge;
        if (ChargedJobCommit == TRUE) {
            PsChangeJobMemoryUsage(PS_JOB_STATUS_REPORT_COMMIT_CHANGES, -(SSIZE_T)RealCharge);
        }
        MiReturnCommitment (RealCharge);
        PsReturnProcessPageFileQuota (CurrentProcess, RealCharge);
    }
    else {

        //
        // Commit has been charged for the copied page, add the charge
        // to the VAD now so it is automatically returned when the VAD is
        // deleted later.
        //

        MM_TRACK_COMMIT (MM_DBG_COMMIT_IMAGE, 1);

        ASSERT (Vad->u.VadFlags.CommitCharge != MM_MAX_COMMIT);
        Vad->u.VadFlags.CommitCharge += 1;
    }

    return STATUS_SUCCESS;
}


NTSTATUS
MmMapViewInSystemSpace (
    IN PVOID Section,
    OUT PVOID *MappedBase,
    IN OUT PSIZE_T ViewSize
    )

/*++

Routine Description:

    This routine maps the specified section into the system's address space.

Arguments:

    Section - Supplies a pointer to the section to map.

    *MappedBase - Returns the address where the section was mapped.

    ViewSize - Supplies the size of the view to map.  If this
               is specified as zero, the whole section is mapped.
               Returns the actual size mapped.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, APC_LEVEL or below.

--*/

{
    PMMSESSION  Session;

    PAGED_CODE();

    Session = &MmSession;

    return MiMapViewInSystemSpace (Section,
                                   Session,
                                   MappedBase,
                                   ViewSize);
}


NTSTATUS
MmMapViewInSessionSpace (
    IN PVOID Section,
    OUT PVOID *MappedBase,
    IN OUT PSIZE_T ViewSize
    )

/*++

Routine Description:

    This routine maps the specified section into the current process's
    session address space.

Arguments:

    Section - Supplies a pointer to the section to map.

    *MappedBase - Returns the address where the section was mapped.

    ViewSize - Supplies the size of the view to map.  If this
               is specified as zero, the whole section is mapped.
               Returns the actual size mapped.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, APC_LEVEL or below.

--*/

{
    PMMSESSION Session;

    PAGED_CODE();

    if ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) {
        return STATUS_NOT_MAPPED_VIEW;
    }

    ASSERT (MmIsAddressValid(MmSessionSpace) == TRUE);
    Session = &MmSessionSpace->Session;

    return MiMapViewInSystemSpace (Section,
                                   Session,
                                   MappedBase,
                                   ViewSize);
}


NTSTATUS
MiMapViewInSystemSpace (
    IN PVOID Section,
    IN PMMSESSION Session,
    OUT PVOID *MappedBase,
    IN OUT PSIZE_T ViewSize
    )

/*++

Routine Description:

    This routine maps the specified section into the system's address space.

Arguments:

    Section - Supplies a pointer to the section to map.

    Session - Supplies the session data structure for this view.

    *MappedBase - Returns the address where the section was mapped.

    ViewSize - Supplies the size of the view to map.  If this
               is specified as zero, the whole section is mapped.
               Returns the actual size mapped.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, IRQL of APC_LEVEL or below.
    
--*/

{
    PVOID Base;
    PSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    PCONTROL_AREA NewControlArea;
    ULONG StartBit;
    ULONG SizeIn64k;
    ULONG NewSizeIn64k;
    PMMPTE BasePte;
    PFN_NUMBER NumberOfPtes;
    SIZE_T NumberOfBytes;
    SIZE_T SectionSize;
    NTSTATUS Status;

    PAGED_CODE();

    //
    // Check to see if a purge operation is in progress and if so, wait
    // for the purge to complete.  In addition, up the count of mapped
    // views for this control area.
    //

    ControlArea = ((PSECTION)Section)->Segment->ControlArea;

    if ((ControlArea->u.Flags.GlobalOnlyPerSession == 0) &&
        (ControlArea->u.Flags.Rom == 0)) {
        Subsection = (PSUBSECTION)(ControlArea + 1);
    }
    else {
        Subsection = (PSUBSECTION)((PLARGE_CONTROL_AREA)ControlArea + 1);
    }

    Status = MiCheckPurgeAndUpMapCount (ControlArea, FALSE);
    
    if (!NT_SUCCESS (Status)) {
        return Status;
    }

#if defined (_WIN64)
    SectionSize = ((PSECTION)Section)->SizeOfSection.QuadPart;
#else
    SectionSize = ((PSECTION)Section)->SizeOfSection.LowPart;
#endif

    if (*ViewSize == 0) {

        *ViewSize = SectionSize;

    }
    else if (*ViewSize > SectionSize) {

        //
        // Section offset or view size past size of section.
        //

        MiDereferenceControlArea (ControlArea);
        return STATUS_INVALID_VIEW_SIZE;
    }

    //
    // Calculate the first prototype PTE field in the Vad.
    //

    SizeIn64k = (ULONG)((*ViewSize / X64K) + ((*ViewSize & (X64K - 1)) != 0));

    //
    // 4GB-64K is the maximum individual view size allowed since we encode
    // this into the bottom 16 bits of the MMVIEW entry.
    //

    if (SizeIn64k >= X64K) {
        MiDereferenceControlArea (ControlArea);
        return STATUS_INVALID_VIEW_SIZE;
    }

    Base = MiInsertInSystemSpace (Session, SizeIn64k, ControlArea);

    if (Base == NULL) {
        MiDereferenceControlArea (ControlArea);
        return STATUS_NO_MEMORY;
    }

    NumberOfBytes = (SIZE_T)SizeIn64k * X64K;

    if (Session == &MmSession) {
        MiFillSystemPageDirectory (Base, NumberOfBytes);
        Status = STATUS_SUCCESS;
    }
    else {

        Status = MiSessionCommitPageTables (Base,
                                (PVOID)((ULONG_PTR)Base + NumberOfBytes));
    }

    if (!NT_SUCCESS (Status)) {

bail:

        MiDereferenceControlArea (ControlArea);

        StartBit = (ULONG) (((ULONG_PTR)Base - (ULONG_PTR)Session->SystemSpaceViewStart) >> 16);

        LOCK_SYSTEM_VIEW_SPACE (Session);

        NewSizeIn64k = MiRemoveFromSystemSpace (Session, Base, &NewControlArea);

        ASSERT (ControlArea == NewControlArea);
        ASSERT (SizeIn64k == NewSizeIn64k);

        RtlClearBits (Session->SystemSpaceBitMap, StartBit, SizeIn64k);

        UNLOCK_SYSTEM_VIEW_SPACE (Session);

        return Status;
    }

    BasePte = MiGetPteAddress (Base);
    NumberOfPtes = BYTES_TO_PAGES (*ViewSize);

    //
    // Setup PTEs to point to prototype PTEs.
    //

    Status = MiAddMappedPtes (BasePte, NumberOfPtes, ControlArea);

    if (!NT_SUCCESS (Status)) {
        goto bail;
    }

    *MappedBase = Base;

    return STATUS_SUCCESS;
}

NTSTATUS
MmGetSessionMappedViewInformation (
    OUT PVOID SystemInformation,
    IN ULONG SystemInformationLength,
    OUT PULONG Length,
    IN PULONG SessionId OPTIONAL
    )

/*++

Routine Description:

    This function returns information about all the per-session mapped
    views in the system.

Arguments:

    SystemInformation - A pointer to a buffer which receives the specified
                        information.

    SystemInformationLength - Specifies the length in bytes of the system
                              information buffer.

    Length - Receives the number of bytes placed (or would have been placed)
             in the system information buffer.

    SessionId - Session Id (-1 indicates enumerate all sessions).

Environment:

    Kernel mode.

Return Value:

    Returns one of the following status codes:

        STATUS_SUCCESS - normal, successful completion.

        STATUS_INVALID_INFO_CLASS - The SystemInformationClass parameter
            did not specify a valid value.

        STATUS_INFO_LENGTH_MISMATCH - The value of the SystemInformationLength
            parameter did not match the length required for the information
            class requested by the SystemInformationClass parameter.

        STATUS_ACCESS_VIOLATION - Either the SystemInformation buffer pointer
            or the Length pointer value specified an invalid address.

        STATUS_WORKING_SET_QUOTA - The process does not have sufficient
            working set to lock the specified output structure in memory.

        STATUS_INSUFFICIENT_RESOURCES - Insufficient system resources exist
            for this request to complete.

--*/

{
    PMMSESSION Session;
    KAPC_STATE ApcState;
    PVOID MappedAddress;
    PVOID OpaqueSession;
    PVOID LockVariable;
    ULONG ContiguousFree;
    ULONG Index;
    ULONG Hint;
    ULONG TotalFree;
    ULONG TotalSize;
    ULONG CurrentSessionId;
    ULONG StartingIndex;
    NTSTATUS status;
    NTSTATUS status1;
    PSYSTEM_SESSION_MAPPED_VIEW_INFORMATION SessionMappedViewInfo;

    *Length = 0;
    TotalSize = 0;
    status = STATUS_SUCCESS;
    SessionMappedViewInfo = NULL;

    if (SystemInformationLength > 0) {

        status1 = ExLockUserBuffer (SystemInformation,
                                    SystemInformationLength,
                                    KeGetPreviousMode(),
                                    IoWriteAccess,
                                    &MappedAddress,
                                    &LockVariable);

        if (!NT_SUCCESS(status1)) {
            return status1;
        }

    }
    else {

        //
        // This indicates the caller just wants to know the size of the
        // buffer to allocate but is not prepared to accept any data content
        // in this instance.
        //

        MappedAddress = NULL;
        LockVariable = NULL;
    }

    for (OpaqueSession = MmGetNextSession (NULL);
         OpaqueSession != NULL;
         OpaqueSession = MmGetNextSession (OpaqueSession)) {

        SessionMappedViewInfo = (PSYSTEM_SESSION_MAPPED_VIEW_INFORMATION)
                                ((PUCHAR)MappedAddress + TotalSize);

        //
        // If a specific session was requested, only extract that one.
        //

        CurrentSessionId = MmGetSessionId (OpaqueSession);

        if ((*SessionId == 0xFFFFFFFF) || (CurrentSessionId == *SessionId)) {

            //
            // Attach to session now to perform operations...
            //

            if (NT_SUCCESS (MmAttachSession (OpaqueSession, &ApcState))) {

                //
                // Session is still alive so include it.
                //

                TotalSize += sizeof (SYSTEM_SESSION_MAPPED_VIEW_INFORMATION);

                if (TotalSize > SystemInformationLength) {

                    status = STATUS_INFO_LENGTH_MISMATCH;

                }
                else {

                    //
                    // Get mapped view information for each session.
                    //

                    Session = &MmSessionSpace->Session;

                    Hint = 0;
                    TotalFree = 0;

                    LOCK_SYSTEM_VIEW_SPACE (Session);

                    ContiguousFree = RtlFindLongestRunClear (
                                                    Session->SystemSpaceBitMap,
                                                    &StartingIndex);

                    do {
                        Index = RtlFindClearBits (Session->SystemSpaceBitMap,
                                                  1,
                                                  Hint);

                        if ((Index < Hint) || (Index == NO_BITS_FOUND)) {
                            break;
                        }

                        TotalFree += 1;
                        Hint = Index + 1;

                    } while (TRUE);

                    UNLOCK_SYSTEM_VIEW_SPACE (Session);

                    SessionMappedViewInfo->NumberOfBytesAvailable =
                            (SIZE_T)TotalFree * X64K;

                    SessionMappedViewInfo->NumberOfBytesAvailableContiguous =
                            (SIZE_T)ContiguousFree * X64K;

                    SessionMappedViewInfo->SessionId = CurrentSessionId;
                    SessionMappedViewInfo->ViewFailures = Session->BitmapFailures;

                    //
                    // Point to next session.
                    //

                    SessionMappedViewInfo->NextEntryOffset = TotalSize;
                }

                //
                // Detach from session.
                //

                MmDetachSession (OpaqueSession, &ApcState);
            }

            //
            // Bail if only this session was of interest.
            //

            if (*SessionId != 0xFFFFFFFF) {
                MmQuitNextSession (OpaqueSession);
                break;
            }
        }
    }

    if ((NT_SUCCESS (status)) && (SessionMappedViewInfo != NULL)) {
        SessionMappedViewInfo->NextEntryOffset = 0;
    }

    if (MappedAddress != NULL) {
        ExUnlockUserBuffer (LockVariable);
    }

    *Length = TotalSize;

    return status;
}

VOID
MiFillSystemPageDirectory (
    IN PVOID Base,
    IN SIZE_T NumberOfBytes
    )

/*++

Routine Description:

    This routine allocates page tables and fills the system page directory
    entries for the specified virtual address range.

Arguments:

    Base - Supplies the virtual address of the view.

    NumberOfBytes - Supplies the number of bytes the view spans.

Return Value:

    None.

Environment:

    Kernel Mode, IRQL of dispatch level.

    This routine could be made PAGELK but it is a high frequency routine
    so it is actually better to keep it nonpaged to avoid bringing in the
    entire PAGELK section.

--*/

{
    PMMPTE FirstPde;
    PMMPTE LastPde;
    PMMPTE FirstSystemPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    KIRQL OldIrql;
#if (_MI_PAGING_LEVELS < 3)
    ULONG i;
#endif

    PAGED_CODE();

    //
    // CODE IS ALREADY LOCKED BY CALLER.
    //

    FirstPde = MiGetPdeAddress (Base);
    LastPde = MiGetPdeAddress ((PVOID)(((PCHAR)Base) + NumberOfBytes - 1));

    PointerPpe = MiGetPpeAddress (Base);
    PointerPxe = MiGetPxeAddress (Base);

#if (_MI_PAGING_LEVELS >= 3)
    FirstSystemPde = FirstPde;
#else
    FirstSystemPde = &MmSystemPagePtes[((ULONG_PTR)FirstPde &
                     (PD_PER_SYSTEM * (PDE_PER_PAGE * sizeof(MMPTE)) - 1)) / sizeof(MMPTE) ];
#endif

    do {

#if (_MI_PAGING_LEVELS >= 4)
        if (PointerPxe->u.Hard.Valid == 0) {

            //
            // No page directory page exists, get a page and map it in.
            //

            TempPte = ValidKernelPde;

            LOCK_PFN (OldIrql);

            if (PointerPxe->u.Hard.Valid == 0) {

                if ((MmAvailablePages < MM_HIGH_LIMIT) &&
                    (MiEnsureAvailablePageOrWait (NULL, PointerPxe, OldIrql))) {

                    //
                    // PFN_LOCK was dropped, redo this loop as another process
                    // could have made this PDE valid.
                    //

                    UNLOCK_PFN (OldIrql);
                    continue;
                }

                MiChargeCommitmentCantExpand (1, TRUE);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_FILL_SYSTEM_DIRECTORY, 1);
                PageFrameIndex = MiRemoveAnyPage (
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPxe));
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (PointerPxe, TempPte);

                MiInitializePfn (PageFrameIndex, PointerPxe, 1);

                KeZeroPages (MiGetVirtualAddressMappedByPte (PointerPxe),
                             PAGE_SIZE);
            }
            UNLOCK_PFN (OldIrql);
        }
#endif

#if (_MI_PAGING_LEVELS >= 3)
        if (PointerPpe->u.Hard.Valid == 0) {

            //
            // No page directory page exists, get a page and map it in.
            //

            TempPte = ValidKernelPde;

            LOCK_PFN (OldIrql);

            if (PointerPpe->u.Hard.Valid == 0) {

                if ((MmAvailablePages < MM_HIGH_LIMIT) &&
                    (MiEnsureAvailablePageOrWait (NULL, PointerPpe, OldIrql))) {

                    //
                    // PFN_LOCK was dropped, redo this loop as another process
                    // could have made this PDE valid.
                    //

                    UNLOCK_PFN (OldIrql);
                    continue;
                }

                MiChargeCommitmentCantExpand (1, TRUE);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_FILL_SYSTEM_DIRECTORY, 1);
                PageFrameIndex = MiRemoveAnyPage (
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPpe));
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (PointerPpe, TempPte);

                MiInitializePfn (PageFrameIndex, PointerPpe, 1);

                KeZeroPages (MiGetVirtualAddressMappedByPte (PointerPpe),
                             PAGE_SIZE);
            }
            UNLOCK_PFN (OldIrql);
        }
#endif

        if (FirstSystemPde->u.Hard.Valid == 0) {

            //
            // No page table page exists, get a page and map it in.
            //

            TempPte = ValidKernelPde;

            LOCK_PFN (OldIrql);

            if (FirstSystemPde->u.Hard.Valid == 0) {

                if ((MmAvailablePages < MM_HIGH_LIMIT) &&
                    (MiEnsureAvailablePageOrWait (NULL, FirstSystemPde, OldIrql))) {

                    //
                    // PFN_LOCK was dropped, redo this loop as another process
                    // could have made this PDE valid.
                    //

                    UNLOCK_PFN (OldIrql);
                    continue;
                }

                MiChargeCommitmentCantExpand (1, TRUE);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_FILL_SYSTEM_DIRECTORY, 1);
                PageFrameIndex = MiRemoveAnyPage (
                                    MI_GET_PAGE_COLOR_FROM_PTE (FirstSystemPde));
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;

                MI_WRITE_VALID_PTE (FirstSystemPde, TempPte);

                //
                // The FirstPde and FirstSystemPde may be identical even on
                // 32-bit machines if we are currently running in the
                // system process, so check for the valid bit first so we
                // don't assert on a checked build.
                //

                if (FirstPde->u.Hard.Valid == 0) {
                    MI_WRITE_VALID_PTE (FirstPde, TempPte);
                }

#if (_MI_PAGING_LEVELS >= 3)
                MiInitializePfn (PageFrameIndex, FirstPde, 1);
#else
                i = (FirstPde - MiGetPdeAddress(0)) / PDE_PER_PAGE;
                MiInitializePfnForOtherProcess (PageFrameIndex,
                                                FirstPde,
                                                MmSystemPageDirectory[i]);
#endif

                KeZeroPages (MiGetVirtualAddressMappedByPte (FirstPde),
                             PAGE_SIZE);
            }
            UNLOCK_PFN (OldIrql);
        }

        FirstSystemPde += 1;
        FirstPde += 1;
#if (_MI_PAGING_LEVELS >= 3)
        if (MiIsPteOnPdeBoundary (FirstPde)) {
            PointerPpe = MiGetPteAddress (FirstPde);
            if (MiIsPteOnPpeBoundary (FirstPde)) {
                PointerPxe = MiGetPdeAddress (FirstPde);
            }
        }
#endif
    } while (FirstPde <= LastPde);
}

NTSTATUS
MmUnmapViewInSystemSpace (
    IN PVOID MappedBase
    )

/*++

Routine Description:

    This routine unmaps the specified section from the system's address space.

Arguments:

    MappedBase - Supplies the address of the view to unmap.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, IRQL of dispatch level.

--*/

{
    PAGED_CODE();

    return MiUnmapViewInSystemSpace (&MmSession, MappedBase);
}

NTSTATUS
MmUnmapViewInSessionSpace (
    IN PVOID MappedBase
    )

/*++

Routine Description:

    This routine unmaps the specified section from the system's address space.

Arguments:

    MappedBase - Supplies the address of the view to unmap.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, IRQL of dispatch level.

--*/

{
    PMMSESSION Session;

    PAGED_CODE();

    if ((PsGetCurrentProcess()->Flags & PS_PROCESS_FLAGS_IN_SESSION) == 0) {
        return STATUS_NOT_MAPPED_VIEW;
    }

    ASSERT (MmIsAddressValid(MmSessionSpace) == TRUE);
    Session = &MmSessionSpace->Session;

    return MiUnmapViewInSystemSpace (Session, MappedBase);
}

NTSTATUS
MiUnmapViewInSystemSpace (
    IN PMMSESSION Session,
    IN PVOID MappedBase
    )

/*++

Routine Description:

    This routine unmaps the specified section from the system's address space.

Arguments:

    Session - Supplies the session data structure for this view.

    MappedBase - Supplies the address of the view to unmap.

Return Value:

    Status of the map view operation.

Environment:

    Kernel Mode, IRQL of dispatch level.

--*/

{
    ULONG StartBit;
    ULONG Size;
    PCONTROL_AREA ControlArea;
    PMMSUPPORT Ws;

    PAGED_CODE();

    if (Session == &MmSession) {
        Ws = &MmSystemCacheWs;
    }
    else {
        Ws = &MmSessionSpace->GlobalVirtualAddress->Vm;
    }

    StartBit =  (ULONG) (((ULONG_PTR)MappedBase - (ULONG_PTR)Session->SystemSpaceViewStart) >> 16);

    LOCK_SYSTEM_VIEW_SPACE (Session);

    Size = MiRemoveFromSystemSpace (Session, MappedBase, &ControlArea);

    RtlClearBits (Session->SystemSpaceBitMap, StartBit, Size);

    //
    // Zero PTEs.
    //

    Size = Size * (X64K >> PAGE_SHIFT);

    MiRemoveMappedPtes (MappedBase, Size, ControlArea, Ws);

    UNLOCK_SYSTEM_VIEW_SPACE (Session);

    return STATUS_SUCCESS;
}


PVOID
MiInsertInSystemSpace (
    IN PMMSESSION Session,
    IN ULONG SizeIn64k,
    IN PCONTROL_AREA ControlArea
    )

/*++

Routine Description:

    This routine creates a view in system space for the specified control
    area (file mapping).

Arguments:

    SizeIn64k - Supplies the size of the view to be created.

    ControlArea - Supplies a pointer to the control area for this view.

Return Value:

    Base address where the view was mapped, NULL if the view could not be
    mapped.

Environment:

    Kernel Mode.

--*/

{

    PVOID Base;
    ULONG_PTR Entry;
    ULONG Hash;
    ULONG i;
    ULONG AllocSize;
    PMMVIEW OldTable;
    ULONG StartBit;
    ULONG NewHashSize;
    POOL_TYPE PoolType;

    PAGED_CODE();

    ASSERT (SizeIn64k < X64K);

    //
    // CODE IS ALREADY LOCKED BY CALLER.
    //

    LOCK_SYSTEM_VIEW_SPACE (Session);

    if (Session->SystemSpaceHashEntries + 8 > Session->SystemSpaceHashSize) {

        //
        // Less than 8 free slots, reallocate and rehash.
        //

        NewHashSize = Session->SystemSpaceHashSize << 1;

        AllocSize = sizeof(MMVIEW) * NewHashSize;

        OldTable = Session->SystemSpaceViewTable;

        //
        // The SystemSpaceViewTable for system (not session) space is only
        // allocated from nonpaged pool so it can be safely torn down during
        // clean shutdowns.  Otherwise it could be allocated from paged pool
        // just like the session SystemSpaceViewTable.
        //

        if (Session == &MmSession) {
            PoolType = NonPagedPool;
        }
        else {
            PoolType = PagedPool;
        }

        Session->SystemSpaceViewTable = ExAllocatePoolWithTag (PoolType,
                                                               AllocSize,
                                                               '  mM');

        if (Session->SystemSpaceViewTable == NULL) {
            Session->SystemSpaceViewTable = OldTable;
        }
        else {
            RtlZeroMemory (Session->SystemSpaceViewTable, AllocSize);

            Session->SystemSpaceHashSize = NewHashSize;
            Session->SystemSpaceHashKey = Session->SystemSpaceHashSize - 1;

            for (i = 0; i < (Session->SystemSpaceHashSize / 2); i += 1) {
                if (OldTable[i].Entry != 0) {
                    Hash = (ULONG) ((OldTable[i].Entry >> 16) % Session->SystemSpaceHashKey);

                    while (Session->SystemSpaceViewTable[Hash].Entry != 0) {
                        Hash += 1;
                        if (Hash >= Session->SystemSpaceHashSize) {
                            Hash = 0;
                        }
                    }
                    Session->SystemSpaceViewTable[Hash] = OldTable[i];
                }
            }
            ExFreePool (OldTable);
        }
    }

    if (Session->SystemSpaceHashEntries == Session->SystemSpaceHashSize) {

        //
        // There are no free hash slots to place a new entry into even
        // though there may still be unused virtual address space.
        //

        UNLOCK_SYSTEM_VIEW_SPACE (Session);
        return NULL;
    }

    StartBit = RtlFindClearBitsAndSet (Session->SystemSpaceBitMap,
                                       SizeIn64k,
                                       0);

    if (StartBit == NO_BITS_FOUND) {
        Session->BitmapFailures += 1;
        UNLOCK_SYSTEM_VIEW_SPACE (Session);
        return NULL;
    }

    Base = (PVOID)((PCHAR)Session->SystemSpaceViewStart + ((ULONG_PTR)StartBit * X64K));

    Entry = (ULONG_PTR) MI_64K_ALIGN(Base) + SizeIn64k;

    Hash = (ULONG) ((Entry >> 16) % Session->SystemSpaceHashKey);

    while (Session->SystemSpaceViewTable[Hash].Entry != 0) {
        Hash += 1;
        if (Hash >= Session->SystemSpaceHashSize) {
            Hash = 0;
        }
    }

    Session->SystemSpaceHashEntries += 1;

    Session->SystemSpaceViewTable[Hash].Entry = Entry;
    Session->SystemSpaceViewTable[Hash].ControlArea = ControlArea;

    UNLOCK_SYSTEM_VIEW_SPACE (Session);
    return Base;
}


ULONG
MiRemoveFromSystemSpace (
    IN PMMSESSION Session,
    IN PVOID Base,
    OUT PCONTROL_AREA *ControlArea
    )

/*++

Routine Description:

    This routine looks up the specified view in the system space hash
    table and unmaps the view from system space and the table.

Arguments:

    Session - Supplies the session data structure for this view.

    Base - Supplies the base address for the view.  If this address is
           NOT found in the hash table, the system bugchecks.

    ControlArea - Returns the control area corresponding to the base
                  address.

Return Value:

    Size of the view divided by 64k.

Environment:

    Kernel Mode, system view hash table locked.

--*/

{
    ULONG_PTR Base16;
    ULONG Hash;
    ULONG Size;
    ULONG count;

    PAGED_CODE();

    count = 0;

    Base16 = (ULONG_PTR)Base >> 16;
    Hash = (ULONG)(Base16 % Session->SystemSpaceHashKey);

    while ((Session->SystemSpaceViewTable[Hash].Entry >> 16) != Base16) {
        Hash += 1;
        if (Hash >= Session->SystemSpaceHashSize) {
            Hash = 0;
            count += 1;
            if (count == 2) {
                KeBugCheckEx (DRIVER_UNMAPPING_INVALID_VIEW,
                              (ULONG_PTR)Base,
                              1,
                              0,
                              0);
            }
        }
    }

    Session->SystemSpaceHashEntries -= 1;
    Size = (ULONG) (Session->SystemSpaceViewTable[Hash].Entry & 0xFFFF);
    Session->SystemSpaceViewTable[Hash].Entry = 0;
    *ControlArea = Session->SystemSpaceViewTable[Hash].ControlArea;
    return Size;
}


LOGICAL
MiInitializeSystemSpaceMap (
    PVOID InputSession OPTIONAL
    )

/*++

Routine Description:

    This routine initializes the tables for mapping views into system space.
    Views are kept in a multiple of 64k bytes in a growable hashed table.

Arguments:

    InputSession - Supplies NULL if this is the initial system session
                   (non-Hydra), a valid session pointer (the pointer must
                   be in global space, not session space) for Hydra session
                   initialization.

Return Value:

    TRUE on success, FALSE on failure.

Environment:

    Kernel Mode, initialization.

--*/

{
    SIZE_T AllocSize;
    SIZE_T Size;
    PCHAR ViewStart;
    PMMSESSION Session;
    POOL_TYPE PoolType;

    if (ARGUMENT_PRESENT (InputSession)) {
        Session = (PMMSESSION)InputSession;
        ViewStart = (PCHAR) MiSessionViewStart;
        Size = MmSessionViewSize;
    }
    else {
        Session = &MmSession;
        ViewStart = (PCHAR)MiSystemViewStart;
        Size = MmSystemViewSize;
    }

    //
    // We are passed a system global address for the address of the session.
    // Save a global pointer to the mutex below because multiple sessions will
    // generally give us a session-space (not a global space) pointer to the
    // MMSESSION in subsequent calls.  We need the global pointer for the mutex
    // field for the kernel primitives to work properly.
    //

    Session->SystemSpaceViewLockPointer = &Session->SystemSpaceViewLock;
    KeInitializeGuardedMutex (Session->SystemSpaceViewLockPointer);

    //
    // If the kernel image has not been biased to allow for 3gb of user space,
    // then the system space view starts at the defined place. Otherwise, it
    // starts 16mb above the kernel image.
    //

    Session->SystemSpaceViewStart = ViewStart;

    MiCreateBitMap (&Session->SystemSpaceBitMap, Size / X64K, NonPagedPool);
    if (Session->SystemSpaceBitMap == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_NONPAGED_POOL);
        return FALSE;
    }

    RtlClearAllBits (Session->SystemSpaceBitMap);

    //
    // Build the view table.
    //

    Session->SystemSpaceHashSize = 31;
    Session->SystemSpaceHashKey = Session->SystemSpaceHashSize - 1;
    Session->SystemSpaceHashEntries = 0;

    AllocSize = sizeof(MMVIEW) * Session->SystemSpaceHashSize;
    ASSERT (AllocSize < PAGE_SIZE);

    //
    // The SystemSpaceViewTable for system (not session) space is only
    // allocated from nonpaged pool so it can be safely torn down during
    // clean shutdowns.  Otherwise it could be allocated from paged pool
    // just like the session SystemSpaceViewTable.
    //

    if (Session == &MmSession) {
        PoolType = NonPagedPool;
    }
    else {
        PoolType = PagedPool;
    }

    Session->SystemSpaceViewTable = ExAllocatePoolWithTag (PoolType,
                                                           AllocSize,
                                                           '  mM');

    if (Session->SystemSpaceViewTable == NULL) {
        MM_BUMP_SESSION_FAILURES (MM_SESSION_FAILURE_NO_SESSION_PAGED_POOL);
        MiRemoveBitMap (&Session->SystemSpaceBitMap);
        return FALSE;
    }

    RtlZeroMemory (Session->SystemSpaceViewTable, AllocSize);

    return TRUE;
}


VOID
MiFreeSessionSpaceMap (
    VOID
    )

/*++

Routine Description:

    This routine frees the tables used for mapping session views.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel Mode.  The caller must be in the correct session context.

--*/

{
    PMMSESSION Session;

    PAGED_CODE();

    Session = &MmSessionSpace->Session;

    //
    // Check for leaks of objects in the view table.
    //

    LOCK_SYSTEM_VIEW_SPACE (Session);

    if (Session->SystemSpaceViewTable && Session->SystemSpaceHashEntries) {

        KeBugCheckEx (SESSION_HAS_VALID_VIEWS_ON_EXIT,
                      (ULONG_PTR)MmSessionSpace->SessionId,
                      Session->SystemSpaceHashEntries,
                      (ULONG_PTR)&Session->SystemSpaceViewTable[0],
                      Session->SystemSpaceHashSize);
#if 0
        ULONG Index;

        for (Index = 0; Index < Session->SystemSpaceHashSize; Index += 1) {

            PMMVIEW Table;
            PVOID Base;

            Table = &Session->SystemSpaceViewTable[Index];

            if (Table->Entry) {

#if DBG
                DbgPrint ("MM: MiFreeSessionSpaceMap: view entry %d leak: ControlArea %p, Addr %p, Size %d\n",
                    Index,
                    Table->ControlArea,
                    Table->Entry & ~0xFFFF,
                    Table->Entry & 0x0000FFFF
                );
#endif

                Base = (PVOID)(Table->Entry & ~0xFFFF);

                //
                // MiUnmapViewInSystemSpace locks the ViewLock.
                //

                UNLOCK_SYSTEM_VIEW_SPACE(Session);

                MiUnmapViewInSystemSpace (Session, Base);

                LOCK_SYSTEM_VIEW_SPACE (Session);

                //
                // The view table may have been deleted while we let go of
                // the lock.
                //

                if (Session->SystemSpaceViewTable == NULL) {
                    break;
                }
            }
        }
#endif

    }

    UNLOCK_SYSTEM_VIEW_SPACE (Session);

    if (Session->SystemSpaceViewTable) {
        ExFreePool (Session->SystemSpaceViewTable);
        Session->SystemSpaceViewTable = NULL;
    }

    if (Session->SystemSpaceBitMap) {
        MiRemoveBitMap (&Session->SystemSpaceBitMap);
    }
}


HANDLE
MmSecureVirtualMemory (
    IN PVOID Address,
    IN SIZE_T Size,
    IN ULONG ProbeMode
    )

/*++

Routine Description:

    This routine probes the requested address range and protects
    the specified address range from having its protection made
    more restricted and being deleted.

    MmUnsecureVirtualMemory is used to allow the range to return
    to a normal state.

Arguments:

    Address - Supplies the base address to probe and secure.

    Size - Supplies the size of the range to secure.

    ProbeMode - Supplies one of PAGE_READONLY or PAGE_READWRITE.

Return Value:

    Returns a handle to be used to unsecure the range.
    If the range could not be locked because of protection
    problems or noncommitted memory, the value (HANDLE)0
    is returned.

Environment:

    Kernel Mode.

--*/

{
    return MiSecureVirtualMemory (Address, Size, ProbeMode, FALSE);
}


HANDLE
MiSecureVirtualMemory (
    IN PVOID Address,
    IN SIZE_T Size,
    IN ULONG ProbeMode,
    IN LOGICAL AddressSpaceMutexHeld
    )

/*++

Routine Description:

    This routine probes the requested address range and protects
    the specified address range from having its protection made
    more restricted and being deleted.

    MmUnsecureVirtualMemory is used to allow the range to return
    to a normal state.

Arguments:

    Address - Supplies the base address to probe and secure.

    Size - Supplies the size of the range to secure.

    ProbeMode - Supplies one of PAGE_READONLY or PAGE_READWRITE.

    AddressSpaceMutexHeld - Supplies TRUE if the mutex is already held, FALSE
                            if not.

Return Value:

    Returns a handle to be used to unsecure the range.
    If the range could not be locked because of protection
    problems or noncommitted memory, the value (HANDLE)0
    is returned.

Environment:

    Kernel Mode.

--*/

{
    PETHREAD Thread;
    ULONG_PTR EndAddress;
    PVOID StartAddress;
    CHAR Temp;
    ULONG Probe;
    HANDLE Handle;
    PMMVAD Vad;
    PMMVAD VadParent;
    PMMVAD_LONG NewVad;
    PMMSECURE_ENTRY Secure;
    PEPROCESS Process;
    PMMPTE PointerPxe;
    PMMPTE PointerPpe;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    ULONG Waited;
#if defined(_WIN64)
    ULONG_PTR PageSize;
#else
    #define PageSize PAGE_SIZE
#endif

    PAGED_CODE();

    if ((ULONG_PTR)Address + Size > (ULONG_PTR)MM_HIGHEST_USER_ADDRESS || (ULONG_PTR)Address + Size <= (ULONG_PTR)Address) {
        return NULL;
    }

    Handle = NULL;

    Probe = (ProbeMode == PAGE_READONLY);

    Thread = PsGetCurrentThread ();
    Process = PsGetCurrentProcessByThread (Thread);

    StartAddress = Address;

    if (AddressSpaceMutexHeld == FALSE) {
        LOCK_ADDRESS_SPACE (Process);
    }

    //
    // Check for a private committed VAD first instead of probing to avoid all
    // the page faults and zeroing.  If we find one, then we run the PTEs
    // instead.
    //

    if (Size >= 64 * 1024) {
        EndAddress = (ULONG_PTR)StartAddress + Size - 1;
        Vad = MiLocateAddress (StartAddress);

        if (Vad == NULL) {
            goto Return1;
        }

        if ((Vad->u.VadFlags.UserPhysicalPages == 1) ||
            (Vad->u.VadFlags.LargePages == 1)) {
            goto Return1;
        }

        if (Vad->u.VadFlags.MemCommit == 0) {
            goto LongWay;
        }

        if (Vad->u.VadFlags.PrivateMemory == 0) {
            goto LongWay;
        }

        if (Vad->u.VadFlags.PhysicalMapping == 1) {
            goto LongWay;
        }

        ASSERT (Vad->u.VadFlags.Protection);

        if ((MI_VA_TO_VPN (StartAddress) < Vad->StartingVpn) ||
            (MI_VA_TO_VPN (EndAddress) > Vad->EndingVpn)) {
            goto Return1;
        }

        if (Vad->u.VadFlags.Protection == MM_NOACCESS) {
            goto LongWay;
        }

        if (ProbeMode == PAGE_READONLY) {
            if (Vad->u.VadFlags.Protection > MM_EXECUTE_WRITECOPY) {
                goto LongWay;
            }
        }
        else {
            if (Vad->u.VadFlags.Protection != MM_READWRITE &&
                Vad->u.VadFlags.Protection != MM_EXECUTE_READWRITE) {
                    goto LongWay;
            }
        }

        //
        // Check individual page permissions.
        //

        PointerPde = MiGetPdeAddress (StartAddress);
        PointerPpe = MiGetPteAddress (PointerPde);
        PointerPxe = MiGetPdeAddress (PointerPde);
        PointerPte = MiGetPteAddress (StartAddress);
        LastPte = MiGetPteAddress ((PVOID)EndAddress);

        LOCK_WS_UNSAFE (Process);

        do {

            while (MiDoesPxeExistAndMakeValid (PointerPxe,
                                               Process,
                                               MM_NOIRQL,
                                               &Waited) == FALSE) {
                //
                // Extended page directory parent entry is empty, go
                // to the next one.
                //

                PointerPxe += 1;
                PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    UNLOCK_WS_UNSAFE (Process);
                    goto EditVad;
                }
            }

#if (_MI_PAGING_LEVELS >= 4)
            Waited = 0;
#endif

            while (MiDoesPpeExistAndMakeValid (PointerPpe,
                                               Process,
                                               MM_NOIRQL,
                                               &Waited) == FALSE) {
                //
                // Page directory parent entry is empty, go to the next one.
                //

                PointerPpe += 1;
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    UNLOCK_WS_UNSAFE (Process);
                    goto EditVad;
                }
#if (_MI_PAGING_LEVELS >= 4)
                if (MiIsPteOnPdeBoundary (PointerPpe)) {
                    PointerPxe = MiGetPteAddress(PointerPpe);
                    Waited = 1;
                    goto restart;
                }
#endif
            }

#if (_MI_PAGING_LEVELS < 4)
            Waited = 0;
#endif

            while (MiDoesPdeExistAndMakeValid (PointerPde,
                                               Process,
                                               MM_NOIRQL,
                                               &Waited) == FALSE) {
                //
                // This page directory entry is empty, go to the next one.
                //

                PointerPde += 1;
                PointerPpe = MiGetPteAddress (PointerPde);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    UNLOCK_WS_UNSAFE (Process);
                    goto EditVad;
                }
#if (_MI_PAGING_LEVELS >= 3)
                if (MiIsPteOnPdeBoundary (PointerPde)) {
                    PointerPxe = MiGetPteAddress(PointerPpe);
                    Waited = 1;
                    break;
                }
#endif
            }

#if (_MI_PAGING_LEVELS >= 4)
restart:
            PointerPxe = PointerPxe;        // satisfy the compiler
#endif

        } while (Waited != 0);

        while (PointerPte <= LastPte) {

            if (MiIsPteOnPdeBoundary (PointerPte)) {

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPteAddress (PointerPde);
                PointerPxe = MiGetPdeAddress (PointerPde);

                do {

                    while (MiDoesPxeExistAndMakeValid (PointerPxe,
                                                       Process,
                                                       MM_NOIRQL,
                                                       &Waited) == FALSE) {
                        //
                        // Page directory parent entry is empty, go to the next one.
                        //

                        PointerPxe += 1;
                        PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

                        if (PointerPte > LastPte) {
                            UNLOCK_WS_UNSAFE (Process);
                            goto EditVad;
                        }
                    }

#if (_MI_PAGING_LEVELS >= 4)
                    Waited = 0;
#endif

                    while (MiDoesPpeExistAndMakeValid (PointerPpe,
                                                       Process,
                                                       MM_NOIRQL,
                                                       &Waited) == FALSE) {
                        //
                        // Page directory parent entry is empty, go to the next one.
                        //

                        PointerPpe += 1;
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);

                        if (PointerPte > LastPte) {
                            UNLOCK_WS_UNSAFE (Process);
                            goto EditVad;
                        }
#if (_MI_PAGING_LEVELS >= 4)
                        if (MiIsPteOnPdeBoundary (PointerPpe)) {
                            PointerPxe = MiGetPteAddress (PointerPpe);
                            Waited = 1;
                            goto restart2;
                        }
#endif
                    }

#if (_MI_PAGING_LEVELS < 4)
                    Waited = 0;
#endif

                    while (MiDoesPdeExistAndMakeValid (PointerPde,
                                                       Process,
                                                       MM_NOIRQL,
                                                       &Waited) == FALSE) {
                        //
                        // This page directory entry is empty, go to the next one.
                        //

                        PointerPde += 1;
                        PointerPpe = MiGetPteAddress (PointerPde);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                        if (PointerPte > LastPte) {
                            UNLOCK_WS_UNSAFE (Process);
                            goto EditVad;
                        }
#if (_MI_PAGING_LEVELS >= 3)
                        if (MiIsPteOnPdeBoundary (PointerPde)) {
                            PointerPxe = MiGetPteAddress (PointerPpe);
                            Waited = 1;
                            break;
                        }
#endif
                    }

#if (_MI_PAGING_LEVELS >= 4)
restart2:
            PointerPxe = PointerPxe;        // satisfy the compiler
#endif
                } while (Waited != 0);
            }
            if (PointerPte->u.Long) {
                UNLOCK_WS_UNSAFE (Process);
                goto LongWay;
            }
            PointerPte += 1;
        }
        UNLOCK_WS_UNSAFE (Process);
    }
    else {
LongWay:

        //
        // Mark this thread as the address space mutex owner so it cannot
        // sneak its stack in as the argument region and trick us into
        // trying to grow it if the reference faults (as this would cause
        // a deadlock since this thread already owns the address space mutex).
        // Note this would have the side effect of not allowing this thread
        // to fault on guard pages in other data regions while the accesses
        // below are ongoing - but that could only happen in an APC and
        // those are blocked right now anyway.
        //

        ASSERT (KeAreAllApcsDisabled () == TRUE);
        ASSERT (Thread->AddressSpaceOwner == 0);
        Thread->AddressSpaceOwner = 1;

#if defined(_WIN64)
        if (Process->Wow64Process != NULL) {
            PageSize = PAGE_SIZE_X86NT;
        } else {
            PageSize = PAGE_SIZE;
        }
#endif

        try {

            if (ProbeMode == PAGE_READONLY) {

                EndAddress = (ULONG_PTR)Address + Size - 1;
                EndAddress = (EndAddress & ~(PageSize - 1)) + PageSize;

                do {
                    Temp = *(volatile CHAR *)Address;
                    Address = (PVOID)(((ULONG_PTR)Address & ~(PageSize - 1)) + PageSize);
                } while ((ULONG_PTR)Address != EndAddress);
            }
            else {
                ProbeForWrite (Address, Size, 1);
            }

        } except (EXCEPTION_EXECUTE_HANDLER) {
            ASSERT (KeAreAllApcsDisabled () == TRUE);
            ASSERT (Thread->AddressSpaceOwner == 1);
            Thread->AddressSpaceOwner = 0;
            goto Return1;
        }

        ASSERT (KeAreAllApcsDisabled () == TRUE);
        ASSERT (Thread->AddressSpaceOwner == 1);
        Thread->AddressSpaceOwner = 0;

        //
        // Locate VAD and add in secure descriptor.
        //

        EndAddress = (ULONG_PTR)StartAddress + Size - 1;
        Vad = MiLocateAddress (StartAddress);

        if (Vad == NULL) {
            goto Return1;
        }

        if ((Vad->u.VadFlags.UserPhysicalPages == 1) ||
            (Vad->u.VadFlags.LargePages == 1)) {
            goto Return1;
        }

        if ((MI_VA_TO_VPN (StartAddress) < Vad->StartingVpn) ||
            (MI_VA_TO_VPN (EndAddress) > Vad->EndingVpn)) {

            //
            // Not within the section virtual address descriptor,
            // return an error.
            //

            goto Return1;
        }
    }

EditVad:

    //
    // If this is a short or regular VAD, it needs to be reallocated as
    // a large VAD.  Note that a short VAD that was previously converted
    // to a long VAD here will still be marked as private memory, thus to
    // handle this case the NoChange bit must also be tested.
    //

    if (((Vad->u.VadFlags.PrivateMemory) && (Vad->u.VadFlags.NoChange == 0)) 
        ||
        (Vad->u2.VadFlags2.LongVad == 0)) {

        if (Vad->u.VadFlags.PrivateMemory == 0) {
            ASSERT (Vad->u2.VadFlags2.OneSecured == 0);
            ASSERT (Vad->u2.VadFlags2.MultipleSecured == 0);
        }

        NewVad = ExAllocatePoolWithTag (NonPagedPool,
                                        sizeof(MMVAD_LONG),
                                        'ldaV');
        if (NewVad == NULL) {
            goto Return1;
        }

        RtlZeroMemory (NewVad, sizeof(MMVAD_LONG));

        if (Vad->u.VadFlags.PrivateMemory) {
            RtlCopyMemory (NewVad, Vad, sizeof(MMVAD_SHORT));
        }
        else {
            RtlCopyMemory (NewVad, Vad, sizeof(MMVAD));
        }

        NewVad->u.VadFlags.NoChange = 1;
        NewVad->u2.VadFlags2.OneSecured = 1;

        NewVad->u2.VadFlags2.LongVad = 1;
        NewVad->u2.VadFlags2.ReadOnly = Probe;

        NewVad->u3.Secured.StartVpn = (ULONG_PTR)StartAddress;
        NewVad->u3.Secured.EndVpn = EndAddress;

        //
        // Replace the current VAD with this expanded VAD.
        //

        LOCK_WS_UNSAFE (Process);

        VadParent = (PMMVAD) SANITIZE_PARENT_NODE (Vad->u1.Parent);
        ASSERT (VadParent != NULL);

        if (VadParent != Vad) {
            if (VadParent->RightChild == Vad) {
                VadParent->RightChild = (PMMVAD) NewVad;
            }
            else {
                ASSERT (VadParent->LeftChild == Vad);
                VadParent->LeftChild = (PMMVAD) NewVad;
            }
        }
        else {
            Process->VadRoot.BalancedRoot.RightChild = (PMMADDRESS_NODE) NewVad;
        }
        if (Vad->LeftChild) {
            Vad->LeftChild->u1.Parent = (PMMVAD) MI_MAKE_PARENT (NewVad, Vad->LeftChild->u1.Balance);
        }
        if (Vad->RightChild) {
            Vad->RightChild->u1.Parent = (PMMVAD) MI_MAKE_PARENT (NewVad, Vad->RightChild->u1.Balance);
        }
        if (Process->VadRoot.NodeHint == Vad) {
            Process->VadRoot.NodeHint = (PMMVAD) NewVad;
        }
        if (Process->VadFreeHint == Vad) {
            Process->VadFreeHint = (PMMVAD) NewVad;
        }

        if ((Vad->u.VadFlags.PhysicalMapping == 1) ||
            (Vad->u.VadFlags.WriteWatch == 1)) {

            MiPhysicalViewAdjuster (Process, Vad, (PMMVAD) NewVad);
        }

        UNLOCK_WS_UNSAFE (Process);
        if (AddressSpaceMutexHeld == FALSE) {
            UNLOCK_ADDRESS_SPACE (Process);
        }

        ExFreePool (Vad);

        //
        // Or in the low bit to denote the secure entry is in the VAD.
        //

        Handle = (HANDLE)((ULONG_PTR)&NewVad->u2.LongFlags2 | 0x1);

        return Handle;
    }

    //
    // This is already a large VAD, add the secure entry.
    //

    ASSERT (Vad->u2.VadFlags2.LongVad == 1);

    if (Vad->u2.VadFlags2.OneSecured) {

        //
        // This VAD already is secured.  Move the info out of the
        // block into pool.
        //

        Secure = ExAllocatePoolWithTag (NonPagedPool,
                                        sizeof (MMSECURE_ENTRY),
                                        'eSmM');
        if (Secure == NULL) {
            goto Return1;
        }

        ASSERT (Vad->u.VadFlags.NoChange == 1);
        Vad->u2.VadFlags2.OneSecured = 0;
        Vad->u2.VadFlags2.MultipleSecured = 1;
        Secure->u2.LongFlags2 = (ULONG) Vad->u.LongFlags;
        Secure->StartVpn = ((PMMVAD_LONG) Vad)->u3.Secured.StartVpn;
        Secure->EndVpn = ((PMMVAD_LONG) Vad)->u3.Secured.EndVpn;

        InitializeListHead (&((PMMVAD_LONG)Vad)->u3.List);
        InsertTailList (&((PMMVAD_LONG)Vad)->u3.List, &Secure->List);
    }

    if (Vad->u2.VadFlags2.MultipleSecured) {

        //
        // This VAD already has a secured element in its list, allocate and
        // add in the new secured element.
        //

        Secure = ExAllocatePoolWithTag (NonPagedPool,
                                        sizeof (MMSECURE_ENTRY),
                                        'eSmM');
        if (Secure == NULL) {
            goto Return1;
        }

        Secure->u2.LongFlags2 = 0;
        Secure->u2.VadFlags2.ReadOnly = Probe;
        Secure->StartVpn = (ULONG_PTR)StartAddress;
        Secure->EndVpn = EndAddress;

        InsertTailList (&((PMMVAD_LONG)Vad)->u3.List, &Secure->List);
        Handle = (HANDLE)Secure;

    }
    else {

        //
        // This list does not have a secure element.  Put it in the VAD.
        // The VAD may be either a regular VAD or a long VAD (it cannot be
        // a short VAD) at this point.  If it is a regular VAD, it must be
        // reallocated as a long VAD before any operation can proceed so
        // the secured range can be inserted.
        //

        Vad->u.VadFlags.NoChange = 1;
        Vad->u2.VadFlags2.OneSecured = 1;
        Vad->u2.VadFlags2.ReadOnly = Probe;
        ((PMMVAD_LONG)Vad)->u3.Secured.StartVpn = (ULONG_PTR)StartAddress;
        ((PMMVAD_LONG)Vad)->u3.Secured.EndVpn = EndAddress;

        //
        // Or in the low bit to denote the secure entry is in the VAD.
        //

        Handle = (HANDLE)((ULONG_PTR)&Vad->u2.LongFlags2 | 0x1);
    }

Return1:
    if (AddressSpaceMutexHeld == FALSE) {
        UNLOCK_ADDRESS_SPACE (Process);
    }
    return Handle;
}


VOID
MmUnsecureVirtualMemory (
    IN HANDLE SecureHandle
    )

/*++

Routine Description:

    This routine unsecures memory previously secured via a call to
    MmSecureVirtualMemory.

Arguments:

    SecureHandle - Supplies the handle returned in MmSecureVirtualMemory.

Return Value:

    None.

Environment:

    Kernel Mode.

--*/

{
    MiUnsecureVirtualMemory (SecureHandle, FALSE);
}


VOID
MiUnsecureVirtualMemory (
    IN HANDLE SecureHandle,
    IN LOGICAL AddressSpaceMutexHeld
    )

/*++

Routine Description:

    This routine unsecures memory previously secured via a call to
    MmSecureVirtualMemory.

Arguments:

    SecureHandle - Supplies the handle returned in MmSecureVirtualMemory.

    AddressSpaceMutexHeld - Supplies TRUE if the mutex is already held, FALSE
                            if not.

Return Value:

    None.

Environment:

    Kernel Mode.

--*/

{
    PMMSECURE_ENTRY Secure;
    PEPROCESS Process;
    PMMVAD_LONG Vad;

    PAGED_CODE();

    Secure = (PMMSECURE_ENTRY)SecureHandle;
    Process = PsGetCurrentProcess ();

    if (AddressSpaceMutexHeld == FALSE) {
        LOCK_ADDRESS_SPACE (Process);
    }

    if ((ULONG_PTR)Secure & 0x1) {
        Secure = (PMMSECURE_ENTRY) ((ULONG_PTR)Secure & ~0x1);
        Vad = CONTAINING_RECORD (Secure,
                                 MMVAD_LONG,
                                 u2.LongFlags2);
    }
    else {
        Vad = (PMMVAD_LONG) MiLocateAddress ((PVOID)Secure->StartVpn);
    }

    ASSERT (Vad);
    ASSERT (Vad->u.VadFlags.NoChange == 1);
    ASSERT (Vad->u2.VadFlags2.LongVad == 1);

    if (Vad->u2.VadFlags2.OneSecured) {
        ASSERT (Secure == (PMMSECURE_ENTRY)&Vad->u2.LongFlags2);
        Vad->u2.VadFlags2.OneSecured = 0;
        ASSERT (Vad->u2.VadFlags2.MultipleSecured == 0);
        if (Vad->u2.VadFlags2.SecNoChange == 0) {

            //
            // No more secure entries in this list, remove the state.
            //

            Vad->u.VadFlags.NoChange = 0;
        }
        if (AddressSpaceMutexHeld == FALSE) {
            UNLOCK_ADDRESS_SPACE (Process);
        }
    }
    else {
        ASSERT (Vad->u2.VadFlags2.MultipleSecured == 1);

        if (Secure == (PMMSECURE_ENTRY)&Vad->u2.LongFlags2) {

            //
            // This was a single block that got converted into a list.
            // Reset the entry.
            //

            Secure = CONTAINING_RECORD (Vad->u3.List.Flink,
                                        MMSECURE_ENTRY,
                                        List);
        }
        RemoveEntryList (&Secure->List);
        if (IsListEmpty (&Vad->u3.List)) {

            //
            // No more secure entries, reset the state.
            //

            Vad->u2.VadFlags2.MultipleSecured = 0;

            if ((Vad->u2.VadFlags2.SecNoChange == 0) &&
               (Vad->u.VadFlags.PrivateMemory == 0)) {

                //
                // No more secure entries in this list, remove the state
                // if and only if this VAD is not private.  If this VAD
                // is private, removing the state NoChange flag indicates
                // that this is a short VAD which it no longer is.
                //

                Vad->u.VadFlags.NoChange = 0;
            }
        }
        if (AddressSpaceMutexHeld == FALSE) {
            UNLOCK_ADDRESS_SPACE (Process);
        }
        ExFreePool (Secure);
    }

    return;
}

#if DBG
VOID
MiDumpConflictingVad (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad
    )
{
    if (NtGlobalFlag & FLG_SHOW_LDR_SNAPS) {
        DbgPrint( "MM: [%p ... %p) conflicted with Vad %p\n",
                  StartingAddress, EndingAddress, Vad);
        if ((Vad->u.VadFlags.PrivateMemory == 1) ||
            (Vad->ControlArea == NULL)) {
            return;
        }
        if (Vad->ControlArea->u.Flags.Image)
            DbgPrint( "    conflict with %Z image at [%p .. %p)\n",
                      &Vad->ControlArea->FilePointer->FileName,
                      MI_VPN_TO_VA (Vad->StartingVpn),
                      MI_VPN_TO_VA_ENDING (Vad->EndingVpn)
                    );
        else
        if (Vad->ControlArea->u.Flags.File)
            DbgPrint( "    conflict with %Z file at [%p .. %p)\n",
                      &Vad->ControlArea->FilePointer->FileName,
                      MI_VPN_TO_VA (Vad->StartingVpn),
                      MI_VPN_TO_VA_ENDING (Vad->EndingVpn)
                    );
        else
            DbgPrint( "    conflict with section at [%p .. %p)\n",
                      MI_VPN_TO_VA (Vad->StartingVpn),
                      MI_VPN_TO_VA_ENDING (Vad->EndingVpn)
                    );
    }
}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\mminit.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    mminit.c

Abstract:

    This module contains the initialization for the memory management
    system.

Author:

    Lou Perazzoli (loup) 20-Mar-1989
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/

#include "mi.h"

PVOID ExPageLockHandle;

PMMPTE MmSharedUserDataPte;

extern PMMPTE MiSpecialPoolFirstPte;
extern ULONG MmPagedPoolCommit;
extern ULONG MmInPageSupportMinimum;
extern PFN_NUMBER MiExpansionPoolPagesInitialCharge;

extern PVOID BBTBuffer;
extern PFN_COUNT BBTPagesToReserve;

//
// Registry-settable.
//

ULONG MmAllocationPreference;

ULONG_PTR MmSubsectionBase;
ULONG_PTR MmSubsectionTopPage;
ULONG MmDataClusterSize;
ULONG MmCodeClusterSize;
PFN_NUMBER MmResidentAvailableAtInit;
PPHYSICAL_MEMORY_DESCRIPTOR MmPhysicalMemoryBlock;
LIST_ENTRY MmProtectedPteList;
KSPIN_LOCK MmProtectedPteLock;

KGUARDED_MUTEX MmPagedPoolMutex;

LOGICAL MmPagedPoolMaximumDesired = FALSE;

LOGICAL MiSafeBooted = FALSE;

PFN_NUMBER MmFreedExpansionPoolMaximum;

RTL_BITMAP MiPfnBitMap;

#if defined (_MI_DEBUG_SUB)
ULONG MiTrackSubs = 0x2000; // Set to nonzero to enable subsection tracking code.
LONG MiSubsectionIndex;
PMI_SUB_TRACES MiSubsectionTraces;
#endif

#if defined (_MI_DEBUG_DIRTY)
ULONG MiTrackDirtys = 0x10000; // Set to nonzero to enable dirty bit tracking code.
LONG MiDirtyIndex;
PMI_DIRTY_TRACES MiDirtyTraces;
#endif

#if defined (_MI_DEBUG_PTE)
LONG MiPteIndex;
MI_PTE_TRACES MiPteTraces[MI_PTE_TRACE_SIZE];
#endif

#if defined (_MI_DEBUG_DATA)
ULONG MiTrackData = 0x10000; // Set to nonzero to enable data tracking code.
LONG MiDataIndex;
PMI_DATA_TRACES MiDataTraces;
#endif

VOID
MiMapBBTMemory (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

VOID
MiEnablePagingTheExecutive (
    VOID
    );

VOID
MiEnablePagingOfDriverAtInit (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte
    );

VOID
MiBuildPagedPool (
    );

VOID
MiWriteProtectSystemImage (
    IN PVOID DllBase
    );

VOID
MiInitializePfnTracing (
    VOID
    );

PFN_NUMBER
MiPagesInLoaderBlock (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PBOOLEAN IncludeType
    );

static
VOID
MiMemoryLicense (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

VOID
MiInitializeCacheOverrides (
    VOID
    );

//
// The thresholds can be overridden by the registry.
//

PFN_NUMBER MmLowMemoryThreshold;
PFN_NUMBER MmHighMemoryThreshold;

//
// A temporary unsignaled event for the available pages & pool events to
// point at early during phase 0.  This is because the exportable events
// with DACLs cannot be created until phase 1.
//

KEVENT  MiTempEvent;

PKEVENT MiLowMemoryEvent;
PKEVENT MiHighMemoryEvent;

PKEVENT MiLowPagedPoolEvent;
PKEVENT MiHighPagedPoolEvent;

PKEVENT MiLowNonPagedPoolEvent;
PKEVENT MiHighNonPagedPoolEvent;

PFN_NUMBER MiLowPagedPoolThreshold;
PFN_NUMBER MiHighPagedPoolThreshold;

PFN_NUMBER MiLowNonPagedPoolThreshold;
PFN_NUMBER MiHighNonPagedPoolThreshold;

NTSTATUS
MiCreateMemoryEvent (
    IN PUNICODE_STRING EventName,
    OUT PKEVENT *Event
    );

LOGICAL
MiInitializeMemoryEvents (
    VOID
    );

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MmInitSystem)
#pragma alloc_text(INIT,MiMapBBTMemory)
#pragma alloc_text(INIT,MmInitializeMemoryLimits)
#pragma alloc_text(INIT,MmFreeLoaderBlock)
#pragma alloc_text(INIT,MiBuildPagedPool)
#pragma alloc_text(INIT,MiFindInitializationCode)
#pragma alloc_text(INIT,MiEnablePagingTheExecutive)
#pragma alloc_text(INIT,MiEnablePagingOfDriverAtInit)
#pragma alloc_text(INIT,MiPagesInLoaderBlock)
#pragma alloc_text(INIT,MiCreateMemoryEvent)
#pragma alloc_text(INIT,MiInitializeMemoryEvents)
#pragma alloc_text(INIT,MiInitializeCacheOverrides)
#pragma alloc_text(INIT,MiInitializeNonPagedPoolThresholds)
#pragma alloc_text(INIT,MiMemoryLicense)
#if defined(_X86_) || defined(_AMD64_)
#pragma alloc_text(INIT,MiAddHalIoMappings)
#endif
#pragma alloc_text(PAGELK,MiFreeInitializationCode)
#endif

//
// Default is a 300 second life span for modified mapped pages -
// This can be overridden in the registry.
//

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg("INITDATA")
#endif
ULONG MmModifiedPageLifeInSeconds = 300;
#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg()
#endif

LARGE_INTEGER MiModifiedPageLife;

BOOLEAN MiTimerPending = FALSE;

KEVENT MiMappedPagesTooOldEvent;

KDPC MiModifiedPageWriterTimerDpc;

KTIMER MiModifiedPageWriterTimer;

//
// The following constants are based on the number PAGES not the
// memory size.  For convenience the number of pages is calculated
// based on a 4k page size.  Hence 12mb with 4k page is 3072.
//

#define MM_SMALL_SYSTEM ((13*1024*1024) / 4096)

#define MM_MEDIUM_SYSTEM ((19*1024*1024) / 4096)

#define MM_MIN_INITIAL_PAGED_POOL ((32*1024*1024) >> PAGE_SHIFT)

#define MM_DEFAULT_IO_LOCK_LIMIT (2 * 1024 * 1024)

extern WSLE_NUMBER MmMaximumWorkingSetSize;

extern ULONG MmEnforceWriteProtection;

extern CHAR MiPteStr[];

extern LONG MiTrimInProgressCount;

#if (_MI_PAGING_LEVELS < 3)
PFN_NUMBER MmSystemPageDirectory[PD_PER_SYSTEM];
PMMPTE MmSystemPagePtes;
#endif

ULONG MmTotalSystemCodePages;

MM_SYSTEMSIZE MmSystemSize;

ULONG MmLargeSystemCache;

ULONG MmProductType;

extern ULONG MiVerifyAllDrivers;

LIST_ENTRY MmLoadedUserImageList;
PPAGE_FAULT_NOTIFY_ROUTINE MmPageFaultNotifyRoutine;

#if defined (_WIN64)
#define MM_ALLOCATION_FRAGMENT (64 * 1024 * 1024)
#define MM_ALLOCATION_FRAGMENT_MAX MM_ALLOCATION_FRAGMENT
#else
#define MM_ALLOCATION_FRAGMENT           (64 * 1024)
#define MM_ALLOCATION_FRAGMENT_MAX (2 * 1024 * 1024)
#endif

//
// Registry-settable.
//

SIZE_T MmAllocationFragment;

#define KERNEL_NAME L"ntoskrnl.exe"
#define HAL_NAME    L"hal.dll"


#if defined(MI_MULTINODE)

HALNUMAPAGETONODE
MiNonNumaPageToNodeColor (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    Return the node color of the page.

Arguments:

    PageFrameIndex - Supplies the physical page number.

Return Value:

    Node color is always zero in non-NUMA configurations.

--*/

{
    UNREFERENCED_PARAMETER (PageFrameIndex);

    return 0;
}

//
// This node determination function pointer is initialized to return 0.
//
// Architecture-dependent initialization may repoint it to a HAL routine
// for NUMA configurations.
//

PHALNUMAPAGETONODE MmPageToNode = MiNonNumaPageToNodeColor;


VOID
MiDetermineNode (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPFN Pfn
    )

/*++

Routine Description:

    This routine is called during initial freelist population or when
    physical memory is being hot-added.  It then determines which node
    (in a multinode NUMA system) the physical memory resides in, and
    marks the PFN entry accordingly.

    N.B.  The actual page to node determination is machine dependent
    and done by a routine in the chipset driver or the HAL, called
    via the MmPageToNode function pointer.

Arguments:

    PageFrameIndex - Supplies the physical page number.

    Pfn - Supplies a pointer to the PFN database element.

Return Value:

    None.

Environment:

    None although typically this routine is called with the PFN
    database locked.

--*/

{
    ULONG Temp;

    ASSERT (Pfn == MI_PFN_ELEMENT(PageFrameIndex));

    Temp = MmPageToNode (PageFrameIndex);

    ASSERT (Temp < MAXIMUM_CCNUMA_NODES);

    Pfn->u3.e1.PageColor = Temp;
}

#endif


BOOLEAN
MmInitSystem (
    IN ULONG Phase,
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This function is called during Phase 0, phase 1 and at the end
    of phase 1 ("phase 2") initialization.

    Phase 0 initializes the memory management paging functions,
    nonpaged and paged pool, the PFN database, etc.

    Phase 1 initializes the section objects, the physical memory
    object, and starts the memory management system threads.

    Phase 2 frees memory used by the OsLoader.

Arguments:

    Phase - System initialization phase.

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    Returns TRUE if the initialization was successful.

Environment:

    Kernel Mode Only.  System initialization.

--*/

{
    ULONG Color;
    PEPROCESS Process;
    PSLIST_ENTRY SingleListEntry;
    PFN_NUMBER NumberOfPages;
    HANDLE ThreadHandle;
    OBJECT_ATTRIBUTES ObjectAttributes;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE StartPde;
    PMMPTE EndPde;
    PMMPFN Pfn1;
    PFN_NUMBER i, j;
    PFN_NUMBER DeferredMdlEntries;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER DirectoryFrameIndex;
    MMPTE TempPte;
    KIRQL OldIrql;
    PLIST_ENTRY NextEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    ULONG_PTR MaximumSystemCacheSize;
    ULONG_PTR MaximumSystemCacheSizeTotal;
    PIMAGE_NT_HEADERS NtHeaders;
    ULONG_PTR SystemPteMultiplier;
    ULONG_PTR DefaultSystemViewSize;
    ULONG_PTR SessionEnd;
    SIZE_T SystemViewMax;
    SIZE_T HydraImageMax;
    SIZE_T HydraViewMax;
    SIZE_T HydraPoolMax;
    SIZE_T HydraSpaceUsedForSystemViews;
    BOOLEAN IncludeType[LoaderMaximum];
    LOGICAL AutosizingFragment;
    PULONG Bitmap;
    PPHYSICAL_MEMORY_RUN Run;
    ULONG VerifierFlags;
#if DBG
    MMPTE Pointer;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    LOGICAL FirstPpe;
    PMMPTE StartPpe;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    LOGICAL FirstPxe;
    PMMPTE StartPxe;
#endif
#if defined(_X86_)
    PCHAR ReducedUserVaOption;
    ULONG UserVaLimit;
    ULONG ReductionInBytes;
    ULONG ReductionInPages;
#endif

    j = 0;
    PointerPde = NULL;

    //
    // Make sure structure alignment is okay.
    //

    if (Phase == 0) {
        MmThrottleTop = 450;
        MmThrottleBottom = 127;

        //
        // Point various event thresholds at a temp unsignaled event
        // since real event creation is not available until phase 1.
        //

        KeInitializeEvent (&MiTempEvent, NotificationEvent, FALSE);

        MiLowMemoryEvent = &MiTempEvent;
        MiHighMemoryEvent = &MiTempEvent;

        MiLowPagedPoolEvent = &MiTempEvent;
        MiHighPagedPoolEvent = &MiTempEvent;

        MiLowNonPagedPoolEvent = &MiTempEvent;
        MiHighNonPagedPoolEvent = &MiTempEvent;

        //
        // Set the highest user address, the system range start address, the
        // user probe address, and the virtual bias.
        //

#if defined(_WIN64)

        MmHighestUserAddress = MI_HIGHEST_USER_ADDRESS;
        MmUserProbeAddress = MI_USER_PROBE_ADDRESS;
        MmSystemRangeStart = MI_SYSTEM_RANGE_START;

#else

        MmHighestUserAddress = (PVOID)(KSEG0_BASE - 0x10000 - 1);
        MmUserProbeAddress = KSEG0_BASE - 0x10000;
        MmSystemRangeStart = (PVOID)KSEG0_BASE;

#endif

        MiHighestUserPte = MiGetPteAddress (MmHighestUserAddress);
        MiHighestUserPde = MiGetPdeAddress (MmHighestUserAddress);

#if (_MI_PAGING_LEVELS >= 4)
        MiHighestUserPpe = MiGetPpeAddress (MmHighestUserAddress);
        MiHighestUserPxe = MiGetPxeAddress (MmHighestUserAddress);
#endif

#if defined(_X86_) || defined(_AMD64_)

        MmBootImageSize = LoaderBlock->Extension->LoaderPagesSpanned;
        MmBootImageSize *= PAGE_SIZE;

        MmBootImageSize = MI_ROUND_TO_SIZE (MmBootImageSize,
                                            MM_VA_MAPPED_BY_PDE);

        ASSERT ((MmBootImageSize % MM_VA_MAPPED_BY_PDE) == 0);
#endif

#if defined(_X86_)
        MmVirtualBias = LoaderBlock->u.I386.VirtualBias;
#endif

        //
        // Initialize system and Hydra mapped view sizes.
        //

        DefaultSystemViewSize = MM_SYSTEM_VIEW_SIZE;
        MmSessionSize = MI_SESSION_SPACE_DEFAULT_TOTAL_SIZE;
        SessionEnd = (ULONG_PTR) MM_SESSION_SPACE_DEFAULT_END;

#define MM_MB_MAPPED_BY_PDE (MM_VA_MAPPED_BY_PDE / (1024*1024))

        //
        // A PDE of virtual space is the minimum system view size allowed.
        //

        if (MmSystemViewSize < (MM_VA_MAPPED_BY_PDE / (1024*1024))) {
            MmSystemViewSize = DefaultSystemViewSize;
        }
        else {

            //
            // The view size has been specified (in megabytes) by the registry.
            // Validate it.
            //

            if (MmVirtualBias == 0) {

                //
                // Round the system view size (in megabytes) to a PDE multiple.
                //

                MmSystemViewSize = MI_ROUND_TO_SIZE (MmSystemViewSize,
                                                     MM_MB_MAPPED_BY_PDE);

                //
                // NT64 locates system views just after systemwide paged pool,
                // so the size of the system views are not limited by session
                // space.  Arbitrarily make the maximum a PPE's worth.
                //
                //
                // NT32 shares system view VA space with session VA space due
                // to the shortage of virtual addresses.  Thus increasing the
                // system view size means potentially decreasing the maximum
                // session space size.
                //

                SystemViewMax = (MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE) / (1024*1024);

#if !defined(_WIN64)

                //
                // Ensure at least enough space is left for
                // the standard default session layout.
                //

                SystemViewMax -= (MmSessionSize / (1024*1024));
#endif

                //
                // Note a view size of -1 will be rounded to zero.  Treat -1
                // as requesting the maximum.
                //

                if ((MmSystemViewSize > SystemViewMax) ||
                    (MmSystemViewSize == 0)) {

                    MmSystemViewSize = SystemViewMax;
                }

                MmSystemViewSize *= (1024*1024);
            }
            else {
                MmSystemViewSize = DefaultSystemViewSize;
            }
        }

#if defined(_WIN64)
        HydraSpaceUsedForSystemViews = 0;
#else
        HydraSpaceUsedForSystemViews = MmSystemViewSize;
#endif
        MiSessionImageEnd = SessionEnd;

        //
        // Select reasonable Hydra image, pool and view virtual sizes.
        // A PDE of virtual space is the minimum size allowed for each type.
        //

        if (MmVirtualBias == 0) {

            if (MmSessionImageSize < MM_MB_MAPPED_BY_PDE) {
                MmSessionImageSize = MI_SESSION_DEFAULT_IMAGE_SIZE;
            }
            else {

                //
                // The Hydra image size has been specified (in megabytes)
                // by the registry.
                //
                // Round it to a PDE multiple and validate it.
                //

                MmSessionImageSize = MI_ROUND_TO_SIZE (MmSessionImageSize,
                                                        MM_MB_MAPPED_BY_PDE);

                HydraImageMax = (MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE - HydraSpaceUsedForSystemViews - (MmSessionSize - MI_SESSION_DEFAULT_IMAGE_SIZE)) / (1024*1024);

                //
                // Note a view size of -1 will be rounded to zero.
                // Treat -1 as requesting the maximum.
                //

                if ((MmSessionImageSize > HydraImageMax) ||
                    (MmSessionImageSize == 0)) {
                    MmSessionImageSize = HydraImageMax;
                }

                MmSessionImageSize *= (1024*1024);
                MmSessionSize -= MI_SESSION_DEFAULT_IMAGE_SIZE;
                MmSessionSize += MmSessionImageSize;
            }

            MiSessionImageStart = SessionEnd - MmSessionImageSize;

            //
            // The session image start and size has been established.
            //
            // Now initialize the session pool and view ranges which lie
            // virtually below it.
            //

            if (MmSessionViewSize < MM_MB_MAPPED_BY_PDE) {
                MmSessionViewSize = MI_SESSION_DEFAULT_VIEW_SIZE;
            }
            else {

                //
                // The Hydra view size has been specified (in megabytes)
                // by the registry.  Validate it.
                //
                // Round the Hydra view size to a PDE multiple.
                //

                MmSessionViewSize = MI_ROUND_TO_SIZE (MmSessionViewSize,
                                                      MM_MB_MAPPED_BY_PDE);

                HydraViewMax = (MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE - HydraSpaceUsedForSystemViews - (MmSessionSize - MI_SESSION_DEFAULT_VIEW_SIZE)) / (1024*1024);

                //
                // Note a view size of -1 will be rounded to zero.
                // Treat -1 as requesting the maximum.
                //

                if ((MmSessionViewSize > HydraViewMax) ||
                    (MmSessionViewSize == 0)) {
                    MmSessionViewSize = HydraViewMax;
                }

                MmSessionViewSize *= (1024*1024);
                MmSessionSize -= MI_SESSION_DEFAULT_VIEW_SIZE;
                MmSessionSize += MmSessionViewSize;
            }

            MiSessionViewStart = SessionEnd - MmSessionImageSize - MI_SESSION_SPACE_WS_SIZE - MI_SESSION_SPACE_STRUCT_SIZE - MmSessionViewSize;

            //
            // The session view start and size has been established.
            //
            // Now initialize the session pool start and size which lies
            // virtually just below it.
            //

            MiSessionPoolEnd = MiSessionViewStart;

            if (MmSessionPoolSize < MM_MB_MAPPED_BY_PDE) {

#if !defined(_WIN64)

                //
                // Professional and below use systemwide paged pool for session
                // allocations (this decision is made in win32k.sys).  Server
                // and above use real session pool and 16mb isn't enough to
                // play high end game applications, etc.  Since we're not
                // booted /3GB, try for an additional 16mb now.
                //

                if ((MmSessionPoolSize == 0) && (MmProductType != 0x00690057)) {

                    HydraPoolMax = MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE - HydraSpaceUsedForSystemViews - MmSessionSize;
                    if (HydraPoolMax >= 2 * MI_SESSION_DEFAULT_POOL_SIZE) {
                        MmSessionPoolSize = 2 * MI_SESSION_DEFAULT_POOL_SIZE;
                        MmSessionSize -= MI_SESSION_DEFAULT_POOL_SIZE;
                        MmSessionSize += MmSessionPoolSize;
                    }
                    else {
                        MmSessionPoolSize = MI_SESSION_DEFAULT_POOL_SIZE;
                    }
                }
                else
#endif
                MmSessionPoolSize = MI_SESSION_DEFAULT_POOL_SIZE;
            }
            else {

                //
                // The Hydra pool size has been specified (in megabytes)
                // by the registry.  Validate it.
                //
                // Round the Hydra pool size to a PDE multiple.
                //

                MmSessionPoolSize = MI_ROUND_TO_SIZE (MmSessionPoolSize,
                                                      MM_MB_MAPPED_BY_PDE);

                HydraPoolMax = (MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE - HydraSpaceUsedForSystemViews - (MmSessionSize - MI_SESSION_DEFAULT_POOL_SIZE)) / (1024*1024);

                //
                // Note a view size of -1 will be rounded to zero.
                // Treat -1 as requesting the maximum.
                //

                if ((MmSessionPoolSize > HydraPoolMax) ||
                    (MmSessionPoolSize == 0)) {
                    MmSessionPoolSize = HydraPoolMax;
                }

                MmSessionPoolSize *= (1024*1024);
                MmSessionSize -= MI_SESSION_DEFAULT_POOL_SIZE;
                MmSessionSize += MmSessionPoolSize;
            }

            MiSessionPoolStart = MiSessionPoolEnd - MmSessionPoolSize;

            MmSessionBase = (ULONG_PTR) MiSessionPoolStart;

#if defined (_WIN64)

            //
            // Session special pool immediately follows session regular pool
            // assuming the user has enabled either the verifier or special
            // pool.
            //

            if ((MmVerifyDriverBufferLength != (ULONG)-1) ||
                ((MmSpecialPoolTag != 0) && (MmSpecialPoolTag != (ULONG)-1))) {

                MmSessionSize = MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE;
                MmSessionSpecialPoolEnd = (PVOID) MiSessionPoolStart;
                MmSessionBase = MM_SESSION_SPACE_DEFAULT;
                MmSessionSpecialPoolStart = (PVOID) MmSessionBase;
            }
#endif

            ASSERT (MmSessionBase + MmSessionSize == SessionEnd);
            MiSessionSpaceEnd = SessionEnd;
            MiSessionSpacePageTables = (ULONG)(MmSessionSize / MM_VA_MAPPED_BY_PDE);
#if !defined (_WIN64)
            MiSystemViewStart = MmSessionBase - MmSystemViewSize;
#endif

        }
        else {

            //
            // When booted /3GB, no size overrides are allowed due to the
            // already severely limited virtual address space.
            // Initialize the other Hydra variables after the system cache.
            //

            MmSessionViewSize = MI_SESSION_DEFAULT_VIEW_SIZE;
            MmSessionPoolSize = MI_SESSION_DEFAULT_POOL_SIZE;
            MmSessionImageSize = MI_SESSION_DEFAULT_IMAGE_SIZE;

            MiSessionImageStart = MiSessionImageEnd - MmSessionImageSize;

#if defined(_X86PAE_)
            MmSystemCacheStart = (PVOID) ((ULONG_PTR) MmSystemCacheStart +
                                        MM_SYSTEM_CACHE_WORKING_SET_3GB_DELTA);
#endif
        }

        //
        // Set the highest section base address.
        //
        // N.B. In 32-bit systems this address must be 2gb or less even for
        //      systems that run with 3gb enabled. Otherwise, it would not
        //      be possible to map based sections identically in all processes.
        //

        MmHighSectionBase = ((PCHAR)MmHighestUserAddress - 0x800000);

        MaximumSystemCacheSize = (MM_SYSTEM_CACHE_END - (ULONG_PTR) MmSystemCacheStart) >> PAGE_SHIFT;

#if defined(_X86_)

        if (MmSizeOfPagedPoolInBytes == (SIZE_T)-1) {

            //
            // The registry indicates that paged pool should be enlarged to
            // the maximum possible.  Steal from the system cache if possible
            // to accomplish this.
            //

            if (MmVirtualBias == 0) {
                ReductionInPages = MaximumSystemCacheSize / 3;
                ReductionInPages = MI_ROUND_TO_SIZE (ReductionInPages, PTE_PER_PAGE);
                MaximumSystemCacheSize -= ReductionInPages;
                MmPagedPoolStart = (PVOID) ((PCHAR)MmPagedPoolStart - (ReductionInPages << PAGE_SHIFT));
            }
        }

        //
        // If boot.ini specified a sane number of MB that the administrator
        // wants to use for user virtual address space then use it.
        //

        UserVaLimit = 0;
        ReducedUserVaOption = strstr(LoaderBlock->LoadOptions, "USERVA");

        if (ReducedUserVaOption != NULL) {

            ReducedUserVaOption = strstr(ReducedUserVaOption,"=");

            if (ReducedUserVaOption != NULL) {

                UserVaLimit = atol(ReducedUserVaOption+1);

                UserVaLimit = MI_ROUND_TO_SIZE (UserVaLimit, ((MM_VA_MAPPED_BY_PDE) / (1024*1024)));
            }
        }

        if (MmVirtualBias != 0) {

            //
            // If the size of the boot image (likely due to a large registry)
            // overflows into where paged pool would normally start, then
            // move paged pool up now.  This costs virtual address space (ie:
            // performance) but more importantly, allows the system to boot.
            //

            if (MmBootImageSize > 16 * 1024 * 1024) {
                MmPagedPoolStart = (PVOID)((PCHAR)MmPagedPoolStart + (MmBootImageSize - 16 * 1024 * 1024));
                ASSERT (((ULONG_PTR)MmPagedPoolStart % MM_VA_MAPPED_BY_PDE) == 0);
            }

            //
            // The system has been biased to an alternate base address to
            // allow 3gb of user address space, set the user probe address
            // and the maximum system cache size.
            //

            if ((UserVaLimit > 2048) && (UserVaLimit < 3072)) {

                //
                // Use any space between the maximum user virtual address
                // and the system for extra system PTEs.
                //
                // Convert input MB to bytes.
                //

                UserVaLimit -= 2048;
                UserVaLimit *= (1024*1024);

                //
                // Don't let the user specify a value which would cause us to
                // prematurely overwrite portions of the kernel & loader block.
                //

                if (UserVaLimit < MmBootImageSize) {
                    UserVaLimit = MmBootImageSize;
                }
            }
            else {
                UserVaLimit = 0x40000000;
            }

            MmHighestUserAddress = ((PCHAR)MmHighestUserAddress + UserVaLimit);
            MmSystemRangeStart = ((PCHAR)MmSystemRangeStart + UserVaLimit);
            MmUserProbeAddress += UserVaLimit;
            MiMaximumWorkingSet += UserVaLimit >> PAGE_SHIFT;

            if (UserVaLimit != 0x40000000) {
                MiUseMaximumSystemSpace = (ULONG_PTR)MmSystemRangeStart;
                MiUseMaximumSystemSpaceEnd = 0xC0000000;
            }

            MiHighestUserPte = MiGetPteAddress (MmHighestUserAddress);
            MiHighestUserPde = MiGetPdeAddress (MmHighestUserAddress);

            //
            // Moving to 3GB means moving session space to just above
            // the system cache (and lowering the system cache max size
            // accordingly).  Here's the visual:
            //
            //                 +------------------------------------+
            //        C1000000 | System cache resides here          |
            //                 | and grows upward.                  |
            //                 |               |                    |
            //                 |               |                    |
            //                 |              \/                    |
            //                 |                                    |
            //                 +------------------------------------+
            //                 | Session space (Hydra).             |
            //                 +------------------------------------+
            //                 | Systemwide global mapped views.    |
            //                 +------------------------------------+
            //                 |                                    |
            //                 |               ^                    |
            //                 |               |                    |
            //                 |               |                    |
            //                 |                                    |
            //                 | Kernel, HAL & boot loaded images   |
            //                 | grow downward from E1000000.       |
            //                 | Total size is specified by         |
            //                 | LoaderBlock->u.I386.BootImageSize. |
            //                 | Note only ntldrs after Build 2195  |
            //                 | are capable of loading the boot    |
            //                 | images in descending order from    |
            //                 | a hardcoded E1000000 on down.      |
            //        E1000000 +------------------------------------+
            //

            MaximumSystemCacheSize -= MmBootImageSize >> PAGE_SHIFT;

            MaximumSystemCacheSize -= MmSessionSize >> PAGE_SHIFT;

            MaximumSystemCacheSize -= MmSystemViewSize >> PAGE_SHIFT;

            MmSessionBase = (ULONG_PTR)((ULONG_PTR)MmSystemCacheStart +
                                  (MaximumSystemCacheSize << PAGE_SHIFT));

            MiSystemViewStart = MmSessionBase + MmSessionSize;

            MiSessionPoolStart = MmSessionBase;
            MiSessionPoolEnd = MiSessionPoolStart + MmSessionPoolSize;
            MiSessionViewStart = MiSessionPoolEnd;

            MiSessionSpaceEnd = (ULONG_PTR)MmSessionBase + MmSessionSize;
            MiSessionSpacePageTables = MmSessionSize / MM_VA_MAPPED_BY_PDE;

            MiSessionImageEnd = MiSessionSpaceEnd;
            MiSessionImageStart = MiSessionImageEnd - MmSessionImageSize;
        }
        else if ((UserVaLimit >= 64) && (UserVaLimit < 2048)) {

            //
            // Convert input MB to bytes.
            //
            // Note there is no protection against users that try to use
            // the USERVA option to reduce user space below 2GB.
            //

            UserVaLimit *= (1024*1024);
            ReductionInBytes = 0x80000000 - UserVaLimit;

            MmHighestUserAddress = ((PCHAR)MmHighestUserAddress - ReductionInBytes);
            MmSystemRangeStart = ((PCHAR)MmSystemRangeStart - ReductionInBytes);
            MmUserProbeAddress -= ReductionInBytes;
            MiMaximumWorkingSet -= ReductionInBytes >> PAGE_SHIFT;

            MiUseMaximumSystemSpace = (ULONG_PTR)MmSystemRangeStart;
            MiUseMaximumSystemSpaceEnd = (ULONG_PTR)MiUseMaximumSystemSpace + ReductionInBytes;

            MmHighSectionBase = (PVOID)((PCHAR)MmHighSectionBase - ReductionInBytes);

            MiHighestUserPte = MiGetPteAddress (MmHighestUserAddress);
            MiHighestUserPde = MiGetPdeAddress (MmHighestUserAddress);
        }

#else

#if !defined (_WIN64)
        MaximumSystemCacheSize -= (MmSystemViewSize >> PAGE_SHIFT);
#endif

#endif

        //
        // Initialize some global session variables.
        //

        MmSessionSpace = (PMM_SESSION_SPACE)((ULONG_PTR)MmSessionBase + MmSessionSize - MmSessionImageSize - MI_SESSION_SPACE_STRUCT_SIZE);

        MiSessionImagePteStart = MiGetPteAddress ((PVOID) MiSessionImageStart);
        MiSessionImagePteEnd = MiGetPteAddress ((PVOID) MiSessionImageEnd);

        MiSessionBasePte = MiGetPteAddress ((PVOID)MmSessionBase);

        MiSessionSpaceWs = MiSessionViewStart + MmSessionViewSize;

        MiSessionLastPte = MiGetPteAddress ((PVOID)MiSessionSpaceEnd);

#if DBG
        //
        // A few sanity checks to ensure things are as they should be.
        //

        if ((sizeof(CONTROL_AREA) % 8) != 0) {
            DbgPrint("control area list is not a quadword sized structure\n");
        }

        if ((sizeof(SUBSECTION) % 8) != 0) {
            DbgPrint("subsection list is not a quadword sized structure\n");
        }

        //
        // Some checks to make sure prototype PTEs can be placed in
        // either paged or nonpaged (prototype PTEs for paged pool are here)
        // can be put into PTE format.
        //

        PointerPte = (PMMPTE)MmPagedPoolStart;
        Pointer.u.Long = MiProtoAddressForPte (PointerPte);
        TempPte = Pointer;
        PointerPde = MiPteToProto(&TempPte);
        if (PointerPte != PointerPde) {
            DbgPrint("unable to map start of paged pool as prototype PTE %p %p\n",
                     PointerPde,
                     PointerPte);
        }

        PointerPte =
                (PMMPTE)((ULONG_PTR)MM_NONPAGED_POOL_END & ~((1 << PTE_SHIFT) - 1));

        Pointer.u.Long = MiProtoAddressForPte (PointerPte);
        TempPte = Pointer;
        PointerPde = MiPteToProto(&TempPte);
        if (PointerPte != PointerPde) {
            DbgPrint("unable to map end of nonpaged pool as prototype PTE %p %p\n",
                     PointerPde,
                     PointerPte);
        }

        PointerPte = (PMMPTE)(((ULONG_PTR)NON_PAGED_SYSTEM_END -
                        0x37000 + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1));

        for (j = 0; j < 20; j += 1) {
            Pointer.u.Long = MiProtoAddressForPte (PointerPte);
            TempPte = Pointer;
            PointerPde = MiPteToProto(&TempPte);
            if (PointerPte != PointerPde) {
                DbgPrint("unable to map end of nonpaged pool as prototype PTE %p %p\n",
                         PointerPde,
                         PointerPte);
            }

            PointerPte += 1;
        }

        PointerPte = (PMMPTE)(((ULONG_PTR)MM_NONPAGED_POOL_END - 0x133448) & ~(ULONG_PTR)7);
        Pointer.u.Long = MiGetSubsectionAddressForPte (PointerPte);
        TempPte = Pointer;
        PointerPde = (PMMPTE)MiGetSubsectionAddress(&TempPte);
        if (PointerPte != PointerPde) {
            DbgPrint("unable to map end of nonpaged pool as section PTE %p %p\n",
                     PointerPde,
                     PointerPte);

            MiFormatPte(&TempPte);
        }

        //
        // End of sanity checks.
        //

#endif

        if (MmEnforceWriteProtection) {
            MiPteStr[0] = (CHAR)1;
        }

        InitializeListHead (&MmLoadedUserImageList);
        InitializeListHead (&MmProtectedPteList);
        InitializeListHead (&MiHotPatchList);

        KeInitializeSpinLock (&MmProtectedPteLock);

#if defined (_MI_INSTRUMENT_WS)
        KeInitializeSpinLock (&MiInstrumentationLock);
#endif


        MmCriticalSectionTimeout.QuadPart = Int32x32To64(
                                                 MmCritsectTimeoutSeconds,
                                                -10000000);

        //
        // Initialize System Address Space creation mutex.
        //

        KeInitializeGuardedMutex (&MmSectionCommitMutex);
        KeInitializeGuardedMutex (&MmSectionBasedMutex);
        KeInitializeGuardedMutex (&MmDynamicMemoryMutex);
        KeInitializeGuardedMutex (&MmPagedPoolMutex);

        KeInitializeMutant (&MmSystemLoadLock, FALSE);

        KeInitializeEvent (&MmAvailablePagesEvent, NotificationEvent, TRUE);
        KeInitializeEvent (&MmAvailablePagesEventMedium, NotificationEvent, TRUE);
        KeInitializeEvent (&MmAvailablePagesEventHigh, NotificationEvent, TRUE);
        KeInitializeEvent (&MmMappedFileIoComplete, NotificationEvent, FALSE);

        KeInitializeEvent (&MmZeroingPageEvent, SynchronizationEvent, FALSE);
        KeInitializeEvent (&MmCollidedFlushEvent, NotificationEvent, FALSE);
        KeInitializeEvent (&MmCollidedLockEvent, NotificationEvent, FALSE);
        KeInitializeEvent (&MiMappedPagesTooOldEvent, NotificationEvent, FALSE);

        KeInitializeDpc (&MiModifiedPageWriterTimerDpc,
                         MiModifiedPageWriterTimerDispatch,
                         NULL);

        KeInitializeTimerEx (&MiModifiedPageWriterTimer, SynchronizationTimer);

        MiModifiedPageLife.QuadPart = Int32x32To64(
                                                 MmModifiedPageLifeInSeconds,
                                                -10000000);

        InitializeListHead (&MmWorkingSetExpansionHead.ListHead);

        InitializeSListHead (&MmDeadStackSListHead);

        InitializeSListHead (&MmEventCountSListHead);

        InitializeSListHead (&MmInPageSupportSListHead);

        MmZeroingPageThreadActive = FALSE;

        MiMemoryLicense (LoaderBlock);

        //
        // include all memory types ...
        //

        for (i = 0; i < LoaderMaximum; i += 1) {
            IncludeType[i] = TRUE;
        }

        //
        // ... except these...
        // If you change this list, please make
        // the corresponding changes over in
        // boot\lib\blmemory.c!BlDetermineOSVisibleMemory()
        //

        IncludeType[LoaderBad] = FALSE;
        IncludeType[LoaderFirmwarePermanent] = FALSE;
        IncludeType[LoaderSpecialMemory] = FALSE;
        IncludeType[LoaderBBTMemory] = FALSE;

        //
        // Compute number of pages in the system.
        //

        NumberOfPages = MiPagesInLoaderBlock (LoaderBlock, IncludeType);

#if defined (_MI_MORE_THAN_4GB_)
        Mm64BitPhysicalAddress = TRUE;
#endif

        //
        // When safebooting, don't enable special pool, the verifier or any
        // other options that track corruption regardless of registry settings.
        //

        if (strstr(LoaderBlock->LoadOptions, SAFEBOOT_LOAD_OPTION_A)) {
            MmLargePageDriverBufferLength = (ULONG)-1;
            MmVerifyDriverBufferLength = (ULONG)-1;
            MiVerifyAllDrivers = 0;
            MmVerifyDriverLevel = 0;
            MmSpecialPoolTag = (ULONG)-1;
            MmSnapUnloads = FALSE;
            MmProtectFreedNonPagedPool = FALSE;
            MmEnforceWriteProtection = 0;
            MmTrackLockedPages = FALSE;
            MiSafeBooted = TRUE;
            MmTrackPtes = 0;

#if defined (_WIN64)
            MmSessionSpecialPoolEnd = NULL;
            MmSessionSpecialPoolStart = NULL;
#endif

            SharedUserData->SafeBootMode = TRUE;
        }
        else {
            MiTriageSystem (LoaderBlock);
        }

        SystemPteMultiplier = 0;

        if (MmNumberOfSystemPtes == 0) {
#if defined (_WIN64)

            //
            // 64-bit NT is not constrained by virtual address space.  No
            // tradeoffs between nonpaged pool, paged pool and system PTEs
            // need to be made.  So just allocate PTEs on a linear scale as
            // a function of the amount of RAM.
            //
            // For example on a Hydra NT64, 4gb of RAM gets 128gb of PTEs.
            // The page table cost is the inversion of the multiplier based
            // on the PTE_PER_PAGE.
            //

            if (ExpMultiUserTS == TRUE) {
                SystemPteMultiplier = 32;
            }
            else {
                SystemPteMultiplier = 16;
            }
            if (NumberOfPages < 0x8000) {
                SystemPteMultiplier >>= 1;
            }
#else
            if (NumberOfPages < MM_MEDIUM_SYSTEM) {
                MmNumberOfSystemPtes = MM_MINIMUM_SYSTEM_PTES;
            }
            else {
                MmNumberOfSystemPtes = MM_DEFAULT_SYSTEM_PTES;
                if (NumberOfPages > 8192) {
                    MmNumberOfSystemPtes += MmNumberOfSystemPtes;

                    //
                    // Any reasonable Hydra machine gets the maximum.
                    //

                    if (ExpMultiUserTS == TRUE) {
                        MmNumberOfSystemPtes = MM_MAXIMUM_SYSTEM_PTES;
                    }
                }
            }
#endif
        }
        else if (MmNumberOfSystemPtes == (ULONG)-1) {

            //
            // This registry setting indicates the maximum number of
            // system PTEs possible for this machine must be allocated.
            // Snap this for later reference.
            //

            MiRequestedSystemPtes = (ULONG) MmNumberOfSystemPtes;

#if defined (_WIN64)
            SystemPteMultiplier = 256;
#else
            MmNumberOfSystemPtes = MM_MAXIMUM_SYSTEM_PTES;
#endif
        }

        if (SystemPteMultiplier != 0) {
            if (NumberOfPages * SystemPteMultiplier > MM_MAXIMUM_SYSTEM_PTES) {
                MmNumberOfSystemPtes = MM_MAXIMUM_SYSTEM_PTES;
            }
            else {
                MmNumberOfSystemPtes = (ULONG)(NumberOfPages * SystemPteMultiplier);
            }
        }

        if (MmNumberOfSystemPtes > MM_MAXIMUM_SYSTEM_PTES)  {
            MmNumberOfSystemPtes = MM_MAXIMUM_SYSTEM_PTES;
        }

        if (MmNumberOfSystemPtes < MM_MINIMUM_SYSTEM_PTES) {
            MmNumberOfSystemPtes = MM_MINIMUM_SYSTEM_PTES;
        }

        if (MmHeapSegmentReserve == 0) {
            MmHeapSegmentReserve = 1024 * 1024;
        }

        if (MmHeapSegmentCommit == 0) {
            MmHeapSegmentCommit = PAGE_SIZE * 2;
        }

        if (MmHeapDeCommitTotalFreeThreshold == 0) {
            MmHeapDeCommitTotalFreeThreshold = 64 * 1024;
        }

        if (MmHeapDeCommitFreeBlockThreshold == 0) {
            MmHeapDeCommitFreeBlockThreshold = PAGE_SIZE;
        }

        //
        // If the registry indicates drivers are in the suspect list,
        // extra system PTEs need to be allocated to support special pool
        // for their allocations.
        //

        if ((MmVerifyDriverBufferLength != (ULONG)-1) ||
            ((MmSpecialPoolTag != 0) && (MmSpecialPoolTag != (ULONG)-1))) {
            MmNumberOfSystemPtes += MM_SPECIAL_POOL_PTES;
        }

        MmNumberOfSystemPtes += BBTPagesToReserve;

#if defined(_X86_)

        //
        // The allocation preference key must be carefully managed.  This is
        // because doing every allocation top-down can cause failures if
        // an ntdll process startup allocation (like the stack trace database)
        // gets a high address which then causes a subsequent system DLL rebase
        // collision.
        //
        // This is circumvented as follows:
        //
        // 1.  For 32-bit machines, the allocation preference key is only
        //     useful when booted /3GB as only then can this key help track
        //     down apps with high virtual address bit sign extension problems.
        //     In 3GB mode, the system DLLs are based just below 2GB so ntdll
        //     would have to allocate more than 1GB of VA space before this
        //     becomes a problem.  So really the problem can only occur for
        //     machines in 2GB mode and since the key doesn't help these
        //     machines anyway, just turn it off in these cases.
        //
        // 2.  For 64-bit machines, there is plenty of VA space above the
        //     addresses system DLLs are based at so it is a non-issue.
        //     EXCEPT for wow64 binaries which run in sandboxed 2GB address
        //     spaces.  Explicit checks are made to detect a wow64 process in
        //     the Mm APIs which check this key and the key is ignored in
        //     this case as it doesn't provide any sign extension help and
        //     therefore we don't allow it to burn up any valuable VA space
        //     which could cause a collision.
        //

        if (MmVirtualBias == 0) {
            MmAllocationPreference = 0;
        }
#endif

        KeInitializeGuardedMutex (&MmSystemCacheWs.WorkingSetMutex);

        MiInitializeDriverVerifierList ();

        //
        // Set the initial commit page limit high enough so initial pool
        // allocations (which happen in the machine dependent init) can
        // succeed.
        //

        MmTotalCommitLimit = _2gb / PAGE_SIZE;
        MmTotalCommitLimitMaximum = MmTotalCommitLimit;

        //
        // Pick a reasonable size for the default prototype PTE allocation
        // chunk size.  Make sure it's always a PAGE_SIZE multiple.  The
        // registry entry is treated as the number of 1K chunks.
        //

        if (MmAllocationFragment == 0) {
            AutosizingFragment = TRUE;
            MmAllocationFragment = MM_ALLOCATION_FRAGMENT;
#if !defined (_WIN64)
            if (NumberOfPages < 64 * 1024) {
                MmAllocationFragment = MM_ALLOCATION_FRAGMENT / 4;
            }
            else if (NumberOfPages < 256 * 1024) {
                MmAllocationFragment = MM_ALLOCATION_FRAGMENT / 2;
            }
#endif
        }
        else {

            //
            // Convert the registry entry from 1K chunks into bytes.
            // Then round it to a PAGE_SIZE multiple.  Finally bound it
            // reasonably.
            //

            AutosizingFragment = FALSE;
            MmAllocationFragment *= 1024;
            MmAllocationFragment = ROUND_TO_PAGES (MmAllocationFragment);

            if (MmAllocationFragment > MM_ALLOCATION_FRAGMENT_MAX) {
                MmAllocationFragment = MM_ALLOCATION_FRAGMENT_MAX;
            }
            else if (MmAllocationFragment < PAGE_SIZE) {
                MmAllocationFragment = PAGE_SIZE;
            }
        }

        MiInitializeIoTrackers ();

        MiInitializeCacheOverrides ();

        //
        // Initialize the machine dependent portion of the hardware.
        //

        MiInitMachineDependent (LoaderBlock);

        MmPhysicalMemoryBlock = MmInitializeMemoryLimits (LoaderBlock,
                                                          IncludeType,
                                                          NULL);

        if (MmPhysicalMemoryBlock == NULL) {
            KeBugCheckEx (INSTALL_MORE_MEMORY,
                          MmNumberOfPhysicalPages,
                          MmLowestPhysicalPage,
                          MmHighestPhysicalPage,
                          0x100);
        }

#if defined(_X86_) || defined(_AMD64_)
        MiReportPhysicalMemory ();
#endif

#if defined (_MI_MORE_THAN_4GB_)
        if (MiNoLowMemory != 0) {
            MiRemoveLowPages (0);
        }
#endif

        //
        // Initialize listhead, spinlock and semaphore for
        // segment dereferencing thread.
        //

        KeInitializeSpinLock (&MmDereferenceSegmentHeader.Lock);
        InitializeListHead (&MmDereferenceSegmentHeader.ListHead);
        KeInitializeSemaphore (&MmDereferenceSegmentHeader.Semaphore, 0, MAXLONG);

        InitializeListHead (&MmUnusedSegmentList);
        InitializeListHead (&MmUnusedSubsectionList);
        KeInitializeEvent (&MmUnusedSegmentCleanup, NotificationEvent, FALSE);

        MiInitializeCommitment ();

        MiInitializePfnTracing ();

#if defined(_X86_)

        //
        // Virtual bias indicates the offset that needs to be added to
        // 0x80000000 to get to the start of the loaded images.  Update it
        // now to indicate the offset to MmSessionBase as that is the lowest
        // system address that process creation needs to make sure to duplicate.
        //
        // This is not done until after machine dependent initialization runs
        // as that initialization relies on the original meaning of VirtualBias.
        //
        // Note if the system is booted with both /3GB & /USERVA, then system
        // PTEs will be allocated below virtual 3GB and that will end up being
        // the lowest system address the process creation needs to duplicate.
        //

        if (MmVirtualBias != 0) {
            MmVirtualBias = (ULONG_PTR)MmSessionBase - CODE_START;
        }
#endif

        //
        // Create the bitmap which represents valid memory.  Note the largest
        // possible size is used because (unlike the MmPhysicalMemoryBlock) we
        // don't want to free and reallocate this bitmap during hotadds because
        // we want callers to be able to reference it lock free.
        //

        ASSERT (MmHighestPossiblePhysicalPage + 1 < _4gb);

        Bitmap = ExAllocatePoolWithTag (
                       NonPagedPool,
                       (((MmHighestPossiblePhysicalPage + 1) + 31) / 32) * 4,
                       '  mM');

        if (Bitmap == NULL) {
            KeBugCheckEx (INSTALL_MORE_MEMORY,
                          MmNumberOfPhysicalPages,
                          MmLowestPhysicalPage,
                          MmHighestPhysicalPage,
                          0x101);
        }

        RtlInitializeBitMap (&MiPfnBitMap,
                             Bitmap,
                             (ULONG)(MmHighestPossiblePhysicalPage + 1));

        RtlClearAllBits (&MiPfnBitMap);

        for (i = 0; i < MmPhysicalMemoryBlock->NumberOfRuns; i += 1) {

            Run = &MmPhysicalMemoryBlock->Run[i];

            if (Run->PageCount != 0) {
                RtlSetBits (&MiPfnBitMap,
                            (ULONG)Run->BasePage,
                            (ULONG)Run->PageCount);
            }
        }

        MiSyncCachedRanges ();

#if defined(_X86_) || defined(_AMD64_)
        MiAddHalIoMappings ();
#endif

        //
        // Create mirroring bitmaps if mirroring is enabled.
        //

        if (MmMirroring & MM_MIRRORING_ENABLED) {

#if defined (_WIN64)

            //
            // All page frame numbers must fit in 32 bits because the bitmap
            // package is currently 32-bit.
            //
            // The bitmaps are deliberately not initialized as each mirroring
            // must reinitialize them anyway.
            //

            if (MmHighestPossiblePhysicalPage + 1 < _4gb) {
#endif

                MiCreateBitMap (&MiMirrorBitMap,
                                MmHighestPossiblePhysicalPage + 1,
                                NonPagedPool);

                if (MiMirrorBitMap != NULL) {
                    MiCreateBitMap (&MiMirrorBitMap2,
                                    MmHighestPossiblePhysicalPage + 1,
                                    NonPagedPool);

                    if (MiMirrorBitMap2 == NULL) {
                        MiRemoveBitMap (&MiMirrorBitMap);
                    }
                }
#if defined (_WIN64)
            }
#endif
        }

#if !defined (_WIN64)
        if ((AutosizingFragment == TRUE) &&
            (NumberOfPages >= 256 * 1024)) {

            //
            // This is a system with at least 1GB of RAM.  Presumably it
            // will be used to cache many files.  Maybe we should factor in
            // pool size here and adjust it accordingly.
            //

            MmAllocationFragment;
        }
#endif

        //
        // Temporarily initialize resident available pages so large page
        // allocations can succeed if the memory exists.
        //

        MmResidentAvailablePages = MmAvailablePages - MM_FLUID_PHYSICAL_PAGES;

        MiInitializeLargePageSupport ();

        MiInitializeDriverLargePageList ();

        //
        // Relocate all the drivers so they can be paged (and protected) on
        // a per-page basis.
        //

        MiReloadBootLoadedDrivers (LoaderBlock);

#if defined (_MI_MORE_THAN_4GB_)
        if (MiNoLowMemory != 0) {
            MiRemoveLowPages (1);
        }
#endif
        MiInitializeVerifyingComponents (LoaderBlock);

        //
        // Setup the system size as small, medium, or large depending
        // on memory available.
        //
        // For internal MM tuning, the following applies
        //
        // 12Mb  is small
        // 12-19 is medium
        // > 19 is large
        //
        //
        // For all other external tuning,
        // < 19 is small
        // 19 - 31 is medium for workstation
        // 19 - 63 is medium for server
        // >= 32 is large for workstation
        // >= 64 is large for server
        //

        if (MmNumberOfPhysicalPages <= MM_SMALL_SYSTEM) {
            MmSystemSize = MmSmallSystem;
            MmMaximumDeadKernelStacks = 0;
            MmModifiedPageMaximum = 100;
            MmDataClusterSize = 0;
            MmCodeClusterSize = 1;
            MmReadClusterSize = 2;
            MmInPageSupportMinimum = 2;
        }
        else if (MmNumberOfPhysicalPages <= MM_MEDIUM_SYSTEM) {
            MmSystemSize = MmSmallSystem;
            MmMaximumDeadKernelStacks = 2;
            MmModifiedPageMaximum = 150;
            MmSystemCacheWsMinimum += 100;
            MmSystemCacheWsMaximum += 150;
            MmDataClusterSize = 1;
            MmCodeClusterSize = 2;
            MmReadClusterSize = 4;
            MmInPageSupportMinimum = 3;
        }
        else {
            MmSystemSize = MmMediumSystem;
            MmMaximumDeadKernelStacks = 5;
            MmModifiedPageMaximum = 300;
            MmSystemCacheWsMinimum += 400;
            MmSystemCacheWsMaximum += 800;
            MmDataClusterSize = 3;
            MmCodeClusterSize = 7;
            MmReadClusterSize = 7;
            MmInPageSupportMinimum = 4;
        }

        if (MmNumberOfPhysicalPages < ((24*1024*1024)/PAGE_SIZE)) {
            MmSystemCacheWsMinimum = 32;
        }

        if (MmNumberOfPhysicalPages >= ((32*1024*1024)/PAGE_SIZE)) {

            //
            // If we are on a workstation, 32Mb and above are considered
            // large systems.
            //

            if (MmProductType == 0x00690057) {
                MmSystemSize = MmLargeSystem;
            }
            else {

                //
                // For servers, 64Mb and greater is a large system
                //

                if (MmNumberOfPhysicalPages >= ((64*1024*1024)/PAGE_SIZE)) {
                    MmSystemSize = MmLargeSystem;
                }
            }
        }

        if (MmNumberOfPhysicalPages > ((33*1024*1024)/PAGE_SIZE)) {
            MmModifiedPageMaximum = 800;
            MmSystemCacheWsMinimum += 500;
            MmSystemCacheWsMaximum += 900;
            MmInPageSupportMinimum += 4;
        }

        if (MmNumberOfPhysicalPages > ((220*1024*1024)/PAGE_SIZE)) {

            //
            // Bump max cache size a bit more.
            //

            if ((LONG)MmSystemCacheWsMinimum < (LONG)((24*1024*1024) >> PAGE_SHIFT) &&
                (LONG)MmSystemCacheWsMaximum < (LONG)((24*1024*1024) >> PAGE_SHIFT)) {
                MmSystemCacheWsMaximum = ((24*1024*1024) >> PAGE_SHIFT);
            }

            ASSERT ((LONG)MmSystemCacheWsMaximum > (LONG)MmSystemCacheWsMinimum);
        } 
        else if (MmNumberOfPhysicalPages > ((110*1024*1024)/PAGE_SIZE)) {

            //
            // Bump max cache size a bit.
            //

            if ((LONG)MmSystemCacheWsMinimum < (LONG)((16*1024*1024) >> PAGE_SHIFT) &&
                (LONG)MmSystemCacheWsMaximum < (LONG)((16*1024*1024) >> PAGE_SHIFT)){
                MmSystemCacheWsMaximum = ((16*1024*1024) >> PAGE_SHIFT);
            }

            ASSERT ((LONG)MmSystemCacheWsMaximum > (LONG)MmSystemCacheWsMinimum);
        }

        if (NT_SUCCESS (MmIsVerifierEnabled (&VerifierFlags))) {

            //
            // The verifier is enabled so don't defer any MDL unlocks because
            // without state, debugging driver bugs in this area is very
            // difficult.
            //

            DeferredMdlEntries = 0;
        }
        else if (MmNumberOfPhysicalPages > ((255*1024*1024)/PAGE_SIZE)) {
            DeferredMdlEntries = 32;
        }
        else if (MmNumberOfPhysicalPages > ((127*1024*1024)/PAGE_SIZE)) {
            DeferredMdlEntries = 8;
        }
        else {
            DeferredMdlEntries = 4;
        }

#if defined(MI_MULTINODE)
        for (i = 0; i < KeNumberNodes; i += 1) {

            InitializeSListHead (&KeNodeBlock[i]->PfnDereferenceSListHead);
            KeNodeBlock[i]->PfnDeferredList = NULL;

            for (j = 0; j < DeferredMdlEntries; j += 1) {

                SingleListEntry = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(MI_PFN_DEREFERENCE_CHUNK),
                                             'mDmM');
        
                if (SingleListEntry != NULL) {
                    InterlockedPushEntrySList (&KeNodeBlock[i]->PfnDereferenceSListHead,
                                               SingleListEntry);
                }
            }
        }
#else
        InitializeSListHead (&MmPfnDereferenceSListHead);

        for (j = 0; j < DeferredMdlEntries; j += 1) {
            SingleListEntry = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(MI_PFN_DEREFERENCE_CHUNK),
                                             'mDmM');
        
            if (SingleListEntry != NULL) {
                InterlockedPushEntrySList (&MmPfnDereferenceSListHead,
                                           SingleListEntry);
            }
        }
#endif
        
        MmFreedExpansionPoolMaximum = 5;

        if (MmNumberOfPhysicalPages > ((1000*1024*1024)/PAGE_SIZE)) {
            MmFreedExpansionPoolMaximum = 300;
        }
        else if (MmNumberOfPhysicalPages > ((500*1024*1024)/PAGE_SIZE)) {
            MmFreedExpansionPoolMaximum = 100;
        }

        ASSERT (SharedUserData->NumberOfPhysicalPages == 0);

        SharedUserData->NumberOfPhysicalPages = (ULONG) MmNumberOfPhysicalPages;

        SharedUserData->LargePageMinimum = 0;

        //
        // Determine if we are on an AS system (Winnt is not AS).
        //

        if (MmProductType == 0x00690057) {
            SharedUserData->NtProductType = NtProductWinNt;
            MmProductType = 0;
            MmThrottleTop = 250;
            MmThrottleBottom = 30;

        }
        else {
            if (MmProductType == 0x0061004c) {
                SharedUserData->NtProductType = NtProductLanManNt;
            }
            else {
                SharedUserData->NtProductType = NtProductServer;
            }

            MmProductType = 1;
            MmThrottleTop = 450;
            MmThrottleBottom = 80;
            MmMinimumFreePages = 81;
            MmInPageSupportMinimum += 8;
        }

        MiAdjustWorkingSetManagerParameters ((LOGICAL)(MmProductType == 0 ? TRUE : FALSE));

        //
        // Set the ResidentAvailablePages to the number of available
        // pages minus the fluid value.
        //

        MmResidentAvailablePages = MmAvailablePages - MM_FLUID_PHYSICAL_PAGES;

        //
        // Subtract off the size of future nonpaged pool expansion
        // so that nonpaged pool will always be able to expand regardless of
        // prior system load activity.
        //

        MmResidentAvailablePages -= MiExpansionPoolPagesInitialCharge;

        //
        // Subtract off the size of the system cache working set.
        //

        MmResidentAvailablePages -= MmSystemCacheWsMinimum;
        MmResidentAvailableAtInit = MmResidentAvailablePages;

        if (MmResidentAvailablePages < 0) {
#if DBG
            DbgPrint("system cache working set too big\n");
#endif
            return FALSE;
        }

        //
        // Initialize spin lock for allowing working set expansion.
        //

        KeInitializeSpinLock (&MmExpansionLock);

        KeInitializeGuardedMutex (&MmPageFileCreationLock);

        //
        // Initialize resources for extending sections.
        //

        ExInitializeResourceLite (&MmSectionExtendResource);
        ExInitializeResourceLite (&MmSectionExtendSetResource);

        //
        // Build the system cache structures.
        //

        StartPde = MiGetPdeAddress (MmSystemCacheWorkingSetList);
        PointerPte = MiGetPteAddress (MmSystemCacheWorkingSetList);

#if (_MI_PAGING_LEVELS >= 3)

        TempPte = ValidKernelPte;

#if (_MI_PAGING_LEVELS >= 4)
        StartPxe = MiGetPdeAddress(StartPde);

        if (StartPxe->u.Hard.Valid == 0) {

            //
            // Map in a page directory parent page for the system cache working
            // set.  Note that we only populate one page table for this.
            //

            DirectoryFrameIndex = MiRemoveAnyPage(
                MI_GET_PAGE_COLOR_FROM_PTE (StartPxe));
            TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;
            *StartPxe = TempPte;

            MiInitializePfn (DirectoryFrameIndex, StartPxe, 1);

            MmResidentAvailablePages -= 1;

            KeZeroPages (MiGetVirtualAddressMappedByPte (StartPxe), PAGE_SIZE);
        }
#endif

        StartPpe = MiGetPteAddress(StartPde);

        if (StartPpe->u.Hard.Valid == 0) {

            //
            // Map in a page directory page for the system cache working set.
            // Note that we only populate one page table for this.
            //

            DirectoryFrameIndex = MiRemoveAnyPage(
                MI_GET_PAGE_COLOR_FROM_PTE (StartPpe));
            TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;
            *StartPpe = TempPte;

            MiInitializePfn (DirectoryFrameIndex, StartPpe, 1);

            MmResidentAvailablePages -= 1;

            KeZeroPages (MiGetVirtualAddressMappedByPte (StartPpe), PAGE_SIZE);
        }

#if (_MI_PAGING_LEVELS >= 4)

        //
        // The shared user data is already initialized and it shares the
        // page table page with the system cache working set list.
        //

        ASSERT (StartPde->u.Hard.Valid == 1);
#else

        //
        // Map in a page table page.
        //

        ASSERT (StartPde->u.Hard.Valid == 0);

        PageFrameIndex = MiRemoveAnyPage(
                                MI_GET_PAGE_COLOR_FROM_PTE (StartPde));
        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
        MI_WRITE_VALID_PTE (StartPde, TempPte);

        MiInitializePfn (PageFrameIndex, StartPde, 1);

        MmResidentAvailablePages -= 1;

        KeZeroPages (MiGetVirtualAddressMappedByPte (StartPde), PAGE_SIZE);
#endif

        StartPpe = MiGetPpeAddress(MmSystemCacheStart);
        StartPde = MiGetPdeAddress(MmSystemCacheStart);
        PointerPte = MiGetVirtualAddressMappedByPte (StartPde);

#else
#if !defined(_X86PAE_)
        ASSERT ((StartPde + 1) == MiGetPdeAddress (MmSystemCacheStart));
#endif
#endif

        MaximumSystemCacheSizeTotal = MaximumSystemCacheSize;

#if defined(_X86_)
        MaximumSystemCacheSizeTotal += MiMaximumSystemCacheSizeExtra;
#endif

        //
        // Size the system cache based on the amount of physical memory.
        //

        i = (MmNumberOfPhysicalPages + 65) / 1024;

        if (i >= 4) {

            //
            // System has at least 4032 pages.  Make the system
            // cache 128mb + 64mb for each additional 1024 pages.
            //

            MmSizeOfSystemCacheInPages = (PFN_COUNT)(
                            ((128*1024*1024) >> PAGE_SHIFT) +
                            ((i - 4) * ((64*1024*1024) >> PAGE_SHIFT)));
            if (MmSizeOfSystemCacheInPages > MaximumSystemCacheSizeTotal) {
                MmSizeOfSystemCacheInPages = MaximumSystemCacheSizeTotal;
            }
        }

        MmSystemCacheEnd = (PVOID)(((PCHAR)MmSystemCacheStart +
                    MmSizeOfSystemCacheInPages * PAGE_SIZE) - 1);

#if defined(_X86_)
        if (MmSizeOfSystemCacheInPages > MaximumSystemCacheSize) {
            ASSERT (MiMaximumSystemCacheSizeExtra != 0);
            MmSystemCacheEnd = (PVOID)(((PCHAR)MmSystemCacheStart +
                        MaximumSystemCacheSize * PAGE_SIZE) - 1);

            MiSystemCacheStartExtra = (PVOID)MiExtraResourceStart;
            MiSystemCacheEndExtra = (PVOID)(((PCHAR)MiSystemCacheStartExtra +
                        (MmSizeOfSystemCacheInPages - MaximumSystemCacheSize) * PAGE_SIZE) - 1);
        }
        else {
            MiSystemCacheStartExtra = MmSystemCacheStart;
            MiSystemCacheEndExtra = MmSystemCacheEnd;
        }
#endif

        EndPde = MiGetPdeAddress(MmSystemCacheEnd);

        TempPte = ValidKernelPte;
        Color = 0;

#if (_MI_PAGING_LEVELS >= 4)
        StartPxe = MiGetPxeAddress(MmSystemCacheStart);
        if (StartPxe->u.Hard.Valid == 0) {
            FirstPxe = TRUE;
            FirstPpe = TRUE;
        }
        else {
            FirstPxe = FALSE;
            FirstPpe = (StartPpe->u.Hard.Valid == 0) ? TRUE : FALSE;
        }
#elif (_MI_PAGING_LEVELS >= 3)
        FirstPpe = (StartPpe->u.Hard.Valid == 0) ? TRUE : FALSE;
#else
        DirectoryFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (MiGetPteAddress(PDE_BASE));
#endif

        while (StartPde <= EndPde) {

#if (_MI_PAGING_LEVELS >= 4)
            if (FirstPxe == TRUE || MiIsPteOnPpeBoundary(StartPde)) {
                FirstPxe = FALSE;
                StartPxe = MiGetPdeAddress(StartPde);

                //
                // Map in a page directory page.
                //

                Color = (MI_SYSTEM_PAGE_COLOR & (MmSecondaryColors - 1));
                MI_SYSTEM_PAGE_COLOR++;

                LOCK_PFN (OldIrql);

                DirectoryFrameIndex = MiRemoveAnyPage (Color);

                TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;

                MI_WRITE_VALID_PTE (StartPxe, TempPte);

                MiInitializePfn (DirectoryFrameIndex,
                                 StartPxe,
                                 1);

                MmResidentAvailablePages -= 1;

                UNLOCK_PFN (OldIrql);

                KeZeroPages (MiGetVirtualAddressMappedByPte (StartPxe), PAGE_SIZE);
            }
#endif

#if (_MI_PAGING_LEVELS >= 3)
            if (FirstPpe == TRUE || MiIsPteOnPdeBoundary(StartPde)) {
                FirstPpe = FALSE;
                StartPpe = MiGetPteAddress(StartPde);

                //
                // Map in a page directory page.
                //

                Color = (MI_SYSTEM_PAGE_COLOR & (MmSecondaryColors - 1));
                MI_SYSTEM_PAGE_COLOR++;

                LOCK_PFN (OldIrql);

                DirectoryFrameIndex = MiRemoveAnyPage (Color);

                TempPte.u.Hard.PageFrameNumber = DirectoryFrameIndex;

                MI_WRITE_VALID_PTE (StartPpe, TempPte);

                MiInitializePfn (DirectoryFrameIndex,
                                 StartPpe,
                                 1);

                MmResidentAvailablePages -= 1;

                UNLOCK_PFN (OldIrql);

                KeZeroPages (MiGetVirtualAddressMappedByPte (StartPpe), PAGE_SIZE);
            }
#endif

            ASSERT (StartPde->u.Hard.Valid == 0);

            //
            // Map in a page table page.
            //

            Color = (MI_SYSTEM_PAGE_COLOR & (MmSecondaryColors - 1));
            MI_SYSTEM_PAGE_COLOR++;

            LOCK_PFN (OldIrql);

            PageFrameIndex = MiRemoveAnyPage (Color);
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (StartPde, TempPte);

            MiInitializePfn (PageFrameIndex, StartPde, 1);

            MmResidentAvailablePages -= 1;

            UNLOCK_PFN (OldIrql);

            KeZeroPages (MiGetVirtualAddressMappedByPte (StartPde), PAGE_SIZE);

            StartPde += 1;
        }

        //
        // Initialize the system cache.  Only set the large system cache if
        // we have a large amount of physical memory.
        //

        if (MmLargeSystemCache != 0 && MmNumberOfPhysicalPages > 0x7FF0) {
            if ((MmAvailablePages >
                    MmSystemCacheWsMaximum + ((64*1024*1024) >> PAGE_SHIFT))) {
                MmSystemCacheWsMaximum =
                            MmAvailablePages - ((32*1024*1024) >> PAGE_SHIFT);
                ASSERT ((LONG)MmSystemCacheWsMaximum > (LONG)MmSystemCacheWsMinimum);
            }
        }

        if (MmSystemCacheWsMaximum > (MM_MAXIMUM_WORKING_SET - 5)) {
            MmSystemCacheWsMaximum = MM_MAXIMUM_WORKING_SET - 5;
        }

        if (MmSystemCacheWsMaximum > MmSizeOfSystemCacheInPages) {
            MmSystemCacheWsMaximum = MmSizeOfSystemCacheInPages;
            if ((MmSystemCacheWsMinimum + 500) > MmSystemCacheWsMaximum) {
                MmSystemCacheWsMinimum = MmSystemCacheWsMaximum - 500;
            }
        }

        MiInitializeSystemCache ((ULONG)MmSystemCacheWsMinimum,
                                 (ULONG)MmSystemCacheWsMaximum);

        MmAttemptForCantExtend.Segment = NULL;
        MmAttemptForCantExtend.RequestedExpansionSize = 1;
        MmAttemptForCantExtend.ActualExpansion = 0;
        MmAttemptForCantExtend.InProgress = FALSE;
        MmAttemptForCantExtend.PageFileNumber = MI_EXTEND_ANY_PAGEFILE;

        KeInitializeEvent (&MmAttemptForCantExtend.Event,
                           NotificationEvent,
                           FALSE);

        //
        // Now that we have booted far enough, replace the temporary
        // commit limits with real ones: set the initial commit page
        // limit to the number of available pages.  This value is
        // updated as paging files are created.
        //

        MmTotalCommitLimit = MmAvailablePages;

        if (MmTotalCommitLimit > 1024) {
            MmTotalCommitLimit -= 1024;
        }

        MmTotalCommitLimitMaximum = MmTotalCommitLimit;

        //
        // Set maximum working set size to 512 pages less than the
        // total available memory.
        //

        MmMaximumWorkingSetSize = (WSLE_NUMBER)(MmAvailablePages - 512);

        if (MmMaximumWorkingSetSize > (MM_MAXIMUM_WORKING_SET - 5)) {
            MmMaximumWorkingSetSize = MM_MAXIMUM_WORKING_SET - 5;
        }

        //
        // Create the modified page writer event.
        //

        KeInitializeEvent (&MmModifiedPageWriterEvent, NotificationEvent, FALSE);

        //
        // Build paged pool.
        //

        MiBuildPagedPool ();

        //
        // Initialize the loaded module list.  This cannot be done until
        // paged pool has been built.
        //

        if (MiInitializeLoadedModuleList (LoaderBlock) == FALSE) {
#if DBG
            DbgPrint("Loaded module list initialization failed\n");
#endif
            return FALSE;
        }

        //
        // Initialize the handle for the PAGELK section now that all drivers
        // have been relocated to their final resting place and the loaded
        // module list has been initialized.
        //
    
        ExPageLockHandle = MmLockPagableCodeSection ((PVOID)(ULONG_PTR)MmShutdownSystem);
        MmUnlockPagableImageSection (ExPageLockHandle);

        //
        // Initialize the unused segment threshold.  Attempt to keep pool usage
        // below this percentage (by trimming the cache) if pool requests
        // can fail.
        //

        if (MmConsumedPoolPercentage == 0) {
            MmConsumedPoolPercentage = 80;
        }
        else if (MmConsumedPoolPercentage < 5) {
            MmConsumedPoolPercentage = 5;
        }
        else if (MmConsumedPoolPercentage > 100) {
            MmConsumedPoolPercentage = 100;
        }
    
        //
        // Add more system PTEs if this is a large memory system.
        // Note that 64 bit systems can determine the right value at the
        // beginning since there is no virtual address space crunch.
        //

#if !defined (_WIN64)
        if (MmNumberOfPhysicalPages > ((127*1024*1024) >> PAGE_SHIFT)) {

            PMMPTE StartingPte;

            PointerPde = MiGetPdeAddress ((PCHAR)MmPagedPoolEnd + 1);
            StartingPte = MiGetPteAddress ((PCHAR)MmPagedPoolEnd + 1);
            j = 0;

            TempPte = ValidKernelPde;
            LOCK_PFN (OldIrql);
            while (PointerPde->u.Hard.Valid == 0) {

                MiChargeCommitmentCantExpand (1, TRUE);
                MM_TRACK_COMMIT (MM_DBG_COMMIT_EXTRA_SYSTEM_PTES, 1);

                PageFrameIndex = MiRemoveZeroPage (
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (PointerPde, TempPte);
                MiInitializePfn (PageFrameIndex, PointerPde, 1);
                PointerPde += 1;
                StartingPte += PAGE_SIZE / sizeof(MMPTE);
                j += PAGE_SIZE / sizeof(MMPTE);
                MmResidentAvailablePages -= 1;
            }

            UNLOCK_PFN (OldIrql);

            if (j != 0) {
                StartingPte = MiGetPteAddress ((PCHAR)MmPagedPoolEnd + 1);
                MmNonPagedSystemStart = MiGetVirtualAddressMappedByPte (StartingPte);
                MmNumberOfSystemPtes += j;
                MiAddSystemPtes (StartingPte, j, SystemPteSpace);
                MiIncrementSystemPtes (j);
            }
        }

        //
        // Snap a copy of the initial page directory so that when large page
        // system PTE mappings are deleted the proper values can be restored.
        //

        MiInitialSystemPageDirectory = ExAllocatePoolWithTag (
                                            NonPagedPool,
                                            PD_PER_SYSTEM * PAGE_SIZE,
                                            'dPmM');

        if (MiInitialSystemPageDirectory == NULL) { 
#if DBG
            DbgPrint("can't snap system page directory\n");
#endif
            return FALSE;
        }

        RtlCopyMemory (MiInitialSystemPageDirectory,
                       (PVOID) PDE_BASE,
                       PD_PER_SYSTEM * PAGE_SIZE);

#endif

#if defined (_MI_DEBUG_SUB)
        if (MiTrackSubs != 0) {
            MiSubsectionTraces = ExAllocatePoolWithTag (NonPagedPool,
                                   MiTrackSubs * sizeof (MI_SUB_TRACES),
                                   'tCmM');
        }
#endif

#if defined (_MI_DEBUG_DIRTY)
        if (MiTrackDirtys != 0) {
            MiDirtyTraces = ExAllocatePoolWithTag (NonPagedPool,
                                   MiTrackDirtys * sizeof (MI_DIRTY_TRACES),
                                   'tCmM');
        }
#endif

#if defined (_MI_DEBUG_DATA)
        if (MiTrackData != 0) {
            MiDataTraces = ExAllocatePoolWithTag (NonPagedPool,
                                   MiTrackData * sizeof (MI_DATA_TRACES),
                                   'tCmM');
        }
#endif

#if DBG
        if (MmDebug & MM_DBG_DUMP_BOOT_PTES) {
            MiDumpValidAddresses ();
            MiDumpPfn ();
        }
#endif

        MmPageFaultNotifyRoutine = NULL;

        return TRUE;
    }

    if (Phase == 1) {

#ifdef _X86_
        if (KeFeatureBits & KF_LARGE_PAGE)
#endif
            SharedUserData->LargePageMinimum = MM_MINIMUM_VA_FOR_LARGE_PAGE;

#if DBG
        MmDebug |= MM_DBG_CHECK_PFN_LOCK;
#endif

#if defined(_X86_) || defined(_AMD64_)
        MiInitMachineDependent (LoaderBlock);
#endif
        MiMapBBTMemory(LoaderBlock);

        if (!MiSectionInitialization ()) {
            return FALSE;
        }

        Process = PsGetCurrentProcess ();

        //
        // Create double mapped page between kernel and user mode.
        // The PTE is deliberately allocated from paged pool so that
        // it will always have a PTE itself instead of being superpaged.
        // This way, checks throughout the fault handler can assume that
        // the PTE can be checked without having to special case this.
        //

        MmSharedUserDataPte = ExAllocatePoolWithTag (PagedPool,
                                                     sizeof(MMPTE),
                                                     '  mM');

        if (MmSharedUserDataPte == NULL) {
            return FALSE;
        }

        PointerPte = MiGetPteAddress ((PVOID)KI_USER_SHARED_DATA);
        ASSERT (PointerPte->u.Hard.Valid == 1);
        PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);

        MI_MAKE_VALID_PTE (TempPte,
                           PageFrameIndex,
                           MM_READONLY,
                           PointerPte);

        *MmSharedUserDataPte = TempPte;

        Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);

        LOCK_PFN (OldIrql);

        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;

        UNLOCK_PFN (OldIrql);

#ifdef _X86_
        if (MmHighestUserAddress < (PVOID) MM_SHARED_USER_DATA_VA) {

            //
            // Install the PTE mapping now as faults will not because the
            // shared user data is in the system portion of the address space.
            // Note the pagetable page has already been allocated and locked
            // down.
            //

            //
            // Make the mapping user accessible.
            //

            ASSERT (MmSharedUserDataPte->u.Hard.Owner == 0);
            MmSharedUserDataPte->u.Hard.Owner = 1;

            PointerPde = MiGetPdeAddress (MM_SHARED_USER_DATA_VA);
            ASSERT (PointerPde->u.Hard.Owner == 0);
            PointerPde->u.Hard.Owner = 1;

            ASSERT (MiUseMaximumSystemSpace != 0);
            PointerPte = MiGetPteAddress (MM_SHARED_USER_DATA_VA);
            ASSERT (PointerPte->u.Hard.Valid == 0);
            MI_WRITE_VALID_PTE (PointerPte, *MmSharedUserDataPte);
        }
#endif

        MiSessionWideInitializeAddresses ();
        MiInitializeSessionWsSupport ();
        MiInitializeSessionIds ();

        //
        // Start the modified page writer.
        //

        InitializeObjectAttributes (&ObjectAttributes, NULL, 0, NULL, NULL);

        if (!NT_SUCCESS(PsCreateSystemThread (&ThreadHandle,
                                              THREAD_ALL_ACCESS,
                                              &ObjectAttributes,
                                              0L,
                                              NULL,
                                              MiModifiedPageWriter,
                                              NULL))) {
            return FALSE;
        }
        ZwClose (ThreadHandle);

        //
        // Initialize the low and high memory events.  This must be done
        // before starting the working set manager.
        //

        if (MiInitializeMemoryEvents () == FALSE) {
            return FALSE;
        }

        //
        // Start the balance set manager.
        //
        // The balance set manager performs stack swapping and working
        // set management and requires two threads.
        //

        KeInitializeEvent (&MmWorkingSetManagerEvent,
                           SynchronizationEvent,
                           FALSE);

        InitializeObjectAttributes (&ObjectAttributes, NULL, 0, NULL, NULL);

        if (!NT_SUCCESS(PsCreateSystemThread (&ThreadHandle,
                                              THREAD_ALL_ACCESS,
                                              &ObjectAttributes,
                                              0L,
                                              NULL,
                                              KeBalanceSetManager,
                                              NULL))) {

            return FALSE;
        }
        ZwClose (ThreadHandle);

        if (!NT_SUCCESS(PsCreateSystemThread (&ThreadHandle,
                                              THREAD_ALL_ACCESS,
                                              &ObjectAttributes,
                                              0L,
                                              NULL,
                                              KeSwapProcessOrStack,
                                              NULL))) {

            return FALSE;
        }
        ZwClose (ThreadHandle);

#if !defined(NT_UP)
        MiStartZeroPageWorkers ();
#endif

#if defined(_X86_)
        MiEnableKernelVerifier ();
#endif

        ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);

        NextEntry = PsLoadedModuleList.Flink;

        for ( ; NextEntry != &PsLoadedModuleList; NextEntry = NextEntry->Flink) {

            DataTableEntry = CONTAINING_RECORD(NextEntry,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

            NtHeaders = RtlImageNtHeader(DataTableEntry->DllBase);

            if ((NtHeaders != NULL) &&
                (NtHeaders->OptionalHeader.MajorOperatingSystemVersion >= 5) &&
                (NtHeaders->OptionalHeader.MajorImageVersion >= 5)) {
                DataTableEntry->Flags |= LDRP_ENTRY_NATIVE;
            }

            MiWriteProtectSystemImage (DataTableEntry->DllBase);
        }
        ExReleaseResourceLite (&PsLoadedModuleResource);

        InterlockedDecrement (&MiTrimInProgressCount);

        return TRUE;
    }

    if (Phase == 2) {
        MiEnablePagingTheExecutive ();
        return TRUE;
    }

    return FALSE;
}

VOID
MiMapBBTMemory (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This function walks through the loader block's memory descriptor list
    and maps memory reserved for the BBT buffer into the system.

    The mapped PTEs are PDE-aligned and made user accessible.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/
{
    PVOID Va;
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;
    PLIST_ENTRY NextMd;
    PFN_NUMBER NumberOfPagesMapped;
    PFN_NUMBER NumberOfPages;
    PFN_NUMBER PageFrameIndex;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PMMPTE LastPde;
    MMPTE TempPte;

    if (BBTPagesToReserve <= 0) {
        return;
    }

    //
    // Request enough PTEs such that protection can be applied to the PDEs.
    //

    NumberOfPages = (BBTPagesToReserve + (PTE_PER_PAGE - 1)) & ~(PTE_PER_PAGE - 1);

    PointerPte = MiReserveAlignedSystemPtes ((ULONG)NumberOfPages,
                                             SystemPteSpace,
                                             MM_VA_MAPPED_BY_PDE);

    if (PointerPte == NULL) {
        BBTPagesToReserve = 0;
        return;
    }

    //
    // Allow user access to the buffer.
    //

    PointerPde = MiGetPteAddress (PointerPte);
    LastPde = MiGetPteAddress (PointerPte + NumberOfPages);

    ASSERT (LastPde != PointerPde);

    do {
        TempPte = *PointerPde;
        TempPte.u.Long |= MM_PTE_OWNER_MASK;
        MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPde, TempPte);
        PointerPde += 1;
    } while (PointerPde < LastPde);

    KeFlushEntireTb (TRUE, TRUE);

    Va = MiGetVirtualAddressMappedByPte (PointerPte);

    TempPte = ValidUserPte;
    NumberOfPagesMapped = 0;

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if (MemoryDescriptor->MemoryType == LoaderBBTMemory) {

            PageFrameIndex = MemoryDescriptor->BasePage;
            NumberOfPages = MemoryDescriptor->PageCount;

            if (NumberOfPagesMapped + NumberOfPages > BBTPagesToReserve) {
                NumberOfPages = BBTPagesToReserve - NumberOfPagesMapped;
            }

            NumberOfPagesMapped += NumberOfPages;

            do {

                TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
                MI_WRITE_VALID_PTE (PointerPte, TempPte);

                PointerPte += 1;
                PageFrameIndex += 1;
                NumberOfPages -= 1;
            } while (NumberOfPages);

            if (NumberOfPagesMapped == BBTPagesToReserve) {
                break;
            }
        }

        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    KeZeroPages (Va, BBTPagesToReserve << PAGE_SHIFT);

    //
    // Tell BBT_Init how many pages were allocated.
    //

    if (NumberOfPagesMapped < BBTPagesToReserve) {
        BBTPagesToReserve = (ULONG)NumberOfPagesMapped;
    }

    *(PULONG)Va = BBTPagesToReserve;

    //
    // At this point instrumentation code will detect the existence of
    // buffer and initialize the structures.
    //

    BBTBuffer = Va;

    PERFINFO_MMINIT_START();
}


PPHYSICAL_MEMORY_DESCRIPTOR
MmInitializeMemoryLimits (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PBOOLEAN IncludeType,
    IN OUT PPHYSICAL_MEMORY_DESCRIPTOR InputMemory OPTIONAL
    )

/*++

Routine Description:

    This function walks through the loader block's memory
    descriptor list and builds a list of contiguous physical
    memory blocks of the desired types.

Arguments:

    LoaderBlock - Supplies a pointer the system loader block.

    IncludeType - Array of BOOLEANS of size LoaderMaximum.
                  TRUE means include this type of memory in return.

    Memory - If non-NULL, supplies the physical memory blocks to place the
             search results in.  If NULL, pool is allocated to hold the
             returned search results in - the caller must free this pool.

Return Value:

    A pointer to the physical memory blocks for the requested search or NULL
    on failure.

Environment:

    Kernel Mode Only.  System initialization.

--*/
{
    PLIST_ENTRY NextMd;
    ULONG i;
    ULONG InitialAllocation;
    PFN_NUMBER NextPage;
    PFN_NUMBER TotalPages;
    PPHYSICAL_MEMORY_DESCRIPTOR Memory;
    PPHYSICAL_MEMORY_DESCRIPTOR Memory2;
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;

    InitialAllocation = 0;

    if (ARGUMENT_PRESENT (InputMemory)) {
        Memory = InputMemory;
    }
    else {

        //
        // The caller wants us to allocate the return result buffer.  Size it
        // by allocating the maximum possibly needed as this should not be
        // very big (relatively).  It is the caller's responsibility to free
        // this.  Obviously this option can only be requested after pool has
        // been initialized.
        //

        NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

        while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {
            InitialAllocation += 1;
            MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                                 MEMORY_ALLOCATION_DESCRIPTOR,
                                                 ListEntry);
            NextMd = MemoryDescriptor->ListEntry.Flink;
        }

        Memory = ExAllocatePoolWithTag (NonPagedPool,
                                        sizeof(PHYSICAL_MEMORY_DESCRIPTOR) + sizeof(PHYSICAL_MEMORY_RUN) * (InitialAllocation - 1),
                                        'lMmM');

        if (Memory == NULL) {
            return NULL;
        }

        Memory->NumberOfRuns = InitialAllocation;
    }

    //
    // Walk through the memory descriptors and build the physical memory list.
    //

    i = 0;
    TotalPages = 0;
    NextPage = (PFN_NUMBER) -1;

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if (MemoryDescriptor->MemoryType < LoaderMaximum &&
            IncludeType [MemoryDescriptor->MemoryType]) {

            TotalPages += MemoryDescriptor->PageCount;

            //
            // Merge runs whenever possible.
            //

            if (MemoryDescriptor->BasePage == NextPage) {
                ASSERT (MemoryDescriptor->PageCount != 0);
                Memory->Run[i - 1].PageCount += MemoryDescriptor->PageCount;
                NextPage += MemoryDescriptor->PageCount;
            }
            else {
                Memory->Run[i].BasePage = MemoryDescriptor->BasePage;
                Memory->Run[i].PageCount = MemoryDescriptor->PageCount;
                NextPage = Memory->Run[i].BasePage + Memory->Run[i].PageCount;
                i += 1;
            }
        }
        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    ASSERT (i <= Memory->NumberOfRuns);

    if (i == 0) {

        //
        // Don't bother shrinking this as the caller will be freeing it
        // shortly as it is just an empty list.
        //

        Memory->Run[i].BasePage = 0;
        Memory->Run[i].PageCount = 0;
    }
    else if (!ARGUMENT_PRESENT (InputMemory)) {

        //
        // Shrink the buffer (if possible) now that the final size is known.
        //

        if (InitialAllocation > i) {
            Memory2 = ExAllocatePoolWithTag (NonPagedPool,
                                             sizeof(PHYSICAL_MEMORY_DESCRIPTOR) + sizeof(PHYSICAL_MEMORY_RUN) * (i - 1),
                                            'lMmM');

            if (Memory2 != NULL) {
                RtlCopyMemory (Memory2->Run,
                               Memory->Run,
                               sizeof(PHYSICAL_MEMORY_RUN) * i);

                ExFreePool (Memory);
                Memory = Memory2;
            }
        }
    }

    Memory->NumberOfRuns = i;
    Memory->NumberOfPages = TotalPages;

    return Memory;
}


PFN_NUMBER
MiPagesInLoaderBlock (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock,
    IN PBOOLEAN IncludeType
    )

/*++

Routine Description:

    This function walks through the loader block's memory
    descriptor list and returns the number of pages of the desired type.

Arguments:

    LoaderBlock - Supplies a pointer the system loader block.

    IncludeType - Array of BOOLEANS of size LoaderMaximum.
                  TRUE means include this type of memory in the returned count.

Return Value:

    The number of pages of the requested type in the loader block list.

Environment:

    Kernel Mode Only.  System initialization.

--*/
{
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;
    PLIST_ENTRY NextMd;
    PFN_NUMBER TotalPages;

    //
    // Walk through the memory descriptors counting pages.
    //

    TotalPages = 0;

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if (MemoryDescriptor->MemoryType < LoaderMaximum &&
            IncludeType [MemoryDescriptor->MemoryType]) {

            TotalPages += MemoryDescriptor->PageCount;
        }
        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    return TotalPages;
}


static
VOID
MiMemoryLicense (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This function walks through the loader block's memory descriptor list
    and based on the system's license, ensures only the proper amount of
    physical memory is used.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/
{
    PLIST_ENTRY NextMd;
    PFN_NUMBER TotalPagesAllowed;
    PFN_NUMBER PageCount;
    PFN_NUMBER HighestPhysicalPage;
    ULONG VirtualBias;
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;

    //
    // The default configuration gets a maximum of 4gb physical memory.
    // On PAE machines the system continues to operate in 8-byte PTE mode.
    //

    TotalPagesAllowed = MI_DEFAULT_MAX_PAGES;

    //
    // If properly licensed (ie: DataCenter) and booted without the
    // 3gb switch, then use all available physical memory.
    //

#if defined(_X86_)
    VirtualBias = LoaderBlock->u.I386.VirtualBias;

    //
    // Limit the highest physical frame number so that it both
    // fits within the PTE width and so it doesn't cause the PFN
    // database to overflow into the page table virtual space.
    //

#if defined(_X86PAE_)
    HighestPhysicalPage = MI_DTC_MAX_PAGES;
#else
    HighestPhysicalPage = MI_DEFAULT_MAX_PAGES;
#endif

#else
    VirtualBias = 0;
    HighestPhysicalPage = 0;
#endif

    if (ExVerifySuite(DataCenter) == TRUE) {

        //
        // Note MmVirtualBias has not yet been initialized at the time of the
        // first call to this routine, so use the LoaderBlock directly.
        //

        if (VirtualBias == 0) {

            //
            // Limit the maximum physical memory to the amount we have
            // actually physically seen in a machine inhouse.
            //

            TotalPagesAllowed = MI_DTC_MAX_PAGES;

        }
        else {

            //
            // The system is booting /3gb, so don't use any physical page
            // above the 16gb physical boundary.  This ensures enough
            // virtual space to map the PFN database in one contiguous chunk.
            //

            TotalPagesAllowed = MI_DTC_BOOTED_3GB_MAX_PAGES;
            HighestPhysicalPage = MI_DTC_BOOTED_3GB_MAX_PAGES;
        }
    }
    else if ((MmProductType != 0x00690057) &&
             (ExVerifySuite(Enterprise) == TRUE)) {

        //
        // Enforce the Advanced Server physical memory limit.
        // On PAE machines the system continues to operate in 8-byte PTE mode.
        //

        TotalPagesAllowed = MI_ADS_MAX_PAGES;

#if defined(_X86_)
        if (VirtualBias != 0) {

            //
            // The system is booting /3gb, so don't use any physical page
            // above the 16gb physical boundary.  This ensures enough
            // virtual space to map the PFN database in one contiguous chunk.
            //

            ASSERT (MI_DTC_BOOTED_3GB_MAX_PAGES < MI_ADS_MAX_PAGES);
            TotalPagesAllowed = MI_DTC_BOOTED_3GB_MAX_PAGES;
            HighestPhysicalPage = MI_DTC_BOOTED_3GB_MAX_PAGES;
        }
#endif
    }
    else if (ExVerifySuite(Blade) == TRUE) {

        //
        // Enforce the Blade physical memory limit.
        //

        TotalPagesAllowed = MI_BLADE_MAX_PAGES;
    }

    //
    // Walk through the memory descriptors and remove or truncate descriptors
    // that exceed the maximum physical memory to be used.
    //

    PageCount = 0;
    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;
    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);

        if ((MemoryDescriptor->MemoryType == LoaderFirmwarePermanent) ||
            (MemoryDescriptor->MemoryType == LoaderBBTMemory) ||
            (MemoryDescriptor->MemoryType == LoaderBad) ||
            (MemoryDescriptor->MemoryType == LoaderSpecialMemory)) {

            NextMd = MemoryDescriptor->ListEntry.Flink;
            continue;
        }

        if (HighestPhysicalPage != 0) {

            if (MemoryDescriptor->BasePage >= HighestPhysicalPage) {

                //
                // This descriptor needs to be removed.
                //

                RemoveEntryList (NextMd);
                NextMd = MemoryDescriptor->ListEntry.Flink;
                continue;
            }

            if (MemoryDescriptor->BasePage + MemoryDescriptor->PageCount > HighestPhysicalPage) {

                //
                // This descriptor needs to be truncated.
                //
                
                MemoryDescriptor->PageCount = (ULONG) (HighestPhysicalPage - 
                                                MemoryDescriptor->BasePage);
            }
        }

        PageCount += MemoryDescriptor->PageCount;

        if (PageCount <= TotalPagesAllowed) {
            NextMd = MemoryDescriptor->ListEntry.Flink;
            continue;
        }

        //
        // This descriptor needs to be removed or truncated.
        //

        if (PageCount - MemoryDescriptor->PageCount >= TotalPagesAllowed) {

            //
            // Completely remove this descriptor.
            //
            // Note since this only adjusts the links and since the entry is
            // not freed, it can still be safely referenced again below to
            // obtain the NextMd.  N.B.  This keeps the memory descriptors
            // sorted in ascending order.
            //

            RemoveEntryList (NextMd);
        }
        else {

            //
            // Truncate this descriptor.
            //

            ASSERT (PageCount - MemoryDescriptor->PageCount < TotalPagesAllowed);
            MemoryDescriptor->PageCount -= (ULONG)(PageCount - TotalPagesAllowed);
            PageCount = TotalPagesAllowed;
        }

        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    return;
}


VOID
MmFreeLoaderBlock (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    )

/*++

Routine Description:

    This function is called as the last routine in phase 1 initialization.
    It frees memory used by the OsLoader.

Arguments:

    LoaderBlock - Supplies a pointer to the system loader block.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/

{
    PLIST_ENTRY NextMd;
    PMEMORY_ALLOCATION_DESCRIPTOR MemoryDescriptor;
    ULONG i;
    PFN_NUMBER NextPhysicalPage;
    PFN_NUMBER PagesFreed;
    PMMPFN Pfn1;
    KIRQL OldIrql;
    PPHYSICAL_MEMORY_RUN RunBase;
    PPHYSICAL_MEMORY_RUN Runs;

    i = 0;
    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {
        i += 1;
        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);
        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    RunBase = ExAllocatePoolWithTag (NonPagedPool,
                                     sizeof(PHYSICAL_MEMORY_RUN) * i,
                                     'lMmM');

    if (RunBase == NULL) {
        return;
    }

    Runs = RunBase;

    //
    //
    // Walk through the memory descriptors and add pages to the
    // free list in the PFN database.
    //

    NextMd = LoaderBlock->MemoryDescriptorListHead.Flink;

    while (NextMd != &LoaderBlock->MemoryDescriptorListHead) {

        MemoryDescriptor = CONTAINING_RECORD(NextMd,
                                             MEMORY_ALLOCATION_DESCRIPTOR,
                                             ListEntry);


        switch (MemoryDescriptor->MemoryType) {
            case LoaderOsloaderHeap:
            case LoaderRegistryData:
            case LoaderNlsData:
            //case LoaderMemoryData:  //this has page table and other stuff.

                //
                // Capture the data to temporary storage so we won't
                // free memory we are referencing.
                //

                Runs->BasePage = MemoryDescriptor->BasePage;
                Runs->PageCount = MemoryDescriptor->PageCount;
                Runs += 1;

                break;

            default:

                break;
        }

        NextMd = MemoryDescriptor->ListEntry.Flink;
    }

    PagesFreed = 0;

    LOCK_PFN (OldIrql);

    if (Runs != RunBase) {
        Runs -= 1;
        do {
            i = (ULONG)Runs->PageCount;
            NextPhysicalPage = Runs->BasePage;

#if defined (_MI_MORE_THAN_4GB_)
            if (MiNoLowMemory != 0) {
                if (NextPhysicalPage < MiNoLowMemory) {

                    //
                    // Don't free this run as it is below the memory threshold
                    // configured for this system.
                    //

                    Runs -= 1;
                    continue;
                }
            }
#endif

            Pfn1 = MI_PFN_ELEMENT (NextPhysicalPage);
            PagesFreed += i;
            while (i != 0) {

                if (Pfn1->u3.e2.ReferenceCount == 0) {
                    if (Pfn1->u1.Flink == 0) {

                        //
                        // Set the PTE address to the physical page for
                        // virtual address alignment checking.
                        //

                        Pfn1->PteAddress =
                                   (PMMPTE)(NextPhysicalPage << PTE_SHIFT);

                        MiDetermineNode (NextPhysicalPage, Pfn1);

                        MiInsertPageInFreeList (NextPhysicalPage);
                    }
                }
                else {

                    if (NextPhysicalPage != 0) {

                        //
                        // Remove PTE and insert into the free list.  If it is
                        // a physical address within the PFN database, the PTE
                        // element does not exist and therefore cannot be
                        // updated.
                        //

                        if (!MI_IS_PHYSICAL_ADDRESS (
                                MiGetVirtualAddressMappedByPte (Pfn1->PteAddress))) {

                            //
                            // Not a physical address.
                            //

                            *(Pfn1->PteAddress) = ZeroPte;
                        }

                        MI_SET_PFN_DELETED (Pfn1);
                        MiDecrementShareCount (Pfn1, NextPhysicalPage);
                    }
                }

                Pfn1 += 1;
                i -= 1;
                NextPhysicalPage += 1;
            }
            Runs -= 1;
        } while (Runs >= RunBase);
    }

#if defined(_X86_)

    if (MmVirtualBias != 0) {

        //
        // If the kernel has been biased to allow for 3gb of user address space,
        // then the first 16mb of memory is doubly mapped to KSEG0_BASE and to
        // ALTERNATE_BASE. Therefore, the KSEG0_BASE entries must be unmapped.
        //

        PMMPTE Pde;
        ULONG NumberOfPdes;

        NumberOfPdes = MmBootImageSize / MM_VA_MAPPED_BY_PDE;

        Pde = MiGetPdeAddress((PVOID)KSEG0_BASE);

        for (i = 0; i < NumberOfPdes; i += 1) {
            MI_WRITE_INVALID_PTE (Pde, ZeroKernelPte);
            Pde += 1;
        }
    }

#endif

    KeFlushEntireTb (TRUE, TRUE);

    UNLOCK_PFN (OldIrql);

    ExFreePool (RunBase);

    //
    // Since systemwide commitment was determined early in Phase 0 and
    // excluded the ranges just freed, add them back in now.
    //

    if (PagesFreed != 0) {
        InterlockedExchangeAddSizeT (&MmTotalCommitLimitMaximum, PagesFreed);
        InterlockedExchangeAddSizeT (&MmTotalCommitLimit, PagesFreed);
    }

    return;
}

VOID
MiBuildPagedPool (
    VOID
    )

/*++

Routine Description:

    This function is called to build the structures required for paged
    pool and initialize the pool.  Once this routine is called, paged
    pool may be allocated.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/

{
    SIZE_T Size;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE LastPde;
    PMMPTE PointerPde;
    MMPTE TempPte;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER ContainingFrame;
    SIZE_T AdditionalCommittedPages;
    KIRQL OldIrql;
    ULONG i;
#if (_MI_PAGING_LEVELS >= 4)
    PMMPTE PointerPxe;
    PMMPTE PointerPxeEnd;
#endif
#if (_MI_PAGING_LEVELS >= 3)
    PVOID LastVa;
    PMMPTE PointerPpe;
    PMMPTE PointerPpeEnd;
#else
    PMMPFN Pfn1;
#endif

    i = 0;
    AdditionalCommittedPages = 0;

#if (_MI_PAGING_LEVELS < 3)

    //
    // Double map system page directory page.
    //

    PointerPte = MiGetPteAddress(PDE_BASE);

    for (i = 0 ; i < PD_PER_SYSTEM; i += 1) {
        MmSystemPageDirectory[i] = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
        Pfn1 = MI_PFN_ELEMENT(MmSystemPageDirectory[i]);
        Pfn1->OriginalPte.u.Long = MM_DEMAND_ZERO_WRITE_PTE;
        PointerPte += 1;
    }

    //
    // Was not mapped physically, map it virtually in system space.
    //

    PointerPte = MiReserveSystemPtes (PD_PER_SYSTEM, SystemPteSpace);

    if (PointerPte == NULL) {
        MiIssueNoPtesBugcheck (PD_PER_SYSTEM, SystemPteSpace);
    }

    MmSystemPagePtes = (PMMPTE)MiGetVirtualAddressMappedByPte (PointerPte);

    TempPte = ValidKernelPde;

    for (i = 0 ; i < PD_PER_SYSTEM; i += 1) {
        TempPte.u.Hard.PageFrameNumber = MmSystemPageDirectory[i];
        MI_WRITE_VALID_PTE (PointerPte, TempPte);
        PointerPte += 1;
    }

#endif

    if (MmPagedPoolMaximumDesired == TRUE) {
        MmSizeOfPagedPoolInBytes =
                    ((PCHAR)MmNonPagedSystemStart - (PCHAR)MmPagedPoolStart);
    }
    else if (MmSizeOfPagedPoolInBytes == 0) {

        //
        // A size of 0 means size the pool based on physical memory.
        //

        MmSizeOfPagedPoolInBytes = 2 * MmMaximumNonPagedPoolInBytes;
#if (_MI_PAGING_LEVELS >= 3)
        MmSizeOfPagedPoolInBytes *= 2;
#endif
    }

    if (MmIsThisAnNtAsSystem()) {
        if ((MmNumberOfPhysicalPages > ((24*1024*1024) >> PAGE_SHIFT)) &&
            (MmSizeOfPagedPoolInBytes < MM_MINIMUM_PAGED_POOL_NTAS)) {

            MmSizeOfPagedPoolInBytes = MM_MINIMUM_PAGED_POOL_NTAS;
        }
    }

    if (MmSizeOfPagedPoolInBytes >
              (ULONG_PTR)((PCHAR)MmNonPagedSystemStart - (PCHAR)MmPagedPoolStart)) {
        MmSizeOfPagedPoolInBytes =
                    ((PCHAR)MmNonPagedSystemStart - (PCHAR)MmPagedPoolStart);
    }

    Size = BYTES_TO_PAGES(MmSizeOfPagedPoolInBytes);

    if (Size < MM_MIN_INITIAL_PAGED_POOL) {
        Size = MM_MIN_INITIAL_PAGED_POOL;
    }

    if (Size > (MM_MAX_PAGED_POOL >> PAGE_SHIFT)) {
        Size = MM_MAX_PAGED_POOL >> PAGE_SHIFT;
    }

#if defined (_WIN64)

    //
    // NT64 places system mapped views directly after paged pool.  Ensure
    // enough VA space is available.
    //

    if (Size + (MmSystemViewSize >> PAGE_SHIFT) > (MM_MAX_PAGED_POOL >> PAGE_SHIFT)) {
        ASSERT (MmSizeOfPagedPoolInBytes > 2 * MmSystemViewSize);
        MmSizeOfPagedPoolInBytes -= MmSystemViewSize;
        Size = BYTES_TO_PAGES(MmSizeOfPagedPoolInBytes);
    }
#endif

    Size = (Size + (PTE_PER_PAGE - 1)) / PTE_PER_PAGE;
    MmSizeOfPagedPoolInBytes = (ULONG_PTR)Size * PAGE_SIZE * PTE_PER_PAGE;

    //
    // Set size to the number of pages in the pool.
    //

    Size = Size * PTE_PER_PAGE;

    //
    // If paged pool is really nonpagable then limit the size based
    // on how much physical memory is actually present.  Disable this
    // feature if not enough physical memory is present to do it.
    //

    if (MmDisablePagingExecutive & MM_PAGED_POOL_LOCKED_DOWN) {

        Size = MmSizeOfPagedPoolInBytes / PAGE_SIZE;

        if ((MI_NONPAGABLE_MEMORY_AVAILABLE() < 2048) ||
            (MmAvailablePages < 2048)) {
                Size = 0;
        }
        else {
            if ((SPFN_NUMBER)(Size) > MI_NONPAGABLE_MEMORY_AVAILABLE() - 2048) {
                Size = (MI_NONPAGABLE_MEMORY_AVAILABLE() - 2048);
            }

            if (Size > MmAvailablePages - 2048) {
                Size = MmAvailablePages - 2048;
            }
        }

        Size = ((Size * PAGE_SIZE) / MM_VA_MAPPED_BY_PDE) * MM_VA_MAPPED_BY_PDE;

        if ((((Size / 5) * 4) >= MmSizeOfPagedPoolInBytes) &&
            (Size >= MM_MIN_INITIAL_PAGED_POOL)) {

            MmSizeOfPagedPoolInBytes = Size;
        }
        else {
            MmDisablePagingExecutive &= ~MM_PAGED_POOL_LOCKED_DOWN;
        }

        Size = MmSizeOfPagedPoolInBytes >> PAGE_SHIFT;
    }

    MmSizeOfPagedPoolInPages = MmSizeOfPagedPoolInBytes >> PAGE_SHIFT;

    ASSERT ((MmSizeOfPagedPoolInBytes + (PCHAR)MmPagedPoolStart) <=
            (PCHAR)MmNonPagedSystemStart);

    ASSERT64 ((MmSizeOfPagedPoolInBytes + (PCHAR)MmPagedPoolStart + MmSystemViewSize) <=
              (PCHAR)MmNonPagedSystemStart);

    MmPagedPoolEnd = (PVOID)(((PUCHAR)MmPagedPoolStart +
                            MmSizeOfPagedPoolInBytes) - 1);

    MmPageAlignedPoolBase[PagedPool] = MmPagedPoolStart;

    //
    // Build page table page for paged pool.
    //

    PointerPde = MiGetPdeAddress (MmPagedPoolStart);

    TempPte = ValidKernelPde;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Map in all the page directory pages to span all of paged pool.
    // This removes the need for a system lookup directory.
    //

    LastVa = (PVOID)((PCHAR)MmPagedPoolEnd + MmSystemViewSize);
    PointerPpe = MiGetPpeAddress (MmPagedPoolStart);
    PointerPpeEnd = MiGetPpeAddress (LastVa);

    MiSystemViewStart = (ULONG_PTR)MmPagedPoolEnd + 1;

    PointerPde = MiGetPdeAddress (MmPagedPoolEnd) + 1;
    LastPde = MiGetPdeAddress (LastVa);

    LOCK_PFN (OldIrql);

#if (_MI_PAGING_LEVELS >= 4)
    PointerPxe = MiGetPxeAddress (MmPagedPoolStart);
    PointerPxeEnd = MiGetPxeAddress (LastVa);

    while (PointerPxe <= PointerPxeEnd) {

        if (PointerPxe->u.Hard.Valid == 0) {
            PageFrameIndex = MiRemoveAnyPage(
                                     MI_GET_PAGE_COLOR_FROM_PTE (PointerPxe));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPxe, TempPte);

            MiInitializePfn (PageFrameIndex, PointerPxe, 1);

            //
            // Make all entries no access since the PDEs may not fill the page.
            //

            MiFillMemoryPte (MiGetVirtualAddressMappedByPte (PointerPxe),
                             PAGE_SIZE / sizeof (MMPTE),
                             MM_KERNEL_NOACCESS_PTE);

            MmResidentAvailablePages -= 1;
            AdditionalCommittedPages += 1;
        }

        PointerPxe += 1;
    }
#endif

    while (PointerPpe <= PointerPpeEnd) {

        if (PointerPpe->u.Hard.Valid == 0) {
            PageFrameIndex = MiRemoveAnyPage(
                                     MI_GET_PAGE_COLOR_FROM_PTE (PointerPpe));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPpe, TempPte);

            MiInitializePfn (PageFrameIndex, PointerPpe, 1);

            //
            // Make all entries no access since the PDEs may not fill the page.
            //

            MiFillMemoryPte (MiGetVirtualAddressMappedByPte (PointerPpe),
                             PAGE_SIZE / sizeof (MMPTE),
                             MM_KERNEL_NOACCESS_PTE);

            MmResidentAvailablePages -= 1;
            AdditionalCommittedPages += 1;
        }

        PointerPpe += 1;
    }

    //
    // Initialize the system view page table pages.
    //

    MmResidentAvailablePages -= (LastPde - PointerPde + 1);
    AdditionalCommittedPages += (LastPde - PointerPde + 1);

    while (PointerPde <= LastPde) {

        ASSERT (PointerPde->u.Hard.Valid == 0);

        PageFrameIndex = MiRemoveAnyPage(
                            MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
        TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
        MI_WRITE_VALID_PTE (PointerPde, TempPte);

        MiInitializePfn (PageFrameIndex, PointerPde, 1);

        KeZeroPages (MiGetVirtualAddressMappedByPte (PointerPde), PAGE_SIZE);

        PointerPde += 1;
    }

    UNLOCK_PFN (OldIrql);

    PointerPde = MiGetPdeAddress (MmPagedPoolStart);

#endif

    PointerPte = MiGetPteAddress (MmPagedPoolStart);
    MmPagedPoolInfo.FirstPteForPagedPool = PointerPte;
    MmPagedPoolInfo.LastPteForPagedPool = MiGetPteAddress (MmPagedPoolEnd);

    MiFillMemoryPte (PointerPde,
                     (1 + MiGetPdeAddress (MmPagedPoolEnd) - PointerPde),
                     MM_KERNEL_NOACCESS_PTE);

    LOCK_PFN (OldIrql);

    //
    // Map in a page table page.
    //

    PageFrameIndex = MiRemoveAnyPage (MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));

    TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
    MI_WRITE_VALID_PTE (PointerPde, TempPte);

#if (_MI_PAGING_LEVELS >= 3)
    ContainingFrame = MI_GET_PAGE_FRAME_FROM_PTE(MiGetPpeAddress (MmPagedPoolStart));
#else
    ContainingFrame = MmSystemPageDirectory[(PointerPde - MiGetPdeAddress(0)) / PDE_PER_PAGE];
#endif

    MiInitializePfnForOtherProcess (PageFrameIndex,
                                    PointerPde,
                                    ContainingFrame);

    MiFillMemoryPte (PointerPte, PAGE_SIZE / sizeof (MMPTE), MM_KERNEL_NOACCESS_PTE);

    MmResidentAvailablePages -= 1;
    AdditionalCommittedPages += 1;

    UNLOCK_PFN (OldIrql);

    MmPagedPoolInfo.NextPdeForPagedPoolExpansion = PointerPde + 1;

    //
    // Build bitmaps for paged pool.
    //

    MiCreateBitMap (&MmPagedPoolInfo.PagedPoolAllocationMap, Size, NonPagedPool);
    RtlSetAllBits (MmPagedPoolInfo.PagedPoolAllocationMap);

    //
    // Indicate first page worth of PTEs are available.
    //

    RtlClearBits (MmPagedPoolInfo.PagedPoolAllocationMap, 0, PTE_PER_PAGE);

    MiCreateBitMap (&MmPagedPoolInfo.EndOfPagedPoolBitmap, Size, NonPagedPool);
    RtlClearAllBits (MmPagedPoolInfo.EndOfPagedPoolBitmap);

    //
    // If verifier is present then build the verifier paged pool bitmap.
    //

    if (MmVerifyDriverBufferLength != (ULONG)-1) {
        MiCreateBitMap (&VerifierLargePagedPoolMap, Size, NonPagedPool);
        RtlClearAllBits (VerifierLargePagedPoolMap);
    }

    //
    // Initialize paged pool.
    //

    InitializePool (PagedPool, 0L);

    //
    // If paged pool is really nonpagable then allocate the memory now.
    //

    if (MmDisablePagingExecutive & MM_PAGED_POOL_LOCKED_DOWN) {

        PointerPde = MiGetPdeAddress (MmPagedPoolStart);
        PointerPde += 1;
        LastPde = MiGetPdeAddress (MmPagedPoolEnd);
        TempPte = ValidKernelPde;

        PointerPte = MiGetPteAddress (MmPagedPoolStart);
        LastPte = MiGetPteAddress (MmPagedPoolEnd);

        ASSERT (MmPagedPoolCommit == 0);
        MmPagedPoolCommit = (ULONG)(LastPte - PointerPte + 1);

        ASSERT (MmPagedPoolInfo.PagedPoolCommit == 0);
        MmPagedPoolInfo.PagedPoolCommit = MmPagedPoolCommit;

#if DBG
        //
        // Ensure no paged pool has been allocated yet.
        //

        for (i = 0; i < PTE_PER_PAGE; i += 1) {
            ASSERT (!RtlCheckBit (MmPagedPoolInfo.PagedPoolAllocationMap, i));
        }

        while (i < MmSizeOfPagedPoolInBytes / PAGE_SIZE) {
            ASSERT (RtlCheckBit (MmPagedPoolInfo.PagedPoolAllocationMap, i));
            i += 1;
        }
#endif

        RtlClearAllBits (MmPagedPoolInfo.PagedPoolAllocationMap);

        LOCK_PFN (OldIrql);

        //
        // Map in the page table pages.
        //

        MmResidentAvailablePages -= (LastPde - PointerPde + 1);
        AdditionalCommittedPages += (LastPde - PointerPde + 1);

        while (PointerPde <= LastPde) {

            ASSERT (PointerPde->u.Hard.Valid == 0);

            PageFrameIndex = MiRemoveAnyPage(
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPde));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPde, TempPte);

#if (_MI_PAGING_LEVELS >= 3)
            ContainingFrame = MI_GET_PAGE_FRAME_FROM_PTE(MiGetPteAddress (PointerPde));
#else
            ContainingFrame = MmSystemPageDirectory[(PointerPde - MiGetPdeAddress(0)) / PDE_PER_PAGE];
#endif

            MiInitializePfnForOtherProcess (PageFrameIndex,
                                            MiGetPteAddress (PointerPde),
                                            ContainingFrame);

            MiFillMemoryPte (MiGetVirtualAddressMappedByPte (PointerPde),
                             PAGE_SIZE / sizeof (MMPTE),
                             MM_KERNEL_NOACCESS_PTE);

            PointerPde += 1;
        }

        MmPagedPoolInfo.NextPdeForPagedPoolExpansion = PointerPde;

        TempPte = ValidKernelPte;
        MI_SET_PTE_DIRTY (TempPte);

        ASSERT (MmAvailablePages > (PFN_COUNT)(LastPte - PointerPte + 1));
        ASSERT (MmResidentAvailablePages > (SPFN_NUMBER)(LastPte - PointerPte + 1));
        MmResidentAvailablePages -= (LastPte - PointerPte + 1);
        AdditionalCommittedPages += (LastPte - PointerPte + 1);

        while (PointerPte <= LastPte) {

            ASSERT (PointerPte->u.Hard.Valid == 0);

            PageFrameIndex = MiRemoveAnyPage(
                                    MI_GET_PAGE_COLOR_FROM_PTE (PointerPte));
            TempPte.u.Hard.PageFrameNumber = PageFrameIndex;
            MI_WRITE_VALID_PTE (PointerPte, TempPte);

            MiInitializePfn (PageFrameIndex, PointerPte, 1);

            PointerPte += 1;
        }

        UNLOCK_PFN (OldIrql);
    }

    //
    // Since the commitment return path is lock free, the total committed
    // page count must be atomically incremented.
    //

    InterlockedExchangeAddSizeT (&MmTotalCommittedPages, AdditionalCommittedPages);

    MiInitializeSpecialPool (NonPagedPool);

    //
    // Initialize the default paged pool signaling thresholds.
    //

    MiLowPagedPoolThreshold = (30 * 1024 * 1024) >> PAGE_SHIFT;

    if ((Size / 5) < MiLowPagedPoolThreshold) {
        MiLowPagedPoolThreshold = Size / 5;
    }

    MiHighPagedPoolThreshold = (60 * 1024 * 1024) >> PAGE_SHIFT;

    if (((Size * 2) / 5) < MiHighPagedPoolThreshold) {
        MiHighPagedPoolThreshold = (Size * 2) / 5;
    }

    ASSERT (MiLowPagedPoolThreshold < MiHighPagedPoolThreshold);

    //
    // Allow mapping of views into system space.
    //

    MiInitializeSystemSpaceMap (NULL);

    return;
}


VOID
MiInitializeNonPagedPoolThresholds (
    VOID
    )

/*++

Routine Description:

    This function is called to initialize the default nonpaged pool
    signaling thresholds.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel Mode Only.  System initialization.

--*/

{
    PFN_NUMBER Size;

    Size = MmMaximumNonPagedPoolInPages;

    //
    // Initialize the default nonpaged pool signaling thresholds.
    //

    MiLowNonPagedPoolThreshold = (8 * 1024 * 1024) >> PAGE_SHIFT;

    if ((Size / 3) < MiLowNonPagedPoolThreshold) {
        MiLowNonPagedPoolThreshold = Size / 3;
    }

    MiHighNonPagedPoolThreshold = (20 * 1024 * 1024) >> PAGE_SHIFT;

    if ((Size / 2) < MiHighNonPagedPoolThreshold) {
        MiHighNonPagedPoolThreshold = Size / 2;
    }

    ASSERT (MiLowNonPagedPoolThreshold < MiHighNonPagedPoolThreshold);

    return;
}


VOID
MiFindInitializationCode (
    OUT PVOID *StartVa,
    OUT PVOID *EndVa
    )

/*++

Routine Description:

    This function locates the start and end of the initialization code for
    each loaded module list entry.  This code resides in the INIT section
    of each image.

Arguments:

    StartVa - Returns the starting address of the init section.

    EndVa - Returns the ending address of the init section.

Return Value:

    None.

Environment:

    Kernel Mode Only.  End of system initialization.

--*/

{
    ULONG Span;
    PKLDR_DATA_TABLE_ENTRY LdrDataTableEntry;
    PVOID CurrentBase;
    PVOID InitStart;
    PVOID InitEnd;
    PLIST_ENTRY Next;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_SECTION_HEADER SectionTableEntry;
    PIMAGE_SECTION_HEADER LastDiscard;
    LONG i;
    LOGICAL DiscardSection;
    PVOID MiFindInitializationCodeAddress;
    PKTHREAD CurrentThread;
    UNICODE_STRING NameString;

    MiFindInitializationCodeAddress = MmGetProcedureAddress((PVOID)(ULONG_PTR)&MiFindInitializationCode);

#if defined(_IA64_)

    //
    // One more indirection is needed due to the PLABEL.
    //

    MiFindInitializationCodeAddress = (PVOID)(*((PULONGLONG)MiFindInitializationCodeAddress));

#endif

    *StartVa = NULL;

    //
    // Walk through the loader blocks looking for the base which
    // contains this routine.
    //

    CurrentThread = KeGetCurrentThread ();
    KeEnterCriticalRegionThread (CurrentThread);

    //
    // Acquire the load lock to ensure that we don't slice into a load
    // in progress (ie: fixups on INIT code may be ongoing) on a driver
    // already in the list.
    //

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);

    Next = PsLoadedModuleList.Flink;

    while (Next != &PsLoadedModuleList) {
        LdrDataTableEntry = CONTAINING_RECORD (Next,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

        if (LdrDataTableEntry->Flags & LDRP_MM_LOADED) {

            //
            // This entry was loaded by MmLoadSystemImage so it's already
            // had its init section removed.
            //

            Next = Next->Flink;
            continue;
        }

        CurrentBase = (PVOID)LdrDataTableEntry->DllBase;
        NtHeader = RtlImageNtHeader (CurrentBase);

        if (NtHeader == NULL) {
            Next = Next->Flink;
            continue;
        }

        SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                                sizeof(ULONG) +
                                sizeof(IMAGE_FILE_HEADER) +
                                NtHeader->FileHeader.SizeOfOptionalHeader);

        //
        // From the image header, locate the sections named 'INIT',
        // PAGEVRF* and PAGESPEC.  INIT always goes, the others go depending
        // on registry configuration.
        //

        i = NtHeader->FileHeader.NumberOfSections;

        InitStart = NULL;
        while (i > 0) {

#if DBG
            if ((*(PULONG)SectionTableEntry->Name == 'tini') ||
                (*(PULONG)SectionTableEntry->Name == 'egap')) {
                DbgPrint("driver %wZ has lower case sections (init or pagexxx)\n",
                    &LdrDataTableEntry->FullDllName);
            }
#endif

            DiscardSection = FALSE;

            //
            // Free any INIT sections (or relocation sections that haven't
            // been already).  Note a driver may have a relocation section
            // but not have any INIT code.
            //

            if ((*(PULONG)SectionTableEntry->Name == 'TINI') ||
                ((SectionTableEntry->Characteristics & IMAGE_SCN_MEM_DISCARDABLE) != 0)) {
                DiscardSection = TRUE;
            }
            else if ((*(PULONG)SectionTableEntry->Name == 'EGAP') &&
                     (SectionTableEntry->Name[4] == 'V') &&
                     (SectionTableEntry->Name[5] == 'R') &&
                     (SectionTableEntry->Name[6] == 'F')) {

                //
                // Discard PAGEVRF* if no drivers are being instrumented.
                //

                if (MmVerifyDriverBufferLength == (ULONG)-1) {
                    DiscardSection = TRUE;
                }
            }
            else if ((*(PULONG)SectionTableEntry->Name == 'EGAP') &&
                (*(PULONG)&SectionTableEntry->Name[4] == 'CEPS')) {

                //
                // Discard PAGESPEC special pool code if it's not enabled.
                //

                if (MiSpecialPoolFirstPte == NULL) {
                    DiscardSection = TRUE;
                }
            }

            if (DiscardSection == TRUE) {

                InitStart = (PVOID)((PCHAR)CurrentBase + SectionTableEntry->VirtualAddress);
                //
                // Generally, SizeOfRawData is larger than VirtualSize for each
                // section because it includes the padding to get to the
                // subsection alignment boundary.  However, if the image is
                // linked with subsection alignment == native page alignment,
                // the linker will have VirtualSize be much larger than
                // SizeOfRawData because it will account for all the bss.
                //

                Span = SectionTableEntry->SizeOfRawData;

                if (Span < SectionTableEntry->Misc.VirtualSize) {
                    Span = SectionTableEntry->Misc.VirtualSize;
                }
                InitEnd = (PVOID)((PCHAR)InitStart + Span - 1);
                InitEnd = (PVOID)((PCHAR)PAGE_ALIGN ((PCHAR)InitEnd +
                        (NtHeader->OptionalHeader.SectionAlignment - 1)) - 1);
                InitStart = (PVOID)ROUND_TO_PAGES (InitStart);

                //
                // Check if more sections are discardable after this one so
                // even small INIT sections can be discarded.
                //

                if (i == 1) {
                    LastDiscard = SectionTableEntry;
                }
                else {
                    LastDiscard = NULL;
                    do {
                        i -= 1;
                        SectionTableEntry += 1;

                        if ((SectionTableEntry->Characteristics &
                             IMAGE_SCN_MEM_DISCARDABLE) != 0) {

                            //
                            // Discard this too.
                            //

                            LastDiscard = SectionTableEntry;
                        }
                        else {
                            break;
                        }
                    } while (i > 1);
                }

                if (LastDiscard) {
                    //
                    // Generally, SizeOfRawData is larger than VirtualSize for each
                    // section because it includes the padding to get to the subsection
                    // alignment boundary.  However, if the image is linked with
                    // subsection alignment == native page alignment, the linker will
                    // have VirtualSize be much larger than SizeOfRawData because it
                    // will account for all the bss.
                    //

                    Span = LastDiscard->SizeOfRawData;

                    if (Span < LastDiscard->Misc.VirtualSize) {
                        Span = LastDiscard->Misc.VirtualSize;
                    }

                    InitEnd = (PVOID)(((PCHAR)CurrentBase +
                                       LastDiscard->VirtualAddress) +
                                      (Span - 1));

                    //
                    // If this isn't the last section in the driver then the
                    // the next section is not discardable.  So the last
                    // section is not rounded down, but all others must be.
                    //

                    if (i != 1) {
                        InitEnd = (PVOID)((PCHAR)PAGE_ALIGN ((PCHAR)InitEnd +
                                                             (NtHeader->OptionalHeader.SectionAlignment - 1)) - 1);
                    }
                }

                if (InitEnd > (PVOID)((PCHAR)CurrentBase +
                                      LdrDataTableEntry->SizeOfImage)) {
                    InitEnd = (PVOID)(((ULONG_PTR)CurrentBase +
                                       (LdrDataTableEntry->SizeOfImage - 1)) |
                                      (PAGE_SIZE - 1));
                }

                if (InitStart <= InitEnd) {
                    if ((MiFindInitializationCodeAddress >= InitStart) &&
                        (MiFindInitializationCodeAddress <= InitEnd)) {

                        //
                        // This init section is in the kernel, don't free it
                        // now as it would free this code!
                        //

                        ASSERT (*StartVa == NULL);
                        *StartVa = InitStart;
                        *EndVa = InitEnd;
                    }
                    else {

                        //
                        // Don't free the INIT code for a driver mapped by
                        // large pages because if it unloads later, we'd have
                        // to deal with discontiguous ranges of pages to free.
                        //
                        // Make a special exception for the kernel & HAL
                        // since those never unload.
                        //

                        if (MI_IS_PHYSICAL_ADDRESS (InitStart)) {

                            NameString.Buffer = (const PUSHORT) KERNEL_NAME;
                            NameString.Length = sizeof (KERNEL_NAME) - sizeof (WCHAR);
                            NameString.MaximumLength = sizeof KERNEL_NAME;

                            if (!RtlEqualUnicodeString (&NameString,
                                                        &LdrDataTableEntry->BaseDllName,
                                                        TRUE)) {
                                MiFreeInitializationCode (InitStart, InitEnd);
                            }

                            NameString.Buffer = (const PUSHORT) HAL_NAME;
                            NameString.Length = sizeof (HAL_NAME) - sizeof (WCHAR);
                            NameString.MaximumLength = sizeof HAL_NAME;

                            if (!RtlEqualUnicodeString (&NameString,
                                                        &LdrDataTableEntry->BaseDllName,
                                                        TRUE)) {
                                MiFreeInitializationCode (InitStart, InitEnd);
                            }
                        }
                        else {
                            MiFreeInitializationCode (InitStart, InitEnd);
                        }
                    }
                }
            }
            i -= 1;
            SectionTableEntry += 1;
        }
        Next = Next->Flink;
    }

    ExReleaseResourceLite (&PsLoadedModuleResource);

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);

    KeLeaveCriticalRegionThread (CurrentThread);

    return;
}


VOID
MiFreeInitializationCode (
    IN PVOID StartVa,
    IN PVOID EndVa
    )

/*++

Routine Description:

    This function is called to delete the initialization code for each
    loaded module list entry.

Arguments:

    StartVa - Supplies the starting address of the range to delete.

    EndVa - Supplies the ending address of the range to delete.

Return Value:

    None.

Environment:

    Kernel Mode Only.  Runs after system initialization.

--*/

{
    PMMPTE PointerPte;
    PFN_NUMBER PagesFreed;
    PFN_NUMBER PageFrameIndex;
    KIRQL OldIrql;
    PMMPFN Pfn1;

    ASSERT (ExPageLockHandle);

#if defined (_MI_MORE_THAN_4GB_)
    if (MiNoLowMemory != 0) {

        //
        // Don't free this range as the kernel is always below the memory
        // threshold configured for this system.
        //

        return;
    }
#endif

    if (MI_IS_PHYSICAL_ADDRESS (StartVa)) {

        PagesFreed = 0;
        MmLockPagableSectionByHandle (ExPageLockHandle);
        LOCK_PFN (OldIrql);

        while (StartVa < EndVa) {

            //
            // On certain architectures (e.g., IA64) virtual addresses
            // may be physical and hence have no corresponding PTE.
            //

            PageFrameIndex = MI_CONVERT_PHYSICAL_TO_PFN (StartVa);

            Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
            Pfn1->u2.ShareCount = 0;
            Pfn1->u3.e2.ReferenceCount = 0;
            MI_SET_PFN_DELETED (Pfn1);
            MiInsertPageInFreeList (PageFrameIndex);
            StartVa = (PVOID)((PUCHAR)StartVa + PAGE_SIZE);
            PagesFreed += 1;
        }

        UNLOCK_PFN (OldIrql);
        MmUnlockPagableImageSection (ExPageLockHandle);
    }
    else {
        PointerPte = MiGetPteAddress (StartVa);

        PagesFreed = (PFN_NUMBER) (1 + MiGetPteAddress (EndVa) - PointerPte);

        PagesFreed = MiDeleteSystemPagableVm (PointerPte,
                                              PagesFreed,
                                              ZeroKernelPte,
                                              FALSE,
                                              NULL);
    }

    if (PagesFreed != 0) {
        MiReturnCommitment (PagesFreed);
        MI_INCREMENT_RESIDENT_AVAILABLE (PagesFreed,
                                         MM_RESAVAIL_FREE_UNLOAD_SYSTEM_IMAGE1);
    }

    return;
}


VOID
MiEnablePagingTheExecutive (
    VOID
    )

/*++

Routine Description:

    This function locates the start and end of the pagable code for
    each loaded module entry.  This code resides in the PAGE section of
    each image.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel Mode Only.  End of system initialization.

--*/

{
    ULONG Span;
    KIRQL OldIrql;
    PVOID StartVa;
    PETHREAD CurrentThread;
    PLONG SectionLockCountPointer;
    PKLDR_DATA_TABLE_ENTRY LdrDataTableEntry;
    PVOID CurrentBase;
    PLIST_ENTRY Next;
    PIMAGE_NT_HEADERS NtHeader;
    PIMAGE_SECTION_HEADER StartSectionTableEntry;
    PIMAGE_SECTION_HEADER SectionTableEntry;
    LONG i;
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE SubsectionStartPte;
    PMMPTE SubsectionLastPte;
    LOGICAL PageSection;
    PVOID SectionBaseAddress;
    LOGICAL AlreadyLockedOnce;
    ULONG Waited;

    //
    // Don't page kernel mode code if customer does not want it paged or if
    // this is a diskless remote boot client.
    //

    if (MmDisablePagingExecutive & MM_SYSTEM_CODE_LOCKED_DOWN) {
        return;
    }

#if defined(REMOTE_BOOT)
    if (IoRemoteBootClient && IoCscInitializationFailed) {
        return;
    }
#endif

    //
    // Initializing LastPte is not needed for correctness, but
    // without it the compiler cannot compile this code W4 to check
    // for use of uninitialized variables.
    //

    LastPte = NULL;

    //
    // Walk through the loader blocks looking for the base which
    // contains this routine.
    //

    CurrentThread = PsGetCurrentThread ();

    KeEnterCriticalRegionThread (&CurrentThread->Tcb);

    //
    // Acquire the load lock to ensure that we don't slice into a load
    // in progress (ie: fixups on INIT code may be ongoing) on a driver
    // already in the list.
    //

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);

    ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);

    Next = PsLoadedModuleList.Flink;

    while (Next != &PsLoadedModuleList) {

        LdrDataTableEntry = CONTAINING_RECORD (Next,
                                               KLDR_DATA_TABLE_ENTRY,
                                               InLoadOrderLinks);

        if (LdrDataTableEntry->SectionPointer != NULL) {

            //
            // This entry was loaded by MmLoadSystemImage so it's already paged.
            //

            Next = Next->Flink;
            continue;
        }

        CurrentBase = (PVOID)LdrDataTableEntry->DllBase;

        if ((MI_IS_PHYSICAL_ADDRESS (CurrentBase)) ||
            (MI_PDE_MAPS_LARGE_PAGE (MiGetPdeAddress (CurrentBase)))) {

            //
            // Mapped physically, can't be paged.
            //

            Next = Next->Flink;
            continue;
        }

        NtHeader = RtlImageNtHeader (CurrentBase);

        if (NtHeader == NULL) {
            Next = Next->Flink;
            continue;
        }

restart:

        StartSectionTableEntry = NULL;
        SectionTableEntry = (PIMAGE_SECTION_HEADER)((PCHAR)NtHeader +
                                sizeof(ULONG) +
                                sizeof(IMAGE_FILE_HEADER) +
                                NtHeader->FileHeader.SizeOfOptionalHeader);

        //
        // From the image header, locate the section named 'PAGE' or '.edata'.
        //

        i = NtHeader->FileHeader.NumberOfSections;

        PointerPte = NULL;

        while (i > 0) {

            SectionBaseAddress = SECTION_BASE_ADDRESS(SectionTableEntry);

            if ((PUCHAR)SectionBaseAddress ==
                            ((PUCHAR)CurrentBase + SectionTableEntry->VirtualAddress)) {
                AlreadyLockedOnce = TRUE;

                //
                // This subsection has already been locked down (and possibly
                // unlocked as well) at least once.  If it is NOT locked down
                // right now and the pages are not in the system working set
                // then include it in the chunk to be paged.
                //

                SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (SectionTableEntry);

                if (*SectionLockCountPointer == 0) {

                    SubsectionStartPte = MiGetPteAddress ((PVOID)(ROUND_TO_PAGES (
                                  (ULONG_PTR)CurrentBase +
                                  SectionTableEntry->VirtualAddress)));

                    //
                    // Generally, SizeOfRawData is larger than VirtualSize for each
                    // section because it includes the padding to get to the subsection
                    // alignment boundary.  However, if the image is linked with
                    // subsection alignment == native page alignment, the linker will
                    // have VirtualSize be much larger than SizeOfRawData because it
                    // will account for all the bss.
                    //

                    Span = SectionTableEntry->SizeOfRawData;

                    if (Span < SectionTableEntry->Misc.VirtualSize) {
                        Span = SectionTableEntry->Misc.VirtualSize;
                    }

                    SubsectionLastPte = MiGetPteAddress ((PVOID)((ULONG_PTR)CurrentBase +
                                 SectionTableEntry->VirtualAddress +
                                 (NtHeader->OptionalHeader.SectionAlignment - 1) +
                                 Span -
                                 PAGE_SIZE));

                    if (SubsectionLastPte >= SubsectionStartPte) {
                        AlreadyLockedOnce = FALSE;
                    }
                }
            }
            else {
                AlreadyLockedOnce = FALSE;
            }

            PageSection = ((*(PULONG)SectionTableEntry->Name == 'EGAP') ||
                          (*(PULONG)SectionTableEntry->Name == 'ade.')) &&
                           (AlreadyLockedOnce == FALSE);

            if (*(PULONG)SectionTableEntry->Name == 'EGAP' &&
                SectionTableEntry->Name[4] == 'K'  &&
                SectionTableEntry->Name[5] == 'D') {

                //
                // Only pageout PAGEKD if KdPitchDebugger is TRUE.
                //

                PageSection = KdPitchDebugger;
            }

            if ((*(PULONG)SectionTableEntry->Name == 'EGAP') &&
                     (SectionTableEntry->Name[4] == 'V') &&
                     (SectionTableEntry->Name[5] == 'R') &&
                     (SectionTableEntry->Name[6] == 'F')) {

                //
                // Pageout PAGEVRF* if no drivers are being instrumented.
                //

                if (MmVerifyDriverBufferLength != (ULONG)-1) {
                    PageSection = FALSE;
                }
            }

            if ((*(PULONG)SectionTableEntry->Name == 'EGAP') &&
                (*(PULONG)&SectionTableEntry->Name[4] == 'CEPS')) {

                //
                // Pageout PAGESPEC special pool code if it's not enabled.
                //

                if (MiSpecialPoolFirstPte != NULL) {
                    PageSection = FALSE;
                }
            }

            if (PageSection) {

                 //
                 // This section is pagable, save away the start and end.
                 //

                 if (PointerPte == NULL) {

                     //
                     // Previous section was NOT pagable, get the start address.
                     //

                     ASSERT (StartSectionTableEntry == NULL);
                     StartSectionTableEntry = SectionTableEntry;
                     PointerPte = MiGetPteAddress ((PVOID)(ROUND_TO_PAGES (
                                  (ULONG_PTR)CurrentBase +
                                  SectionTableEntry->VirtualAddress)));
                 }

                //
                // Generally, SizeOfRawData is larger than VirtualSize for each
                // section because it includes the padding to get to the subsection
                // alignment boundary.  However, if the image is linked with
                // subsection alignment == native page alignment, the linker will
                // have VirtualSize be much larger than SizeOfRawData because it
                // will account for all the bss.
                //

                Span = SectionTableEntry->SizeOfRawData;

                if (Span < SectionTableEntry->Misc.VirtualSize) {
                    Span = SectionTableEntry->Misc.VirtualSize;
                }

                LastPte = MiGetPteAddress ((PVOID)((ULONG_PTR)CurrentBase +
                             SectionTableEntry->VirtualAddress +
                             (NtHeader->OptionalHeader.SectionAlignment - 1) +
                             Span - PAGE_SIZE));
            }
            else {

                //
                // This section is not pagable, if the previous section was
                // pagable, enable it.
                //

                if (PointerPte != NULL) {

                    ASSERT (StartSectionTableEntry != NULL);
                    LOCK_SYSTEM_WS (CurrentThread);
                    LOCK_PFN (OldIrql);

                    StartVa = PAGE_ALIGN (StartSectionTableEntry);
                    while (StartVa < (PVOID) SectionTableEntry) {

                        Waited = MiMakeSystemAddressValidPfnSystemWs (StartVa, OldIrql);

                        if (Waited != 0) {

                            //
                            // Restart at the top as the locks were released.
                            //

                            UNLOCK_PFN (OldIrql);
                            UNLOCK_SYSTEM_WS ();
                            goto restart;
                        }
                        StartVa = (PVOID)((PCHAR)StartVa + PAGE_SIZE);
                    }

                    //
                    // Now that we're holding the proper locks, rewalk all
                    // the sections to make sure they weren't locked down
                    // after we checked above.
                    //

                    while (StartSectionTableEntry < SectionTableEntry) {
                        SectionBaseAddress = SECTION_BASE_ADDRESS(StartSectionTableEntry);

                        SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (StartSectionTableEntry);
                        if (((PUCHAR)SectionBaseAddress ==
                                        ((PUCHAR)CurrentBase + StartSectionTableEntry->VirtualAddress)) &&
                        (*SectionLockCountPointer != 0)) {

                            //
                            // Restart at the top as the section has been
                            // explicitly locked by a driver since we first
                            // checked above.
                            //

                            UNLOCK_PFN (OldIrql);
                            UNLOCK_SYSTEM_WS ();
                            goto restart;
                        }
                        StartSectionTableEntry += 1;
                    }

                    MiEnablePagingOfDriverAtInit (PointerPte, LastPte);

                    UNLOCK_PFN (OldIrql);
                    UNLOCK_SYSTEM_WS ();

                    PointerPte = NULL;
                    StartSectionTableEntry = NULL;
                }
            }
            i -= 1;
            SectionTableEntry += 1;
        }

        if (PointerPte != NULL) {
            ASSERT (StartSectionTableEntry != NULL);
            LOCK_SYSTEM_WS (CurrentThread);
            LOCK_PFN (OldIrql);

            StartVa = PAGE_ALIGN (StartSectionTableEntry);
            while (StartVa < (PVOID) SectionTableEntry) {

                Waited = MiMakeSystemAddressValidPfnSystemWs (StartVa, OldIrql);

                if (Waited != 0) {

                    //
                    // Restart at the top as the locks were released.
                    //

                    UNLOCK_PFN (OldIrql);
                    UNLOCK_SYSTEM_WS ();
                    goto restart;
                }
                StartVa = (PVOID)((PCHAR)StartVa + PAGE_SIZE);
            }

            //
            // Now that we're holding the proper locks, rewalk all
            // the sections to make sure they weren't locked down
            // after we checked above.
            //

            while (StartSectionTableEntry < SectionTableEntry) {
                SectionBaseAddress = SECTION_BASE_ADDRESS(StartSectionTableEntry);

                SectionLockCountPointer = SECTION_LOCK_COUNT_POINTER (StartSectionTableEntry);
                if (((PUCHAR)SectionBaseAddress ==
                                ((PUCHAR)CurrentBase + StartSectionTableEntry->VirtualAddress)) &&
                (*SectionLockCountPointer != 0)) {

                    //
                    // Restart at the top as the section has been
                    // explicitly locked by a driver since we first
                    // checked above.
                    //

                    UNLOCK_PFN (OldIrql);
                    UNLOCK_SYSTEM_WS ();
                    goto restart;
                }
                StartSectionTableEntry += 1;
            }
            MiEnablePagingOfDriverAtInit (PointerPte, LastPte);

            UNLOCK_PFN (OldIrql);
            UNLOCK_SYSTEM_WS ();
        }

        Next = Next->Flink;
    }

    ExReleaseResourceLite (&PsLoadedModuleResource);

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);

    KeLeaveCriticalRegionThread (&CurrentThread->Tcb);

    return;
}


VOID
MiEnablePagingOfDriverAtInit (
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte
    )

/*++

Routine Description:

    This routine marks the specified range of PTEs as pagable.

Arguments:

    PointerPte - Supplies the starting PTE.

    LastPte - Supplies the ending PTE.

Return Value:

    None.

Environment:

    Working set mutex AND PFN lock held.

--*/

{
    PVOID Base;
    PFN_NUMBER PageFrameIndex;
    PMMPFN Pfn;
    MMPTE TempPte;
    LOGICAL SessionAddress;
    MMPTE_FLUSH_LIST PteFlushList;

    PteFlushList.Count = 0;

    MM_PFN_LOCK_ASSERT();

    Base = MiGetVirtualAddressMappedByPte (PointerPte);
    SessionAddress = MI_IS_SESSION_PTE (PointerPte);

    while (PointerPte <= LastPte) {

        //
        // The PTE must be carefully checked as drivers may call MmPageEntire
        // during their DriverEntry yet faults may occur prior to this routine
        // running which cause pages to already be resident and in the working
        // set at this point.  So checks for validity and wsindex must be
        // applied.
        //

        if (PointerPte->u.Hard.Valid == 1) {
            PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (PointerPte);
            Pfn = MI_PFN_ELEMENT (PageFrameIndex);
            ASSERT (Pfn->u2.ShareCount == 1);

            if (Pfn->u1.WsIndex == 0) {

                //
                // Set the working set index to zero.  This allows page table
                // pages to be brought back in with the proper WSINDEX.
                //

                MI_ZERO_WSINDEX (Pfn);

                //
                // Original PTE may need to be set for drivers loaded via
                // ntldr.
                //

                if (Pfn->OriginalPte.u.Long == 0) {
                    Pfn->OriginalPte.u.Long = MM_KERNEL_DEMAND_ZERO_PTE;
                    Pfn->OriginalPte.u.Soft.Protection |= MM_EXECUTE;
                }

                MI_SET_MODIFIED (Pfn, 1, 0x11);

                TempPte = *PointerPte;

                MI_MAKE_VALID_PTE_TRANSITION (TempPte,
                                              Pfn->OriginalPte.u.Soft.Protection);

                MI_WRITE_INVALID_PTE (PointerPte, TempPte);

                if (PteFlushList.Count < MM_MAXIMUM_FLUSH_COUNT) {
                    PteFlushList.FlushVa[PteFlushList.Count] = Base;
                    PteFlushList.Count += 1;
                }

                //
                // Flush the TB and decrement the number of valid PTEs
                // within the containing page table page.  Note that for a
                // private page, the page table page is still needed because
                // the page is in transition.
                //

                MiDecrementShareCount (Pfn, PageFrameIndex);

                MI_INCREMENT_RESIDENT_AVAILABLE (1, MM_RESAVAIL_FREE_PAGE_DRIVER);

                MmTotalSystemCodePages += 1;
            }
            else {

                //
                // This would need to be taken out of the WSLEs so skip it for
                // now and let the normal paging algorithms remove it if we
                // run into memory pressure.
                //
            }

        }
        Base = (PVOID)((PCHAR)Base + PAGE_SIZE);
        PointerPte += 1;
    }

    if (PteFlushList.Count != 0) {

        if (SessionAddress == TRUE) {

            //
            // Session space has no ASN - flush the entire TB.
            //

            MI_FLUSH_ENTIRE_SESSION_TB (TRUE, TRUE);
        }

        MiFlushPteList (&PteFlushList, TRUE);
    }

    return;
}


MM_SYSTEMSIZE
MmQuerySystemSize (
    VOID
    )
{
    //
    // 12Mb  is small
    // 12-19 is medium
    // > 19 is large
    //
    return MmSystemSize;
}

NTKERNELAPI
BOOLEAN
MmIsThisAnNtAsSystem (
    VOID
    )
{
    return (BOOLEAN)MmProductType;
}

NTKERNELAPI
VOID
FASTCALL
MmSetPageFaultNotifyRoutine (
    PPAGE_FAULT_NOTIFY_ROUTINE NotifyRoutine
    )
{
    MmPageFaultNotifyRoutine = NotifyRoutine;
}

#define CONSTANT_UNICODE_STRING(s)   { sizeof( s ) - sizeof( WCHAR ), sizeof( s ), s }

VOID
MiNotifyMemoryEvents (
    VOID
    )

// PFN lock is held.
{
    if (MmAvailablePages < MmLowMemoryThreshold) {

        if (KeReadStateEvent (MiHighMemoryEvent) != 0) {
            KeClearEvent (MiHighMemoryEvent);
        }

        if (KeReadStateEvent (MiLowMemoryEvent) == 0) {
            KeSetEvent (MiLowMemoryEvent, 0, FALSE);
        }
    }
    else if (MmAvailablePages < MmHighMemoryThreshold) {

        //
        // Gray zone, make sure both events are cleared.
        //

        if (KeReadStateEvent (MiHighMemoryEvent) != 0) {
            KeClearEvent (MiHighMemoryEvent);
        }

        if (KeReadStateEvent (MiLowMemoryEvent) != 0) {
            KeClearEvent (MiLowMemoryEvent);
        }
    }
    else {
        if (KeReadStateEvent (MiHighMemoryEvent) == 0) {
            KeSetEvent (MiHighMemoryEvent, 0, FALSE);
        }

        if (KeReadStateEvent (MiLowMemoryEvent) != 0) {
            KeClearEvent (MiLowMemoryEvent);
        }
    }

    return;
}

LOGICAL
MiInitializeMemoryEvents (
    VOID
    )
{
    KIRQL OldIrql;
    NTSTATUS Status;
    UNICODE_STRING LowMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\LowMemoryCondition");
    UNICODE_STRING HighMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\HighMemoryCondition");
    UNICODE_STRING LowPagedPoolMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\LowPagedPoolCondition");
    UNICODE_STRING HighPagedPoolMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\HighPagedPoolCondition");
    UNICODE_STRING LowNonPagedPoolMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\LowNonPagedPoolCondition");
    UNICODE_STRING HighNonPagedPoolMem = CONSTANT_UNICODE_STRING(L"\\KernelObjects\\HighNonPagedPoolCondition");

    //
    // The thresholds may be set in the registry, if so, they are interpreted
    // in megabytes so convert them to pages now.
    //
    // If the user modifies the registry to introduce his own values, don't
    // bother error checking them as they can't hurt the system regardless (bad
    // values just may result in events not getting signaled or staying
    // signaled when they shouldn't, but that's not fatal).
    //

    if (MmLowMemoryThreshold != 0) {
        MmLowMemoryThreshold *= ((1024 * 1024) / PAGE_SIZE);
    }
    else {

        //
        // Scale the threshold so on servers the low threshold is
        // approximately 32MB per 4GB, capping it at 64MB.
        //

        MmLowMemoryThreshold = MmPlentyFreePages;

        if (MmNumberOfPhysicalPages > 0x40000) {
            MmLowMemoryThreshold = (32 * 1024 * 1024) / PAGE_SIZE;
            MmLowMemoryThreshold += ((MmNumberOfPhysicalPages - 0x40000) >> 7);
        }
        else if (MmNumberOfPhysicalPages > 0x8000) {
            MmLowMemoryThreshold += ((MmNumberOfPhysicalPages - 0x8000) >> 5);
        }

        if (MmLowMemoryThreshold > (64 * 1024 * 1024) / PAGE_SIZE) {
            MmLowMemoryThreshold = (64 * 1024 * 1024) / PAGE_SIZE;
        }
    }

    if (MmHighMemoryThreshold != 0) {
        MmHighMemoryThreshold *= ((1024 * 1024) / PAGE_SIZE);
    }
    else {
        MmHighMemoryThreshold = 3 * MmLowMemoryThreshold;
        ASSERT (MmHighMemoryThreshold > MmLowMemoryThreshold);
    }

    if (MmHighMemoryThreshold < MmLowMemoryThreshold) {
        MmHighMemoryThreshold = MmLowMemoryThreshold;
    }

    Status = MiCreateMemoryEvent (&LowMem, &MiLowMemoryEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrint ("MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    Status = MiCreateMemoryEvent (&HighMem, &MiHighMemoryEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrint ("MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    //
    // Create the events for the pool thresholds.
    //

    Status = MiCreateMemoryEvent (&LowPagedPoolMem, &MiLowPagedPoolEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrint ("MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    Status = MiCreateMemoryEvent (&HighPagedPoolMem, &MiHighPagedPoolEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrint ("MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    Status = MiCreateMemoryEvent (&LowNonPagedPoolMem, &MiLowNonPagedPoolEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrint ("MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    Status = MiCreateMemoryEvent (&HighNonPagedPoolMem, &MiHighNonPagedPoolEvent);

    if (!NT_SUCCESS (Status)) {
#if DBG
        DbgPrint ("MM: Memory event initialization failed %x\n", Status);
#endif
        return FALSE;
    }

    //
    // Initialize the pool threshold events based on the current system
    // values.
    //

    MiInitializePoolEvents ();

    //
    // Initialize the event values.
    //

    LOCK_PFN (OldIrql);

    MiNotifyMemoryEvents ();

    UNLOCK_PFN (OldIrql);

    return TRUE;
}

extern POBJECT_TYPE ExEventObjectType;

NTSTATUS
MiCreateMemoryEvent (
    IN PUNICODE_STRING EventName,
    OUT PKEVENT *Event
    )
{
    PACL Dacl;
    HANDLE EventHandle;
    ULONG DaclLength;
    NTSTATUS Status;
    OBJECT_ATTRIBUTES ObjectAttributes;
    SECURITY_DESCRIPTOR SecurityDescriptor;

    Status = RtlCreateSecurityDescriptor (&SecurityDescriptor,
                                          SECURITY_DESCRIPTOR_REVISION);

    if (!NT_SUCCESS (Status)) {
        return Status;
    }

    DaclLength = sizeof (ACL) + sizeof (ACCESS_ALLOWED_ACE) * 3 +
                 RtlLengthSid (SeLocalSystemSid) +
                 RtlLengthSid (SeAliasAdminsSid) +
                 RtlLengthSid (SeWorldSid);

    Dacl = ExAllocatePoolWithTag (PagedPool, DaclLength, 'lcaD');

    if (Dacl == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    Status = RtlCreateAcl (Dacl, DaclLength, ACL_REVISION);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     SYNCHRONIZE|EVENT_QUERY_STATE|READ_CONTROL,
                                     SeWorldSid);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     EVENT_ALL_ACCESS,
                                     SeAliasAdminsSid);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }

    Status = RtlAddAccessAllowedAce (Dacl,
                                     ACL_REVISION,
                                     EVENT_ALL_ACCESS,
                                     SeLocalSystemSid);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }

  
    Status = RtlSetDaclSecurityDescriptor (&SecurityDescriptor,
                                           TRUE,
                                           Dacl,
                                           FALSE);

    if (!NT_SUCCESS (Status)) {
        ExFreePool (Dacl);
        return Status;
    }
  
    InitializeObjectAttributes (&ObjectAttributes,
                                EventName,
                                OBJ_KERNEL_HANDLE | OBJ_PERMANENT,
                                NULL,
                                &SecurityDescriptor);

    Status = ZwCreateEvent (&EventHandle,
                            EVENT_ALL_ACCESS,
                            &ObjectAttributes,
                            NotificationEvent,
                            FALSE);

    ExFreePool (Dacl);

    if (NT_SUCCESS (Status)) {
        Status = ObReferenceObjectByHandle (EventHandle,
                                            EVENT_MODIFY_STATE,
                                            ExEventObjectType,
                                            KernelMode,
                                            (PVOID *)Event,
                                            NULL);
    }

    ZwClose (EventHandle);

    return Status;
}

VOID
MiInitializeCacheOverrides (
    VOID
    )
{
#if defined (_WIN64)

    ULONG NumberOfBytes;
    NTSTATUS Status;
    HAL_PLATFORM_INFORMATION Information;

    //
    // Gather platform information from the HAL.
    //

    Status = HalQuerySystemInformation (HalPlatformInformation, 
                                        sizeof (Information),
                                        &Information,
                                        &NumberOfBytes);

    if (!NT_SUCCESS (Status)) {
        return;
    }

    //
    // Apply mapping modifications based on platform information flags.
    //
    // It would be better if the platform returned what the new cachetype
    // should be.
    //

    if (Information.PlatformFlags & HAL_PLATFORM_DISABLE_UC_MAIN_MEMORY) {
          MI_SET_CACHETYPE_TRANSLATION (MmNonCached, 0, MiCached);
    }

    if (Information.PlatformFlags & HAL_PLATFORM_DISABLE_WRITE_COMBINING) {
        MI_SET_CACHETYPE_TRANSLATION (MmWriteCombined, 0, MiCached);

        if ((Information.PlatformFlags & HAL_PLATFORM_ENABLE_WRITE_COMBINING_MMIO) == 0) {
            MI_SET_CACHETYPE_TRANSLATION (MmWriteCombined, 1, MiNonCached);
        }
    }

#endif

    return;
}

#if defined(_X86_) || defined(_AMD64_)

VOID
MiAddHalIoMappings (
    VOID
    )

/*++

Routine Description:

    This function scans the page directory and page tables for HAL I/O space
    mappings so they can be added to the page attribute table (to prevent
    any subsequent mappings from using a conflicting attribute).  This also
    lets the debugger automatically apply the correct attribute so !dd on
    any of these ranges just works.

Arguments:

    None.

Return Value:

    None.

Environment:

    Kernel mode, Phase 0 only.

--*/

{
    ULONG i;
    ULONG j;
    ULONG PdeCount;
    MMPTE PteContents;
    PMMPTE PointerPte;
    PMMPTE PointerPde;
    PFN_NUMBER PageFrameIndex;
    PVOID VirtualAddress;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;

#ifdef _X86_
    VirtualAddress = (PVOID) 0xFFC00000;
#elif defined(_AMD64_)
    VirtualAddress = (PVOID) HAL_VA_START;
#endif

    PointerPde = MiGetPdeAddress (VirtualAddress);

    ASSERT (MiGetPteOffset (VirtualAddress) == 0);

    PdeCount = PDE_PER_PAGE - MiGetPdeOffset (VirtualAddress);

    for (i = 0; i < PdeCount; i += 1) {

        if ((PointerPde->u.Hard.Valid == 1) &&
            (PointerPde->u.Hard.LargePage == 0)) {

            PointerPte = MiGetPteAddress (VirtualAddress);

            for (j = 0 ; j < PTE_PER_PAGE; j += 1) {

                PteContents = *PointerPte;

                if (PteContents.u.Hard.Valid == 1) {

                    PageFrameIndex = MI_GET_PAGE_FRAME_FROM_PTE (&PteContents);

                    if (!MI_IS_PFN (PageFrameIndex)) {

                        CacheAttribute = MiCached;

                        if ((MI_IS_CACHING_DISABLED (&PteContents)) &&
                            (PteContents.u.Hard.WriteThrough == 1)) {

                            CacheAttribute = MiNonCached;
                        }
                        else if ((MiWriteCombiningPtes == TRUE) &&
                                (PteContents.u.Hard.CacheDisable == 0) &&
                                (PteContents.u.Hard.WriteThrough == 1)) {

                            CacheAttribute = MiWriteCombined;
                        }
                        else if ((MiWriteCombiningPtes == FALSE) &&
                                (PteContents.u.Hard.CacheDisable == 1) &&
                                (PteContents.u.Hard.WriteThrough == 0)) {

                            CacheAttribute = MiWriteCombined;
                        }

                        MiInsertIoSpaceMap (VirtualAddress,
                                            PageFrameIndex,
                                            1,
                                            CacheAttribute);
                    }
                }

                VirtualAddress = (PVOID) ((PCHAR)VirtualAddress + PAGE_SIZE);
                PointerPte += 1;
            }
        }
        else {
            VirtualAddress = (PVOID) ((PCHAR)VirtualAddress + MM_VA_MAPPED_BY_PDE);
        }

        PointerPde += 1;
    }
}

#endif

__declspec(noinline)
PVOID
MiGetInstructionPointer (
    VOID
    )
{
    return (PVOID) _ReturnAddress ();
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\mi.h ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

    mi.h

Abstract:

    This module contains the private data structures and procedure
    prototypes for the memory management system.

Author:

    Lou Perazzoli (loup) 20-Mar-1989
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/

#ifndef _MI_
#define _MI_

#pragma warning(disable:4214)   // bit field types other than int
#pragma warning(disable:4201)   // nameless struct/union
#pragma warning(disable:4324)   // alignment sensitive to declspec
#pragma warning(disable:4127)   // condition expression is constant
#pragma warning(disable:4115)   // named type definition in parentheses
#pragma warning(disable:4232)   // dllimport not static
#pragma warning(disable:4206)   // translation unit empty

#include "ntos.h"
#include "ntimage.h"
#include "ki.h"
#include "fsrtl.h"
#include "zwapi.h"
#include "pool.h"
#include "stdio.h"
#include "string.h"
#include "safeboot.h"
#include "triage.h"
#include "xip.h"

#if defined(_X86_)
#include "..\mm\i386\mi386.h"

#elif defined(_AMD64_)
#include "..\mm\amd64\miamd.h"

#elif defined(_IA64_)
#include "..\mm\ia64\miia64.h"

#else
#error "mm: a target architecture must be defined."
#endif

#if defined (_WIN64)
#define ASSERT32(exp)
#define ASSERT64(exp)   ASSERT(exp)

//
// This macro is used to satisfy the compiler -
// note the assignments are not needed for correctness
// but without it the compiler cannot compile this code
// W4 to check for use of uninitialized variables.
//

#define SATISFY_OVERZEALOUS_COMPILER(x) x
#else
#define ASSERT32(exp)   ASSERT(exp)
#define ASSERT64(exp)
#define SATISFY_OVERZEALOUS_COMPILER(x) x
#endif

//
// Special pool constants
//
#define MI_SPECIAL_POOL_PAGABLE         0x8000
#define MI_SPECIAL_POOL_VERIFIER        0x4000
#define MI_SPECIAL_POOL_IN_SESSION      0x2000
#define MI_SPECIAL_POOL_PTE_PAGABLE     0x0002
#define MI_SPECIAL_POOL_PTE_NONPAGABLE  0x0004


#define _2gb  0x80000000                // 2 gigabytes
#define _3gb  0xC0000000                // 3 gigabytes
#define _4gb 0x100000000                // 4 gigabytes

#define MM_FLUSH_COUNTER_MASK (0xFFFFF)

#define MM_FREE_WSLE_SHIFT 4

#define WSLE_NULL_INDEX ((((WSLE_NUMBER)-1) >> MM_FREE_WSLE_SHIFT))

#define MM_FREE_POOL_SIGNATURE (0x50554F4C)

#define MM_MINIMUM_PAGED_POOL_NTAS ((SIZE_T)(48*1024*1024))

#define MM_ALLOCATION_FILLS_VAD ((PMMPTE)(ULONG_PTR)~3)

#define MM_WORKING_SET_LIST_SEARCH 17

#define MM_FLUID_WORKING_SET 8

#define MM_FLUID_PHYSICAL_PAGES 32  //see MmResidentPages below.

#define MM_USABLE_PAGES_FREE 32

#define X64K (ULONG)65536

#define MM_HIGHEST_VAD_ADDRESS ((PVOID)((ULONG_PTR)MM_HIGHEST_USER_ADDRESS - (64 * 1024)))


#define MM_WS_NOT_LISTED    ((PLIST_ENTRY)0)
#define MM_WS_TRIMMING      ((PLIST_ENTRY)1)
#define MM_WS_SWAPPED_OUT   ((PLIST_ENTRY)2)

#if DBG
#define MM_IO_IN_PROGRESS ((PLIST_ENTRY)97)
#endif

#define MM4K_SHIFT    12  //MUST BE LESS THAN OR EQUAL TO PAGE_SHIFT
#define MM4K_MASK  0xfff

#define MMSECTOR_SHIFT 9  //MUST BE LESS THAN OR EQUAL TO PAGE_SHIFT

#define MMSECTOR_MASK 0x1ff

#define MM_LOCK_BY_REFCOUNT 0

#define MM_LOCK_BY_NONPAGE 1

#define MM_MAXIMUM_WRITE_CLUSTER (MM_MAXIMUM_DISK_IO_SIZE / PAGE_SIZE)

//
// Number of PTEs to flush singularly before flushing the entire TB.
//

#define MM_MAXIMUM_FLUSH_COUNT (FLUSH_MULTIPLE_MAXIMUM-1)

//
// Page protections
//

#define MM_ZERO_ACCESS         0  // this value is not used.
#define MM_READONLY            1
#define MM_EXECUTE             2
#define MM_EXECUTE_READ        3
#define MM_READWRITE           4  // bit 2 is set if this is writable.
#define MM_WRITECOPY           5
#define MM_EXECUTE_READWRITE   6
#define MM_EXECUTE_WRITECOPY   7

#define MM_NOCACHE            0x8
#define MM_GUARD_PAGE         0x10
#define MM_DECOMMIT           0x10   //NO_ACCESS, Guard page
#define MM_NOACCESS           0x18   //NO_ACCESS, Guard_page, nocache.
#define MM_UNKNOWN_PROTECTION 0x100  //bigger than 5 bits!
#define MM_LARGE_PAGES        0x111

#define MM_INVALID_PROTECTION ((ULONG)-1)  //bigger than 5 bits!

#define MM_KSTACK_OUTSWAPPED  0x1F   // Denotes outswapped kernel stack pages.

#define MM_PROTECTION_WRITE_MASK     4
#define MM_PROTECTION_COPY_MASK      1
#define MM_PROTECTION_OPERATION_MASK 7 // mask off guard page and nocache.
#define MM_PROTECTION_EXECUTE_MASK   2

#define MM_SECURE_DELETE_CHECK 0x55

#if defined(_X86PAE_)

//
// PAE mode makes most kernel resources executable to improve
// compatibility with existing driver binaries.
//

#define MI_ADD_EXECUTE_TO_VALID_PTE_IF_PAE(TempPte)         \
                    ASSERT ((TempPte).u.Hard.Valid == 1);   \
                    ((TempPte).u.Long &= ~MmPaeMask);

#define MI_ADD_EXECUTE_TO_INVALID_PTE_IF_PAE(TempPte)       \
                    ASSERT ((TempPte).u.Hard.Valid == 0);   \
                    ((TempPte).u.Soft.Protection |= MM_EXECUTE);

#else

//
// NT64 drivers derived from 32-bit source have to be recompiled so there's
// no need to make everything executable - drivers can specify it explicitly.
//

#define MI_ADD_EXECUTE_TO_VALID_PTE_IF_PAE(TempPte)
#define MI_ADD_EXECUTE_TO_INVALID_PTE_IF_PAE(TempPte)
#endif

//
// Debug flags
//

#define MM_DBG_WRITEFAULT       0x1
#define MM_DBG_PTE_UPDATE       0x2
#define MM_DBG_DUMP_WSL         0x4
#define MM_DBG_PAGEFAULT        0x8
#define MM_DBG_WS_EXPANSION     0x10
#define MM_DBG_MOD_WRITE        0x20
#define MM_DBG_CHECK_PTE        0x40
#define MM_DBG_VAD_CONFLICT     0x80
#define MM_DBG_SECTIONS         0x100
#define MM_DBG_STOP_ON_WOW64_ACCVIO   0x200
#define MM_DBG_SYS_PTES         0x400
#define MM_DBG_CLEAN_PROCESS    0x800
#define MM_DBG_COLLIDED_PAGE    0x1000
#define MM_DBG_DUMP_BOOT_PTES   0x2000
#define MM_DBG_FORK             0x4000
#define MM_DBG_DIR_BASE         0x8000
#define MM_DBG_FLUSH_SECTION    0x10000
#define MM_DBG_PRINTS_MODWRITES 0x20000
#define MM_DBG_PAGE_IN_LIST     0x40000
#define MM_DBG_CHECK_PFN_LOCK   0x80000
#define MM_DBG_PRIVATE_PAGES    0x100000
#define MM_DBG_WALK_VAD_TREE    0x200000
#define MM_DBG_SWAP_PROCESS     0x400000
#define MM_DBG_LOCK_CODE        0x800000
#define MM_DBG_STOP_ON_ACCVIO   0x1000000
#define MM_DBG_PAGE_REF_COUNT   0x2000000
#define MM_DBG_SHOW_FAULTS      0x40000000
#define MM_DBG_SESSIONS         0x80000000

//
// If the PTE.protection & MM_COPY_ON_WRITE_MASK == MM_COPY_ON_WRITE_MASK
// then the PTE is copy on write.
//

#define MM_COPY_ON_WRITE_MASK  5

extern ULONG MmProtectToValue[32];

extern
#if (defined(_WIN64) || defined(_X86PAE_))
ULONGLONG
#else
ULONG
#endif
MmProtectToPteMask[32];
extern ULONG MmMakeProtectNotWriteCopy[32];
extern ACCESS_MASK MmMakeSectionAccess[8];
extern ACCESS_MASK MmMakeFileAccess[8];


//
// Time constants
//

extern const LARGE_INTEGER MmSevenMinutes;
const extern LARGE_INTEGER MmOneSecond;
const extern LARGE_INTEGER MmTwentySeconds;
const extern LARGE_INTEGER MmSeventySeconds;
const extern LARGE_INTEGER MmShortTime;
const extern LARGE_INTEGER MmHalfSecond;
const extern LARGE_INTEGER Mm30Milliseconds;
extern LARGE_INTEGER MmCriticalSectionTimeout;

//
// A month's worth
//

extern ULONG MmCritsectTimeoutSeconds;

//
// this is the csrss process !
//

extern PEPROCESS ExpDefaultErrorPortProcess;

extern SIZE_T MmExtendedCommit;

extern SIZE_T MmTotalProcessCommit;

#if !defined(_WIN64)
extern LIST_ENTRY MmProcessList;
extern PMMPTE MiLargePageHyperPte;
extern PMMPTE MiInitialSystemPageDirectory;
#endif

//
// The total number of pages needed for the loader to successfully hibernate.
//

extern PFN_NUMBER MmHiberPages;

//
//  The counters and reasons to retry IO to protect against verifier induced
//  failures and temporary conditions.
//

extern ULONG MiIoRetryMask;
extern ULONG MiFaultRetryMask;
extern ULONG MiUserFaultRetryMask;

#define MmIsRetryIoStatus(S) (((S) == STATUS_INSUFFICIENT_RESOURCES) || \
                              ((S) == STATUS_WORKING_SET_QUOTA) ||      \
                              ((S) == STATUS_NO_MEMORY))

#if defined (_MI_MORE_THAN_4GB_)

extern PFN_NUMBER MiNoLowMemory;

#if defined (_WIN64)
#define MI_MAGIC_4GB_RECLAIM     0xffffedf0
#else
#define MI_MAGIC_4GB_RECLAIM     0xffedf0
#endif

#define MI_LOWMEM_MAGIC_BIT     (0x80000000)

extern PRTL_BITMAP MiLowMemoryBitMap;
#endif

//
// This is a version of COMPUTE_PAGES_SPANNED that works for 32 and 64 ranges.
//

#define MI_COMPUTE_PAGES_SPANNED(Va, Size) \
    ((((ULONG_PTR)(Va) & (PAGE_SIZE -1)) + (Size) + (PAGE_SIZE - 1)) >> PAGE_SHIFT)

//++
//
// ULONG
// MI_CONVERT_FROM_PTE_PROTECTION (
//     IN ULONG PROTECTION_MASK
//     )
//
// Routine Description:
//
//  This routine converts a PTE protection into a Protect value.
//
// Arguments:
//
//
// Return Value:
//
//     Returns the
//
//--

#define MI_CONVERT_FROM_PTE_PROTECTION(PROTECTION_MASK)      \
                                     (MmProtectToValue[PROTECTION_MASK])

#define MI_IS_PTE_PROTECTION_COPY_WRITE(PROTECTION_MASK)  \
   (((PROTECTION_MASK) & MM_COPY_ON_WRITE_MASK) == MM_COPY_ON_WRITE_MASK)

//++
//
// ULONG
// MI_ROUND_TO_64K (
//     IN ULONG LENGTH
//     )
//
// Routine Description:
//
//
// The ROUND_TO_64k macro takes a LENGTH in bytes and rounds it up to a multiple
// of 64K.
//
// Arguments:
//
//     LENGTH - LENGTH in bytes to round up to 64k.
//
// Return Value:
//
//     Returns the LENGTH rounded up to a multiple of 64k.
//
//--

#define MI_ROUND_TO_64K(LENGTH)  (((LENGTH) + X64K - 1) & ~((ULONG_PTR)X64K - 1))

extern ULONG MiLastVadBit;

//++
//
// ULONG
// MI_ROUND_TO_SIZE (
//     IN ULONG LENGTH,
//     IN ULONG ALIGNMENT
//     )
//
// Routine Description:
//
//
// The ROUND_TO_SIZE macro takes a LENGTH in bytes and rounds it up to a
// multiple of the alignment.
//
// Arguments:
//
//     LENGTH - LENGTH in bytes to round up to.
//
//     ALIGNMENT - alignment to round to, must be a power of 2, e.g, 2**n.
//
// Return Value:
//
//     Returns the LENGTH rounded up to a multiple of the alignment.
//
//--

#define MI_ROUND_TO_SIZE(LENGTH,ALIGNMENT)     \
                    (((LENGTH) + ((ALIGNMENT) - 1)) & ~((ALIGNMENT) - 1))

//++
//
// PVOID
// MI_64K_ALIGN (
//     IN PVOID VA
//     )
//
// Routine Description:
//
//
// The MI_64K_ALIGN macro takes a virtual address and returns a 64k-aligned
// virtual address for that page.
//
// Arguments:
//
//     VA - Virtual address.
//
// Return Value:
//
//     Returns the 64k aligned virtual address.
//
//--

#define MI_64K_ALIGN(VA) ((PVOID)((ULONG_PTR)(VA) & ~((LONG)X64K - 1)))


//++
//
// PVOID
// MI_ALIGN_TO_SIZE (
//     IN PVOID VA
//     IN ULONG ALIGNMENT
//     )
//
// Routine Description:
//
//
// The MI_ALIGN_TO_SIZE macro takes a virtual address and returns a
// virtual address for that page with the specified alignment.
//
// Arguments:
//
//     VA - Virtual address.
//
//     ALIGNMENT - alignment to round to, must be a power of 2, e.g, 2**n.
//
// Return Value:
//
//     Returns the aligned virtual address.
//
//--

#define MI_ALIGN_TO_SIZE(VA,ALIGNMENT) ((PVOID)((ULONG_PTR)(VA) & ~((ULONG_PTR) ALIGNMENT - 1)))

//++
//
// LONGLONG
// MI_STARTING_OFFSET (
//     IN PSUBSECTION SUBSECT
//     IN PMMPTE PTE
//     )
//
// Routine Description:
//
//    This macro takes a pointer to a PTE within a subsection and a pointer
//    to that subsection and calculates the offset for that PTE within the
//    file.
//
// Arguments:
//
//     PTE - PTE within subsection.
//
//     SUBSECT - Subsection
//
// Return Value:
//
//     Offset for issuing I/O from.
//
//--

#define MI_STARTING_OFFSET(SUBSECT,PTE) \
           (((LONGLONG)((ULONG_PTR)((PTE) - ((SUBSECT)->SubsectionBase))) << PAGE_SHIFT) + \
             ((LONGLONG)((SUBSECT)->StartingSector) << MMSECTOR_SHIFT));


// NTSTATUS
// MiFindEmptyAddressRangeDown (
//    IN ULONG_PTR SizeOfRange,
//    IN PVOID HighestAddressToEndAt,
//    IN ULONG_PTR Alignment,
//    OUT PVOID *Base
//    )
//
//
// Routine Description:
//
//    The function examines the virtual address descriptors to locate
//    an unused range of the specified size and returns the starting
//    address of the range.  This routine looks from the top down.
//
// Arguments:
//
//    SizeOfRange - Supplies the size in bytes of the range to locate.
//
//    HighestAddressToEndAt - Supplies the virtual address to begin looking
//                            at.
//
//    Alignment - Supplies the alignment for the address.  Must be
//                 a power of 2 and greater than the page_size.
//
//Return Value:
//
//    Returns the starting address of a suitable range.
//

#define MiFindEmptyAddressRangeDown(Root,SizeOfRange,HighestAddressToEndAt,Alignment,Base) \
               (MiFindEmptyAddressRangeDownTree(                             \
                    (SizeOfRange),                                           \
                    (HighestAddressToEndAt),                                 \
                    (Alignment),                                             \
                    Root,                                                    \
                    (Base)))

// PMMVAD
// MiGetPreviousVad (
//     IN PMMVAD Vad
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically precedes the specified virtual
//     address descriptor.
//
// Arguments:
//
//     Vad - Supplies a pointer to a virtual address descriptor.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     next address range, NULL if none.
//
//

#define MiGetPreviousVad(VAD) ((PMMVAD)MiGetPreviousNode((PMMADDRESS_NODE)(VAD)))


// PMMVAD
// MiGetNextVad (
//     IN PMMVAD Vad
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically follows the specified address range.
//
// Arguments:
//
//     VAD - Supplies a pointer to a virtual address descriptor.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     next address range, NULL if none.
//

#define MiGetNextVad(VAD) ((PMMVAD)MiGetNextNode((PMMADDRESS_NODE)(VAD)))



// PMMVAD
// MiGetFirstVad (
//     Process
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically is first within the address space.
//
// Arguments:
//
//     Process - Specifies the process in which to locate the VAD.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     first address range, NULL if none.

#define MiGetFirstVad(Process) \
    ((PMMVAD)MiGetFirstNode(&Process->VadRoot))


LOGICAL
MiCheckForConflictingVadExistence (
    IN PEPROCESS Process,
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    );

// PMMVAD
// MiCheckForConflictingVad (
//     IN PVOID StartingAddress,
//     IN PVOID EndingAddress
//     )
//
// Routine Description:
//
//     The function determines if any addresses between a given starting and
//     ending address is contained within a virtual address descriptor.
//
// Arguments:
//
//     StartingAddress - Supplies the virtual address to locate a containing
//                       descriptor.
//
//     EndingAddress - Supplies the virtual address to locate a containing
//                       descriptor.
//
// Return Value:
//
//     Returns a pointer to the first conflicting virtual address descriptor
//     if one is found, otherwise a NULL value is returned.
//

#define MiCheckForConflictingVad(CurrentProcess,StartingAddress,EndingAddress) \
    ((PMMVAD)MiCheckForConflictingNode(                                   \
                    MI_VA_TO_VPN(StartingAddress),                        \
                    MI_VA_TO_VPN(EndingAddress),                          \
                    &CurrentProcess->VadRoot))

// PMMCLONE_DESCRIPTOR
// MiGetNextClone (
//     IN PMMCLONE_DESCRIPTOR Clone
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically follows the specified address range.
//
// Arguments:
//
//     Clone - Supplies a pointer to a virtual address descriptor.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     next address range, NULL if none.
//
//

#define MiGetNextClone(CLONE) \
 ((PMMCLONE_DESCRIPTOR)MiGetNextNode((PMMADDRESS_NODE)(CLONE)))



// PMMCLONE_DESCRIPTOR
// MiGetPreviousClone (
//     IN PMMCLONE_DESCRIPTOR Clone
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically precedes the specified virtual
//     address descriptor.
//
// Arguments:
//
//     Clone - Supplies a pointer to a virtual address descriptor.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     next address range, NULL if none.


#define MiGetPreviousClone(CLONE)  \
             ((PMMCLONE_DESCRIPTOR)MiGetPreviousNode((PMMADDRESS_NODE)(CLONE)))



// PMMCLONE_DESCRIPTOR
// MiGetFirstClone (
//     )
//
// Routine Description:
//
//     This function locates the virtual address descriptor which contains
//     the address range which logically is first within the address space.
//
// Arguments:
//
//     None.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor containing the
//     first address range, NULL if none.
//


#define MiGetFirstClone(_CurrentProcess) \
        (((PMM_AVL_TABLE)(_CurrentProcess->CloneRoot))->NumberGenericTableElements == 0 ? NULL : (PMMCLONE_DESCRIPTOR)MiGetFirstNode((PMM_AVL_TABLE)(_CurrentProcess->CloneRoot)))



// VOID
// MiInsertClone (
//     IN PMMCLONE_DESCRIPTOR Clone
//     )
//
// Routine Description:
//
//     This function inserts a virtual address descriptor into the tree and
//     reorders the splay tree as appropriate.
//
// Arguments:
//
//     Clone - Supplies a pointer to a virtual address descriptor.
//
//
// Return Value:
//
//     None.
//

#define MiInsertClone(_CurrentProcess, CLONE) \
    {                                           \
        ASSERT ((CLONE)->NumberOfPtes != 0);     \
        ASSERT (_CurrentProcess->CloneRoot != NULL); \
        MiInsertNode(((PMMADDRESS_NODE)(CLONE)),(PMM_AVL_TABLE)(_CurrentProcess->CloneRoot)); \
    }




// VOID
// MiRemoveClone (
//     IN PMMCLONE_DESCRIPTOR Clone
//     )
//
// Routine Description:
//
//     This function removes a virtual address descriptor from the tree and
//     reorders the splay tree as appropriate.
//
// Arguments:
//
//     Clone - Supplies a pointer to a virtual address descriptor.
//
// Return Value:
//
//     None.
//

#define MiRemoveClone(_CurrentProcess, CLONE) \
    ASSERT (_CurrentProcess->CloneRoot != NULL); \
    ASSERT (((PMM_AVL_TABLE)_CurrentProcess->CloneRoot)->NumberGenericTableElements != 0); \
    MiRemoveNode((PMMADDRESS_NODE)(CLONE),(PMM_AVL_TABLE)(_CurrentProcess->CloneRoot));



// PMMCLONE_DESCRIPTOR
// MiLocateCloneAddress (
//     IN PVOID VirtualAddress
//     )
//
// /*++
//
// Routine Description:
//
//     The function locates the virtual address descriptor which describes
//     a given address.
//
// Arguments:
//
//     VirtualAddress - Supplies the virtual address to locate a descriptor
//                      for.
//
// Return Value:
//
//     Returns a pointer to the virtual address descriptor which contains
//     the supplied virtual address or NULL if none was located.
//

#define MiLocateCloneAddress(_CurrentProcess, VA)                           \
    (_CurrentProcess->CloneRoot ?                                           \
        ((PMMCLONE_DESCRIPTOR)MiLocateAddressInTree(((ULONG_PTR)VA),        \
                   (PMM_AVL_TABLE)(_CurrentProcess->CloneRoot))) :     \
        NULL)


#define MI_VA_TO_PAGE(va) ((ULONG_PTR)(va) >> PAGE_SHIFT)

#define MI_VA_TO_VPN(va)  ((ULONG_PTR)(va) >> PAGE_SHIFT)

#define MI_VPN_TO_VA(vpn)  (PVOID)((vpn) << PAGE_SHIFT)

#define MI_VPN_TO_VA_ENDING(vpn)  (PVOID)(((vpn) << PAGE_SHIFT) | (PAGE_SIZE - 1))

#define MiGetByteOffset(va) ((ULONG_PTR)(va) & (PAGE_SIZE - 1))

#define MI_PFN_ELEMENT(index) (&MmPfnDatabase[index])

//
// Make a write-copy PTE, only writable.
//

#define MI_MAKE_PROTECT_NOT_WRITE_COPY(PROTECT) \
            (MmMakeProtectNotWriteCopy[PROTECT])

//
// Define macros to lock and unlock the PFN database.
//

#define MiLockPfnDatabase(OldIrql) \
    OldIrql = KeAcquireQueuedSpinLock(LockQueuePfnLock)

#define MiUnlockPfnDatabase(OldIrql) \
    KeReleaseQueuedSpinLock(LockQueuePfnLock, OldIrql)

#define MiLockPfnDatabaseAtDpcLevel() \
    KeAcquireQueuedSpinLockAtDpcLevel(&KeGetCurrentPrcb()->LockQueue[LockQueuePfnLock])

#define MiUnlockPfnDatabaseFromDpcLevel() \
    KeReleaseQueuedSpinLockFromDpcLevel(&KeGetCurrentPrcb()->LockQueue[LockQueuePfnLock])

#define MiReleasePfnLock() \
    KeReleaseQueuedSpinLockFromDpcLevel(&KeGetCurrentPrcb()->LockQueue[LockQueuePfnLock])

#define MiLockSystemSpace(OldIrql) \
    OldIrql = KeAcquireQueuedSpinLock(LockQueueSystemSpaceLock)

#define MiUnlockSystemSpace(OldIrql) \
    KeReleaseQueuedSpinLock(LockQueueSystemSpaceLock, OldIrql)

#define MiLockSystemSpaceAtDpcLevel() \
    KeAcquireQueuedSpinLockAtDpcLevel(&KeGetCurrentPrcb()->LockQueue[LockQueueSystemSpaceLock])

#define MiUnlockSystemSpaceFromDpcLevel() \
    KeReleaseQueuedSpinLockFromDpcLevel(&KeGetCurrentPrcb()->LockQueue[LockQueueSystemSpaceLock])

#define MI_MAX_PFN_CALLERS  500

typedef struct _MMPFNTIMINGS {
    LARGE_INTEGER HoldTime;     // Low bit is set if another processor waited
    PVOID AcquiredAddress;
    PVOID ReleasedAddress;
} MMPFNTIMINGS, *PMMPFNTIMINGS;

extern ULONG MiPfnTimings;
extern PVOID MiPfnAcquiredAddress;
extern MMPFNTIMINGS MiPfnSorted[];
extern LARGE_INTEGER MiPfnAcquired;
extern LARGE_INTEGER MiPfnReleased;
extern LARGE_INTEGER MiPfnThreshold;

PVOID
MiGetExecutionAddress (
    VOID
    );

LARGE_INTEGER
MiQueryPerformanceCounter (
    IN PLARGE_INTEGER PerformanceFrequency 
    );

VOID
MiAddLockToTable (
    IN PVOID AcquireAddress,
    IN PVOID ReleaseAddress,
    IN LARGE_INTEGER HoldTime
    );

#if defined(_X86_) || defined(_AMD64_)
#define MI_GET_EXECUTION_ADDRESS(varname) varname = MiGetExecutionAddress();
#else
#define MI_GET_EXECUTION_ADDRESS(varname) varname = NULL;
#endif

// #define _MI_INSTRUMENT_PFN 1

#if defined (_MI_INSTRUMENT_PFN)

#define LOCK_PFN_TIMESTAMP()                                \
        {                                                   \
            MiPfnAcquired = MiQueryPerformanceCounter (NULL);\
            MI_GET_EXECUTION_ADDRESS(MiPfnAcquiredAddress); \
        }

#define UNLOCK_PFN_TIMESTAMP()                                  \
        {                                                       \
            PVOID ExecutionAddress;                             \
            LARGE_INTEGER PfnHoldTime;                          \
                                                                \
            MiPfnReleased = MiQueryPerformanceCounter (NULL);   \
            MI_GET_EXECUTION_ADDRESS(ExecutionAddress);         \
            PfnHoldTime.QuadPart = (MiPfnReleased.QuadPart - MiPfnAcquired.QuadPart) & ~0x1; \
            MiAddLockToTable (MiPfnAcquiredAddress, ExecutionAddress, PfnHoldTime); \
        }
#else
#define LOCK_PFN_TIMESTAMP()
#define UNLOCK_PFN_TIMESTAMP()
#endif

#define LOCK_PFN(OLDIRQL) ASSERT (KeGetCurrentIrql() <= APC_LEVEL); \
                          MiLockPfnDatabase(OLDIRQL);               \
                          LOCK_PFN_TIMESTAMP();

#define UNLOCK_PFN(OLDIRQL)                                        \
    ASSERT (OLDIRQL <= APC_LEVEL);                                 \
    UNLOCK_PFN_TIMESTAMP();                                        \
    MiUnlockPfnDatabase(OLDIRQL);                                  \
    ASSERT(KeGetCurrentIrql() <= APC_LEVEL);

#define LOCK_PFN2(OLDIRQL) ASSERT (KeGetCurrentIrql() <= DISPATCH_LEVEL); \
                          MiLockPfnDatabase(OLDIRQL);               \
                          LOCK_PFN_TIMESTAMP();

#define UNLOCK_PFN2(OLDIRQL)                                       \
    ASSERT (OLDIRQL <= DISPATCH_LEVEL);                            \
    UNLOCK_PFN_TIMESTAMP();                                        \
    MiUnlockPfnDatabase(OLDIRQL);                                  \
    ASSERT(KeGetCurrentIrql() <= DISPATCH_LEVEL);

#define LOCK_PFN_AT_DPC() ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL); \
                          MiLockPfnDatabaseAtDpcLevel();                 \
                          LOCK_PFN_TIMESTAMP();

#define UNLOCK_PFN_FROM_DPC()                                      \
    UNLOCK_PFN_TIMESTAMP();                                        \
    MiUnlockPfnDatabaseFromDpcLevel();                             \
    ASSERT(KeGetCurrentIrql() == DISPATCH_LEVEL);

#define UNLOCK_PFN_AND_THEN_WAIT(OLDIRQL)                          \
                {                                                  \
                    KIRQL XXX;                                     \
                    ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL); \
                    ASSERT (OLDIRQL <= APC_LEVEL);                 \
                    KiLockDispatcherDatabase (&XXX);               \
                    UNLOCK_PFN_TIMESTAMP();                        \
                    MiReleasePfnLock();                            \
                    (KeGetCurrentThread())->WaitIrql = OLDIRQL;    \
                    (KeGetCurrentThread())->WaitNext = TRUE;       \
                }

extern KMUTANT MmSystemLoadLock;

#if DBG
#define SYSLOAD_LOCK_OWNED_BY_ME()      ASSERT (MmSystemLoadLock.OwnerThread == KeGetCurrentThread())
#else
#define SYSLOAD_LOCK_OWNED_BY_ME()
#endif

#if DBG

#if defined (_MI_COMPRESSION)

extern KIRQL MiCompressionIrql;

#define MM_PFN_LOCK_ASSERT() \
    if (MmDebug & 0x80000) { \
        KIRQL _OldIrql; \
        _OldIrql = KeGetCurrentIrql(); \
        ASSERT ((_OldIrql == DISPATCH_LEVEL) || \
                ((MiCompressionIrql != 0) && (_OldIrql == MiCompressionIrql))); \
    }

#else

#define MM_PFN_LOCK_ASSERT() \
    if (MmDebug & 0x80000) { \
        KIRQL _OldIrql; \
        _OldIrql = KeGetCurrentIrql(); \
        ASSERT (_OldIrql == DISPATCH_LEVEL); \
    }

#endif

extern PETHREAD MiExpansionLockOwner;

#define MM_SET_EXPANSION_OWNER()  ASSERT (MiExpansionLockOwner == NULL); \
                                  MiExpansionLockOwner = PsGetCurrentThread();

#define MM_CLEAR_EXPANSION_OWNER()  ASSERT (MiExpansionLockOwner == PsGetCurrentThread()); \
                                    MiExpansionLockOwner = NULL;

#else
#define MM_PFN_LOCK_ASSERT()
#define MM_SET_EXPANSION_OWNER()
#define MM_CLEAR_EXPANSION_OWNER()
#endif //DBG


#define LOCK_EXPANSION(OLDIRQL)     ASSERT (KeGetCurrentIrql() <= APC_LEVEL); \
                                ExAcquireSpinLock (&MmExpansionLock, &OLDIRQL);\
                                MM_SET_EXPANSION_OWNER ();

#define UNLOCK_EXPANSION(OLDIRQL)    MM_CLEAR_EXPANSION_OWNER (); \
                                ExReleaseSpinLock (&MmExpansionLock, OLDIRQL); \
                                ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

#define LOCK_EXPANSION2(OLDIRQL)     ASSERT (KeGetCurrentIrql() <= DISPATCH_LEVEL); \
                                ExAcquireSpinLock (&MmExpansionLock, &OLDIRQL);\
                                MM_SET_EXPANSION_OWNER ();

#define UNLOCK_EXPANSION2(OLDIRQL)    MM_CLEAR_EXPANSION_OWNER (); \
                                ExReleaseSpinLock (&MmExpansionLock, OLDIRQL); \
                                ASSERT (KeGetCurrentIrql() <= DISPATCH_LEVEL);

// #define _MI_INSTRUMENT_WS 1

#if defined (_MI_INSTRUMENT_WS)

#if defined (_MI_INSTRUMENT_PFN)
error - cannot enable both WS & PFN tracing concurrently yet.
#endif

extern KSPIN_LOCK MiInstrumentationLock;
extern EPROCESS MiSystemCacheDummyProcess;

#define LOCK_WS_TIMESTAMP(PROCESS)                                      \
        if (PROCESS != NULL) {                                          \
            LARGE_INTEGER TimeNow;                                      \
            TimeNow = MiQueryPerformanceCounter (NULL);                 \
            PROCESS->Spare0 = (PVOID) (ULONG_PTR) TimeNow.LowPart;      \
            PROCESS->Spare1 = (PVOID) MiGetExecutionAddress();          \
        }

#define UNLOCK_WS_TIMESTAMP(PROCESS)                                    \
        if (PROCESS != NULL) {                                          \
            PVOID ExecutionAddress;                                     \
            LARGE_INTEGER WsHoldTime;                                   \
            LARGE_INTEGER WsReleased;                                   \
                                                                        \
            WsReleased = MiQueryPerformanceCounter (NULL);              \
            MI_GET_EXECUTION_ADDRESS(ExecutionAddress);                 \
            WsHoldTime.QuadPart = ((ULONG_PTR)WsReleased.LowPart - (ULONG_PTR)PROCESS->Spare0) & ~0x1; \
            MiAddLockToTable (PROCESS->Spare1, ExecutionAddress, WsHoldTime); \
        }

#else
#define LOCK_WS_TIMESTAMP(PROCESS)
#define UNLOCK_WS_TIMESTAMP(PROCESS)
#endif

#define MM_WS_LOCK_ASSERT(WSINFO)                               \
        ASSERT (KeGetCurrentThread () == KeGetOwnerGuardedMutex (&(WSINFO)->WorkingSetMutex))

//
// System working set synchronization definitions.
//

#define MM_SYSTEM_WS_LOCK_TIMESTAMP()                           \
        LOCK_WS_TIMESTAMP(((PEPROCESS)&MiSystemCacheDummyProcess));

#define MM_SYSTEM_WS_UNLOCK_TIMESTAMP()                         \
        UNLOCK_WS_TIMESTAMP(((PEPROCESS)&MiSystemCacheDummyProcess));

#define LOCK_SYSTEM_WS(_Thread)                                         \
            KeAcquireGuardedMutex (&MmSystemCacheWs.WorkingSetMutex);   \
            MM_SYSTEM_WS_LOCK_TIMESTAMP();

#define UNLOCK_SYSTEM_WS()                                              \
            MM_SYSTEM_WS_UNLOCK_TIMESTAMP();                            \
            KeReleaseGuardedMutex (&MmSystemCacheWs.WorkingSetMutex);

//
// Generic working set synchronization definitions.
//

#define LOCK_WORKING_SET(WSINFO)                                        \
            ASSERT (MI_IS_SESSION_ADDRESS(WSINFO) == FALSE);            \
            KeAcquireGuardedMutex (&(WSINFO)->WorkingSetMutex);         \

#define UNLOCK_WORKING_SET(WSINFO)                                      \
            ASSERT (MI_IS_SESSION_ADDRESS(WSINFO) == FALSE);            \
            KeReleaseGuardedMutex (&(WSINFO)->WorkingSetMutex);

//
// Session working set synchronization definitions.
//

#define MM_SESSION_SPACE_WS_LOCK_ASSERT()                               \
            MM_WS_LOCK_ASSERT (&MmSessionSpace->Vm);

//
// Process working set synchronization definitions.
//

#define MI_WS_OWNER(PROCESS) (KeGetOwnerGuardedMutex (&(PROCESS)->Vm.WorkingSetMutex) == KeGetCurrentThread ())
#define MI_NOT_WS_OWNER(PROCESS) (!MI_WS_OWNER(PROCESS))

#define MI_IS_WS_UNSAFE(PROCESS) ((PROCESS)->Vm.Flags.AcquiredUnsafe == 1)

#define LOCK_WS(PROCESS)                                            \
            KeAcquireGuardedMutex (&((PROCESS)->Vm.WorkingSetMutex));      \
            LOCK_WS_TIMESTAMP(PROCESS);                             \
            ASSERT (!MI_IS_WS_UNSAFE(PROCESS));

#define LOCK_WS_UNSAFE(PROCESS)                                     \
            ASSERT (KeAreAllApcsDisabled () == TRUE);               \
            KeAcquireGuardedMutexUnsafe (&((PROCESS)->Vm.WorkingSetMutex));\
            LOCK_WS_TIMESTAMP(PROCESS);                             \
            ASSERT (!MI_IS_WS_UNSAFE(PROCESS));                     \
            (PROCESS)->Vm.Flags.AcquiredUnsafe = 1;

#define MI_MUST_BE_UNSAFE(PROCESS)                                  \
            ASSERT (KeGetCurrentIrql() <= APC_LEVEL);               \
            ASSERT (KeAreAllApcsDisabled () == TRUE);               \
            ASSERT (MI_WS_OWNER(PROCESS));                          \
            ASSERT (MI_IS_WS_UNSAFE(PROCESS));

#define MI_MUST_BE_SAFE(PROCESS)                                    \
            ASSERT (MI_WS_OWNER(PROCESS));                          \
            ASSERT (!MI_IS_WS_UNSAFE(PROCESS));

#define UNLOCK_WS(PROCESS)                                          \
            MI_MUST_BE_SAFE(PROCESS);                               \
            UNLOCK_WS_TIMESTAMP(PROCESS);                           \
            KeReleaseGuardedMutex (&((PROCESS)->Vm.WorkingSetMutex));

#define UNLOCK_WS_UNSAFE(PROCESS)                                   \
            MI_MUST_BE_UNSAFE(PROCESS);                             \
            ASSERT (KeAreAllApcsDisabled () == TRUE);               \
            (PROCESS)->Vm.Flags.AcquiredUnsafe = 0;                 \
            UNLOCK_WS_TIMESTAMP(PROCESS);                           \
            KeReleaseGuardedMutexUnsafe(&((PROCESS)->Vm.WorkingSetMutex)); \
            ASSERT (KeGetCurrentIrql() <= APC_LEVEL);

//
// Address space synchronization definitions.
//

#define LOCK_ADDRESS_SPACE(PROCESS)                                  \
            KeAcquireGuardedMutex (&((PROCESS)->AddressCreationLock));

#define LOCK_WS_AND_ADDRESS_SPACE(PROCESS)                          \
        LOCK_ADDRESS_SPACE(PROCESS);                                \
        LOCK_WS_UNSAFE(PROCESS);

#define UNLOCK_WS_AND_ADDRESS_SPACE(PROCESS)                        \
        UNLOCK_WS_UNSAFE(PROCESS);                                  \
        UNLOCK_ADDRESS_SPACE(PROCESS);

#define UNLOCK_ADDRESS_SPACE(PROCESS)                               \
            KeReleaseGuardedMutex (&((PROCESS)->AddressCreationLock));

//
// The working set lock may have been acquired safely or unsafely.
// Release and reacquire it regardless.
//

#define UNLOCK_WS_REGARDLESS(PROCESS, WSHELDSAFE)                   \
            ASSERT (MI_WS_OWNER (PROCESS));                         \
            if (MI_IS_WS_UNSAFE (PROCESS)) {                        \
                UNLOCK_WS_UNSAFE (PROCESS);                         \
                WSHELDSAFE = FALSE;                                 \
            }                                                       \
            else {                                                  \
                UNLOCK_WS (PROCESS);                                \
                WSHELDSAFE = TRUE;                                  \
            }

#define LOCK_WS_REGARDLESS(PROCESS, WSHELDSAFE)                     \
            if (WSHELDSAFE == TRUE) {                               \
                LOCK_WS (PROCESS);                                  \
            }                                                       \
            else {                                                  \
                LOCK_WS_UNSAFE (PROCESS);                           \
            }

//
// Hyperspace synchronization definitions.
//

#define LOCK_HYPERSPACE(_Process, OLDIRQL)                      \
    ASSERT (_Process == PsGetCurrentProcess ());                \
    ExAcquireSpinLock (&_Process->HyperSpaceLock, OLDIRQL);

#define UNLOCK_HYPERSPACE(_Process, VA, OLDIRQL)                \
    ASSERT (_Process == PsGetCurrentProcess ());                \
    MiGetPteAddress(VA)->u.Long = 0;                            \
    ExReleaseSpinLock (&_Process->HyperSpaceLock, OLDIRQL);

#define LOCK_HYPERSPACE_AT_DPC(_Process)                        \
    ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL);              \
    ASSERT (_Process == PsGetCurrentProcess ());                \
    ExAcquireSpinLockAtDpcLevel (&_Process->HyperSpaceLock);

#define UNLOCK_HYPERSPACE_FROM_DPC(_Process, VA)                \
    ASSERT (KeGetCurrentIrql() == DISPATCH_LEVEL);              \
    ASSERT (_Process == PsGetCurrentProcess ());                \
    MiGetPteAddress(VA)->u.Long = 0;                            \
    ExReleaseSpinLockFromDpcLevel (&_Process->HyperSpaceLock);

#define MiUnmapPageInHyperSpace(_Process, VA, OLDIRQL) UNLOCK_HYPERSPACE(_Process, VA, OLDIRQL)

#define MiUnmapPageInHyperSpaceFromDpc(_Process, VA) UNLOCK_HYPERSPACE_FROM_DPC(_Process, VA)

#define ZERO_LARGE(LargeInteger)                \
        (LargeInteger).LowPart = 0;             \
        (LargeInteger).HighPart = 0;

#define NO_BITS_FOUND   ((ULONG)-1)

//++
//
// ULONG
// MI_CHECK_BIT (
//     IN PULONG ARRAY
//     IN ULONG BIT
//     )
//
// Routine Description:
//
//     The MI_CHECK_BIT macro checks to see if the specified bit is
//     set within the specified array.
//
// Arguments:
//
//     ARRAY - First element of the array to check.
//
//     BIT - bit number (first bit is 0) to check.
//
// Return Value:
//
//     Returns the value of the bit (0 or 1).
//
//--

#define MI_CHECK_BIT(ARRAY,BIT)  \
        (((ULONG)ARRAY[(BIT) / (sizeof(ULONG)*8)] >> ((BIT) & 0x1F)) & 1)


//++
//
// VOID
// MI_SET_BIT (
//     IN PULONG ARRAY
//     IN ULONG BIT
//     )
//
// Routine Description:
//
//     The MI_SET_BIT macro sets the specified bit within the
//     specified array.
//
// Arguments:
//
//     ARRAY - First element of the array to set.
//
//     BIT - bit number.
//
// Return Value:
//
//     None.
//
//--

#define MI_SET_BIT(ARRAY,BIT)  \
        ARRAY[(BIT) / (sizeof(ULONG)*8)] |= (1 << ((BIT) & 0x1F))


//++
//
// VOID
// MI_CLEAR_BIT (
//     IN PULONG ARRAY
//     IN ULONG BIT
//     )
//
// Routine Description:
//
//     The MI_CLEAR_BIT macro sets the specified bit within the
//     specified array.
//
// Arguments:
//
//     ARRAY - First element of the array to clear.
//
//     BIT - bit number.
//
// Return Value:
//
//     None.
//
//--

#define MI_CLEAR_BIT(ARRAY,BIT)  \
        ARRAY[(BIT) / (sizeof(ULONG)*8)] &= ~(1 << ((BIT) & 0x1F))

//
// These control the mirroring capabilities.
//

extern ULONG MmMirroring;

#define MM_MIRRORING_ENABLED    0x1 // Enable mirroring capabilities.
#define MM_MIRRORING_VERIFYING  0x2 // The HAL wants to verify the copy.

extern PRTL_BITMAP MiMirrorBitMap;
extern PRTL_BITMAP MiMirrorBitMap2;
extern LOGICAL MiMirroringActive;

#if defined (_WIN64)
#define MI_MAGIC_AWE_PTEFRAME   0x3ffedcbffffedcb
#else
#define MI_MAGIC_AWE_PTEFRAME   0x3ffedcb
#endif

#define MI_PFN_IS_AWE(Pfn1)                                     \
        ((Pfn1->u2.ShareCount <= 3) &&                          \
         (Pfn1->u3.e1.PageLocation == ActiveAndValid) &&        \
         (Pfn1->u4.VerifierAllocation == 0) &&                  \
         (Pfn1->u3.e1.LargeSessionAllocation == 0) &&           \
         (Pfn1->u3.e1.StartOfAllocation == 1) &&                \
         (Pfn1->u3.e1.EndOfAllocation == 1) &&                  \
         (Pfn1->u4.PteFrame == MI_MAGIC_AWE_PTEFRAME))

//
// The cache type definitions are carefully chosen to line up with the
// MEMORY_CACHING_TYPE definitions to ease conversions.  Any changes here must
// be reflected throughout the code.
//

typedef enum _MI_PFN_CACHE_ATTRIBUTE {
    MiNonCached,
    MiCached,
    MiWriteCombined,
    MiNotMapped
} MI_PFN_CACHE_ATTRIBUTE, *PMI_PFN_CACHE_ATTRIBUTE;

//
// This conversion array is unfortunately needed because not all
// hardware platforms support all possible cache values.  Note that
// the first range is for system RAM, the second range is for I/O space.
//

extern MI_PFN_CACHE_ATTRIBUTE MiPlatformCacheAttributes[2 * MmMaximumCacheType];

//++
//
// MI_PFN_CACHE_ATTRIBUTE
// MI_TRANSLATE_CACHETYPE (
//     IN MEMORY_CACHING_TYPE InputCacheType,
//     IN ULONG IoSpace
//     )
//
// Routine Description:
//
//     Returns the hardware supported cache type for the requested cachetype.
//
// Arguments:
//
//     InputCacheType - Supplies the desired cache type.
//
//     IoSpace - Supplies nonzero (not necessarily 1 though) if this is
//               I/O space, zero if it is main memory.
//
// Return Value:
//
//     The actual cache type.
//
//--
FORCEINLINE
MI_PFN_CACHE_ATTRIBUTE
MI_TRANSLATE_CACHETYPE(
    IN MEMORY_CACHING_TYPE InputCacheType,
    IN ULONG IoSpace
    )
{
    ASSERT (InputCacheType <= MmWriteCombined);

    if (IoSpace != 0) {
        IoSpace = MmMaximumCacheType;
    }
    return MiPlatformCacheAttributes[IoSpace + InputCacheType];
}

//++
//
// VOID
// MI_SET_CACHETYPE_TRANSLATION (
//     IN MEMORY_CACHING_TYPE    InputCacheType,
//     IN ULONG                  IoSpace,
//     IN MI_PFN_CACHE_ATTRIBUTE NewAttribute
//     )
//
// Routine Description:
//
//     This function sets the hardware supported cachetype for the
//     specified cachetype.
//
// Arguments:
//
//     InputCacheType - Supplies the desired cache type.
//
//     IoSpace - Supplies nonzero (not necessarily 1 though) if this is
//               I/O space, zero if it is main memory.
//
//     NewAttribute - Supplies the desired attribute.
//
// Return Value:
//
//     None.
//
//--
FORCEINLINE
VOID
MI_SET_CACHETYPE_TRANSLATION(
    IN MEMORY_CACHING_TYPE    InputCacheType,
    IN ULONG                  IoSpace,
    IN MI_PFN_CACHE_ATTRIBUTE NewAttribute
    )
{
    ASSERT (InputCacheType <= MmWriteCombined);

    if (IoSpace != 0) {
        IoSpace = MmMaximumCacheType;
    }

    MiPlatformCacheAttributes[IoSpace + InputCacheType] = NewAttribute;
}

#define MI_MARK_FRAME_AS_KSTACK(_PageFrameIndex) {      \
        PMMPFN _Pfn1;                                   \
        _Pfn1 = MI_PFN_ELEMENT (_PageFrameIndex);       \
        ASSERT (_Pfn1->u4.KernelStack == 0);            \
        _Pfn1->u4.KernelStack = 1;                      \
}

#define MI_UNMARK_PFN_AS_KSTACK(_Pfn1)                  \
        ASSERT (_Pfn1->u4.KernelStack == 1);            \
        _Pfn1->u4.KernelStack = 0;



//
// PFN database element.
//

//
// Define pseudo fields for start and end of allocation.
//

#define StartOfAllocation ReadInProgress

#define EndOfAllocation WriteInProgress

#define LargeSessionAllocation PrototypePte

typedef struct _MMPFNENTRY {
    ULONG Modified : 1;
    ULONG ReadInProgress : 1;
    ULONG WriteInProgress : 1;
    ULONG PrototypePte: 1;
    ULONG PageColor : 4;
    ULONG PageLocation : 3;
    ULONG RemovalRequested : 1;
    ULONG CacheAttribute : 2;
    ULONG Rom : 1;
    ULONG ParityError : 1;
    ULONG DontUse : 16;         // overlays USHORT for reference count field.
} MMPFNENTRY;

#if defined (_X86PAE_)
#pragma pack(1)
#endif

typedef struct _MMPFN {
    union {
        PFN_NUMBER Flink;
        WSLE_NUMBER WsIndex;
        PKEVENT Event;
        NTSTATUS ReadStatus;

        //
        // Note: NextStackPfn is actually used as SLIST_ENTRY, however
        // because of its alignment characteristics, using that type would
        // unnecessarily add padding to this structure.
        //

        SINGLE_LIST_ENTRY NextStackPfn;
    } u1;
    PMMPTE PteAddress;
    union {
        PFN_NUMBER Blink;
        ULONG_PTR ShareCount;
    } u2;
    union {
        MMPFNENTRY e1;
        struct {
            USHORT ShortFlags;
            USHORT ReferenceCount;
        } e2;
    } u3;
#if defined (_WIN64)
    ULONG UsedPageTableEntries;
#endif
    union {
        MMPTE OriginalPte;
        LONG AweReferenceCount;
    };
    union {
        ULONG_PTR EntireFrame;
        struct {
#if defined (_WIN64)
#define MM_NO_PTE_FRAME (0x3FFFFFFFFFFFFFF)
            ULONG_PTR PteFrame: 58;
#else
#define MM_NO_PTE_FRAME (0x3FFFFFF)
            ULONG_PTR PteFrame: 26;
#endif
            ULONG_PTR InPageError : 1;
            ULONG_PTR VerifierAllocation : 1;
            ULONG_PTR AweAllocation : 1;
            ULONG_PTR LockCharged : 1;      // maintained for DBG only
            ULONG_PTR KernelStack : 1;      // only for valid (not trans) pages
            ULONG_PTR MustBeCached : 1;
        };
    } u4;

} MMPFN, *PMMPFN;

#if defined (_X86PAE_)
#pragma pack()
#endif

//
// No multiplier reciprocal needs to be inlined because the compiler (using Oxt)
// automatically computes the correct number, avoiding the expensive divide
// instruction.
//

#define MI_PFN_ELEMENT_TO_INDEX(_Pfn) ((PFN_NUMBER)(((ULONG_PTR)(_Pfn) - (ULONG_PTR)MmPfnDatabase) / sizeof (MMPFN)))

PVOID
MiGetInstructionPointer (
    VOID
    );

// #define _MI_DEBUG_DIRTY 1         // Uncomment this for dirty bit logging

#if defined (_MI_DEBUG_DIRTY)

extern ULONG MiTrackDirtys;

#define MI_DIRTY_BACKTRACE_LENGTH 17

typedef struct _MI_DIRTY_TRACES {

    PETHREAD Thread;
    PEPROCESS Process;
    PMMPFN Pfn;
    PMMPTE PointerPte;
    ULONG_PTR CallerId;
    ULONG_PTR ShareCount;
    USHORT ShortFlags;
    USHORT ReferenceCount;
    PVOID StackTrace [MI_DIRTY_BACKTRACE_LENGTH];

} MI_DIRTY_TRACES, *PMI_DIRTY_TRACES;

extern LONG MiDirtyIndex;

extern PMI_DIRTY_TRACES MiDirtyTraces;

VOID
FORCEINLINE
MiSnapDirty (
    IN PMMPFN Pfn,
    IN ULONG NewValue,
    IN ULONG CallerId
    )
{
    PEPROCESS Process;
    PMI_DIRTY_TRACES Information;
    ULONG Index;
    ULONG Hash;

    if (MiDirtyTraces == NULL) {
        return;
    }

    Index = InterlockedIncrement (&MiDirtyIndex);
    Index &= (MiTrackDirtys - 1);
    Information = &MiDirtyTraces[Index];

    Process = PsGetCurrentProcess ();

    Information->Thread = PsGetCurrentThread ();
    Information->Process = Process; 
    Information->Pfn = Pfn;
    Information->PointerPte = Pfn->PteAddress;
    Information->CallerId = CallerId;
    Information->ShareCount = Pfn->u2.ShareCount;
    Information->ShortFlags = Pfn->u3.e2.ShortFlags;
    Information->ReferenceCount = Pfn->u3.e2.ReferenceCount;

    if (NewValue != 0) {
        Information->Process = (PEPROCESS) ((ULONG_PTR)Process | 0x1);
    }

    RtlZeroMemory (&Information->StackTrace[0], MI_DIRTY_BACKTRACE_LENGTH * sizeof(PVOID)); 

#if defined (_WIN64)
    if (KeAreAllApcsDisabled () == TRUE) {
        Information->StackTrace[1] = (PVOID) _ReturnAddress ();
        Information->StackTrace[0] = MiGetInstructionPointer ();
    }
    else
#endif
    RtlCaptureStackBackTrace (0, MI_DIRTY_BACKTRACE_LENGTH, Information->StackTrace, &Hash);
}

#define MI_SNAP_DIRTY(_Pfn, _NewValue, _Callerid) MiSnapDirty(_Pfn, _NewValue, _Callerid)

#else
#define MI_SNAP_DIRTY(_Pfn, _NewValue, _Callerid)
#endif

#if DBG || defined (_MI_DEBUG_ALTPTE)
#define _MI_DEBUG_PTE 1         // Enable PTE change logging
#endif

#if defined (_MI_DEBUG_PTE)

#define MI_PTE_BACKTRACE_LENGTH 7

typedef struct _MI_PTE_TRACES {

    PETHREAD Thread;
    PEPROCESS Process;
    PMMPTE PointerPte;
    MMPTE PteContents;
    MMPTE NewPteContents;
    PVOID StackTrace [MI_PTE_BACKTRACE_LENGTH];

} MI_PTE_TRACES, *PMI_PTE_TRACES;

extern LONG MiPteIndex;

#define MI_PTE_TRACE_SIZE   0x4000

extern MI_PTE_TRACES MiPteTraces[MI_PTE_TRACE_SIZE];

extern LONG MiInDebugger;

VOID
FORCEINLINE
MiSnapPte (
    IN PMMPTE PointerPte,
    IN MMPTE NewValue
    )
{
    ULONG Hash;
    ULONG Index;
    PMI_PTE_TRACES Information;

    Index = InterlockedIncrement (&MiPteIndex);
    Index &= (MI_PTE_TRACE_SIZE - 1);
    Information = &MiPteTraces[Index];

    Information->Thread = PsGetCurrentThread ();
    Information->Process = PsGetCurrentProcess ();
    Information->PteContents = *PointerPte;
    Information->NewPteContents = NewValue;
    Information->PointerPte = PointerPte;

    if ((PointerPte < MiGetPteAddress (MmHighestUserAddress)) &&
        (PointerPte >= MiGetPteAddress (0))) {

        //
        // The current thread must own this process' working set mutex.
        //

        if (MiInDebugger == 0) {
            ASSERT (MI_WS_OWNER (PsGetCurrentProcess()));
        }

#if !defined(_IA64_)
        if ((NewValue.u.Hard.Valid == 1) && (NewValue.u.Hard.LargePage == 1)) {
            DbgPrint ("Marking PTE %p as large %p\n", PointerPte, NewValue.u.Long);
            DbgBreakPoint ();
        }
#endif
    }

    RtlZeroMemory (&Information->StackTrace[0], MI_PTE_BACKTRACE_LENGTH * sizeof(PVOID)); 

#if defined (_WIN64)
    if (KeAreAllApcsDisabled () == TRUE) {
        Information->StackTrace[1] = (PVOID) _ReturnAddress ();
        Information->StackTrace[0] = MiGetInstructionPointer ();
    }
    else
#endif
    RtlCaptureStackBackTrace (0, MI_PTE_BACKTRACE_LENGTH, Information->StackTrace, &Hash);

#if defined (_MI_DEBUG_ALTPTE)
    if (PsGetCurrentProcess()->Wow64Process != NULL) {
        MiLogPteInAltTrace ((PVOID) Information);
    }
#endif
}

#define MI_LOG_PTE_CHANGE(_PointerPte, _PteContents)    MiSnapPte(_PointerPte, _PteContents)

#else
#define MI_LOG_PTE_CHANGE(_PointerPte, _PteContents)
#endif


#define MI_DEBUGGER_WRITE_VALID_PTE_NEW_PROTECTION(_PointerPte, _PteContents) \
            InterlockedIncrement (&MiInDebugger);                             \
            MI_WRITE_VALID_PTE_NEW_PROTECTION(_PointerPte, _PteContents);     \
            InterlockedDecrement (&MiInDebugger);




#if 0
#define MI_STAMP_MODIFIED(Pfn,id)   (Pfn)->u4.Reserved = (id);
#else
#define MI_STAMP_MODIFIED(Pfn,id)
#endif

//++
//
// VOID
// MI_SET_MODIFIED (
//     IN PMMPFN Pfn,
//     IN ULONG NewValue,
//     IN ULONG CallerId
//     )
//
// Routine Description:
//
//     Set (or clear) the modified bit in the PFN database element.
//     The PFN lock must be held.
//
// Arguments:
//
//     Pfn - Supplies the PFN to operate on.
//
//     NewValue - Supplies 1 to set the modified bit, 0 to clear it.
//
//     CallerId - Supplies a caller ID useful for debugging purposes.
//
// Return Value:
//
//     None.
//
//--
//
#define MI_SET_MODIFIED(_Pfn, _NewValue, _CallerId)             \
            ASSERT ((_Pfn)->u3.e1.Rom == 0);                    \
            MI_SNAP_DIRTY (_Pfn, _NewValue, _CallerId);         \
            if ((_NewValue) != 0) {                             \
                MI_STAMP_MODIFIED (_Pfn, _CallerId);            \
            }                                                   \
            (_Pfn)->u3.e1.Modified = (_NewValue);

//
// ccNUMA is supported in multiprocessor PAE and WIN64 systems only.
//

#if (defined(_WIN64) || defined(_X86PAE_)) && !defined(NT_UP)
#define MI_MULTINODE

VOID
MiDetermineNode (
    IN PFN_NUMBER Page,
    IN PMMPFN Pfn
    );

#else

#define MiDetermineNode(x,y)    ((y)->u3.e1.PageColor = 0)

#endif

#if defined (_WIN64)

//
// Note there are some places where these portable macros are not currently
// used because we are not in the correct address space required.
//

#define MI_CAPTURE_USED_PAGETABLE_ENTRIES(PFN) \
        ASSERT ((PFN)->UsedPageTableEntries <= PTE_PER_PAGE); \
        (PFN)->OriginalPte.u.Soft.UsedPageTableEntries = (PFN)->UsedPageTableEntries;

#define MI_RETRIEVE_USED_PAGETABLE_ENTRIES_FROM_PTE(RBL, PTE) \
        ASSERT ((PTE)->u.Soft.UsedPageTableEntries <= PTE_PER_PAGE); \
        (RBL)->UsedPageTableEntries = (ULONG)(((PMMPTE)(PTE))->u.Soft.UsedPageTableEntries);

#define MI_ZERO_USED_PAGETABLE_ENTRIES_IN_INPAGE_SUPPORT(INPAGE_SUPPORT) \
            (INPAGE_SUPPORT)->UsedPageTableEntries = 0;

#define MI_ZERO_USED_PAGETABLE_ENTRIES_IN_PFN(PFN) (PFN)->UsedPageTableEntries = 0;

#define MI_INSERT_USED_PAGETABLE_ENTRIES_IN_PFN(PFN, INPAGE_SUPPORT) \
        ASSERT ((INPAGE_SUPPORT)->UsedPageTableEntries <= PTE_PER_PAGE); \
        (PFN)->UsedPageTableEntries = (INPAGE_SUPPORT)->UsedPageTableEntries;

#define MI_ZERO_USED_PAGETABLE_ENTRIES(PFN) \
        (PFN)->UsedPageTableEntries = 0;

#define MI_CHECK_USED_PTES_HANDLE(VA) \
        ASSERT (MiGetPdeAddress(VA)->u.Hard.Valid == 1);

#define MI_GET_USED_PTES_HANDLE(VA) \
        ((PVOID)MI_PFN_ELEMENT((PFN_NUMBER)MiGetPdeAddress(VA)->u.Hard.PageFrameNumber))

#define MI_GET_USED_PTES_FROM_HANDLE(PFN) \
        ((ULONG)(((PMMPFN)(PFN))->UsedPageTableEntries))

#define MI_INCREMENT_USED_PTES_BY_HANDLE(PFN) \
        (((PMMPFN)(PFN))->UsedPageTableEntries += 1); \
        ASSERT (((PMMPFN)(PFN))->UsedPageTableEntries <= PTE_PER_PAGE)

#define MI_INCREMENT_USED_PTES_BY_HANDLE_CLUSTER(PFN,INCR) \
        (((PMMPFN)(PFN))->UsedPageTableEntries += (ULONG)(INCR)); \
        ASSERT (((PMMPFN)(PFN))->UsedPageTableEntries <= PTE_PER_PAGE)

#define MI_DECREMENT_USED_PTES_BY_HANDLE(PFN) \
        (((PMMPFN)(PFN))->UsedPageTableEntries -= 1); \
        ASSERT (((PMMPFN)(PFN))->UsedPageTableEntries < PTE_PER_PAGE)

#else

#define MI_CAPTURE_USED_PAGETABLE_ENTRIES(PFN)
#define MI_RETRIEVE_USED_PAGETABLE_ENTRIES_FROM_PTE(RBL, PTE)
#define MI_ZERO_USED_PAGETABLE_ENTRIES_IN_INPAGE_SUPPORT(INPAGE_SUPPORT)
#define MI_ZERO_USED_PAGETABLE_ENTRIES_IN_PFN(PFN)

#define MI_INSERT_USED_PAGETABLE_ENTRIES_IN_PFN(PFN, INPAGE_SUPPORT)

#define MI_CHECK_USED_PTES_HANDLE(VA)

#define MI_GET_USED_PTES_HANDLE(VA) ((PVOID)&MmWorkingSetList->UsedPageTableEntries[MiGetPdeIndex(VA)])

#define MI_GET_USED_PTES_FROM_HANDLE(PDSHORT) ((ULONG)(*(PUSHORT)(PDSHORT)))

#define MI_INCREMENT_USED_PTES_BY_HANDLE(PDSHORT) \
    ((*(PUSHORT)(PDSHORT)) += 1); \
    ASSERT (((*(PUSHORT)(PDSHORT)) <= PTE_PER_PAGE))

#define MI_INCREMENT_USED_PTES_BY_HANDLE_CLUSTER(PDSHORT,INCR) \
    (*(PUSHORT)(PDSHORT)) = (USHORT)((*(PUSHORT)(PDSHORT)) + (INCR)); \
    ASSERT (((*(PUSHORT)(PDSHORT)) <= PTE_PER_PAGE))

#define MI_DECREMENT_USED_PTES_BY_HANDLE(PDSHORT) \
    ((*(PUSHORT)(PDSHORT)) -= 1); \
    ASSERT (((*(PUSHORT)(PDSHORT)) < PTE_PER_PAGE))

#endif

extern PFN_NUMBER MmDynamicPfn;

extern KGUARDED_MUTEX MmDynamicMemoryMutex;

//
// Cache attribute tracking for I/O space mappings.
//

#define MI_IO_BACKTRACE_LENGTH  6

typedef struct _MMIO_TRACKER {
    LIST_ENTRY ListEntry;
    PVOID BaseVa;
    PFN_NUMBER PageFrameIndex;
    PFN_NUMBER NumberOfPages;
    MI_PFN_CACHE_ATTRIBUTE CacheAttribute;
    PVOID StackTrace[MI_IO_BACKTRACE_LENGTH];
} MMIO_TRACKER, *PMMIO_TRACKER;

extern KSPIN_LOCK MmIoTrackerLock;
extern LIST_ENTRY MmIoHeader;

extern PCHAR MiCacheStrings[];

MI_PFN_CACHE_ATTRIBUTE
MiInsertIoSpaceMap (
    IN PVOID BaseVa,
    IN PFN_NUMBER PageFrameIndex,
    IN PFN_NUMBER NumberOfPages,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute
    );

//
// Total number of committed pages.
//

extern SIZE_T MmTotalCommittedPages;

extern SIZE_T MmTotalCommitLimit;

extern SIZE_T MmSharedCommit;

#if DBG

extern SPFN_NUMBER MiLockedCommit;

#define MI_INCREMENT_LOCKED_COMMIT()        \
            MiLockedCommit += 1;

#define MI_DECREMENT_LOCKED_COMMIT()        \
            ASSERT (MiLockedCommit > 0);    \
            MiLockedCommit -= 1;

#else

#define MI_INCREMENT_LOCKED_COMMIT()
#define MI_DECREMENT_LOCKED_COMMIT()

#endif

#if defined(_WIN64)

#define MiChargeCommitmentRegardless() \
    MI_INCREMENT_LOCKED_COMMIT(); \
    InterlockedIncrement64 ((PLONGLONG) &MmTotalCommittedPages);

#define MiReturnCommitmentRegardless() \
    MI_DECREMENT_LOCKED_COMMIT(); \
    InterlockedDecrement64 ((PLONGLONG) &MmTotalCommittedPages);

#else

#define MiChargeCommitmentRegardless() \
    MI_INCREMENT_LOCKED_COMMIT(); \
    InterlockedIncrement ((PLONG) &MmTotalCommittedPages);

#define MiReturnCommitmentRegardless() \
    MI_DECREMENT_LOCKED_COMMIT(); \
    InterlockedDecrement ((PLONG) &MmTotalCommittedPages);

#endif

extern ULONG MiChargeCommitmentFailures[3];   // referenced also in mi.h macros.

FORCEINLINE
LOGICAL
MiChargeCommitmentPfnLockHeld (
    IN LOGICAL Force
    )

/*++

Routine Description:

    This routine charges the specified commitment without attempting
    to expand paging files and waiting for the expansion.

Arguments:

    Force - Supplies TRUE if the lock is short-term and should be forced through
            if necessary.

Return Value:

    TRUE if the commitment was permitted, FALSE if not.

Environment:

    Kernel mode, PFN lock is held.

--*/

{
    if (MmTotalCommittedPages > MmTotalCommitLimit - 64) {

        if ((Force == 0) && (PsGetCurrentThread()->MemoryMaker == 0)) {
            MiChargeCommitmentFailures[2] += 1;
            return FALSE;
        }
    }

    //
    // No need to do an InterlockedCompareExchange for this, keep it fast.
    //

    MiChargeCommitmentRegardless ();
                                                             
    return TRUE;
}

extern PFN_NUMBER MmSystemLockPagesCount;

#if DBG

#define MI_LOCK_ID_COUNTER_MAX 64
ULONG MiLockIds[MI_LOCK_ID_COUNTER_MAX];

#define MI_MARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId)      \
         ASSERT (Pfn->u4.LockCharged == 0);          \
         ASSERT (CallerId < MI_LOCK_ID_COUNTER_MAX);    \
         MiLockIds[CallerId] += 1;                      \
         Pfn->u4.LockCharged = 1;

#define MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId)    \
         ASSERT (Pfn->u4.LockCharged == 1);          \
         ASSERT (CallerId < MI_LOCK_ID_COUNTER_MAX);    \
         MiLockIds[CallerId] += 1;                      \
         Pfn->u4.LockCharged = 0;

#else
#define MI_MARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId)
#define MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId)
#endif

FORCEINLINE
LOGICAL
MI_ADD_LOCKED_PAGE_CHARGE (
    IN PMMPFN Pfn1,
    IN LOGICAL Force,
    IN ULONG CallerId
    )

/*++

Routine Description:

    Charge the systemwide count of locked pages if this is the initial
    lock for this page (multiple concurrent locks are only charged once).

Arguments:

    Pfn - Supplies the PFN index to operate on.

    Force - Supplies TRUE if the lock is short-term and should be forced through
            if necessary.

    CallerId - Supplies the ID of the caller, only used in debug builds.

Return Value:

    TRUE if the charge succeeded, FALSE if not.

Environment:

    Kernel mode.  PFN lock held.

--*/

{
#if !DBG
    UNREFERENCED_PARAMETER (CallerId);
#endif

    ASSERT (Pfn1->u3.e2.ReferenceCount != 0);

    if (Pfn1->u3.e2.ReferenceCount == 1) {

        if (Pfn1->u2.ShareCount != 0) {

            ASSERT (Pfn1->u3.e1.PageLocation == ActiveAndValid);

            if ((Pfn1->u3.e1.PrototypePte == 1) &&
                (Pfn1->OriginalPte.u.Soft.Prototype == 1)) {

                //
                // This is a filesystem-backed page - charge commit for
                // it as we have no way to tell when the caller will
                // unlock the page.
                //

                if (MiChargeCommitmentPfnLockHeld (Force) == FALSE) {
                    return FALSE;
                }
            }

            MI_MARK_PFN_AS_LOCK_CHARGED (Pfn1, CallerId);

            MmSystemLockPagesCount += 1;
        }
        else {
            ASSERT (Pfn1->u4.LockCharged == 1);
        }
    }

    return TRUE;
}

FORCEINLINE
LOGICAL
MI_ADD_LOCKED_PAGE_CHARGE_FOR_MODIFIED_PAGE (
    IN PMMPFN Pfn1,
    IN LOGICAL Force,
    IN ULONG CallerId
    )

/*++

Routine Description:

    Charge the systemwide count of locked pages if this is the initial
    lock for this page (multiple concurrent locks are only charged once).

Arguments:

    Pfn - the PFN index to operate on.

    Force - Supplies TRUE if the lock is short-term and should be forced through
            if necessary.

    CallerId - the ID of the caller, only used in debug builds.

Return Value:

    TRUE if the charge succeeded, FALSE if not.

Environment:

    Kernel mode.  PFN lock held.

--*/

{
#if !DBG
    UNREFERENCED_PARAMETER (CallerId);
#endif

    ASSERT (Pfn1->u3.e1.PageLocation != ActiveAndValid);
    ASSERT (Pfn1->u2.ShareCount == 0);

    if (Pfn1->u3.e2.ReferenceCount == 0) {

        if ((Pfn1->u3.e1.PrototypePte == 1) &&
            (Pfn1->OriginalPte.u.Soft.Prototype == 1)) {

            //
            // This is a filesystem-backed page - charge commit for
            // it as we have no way to tell when the caller will
            // unlock the page.
            //

            if (MiChargeCommitmentPfnLockHeld (Force) == FALSE) {
                return FALSE;
            }
        }

        MI_MARK_PFN_AS_LOCK_CHARGED(Pfn1, CallerId);

        MmSystemLockPagesCount += 1;
    }

    return TRUE;
}

//++
//
// VOID
// MI_REMOVE_LOCKED_PAGE_CHARGE (
//     IN PMMPFN Pfn
//     )
//
// Routine Description:
//
//     Remove the charge from the systemwide count of locked pages if this
//     is the last lock for this page (multiple concurrent locks are only
//     charged once).
//
//     The PFN reference checks are carefully ordered so the most common case
//     is handled first, the next most common case next, etc.  The case of
//     a reference count of 2 occurs more than 1000x (yes, 3 orders of
//     magnitude) more than a reference count of 1.  And reference counts of >2
//     occur 3 orders of magnitude more frequently than reference counts of
//     exactly 1.
//
// Arguments:
//
//     Pfn - the PFN index to operate on.
//
// Return Value:
//
//     None.
//
//--
//
#define MI_REMOVE_LOCKED_PAGE_CHARGE(Pfn, CallerId)                         \
        ASSERT (Pfn->u3.e2.ReferenceCount != 0);                            \
        if (Pfn->u3.e2.ReferenceCount == 2) {                               \
            if (Pfn->u2.ShareCount >= 1) {                                  \
                ASSERT (Pfn->u3.e1.PageLocation == ActiveAndValid);         \
                MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId);               \
                if ((Pfn->u3.e1.PrototypePte == 1) &&                       \
                    (Pfn->OriginalPte.u.Soft.Prototype == 1)) {             \
                    MiReturnCommitmentRegardless();                         \
                }                                                           \
                MmSystemLockPagesCount -= 1;                                \
            }                                                               \
            else {                                                          \
                /*                                                          \
                 * There are multiple referencers to this page and the      \
                 * page is no longer valid in any process address space.    \
                 * The systemwide lock count can only be decremented        \
                 * by the last dereference.                                 \
                 */                                                         \
                NOTHING;                                                    \
            }                                                               \
        }                                                                   \
        else if (Pfn->u3.e2.ReferenceCount != 1) {                          \
            /*                                                              \
             * There are still multiple referencers to this page (it may    \
             * or may not be resident in any process address space).        \
             * Since the systemwide lock count can only be decremented      \
             * by the last dereference (and this is not it), no action      \
             * is taken here.                                               \
             */                                                             \
            ASSERT (Pfn->u3.e2.ReferenceCount > 2);                         \
            NOTHING;                                                        \
        }                                                                   \
        else {                                                              \
            /*                                                              \
             * This page has already been deleted from all process address  \
             * spaces.  It is sitting in limbo (not on any list) awaiting   \
             * this last dereference.                                       \
             */                                                             \
            ASSERT (Pfn->u3.e2.ReferenceCount == 1);                        \
            ASSERT (Pfn->u3.e1.PageLocation != ActiveAndValid);             \
            ASSERT (Pfn->u2.ShareCount == 0);                               \
            MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId);                   \
            if ((Pfn->u3.e1.PrototypePte == 1) &&                           \
                (Pfn->OriginalPte.u.Soft.Prototype == 1)) {                 \
                MiReturnCommitmentRegardless();                             \
            }                                                               \
            MmSystemLockPagesCount -= 1;                                    \
        }

//++
//
// VOID
// MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF (
//     IN PMMPFN Pfn
//     )
//
// Routine Description:
//
//     Remove the charge from the systemwide count of locked pages if this
//     is the last lock for this page (multiple concurrent locks are only
//     charged once).
//
//     The PFN reference checks are carefully ordered so the most common case
//     is handled first, the next most common case next, etc.  The case of
//     a reference count of 2 occurs more than 1000x (yes, 3 orders of
//     magnitude) more than a reference count of 1.  And reference counts of >2
//     occur 3 orders of magnitude more frequently than reference counts of
//     exactly 1.
//
//     The PFN reference count is then decremented.
//
// Arguments:
//
//     Pfn - the PFN index to operate on.
//
// Return Value:
//
//     None.
//
//--
//
#define MI_REMOVE_LOCKED_PAGE_CHARGE_AND_DECREF(Pfn, CallerId)              \
        ASSERT (Pfn->u3.e2.ReferenceCount != 0);                            \
        if (Pfn->u3.e2.ReferenceCount == 2) {                               \
            if (Pfn->u2.ShareCount >= 1) {                                  \
                ASSERT (Pfn->u3.e1.PageLocation == ActiveAndValid);         \
                MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId);               \
                if ((Pfn->u3.e1.PrototypePte == 1) &&                       \
                    (Pfn->OriginalPte.u.Soft.Prototype == 1)) {             \
                    MiReturnCommitmentRegardless();                         \
                }                                                           \
                MmSystemLockPagesCount -= 1;                                \
            }                                                               \
            else {                                                          \
                /*                                                          \
                 * There are multiple referencers to this page and the      \
                 * page is no longer valid in any process address space.    \
                 * The systemwide lock count can only be decremented        \
                 * by the last dereference.                                 \
                 */                                                         \
                NOTHING;                                                    \
            }                                                               \
            Pfn->u3.e2.ReferenceCount -= 1;                                 \
        }                                                                   \
        else if (Pfn->u3.e2.ReferenceCount != 1) {                          \
            /*                                                              \
             * There are still multiple referencers to this page (it may    \
             * or may not be resident in any process address space).        \
             * Since the systemwide lock count can only be decremented      \
             * by the last dereference (and this is not it), no action      \
             * is taken here.                                               \
             */                                                             \
            ASSERT (Pfn->u3.e2.ReferenceCount > 2);                         \
            Pfn->u3.e2.ReferenceCount -= 1;                                 \
        }                                                                   \
        else {                                                              \
            /*                                                              \
             * This page has already been deleted from all process address  \
             * spaces.  It is sitting in limbo (not on any list) awaiting   \
             * this last dereference.                                       \
             */                                                             \
            PFN_NUMBER _PageFrameIndex;                                     \
            ASSERT (Pfn->u3.e2.ReferenceCount == 1);                        \
            ASSERT (Pfn->u3.e1.PageLocation != ActiveAndValid);             \
            ASSERT (Pfn->u2.ShareCount == 0);                               \
            MI_UNMARK_PFN_AS_LOCK_CHARGED(Pfn, CallerId);                   \
            if ((Pfn->u3.e1.PrototypePte == 1) &&                           \
                (Pfn->OriginalPte.u.Soft.Prototype == 1)) {                 \
                MiReturnCommitmentRegardless();                             \
            }                                                               \
            MmSystemLockPagesCount -= 1;                                    \
            _PageFrameIndex = MI_PFN_ELEMENT_TO_INDEX(Pfn);                 \
            MiDecrementReferenceCount (Pfn, _PageFrameIndex);               \
        }

//++
//
// VOID
// MI_ZERO_WSINDEX (
//     IN PMMPFN Pfn
//     )
//
// Routine Description:
//
//     Zero the Working Set Index field of the argument PFN entry.
//     There is a subtlety here on systems where the WsIndex ULONG is
//     overlaid with an Event pointer and sizeof(ULONG) != sizeof(PKEVENT).
//     Note this will need to be updated if we ever decide to allocate bodies of
//     thread objects on 4GB boundaries.
//
// Arguments:
//
//     Pfn - the PFN index to operate on.
//
// Return Value:
//
//     None.
//
//--
//
#define MI_ZERO_WSINDEX(Pfn) \
    Pfn->u1.Event = NULL;

typedef enum _MMSHARE_TYPE {
    Normal,
    ShareCountOnly,
    AndValid
} MMSHARE_TYPE;

typedef struct _MMWSLE_HASH {
    PVOID Key;
    WSLE_NUMBER Index;
} MMWSLE_HASH, *PMMWSLE_HASH;

//++
//
// WSLE_NUMBER
// MI_WSLE_HASH (
//     IN ULONG_PTR VirtualAddress,
//     IN PMMWSL WorkingSetList
//     )
//
// Routine Description:
//
//     Hash the address
//
// Arguments:
//
//     VirtualAddress - the address to hash.
//
//     WorkingSetList - the working set to hash the address into.
//
// Return Value:
//
//     The hash key.
//
//--
//
#define MI_WSLE_HASH(Address, Wsl) \
    ((WSLE_NUMBER)(((ULONG_PTR)PAGE_ALIGN(Address) >> (PAGE_SHIFT - 2)) % \
        ((Wsl)->HashTableSize - 1)))

//
// Working Set List Entry.
//

typedef struct _MMWSLENTRY {
    ULONG_PTR Valid : 1;
    ULONG_PTR LockedInWs : 1;
    ULONG_PTR LockedInMemory : 1;
    ULONG_PTR Protection : 5;
    ULONG_PTR SameProtectAsProto : 1;
    ULONG_PTR Direct : 1;
    ULONG_PTR Age : 2;
#if MM_VIRTUAL_PAGE_FILLER
    ULONG_PTR Filler : MM_VIRTUAL_PAGE_FILLER;
#endif
    ULONG_PTR VirtualPageNumber : MM_VIRTUAL_PAGE_SIZE;
} MMWSLENTRY;

typedef struct _MMWSLE {
    union {
        PVOID VirtualAddress;
        ULONG_PTR Long;
        MMWSLENTRY e1;
    } u1;
} MMWSLE;

#define MI_GENERATE_VALID_WSLE(Wsle)                   \
        ((PVOID)(ULONG_PTR)((Wsle)->u1.Long & (~(PAGE_SIZE - 1) | 0x1)))

#define MI_GET_PROTECTION_FROM_WSLE(Wsl) ((Wsl)->u1.e1.Protection)

typedef MMWSLE *PMMWSLE;

//
// Working Set List.  Must be quadword sized.
//

typedef struct _MMWSL {
    WSLE_NUMBER FirstFree;
    WSLE_NUMBER FirstDynamic;
    WSLE_NUMBER LastEntry;
    WSLE_NUMBER NextSlot;               // The next slot to trim
    PMMWSLE Wsle;
    WSLE_NUMBER LastInitializedWsle;
    WSLE_NUMBER NonDirectCount;
    PMMWSLE_HASH HashTable;
    ULONG HashTableSize;
    ULONG NumberOfCommittedPageTables;
    PVOID HashTableStart;
    PVOID HighestPermittedHashAddress;
    ULONG NumberOfImageWaiters;
    ULONG VadBitMapHint;

#if _WIN64
    PVOID HighestUserAddress;           // Maintained for wow64 processes only
#endif

#if defined(_MIALT4K_)
    PMMPTE HighestUserPte;
    PMMPTE HighestAltPte;
#endif

#if (_MI_PAGING_LEVELS >= 4)
    PULONG CommittedPageTables;

    ULONG NumberOfCommittedPageDirectories;
    PULONG CommittedPageDirectories;

    ULONG NumberOfCommittedPageDirectoryParents;
    ULONG CommittedPageDirectoryParents[(MM_USER_PAGE_DIRECTORY_PARENT_PAGES + sizeof(ULONG)*8-1)/(sizeof(ULONG)*8)];

#elif (_MI_PAGING_LEVELS >= 3)
    PULONG CommittedPageTables;

    ULONG NumberOfCommittedPageDirectories;
    ULONG CommittedPageDirectories[(MM_USER_PAGE_DIRECTORY_PAGES + sizeof(ULONG)*8-1)/(sizeof(ULONG)*8)];

#else

    //
    // This must be at the end.
    // Not used in system cache or session working set lists.
    //

    USHORT UsedPageTableEntries[MM_USER_PAGE_TABLE_PAGES];

    ULONG CommittedPageTables[MM_USER_PAGE_TABLE_PAGES/(sizeof(ULONG)*8)];
#endif

} MMWSL, *PMMWSL;

// #define _MI_DEBUG_WSLE 1         // Enable WSLE change logging

#if defined (_MI_DEBUG_WSLE)

#define MI_WSLE_BACKTRACE_LENGTH 8

typedef struct _MI_WSLE_TRACES {

    PETHREAD Thread;
    PVOID Pad0;
    PMMWSLE Wsle;
    MMWSLE WsleContents;

    MMWSLE NewWsleContents;
    WSLE_NUMBER WorkingSetSize;
    WSLE_NUMBER Quota;
    WSLE_NUMBER LastInitializedWsle;

    PVOID StackTrace [MI_WSLE_BACKTRACE_LENGTH];

} MI_WSLE_TRACES, *PMI_WSLE_TRACES;

extern LONG MiWsleIndex;

#define MI_WSLE_TRACE_SIZE   0x1000

extern MI_WSLE_TRACES MiWsleTraces[MI_WSLE_TRACE_SIZE];

#if defined(_X86_)
extern PMMWSL MmWorkingSetList;
#endif

VOID
FORCEINLINE
MiSnapWsle (
    IN PMMWSL WorkingSetList,
    IN WSLE_NUMBER WorkingSetIndex,
    IN MMWSLE WsleValue
    )
{
    PMMSUPPORT WsInfo;
    ULONG Hash;
    ULONG Index;
    PMI_WSLE_TRACES Information;
    PVOID MatchVa;
    WSLE_NUMBER j;
    PMMWSLE Wsle;
    PEPROCESS Process;

    if (WorkingSetList != MmWorkingSetList) {
        return;
    }

    Process = PsGetCurrentProcess ();
    Information = (PMI_WSLE_TRACES) Process->Spare3[0];
    if (Information == NULL) {
        return;
    }

    Wsle = WorkingSetList->Wsle;

    if (WsleValue.u1.e1.Valid == 1) {
        MatchVa = PAGE_ALIGN (WsleValue.u1.VirtualAddress);

        for (j = 0; j <= WorkingSetList->LastInitializedWsle; j += 1) {

            if ((Wsle->u1.e1.Valid == 1) &&
                (PAGE_ALIGN (Wsle->u1.VirtualAddress) == MatchVa) &&
                (j != WorkingSetIndex)) {

                DbgPrint ("MMWSLE2: DUP %p %x %x\n", WsleValue, WorkingSetIndex, j);
                DbgBreakPoint ();
            }
            Wsle += 1;
        }
    }

    WsInfo = &Process->Vm;

    Index = InterlockedIncrement ((PLONG)&Process->Spare3[1]);
    Index &= (MI_WSLE_TRACE_SIZE - 1);
    Information += Index;

    Information->Thread = PsGetCurrentThread ();
    Information->Wsle = &WorkingSetList->Wsle[WorkingSetIndex];
    Information->WsleContents = WorkingSetList->Wsle[WorkingSetIndex];
    Information->NewWsleContents = WsleValue;

    Information->WorkingSetSize = WsInfo->WorkingSetSize;
    Information->Quota = WorkingSetList->Quota;
    Information->LastInitializedWsle = WorkingSetList->LastInitializedWsle;

    if ((PointerPte < MiGetPteAddress (MmHighestUserAddress)) &&
        (PointerPte >= MiGetPteAddress (0))) {


        //
        // The current thread must own this process' working set mutex.
        //

        ASSERT (MI_WS_OWNER (PsGetCurrentProcess()));
    }

    RtlZeroMemory (&Information->StackTrace[0], MI_WSLE_BACKTRACE_LENGTH * sizeof(PVOID)); 

#if defined (_WIN64)
    if (KeAreAllApcsDisabled () == TRUE) {
        Information->StackTrace[1] = (PVOID) _ReturnAddress ();
        Information->StackTrace[0] = MiGetInstructionPointer ();
    }
    else
#endif
    RtlCaptureStackBackTrace (0, MI_WSLE_BACKTRACE_LENGTH, Information->StackTrace, &Hash);
}

#define MI_LOG_WSLE_CHANGE(_WorkingSetList, _WorkingSetIndex, _WsleValue)    MiSnapWsle(_WorkingSetList, _WorkingSetIndex, _WsleValue)

#else
#define MI_LOG_WSLE_CHANGE(_WorkingSetList, _WorkingSetIndex, _WsleValue)
#endif

#if defined(_X86_)
extern PMMWSL MmWorkingSetList;
#endif

extern PKEVENT MiHighMemoryEvent;
extern PKEVENT MiLowMemoryEvent;

//
// The claim estimate of unused pages in a working set is limited
// to grow by this amount per estimation period.
//

#define MI_CLAIM_INCR 30

//
// The maximum number of different ages a page can be.
//

#define MI_USE_AGE_COUNT 4
#define MI_USE_AGE_MAX (MI_USE_AGE_COUNT - 1)

//
// If more than this "percentage" of the working set is estimated to
// be used then allow it to grow freely.
//

#define MI_REPLACEMENT_FREE_GROWTH_SHIFT 5

//
// If more than this "percentage" of the working set has been claimed
// then force replacement in low memory.
//

#define MI_REPLACEMENT_CLAIM_THRESHOLD_SHIFT 3

//
// If more than this "percentage" of the working set is estimated to
// be available then force replacement in low memory.
//

#define MI_REPLACEMENT_EAVAIL_THRESHOLD_SHIFT 3

//
// If while doing replacement a page is found of this age or older then
// replace it.  Otherwise the oldest is selected.
//

#define MI_IMMEDIATE_REPLACEMENT_AGE 2

//
// When trimming, use these ages for different passes.
//

#define MI_MAX_TRIM_PASSES 4
#define MI_PASS0_TRIM_AGE 2
#define MI_PASS1_TRIM_AGE 1
#define MI_PASS2_TRIM_AGE 1
#define MI_PASS3_TRIM_AGE 1
#define MI_PASS4_TRIM_AGE 0

//
// If not a forced trim, trim pages older than this age.
//

#define MI_TRIM_AGE_THRESHOLD 2

//
// This "percentage" of a claim is up for grabs in a foreground process.
//

#define MI_FOREGROUND_CLAIM_AVAILABLE_SHIFT 3

//
// This "percentage" of a claim is up for grabs in a background process.
//

#define MI_BACKGROUND_CLAIM_AVAILABLE_SHIFT 1

//++
//
// DWORD
// MI_CALC_NEXT_VALID_ESTIMATION_SLOT (
//     DWORD Previous,
//     DWORD Minimum,
//     DWORD Maximum,
//     MI_NEXT_ESTIMATION_SLOT_CONST NextEstimationSlotConst,
//     PMMWSLE Wsle
//     )
//
// Routine Description:
//
//      We iterate through the working set array in a non-sequential
//      manner so that the sample is independent of any aging or trimming.
//
//      This algorithm walks through the working set with a stride of
//      2^MiEstimationShift elements.
//
// Arguments:
//
//      Previous - Last slot used
//
//      Minimum - Minimum acceptable slot (ie. the first dynamic one)
//
//      Maximum - max slot number + 1
//
//      NextEstimationSlotConst - for this algorithm it contains the stride
//
//      Wsle - the working set array
//
// Return Value:
//
//      Next slot.
//
// Environment:
//
//      Kernel mode, APCs disabled, working set lock held and PFN lock held.
//
//--

typedef struct _MI_NEXT_ESTIMATION_SLOT_CONST {
    WSLE_NUMBER Stride;
} MI_NEXT_ESTIMATION_SLOT_CONST;


#define MI_CALC_NEXT_ESTIMATION_SLOT_CONST(NextEstimationSlotConst, WorkingSetList) \
    (NextEstimationSlotConst).Stride = 1 << MiEstimationShift;

#define MI_NEXT_VALID_ESTIMATION_SLOT(Previous, StartEntry, Minimum, Maximum, NextEstimationSlotConst, Wsle) \
    ASSERT(((Previous) >= Minimum) && ((Previous) <= Maximum)); \
    ASSERT(((StartEntry) >= Minimum) && ((StartEntry) <= Maximum)); \
    do { \
        (Previous) += (NextEstimationSlotConst).Stride; \
        if ((Previous) > Maximum) { \
            (Previous) = Minimum + ((Previous + 1) & (NextEstimationSlotConst.Stride - 1)); \
            StartEntry += 1; \
            (Previous) = StartEntry; \
        } \
        if ((Previous) > Maximum || (Previous) < Minimum) { \
            StartEntry = Minimum; \
            (Previous) = StartEntry; \
        } \
    } while (Wsle[Previous].u1.e1.Valid == 0);

//++
//
// WSLE_NUMBER
// MI_NEXT_VALID_AGING_SLOT (
//     DWORD Previous,
//     DWORD Minimum,
//     DWORD Maximum,
//     PMMWSLE Wsle
//     )
//
// Routine Description:
//
//      This finds the next slot to valid slot to age.  It walks
//      through the slots sequentialy.
//
// Arguments:
//
//      Previous - Last slot used
//
//      Minimum - Minimum acceptable slot (ie. the first dynamic one)
//
//      Maximum - Max slot number + 1
//
//      Wsle - the working set array
//
// Return Value:
//
//      None.
//
// Environment:
//
//      Kernel mode, APCs disabled, working set lock held and PFN lock held.
//
//--

#define MI_NEXT_VALID_AGING_SLOT(Previous, Minimum, Maximum, Wsle) \
    ASSERT(((Previous) >= Minimum) && ((Previous) <= Maximum)); \
    do { \
        (Previous) += 1; \
        if ((Previous) > Maximum) { \
            Previous = Minimum; \
        } \
    } while ((Wsle[Previous].u1.e1.Valid == 0));

//++
//
// ULONG
// MI_CALCULATE_USAGE_ESTIMATE (
//     IN PULONG SampledAgeCounts.
//     IN ULONG CounterShift
//     )
//
// Routine Description:
//
//      In Usage Estimation, we count the number of pages of each age in
//      a sample.  The function turns the SampledAgeCounts into an
//      estimate of the unused pages.
//
// Arguments:
//
//      SampledAgeCounts - counts of pages of each different age in the sample
//
//      CounterShift - shift necessary to apply sample to entire WS
//
// Return Value:
//
//      The number of pages to walk in the working set to get a good
//      estimate of the number available.
//
//--

#define MI_CALCULATE_USAGE_ESTIMATE(SampledAgeCounts, CounterShift) \
                (((SampledAgeCounts)[1] + \
                    (SampledAgeCounts)[2] + (SampledAgeCounts)[3]) \
                    << (CounterShift))

//++
//
// VOID
// MI_RESET_WSLE_AGE (
//     IN PMMPTE PointerPte,
//     IN PMMWSLE Wsle
//     )
//
// Routine Description:
//
//      Clear the age counter for the working set entry.
//
// Arguments:
//
//      PointerPte - pointer to the working set list entry's PTE.
//
//      Wsle - pointer to the working set list entry.
//
// Return Value:
//
//      None.
//
//--
#define MI_RESET_WSLE_AGE(PointerPte, Wsle) \
    (Wsle)->u1.e1.Age = 0;

//++
//
// ULONG
// MI_GET_WSLE_AGE (
//     IN PMMPTE PointerPte,
//     IN PMMWSLE Wsle
//     )
//
// Routine Description:
//
//      Clear the age counter for the working set entry.
//
// Arguments:
//
//      PointerPte - pointer to the working set list entry's PTE
//      Wsle - pointer to the working set list entry
//
// Return Value:
//
//      Age group of the working set entry
//
//--
#define MI_GET_WSLE_AGE(PointerPte, Wsle) \
    ((ULONG)((Wsle)->u1.e1.Age))

//++
//
// VOID
// MI_INC_WSLE_AGE (
//     IN PMMPTE PointerPte,
//     IN PMMWSLE Wsle,
//     )
//
// Routine Description:
//
//      Increment the age counter for the working set entry.
//
// Arguments:
//
//      PointerPte - pointer to the working set list entry's PTE.
//
//      Wsle - pointer to the working set list entry.
//
// Return Value:
//
//      None
//
//--

#define MI_INC_WSLE_AGE(PointerPte, Wsle) \
    if ((Wsle)->u1.e1.Age < 3) { \
        (Wsle)->u1.e1.Age += 1; \
    }

//++
//
// VOID
// MI_UPDATE_USE_ESTIMATE (
//     IN PMMPTE PointerPte,
//     IN PMMWSLE Wsle,
//     IN ULONG *SampledAgeCounts
//     )
//
// Routine Description:
//
//      Update the sampled age counts.
//
// Arguments:
//
//      PointerPte - pointer to the working set list entry's PTE.
//
//      Wsle - pointer to the working set list entry.
//
//      SampledAgeCounts - array of age counts to be updated.
//
// Return Value:
//
//      None
//
//--

#define MI_UPDATE_USE_ESTIMATE(PointerPte, Wsle, SampledAgeCounts) \
    (SampledAgeCounts)[(Wsle)->u1.e1.Age] += 1;

//++
//
// BOOLEAN
// MI_WS_GROWING_TOO_FAST (
//     IN PMMSUPPORT VmSupport
//     )
//
// Routine Description:
//
//      Limit the growth rate of processes as the
//      available memory approaches zero.  Note the caller must ensure that
//      MmAvailablePages is low enough so this calculation does not wrap.
//
// Arguments:
//
//      VmSupport - a working set.
//
// Return Value:
//
//      TRUE if the growth rate is too fast, FALSE otherwise.
//
//--

#define MI_WS_GROWING_TOO_FAST(VmSupport) \
    ((VmSupport)->GrowthSinceLastEstimate > \
        (((MI_CLAIM_INCR * (MmAvailablePages*MmAvailablePages)) / (64*64)) + 1))

#define SECTION_BASE_ADDRESS(_NtSection) \
    (*((PVOID *)&(_NtSection)->PointerToRelocations))

#define SECTION_LOCK_COUNT_POINTER(_NtSection) \
    ((PLONG)&(_NtSection)->NumberOfRelocations)

//
// Memory Management Object structures.
//

typedef enum _SECTION_CHECK_TYPE {
    CheckDataSection,
    CheckImageSection,
    CheckUserDataSection,
    CheckBothSection
} SECTION_CHECK_TYPE;

typedef struct _MMEXTEND_INFO {
    UINT64 CommittedSize;
    ULONG ReferenceCount;
} MMEXTEND_INFO, *PMMEXTEND_INFO;

typedef struct _SEGMENT_FLAGS {
    ULONG_PTR TotalNumberOfPtes4132 : 10;
    ULONG_PTR ExtraSharedWowSubsections : 1;
#if defined (_WIN64)
    ULONG_PTR Spare : 53;
#else
    ULONG_PTR Spare : 21;
#endif
} SEGMENT_FLAGS, *PSEGMENT_FLAGS;

typedef struct _SEGMENT {
    struct _CONTROL_AREA *ControlArea;
    ULONG TotalNumberOfPtes;
    ULONG NonExtendedPtes;
    ULONG WritableUserReferences;

    UINT64 SizeOfSegment;
    MMPTE SegmentPteTemplate;

    SIZE_T NumberOfCommittedPages;
    PMMEXTEND_INFO ExtendInfo;

    SEGMENT_FLAGS SegmentFlags;
    PVOID BasedAddress;

    //
    // The fields below are for image & pagefile-backed sections only.
    // Common fields are above and new common entries must be added to
    // both the SEGMENT and MAPPED_FILE_SEGMENT declarations.
    //

    union {
        SIZE_T ImageCommitment;     // for image-backed sections only
        PEPROCESS CreatingProcess;  // for pagefile-backed sections only
    } u1;

    union {
        PSECTION_IMAGE_INFORMATION ImageInformation;    // for images only
        PVOID FirstMappedVa;        // for pagefile-backed sections only
    } u2;

    PMMPTE PrototypePte;
    MMPTE ThePtes[MM_PROTO_PTE_ALIGNMENT / PAGE_SIZE];

} SEGMENT, *PSEGMENT;

typedef struct _MAPPED_FILE_SEGMENT {
    struct _CONTROL_AREA *ControlArea;
    ULONG TotalNumberOfPtes;
    ULONG NonExtendedPtes;
    ULONG WritableUserReferences;

    UINT64 SizeOfSegment;
    MMPTE SegmentPteTemplate;

    SIZE_T NumberOfCommittedPages;
    PMMEXTEND_INFO ExtendInfo;

    SEGMENT_FLAGS SegmentFlags;
    PVOID BasedAddress;

    struct _MSUBSECTION *LastSubsectionHint;

} MAPPED_FILE_SEGMENT, *PMAPPED_FILE_SEGMENT;

typedef struct _EVENT_COUNTER {
    SLIST_ENTRY ListEntry;
    ULONG RefCount;
    KEVENT Event;
} EVENT_COUNTER, *PEVENT_COUNTER;

typedef struct _MMSECTION_FLAGS {
    unsigned BeingDeleted : 1;
    unsigned BeingCreated : 1;
    unsigned BeingPurged : 1;
    unsigned NoModifiedWriting : 1;

    unsigned FailAllIo : 1;
    unsigned Image : 1;
    unsigned Based : 1;
    unsigned File : 1;

    unsigned Networked : 1;
    unsigned NoCache : 1;
    unsigned PhysicalMemory : 1;
    unsigned CopyOnWrite : 1;

    unsigned Reserve : 1;  // not a spare bit!
    unsigned Commit : 1;
    unsigned FloppyMedia : 1;
    unsigned WasPurged : 1;

    unsigned UserReference : 1;
    unsigned GlobalMemory : 1;
    unsigned DeleteOnClose : 1;
    unsigned FilePointerNull : 1;

    unsigned DebugSymbolsLoaded : 1;
    unsigned SetMappedFileIoComplete : 1;
    unsigned CollidedFlush : 1;
    unsigned NoChange : 1;

    unsigned HadUserReference : 1;
    unsigned ImageMappedInSystemSpace : 1;
    unsigned UserWritable : 1;
    unsigned Accessed : 1;

    unsigned GlobalOnlyPerSession : 1;
    unsigned Rom : 1;
    unsigned filler : 2;
} MMSECTION_FLAGS;

typedef struct _CONTROL_AREA {
    PSEGMENT Segment;
    LIST_ENTRY DereferenceList;
    ULONG NumberOfSectionReferences;    // All section refs & image flushes
    ULONG NumberOfPfnReferences;        // valid + transition prototype PTEs
    ULONG NumberOfMappedViews;          // total # mapped views, including
                                        // system cache & system space views
    ULONG NumberOfSystemCacheViews;     // system cache views only
    ULONG NumberOfUserReferences;       // user section & view references
    union {
        ULONG LongFlags;
        MMSECTION_FLAGS Flags;
    } u;
    PFILE_OBJECT FilePointer;
    PEVENT_COUNTER WaitingForDeletion;
    USHORT ModifiedWriteCount;
    USHORT FlushInProgressCount;
} CONTROL_AREA, *PCONTROL_AREA;

typedef struct _LARGE_CONTROL_AREA {
    PSEGMENT Segment;
    LIST_ENTRY DereferenceList;
    ULONG NumberOfSectionReferences;
    ULONG NumberOfPfnReferences;
    ULONG NumberOfMappedViews;
    USHORT NumberOfSubsections;
    USHORT FlushInProgressCount;
    ULONG NumberOfUserReferences;
    union {
        ULONG LongFlags;
        MMSECTION_FLAGS Flags;
    } u;
    PFILE_OBJECT FilePointer;
    PEVENT_COUNTER WaitingForDeletion;
    USHORT ModifiedWriteCount;
    USHORT NumberOfSystemCacheViews;
    PFN_NUMBER StartingFrame;       // only used if Flags.Rom == 1.
    LIST_ENTRY UserGlobalList;
    ULONG SessionId;
} LARGE_CONTROL_AREA, *PLARGE_CONTROL_AREA;

typedef struct _MMSUBSECTION_FLAGS {
    unsigned ReadOnly : 1;
    unsigned ReadWrite : 1;
    unsigned SubsectionStatic : 1;
    unsigned GlobalMemory: 1;
    unsigned Protection : 5;
    unsigned Spare : 1;
    unsigned StartingSector4132 : 10;   // 2 ** (42+12) == 4MB*4GB == 16K TB
    unsigned SectorEndOffset : 12;
} MMSUBSECTION_FLAGS;

typedef struct _SUBSECTION { // Must start on quadword boundary and be quad sized
    PCONTROL_AREA ControlArea;
    union {
        ULONG LongFlags;
        MMSUBSECTION_FLAGS SubsectionFlags;
    } u;
    ULONG StartingSector;
    ULONG NumberOfFullSectors;  // (4GB-1) * 4K == 16TB-4K limit per subsection
#if defined(_MIALT4K_)
    ULONG LastSplitPageProtection;   // Protection of last split page in this
                                     // subsection.  This must be saved here
                                     // because the final subsection may end
                                     // on a split (merged) page and so we
                                     // cannot just look forward to the next
                                     // subsection in that case to obtain
                                     // the correct permissions.
#endif
    PMMPTE SubsectionBase;
    ULONG UnusedPtes;
    ULONG PtesInSubsection;
    struct _SUBSECTION *NextSubsection;
} SUBSECTION, *PSUBSECTION;

extern const ULONG MMSECT;

//
// Accesses to MMSUBSECTION_FLAGS2 are synchronized via the PFN lock
// (unlike MMSUBSECTION_FLAGS access which is not lock protected at all).
//

typedef struct _MMSUBSECTION_FLAGS2 {
    unsigned SubsectionAccessed : 1;
    unsigned SubsectionConverted : 1;       // only needed for debug
    unsigned Reserved : 30;
} MMSUBSECTION_FLAGS2;

//
// Mapped data file subsection structure.  Not used for images
// or pagefile-backed shared memory.
//

typedef struct _MSUBSECTION { // Must start on quadword boundary and be quad sized
    PCONTROL_AREA ControlArea;
    union {
        ULONG LongFlags;
        MMSUBSECTION_FLAGS SubsectionFlags;
    } u;
    ULONG StartingSector;
    ULONG NumberOfFullSectors;  // (4GB-1) * 4K == 16TB-4K limit per subsection
    PMMPTE SubsectionBase;
    ULONG UnusedPtes;
    ULONG PtesInSubsection;
    struct _SUBSECTION *NextSubsection;
    LIST_ENTRY DereferenceList;
    ULONG_PTR NumberOfMappedViews;
    union {
        ULONG LongFlags2;
        MMSUBSECTION_FLAGS2 SubsectionFlags2;
    } u2;
} MSUBSECTION, *PMSUBSECTION;

#define MI_MAXIMUM_SECTION_SIZE ((UINT64)16*1024*1024*1024*1024*1024 - ((UINT64)1<<MM4K_SHIFT))

VOID
MiDecrementSubsections (
    IN PSUBSECTION FirstSubsection,
    IN PSUBSECTION LastSubsection OPTIONAL
    );

NTSTATUS
MiAddViewsForSectionWithPfn (
    IN PMSUBSECTION StartMappedSubsection,
    IN UINT64 LastPteOffset OPTIONAL
    );

NTSTATUS
MiAddViewsForSection (
    IN PMSUBSECTION MappedSubsection,
    IN UINT64 LastPteOffset OPTIONAL,
    IN KIRQL OldIrql,
    OUT PULONG Waited
    );

LOGICAL
MiReferenceSubsection (
    IN PMSUBSECTION MappedSubsection
    );

VOID
MiRemoveViewsFromSection (
    IN PMSUBSECTION StartMappedSubsection,
    IN UINT64 LastPteOffset OPTIONAL
    );

VOID
MiRemoveViewsFromSectionWithPfn (
    IN PMSUBSECTION StartMappedSubsection,
    IN UINT64 LastPteOffset OPTIONAL
    );

VOID
MiSubsectionConsistent(
    IN PSUBSECTION Subsection
    );

#if DBG
#define MI_CHECK_SUBSECTION(_subsection) MiSubsectionConsistent((PSUBSECTION)(_subsection))
#else
#define MI_CHECK_SUBSECTION(_subsection)
#endif

//++
//ULONG
//Mi4KStartForSubsection (
//    IN PLARGE_INTEGER address,
//    IN OUT PSUBSECTION subsection
//    );
//
// Routine Description:
//
//    This macro sets into the specified subsection the supplied information
//    indicating the start address (in 4K units) of this portion of the file.
//
// Arguments
//
//    address - Supplies the 64-bit address (in 4K units) of the start of this
//              portion of the file.
//
//    subsection - Supplies the subsection address to store the address in.
//
// Return Value:
//
//    None.
//
//--

#define Mi4KStartForSubsection(address, subsection)  \
   subsection->StartingSector = ((PLARGE_INTEGER)address)->LowPart; \
   subsection->u.SubsectionFlags.StartingSector4132 = \
        (((PLARGE_INTEGER)(address))->HighPart & 0x3ff);

//++
//ULONG
//Mi4KStartFromSubsection (
//    IN OUT PLARGE_INTEGER address,
//    IN PSUBSECTION subsection
//    );
//
// Routine Description:
//
//    This macro gets the start 4K offset from the specified subsection.
//
// Arguments
//
//    address - Supplies the 64-bit address (in 4K units) to place the
//              start of this subsection into.
//
//    subsection - Supplies the subsection address to get the address from.
//
// Return Value:
//
//    None.
//
//--

#define Mi4KStartFromSubsection(address, subsection)  \
   ((PLARGE_INTEGER)address)->LowPart = subsection->StartingSector; \
   ((PLARGE_INTEGER)address)->HighPart = subsection->u.SubsectionFlags.StartingSector4132;

typedef struct _MMDEREFERENCE_SEGMENT_HEADER {
    KSPIN_LOCK Lock;
    KSEMAPHORE Semaphore;
    LIST_ENTRY ListHead;
} MMDEREFERENCE_SEGMENT_HEADER;

//
// This entry is used for calling the segment dereference thread
// to perform page file expansion.  It has a similar structure
// to a control area to allow either a control area or a page file
// expansion entry to be placed on the list.  Note that for a control
// area the segment pointer is valid whereas for page file expansion
// it is null.
//

typedef struct _MMPAGE_FILE_EXPANSION {
    PSEGMENT Segment;
    LIST_ENTRY DereferenceList;
    SIZE_T RequestedExpansionSize;
    SIZE_T ActualExpansion;
    KEVENT Event;
    LONG InProgress;
    ULONG PageFileNumber;
} MMPAGE_FILE_EXPANSION, *PMMPAGE_FILE_EXPANSION;

#define MI_EXTEND_ANY_PAGEFILE      ((ULONG)-1)
#define MI_CONTRACT_PAGEFILES       ((SIZE_T)-1)

typedef struct _MMWORKING_SET_EXPANSION_HEAD {
    LIST_ENTRY ListHead;
} MMWORKING_SET_EXPANSION_HEAD;

#define SUBSECTION_READ_ONLY      1L
#define SUBSECTION_READ_WRITE     2L
#define SUBSECTION_COPY_ON_WRITE  4L
#define SUBSECTION_SHARE_ALLOW    8L

//
// The MMINPAGE_FLAGS relies on the fact that a pool allocation is always
// QUADWORD aligned so the low 3 bits are always available.
//

typedef struct _MMINPAGE_FLAGS {
    ULONG_PTR Completed : 1;
    ULONG_PTR Available1 : 1;
    ULONG_PTR Available2 : 1;
#if defined (_WIN64)
    ULONG_PTR PrefetchMdlHighBits : 61;
#else
    ULONG_PTR PrefetchMdlHighBits : 29;
#endif
} MMINPAGE_FLAGS, *PMMINPAGE_FLAGS;

#define MI_EXTRACT_PREFETCH_MDL(_Support) ((PMDL)((ULONG_PTR)(_Support->u1.PrefetchMdl) & ~(sizeof(QUAD) - 1)))

typedef struct _MMINPAGE_SUPPORT {
    KEVENT Event;
    IO_STATUS_BLOCK IoStatus;
    LARGE_INTEGER ReadOffset;
    LONG WaitCount;
#if defined (_WIN64)
    ULONG UsedPageTableEntries;
#endif
    PETHREAD Thread;
    PFILE_OBJECT FilePointer;
    PMMPTE BasePte;
    PMMPFN Pfn;
    union {
        MMINPAGE_FLAGS e1;
        ULONG_PTR LongFlags;
        PMDL PrefetchMdl;       // Only used under _PREFETCH_
    } u1;
    MDL Mdl;
    PFN_NUMBER Page[MM_MAXIMUM_READ_CLUSTER_SIZE + 1];
    SINGLE_LIST_ENTRY ListEntry;
} MMINPAGE_SUPPORT, *PMMINPAGE_SUPPORT;

#define MI_PF_DUMMY_PAGE_PTE ((PMMPTE)0x23452345)   // Only used by _PREFETCH_

//
// Section support.
//

typedef struct _SECTION {
    MMADDRESS_NODE Address;
    PSEGMENT Segment;
    LARGE_INTEGER SizeOfSection;
    union {
        ULONG LongFlags;
        MMSECTION_FLAGS Flags;
    } u;
    ULONG InitialPageProtection;
} SECTION, *PSECTION;

//
// Banked memory descriptor.  Pointed to by VAD which has
// the PhysicalMemory flags set and the Banked pointer field as
// non-NULL.
//

typedef struct _MMBANKED_SECTION {
    PFN_NUMBER BasePhysicalPage;
    PMMPTE BasedPte;
    ULONG BankSize;
    ULONG BankShift; //shift for PTEs to calculate bank number
    PBANKED_SECTION_ROUTINE BankedRoutine;
    PVOID Context;
    PMMPTE CurrentMappedPte;
    MMPTE BankTemplate[1];
} MMBANKED_SECTION, *PMMBANKED_SECTION;


//
// Virtual address descriptor
//
// ***** NOTE **********
//  The first part of a virtual address descriptor is a MMADDRESS_NODE!!!
//

#if defined (_WIN64)

#define COMMIT_SIZE 51

#if ((COMMIT_SIZE + PAGE_SHIFT) < 63)
#error COMMIT_SIZE too small
#endif

#else
#define COMMIT_SIZE 19

#if ((COMMIT_SIZE + PAGE_SHIFT) < 31)
#error COMMIT_SIZE too small
#endif
#endif

#define MM_MAX_COMMIT (((ULONG_PTR) 1 << COMMIT_SIZE) - 1)

#define MM_VIEW_UNMAP 0
#define MM_VIEW_SHARE 1

typedef struct _MMVAD_FLAGS {
    ULONG_PTR CommitCharge : COMMIT_SIZE; //limits system to 4k pages or bigger!
    ULONG_PTR PhysicalMapping : 1;      // Device\PhysicalMemory
    ULONG_PTR ImageMap : 1;
    ULONG_PTR UserPhysicalPages : 1;    // AWE
    ULONG_PTR NoChange : 1;
    ULONG_PTR WriteWatch : 1;
    ULONG_PTR Protection : 5;
    ULONG_PTR LargePages : 1;
    ULONG_PTR MemCommit: 1;
    ULONG_PTR PrivateMemory : 1;    //used to tell VAD from VAD_SHORT
} MMVAD_FLAGS;

typedef struct _MMVAD_FLAGS2 {
    unsigned FileOffset : 24;       // number of 64k units into file
    unsigned SecNoChange : 1;       // set if SEC_NOCHANGE specified
    unsigned OneSecured : 1;        // set if u3 field is a range
    unsigned MultipleSecured : 1;   // set if u3 field is a list head
    unsigned ReadOnly : 1;          // protected as ReadOnly
    unsigned LongVad : 1;           // set if VAD is a long VAD
    unsigned ExtendableFile : 1;
    unsigned Inherit : 1;           //1 = ViewShare, 0 = ViewUnmap
    unsigned CopyOnWrite : 1;
} MMVAD_FLAGS2;

typedef struct _MMADDRESS_LIST {
    ULONG_PTR StartVpn;
    ULONG_PTR EndVpn;
} MMADDRESS_LIST, *PMMADDRESS_LIST;

typedef struct _MMSECURE_ENTRY {
    union {
        ULONG_PTR LongFlags2;
        MMVAD_FLAGS2 VadFlags2;
    } u2;
    ULONG_PTR StartVpn;
    ULONG_PTR EndVpn;
    LIST_ENTRY List;
} MMSECURE_ENTRY, *PMMSECURE_ENTRY;

typedef struct _ALIAS_VAD_INFO {
    KAPC Apc;
    ULONG NumberOfEntries;
    ULONG MaximumEntries;
} ALIAS_VAD_INFO, *PALIAS_VAD_INFO;

typedef struct _ALIAS_VAD_INFO2 {
    ULONG BaseAddress;
    HANDLE SecureHandle;
} ALIAS_VAD_INFO2, *PALIAS_VAD_INFO2;

typedef struct _MMVAD {
    union {
        LONG_PTR Balance : 2;
        struct _MMVAD *Parent;
    } u1;
    struct _MMVAD *LeftChild;
    struct _MMVAD *RightChild;
    ULONG_PTR StartingVpn;
    ULONG_PTR EndingVpn;

    union {
        ULONG_PTR LongFlags;
        MMVAD_FLAGS VadFlags;
    } u;
    PCONTROL_AREA ControlArea;
    PMMPTE FirstPrototypePte;
    PMMPTE LastContiguousPte;
    union {
        ULONG LongFlags2;
        MMVAD_FLAGS2 VadFlags2;
    } u2;
} MMVAD, *PMMVAD;

typedef struct _MMVAD_LONG {
    union {
        LONG_PTR Balance : 2;
        struct _MMVAD *Parent;
    } u1;
    struct _MMVAD *LeftChild;
    struct _MMVAD *RightChild;
    ULONG_PTR StartingVpn;
    ULONG_PTR EndingVpn;

    union {
        ULONG_PTR LongFlags;
        MMVAD_FLAGS VadFlags;
    } u;
    PCONTROL_AREA ControlArea;
    PMMPTE FirstPrototypePte;
    PMMPTE LastContiguousPte;
    union {
        ULONG LongFlags2;
        MMVAD_FLAGS2 VadFlags2;
    } u2;
    union {
        LIST_ENTRY List;
        MMADDRESS_LIST Secured;
    } u3;
    union {
        PMMBANKED_SECTION Banked;
        PMMEXTEND_INFO ExtendedInfo;
    } u4;
#if defined(_MIALT4K_)
    PALIAS_VAD_INFO AliasInformation;
#endif
} MMVAD_LONG, *PMMVAD_LONG;

typedef struct _MMVAD_SHORT {
    union {
        LONG_PTR Balance : 2;
        struct _MMVAD *Parent;
    } u1;
    struct _MMVAD *LeftChild;
    struct _MMVAD *RightChild;
    ULONG_PTR StartingVpn;
    ULONG_PTR EndingVpn;

    union {
        ULONG_PTR LongFlags;
        MMVAD_FLAGS VadFlags;
    } u;
} MMVAD_SHORT, *PMMVAD_SHORT;

#define MI_GET_PROTECTION_FROM_VAD(_Vad) ((ULONG)(_Vad)->u.VadFlags.Protection)

#define MI_PHYSICAL_VIEW_AWE    0x1         // AWE region
#define MI_PHYSICAL_VIEW_PHYS   0x2         // Device\PhysicalMemory region
#define MI_PHYSICAL_VIEW_LARGE  0x4         // Large page region

typedef struct _MI_PHYSICAL_VIEW {
    union {
        LONG_PTR Balance : 2;
        struct _MMADDRESS_NODE *Parent;
    } u1;
    struct _MMADDRESS_NODE *LeftChild;
    struct _MMADDRESS_NODE *RightChild;
    ULONG_PTR StartingVpn;      // Actually a virtual address, not a VPN
    ULONG_PTR EndingVpn;        // Actually a virtual address, not a VPN
    PMMVAD Vad;
    union {
        ULONG_PTR LongFlags;    // physical, AWE or largepage Vad identification
        PRTL_BITMAP BitMap;     // only if Vad->u.VadFlags.WriteWatch == 1
    } u;
} MI_PHYSICAL_VIEW, *PMI_PHYSICAL_VIEW;

#define MI_PHYSICAL_VIEW_ROOT_KEY   'rpmM'
#define MI_PHYSICAL_VIEW_KEY        'vpmM'
#define MI_WRITEWATCH_VIEW_KEY      'wWmM'

//
// Stuff for support of Write Watch.
//

VOID
MiCaptureWriteWatchDirtyBit (
    IN PEPROCESS Process,
    IN PVOID VirtualAddress
    );

//
// Stuff for support of AWE (Address Windowing Extensions).
//

typedef struct _AWEINFO {
    PRTL_BITMAP VadPhysicalPagesBitMap;
    ULONG_PTR VadPhysicalPages;
    ULONG_PTR VadPhysicalPagesLimit;

    //
    // The PushLock is used to allow most of the NtMapUserPhysicalPages{Scatter}
    // to execute in parallel as this is acquired shared for these calls.
    // Exclusive acquisitions are used to protect maps against frees of the
    // pages as well as to protect updates to the AweVadList.  Collisions
    // should be rare because the exclusive acquisitions should be rare.
    //

    PEX_PUSH_LOCK_CACHE_AWARE PushLock;
    PMI_PHYSICAL_VIEW PhysicalViewHint[MAXIMUM_PROCESSORS];

    MM_AVL_TABLE AweVadRoot;
} AWEINFO, *PAWEINFO;

VOID
MiAweViewInserter (
    IN PEPROCESS Process,
    IN PMI_PHYSICAL_VIEW PhysicalView
    );

VOID
MiAweViewRemover (
    IN PEPROCESS Process,
    IN PMMVAD Vad
    );

//
// Stuff for support of POSIX Fork.
//

typedef struct _MMCLONE_BLOCK {
    MMPTE ProtoPte;
    LONG CloneRefCount;
} MMCLONE_BLOCK, *PMMCLONE_BLOCK;

typedef struct _MMCLONE_HEADER {
    ULONG NumberOfPtes;
    LONG NumberOfProcessReferences;
    PMMCLONE_BLOCK ClonePtes;
} MMCLONE_HEADER, *PMMCLONE_HEADER;

typedef struct _MMCLONE_DESCRIPTOR {
    union {
        LONG_PTR Balance : 2;
        struct _MMADDRESS_NODE *Parent;
    } u1;
    struct _MMADDRESS_NODE *LeftChild;
    struct _MMADDRESS_NODE *RightChild;
    ULONG_PTR StartingVpn;
    ULONG_PTR EndingVpn;
    ULONG NumberOfPtes;
    PMMCLONE_HEADER CloneHeader;
    LONG NumberOfReferences;
    LONG FinalNumberOfReferences;
    SIZE_T PagedPoolQuotaCharge;
} MMCLONE_DESCRIPTOR, *PMMCLONE_DESCRIPTOR;

//
// The following macro allocates and initializes a bitmap from the
// specified pool of the specified size.
//
//      VOID
//      MiCreateBitMap (
//          OUT PRTL_BITMAP *BitMapHeader,
//          IN SIZE_T SizeOfBitMap,
//          IN POOL_TYPE PoolType
//          );
//

#define MiCreateBitMap(BMH,S,P) {                          \
    ULONG _S;                                              \
    ASSERT ((ULONG64)(S) < _4gb);                          \
    _S = sizeof(RTL_BITMAP) + (ULONG)((((S) + 31) / 32) * 4);         \
    *(BMH) = (PRTL_BITMAP)ExAllocatePoolWithTag( (P), _S, '  mM');       \
    if (*(BMH)) { \
        RtlInitializeBitMap( *(BMH), (PULONG)((*(BMH))+1), (ULONG)(S)); \
    }                                                          \
}

#define MiRemoveBitMap(BMH)     {                          \
    ExFreePool(*(BMH));                                    \
    *(BMH) = NULL;                                         \
}

#define MI_INITIALIZE_ZERO_MDL(MDL) { \
    MDL->Next = (PMDL) NULL; \
    MDL->MdlFlags = 0; \
    MDL->StartVa = NULL; \
    MDL->ByteOffset = 0; \
    MDL->ByteCount = 0; \
    }

//
// Page File structures.
//

typedef struct _MMMOD_WRITER_LISTHEAD {
    LIST_ENTRY ListHead;
    KEVENT Event;
} MMMOD_WRITER_LISTHEAD, *PMMMOD_WRITER_LISTHEAD;

typedef struct _MMMOD_WRITER_MDL_ENTRY {
    LIST_ENTRY Links;
    LARGE_INTEGER WriteOffset;
    union {
        IO_STATUS_BLOCK IoStatus;
        LARGE_INTEGER LastByte;
    } u;
    PIRP Irp;
    ULONG_PTR LastPageToWrite;
    PMMMOD_WRITER_LISTHEAD PagingListHead;
    PLIST_ENTRY CurrentList;
    struct _MMPAGING_FILE *PagingFile;
    PFILE_OBJECT File;
    PCONTROL_AREA ControlArea;
    PERESOURCE FileResource;
    LARGE_INTEGER IssueTime;
    MDL Mdl;
    PFN_NUMBER Page[1];
} MMMOD_WRITER_MDL_ENTRY, *PMMMOD_WRITER_MDL_ENTRY;


#define MM_PAGING_FILE_MDLS 2

typedef struct _MMPAGING_FILE {
    PFN_NUMBER Size;
    PFN_NUMBER MaximumSize;
    PFN_NUMBER MinimumSize;
    PFN_NUMBER FreeSpace;
    PFN_NUMBER CurrentUsage;
    PFN_NUMBER PeakUsage;
    PFN_NUMBER HighestPage;
    PFILE_OBJECT File;
    PMMMOD_WRITER_MDL_ENTRY Entry[MM_PAGING_FILE_MDLS];
    UNICODE_STRING PageFileName;
    PRTL_BITMAP Bitmap;
    struct {
        ULONG PageFileNumber :  4;
        ULONG ReferenceCount :  4;      // really only need 1 bit for this.
        ULONG BootPartition  :  1;
        ULONG Reserved       : 23;
    };
    HANDLE FileHandle;
} MMPAGING_FILE, *PMMPAGING_FILE;

//
// System PTE structures.
//

typedef struct _MMFREE_POOL_ENTRY {
    LIST_ENTRY List;        // maintained free&chk, 1st entry only
    PFN_NUMBER Size;        // maintained free&chk, 1st entry only
    ULONG Signature;        // maintained chk only, all entries
    struct _MMFREE_POOL_ENTRY *Owner; // maintained free&chk, all entries
} MMFREE_POOL_ENTRY, *PMMFREE_POOL_ENTRY;


typedef struct _MMLOCK_CONFLICT {
    LIST_ENTRY List;
    PETHREAD Thread;
} MMLOCK_CONFLICT, *PMMLOCK_CONFLICT;

//
// System view structures
//

typedef struct _MMVIEW {
    ULONG_PTR Entry;
    PCONTROL_AREA ControlArea;
} MMVIEW, *PMMVIEW;

//
// The MMSESSION structure represents kernel memory that is only valid on a
// per-session basis, thus the calling thread must be in the proper session
// to access this structure.
//

typedef struct _MMSESSION {

    //
    // Never refer to the SystemSpaceViewLock directly - always use the pointer
    // following it or you will break support for multiple concurrent sessions.
    //

    KGUARDED_MUTEX SystemSpaceViewLock;

    //
    // This points to the mutex above and is needed because the MMSESSION
    // is mapped in session space and the mutex needs to be globally
    // visible for proper KeWaitForSingleObject & KeSetEvent operation.
    //

    PKGUARDED_MUTEX SystemSpaceViewLockPointer;
    PCHAR SystemSpaceViewStart;
    PMMVIEW SystemSpaceViewTable;
    ULONG SystemSpaceHashSize;
    ULONG SystemSpaceHashEntries;
    ULONG SystemSpaceHashKey;
    ULONG BitmapFailures;
    PRTL_BITMAP SystemSpaceBitMap;

} MMSESSION, *PMMSESSION;

extern MMSESSION   MmSession;

#define LOCK_SYSTEM_VIEW_SPACE(_Session) \
            KeAcquireGuardedMutex (_Session->SystemSpaceViewLockPointer)

#define UNLOCK_SYSTEM_VIEW_SPACE(_Session) \
            KeReleaseGuardedMutex (_Session->SystemSpaceViewLockPointer)

//
// List for flushing TBs singularly.
//

typedef struct _MMPTE_FLUSH_LIST {
    ULONG Count;
    PVOID FlushVa[MM_MAXIMUM_FLUSH_COUNT];
} MMPTE_FLUSH_LIST, *PMMPTE_FLUSH_LIST;

//
// List for flushing WSLEs and TBs singularly.
//

typedef struct _MMWSLE_FLUSH_LIST {
    ULONG Count;
    WSLE_NUMBER FlushIndex[MM_MAXIMUM_FLUSH_COUNT];
} MMWSLE_FLUSH_LIST, *PMMWSLE_FLUSH_LIST;

typedef struct _LOCK_TRACKER {
    LIST_ENTRY ListEntry;
    PMDL Mdl;
    PVOID StartVa;
    PFN_NUMBER Count;
    ULONG Offset;
    ULONG Length;
    PFN_NUMBER Page;
    PVOID CallingAddress;
    PVOID CallersCaller;
    ULONG Who;
    PEPROCESS Process;
} LOCK_TRACKER, *PLOCK_TRACKER;

extern LOGICAL  MmTrackLockedPages;
extern BOOLEAN  MiTrackingAborted;

typedef struct _LOCK_HEADER {
    LIST_ENTRY ListHead;
    PFN_NUMBER Count;
    KSPIN_LOCK Lock;
    LOGICAL Valid;
} LOCK_HEADER, *PLOCK_HEADER;

extern LOGICAL MmSnapUnloads;

#define MI_UNLOADED_DRIVERS 50

extern ULONG MmLastUnloadedDriver;
extern PUNLOADED_DRIVERS MmUnloadedDrivers;


VOID
MiInitMachineDependent (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

VOID
MiAddHalIoMappings (
    VOID
    );

VOID
MiReportPhysicalMemory (
    VOID
    );

extern PFN_NUMBER MiNumberOfCompressionPages;

NTSTATUS
MiArmCompressionInterrupt (
    VOID
    );

VOID
MiBuildPagedPool (
    VOID
    );

VOID
MiInitializeNonPagedPool (
    VOID
    );

VOID
MiInitializePoolEvents (
    VOID
    );

VOID
MiInitializeNonPagedPoolThresholds (
    VOID
    );

LOGICAL
MiInitializeSystemSpaceMap (
    PVOID Session OPTIONAL
    );

VOID
MiFindInitializationCode (
    OUT PVOID *StartVa,
    OUT PVOID *EndVa
    );

VOID
MiFreeInitializationCode (
    IN PVOID StartVa,
    IN PVOID EndVa
    );

extern ULONG MiNonCachedCollisions;

//
// If /NOLOWMEM is used, this is set to the boundary PFN (pages below this
// value are not used whenever possible).
//

extern PFN_NUMBER MiNoLowMemory;

PVOID
MiAllocateLowMemory (
    IN SIZE_T NumberOfBytes,
    IN PFN_NUMBER LowestAcceptablePfn,
    IN PFN_NUMBER HighestAcceptablePfn,
    IN PFN_NUMBER BoundaryPfn,
    IN PVOID CallingAddress,
    IN MEMORY_CACHING_TYPE CacheType,
    IN ULONG Tag
    );

LOGICAL
MiFreeLowMemory (
    IN PVOID BaseAddress,
    IN ULONG Tag
    );

//
// Move drivers out of the low 16mb that ntldr placed them at - this makes more
// memory below 16mb available for ISA-type drivers that cannot run without it.
//

extern LOGICAL MmMakeLowMemory;

VOID
MiRemoveLowPages (
    IN ULONG RemovePhase
    );

ULONG
MiSectionInitialization (
    VOID
    );

#define MI_MAX_DEREFERENCE_CHUNK (64 * 1024 / PAGE_SIZE)

typedef struct _MI_PFN_DEREFERENCE_CHUNK {
    SINGLE_LIST_ENTRY ListEntry;
    CSHORT Flags;
    USHORT NumberOfPages;
    PFN_NUMBER Pfns[MI_MAX_DEREFERENCE_CHUNK];
} MI_PFN_DEREFERENCE_CHUNK, *PMI_PFN_DEREFERENCE_CHUNK;

extern SLIST_HEADER MmPfnDereferenceSListHead;
extern PSLIST_ENTRY MmPfnDeferredList;

#define MI_DEFER_PFN_HELD               0x1
#define MI_DEFER_DRAIN_LOCAL_ONLY       0x2

VOID
MiDeferredUnlockPages (
     ULONG Flags
     );

LOGICAL
MiFreeAllExpansionNonPagedPool (
    VOID
    );

VOID
MiDecrementReferenceCountForAwePage (
    IN PMMPFN Pfn1,
    IN LOGICAL PfnHeld
    );

VOID
FASTCALL
MiDecrementReferenceCount (
    IN PMMPFN Pfn1,
    IN PFN_NUMBER PageFrameIndex
    );

//++
//VOID
//MiDecrementReferenceCountInline (
//    IN PMMPFN PFN
//    IN PFN_NUMBER FRAME
//    );
//
// Routine Description:
//
//    MiDecrementReferenceCountInline decrements the reference count inline,
//    only calling MiDecrementReferenceCount if the count would go to zero
//    which would cause the page to be released.
//
// Arguments:
//
//    PFN - Supplies the PFN to decrement.
//
//    FRAME - Supplies the frame matching the above PFN.
//
// Return Value:
//
//    None.
//
// Environment:
//
//    PFN lock held.
//
//--

#define MiDecrementReferenceCountInline(PFN, FRAME)                     \
            MM_PFN_LOCK_ASSERT();                                       \
            ASSERT (MI_PFN_ELEMENT(FRAME) == (PFN));                    \
            ASSERT ((FRAME) <= MmHighestPhysicalPage);                  \
            ASSERT ((PFN)->u3.e2.ReferenceCount != 0);                  \
            if ((PFN)->u3.e2.ReferenceCount != 1) {                     \
                (PFN)->u3.e2.ReferenceCount -= 1;                       \
            }                                                           \
            else {                                                      \
                MiDecrementReferenceCount (PFN, FRAME);                 \
            }

VOID
FASTCALL
MiDecrementShareCount (
    IN PMMPFN Pfn1,
    IN PFN_NUMBER PageFrameIndex
    );

//++
//VOID
//MiDecrementShareCountInline (
//    IN PMMPFN PFN,
//    IN PFN_NUMBER FRAME
//    );
//
// Routine Description:
//
//    MiDecrementShareCountInline decrements the share count inline,
//    only calling MiDecrementShareCount if the count would go to zero
//    which would cause the page to be released.
//
// Arguments:
//
//    PFN - Supplies the PFN to decrement.
//
//    FRAME - Supplies the frame matching the above PFN.
//
// Return Value:
//
//    None.
//
// Environment:
//
//    PFN lock held.
//
//--

#define MiDecrementShareCountInline(PFN, FRAME)                         \
            MM_PFN_LOCK_ASSERT();                                       \
            ASSERT (((FRAME) <= MmHighestPhysicalPage) && ((FRAME) > 0));   \
            ASSERT (MI_PFN_ELEMENT(FRAME) == (PFN));                    \
            ASSERT ((PFN)->u2.ShareCount != 0);                         \
            if ((PFN)->u3.e1.PageLocation != ActiveAndValid && (PFN)->u3.e1.PageLocation != StandbyPageList) {                                            \
                KeBugCheckEx (PFN_LIST_CORRUPT, 0x99, FRAME, (PFN)->u3.e1.PageLocation, 0);                                                             \
            }                                                           \
            if ((PFN)->u2.ShareCount != 1) {                            \
                (PFN)->u2.ShareCount -= 1;                              \
                PERFINFO_DECREFCNT((PFN), PERF_SOFT_TRIM, PERFINFO_LOG_TYPE_DECSHARCNT); \
                ASSERT ((PFN)->u2.ShareCount < 0xF000000);              \
            }                                                           \
            else {                                                      \
                MiDecrementShareCount (PFN, FRAME);                     \
            }

//
// Routines which operate on the Page Frame Database Lists
//

VOID
FASTCALL
MiInsertPageInList (
    IN PMMPFNLIST ListHead,
    IN PFN_NUMBER PageFrameIndex
    );

VOID
FASTCALL
MiInsertPageInFreeList (
    IN PFN_NUMBER PageFrameIndex
    );

VOID
FASTCALL
MiInsertStandbyListAtFront (
    IN PFN_NUMBER PageFrameIndex
    );

PFN_NUMBER
FASTCALL
MiRemovePageFromList (
    IN PMMPFNLIST ListHead
    );

VOID
FASTCALL
MiUnlinkPageFromList (
    IN PMMPFN Pfn
    );

VOID
MiUnlinkFreeOrZeroedPage (
    IN PMMPFN Pfn
    );

VOID
FASTCALL
MiInsertFrontModifiedNoWrite (
    IN PFN_NUMBER PageFrameIndex
    );

//
// These are the thresholds for handing out an available page.
//

#define MM_LOW_LIMIT                2

#define MM_MEDIUM_LIMIT            32

#define MM_HIGH_LIMIT             128

//
// These are thresholds for enabling various optimizations.
//

#define MM_TIGHT_LIMIT            256

#define MM_PLENTY_FREE_LIMIT     1024

#define MM_VERY_HIGH_LIMIT      10000

#define MM_ENORMOUS_LIMIT       20000

ULONG
FASTCALL
MiEnsureAvailablePageOrWait (
    IN PEPROCESS Process,
    IN PVOID VirtualAddress,
    IN KIRQL OldIrql
    );

PFN_NUMBER
MiAllocatePfn (
    IN PMMPTE PointerPte,
    IN ULONG Protection
    );

PFN_NUMBER
FASTCALL
MiRemoveAnyPage (
    IN ULONG PageColor
    );

PFN_NUMBER
FASTCALL
MiRemoveZeroPage (
    IN ULONG PageColor
    );

typedef struct _COLORED_PAGE_INFO {
    union {
        PFN_NUMBER PagesLeftToScan;
#if defined(MI_MULTINODE) 
        KAFFINITY Affinity;
#endif
    };
    PFN_COUNT PagesQueued;
    SCHAR BasePriority;
    PMMPFN PfnAllocation;
    KEVENT Event;
} COLORED_PAGE_INFO, *PCOLORED_PAGE_INFO;

VOID
MiZeroInParallel (
    IN PCOLORED_PAGE_INFO ColoredPageInfoBase
    );

VOID
MiStartZeroPageWorkers (
    VOID
    );

VOID
MiPurgeTransitionList (
    VOID
    );

typedef struct _MM_LDW_WORK_CONTEXT {
    WORK_QUEUE_ITEM WorkItem;
    PFILE_OBJECT FileObject;
} MM_LDW_WORK_CONTEXT, *PMM_LDW_WORK_CONTEXT;

VOID
MiLdwPopupWorker (
    IN PVOID Context
    );

LOGICAL
MiDereferenceLastChanceLdw (
    IN PMM_LDW_WORK_CONTEXT LdwContext
    );

#define MI_LARGE_PAGE_DRIVER_BUFFER_LENGTH 512

extern WCHAR MmLargePageDriverBuffer[];
extern ULONG MmLargePageDriverBufferLength;

VOID
MiInitializeDriverLargePageList (
    VOID
    );

VOID
MiInitializeLargePageSupport (
    VOID
    );

NTSTATUS
MiAllocateLargePages (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    );

PFN_NUMBER
MiFindLargePageMemory (
    IN PCOLORED_PAGE_INFO ColoredPageInfo,
    IN PFN_NUMBER SizeInPages,
    OUT PPFN_NUMBER OutZeroCount
    );

VOID
MiFreeLargePageMemory (
    IN PFN_NUMBER PageFrameIndex,
    IN PFN_NUMBER SizeInPages
    );

VOID
MiFreeLargePages (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    );

PVOID
MiMapWithLargePages (
    IN PFN_NUMBER PageFrameIndex,
    IN PFN_NUMBER NumberOfPages,
    IN ULONG Protection,
    IN MEMORY_CACHING_TYPE CacheType
    );

VOID
MiUnmapLargePages (
    IN PVOID BaseAddress,
    IN SIZE_T NumberOfBytes
    );




LOGICAL
MiMustFrameBeCached (
    IN PFN_NUMBER PageFrameIndex
    );

VOID
MiSyncCachedRanges (
    VOID
    );

LOGICAL
MiAddCachedRange (
    IN PFN_NUMBER PageFrameIndex,
    IN PFN_NUMBER LastPageFrameIndex
    );

VOID
MiRemoveCachedRange (
    IN PFN_NUMBER PageFrameIndex,
    IN PFN_NUMBER LastPageFrameIndex
    );

#define MI_PAGE_FRAME_INDEX_MUST_BE_CACHED(PageFrameIndex)                  \
            MiMustFrameBeCached(PageFrameIndex)



VOID
MiFreeContiguousPages (
    IN PFN_NUMBER PageFrameIndex,
    IN PFN_NUMBER SizeInPages
    );

PFN_NUMBER
MiFindContiguousPages (
    IN PFN_NUMBER LowestPfn,
    IN PFN_NUMBER HighestPfn,
    IN PFN_NUMBER BoundaryPfn,
    IN PFN_NUMBER SizeInPages,
    IN MEMORY_CACHING_TYPE CacheType
    );

PVOID
MiFindContiguousMemory (
    IN PFN_NUMBER LowestPfn,
    IN PFN_NUMBER HighestPfn,
    IN PFN_NUMBER BoundaryPfn,
    IN PFN_NUMBER SizeInPages,
    IN MEMORY_CACHING_TYPE CacheType,
    IN PVOID CallingAddress
    );

PVOID
MiCheckForContiguousMemory (
    IN PVOID BaseAddress,
    IN PFN_NUMBER BaseAddressPages,
    IN PFN_NUMBER SizeInPages,
    IN PFN_NUMBER LowestPfn,
    IN PFN_NUMBER HighestPfn,
    IN PFN_NUMBER BoundaryPfn,
    IN MI_PFN_CACHE_ATTRIBUTE CacheAttribute
    );

//
// Routines which operate on the page frame database entry.
//

VOID
MiInitializePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN ULONG ModifiedState
    );

VOID
MiInitializePfnForOtherProcess (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN PFN_NUMBER ContainingPageFrame
    );

VOID
MiInitializeCopyOnWritePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte,
    IN WSLE_NUMBER WorkingSetIndex,
    IN PVOID SessionSpace
    );

VOID
MiInitializeTransitionPfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPte
    );

extern SLIST_HEADER MmInPageSupportSListHead;

VOID
MiFreeInPageSupportBlock (
    IN PMMINPAGE_SUPPORT Support
    );

PMMINPAGE_SUPPORT
MiGetInPageSupportBlock (
    IN KIRQL OldIrql,
    IN PNTSTATUS Status
    );

//
// Routines which require a physical page to be mapped into hyperspace
// within the current process.
//

VOID
FASTCALL
MiZeroPhysicalPage (
    IN PFN_NUMBER PageFrameIndex,
    IN ULONG Color
    );

VOID
FASTCALL
MiRestoreTransitionPte (
    IN PMMPFN Pfn1
    );

PSUBSECTION
MiGetSubsectionAndProtoFromPte (
    IN PMMPTE PointerPte,
    IN PMMPTE *ProtoPte
    );

PVOID
MiMapPageInHyperSpace (
    IN PEPROCESS Process,
    IN PFN_NUMBER PageFrameIndex,
    OUT PKIRQL OldIrql
    );

PVOID
MiMapPageInHyperSpaceAtDpc (
    IN PEPROCESS Process,
    IN PFN_NUMBER PageFrameIndex
    );

VOID
MiUnmapPagesInZeroSpace (
    IN PVOID VirtualAddress,
    IN PFN_COUNT NumberOfPages
    );

PVOID
MiMapImageHeaderInHyperSpace (
    IN PFN_NUMBER PageFrameIndex
    );

VOID
MiUnmapImageHeaderInHyperSpace (
    VOID
    );

PFN_NUMBER
MiGetPageForHeader (
    LOGICAL ZeroPage
    );

VOID
MiRemoveImageHeaderPage (
    IN PFN_NUMBER PageFrameNumber
    );

PVOID
MiMapPagesToZeroInHyperSpace (
    IN PMMPFN PfnAllocation,
    IN PFN_COUNT NumberOfPages
    );


//
// Routines to obtain and release system PTEs.
//

PMMPTE
MiReserveSystemPtes (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPteType
    );

PMMPTE
MiReserveAlignedSystemPtes (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType,
    IN ULONG Alignment
    );

VOID
MiReleaseSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPteType
    );

VOID
MiReleaseSplitSystemPtes (
    IN PMMPTE StartingPte,
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    );

VOID
MiIncrementSystemPtes (
    IN ULONG  NumberOfPtes
    );

VOID
MiIssueNoPtesBugcheck (
    IN ULONG NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPteType
    );

VOID
MiInitializeSystemPtes (
    IN PMMPTE StartingPte,
    IN PFN_NUMBER NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPteType
    );

NTSTATUS
MiAddMappedPtes (
    IN PMMPTE FirstPte,
    IN PFN_NUMBER NumberOfPtes,
    IN PCONTROL_AREA ControlArea
    );

VOID
MiInitializeIoTrackers (
    VOID
    );

PVOID
MiMapSinglePage (
     IN PVOID VirtualAddress OPTIONAL,
     IN PFN_NUMBER PageFrameIndex,
     IN MEMORY_CACHING_TYPE CacheType,
     IN MM_PAGE_PRIORITY Priority
     );

VOID
MiUnmapSinglePage (
     IN PVOID BaseAddress
     );

typedef struct _MM_PTE_MAPPING {
    LIST_ENTRY ListEntry;
    PVOID SystemVa;
    PVOID SystemEndVa;
    ULONG Protection;
} MM_PTE_MAPPING, *PMM_PTE_MAPPING;

extern LIST_ENTRY MmProtectedPteList;

extern KSPIN_LOCK MmProtectedPteLock;

LOGICAL
MiCheckSystemPteProtection (
    IN ULONG_PTR StoreInstruction,
    IN PVOID VirtualAddress
    );

//
// Access Fault routines.
//

#define STATUS_ISSUE_PAGING_IO (0xC0033333)

#define MM_NOIRQL (HIGH_LEVEL + 2)

NTSTATUS
MiDispatchFault (
    IN ULONG_PTR FaultStatus,
    IN PVOID VirtualAdress,
    IN PMMPTE PointerPte,
    IN PMMPTE PointerProtoPte,
    IN LOGICAL RecheckAccess,
    IN PEPROCESS Process,
    IN PMMVAD Vad,
    OUT PLOGICAL ApcNeeded
    );

NTSTATUS
MiResolveDemandZeroFault (
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PEPROCESS Process,
    IN KIRQL OldIrql
    );

BOOLEAN
MiIsAddressValid (
    IN PVOID VirtualAddress,
    IN LOGICAL UseForceIfPossible
    );

VOID
MiAllowWorkingSetExpansion (
    IN PMMSUPPORT WsInfo
    );

WSLE_NUMBER
MiAddValidPageToWorkingSet (
    IN PVOID VirtualAddress,
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn1,
    IN ULONG WsleMask
    );

VOID
MiTrimPte (
    IN PVOID VirtualAddress,
    IN PMMPTE ReadPte,
    IN PMMPFN Pfn1,
    IN PEPROCESS CurrentProcess,
    IN MMPTE NewPteContents
    );

NTSTATUS
MiWaitForInPageComplete (
    IN PMMPFN Pfn,
    IN PMMPTE PointerPte,
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPteContents,
    IN PMMINPAGE_SUPPORT InPageSupport,
    IN PEPROCESS CurrentProcess
    );

LOGICAL
FASTCALL
MiCopyOnWrite (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte
    );

VOID
MiSetDirtyBit (
    IN PVOID FaultingAddress,
    IN PMMPTE PointerPte,
    IN ULONG PfnHeld
    );

VOID
MiSetModifyBit (
    IN PMMPFN Pfn
    );

PMMPTE
MiFindActualFaultingPte (
    IN PVOID FaultingAddress
    );

VOID
MiInitializeReadInProgressSinglePfn (
    IN PFN_NUMBER PageFrameIndex,
    IN PMMPTE BasePte,
    IN PKEVENT Event,
    IN WSLE_NUMBER WorkingSetIndex
    );

VOID
MiInitializeReadInProgressPfn (
    IN PMDL Mdl,
    IN PMMPTE BasePte,
    IN PKEVENT Event,
    IN WSLE_NUMBER WorkingSetIndex
    );

NTSTATUS
MiAccessCheck (
    IN PMMPTE PointerPte,
    IN ULONG_PTR WriteOperation,
    IN KPROCESSOR_MODE PreviousMode,
    IN ULONG Protection,
    IN BOOLEAN CallerHoldsPfnLock
    );

NTSTATUS
FASTCALL
MiCheckForUserStackOverflow (
    IN PVOID FaultingAddress
    );

PMMPTE
MiCheckVirtualAddress (
    IN PVOID VirtualAddress,
    OUT PULONG ProtectCode,
    OUT PMMVAD *VadOut
    );

NTSTATUS
FASTCALL
MiCheckPdeForPagedPool (
    IN PVOID VirtualAddress
    );

#if defined (_WIN64)
#define MI_IS_WOW64_PROCESS(PROCESS) (PROCESS->Wow64Process)
#else
#define MI_IS_WOW64_PROCESS(PROCESS) NULL
#endif

#if DBG || defined (_MI_DEBUG_ALTPTE)
#define MI_BREAK_ON_AV(VirtualAddress, Id)                                  \
        if (MmDebug & MM_DBG_STOP_ON_ACCVIO) {                              \
            DbgPrint ("MM:access violation - %p %u\n", VirtualAddress, Id); \
            DbgBreakPoint ();                                               \
        }                                                                   \
        if (MmDebug & MM_DBG_STOP_ON_WOW64_ACCVIO) {                        \
            if (MI_IS_WOW64_PROCESS(PsGetCurrentProcess())) {               \
                DbgPrint ("MM:wow64 access violation - %p %u\n", VirtualAddress, Id); \
                DbgBreakPoint ();                                           \
            }                                                               \
        }
#else
#define MI_BREAK_ON_AV(VirtualAddress, Id)
#endif

//
// Routines which operate on an address tree.
//

PMMADDRESS_NODE
FASTCALL
MiGetNextNode (
    IN PMMADDRESS_NODE Node
    );

PMMADDRESS_NODE
FASTCALL
MiGetPreviousNode (
    IN PMMADDRESS_NODE Node
    );


PMMADDRESS_NODE
FASTCALL
MiGetFirstNode (
    IN PMM_AVL_TABLE Root
    );

PMMADDRESS_NODE
MiGetLastNode (
    IN PMM_AVL_TABLE Root
    );

VOID
FASTCALL
MiInsertNode (
    IN PMMADDRESS_NODE Node,
    IN PMM_AVL_TABLE Root
    );

VOID
FASTCALL
MiRemoveNode (
    IN PMMADDRESS_NODE Node,
    IN PMM_AVL_TABLE Root
    );

PMMADDRESS_NODE
FASTCALL
MiLocateAddressInTree (
    IN ULONG_PTR Vpn,
    IN PMM_AVL_TABLE Root
    );

PMMADDRESS_NODE
MiCheckForConflictingNode (
    IN ULONG_PTR StartVpn,
    IN ULONG_PTR EndVpn,
    IN PMM_AVL_TABLE Root
    );

NTSTATUS
MiFindEmptyAddressRangeInTree (
    IN SIZE_T SizeOfRange,
    IN ULONG_PTR Alignment,
    IN PMM_AVL_TABLE Root,
    OUT PMMADDRESS_NODE *PreviousVad,
    OUT PVOID *Base
    );

NTSTATUS
MiFindEmptyAddressRangeDownTree (
    IN SIZE_T SizeOfRange,
    IN PVOID HighestAddressToEndAt,
    IN ULONG_PTR Alignment,
    IN PMM_AVL_TABLE Root,
    OUT PVOID *Base
    );

NTSTATUS
MiFindEmptyAddressRangeDownBasedTree (
    IN SIZE_T SizeOfRange,
    IN PVOID HighestAddressToEndAt,
    IN ULONG_PTR Alignment,
    IN PMM_AVL_TABLE Root,
    OUT PVOID *Base
    );

PVOID
MiEnumerateGenericTableWithoutSplayingAvl (
    IN PMM_AVL_TABLE Table,
    IN PVOID *RestartKey
    );

VOID
NodeTreeWalk (
    PMMADDRESS_NODE Start
    );

TABLE_SEARCH_RESULT
MiFindNodeOrParent (
    IN PMM_AVL_TABLE Table,
    IN ULONG_PTR StartingVpn,
    OUT PMMADDRESS_NODE *NodeOrParent
    );

//
// Routines which operate on the tree of virtual address descriptors.
//

NTSTATUS
MiInsertVad (
    IN PMMVAD Vad
    );

VOID
MiRemoveVad (
    IN PMMVAD Vad
    );

PMMVAD
FASTCALL
MiLocateAddress (
    IN PVOID VirtualAddress
    );

NTSTATUS
MiFindEmptyAddressRange (
    IN SIZE_T SizeOfRange,
    IN ULONG_PTR Alignment,
    IN ULONG QuickCheck,
    IN PVOID *Base
    );

//
// Routines which operate on the clone tree structure.
//


NTSTATUS
MiCloneProcessAddressSpace (
    IN PEPROCESS ProcessToClone,
    IN PEPROCESS ProcessToInitialize
    );


ULONG
MiDecrementCloneBlockReference (
    IN PMMCLONE_DESCRIPTOR CloneDescriptor,
    IN PMMCLONE_BLOCK CloneBlock,
    IN PEPROCESS CurrentProcess,
    IN KIRQL OldIrql
    );

LOGICAL
MiWaitForForkToComplete (
    IN PEPROCESS CurrentProcess
    );

//
// Routines which operate on the working set list.
//

WSLE_NUMBER
MiAllocateWsle (
    IN PMMSUPPORT WsInfo,
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn1,
    IN ULONG_PTR WsleMask
    );

VOID
MiReleaseWsle (
    IN WSLE_NUMBER WorkingSetIndex,
    IN PMMSUPPORT WsInfo
    );

VOID
MiInitializeWorkingSetList (
    IN PEPROCESS CurrentProcess
    );

VOID
MiGrowWsleHash (
    IN PMMSUPPORT WsInfo
    );

WSLE_NUMBER
MiTrimWorkingSet (
    IN WSLE_NUMBER Reduction,
    IN PMMSUPPORT WsInfo,
    IN ULONG TrimAge
    );

#if defined(_AMD64_)
#define MM_PROCESS_COMMIT_CHARGE 6
#elif defined(_IA64_)
#define MM_PROCESS_COMMIT_CHARGE 5
#elif defined (_X86PAE_)
#define MM_PROCESS_COMMIT_CHARGE 8
#else
#define MM_PROCESS_COMMIT_CHARGE 4
#endif

#define MI_SYSTEM_GLOBAL    0
#define MI_USER_LOCAL       1
#define MI_SESSION_LOCAL    2

LOGICAL
MiTrimAllSystemPagableMemory (
    IN ULONG MemoryType,
    IN LOGICAL PurgeTransition
    );

VOID
MiRemoveWorkingSetPages (
    IN PMMSUPPORT WsInfo
    );

VOID
MiAgeAndEstimateAvailableInWorkingSet (
    IN PMMSUPPORT VmSupport,
    IN LOGICAL DoAging,
    IN PWSLE_NUMBER WslesScanned,
    IN OUT PPFN_NUMBER TotalClaim,
    IN OUT PPFN_NUMBER TotalEstimatedAvailable
    );

VOID
FASTCALL
MiInsertWsleHash (
    IN WSLE_NUMBER Entry,
    IN PMMSUPPORT WsInfo
    );

VOID
FASTCALL
MiRemoveWsle (
    IN WSLE_NUMBER Entry,
    IN PMMWSL WorkingSetList
    );

WSLE_NUMBER
FASTCALL
MiLocateWsle (
    IN PVOID VirtualAddress,
    IN PMMWSL WorkingSetList,
    IN WSLE_NUMBER WsPfnIndex
    );

ULONG
MiFreeWsle (
    IN WSLE_NUMBER WorkingSetIndex,
    IN PMMSUPPORT WsInfo,
    IN PMMPTE PointerPte
    );

WSLE_NUMBER
MiFreeWsleList (
    IN PMMSUPPORT WsInfo,
    IN PMMWSLE_FLUSH_LIST WsleFlushList
    );

VOID
MiSwapWslEntries (
    IN WSLE_NUMBER SwapEntry,
    IN WSLE_NUMBER Entry,
    IN PMMSUPPORT WsInfo,
    IN LOGICAL EntryNotInHash
    );

VOID
MiRemoveWsleFromFreeList (
    IN WSLE_NUMBER Entry,
    IN PMMWSLE Wsle,
    IN PMMWSL WorkingSetList
    );

ULONG
MiRemovePageFromWorkingSet (
    IN PMMPTE PointerPte,
    IN PMMPFN Pfn1,
    IN PMMSUPPORT WsInfo
    );

PFN_NUMBER
MiDeleteSystemPagableVm (
    IN PMMPTE PointerPte,
    IN PFN_NUMBER NumberOfPtes,
    IN MMPTE NewPteValue,
    IN LOGICAL SessionAllocation,
    OUT PPFN_NUMBER ResidentPages OPTIONAL
    );

VOID
MiLockCode (
    IN PMMPTE FirstPte,
    IN PMMPTE LastPte,
    IN ULONG LockType
    );

PKLDR_DATA_TABLE_ENTRY
MiLookupDataTableEntry (
    IN PVOID AddressWithinSection,
    IN ULONG ResourceHeld
    );

//
// Routines which perform working set management.
//

VOID
MiObtainFreePages (
    VOID
    );

VOID
MiModifiedPageWriter (
    IN PVOID StartContext
    );

VOID
MiMappedPageWriter (
    IN PVOID StartContext
    );

LOGICAL
MiIssuePageExtendRequest (
    IN PMMPAGE_FILE_EXPANSION PageExtend
    );

VOID
MiIssuePageExtendRequestNoWait (
    IN PFN_NUMBER SizeInPages
    );

SIZE_T
MiExtendPagingFiles (
    IN PMMPAGE_FILE_EXPANSION PageExpand
    );

VOID
MiContractPagingFiles (
    VOID
    );

VOID
MiAttemptPageFileReduction (
    VOID
    );

LOGICAL
MiCancelWriteOfMappedPfn (
    IN PFN_NUMBER PageToStop,
    IN KIRQL OldIrql
    );

//
// Routines to delete address space.
//

VOID
MiDeletePteRange (
    IN PMMSUPPORT WsInfo,
    IN PMMPTE PointerPte,
    IN PMMPTE LastPte,
    IN LOGICAL AddressSpaceDeletion
    );

VOID
MiDeleteVirtualAddresses (
    IN PUCHAR StartingAddress,
    IN PUCHAR EndingAddress,
    IN PMMVAD Vad
    );

ULONG
MiDeletePte (
    IN PMMPTE PointerPte,
    IN PVOID VirtualAddress,
    IN ULONG AddressSpaceDeletion,
    IN PEPROCESS CurrentProcess,
    IN PMMPTE PrototypePte,
    IN PMMPTE_FLUSH_LIST PteFlushList OPTIONAL,
    IN KIRQL OldIrql
    );

VOID
MiDeleteValidSystemPte (
    IN PMMPTE PointerPte,
    IN PVOID VirtualAddress,
    IN PMMSUPPORT WsInfo,
    IN PMMPTE_FLUSH_LIST PteFlushList OPTIONAL
    );

VOID
MiDeletePageTablesForPhysicalRange (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress
    );

VOID
MiFlushPteList (
    IN PMMPTE_FLUSH_LIST PteFlushList,
    IN ULONG AllProcessors
    );

ULONG
FASTCALL
MiReleasePageFileSpace (
    IN MMPTE PteContents
    );

VOID
FASTCALL
MiUpdateModifiedWriterMdls (
    IN ULONG PageFileNumber
    );

PVOID
MiAllocateAweInfo (
    VOID
    );

VOID
MiRemoveUserPhysicalPagesVad (
    IN PMMVAD_SHORT FoundVad
    );

VOID
MiCleanPhysicalProcessPages (
    IN PEPROCESS Process
    );

VOID
MiInsertPhysicalVadRoot (
    IN PEPROCESS Process,
    IN PMM_AVL_TABLE PhysicalVadRoot
    );

VOID
MiPhysicalViewInserter (
    IN PEPROCESS Process,
    IN PMI_PHYSICAL_VIEW PhysicalView
    );

VOID
MiPhysicalViewRemover (
    IN PEPROCESS Process,
    IN PMMVAD Vad
    );

VOID
MiPhysicalViewAdjuster (
    IN PEPROCESS Process,
    IN PMMVAD OldVad,
    IN PMMVAD NewVad
    );

//
// MM_SYSTEM_PAGE_COLOR - MmSystemPageColor
//
// This variable is updated frequently, on MP systems we keep
// a separate system color per processor to avoid cache line
// thrashing.
//

#if defined(NT_UP)

#define MI_SYSTEM_PAGE_COLOR    MmSystemPageColor

#else

#define MI_SYSTEM_PAGE_COLOR    (KeGetCurrentPrcb()->PageColor)

#endif

#if defined(MI_MULTINODE)

extern PKNODE KeNodeBlock[];

#define MI_NODE_FROM_COLOR(c)                                               \
        (KeNodeBlock[(c) >> MmSecondaryColorNodeShift])

#define MI_GET_COLOR_FROM_LIST_ENTRY(index,pfn)                             \
    ((ULONG)(((pfn)->u3.e1.PageColor << MmSecondaryColorNodeShift) |        \
         MI_GET_SECONDARY_COLOR((index),(pfn))))

#define MI_ADJUST_COLOR_FOR_NODE(c,n)   ((c) | (n)->Color)
#define MI_CURRENT_NODE_COLOR           (KeGetCurrentNode()->MmShiftedColor)

#define MiRemoveZeroPageIfAny(COLOR)                                        \
        KeNumberNodes > 1 ? (KeGetCurrentNode()->FreeCount[ZeroedPageList] != 0) ? MiRemoveZeroPage(COLOR) : 0 :                                           \
    (MmFreePagesByColor[ZeroedPageList][COLOR].Flink != MM_EMPTY_LIST) ?    \
                       MiRemoveZeroPage(COLOR) : 0

#define MI_GET_PAGE_COLOR_NODE(n)                                           \
        (((MI_SYSTEM_PAGE_COLOR++) & MmSecondaryColorMask) |                \
         KeNodeBlock[n]->MmShiftedColor)

#else

#define MI_NODE_FROM_COLOR(c)

#define MI_GET_COLOR_FROM_LIST_ENTRY(index,pfn)                             \
         ((ULONG)MI_GET_SECONDARY_COLOR((index),(pfn)))

#define MI_ADJUST_COLOR_FOR_NODE(c,n)   (c)
#define MI_CURRENT_NODE_COLOR           0

#define MiRemoveZeroPageIfAny(COLOR)   \
    (MmFreePagesByColor[ZeroedPageList][COLOR].Flink != MM_EMPTY_LIST) ? \
                       MiRemoveZeroPage(COLOR) : 0

#define MI_GET_PAGE_COLOR_NODE(n)                                           \
        ((MI_SYSTEM_PAGE_COLOR++) & MmSecondaryColorMask)

#endif

FORCEINLINE
PFN_NUMBER
MiRemoveZeroPageMayReleaseLocks (
    IN ULONG Color,
    IN KIRQL OldIrql
    )

/*++

Routine Description:

    This routine returns a zeroed page.
    
    It may release and reacquire the PFN lock to do so, as well as mapping
    the page in hyperspace to perform the actual zeroing if necessary.

Environment:

    Kernel mode.  PFN lock held, hyperspace lock NOT held.

--*/

{
    PFN_NUMBER PageFrameIndex;

    PageFrameIndex = MiRemoveZeroPageIfAny (Color);

    if (PageFrameIndex == 0) {
        PageFrameIndex = MiRemoveAnyPage (Color);
        UNLOCK_PFN (OldIrql);
        MiZeroPhysicalPage (PageFrameIndex, Color);
        LOCK_PFN (OldIrql);
    }

    return PageFrameIndex;
}

//
// General support routines.
//

#if (_MI_PAGING_LEVELS <= 3)

//++
//PMMPTE
//MiGetPxeAddress (
//    IN PVOID va
//    );
//
// Routine Description:
//
//    MiGetPxeAddress returns the address of the extended page directory parent
//    entry which maps the given virtual address.  This is one level above the
//    page parent directory.
//
// Arguments
//
//    Va - Supplies the virtual address to locate the PXE for.
//
// Return Value:
//
//    The address of the PXE.
//
//--

#define MiGetPxeAddress(va)   ((PMMPTE)0)

//++
//LOGICAL
//MiIsPteOnPxeBoundary (
//    IN PVOID PTE
//    );
//
// Routine Description:
//
//    MiIsPteOnPxeBoundary returns TRUE if the PTE is
//    on an extended page directory parent entry boundary.
//
// Arguments
//
//    PTE - Supplies the PTE to check.
//
// Return Value:
//
//    TRUE if on a boundary, FALSE if not.
//
//--

#define MiIsPteOnPxeBoundary(PTE) (FALSE)

#endif

#if (_MI_PAGING_LEVELS <= 2)

//++
//PMMPTE
//MiGetPpeAddress (
//    IN PVOID va
//    );
//
// Routine Description:
//
//    MiGetPpeAddress returns the address of the page directory parent entry
//    which maps the given virtual address.  This is one level above the
//    page directory.
//
// Arguments
//
//    Va - Supplies the virtual address to locate the PPE for.
//
// Return Value:
//
//    The address of the PPE.
//
//--

#define MiGetPpeAddress(va)  ((PMMPTE)0)

//++
//LOGICAL
//MiIsPteOnPpeBoundary (
//    IN PVOID VA
//    );
//
// Routine Description:
//
//    MiIsPteOnPpeBoundary returns TRUE if the PTE is
//    on a page directory parent entry boundary.
//
// Arguments
//
//    VA - Supplies the virtual address to check.
//
// Return Value:
//
//    TRUE if on a boundary, FALSE if not.
//
//--

#define MiIsPteOnPpeBoundary(PTE) (FALSE)

#endif

ULONG
MiDoesPdeExistAndMakeValid (
    IN PMMPTE PointerPde,
    IN PEPROCESS TargetProcess,
    IN KIRQL OldIrql,
    OUT PULONG Waited
    );

#if (_MI_PAGING_LEVELS >= 3)
#define MiDoesPpeExistAndMakeValid(PPE, PROCESS, PFNLOCKIRQL, WAITED) \
            MiDoesPdeExistAndMakeValid(PPE, PROCESS, PFNLOCKIRQL, WAITED)
#else
#define MiDoesPpeExistAndMakeValid(PPE, PROCESS, PFNLOCKIRQL, WAITED) 1
#endif

#if (_MI_PAGING_LEVELS >= 4)
#define MiDoesPxeExistAndMakeValid(PXE, PROCESS, PFNLOCKIRQL, WAITED) \
            MiDoesPdeExistAndMakeValid(PXE, PROCESS, PFNLOCKIRQL, WAITED)
#else
#define MiDoesPxeExistAndMakeValid(PXE, PROCESS, PFNLOCKIRQL, WAITED) 1
#endif

VOID
MiMakePdeExistAndMakeValid (
    IN PMMPTE PointerPde,
    IN PEPROCESS TargetProcess,
    IN KIRQL OldIrql
    );

ULONG
FASTCALL
MiMakeSystemAddressValid (
    IN PVOID VirtualAddress,
    IN PEPROCESS CurrentProcess
    );

ULONG
FASTCALL
MiMakeSystemAddressValidPfnWs (
    IN PVOID VirtualAddress,
    IN PEPROCESS CurrentProcess OPTIONAL,
    IN KIRQL OldIrql
    );

ULONG
FASTCALL
MiMakeSystemAddressValidPfnSystemWs (
    IN PVOID VirtualAddress,
    IN KIRQL OldIrql
    );

ULONG
FASTCALL
MiMakeSystemAddressValidPfn (
    IN PVOID VirtualAddress,
    IN KIRQL OldIrql
    );

VOID
FASTCALL
MiLockPagedAddress (
    IN PVOID VirtualAddress
    );

VOID
FASTCALL
MiUnlockPagedAddress (
    IN PVOID VirtualAddress,
    IN ULONG PfnLockHeld
    );

ULONG
FASTCALL
MiIsPteDecommittedPage (
    IN PMMPTE PointerPte
    );

ULONG
FASTCALL
MiIsProtectionCompatible (
    IN ULONG OldProtect,
    IN ULONG NewProtect
    );

ULONG
FASTCALL
MiIsPteProtectionCompatible (
    IN ULONG OldPteProtection,
    IN ULONG NewProtect
    );

ULONG
FASTCALL
MiMakeProtectionMask (
    IN ULONG Protect
    );

ULONG
MiIsEntireRangeCommitted (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    );

ULONG
MiIsEntireRangeDecommitted (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    );

LOGICAL
MiCheckProtoPtePageState (
    IN PMMPTE PrototypePte,
    IN KIRQL OldIrql,
    OUT PLOGICAL DroppedPfnLock
    );

//++
//PMMPTE
//MiGetProtoPteAddress (
//    IN PMMPTE VAD,
//    IN PVOID VA
//    );
//
// Routine Description:
//
//    MiGetProtoPteAddress returns a pointer to the prototype PTE which
//    is mapped by the given virtual address descriptor and address within
//    the virtual address descriptor.
//
// Arguments
//
//    VAD - Supplies a pointer to the virtual address descriptor that contains
//          the VA.
//
//    VPN - Supplies the virtual page number.
//
// Return Value:
//
//    A pointer to the proto PTE which corresponds to the VA.
//
//--


#define MiGetProtoPteAddress(VAD,VPN)                                        \
    ((((((VPN) - (VAD)->StartingVpn) << PTE_SHIFT) +                         \
      (ULONG_PTR)(VAD)->FirstPrototypePte) <= (ULONG_PTR)(VAD)->LastContiguousPte) ? \
    ((PMMPTE)(((((VPN) - (VAD)->StartingVpn) << PTE_SHIFT) +                 \
        (ULONG_PTR)(VAD)->FirstPrototypePte))) :                                  \
        MiGetProtoPteAddressExtended ((VAD),(VPN)))

PMMPTE
FASTCALL
MiGetProtoPteAddressExtended (
    IN PMMVAD Vad,
    IN ULONG_PTR Vpn
    );

PSUBSECTION
FASTCALL
MiLocateSubsection (
    IN PMMVAD Vad,
    IN ULONG_PTR Vpn
    );

VOID
MiInitializeSystemCache (
    IN ULONG MinimumWorkingSet,
    IN ULONG MaximumWorkingSet
    );

VOID
MiAdjustWorkingSetManagerParameters(
    IN LOGICAL WorkStation
    );

#if defined (_MI_COMPRESSION)
VOID
MiNotifyMemoryEvents (
    VOID
    );
#endif

extern PFN_NUMBER MmLowMemoryThreshold;
extern PFN_NUMBER MmHighMemoryThreshold;

extern PFN_NUMBER MiLowPagedPoolThreshold;
extern PFN_NUMBER MiHighPagedPoolThreshold;

extern PFN_NUMBER MiLowNonPagedPoolThreshold;
extern PFN_NUMBER MiHighNonPagedPoolThreshold;

extern PKEVENT MiLowPagedPoolEvent;
extern PKEVENT MiHighPagedPoolEvent;

extern PKEVENT MiLowNonPagedPoolEvent;
extern PKEVENT MiHighNonPagedPoolEvent;

//
// Section support
//

VOID
FASTCALL
MiInsertBasedSection (
    IN PSECTION Section
    );

NTSTATUS
MiMapViewOfPhysicalSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN ULONG ProtectionMask,
    IN ULONG_PTR ZeroBits,
    IN ULONG AllocationType,
    IN LOGICAL WriteCombined
    );

NTSTATUS
MiMapViewOfDataSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PLARGE_INTEGER SectionOffset,
    IN PSIZE_T CapturedViewSize,
    IN PSECTION Section,
    IN SECTION_INHERIT InheritDisposition,
    IN ULONG ProtectionMask,
    IN SIZE_T CommitSize,
    IN ULONG_PTR ZeroBits,
    IN ULONG AllocationType
    );

NTSTATUS
MiUnmapViewOfSection (
    IN PEPROCESS Process,
    IN PVOID BaseAddress,
    IN LOGICAL AddressSpaceMutexHeld
    );

VOID
MiRemoveImageSectionObject(
    IN PFILE_OBJECT File,
    IN PCONTROL_AREA ControlArea
    );

VOID
MiAddSystemPtes(
    IN PMMPTE StartingPte,
    IN ULONG  NumberOfPtes,
    IN MMSYSTEM_PTE_POOL_TYPE SystemPtePoolType
    );

VOID
MiRemoveMappedView (
    IN PEPROCESS CurrentProcess,
    IN PMMVAD Vad
    );

VOID
MiSegmentDelete (
    PSEGMENT Segment
    );

VOID
MiSectionDelete (
    IN PVOID Object
    );

VOID
MiDereferenceSegmentThread (
    IN PVOID StartContext
    );

NTSTATUS
MiCreateImageFileMap (
    IN PFILE_OBJECT File,
    OUT PSEGMENT *Segment
    );

NTSTATUS
MiCreateDataFileMap (
    IN PFILE_OBJECT File,
    OUT PSEGMENT *Segment,
    IN PUINT64 MaximumSize,
    IN ULONG SectionPageProtection,
    IN ULONG AllocationAttributes,
    IN ULONG IgnoreFileSizing
    );

NTSTATUS
MiCreatePagingFileMap (
    OUT PSEGMENT *Segment,
    IN PUINT64 MaximumSize,
    IN ULONG ProtectionMask,
    IN ULONG AllocationAttributes
    );

VOID
MiPurgeSubsectionInternal (
    IN PSUBSECTION Subsection,
    IN ULONG PteOffset
    );

VOID
MiPurgeImageSection (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS Process OPTIONAL,
    IN KIRQL OldIrql
    );

VOID
MiCleanSection (
    IN PCONTROL_AREA ControlArea,
    IN LOGICAL DirtyDataPagesOk
    );

VOID
MiDereferenceControlArea (
    IN PCONTROL_AREA ControlArea
    );

VOID
MiCheckControlArea (
    IN PCONTROL_AREA ControlArea,
    IN PEPROCESS CurrentProcess,
    IN KIRQL PreviousIrql
    );

NTSTATUS
MiCheckPurgeAndUpMapCount (
    IN PCONTROL_AREA ControlArea,
    IN LOGICAL FailIfSystemViews
    );

VOID
MiCheckForControlAreaDeletion (
    IN PCONTROL_AREA ControlArea
    );

LOGICAL
MiCheckControlAreaStatus (
    IN SECTION_CHECK_TYPE SectionCheckType,
    IN PSECTION_OBJECT_POINTERS SectionObjectPointers,
    IN ULONG DelayClose,
    OUT PCONTROL_AREA *ControlArea,
    OUT PKIRQL OldIrql
    );

extern SLIST_HEADER MmEventCountSListHead;

PEVENT_COUNTER
MiGetEventCounter (
    VOID
    );

VOID
MiFreeEventCounter (
    IN PEVENT_COUNTER Support
    );

ULONG
MiCanFileBeTruncatedInternal (
    IN PSECTION_OBJECT_POINTERS SectionPointer,
    IN PLARGE_INTEGER NewFileSize OPTIONAL,
    IN LOGICAL BlockNewViews,
    OUT PKIRQL PreviousIrql
    );

#define STATUS_MAPPED_WRITER_COLLISION (0xC0033333)

NTSTATUS
MiFlushSectionInternal (
    IN PMMPTE StartingPte,
    IN PMMPTE FinalPte,
    IN PSUBSECTION FirstSubsection,
    IN PSUBSECTION LastSubsection,
    IN ULONG Synchronize,
    IN LOGICAL WriteInProgressOk,
    OUT PIO_STATUS_BLOCK IoStatus
    );

//
// protection stuff...
//

NTSTATUS
MiProtectVirtualMemory (
    IN PEPROCESS Process,
    IN PVOID *CapturedBase,
    IN PSIZE_T CapturedRegionSize,
    IN ULONG Protect,
    IN PULONG LastProtect
    );

ULONG
MiGetPageProtection (
    IN PMMPTE PointerPte,
    IN PEPROCESS Process,
    IN LOGICAL PteCapturedToLocalStack
    );

NTSTATUS
MiSetProtectionOnSection (
    IN PEPROCESS Process,
    IN PMMVAD Vad,
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN ULONG NewProtect,
    OUT PULONG CapturedOldProtect,
    IN ULONG DontCharge,
    OUT PULONG Locked
    );

NTSTATUS
MiCheckSecuredVad (
    IN PMMVAD Vad,
    IN PVOID Base,
    IN ULONG_PTR Size,
    IN ULONG ProtectionMask
    );

HANDLE
MiSecureVirtualMemory (
    IN PVOID Address,
    IN SIZE_T Size,
    IN ULONG ProbeMode,
    IN LOGICAL AddressSpaceMutexHeld
    );

VOID
MiUnsecureVirtualMemory (
    IN HANDLE SecureHandle,
    IN LOGICAL AddressSpaceMutexHeld
    );

ULONG
MiChangeNoAccessForkPte (
    IN PMMPTE PointerPte,
    IN ULONG ProtectionMask
    );

//
// Routines for charging quota and commitment.
//

VOID
MiTrimSegmentCache (
    VOID
    );

VOID
MiInitializeCommitment (
    VOID
    );

LOGICAL
FASTCALL
MiChargeCommitment (
    IN SIZE_T QuotaCharge,
    IN PEPROCESS Process OPTIONAL
    );

LOGICAL
FASTCALL
MiChargeCommitmentCantExpand (
    IN SIZE_T QuotaCharge,
    IN ULONG MustSucceed
    );

LOGICAL
FASTCALL
MiChargeTemporaryCommitmentForReduction (
    IN SIZE_T QuotaCharge
    );

#if defined (_MI_DEBUG_COMMIT_LEAKS)

VOID
FASTCALL
MiReturnCommitment (
    IN SIZE_T QuotaCharge
    );

#else

#define MiReturnCommitment(_QuotaCharge)                                \
            ASSERT ((SSIZE_T)(_QuotaCharge) >= 0);                      \
            ASSERT (MmTotalCommittedPages >= (_QuotaCharge));           \
            InterlockedExchangeAddSizeT (&MmTotalCommittedPages, 0-((SIZE_T)(_QuotaCharge))); \
            MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_NORMAL, (_QuotaCharge));

#endif

VOID
MiCauseOverCommitPopup (
    VOID
    );


extern SIZE_T MmTotalCommitLimitMaximum;

SIZE_T
MiCalculatePageCommitment (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    );

VOID
MiReturnPageTablePageCommitment (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PEPROCESS CurrentProcess,
    IN PMMVAD PreviousVad,
    IN PMMVAD NextVad
    );

VOID
MiFlushAllPages (
    VOID
    );

VOID
MiModifiedPageWriterTimerDispatch (
    IN PKDPC Dpc,
    IN PVOID DeferredContext,
    IN PVOID SystemArgument1,
    IN PVOID SystemArgument2
    );

LONGLONG
MiStartingOffset(
    IN PSUBSECTION Subsection,
    IN PMMPTE PteAddress
    );

LARGE_INTEGER
MiEndingOffset(
    IN PSUBSECTION Subsection
    );

VOID
MiReloadBootLoadedDrivers (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

VOID
MiSetSystemCodeProtection (
    IN PMMPTE FirstPte,
    IN PMMPTE LastPte,
    IN ULONG ProtectionMask
    );

VOID
MiWriteProtectSystemImage (
    IN PVOID DllBase
    );

VOID
MiSetIATProtect (
    IN PVOID DllBase,
    IN ULONG Protection
    );

VOID
MiMakeEntireImageCopyOnWrite (
    IN PSUBSECTION Subsection
    );

LOGICAL
MiInitializeLoadedModuleList (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

#define UNICODE_TAB               0x0009
#define UNICODE_LF                0x000A
#define UNICODE_CR                0x000D
#define UNICODE_SPACE             0x0020
#define UNICODE_CJK_SPACE         0x3000

#define UNICODE_WHITESPACE(_ch)     (((_ch) == UNICODE_TAB) || \
                                     ((_ch) == UNICODE_LF) || \
                                     ((_ch) == UNICODE_CR) || \
                                     ((_ch) == UNICODE_SPACE) || \
                                     ((_ch) == UNICODE_CJK_SPACE) || \
                                     ((_ch) == UNICODE_NULL))

extern ULONG MmSpecialPoolTag;
extern PVOID MmSpecialPoolStart;
extern PVOID MmSpecialPoolEnd;
extern PVOID MmSessionSpecialPoolStart;
extern PVOID MmSessionSpecialPoolEnd;

LOGICAL
MiInitializeSpecialPool (
    IN POOL_TYPE PoolType
    );

LOGICAL
MiIsSpecialPoolAddressNonPaged (
    IN PVOID VirtualAddress
    );

#if defined (_WIN64)
LOGICAL
MiInitializeSessionSpecialPool (
    VOID
    );

VOID
MiDeleteSessionSpecialPool (
    VOID
    );
#endif

#if defined (_X86_)
LOGICAL
MiRecoverSpecialPtes (
    IN ULONG NumberOfPtes
    );
#endif

VOID
MiEnableRandomSpecialPool (
    IN LOGICAL Enable
    );

LOGICAL
MiTriageSystem (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

LOGICAL
MiTriageAddDrivers (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

LOGICAL
MiTriageVerifyDriver (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

extern ULONG MmTriageActionTaken;

#if defined (_WIN64)
#define MM_SPECIAL_POOL_PTES ((1024 * 1024) / sizeof (MMPTE))
#else
#define MM_SPECIAL_POOL_PTES (24 * PTE_PER_PAGE)
#endif

#define MI_SUSPECT_DRIVER_BUFFER_LENGTH 512

extern WCHAR MmVerifyDriverBuffer[];
extern ULONG MmVerifyDriverBufferLength;
extern ULONG MmVerifyDriverLevel;

extern LOGICAL MmSnapUnloads;
extern LOGICAL MmProtectFreedNonPagedPool;
extern ULONG MmEnforceWriteProtection;
extern LOGICAL MmTrackLockedPages;
extern ULONG MmTrackPtes;

#define VI_POOL_FREELIST_END  ((ULONG_PTR)-1)

#define VI_POOL_PAGE_HEADER_SIGNATURE 0x21321345

typedef struct _VI_POOL_PAGE_HEADER {
    PSLIST_ENTRY NextPage;
    PVOID VerifierEntry;
    ULONG_PTR Signature;
} VI_POOL_PAGE_HEADER, *PVI_POOL_PAGE_HEADER;

typedef struct _VI_POOL_ENTRY_INUSE {
    PVOID VirtualAddress;
    PVOID CallingAddress;
    SIZE_T NumberOfBytes;
    ULONG_PTR Tag;
} VI_POOL_ENTRY_INUSE, *PVI_POOL_ENTRY_INUSE;

typedef struct _VI_POOL_ENTRY {
    union {
        VI_POOL_PAGE_HEADER PageHeader;
        VI_POOL_ENTRY_INUSE InUse;
        PSLIST_ENTRY NextFree;
    };
} VI_POOL_ENTRY, *PVI_POOL_ENTRY;

#define MI_VERIFIER_ENTRY_SIGNATURE            0x98761940

typedef struct _MI_VERIFIER_DRIVER_ENTRY {
    LIST_ENTRY Links;
    ULONG Loads;
    ULONG Unloads;

    UNICODE_STRING BaseName;
    PVOID StartAddress;
    PVOID EndAddress;

#define VI_VERIFYING_DIRECTLY   0x1
#define VI_VERIFYING_INVERSELY  0x2
#define VI_DISABLE_VERIFICATION 0x4

    ULONG Flags;
    ULONG_PTR Signature;

    SLIST_HEADER PoolPageHeaders;
    SLIST_HEADER PoolTrackers;

    ULONG CurrentPagedPoolAllocations;
    ULONG CurrentNonPagedPoolAllocations;
    ULONG PeakPagedPoolAllocations;
    ULONG PeakNonPagedPoolAllocations;

    SIZE_T PagedBytes;
    SIZE_T NonPagedBytes;
    SIZE_T PeakPagedBytes;
    SIZE_T PeakNonPagedBytes;

} MI_VERIFIER_DRIVER_ENTRY, *PMI_VERIFIER_DRIVER_ENTRY;

typedef struct _MI_VERIFIER_POOL_HEADER {
    PVI_POOL_ENTRY VerifierPoolEntry;
} MI_VERIFIER_POOL_HEADER, *PMI_VERIFIER_POOL_HEADER;

typedef struct _MM_DRIVER_VERIFIER_DATA {
    ULONG Level;
    ULONG RaiseIrqls;
    ULONG AcquireSpinLocks;
    ULONG SynchronizeExecutions;

    ULONG AllocationsAttempted;
    ULONG AllocationsSucceeded;
    ULONG AllocationsSucceededSpecialPool;
    ULONG AllocationsWithNoTag;

    ULONG TrimRequests;
    ULONG Trims;
    ULONG AllocationsFailed;
    ULONG AllocationsFailedDeliberately;

    ULONG Loads;
    ULONG Unloads;
    ULONG UnTrackedPool;
    ULONG UserTrims;

    ULONG CurrentPagedPoolAllocations;
    ULONG CurrentNonPagedPoolAllocations;
    ULONG PeakPagedPoolAllocations;
    ULONG PeakNonPagedPoolAllocations;

    SIZE_T PagedBytes;
    SIZE_T NonPagedBytes;
    SIZE_T PeakPagedBytes;
    SIZE_T PeakNonPagedBytes;

    ULONG BurstAllocationsFailedDeliberately;
    ULONG SessionTrims;
    ULONG Reserved[2];

} MM_DRIVER_VERIFIER_DATA, *PMM_DRIVER_VERIFIER_DATA;

VOID
MiInitializeDriverVerifierList (
    VOID
    );

LOGICAL
MiInitializeVerifyingComponents (
    IN PLOADER_PARAMETER_BLOCK LoaderBlock
    );

LOGICAL
MiApplyDriverVerifier (
    IN PKLDR_DATA_TABLE_ENTRY,
    IN PMI_VERIFIER_DRIVER_ENTRY Verifier
    );

VOID
MiReApplyVerifierToLoadedModules(
    IN PLIST_ENTRY ModuleListHead
    );

VOID
MiVerifyingDriverUnloading (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

VOID
MiVerifierCheckThunks (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

extern ULONG MiActiveVerifierThunks;
extern LIST_ENTRY MiSuspectDriverList;

extern ULONG MiVerifierThunksAdded;

VOID
MiEnableKernelVerifier (
    VOID
    );

extern LOGICAL KernelVerifier;

extern MM_DRIVER_VERIFIER_DATA MmVerifierData;

#define MI_FREED_SPECIAL_POOL_SIGNATURE 0x98764321

#define MI_STACK_BYTES 1024

typedef struct _MI_FREED_SPECIAL_POOL {
    POOL_HEADER OverlaidPoolHeader;
    MI_VERIFIER_POOL_HEADER OverlaidVerifierPoolHeader;

    ULONG Signature;
    ULONG TickCount;
    ULONG NumberOfBytesRequested;
    ULONG Pagable;

    PVOID VirtualAddress;
    PVOID StackPointer;
    ULONG StackBytes;
    PETHREAD Thread;

    UCHAR StackData[MI_STACK_BYTES];
} MI_FREED_SPECIAL_POOL, *PMI_FREED_SPECIAL_POOL;

#define MM_DBG_COMMIT_NONPAGED_POOL_EXPANSION           0
#define MM_DBG_COMMIT_PAGED_POOL_PAGETABLE              1
#define MM_DBG_COMMIT_PAGED_POOL_PAGES                  2
#define MM_DBG_COMMIT_SESSION_POOL_PAGE_TABLES          3
#define MM_DBG_COMMIT_ALLOCVM1                          4
#define MM_DBG_COMMIT_ALLOCVM_SEGMENT                   5
#define MM_DBG_COMMIT_IMAGE                             6
#define MM_DBG_COMMIT_PAGEFILE_BACKED_SHMEM             7
#define MM_DBG_COMMIT_INDEPENDENT_PAGES                 8
#define MM_DBG_COMMIT_CONTIGUOUS_PAGES                  9
#define MM_DBG_COMMIT_MDL_PAGES                         0xA
#define MM_DBG_COMMIT_NONCACHED_PAGES                   0xB
#define MM_DBG_COMMIT_MAPVIEW_DATA                      0xC
#define MM_DBG_COMMIT_FILL_SYSTEM_DIRECTORY             0xD
#define MM_DBG_COMMIT_EXTRA_SYSTEM_PTES                 0xE
#define MM_DBG_COMMIT_DRIVER_PAGING_AT_INIT             0xF
#define MM_DBG_COMMIT_PAGEFILE_FULL                     0x10
#define MM_DBG_COMMIT_PROCESS_CREATE                    0x11
#define MM_DBG_COMMIT_KERNEL_STACK_CREATE               0x12
#define MM_DBG_COMMIT_SET_PROTECTION                    0x13
#define MM_DBG_COMMIT_SESSION_CREATE                    0x14
#define MM_DBG_COMMIT_SESSION_IMAGE_PAGES               0x15
#define MM_DBG_COMMIT_SESSION_PAGETABLE_PAGES           0x16
#define MM_DBG_COMMIT_SESSION_SHARED_IMAGE              0x17
#define MM_DBG_COMMIT_DRIVER_PAGES                      0x18
#define MM_DBG_COMMIT_INSERT_VAD                        0x19
#define MM_DBG_COMMIT_SESSION_WS_INIT                   0x1A
#define MM_DBG_COMMIT_SESSION_ADDITIONAL_WS_PAGES       0x1B
#define MM_DBG_COMMIT_SESSION_ADDITIONAL_WS_HASHPAGES   0x1C
#define MM_DBG_COMMIT_SPECIAL_POOL_PAGES                0x1D
#define MM_DBG_COMMIT_SPECIAL_POOL_MAPPING_PAGES        0x1E
#define MM_DBG_COMMIT_SMALL                             0x1F
#define MM_DBG_COMMIT_EXTRA_WS_PAGES                    0x20
#define MM_DBG_COMMIT_EXTRA_INITIAL_SESSION_WS_PAGES    0x21
#define MM_DBG_COMMIT_ALLOCVM_PROCESS                   0x22
#define MM_DBG_COMMIT_INSERT_VAD_PT                     0x23
#define MM_DBG_COMMIT_ALLOCVM_PROCESS2                  0x24
#define MM_DBG_COMMIT_CHARGE_NORMAL                     0x25
#define MM_DBG_COMMIT_CHARGE_CAUSE_POPUP                0x26
#define MM_DBG_COMMIT_CHARGE_CANT_EXPAND                0x27
#define MM_DBG_COMMIT_LARGE_VA_PAGES                    0x28
#define MM_DBG_COMMIT_LOAD_SYSTEM_IMAGE_TEMP            0x29

#define MM_DBG_COMMIT_RETURN_NONPAGED_POOL_EXPANSION    0x40
#define MM_DBG_COMMIT_RETURN_PAGED_POOL_PAGES           0x41
#define MM_DBG_COMMIT_RETURN_SESSION_DATAPAGE           0x42
#define MM_DBG_COMMIT_RETURN_ALLOCVM_SEGMENT            0x43
#define MM_DBG_COMMIT_RETURN_ALLOCVM2                   0x44

#define MM_DBG_COMMIT_RETURN_IMAGE_NO_LARGE_CA          0x46
#define MM_DBG_COMMIT_RETURN_PTE_RANGE                  0x47
#define MM_DBG_COMMIT_RETURN_NTFREEVM1                  0x48
#define MM_DBG_COMMIT_RETURN_NTFREEVM2                  0x49
#define MM_DBG_COMMIT_RETURN_INDEPENDENT_PAGES          0x4A
#define MM_DBG_COMMIT_RETURN_AWE_EXCESS                 0x4B
#define MM_DBG_COMMIT_RETURN_MDL_PAGES                  0x4C
#define MM_DBG_COMMIT_RETURN_NONCACHED_PAGES            0x4D
#define MM_DBG_COMMIT_RETURN_SESSION_CREATE_FAILURE     0x4E
#define MM_DBG_COMMIT_RETURN_PAGETABLES                 0x4F
#define MM_DBG_COMMIT_RETURN_PROTECTION                 0x50
#define MM_DBG_COMMIT_RETURN_SEGMENT_DELETE1            0x51
#define MM_DBG_COMMIT_RETURN_SEGMENT_DELETE2            0x52
#define MM_DBG_COMMIT_RETURN_PAGEFILE_FULL              0x53
#define MM_DBG_COMMIT_RETURN_SESSION_DEREFERENCE        0x54
#define MM_DBG_COMMIT_RETURN_VAD                        0x55
#define MM_DBG_COMMIT_RETURN_PROCESS_CREATE_FAILURE1    0x56
#define MM_DBG_COMMIT_RETURN_PROCESS_DELETE             0x57
#define MM_DBG_COMMIT_RETURN_PROCESS_CLEAN_PAGETABLES   0x58
#define MM_DBG_COMMIT_RETURN_KERNEL_STACK_DELETE        0x59
#define MM_DBG_COMMIT_RETURN_SESSION_DRIVER_LOAD_FAILURE1 0x5A
#define MM_DBG_COMMIT_RETURN_DRIVER_INIT_CODE           0x5B
#define MM_DBG_COMMIT_RETURN_DRIVER_UNLOAD              0x5C
#define MM_DBG_COMMIT_RETURN_DRIVER_UNLOAD1             0x5D
#define MM_DBG_COMMIT_RETURN_NORMAL                     0x5E
#define MM_DBG_COMMIT_RETURN_PF_FULL_EXTEND             0x5F
#define MM_DBG_COMMIT_RETURN_EXTENDED                   0x60
#define MM_DBG_COMMIT_RETURN_SEGMENT_DELETE3            0x61
#define MM_DBG_COMMIT_CHARGE_LARGE_PAGES                0x62
#define MM_DBG_COMMIT_RETURN_LARGE_PAGES                0x63

#if 0

#define MM_COMMIT_COUNTER_MAX 0x80

#define MM_TRACK_COMMIT(_index, bump) \
    if (_index >= MM_COMMIT_COUNTER_MAX) { \
        DbgPrint("Mm: Invalid commit counter %d %d\n", _index, MM_COMMIT_COUNTER_MAX); \
        DbgBreakPoint(); \
    } \
    else { \
        InterlockedExchangeAddSizeT (&MmTrackCommit[_index], bump); \
    }

#define MM_TRACK_COMMIT_REDUCTION(_index, bump) \
    if (_index >= MM_COMMIT_COUNTER_MAX) { \
        DbgPrint("Mm: Invalid commit counter %d %d\n", _index, MM_COMMIT_COUNTER_MAX); \
        DbgBreakPoint(); \
    } \
    else { \
        InterlockedExchangeAddSizeT (&MmTrackCommit[_index], 0 - (bump)); \
    }

extern SIZE_T MmTrackCommit[MM_COMMIT_COUNTER_MAX];

#define MI_INCREMENT_TOTAL_PROCESS_COMMIT(_charge) InterlockedExchangeAddSizeT (&MmTotalProcessCommit, (_charge));

#else

#define MM_TRACK_COMMIT(_index, bump)
#define MM_TRACK_COMMIT_REDUCTION(_index, bump)
#define MI_INCREMENT_TOTAL_PROCESS_COMMIT(_charge)

#endif

//
// Types of resident available page charges.
//

#define MM_RESAVAIL_ALLOCATE_ZERO_PAGE_CLUSTERS          0
#define MM_RESAVAIL_ALLOCATE_PAGETABLES_FOR_PAGED_POOL   1
#define MM_RESAVAIL_ALLOCATE_GROW_BSTORE                 2
#define MM_RESAVAIL_ALLOCATE_CONTIGUOUS                  3
#define MM_RESAVAIL_FREE_OUTPAGE_BSTORE                  4
#define MM_RESAVAIL_FREE_PAGE_DRIVER                     5
#define MM_RESAVAIL_ALLOCATE_CREATE_PROCESS              6
#define MM_RESAVAIL_FREE_DELETE_PROCESS                  7
#define MM_RESAVAIL_FREE_CLEAN_PROCESS2                  8
#define MM_RESAVAIL_ALLOCATE_CREATE_STACK                9

#define MM_RESAVAIL_FREE_DELETE_STACK                   10
#define MM_RESAVAIL_ALLOCATE_GROW_STACK                 11
#define MM_RESAVAIL_FREE_OUTPAGE_STACK                  12
#define MM_RESAVAIL_FREE_LOAD_SYSTEM_IMAGE_EXCESS       13
#define MM_RESAVAIL_ALLOCATE_LOAD_SYSTEM_IMAGE          14
#define MM_RESAVAIL_FREE_LOAD_SYSTEM_IMAGE1             15
#define MM_RESAVAIL_FREE_LOAD_SYSTEM_IMAGE2             16
#define MM_RESAVAIL_FREE_LOAD_SYSTEM_IMAGE3             17
#define MM_RESAVAIL_FREE_DRIVER_INITIALIZATION          18
#define MM_RESAVAIL_FREE_SET_DRIVER_PAGING              19

#define MM_RESAVAIL_FREE_CONTIGUOUS2                    20
#define MM_RESAVAIL_FREE_UNLOAD_SYSTEM_IMAGE1           21
#define MM_RESAVAIL_FREE_UNLOAD_SYSTEM_IMAGE            22
#define MM_RESAVAIL_FREE_EXPANSION_NONPAGED_POOL        23
#define MM_RESAVAIL_ALLOCATE_EXPANSION_NONPAGED_POOL    24
#define MM_RESAVAIL_ALLOCATE_LOCK_CODE1                 25
#define MM_RESAVAIL_ALLOCATE_LOCK_CODE3                 26
#define MM_RESAVAIL_ALLOCATEORFREE_WS_ADJUST            27
#define MM_RESAVAIL_ALLOCATE_INDEPENDENT                28
#define MM_RESAVAIL_ALLOCATE_LOCK_CODE2                 29

#define MM_RESAVAIL_FREE_INDEPENDENT                    30
#define MM_RESAVAIL_ALLOCATE_NONPAGED_SPECIAL_POOL      31
#define MM_RESAVAIL_FREE_CONTIGUOUS                     32
#define MM_RESAVAIL_ALLOCATE_SPECIAL_POOL_EXPANSION     33
#define MM_RESAVAIL_ALLOCATE_FOR_MDL                    34
#define MM_RESAVAIL_FREE_FROM_MDL                       35
#define MM_RESAVAIL_FREE_AWE                            36
#define MM_RESAVAIL_FREE_NONPAGED_SPECIAL_POOL          37
#define MM_RESAVAIL_FREE_FOR_MDL_EXCESS                 38
#define MM_RESAVAIL_ALLOCATE_HOTADD_PFNDB               39

#define MM_RESAVAIL_ALLOCATE_CREATE_SESSION             40
#define MM_RESAVAIL_FREE_CLEAN_PROCESS1                 41
#define MM_RESAVAIL_ALLOCATE_SINGLE_PFN                 42
#define MM_RESAVAIL_ALLOCATEORFREE_WS_ADJUST1           43
#define MM_RESAVAIL_ALLOCATE_SESSION_PAGE_TABLES        44
#define MM_RESAVAIL_ALLOCATE_SESSION_IMAGE              45
#define MM_RESAVAIL_ALLOCATE_BUILDMDL                   46
#define MM_RESAVAIL_FREE_BUILDMDL_EXCESS                47
#define MM_RESAVAIL_ALLOCATE_ADD_WS_PAGE                48
#define MM_RESAVAIL_FREE_CREATE_SESSION                 49

#define MM_RESAVAIL_ALLOCATE_INIT_SESSION_WS            50
#define MM_RESAVAIL_FREE_SESSION_PAGE_TABLE             51
#define MM_RESAVAIL_FREE_DEREFERENCE_SESSION            52
#define MM_RESAVAIL_FREE_DEREFERENCE_SESSION_PAGES      53
#define MM_RESAVAIL_ALLOCATEORFREE_WS_ADJUST2           54
#define MM_RESAVAIL_ALLOCATEORFREE_WS_ADJUST3           55
#define MM_RESAVAIL_FREE_DEREFERENCE_SESSION_WS         56
#define MM_RESAVAIL_FREE_LOAD_SESSION_IMAGE1            57
#define MM_RESAVAIL_ALLOCATE_USER_PAGE_TABLE            58
#define MM_RESAVAIL_FREE_USER_PAGE_TABLE                59

#define MM_RESAVAIL_FREE_HOTADD_MEMORY                  60
#define MM_RESAVAIL_ALLOCATE_HOTREMOVE_MEMORY           61
#define MM_RESAVAIL_FREE_HOTREMOVE_MEMORY1              62
#define MM_RESAVAIL_FREE_HOTREMOVE_FAILED               63
#define MM_RESAVAIL_FREE_HOTADD_ECC                     64
#define MM_RESAVAIL_ALLOCATE_COMPRESSION                65
#define MM_RESAVAIL_FREE_COMPRESSION                    66
#define MM_RESAVAIL_ALLOCATE_LARGE_PAGES                67
#define MM_RESAVAIL_FREE_LARGE_PAGES                    68
#define MM_RESAVAIL_ALLOCATE_LOAD_SYSTEM_IMAGE_TEMP     69

#define MM_RESAVAIL_ALLOCATE_WSLE_HASH                  70
#define MM_RESAVAIL_FREE_WSLE_HASH                      71
#define MM_RESAVAIL_FREE_CLEAN_PROCESS_WS               72
#define MM_RESAVAIL_FREE_SESSION_PAGE_TABLES_EXCESS     73

#define MM_BUMP_COUNTER_MAX 74

extern SIZE_T MmResTrack[MM_BUMP_COUNTER_MAX];

#define MI_INCREMENT_RESIDENT_AVAILABLE(bump, _index)                        \
    InterlockedExchangeAddSizeT (&MmResidentAvailablePages, (SIZE_T)(bump)); \
    ASSERT (_index < MM_BUMP_COUNTER_MAX);                                   \
    InterlockedExchangeAddSizeT (&MmResTrack[_index], (SIZE_T)(bump));

#define MI_DECREMENT_RESIDENT_AVAILABLE(bump, _index)                          \
    InterlockedExchangeAddSizeT (&MmResidentAvailablePages, 0-(SIZE_T)(bump)); \
    ASSERT (_index < MM_BUMP_COUNTER_MAX);                                     \
    InterlockedExchangeAddSizeT (&MmResTrack[_index], (SIZE_T)(bump));

//++
//PFN_NUMBER
//MI_NONPAGABLE_MEMORY_AVAILABLE(
//    VOID
//    );
//
// Routine Description:
//
//    This routine lets callers know how many pages can be charged against
//    the resident available, factoring in earlier Mm promises that
//    may not have been redeemed at this point (ie: nonpaged pool expansion,
//    etc, that must be honored at a later point if requested).
//
// Arguments
//
//    None.
//
// Return Value:
//
//    The number of currently available pages in the resident available.
//
//    N.B.  This is a signed quantity and can be negative.
//
//--
#define MI_NONPAGABLE_MEMORY_AVAILABLE()                                    \
        ((SPFN_NUMBER)                                                      \
            (MmResidentAvailablePages -                                     \
             MmSystemLockPagesCount))

extern ULONG MmLargePageMinimum;

//
// hack stuff for testing.
//

VOID
MiDumpValidAddresses (
    VOID
    );

VOID
MiDumpPfn ( VOID );

VOID
MiDumpWsl ( VOID );


VOID
MiFormatPte (
    IN PMMPTE PointerPte
    );

VOID
MiCheckPfn ( VOID );

VOID
MiCheckPte ( VOID );

VOID
MiFormatPfn (
    IN PMMPFN PointerPfn
    );




extern const MMPTE ZeroPte;

extern const MMPTE ZeroKernelPte;

extern const MMPTE ValidKernelPteLocal;

extern MMPTE ValidKernelPte;

extern MMPTE ValidKernelPde;

extern const MMPTE ValidKernelPdeLocal;

extern const MMPTE ValidUserPte;

extern const MMPTE ValidPtePte;

extern const MMPTE ValidPdePde;

extern MMPTE DemandZeroPde;

extern const MMPTE DemandZeroPte;

extern MMPTE KernelPrototypePte;

extern const MMPTE TransitionPde;

extern MMPTE PrototypePte;

extern const MMPTE NoAccessPte;

extern ULONG_PTR MmSubsectionBase;

extern ULONG_PTR MmSubsectionTopPage;

extern ULONG ExpMultiUserTS;

//
// Virtual alignment for PTEs (machine specific) minimum value is
// 4k maximum value is 64k.  The maximum value can be raised by
// changing the MM_PROTO_PTE_ALIGNMENT constant and adding more
// reserved mapping PTEs in hyperspace.
//

//
// Total number of physical pages on the system.
//

extern PFN_COUNT MmNumberOfPhysicalPages;

//
// Highest possible physical page number in the system.
//

extern PFN_NUMBER MmHighestPossiblePhysicalPage;

#if defined (_WIN64)

#define MI_DTC_MAX_PAGES ((PFN_NUMBER)(((ULONG64)512 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#define MI_DTC_BOOTED_3GB_MAX_PAGES     MI_DTC_MAX_PAGES

#define MI_ADS_MAX_PAGES ((PFN_NUMBER)(((ULONG64)64 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#define MI_DEFAULT_MAX_PAGES ((PFN_NUMBER)(((ULONG64)16 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#else

#define MI_DTC_MAX_PAGES ((PFN_NUMBER)(((ULONG64)128 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#define MI_DTC_BOOTED_3GB_MAX_PAGES ((PFN_NUMBER)(((ULONG64)16 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#define MI_ADS_MAX_PAGES ((PFN_NUMBER)(((ULONG64)32 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#define MI_DEFAULT_MAX_PAGES ((PFN_NUMBER)(((ULONG64)4 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

#endif

#define MI_BLADE_MAX_PAGES ((PFN_NUMBER)(((ULONG64)2 * 1024 * 1024 * 1024) >> PAGE_SHIFT))

extern RTL_BITMAP MiPfnBitMap;


FORCEINLINE
LOGICAL
MI_IS_PFN (
    IN PFN_NUMBER PageFrameIndex
    )

/*++

Routine Description:

    Check if a given address is backed by RAM or IO space.

Arguments:

    PageFrameIndex - Supplies a page frame number to check.

Return Value:

    TRUE - If the address is backed by RAM.

    FALSE - If the address is IO mapped memory.

Environment:

    Kernel mode.  PFN lock or dynamic memory mutex may be held.

--*/

{
    if (PageFrameIndex > MmHighestPossiblePhysicalPage) {
        return FALSE;
    }

    return MI_CHECK_BIT (MiPfnBitMap.Buffer, PageFrameIndex);
}

//
// Total number of available pages on the system.  This
// is the sum of the pages on the zeroed, free and standby lists.
//

extern PFN_NUMBER MmAvailablePages;

//
// Total number physical pages which would be usable if every process
// was at it's minimum working set size.  This value is initialized
// at system initialization to MmAvailablePages - MM_FLUID_PHYSICAL_PAGES.
// Everytime a thread is created, the kernel stack is subtracted from
// this and every time a process is created, the minimum working set
// is subtracted from this.  If the value would become negative, the
// operation (create process/kernel stack/ adjust working set) fails.
// The PFN LOCK must be owned to manipulate this value.
//

extern SPFN_NUMBER MmResidentAvailablePages;

//
// The total number of pages which would be removed from working sets
// if every working set was at its minimum.
//

extern PFN_NUMBER MmPagesAboveWsMinimum;

//
// If memory is becoming short and MmPagesAboveWsMinimum is
// greater than MmPagesAboveWsThreshold, trim working sets.
//

extern PFN_NUMBER MmPlentyFreePages;

extern PFN_NUMBER MmPagesAboveWsThreshold;

extern LONG MiDelayPageFaults;

extern PMMPFN MmPfnDatabase;

extern MMPFNLIST MmZeroedPageListHead;

extern MMPFNLIST MmFreePageListHead;

extern MMPFNLIST MmStandbyPageListHead;

extern MMPFNLIST MmRomPageListHead;

extern MMPFNLIST MmModifiedPageListHead;

extern MMPFNLIST MmModifiedNoWritePageListHead;

extern MMPFNLIST MmBadPageListHead;

extern PMMPFNLIST MmPageLocationList[NUMBER_OF_PAGE_LISTS];

extern MMPFNLIST MmModifiedPageListByColor[MM_MAXIMUM_NUMBER_OF_COLORS];

//
// Mask for isolating secondary color from physical page number.
//

extern ULONG MmSecondaryColorMask;

//
// Mask for isolating node color from combined node and secondary
// color.
//

extern ULONG MmSecondaryColorNodeMask;

//
// Width of MmSecondaryColorMask in bits.   In multi node systems,
// the node number is combined with the secondary color to make up
// the page color.
//

extern UCHAR MmSecondaryColorNodeShift;

//
// Events for available pages, set means pages are available.
//

extern KEVENT MmAvailablePagesEvent;

extern KEVENT MmAvailablePagesEventMedium;

extern KEVENT MmAvailablePagesEventHigh;

//
// Event for the zeroing page thread.
//

extern KEVENT MmZeroingPageEvent;

//
// Boolean to indicate if the zeroing page thread is currently
// active.  This is set to true when the zeroing page event is
// set and set to false when the zeroing page thread is done
// zeroing all the pages on the free list.
//

extern BOOLEAN MmZeroingPageThreadActive;

//
// Minimum number of free pages before zeroing page thread starts.
//

extern PFN_NUMBER MmMinimumFreePagesToZero;

//
// Global event to synchronize mapped writing with cleaning segments.
//

extern KEVENT MmMappedFileIoComplete;

//
// Hyper space items.
//

extern PMMPTE MmFirstReservedMappingPte;

extern PMMPTE MmLastReservedMappingPte;

//
// System space sizes - MmNonPagedSystemStart to MM_NON_PAGED_SYSTEM_END
// defines the ranges of PDEs which must be copied into a new process's
// address space.
//

extern PVOID MmNonPagedSystemStart;

extern PCHAR MmSystemSpaceViewStart;

extern LOGICAL MmProtectFreedNonPagedPool;

//
// Pool sizes.
//

extern SIZE_T MmSizeOfNonPagedPoolInBytes;

extern SIZE_T MmMinimumNonPagedPoolSize;

extern SIZE_T MmDefaultMaximumNonPagedPool;

extern ULONG MmMaximumNonPagedPoolPercent;

extern ULONG MmMinAdditionNonPagedPoolPerMb;

extern ULONG MmMaxAdditionNonPagedPoolPerMb;

extern SIZE_T MmSizeOfPagedPoolInBytes;
extern PFN_NUMBER MmSizeOfPagedPoolInPages;

extern SIZE_T MmMaximumNonPagedPoolInBytes;

extern PFN_NUMBER MmMaximumNonPagedPoolInPages;

extern PFN_NUMBER MmAllocatedNonPagedPool;

extern PVOID MmNonPagedPoolExpansionStart;

extern ULONG MmExpandedPoolBitPosition;

extern PFN_NUMBER MmNumberOfFreeNonPagedPool;

extern PFN_NUMBER MmNumberOfSystemPtes;

extern ULONG MiRequestedSystemPtes;

extern PMMPTE MmSystemPagePtes;

extern ULONG MmSystemPageDirectory[];

extern SIZE_T MmHeapSegmentReserve;

extern SIZE_T MmHeapSegmentCommit;

extern SIZE_T MmHeapDeCommitTotalFreeThreshold;

extern SIZE_T MmHeapDeCommitFreeBlockThreshold;

#define MI_MAX_FREE_LIST_HEADS  4

extern LIST_ENTRY MmNonPagedPoolFreeListHead[MI_MAX_FREE_LIST_HEADS];

//
// Counter for flushes of the entire TB.
//

extern ULONG MmFlushCounter;

//
// Pool start and end.
//

extern PVOID MmNonPagedPoolStart;

extern PVOID MmNonPagedPoolEnd;

extern PVOID MmPagedPoolStart;

extern PVOID MmPagedPoolEnd;

//
// Pool bit maps and other related structures.
//

typedef struct _MM_PAGED_POOL_INFO {

    PRTL_BITMAP PagedPoolAllocationMap;
    PRTL_BITMAP EndOfPagedPoolBitmap;
    PMMPTE FirstPteForPagedPool;
    PMMPTE LastPteForPagedPool;
    PMMPTE NextPdeForPagedPoolExpansion;
    ULONG PagedPoolHint;
    SIZE_T PagedPoolCommit;
    SIZE_T AllocatedPagedPool;

} MM_PAGED_POOL_INFO, *PMM_PAGED_POOL_INFO;

extern MM_PAGED_POOL_INFO MmPagedPoolInfo;

extern PVOID MmPageAlignedPoolBase[2];

extern PRTL_BITMAP VerifierLargePagedPoolMap;

//
// MmFirstFreeSystemPte contains the offset from the
// Nonpaged system base to the first free system PTE.
// Note, that an offset of zero indicates an empty list.
//

extern MMPTE MmFirstFreeSystemPte[MaximumPtePoolTypes];

extern ULONG_PTR MiSystemViewStart;

//
// System cache sizes.
//

extern PMMWSL MmSystemCacheWorkingSetList;

extern PMMWSLE MmSystemCacheWsle;

extern PVOID MmSystemCacheStart;

extern PVOID MmSystemCacheEnd;

extern PFN_NUMBER MmSystemCacheWsMinimum;

extern PFN_NUMBER MmSystemCacheWsMaximum;

//
// Virtual alignment for PTEs (machine specific) minimum value is
// 0 (no alignment) maximum value is 64k.  The maximum value can be raised by
// changing the MM_PROTO_PTE_ALIGNMENT constant and adding more
// reserved mapping PTEs in hyperspace.
//

extern ULONG MmAliasAlignment;

//
// Mask to AND with virtual address to get an offset to go
// with the alignment.  This value is page aligned.
//

extern ULONG MmAliasAlignmentOffset;

//
// Mask to and with PTEs to determine if the alias mapping is compatible.
// This value is usually (MmAliasAlignment - 1)
//

extern ULONG MmAliasAlignmentMask;

//
// Cells to track unused thread kernel stacks to avoid TB flushes
// every time a thread terminates.
//

extern ULONG MmMaximumDeadKernelStacks;
extern SLIST_HEADER MmDeadStackSListHead;

//
// MmSystemPteBase contains the address of 1 PTE before
// the first free system PTE (zero indicates an empty list).
// The value of this field does not change once set.
//

extern PMMPTE MmSystemPteBase;

//
// Root of system space virtual address descriptors.  These define
// the pagable portion of the system.
//

extern PMMVAD MmVirtualAddressDescriptorRoot;

extern MM_AVL_TABLE MmSectionBasedRoot;

extern PVOID MmHighSectionBase;

//
// Section commit mutex.
//

extern KGUARDED_MUTEX MmSectionCommitMutex;

//
// Section base address mutex.
//

extern KGUARDED_MUTEX MmSectionBasedMutex;

//
// Resource for section extension.
//

extern ERESOURCE MmSectionExtendResource;
extern ERESOURCE MmSectionExtendSetResource;

//
// Inpage cluster sizes for executable pages (set based on memory size).
//

extern ULONG MmDataClusterSize;

extern ULONG MmCodeClusterSize;

//
// Pagefile creation mutex.
//

extern KGUARDED_MUTEX MmPageFileCreationLock;

//
// Event to set when first paging file is created.
//

extern PKEVENT MmPagingFileCreated;

//
// Paging file debug information.
//

extern ULONG_PTR MmPagingFileDebug[];

//
// Fast mutex which guards the working set list for the system shared
// address space (paged pool, system cache, pagable drivers).
//

extern FAST_MUTEX MmSystemWsLock;

//
// Spin lock for allowing working set expansion.
//

extern KSPIN_LOCK MmExpansionLock;

//
// To prevent optimizations.
//

extern MMPTE GlobalPte;

//
// Page color for system working set.
//

extern ULONG MmSystemPageColor;

extern ULONG MmSecondaryColors;

extern ULONG MmProcessColorSeed;

//
// Set from ntos\config\CMDAT3.C  Used by customers to disable paging
// of executive on machines with lots of memory.  Worth a few TPS on a
// data base server.
//

#define MM_SYSTEM_CODE_LOCKED_DOWN 0x1
#define MM_PAGED_POOL_LOCKED_DOWN  0x2

extern ULONG MmDisablePagingExecutive;


//
// For debugging.


#if DBG
extern ULONG MmDebug;
#endif

//
// Unused segment management
//

extern MMDEREFERENCE_SEGMENT_HEADER MmDereferenceSegmentHeader;

extern LIST_ENTRY MmUnusedSegmentList;

extern LIST_ENTRY MmUnusedSubsectionList;

extern KEVENT MmUnusedSegmentCleanup;

extern ULONG MmConsumedPoolPercentage;

extern ULONG MmUnusedSegmentCount;

extern ULONG MmUnusedSubsectionCount;

extern ULONG MmUnusedSubsectionCountPeak;

extern SIZE_T MiUnusedSubsectionPagedPool;

extern SIZE_T MiUnusedSubsectionPagedPoolPeak;

#define MI_UNUSED_SUBSECTIONS_COUNT_INSERT(_MappedSubsection) \
        MmUnusedSubsectionCount += 1; \
        if (MmUnusedSubsectionCount > MmUnusedSubsectionCountPeak) { \
            MmUnusedSubsectionCountPeak = MmUnusedSubsectionCount; \
        } \
        MiUnusedSubsectionPagedPool += EX_REAL_POOL_USAGE((_MappedSubsection->PtesInSubsection + _MappedSubsection->UnusedPtes) * sizeof (MMPTE)); \
        if (MiUnusedSubsectionPagedPool > MiUnusedSubsectionPagedPoolPeak) { \
            MiUnusedSubsectionPagedPoolPeak = MiUnusedSubsectionPagedPool; \
        } \

#define MI_UNUSED_SUBSECTIONS_COUNT_REMOVE(_MappedSubsection) \
        MmUnusedSubsectionCount -= 1; \
        MiUnusedSubsectionPagedPool -= EX_REAL_POOL_USAGE((_MappedSubsection->PtesInSubsection + _MappedSubsection->UnusedPtes) * sizeof (MMPTE));

#define MI_FILESYSTEM_NONPAGED_POOL_CHARGE 150

#define MI_FILESYSTEM_PAGED_POOL_CHARGE 1024

//++
//LOGICAL
//MI_UNUSED_SEGMENTS_SURPLUS (
//    IN PVOID va
//    );
//
// Routine Description:
//
//    This routine determines whether a surplus of unused
//    segments exist.  If so, the caller can initiate a trim to free pool.
//
// Arguments
//
//    None.
//
// Return Value:
//
//    TRUE if unused segment trimming should be initiated, FALSE if not.
//
//--
#define MI_UNUSED_SEGMENTS_SURPLUS()                                    \
        (((ULONG)((MmPagedPoolInfo.AllocatedPagedPool * 100) / (MmSizeOfPagedPoolInBytes >> PAGE_SHIFT)) > MmConsumedPoolPercentage) || \
        ((ULONG)((MmAllocatedNonPagedPool * 100) / MmMaximumNonPagedPoolInPages) > MmConsumedPoolPercentage))

VOID
MiConvertStaticSubsections (
    IN PCONTROL_AREA ControlArea
    );

//++
//VOID
//MI_INSERT_UNUSED_SEGMENT (
//    IN PCONTROL_AREA _ControlArea
//    );
//
// Routine Description:
//
//    This routine inserts a control area into the unused segment list,
//    also managing the associated pool charges.
//
// Arguments
//
//    _ControlArea - Supplies the control area to obtain the pool charges from.
//
// Return Value:
//
//    None.
//
//--
#define MI_INSERT_UNUSED_SEGMENT(_ControlArea)                               \
        {                                                                    \
           MM_PFN_LOCK_ASSERT();                                             \
           if ((_ControlArea->u.Flags.Image == 0) &&                         \
               (_ControlArea->FilePointer != NULL) &&                        \
               (_ControlArea->u.Flags.PhysicalMemory == 0)) {                \
               MiConvertStaticSubsections(_ControlArea);                     \
           }                                                                 \
           InsertTailList (&MmUnusedSegmentList, &_ControlArea->DereferenceList); \
           MmUnusedSegmentCount += 1; \
        }

//++
//VOID
//MI_UNUSED_SEGMENTS_REMOVE_CHARGE (
//    IN PCONTROL_AREA _ControlArea
//    );
//
// Routine Description:
//
//    This routine manages pool charges during removals of segments from
//    the unused segment list.
//
// Arguments
//
//    _ControlArea - Supplies the control area to obtain the pool charges from.
//
// Return Value:
//
//    None.
//
//--
#define MI_UNUSED_SEGMENTS_REMOVE_CHARGE(_ControlArea)                       \
        {                                                                    \
           MM_PFN_LOCK_ASSERT();                                             \
           MmUnusedSegmentCount -= 1; \
        }

//
// List heads
//

extern MMWORKING_SET_EXPANSION_HEAD MmWorkingSetExpansionHead;

extern MMPAGE_FILE_EXPANSION MmAttemptForCantExtend;

//
// Paging files
//

extern MMMOD_WRITER_LISTHEAD MmPagingFileHeader;

extern MMMOD_WRITER_LISTHEAD MmMappedFileHeader;

extern PMMPAGING_FILE MmPagingFile[MAX_PAGE_FILES];

extern LIST_ENTRY MmFreePagingSpaceLow;

extern ULONG MmNumberOfActiveMdlEntries;

extern ULONG MmNumberOfPagingFiles;

extern KEVENT MmModifiedPageWriterEvent;

extern KEVENT MmCollidedFlushEvent;

extern KEVENT MmCollidedLockEvent;

// #define _MI_DEBUG_DATA 1         // Uncomment this for data logging

#if defined (_MI_DEBUG_DATA)

#define MI_DATA_BACKTRACE_LENGTH 8

typedef struct _MI_DATA_TRACES {

    PETHREAD Thread;
    PMMPFN Pfn;
    PMMPTE PointerPte;
    MMPFN PfnData;
    ULONG CallerId;
    ULONG DataInThePage[2];
    PVOID StackTrace [MI_DATA_BACKTRACE_LENGTH];

} MI_DATA_TRACES, *PMI_DATA_TRACES;

extern LONG MiDataIndex;

extern ULONG MiTrackData;

extern PMI_DATA_TRACES MiDataTraces;

VOID
FORCEINLINE
MiSnapData (
    IN PMMPFN Pfn,
    IN PMMPTE PointerPte,
    IN ULONG CallerId
    )
{
    KIRQL OldIrql;
    PVOID Va;
    PMI_DATA_TRACES Information;
    ULONG Index;
    ULONG Hash;
    PEPROCESS CurrentProcess;

    if (MiDataTraces == NULL) {
        return;
    }

    Index = InterlockedIncrement (&MiDataIndex);
    Index &= (MiTrackData - 1);
    Information = &MiDataTraces[Index];

    Information->Thread = PsGetCurrentThread ();
    Information->Pfn = Pfn;
    Information->PointerPte = PointerPte;
    Information->PfnData = *Pfn;
    Information->CallerId = CallerId;

    CurrentProcess = PsGetCurrentProcess ();
    Va = MiMapPageInHyperSpace (CurrentProcess, MI_PFN_ELEMENT_TO_INDEX (Pfn), &OldIrql);

    RtlCopyMemory (&Information->DataInThePage[0],
                   Va,
                   sizeof (Information->DataInThePage));

    MiUnmapPageInHyperSpace (CurrentProcess, Va, OldIrql);

    RtlZeroMemory (&Information->StackTrace[0], MI_DATA_BACKTRACE_LENGTH * sizeof(PVOID));                                                 \

#if defined (_WIN64)
    if (KeAreAllApcsDisabled () == TRUE) {
        Information->StackTrace[1] = (PVOID) _ReturnAddress ();
        Information->StackTrace[0] = MiGetInstructionPointer ();
    }
    else
#endif
    RtlCaptureStackBackTrace (0, MI_DATA_BACKTRACE_LENGTH, Information->StackTrace, &Hash);
}

#define MI_SNAP_DATA(_Pfn, _Pte, _CallerId) MiSnapData(_Pfn, _Pte, _CallerId)

#else
#define MI_SNAP_DATA(_Pfn, _Pte, _CallerId)
#endif


//
// Modified page writer.
//


VOID
FORCEINLINE
MiReleaseConfirmedPageFileSpace (
    IN MMPTE PteContents
    )

/*++

Routine Description:

    This routine frees the paging file allocated to the specified PTE.

Arguments:

    PteContents - Supplies the PTE which is in page file format.

Return Value:

    Returns TRUE if any paging file space was deallocated.

Environment:

    Kernel mode, APCs disabled, PFN lock held.

--*/

{
    ULONG FreeBit;
    ULONG PageFileNumber;
    PMMPAGING_FILE PageFile;

    MM_PFN_LOCK_ASSERT();

    ASSERT (PteContents.u.Soft.Prototype == 0);

    FreeBit = GET_PAGING_FILE_OFFSET (PteContents);

    ASSERT ((FreeBit != 0) && (FreeBit != MI_PTE_LOOKUP_NEEDED));

    PageFileNumber = GET_PAGING_FILE_NUMBER (PteContents);

    PageFile = MmPagingFile[PageFileNumber];

    ASSERT (RtlCheckBit( PageFile->Bitmap, FreeBit) == 1);

#if DBG
    if ((FreeBit < 8192) && (PageFileNumber == 0)) {
        ASSERT ((MmPagingFileDebug[FreeBit] & 1) != 0);
        MmPagingFileDebug[FreeBit] ^= 1;
    }
#endif

    MI_CLEAR_BIT (PageFile->Bitmap->Buffer, FreeBit);

    PageFile->FreeSpace += 1;
    PageFile->CurrentUsage -= 1;

    //
    // Check to see if we should move some MDL entries for the
    // modified page writer now that more free space is available.
    //

    if ((MmNumberOfActiveMdlEntries == 0) ||
        (PageFile->FreeSpace == MM_USABLE_PAGES_FREE)) {

        MiUpdateModifiedWriterMdls (PageFileNumber);
    }
}

extern PFN_NUMBER MmMinimumFreePages;

extern PFN_NUMBER MmFreeGoal;

extern PFN_NUMBER MmModifiedPageMaximum;

extern ULONG MmModifiedWriteClusterSize;

extern ULONG MmMinimumFreeDiskSpace;

extern ULONG MmPageFileExtension;

extern ULONG MmMinimumPageFileReduction;

extern LARGE_INTEGER MiModifiedPageLife;

extern BOOLEAN MiTimerPending;

extern KEVENT MiMappedPagesTooOldEvent;

extern KDPC MiModifiedPageWriterTimerDpc;

extern KTIMER MiModifiedPageWriterTimer;

//
// System process working set sizes.
//

extern PFN_NUMBER MmSystemProcessWorkingSetMin;

extern PFN_NUMBER MmSystemProcessWorkingSetMax;

extern PFN_NUMBER MmMinimumWorkingSetSize;

//
// Support for debugger's mapping physical memory.
//

extern PMMPTE MmDebugPte;

extern PMMPTE MmCrashDumpPte;

extern ULONG MiOverCommitCallCount;

//
// Event tracing routines
//

extern PPAGE_FAULT_NOTIFY_ROUTINE MmPageFaultNotifyRoutine;

extern SIZE_T MmSystemViewSize;

VOID
FASTCALL
MiIdentifyPfn (
    IN PMMPFN Pfn1,
    OUT PMMPFN_IDENTITY PfnIdentity
    );

#if defined (_WIN64)
#define InterlockedExchangeAddSizeT(a, b) InterlockedExchangeAdd64((PLONGLONG)a, b)
#else
#define InterlockedExchangeAddSizeT(a, b) InterlockedExchangeAdd((PLONG)(a), b)
#endif

//
// This is a special value loaded into an EPROCESS pointer to indicate that
// the action underway is for a Hydra session, not really the current process.
// Any value could have been used here that is not a valid system pointer
// or NULL - 1 was chosen because it simplifies checks for both NULL &
// HydraProcess by comparing for greater than HydraProcess.
//

#define HYDRA_PROCESS   ((PEPROCESS)1)

#define PREFETCH_PROCESS   ((PEPROCESS)2)

#define MI_SESSION_SPACE_STRUCT_SIZE MM_ALLOCATION_GRANULARITY

#if defined (_WIN64)

/*++

  Virtual memory layout of session space when loaded down from
  0x2000.0002.0000.0000 (IA64) or FFFF.F980.0000.0000 (AMD64) :

  Note that the sizes of mapped views, paged pool & images are registry tunable.

                        +------------------------------------+
    2000.0002.0000.0000 |                                    |
                        |   win32k.sys & video drivers       |
                        |             (16MB)                 |
                        |                                    |
                        +------------------------------------+
    2000.0001.FF00.0000 |                                    |
                        |   MM_SESSION_SPACE & Session WSLs  |
                        |              (16MB)                |
                        |                                    |
    2000.0001.FEFF.0000 +------------------------------------+
                        |                                    |
                        |              ...                   |
                        |                                    |
                        +------------------------------------+
    2000.0001.FE80.0000 |                                    |
                        |   Mapped Views for this session    |
                        |              (104MB)               |
                        |                                    |
                        +------------------------------------+
    2000.0001.F800.0000 |                                    |
                        |   Paged Pool for this session      |
                        |              (64MB)                |
                        |                                    |
    2000.0001.F400.0000 +------------------------------------+
                        |   Special Pool for this session    |
                        |              (64MB)                |
                        |                                    |
    2000.0000.0000.0000 +------------------------------------+

--*/

#define MI_SESSION_SPACE_WS_SIZE  ((ULONG_PTR)(16*1024*1024) - MI_SESSION_SPACE_STRUCT_SIZE)

#define MI_SESSION_DEFAULT_IMAGE_SIZE     ((ULONG_PTR)(16*1024*1024))

#define MI_SESSION_DEFAULT_VIEW_SIZE      ((ULONG_PTR)(104*1024*1024))

#define MI_SESSION_DEFAULT_POOL_SIZE      ((ULONG_PTR)(64*1024*1024))

#define MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE (MM_VA_MAPPED_BY_PPE)

#else

/*++

  Virtual memory layout of session space when loaded down from 0xC0000000.

  Note that the sizes of mapped views, paged pool and images are registry
  tunable on 32-bit systems (if NOT booted /3GB, as 3GB has very limited
  address space).

                 +------------------------------------+
        C0000000 |                                    |
                 | win32k.sys, video drivers and any  |
                 | rebased NT4 printer drivers.       |
                 |                                    |
                 |             (8MB)                  |
                 |                                    |
                 +------------------------------------+
        BF800000 |                                    |
                 |   MM_SESSION_SPACE & Session WSLs  |
                 |              (4MB)                 |
                 |                                    |
                 +------------------------------------+
        BF400000 |                                    |
                 |   Mapped views for this session    |
                 |     (20MB by default, but is       |
                 |      registry configurable)        |
                 |                                    |
                 +------------------------------------+
        BE000000 |                                    |
                 |   Paged pool for this session      |
                 |     (16MB by default, but is       |
                 |      registry configurable)        |
                 |                                    |
        BD000000 +------------------------------------+

--*/

#define MI_SESSION_SPACE_WS_SIZE  (4*1024*1024 - MI_SESSION_SPACE_STRUCT_SIZE)

#define MI_SESSION_DEFAULT_IMAGE_SIZE      (8*1024*1024)

#define MI_SESSION_DEFAULT_VIEW_SIZE      (20*1024*1024)

#define MI_SESSION_DEFAULT_POOL_SIZE      (16*1024*1024)

#define MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE \
            (MM_SYSTEM_CACHE_END_EXTRA - MM_KSEG2_BASE)

#endif



#define MI_SESSION_SPACE_DEFAULT_TOTAL_SIZE \
            (MI_SESSION_DEFAULT_IMAGE_SIZE + \
             MI_SESSION_SPACE_STRUCT_SIZE + \
             MI_SESSION_SPACE_WS_SIZE + \
             MI_SESSION_DEFAULT_VIEW_SIZE + \
             MI_SESSION_DEFAULT_POOL_SIZE)

extern ULONG_PTR MmSessionBase;
extern PMMPTE MiSessionBasePte;
extern PMMPTE MiSessionLastPte;

extern ULONG_PTR MiSessionSpaceWs;

extern ULONG_PTR MiSessionViewStart;
extern SIZE_T MmSessionViewSize;

extern ULONG_PTR MiSessionImageStart;
extern ULONG_PTR MiSessionImageEnd;
extern SIZE_T MmSessionImageSize;

extern PMMPTE MiSessionImagePteStart;
extern PMMPTE MiSessionImagePteEnd;

extern ULONG_PTR MiSessionPoolStart;
extern ULONG_PTR MiSessionPoolEnd;
extern SIZE_T MmSessionPoolSize;

extern ULONG_PTR MiSessionSpaceEnd;

extern ULONG MiSessionSpacePageTables;

//
// The number of page table pages required to map all of session space.
//

#define MI_SESSION_SPACE_MAXIMUM_PAGE_TABLES \
            (MI_SESSION_SPACE_MAXIMUM_TOTAL_SIZE / MM_VA_MAPPED_BY_PDE)

extern SIZE_T MmSessionSize;        // size of the entire session space.

//
// Macros to determine if a given address lies in the specified session range.
//

#define MI_IS_SESSION_IMAGE_ADDRESS(VirtualAddress) \
        ((PVOID)(VirtualAddress) >= (PVOID)MiSessionImageStart && (PVOID)(VirtualAddress) < (PVOID)(MiSessionImageEnd))

#define MI_IS_SESSION_POOL_ADDRESS(VirtualAddress) \
        ((PVOID)(VirtualAddress) >= (PVOID)MiSessionPoolStart && (PVOID)(VirtualAddress) < (PVOID)MiSessionPoolEnd)

#define MI_IS_SESSION_ADDRESS(_VirtualAddress) \
        ((PVOID)(_VirtualAddress) >= (PVOID)MmSessionBase && (PVOID)(_VirtualAddress) < (PVOID)(MiSessionSpaceEnd))

#define MI_IS_SESSION_PTE(_Pte) \
        ((PMMPTE)(_Pte) >= MiSessionBasePte && (PMMPTE)(_Pte) < MiSessionLastPte)

#define MI_IS_SESSION_IMAGE_PTE(_Pte) \
        ((PMMPTE)(_Pte) >= MiSessionImagePteStart && (PMMPTE)(_Pte) < MiSessionImagePteEnd)

#define SESSION_GLOBAL(_Session)    (_Session->GlobalVirtualAddress)

#define MM_DBG_SESSION_INITIAL_PAGETABLE_ALLOC          0
#define MM_DBG_SESSION_INITIAL_PAGETABLE_FREE_RACE      1
#define MM_DBG_SESSION_INITIAL_PAGE_ALLOC               2
#define MM_DBG_SESSION_INITIAL_PAGE_FREE_FAIL1          3
#define MM_DBG_SESSION_INITIAL_PAGETABLE_FREE_FAIL1     4
#define MM_DBG_SESSION_WS_PAGE_FREE                     5
#define MM_DBG_SESSION_PAGETABLE_ALLOC                  6
#define MM_DBG_SESSION_SYSMAPPED_PAGES_ALLOC            7
#define MM_DBG_SESSION_WS_PAGETABLE_ALLOC               8
#define MM_DBG_SESSION_PAGEDPOOL_PAGETABLE_ALLOC        9
#define MM_DBG_SESSION_PAGEDPOOL_PAGETABLE_FREE_FAIL1   10
#define MM_DBG_SESSION_WS_PAGE_ALLOC                    11
#define MM_DBG_SESSION_WS_PAGE_ALLOC_GROWTH             12
#define MM_DBG_SESSION_INITIAL_PAGE_FREE                13
#define MM_DBG_SESSION_PAGETABLE_FREE                   14
#define MM_DBG_SESSION_PAGEDPOOL_PAGETABLE_ALLOC1       15
#define MM_DBG_SESSION_DRIVER_PAGES_LOCKED              16
#define MM_DBG_SESSION_DRIVER_PAGES_UNLOCKED            17
#define MM_DBG_SESSION_WS_HASHPAGE_ALLOC                18
#define MM_DBG_SESSION_SYSMAPPED_PAGES_COMMITTED        19

#define MM_DBG_SESSION_COMMIT_PAGEDPOOL_PAGES           30
#define MM_DBG_SESSION_COMMIT_DELETE_VM_RETURN          31
#define MM_DBG_SESSION_COMMIT_POOL_FREED                32
#define MM_DBG_SESSION_COMMIT_IMAGE_UNLOAD              33
#define MM_DBG_SESSION_COMMIT_IMAGELOAD_FAILED1         34
#define MM_DBG_SESSION_COMMIT_IMAGELOAD_FAILED2         35
#define MM_DBG_SESSION_COMMIT_IMAGELOAD_NOACCESS        36

#define MM_DBG_SESSION_NP_LOCK_CODE1                    38
#define MM_DBG_SESSION_NP_LOCK_CODE2                    39
#define MM_DBG_SESSION_NP_SESSION_CREATE                40
#define MM_DBG_SESSION_NP_PAGETABLE_ALLOC               41
#define MM_DBG_SESSION_NP_POOL_CREATE                   42
#define MM_DBG_SESSION_NP_COMMIT_IMAGE                  43
#define MM_DBG_SESSION_NP_COMMIT_IMAGE_PT               44
#define MM_DBG_SESSION_NP_INIT_WS                       45
#define MM_DBG_SESSION_NP_WS_GROW                       46
#define MM_DBG_SESSION_NP_HASH_GROW                     47

#define MM_DBG_SESSION_NP_PAGE_DRIVER                   48
#define MM_DBG_SESSION_NP_POOL_CREATE_FAILED            49
#define MM_DBG_SESSION_NP_WS_PAGE_FREE                  50
#define MM_DBG_SESSION_NP_SESSION_DESTROY               51
#define MM_DBG_SESSION_NP_SESSION_PTDESTROY             52
#define MM_DBG_SESSION_NP_DELVA                         53
#define MM_DBG_SESSION_NP_HASH_SHRINK                   54
#define MM_DBG_SESSION_WS_HASHPAGE_FREE                 55

#if DBG
#define MM_SESS_COUNTER_MAX 56

#define MM_BUMP_SESS_COUNTER(_index, bump) \
    if (_index >= MM_SESS_COUNTER_MAX) { \
        DbgPrint("Mm: Invalid bump counter %d %d\n", _index, MM_SESS_COUNTER_MAX); \
        DbgBreakPoint(); \
    } \
    MmSessionSpace->Debug[_index] += (bump);

typedef struct _MM_SESSION_MEMORY_COUNTERS {
    SIZE_T NonPagablePages;
    SIZE_T CommittedPages;
} MM_SESSION_MEMORY_COUNTERS, *PMM_SESSION_MEMORY_COUNTERS;

#define MM_SESS_MEMORY_COUNTER_MAX  8

#define MM_SNAP_SESS_MEMORY_COUNTERS(_index) \
    if (_index >= MM_SESS_MEMORY_COUNTER_MAX) { \
        DbgPrint("Mm: Invalid session mem counter %d %d\n", _index, MM_SESS_MEMORY_COUNTER_MAX); \
        DbgBreakPoint(); \
    } \
    else { \
        MmSessionSpace->Debug2[_index].NonPagablePages = MmSessionSpace->NonPagablePages; \
        MmSessionSpace->Debug2[_index].CommittedPages = MmSessionSpace->CommittedPages; \
    }

#else
#define MM_BUMP_SESS_COUNTER(_index, bump)
#define MM_SNAP_SESS_MEMORY_COUNTERS(_index)
#endif


#define MM_SESSION_FAILURE_NO_IDS                   0
#define MM_SESSION_FAILURE_NO_COMMIT                1
#define MM_SESSION_FAILURE_NO_RESIDENT              2
#define MM_SESSION_FAILURE_RACE_DETECTED            3
#define MM_SESSION_FAILURE_NO_SYSPTES               4
#define MM_SESSION_FAILURE_NO_PAGED_POOL            5
#define MM_SESSION_FAILURE_NO_NONPAGED_POOL         6
#define MM_SESSION_FAILURE_NO_IMAGE_VA_SPACE        7
#define MM_SESSION_FAILURE_NO_SESSION_PAGED_POOL    8
#define MM_SESSION_FAILURE_NO_AVAILABLE             9
#define MM_SESSION_FAILURE_IMAGE_ZOMBIE             10

#define MM_SESSION_FAILURE_CAUSES                   11

ULONG MmSessionFailureCauses[MM_SESSION_FAILURE_CAUSES];

#define MM_BUMP_SESSION_FAILURES(_index) MmSessionFailureCauses[_index] += 1;

typedef struct _MM_SESSION_SPACE_FLAGS {
    ULONG Initialized : 1;
    ULONG DeletePending : 1;
    ULONG Filler : 30;
} MM_SESSION_SPACE_FLAGS;

//
// The value of SESSION_POOL_SMALL_LISTS is very carefully chosen for each
// architecture to avoid spilling over into an additional session data page.
//

#if defined(_AMD64_)
#define SESSION_POOL_SMALL_LISTS        21
#elif defined(_IA64_)
#define SESSION_POOL_SMALL_LISTS        53
#elif defined(_X86_)
#define SESSION_POOL_SMALL_LISTS        26
#else
#error "no target architecture"
#endif

//
// The session space data structure - allocated per session and only visible at
// MM_SESSION_SPACE_BASE when in the context of a process from the session.
// This virtual address space is rotated at context switch time when switching
// from a process in session A to a process in session B.  This rotation is
// useful for things like providing paged pool per session so many sessions
// won't exhaust the VA space which backs the system global pool.
//
// A kernel PTE is also allocated to double map this page so that global
// pointers can be maintained to provide system access from any process context.
// This is needed for things like mutexes and WSL chains.
//

typedef struct _MM_SESSION_SPACE {

    //
    // This is a pointer in global system address space, used to make various
    // fields that can be referenced from any process visible from any process
    // context.  This is for things like mutexes, WSL chains, etc.
    //

    struct _MM_SESSION_SPACE *GlobalVirtualAddress;

    ULONG ReferenceCount;

    union {
        ULONG LongFlags;
        MM_SESSION_SPACE_FLAGS Flags;
    } u;

    ULONG SessionId;

    //
    // This is the list of the processes in this group that have
    // session space entries.
    //

    LIST_ENTRY ProcessList;

    LARGE_INTEGER LastProcessSwappedOutTime;

    //
    // All the page tables for session space use this as their parent.
    // Note that it's not really a page directory - it's really a page
    // table page itself (the one used to map this very structure).
    //
    // This provides a reference to something that won't go away and
    // is relevant regardless of which process within the session is current.
    //

    PFN_NUMBER SessionPageDirectoryIndex;

    //
    // This is the count of non paged allocations to support this session
    // space.  This includes the session structure page table and data pages,
    // WSL page table and data pages, session pool page table pages and session
    // image page table pages.  These are all charged against
    // MmResidentAvailable.
    //

    SIZE_T NonPagablePages;

    //
    // This is the count of pages in this session that have been charged against
    // the systemwide commit.  This includes all the NonPagablePages plus the
    // data pages they typically map.
    //

    SIZE_T CommittedPages;

    //
    // Start of session paged pool virtual space.
    //

    PVOID PagedPoolStart;

    //
    // Current end of pool virtual space. Can be extended to the
    // end of the session space.
    //

    PVOID PagedPoolEnd;

    //
    // PTE pointers for pool.
    //

    PMMPTE PagedPoolBasePde;

    ULONG Color;

    ULONG ProcessOutSwapCount;

    ULONG SessionPoolAllocationFailures[4];

    //
    // This is the list of system images currently valid in
    // this session space.  This information is in addition
    // to the module global information in PsLoadedModuleList.
    //

    LIST_ENTRY ImageList;

    LCID LocaleId;

    //
    // The count of "known attachers and the associated event.
    //

    ULONG AttachCount;

    KEVENT AttachEvent;

    PEPROCESS LastProcess;

    //
    // This is generally decremented in process delete (not clean) so that
    // the session data page and mapping PTE can finally be freed when this
    // reaches zero.  smss is the only process that decrements it in other
    // places as smss never exits.
    //

    LONG ProcessReferenceToSession;

    //
    // This chain is in global system addresses (not session VAs) and can
    // be walked from any system context, ie: for WSL trimming.
    //

    LIST_ENTRY WsListEntry;

    //
    // Session lookasides for fast pool allocation/freeing.
    //

    GENERAL_LOOKASIDE Lookaside[SESSION_POOL_SMALL_LISTS];

    //
    // Support for mapping system views into session space.  Each desktop
    // allocates a 3MB heap and the global system view space is only 48M
    // total.  This would limit us to only 20-30 users - rotating the
    // system view space with each session removes this limitation.
    //

    MMSESSION Session;

    //
    // Session space paged pool support.
    //

    KGUARDED_MUTEX PagedPoolMutex;

    MM_PAGED_POOL_INFO PagedPoolInfo;

    //
    // Working set information.
    //

    MMSUPPORT  Vm;
    PMMWSLE    Wsle;

    PDRIVER_UNLOAD Win32KDriverUnload;

    //
    // Pool descriptor for less than 1 page allocations.
    //

    POOL_DESCRIPTOR PagedPool;

#if (_MI_PAGING_LEVELS >= 3)

    //
    // The page directory that maps session space is saved here so
    // trimmers can attach.
    //

    MMPTE PageDirectory;

#else

    //
    // The second level page tables that map session space are shared
    // by all processes in the session.
    //

    PMMPTE PageTables;

#endif

#if defined (_WIN64)

    //
    // NT64 has enough virtual address space to support per-session special
    // pool.
    //

    PMMPTE SpecialPoolFirstPte;
    PMMPTE SpecialPoolLastPte;
    PMMPTE NextPdeForSpecialPoolExpansion;
    PMMPTE LastPdeForSpecialPoolExpansion;
    PFN_NUMBER SpecialPagesInUse;
#endif

#if defined(_IA64_)
    REGION_MAP_INFO SessionMapInfo;
    PFN_NUMBER PageDirectoryParentPage;
#endif

    LONG ImageLoadingCount;

#if DBG
    ULONG Debug[MM_SESS_COUNTER_MAX];

    MM_SESSION_MEMORY_COUNTERS Debug2[MM_SESS_MEMORY_COUNTER_MAX];
#endif

} MM_SESSION_SPACE, *PMM_SESSION_SPACE;

extern PMM_SESSION_SPACE MmSessionSpace;

extern ULONG MiSessionCount;

//
// This flushes just the non-global TB entries.
//

#define MI_FLUSH_SESSION_TB() KeFlushProcessTb (TRUE);

//
// The default number of pages for the session working set minimum & maximum.
//

#define MI_SESSION_SPACE_WORKING_SET_MINIMUM 20

#define MI_SESSION_SPACE_WORKING_SET_MAXIMUM 384

NTSTATUS
MiSessionInsertImage (
    IN PVOID BaseAddress
    );

NTSTATUS
MiSessionCommitPageTables (
    PVOID StartVa,
    PVOID EndVa
    );

NTSTATUS
MiInitializeAndChargePfn (
    OUT PPFN_NUMBER PageFrameIndex,
    IN PMMPTE PointerPde,
    IN PFN_NUMBER ContainingPageFrame,
    IN LOGICAL SessionAllocation
    );

VOID
MiSessionPageTableRelease (
    IN PFN_NUMBER PageFrameIndex
    );

NTSTATUS
MiInitializeSessionPool (
    VOID
    );

VOID
MiCheckSessionPoolAllocations (
    VOID
    );

VOID
MiFreeSessionPoolBitMaps (
    VOID
    );

VOID
MiDetachSession (
    VOID
    );

VOID
MiAttachSession (
    IN PMM_SESSION_SPACE SessionGlobal
    );

VOID
MiReleaseProcessReferenceToSessionDataPage (
    PMM_SESSION_SPACE SessionGlobal
    );

extern PMMPTE MiHighestUserPte;
extern PMMPTE MiHighestUserPde;
#if (_MI_PAGING_LEVELS >= 4)
extern PMMPTE MiHighestUserPpe;
extern PMMPTE MiHighestUserPxe;
#endif

NTSTATUS
MiEmptyWorkingSet (
    IN PMMSUPPORT WsInfo,
    IN LOGICAL NeedLock
    );

//++
//ULONG
//MiGetPdeSessionIndex (
//    IN PVOID va
//    );
//
// Routine Description:
//
//    MiGetPdeSessionIndex returns the session structure index for the PDE
//    will (or does) map the given virtual address.
//
// Arguments
//
//    Va - Supplies the virtual address to locate the PDE index for.
//
// Return Value:
//
//    The index of the PDE entry.
//
//--

#define MiGetPdeSessionIndex(va)  ((ULONG)(((ULONG_PTR)(va) - (ULONG_PTR)MmSessionBase) >> PDI_SHIFT))

//
// Session space contains the image loader and tracker, virtual
// address allocator, paged pool allocator, system view image mappings,
// and working set for kernel mode virtual addresses that are instanced
// for groups of processes in a Session process group. This
// process group is identified by a SessionId.
//
// Each Session process group's loaded kernel modules, paged pool
// allocations, working set, and mapped system views are separate from
// other Session process groups, even though they have the same
// virtual addresses.
//
// This is to support the Hydra multi-user Windows NT system by
// replicating WIN32K.SYS, and its complement of video and printer drivers,
// desktop heaps, memory allocations, etc.
//

//
// Structure linked into a session space structure to describe
// which system images in PsLoadedModuleTable and
// SESSION_DRIVER_GLOBAL_LOAD_ADDRESS's
// have been allocated for the current session space.
//
// The reference count tracks the number of loads of this image within
// this session.
//

typedef struct _SESSION_GLOBAL_SUBSECTION_INFO {
    ULONG_PTR PteIndex;
    ULONG PteCount;
    ULONG Protection;
} SESSION_GLOBAL_SUBSECTION_INFO, *PSESSION_GLOBAL_SUBSECTION_INFO;

typedef struct _IMAGE_ENTRY_IN_SESSION {
    LIST_ENTRY Link;
    PVOID Address;
    PVOID LastAddress;
    ULONG ImageCountInThisSession;
    LOGICAL ImageLoading;  // Mods to this field protected by system load mutant
    PMMPTE PrototypePtes;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    PSESSION_GLOBAL_SUBSECTION_INFO GlobalSubs;
} IMAGE_ENTRY_IN_SESSION, *PIMAGE_ENTRY_IN_SESSION;

extern LIST_ENTRY MiSessionWsList;

NTSTATUS
FASTCALL
MiCheckPdeForSessionSpace(
    IN PVOID VirtualAddress
    );

NTSTATUS
MiShareSessionImage (
    IN PVOID BaseAddress,
    IN PSECTION Section
    );

VOID
MiSessionWideInitializeAddresses (
    VOID
    );

NTSTATUS
MiSessionWideReserveImageAddress (
    IN PSECTION Section,
    OUT PVOID *AssignedAddress,
    OUT PSECTION *NewSectionPointer
    );

VOID
MiInitializeSessionIds (
    VOID
    );

VOID
MiInitializeSessionWsSupport(
    VOID
    );

VOID
MiSessionAddProcess (
    IN PEPROCESS NewProcess
    );

VOID
MiSessionRemoveProcess (
    VOID
    );

VOID
MiRemoveImageSessionWide (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry OPTIONAL,
    IN PVOID BaseAddress,
    IN ULONG_PTR NumberOfBytes
    );

NTSTATUS
MiDeleteSessionVirtualAddresses(
    IN PVOID VirtualAddress,
    IN SIZE_T NumberOfBytes
    );

NTSTATUS
MiUnloadSessionImageByForce (
    IN SIZE_T NumberOfPtes,
    IN PVOID ImageBase
    );

PIMAGE_ENTRY_IN_SESSION
MiSessionLookupImage (
    IN PVOID BaseAddress
    );

VOID
MiSessionUnloadAllImages (
    VOID
    );

VOID
MiFreeSessionSpaceMap (
    VOID
    );

NTSTATUS
MiSessionInitializeWorkingSetList (
    VOID
    );

VOID
MiSessionUnlinkWorkingSet (
    VOID
    );

VOID
MiSessionOutSwapProcess (
    IN PEPROCESS Process
    );

VOID
MiSessionInSwapProcess (
    IN PEPROCESS Process
    );

#if !defined (_X86PAE_)

#define MI_GET_DIRECTORY_FRAME_FROM_PROCESS(_Process) \
        MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)(&((_Process)->Pcb.DirectoryTableBase[0])))

#define MI_GET_HYPER_PAGE_TABLE_FRAME_FROM_PROCESS(_Process) \
        MI_GET_PAGE_FRAME_FROM_PTE((PMMPTE)(&((_Process)->Pcb.DirectoryTableBase[1])))

#else

#define MI_GET_DIRECTORY_FRAME_FROM_PROCESS(_Process) \
        (MI_GET_PAGE_FRAME_FROM_PTE(((PMMPTE)((_Process)->PaeTop)) + PD_PER_SYSTEM - 1))

#define MI_GET_HYPER_PAGE_TABLE_FRAME_FROM_PROCESS(_Process) \
        ((PFN_NUMBER)((_Process)->Pcb.DirectoryTableBase[1]))

#endif

#if defined(_MIALT4K_)
NTSTATUS
MiSetCopyPagesFor4kPage (
    IN PEPROCESS Process,
    IN PMMVAD Vad,
    IN OUT PVOID StartingAddress,
    IN OUT PVOID EndingAddress,
    IN ULONG ProtectionMask,
    OUT PMMVAD *CallerNewVad
    );

VOID
MiRemoveAliasedVads (
    IN PEPROCESS Process,
    IN PMMVAD Vad
    );

PVOID
MiDuplicateAliasVadList (
    IN PMMVAD Vad
    );
#endif

//
// The LDR_DATA_TABLE_ENTRY->LoadedImports is used as a list of imported DLLs.
//
// This field is zero if the module was loaded at boot time and the
// import information was never filled in.
//
// This field is -1 if no imports are defined by the module.
//
// This field contains a valid paged pool PLDR_DATA_TABLE_ENTRY pointer
// with a low-order (bit 0) tag of 1 if there is only 1 usable import needed
// by this driver.
//
// This field will contain a valid paged pool PLOAD_IMPORTS pointer in all
// other cases (ie: where at least 2 imports exist).
//

typedef struct _LOAD_IMPORTS {
    SIZE_T                  Count;
    PKLDR_DATA_TABLE_ENTRY   Entry[1];
} LOAD_IMPORTS, *PLOAD_IMPORTS;

#define LOADED_AT_BOOT  ((PLOAD_IMPORTS)0)
#define NO_IMPORTS_USED ((PLOAD_IMPORTS)-2)

#define SINGLE_ENTRY(ImportVoid)    ((ULONG)((ULONG_PTR)(ImportVoid) & 0x1))

#define SINGLE_ENTRY_TO_POINTER(ImportVoid)    ((PKLDR_DATA_TABLE_ENTRY)((ULONG_PTR)(ImportVoid) & ~0x1))

#define POINTER_TO_SINGLE_ENTRY(Pointer)    ((PKLDR_DATA_TABLE_ENTRY)((ULONG_PTR)(Pointer) | 0x1))

// #define _MI_DEBUG_RONLY 1     // Uncomment this for session readonly tracking

#if _MI_DEBUG_RONLY

VOID
MiAssertNotSessionData (
    IN PMMPTE PointerPte
    );

VOID
MiLogSessionDataStart (
    IN PKLDR_DATA_TABLE_ENTRY DataTableEntry
    );

#define MI_ASSERT_NOT_SESSION_DATA(PTE) MiAssertNotSessionData(PTE)
#define MI_LOG_SESSION_DATA_START(DataTableEntry) MiLogSessionDataStart(DataTableEntry)
#else
#define MI_ASSERT_NOT_SESSION_DATA(PTE)
#define MI_LOG_SESSION_DATA_START(DataTableEntry)
#endif

//
// This tracks driver-specified individual verifier thunks.
//

typedef struct _DRIVER_SPECIFIED_VERIFIER_THUNKS {
    LIST_ENTRY ListEntry;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry;
    ULONG NumberOfThunks;
} DRIVER_SPECIFIED_VERIFIER_THUNKS, *PDRIVER_SPECIFIED_VERIFIER_THUNKS;

// #define _MI_DEBUG_SUB 1         // Uncomment this for subsection logging

#if defined (_MI_DEBUG_SUB)

extern ULONG MiTrackSubs;

#define MI_SUB_BACKTRACE_LENGTH 8

typedef struct _MI_SUB_TRACES {

    PETHREAD Thread;
    PMSUBSECTION Subsection;
    PCONTROL_AREA ControlArea;
    ULONG_PTR CallerId;
    PVOID StackTrace [MI_SUB_BACKTRACE_LENGTH];

    MSUBSECTION SubsectionContents;
    CONTROL_AREA ControlAreaContents;

} MI_SUB_TRACES, *PMI_SUB_TRACES;

extern LONG MiSubsectionIndex;

extern PMI_SUB_TRACES MiSubsectionTraces;

VOID
FORCEINLINE
MiSnapSubsection (
    IN PMSUBSECTION Subsection,
    IN ULONG CallerId
    )
{
    PMI_SUB_TRACES Information;
    PCONTROL_AREA ControlArea;
    ULONG Index;
    ULONG Hash;

    if (MiSubsectionTraces == NULL) {
        return;
    }

    ControlArea = Subsection->ControlArea;

    Index = InterlockedIncrement (&MiSubsectionIndex);
    Index &= (MiTrackSubs - 1);
    Information = &MiSubsectionTraces[Index];

    Information->Subsection = Subsection;
    Information->ControlArea = ControlArea;
    *(PMSUBSECTION)&Information->SubsectionContents = *Subsection;
    *(PCONTROL_AREA)&Information->ControlAreaContents = *ControlArea;
    Information->Thread = PsGetCurrentThread();
    Information->CallerId = CallerId;

#if defined (_WIN64)
    if (KeAreAllApcsDisabled () == TRUE) {
        Information->StackTrace[1] = (PVOID) _ReturnAddress ();
        Information->StackTrace[0] = MiGetInstructionPointer ();
    }
    else
#endif
    RtlCaptureStackBackTrace (0, MI_SUB_BACKTRACE_LENGTH, Information->StackTrace, &Hash);
}

#define MI_SNAP_SUB(_Sub, callerid) MiSnapSubsection(_Sub, callerid)

#else
#define MI_SNAP_SUB(_Sub, callerid)
#endif

//
//  Hot-patching private definitions
//

extern LIST_ENTRY MiHotPatchList;

#endif  // MI
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\mmquota.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   mmquota.c

Abstract:

    This module contains the routines which implement the quota and
    commitment charging for memory management.

Author:

    Lou Perazzoli (loup) 12-December-89
    Landy Wang (landyw) 02-Jun-1997

Revision History:

--*/

#include "mi.h"

#define MM_MINIMAL_COMMIT_INCREASE 512

SIZE_T MmPeakCommitment;

LONG MiCommitPopups[2];

extern ULONG_PTR MmAllocatedPagedPool;

#ifdef ALLOC_PRAGMA
#pragma alloc_text(INIT,MiInitializeCommitment)
#pragma alloc_text(PAGE,MiCalculatePageCommitment)
#pragma alloc_text(PAGE,MiReturnPageTablePageCommitment)
#endif

SIZE_T MmSystemCommitReserve = (5 * 1024 * 1024) / PAGE_SIZE;

VOID
MiInitializeCommitment (
    VOID
    )
{
    if (MmNumberOfPhysicalPages < (33 * 1024 * 1024) / PAGE_SIZE) {
        MmSystemCommitReserve = (1 * 1024 * 1024) / PAGE_SIZE;
    }

#if defined (_MI_DEBUG_COMMIT_LEAKS)
    MiCommitTraces = ExAllocatePoolWithTag (NonPagedPool,
                           MI_COMMIT_TRACE_MAX * sizeof (MI_COMMIT_TRACES),
                           'tCmM');
#endif
}

LOGICAL
FASTCALL
MiChargeCommitment (
    IN SIZE_T QuotaCharge,
    IN PEPROCESS Process OPTIONAL
    )

/*++

Routine Description:

    This routine checks to ensure the system has sufficient page file
    space remaining.

    Since this routine is generally used to charge commitment on behalf of
    usermode or other optional actions, this routine does not allow the
    caller to use up the very last morsels of commit on the premise that
    the operating system and drivers can put those to better use than any
    application in order to prevent the appearance of system hangs.

Arguments:

    QuotaCharge - Supplies the quota amount to charge.

    Process - Optionally supplies the current process IF AND ONLY IF
              the working set mutex is held.  If the paging file
              is being extended, the working set mutex is released if
              this is non-null.

Return Value:

    TRUE if there is sufficient space, FALSE if not.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock and AddressCreation mutexes
    held.

--*/

{
    SIZE_T OldCommitValue;
    SIZE_T NewCommitValue;
    SIZE_T CommitLimit;
    MMPAGE_FILE_EXPANSION PageExtend;
    LOGICAL WsHeldSafe;

    ASSERT ((SSIZE_T)QuotaCharge > 0);

#if DBG
    if (InitializationPhase > 1) {
        ULONG i;
        PKTHREAD Thread;

        Thread = KeGetCurrentThread ();
        for (i = 0; i < (ULONG)KeNumberProcessors; i += 1) {
            if (KiProcessorBlock[i]->IdleThread == Thread) {
                DbgPrint ("MMQUOTA: %x %p\n", i, Thread);
                DbgBreakPoint ();
            }
        }
    }
#endif

    //
    // Initializing WsHeldSafe is not needed for correctness, but without it
    // the compiler cannot compile this code W4 to check for use of
    // uninitialized variables.
    //

    WsHeldSafe = FALSE;

    do {

        OldCommitValue = MmTotalCommittedPages;

        NewCommitValue = OldCommitValue + QuotaCharge;

        while (NewCommitValue + MmSystemCommitReserve > MmTotalCommitLimit) {

            //
            // If the pagefiles are already at the maximum, then don't
            // bother trying to extend them, but do trim the cache.
            //

            if (MmTotalCommitLimit + 100 >= MmTotalCommitLimitMaximum) {

                MiChargeCommitmentFailures[1] += 1;

                MiTrimSegmentCache ();

                if (MmTotalCommitLimit >= MmTotalCommitLimitMaximum) {
                    MiCauseOverCommitPopup ();
                    return FALSE;
                }
            }

            if (Process != NULL) {

                //
                // The working set lock may have been acquired safely or
                // unsafely by our caller.  Handle both cases here and below.
                //

                UNLOCK_WS_REGARDLESS(Process, WsHeldSafe);
            }

            //
            // Queue a message to the segment dereferencing / pagefile extending
            // thread to see if the page file can be extended.  This is done
            // in the context of a system thread due to mutexes which may
            // currently be held.
            //

            PageExtend.InProgress = 1;
            PageExtend.ActualExpansion = 0;
            PageExtend.RequestedExpansionSize = QuotaCharge;
            PageExtend.Segment = NULL;
            PageExtend.PageFileNumber = MI_EXTEND_ANY_PAGEFILE;
            KeInitializeEvent (&PageExtend.Event, NotificationEvent, FALSE);

            if ((MiIssuePageExtendRequest (&PageExtend) == FALSE) ||
                (PageExtend.ActualExpansion == 0)) {

                MiCauseOverCommitPopup ();

                MiChargeCommitmentFailures[0] += 1;

                if (Process != NULL) {
                    LOCK_WS_REGARDLESS(Process, WsHeldSafe);
                }

                return FALSE;
            }

            if (Process != NULL) {
                LOCK_WS_REGARDLESS(Process, WsHeldSafe);
            }

            OldCommitValue = MmTotalCommittedPages;

            NewCommitValue = OldCommitValue + QuotaCharge;
        }

#if defined(_WIN64)
        NewCommitValue = InterlockedCompareExchange64 (
                                (PLONGLONG) &MmTotalCommittedPages,
                                (LONGLONG)  NewCommitValue,
                                (LONGLONG)  OldCommitValue);
#else
        NewCommitValue = InterlockedCompareExchange (
                                (PLONG) &MmTotalCommittedPages,
                                (LONG)  NewCommitValue,
                                (LONG)  OldCommitValue);
#endif
                                                             
    } while (NewCommitValue != OldCommitValue);

    //
    // Success.
    //

    MM_TRACK_COMMIT (MM_DBG_COMMIT_CHARGE_NORMAL, QuotaCharge);

    if (MmTotalCommittedPages > MmPeakCommitment) {
        MmPeakCommitment = MmTotalCommittedPages;
    }

    //
    // Success.  If system commit exceeds 90%, attempt a preemptive pagefile
    // increase anyway.
    //

    NewCommitValue = MmTotalCommittedPages;
    CommitLimit = MmTotalCommitLimit;

    if (NewCommitValue > ((CommitLimit/10)*9)) {

        if (CommitLimit < MmTotalCommitLimitMaximum) {

            //
            // Attempt to expand the paging file, but don't wait
            // to see if it succeeds.
            //

            NewCommitValue = NewCommitValue - ((CommitLimit/100)*85);

            MiIssuePageExtendRequestNoWait (NewCommitValue);
        }
        else {

            //
            // If the pagefiles are already at the maximum, then don't
            // bother trying to extend them, but do trim the cache.
            //

            if (MmTotalCommitLimit + 100 >= MmTotalCommitLimitMaximum) {
                MiTrimSegmentCache ();
            }
        }
    }

    return TRUE;
}

LOGICAL
FASTCALL
MiChargeCommitmentCantExpand (
    IN SIZE_T QuotaCharge,
    IN ULONG MustSucceed
    )

/*++

Routine Description:

    This routine charges the specified commitment without attempting
    to expand paging files and waiting for the expansion.  The routine
    determines if the paging file space is exhausted, and if so,
    it attempts to ascertain if the paging file space could be expanded.

Arguments:

    QuotaCharge - Supplies the quota amount to charge.

    MustSucceed - Supplies TRUE if the charge must succeed.

Return Value:

    TRUE if the commitment was permitted, FALSE if not.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    SIZE_T CommitLimit;
    SIZE_T ExtendAmount;
    SIZE_T OldCommitValue;
    SIZE_T NewCommitValue;

    ASSERT ((SSIZE_T)QuotaCharge > 0);

    ASSERT32 ((QuotaCharge < 0x100000) || (QuotaCharge < MmTotalCommitLimit));

    do {

        OldCommitValue = MmTotalCommittedPages;

        NewCommitValue = OldCommitValue + QuotaCharge;

        if ((NewCommitValue > MmTotalCommitLimit) && (!MustSucceed)) {

            if ((NewCommitValue < MmTotalCommittedPages) ||
                (MmTotalCommitLimit + 100 >= MmTotalCommitLimitMaximum)) {

                MiChargeCommitmentFailures[1] += 1;
                return FALSE;
            }

            //
            // Attempt to expand the paging file, but don't wait
            // to see if it succeeds.
            //

            MiChargeCommitmentFailures[0] += 1;
            MiIssuePageExtendRequestNoWait (MM_MINIMAL_COMMIT_INCREASE);
            return FALSE;
        }

#if defined(_WIN64)
        NewCommitValue = InterlockedCompareExchange64 (
                                (PLONGLONG) &MmTotalCommittedPages,
                                (LONGLONG)  NewCommitValue,
                                (LONGLONG)  OldCommitValue);
#else
        NewCommitValue = InterlockedCompareExchange (
                                (PLONG) &MmTotalCommittedPages,
                                (LONG)  NewCommitValue,
                                (LONG)  OldCommitValue);
#endif
                                                             
    } while (NewCommitValue != OldCommitValue);

    MM_TRACK_COMMIT (MM_DBG_COMMIT_CHARGE_CANT_EXPAND, QuotaCharge);

    //
    // Success.  If system commit exceeds 90%, attempt a preemptive pagefile
    // increase anyway.
    //

    NewCommitValue = MmTotalCommittedPages;
    CommitLimit = MmTotalCommitLimit;

    if ((NewCommitValue > ((CommitLimit/10)*9)) &&
        (CommitLimit < MmTotalCommitLimitMaximum)) {

        //
        // Attempt to expand the paging file, but don't wait
        // to see if it succeeds.
        //
        // Queue a message to the segment dereferencing / pagefile extending
        // thread to see if the page file can be extended.  This is done
        // in the context of a system thread due to mutexes which may
        // currently be held.
        //

        ExtendAmount = NewCommitValue - ((CommitLimit/100)*85);

        if (QuotaCharge > ExtendAmount) {
            ExtendAmount = QuotaCharge;
        }

        MiIssuePageExtendRequestNoWait (ExtendAmount);
    }

    return TRUE;
}


LOGICAL
FASTCALL
MiChargeTemporaryCommitmentForReduction (
    IN SIZE_T QuotaCharge
    )

/*++

Routine Description:

    This routine attempts to charge the specified commitment without
    expanding the paging file.

    This is typically called just prior to reducing the pagefile size.

Arguments:

    QuotaCharge - Supplies the quota amount to charge.

Return Value:

    TRUE if the commitment was permitted, FALSE if not.

Environment:

    Kernel mode, APCs disabled.

--*/

{
    SIZE_T OldCommitValue;
    SIZE_T NewCommitValue;

    ASSERT ((SSIZE_T)QuotaCharge > 0);

    ASSERT32 (QuotaCharge < 0x100000);

    do {

        OldCommitValue = MmTotalCommittedPages;

        NewCommitValue = OldCommitValue + QuotaCharge;

        if (NewCommitValue > MmTotalCommitLimit) {
            return FALSE;
        }

#if defined(_WIN64)
        NewCommitValue = InterlockedCompareExchange64 (
                                (PLONGLONG) &MmTotalCommittedPages,
                                (LONGLONG)  NewCommitValue,
                                (LONGLONG)  OldCommitValue);
#else
        NewCommitValue = InterlockedCompareExchange (
                                (PLONG) &MmTotalCommittedPages,
                                (LONG)  NewCommitValue,
                                (LONG)  OldCommitValue);
#endif
                                                             
    } while (NewCommitValue != OldCommitValue);

    //
    // Success.
    //

    MM_TRACK_COMMIT (MM_DBG_COMMIT_CHARGE_NORMAL, QuotaCharge);

    if (MmTotalCommittedPages > MmPeakCommitment) {
        MmPeakCommitment = MmTotalCommittedPages;
    }

    return TRUE;
}


SIZE_T
MiCalculatePageCommitment (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PMMVAD Vad,
    IN PEPROCESS Process
    )

/*++

Routine Description:

    This routine examines the range of pages from the starting address
    up to and including the ending address and returns the commit charge
    for the pages within the range.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

    Vad - Supplies the virtual address descriptor which describes the range.

    Process - Supplies the current process.

Return Value:

    Commitment charge for the range.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock and AddressCreation mutexes
    held.

--*/

{
    PMMPTE PointerPte;
    PMMPTE LastPte;
    PMMPTE PointerPde;
    PMMPTE PointerPpe;
    PMMPTE PointerPxe;
    SIZE_T NumberOfCommittedPages;
    ULONG Waited;

    PointerPxe = MiGetPxeAddress (StartingAddress);
    PointerPpe = MiGetPpeAddress (StartingAddress);
    PointerPde = MiGetPdeAddress (StartingAddress);
    PointerPte = MiGetPteAddress (StartingAddress);

    LastPte = MiGetPteAddress (EndingAddress);

    if (Vad->u.VadFlags.MemCommit == 1) {

        //
        // All the pages are committed within this range.
        //

        NumberOfCommittedPages = BYTES_TO_PAGES ((PCHAR)EndingAddress -
                                                       (PCHAR)StartingAddress);

        //
        // Examine the PTEs to determine how many pages are committed.
        //

        do {

#if (_MI_PAGING_LEVELS >= 4)
retry:
#endif

            while (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                                Process,
                                                MM_NOIRQL,
                                                &Waited)) {
    
                //
                // No PXE exists for the starting address, therefore the page
                // is not committed.
                //
    
                PointerPxe += 1;
                PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    return NumberOfCommittedPages;
                }
            }

#if (_MI_PAGING_LEVELS >= 4)
            Waited = 0;
#endif

            while (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                                Process,
                                                MM_NOIRQL,
                                                &Waited)) {
    
                //
                // No PPE exists for the starting address, therefore the page
                // is not committed.
                //
    
                PointerPpe += 1;
                PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    return NumberOfCommittedPages;
                }
#if (_MI_PAGING_LEVELS >= 4)
                if (MiIsPteOnPdeBoundary (PointerPpe)) {
                    PointerPxe = MiGetPteAddress (PointerPpe);
                    goto retry;
                }
#endif
            }

#if (_MI_PAGING_LEVELS < 4)
            Waited = 0;
#endif

            while (!MiDoesPdeExistAndMakeValid (PointerPde,
                                                Process,
                                                MM_NOIRQL,
                                                &Waited)) {
    
                //
                // No PDE exists for the starting address, therefore the page
                // is not committed.
                //
    
                PointerPde += 1;
                PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
                if (PointerPte > LastPte) {
                    return NumberOfCommittedPages;
                }
#if (_MI_PAGING_LEVELS >= 3)
                if (MiIsPteOnPdeBoundary (PointerPde)) {
                    PointerPpe = MiGetPteAddress (PointerPde);
                    PointerPxe = MiGetPdeAddress (PointerPde);
                    Waited = 1;
                    break;
                }
#endif
            }

        } while (Waited != 0);

restart:

        while (PointerPte <= LastPte) {

            if (MiIsPteOnPdeBoundary (PointerPte)) {

                //
                // This is a PDE boundary, check to see if the all the
                // PXE/PPE/PDE pages exist.
                //

                PointerPde = MiGetPteAddress (PointerPte);
                PointerPpe = MiGetPteAddress (PointerPde);
                PointerPxe = MiGetPteAddress (PointerPpe);

                do {

                    if (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                                     Process,
                                                     MM_NOIRQL,
                                                     &Waited)) {
    
                        //
                        // No PDE exists for the starting address, check the VAD
                        // to see if the pages are not committed.
                        //
    
                        PointerPxe += 1;
                        PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                        //
                        // Check next page.
                        //
    
                        goto restart;
                    }
    
#if (_MI_PAGING_LEVELS >= 4)
                    Waited = 0;
#endif
    
                    if (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                                     Process,
                                                     MM_NOIRQL,
                                                     &Waited)) {
    
                        //
                        // No PDE exists for the starting address, check the VAD
                        // to see if the pages are not committed.
                        //
    
                        PointerPpe += 1;
                        PointerPxe = MiGetPteAddress (PointerPpe);
                        PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                        //
                        // Check next page.
                        //
    
                        goto restart;
                    }
    
#if (_MI_PAGING_LEVELS < 4)
                    Waited = 0;
#endif
    
                    if (!MiDoesPdeExistAndMakeValid (PointerPde,
                                                     Process,
                                                     MM_NOIRQL,
                                                     &Waited)) {
    
                        //
                        // No PDE exists for the starting address, check the VAD
                        // to see if the pages are not committed.
                        //
    
                        PointerPde += 1;
                        PointerPpe = MiGetPteAddress (PointerPde);
                        PointerPxe = MiGetPteAddress (PointerPpe);
                        PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                        //
                        // Check next page.
                        //
    
                        goto restart;
                    }
                } while (Waited != 0);
            }

            //
            // The PDE exists, examine the PTE.
            //

            if (PointerPte->u.Long != 0) {

                //
                // Has this page been explicitly decommitted?
                //

                if (MiIsPteDecommittedPage (PointerPte)) {

                    //
                    // This page is decommitted, remove it from the count.
                    //

                    NumberOfCommittedPages -= 1;

                }
            }

            PointerPte += 1;
        }

        return NumberOfCommittedPages;
    }

    //
    // Examine non committed range.
    //

    NumberOfCommittedPages = 0;

    do {

#if (_MI_PAGING_LEVELS >= 4)
retry2:
#endif
        while (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                            Process,
                                            MM_NOIRQL,
                                            &Waited)) {
    
    
            //
            // No PXE exists for the starting address, therefore the page
            // is not committed.
            //
    
            PointerPxe += 1;
            PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            if (PointerPte > LastPte) {
               return NumberOfCommittedPages;
            }
        }

#if (_MI_PAGING_LEVELS >= 4)
        Waited = 0;
#endif

        while (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                            Process,
                                            MM_NOIRQL,
                                            &Waited)) {
    
    
            //
            // No PPE exists for the starting address, therefore the page
            // is not committed.
            //
    
            PointerPpe += 1;
            PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            if (PointerPte > LastPte) {
               return NumberOfCommittedPages;
            }
#if (_MI_PAGING_LEVELS >= 4)
            if (MiIsPteOnPdeBoundary (PointerPpe)) {
                PointerPxe = MiGetPteAddress (PointerPpe);
                goto retry2;
            }
#endif
        }

#if (_MI_PAGING_LEVELS < 4)
        Waited = 0;
#endif

        while (!MiDoesPdeExistAndMakeValid (PointerPde,
                                            Process,
                                            MM_NOIRQL,
                                            &Waited)) {
    
            //
            // No PDE exists for the starting address, therefore the page
            // is not committed.
            //
    
            PointerPde += 1;
            PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
            if (PointerPte > LastPte) {
               return NumberOfCommittedPages;
            }
#if (_MI_PAGING_LEVELS >= 3)
            if (MiIsPteOnPdeBoundary (PointerPde)) {
                PointerPpe = MiGetPteAddress (PointerPde);
                PointerPxe = MiGetPdeAddress (PointerPde);
                Waited = 1;
                break;
            }
#endif
        }

    } while (Waited != 0);

restart2:

    while (PointerPte <= LastPte) {

        if (MiIsPteOnPdeBoundary (PointerPte)) {

            //
            // This is a PDE boundary, check to see if the entire
            // PXE/PPE/PDE pages exist.
            //

            PointerPde = MiGetPteAddress (PointerPte);
            PointerPpe = MiGetPteAddress (PointerPde);
            PointerPxe = MiGetPdeAddress (PointerPde);

            do {

                if (!MiDoesPxeExistAndMakeValid (PointerPxe,
                                                 Process,
                                                 MM_NOIRQL,
                                                 &Waited)) {
    
                    //
                    // No PXE exists for the starting address, check the VAD
                    // to see if the pages are not committed.
                    //
    
                    PointerPxe += 1;
                    PointerPpe = MiGetVirtualAddressMappedByPte (PointerPxe);
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                    //
                    // Check next page.
                    //
    
                    goto restart2;
                }
    
#if (_MI_PAGING_LEVELS >= 4)
                Waited = 0;
#endif

                if (!MiDoesPpeExistAndMakeValid (PointerPpe,
                                                 Process,
                                                 MM_NOIRQL,
                                                 &Waited)) {
    
                    //
                    // No PPE exists for the starting address, check the VAD
                    // to see if the pages are not committed.
                    //
    
                    PointerPpe += 1;
                    PointerPxe = MiGetPteAddress (PointerPpe);
                    PointerPde = MiGetVirtualAddressMappedByPte (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                    //
                    // Check next page.
                    //
    
                    goto restart2;
                }
    
#if (_MI_PAGING_LEVELS < 4)
                Waited = 0;
#endif
    
                if (!MiDoesPdeExistAndMakeValid (PointerPde,
                                                 Process,
                                                 MM_NOIRQL,
                                                 &Waited)) {
    
                    //
                    // No PDE exists for the starting address, check the VAD
                    // to see if the pages are not committed.
                    //
    
                    PointerPde += 1;
                    PointerPpe = MiGetPteAddress (PointerPde);
                    PointerPxe = MiGetPteAddress (PointerPpe);
                    PointerPte = MiGetVirtualAddressMappedByPte (PointerPde);
    
                    //
                    // Check next page.
                    //
    
                    goto restart2;
                }

            } while (Waited != 0);
        }

        //
        // The PDE exists, examine the PTE.
        //

        if ((PointerPte->u.Long != 0) &&
             (!MiIsPteDecommittedPage (PointerPte))) {

            //
            // This page is committed, count it.
            //

            NumberOfCommittedPages += 1;
        }

        PointerPte += 1;
    }

    return NumberOfCommittedPages;
}

VOID
MiReturnPageTablePageCommitment (
    IN PVOID StartingAddress,
    IN PVOID EndingAddress,
    IN PEPROCESS CurrentProcess,
    IN PMMVAD PreviousVad,
    IN PMMVAD NextVad
    )

/*++

Routine Description:

    This routine returns commitment for COMPLETE page table pages which
    span the virtual address range.  For example (assuming 4k pages),
    if the StartingAddress =  64k and the EndingAddress = 5mb, no
    page table charges would be freed as a complete page table page is
    not covered by the range.  However, if the StartingAddress was 4mb
    and the EndingAddress was 9mb, 1 page table page would be freed.

Arguments:

    StartingAddress - Supplies the starting address of the range.

    EndingAddress - Supplies the ending address of the range.

    CurrentProcess - Supplies a pointer to the current process.

    PreviousVad - Supplies a pointer to the previous VAD, NULL if none.

    NextVad - Supplies a pointer to the next VAD, NULL if none.

Return Value:

    None.

Environment:

    Kernel mode, APCs disabled, WorkingSetLock and AddressCreation mutexes
    held.

--*/

{
    RTL_BITMAP VadBitMap;
    ULONG NumberToClear;
    ULONG StartBit;
    ULONG EndBit;
    LONG FirstPage;
    LONG LastPage;
    LONG PreviousPage;
    LONG NextPage;
#if (_MI_PAGING_LEVELS >= 3)
    LONG FirstPdPage;
    LONG LastPdPage;
    LONG PreviousPdPage;
    LONG NextPdPage;
#endif
#if (_MI_PAGING_LEVELS >= 4)
    LONG FirstPpPage;
    LONG LastPpPage;
    LONG PreviousPpPage;
    LONG NextPpPage;
#endif

    //
    // Check to see if any page table pages would be freed.
    //

    ASSERT (StartingAddress != EndingAddress);

    StartBit = (ULONG) (((ULONG_PTR) MI_64K_ALIGN (StartingAddress)) / X64K);
    EndBit = (ULONG) (((ULONG_PTR) MI_64K_ALIGN (EndingAddress)) / X64K);

    if (PreviousVad == NULL) {
        PreviousPage = -1;
#if (_MI_PAGING_LEVELS >= 3)
        PreviousPdPage = -1;
#endif
#if (_MI_PAGING_LEVELS >= 4)
        PreviousPpPage = -1;
#endif
    }
    else {
        PreviousPage = MiGetPdeIndex (MI_VPN_TO_VA (PreviousVad->EndingVpn));
#if (_MI_PAGING_LEVELS >= 3)
        PreviousPdPage = MiGetPpeIndex (MI_VPN_TO_VA (PreviousVad->EndingVpn));
#endif
#if (_MI_PAGING_LEVELS >= 4)
        PreviousPpPage = MiGetPxeIndex (MI_VPN_TO_VA (PreviousVad->EndingVpn));
#endif
        if (MI_64K_ALIGN (MI_VPN_TO_VA (PreviousVad->EndingVpn)) ==
            MI_64K_ALIGN (StartingAddress)) {
                StartBit += 1;
        }
    }

    if (NextVad == NULL) {
        NextPage = MiGetPdeIndex (MM_HIGHEST_USER_ADDRESS) + 1;
#if (_MI_PAGING_LEVELS >= 3)
        NextPdPage = MiGetPpeIndex (MM_HIGHEST_USER_ADDRESS) + 1;
#endif
#if (_MI_PAGING_LEVELS >= 4)
        NextPpPage = MiGetPxeIndex (MM_HIGHEST_USER_ADDRESS) + 1;
#endif
    }
    else {
        NextPage = MiGetPdeIndex (MI_VPN_TO_VA (NextVad->StartingVpn));
#if (_MI_PAGING_LEVELS >= 3)
        NextPdPage = MiGetPpeIndex (MI_VPN_TO_VA (NextVad->StartingVpn));
#endif
#if (_MI_PAGING_LEVELS >= 4)
        NextPpPage = MiGetPxeIndex (MI_VPN_TO_VA (NextVad->StartingVpn));
#endif
        if (MI_64K_ALIGN (MI_VPN_TO_VA (NextVad->StartingVpn)) ==
            MI_64K_ALIGN (EndingAddress)) {
                EndBit -= 1;
        }
    }

    ASSERT (PreviousPage <= NextPage);
    ASSERT64 (PreviousPdPage <= NextPdPage);
#if (_MI_PAGING_LEVELS >= 4)
    ASSERT64 (PreviousPpPage <= NextPpPage);
#endif

    FirstPage = MiGetPdeIndex (StartingAddress);

    LastPage = MiGetPdeIndex (EndingAddress);

    if (PreviousPage == FirstPage) {

        //
        // A VAD is within the starting page table page.
        //

        FirstPage += 1;
    }

    if (NextPage == LastPage) {

        //
        // A VAD is within the ending page table page.
        //

        LastPage -= 1;
    }

    if (StartBit <= EndBit) {

        //
        // Initialize the bitmap inline for speed.
        //

        VadBitMap.SizeOfBitMap = MiLastVadBit + 1;
        VadBitMap.Buffer = VAD_BITMAP_SPACE;

#if defined (_WIN64) || defined (_X86PAE_)

        //
        // Only the first (PAGE_SIZE*8*64K) of VA space on NT64 is bitmapped.
        //

        if (EndBit > MiLastVadBit) {
            EndBit = MiLastVadBit;
        }

        if (StartBit <= MiLastVadBit) {
            RtlClearBits (&VadBitMap, StartBit, EndBit - StartBit + 1);

            if (MmWorkingSetList->VadBitMapHint > StartBit) {
                MmWorkingSetList->VadBitMapHint = StartBit;
            }
        }
#else
        RtlClearBits (&VadBitMap, StartBit, EndBit - StartBit + 1);

        if (MmWorkingSetList->VadBitMapHint > StartBit) {
            MmWorkingSetList->VadBitMapHint = StartBit;
        }
#endif
    }

    //
    // Indicate that the page table page is not in use.
    //

    if (FirstPage > LastPage) {
        return;
    }

    NumberToClear = 1 + LastPage - FirstPage;

    while (FirstPage <= LastPage) {
        ASSERT (MI_CHECK_BIT (MmWorkingSetList->CommittedPageTables,
                              FirstPage));

        MI_CLEAR_BIT (MmWorkingSetList->CommittedPageTables, FirstPage);
        FirstPage += 1;
    }

    MmWorkingSetList->NumberOfCommittedPageTables -= NumberToClear;

#if (_MI_PAGING_LEVELS >= 4)

    //
    // Return page directory parent charges here.
    //

    FirstPpPage = MiGetPxeIndex (StartingAddress);

    LastPpPage = MiGetPxeIndex (EndingAddress);

    if (PreviousPpPage == FirstPpPage) {

        //
        // A VAD is within the starting page directory parent page.
        //

        FirstPpPage += 1;
    }

    if (NextPpPage == LastPpPage) {

        //
        // A VAD is within the ending page directory parent page.
        //

        LastPpPage -= 1;
    }

    //
    // Indicate that the page directory page parent is not in use.
    //

    if (FirstPpPage <= LastPpPage) {

        MmWorkingSetList->NumberOfCommittedPageDirectoryParents -= (1 + LastPpPage - FirstPpPage);

        NumberToClear += (1 + LastPpPage - FirstPpPage);
    
        while (FirstPpPage <= LastPpPage) {
            ASSERT (MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectoryParents,
                                  FirstPpPage));
    
            MI_CLEAR_BIT (MmWorkingSetList->CommittedPageDirectoryParents, FirstPpPage);
            FirstPpPage += 1;
        }
    }
    
#endif

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Return page directory charges here.
    //

    FirstPdPage = MiGetPpeIndex (StartingAddress);

    LastPdPage = MiGetPpeIndex (EndingAddress);

    if (PreviousPdPage == FirstPdPage) {

        //
        // A VAD is within the starting page directory page.
        //

        FirstPdPage += 1;
    }

    if (NextPdPage == LastPdPage) {

        //
        // A VAD is within the ending page directory page.
        //

        LastPdPage -= 1;
    }

    //
    // Indicate that the page directory page is not in use.
    //

    if (FirstPdPage <= LastPdPage) {

        MmWorkingSetList->NumberOfCommittedPageDirectories -= (1 + LastPdPage - FirstPdPage);

        NumberToClear += (1 + LastPdPage - FirstPdPage);
    
        while (FirstPdPage <= LastPdPage) {
            ASSERT (MI_CHECK_BIT (MmWorkingSetList->CommittedPageDirectories,
                                  FirstPdPage));
    
            MI_CLEAR_BIT (MmWorkingSetList->CommittedPageDirectories, FirstPdPage);
            FirstPdPage += 1;
        }
    }
    
#endif

    MiReturnCommitment (NumberToClear);
    MM_TRACK_COMMIT (MM_DBG_COMMIT_RETURN_PAGETABLES, NumberToClear);
    PsReturnProcessPageFileQuota (CurrentProcess, NumberToClear);

    if (CurrentProcess->JobStatus & PS_JOB_STATUS_REPORT_COMMIT_CHANGES) {
        PsChangeJobMemoryUsage(PS_JOB_STATUS_REPORT_COMMIT_CHANGES, -(SSIZE_T)NumberToClear);
    }
    CurrentProcess->CommitCharge -= NumberToClear;

    MI_INCREMENT_TOTAL_PROCESS_COMMIT (0 - NumberToClear);

    return;
}


VOID
MiCauseOverCommitPopup (
    VOID
    )

/*++

Routine Description:

    This function causes an over commit popup to occur (if the popup has never
    been sent before).

Arguments:

    None.

Return Value:

    None.

--*/

{
    LONG PopupNumber;

    //
    // Give the user a meaningful message - either to increase the minimum,
    // maximum, or both.
    //

    if (MmTotalCommittedPages > MmTotalCommitLimitMaximum - 100) {
        if (InterlockedIncrement (&MiCommitPopups[0]) > 1) {
            InterlockedDecrement (&MiCommitPopups[0]);
            return;
        }
        PopupNumber = STATUS_COMMITMENT_LIMIT;
    }
    else {
        if (InterlockedIncrement (&MiCommitPopups[1]) > 1) {
            InterlockedDecrement (&MiCommitPopups[1]);
            return;
        }
        PopupNumber = STATUS_COMMITMENT_MINIMUM;
    }

    IoRaiseInformationalHardError (PopupNumber, NULL, NULL);
}


SIZE_T MmTotalPagedPoolQuota;
SIZE_T MmTotalNonPagedPoolQuota;

BOOLEAN
MmRaisePoolQuota(
    IN POOL_TYPE PoolType,
    IN SIZE_T OldQuotaLimit,
    OUT PSIZE_T NewQuotaLimit
    )

/*++

Routine Description:

    This function is called (with a spinlock) whenever PS detects a quota
    limit has been exceeded. The purpose of this function is to attempt to
    increase the specified quota.

Arguments:

    PoolType - Supplies the pool type of the quota to be raised

    OldQuotaLimit - Supplies the current quota limit for this pool type

    NewQuotaLimit - Returns the new limit

Return Value:

    TRUE - The API succeeded and the quota limit was raised.

    FALSE - We were unable to raise the quota limit.

Environment:

    Kernel mode, QUOTA SPIN LOCK HELD!!

--*/

{
    SIZE_T Limit;
    PMM_PAGED_POOL_INFO PagedPoolInfo;

    if (PoolType == PagedPool) {

        //
        // Check commit limit and make sure at least 1mb is available.
        // Check to make sure 4mb of paged pool still exists.
        //

        PagedPoolInfo = &MmPagedPoolInfo;

        if (MmSizeOfPagedPoolInPages <
            (PagedPoolInfo->AllocatedPagedPool + ((MMPAGED_QUOTA_CHECK) >> PAGE_SHIFT))) {

            return FALSE;
        }

        MmTotalPagedPoolQuota += (MMPAGED_QUOTA_INCREASE);
        *NewQuotaLimit = OldQuotaLimit + (MMPAGED_QUOTA_INCREASE);
        return TRUE;

    } else {

        if ( (ULONG_PTR)(MmAllocatedNonPagedPool + ((1*1024*1024) >> PAGE_SHIFT)) < MmMaximumNonPagedPoolInPages) {
            goto aok;
        }

        //
        // Make sure 200 pages and 5mb of nonpaged pool expansion
        // available.  Raise quota by 64k.
        //

        if ((MmAvailablePages < 200) ||
            (MmResidentAvailablePages < ((MMNONPAGED_QUOTA_CHECK) >> PAGE_SHIFT))) {

            return FALSE;
        }

        if (MmAvailablePages > ((4*1024*1024) >> PAGE_SHIFT)) {
            Limit = (1*1024*1024) >> PAGE_SHIFT;
        } else {
            Limit = (4*1024*1024) >> PAGE_SHIFT;
        }

        if (MmMaximumNonPagedPoolInPages < MmAllocatedNonPagedPool + Limit) {

            return FALSE;
        }
aok:
        MmTotalNonPagedPoolQuota += (MMNONPAGED_QUOTA_INCREASE);
        *NewQuotaLimit = OldQuotaLimit + (MMNONPAGED_QUOTA_INCREASE);
        return TRUE;
    }
}


VOID
MmReturnPoolQuota(
    IN POOL_TYPE PoolType,
    IN SIZE_T ReturnedQuota
    )

/*++

Routine Description:

    Returns pool quota.

Arguments:

    PoolType - Supplies the pool type of the quota to be returned.

    ReturnedQuota - Number of bytes returned.

Return Value:

    NONE.

Environment:

    Kernel mode, QUOTA SPIN LOCK HELD!!

--*/

{

    if (PoolType == PagedPool) {
        MmTotalPagedPoolQuota -= ReturnedQuota;
    } else {
        MmTotalNonPagedPoolQuota -= ReturnedQuota;
    }

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\mirror.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   mirror.c

Abstract:

    This module contains the routines to support memory mirroring.

Author:

    Landy Wang (landyw) 17-Jan-2000

Revision History:

--*/

#include "mi.h"


#define MIRROR_MAX_PHASE_ZERO_PASSES 8

//
// This is set via the registry.
//

ULONG MmMirroring = 0;

//
// These bitmaps are allocated at system startup if the
// registry key above is set.
//

PRTL_BITMAP MiMirrorBitMap;
PRTL_BITMAP MiMirrorBitMap2;

//
// This is set if a mirroring operation is in progress.
//

LOGICAL MiMirroringActive = FALSE;

extern LOGICAL MiZeroingDisabled;

#if DBG
ULONG MiMirrorDebug = 1;
ULONG MiMirrorPassMax[2];
#endif

#pragma alloc_text(PAGELK, MmCreateMirror)

NTKERNELAPI
NTSTATUS
MmCreateMirror (
    VOID
    )
{
    KIRQL OldIrql;
    KIRQL ExitIrql;
    ULONG Limit;
    ULONG Color;
    ULONG IterationCount;
    PMMPFN Pfn1;
    PMMPFNLIST ListHead;
    PFN_NUMBER PreviousPage;
    PFN_NUMBER ThisPage;
    PFN_NUMBER PageFrameIndex;
    MMLISTS MemoryList;
    ULONG LengthOfClearRun;
    ULONG LengthOfSetRun;
    ULONG StartingRunIndex;
    ULONG BitMapIndex;
    ULONG BitMapHint;
    ULONG BitMapBytes;
    PULONG BitMap1;
    PULONG BitMap2;
    PHYSICAL_ADDRESS PhysicalAddress;
    LARGE_INTEGER PhysicalBytes;
    NTSTATUS Status;
    ULONG BitMapSize;
    PFN_NUMBER PagesWritten;
    PFN_NUMBER PagesWrittenLast;
    KPROCESSOR_MODE PreviousMode;
#if DBG
    ULONG PassMaxRun;
    PFN_NUMBER PagesVerified;
#endif

    ASSERT (KeGetCurrentIrql() == PASSIVE_LEVEL);

    PreviousMode = KeGetPreviousMode ();

    if ((PreviousMode != KernelMode) &&
        (SeSinglePrivilegeCheck(SeShutdownPrivilege, PreviousMode) == FALSE)) {
        return STATUS_PRIVILEGE_NOT_HELD;
    }

    if ((MmMirroring & MM_MIRRORING_ENABLED) == 0) {
        return STATUS_NOT_SUPPORTED;
    }

    if (MiMirrorBitMap == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if ((ExVerifySuite(DataCenter) == TRUE) ||
        ((MmProductType != 0x00690057) && (ExVerifySuite(Enterprise) == TRUE))) {
        //
        // DataCenter and Advanced Server are the only appropriate mirroring
        // platforms, allow them to proceed.
        //

        NOTHING;
    }
    else {
        return STATUS_LICENSE_VIOLATION;
    }

    //
    // Serialize here with dynamic memory additions and removals.
    //

    KeAcquireGuardedMutex (&MmDynamicMemoryMutex);

    ASSERT (MiMirroringActive == FALSE);

    MmLockPagableSectionByHandle (ExPageLockHandle);

    //
    // Setting all the bits here states all the pages need to be mirrored.
    // In the Phase0 loop below, the bits will be cleared as pages are
    // found on the lists and marked to be sent to the mirror. Bits are 
    // set again if the pages are reclaimed for active use.
    //

    RtlSetAllBits (MiMirrorBitMap2);

    //
    // Put all readonly nonpaged kernel and HAL subsection pages into the
    // Phase0 list.  The only way these could get written between Phase0
    // starting and Phase1 ending is via debugger breakpoints and those
    // don't matter.  This is worth a couple of megabytes and could be done
    // at some point in the future if a reasonable perf gain can be shown.
    //

    MiZeroingDisabled = TRUE;
    IterationCount = 0;

    //
    // Compute initial "pages copied" so convergence can be ascertained
    // in the main loop below.
    //

    PagesWrittenLast = 0;

#if DBG
    if (MiMirrorDebug != 0) {
        for (MemoryList = ZeroedPageList; MemoryList <= ModifiedNoWritePageList; MemoryList += 1) {
            PagesWrittenLast += (PFN_COUNT)MmPageLocationList[MemoryList]->Total;
        }
	    DbgPrint ("Mirror P0 starting with %x pages\n", PagesWrittenLast);
        PagesWrittenLast = 0;
	}
#endif

    //
    // Initiate Phase0 copying.
    // Inform the HAL so it can initialize if need be.
    //

    Status = HalStartMirroring ();

    if (!NT_SUCCESS(Status)) {
        MmUnlockPagableImageSection(ExPageLockHandle);
        MiZeroingDisabled = FALSE;
        ASSERT (MiMirroringActive == FALSE);
        KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
        return Status;
    }
    
    //
    // Scan system memory and mirror pages until a pass
    // doesn't find many pages to transfer.
    //

    do {

        //
        // The list of pages to be transferred on this iteration will be
        // formed in the MiMirrorBitMap array.  Clear out prior usage.
        //

        RtlClearAllBits (MiMirrorBitMap);

        //
        // Trim all pages from all process working sets so that as many pages
        // as possible will be on the standby, modified and modnowrite lists.
        // These lists are written during Phase0 mirroring where locks are
        // not held and thus the system is still somewhat operational from
        // an application's perspective.
        //

        MmEmptyAllWorkingSets ();
    
        MiFreeAllExpansionNonPagedPool ();
    
        LOCK_PFN (OldIrql);
    
        //
        // Scan all the page lists so they can be copied during Phase0
        // mirroring.
        //
        
        for (MemoryList = ZeroedPageList; MemoryList <= ModifiedNoWritePageList; MemoryList += 1) {
    
            ListHead = MmPageLocationList[MemoryList];
    
            if (ListHead->Total == 0) {
                continue;
            }
    
            if ((MemoryList == ModifiedPageList) &&
                (ListHead->Total == MmTotalPagesForPagingFile)) {
                    continue;
            }
    
            PageFrameIndex = ListHead->Flink;
    
            do {
    
                //
                // The scan is operating via the lists rather than the PFN
                // entries as read-in-progress pages are not on lists and
                // therefore do not have to be special cased here and elsewhere.
                //
    
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                ASSERT (Pfn1->u3.e1.ReadInProgress == 0);
    
                //
                // Setting the bit in BitMap means this page is to be copied
                // in this Phase0 iteration.  If it is reused after this
                // point (as indicated by its bit being set again in BitMap2),
                // it will be recopied on a later iteration or in Phase1.
                //

                if (RtlCheckBit(MiMirrorBitMap2, (ULONG)PageFrameIndex)) {
                    RtlSetBit (MiMirrorBitMap, (ULONG)PageFrameIndex);
                    RtlClearBit (MiMirrorBitMap2, (ULONG)PageFrameIndex);
                }
    
                PageFrameIndex = Pfn1->u1.Flink;
            } while (PageFrameIndex != MM_EMPTY_LIST);
        }
    
        //
        // Scan for modified pages destined for the paging file.
        //
    
        for (Color = 0; Color < MM_MAXIMUM_NUMBER_OF_COLORS; Color += 1) {
    
            ListHead = &MmModifiedPageListByColor[Color];
    
            if (ListHead->Total == 0) {
                continue;
            }
    
            PageFrameIndex = ListHead->Flink;
    
            do {
    
                //
                // The scan is operating via the lists rather than the PFN
                // entries as read-in-progress are not on lists.  Thus this
                // case does not have to be handled here and just works out.
                //
    
                Pfn1 = MI_PFN_ELEMENT (PageFrameIndex);
                ASSERT (Pfn1->u3.e1.ReadInProgress == 0);
    
                //
                // Setting the bit in BitMap means this page is to be copied
                // on this iteration of Phase0.  If it is reused after this
                // point (as indicated by its bit being set again in BitMap2),
                // it will be recopied on a later iteration or in Phase1.
                //

                if (RtlCheckBit(MiMirrorBitMap2, (ULONG)PageFrameIndex)) {
                    RtlSetBit (MiMirrorBitMap, (ULONG)PageFrameIndex);
                    RtlClearBit (MiMirrorBitMap2, (ULONG)PageFrameIndex);
                }
    
                PageFrameIndex = Pfn1->u1.Flink;
            } while (PageFrameIndex != MM_EMPTY_LIST);
        }
    
#if DBG
        if (MiMirrorDebug != 0) {
            DbgPrint ("Mirror P0 pass %d: Transfer %x pages\n", 
		      IterationCount,
		      RtlNumberOfSetBits(MiMirrorBitMap));
        }
#endif
    
        MiMirroringActive = TRUE;
    
        //
        // The dirty PFN bitmap has been initialized and the flag set.
        // There are very intricate rules governing how different places in
        // memory management MUST update the bitmap when we are in this mode.
        //
        // The rules are:
        //
        // Anyone REMOVING a page from the zeroed, free, transition, modified
        // or modnowrite lists must update the bitmap IFF that page could
        // potentially be subsequently modified.  Pages that are in transition
        // BUT NOT on one of these lists (ie inpages, freed pages that are
        // dangling due to nonzero reference counts, etc) do NOT need to
        // update the bitmap as they are not one of these lists.  If the page
        // is removed from one of the five lists just to be immediately
        // placed--without modification--on another list, then the bitmap
        // does NOT need updating.
        //
        // Therefore :
        //
        // MiUnlinkPageFromList updates the bitmap.  While some callers
        // immediately put a page acquired this way back on one of the 3 lists
        // above, this is generally rare.  Having this routine update the
        // bitmap means cases like restoring a transition PTE "just work".
        //
        // Callers of MiRemovePageFromList where list >= Transition, must do the
        // bitmap updates as only they know if the page is immediately going
        // back to one of the five lists above or being
        // reused (reused == update REQUIRED).
        //
        // MiRemoveZeroPage updates the bitmap as the page is immediately
        // going to be modified.  MiRemoveAnyPage does this also.
        //
        // Inserts into ANY list do not need to update bitmaps, as a remove had
        // to occur first (which would do the update) or it wasn't on a list to
        // begin with and thus wasn't subtracted above and therefore doesn't
        // need to update the bitmap either.
        //
    
        UNLOCK_PFN (OldIrql);
    
        BitMapHint = 0;
        PagesWritten = 0;
#if DBG
        PassMaxRun = 0;
#endif
    
        do {
    
            BitMapIndex = RtlFindSetBits (MiMirrorBitMap, 1, BitMapHint);
        
            if (BitMapIndex < BitMapHint) {
                break;
            }
        
            if (BitMapIndex == NO_BITS_FOUND) {
                break;
            }
    
            //
            // Found at least one page to copy - try for a cluster.
            //
    
            LengthOfClearRun = RtlFindNextForwardRunClear (MiMirrorBitMap,
                                                           BitMapIndex,
                                                           &StartingRunIndex);
    
            if (LengthOfClearRun != 0) {
                LengthOfSetRun = StartingRunIndex - BitMapIndex;
            }
            else {
                LengthOfSetRun = MiMirrorBitMap->SizeOfBitMap - BitMapIndex;
            }

            PagesWritten += LengthOfSetRun;
    
#if DBG
            if (LengthOfSetRun > PassMaxRun) {
                PassMaxRun = LengthOfSetRun;
            }
#endif
            //
            // Write out the page(s).
            //
    
            PhysicalAddress.QuadPart = BitMapIndex;
            PhysicalAddress.QuadPart = PhysicalAddress.QuadPart << PAGE_SHIFT;
    
            PhysicalBytes.QuadPart = LengthOfSetRun;
            PhysicalBytes.QuadPart = PhysicalBytes.QuadPart << PAGE_SHIFT;
    
            Status = HalMirrorPhysicalMemory (PhysicalAddress, PhysicalBytes);
    
            if (!NT_SUCCESS(Status)) {
                MiZeroingDisabled = FALSE;
                MmUnlockPagableImageSection(ExPageLockHandle);
                MiMirroringActive = FALSE;
                KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
                return Status;
            }
    
            BitMapHint = BitMapIndex + LengthOfSetRun + LengthOfClearRun;
    
        } while (BitMapHint < MiMirrorBitMap->SizeOfBitMap);
    
        ASSERT (RtlNumberOfSetBits(MiMirrorBitMap) == PagesWritten);
    
#if DBG
        if (PassMaxRun > MiMirrorPassMax[0]) {
            MiMirrorPassMax[0] = PassMaxRun;
        }

        if (MiMirrorDebug != 0) {
            DbgPrint ("Mirror P0 pass %d: ended with %x (last= %x) pages\n", 
                  IterationCount, PagesWritten, PagesWrittenLast);
        }
#endif

        ASSERT (MiMirroringActive == TRUE);

	    //
	    // Stop when PagesWritten by the current pass is not somewhat
	    // better than the preceeding pass.  If improvement is vanishing,
	    // the method is at the steady state where working set removals and
	    // transition faults are in balance.  Also stop if PagesWritten is
	    // small in absolute terms.  Finally, there is a limit on iterations
	    // for the misbehaving cases.
	    //

        if (((PagesWritten > PagesWrittenLast - 256) && (IterationCount > 0)) ||
            (PagesWritten < 1024)) {
            break;
        }

        ASSERT (MiMirroringActive == TRUE);
        PagesWrittenLast = PagesWritten;

        IterationCount += 1;

        if (IterationCount == 2) {

            //
            // We've made 2 passes at trimming pages without getting enough
            // to move to Phase1.  Throttle faults so threads cannot add
            // these back quickly.
            //

            InterlockedIncrement (&MiDelayPageFaults);
        }

    } while (IterationCount < MIRROR_MAX_PHASE_ZERO_PASSES);

    ASSERT (MiMirroringActive == TRUE);

    //
    // Notify the HAL that Phase0 is complete.  The HAL is responsible for
    // doing things like disabling interrupts, processors and preparing the
    // hardware for Phase1.  Note that some HALs may return from this
    // call at DISPATCH_LEVEL, so snap current IRQL now.
    //

    ExitIrql = KeGetCurrentIrql ();
    ASSERT (ExitIrql <= APC_LEVEL);
    ASSERT (KeAreAllApcsDisabled () == TRUE);

    Status = HalEndMirroring (0);

    if (!NT_SUCCESS(Status)) {

        if (IterationCount >= 2) {
            InterlockedDecrement (&MiDelayPageFaults);
        }

        ASSERT (KeGetCurrentIrql () <= APC_LEVEL);
        MmUnlockPagableImageSection(ExPageLockHandle);
        MiZeroingDisabled = FALSE;
        MiMirroringActive = FALSE;
        KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
        return Status;
    }

    ASSERT (KeGetCurrentIrql () <= DISPATCH_LEVEL);
    
    //
    // Phase0 copying is now complete.
    //
    // BitMap2 contains the list of safely transmitted (bit == 0) and
    // pages needing transmission (bit == 1).
    //
    // BitMap content is obsolete and if mirror verification is enabled,
    // BitMap will be reused below to accumulate the pages needing
    // verification in the following steps.
    //
    // Prepare for Phase1:
    //
    //  1.  Assume all pages are to be verified (set all bits in BitMap).
    //  2.  Synchronize list updates by acquiring the PFN lock.
    //  3.  Exclude all holes in the PFN database.
    //
    // Phase 1:
    //
    //  4.  Copy all the remaining pages whose bits are set.
    //  5.  Transmit the list of pages to be verified if so configured.
    //

    BitMapBytes = (ULONG)((((MiMirrorBitMap->SizeOfBitMap) + 31) / 32) * 4);

    BitMap1 = MiMirrorBitMap->Buffer;
    BitMap2 = MiMirrorBitMap2->Buffer;

    BitMapSize = MiMirrorBitMap->SizeOfBitMap;
    ASSERT (BitMapSize == MiMirrorBitMap2->SizeOfBitMap);

    //
    // Step 1:  Assume all pages are to be verified (set all bits in BitMap).
    //

    if (MmMirroring & MM_MIRRORING_VERIFYING) {
        RtlSetAllBits(MiMirrorBitMap);
    }

    //
    //  Step 2:  Synchronize list updates by acquiring the PFN lock.
    //

    LOCK_PFN2 (OldIrql);

    //
    // No need to throttle faults at this point now that the PFN lock is held.
    //

    if (IterationCount >= 2) {
        InterlockedDecrement (&MiDelayPageFaults);
    }

    //
    // No more updates of the bitmaps are needed - we've already snapped the
    // information we need and are going to hold the PFN lock from here until
    // we're done.
    //

    MiMirroringActive = FALSE;

    //
    // Step 3: Exclude any memory gaps.
    //

    Limit = 0;
    PreviousPage = 0;

    do {

        ThisPage = MmPhysicalMemoryBlock->Run[Limit].BasePage;

        if (ThisPage != PreviousPage) {
            RtlClearBits (MiMirrorBitMap2,
                          (ULONG)PreviousPage,
                          (ULONG)(ThisPage - PreviousPage));
	    
            if (MmMirroring & MM_MIRRORING_VERIFYING) {
                RtlClearBits (MiMirrorBitMap,
                          (ULONG)PreviousPage,
                          (ULONG)(ThisPage - PreviousPage));
            }
        }

        PreviousPage = ThisPage + MmPhysicalMemoryBlock->Run[Limit].PageCount;
        Limit += 1;

    } while (Limit != MmPhysicalMemoryBlock->NumberOfRuns);

    if (PreviousPage != MmHighestPossiblePhysicalPage + 1) {

        RtlClearBits (MiMirrorBitMap2,
                      (ULONG)PreviousPage,
                      (ULONG)(MmHighestPossiblePhysicalPage + 1 - PreviousPage));
        if (MmMirroring & MM_MIRRORING_VERIFYING) {
            RtlClearBits (MiMirrorBitMap,
                          (ULONG)PreviousPage,
                          (ULONG)(MmHighestPossiblePhysicalPage + 1 - PreviousPage));
        }
    }

    //
    // Step 4: Initiate Phase1 copying.
    //
    // N.B.  If this code or code that it calls, writes to non-stack
    // memory between this point and the completion of the call to
    // HalEndMirroring(1), the mirror *BREAKS*, because MmCreateMirror
    // does not know when that non-stack data will be transferred to
    // the new memory. [This rule can be broken if special arrangements
    // are made to re-copy the memory after the final write takes place.]
    //
    // N.B.  The HAL *MUST* handle the writes into this routine's stack
    // frame at the same time it deals with the stack frame of HalEndMirroring
    // and any other frames pushed by the HAL.
    //

    BitMapHint = 0;
#if DBG
    PagesWritten = 0;
    PassMaxRun = 0;
#endif

    do {

        BitMapIndex = RtlFindSetBits (MiMirrorBitMap2, 1, BitMapHint);
    
        if (BitMapIndex < BitMapHint) {
            break;
        }
    
        if (BitMapIndex == NO_BITS_FOUND) {
            break;
        }

        //
        // Found at least one page to copy - try for a cluster.
        //

        LengthOfClearRun = RtlFindNextForwardRunClear (MiMirrorBitMap2,
                                                       BitMapIndex,
                                                       &StartingRunIndex);

        if (LengthOfClearRun != 0) {
            LengthOfSetRun = StartingRunIndex - BitMapIndex;
        }
        else {
            LengthOfSetRun = MiMirrorBitMap2->SizeOfBitMap - BitMapIndex;
        }

#if DBG
        PagesWritten += LengthOfSetRun;

        if (LengthOfSetRun > PassMaxRun) {
            PassMaxRun = LengthOfSetRun;
        }
#endif

        //
        // Write out the page(s).
        //

        PhysicalAddress.QuadPart = BitMapIndex;
        PhysicalAddress.QuadPart = PhysicalAddress.QuadPart << PAGE_SHIFT;

        PhysicalBytes.QuadPart = LengthOfSetRun;
        PhysicalBytes.QuadPart = PhysicalBytes.QuadPart << PAGE_SHIFT;

        Status = HalMirrorPhysicalMemory (PhysicalAddress, PhysicalBytes);

        if (!NT_SUCCESS(Status)) {
            UNLOCK_PFN2 (ExitIrql);
            MiZeroingDisabled = FALSE;
            MmUnlockPagableImageSection(ExPageLockHandle);
            KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
            return Status;
        }

        BitMapHint = BitMapIndex + LengthOfSetRun + LengthOfClearRun;

    } while (BitMapHint < MiMirrorBitMap2->SizeOfBitMap);

    //
    // Phase1 copying is now complete.
    //

    //
    // Step 5:
    //
    // If HAL verification is enabled, inform the HAL of the ranges the
    // systems expects were mirrored.  Any range not in this list means
    // that the system doesn't care if it was mirrored and the contents may
    // very well be different between the mirrors.  Note the PFN lock is still
    // held so that the HAL can see things consistently.
    //

#if DBG
    PagesVerified = 0;
#endif

    if (MmMirroring & MM_MIRRORING_VERIFYING) {
        BitMapHint = 0;

        do {
    
            BitMapIndex = RtlFindSetBits (MiMirrorBitMap, 1, BitMapHint);
        
            if (BitMapIndex < BitMapHint) {
                break;
            }
        
            if (BitMapIndex == NO_BITS_FOUND) {
                break;
            }
    
            //
            // Found at least one page in this mirror range - try for a cluster.
            //
    
            LengthOfClearRun = RtlFindNextForwardRunClear (MiMirrorBitMap,
                                                           BitMapIndex,
                                                           &StartingRunIndex);
    
            if (LengthOfClearRun != 0) {
                LengthOfSetRun = StartingRunIndex - BitMapIndex;
            }
            else {
                LengthOfSetRun = MiMirrorBitMap->SizeOfBitMap - BitMapIndex;
            }
    
#if DBG
            PagesVerified += LengthOfSetRun;
#endif
    
            //
            // Tell the HAL that this range must be in a mirrored state.
            //
    
            PhysicalAddress.QuadPart = BitMapIndex;
            PhysicalAddress.QuadPart = PhysicalAddress.QuadPart << PAGE_SHIFT;
    
            PhysicalBytes.QuadPart = LengthOfSetRun;
            PhysicalBytes.QuadPart = PhysicalBytes.QuadPart << PAGE_SHIFT;
    
            Status = HalMirrorVerify (PhysicalAddress, PhysicalBytes);
    
            if (!NT_SUCCESS(Status)) {
                UNLOCK_PFN2 (ExitIrql);
                MiZeroingDisabled = FALSE;
                MmUnlockPagableImageSection(ExPageLockHandle);
                KeReleaseGuardedMutex (&MmDynamicMemoryMutex);
                return Status;
            }
    
            BitMapHint = BitMapIndex + LengthOfSetRun + LengthOfClearRun;
    
        } while (BitMapHint < MiMirrorBitMap->SizeOfBitMap);
    }
    
    //
    // Phase1 verification is now complete.
    //

    //
    // Notify the HAL that everything's done while still holding
    // the PFN lock - the HAL will now complete copying of all pages and
    // any other needed state before returning from this call.
    //

    Status = HalEndMirroring (1);

    UNLOCK_PFN2 (ExitIrql);

#if DBG
    if (MiMirrorDebug != 0) {
        DbgPrint ("Mirror P1: %x pages copied\n", PagesWritten);
        if (MmMirroring & MM_MIRRORING_VERIFYING) {
            DbgPrint ("Mirror P1: %x pages verified\n", PagesVerified);
        }
    }
    if (PassMaxRun > MiMirrorPassMax[1]) {
        MiMirrorPassMax[1] = PassMaxRun;
    }
#endif

    MiZeroingDisabled = FALSE;

    MmUnlockPagableImageSection(ExPageLockHandle);

    KeReleaseGuardedMutex (&MmDynamicMemoryMutex);

    return Status;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\miglobal.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   miglobal.c

Abstract:

    This module contains the private global storage for the memory
    management subsystem.

Author:

    Lou Perazzoli (loup) 6-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/
#include "mi.h"

//
// Highest user address;
//

PVOID MmHighestUserAddress;

//
// Start of system address range.
//

PVOID MmSystemRangeStart;

//
// User probe address;
//

ULONG_PTR MmUserProbeAddress;

#if !defined(_WIN64)

//
// Virtual bias applied during the loading of the kernel image.
//

ULONG_PTR MmVirtualBias;

#endif

//
// Number of secondary colors, based on level 2 d cache size.
//

ULONG MmSecondaryColors;

//
// The starting color index seed, incremented at each process creation.
//

ULONG MmProcessColorSeed = 0x12345678;

//
// Total number of physical pages available on the system.
//

PFN_COUNT MmNumberOfPhysicalPages;

//
// Lowest physical page number in the system.
//

PFN_NUMBER MmLowestPhysicalPage = (PFN_NUMBER)-1;

//
// Highest physical page number in the system.
//

PFN_NUMBER MmHighestPhysicalPage;

//
// Highest possible physical page number in the system.
//

PFN_NUMBER MmHighestPossiblePhysicalPage;

//
// Total number of available pages in the system.  This
// is the sum of the pages on the zeroed, free and standby lists.
//

PFN_NUMBER MmAvailablePages;
PFN_NUMBER MmThrottleTop;
PFN_NUMBER MmThrottleBottom;

//
// Highest VAD index used to create bitmaps.
//

ULONG MiLastVadBit = 1;

//
// System wide memory management statistics block.
//

MMINFO_COUNTERS MmInfoCounters;

//
// Total number of physical pages which would be usable if every process
// was at its minimum working set size.  This value is initialized
// at system initialization to MmAvailablePages - MM_FLUID_PHYSICAL_PAGES.
// Every time a thread is created, the kernel stack is subtracted from
// this and every time a process is created, the minimum working set
// is subtracted from this.  If the value would become negative, the
// operation (create process/kernel stack/ adjust working set) fails.
// The PFN LOCK must be owned to manipulate this value.
//

SPFN_NUMBER MmResidentAvailablePages;

//
// The total number of pages which would be removed from working sets
// if every working set was at its minimum.
//

PFN_NUMBER MmPagesAboveWsMinimum;

//
// If memory is becoming short and MmPagesAboveWsMinimum is
// greater than MmPagesAboveWsThreshold, trim working sets.
//

PFN_NUMBER MmPagesAboveWsThreshold = 37;

//
// The total number of pages needed for the loader to successfully hibernate.
// Make it big so we can handle a loader that may use a large bootfont.bin
//

PFN_NUMBER MmHiberPages = 768;

//
// The following values are frequently used together.  They tend
// not to be modified once the system has initialized so should
// not be grouped with data whose values change frequently to
// eliminate false sharing.
//

ULONG MmSecondaryColorMask;
UCHAR MmSecondaryColorNodeShift;

//
// Registry-settable threshold for using large pages.  x86 only.
//

ULONG MmLargePageMinimum;

PMMPFN MmPfnDatabase;

MMPFNLIST MmZeroedPageListHead = {
                    0, // Total
                    ZeroedPageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

MMPFNLIST MmFreePageListHead = {
                    0, // Total
                    FreePageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

MMPFNLIST MmStandbyPageListHead = {
                    0, // Total
                    StandbyPageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

MMPFNLIST MmModifiedPageListHead = {
                    0, // Total
                    ModifiedPageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

MMPFNLIST MmModifiedNoWritePageListHead = {
                    0, // Total
                    ModifiedNoWritePageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

MMPFNLIST MmBadPageListHead = {
                    0, // Total
                    BadPageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };

//
// Note the ROM page listhead is deliberately not in the set
// of MmPageLocationList ranges.
//

MMPFNLIST MmRomPageListHead = {
                    0, // Total
                    StandbyPageList, // ListName
                    MM_EMPTY_LIST, //Flink
                    MM_EMPTY_LIST  // Blink
                    };


PMMPFNLIST MmPageLocationList[NUMBER_OF_PAGE_LISTS] = {
                                      &MmZeroedPageListHead,
                                      &MmFreePageListHead,
                                      &MmStandbyPageListHead,
                                      &MmModifiedPageListHead,
                                      &MmModifiedNoWritePageListHead,
                                      &MmBadPageListHead,
                                      NULL,
                                      NULL };

PMMPTE MiHighestUserPte;
PMMPTE MiHighestUserPde;
#if (_MI_PAGING_LEVELS >= 4)
PMMPTE MiHighestUserPpe;
PMMPTE MiHighestUserPxe;
#endif

PMMPTE MiSessionBasePte;
PMMPTE MiSessionLastPte;

//
// Hyper space items.
//

PMMPTE MmFirstReservedMappingPte;

PMMPTE MmLastReservedMappingPte;

//
// Event for available pages, set means pages are available.
//

KEVENT MmAvailablePagesEvent;

//
// Event for the zeroing page thread.
//

KEVENT MmZeroingPageEvent;

//
// Boolean to indicate if the zeroing page thread is currently
// active.  This is set to true when the zeroing page event is
// set and set to false when the zeroing page thread is done
// zeroing all the pages on the free list.
//

BOOLEAN MmZeroingPageThreadActive;

//
// Minimum number of free pages before zeroing page thread starts.
//

PFN_NUMBER MmMinimumFreePagesToZero = 8;

//
// System space sizes - MmNonPagedSystemStart to MM_NON_PAGED_SYSTEM_END
// defines the ranges of PDEs which must be copied into a new process's
// address space.
//

PVOID MmNonPagedSystemStart;

LOGICAL MmProtectFreedNonPagedPool;

//
// This is set in the registry to the maximum number of gigabytes of RAM
// that can be added to this machine (ie: #of DIMM slots times maximum
// supported DIMM size).  This lets configurations that won't use the absolute
// maximum indicate that a smaller (virtually) PFN database size can be used
// thus leaving more virtual address space for things like system PTEs, etc.
//

PFN_NUMBER MmDynamicPfn;

#ifdef MM_BUMP_COUNTER_MAX
SIZE_T MmResTrack[MM_BUMP_COUNTER_MAX];
#endif

#ifdef MM_COMMIT_COUNTER_MAX
SIZE_T MmTrackCommit[MM_COMMIT_COUNTER_MAX];
#endif

//
// Set via the registry to identify which drivers are leaking locked pages.
//

LOGICAL  MmTrackLockedPages;

//
// Set via the registry to identify drivers which unload without releasing
// resources or still have active timers, etc.
//

LOGICAL MmSnapUnloads = TRUE;

#if DBG
PETHREAD MiExpansionLockOwner;
#endif

//
// Pool sizes.
//

SIZE_T MmSizeOfNonPagedPoolInBytes;

SIZE_T MmMaximumNonPagedPoolInBytes;

PFN_NUMBER MmMaximumNonPagedPoolInPages;

ULONG MmMaximumNonPagedPoolPercent;

SIZE_T MmMinimumNonPagedPoolSize = 256 * 1024; // 256k

ULONG MmMinAdditionNonPagedPoolPerMb = 32 * 1024; // 32k

SIZE_T MmDefaultMaximumNonPagedPool = 1024 * 1024;  // 1mb

ULONG MmMaxAdditionNonPagedPoolPerMb = 400 * 1024;  //400k

SIZE_T MmSizeOfPagedPoolInBytes = 32 * 1024 * 1024; // 32 MB.
PFN_NUMBER MmSizeOfPagedPoolInPages = (32 * 1024 * 1024) / PAGE_SIZE; // 32 MB.

PFN_NUMBER MmNumberOfSystemPtes;

ULONG MiRequestedSystemPtes;

PMMPTE MmFirstPteForPagedPool;

PMMPTE MmLastPteForPagedPool;

//
// Pool bit maps and other related structures.
//

PVOID MmPageAlignedPoolBase[2];

ULONG MmExpandedPoolBitPosition;

PFN_NUMBER MmNumberOfFreeNonPagedPool;

//
// MmFirstFreeSystemPte contains the offset from the
// Nonpaged system base to the first free system PTE.
// Note that an offset of -1 indicates an empty list.
//

MMPTE MmFirstFreeSystemPte[MaximumPtePoolTypes];

//
// System cache sizes.
//

PMMWSL MmSystemCacheWorkingSetList = (PMMWSL)MM_SYSTEM_CACHE_WORKING_SET;

MMSUPPORT MmSystemCacheWs;

PMMWSLE MmSystemCacheWsle;

PVOID MmSystemCacheStart = (PVOID)MM_SYSTEM_CACHE_START;

PVOID MmSystemCacheEnd;

PRTL_BITMAP MmSystemCacheAllocationMap;

PRTL_BITMAP MmSystemCacheEndingMap;

//
// This value should not be greater than 256MB in a system with 1GB of
// system space.
//

ULONG_PTR MmSizeOfSystemCacheInPages = 64 * 256; //64MB.

//
// Default sizes for the system cache.
//

PFN_NUMBER MmSystemCacheWsMinimum = 288;

PFN_NUMBER MmSystemCacheWsMaximum = 350;

//
// Cells to track unused thread kernel stacks to avoid TB flushes
// every time a thread terminates.
//

ULONG MmMaximumDeadKernelStacks = 5;
SLIST_HEADER MmDeadStackSListHead;

//
// Cells to track control area synchronization.
//

SLIST_HEADER MmEventCountSListHead;

SLIST_HEADER MmInPageSupportSListHead;

//
// MmSystemPteBase contains the address of 1 PTE before
// the first free system PTE (zero indicates an empty list).
// The value of this field does not change once set.
//

PMMPTE MmSystemPteBase;

MM_AVL_TABLE MmSectionBasedRoot;

PVOID MmHighSectionBase;

//
// Section object type.
//

POBJECT_TYPE MmSectionObjectType;

//
// Section commit mutex.
//

KGUARDED_MUTEX MmSectionCommitMutex;

//
// Section base address mutex.
//

KGUARDED_MUTEX MmSectionBasedMutex;

//
// Resource for section extension.
//

ERESOURCE MmSectionExtendResource;
ERESOURCE MmSectionExtendSetResource;

//
// Pagefile creation lock.
//

KGUARDED_MUTEX MmPageFileCreationLock;

MMDEREFERENCE_SEGMENT_HEADER MmDereferenceSegmentHeader;

LIST_ENTRY MmUnusedSegmentList;

LIST_ENTRY MmUnusedSubsectionList;

KEVENT MmUnusedSegmentCleanup;

ULONG MmUnusedSegmentCount;

ULONG MmUnusedSubsectionCount;

ULONG MmUnusedSubsectionCountPeak;

SIZE_T MiUnusedSubsectionPagedPool;

SIZE_T MiUnusedSubsectionPagedPoolPeak;

//
// If more than this percentage of pool is consumed and pool allocations
// might fail, then trim unused segments & subsections to get back to
// this percentage.
//

ULONG MmConsumedPoolPercentage;

MMWORKING_SET_EXPANSION_HEAD MmWorkingSetExpansionHead;

MMPAGE_FILE_EXPANSION MmAttemptForCantExtend;

//
// Paging files
//

MMMOD_WRITER_LISTHEAD MmPagingFileHeader;

MMMOD_WRITER_LISTHEAD MmMappedFileHeader;

LIST_ENTRY MmFreePagingSpaceLow;

ULONG MmNumberOfActiveMdlEntries;

PMMPAGING_FILE MmPagingFile[MAX_PAGE_FILES];

ULONG MmNumberOfPagingFiles;

KEVENT MmModifiedPageWriterEvent;

KEVENT MmWorkingSetManagerEvent;

KEVENT MmCollidedFlushEvent;

//
// Total number of committed pages.
//

SIZE_T MmTotalCommittedPages;

#if DBG
SPFN_NUMBER MiLockedCommit;
#endif

//
// Limit on committed pages.  When MmTotalCommittedPages would become
// greater than or equal to this number the paging files must be expanded.
//

SIZE_T MmTotalCommitLimit;

SIZE_T MmTotalCommitLimitMaximum;

ULONG MiChargeCommitmentFailures[3];        // referenced also in mi.h macros.

//
// Modified page writer.
//


//
// Minimum number of free pages before working set trimming and
// aggressive modified page writing is started.
//

PFN_NUMBER MmMinimumFreePages = 26;

//
// Stop writing modified pages when MmFreeGoal pages exist.
//

PFN_NUMBER MmFreeGoal = 100;

//
// Start writing pages if more than this number of pages
// is on the modified page list.
//

PFN_NUMBER MmModifiedPageMaximum;

//
// Amount of disk space that must be free after the paging file is
// extended.
//

ULONG MmMinimumFreeDiskSpace = 1024 * 1024;

//
// Minimum size in pages to extend the paging file by.
//

ULONG MmPageFileExtension = 256;

//
// Size to reduce the paging file by.
//

ULONG MmMinimumPageFileReduction = 256;  //256 pages (1mb)

//
// Number of pages to write in a single I/O.
//

ULONG MmModifiedWriteClusterSize = MM_MAXIMUM_WRITE_CLUSTER;

//
// Number of pages to read in a single I/O if possible.
//

ULONG MmReadClusterSize = 7;

const ULONG MMSECT = 'tSmM';              // This is exported to special pool.

//
// Spin lock for allowing working set expansion.
//

KSPIN_LOCK MmExpansionLock;

//
// System process working set sizes.
//

PFN_NUMBER MmSystemProcessWorkingSetMin = 50;

PFN_NUMBER MmSystemProcessWorkingSetMax = 450;

WSLE_NUMBER MmMaximumWorkingSetSize;

PFN_NUMBER MmMinimumWorkingSetSize = 20;


//
// Page color for system working set.
//

ULONG MmSystemPageColor;

//
// Time constants
//

const LARGE_INTEGER MmSevenMinutes = {0, -1};

const LARGE_INTEGER MmOneSecond = {(ULONG)(-1 * 1000 * 1000 * 10), -1};
const LARGE_INTEGER MmTwentySeconds = {(ULONG)(-20 * 1000 * 1000 * 10), -1};
const LARGE_INTEGER MmSeventySeconds = {(ULONG)(-70 * 1000 * 1000 * 10), -1};
const LARGE_INTEGER MmShortTime = {(ULONG)(-10 * 1000 * 10), -1}; // 10 milliseconds
const LARGE_INTEGER MmHalfSecond = {(ULONG)(-5 * 100 * 1000 * 10), -1};
const LARGE_INTEGER Mm30Milliseconds = {(ULONG)(-30 * 1000 * 10), -1};

//
// Parameters for user mode passed up via PEB in MmCreatePeb.
//

LARGE_INTEGER MmCriticalSectionTimeout;     // Filled in by mminit.c
SIZE_T MmHeapSegmentReserve = 1024 * 1024;
SIZE_T MmHeapSegmentCommit = PAGE_SIZE * 2;
SIZE_T MmHeapDeCommitTotalFreeThreshold = 64 * 1024;
SIZE_T MmHeapDeCommitFreeBlockThreshold = PAGE_SIZE;

//
// Set from ntos\config\CMDAT3.C  Used by customers to disable paging
// of executive on machines with lots of memory.  Worth a few TPS on a
// database server.
//

ULONG MmDisablePagingExecutive;

BOOLEAN Mm64BitPhysicalAddress;

#if DBG
ULONG MmDebug;
#endif

//
// Map a page protection from the Pte.Protect field into a protection mask.
//

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg("PAGEDATA")
#endif

ULONG MmProtectToValue[32] = {
                            PAGE_NOACCESS,
                            PAGE_READONLY,
                            PAGE_EXECUTE,
                            PAGE_EXECUTE_READ,
                            PAGE_READWRITE,
                            PAGE_WRITECOPY,
                            PAGE_EXECUTE_READWRITE,
                            PAGE_EXECUTE_WRITECOPY,
                            PAGE_NOACCESS,
                            PAGE_NOCACHE | PAGE_READONLY,
                            PAGE_NOCACHE | PAGE_EXECUTE,
                            PAGE_NOCACHE | PAGE_EXECUTE_READ,
                            PAGE_NOCACHE | PAGE_READWRITE,
                            PAGE_NOCACHE | PAGE_WRITECOPY,
                            PAGE_NOCACHE | PAGE_EXECUTE_READWRITE,
                            PAGE_NOCACHE | PAGE_EXECUTE_WRITECOPY,
                            PAGE_NOACCESS,
                            PAGE_GUARD | PAGE_READONLY,
                            PAGE_GUARD | PAGE_EXECUTE,
                            PAGE_GUARD | PAGE_EXECUTE_READ,
                            PAGE_GUARD | PAGE_READWRITE,
                            PAGE_GUARD | PAGE_WRITECOPY,
                            PAGE_GUARD | PAGE_EXECUTE_READWRITE,
                            PAGE_GUARD | PAGE_EXECUTE_WRITECOPY,
                            PAGE_NOACCESS,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_READONLY,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_EXECUTE,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_EXECUTE_READ,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_READWRITE,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_WRITECOPY,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_EXECUTE_READWRITE,
                            PAGE_NOCACHE | PAGE_GUARD | PAGE_EXECUTE_WRITECOPY
                          };

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg()
#endif

#if (defined(_WIN64) || defined(_X86PAE_))
ULONGLONG
#else
ULONG
#endif
MmProtectToPteMask[32] = {
                       MM_PTE_NOACCESS,
                       MM_PTE_READONLY | MM_PTE_CACHE,
                       MM_PTE_EXECUTE | MM_PTE_CACHE,
                       MM_PTE_EXECUTE_READ | MM_PTE_CACHE,
                       MM_PTE_READWRITE | MM_PTE_CACHE,
                       MM_PTE_WRITECOPY | MM_PTE_CACHE,
                       MM_PTE_EXECUTE_READWRITE | MM_PTE_CACHE,
                       MM_PTE_EXECUTE_WRITECOPY | MM_PTE_CACHE,
                       MM_PTE_NOACCESS,
                       MM_PTE_NOCACHE | MM_PTE_READONLY,
                       MM_PTE_NOCACHE | MM_PTE_EXECUTE,
                       MM_PTE_NOCACHE | MM_PTE_EXECUTE_READ,
                       MM_PTE_NOCACHE | MM_PTE_READWRITE,
                       MM_PTE_NOCACHE | MM_PTE_WRITECOPY,
                       MM_PTE_NOCACHE | MM_PTE_EXECUTE_READWRITE,
                       MM_PTE_NOCACHE | MM_PTE_EXECUTE_WRITECOPY,
                       MM_PTE_NOACCESS,
                       MM_PTE_GUARD | MM_PTE_READONLY | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_EXECUTE | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_EXECUTE_READ | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_READWRITE | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_WRITECOPY | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_EXECUTE_READWRITE | MM_PTE_CACHE,
                       MM_PTE_GUARD | MM_PTE_EXECUTE_WRITECOPY | MM_PTE_CACHE,
                       MM_PTE_NOACCESS,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_READONLY,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_EXECUTE,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_EXECUTE_READ,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_READWRITE,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_WRITECOPY,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_EXECUTE_READWRITE,
                       MM_PTE_NOCACHE | MM_PTE_GUARD | MM_PTE_EXECUTE_WRITECOPY
                    };

//
// Conversion which takes a Pte.Protect and builds a new Pte.Protect which
// is not copy-on-write.
//

ULONG MmMakeProtectNotWriteCopy[32] = {
                       MM_NOACCESS,
                       MM_READONLY,
                       MM_EXECUTE,
                       MM_EXECUTE_READ,
                       MM_READWRITE,
                       MM_READWRITE,        //not copy
                       MM_EXECUTE_READWRITE,
                       MM_EXECUTE_READWRITE,
                       MM_NOACCESS,
                       MM_NOCACHE | MM_READONLY,
                       MM_NOCACHE | MM_EXECUTE,
                       MM_NOCACHE | MM_EXECUTE_READ,
                       MM_NOCACHE | MM_READWRITE,
                       MM_NOCACHE | MM_READWRITE,
                       MM_NOCACHE | MM_EXECUTE_READWRITE,
                       MM_NOCACHE | MM_EXECUTE_READWRITE,
                       MM_NOACCESS,
                       MM_GUARD_PAGE | MM_READONLY,
                       MM_GUARD_PAGE | MM_EXECUTE,
                       MM_GUARD_PAGE | MM_EXECUTE_READ,
                       MM_GUARD_PAGE | MM_READWRITE,
                       MM_GUARD_PAGE | MM_READWRITE,
                       MM_GUARD_PAGE | MM_EXECUTE_READWRITE,
                       MM_GUARD_PAGE | MM_EXECUTE_READWRITE,
                       MM_NOACCESS,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_READONLY,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_EXECUTE,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_EXECUTE_READ,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_READWRITE,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_READWRITE,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_EXECUTE_READWRITE,
                       MM_NOCACHE | MM_GUARD_PAGE | MM_EXECUTE_READWRITE
                       };

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg("PAGEDATA")
#endif

//
// Converts a protection code to an access right for section access.
// This uses only the lower 3 bits of the 5 bit protection code.
//

ACCESS_MASK MmMakeSectionAccess[8] = { SECTION_MAP_READ,
                                       SECTION_MAP_READ,
                                       SECTION_MAP_EXECUTE,
                                       SECTION_MAP_EXECUTE | SECTION_MAP_READ,
                                       SECTION_MAP_WRITE,
                                       SECTION_MAP_READ,
                                       SECTION_MAP_EXECUTE | SECTION_MAP_WRITE,
                                       SECTION_MAP_EXECUTE | SECTION_MAP_READ };

//
// Converts a protection code to an access right for file access.
// This uses only the lower 3 bits of the 5 bit protection code.
//

ACCESS_MASK MmMakeFileAccess[8] = { FILE_READ_DATA,
                                FILE_READ_DATA,
                                FILE_EXECUTE,
                                FILE_EXECUTE | FILE_READ_DATA,
                                FILE_WRITE_DATA | FILE_READ_DATA,
                                FILE_READ_DATA,
                                FILE_EXECUTE | FILE_WRITE_DATA | FILE_READ_DATA,
                                FILE_EXECUTE | FILE_READ_DATA };

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg()
#endif

MM_PAGED_POOL_INFO MmPagedPoolInfo;

//
// Some Hydra variables.
//

ULONG_PTR MmSessionBase;

PMM_SESSION_SPACE MmSessionSpace;

ULONG_PTR MiSessionSpaceWs;

SIZE_T MmSessionSize;

LIST_ENTRY MiSessionWsList;

ULONG_PTR MiSystemViewStart;

SIZE_T MmSystemViewSize;

ULONG_PTR MiSessionPoolStart;

ULONG_PTR MiSessionPoolEnd;

ULONG_PTR MiSessionSpaceEnd;

ULONG_PTR MiSessionViewStart;

ULONG MiSessionSpacePageTables;

SIZE_T MmSessionViewSize;

SIZE_T MmSessionPoolSize;

ULONG_PTR MiSessionImageStart;

ULONG_PTR MiSessionImageEnd;

PMMPTE MiSessionImagePteStart;

PMMPTE MiSessionImagePteEnd;

SIZE_T MmSessionImageSize;

//
// Cache control stuff.  Note this may be overridden by deficient hardware
// platforms at startup.
//

MI_PFN_CACHE_ATTRIBUTE MiPlatformCacheAttributes[2 * MmMaximumCacheType] =
{
    //
    // Memory space
    //

    MiNonCached,
    MiCached,
    MiWriteCombined,
    MiCached,
    MiNonCached,
    MiWriteCombined,

    //
    // I/O space
    //

    MiNonCached,
    MiCached,
    MiWriteCombined,
    MiCached,
    MiNonCached,
    MiWriteCombined
};

//
// Note the Driver Verifier can reinitialize the mask values.
//

ULONG MiIoRetryMask = 0x1f;
ULONG MiFaultRetryMask = 0x1f;
ULONG MiUserFaultRetryMask = 0xF;

#if defined (_MI_INSTRUMENT_PFN) || defined (_MI_INSTRUMENT_WS)

EPROCESS MiSystemCacheDummyProcess;

//
// Instrumentation code to track PFN lock duration.
//

ULONG MiPfnTimings;
PVOID MiPfnAcquiredAddress;
LARGE_INTEGER MiPfnAcquired;
LARGE_INTEGER MiPfnReleased;
LARGE_INTEGER MiPfnThreshold;

MMPFNTIMINGS MiPfnSorted[MI_MAX_PFN_CALLERS];
ULONG MiMaxPfnTimings = MI_MAX_PFN_CALLERS;

PVOID
MiGetExecutionAddress (
    VOID
    )
{
#if defined(_X86_)
    _asm {
        push    dword ptr [esp]
        pop     eax
    }
#else
    PVOID CallingAddress;
    PVOID CallersCaller;

    RtlGetCallersAddress (&CallingAddress, &CallersCaller);
    return CallingAddress;
#endif
}

LARGE_INTEGER
MiQueryPerformanceCounter (
    IN PLARGE_INTEGER PerformanceFrequency 
    )
{
#if defined(_X86_)

    UNREFERENCED_PARAMETER (PerformanceFrequency);

    _asm {
        rdtsc
    }
#else
    return KeQueryPerformanceCounter (PerformanceFrequency);
#endif
}

#if defined (_MI_INSTRUMENT_WS)
KSPIN_LOCK MiInstrumentationLock;
#endif

VOID
MiAddLockToTable (
    IN PVOID AcquireAddress,
    IN PVOID ReleaseAddress,
    IN LARGE_INTEGER HoldTime
    )
{
    ULONG i;
#if defined (_MI_INSTRUMENT_WS)
    KIRQL OldIrql;
#endif

    i = MI_MAX_PFN_CALLERS - 1;

#if defined (_MI_INSTRUMENT_WS)
    ExAcquireSpinLock (&MiInstrumentationLock, &OldIrql);
#endif

    do {
        if (HoldTime.QuadPart < MiPfnSorted[i].HoldTime.QuadPart) {
            break;
        }
        i -= 1;
    } while (i != (ULONG)-1);

    if (i != MI_MAX_PFN_CALLERS - 1) {

        i += 1;

        if (i != MI_MAX_PFN_CALLERS - 1) {
            RtlMoveMemory (&MiPfnSorted[i+1], &MiPfnSorted[i], (MI_MAX_PFN_CALLERS-(i+1)) * sizeof(MMPFNTIMINGS));
        }

        MiPfnSorted[i].HoldTime = HoldTime;

#if defined (_MI_INSTRUMENT_WS)
        if (PsGetCurrentProcess()->WorkingSetLock.Count != 0)
#else
        if (KeTestForWaitersQueuedSpinLock (LockQueuePfnLock) == TRUE)
#endif
        {
            MiPfnSorted[i].HoldTime.LowPart |= 0x1;
        }

        MiPfnSorted[i].AcquiredAddress = AcquireAddress;
        MiPfnSorted[i].ReleasedAddress = ReleaseAddress;
    }

    if ((MiPfnTimings & 0x2) && (HoldTime.QuadPart >= MiPfnThreshold.QuadPart)) {
        DbgBreakPoint ();
    }

    if (MiPfnTimings & 0x1) {
        MiPfnTimings &= ~0x1;
        RtlZeroMemory (&MiPfnSorted[0], MI_MAX_PFN_CALLERS * sizeof(MMPFNTIMINGS));
    }

#if defined (_MI_INSTRUMENT_WS)
    ExReleaseSpinLock (&MiInstrumentationLock, OldIrql);
#endif

    return;
}

#endif

#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg("INIT")
#endif

WCHAR MmVerifyDriverBuffer[MI_SUSPECT_DRIVER_BUFFER_LENGTH] = {0};
ULONG MmVerifyDriverBufferType = REG_NONE;
ULONG MmVerifyDriverLevel = (ULONG)-1;
ULONG MmCritsectTimeoutSeconds = 2592000;

ULONG MmLargePageDriverBufferType = REG_NONE;
#ifdef ALLOC_DATA_PRAGMA
#pragma data_seg()
#endif

WCHAR MmLargePageDriverBuffer[MI_LARGE_PAGE_DRIVER_BUFFER_LENGTH] = {0};

ULONG MmVerifyDriverBufferLength = sizeof(MmVerifyDriverBuffer);
ULONG MmLargePageDriverBufferLength = sizeof(MmLargePageDriverBuffer);
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\mmpatch.c ===
/*

Copyright (c) 2001  Microsoft Corporation

File name:

    mmpatch.c
   
Author:
    
    Adrian Marinescu (adrmarin)  Dec 20 2001

Environment:

    Kernel mode only.

Revision History:

*/

#include "mi.h"
#pragma hdrstop

#define NTOS_KERNEL_RUNTIME

#include "hotpatch.h"

NTSTATUS
MiPerformHotPatch (
    IN PKLDR_DATA_TABLE_ENTRY PatchHandle,
    IN PVOID ImageBaseAddress,
    IN ULONG PatchFlags
    );

VOID
MiRundownHotpatchList (
    IN PRTL_PATCH_HEADER PatchHead
    );
                   
#ifdef ALLOC_PRAGMA

#pragma alloc_text(PAGE,MmLockAndCopyMemory)
#pragma alloc_text(PAGE,MiPerformHotPatch)
#pragma alloc_text(PAGE,MmHotPatchRoutine)
#pragma alloc_text(PAGE,MiRundownHotpatchList)

#endif

LIST_ENTRY MiHotPatchList;

#define MiInValidRange(s,offset,size,total) \
    (((s).offset>(total)) ||                 \
     ((s).size>(total)) ||                   \
     (((s).offset + (s).size)>(total)))      

VOID
MiDoCopyMemory (
    IN PKDPC Dpc,
    IN PVOID DeferredContext,
    IN PVOID SystemArgument1,
    IN PVOID SystemArgument2
    )

/*++

Routine Description:

    This target function copies a captured buffer containing the new code over
    existing code.

Arguments:

    Dpc - Supplies a pointer to a control object of type DPC.

    DeferredContext - Deferred context.

    SystemArgument1 - Used to signal completion of this call.

    SystemArgument2 - Used for internal lockstepping during this call.

Return Value:

    None.

Environment:

    Kernel mode, DISPATCH_LEVEL as the target of a broadcast DPC.

--*/

{
    ULONG i;
    KIRQL OldIrql;
    PSYSTEM_HOTPATCH_CODE_INFORMATION PatchInfo;

    ASSERT (KeGetCurrentIrql () == DISPATCH_LEVEL);

    UNREFERENCED_PARAMETER (Dpc);

    PatchInfo = (PSYSTEM_HOTPATCH_CODE_INFORMATION)DeferredContext;

    //
    // Raise IRQL and wait for all processors to synchronize to ensure no
    // processor can be executing the code we're about to modify.
    //

    KeRaiseIrql (IPI_LEVEL - 1, &OldIrql);
    
    if (KeSignalCallDpcSynchronize (SystemArgument2)) {

        PatchInfo->Flags &= ~FLG_HOTPATCH_VERIFICATION_ERROR;

        //
        // Compare the existing code.
        //

        for (i = 0; i < PatchInfo->CodeInfo.DescriptorsCount; i += 1) {

            if (PatchInfo->Flags & FLG_HOTPATCH_ACTIVE) {
                
                if (RtlCompareMemory (PatchInfo->CodeInfo.CodeDescriptors[i].MappedAddress, 
                                      (PUCHAR)PatchInfo + PatchInfo->CodeInfo.CodeDescriptors[i].ValidationOffset, 
                                      PatchInfo->CodeInfo.CodeDescriptors[i].ValidationSize) 
                        != PatchInfo->CodeInfo.CodeDescriptors[i].ValidationSize) {

                    //
                    // Maybe this instruction has been previously patched. See if the OrigCodeOffset matches 
                    // in this case
                    //

                    if (RtlCompareMemory (PatchInfo->CodeInfo.CodeDescriptors[i].MappedAddress, 
                                          (PUCHAR)PatchInfo + PatchInfo->CodeInfo.CodeDescriptors[i].OrigCodeOffset, 
                                          PatchInfo->CodeInfo.CodeDescriptors[i].CodeSize) 
                            != PatchInfo->CodeInfo.CodeDescriptors[i].CodeSize) {

                        PatchInfo->Flags |= FLG_HOTPATCH_VERIFICATION_ERROR;
                        break;
                    }
                }
            }
            else {
                
                if (RtlCompareMemory (PatchInfo->CodeInfo.CodeDescriptors[i].MappedAddress, 
                                      (PUCHAR)PatchInfo + PatchInfo->CodeInfo.CodeDescriptors[i].CodeOffset, 
                                      PatchInfo->CodeInfo.CodeDescriptors[i].CodeSize) 
                        != PatchInfo->CodeInfo.CodeDescriptors[i].CodeSize) {

                    PatchInfo->Flags |= FLG_HOTPATCH_VERIFICATION_ERROR;
                    break;
                }
            }
        }

        if (!(PatchInfo->Flags & FLG_HOTPATCH_VERIFICATION_ERROR)) {

            for (i = 0; i < PatchInfo->CodeInfo.DescriptorsCount; i += 1) {

                if (PatchInfo->Flags & FLG_HOTPATCH_ACTIVE) {

                    RtlCopyMemory (PatchInfo->CodeInfo.CodeDescriptors[i].MappedAddress, 
                                   (PUCHAR)PatchInfo + PatchInfo->CodeInfo.CodeDescriptors[i].CodeOffset, 
                                   PatchInfo->CodeInfo.CodeDescriptors[i].CodeSize );
                } else {

                    RtlCopyMemory (PatchInfo->CodeInfo.CodeDescriptors[i].MappedAddress, 
                                   (PUCHAR)PatchInfo + PatchInfo->CodeInfo.CodeDescriptors[i].OrigCodeOffset, 
                                   PatchInfo->CodeInfo.CodeDescriptors[i].CodeSize );
                }
            }
        }
    }

    KeSignalCallDpcSynchronize (SystemArgument2);
    
    KeSweepCurrentIcache ();
    
    KeLowerIrql (OldIrql);
    
    //
    // Signal that all processing has been done.
    //

    KeSignalCallDpcDone (SystemArgument1);

    return;
}

NTSTATUS
MmLockAndCopyMemory (
    IN PSYSTEM_HOTPATCH_CODE_INFORMATION PatchInfo,
    IN KPROCESSOR_MODE ProbeMode
    )

/*++

Routine Description:

    This function locks the code pages for IoWriteAccess and 
    copy the new code at DPC, if all validations succeed.

Arguments:

    PatchInfo - Supplies the descriptors for the target code and validation
    
    ProbeMode - Supplied the probe mode for ExLockUserBuffer

Return Value:

    NTSTATUS.

--*/

{
    PVOID * Locks;
    ULONG i;
    NTSTATUS Status;
    
    ASSERT (KeGetCurrentIrql () <= APC_LEVEL);

    if (PatchInfo->CodeInfo.DescriptorsCount == 0) {

        //
        //  Nothing to change
        //

        return STATUS_SUCCESS;
    }

    Locks = ExAllocatePoolWithQuotaTag (PagedPool | POOL_QUOTA_FAIL_INSTEAD_OF_RAISE,
                                        PatchInfo->CodeInfo.DescriptorsCount * sizeof(PVOID), 
                                        'PtoH');

    if (Locks == NULL) {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    RtlZeroMemory (Locks, PatchInfo->CodeInfo.DescriptorsCount * sizeof(PVOID));

    Status = STATUS_INVALID_PARAMETER;

    for (i = 0; i < PatchInfo->CodeInfo.DescriptorsCount; i += 1) {

        if (MiInValidRange (PatchInfo->CodeInfo.CodeDescriptors[i],CodeOffset,CodeSize, PatchInfo->InfoSize )
                ||
            MiInValidRange (PatchInfo->CodeInfo.CodeDescriptors[i],OrigCodeOffset,CodeSize, PatchInfo->InfoSize )
                ||
            MiInValidRange (PatchInfo->CodeInfo.CodeDescriptors[i],ValidationOffset,ValidationSize, PatchInfo->InfoSize )
                ||
            (PatchInfo->CodeInfo.CodeDescriptors[i].CodeSize == 0)
                ||
            (PatchInfo->CodeInfo.CodeDescriptors[i].ValidationSize < PatchInfo->CodeInfo.CodeDescriptors[i].CodeSize) ) {

            Status = STATUS_INVALID_PARAMETER;
            break;
        }

        Status = ExLockUserBuffer ((PVOID)PatchInfo->CodeInfo.CodeDescriptors[i].TargetAddress,
                                  (ULONG)PatchInfo->CodeInfo.CodeDescriptors[i].CodeSize,
                                  ProbeMode,
                                  IoWriteAccess,
                                  (PVOID)&PatchInfo->CodeInfo.CodeDescriptors[i].MappedAddress,
                                  &Locks[i]
                                 );

        if (!NT_SUCCESS(Status)) {

            break;
        }
    }

    if (NT_SUCCESS(Status)) {

        PatchInfo->Flags ^= FLG_HOTPATCH_ACTIVE;

        KeGenericCallDpc (MiDoCopyMemory, PatchInfo);

        if (PatchInfo->Flags & FLG_HOTPATCH_VERIFICATION_ERROR) {

            PatchInfo->Flags ^= FLG_HOTPATCH_ACTIVE;
            PatchInfo->Flags &= ~FLG_HOTPATCH_VERIFICATION_ERROR;

            Status = STATUS_DATA_ERROR;
        }
    }

    for (i = 0; i < PatchInfo->CodeInfo.DescriptorsCount; i += 1) {

        if (Locks[i] != NULL) {

            ExUnlockUserBuffer (Locks[i]);
        }
    }

    ExFreePool (Locks);

    return Status;
}

NTSTATUS
MiPerformHotPatch (
    IN PKLDR_DATA_TABLE_ENTRY PatchHandle,
    IN PVOID ImageBaseAddress,
    IN ULONG PatchFlags
    )

/*++

Routine Description:

    This function performs the actual patch on the kernel or driver code.

Arguments:

    PatchHandle - Supplies the handle for the patch module.
        
    ImageBaseAddress - Supplies the base address for the patch module.  Note
                       that the contents of the patch image include the
                       names of the target drivers to be patched.
        
    PatchFlags - Supplies the flags for the patch being applied.
        
Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  Normal APCs disabled (system load mutant is held).

--*/

{
    PHOTPATCH_HEADER Patch;
    PRTL_PATCH_HEADER RtlPatchData;
    PKLDR_DATA_TABLE_ENTRY DataTableEntry = NULL;
    NTSTATUS Status;
    LOGICAL FirstLoad;
    PVOID KernelMappedAddress;
    PVOID KernelLockVariable;
    PLIST_ENTRY Next;

    Patch = RtlGetHotpatchHeader(ImageBaseAddress);

    if (Patch == NULL) {

        return (ULONG)STATUS_INVALID_IMAGE_FORMAT;
    }

    // 
    // The caller loaded the patch driver (if it was not already loaded).
    //
    // Check whether the patch has *EVER* been applied.  It's only in the
    // list if it has.  This means being in the list says it may be active
    // OR inactive right now.
    //
    
    RtlPatchData = RtlFindRtlPatchHeader (&MiHotPatchList, PatchHandle);

    if (RtlPatchData == NULL) {

        if (!(PatchFlags & FLG_HOTPATCH_ACTIVE)) {

            return STATUS_NOT_SUPPORTED;
        }

        Status = RtlCreateHotPatch (&RtlPatchData, Patch, PatchHandle, PatchFlags);

        if (!NT_SUCCESS(Status)) {

            return Status;
        }

        //
        // Walk the table entry list to find the target driver that needs to
        // be patched.
        //

        ExAcquireResourceExclusiveLite (&PsLoadedModuleResource, TRUE);

        Next = PsLoadedModuleList.Flink;

        for ( ; Next != &PsLoadedModuleList; Next = Next->Flink) {

            DataTableEntry = CONTAINING_RECORD (Next,
                                                KLDR_DATA_TABLE_ENTRY,
                                                InLoadOrderLinks);

            //
            // Skip the session images because they are generally copy on
            // write (barring address collisions) and will need a different
            // mechanism to update.
            //

            if (MI_IS_SESSION_IMAGE_ADDRESS (DataTableEntry->DllBase)) {
                continue;
            }

            if (RtlpIsSameImage(RtlPatchData, DataTableEntry)) {

                break;
            }
        }

        ExReleaseResourceLite (&PsLoadedModuleResource);
        
        //
        // The target DLL is not loaded, just return the status.
        //

        if (RtlPatchData->TargetDllBase == NULL) {

            RtlFreeHotPatchData(RtlPatchData);
            return STATUS_DLL_NOT_FOUND;
        }

        //
        // Create the new rtl patch structure here.
        // This requires some relocation info to be processed,
        // so we need to allow write access to the patch DLL.
        //

        Status = ExLockUserBuffer ((PVOID)PatchHandle->DllBase,
                                   PatchHandle->SizeOfImage, 
                                   KernelMode,
                                   IoWriteAccess,
                                   &KernelMappedAddress,
                                   &KernelLockVariable);

        if (!NT_SUCCESS(Status)) {
            RtlFreeHotPatchData(RtlPatchData);
            return Status;
        }

        Status = RtlInitializeHotPatch( RtlPatchData,
                                        (ULONG_PTR)KernelMappedAddress - (ULONG_PTR)PatchHandle->DllBase);

        //
        // Release the locked pages and system PTE alternate address.
        //

        ExUnlockUserBuffer (KernelLockVariable);

        if (!NT_SUCCESS(Status)) {
            RtlFreeHotPatchData(RtlPatchData);
            return Status;
        }

        FirstLoad = TRUE;
    }
    else {
        
        //
        // The patch has already been applied.  It may currently be enabled
        // OR disabled.  We allow changing the state, as well as reapplying
        // if the previous call failed for some code paths.
        //
        
        FirstLoad = FALSE;
        
        if (((PatchFlags ^ RtlPatchData->CodeInfo->Flags) & FLG_HOTPATCH_ACTIVE) == 0) {

            return STATUS_NOT_SUPPORTED;
        }
        
        //
        //  Rebuild the hook information, if the hotpatch was not active
        //

        if ((RtlPatchData->CodeInfo->Flags & FLG_HOTPATCH_ACTIVE) == 0) {

            Status = RtlReadHookInformation( RtlPatchData );

            if (!NT_SUCCESS(Status)) {

                return Status;
            }
        }
    }

    Status = MmLockAndCopyMemory (RtlPatchData->CodeInfo, KernelMode);

    if (NT_SUCCESS (Status)) {

        //
        // Add the patch to the driver's loader entry the first time
        // this patch is loaded.
        //

        if (FirstLoad == TRUE) {

            if (DataTableEntry->PatchInformation != NULL) {

                //
                // Push the new patch on the existing list.
                //

                RtlPatchData->NextPatch = (PRTL_PATCH_HEADER) DataTableEntry->PatchInformation;
            }
            else {

                //
                // First time the target driver has gotten any patch.
                // Fall through.
                //
            }

            DataTableEntry->PatchInformation = RtlPatchData;

            InsertTailList (&MiHotPatchList, &RtlPatchData->PatchList);
        }
    }
    else {
        if (FirstLoad == TRUE) {
            RtlFreeHotPatchData (RtlPatchData);
        }
    }
    
    return Status;
}


NTSTATUS
MmHotPatchRoutine (
    IN PSYSTEM_HOTPATCH_CODE_INFORMATION KernelPatchInfo
    )

/*++

Routine Description:

    This is the main routine responsible for kernel hotpatching.
    It loads the patch module in memory, initializes the patch information
    and finally applies the fixups to the existing code.
    
    NOTE: This function assumes that the KernelPatchInfo structure is properly 
          captured and validated

Arguments:

    KernelPatchInfo - Supplies a pointer to a kernel buffer containing
                      the image name of the patch.
        
Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  PASSIVE_LEVEL on entry.

--*/

{
    NTSTATUS Status;
    NTSTATUS PatchStatus;
    ULONG PatchFlags;
    PVOID ImageBaseAddress;
    PVOID ImageHandle;
    UNICODE_STRING PatchImageName;
    PKTHREAD CurrentThread;

    ASSERT (KeGetCurrentIrql () == PASSIVE_LEVEL);
    
    PatchImageName.Buffer = (PWCHAR)((PUCHAR)KernelPatchInfo + KernelPatchInfo->KernelInfo.NameOffset);
    PatchImageName.Length = KernelPatchInfo->KernelInfo.NameLength;
    PatchImageName.MaximumLength = PatchImageName.Length;
    PatchFlags = KernelPatchInfo->Flags;

    CurrentThread = KeGetCurrentThread ();

    KeEnterCriticalRegionThread (CurrentThread);

    //
    // Acquire the loader mutant because we may discover the patch we are
    // trying to load is already loaded.  And we want to prevent it from
    // being unloaded while we are using it.
    //

    KeWaitForSingleObject (&MmSystemLoadLock,
                           WrVirtualMemory,
                           KernelMode,
                           FALSE,
                           (PLARGE_INTEGER)NULL);
    
    Status = MmLoadSystemImage (&PatchImageName,
                                NULL,
                                NULL,
                                0,
                                &ImageHandle,
                                &ImageBaseAddress);

    if (NT_SUCCESS (Status) || (Status == STATUS_IMAGE_ALREADY_LOADED)) {

        PatchStatus = MiPerformHotPatch (ImageHandle,
                                         ImageBaseAddress,
                                         PatchFlags);

        if ((!NT_SUCCESS (PatchStatus)) &&
            (Status != STATUS_IMAGE_ALREADY_LOADED)) {

            //
            // Unload the patch DLL if applying the hotpatch failed and
            // we were the initial (and only) load of the patch DLL.
            //

            MmUnloadSystemImage (ImageHandle);
        }

        Status = PatchStatus;
    }

    KeReleaseMutant (&MmSystemLoadLock, 1, FALSE, FALSE);
    KeLeaveCriticalRegionThread (CurrentThread);

    return Status;
}

VOID
MiRundownHotpatchList (
    IN PRTL_PATCH_HEADER PatchHead
    )

/*++

Routine Description:

    The function walks the hotpatch list and unloads each patch module and
    free all data.

Arguments:

    PatchHead - Supplies a pointer to the head of the patch list.

Return Value:

    NTSTATUS.

Environment:

    Kernel mode.  System load lock held with APCs disabled.

--*/

{
    PRTL_PATCH_HEADER CrtPatch;

    SYSLOAD_LOCK_OWNED_BY_ME ();

    while (PatchHead) {
        
        CrtPatch = PatchHead;

        PatchHead = PatchHead->NextPatch;

        RemoveEntryList (&CrtPatch->PatchList);
        
        //
        // Unload all instances for this DLL.
        //
        
        if (CrtPatch->PatchLdrDataTableEntry) {
                
            MmUnloadSystemImage (CrtPatch->PatchLdrDataTableEntry);
        }
        
        RtlFreeHotPatchData (CrtPatch);
    }

    return;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\base\ntos\mm\mmfault.c ===
/*++

Copyright (c) 1989  Microsoft Corporation

Module Name:

   mmfault.c

Abstract:

    This module contains the handlers for access check, page faults
    and write faults.

Author:

    Lou Perazzoli (loup) 6-Apr-1989
    Landy Wang (landyw) 02-June-1997

Revision History:

--*/

#include "mi.h"

#define PROCESS_FOREGROUND_PRIORITY (9)

LONG MiDelayPageFaults;

#if DBG
ULONG MmProtoPteVadLookups = 0;
ULONG MmProtoPteDirect = 0;
ULONG MmAutoEvaluate = 0;

PMMPTE MmPteHit = NULL;
extern PVOID PsNtosImageEnd;
ULONG MmInjectUserInpageErrors;
ULONG MmInjectedUserInpageErrors;
ULONG MmInpageFraction = 0x1F;      // Fail 1 out of every 32 inpages.

#define MI_INPAGE_BACKTRACE_LENGTH 6

typedef struct _MI_INPAGE_TRACES {

    PVOID FaultingAddress;
    PETHREAD Thread;
    PVOID StackTrace [MI_INPAGE_BACKTRACE_LENGTH];

} MI_INPAGE_TRACES, *PMI_INPAGE_TRACES;

#define MI_INPAGE_TRACE_SIZE 64

LONG MiInpageIndex;

MI_INPAGE_TRACES MiInpageTraces[MI_INPAGE_TRACE_SIZE];

VOID
FORCEINLINE
MiSnapInPageError (
    IN PVOID FaultingAddress
    )
{
    PMI_INPAGE_TRACES Information;
    ULONG Index;
    ULONG Hash;

    Index = InterlockedIncrement(&MiInpageIndex);
    Index &= (MI_INPAGE_TRACE_SIZE - 1);
    Information = &MiInpageTraces[Index];

    Information->FaultingAddress = FaultingAddress;
    Information->Thread = PsGetCurrentThread ();

    RtlZeroMemory (&Information->StackTrace[0], MI_INPAGE_BACKTRACE_LENGTH * sizeof(PVOID)); 

    RtlCaptureStackBackTrace (0, MI_INPAGE_BACKTRACE_LENGTH, Information->StackTrace, &Hash);
}

#endif


NTSTATUS
MmAccessFault (
    IN ULONG_PTR FaultStatus,
    IN PVOID VirtualAddress,
    IN KPROCESSOR_MODE PreviousMode,
    IN PVOID TrapInformation
    )

/*++

Routine Description:

    This function is called by the kernel on data or instruction
    access faults.  The access fault was detected due to either
    an access violation, a PTE with the present bit clear, or a
    valid PTE with the dirty bit clear and a write operation.

    Also note that the access violation and the page fault could
    occur because of the Page Directory Entry contents as well.

    This routine determines what type of fault it is and calls
    the appropriate routine to handle the page fault or the write
    fault.

Arguments:

    FaultStatus - Supplies fault status information bits.

    VirtualAddress - Supplies the virtual address which caused the fault.

    PreviousMode - Supplies the mode (kernel or user) in which the fault
                   occurred.

    TrapInformation - Opaque information about the trap, interpreted by the
                      kernel, not Mm.  Needed to allow fast interlocked access
                      to operate correctly.

Return Value:

    Returns the status of the fault handling operation.  Can be one of:
        - Success.
        - Access Violation.
        - Guard Page Violation.
        - In-page Error.

Environment:

    Kernel mode.

--*/

{
    PMMVAD ProtoVad;
    PMMPTE PointerPxe;
    PMMPTE PointerPpe;
    PMMPTE PointerPde;
    PMMPTE PointerPte;
    PMMPTE PointerProtoPte;
    ULONG ProtectionCode;
    MMPTE TempPte;
    PEPROCESS CurrentProcess;
    KIRQL PreviousIrql;
    KIRQL WsIrql;
    NTSTATUS status;
    ULONG ProtectCode;
    PFN_NUMBER PageFrameIndex;
    WSLE_NUMBER WorkingSetIndex;
    KIRQL OldIrql;
    PMMPFN Pfn1;
    PPAGE_FAULT_NOTIFY_ROUTINE NotifyRoutine;
    PEPROCESS FaultProcess;
    PMMSUPPORT Ws;
    LOGICAL SessionAddress;
    PVOID UsedPageTableHandle;
    ULONG BarrierStamp;
    LOGICAL ApcNeeded;
    LOGICAL RecheckAccess;

    PointerProtoPte = NULL;

    //
    // If the address is not canonical then return FALSE as the caller (which
    // may be the kernel debugger) is not expecting to get an unimplemented
    // address bit fault.
    //

    if (MI_RESERVED_BITS_CANONICAL(VirtualAddress) == FALSE) {

        if (PreviousMode == UserMode) {
            return STATUS_ACCESS_VIOLATION;
        }

        if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
            return STATUS_ACCESS_VIOLATION;
        }

        KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                      (ULONG_PTR)VirtualAddress,
                      FaultStatus,
                      (ULONG_PTR)TrapInformation,
                      4);
    }

    PreviousIrql = KeGetCurrentIrql ();

    //
    // Get the pointer to the PDE and the PTE for this page.
    //

    PointerPte = MiGetPteAddress (VirtualAddress);
    PointerPde = MiGetPdeAddress (VirtualAddress);
    PointerPpe = MiGetPpeAddress (VirtualAddress);
    PointerPxe = MiGetPxeAddress (VirtualAddress);

#if DBG
    if (PointerPte == MmPteHit) {
        DbgPrint("MM: PTE hit at %p\n", MmPteHit);
        DbgBreakPoint();
    }
#endif

    if (PreviousIrql > APC_LEVEL) {

        //
        // The PFN database lock is an executive spin-lock.  The pager could
        // get dirty faults or lock faults while servicing and it already owns
        // the PFN database lock.
        //

#if (_MI_PAGING_LEVELS < 3)
        MiCheckPdeForPagedPool (VirtualAddress);
#endif

        if (
#if (_MI_PAGING_LEVELS >= 4)
            (PointerPxe->u.Hard.Valid == 0) ||
#endif
#if (_MI_PAGING_LEVELS >= 3)
            (PointerPpe->u.Hard.Valid == 0) ||
#endif
            (PointerPde->u.Hard.Valid == 0) ||

            ((!MI_PDE_MAPS_LARGE_PAGE (PointerPde)) && (PointerPte->u.Hard.Valid == 0))) {

            KdPrint(("MM:***PAGE FAULT AT IRQL > 1  Va %p, IRQL %lx\n",
                     VirtualAddress,
                     PreviousIrql));

            if (TrapInformation != NULL) {
                MI_DISPLAY_TRAP_INFORMATION (TrapInformation);
            }

            //
            // Signal the fatal error to the trap handler.
            //

            return STATUS_IN_PAGE_ERROR | 0x10000000;

        }

        if (MI_PDE_MAPS_LARGE_PAGE (PointerPde)) {
            return STATUS_SUCCESS;
        }

        if ((MI_FAULT_STATUS_INDICATES_WRITE (FaultStatus)) &&
            (PointerPte->u.Hard.CopyOnWrite != 0)) {

            KdPrint(("MM:***PAGE FAULT AT IRQL > 1  Va %p, IRQL %lx\n",
                     VirtualAddress,
                     PreviousIrql));

            if (TrapInformation != NULL) {
                MI_DISPLAY_TRAP_INFORMATION (TrapInformation);
            }

            //
            // use reserved bit to signal fatal error to trap handlers
            //

            return STATUS_IN_PAGE_ERROR | 0x10000000;
        }

        //
        // If PTE mappings with various protections are active and the faulting
        // address lies within these mappings, resolve the fault with
        // the appropriate protections.
        //

        if (!IsListEmpty (&MmProtectedPteList)) {

            if (MiCheckSystemPteProtection (
                                MI_FAULT_STATUS_INDICATES_WRITE (FaultStatus),
                                VirtualAddress) == TRUE) {

                return STATUS_SUCCESS;
            }
        }

        //
        // The PTE is valid and accessible, another thread must
        // have faulted the PTE in already, or the access bit
        // is clear and this is a access fault - blindly set the
        // access bit and dismiss the fault.
        //

        if (MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus)) {

            Pfn1 = MI_PFN_ELEMENT (PointerPte->u.Hard.PageFrameNumber);

            if (((PointerPte->u.Long & MM_PTE_WRITE_MASK) == 0) &&
                ((Pfn1->OriginalPte.u.Soft.Protection & MM_READWRITE) == 0)) {
                
                KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                              (ULONG_PTR)VirtualAddress,
                              (ULONG_PTR)PointerPte->u.Long,
                              (ULONG_PTR)TrapInformation,
                              10);
            }
        }

        //
        // Ensure execute access is enabled in the PTE.
        //

        if ((MI_FAULT_STATUS_INDICATES_EXECUTION (FaultStatus)) &&
            (!MI_IS_PTE_EXECUTABLE (PointerPte))) {

            KeBugCheckEx (ATTEMPTED_EXECUTE_OF_NOEXECUTE_MEMORY,
                          (ULONG_PTR)VirtualAddress,
                          (ULONG_PTR)PointerPte->u.Long,
                          (ULONG_PTR)TrapInformation,
                          0);
        }

        MI_NO_FAULT_FOUND (FaultStatus, PointerPte, VirtualAddress, FALSE);
        return STATUS_SUCCESS;
    }

    ApcNeeded = FALSE;
    WsIrql = MM_NOIRQL;

    if (VirtualAddress >= MmSystemRangeStart) {

        //
        // This is a fault in the system address space.  User
        // mode access is not allowed.
        //

        if (PreviousMode == UserMode) {
            return STATUS_ACCESS_VIOLATION;
        }

#if (_MI_PAGING_LEVELS >= 4)
        if (PointerPxe->u.Hard.Valid == 0) {

            if (KeInvalidAccessAllowed (TrapInformation) == TRUE) {
                return STATUS_ACCESS_VIOLATION;
            }

            KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                          (ULONG_PTR)VirtualAddress,
                          FaultStatus,
                          (ULONG_PTR)TrapInformation,
                          7);
        }
#endif

#if (_MI_PAGING_LEVELS >= 3)
        if (PointerPpe->u.Hard.Valid == 0) {

            if (KeInvalidAccessAllowed (TrapInformation) == TRUE) {
                return STATUS_ACCESS_VIOLATION;
            }

            KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                          (ULONG_PTR)VirtualAddress,
                          FaultStatus,
                          (ULONG_PTR)TrapInformation,
                          5);
        }
#endif

        if (PointerPde->u.Hard.Valid == 1) {

            if (MI_PDE_MAPS_LARGE_PAGE (PointerPde)) {
                return STATUS_SUCCESS;
            }

        }
        else {

#if (_MI_PAGING_LEVELS >= 3)
            if ((VirtualAddress >= (PVOID)PTE_BASE) &&
                (VirtualAddress < (PVOID)MiGetPteAddress (HYPER_SPACE))) {

                //
                // This is a user mode PDE entry being faulted in by the Mm
                // referencing the page table page.  This needs to be done
                // with the working set lock so that the PPE validity can be
                // relied on throughout the fault processing.
                //
                // The case when Mm faults in PPE entries by referencing the
                // page directory page is correctly handled by falling through
                // the below code.
                //
    
                goto UserFault;
            }

#if defined (_MIALT4K_)
            if ((VirtualAddress >= (PVOID)ALT4KB_PERMISSION_TABLE_START) && 
                (VirtualAddress < (PVOID)ALT4KB_PERMISSION_TABLE_END)) {

                if ((PMMPTE)VirtualAddress >= MmWorkingSetList->HighestAltPte) {

                    if (KeInvalidAccessAllowed (TrapInformation) == TRUE) {
                        return STATUS_ACCESS_VIOLATION;
                    }

                    KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                                  (ULONG_PTR)VirtualAddress,
                                  FaultStatus,
                                  (ULONG_PTR)TrapInformation,
                                  9);
                }

                goto UserFault;
            }
#endif

#else
            MiCheckPdeForPagedPool (VirtualAddress);
#endif

            if (PointerPde->u.Hard.Valid == 0) {

                if (KeInvalidAccessAllowed (TrapInformation) == TRUE) {
                    return STATUS_ACCESS_VIOLATION;
                }

                KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                              (ULONG_PTR)VirtualAddress,
                              FaultStatus,
                              (ULONG_PTR)TrapInformation,
                              2);
            }

            //
            // The PDE is now valid, the PTE can be examined below.
            //
        }

        SessionAddress = MI_IS_SESSION_ADDRESS (VirtualAddress);

        TempPte = *PointerPte;

        if (TempPte.u.Hard.Valid == 1) {

            //
            // If PTE mappings with various protections are active
            // and the faulting address lies within these mappings,
            // resolve the fault with the appropriate protections.
            //

            if (!IsListEmpty (&MmProtectedPteList)) {

                if (MiCheckSystemPteProtection (
                        MI_FAULT_STATUS_INDICATES_WRITE (FaultStatus),
                        VirtualAddress) == TRUE) {

                    return STATUS_SUCCESS;
                }
            }

            //
            // Ensure execute access is enabled in the PTE.
            //

            if ((MI_FAULT_STATUS_INDICATES_EXECUTION (FaultStatus)) &&
                (!MI_IS_PTE_EXECUTABLE (&TempPte))) {

                KeBugCheckEx (ATTEMPTED_EXECUTE_OF_NOEXECUTE_MEMORY,
                              (ULONG_PTR)VirtualAddress,
                              (ULONG_PTR)TempPte.u.Long,
                              (ULONG_PTR)TrapInformation,
                              1);
            }

            //
            // Session space faults cannot early exit here because
            // it may be a copy on write which must be checked for
            // and handled below.
            //

            if (SessionAddress == FALSE) {

                //
                // Acquire the PFN lock, check to see if the address is
                // still valid if writable, update dirty bit.
                //

                LOCK_PFN (OldIrql);

                TempPte = *PointerPte;

                if (TempPte.u.Hard.Valid == 1) {

                    Pfn1 = MI_PFN_ELEMENT (TempPte.u.Hard.PageFrameNumber);

                    if ((MI_FAULT_STATUS_INDICATES_WRITE(FaultStatus)) &&
                        ((TempPte.u.Long & MM_PTE_WRITE_MASK) == 0) &&
                        ((Pfn1->OriginalPte.u.Soft.Protection & MM_READWRITE) == 0)) {
            
                        KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                                      (ULONG_PTR)VirtualAddress,
                                      (ULONG_PTR)TempPte.u.Long,
                                      (ULONG_PTR)TrapInformation,
                                      11);
                    }
                    MI_NO_FAULT_FOUND (FaultStatus, PointerPte, VirtualAddress, TRUE);
                }
                UNLOCK_PFN (OldIrql);
                return STATUS_SUCCESS;
            }
        }

#if (_MI_PAGING_LEVELS < 3)

        //
        // Session data had its PDE validated above.  Session PTEs haven't
        // had the hardware PDE validated because the checks above would
        // have gone to the selfmap entry.
        //
        // So validate the real session PDE now if the fault was on a PTE.
        //

        if (MI_IS_SESSION_PTE (VirtualAddress)) {

            status = MiCheckPdeForSessionSpace (VirtualAddress);

            if (!NT_SUCCESS (status)) {

                //
                // This thread faulted on a session space access, but this
                // process does not have one.
                //
                // The system process which contains the worker threads
                // NEVER has a session space - if code accidentally queues a
                // worker thread that points to a session space buffer, a
                // fault will occur.
                //
                // The only exception to this is when the working set manager
                // attaches to a session to age or trim it.  However, the
                // working set manager will never fault and so the bugcheck
                // below is always valid.  Note that a worker thread can get
                // away with a bad access if it happens while the working set
                // manager is attached, but there's really no way to prevent
                // this case which is a driver bug anyway.
                //

                if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                    return STATUS_ACCESS_VIOLATION;
                }

                KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                              (ULONG_PTR)VirtualAddress,
                              FaultStatus,
                              (ULONG_PTR)TrapInformation,
                              6);
            }
        }

#endif

        //
        // Handle page table or hyperspace faults as if they were user faults.
        //

        if (MI_IS_PAGE_TABLE_OR_HYPER_ADDRESS (VirtualAddress)) {

#if (_MI_PAGING_LEVELS < 3)

            if (MiCheckPdeForPagedPool (VirtualAddress) == STATUS_WAIT_1) {
                return STATUS_SUCCESS;
            }

#endif

            goto UserFault;
        }

        //
        //
        // Acquire the relevant working set mutex.  While the mutex
        // is held, this virtual address cannot go from valid to invalid.
        //
        // Fall though to further fault handling.
        //

        if (SessionAddress == FALSE) {
            FaultProcess = NULL;
            Ws = &MmSystemCacheWs;
        }
        else {
            FaultProcess = HYDRA_PROCESS;
            Ws = &MmSessionSpace->GlobalVirtualAddress->Vm;
        }

        if (KeGetCurrentThread () == KeGetOwnerGuardedMutex (&Ws->WorkingSetMutex)) {

            if (KeInvalidAccessAllowed (TrapInformation) == TRUE) {
                return STATUS_ACCESS_VIOLATION;
            }

            //
            // Recursively trying to acquire the system or session working set
            // mutex - cause an IRQL > 1 bug check.
            //

            return STATUS_IN_PAGE_ERROR | 0x10000000;
        }

        //
        // Explicitly raise irql because MiDispatchFault will need
        // to release the working set mutex if an I/O is needed.
        //
        // Since releasing the mutex lowers irql to the value stored
        // in the mutex, we must ensure the stored value is at least
        // APC_LEVEL.  Otherwise a kernel special APC (which can
        // reference paged pool) can arrive when the mutex is released
        // which is before the I/O is actually queued --> ie: deadlock!
        //

        KeRaiseIrql (APC_LEVEL, &WsIrql);
        LOCK_WORKING_SET (Ws);

        TempPte = *PointerPte;

        if (TempPte.u.Hard.Valid != 0) {

            //
            // The PTE is already valid, this must be an access, dirty or
            // copy on write fault.
            //

            if ((MI_FAULT_STATUS_INDICATES_WRITE (FaultStatus)) &&
                ((TempPte.u.Long & MM_PTE_WRITE_MASK) == 0) &&
                (TempPte.u.Hard.CopyOnWrite == 0)) {

                Pfn1 = MI_PFN_ELEMENT (MI_GET_PAGE_FRAME_FROM_PTE (&TempPte));

                if ((Pfn1->OriginalPte.u.Soft.Protection & MM_READWRITE) == 0) {
        
                    PLIST_ENTRY NextEntry;
                    PIMAGE_ENTRY_IN_SESSION Image;

                    Image = (PIMAGE_ENTRY_IN_SESSION) -1;

                    if (SessionAddress == TRUE) {

                        NextEntry = MmSessionSpace->ImageList.Flink;

                        while (NextEntry != &MmSessionSpace->ImageList) {

                            Image = CONTAINING_RECORD (NextEntry, IMAGE_ENTRY_IN_SESSION, Link);

                            if ((VirtualAddress >= Image->Address) &&
                                (VirtualAddress <= Image->LastAddress)) {

                                if (Image->ImageLoading) {

                                    ASSERT (Pfn1->u3.e1.PrototypePte == 1);

                                    TempPte.u.Hard.CopyOnWrite = 1;

                                    //
                                    // Temporarily make the page copy on write,
                                    // this will be stripped shortly and made
                                    // fully writable.  No TB flush is necessary
                                    // as the PTE will be processed below.
                                    //
                                    // Even though the page's current backing
                                    // is the image file, the modified writer
                                    // will convert it to pagefile backing
                                    // when it notices the change later.
                                    //

                                    MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, TempPte);
                                    Image = NULL;

                                }
                                break;
                            }
                            NextEntry = NextEntry->Flink;
                        }
                    }

                    if (Image != NULL) {
                        KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                                      (ULONG_PTR)VirtualAddress,
                                      (ULONG_PTR)TempPte.u.Long,
                                      (ULONG_PTR)TrapInformation,
                                      12);
                    }
                }
            }

            //
            // Ensure execute access is enabled in the PTE.
            //

            if ((MI_FAULT_STATUS_INDICATES_EXECUTION (FaultStatus)) &&
                (!MI_IS_PTE_EXECUTABLE (&TempPte))) {

                KeBugCheckEx (ATTEMPTED_EXECUTE_OF_NOEXECUTE_MEMORY,
                              (ULONG_PTR)VirtualAddress,
                              (ULONG_PTR)TempPte.u.Long,
                              (ULONG_PTR)TrapInformation,
                              2);
            }

            //
            // Set the dirty bit in the PTE and the page frame.
            //

            if ((SessionAddress == TRUE) &&
                (MI_FAULT_STATUS_INDICATES_WRITE (FaultStatus)) &&
                (TempPte.u.Hard.Write == 0)) {

                //
                // Check for copy-on-write.
                //
    
                ASSERT (MI_IS_SESSION_IMAGE_ADDRESS (VirtualAddress));

                if (TempPte.u.Hard.CopyOnWrite == 0) {
            
                    KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                                  (ULONG_PTR)VirtualAddress,
                                  (ULONG_PTR)TempPte.u.Long,
                                  (ULONG_PTR)TrapInformation,
                                  13);
                }
    
                MiCopyOnWrite (VirtualAddress, PointerPte);
            }
            else {
                LOCK_PFN (OldIrql);
                MI_NO_FAULT_FOUND (FaultStatus, PointerPte, VirtualAddress, TRUE);
                UNLOCK_PFN (OldIrql);
            }

            UNLOCK_WORKING_SET (Ws);
            ASSERT (WsIrql != MM_NOIRQL);
            KeLowerIrql (WsIrql);
            return STATUS_SUCCESS;
        }

        if (TempPte.u.Soft.Prototype != 0) {

            if ((MmProtectFreedNonPagedPool == TRUE) &&

                ((VirtualAddress >= MmNonPagedPoolStart) &&
                 (VirtualAddress < (PVOID)((ULONG_PTR)MmNonPagedPoolStart + MmSizeOfNonPagedPoolInBytes))) ||

                ((VirtualAddress >= MmNonPagedPoolExpansionStart) &&
                 (VirtualAddress < MmNonPagedPoolEnd))) {

                //
                // This is an access to previously freed
                // non paged pool - bugcheck!
                //

                if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                    goto KernelAccessViolation;
                }

                KeBugCheckEx (DRIVER_CAUGHT_MODIFYING_FREED_POOL,
                              (ULONG_PTR) VirtualAddress,
                              FaultStatus,
                              PreviousMode,
                              4);
            }

            //
            // This is a PTE in prototype format, locate the corresponding
            // prototype PTE.
            //

            PointerProtoPte = MiPteToProto (&TempPte);

            if ((SessionAddress == TRUE) &&
                (TempPte.u.Soft.PageFileHigh == MI_PTE_LOOKUP_NEEDED)) {

                PointerProtoPte = MiCheckVirtualAddress (VirtualAddress,
                                                         &ProtectionCode,
                                                         &ProtoVad);
                if (PointerProtoPte == NULL) {
                    UNLOCK_WORKING_SET (Ws);
                    ASSERT (WsIrql != MM_NOIRQL);
                    KeLowerIrql (WsIrql);
                    return STATUS_IN_PAGE_ERROR | 0x10000000;
                }
            }
        }
        else if ((TempPte.u.Soft.Transition == 0) &&
                 (TempPte.u.Soft.Protection == 0)) {

            //
            // Page file format.  If the protection is ZERO, this
            // is a page of free system PTEs - bugcheck!
            //

            if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                goto KernelAccessViolation;
            }

            KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                          (ULONG_PTR)VirtualAddress,
                          FaultStatus,
                          (ULONG_PTR)TrapInformation,
                          0);
        }
        else if (TempPte.u.Soft.Protection == MM_NOACCESS) {

            if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                goto KernelAccessViolation;
            }

            KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                          (ULONG_PTR)VirtualAddress,
                          FaultStatus,
                          (ULONG_PTR)TrapInformation,
                          1);
        }
        else if (TempPte.u.Soft.Protection == MM_KSTACK_OUTSWAPPED) {

            if (KeInvalidAccessAllowed(TrapInformation) == TRUE) {
                goto KernelAccessViolation;
            }

            KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                          (ULONG_PTR)VirtualAddress,
                          FaultStatus,
                          (ULONG_PTR)TrapInformation,
                          3);
        }

        if ((MI_FAULT_STATUS_INDICATES_WRITE (FaultStatus)) &&
            (PointerProtoPte == NULL) &&
            (SessionAddress == FALSE) &&
            (TempPte.u.Hard.Valid == 0)) {

            if (TempPte.u.Soft.Transition == 1) {
                ProtectionCode = (ULONG) TempPte.u.Trans.Protection;
            }
            else {
                ProtectionCode = (ULONG) TempPte.u.Soft.Protection;
            }
                
            if ((ProtectionCode & MM_READWRITE) == 0) {

                KeBugCheckEx (ATTEMPTED_WRITE_TO_READONLY_MEMORY,
                              (ULONG_PTR)VirtualAddress,
                              (ULONG_PTR)TempPte.u.Long,
                              (ULONG_PTR)TrapInformation,
                              14);
            }
        }

        status = MiDispatchFault (FaultStatus,
                                  VirtualAddress,
                                  PointerPte,
                                  PointerProtoPte,
                                  FALSE,
                                  FaultProcess,
                                  NULL,
                                  &ApcNeeded);

        ASSERT (ApcNeeded == FALSE);
        ASSERT (KeAreAllApcsDisabled () == TRUE);

        if (Ws->Flags.GrowWsleHash == 1) {
            MiGrowWsleHash (Ws);
        }

        UNLOCK_WORKING_SET (Ws);

        ASSERT (WsIrql != MM_NOIRQL);
        KeLowerIrql (WsIrql);

        if (((Ws->PageFaultCount & 0xFFF) == 0) &&
            (MmAvailablePages < MM_PLENTY_FREE_LIMIT)) {

            //
            // The system cache or this session is taking too many faults,
            // delay execution so the modified page writer gets a quick shot.
            //

            if (PsGetCurrentThread()->MemoryMaker == 0) {

                KeDelayExecutionThread (KernelMode,
                                        FALSE,
                                        (PLARGE_INTEGER) &MmShortTime);
            }
        }
        goto ReturnStatus3;
    }

UserFault:

    //
    // FAULT IN USER SPACE OR PAGE DIRECTORY/PAGE TABLE PAGES.
    //

    CurrentProcess = PsGetCurrentProcess ();

    if (MiDelayPageFaults ||
        ((MmModifiedPageListHead.Total >= (MmModifiedPageMaximum + 100)) &&
        (MmAvailablePages < (1024*1024 / PAGE_SIZE)) &&
            (CurrentProcess->ModifiedPageCount > ((64*1024)/PAGE_SIZE)))) {

        //
        // This process has placed more than 64k worth of pages on the modified
        // list.  Delay for a short period and set the count to zero.
        //

        KeDelayExecutionThread (KernelMode,
                                FALSE,
             (CurrentProcess->Pcb.BasePriority < PROCESS_FOREGROUND_PRIORITY) ?
                                    (PLARGE_INTEGER)&MmHalfSecond : (PLARGE_INTEGER)&Mm30Milliseconds);
        CurrentProcess->ModifiedPageCount = 0;
    }

#if DBG
    if ((PreviousMode == KernelMode) &&
        (PAGE_ALIGN(VirtualAddress) != (PVOID) MM_SHARED_USER_DATA_VA)) {

        if ((MmInjectUserInpageErrors & 0x2) ||
            (CurrentProcess->Flags & PS_PROCESS_INJECT_INPAGE_ERRORS)) {

            LARGE_INTEGER CurrentTime;
            ULONG_PTR InstructionPointer;

            KeQueryTickCount(&CurrentTime);

            if ((CurrentTime.LowPart & MmInpageFraction) == 0) {

                if (TrapInformation != NULL) {
#if defined(_X86_)
                    InstructionPointer = ((PKTRAP_FRAME)TrapInformation)->Eip;
#elif defined(_IA64_)
                    InstructionPointer = ((PKTRAP_FRAME)TrapInformation)->StIIP;
#elif defined(_AMD64_)
                    InstructionPointer = ((PKTRAP_FRAME)TrapInformation)->Rip;
#else
                    error
#endif

                    if (MmInjectUserInpageErrors & 0x1) {
                        MmInjectedUserInpageErrors += 1;
                        MiSnapInPageError (VirtualAddress);
                        status = STATUS_DEVICE_NOT_CONNECTED;
                        LOCK_WS (CurrentProcess);
                        goto ReturnStatus2;
                    }

                    if ((InstructionPointer >= (ULONG_PTR) PsNtosImageBase) &&
                        (InstructionPointer < (ULONG_PTR) PsNtosImageEnd)) {

                        MmInjectedUserInpageErrors += 1;
                        MiSnapInPageError (VirtualAddress);
                        status = STATUS_DEVICE_NOT_CONNECTED;
                        LOCK_WS (CurrentProcess);
                        goto ReturnStatus2;
                    }
                }
            }
        }
    }
#endif

    //
    // Block APCs and acquire the working set mutex.  This prevents any
    // changes to the address space and it prevents valid PTEs from becoming
    // invalid.
    //

    LOCK_WS (CurrentProcess);

#if (_MI_PAGING_LEVELS >= 4)

    //
    // Locate the Extended Page Directory Parent Entry which maps this virtual
    // address and check for accessibility and validity.  The page directory
    // parent page must be made valid before any other checks are made.
    //

    if (PointerPxe->u.Hard.Valid == 0) {

        //
        // If the PXE is zero, check to see if there is a virtual address
        // mapped at this location, and if so create the necessary
        // structures to map it.
        //

        if ((PointerPxe->u.Long == MM_ZERO_PTE) ||
            (PointerPxe->u.Long == MM_ZERO_KERNEL_PTE)) {

            MiCheckVirtualAddress (VirtualAddress, &ProtectCode, &ProtoVad);

            if (ProtectCode == MM_NOACCESS) {

                status = STATUS_ACCESS_VIOLATION;

                MI_BREAK_ON_AV (VirtualAddress, 0);

                goto ReturnStatus2;

            }

            //
            // Build a demand zero PXE and operate on it.
            //

#if (_MI_PAGING_LEVELS > 4)
            ASSERT (FALSE);     // UseCounts will need to be kept.
#endif

            MI_WRITE_INVALID_PTE (PointerPxe, DemandZeroPde);
        }

        //
        // The PXE is not valid, call the page fault routine passing
        // in the address of the PXE.  If the PXE is valid, determine
        // the status of the corresponding PPE.
        //
        // Note this call may result in ApcNeeded getting set to TRUE.
        // This is deliberate as there may be another call to MiDispatchFault
        // issued later in this routine and we don't want to lose the APC
        // status.
        //

        status = MiDispatchFault (TRUE,  //page table page always written
                                  PointerPpe,   // Virtual address
                                  PointerPxe,   // PTE (PXE in this case)
                                  NULL,
                                  FALSE,
                                  CurrentProcess,
                                  NULL,
                                  &ApcNeeded);

#if DBG
        if (ApcNeeded == TRUE) {
            ASSERT (PsGetCurrentThread()->NestedFaultCount == 0);
            ASSERT (PsGetCurrentThread()->ApcNeeded == 0);
        }
#endif

        ASSERT (KeAreAllApcsDisabled () == TRUE);
        if (PointerPxe->u.Hard.Valid == 0) {

            //
            // The PXE is not valid, return the status.
            //

            goto ReturnStatus1;
        }

        MI_SET_PAGE_DIRTY (PointerPxe, PointerPde, FALSE);

        //
        // Now that the PXE is accessible, fall through and get the PPE.
        //
    }
#endif

#if (_MI_PAGING_LEVELS >= 3)

    //
    // Locate the Page Directory Parent Entry which maps this virtual
    // address and check for accessibility and validity.  The page directory
    // page must be made valid before any other checks are made.
    //

    if (PointerPpe->u.Hard.Valid == 0) {

        //
        // If the PPE is zero, check to see if there is a virtual address
        // mapped at this location, and if so create the necessary
        // structures to map it.
        //

        if ((PointerPpe->u.Long == MM_ZERO_PTE) ||
            (PointerPpe->u.Long == MM_ZERO_KERNEL_PTE)) {

            MiCheckVirtualAddress (VirtualAddress, &ProtectCode, &ProtoVad);

            if (ProtectCode == MM_NOACCESS) {

                status = STATUS_ACCESS_VIOLATION;

                MI_BREAK_ON_AV (VirtualAddress, 1);

                goto ReturnStatus2;

            }

#if (_MI_PAGING_LEVELS >= 4)

            //
            // Increment the count of non-zero page directory parent entries
            // for this page directory parent.
            //

            if (VirtualAddress <= MM_HIGHEST_USER_ADDRESS) {
                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPde);
                MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
            }
#endif

            //
            // Build a demand zero PPE and operate on it.
            //

            MI_WRITE_INVALID_PTE (PointerPpe, DemandZeroPde);
        }

        //
        // The PPE is not valid, call the page fault routine passing
        // in the address of the PPE.  If the PPE is valid, determine
        // the status of the corresponding PDE.
        //
        // Note this call may result in ApcNeeded getting set to TRUE.
        // This is deliberate as there may be another call to MiDispatchFault
        // issued later in this routine and we don't want to lose the APC
        // status.
        //

        status = MiDispatchFault (TRUE,  //page table page always written
                                  PointerPde,   //Virtual address
                                  PointerPpe,   // PTE (PPE in this case)
                                  NULL,
                                  FALSE,
                                  CurrentProcess,
                                  NULL,
                                  &ApcNeeded);

#if DBG
        if (ApcNeeded == TRUE) {
            ASSERT (PsGetCurrentThread()->NestedFaultCount == 0);
            ASSERT (PsGetCurrentThread()->ApcNeeded == 0);
        }
#endif

        ASSERT (KeAreAllApcsDisabled () == TRUE);
        if (PointerPpe->u.Hard.Valid == 0) {

            //
            // The PPE is not valid, return the status.
            //

            goto ReturnStatus1;
        }

        MI_SET_PAGE_DIRTY (PointerPpe, PointerPde, FALSE);

        //
        // Now that the PPE is accessible, fall through and get the PDE.
        //
    }
#endif

    //
    // Locate the Page Directory Entry which maps this virtual
    // address and check for accessibility and validity.
    //

    //
    // Check to see if the page table page (PDE entry) is valid.
    // If not, the page table page must be made valid first.
    //

    if (PointerPde->u.Hard.Valid == 0) {

        //
        // If the PDE is zero, check to see if there is a virtual address
        // mapped at this location, and if so create the necessary
        // structures to map it.
        //

        if ((PointerPde->u.Long == MM_ZERO_PTE) ||
            (PointerPde->u.Long == MM_ZERO_KERNEL_PTE)) {

            MiCheckVirtualAddress (VirtualAddress, &ProtectCode, &ProtoVad);

            if (ProtectCode == MM_NOACCESS) {

                status = STATUS_ACCESS_VIOLATION;

#if (_MI_PAGING_LEVELS < 3)

                MiCheckPdeForPagedPool (VirtualAddress);

                if (PointerPde->u.Hard.Valid == 1) {
                    status = STATUS_SUCCESS;
                }

#endif

                if (status == STATUS_ACCESS_VIOLATION) {
                    MI_BREAK_ON_AV (VirtualAddress, 2);
                }

                goto ReturnStatus2;

            }

#if (_MI_PAGING_LEVELS >= 3)

            //
            // Increment the count of non-zero page directory entries for this
            // page directory.
            //

            if (VirtualAddress <= MM_HIGHEST_USER_ADDRESS) {
                UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPte);
                MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
            }
#if (_MI_PAGING_LEVELS >= 4)
            else if (MI_IS_PAGE_TABLE_ADDRESS(VirtualAddress)) {
                PVOID RealVa;

                RealVa = MiGetVirtualAddressMappedByPte(VirtualAddress);

                if (RealVa <= MM_HIGHEST_USER_ADDRESS) {

                    //
                    // This is really a page directory page.  Increment the
                    // use count on the appropriate page directory.
                    //

                    UsedPageTableHandle = MI_GET_USED_PTES_HANDLE (PointerPte);
                    MI_INCREMENT_USED_PTES_BY_HANDLE (UsedPageTableHandle);
                }
            }
#endif
#endif
            //
            // Build a demand zero PDE and operate on it.
            //

            MI_WRITE_INVALID_PTE (PointerPde, DemandZeroPde);
        }

        //
        // The PDE is not valid, call the page fault routine passing
        // in the address of the PDE.  If the PDE is valid, determine
        // the status of the corresponding PTE.
        //

        status = MiDispatchFault (TRUE,  //page table page always written
                                  PointerPte,   //Virtual address
                                  PointerPde,   // PTE (PDE in this case)
                                  NULL,
                                  FALSE,
                                  CurrentProcess,
                                  NULL,
                                  &ApcNeeded);

#if DBG
        if (ApcNeeded == TRUE) {
            ASSERT (PsGetCurrentThread()->NestedFaultCount == 0);
            ASSERT (PsGetCurrentThread()->ApcNeeded == 0);
        }
#endif

        ASSERT (KeAreAllApcsDisabled () == TRUE);

#if (_MI_PAGING_LEVELS >= 4)

        //
        // Note that the page directory parent page itself could have been
        // paged out or deleted while MiDispatchFault was executing without
        // the working set lock, so this must be checked for here in the PXE.
        //

        if (PointerPxe->u.Hard.Valid == 0) {

            //
            // The PXE is not valid, return the status.
            //

            goto ReturnStatus1;
        }
#endif

#if (_MI_PAGING_LEVELS >= 3)

        //
        // Note that the page directory page itself could have been paged out
        // or deleted while MiDispatchFault was executing without the working
        // set lock, so this must be checked for here in the PPE.
        //

        if (PointerPpe->u.Hard.Valid == 0) {

            //
            // The PPE is not valid, return the status.
            //

            goto ReturnStatus1;
        }
#endif

        if (PointerPde->u.Hard.Valid == 0) {

            //
            // The PDE is not valid, return the status.
            //

            goto ReturnStatus1;
        }

        MI_SET_PAGE_DIRTY (PointerPde, PointerPte, FALSE);

        //
        // Now that the PDE is accessible, get the PTE - let this fall
        // through.
        //
    }
    else if (MI_PDE_MAPS_LARGE_PAGE (PointerPde)) {
        status = STATUS_SUCCESS;
        goto ReturnStatus1;
    }

    //
    // The PDE is valid and accessible, get the PTE contents.
    //

    TempPte = *PointerPte;
    if (TempPte.u.Hard.Valid != 0) {

        if (MI_PDE_MAPS_LARGE_PAGE (PointerPte)) {

#if defined (_MIALT4K_)
            if ((CurrentProcess->Wow64Process != NULL) &&
                (VirtualAddress < MmSystemRangeStart)) {

                //
                // Alternate PTEs for emulated processes share the same
                // encoding as large pages so for these it's ok to proceed.
                //

                NOTHING;
            }
            else {
                
                //
                // This may be a 64-bit process that's loaded a 32-bit DLL as
                // an image and is cracking the DLL PE header, etc.  Since
                // the most relaxed permissions are already in the native PTE
                // just strip the split permissions bit.
                //

                MI_ENABLE_CACHING (TempPte);

                MI_WRITE_VALID_PTE_NEW_PROTECTION (PointerPte, TempPte);

                status = STATUS_SUCCESS;

                goto ReturnStatus2;
            }
#else
            KeBugCheckEx (PAGE_FAULT_IN_NONPAGED_AREA,
                          (ULONG_PTR)VirtualAddress,
                          FaultStatus,
                          (ULONG_PTR)TrapInformation,
                          8);
#endif
        }

        //
        // The PTE is valid and accessible : this is either a copy on write
        // fault or an accessed/modified bit that needs to be set.
        //

#if DBG
        if (MmDebug & MM_DBG_PTE_UPDATE) {
            MiFormatPte (PointerPte);
        }
#endif

        status = STATUS_SUCCESS;

        if (MI_FAULT_STATUS_INDICATES_WRITE (FaultStatus)) {

            //
            // This was a write operation.  If the copy on write
            // bit is set in the PTE perform the copy on write,
            // else check to ensure write access to the PTE.
            //

            if (TempPte.u.Hard.CopyOnWrite != 0) {

                MiCopyOnWrite (VirtualAddress, PointerPte);

                status = STATUS_PAGE_FAULT_COPY_ON_WRITE;
                goto ReturnStatus2;
            }

            if (TempPte.u.Hard.Write == 0) {
                status = STATUS_ACCESS_VIOLATION;
                MI_BREAK_ON_AV (VirtualAddress, 3);
            }
        }
        else if (MI_FAULT_STATUS_INDICATES_EXECUTION (FaultStatus)) {

            //
            // Ensure execute access is enabled in the PTE.
            //

            if (!MI_IS_PTE_EXECUTABLE (&TempPte)) {
                status = STATUS_ACCESS_VIOLATION;
                MI_BREAK_ON_AV (VirtualAddress, 4);
            }
        }
        else {

            //
            // The PTE is valid and accessible, another thread must
            // have updated the PTE already, or the access/modified bits
            // are clear and need to be updated.
            //

#if DBG
            if (MmDebug & MM_DBG_SHOW_FAULTS) {
                DbgPrint("MM:no fault found - PTE is %p\n", PointerPte->u.Long);
            }
#endif
        }

        if (status == STATUS_SUCCESS) {
#if defined(_X86_) || defined(_AMD64_)
#if !defined(NT_UP)

            ASSERT (PointerPte->u.Hard.Valid != 0);
            ASSERT (MI_GET_PAGE_FRAME_FROM_PTE (PointerPte) == MI_GET_PAGE_FRAME_FROM_PTE (&TempPte));

            //
            // The access bit is set (and TB inserted) automatically by the
            // processor if the valid bit is set so there is no need to
            // set it here in software.
            //
            // The modified bit is al