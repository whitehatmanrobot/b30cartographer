		{
		const STATE sReal = GetState_( ipageT );
		if( sFree == sReal )
			{
			const CHAR * rgsz[2];
			INT irgsz = 0;

			CHAR szStateExpected[16];
			CHAR szStateActual[16];

			sprintf( szStateExpected, "%d", sFree );
			sprintf( szStateActual, "%d", sReal );
			
			rgsz[irgsz++] = szStateExpected;
			rgsz[irgsz++] = szStateActual;
			
			UtilReportEvent(	eventError,
								DATABASE_CORRUPTION_CATEGORY,
								CORRUPT_SLV_SPACE_ID,
								irgsz,
								rgsz );
								
			EnforceSz( fFalse, "SLV Space Map corrupted" );
			}
		}
#endif	//	DEBUG

	SetState_( ipage, cpg, sFree );
	ChangeCpgAvail_( cpg );	

#ifdef DEBUG
	AssertPageState( ipage, cpg, sFree );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::FreeReserved( const LONG ipage, const LONG cpg, LONG * const pcpgFreed )
//  ================================================================
//
//  When freeing from the reserved state some of the pages in the 
//  SLVSPACENODE may have been freed already by a rollback from the
//  committed state.
//
//-
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	

	LONG cpgActual = 0;
	INT ipageT;
	for( ipageT = ipage; ipageT < ipage + cpg; ++ipageT )
		{
		if( sFree != GetState_( ipageT ) )
			{
			++cpgActual;
			}
		}

	SetState_( ipage, cpg, sFree );
	ChangeCpgAvail_( cpgActual );	
	*pcpgFreed = cpgActual;

#ifdef DEBUG
	AssertPageState( ipage, cpg, sFree );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::Reserve( const LONG ipage, const LONG cpg )
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	

#ifdef DEBUG	//	checked already by "CheckXXX" version of this method
	EnforcePageState( ipage, cpg, sFree );
#endif	//	DEBUG
	
	SetState_( ipage, cpg, sReserved );
	ChangeCpgAvail_( -cpg );	
	
#ifdef DEBUG
	AssertPageState( ipage, cpg, sReserved );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::CommitFromFree( const LONG ipage, const LONG cpg )
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
#ifdef DEBUG	//	checked already by "CheckXXX" version of this method
	EnforcePageState( ipage, cpg, sFree );
#endif	//	DEBUG

	SetState_( ipage, cpg, sCommitted );
	ChangeCpgAvail_( -cpg );	

#ifdef DEBUG
	AssertPageState( ipage, cpg, sCommitted );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::CommitFromReserved( const LONG ipage, const LONG cpg )
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
#ifdef DEBUG	//	checked already by "CheckXXX" version of this method
	EnforcePageState( ipage, cpg, sReserved );
#endif	//	DEBUG

	SetState_( ipage, cpg, sCommitted );

#ifdef DEBUG
	AssertPageState( ipage, cpg, sCommitted );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::CommitFromDeleted( const LONG ipage, const LONG cpg )
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
#ifdef DEBUG	//	checked already by "CheckXXX" version of this method
	EnforcePageState( ipage, cpg, sDeleted );
#endif	//	DEBUG

	SetState_( ipage, cpg, sCommitted );

#ifdef DEBUG
	AssertPageState( ipage, cpg, sCommitted );
#endif	//	DEBUG
	ASSERT_VALID( this );
	}


//  ================================================================
VOID SLVSPACENODE::DeleteFromCommitted( const LONG ipage, const LONG cpg )
//  ================================================================
	{
	Assert( ipage >= 0 );
	Assert( ipage < SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	ASSERT_VALID( this );	
	
#ifdef DEBUG	//	checked already by "CheckXXX" version of this method
	EnforcePageState( ipage, cpg, sCommitted );
#endif	//	DEBUG

	SetState_( ipage, cpg, sDeleted );
	ASSERT_VALID( this );
	}

//  ================================================================
SLVSPACENODE::STATE SLVSPACENODE::GetState_( const LONG ipage ) const
//  ================================================================
	{
	const INT ib 			= ipage / cpagesPerByte;
	const INT shf 			= ( ipage % cpagesPerByte ) * cbitsPerPage;
	const BYTE b			= m_rgbMap[ib];
	const STATE state		= (STATE)( ( b >> shf ) & 0x3 );
	return state;
	}

	
//  ================================================================
VOID SLVSPACENODE::SetState_( const LONG ipage, const LONG cpg, const STATE state )
//  ================================================================
	{
	Assert( state >= 0 );
	Assert( state <= 3 );

	INT ipageT;
	for( ipageT = ipage; ipageT < ipage + cpg; ++ipageT )
		{
		const INT ib 			= ipageT / cpagesPerByte;
		const INT shf 			= ( ipageT % cpagesPerByte ) * cbitsPerPage;
		Assert( 0 <= shf );
		Assert( shf <= 6 );
		const BYTE b			= m_rgbMap[ib];
		const BYTE bmask		= (BYTE)( ~( 0x3 << shf ) );
		const BYTE bset			= (BYTE)( state << shf );
		const BYTE bnew			= (BYTE)( ( b & bmask ) | bset );

		m_rgbMap[ib] 			= bnew;
		}
		
	}


//  ================================================================	
VOID SLVSPACENODE::ChangeCpgAvail_( const LONG cpgDiff )
//  ================================================================
	{
	Assert( abs( cpgDiff ) <= cpageMap );
	if ( cpgDiff > 0 )
		{
		Assert( m_cpgAvail + cpgDiff <= cpageMap );
		}
	else
		{
		Assert( m_cpgAvail >= cpgDiff );
		}
	m_cpgAvail = USHORT( m_cpgAvail + cpgDiff );
	}


ERR ErrSLVCreateOwnerMap( PIB *ppib, FUCB *pfucbDb )
	{
	Assert ( ppibNil != ppib );
	Assert ( pfucbNil != pfucbDb );
	
	ERR				err 					= JET_errSuccess;
	const IFMP		ifmp					= pfucbDb->ifmp;
	FMP::AssertVALIDIFMP( ifmp );
	
	PGNO			pgnoSLVOwnerMapFDP 		= pgnoNull;
	OBJID			objidSLVOwnerMap 		= objidNil;
	const BOOL		fTempDb					= ( dbidTemp == rgfmp[ifmp].Dbid() );

	Assert( rgfmp[ifmp].FCreatingDB() || fGlobalRepair );
	Assert( 0 == ppib->level || ( 1 == ppib->level && rgfmp[ifmp].FLogOn() ) || fGlobalRepair );
	Assert( pfucbDb->u.pfcb->FTypeDatabase() );
	
	CallR( ErrDIRCreateDirectory(
				pfucbDb,
				cpgSLVOwnerMapTree,
				&pgnoSLVOwnerMapFDP,
				&objidSLVOwnerMap,
				CPAGE::fPageSLVOwnerMap,
				fSPMultipleExtent|fSPUnversionedExtent ) );

	Assert( pgnoSLVOwnerMapFDP > pgnoSystemRoot );
	Assert( pgnoSLVOwnerMapFDP <= pgnoSysMax );
	
	if ( !fTempDb )
		{
		Assert( objidSLVOwnerMap > objidSystemRoot );
		Call( ErrCATAddDbSLVOwnerMap(
					ppib,
					ifmp,
					szSLVOwnerMap,
					pgnoSLVOwnerMapFDP,
					objidSLVOwnerMap ) );
		}
	else
		{
		Assert( pgnoTempDbSLVOwnerMap == pgnoSLVOwnerMapFDP );
		Assert( objidTempDbSLVOwnerMap == objidSLVOwnerMap );
		}

	Assert( pfcbNil == rgfmp[ifmp].PfcbSLVOwnerMap() );
	Call ( ErrSLVOwnerMapInit( ppib, ifmp, pgnoSLVOwnerMapFDP ) );
	Assert( pfcbNil != rgfmp[ifmp].PfcbSLVOwnerMap() );

	Call( ErrSLVOwnerMapNewSize( ppib, ifmp, cpgSLVFileMin, fSLVOWNERMAPNewSLVInit ) );

HandleError:
	return err;
	}


BOOL SLVOWNERMAP::FValidData( const DATA& data )
	{
#ifdef DEBUG	
	data.AssertValid();
#endif

	const SLVOWNERMAP	* const pslvownermap	= (SLVOWNERMAP *)data.Pv();
	const USHORT		cbKey					= pslvownermap->CbKey();
	const BOOL			fPartialPageSupport		= pslvownermap->FPartialPageSupport();
	const ULONG			cbAdjusted				= data.Cb() + ( fPartialPageSupport ? 0 : sizeof(USHORT) );

	if ( cbAdjusted > sizeof(SLVOWNERMAP)
		|| cbAdjusted < cbHeader + cbPreallocatedKeySize )
		{
		return fFalse;
		}

	if ( cbKey >= cbPreallocatedKeySize )
		{
		if ( cbAdjusted != cbHeader + cbKey )
			{
			return fFalse;
			}
		}
	else
		{
		if ( cbAdjusted != cbHeader + cbPreallocatedKeySize )
			{
			return fFalse;
			}		
		}

	return fTrue;			
	}


// always insert a entry for the page m_page and null (objid, columnid, key)
ERR SLVOWNERMAPNODE::ErrNew(PIB *ppib)
	{
	FMP::AssertVALIDIFMP( m_ifmp );
	Assert ( ppibNil != ppib );
	
	ERR 		err 	= JET_errSuccess;
	FMP * 		pfmp 	= &rgfmp[m_ifmp];
	
	Assert ( pfmp->FInUse() );

	// if new allowed we must have the cursor
	Assert ( !FTryNext() ||pfucbNil != Pfucb() );
	
	// we try to open fucb if not opened
	// we will close in the destructor
	CallR ( ErrOpenOwnerMap( ppib ) );
	Assert ( pfucbNil != Pfucb() );

	KEY 	key;
	DATA 	data;
	PGNO 	pageReversed;

	key.prefix.SetPv( NULL );
	key.prefix.SetCb( 0 );

	KeyFromLong( reinterpret_cast<BYTE *>( &pageReversed ), m_page );
	
	key.suffix.SetPv( (void *)&pageReversed );
	key.suffix.SetCb( sizeof(m_page) );

	Nullify();	
	data.SetPv( PvNodeData() );
	data.SetCb( CbNodeData() );


	if ( FTryNext() )
		{
		// don;t try next time unless NextPage is called
		m_fTryNext = fFalse;
		
		Assert( Pcsr( Pfucb() )->FLatched() );

		BTUp( Pfucb() );

		//	we can't call append because OLDSLV may have shrunk the table
		
		Call( ErrBTInsert( Pfucb(), key, data, fDIRNoVersion ) );
		Pfucb()->locLogical = locOnCurBM;

		//  keep the latch so that we can continue with append
		
		Assert( Pcsr( Pfucb() )->FLatched() );
		Assert( latchWrite == Pcsr( Pfucb() )->Latch() );
		}
	else
		{
		FUCBSetLevelNavigate( Pfucb(), Pfucb()->ppib->level );

		Assert( !Pcsr( Pfucb() )->FLatched() );

		Call( ErrBTInsert( Pfucb(), key, data, fDIRNoVersion ) );
		Pfucb()->locLogical = locOnCurBM;

		//  keep the latch so that we can continue with append
		
		Assert( Pcsr( Pfucb() )->FLatched() );
		Assert( latchWrite == Pcsr( Pfucb() )->Latch() );
		}

HandleError:
	return err;	
	}

ERR SLVOWNERMAPNODE::ErrResetData(PIB *ppib)
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI();
	if ( JET_errSuccess == err )
		{
		err = ErrReset( Pfucb() );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	}

ERR SLVOWNERMAPNODE::ErrDelete(PIB *ppib)
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI();
	if ( JET_errSuccess == err )
		{
		if ( !FValidData( Pfucb()->kdfCurr.data ) )
			{
			AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
			return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
			}
		
		err = ErrDIRDelete( Pfucb(), fDIRNull  );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	
	}

ERR SLVOWNERMAPNODE::ErrGetChecksum(
	PIB			* ppib,
	ULONG		* const pulChecksum,
	ULONG		* const pcbChecksummed )
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	Assert( NULL != pulChecksum );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		if ( !FValidData( Pfucb()->kdfCurr.data ) )
			{
			return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
			}
			
		const SLVOWNERMAP	* const pslvownermap	= (SLVOWNERMAP *)Pfucb()->kdfCurr.data.Pv();
		if ( pslvownermap->FValidChecksum() )
			{
			*pulChecksum = pslvownermap->UlChecksum();
			if ( pslvownermap->FPartialPageSupport() )
				{
				*pcbChecksummed = pslvownermap->CbDataChecksummed();
				}
			else
				{
				*pcbChecksummed = SLVPAGE_SIZE;
				}
			}
		else
			{
			err = ErrERRCheck ( errSLVInvalidOwnerMapChecksum );
			}
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;	
	}


ERR SLVOWNERMAPNODE::ErrSetChecksum(
	PIB				* ppib,
	const ULONG		ulChecksum,
	const ULONG		cbChecksummed )
	{
	ERR 			err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );
	Assert( cbChecksummed > 0 );
	Assert( cbChecksummed <= SLVPAGE_SIZE );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		err = ErrUpdateChecksum( Pfucb(), ulChecksum, cbChecksummed );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	}


ERR SLVOWNERMAPNODE::ErrResetChecksum(
	PIB				* ppib )
	{
	ERR 			err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		err = ErrInvalidateChecksum( Pfucb() );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	}


ERR SLVOWNERMAP::ErrSet( FUCB * const pfucb, const BOOL fForceOwner )
	{
	ERR				err			= JET_errSuccess;
	SLVOWNERMAP 	slvownermapT;
	
	if ( !FValidData( pfucb->kdfCurr.data ) )
		{
		AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
		return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
		}

	Assert( CbKey() != 0 );
	Assert( Objid() != objidNil );
	Assert( Columnid() != 0 );

	slvownermapT.Retrieve( pfucb->kdfCurr.data );

	if ( objidNil == slvownermapT.Objid() )
		{
		Assert( 0 == slvownermapT.Columnid() );
		Assert( 0 == slvownermapT.CbKey() );
		}
	else if ( Objid() != slvownermapT.Objid() )
		{
		AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
		return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
		}

	if ( slvownermapT.Columnid() != 0
		&& slvownermapT.Columnid() != Columnid() )
		{
		AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
		return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
		}

	if ( 0 != slvownermapT.CbKey()
		&& !fForceOwner )
		{
		if ( slvownermapT.CbKey() != CbKey()
			|| 0 != memcmp( slvownermapT.PvKey(), PvKey(), min( CbKey(), cbPreallocatedKeySize ) ) )
			{
			AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
			return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
			}
		}

	if ( slvownermapT.Objid() == objidNil || fForceOwner )
		{
		Assert( slvownermapT.Columnid() == 0 || fForceOwner );
		Assert( slvownermapT.CbKey() == 0 || fForceOwner );

		// we need to copy the checksum from the existing node
		Assert( FPartialPageSupport() );
		SetUlChecksum( slvownermapT.UlChecksum() );
		SetCbDataChecksummed( slvownermapT.CbDataChecksummed() );
		if ( slvownermapT.FValidChecksum() )
			{
			SetFValidChecksum();
			}
		else
			{
			ResetFValidChecksum();
			}
		
		DATA data;
		data.SetPv( this );
		data.SetCb( cbHeader + max( CbKey(), cbPreallocatedKeySize ) );
		Call ( ErrDIRReplace( pfucb, data, fDIRReplace ) );
		}
	else
		{
		// if page is already marked as belonging to this page, no replace is needed
		Assert( slvownermapT.Columnid() == Columnid() );
		Assert( slvownermapT.CbKey() == CbKey() );
		Assert( 0 == memcmp( slvownermapT.PvKey(), PvKey(), min( CbKey(), cbPreallocatedKeySize ) ) );
		}

HandleError:
	return err;
	}

ERR SLVOWNERMAPNODE::ErrSetData( PIB *ppib, const BOOL fForceOwner )
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR ( ErrOpenOwnerMap( ppib ) );
	Assert ( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		err = ErrSet( Pfucb(), fForceOwner );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	}

ERR SLVOWNERMAPNODE::ErrSearchAndCheckInRange( PIB *ppib )
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		err = ErrCheckInRange( Pfucb() );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}

	return err;
	
	}

ERR SLVOWNERMAPNODE::ErrSearchI( )
	{
	FMP::AssertVALIDIFMP( m_ifmp );
	Assert ( pfucbNil != Pfucb() );
	
	ERR 		err 	= JET_errSuccess;
	BOOKMARK 	bm;
	DIB			dib;
	PGNO 		pageReversed;

	KeyFromLong( reinterpret_cast<BYTE *>( &pageReversed ), m_page );

	bm.key.prefix.Nullify();
	bm.key.suffix.SetPv( reinterpret_cast<BYTE *>( &pageReversed ) );
	bm.key.suffix.SetCb( sizeof( PGNO ) );
	bm.data.Nullify();


	if ( FTryNext() )
		{
		PGNO currentLatchPage;
		
		// don;t try next time unless NextPage is called
		m_fTryNext = fFalse;
		
		err = ErrDIRNext( Pfucb(), fDIRNull );

		if ( JET_errSuccess != err )
			{
			if ( err > 0 || JET_errNoCurrentRecord == err )
				err = ErrERRCheck( JET_errRecordNotFound );
			Call( err );
			}

		Assert ( Pcsr( Pfucb() )->FLatched() );					
		Assert ( sizeof(PGNO) == Pfucb()->kdfCurr.key.Cb() );
		LongFromKey( &currentLatchPage, Pfucb()->kdfCurr.key );

		// check if we are where we want to be
		if ( currentLatchPage != m_page )
			{
			AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
			Call( ErrERRCheck( JET_errSLVOwnerMapCorrupted ) );
			}

		return JET_errSuccess;
		}

	if ( Pcsr( Pfucb() )->FLatched() )
		{
		PGNO	pgnoCurr;

		Assert ( Pcsr( Pfucb() )->FLatched() );					
		Assert ( sizeof(PGNO) == Pfucb()->kdfCurr.key.Cb() );
		LongFromKey( &pgnoCurr, Pfucb()->kdfCurr.key );
		
		// check if we are where we want to be
		if ( pgnoCurr == m_page )
			{
			return JET_errSuccess;
			}

		ResetCursor();
		}

	Assert ( !Pcsr( Pfucb() )->FLatched() );

	dib.dirflag = fDIRExact;
	dib.pos = posDown;
	dib.pbm = &bm;	
	err = ErrDIRDown( Pfucb(), &dib );

	// we care only if we found or not the record for the page
	if ( JET_errSuccess == err )
		{
		Assert ( JET_errSuccess == err );
		Assert ( Pcsr( Pfucb() )->FLatched() );
		
		PGNO foundPage;
		Assert ( sizeof(PGNO) == Pfucb()->kdfCurr.key.Cb() );
		if ( sizeof(PGNO) != Pfucb()->kdfCurr.key.Cb() )
			{
			AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
			Call( ErrERRCheck( JET_errSLVOwnerMapCorrupted ) );
			}
		else
			{
			LongFromKey( &foundPage, Pfucb()->kdfCurr.key );
			Assert ( foundPage == m_page );
			if ( foundPage != m_page )
				{
				AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );				
				Call( ErrERRCheck( JET_errSLVOwnerMapCorrupted ) );
				}
			}
		}
	else
		{
#ifdef DEBUG		
		switch ( err )
			{
			case wrnNDFoundGreater:
			case wrnNDFoundLess:
			case JET_errDiskIO:
			case JET_errReadVerifyFailure:
			case JET_errRecordNotFound:
			case JET_errOutOfBuffers:
				break;
			default:
				Assert( JET_errNoCurrentRecord != err );
				CallS( err );		//	force assert to report unexpected error
			}
#endif			

		if ( err > 0 )
			err = ErrERRCheck( JET_errRecordNotFound );
		Call( err );
		}

HandleError:
	return err;
	}


ERR SLVOWNERMAPNODE::ErrSearch( PIB *ppib )
	{
	ERR 		err;

	FMP::AssertVALIDIFMP( m_ifmp );
	Assert( rgfmp[m_ifmp].FInUse() );
	Assert( ppibNil != ppib );

	// we try to open fucb if not opened
	// we will close in the destructor
	CallR( ErrOpenOwnerMap( ppib ) );
	Assert( pfucbNil != Pfucb() );

	err = ErrSearchI( );
	if ( JET_errSuccess == err )
		{
		// copy into class fields the data from found node		
		if ( !FValidData( Pfucb()->kdfCurr.data ) )
			{
			AssertSz( fFalse, "JET_errSLVOwnerMapCorrupted" );			
			return ErrERRCheck( JET_errSLVOwnerMapCorrupted );
			}

		Retrieve( Pfucb()->kdfCurr.data );
		}
	else if ( JET_errRecordNotFound == err )
		{
		err = ErrERRCheck( JET_errSLVOwnerMapPageNotFound );
		}
		
	return err;
	}


ERR SLVOWNERMAPNODE::ErrCreate(
	const IFMP		ifmp,
	const PGNO		pgno,
	const OBJID		objid,
	const COLUMNID	columnid,
	BOOKMARK		* pbm ) 
	{ 
	FMP::AssertVALIDIFMP( ifmp ); 
	FMP * pfmp = &rgfmp[ifmp];
	
	Assert ( pfmp->FInUse() );
	Assert( pgnoNull < pgno );

	m_ifmp = ifmp; 
	m_page = pgno;
	m_fTryNext = fFalse;

	SetObjid( objid );
	SetColumnid( columnid );
	
	if ( NULL != pbm )
		{
		Assert( JET_cbPrimaryKeyMost >= pbm->key.Cb() );
		SetCbKey( BYTE( pbm->key.Cb() ) );
		pbm->key.CopyIntoBuffer( PvKey() );
		}
	else
		{
		SetCbKey( 0 );
		}
		
	return JET_errSuccess;
	}

ERR SLVOWNERMAPNODE::ErrCreateForSearch(const IFMP ifmp, const PGNO pgno)
	{ 
	return ErrCreate( ifmp, pgno, objidNil, 0 /*columnidNil */ , NULL); 
	} 

ERR SLVOWNERMAPNODE::ErrCreateForSet(
	const IFMP		ifmp,
	const PGNO		pgno,
	const OBJID		objid,
	const COLUMNID	columnid,
	BOOKMARK		* pbm )
	{ 
	FMP::AssertVALIDIFMP( ifmp );
	Assert( rgfmp[ifmp].FInUse() );

	Assert( FidOfColumnid( columnid ) >= fidTaggedLeast );
	Assert( FidOfColumnid( columnid ) <= fidTaggedMost );

#ifdef DEBUG
	Assert( pbm );
	pbm->AssertValid();
	pbm->key.AssertValid();
#endif

	Assert( pbm->key.Cb() <= JET_cbPrimaryKeyMost );

	return ErrCreate( ifmp, pgno, objid, columnid, pbm );
	}


INLINE ERR ErrSLVIOwnerMapDeleteRange(
	PIB						*ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	const CPG				cpg,
	const SLVOWNERMAP_FLAGS	fFlags )
	{
	ERR 					err 			= JET_errSuccess;
	SLVOWNERMAPNODE			slvownermapNode;
	CPG 					ipg 			= 0;

	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
	slvownermapNode.SetDebugFlags( fFlags );
	
	for( ipg = 0; ipg < cpg; ipg++)
		{
		Call( slvownermapNode.ErrDelete( ppib ) );
		slvownermapNode.NextPage();
		}
	
HandleError:	
	return err;	
	}

INLINE ERR ErrSLVIOwnerMapNewRange(
	PIB						*ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	const CPG				cpg,
	const SLVOWNERMAP_FLAGS	fFlags )
	{
	ERR 					err 			= JET_errSuccess;
	SLVOWNERMAPNODE			slvownermapNode;
	CPG 					ipg 			= 0;

	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
	slvownermapNode.SetDebugFlags( fFlags );
	
	for( ipg = 0; ipg < cpg; ipg++)
		{
		Call( slvownermapNode.ErrNew( ppib ) );
		slvownermapNode.NextPage(); // allow DIRAppend
		}
	
HandleError:	
	return err;	
	}

ERR ErrSLVOwnerMapSetUsageRange(
	PIB						* ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	const CPG				cpg,
	const OBJID				objid,
	const COLUMNID			columnid,
	BOOKMARK				* pbm,
	const SLVOWNERMAP_FLAGS	fFlags,
	const BOOL				fForceOwner )
	{
	ERR 					err 			= JET_errSuccess;
	SLVOWNERMAPNODE			slvownermapNode;
	CPG 					ipg 			= 0;
		
	Call( slvownermapNode.ErrCreateForSet( ifmp, pgno, objid, columnid, pbm ) );
	slvownermapNode.SetDebugFlags( fFlags );
	
	for( ipg = 0; ipg < cpg; ipg++ )
		{
		Call( slvownermapNode.ErrSetData( ppib, fForceOwner ) );
		slvownermapNode.NextPage();
		}
		
HandleError:	
	return err;	
	}

ERR ErrSLVOwnerMapSetChecksum(
	PIB						*ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	const ULONG				ulChecksum,
	const ULONG				cbChecksummed )
	{
	ERR 					err 			= JET_errSuccess;

// if we DON'T set frontdoor checksum and frontdoor is enabled
// return (ValidChecksum flag will remaing unset)
#ifndef SLV_USE_CHECKSUM_FRONTDOOR
	if ( !PinstFromIfmp( ifmp )->FSLVProviderEnabled() )
		return JET_errSuccess;
#endif // SLV_USE_CHECKSUM_FRONTDOOR

	SLVOWNERMAPNODE			slvownermapNode;

	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
	Call( slvownermapNode.ErrSetChecksum( ppib, ulChecksum, cbChecksummed ) );

HandleError:	
	return err;	
	}

ERR ErrSLVOwnerMapGetChecksum(
	PIB				* ppib,
	const IFMP		ifmp,
	const PGNO		pgno,
	ULONG			* const pulChecksum,
	ULONG			* const pcbChecksummed )
	{
	ERR 			err 			= JET_errSuccess;
	SLVOWNERMAPNODE	slvownermapNode;
	
	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
	Call( slvownermapNode.ErrGetChecksum( ppib, pulChecksum, pcbChecksummed ) );
		
HandleError:	
	return err;	
	}


ERR ErrSLVOwnerMapCheckChecksum(
	PIB *ppib,
	const IFMP ifmp,
	const PGNO pgno,
	const CPG cpg,
	const void * pv)
	{
	ERR 					err 			= JET_errSuccess;
	CPG 					ipg 			= 0;

// if we DON'T test frontdoor checksum and 
// frontdoor is enabled, return checksum OK
#ifndef SLV_USE_CHECKSUM_FRONTDOOR
	if ( !PinstFromIfmp( ifmp )->FSLVProviderEnabled() )
		return JET_errSuccess;
#endif // SLV_USE_CHECKSUM_FRONTDOOR

	for( ipg = 0; ipg < cpg; ipg++)
		{
		ULONG	ulChecksum;
		ULONG	cbChecksummed;
		
		err = ErrSLVOwnerMapGetChecksum( ppib, ifmp, pgno + ipg, &ulChecksum, &cbChecksummed );
		if ( errSLVInvalidOwnerMapChecksum != err )
			{
			Call ( err );				

			ULONG ulPageChecksum;

			Assert( 0 == SLVPAGE_SIZE % sizeof( DWORD ) );
			ulPageChecksum = UlChecksumSLV(
								(BYTE*)pv + SLVPAGE_SIZE * ipg,
								(BYTE*)pv + SLVPAGE_SIZE * ipg + cbChecksummed );

			Assert( ulPageChecksum == ulChecksum );
			
			// at this point we got the checksum from the OwnerMap tree and from page
			if ( ulPageChecksum != ulChecksum )
				{
				Call ( ErrERRCheck( JET_errSLVReadVerifyFailure ) );
				}
			}
		}		

	err = JET_errSuccess;
HandleError:	
	return err;	
	}


ERR ErrSLVOwnerMapGet(
	PIB						* ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	SLVOWNERMAP				* const pslvownermapRetrieved )
	{
	ERR 					err;
	SLVOWNERMAPNODE			slvownermapNode;

	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
	Call( slvownermapNode.ErrSearch( ppib ) );
	slvownermapNode.CopyInto( pslvownermapRetrieved );

HandleError:
	return err;
	}


ERR ErrSLVOwnerMapCheckUsageRange(
	PIB				* ppib,
	const IFMP		ifmp,
	const PGNO		pgno,
	const CPG		cpg,
	const OBJID		objid,
	const COLUMNID	columnid,
	BOOKMARK		* pbm )
	{
	ERR 			err 			= JET_errSuccess;
	SLVOWNERMAPNODE	slvownermapNode;
	CPG 			ipg 			= 0;

	Call( slvownermapNode.ErrCreateForSet( ifmp, pgno, objid, columnid, pbm ) );
	
	for( ipg = 0; ipg < cpg; ipg++)
		{
		Call( slvownermapNode.ErrSearchAndCheckInRange( ppib ) );
		slvownermapNode.NextPage();
		}
		
HandleError:	
	return err;	
	}

ERR ErrSLVOwnerMapResetUsageRange(
	PIB						*ppib,
	const IFMP				ifmp,
	const PGNO				pgno,
	const CPG				cpg,
	const SLVOWNERMAP_FLAGS	fFlags )
	{
	ERR 					err 			= JET_errSuccess;
	SLVOWNERMAPNODE			slvownermapNode;
	CPG 					ipg 			= 0;

	Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );	
	slvownermapNode.SetDebugFlags( fFlags );

	for( ipg = 0; ipg < cpg; ipg++)
		{		
		Call( slvownermapNode.ErrResetData( ppib ) );
		slvownermapNode.NextPage();
		}

HandleError:	
	return err;		
	}

ERR ErrSLVOwnerMapNewSize(
	PIB						*ppib,
	const IFMP				ifmp,
	const PGNO				pgnoSLVLast,
	const SLVOWNERMAP_FLAGS	fFlags )
	{
	ERR 					err 		= JET_errSuccess;
	FMP						* pfmp 		= &rgfmp[ifmp];
	PGNO 					pgnoLast 	= pgnoNull;
	FUCB					* pfucb 	= pfucbNil;
	DIB						dib;
		
	Assert ( NULL != pfmp->PfcbSLVOwnerMap() );

	CallR ( ErrDIROpen( ppib, pfmp->PfcbSLVOwnerMap(), &pfucb ) );
	Assert ( pfucbNil != pfucb );

	dib.dirflag = fDIRNull;
	dib.pos   = posLast;
	err = ErrDIRDown( pfucb, &dib );

	if ( JET_errRecordNotFound == err )
		{
		err = JET_errSuccess;
		pgnoLast = 0;
		}
	else if (JET_errSuccess <= err )
		{
		Assert( pfucb->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoLast, pfucb->kdfCurr.key );
		}
	Call ( err );

	Assert (pfucbNil != pfucb);
	DIRClose(pfucb);
	pfucb = pfucbNil;

	if (pgnoLast < pgnoSLVLast)
		{
		Call ( ErrSLVIOwnerMapNewRange(ppib, ifmp, pgnoLast + 1, pgnoSLVLast - pgnoLast, fFlags ) );		
		}
	else if (pgnoLast > pgnoSLVLast)
		{
		Call ( ErrSLVIOwnerMapDeleteRange( ppib, ifmp, pgnoSLVLast + 1, pgnoLast - pgnoSLVLast , fFlags) );
		}
	
HandleError:	
	if (pfucbNil != pfucb)
		{
		DIRClose(pfucb);
		}
	return err;	
	}

#define SLVVERIFIER_CODE

#ifdef SLVVERIFIER_CODE

SLVVERIFIER::SLVVERIFIER( const IFMP ifmp, ERR* const pErr )
	: m_rgfValidChecksum( NULL ),
	m_rgulChecksums( NULL ),
	m_rgcbChecksummed( NULL ),
	m_cChecksums( 0 ),
	m_pgnoFirstChecksum( pgnoNull ),
	m_slvownermapNode( fTrue )
#ifndef SLV_USE_CHECKSUM_FRONTDOOR
	,
	m_fCheckChecksum( fTrue )
#endif
	{
	FMP::AssertVALIDIFMP( ifmp );
	Assert( pErr );

#ifndef SLV_USE_CHECKSUM_FRONTDOOR
	if ( !PinstFromIfmp( ifmp )->FSLVProviderEnabled() )
		m_fCheckChecksum = fFalse;
#endif
	
	//	verifier always goes from first SLV page to last
	const PGNO pgnoFirstSLVPage = 1;
	Assert( pgnoNull != pgnoFirstSLVPage );
	*pErr = m_slvownermapNode.ErrCreateForSearch( ifmp, pgnoFirstSLVPage );
	m_ifmp = ifmp;
	}

SLVVERIFIER::~SLVVERIFIER()
	{
	(VOID) ErrDropChecksums();
	}

//	Call before verifying chunks of pages to batch up a bunch of
//	checksums from OwnerMap all at once. Gets checksums for range of
//	file from byte ib for count of cb bytes.

ERR SLVVERIFIER::ErrGrabChecksums( const QWORD ib, const DWORD cb, PIB* const ppib ) 
	{
#ifdef SLV_USE_CHECKSUM_FRONTDOOR
#else
	if ( ! m_fCheckChecksum )
		{
		return JET_errSuccess;
		}
#endif
	ERR	err = JET_errSuccess;
	CPG	ipg = 0;

	const DWORD cbNonData = cpgDBReserved * SLVPAGE_SIZE;
	const QWORD	qwpg = ib / QWORD( SLVPAGE_SIZE );
	const CPG	dwpg = CPG( qwpg );
	Assert( qwpg == dwpg );
	PGNO	pgnoFirst = dwpg - cpgDBReserved + 1;
	CPG		cpg = cb / SLVPAGE_SIZE;
	// if the block is at the beginning of the file, skip over the header & shadow
	// of the SLV file.
	if ( ib < cbNonData )
		{
		// For goodness sakes, let's keep this simple...
		Assert( 0 == ib );
		Assert( cb >= cbNonData );

		// header and shadow are not recognized as pages
		pgnoFirst = 1;
		cpg -= cpgDBReserved;
		}

	Assert( pgnoNull != pgnoFirst );
	Assert( cpg > 0 );
	Assert( ppib );
	
	m_cChecksums = cpg;
	m_pgnoFirstChecksum = pgnoFirst;
	m_rgulChecksums = new ULONG[ cpg ];
	if ( ! m_rgulChecksums )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	m_rgcbChecksummed = new ULONG[ cpg ];
	if ( ! m_rgcbChecksummed )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	m_rgfValidChecksum = new BOOL[ cpg ];
	if ( ! m_rgfValidChecksum )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	
	for ( ipg = 0; ipg < cpg; ++ipg )
		{
		Assert( ipg < m_cChecksums );
		m_slvownermapNode.AssertOnPage( pgnoFirst + ipg );
		err = m_slvownermapNode.ErrGetChecksum( ppib, &m_rgulChecksums[ ipg ], &m_rgcbChecksummed[ ipg ] );
		if ( errSLVInvalidOwnerMapChecksum == err )
			{
			m_rgfValidChecksum[ ipg ] = fFalse;
			}
		else
			{
			Call( err );
			m_rgfValidChecksum[ ipg ] = fTrue;
			}
		m_slvownermapNode.NextPage();
		}

	//	release any latch that may exist on OwnerMap
	//	we retain currency on this record in case this function
	//	gets called again for the next chunk
	Assert( cpg > 0 );
	Assert( m_slvownermapNode.FTryNext() );
	Call( m_slvownermapNode.ErrReleaseCursor() );

	return JET_errSuccess;

HandleError:
	(VOID) ErrDropChecksums();

	m_slvownermapNode.ResetCursor();

	return err;
	}

// call after batch of checksums is no longer needed

ERR SLVVERIFIER::ErrDropChecksums()
	{
	delete[] m_rgulChecksums;
	m_rgulChecksums = NULL;
	delete[] m_rgcbChecksummed;
	m_rgcbChecksummed = NULL;
	delete[] m_rgfValidChecksum;
	m_rgfValidChecksum = NULL;
	m_cChecksums = 0;
	m_pgnoFirstChecksum = pgnoNull;

	return JET_errSuccess;
	}

//	Verify checksums of a bunch of pages starting at offset ib in the SLV file.
//	Buffer described by { pb, cb }

ERR	SLVVERIFIER::ErrVerify( const BYTE* const pb, const DWORD cb, const QWORD ib ) const
	{
	ERR		err					= JET_errSuccess;
	QWORD	ibOffset;
	DWORD	cbLength;
	ULONG	ulChecksumExpected;
	ULONG	ulChecksumActual;

	Assert( pb );
	Assert( cb > 0 );

	//	Header and shadow		
	const DWORD cbNonData = cpgDBReserved * SLVPAGE_SIZE;
	const BYTE* pbData = pb;
	DWORD cbData = cb;
	const QWORD qwpg = ib / QWORD( SLVPAGE_SIZE );
	const CPG dwpg = CPG( qwpg );
	Assert( dwpg == qwpg );
	PGNO pgnoFirst = dwpg - cpgDBReserved + 1;
	// if the block is at the beginning of the file, check the header & shadow
	if ( ib < cbNonData )
		{
		// For goodness sakes, let's keep this simple...
		Assert( 0 == ib );
		Assert( cb >= cbNonData );

		//	Checksum the header and the shadow

		const DWORD	cbHeader = g_cbPage;
		Assert( g_cbPage == SLVPAGE_SIZE );

		Assert( cb >= cbHeader * 2 );

		ulChecksumExpected	= *( (LittleEndian<DWORD>*) pb );
		ulChecksumActual	= UlUtilChecksum( pb, cbHeader );
		if ( ulChecksumExpected != ulChecksumActual )
			{
			ibOffset	= 0;
			cbLength	= cbHeader;
			Call( ErrERRCheck( JET_errSLVReadVerifyFailure ) );
			}

		ulChecksumExpected	= *( (LittleEndian<DWORD>*) ( pb + cbHeader ) );
		ulChecksumActual	= UlUtilChecksum( pb + cbHeader, cbHeader );
		if ( ulChecksumExpected != ulChecksumActual )
			{
			ibOffset	= cbHeader;
			cbLength	= cbHeader;
			Call( ErrERRCheck( JET_errSLVReadVerifyFailure ) );
			}

		// jump over header
		pbData += cbNonData;
		cbData -= cbNonData;

		pgnoFirst = 1;
		}

	// This check is placed down here, so we can at least check the SLV header & shadow if frontdoor checksumming is off.
#ifndef SLV_USE_CHECKSUM_FRONTDOOR
	if ( ! m_fCheckChecksum )
		{
		return JET_errSuccess;
		}
#endif

	Assert( pgnoNull != pgnoFirst );
	
	Assert( ( pbData >= pb ) && ( pbData < pb + cb ) );
	Assert( cbData > 0 );

		{
		Assert( 0 == ( cbData % SLVPAGE_SIZE ) );
		const CPG cpg = cbData / SLVPAGE_SIZE;
		Assert( cpg > 0 );
		Assert( m_rgulChecksums );
		Assert( m_rgcbChecksummed );
		Assert( m_rgfValidChecksum );
		Assert( m_cChecksums > 0 );
		Assert( pgnoNull != m_pgnoFirstChecksum );
		
		for ( PGNO pgno = pgnoFirst; pgno < pgnoFirst + cpg; ++pgno )
			{
			Assert( pgno >= m_pgnoFirstChecksum );
			const UINT ichecksum = pgno - m_pgnoFirstChecksum;
			Assert( ichecksum < m_cChecksums );
			
			if ( m_rgfValidChecksum[ ichecksum ] )
				{
				ulChecksumExpected	= m_rgulChecksums[ ichecksum ];
				ulChecksumActual	= UlChecksumSLV(	pbData + SLVPAGE_SIZE * ( pgno - pgnoFirst ),
														pbData + SLVPAGE_SIZE * ( pgno - pgnoFirst ) + m_rgcbChecksummed[ ichecksum ] );
				if ( ulChecksumExpected != ulChecksumActual )
					{
					ibOffset	= OffsetOfPgno( pgno );
					cbLength	= m_rgcbChecksummed[ ichecksum ];
					Call( ErrERRCheck( JET_errSLVReadVerifyFailure ) );
					}
				}
			}
		}

HandleError:
	if ( err == JET_errSLVReadVerifyFailure )
		{
		const _TCHAR*	rgpsz[ 6 ];
		DWORD			irgpsz		= 0;
		_TCHAR			szAbsPath[ IFileSystemAPI::cchPathMax ];
		_TCHAR			szOffset[ 64 ];
		_TCHAR			szLength[ 64 ];
		_TCHAR			szError[ 64 ];
		_TCHAR			szChecksumExpected[ 64 ];
		_TCHAR			szChecksumActual[ 64 ];

		CallS( rgfmp[ m_ifmp ].PfapiSLV()->ErrPath( szAbsPath ) );
		_stprintf( szOffset, _T( "%I64i (0x%016I64x)" ), ibOffset, ibOffset );
		_stprintf( szLength, _T( "%u (0x%08x)" ), cbLength, cbLength );
		_stprintf( szError, _T( "%i (0x%08x)" ), err, err );
		_stprintf( szChecksumExpected, _T( "%u (0x%08x)" ), ulChecksumExpected, ulChecksumExpected );
		_stprintf( szChecksumActual, _T( "%u (0x%08x)" ), ulChecksumActual, ulChecksumActual );

		rgpsz[ irgpsz++ ]	= szAbsPath;
		rgpsz[ irgpsz++ ]	= szOffset;
		rgpsz[ irgpsz++ ]	= szLength;
		rgpsz[ irgpsz++ ]	= szError;
		rgpsz[ irgpsz++ ]	= szChecksumExpected;
		rgpsz[ irgpsz++ ]	= szChecksumActual;

		UtilReportEvent(	eventError,
							LOGGING_RECOVERY_CATEGORY,
							SLV_PAGE_CHECKSUM_MISMATCH_ID,
							irgpsz,
							rgpsz );
		}
	return err;
	}

#endif

#endif	//	DISABLE_SLV
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\sort.cxx ===
#include "std.hxx"

//  SORT internal functions

INLINE VOID SrecToKeydataflags( const SREC * psrec, FUCB * pfucb );
LOCAL LONG IspairSORTISeekByKey( const BYTE * rgbRec, const SPAIR * rgspair, LONG ispairMac, const KEY * pkey, BOOL fGT );
INLINE INT ISORTICmpKey( const SREC * psrec1, const SREC * psrec2 );
INLINE INT ISORTICmpKeyData( const SREC * psrec1, const SREC * psrec2 );
INLINE BOOL FSORTIDuplicate( SCB* const pscb, const SREC * psrec1, const SREC * psrec2 );
LOCAL LONG CspairSORTIUnique( SCB* pscb, BYTE * rgbRec, SPAIR * rgspair, const LONG ispairMac );
LOCAL ERR ErrSORTIOutputRun( SCB * pscb );
INLINE INT ISORTICmpPspairPspair( const SCB * pscb, const SPAIR * pspair1, const SPAIR * pspair2 );
LOCAL INT ISORTICmp2PspairPspair( const SCB * pscb, const SPAIR * pspair1, const SPAIR * pspair2 );
INLINE VOID SWAPPspair( SPAIR **ppspair1, SPAIR **ppspair2 );
INLINE VOID SWAPSpair( SPAIR *pspair1, SPAIR *pspair2 );
INLINE VOID SWAPPsrec( SREC **ppsrec1, SREC **ppsrec2 );
INLINE VOID SWAPPmtnode( MTNODE **ppmtnode1, MTNODE **ppmtnode2 );
LOCAL VOID SORTIInsertionSort( SCB *pscb, SPAIR *pspairMinIn, SPAIR *pspairMaxIn );
LOCAL VOID SORTIQuicksort( SCB * pscb, SPAIR *pspairMinIn, SPAIR *pspairMaxIn );
LOCAL ERR ErrSORTIRunStart( SCB *pscb, QWORD cb, RUNINFO *pruninfo );
LOCAL ERR ErrSORTIRunInsert( SCB *pscb, RUNINFO* pruninfo, SREC *psrec );
INLINE VOID SORTIRunEnd( SCB * pscb, RUNINFO* pruninfo );
INLINE VOID SORTIRunDelete( SCB * pscb, const RUNINFO * pruninfo );
LOCAL VOID	SORTIRunDeleteList( SCB *pscb, RUNLINK **pprunlink, LONG crun );
LOCAL VOID	SORTIRunDeleteListMem( SCB *pscb, RUNLINK **pprunlink, LONG crun );
LOCAL ERR ErrSORTIRunOpen( SCB *pscb, RUNINFO *pruninfo, RCB **pprcb );
LOCAL ERR ErrSORTIRunNext( RCB * prcb, SREC **ppsrec );
LOCAL VOID SORTIRunClose( RCB *prcb );
INLINE ERR ErrSORTIRunReadPage( RCB *prcb, PGNO pgno, LONG ipbf );
LOCAL ERR ErrSORTIMergeToRun( SCB *pscb, RUNLINK *prunlinkSrc, RUNLINK **pprunlinkDest );
LOCAL ERR ErrSORTIMergeStart( SCB *pscb, RUNLINK *prunlinkSrc );
LOCAL ERR ErrSORTIMergeFirst( SCB *pscb, SREC **ppsrec );
LOCAL ERR ErrSORTIMergeNext( SCB *pscb, SREC **ppsrec );
LOCAL VOID SORTIMergeEnd( SCB *pscb );
LOCAL ERR ErrSORTIMergeNextChamp( SCB *pscb, SREC **ppsrec );
INLINE VOID SORTIOptTreeInit( SCB *pscb );
LOCAL ERR ErrSORTIOptTreeAddRun( SCB *pscb, RUNINFO *pruninfo );
LOCAL ERR ErrSORTIOptTreeMerge( SCB *pscb );
INLINE VOID SORTIOptTreeTerm( SCB *pscb );
LOCAL ERR ErrSORTIOptTreeBuild( SCB *pscb, OTNODE **ppotnode );
LOCAL ERR ErrSORTIOptTreeMergeDF( SCB *pscb, OTNODE *potnode, RUNLINK **pprunlink );
LOCAL VOID SORTIOptTreeFree( SCB *pscb, OTNODE *potnode );

//----------------------------------------------------------
//  put the current sort record into the FUCB
//----------------------------------------------------------
INLINE VOID SrecToKeydataflags( const SREC * psrec, FUCB * pfucb )
	{ 
	pfucb->locLogical				= locOnCurBM;				//  CSR on record
	pfucb->kdfCurr.key.prefix.Nullify();
	pfucb->kdfCurr.key.suffix.SetCb( CbSRECKeyPsrec( psrec ) );	//  size of key
	pfucb->kdfCurr.key.suffix.SetPv( PbSRECKeyPsrec( psrec ) );	//  key
	pfucb->kdfCurr.data.SetCb( CbSRECDataPsrec( psrec ) );		//  size of data
	pfucb->kdfCurr.data.SetPv( PbSRECDataPsrec( psrec ) );		//  data
	pfucb->kdfCurr.fFlags			= 0;
	}

//----------------------------------------------------------
//	ErrSORTOpen( PIB *ppib, FUCB **pfucb, INT fFlags )
//
//	This function returns a pointer to an FUCB which can be 
//	use to add records to a collection of records to be sorted.  
//	Then the records can be retrieved in sorted order.  
//	
//	The fFlags fUnique flag indicates that records with duplicate
//	keys should be eliminated.
//----------------------------------------------------------

ERR ErrSORTOpen( PIB *ppib, FUCB **ppfucb, const BOOL fRemoveDuplicateKey, const BOOL fRemoveDuplicateKeyData )
	{
	ERR		err			= JET_errSuccess;
	FUCB   	* pfucb		= pfucbNil;
	SCB		* pscb		= pscbNil;
	SPAIR	* rgspair	= 0;
	BYTE	* rgbRec	= 0;

	/*	allocate a new SCB
	/**/
	INST *pinst = PinstFromPpib( ppib );
	IFMP ifmpTemp = pinst->m_mpdbidifmp[ dbidTemp ];
	CallR( ErrFUCBOpen( ppib, ifmpTemp, &pfucb ) );
	if ( ( pscb = PscbMEMAlloc( pinst ) ) == pscbNil )
		Error( ErrERRCheck( JET_errTooManySorts ), HandleError );

	Assert( FAlignedForAllPlatforms( pscb ) );

	/*	initialize sort context to insert mode
	/**/
	FCBInitFCB( &pscb->fcb );
	new( &pscb->fcb ) FCB( ifmpTemp, pgnoNull );
	pscb->fcb.SetTypeSort();
	pscb->fcb.SetFixedDDL();
	pscb->fcb.SetPrimaryIndex();
	pscb->fcb.SetSequentialIndex();

	//	finish the initialization of this FCB
	
	pscb->fcb.CreateComplete();

	FUCBSetSort( pfucb );
	SCBSetInsert( pscb );
	SCBResetRemoveDuplicateKey( pscb );
	SCBResetRemoveDuplicateKeyData( pscb );
	if ( fRemoveDuplicateKey )
		{
		SCBSetRemoveDuplicateKey( pscb );
		}
	if ( fRemoveDuplicateKeyData )
		{
		SCBSetRemoveDuplicateKeyData( pscb );
		}
	pscb->cRecords	= 0;

	/*	allocate sort pair buffer and record buffer
	/**/
	if ( !( rgspair = ( SPAIR * )( PvOSMemoryPageAlloc( cbSortMemFastUsed, NULL ) ) ) )
		Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
	pscb->rgspair	= rgspair;
	if ( !( rgbRec = ( BYTE * )( PvOSMemoryPageAlloc( cbSortMemNormUsed, NULL ) ) ) )
		Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
	pscb->rgbRec	= rgbRec;

	/*	initialize sort pair buffer
	/**/
	pscb->ispairMac	= 0;

	/*	initialize record buffer
	/**/
	pscb->irecMac	= 0;
	pscb->crecBuf	= 0;
	pscb->cbData	= 0;

	/*	reset run count to zero
	/**/
	pscb->crun = 0;

	/*	link FUCB to FCB in SCB
	/**/
	pscb->fcb.Link( pfucb );

	/*	defer allocating space for a disk merge as well as initializing for a
	/*	 merge until we are forced to perform one
	/**/
	Assert( pscb->fcb.PgnoFDP() == pgnoNull );

	/*	return initialized FUCB
	/**/
	*ppfucb = pfucb;

	err = JET_errSuccess;

HandleError:
	if ( JET_errSuccess != err )
		{
		if ( rgbRec != NULL )
			{
			OSMemoryPageFree( rgbRec );
			}
		if ( rgspair != NULL )
			{
			OSMemoryPageFree( rgspair );
			}
		if ( pscb != pscbNil )
			{
			MEMReleasePscb( pinst, pscb );
			}
		if ( pfucb != pfucbNil )
			{
			FUCBClose( pfucb );
			}
		}

	return err;
	}


//----------------------------------------------------------
//	ErrSORTInsert
//
//	Add the record represented by key-data 
//	to the collection of sort records.
//	Here, data is the primary key bookmark
//----------------------------------------------------------

ERR ErrSORTInsert( FUCB *pfucb, const KEY& key, const DATA& data )
	{
	SCB		* const pscb	= pfucb->u.pscb;
	LONG	cbKey;
	LONG	cbData;
	LONG	irec;
	SREC	* psrec			= 0;
	SPAIR	* pspair		= 0;
	BYTE	* pbSrc			= 0;
	BYTE	* pbSrcMac		= 0;
	BYTE	* pbDest		= 0;
	BYTE	* pbDestMic		= 0;
	ERR		err				= JET_errSuccess;

	//  check input and input mode
	Assert( key.Cb() <= KEY::cbKeyMax );	// may be secondary key if called from BuildIndex
	Assert( FSCBInsert( pscb ) );

	//  check SCB
	
	Assert( pscb->crecBuf <= cspairSortMax );
	Assert( pscb->irecMac <= irecSortMax );

	//  calculate required normal memory/record indexes to store this record

	INT cbNormNeeded = CbSRECSizeCbCb( key.Cb(), data.Cb() );
	INT cirecNeeded = CirecToStoreCb( cbNormNeeded );

	//  if we are out of fast or normal memory, output a run
	
	if (	pscb->irecMac * cbIndexGran + cbNormNeeded > cbSortMemNormUsed ||
			pscb->crecBuf == cspairSortMax )
		{
		//  sort previously inserted records into a run

		SORTIQuicksort( pscb, pscb->rgspair, pscb->rgspair + pscb->ispairMac );

		//  move the new run to disk
		
		Call( ErrSORTIOutputRun( pscb ) );
		}

	//  create and add the sort record for this record

	irec = pscb->irecMac;
	psrec = PsrecFromPbIrec( pscb->rgbRec, irec );
	pscb->irecMac += cirecNeeded;
	pscb->crecBuf++;
	pscb->cbData += cbNormNeeded;
	SRECSizePsrecCb( psrec, cbNormNeeded );
	SRECKeySizePsrecCb( psrec, key.Cb() );
	key.CopyIntoBuffer( PbSRECKeyPsrec( psrec ) ); 
	UtilMemCpy( PbSRECDataPsrec( psrec ), data.Pv(), data.Cb() );

	//  create and add the sort pair for this record

	//  get new SPAIR pointer and advance SPAIR counter

	pspair = pscb->rgspair + pscb->ispairMac++;

	//  copy key into prefix buffer BACKWARDS for fast compare

	cbKey = CbSRECKeyPsrec( psrec );
	pbSrc = PbSRECKeyPsrec( psrec );
	pbSrcMac = pbSrc + min( cbKey, cbKeyPrefix );
	pbDest = pspair->rgbKey + cbKeyPrefix - 1;

	while ( pbSrc < pbSrcMac )
		*( pbDest-- ) = *( pbSrc++ );

	//  do we have any unused buffer space?

	if ( pbDest >= pspair->rgbKey )
		{
		//  copy data into prefix buffer BACKWARDS for fast compare

		cbData = (LONG)min( pbDest - pspair->rgbKey + 1, CbSRECDataPsrec( psrec ) );
		pbSrc = PbSRECDataPsrec( psrec );
		pbDestMic = max( pspair->rgbKey, pbDest - cbData + 1 );

		while ( pbDest >= pbDestMic )
			*( pbDest-- ) = *( pbSrc++ );

		//  zero any remaining key space

		if ( pbDest >= pspair->rgbKey )
			memset( pspair->rgbKey, 0, pbDest - pspair->rgbKey + 1 );
		}


	//  set compressed pointer to full record
	
	pspair->irec = (USHORT) irec;

	//  keep track of record count

	pscb->cRecords++;

	//  check SCB
	
	Assert( pscb->crecBuf <= cspairSortMax );
	Assert( pscb->irecMac <= irecSortMax );

HandleError:
	return err;
	}


//----------------------------------------------------------
//	ErrSORTEndInsert
//
//	This function is called to indicate that no more records 
//	will be added to the sort.  It performs all work that needs 
//	to be done before the first record can be retrieved.  
//----------------------------------------------------------

ERR ErrSORTEndInsert( FUCB *pfucb )
	{
	SCB		* const pscb	= pfucb->u.pscb;
	ERR		err		= JET_errSuccess;

	//  verify insert mode

	Assert( FSCBInsert( pscb ) );

	//  deactivate insert mode
	
	SCBResetInsert( pscb );

	//  move CSR to before the first record (if any)
	SORTBeforeFirst( pfucb );	

	//  if we have no records, we're done

	if ( !pscb->cRecords )
		return JET_errSuccess;

	//  sort records in memory

	SORTIQuicksort( pscb, pscb->rgspair, pscb->rgspair + pscb->ispairMac );

	//  do we have any runs on disk?

	if ( pscb->crun )
		{
		//	empty sort buffer into final run

		Call( ErrSORTIOutputRun( pscb ) );

		//  free sort memory

		OSMemoryPageFree( pscb->rgspair );
		pscb->rgspair = NULL;
		OSMemoryPageFree( pscb->rgbRec );
		pscb->rgbRec = NULL;
		
		//	perform all but final merge

		Call( ErrSORTIOptTreeMerge( pscb ) );

		// initialize final merge
		
		Call( ErrSORTIMergeStart( pscb, pscb->runlist.prunlinkHead ) );
		}

	//  we have no runs on disk, so remove duplicates in sort buffer, if requested

	else
		{
		pscb->ispairMac = CspairSORTIUnique(	pscb,
												pscb->rgbRec,
												pscb->rgspair,
												pscb->ispairMac );
		pscb->cRecords = pscb->ispairMac;
		}

	//  return a warning if TT doesn't fit in memory, but success otherwise

	err = ( pscb->crun > 0 || pscb->irecMac * cbIndexGran > cbResidentTTMax ) ?
				ErrERRCheck( JET_wrnSortOverflow ) :
				JET_errSuccess;

HandleError:
	return err;
	}


//----------------------------------------------------------
//	ErrSORTFirst
//
//	Move to first record in sort or return an error if the sort
//  has no records.
//----------------------------------------------------------
ERR ErrSORTFirst( FUCB * pfucb )
	{
	SCB		* const pscb	= pfucb->u.pscb;
	SREC	* psrec			= 0;
	LONG	irec;
	ERR		err				= JET_errSuccess;

	//  verify that we are not in insert mode

	Assert( !FSCBInsert( pscb ) );

	//	reset index range

	FUCBResetLimstat( pfucb );

	//  if we have no records, error

	if ( !pscb->cRecords )
		{
		DIRBeforeFirst( pfucb );
		return ErrERRCheck( JET_errNoCurrentRecord );
		}

	Assert( pscb->crun > 0 || pscb->ispairMac > 0 );
		
	//  if we have runs, start last merge and get first record
	
	if ( pscb->crun )
		{
		CallR( ErrSORTIMergeFirst( pscb, &psrec ) );
		}

	//  we have no runs, so just get first record in memory
	
	else
		{
		pfucb->ispairCurr = 0L;
		irec = pscb->rgspair[pfucb->ispairCurr].irec;
		psrec = PsrecFromPbIrec( pscb->rgbRec, irec );
		}

	//	get current record

	SrecToKeydataflags( psrec, pfucb );

	return JET_errSuccess;
	}
	

ERR ErrSORTLast( FUCB *pfucb )
	{
	SCB		*pscb	= pfucb->u.pscb;
	SREC	*psrec	= 0;
	LONG	irec;
	ERR		err = JET_errSuccess;

	//  verify that we are not in insert mode

	Assert( !FSCBInsert( pscb ) );

	//	reset index range

	FUCBResetLimstat( pfucb );

	//  if we have no records, error

	if ( !pscb->cRecords )
		{
		DIRAfterLast( pfucb );
		return ErrERRCheck( JET_errNoCurrentRecord );
		}

	Assert( pscb->crun > 0 || pscb->ispairMac > 0 );
		
	//  if we have runs, get next record from last merge

	if ( pscb->crun )
		{
		err = ErrSORTIMergeNext( pscb, &psrec );
		while ( err >= 0 )
			{
			CallS( err );		// warnings not expected.

			// cache current record so we can revert to something
			// once we move past the end.
			SrecToKeydataflags( psrec, pfucb );

			err = ErrSORTIMergeNext( pscb, &psrec );
			}

		if ( JET_errNoCurrentRecord == err )
			{
			// Currency will be set to the last record cached
			// in the loop above.
			err = JET_errSuccess;
			}
		else
			{
			DIRAfterLast( pfucb );
			}
		}
		
	//  we have no runs, so just get last record in memory
	
	else
		{
		pfucb->ispairCurr = pscb->ispairMac - 1;
		irec = pscb->rgspair[pfucb->ispairCurr].irec;
		psrec = PsrecFromPbIrec( pscb->rgbRec, irec );
		
		//	get current record
		SrecToKeydataflags( psrec, pfucb );
		}

	return err;
	}

//----------------------------------------------------------
//	ErrSORTNext
//
//	Return the next record, in sort order, after the previously 
//	returned record.  If no records have been returned yet, 
//	or the currency has been reset, this function returns 
//	the first record.
//----------------------------------------------------------

ERR ErrSORTNext( FUCB *pfucb )
	{
	SCB		*pscb	= pfucb->u.pscb;
	SREC	*psrec	= 0;
	LONG	irec;
	ERR		err = JET_errSuccess;

	//  verify that we are not in insert mode

	Assert( !FSCBInsert( pscb ) );

	//  if we have runs, get next record from last merge

	if ( pscb->crun )
		{
		Call( ErrSORTIMergeNext( pscb, &psrec ) );
		}
	else
		{
		Assert( pfucb->ispairCurr <= pscb->ispairMac );	// may already be at AfterLast
		
		//  we have no runs, so get next record from memory

		if ( ++pfucb->ispairCurr < pscb->ispairMac )
			{
			irec = pscb->rgspair[pfucb->ispairCurr].irec;
			psrec = PsrecFromPbIrec( pscb->rgbRec, irec );
			}

		//  we have no more records in memory, so return no current record
		
		else
			{
			pfucb->ispairCurr = pscb->ispairMac;
			Call( ErrERRCheck( JET_errNoCurrentRecord ) );
			}
		}

	//	get current record
	SrecToKeydataflags( psrec, pfucb );

	//  handle index range, if requested

	if ( FFUCBLimstat( pfucb ) && FFUCBUpper( pfucb ) )
		CallR( ErrSORTCheckIndexRange( pfucb ) );

	return JET_errSuccess;

HandleError:
	Assert( err < 0 );
	if ( JET_errNoCurrentRecord == err )
		DIRAfterLast( pfucb );

	return err;
	}


//----------------------------------------------------------
//	ErrSORTPrev
//
//	Return the previous record, in sort order, before the
//  previously returned record.  If no records have been
//  returned yet, the currency will be set to before the
//  first record.
//
//  NOTE:  This function supports in memory sorts only!
//  Larger sorts must be materialized for this functionality.
//----------------------------------------------------------

ERR ErrSORTPrev( FUCB *pfucb )
	{
	SCB		* const pscb	= pfucb->u.pscb;
	ERR		err				= JET_errSuccess;

	//  verify that we have an in memory sort

	Assert( !pscb->crun );
	
	//  verify that we are not in insert mode

	Assert( !FSCBInsert( pscb ) );

	Assert( pfucb->ispairCurr >= -1L );	// may already be at BeforeFirst
	if ( pfucb->ispairCurr <= 0L )
		{
		//  we have no more records in memory, so return no current record
		SORTBeforeFirst( pfucb );
		return ErrERRCheck( JET_errNoCurrentRecord );
		}

	//  get previous record from memory
	pfucb->ispairCurr--;
	Assert( pfucb->ispairCurr >= 0L );

	const LONG	irec = pscb->rgspair[pfucb->ispairCurr].irec;
	const SREC	*psrec = PsrecFromPbIrec( pscb->rgbRec, irec );

	//	get current record
	SrecToKeydataflags( psrec, pfucb );

	//  handle index range, if requested

	if ( FFUCBLimstat( pfucb ) && FFUCBUpper( pfucb ) )
		CallR( ErrSORTCheckIndexRange( pfucb ) );

	return JET_errSuccess;
	}


//----------------------------------------------------------
//	ErrSORTSeek
//
//	Return the first record with key >= pkey.  
//	If pkey == NULL then return the first record.
//
//  Return Value
//		JET_errSuccess				record with key == pkey is found
//		JET_wrnSeekNotEqual			record with key > pkey is found
//		JET_errNoCurrentRecord		no record with key >= pkey is found
//
//  NOTE:  This function supports in memory sorts only!
//  Larger sorts must be materialized for this functionality.
//----------------------------------------------------------

ERR ErrSORTSeek( FUCB *pfucb, const KEY * pkey, BOOL fGT )
	{
	SCB		* const pscb	= pfucb->u.pscb;

	//  verify that we have an in memory sort

	Assert( FFUCBSort( pfucb ) );
	Assert( !pscb->crun );
	
	//  verify that we are not in insert mode

	Assert( !FSCBInsert( pscb ) );

	//  verify that we are scrollable or indexed or the key is NULL
	
	Assert( ( pfucb->u.pscb->grbit & JET_bitTTScrollable ) ||
		( pfucb->u.pscb->grbit & JET_bitTTIndexed ) ||
		( pkey == NULL ) );

	//  if we have no records, return error

	if ( !pscb->cRecords )
		return ErrERRCheck( JET_errRecordNotFound );

	//  verify that we have a valid key -- note that sorts and temp. tables
	//	don't currently support secondary indexes
	Assert( pkey->Cb() <= JET_cbPrimaryKeyMost );

	//  seek to key or next highest key
	
	pfucb->ispairCurr = IspairSORTISeekByKey(	pscb->rgbRec,
												pscb->rgspair,
												pscb->ispairMac,
												pkey,
												fGT );

	//  if we are after last pair, record not found
	
	if ( pfucb->ispairCurr == pscb->ispairMac )
		return ErrERRCheck( JET_errRecordNotFound );

	//	get current record

	const INT irec				= pscb->rgspair[pfucb->ispairCurr].irec;
	const SREC * const psrec	= PsrecFromPbIrec( pscb->rgbRec, irec );

	SrecToKeydataflags( psrec, pfucb );

	//  return warning if key not equal, success otherwise

	return ( FKeysEqual( pfucb->kdfCurr.key, *pkey ) ?
				JET_errSuccess :
				ErrERRCheck( JET_wrnSeekNotEqual ) );
	}


VOID SORTICloseRun( PIB * const ppib, SCB * const pscb )
	{
	Assert( pscb->fcb.WRefCount() == 1 );
	Assert( pscb->fcb.PgnoFDP() != pgnoNull );
	Assert( !pscb->fcb.FInList() );		// sorts don't go in global list
	
	/*	if we were merging, end merge
	/**/
	if ( pscb->crunMerge )
		SORTIMergeEnd( pscb );

	/*	free merge method resources
	/**/
	SORTIOptTreeTerm( pscb );

	/*	if our output buffer is still latched, unlatch it
	/**/
	if ( pscb->bflOut.pv != NULL )
		{
		BFWriteUnlatch( &pscb->bflOut );
		pscb->bflOut.pv			= NULL;
		pscb->bflOut.dwContext	= NULL;
		}

	// Versioning doesn't occur on sorts.
	Assert( pscb->fcb.PrceOldest() == prceNil );
	Assert( pscb->fcb.PrceNewest() == prceNil );

	// Remove from hash table, so FCB will be available for
	// another file using the FDP that is about to be freed.
	Assert( !pscb->fcb.FDeleteCommitted() );
	pscb->fcb.SetDeleteCommitted();
	SCBDeleteHashTable( pscb );
	
	// Free FDP and allocated sort space (including runs)
	Assert( rgfmp[ pscb->fcb.Ifmp() ].Dbid() == dbidTemp );
	(VOID)ErrSPFreeFDP( ppib, &pscb->fcb, pgnoSystemRoot );
	
	pscb->fcb.ResetSortSpace();
	}


//----------------------------------------------------------
//	SORTClose
//
//  Release sort FUCB and the sort itself if it is no longer
//  needed.
//----------------------------------------------------------

VOID SORTClose( FUCB *pfucb )
	{
	SCB	* const pscb	= pfucb->u.pscb;

	Assert( dbidTemp == rgfmp[ pfucb->ifmp ].Dbid() );
	
	//	if this is the last cursor on sort, then release sort resources
	Assert( !pscb->fcb.FInList() );		// sorts don't go in global list
	Assert( pscb->fcb.WRefCount() >= 1 );
	if ( pscb->fcb.WRefCount() == 1 )
		{
		//  if we have allocated sort space, free it and end all ongoing merge
		//  and output activities

		if ( pscb->crun > 0 )
			{
			SORTICloseRun( pfucb->ppib, pscb );
			}

		//	unlink the FUCB from the FCB without allowing the FCB to move into
		//		the avail LRU list because it will be disappearing via SORTClosePscb

	  	FCBUnlinkWithoutMoveToAvailList( pfucb );

	  	//	close the FUCB
	  	
		FUCBClose( pfucb );
			
		/*	since there are no more references to this sort, free its resources
		/**/
		Assert( pscb->fcb.WRefCount() == 0 );
		SORTClosePscb( pscb );
		}
	else
		{
		//	unlink the FUCB from the FCB without allowing the FCB to move into
		//		the avail LRU list because it will eventually be disappearing 
		//		via SORTClosePscb

	  	FCBUnlinkWithoutMoveToAvailList( pfucb );

		//	close the FUCB

		FUCBClose( pfucb );
		}
	}


//----------------------------------------------------------
//	SORTClosePscb
//
//  Release this SCB and all its resources.
//----------------------------------------------------------

VOID SORTClosePscb( SCB *pscb )
	{
	INST *pinst = PinstFromIfmp( pscb->fcb.Ifmp() );
	
	Assert( rgfmp[ pscb->fcb.Ifmp() ].Dbid() == dbidTemp );
	Assert( pscb->fcb.PgnoFDP() == pgnoNull );
	if ( pscb->rgspair != NULL )
		{
		OSMemoryPageFree( pscb->rgspair );
		}
	if ( pscb->rgbRec != NULL )
		{
		OSMemoryPageFree( pscb->rgbRec );
		}
	if ( pscb->fcb.Pidb() != pidbNil )
		{
		// Sort indexes don't have names.
		Assert( 0 == pscb->fcb.Pidb()->ItagIndexName() );

		// No need to free index name or idxseg array, since memory
		// pool will be freed when TDB is deleted below.
		pscb->fcb.ReleasePidb();
		}
	if ( pscb->fcb.Ptdb() != ptdbNil )
		{
		Assert( pscb->fcb.Ptdb()->PfcbLV() == pfcbNil );	// sorts don't have LV's (would have been materialized)
		pscb->fcb.Ptdb()->Delete( pinst );
		}

	MEMReleasePscb( pinst, pscb );
	}


//----------------------------------------------------------
//	ErrSORTCheckIndexRange
//
//  Restrain currency to a specific range.
//----------------------------------------------------------

ERR ErrSORTCheckIndexRange( FUCB *pfucb )
	{
	SCB		* const pscb = pfucb->u.pscb;

	//  range check FUCB

	ERR err =  ErrFUCBCheckIndexRange( pfucb, pfucb->kdfCurr.key );
	Assert( JET_errSuccess == err || JET_errNoCurrentRecord == err );

	//  if there is no current record, we must have wrapped around
	
	if ( err == JET_errNoCurrentRecord )
		{
		//  wrap around to bottom of sort
		DIRUp( pfucb );

		if ( FFUCBUpper( pfucb ) )
			{
			pfucb->ispairCurr = pscb->ispairMac;
			DIRAfterLast( pfucb );
			}

		//  wrap around to top of sort
		
		else
			{
			SORTBeforeFirst( pfucb );
			}
		}

	//  verify that currency is valid

	Assert( pfucb->locLogical == locBeforeFirst ||
			pfucb->locLogical == locOnCurBM ||
			pfucb->locLogical == locAfterLast );
	Assert( pfucb->ispairCurr >= -1 );
	Assert( pfucb->ispairCurr <= pscb->ispairMac );

	return err;
	}


//----------------------------------------------------------
//	Module internal functions
//----------------------------------------------------------


//	returns index of first entry >= pbKey, or the index past the end of the array

LOCAL LONG IspairSORTISeekByKey( const BYTE * rgbRec, const SPAIR * rgspair, LONG ispairMac, const KEY * pkey, BOOL fGT )
	{
	//  if there are no pairs, return end of array

	if ( !ispairMac )
		return 0;

	//  b-search array

	LONG	ispairBeg	= 0;
	LONG	ispairEnd	= ispairMac;

	do  {
		//  calculate midpoint of this partition
		
		const LONG ispairMid		= ispairBeg + ( ispairEnd - ispairBeg ) / 2;

		//  compare full keys
		
		const LONG irec				= rgspair[ispairMid].irec;
		const SREC * const psrec	= PsrecFromPbIrec( rgbRec, irec );

		KEY keyT;
		keyT.prefix.Nullify();
		keyT.suffix.SetPv( PbSRECKeyPsrec( psrec ) );
		keyT.suffix.SetCb( CbSRECKeyPsrec( psrec ) );
		const INT wCmp = CmpKey( keyT, *pkey);

		//  select partition containing destination

		if ( ( wCmp < 0 ) || ( fGT && wCmp == 0 ) )
			{
			ispairBeg = ispairMid + 1;
			}
		else
			{
			ispairEnd = ispairMid;
			}

		}
	while ( ispairBeg != ispairEnd );

	return ispairEnd;
	}


//  compares two SRECs by key

INLINE INT ISORTICmpKey( const SREC * psrec1, const SREC * psrec2 )
	{
	const BYTE*		stKey1	= StSRECKeyPsrec( psrec1 );
	const USHORT	cbKey1	= *( UnalignedLittleEndian< USHORT > *)stKey1;
	const BYTE*		stKey2	= StSRECKeyPsrec( psrec2 );
	const USHORT	cbKey2	= *( UnalignedLittleEndian< USHORT > *)stKey2;

	Assert( cbKey1 <= KEY::cbKeyMax );
	Assert( cbKey2 <= KEY::cbKeyMax );
	
	const INT w = memcmp(	stKey1 + cbKeyCount,
							stKey2 + cbKeyCount,
							min( cbKey1, cbKey2 ) );
	
	return w ? w : cbKey1 - cbKey2;
	}

//  compares two SRECs by key and data

INLINE INT ISORTICmpKeyData( const SREC * psrec1, const SREC * psrec2 )
	{
	//  compare the keys first, then the data if necessary

	INT cmp = ISORTICmpKey( psrec1, psrec2 );
	if ( 0 == cmp )
		{
		const LONG	cbData1	= CbSRECDataPsrec( psrec1 );
		const LONG	cbData2	= CbSRECDataPsrec( psrec2 );
		cmp = memcmp(	PbSRECDataPsrec( psrec1 ),
						PbSRECDataPsrec( psrec2 ),
						min( cbData1, cbData2 ) );
		if ( 0 == cmp )
			{
			cmp = cbData1 - cbData2;
			}
		}

	return cmp;
	}

//  returns fTrue if the two SRECs are considered duplicates according to the
//  current flags in the SCB

INLINE BOOL FSORTIDuplicate( SCB* const pscb, const SREC * psrec1, const SREC * psrec2 )
	{
	if ( FSCBRemoveDuplicateKey( pscb ) )
		{
		return !ISORTICmpKey( psrec1, psrec2 );
		}
	else if ( FSCBRemoveDuplicateKeyData( pscb ) )
		{
		return !ISORTICmpKeyData( psrec1, psrec2 );
		}
	else
		{
		return fFalse;
		}
	}
	

//  remove duplicates

LOCAL LONG CspairSORTIUnique( SCB* pscb, BYTE * rgbRec, SPAIR * rgspair, const LONG ispairMac )
	{
	//  if we don't need to perform duplicate removal then return immediately

	if (	!ispairMac ||
			!FSCBRemoveDuplicateKeyData( pscb ) && !FSCBRemoveDuplicateKey( pscb ) )
		{
		return ispairMac;
		}

	//  loop through records, moving unique records towards front of array

	LONG	ispairDest;
	LONG	ispairSrc;
	for ( ispairDest = 0, ispairSrc = 1; ispairSrc < ispairMac; ispairSrc++ )
		{
		//  get sort record pointers for src/dest
		
		const LONG		irecDest	= rgspair[ispairDest].irec;
		SREC * const	psrecDest	= PsrecFromPbIrec( rgbRec, irecDest );
		const LONG		irecSrc		= rgspair[ispairSrc].irec;
		SREC * const	psrecSrc	= PsrecFromPbIrec( rgbRec, irecSrc );

		//  if the keys are unequal, copy them forward
		
		if ( !FSORTIDuplicate( pscb, psrecSrc, psrecDest ) )
			{
			rgspair[++ispairDest] = rgspair[ispairSrc];
			}
		}
		
	Assert( ispairDest + 1 <= ispairMac );

	return ispairDest + 1;
	}


//  output current sort buffer to disk in a run
//
LOCAL ERR ErrSORTIOutputRun( SCB * pscb )
	{
	ERR		err;
	RUNINFO	runinfo;
	LONG	ispair;
	LONG	irec;
	SREC*	psrec		= NULL;
	SREC*	psrecLast;

	//  verify that there are records to put to disk
	//
	Assert( pscb->ispairMac );

	//  create sort space on disk now if not done already
	//
	if ( pscb->fcb.PgnoFDP() == pgnoNull )
		{
		FUCB 	* const pfucb	= pscb->fcb.Pfucb();
		PGNO	pgnoFDP			= pgnoNull;
		OBJID	objidFDP		= objidNil;
		
		//  allocate FDP and primary sort space
		//
		//  NOTE:  enough space is allocated to avoid file extension for a single
		//         level merge, based on the data size of the first run
		//	
		const CPG cpgMin = (PGNO) ( ( pscb->cbData + cbFreeSPAGE - 1 ) / cbFreeSPAGE * crunFanInMax );
		CPG cpgReq = cpgMin;

		CallR( ErrSPGetExt(	pfucb,
		 		pgnoSystemRoot,
				&cpgReq,
				cpgMin,
				&pgnoFDP,
				fSPNewFDP|fSPMultipleExtent|fSPUnversionedExtent,
				CPAGE::fPagePrimary,
				&objidFDP ) );

		//	place SCB in FCB hash table so BTOpen will find it instead
		//	of trying to allocate a new one.
		//
		Assert( pgnoNull != pgnoFDP );
		Assert( objidFDP > objidSystemRoot );
		Assert( !pscb->fcb.FSpaceInitialized() );
		pscb->fcb.SetSortSpace( pgnoFDP, objidFDP );
	 	SCBInsertHashTable( pscb );

		//  initialize merge process
		//
		SORTIOptTreeInit( pscb );

		//  reset sort/merge run output
		//
		pscb->bflOut.pv			= NULL;
		pscb->bflOut.dwContext	= NULL;

		//  reset merge run input
		//
		pscb->crunMerge = 0;
		}

	//  begin a new run big enough to store all our data
	//
	CallR( ErrSORTIRunStart( pscb, QWORD( pscb->cbData ), &runinfo ) );

	//  scatter-gather our sorted records into the run while performing
	//  duplicate removal
	//
	for ( ispair = 0; ispair < pscb->ispairMac; ispair++ )
		{
		psrecLast = psrec;
		
		irec = pscb->rgspair[ispair].irec;
		psrec = PsrecFromPbIrec( pscb->rgbRec, irec );

		if (	!psrecLast ||
				!FSORTIDuplicate( pscb, psrec, psrecLast ) )
			{
			CallJ( ErrSORTIRunInsert( pscb, &runinfo, psrec ), EndRun );
			}
		}

	//  end run and add to merge
	//
	SORTIRunEnd( pscb, &runinfo );
	CallJ( ErrSORTIOptTreeAddRun( pscb, &runinfo ), DeleteRun );
	
	//	reinitialize the SCB for another memory sort
	//
	pscb->ispairMac	= 0;
	pscb->irecMac	= 0;
	pscb->crecBuf	= 0;
	pscb->cbData	= 0;

	return JET_errSuccess;

EndRun:
	SORTIRunEnd( pscb, &runinfo );
DeleteRun:
	SORTIRunDelete( pscb, &runinfo );
	return err;
	}

#ifdef DEBUG
LOCAL INT IDBGICmp2PspairPspair( const SCB * pscb, const SPAIR * pspair1, const SPAIR * pspair2 )
	{
	INT	cmp;
	INT	db;
	
	//  get the addresses of the sort records associated with these pairs
	
	SREC * const psrec1 = PsrecFromPbIrec( pscb->rgbRec, pspair1->irec );
	SREC * const psrec2 = PsrecFromPbIrec( pscb->rgbRec, pspair2->irec );

	//  calculate the length of full key remaining that we can compare
	//  we want to compare the key and the data (if the keys differ we
	//  won't compare the data) the data is stored after the key so it
	//  is O.K.
	const LONG cbKey1	= CbSRECKeyPsrec( psrec1 );
	const LONG cbKey2	= CbSRECKeyPsrec( psrec2 );
		
	if ( cbKey1 > cbKeyPrefix )
		{
		db = cbKey1 - cbKey2;
		cmp = memcmp( PbSRECKeyPsrec( psrec1 ) + cbKeyPrefix,
							PbSRECKeyPsrec( psrec2 ) + cbKeyPrefix,
							( db < 0 ? cbKey1 : cbKey2 ) - cbKeyPrefix );
		if ( 0 == cmp )
			cmp = db;
		if ( 0 != cmp )
			return cmp;
		}
	else
		{
		// If keys are less than or equal to prefix, then they must be equal
		// (otherwise this routine would not have been called).
		Assert( cbKey1 == cbKey2 );
		Assert( memcmp( PbSRECKeyPsrec( psrec1 ), PbSRECKeyPsrec( psrec2 ), cbKey1 ) == 0 );
		}


	// Full keys are identical.  Must resort to data comparison.
	const LONG cbData1 = CbSRECDataPsrec( psrec1 );
	const LONG cbData2 = CbSRECDataPsrec( psrec2 );

	db = cbData1 - cbData2;
	cmp = memcmp( PbSRECDataPsrec( psrec1 ),
					PbSRECDataPsrec( psrec2 ),
					db < 0 ? cbData1 : cbData2 );

	return ( 0 == cmp ? db : cmp );
	}
#endif	
	
//  ISORTICmpPspairPspair compares two SPAIRs for the cache optimized Quicksort.
//  Only the key prefixes are compared, unless there is a tie in which case we
//  are forced to go to the full record at the cost of several wait states.
//
INLINE INT ISORTICmpPspairPspair( const SCB * pscb, const SPAIR * pspair1, const SPAIR * pspair2 )
	{
	const BYTE	*rgb1	= (BYTE *) pspair1;
	const BYTE	*rgb2	= (BYTE *) pspair2;

	//  Compare prefixes first.  If they aren't equal, we're done.  Prefixes are
	//  stored in such a way as to allow very fast integer comparisons instead
	//  of byte by byte comparisons like memcmp.  Note that these comparisons are
	//  made scanning backwards.

	//  NOTE:  special case code:  cbKeyPrefix = 14, irec is first

	Assert( cbKeyPrefix == 14 );
	Assert( OffsetOf( SPAIR, irec ) == 0 );

#ifdef _X86_

	//  bytes 15 - 12
	if ( *( (DWORD *) ( rgb1 + 12 ) ) < *( (DWORD *) ( rgb2 + 12 ) ) )
		return -1;
	if ( *( (DWORD *) ( rgb1 + 12 ) ) > *( (DWORD *) ( rgb2 + 12 ) ) )
		return 1;

	//  bytes 11 - 8
	if ( *( (DWORD *) ( rgb1 + 8 ) ) < *( (DWORD *) ( rgb2 + 8 ) ) )
		return -1;
	if ( *( (DWORD *) ( rgb1 + 8 ) ) > *( (DWORD *) ( rgb2 + 8 ) ) )
		return 1;

	//  bytes 7 - 4
	if ( *( (DWORD *) ( rgb1 + 4 ) ) < *( (DWORD *) ( rgb2 + 4 ) ) )
		return -1;
	if ( *( (DWORD *) ( rgb1 + 4 ) ) > *( (DWORD *) ( rgb2 + 4 ) ) )
		return 1;

	//  bytes 3 - 2
	if ( *( (USHORT *) ( rgb1 + 2 ) ) < *( (USHORT *) ( rgb2 + 2 ) ) )
		return -1;
	if ( *( (USHORT *) ( rgb1 + 2 ) ) > *( (USHORT *) ( rgb2 + 2 ) ) )
		return 1;

#else  //  !_X86_

	//  bytes 15 - 8
	if ( *( (LittleEndian<QWORD> *) ( rgb1 + 8 ) ) < *( (LittleEndian<QWORD> *) ( rgb2 + 8 ) ) )
		return -1;
	if ( *( (LittleEndian<QWORD> *) ( rgb2 + 8 ) ) < *( (LittleEndian<QWORD> *) ( rgb1 + 8 ) ) )
		return 1;

	//  bytes 7 - 2
	if (	( *( (LittleEndian<QWORD> *) ( rgb1 + 0 ) ) & 0xFFFFFFFFFFFF0000 ) <
			( *( (LittleEndian<QWORD> *) ( rgb2 + 0 ) ) & 0xFFFFFFFFFFFF0000 ) )
		return -1;
	if (	( *( (LittleEndian<QWORD> *) ( rgb1 + 0 ) ) & 0xFFFFFFFFFFFF0000 ) >
			( *( (LittleEndian<QWORD> *) ( rgb2 + 0 ) ) & 0xFFFFFFFFFFFF0000 ) )
		return 1;

#endif  //  _X86_
	
	//  perform secondary comparison and return result if prefixes identical

	INT i = ISORTICmp2PspairPspair( pscb, pspair1, pspair2 );

#ifdef DEBUG
	// Verify two key/data pairs are key-equivalent, whether their key
	// and data parts are compared separately or concatenated together.
	INT j = IDBGICmp2PspairPspair( pscb, pspair1, pspair2 );

	Assert( ( i < 0 && j < 0 )
			|| ( i == 0 && j == 0 )
			|| ( i > 0 && j > 0 ) );
#endif			

	return i;
	}

		
LOCAL INT ISORTICmp2PspairPspair( const SCB * pscb, const SPAIR * pspair1, const SPAIR * pspair2 )
	{
	//  get the addresses of the sort records associated with these pairs
	
	SREC * const psrec1 = PsrecFromPbIrec( pscb->rgbRec, pspair1->irec );
	SREC * const psrec2 = PsrecFromPbIrec( pscb->rgbRec, pspair2->irec );

	//  calculate the length of full key remaining that we can compare
	//  we want to compare the key and the data (if the keys differ we
	//  won't compare the data) the data is stored after the key so it
	//  is O.K.

	const LONG cbKey1 = CbSRECKeyDataPsrec( psrec1 );
	const LONG cbKey2 = CbSRECKeyDataPsrec( psrec2 );

	INT w = min( cbKey1, cbKey2 ) - cbKeyPrefix;

	//  compare the remainder of the full keys and then the data (if necessary)

	if ( w > 0 )
		{
		//	both keys are greater in length than cbKeyPrefix
		Assert( cbKey1 > cbKeyPrefix );
		Assert( cbKey2 > cbKeyPrefix );
		w = memcmp(	PbSRECKeyPsrec( psrec1 ) + cbKeyPrefix,
					PbSRECKeyPsrec( psrec2 ) + cbKeyPrefix,
					w );
		if ( 0 == w )
			{
			w = cbKey1 - cbKey2;
			}
		}
	else
		{
		//	prefix must be the same (as calculated by ISORTICmpPspairPspair()),
		//	so return comparison result based on key size
		w = cbKey1 - cbKey2;
		}

	return w;
	}


//  Swap functions

INLINE VOID SWAPPspair( SPAIR **ppspair1, SPAIR **ppspair2 )
	{
	SPAIR *pspairT;

	pspairT = *ppspair1;
	*ppspair1 = *ppspair2;
	*ppspair2 = pspairT;
	}


//  we do not use cache aligned memory for spairT (is this bad?)

INLINE VOID SWAPSpair( SPAIR *pspair1, SPAIR *pspair2 )
	{
	SPAIR spairT;

	spairT = *pspair1;
	*pspair1 = *pspair2;
	*pspair2 = spairT;
	}


INLINE VOID SWAPPsrec( SREC **ppsrec1, SREC **ppsrec2 )
	{
	SREC *psrecT;

	psrecT = *ppsrec1;
	*ppsrec1 = *ppsrec2;
	*ppsrec2 = psrecT;
	}


INLINE VOID SWAPPmtnode( MTNODE **ppmtnode1, MTNODE **ppmtnode2 )
	{
	MTNODE *pmtnodeT;

	pmtnodeT = *ppmtnode1;
	*ppmtnode1 = *ppmtnode2;
	*ppmtnode2 = pmtnodeT;
	}


//  SORTIInsertionSort is a cache optimized version of the standard Insertion
//  sort.  It is used to sort small partitions for SORTIQuicksort because it
//  provides a statistical speed advantage over a pure Quicksort.

LOCAL VOID SORTIInsertionSort( SCB *pscb, SPAIR *pspairMinIn, SPAIR *pspairMaxIn )
	{
	SPAIR	*pspairLast;
	SPAIR	*pspairFirst;
	SPAIR	*pspairKey = pscb->rgspair + cspairSortMax;

	//  This loop is optimized so that we only scan for the current pair's new
	//  position if the previous pair in the list is greater than the current
	//  pair.  This avoids unnecessary pair copying for the key, which is
	//  expensive for sort pairs.

	for (	pspairFirst = pspairMinIn, pspairLast = pspairMinIn + 1;
			pspairLast < pspairMaxIn;
			pspairFirst = pspairLast++ )
		if ( ISORTICmpPspairPspair( pscb, pspairFirst, pspairLast ) > 0 )
			{
			//  save current pair as the "key"

			*pspairKey = *pspairLast;

			//  move previous pair into this pair's position

			*pspairLast = *pspairFirst;
			
			//  insert key into the (sorted) first part of the array (MinIn through
			//  Last - 1), moving already sorted pairs out of the way

			while (	--pspairFirst >= pspairMinIn &&
					( ISORTICmpPspairPspair( pscb, pspairFirst, pspairKey ) ) > 0 )
				{
				*( pspairFirst + 1 ) = *pspairFirst;
				}
			*( pspairFirst + 1 ) = *pspairKey;
			}
	}


//  SORTIQuicksort is a cache optimized Quicksort that sorts sort pair arrays
//  generated by ErrSORTInsert.  It is designed to sort large arrays of data
//  without any CPU data cache misses.  To do this, it uses a special comparator
//  designed to work with the sort pairs (see ISORTICmpPspairPspair).

LOCAL VOID SORTIQuicksort( SCB * pscb, SPAIR *pspairMinIn, SPAIR *pspairMaxIn )
	{
	//  partition stack
	struct _part
		{
		SPAIR	*pspairMin;
		SPAIR	*pspairMax;
		}	rgpart[cpartQSortMax];
	LONG	cpart		= 0;

	SPAIR	*pspairFirst;
	SPAIR	*pspairLast;

	//  current partition = partition passed in arguments

	SPAIR	*pspairMin	= pspairMinIn;
	SPAIR	*pspairMax	= pspairMaxIn;

	//  Quicksort current partition
	
	forever
		{
		//  if this partition is small enough, insertion sort it

		if ( pspairMax - pspairMin < cspairQSortMin )
			{
			SORTIInsertionSort( pscb, pspairMin, pspairMax );
			
			//  if there are no more partitions to sort, we're done

			if ( !cpart )
				break;

			//  pop a partition off the stack and make it the current partition

			pspairMin = rgpart[--cpart].pspairMin;	//lint !e530
			pspairMax = rgpart[cpart].pspairMax;	//lint !e530
			continue;
			}

		//  determine divisor by sorting the first, middle, and last pairs and
		//  taking the resulting middle pair as the divisor (stored in first place)

		pspairFirst	= pspairMin + ( ( pspairMax - pspairMin ) >> 1 );
		pspairLast	= pspairMax - 1;

		if ( ISORTICmpPspairPspair( pscb, pspairFirst, pspairMin ) > 0 )
			SWAPSpair( pspairFirst, pspairMin );
		if ( ISORTICmpPspairPspair( pscb, pspairFirst, pspairLast ) > 0 )
			SWAPSpair( pspairFirst, pspairLast );
		if ( ISORTICmpPspairPspair( pscb, pspairMin, pspairLast ) > 0 )
			SWAPSpair( pspairMin, pspairLast );

		//  sort large partition into two smaller partitions (<=, >)
		//
		//  NOTE:  we are not sorting the two end pairs as the first pair is the
		//  divisor and the last pair is already known to be > the divisor

		pspairFirst = pspairMin + 1;
		pspairLast--;

		Assert( pspairFirst <= pspairLast );
		
		forever
			{
			//  advance past all pairs <= the divisor
			
			while (	pspairFirst <= pspairLast &&
					ISORTICmpPspairPspair( pscb, pspairFirst, pspairMin ) <= 0 )
				pspairFirst++;

			//  advance past all pairs > the divisor
			
			while (	pspairFirst <= pspairLast &&
					ISORTICmpPspairPspair( pscb, pspairLast, pspairMin ) > 0 )
				pspairLast--;

			//  if we have found a pair to swap, swap them and continue

			Assert( pspairFirst != pspairLast );
			
			if ( pspairFirst < pspairLast )
				SWAPSpair( pspairFirst++, pspairLast-- );

			//  no more pairs to compare, partitioning complete
			
			else
				break;
			}

		//  place the divisor at the end of the <= partition

		if ( pspairLast != pspairMin )
			SWAPSpair( pspairMin, pspairLast );

		//  set first/last to delimit larger partition (as min/max) and set
		//  min/max to delimit smaller partition for next iteration

		if ( pspairMax - pspairLast - 1 > pspairLast - pspairMin )
			{
			pspairFirst	= pspairLast + 1;
			SWAPPspair( &pspairLast, &pspairMax );
			}
		else
			{
			pspairFirst	= pspairMin;
			pspairMin	= pspairLast + 1;
			}

		//  push the larger partition on the stack (recurse if there is no room)

		if ( cpart < cpartQSortMax )
			{
			rgpart[cpart].pspairMin		= pspairFirst;
			rgpart[cpart++].pspairMax	= pspairLast;
			}
		else
			SORTIQuicksort( pscb, pspairFirst, pspairLast );
		}
	}

//  Create a new run with the supplied parameters.  The new run's id and size
//  in pages is returned on success

LOCAL ERR ErrSORTIRunStart( SCB *pscb, QWORD cb, RUNINFO *pruninfo )
	{
	ERR				err;
	const QWORD		cpgAlloc	= ( cb + cbFreeSPAGE - 1 ) / cbFreeSPAGE;

	//	ensure we don't exceed max page count
	//
	if ( cpgAlloc > LONG_MAX )
		{
		CallR( ErrERRCheck( JET_errOutOfDatabaseSpace ) );
		}

	//  allocate space for new run according to given info

	pruninfo->run		= runNull;
	pruninfo->cpg		= CPG( cpgAlloc );
	pruninfo->cbRun		= 0;
	pruninfo->crecRun	= 0;
	pruninfo->cpgUsed	= pruninfo->cpg;

	CallR( ErrSPGetExt(
				pscb->fcb.Pfucb(),
				pscb->fcb.PgnoFDP(),
				&pruninfo->cpg,
				pruninfo->cpgUsed,
				&pruninfo->run ) );
	Assert( pruninfo->cpg >= pruninfo->cpgUsed );

	//  initialize output run data

	pscb->pgnoNext			= pruninfo->run;
	pscb->bflOut.pv			= NULL;
	pscb->bflOut.dwContext	= NULL;
	pscb->pbOutMac			= NULL;
	pscb->pbOutMax			= NULL;

	return JET_errSuccess;
	}


//  Inserts the given record into the run.  Records are stored compactly and
//  are permitted to cross page boundaries to avoid wasted space.

LOCAL ERR ErrSORTIRunInsert( SCB *pscb, RUNINFO* pruninfo, SREC *psrec )
	{
	ERR	  		err;
	LONG		cb;
	PGNO		pgnoNext;
	SPAGE_FIX	*pspage;
	LONG		cbToWrite;

	//  assumption:  record size < free sort page data size (and is valid)

	Assert(	CbSRECSizePsrec( psrec ) > CbSRECSizeCbCb( 0, 0 ) &&
			CbSRECSizePsrec( psrec ) < cbFreeSPAGE );

	//  calculate number of bytes that will fit on the current page

	cb = (LONG)min(	pscb->pbOutMax - pscb->pbOutMac, (LONG)CbSRECSizePsrec( psrec ) );

	//  if some data will fit, write it

	if ( cb )
		{
		UtilMemCpy( pscb->pbOutMac, psrec, cb );
		pscb->pbOutMac += cb;
		}

	//  all the data did not fit on this page

	if ( cb < (LONG) CbSRECSizePsrec( psrec ) )
		{
		//  page is full, so release it so it can be lazily-written to disk

		if ( pscb->bflOut.pv != NULL )
			{
			BFWriteUnlatch( &pscb->bflOut );
			pscb->bflOut.pv			= NULL;
			pscb->bflOut.dwContext	= NULL;
			}

		//  allocate a buffer for the next page in the run and latch it
		
		pgnoNext = pscb->pgnoNext++;

		CallR( ErrBFWriteLatchPage(	&pscb->bflOut,
									pscb->fcb.Pfucb()->ifmp,
									pgnoNext,
									bflfNew ) );
		BFDirty( &pscb->bflOut );

		//  initialize page

		pspage = (SPAGE_FIX *) pscb->bflOut.pv;

		pspage->pgnoThisPage = pgnoNext;

		//  initialize data pointers for this page

		pscb->pbOutMac = PbDataStartPspage( pspage );
		pscb->pbOutMax = PbDataEndPspage( pspage );

		//  write the remainder of the data to this page

		cbToWrite = CbSRECSizePsrec( psrec ) - cb;
		UtilMemCpy( pscb->pbOutMac, ( (BYTE *) psrec ) + cb, cbToWrite );
		pscb->pbOutMac += cbToWrite;
		}

	//  update this run's stats

	pruninfo->cbRun += CbSRECSizePsrec( psrec );
	pruninfo->crecRun++;
	return JET_errSuccess;
	}


//  ends current output run

INLINE VOID SORTIRunEnd( SCB * pscb, RUNINFO* pruninfo )
	{
	//  unlatch page so it can be lazily-written to disk
	
	if ( pscb->bflOut.pv != NULL )
		{
		BFWriteUnlatch( &pscb->bflOut );
		pscb->bflOut.pv			= NULL;
		pscb->bflOut.dwContext	= NULL;
		}

	//  trim our space usage for this run

	pruninfo->cpgUsed = CPG( ( pruninfo->cbRun + cbFreeSPAGE - 1 ) / cbFreeSPAGE );

	if ( pruninfo->cpg - pruninfo->cpgUsed > 0 )
		{
		FUCB * const	pfucbT		= pscb->fcb.Pfucb();
		Assert( !FFUCBSpace( pfucbT ) );

		const ERR		errFreeExt	= ErrSPFreeExt(
											pfucbT,
											pruninfo->run + pruninfo->cpgUsed,
											pruninfo->cpg - pruninfo->cpgUsed );
#ifdef DEBUG
		if ( !FSPExpectedError( errFreeExt ) )
			{
			CallS( errFreeExt );
			}
#endif
		}

	pruninfo->cpg = pruninfo->cpgUsed;
	}


//  Deletes a run from disk.  No error is returned because if delete fails,
//  it is not fatal (only wasted space in the temporary database).

INLINE VOID SORTIRunDelete( SCB * pscb, const RUNINFO * pruninfo )
	{
	//  delete run

	if ( pruninfo->cpg )
		{
		FUCB * const	pfucbT		= pscb->fcb.Pfucb();
		Assert( !FFUCBSpace( pfucbT ) );

		const ERR		errFreeExt	= ErrSPFreeExt(
											pfucbT,
											pruninfo->run,
											pruninfo->cpg );
#ifdef DEBUG
		if ( !FSPExpectedError( errFreeExt ) )
			{
			CallS( errFreeExt );
			}
#endif
		}
	}


//  Deletes crun runs in the specified run list, if possible

LOCAL VOID	SORTIRunDeleteList( SCB *pscb, RUNLINK **pprunlink, LONG crun )
	{
	LONG	irun;

	//  walk list, deleting runs

	for ( irun = 0; *pprunlink != prunlinkNil && irun < crun; irun++ )
		{
		//  delete run
		
		SORTIRunDelete( pscb, &( *pprunlink )->runinfo );

		//  get next run to free

		RUNLINK	* prunlinkT = *pprunlink;
		*pprunlink = ( *pprunlink )->prunlinkNext;

		//  free RUNLINK

		RUNLINKReleasePrunlink( prunlinkT );
		}
	}


//  Deletes the memory for crun runs in the specified run list, but does not
//  bother to delete the runs from disk

LOCAL VOID	SORTIRunDeleteListMem( SCB *pscb, RUNLINK **pprunlink, LONG crun )
	{
	LONG	irun;

	//  walk list, deleting runs

	for ( irun = 0; *pprunlink != prunlinkNil && irun < crun; irun++ )
		{
		//  get next run to free

		RUNLINK	* prunlinkT = *pprunlink;
		*pprunlink = ( *pprunlink )->prunlinkNext;

		//  free RUNLINK

		RUNLINKReleasePrunlink( prunlinkT );
		}
	}


//  Opens the specified run for reading.

LOCAL ERR ErrSORTIRunOpen( SCB *pscb, RUNINFO *pruninfo, RCB **pprcb )
	{
	ERR		err;
	RCB		*prcb	= prcbNil;
	LONG	ipbf;
	CPG		cpgRead;
	
	//  allocate a new RCB

	if ( ( prcb = PrcbRCBAlloc() ) == prcbNil )
		Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );

	//  initialize RCB

	prcb->pscb = pscb;
	prcb->runinfo = *pruninfo;
	
	for ( ipbf = 0; ipbf < cpgClusterSize; ipbf++ )
		{
		prcb->rgbfl[ipbf].pv		= NULL;
		prcb->rgbfl[ipbf].dwContext	= NULL;
		}

	prcb->ipbf			= cpgClusterSize;
	prcb->pbInMac		= NULL;
	prcb->pbInMax		= NULL;
	prcb->cbRemaining	= prcb->runinfo.cbRun;
	prcb->pvAssy		= NULL;

	//  preread the first part of the run, to be access paged later as required

	cpgRead = min( prcb->runinfo.cpgUsed, 2 * cpgClusterSize );

	BFPrereadPageRange(	pscb->fcb.Pfucb()->ifmp,
						(PGNO) prcb->runinfo.run,
						cpgRead );

	//  return the initialized RCB

	*pprcb = prcb;
	return JET_errSuccess;

HandleError:
	*pprcb = prcbNil;
	return err;
	}


//  Returns next record in opened run (the first if the run was just opened).
//  Returns JET_errNoCurrentRecord if all records have been read.  The record
//  retrieved during the previous call is guaranteed to still be in memory
//  after this call for the purpose of duplicate removal comparisons.
//
//  Special care must be taken when reading the records because they could
//  be broken at arbitrary points across page boundaries.  If this happens,
//  the record is assembled in a temporary buffer, to which the pointer is
//  returned.  This memory is freed by this function or ErrSORTIRunClose.

LOCAL ERR ErrSORTIRunNext( RCB * prcb, SREC **ppsrec )
	{
	ERR		err;
	SCB		*pscb = prcb->pscb;
	SIZE_T	cbUnread;
	SHORT	cbRec;
	SPAGE_FIX	*pspage;
	LONG	ipbf;
	PGNO	pgnoNext;
	CPG		cpgRead;
	SIZE_T	cbToRead;

	//  free second to last assembly buffer, if present, and make last
	//  assembly buffer the second to last assembly buffer

	if ( pscb->pvAssyLast != NULL )
		{
		BFFree( pscb->pvAssyLast );
		}
	pscb->pvAssyLast = prcb->pvAssy;
	prcb->pvAssy = NULL;

	//  abandon last buffer, if present

	if ( pscb->bflLast.pv != NULL )
		{
		CLockDeadlockDetectionInfo::DisableOwnershipTracking();
		BFPurge( &pscb->bflLast, fTrue );
		CLockDeadlockDetectionInfo::EnableOwnershipTracking();
		pscb->bflLast.pv		= NULL;
		pscb->bflLast.dwContext	= NULL;
		}
	
	//  are there no more records to read?

	if ( !prcb->cbRemaining )
		{
		//  make sure we don't hold on to the last page of the run

		if ( prcb->rgbfl[prcb->ipbf].pv != NULL )
			{
			pscb->bflLast.pv		= prcb->rgbfl[prcb->ipbf].pv;
			pscb->bflLast.dwContext	= prcb->rgbfl[prcb->ipbf].dwContext;
			prcb->rgbfl[prcb->ipbf].pv			= NULL;
			prcb->rgbfl[prcb->ipbf].dwContext	= NULL;
			}
			
		//  return No Current Record
		
		Error( ErrERRCheck( JET_errNoCurrentRecord ), HandleError );
		}
	
	//  calculate size of unread data still in page

	cbUnread = prcb->pbInMax - prcb->pbInMac;

	//  is there any more data on this page?

	if ( cbUnread )
		{
		//  if the record is entirely on this page, return it

		if (	cbUnread > cbSRECReadMin &&
				(LONG) CbSRECSizePsrec( (SREC *) prcb->pbInMac ) <= cbUnread )
			{
			cbRec = (SHORT) CbSRECSizePsrec( (SREC *) prcb->pbInMac );
			*ppsrec = (SREC *) prcb->pbInMac;
			prcb->pbInMac += cbRec;
			prcb->cbRemaining -= cbRec;
			return JET_errSuccess;
			}

		//  allocate a new assembly buffer

		BFAlloc( &prcb->pvAssy );

		//  copy what there is of the record on this page into assembly buffer

		UtilMemCpy( prcb->pvAssy, prcb->pbInMac, cbUnread );
		prcb->cbRemaining -= cbUnread;
		}

	//  get next page number

	if ( prcb->ipbf < cpgClusterSize )
		{
		//  next page is sequentially after the used up buffer's page number
		
		pgnoNext = ( ( SPAGE_FIX * )prcb->rgbfl[prcb->ipbf].pv )->pgnoThisPage + 1;
		
		//  move the used up buffer to the last buffer
		//  to guarantee validity of record read last call

		pscb->bflLast.pv		= prcb->rgbfl[prcb->ipbf].pv;
		pscb->bflLast.dwContext	= prcb->rgbfl[prcb->ipbf].dwContext;
		prcb->rgbfl[prcb->ipbf].pv			= NULL;
		prcb->rgbfl[prcb->ipbf].dwContext	= NULL;
		}
	else
		{
		//  no pages are resident yet, so next page is the first page in the run
		
		pgnoNext = (PGNO) prcb->runinfo.run;
		}

	//  is there another pinned buffer available?

	if ( ++prcb->ipbf < cpgClusterSize )
		{
		//  yes, then this pbf should never be null

		Assert( prcb->rgbfl[prcb->ipbf].pv != NULL );
		Assert( prcb->rgbfl[prcb->ipbf].dwContext != NULL );
		
		//  set new page data pointers

		pspage = (SPAGE_FIX *) prcb->rgbfl[prcb->ipbf].pv;
		prcb->pbInMac = PbDataStartPspage( pspage );
		prcb->pbInMax = PbDataEndPspage( pspage );
		}
	else
		{
		//  no, get and pin all buffers that were read ahead last time

		cpgRead = min(	(LONG) ( prcb->runinfo.run + prcb->runinfo.cpgUsed - pgnoNext ),
						cpgClusterSize );
		Assert( cpgRead > 0 );
		
		for ( ipbf = 0; ipbf < cpgRead; ipbf++ )
			Call( ErrSORTIRunReadPage( prcb, pgnoNext + ipbf, ipbf ) );

		//  set new page data pointers

		prcb->ipbf		= 0;
		pspage			= (SPAGE_FIX *) prcb->rgbfl[prcb->ipbf].pv;
		prcb->pbInMac	= PbDataStartPspage( pspage );
		prcb->pbInMax	= PbDataEndPspage( pspage );
		
		//  issue prefetch for next cluster (if needed)

		pgnoNext += cpgClusterSize;
		cpgRead = min(	(LONG) ( prcb->runinfo.run + prcb->runinfo.cpgUsed - pgnoNext ),
						cpgClusterSize );
		if ( cpgRead > 0 )
			{
			Assert( pgnoNext >= prcb->runinfo.run );
			Assert(	pgnoNext + cpgRead - 1 <= prcb->runinfo.run + prcb->runinfo.cpgUsed - 1 );
			BFPrereadPageRange(	pscb->fcb.Pfucb()->ifmp,
								pgnoNext,
								cpgRead );
			}
		}

	//  if there was no data last time, entire record must be at the top of the
	//  page, so return it

	if ( !cbUnread )
		{
		cbRec = (SHORT) CbSRECSizePsrec( (SREC *) prcb->pbInMac );
		Assert( cbRec > (LONG) CbSRECSizeCbCb( 0, 0 ) && cbRec < cbFreeSPAGE );
		*ppsrec = (SREC *) prcb->pbInMac;
		prcb->pbInMac += cbRec;
		prcb->cbRemaining -= cbRec;
		return JET_errSuccess;
		}

	//  if we couldn't get the record size from the last page, copy enough data
	//  to the assembly buffer to get the record size

	if ( cbUnread < cbSRECReadMin )
		UtilMemCpy(	( (BYTE *) prcb->pvAssy ) + cbUnread,
				prcb->pbInMac,
				cbSRECReadMin - cbUnread );

	//  if not, copy remainder of record into assembly buffer

	cbToRead = CbSRECSizePsrec( (SREC *) prcb->pvAssy ) - cbUnread;
	UtilMemCpy( ( (BYTE *) prcb->pvAssy ) + cbUnread, prcb->pbInMac, cbToRead );
	prcb->pbInMac += cbToRead;
	prcb->cbRemaining -= cbToRead;

	//  return pointer to assembly buffer

	*ppsrec = (SREC *) prcb->pvAssy;
	return JET_errSuccess;

HandleError:
	for ( ipbf = 0; ipbf < cpgClusterSize; ipbf++ )
		if ( prcb->rgbfl[ipbf].pv != NULL )
			{
			CLockDeadlockDetectionInfo::DisableOwnershipTracking();
			BFPurge( &prcb->rgbfl[ipbf], fTrue );
			CLockDeadlockDetectionInfo::EnableOwnershipTracking();
			prcb->rgbfl[ipbf].pv		= NULL;
			prcb->rgbfl[ipbf].dwContext	= NULL;
			}
	*ppsrec = NULL;
	return err;
	}


//  Closes an opened run

LOCAL VOID SORTIRunClose( RCB *prcb )
	{
	LONG	ipbf;
	
	//  free record assembly buffer

	if ( prcb->pvAssy != NULL )
		{
		BFFree( prcb->pvAssy );
		}

	//  unpin all read-ahead buffers
	
	for ( ipbf = 0; ipbf < cpgClusterSize; ipbf++ )
		if ( prcb->rgbfl[ipbf].pv != NULL )
			{
			CLockDeadlockDetectionInfo::DisableOwnershipTracking();
			BFPurge( &prcb->rgbfl[ipbf], fTrue );
			CLockDeadlockDetectionInfo::EnableOwnershipTracking();
			prcb->rgbfl[ipbf].pv		= NULL;
			prcb->rgbfl[ipbf].dwContext	= NULL;
			}

	//  free RCB
	
	RCBReleasePrcb( prcb );
	}


//  get read access to a page in a run (buffer is pinned in memory)

INLINE ERR ErrSORTIRunReadPage( RCB *prcb, PGNO pgno, LONG ipbf )
	{
	ERR		err;

	//  verify that we are trying to read a page that is used in the run

	Assert( pgno >= prcb->runinfo.run );
	Assert( pgno < prcb->runinfo.run + prcb->runinfo.cpgUsed );
	
	//  read page

	CLockDeadlockDetectionInfo::DisableOwnershipTracking();
	err = ErrBFReadLatchPage(	&prcb->rgbfl[ipbf],
								prcb->pscb->fcb.Pfucb()->ifmp,
								pgno,
								bflfNoTouch );
	CLockDeadlockDetectionInfo::EnableOwnershipTracking();

	return err;
	}


//  Merges the specified number of runs from the source list into a new run in
//  the destination list

LOCAL ERR ErrSORTIMergeToRun( SCB *pscb, RUNLINK *prunlinkSrc, RUNLINK **pprunlinkDest )
	{
	ERR		err;
	LONG	irun;
	QWORD	cbRun;
	RUNLINK	*prunlink = prunlinkNil;
	SREC	*psrec;

	//  initialize merge
	
	CallR( ErrSORTIMergeStart( pscb, prunlinkSrc ) );

	//  calculate new run size

	for ( cbRun = 0, irun = 0; irun < pscb->crunMerge; irun++ )
		{
		cbRun += pscb->rgmtnode[irun].prcb->runinfo.cbRun;
		}

	//  create a new run to receive merge data

	if ( ( prunlink = PrunlinkRUNLINKAlloc() ) == prunlinkNil )
		Error( ErrERRCheck( JET_errOutOfMemory ), EndMerge );

	CallJ( ErrSORTIRunStart( pscb, cbRun, &prunlink->runinfo ), FreeRUNLINK );

	//  stream data from merge into run

	while ( ( err = ErrSORTIMergeNext( pscb, &psrec ) ) >= 0 )
		CallJ( ErrSORTIRunInsert( pscb, &prunlink->runinfo, psrec ), DeleteRun );

	if ( err < 0 && err != JET_errNoCurrentRecord )
		goto DeleteRun;

	SORTIRunEnd( pscb, &prunlink->runinfo );
	SORTIMergeEnd( pscb );

	//  add new run to destination run list

	prunlink->prunlinkNext = *pprunlinkDest;
	*pprunlinkDest = prunlink;

	return JET_errSuccess;

DeleteRun:
	SORTIRunEnd( pscb, &prunlink->runinfo );
	SORTIRunDelete( pscb, &prunlink->runinfo );
FreeRUNLINK:
	RUNLINKReleasePrunlink( prunlink );
EndMerge:
	SORTIMergeEnd( pscb );
	return err;
	}


/*	starts an n-way merge of the first n runs from the source run list.  The merge
/*	will remove duplicate values from the output if desired.
/**/
LOCAL ERR ErrSORTIMergeStart( SCB *pscb, RUNLINK *prunlinkSrc )
	{
	ERR		err;
	RUNLINK	*prunlink;
	LONG	crun;
	LONG	irun;
	MTNODE	*pmtnode;
#if defined( DEBUG ) || defined( PERFDUMP )
	char	szT[1024];
#endif

	/*	if termination in progress, then fail sort
	/**/
	if ( PinstFromIfmp( pscb->fcb.Ifmp() )->m_fTermInProgress )
		return ErrERRCheck( JET_errTermInProgress );

	/*	determine number of runs to merge
	/**/
	prunlink = prunlinkSrc;
	crun = 1;
	while ( prunlink->prunlinkNext != prunlinkNil )
		{
		prunlink = prunlink->prunlinkNext;
		crun++;
		}

	/*	we only support merging two or more runs
	/**/
	Assert( crun > 1 );

	/*	init merge data in SCB
	/**/
	pscb->crunMerge				= crun;
	pscb->bflLast.pv			= NULL;
	pscb->bflLast.dwContext		= NULL;
	pscb->pvAssyLast			= NULL;

#if defined( DEBUG ) || defined( PERFDUMP )
	sprintf( szT, "MERGE:  %ld runs -", crun );
#endif
	
	/*	initialize merge tree
	/**/
	prunlink = prunlinkSrc;
	for ( irun = 0; irun < crun; irun++ )
		{
		//  initialize external node

		pmtnode = pscb->rgmtnode + irun;
		Call( ErrSORTIRunOpen( pscb, &prunlink->runinfo, &pmtnode->prcb ) );
		pmtnode->pmtnodeExtUp = pscb->rgmtnode + ( irun + crun ) / 2;
		
		//  initialize internal node

		pmtnode->psrec = psrecNegInf;
		pmtnode->pmtnodeSrc = pmtnode;
		pmtnode->pmtnodeIntUp = pscb->rgmtnode + irun / 2;
		
#if defined( DEBUG ) || defined( PERFDUMP )
		sprintf(	szT + strlen( szT ),
					" %ld(%ld)",
					pmtnode->prcb->runinfo.run,
					pmtnode->prcb->runinfo.cpgUsed );
#endif

		//  get next run to open

		prunlink = prunlink->prunlinkNext;
		}

	return JET_errSuccess;

HandleError:
	pscb->crunMerge = 0;
	for ( irun--; irun >= 0; irun-- )
		SORTIRunClose( pscb->rgmtnode[irun].prcb );
	return err;
	}


//  Returns the first record of the current merge.  This function can be called
//  any number of times before ErrSORTIMergeNext is called to return the first
//  record, but it cannot be used to rewind to the first record after
//  ErrSORTIMergeNext is called.

LOCAL ERR ErrSORTIMergeFirst( SCB *pscb, SREC **ppsrec )
	{
	ERR		err;
	
	//  if the tree still has init records, read past them to first record

	while ( pscb->rgmtnode[0].psrec == psrecNegInf )
		Call( ErrSORTIMergeNextChamp( pscb, ppsrec ) );

	//  return first record

	*ppsrec = pscb->rgmtnode[0].psrec;

	return JET_errSuccess;

HandleError:
	Assert( err != JET_errNoCurrentRecord );
	*ppsrec = NULL;
	return err;
	}


//  Returns the next record of the current merge, or JET_errNoCurrentRecord
//  if no more records are available.  You can call this function without
//  calling ErrSORTIMergeFirst to get the first record.

LOCAL ERR ErrSORTIMergeNext( SCB *pscb, SREC **ppsrec )
	{
	ERR		err;
	SREC	*psrecLast;
	
	//  if the tree still has init records, return first record

	if ( pscb->rgmtnode[0].psrec == psrecNegInf )
		return ErrSORTIMergeFirst( pscb, ppsrec );

	//  get next record, performing duplicate removal

	do	{
		psrecLast = pscb->rgmtnode[0].psrec;
		CallR( ErrSORTIMergeNextChamp( pscb, ppsrec ) );
		}
	while ( FSORTIDuplicate( pscb, *ppsrec, psrecLast ) );

	return JET_errSuccess;
	}


//  Ends the current merge operation

LOCAL VOID SORTIMergeEnd( SCB *pscb )
	{
	LONG	irun;

	//  free / abandon BFs
	
	if ( pscb->bflLast.pv != NULL )
		{
		CLockDeadlockDetectionInfo::DisableOwnershipTracking();
		BFPurge( &pscb->bflLast, fTrue );
		CLockDeadlockDetectionInfo::EnableOwnershipTracking();
		pscb->bflLast.pv		= NULL;
		pscb->bflLast.dwContext	= NULL;
		}
	if ( pscb->pvAssyLast != NULL )
		{
		BFFree( pscb->pvAssyLast );
		pscb->pvAssyLast = NULL;
		}

	//  close all input runs
	
	for ( irun = 0; irun < pscb->crunMerge; irun++ )
		SORTIRunClose( pscb->rgmtnode[irun].prcb );
	pscb->crunMerge = 0;
	}


//  Returns next champion of the replacement-selection tournament on input
//  data.  If there is no more data, it will return JET_errNoCurrentRecord.
//  The tree is stored in losers' representation, meaning that the loser of
//  each tournament is stored at each node, not the winner.

LOCAL ERR ErrSORTIMergeNextChamp( SCB *pscb, SREC **ppsrec )
	{
	ERR		err;
	MTNODE	*pmtnodeChamp;
	MTNODE	*pmtnodeLoser;

	//  goto exterior source node of last champ

	pmtnodeChamp = pscb->rgmtnode + 0;
	pmtnodeLoser = pmtnodeChamp->pmtnodeSrc;

	//  read next record (or lack thereof) from input run as the new
	//  contender for champ

	*ppsrec = NULL;
	err = ErrSORTIRunNext( pmtnodeLoser->prcb, &pmtnodeChamp->psrec );
	if ( err < 0 && err != JET_errNoCurrentRecord )
		return err;

	//  go up tree to first internal node

	pmtnodeLoser = pmtnodeLoser->pmtnodeExtUp;

	//  select the new champion by walking up the tree, swapping for lower
	//  and lower keys (or sentinel values)

	do	{
		//  if loser is psrecInf or champ is psrecNegInf, do not swap (if this
		//  is the case, we can't do better than we have already)

		if ( pmtnodeLoser->psrec == psrecInf || pmtnodeChamp->psrec == psrecNegInf )
			continue;

		//  if the loser is psrecNegInf or the current champ is psrecInf, or the
		//  loser is less than the champ, swap records

		if (	pmtnodeChamp->psrec == psrecInf ||
				pmtnodeLoser->psrec == psrecNegInf ||
				ISORTICmpKeyData( pmtnodeLoser->psrec, pmtnodeChamp->psrec ) < 0 )
			{
			SWAPPsrec( &pmtnodeLoser->psrec, &pmtnodeChamp->psrec );
			SWAPPmtnode( &pmtnodeLoser->pmtnodeSrc, &pmtnodeChamp->pmtnodeSrc );
			}
		}
	while ( ( pmtnodeLoser = pmtnodeLoser->pmtnodeIntUp ) != pmtnodeChamp );

	//  return the new champion

	if ( ( *ppsrec = pmtnodeChamp->psrec ) == NULL )
		return ErrERRCheck( JET_errNoCurrentRecord );

	return JET_errSuccess;
	}


//  initializes optimized tree merge

INLINE VOID SORTIOptTreeInit( SCB *pscb )
	{
	//  initialize runlist

	pscb->runlist.prunlinkHead		= prunlinkNil;
	pscb->runlist.crun				= 0;
	}


//  adds an initial run to be merged by optimized tree merge process

LOCAL ERR ErrSORTIOptTreeAddRun( SCB *pscb, RUNINFO *pruninfo )
	{
	RUNLINK	*prunlink;

	//  allocate and build a new RUNLINK for the new run

	if ( ( prunlink = PrunlinkRUNLINKAlloc() ) == prunlinkNil )
		return ErrERRCheck( JET_errOutOfMemory );
	prunlink->runinfo = *pruninfo;

	//  add the new run to the disk-resident runlist
	//
	//  NOTE:  by adding at the head of the list, we will guarantee that the
	//         list will be in ascending order by record count

	prunlink->prunlinkNext = pscb->runlist.prunlinkHead;
	pscb->runlist.prunlinkHead = prunlink;
	pscb->runlist.crun++;
	pscb->crun++;

	return JET_errSuccess;
	}


//  Performs an optimized tree merge of all runs previously added with
//  ErrSORTIOptTreeAddRun down to the last merge level (which is reserved
//  to be computed through the SORT iterators).  This algorithm is designed
//  to use the maximum fan-in as much as possible.

LOCAL ERR ErrSORTIOptTreeMerge( SCB *pscb )
	{
	ERR		err;
	OTNODE	*potnode = potnodeNil;
	
	//  If there are less than or equal to crunFanInMax runs, there is only
	//  one merge level -- the last one, which is to be done via the SORT
	//  iterators.  We are done.

	if ( pscb->runlist.crun <= crunFanInMax )
		return JET_errSuccess;

	//  build the optimized tree merge tree

	CallR( ErrSORTIOptTreeBuild( pscb, &potnode ) );

	//  perform all but the final merge

	Call( ErrSORTIOptTreeMergeDF( pscb, potnode, NULL ) );

	//  update the runlist information for the final merge

	Assert( pscb->runlist.crun == 0 );
	Assert( pscb->runlist.prunlinkHead == prunlinkNil );
	Assert( potnode->runlist.crun == crunFanInMax );
	Assert( potnode->runlist.prunlinkHead != prunlinkNil );
	pscb->runlist = potnode->runlist;

	//  free last node and return

	OTNODEReleasePotnode( potnode );
	return JET_errSuccess;

HandleError:
	if ( potnode != potnodeNil )
		{
		SORTIOptTreeFree( pscb, potnode );
		OTNODEReleasePotnode( potnode );
		}
	return err;
	}


//  free all optimized tree merge resources

INLINE VOID SORTIOptTreeTerm( SCB *pscb )
	{
	//  delete all runlists

	SORTIRunDeleteListMem( pscb, &pscb->runlist.prunlinkHead, crunAll );
	}


//  Builds the optimized tree merge tree by level in such a way that we use the
//  maximum fan-in as often as possible and the smallest merges (by length in
//  records) will be on the left side of the tree (smallest index in the array).
//  This will provide very high BF cache STATICity when the merge is performed
//  depth first, visiting subtrees left to right.

LOCAL ERR ErrSORTIOptTreeBuild( SCB *pscb, OTNODE **ppotnode )
	{
	ERR		err;
	OTNODE	*potnodeAlloc	= potnodeNil;
	OTNODE	*potnodeT;
	OTNODE	*potnodeLast2;
	LONG	crunLast2;
	OTNODE	*potnodeLast;
	LONG	crunLast;
	OTNODE	*potnodeThis;
	LONG	crunThis;
	LONG	crunFanInFirst;
	OTNODE	*potnodeFirst;
	LONG	ipotnode;
	LONG	irun;

	//  Set the original number of runs left for us to use.  If a last level
	//  pointer is potnodeLevel0, this means that we should use original runs for
	//  making the new merge level.  These runs come from this number.  We do
	//  not actually assign original runs to merge nodes until we actually
	//  perform the merge.

	potnodeLast2	= potnodeNil;
	crunLast2		= 0;
	potnodeLast		= potnodeLevel0;
	crunLast		= pscb->crun;
	potnodeThis		= potnodeNil;
	crunThis		= 0;

	//  create levels until the last level has only one node (the root node)

	do	{
		//  Create the first merge of this level, using a fan in that will result
		//  in the use of the maximum fan in as much as possible during the merge.
		//  We calculate this value every level, but it should only be less than
		//  the maximum fan in for the first merge level (but doesn't have to be).

		//  number of runs to merge

		if ( crunLast2 + crunLast <= crunFanInMax )
			crunFanInFirst = crunLast2 + crunLast;
		else
			crunFanInFirst = 2 + ( crunLast2 + crunLast - crunFanInMax - 1 ) % ( crunFanInMax - 1 );
		Assert( potnodeLast == potnodeLevel0 || crunFanInFirst == crunFanInMax );

		//  allocate and initialize merge node
		
		if ( ( potnodeT = PotnodeOTNODEAlloc() ) == potnodeNil )
			Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
		memset( potnodeT, 0, sizeof( OTNODE ) );
		potnodeT->potnodeAllocNext = potnodeAlloc;
		potnodeAlloc = potnodeT;
		ipotnode = 0;

		//  Add any leftover runs from the second to last level (the level before
		//  the last level) to the first merge of this level.

		Assert( crunLast2 < crunFanInMax );

		if ( potnodeLast2 == potnodeLevel0 )
			{
			Assert( potnodeT->runlist.crun == 0 );
			potnodeT->runlist.crun = crunLast2;
			}
		else
			{
			while ( potnodeLast2 != potnodeNil )
				{
				Assert( ipotnode < crunFanInMax );
				potnodeT->rgpotnode[ipotnode++] = potnodeLast2;
				potnodeLast2 = potnodeLast2->potnodeLevelNext;
				}
			}
		crunFanInFirst -= crunLast2;
		crunLast2 = 0;
			
		//  take runs from last level

		if ( potnodeLast == potnodeLevel0 )
			{
			Assert( potnodeT->runlist.crun == 0 );
			potnodeT->runlist.crun = crunFanInFirst;
			}
		else
			{
			for ( irun = 0; irun < crunFanInFirst; irun++ )
				{
				Assert( ipotnode < crunFanInMax );
				potnodeT->rgpotnode[ipotnode++] = potnodeLast;
				potnodeLast = potnodeLast->potnodeLevelNext;
				}
			}
		crunLast -= crunFanInFirst;

		//  save this node to add to this level later

		potnodeFirst = potnodeT;

		//  Create as many full merges for this level as possible, using the
		//  maximum fan in.
		
		while ( crunLast >= crunFanInMax )
			{
			//  allocate and initialize merge node

			if ( ( potnodeT = PotnodeOTNODEAlloc() ) == potnodeNil )
				Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
			memset( potnodeT, 0, sizeof( OTNODE ) );
			potnodeT->potnodeAllocNext = potnodeAlloc;
			potnodeAlloc = potnodeT;
			ipotnode = 0;

			//  take runs from last level

			if ( potnodeLast == potnodeLevel0 )
				{
				Assert( potnodeT->runlist.crun == 0 );
				potnodeT->runlist.crun = crunFanInMax;
				}
			else
				{
				for ( irun = 0; irun < crunFanInMax; irun++ )
					{
					Assert( ipotnode < crunFanInMax );
					potnodeT->rgpotnode[ipotnode++] = potnodeLast;
					potnodeLast = potnodeLast->potnodeLevelNext;
					}
				}
			crunLast -= crunFanInMax;

			//  add this node to the current level

			potnodeT->potnodeLevelNext = potnodeThis;
			potnodeThis = potnodeT;
			crunThis++;
			}

		//  add the first merge to the current level

		potnodeFirst->potnodeLevelNext = potnodeThis;
		potnodeThis = potnodeFirst;
		crunThis++;

		//  Move level history back one level in preparation for next level.

		Assert( potnodeLast2 == potnodeNil || potnodeLast2 == potnodeLevel0 );
		Assert( crunLast2 == 0 );
		
		potnodeLast2	= potnodeLast;
		crunLast2		= crunLast;
		potnodeLast		= potnodeThis;
		crunLast		= crunThis;
		potnodeThis		= potnodeNil;
		crunThis		= 0;
		}
	while ( crunLast2 + crunLast > 1 );

	//  verify that all nodes / runs were used

	Assert( potnodeLast2 == potnodeNil || potnodeLast2 == potnodeLevel0 );
	Assert( crunLast2 == 0 );
	Assert(	potnodeLast != potnodeNil
			&& potnodeLast->potnodeLevelNext == potnodeNil );
	Assert( crunLast == 1 );

	//  return root node pointer

	*ppotnode = potnodeLast;
	return JET_errSuccess;
	
HandleError:
	while ( potnodeAlloc != potnodeNil )
		{
		SORTIRunDeleteListMem( pscb, &potnodeAlloc->runlist.prunlinkHead, crunAll );
		potnodeT = potnodeAlloc->potnodeAllocNext;
		OTNODEReleasePotnode( potnodeAlloc );
		potnodeAlloc = potnodeT;
		}
	*ppotnode = potnodeNil;
	return err;
	}

//  Performs an optimized tree merge depth first according to the provided
//  optimized tree.  When pprunlink is NULL, the current level is not
//  merged (this is used to save the final merge for the SORT iterator).

LOCAL ERR ErrSORTIOptTreeMergeDF( SCB *pscb, OTNODE *potnode, RUNLINK **pprunlink )
	{
	ERR		err;
	LONG	crunPhantom = 0;
	LONG	ipotnode;
	LONG	irun;
	RUNLINK	*prunlinkNext;

	//  if we have phantom runs, save how many so we can get them later

	if ( potnode->runlist.prunlinkHead == prunlinkNil )
		crunPhantom = potnode->runlist.crun;

	//  recursively merge all trees below this node

	for ( ipotnode = 0; ipotnode < crunFanInMax; ipotnode++ )
		{
		//  if this subtree pointer is potnodeNil, skip it

		if ( potnode->rgpotnode[ipotnode] == potnodeNil )
			continue;

		//  merge this subtree

		CallR( ErrSORTIOptTreeMergeDF(	pscb,
										potnode->rgpotnode[ipotnode],
										&potnode->runlist.prunlinkHead ) );
		OTNODEReleasePotnode( potnode->rgpotnode[ipotnode] );
		potnode->rgpotnode[ipotnode] = potnodeNil;
		potnode->runlist.crun++;
		}

	//  If this node has phantom (unbound) runs, we must grab the runs to merge
	//  from the list of original runs.  This is done to ensure that we use the
	//  original runs in the reverse order that they were generated to maximize
	//  the possibility of a BF cache hit.

	if ( crunPhantom > 0 )
		{
		for ( irun = 0; irun < crunPhantom; irun++ )
			{
			prunlinkNext = pscb->runlist.prunlinkHead->prunlinkNext;
			pscb->runlist.prunlinkHead->prunlinkNext = potnode->runlist.prunlinkHead;
			potnode->runlist.prunlinkHead = pscb->runlist.prunlinkHead;
			pscb->runlist.prunlinkHead = prunlinkNext;
			}
		pscb->runlist.crun -= crunPhantom;
		}

	//  merge all runs for this node

	if ( pprunlink != NULL )
		{
		//  merge the runs in the runlist
		
		CallR( ErrSORTIMergeToRun(	pscb,
									potnode->runlist.prunlinkHead, 
									pprunlink ) );
		SORTIRunDeleteList( pscb, &potnode->runlist.prunlinkHead, crunAll );
		potnode->runlist.crun = 0;
		}

	return JET_errSuccess;
	}


//  frees an optimized tree merge tree (except the given OTNODE's memory)

LOCAL VOID SORTIOptTreeFree( SCB *pscb, OTNODE *potnode )
	{
	LONG	ipotnode;

	//  recursively free all trees below this node

	for ( ipotnode = 0; ipotnode < crunFanInMax; ipotnode++ )
		{
		if ( potnode->rgpotnode[ipotnode] == potnodeNil )
			continue;

		SORTIOptTreeFree( pscb, potnode->rgpotnode[ipotnode] );
		OTNODEReleasePotnode( potnode->rgpotnode[ipotnode] );
		}

	//  free all runlists for this node

	SORTIRunDeleteListMem( pscb, &potnode->runlist.prunlinkHead, crunAll );
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\slv.cxx ===
#include "std.hxx"

#ifdef DISABLE_SLV
#else

#define SLV_USE_VIEWS			//  use views of an SLV File to log data when
								//  the SLV Provider is enabled.  views allow
								//  us to avoid a memory copy but risk page
								//  faults on large files that have pages that
								//  have fallen out of memory.  views also save
								//  us from reopening an SLV File for I/O

//#define SLV_USE_RAW_FILE		//  use the backing file to log data when the SLV
								//  Provider is enabled.  use if and only if the
								//  streaming file is cached and not each SLV
								//  File.  reading from the backing file in this
								//  way saves us from opening or reopening the
								//  SLV File for I/O



//  ================================================================
CGROWABLEARRAY::CGROWABLEARRAY() :
	m_culEntries( 0 )
//  ================================================================
	{
	INT ipul;
	for( ipul = 0; ipul < m_cpul; ++ipul )
		{
		m_rgpul[ipul] = NULL;
		}
	}

	
//  ================================================================
CGROWABLEARRAY::~CGROWABLEARRAY()
//  ================================================================
//
//  Free all the memory allocated for the array. We can stop at the
//  first NULL entry
//
//-
	{
	INT ipul;
	for( ipul = 0; ipul < m_cpul && m_rgpul[ipul]; ++ipul )
		{
		delete [] m_rgpul[ipul];
		}
	}


//  ================================================================
ERR	CGROWABLEARRAY::ErrGrow( const INT culEntriesNew )
//  ================================================================
	{
	const INT ipul 		= IpulEntry_( culEntriesNew + 1 );
	INT ipulT;
	for( ipulT = ipul; ipulT >= 0; --ipulT )
		{
		if( NULL != m_rgpul[ipulT] )
			{
			//  everything below this will be filled in
			break;
			}
		else
			{
			const INT cul = CulPul_( ipulT );
			ULONG * const pul = new ULONG[ cul ];
			if( NULL == pul )
				{
				//	delete currently allocated chunks
				for ( ipulT++; ipul >= ipulT; ipulT++ )
					{
					delete m_rgpul[ipulT];
					m_rgpul[ipulT] = NULL;
					}
				return ErrERRCheck( JET_errOutOfMemory );
				}
			INT iul;
			for( iul = 0; iul < cul; ++iul )
				{
				pul[iul] = m_ulDefault;
				}
			//  to make growing multi-threaded, using AtomicCompareExchange here
			//  to make sure that the pointer is still NULL
			m_rgpul[ipulT] = pul;
			}
		}
	m_culEntries = culEntriesNew;
	return JET_errSuccess;
	}


//  ================================================================
ERR	CGROWABLEARRAY::ErrShrink( const INT culEntriesNew )
//  ================================================================
	{
	Assert( culEntriesNew > 0 );

	//  Reset all the entries we are removing
	
	INT iul;
	for( iul = culEntriesNew; iul < m_culEntries; ++iul )
		{
		ULONG * const pul = PulEntry( iul );
		*pul = m_ulDefault;
		}
	
	m_culEntries = culEntriesNew;
	return JET_errSuccess;
	}


//  ================================================================
ULONG * CGROWABLEARRAY::PulEntry( const INT iul )
//  ================================================================
	{
	Assert( iul < m_culEntries );
	return PulEntry_( iul );
	}


//  ================================================================
const ULONG * CGROWABLEARRAY::PulEntry( const INT iul ) const
//  ================================================================
	{
	Assert( iul < m_culEntries );
	return PulEntry_( iul );
	}


//  ================================================================
INT CGROWABLEARRAY::CulEntries() const
//  ================================================================
	{
	return m_culEntries;
	}


//  ================================================================
BOOL CGROWABLEARRAY::FFindBest(
		const ULONG ulMin,
		const ULONG ulMax,
		const INT iulStart,
		INT * const piul,
		const INT * const rgiulIgnore,
		const INT ciulIgnore ) const
//  ================================================================
	{
	const INT ipulStart	= IpulEntry_( iulStart );
	INT iulActual 		= iulStart;	
	INT ipul;

	ULONG	ulBest		= m_ulDefault;
	BOOL	fFoundMatch	= fFalse;

	for( ipul = ipulStart; ipul < m_cpul && m_rgpul[ipul]; ++ipul )
		{
		const ULONG * const pul = m_rgpul[ipul];
		const INT cul = CulPul_( ipul );
		INT iul;
		if( ipulStart == ipul )
			{
			iul = iulStart - CulPulTotal_( ipul );
			}
		else
			{
			iul = 0;
			}
		for( ; iul < cul; ++iul )
			{
			if( pul[iul] > ulMin
				&& pul[iul] > ulBest
				&& pul[iul] < ulMax )
				{				
				INT iulT;
				for( iulT = 0; iulT < ciulIgnore; ++iulT )
					{
					if( iulActual == rgiulIgnore[iulT] )
						{
						break;
						}
					}

				if( ciulIgnore == iulT )
					{
					*piul 		= iulActual;
					ulBest		= pul[iul];
					fFoundMatch = fTrue;
					}
				}
			if( ++iulActual >= m_culEntries )
				{
				break;
				}
			}
		}
	return fFoundMatch;
	}


//  ================================================================
BOOL CGROWABLEARRAY::FFindFirst( const ULONG ulMic, const ULONG ulMax, const INT iulStart, INT * const piul ) const
//  ================================================================
	{
	const INT ipulStart	= IpulEntry_( iulStart );
	INT iulActual 		= iulStart;	
	INT ipul;

	BOOL	fFoundMatch	= fFalse;

	for( ipul = ipulStart; ipul < m_cpul && m_rgpul[ipul]; ++ipul )
		{
		const ULONG * const pul = m_rgpul[ipul];
		const INT cul = CulPul_( ipul );
		INT iul;
		if( ipulStart == ipul )
			{
			iul = iulStart - CulPulTotal_( ipul );
			}
		else
			{
			iul = 0;
			}
		for( ; iul < cul; ++iul )
			{
			if( pul[iul] >= ulMic
				&& pul[iul] < ulMax )
				{
				*piul 		= iulActual;
				fFoundMatch = fTrue;
				break;
				}
			if( ++iulActual >= m_culEntries )
				{
				break;
				}
			}
		}
	return fFoundMatch;
	}
	
		
#ifndef RTM


//  ================================================================
VOID CGROWABLEARRAY::AssertValid() const
//  ================================================================
	{
	//  check that NULL and non-NULL entries are not intermingled
	BOOL fSeenNull = fFalse;
	INT ipul;
	for( ipul = 0; ipul < m_cpul; ++ipul )
		{
		if( fSeenNull )
			{
			AssertRTL( NULL == m_rgpul[ipul] );
			}
		else
			{
			fSeenNull = ( NULL == m_rgpul[ipul] );
			}
		}
	}

	
#endif	//	RTM

#ifndef RTM


//  ================================================================
ERR CGROWABLEARRAY::ErrUnitTest()
//  ================================================================
//
//  this is a STATIC function
//
//-
	{	
	ERR err;
	CGROWABLEARRAY cga;

	AssertRTL( 1 * m_culInitial == cga.CulPul_( 0 ) );
	AssertRTL( 2 * m_culInitial == cga.CulPul_( 1 ) );
	AssertRTL( 4 * m_culInitial == cga.CulPul_( 2 ) );
	AssertRTL( 8 * m_culInitial == cga.CulPul_( 3 ) );
	AssertRTL( 16 * m_culInitial == cga.CulPul_( 4 ) );
	AssertRTL( 32 * m_culInitial == cga.CulPul_( 5 ) );

	AssertRTL( 0 * m_culInitial == cga.CulPulTotal_( 0 ) );
	AssertRTL( 1 * m_culInitial == cga.CulPulTotal_( 1 ) );
	AssertRTL( 3 * m_culInitial == cga.CulPulTotal_( 2 ) );
	AssertRTL( 7 * m_culInitial == cga.CulPulTotal_( 3 ) );
	AssertRTL( 15 * m_culInitial == cga.CulPulTotal_( 4 ) );
	AssertRTL( 31 * m_culInitial == cga.CulPulTotal_( 5 ) );

	AssertRTL( 0 == cga.IpulEntry_( m_culInitial * 0 ) );
	AssertRTL( 1 == cga.IpulEntry_( m_culInitial * 1) );
	AssertRTL( 1 == cga.IpulEntry_( m_culInitial * 2 ) );
	AssertRTL( 2 == cga.IpulEntry_( m_culInitial * 3 ) );
	AssertRTL( 2 == cga.IpulEntry_( m_culInitial * 4 ) );
	AssertRTL( 2 == cga.IpulEntry_( m_culInitial * 5 ) );
	AssertRTL( 2 == cga.IpulEntry_( m_culInitial * 6 ) );
	AssertRTL( 3 == cga.IpulEntry_( m_culInitial * 7 ) );
	AssertRTL( 3 == cga.IpulEntry_( m_culInitial * 10 ) );
	AssertRTL( 4 == cga.IpulEntry_( m_culInitial * 15 ) );
	AssertRTL( 4 == cga.IpulEntry_( m_culInitial * 25 ) );
	AssertRTL( 4 == cga.IpulEntry_( m_culInitial * 30 ) );
	AssertRTL( 5 == cga.IpulEntry_( m_culInitial * 31 ) );

	INT cul;
	for( cul = 1; cul <= 4097; ++cul )
		{
		Call( cga.ErrGrow( cul ) );
		ASSERT_VALID( &cga );
		}

	for( cul = 4098; cul <= 10000; ++cul )
		{
		Call( cga.ErrGrow( cul ) );
		ASSERT_VALID( &cga );
		}

	for( cul = 10000; cul > 4097; --cul )
		{
		CallS( cga.ErrShrink( cul ) );
		ASSERT_VALID( &cga );
		}

	CallS( cga.ErrGrow( 10000 ) );

	INT iul;
	for( iul = 4098; iul <= 9999; ++iul )
		{
		ULONG * const pul = cga.PulEntry( iul );
		AssertRTL( m_ulDefault == *pul );
		}
		
	CallS( cga.ErrShrink( 4097 ) );

	for( iul = 0; iul < 4096; ++iul )
		{
		ULONG * const pul = cga.PulEntry( iul );
		AssertRTL( m_ulDefault == *pul );
		}

	ULONG * pul;
	pul = cga.PulEntry( 2037 );
	*pul = 59;

	INT iulT;
	for( iulT = 0; iulT <= 2037; ++iulT )
		{
		AssertRTL( cga.FFindBest( ( iulT % 50 ) + 1, 2038+iulT, iulT, &iul, NULL, 0 ) );
		AssertRTL( 2037 == iul );
		}

	AssertRTL( cga.FFindBest( 57, 2039, 0, &iul, NULL, 0 ) );
	AssertRTL( 2037 == iul );

	AssertRTL( cga.FFindFirst( 57, 2039, 0, &iul ) );
	AssertRTL( 2037 == iul );

	AssertRTL( cga.FFindBest( 58, 60, 0, &iul, NULL, 0 ) );
	AssertRTL( 2037 == iul );

	AssertRTL( cga.FFindFirst( 58, 60, 0, &iul ) );
	AssertRTL( 2037 == iul );

	AssertRTL( !cga.FFindBest( 59, 2038, 1, &iul, NULL, 0 ) );	

	AssertRTL( cga.FFindFirst( 59, 60, 0, &iul ) );
	AssertRTL( 2037 == iul );

	AssertRTL( cga.FFindFirst( 0, 4000, 2038, &iul ) );
	AssertRTL( 2038 == iul );

	AssertRTL( !cga.FFindBest( 60, 10000, 2, &iul, NULL, 0 ) );
	AssertRTL( !cga.FFindBest( 0, 4000, 2038, &iul, NULL, 0 ) );
	AssertRTL( !cga.FFindBest( 0, 4000, 2039, &iul, NULL, 0 ) );

	AssertRTL( !cga.FFindFirst( 60, 10000, 2, &iul ) );
	AssertRTL( !cga.FFindFirst( 1, 4000, 2038, &iul ) );
	AssertRTL( !cga.FFindFirst( 1, 4000, 2039, &iul ) );

	pul = cga.PulEntry( 2038 );
	*pul = 60;
	pul = cga.PulEntry( 2039 );
	*pul = 61;
	pul = cga.PulEntry( 2040 );
	*pul = 62;
	pul = cga.PulEntry( 2041 );
	*pul = 63;

	AssertRTL( cga.FFindBest( 0, 0xffffffff, 0, &iul, NULL, 0 ) );
	AssertRTL( 2041 == iul );
	AssertRTL( cga.FFindBest( 0, 64, 0, &iul, NULL, 0 ) );
	AssertRTL( 2041 == iul );
	AssertRTL( cga.FFindBest( 0, 63, 0, &iul, NULL, 0 ) );
	AssertRTL( 2040 == iul );
	AssertRTL( cga.FFindBest( 62, 64, 0, &iul, NULL, 0 ) );
	AssertRTL( 2041 == iul );
	AssertRTL( cga.FFindFirst( 62, 64, 0, &iul ) );
	AssertRTL( 2040 == iul );

	for( iul = 0; iul < 4096; ++iul )
		{
		ULONG * const pul = cga.PulEntry( iul );
		*pul = 0xbaadf00d;
		}

	AssertRTL( JET_errSuccess == cga.ErrShrink( 2049 ) );
	AssertRTL( JET_errSuccess == cga.ErrShrink( 2047 ) );
	AssertRTL( JET_errSuccess == cga.ErrShrink( 255 ) );
	AssertRTL( JET_errSuccess == cga.ErrShrink( 1 ) );	

	Call( cga.ErrGrow( 100 ) );

	pul = cga.PulEntry( 1 );
	*pul = 10;
	pul = cga.PulEntry( 10 );
	*pul = 28;
	pul = cga.PulEntry( 20 );
	*pul = 14;
	pul = cga.PulEntry( 30 );
	*pul = 24;
	pul = cga.PulEntry( 40 );
	*pul = 18;
	pul = cga.PulEntry( 50 );
	*pul = 20;
	pul = cga.PulEntry( 60 );
	*pul = 20;
	pul = cga.PulEntry( 70 );
	*pul = 22;
	pul = cga.PulEntry( 80 );
	*pul = 26;
	pul = cga.PulEntry( 90 );
	*pul = 12;

	INT rgiulIgnore[5];

	rgiulIgnore[0] = 10;
	AssertRTL( !cga.FFindBest( 29, 31, 0, &iul, rgiulIgnore, 1 ) );
	AssertRTL( cga.FFindBest( 0, 31, 0, &iul, rgiulIgnore, 1 ) );
	AssertRTL( 80 == iul );

	rgiulIgnore[1] = 80;
	AssertRTL( !cga.FFindBest( 25, 31, 0, &iul, rgiulIgnore, 2 ) );
	AssertRTL( cga.FFindBest( 0, 31, 0, &iul, rgiulIgnore, 2 ) );
	AssertRTL( 30 == iul );

	rgiulIgnore[2] = 30;
	rgiulIgnore[3] = 70;
	AssertRTL( !cga.FFindBest( 21, 31, 0, &iul, rgiulIgnore, 4 ) );
	AssertRTL( cga.FFindBest( 0, 31, 0, &iul, rgiulIgnore, 4 ) );
	AssertRTL( 50 == iul || 60 == iul );

	rgiulIgnore[4] = 50;
	AssertRTL( !cga.FFindBest( 21, 31, 0, &iul, rgiulIgnore, 5 ) );
	AssertRTL( cga.FFindBest( 0, 31, 0, &iul, rgiulIgnore, 5 ) );
	AssertRTL( 60 == iul );
	
	return JET_errSuccess;

HandleError:
	return err;
	}

	
#endif	//	!RTM


//-
//
//  For these calulations, imagine the case where m_culInitial = 1
//  (See ErrUnitTest)
//
//  iul		cpul	cpulTotal	entries in this array
//	-------------------------------------------------
//	0		1		0			[0,1)
//	1		2		1			[1,3)
//	2		4		3			[3,7)
//	3		8		7			[7,15)
//	4		16		15			[15,31)
//	5		32		31			[31,63)
//
//  We calculate these numbers and scale by m_culInitial
//
//-


//  ================================================================
INT CGROWABLEARRAY::IpulEntry_( const INT iul ) const
//  ================================================================
	{
	INT ipul;
	for( ipul = 0; ipul < m_cpul; ++ipul )
		{
		if( CulPulTotal_( ipul ) > iul )
			{
			return ipul - 1;
			}
		}
	AssertRTL( fFalse );
	return 0xffffffff;
	}


//  ================================================================
INT CGROWABLEARRAY::CulPulTotal_( const INT ipul ) const
//  ================================================================
	{
	//  ( 2^ipul - 1 ) * inital size
	const culPulTotal = ( ( 1 << ipul ) - 1 ) * m_culInitial;
	return culPulTotal;
	}


//  ================================================================
INT CGROWABLEARRAY::CulPul_( const INT ipul ) const
//  ================================================================
	{
	//  2^ipul * initial size
	const INT culPul = ( 1 << ipul ) * m_culInitial;
	Assert( culPul > 0 );
	return culPul;
	}

		
//  ================================================================
ULONG * CGROWABLEARRAY::PulEntry_( const INT iul ) const
//  ================================================================
	{
	//  which array in m_rgpul do we want
	const INT ipul 		= IpulEntry_( iul );
	//  how many entries are in the arrays pointed to by entries
	//  less than this one in m_rgpul
	const INT culTotal	= CulPulTotal_( ipul );
	//  which offset in the array do we want
	const INT iulOffset	= iul - culTotal;
	
	return m_rgpul[ipul] + iulOffset;
	}


//  ================================================================
SLVSPACENODECACHE::SLVSPACENODECACHE( 
	const LONG lSLVDefragFreeThreshold,			
	const LONG lSLVDefragDefragDestThreshold,	
	const LONG lSLVDefragDefragMoveThreshold ) :	
	m_crit( CLockBasicInfo( CSyncBasicInfo( szCritSLVSPACENODECACHE ), rankCritSLVSPACENODECACHE, 0 ) ),
	m_cgrowablearray(),
	m_cpgFullPages( 0 ),
	m_cpgEmptyPages( 0 )
//  ================================================================
	{
	m_rgcpgThresholdMin[ipgnoNewInsertion]		= ( lSLVDefragFreeThreshold * SLVSPACENODE::cpageMap ) / 100;
	m_rgcpgThresholdMin[ipgnoDefragInsertion]	= ( lSLVDefragDefragDestThreshold * SLVSPACENODE::cpageMap ) / 100; 
	m_rgcpgThresholdMin[ipgnoDefragMove]		= ( lSLVDefragDefragMoveThreshold * SLVSPACENODE::cpageMap ) / 100; 

	m_rgcpgThresholdMax[ipgnoNewInsertion]		= SLVSPACENODE::cpageMap + 1;
	m_rgcpgThresholdMax[ipgnoDefragInsertion]	= SLVSPACENODE::cpageMap + 1; 
	m_rgcpgThresholdMax[ipgnoDefragMove]		= m_rgcpgThresholdMin[ipgnoNewInsertion]; 

	INT ipgnoCached;
	for( ipgnoCached = 0; ipgnoCached < cpgnoCached; ++ipgnoCached )
		{
		m_rgpgnoCached[ipgnoCached]	= pgnoNull;
		}

	ASSERT_VALID( this );
	}


//  ================================================================
SLVSPACENODECACHE::~SLVSPACENODECACHE()
//  ================================================================
	{
	}


//  ================================================================
CPG SLVSPACENODECACHE::CpgCacheSize() const
//  ================================================================
	{
	return m_cgrowablearray.CulEntries() * SLVSPACENODE::cpageMap;
	}


//  ================================================================
CPG SLVSPACENODECACHE::CpgEmpty() const
//  ================================================================
	{
	return m_cpgEmptyPages;
	}


//  ================================================================
CPG SLVSPACENODECACHE::CpgFull() const
//  ================================================================
	{
	return m_cpgFullPages;
	}
	


		//	Prepare to increase the size of the cache to include this many pages
		//	in the SLV tree. This ensures that a subsequent call to ErrGrowCache
		//	for the same or fewer number of pages will not fail
//  ================================================================
ERR SLVSPACENODECACHE::ErrReserve( const CPG cpgNewSLV )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );

	const ERR err = ErrReserve_( cpgNewSLV );

	ASSERT_VALID_RTL( this );
	return err;	
	}


//  ================================================================
ERR	SLVSPACENODECACHE::ErrGrowCache( const CPG cpgNewSLV )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	const INT culCacheEntriesOld = m_cgrowablearray.CulEntries();
	const INT culCacheEntriesNew = ( cpgNewSLV + SLVSPACENODE::cpageMap - 1 ) / SLVSPACENODE::cpageMap;

	Assert( culCacheEntriesOld <= culCacheEntriesNew );
	
	const ERR err = m_cgrowablearray.ErrGrow( culCacheEntriesNew );

	if( JET_errSuccess == err )
		{
		
		//	these chunks default to full
		
		m_cpgFullPages += ( culCacheEntriesNew - culCacheEntriesOld );
		}
	
	ASSERT_VALID_RTL( this );
	return err;
	}


//  ================================================================
ERR	SLVSPACENODECACHE::ErrShrinkCache( const CPG cpgNewSLV )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	Assert( ( cpgNewSLV % SLVSPACENODE::cpageMap ) == 0 );
	const INT culCacheEntriesOld = m_cgrowablearray.CulEntries();
	const INT culCacheEntriesNew = ( cpgNewSLV + SLVSPACENODE::cpageMap - 1 ) / SLVSPACENODE::cpageMap;
	const INT culCacheEntriesShrink = culCacheEntriesOld - culCacheEntriesNew;

#ifdef DEBUG

	//	we should be shrinking only empty pages

	Assert( culCacheEntriesShrink > 0 );
	Assert( m_cpgEmptyPages >= culCacheEntriesShrink );

	INT iul;
	for( iul = culCacheEntriesOld; iul < culCacheEntriesNew; ++iul )
		{
		const ULONG * const pul = m_cgrowablearray.PulEntry( iul );	
		Assert( *pul == SLVSPACENODE::cpageMap );
		}

#endif	//	DEBUG

	//	we are removing only empty pages, decrease the number of empty pages
	
	m_cpgEmptyPages -= culCacheEntriesShrink;

	//	one or more of the cached locations may have been in the range that
	//	was shrunk. invalidate the cache
	//	CONSIDER: only invalidate cached entries that are in the shrinking region
	
	INT ipgnoCached;
	for( ipgnoCached = 0; ipgnoCached < cpgnoCached; ++ipgnoCached )
		{
		m_rgpgnoCached[ipgnoCached]	= pgnoNull;
		}
	
	const ERR err = m_cgrowablearray.ErrShrink( culCacheEntriesNew );
	
	ASSERT_VALID_RTL( this );
	return err;
	}


//  ================================================================
CPG SLVSPACENODECACHE::SetCpgAvail( const PGNO pgno, const CPG cpg )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	Assert( 0 == pgno % SLVSPACENODE::cpageMap );
	Assert( cpg >= 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );
	
	ULONG * const pul 	= PulMapPgnoToCache_( pgno );
	const CPG cpgOld	= *pul;
	*pul = cpg;

	if( 0 == cpgOld )
		{
		--m_cpgFullPages;
		}
	else if ( SLVSPACENODE::cpageMap == cpgOld )
		{
		--m_cpgEmptyPages;
		}

	if( 0 == cpg )
		{
		++m_cpgFullPages;
		}
	else if ( SLVSPACENODE::cpageMap == cpg )
		{
		++m_cpgEmptyPages;
		}
		
	ASSERT_VALID_RTL( this );
	return cpgOld;
	}


//  ================================================================
CPG SLVSPACENODECACHE::IncreaseCpgAvail( const PGNO pgno, const CPG cpg )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	Assert( 0 == pgno % SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );

	ULONG * const pul 	= PulMapPgnoToCache_( pgno );
	const ULONG ulOld 	= *pul;
	*pul += cpg;

	if( 0 == ulOld )
		{
		--m_cpgFullPages;
		}
		
	if( SLVSPACENODE::cpageMap == *pul )
		{
		++m_cpgEmptyPages;

		//  we have completely emptied a chunk. the chunk may be the cached
		//  one defrag is moving things out of. recalculate a new location to
		//  move defragged things from

		//	UNDONE: get rid of this loop
		
		INT ipgnoCached;
		for( ipgnoCached = 0; ipgnoCached < cpgnoCached; ++ipgnoCached )
			{
			
			//  a rollback could be re-emptying the point we are allocating space from
			//  check to see if this is the defrag point
			
			if( pgno == m_rgpgnoCached[ipgnoCached]
				&& ipgnoDefragMove == ipgnoCached )
				{
				(VOID)FFindNewCachedLocation_( ipgnoCached, IulMapPgnoToCache_( pgno ) );
				break;
				}
			}
		}

	ASSERT_VALID_RTL( this );
	return ulOld;
	}


//  ================================================================
CPG SLVSPACENODECACHE::DecreaseCpgAvail( const PGNO pgno, const CPG cpg )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	Assert( 0 == pgno % SLVSPACENODE::cpageMap );
	Assert( cpg > 0 );
	Assert( cpg <= SLVSPACENODE::cpageMap );

	ULONG * const pul 	= PulMapPgnoToCache_( pgno );
	const ULONG ulOld 	= *pul;
	*pul -= cpg;

	if( SLVSPACENODE::cpageMap == ulOld )
		{
		--m_cpgEmptyPages;
		}

	if( 0 == *pul )
		{
		++m_cpgFullPages;
		
		//  we have completely fulled a chunk. the chunk should be one of the
		//  cached ones used for insertion. recalculate a new cached chunk

		INT ipgnoCached;
		for( ipgnoCached = 0; ipgnoCached < cpgnoCached; ++ipgnoCached )
			{
			if( pgno == m_rgpgnoCached[ipgnoCached] )
				{
				AssertRTL( ipgnoDefragMove != ipgnoCached );
				(VOID)FFindNewCachedLocation_( ipgnoCached, IulMapPgnoToCache_( pgno ) );
				break;
				}
			}
		}
		
	ASSERT_VALID_RTL( this );
	return ulOld;
	}


//  ================================================================
BOOL SLVSPACENODECACHE::FGetCachedLocationForNewInsertion( PGNO * const ppgno )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	const BOOL fResult = FGetCachedLocation_( ppgno, ipgnoNewInsertion );

	ASSERT_VALID_RTL( this );
	return fResult;	
	}

	
//  ================================================================
BOOL SLVSPACENODECACHE::FGetCachedLocationForDefragInsertion( PGNO * const ppgno )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	const BOOL fResult = FGetCachedLocation_( ppgno, ipgnoDefragInsertion );

	ASSERT_VALID_RTL( this );
	return fResult;		
	}

	
//  ================================================================
BOOL SLVSPACENODECACHE::FGetCachedLocationForDefragMove( PGNO * const ppgno )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );
	
	const BOOL fResult = FGetCachedLocation_( ppgno, ipgnoDefragMove );

	ASSERT_VALID_RTL( this );
	return fResult;		
	}


//  ================================================================
BOOL SLVSPACENODECACHE::FGetNextCachedLocationForDefragMove( )
//  ================================================================
	{
	ENTERCRITICALSECTION ecs( &m_crit );
	ASSERT_VALID_RTL( this );

	const BOOL fResult = ( pgnoNull == m_rgpgnoCached[ipgnoDefragMove] ) ?
			FFindNewCachedLocation_( ipgnoDefragMove, 0 ) :
			FFindNewCachedLocation_( ipgnoDefragMove, IulMapPgnoToCache_( m_rgpgnoCached[ipgnoDefragMove] ) );

	ASSERT_VALID_RTL( this );
	return fResult;			
	}


#if defined( DEBUG ) || !defined( RTM )


//  ================================================================
VOID SLVSPACENODECACHE::AssertValid() const
//  ================================================================
	{
	ASSERT_VALID_RTL( &m_cgrowablearray );

	AssertRTL( m_cpgEmptyPages >= 0 );
	AssertRTL( m_cpgFullPages >= 0 );
	AssertRTL( m_cpgEmptyPages <= m_cgrowablearray.CulEntries() );
	AssertRTL( m_cpgFullPages <= m_cgrowablearray.CulEntries() );
	
	BOOL fPagesCached = fFalse;
	
	//  make sure no page is cached twice
	
	INT ipgno;
	for( ipgno = 0; ipgno < cpgnoCached; ++ipgno )
		{
		if( pgnoNull == m_rgpgnoCached[ipgno] )
			{
			continue;
			}
		else
			{
			fPagesCached = fTrue;
			}
			
		INT ipgnoT;
		for( ipgnoT = 0; ipgnoT < cpgnoCached; ++ipgnoT )
			{
			if( ipgnoT == ipgno )
				{
				continue;
				}
			AssertRTL( m_rgpgnoCached[ipgno] != m_rgpgnoCached[ipgnoT] );
			}
		}

	//

	if( fPagesCached )
		{
		AssertRTL( m_cgrowablearray.CulEntries() > 0 );
		}
		
	//  make sure no cached extent has too many pages

	INT cpgEmptyPages 	= 0;
	INT cpgFullPages	= 0;
	
	INT iul;
	for( iul = 0; iul < m_cgrowablearray.CulEntries(); ++iul )
		{
		const ULONG * const pul 	= m_cgrowablearray.PulEntry( iul );	
		AssertRTL( *pul <= SLVSPACENODE::cpageMap );

		if( 0 == *pul )
			{
			++cpgFullPages;
			}
		else if ( SLVSPACENODE::cpageMap == *pul )
			{
			++cpgEmptyPages;
			}
		}

	AssertRTL( m_cpgEmptyPages == cpgEmptyPages );
	AssertRTL( m_cpgFullPages == cpgFullPages );
	}

		
#endif	//	DEBUG || !RTM

#ifndef RTM


//  ================================================================
ERR SLVSPACENODECACHE::ErrValidate( FUCB * const pfucbSLVAvail ) const
//  ================================================================
	{
	ERR err;
	
	DIB dib;

	dib.pos = posFirst;
	dib.pbm = NULL;
	dib.dirflag = fDIRNull;

	err = ErrDIRDown( pfucbSLVAvail, &dib );
	if( JET_errRecordNotFound == err )
		{
		//  the SLV Avail tree is empty
		return JET_errSuccess;
		}

	do
		{
		PGNO pgno;
		LongFromKey( &pgno, pfucbSLVAvail->kdfCurr.key );
		Assert( 0 == ( pgno % SLVSPACENODE::cpageMap ) );

		Assert( sizeof( SLVSPACENODE ) == pfucbSLVAvail->kdfCurr.data.Cb() );
		const SLVSPACENODE * const pslvspacenode = (SLVSPACENODE *)pfucbSLVAvail->kdfCurr.data.Pv();

		const INT iulCache 		= IulMapPgnoToCache_( pgno );
		const ULONG * const pul = m_cgrowablearray.PulEntry( iulCache );

		const LONG cpgAvailReal 	= pslvspacenode->CpgAvail();
		const LONG cpgAvailCached	= *pul;

		AssertRTL( cpgAvailReal == cpgAvailCached );
		
		} while( JET_errSuccess == ( err = ErrDIRNext( pfucbSLVAvail, fDIRNull ) ) );

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
		
	DIRUp( pfucbSLVAvail );
	return err;
	}


//  ================================================================
ERR SLVSPACENODECACHE::ErrUnitTest()
//  ================================================================
// 
//  STATIC method
//
//-
	{
	return CGROWABLEARRAY::ErrUnitTest();
	}
	

#endif  //  !RTM



//  ================================================================
ERR	SLVSPACENODECACHE::ErrReserve_( const CPG cpgNewSLV )
//  ================================================================
	{
	const INT culCacheEntriesOld = m_cgrowablearray.CulEntries();
	const INT culCacheEntriesNew = ( cpgNewSLV + SLVSPACENODE::cpageMap - 1 ) / SLVSPACENODE::cpageMap;

	Assert( culCacheEntriesOld <= culCacheEntriesNew );
	
	const ERR err = m_cgrowablearray.ErrGrow( culCacheEntriesNew );
	if( JET_errSuccess == err )
		{
		CallS( m_cgrowablearray.ErrShrink( culCacheEntriesOld ) );
		}

	return err;		
	}


//  ================================================================
BOOL SLVSPACENODECACHE::FGetCachedLocation_( PGNO * const ppgno, const INT ipgnoCached )
//  ================================================================
	{		
	if( pgnoNull == m_rgpgnoCached[ipgnoCached]
		&& !FFindNewCachedLocation_( ipgnoCached, 0 ) )
		{
		return fFalse;	
		}

	*ppgno = m_rgpgnoCached[ipgnoCached];
	return fTrue;
	}


//  ================================================================
BOOL SLVSPACENODECACHE::FFindNewCachedLocation_( const INT ipgnoCached, const INT iulStart )
//  ================================================================
//
//  UNDONE:  make this able to wrap-around
//
//-
	{
	CPG	cpgThresholdMin	= m_rgcpgThresholdMin[ipgnoCached];
	CPG	cpgThresholdMax	= m_rgcpgThresholdMax[ipgnoCached];

	INT rgiulIgnore[cpgnoCached];
	INT i;
	for( i = 0; i < cpgnoCached; ++i )
		{
		rgiulIgnore[i] = ( m_rgpgnoCached[i] / SLVSPACENODE::cpageMap ) - 1;
		}

	INT iul;
	const BOOL f = m_cgrowablearray.FFindBest(
			cpgThresholdMin,
			cpgThresholdMax,
			0,
			&iul,
			rgiulIgnore,
			cpgnoCached
			);
			
	if( !f )
		{
		//  no entries have enough space
		m_rgpgnoCached[ipgnoCached] = pgnoNull;		
		}
	else
		{
		const PGNO pgno = ( iul + 1 ) * SLVSPACENODE::cpageMap;		
		m_rgpgnoCached[ipgnoCached] = pgno;
		}
	return f;
	}


//  ================================================================
INT SLVSPACENODECACHE::IulMapPgnoToCache_( const PGNO pgno ) const
//  ================================================================
	{
	Assert( 0 == pgno % SLVSPACENODE::cpageMap );
	return ( pgno / SLVSPACENODE::cpageMap ) - 1;
	}


//  ================================================================
ULONG * SLVSPACENODECACHE::PulMapPgnoToCache_( const PGNO pgno )
//  ================================================================
	{
	const INT iulCache 	= IulMapPgnoToCache_( pgno );
	ULONG * const pul 	= m_cgrowablearray.PulEntry( iulCache );	
	return pul;
	}


//  ================================================================
const ULONG * SLVSPACENODECACHE::PulMapPgnoToCache_( const PGNO pgno ) const
//  ================================================================
	{
	const INT iulCache 		= IulMapPgnoToCache_( pgno );
	const ULONG * const pul = m_cgrowablearray.PulEntry( iulCache );	
	return pul;
	}

VOID SLVSoftSyncHeaderSLVDB( SLVFILEHDR * pslvfilehdr, const DBFILEHDR * const pdbfilehdr )
	{
	Assert( pdbfilehdr != NULL );
	Assert( pslvfilehdr != NULL );
	Assert( pdbfilehdr->le_attrib == attribDb );
	Assert( pslvfilehdr->le_attrib == attribSLV );

	Assert( memcmp( &pslvfilehdr->signSLV, &pdbfilehdr->signSLV, sizeof(SIGNATURE) ) == 0 );

	pslvfilehdr->signDb = pdbfilehdr->signDb;
	pslvfilehdr->signLog = pdbfilehdr->signLog;

	pslvfilehdr->le_lgposAttach 		= pdbfilehdr->le_lgposAttach;
	pslvfilehdr->le_lgposConsistent 	= pdbfilehdr->le_lgposConsistent;
	pslvfilehdr->le_dbstate 			= pdbfilehdr->le_dbstate;

	pslvfilehdr->le_ulVersion = pdbfilehdr->le_ulVersion;
	pslvfilehdr->le_ulUpdate = pdbfilehdr->le_ulUpdate;
	
	if ( pdbfilehdr->FDbFromRecovery() )
		{
		pslvfilehdr->SetDbFromRecovery();
		}
	else
		{
		pslvfilehdr->ResetDbFromRecovery();
		}
	}

//	Updates slv header
//	Requres signSLV match for DB and SLV header

ERR ErrSLVSyncHeader(	IFileSystemAPI* const	pfsapi, 
						const BOOL				fReadOnly,
						const CHAR* const		szSLVName,
						DBFILEHDR* const		pdbfilehdr,
						SLVFILEHDR*				pslvfilehdr )
	{
	ERR	err = JET_errSuccess;

	Assert( pdbfilehdr != NULL );
	Assert( pdbfilehdr->FSLVExists() );
	Assert( !fReadOnly );
	Assert( NULL != szSLVName );
	
	BOOL fAllocMem = fFalse;
	if ( pslvfilehdr == NULL )
		{
		err = ErrSLVAllocAndReadHeader(	pfsapi, 
										fReadOnly,
										szSLVName,
										pdbfilehdr,
										&pslvfilehdr );
		if ( pslvfilehdr != NULL )
			{
			Assert( err == JET_errSuccess || err == JET_errDatabaseStreamingFileMismatch );
			fAllocMem = fTrue;
			}
		else
			{
			Call( err );
			}
		}
	else
		{
		if ( memcmp( &pslvfilehdr->signSLV, &pdbfilehdr->signSLV, sizeof( SIGNATURE ) ) != 0 )
			{
			Call( ErrERRCheck( JET_errDatabaseStreamingFileMismatch ) );
			}
		}
	SLVSoftSyncHeaderSLVDB( pslvfilehdr, pdbfilehdr );
	Call( ErrUtilWriteShadowedHeader(	pfsapi, 
										szSLVName,
										fFalse,
										(BYTE *)pslvfilehdr, 
										g_cbPage ) );
HandleError:
	AssertSz( err != 0 || ErrSLVCheckDBSLVMatch( pdbfilehdr, pslvfilehdr ) == JET_errSuccess,
		"Why did you forget to update SLV header?" );
	if ( fAllocMem )
		{
		Assert( pslvfilehdr != NULL );
		OSMemoryPageFree( (VOID *)pslvfilehdr );
		}

	return err;	
	}

//	Allocate memory for SLV header and read it.
//	RETURN: JET_errSuccess
//			JET_errDatabaseStreamingFileMismatch - DB and SLV headers are mismathing,
//				slv header will be preserved if signSLV matches.
//			On any other errors the slv header will be released and NULL will be returned

ERR ErrSLVAllocAndReadHeader(	IFileSystemAPI *const	pfsapi, 
								const BOOL				fReadOnly,
								const CHAR* const		szSLVName,
								DBFILEHDR* const		pdbfilehdr,
								SLVFILEHDR** const		ppslvfilehdr )
	{
	ERR err;

	Assert( ppslvfilehdr != NULL );
	Assert( *ppslvfilehdr == NULL );
	Assert( NULL != szSLVName );

	*ppslvfilehdr = (SLVFILEHDR *)PvOSMemoryPageAlloc( g_cbPage, NULL );
	if ( NULL == *ppslvfilehdr )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	err = ErrSLVReadHeader(	pfsapi, 
							fReadOnly,
							szSLVName,
							pdbfilehdr,
							*ppslvfilehdr,
							NULL );

	//	result must be successfull read of SLV header at least
	//	so it means that result is no error or database and SLV file mismatch,
	//	but at least signSLV must match
	if ( ( err < 0 && err != JET_errDatabaseStreamingFileMismatch ) ||
		 memcmp( &(*ppslvfilehdr)->signSLV, &pdbfilehdr->signSLV, sizeof( SIGNATURE ) ) != 0 )
		{
		OSMemoryPageFree( (VOID *)*ppslvfilehdr );
		*ppslvfilehdr = NULL;
		}
	return err;
	}


ERR ErrSLVCheckDBSLVMatch( const DBFILEHDR * const pdbfilehdr, const SLVFILEHDR * const pslvfilehdr )
	{ 
	Assert( pdbfilehdr != NULL );
	Assert( pslvfilehdr != NULL );
	Assert( pdbfilehdr->le_attrib == attribDb );
	Assert( pslvfilehdr->le_attrib == attribSLV );
	static const SIGNATURE signNil;
	
	if ( memcmp( &pslvfilehdr->signSLV, &pdbfilehdr->signSLV, sizeof(SIGNATURE) ) == 0 
		&& memcmp( &pslvfilehdr->signDb, &pdbfilehdr->signDb, sizeof(SIGNATURE) ) == 0
		&& ( memcmp( &pslvfilehdr->signLog, &pdbfilehdr->signLog, sizeof(SIGNATURE) ) == 0
			// old slv file
			|| memcmp( &pslvfilehdr->signLog, &signNil, sizeof(SIGNATURE) ) == 0 ) )
		{
		if ( CmpLgpos( &pslvfilehdr->le_lgposAttach, &pdbfilehdr->le_lgposAttach ) == 0
			&& CmpLgpos( &pslvfilehdr->le_lgposConsistent, &pdbfilehdr->le_lgposConsistent ) == 0 )
			{
			return JET_errSuccess;
			}
		// old slv file
		if ( CmpLgpos( &pslvfilehdr->le_lgposAttach, &lgposMin ) == 0
			&& CmpLgpos( &pslvfilehdr->le_lgposConsistent, &lgposMin ) == 0 )
			{
			return JET_errSuccess;
			}
		}
	return ErrERRCheck( JET_errDatabaseStreamingFileMismatch );
	}

INLINE ERR ErrSLVCreate( PIB *ppib, const IFMP ifmp )
	{
	ERR			err;
	FUCB		*pfucbDb		= pfucbNil;

	Assert( ppibNil != ppib );
	Assert( rgfmp[ifmp].FCreatingDB() );
	Assert( 0 == ppib->level || ( 1 == ppib->level && rgfmp[ifmp].FLogOn() ) );
	
	//  open the parent directory (so we are on the pgnoFDP ) and create the LV tree
	CallR( ErrDIROpen( ppib, pgnoSystemRoot, ifmp, &pfucbDb ) );
	Assert( pfucbNil != pfucbDb );

	Call( ErrSLVCreateAvailMap( ppib, pfucbDb ) );

	Call( ErrSLVCreateOwnerMap( ppib, pfucbDb ) );

HandleError:
	DIRClose( pfucbDb );

	return err;
	}

//  ================================================================
ERR ErrFILECreateSLV( IFileSystemAPI *const pfsapi, PIB *ppib, const IFMP ifmp, const int createOptions )
//  ================================================================
//
//  Creates the LV tree for the given table. 
//	
//-
	{
	ERR				err				= JET_errSuccess;
	IFileAPI		*pfapiSLV		= NULL;
	const BOOL		fRecovering		= PinstFromIfmp( ifmp )->FRecovering();
	const QWORD		cbSize			= OffsetOfPgno( cpgSLVFileMin + 1 );
	const FMP		*pfmp			= &rgfmp[ ifmp & ifmpMask ];
	BOOL			fInTransaction	= fFalse;
	SLVFILEHDR		*pslvfilehdr	= NULL;

	Assert( NULL != rgfmp[ifmp].SzSLVName() );

	Assert( !rgfmp[ifmp].PfapiSLV() );

	Assert( rgfmp[ifmp].FCreatingDB() );

	if ( pfmp->FLogOn() )
		{
		Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
		fInTransaction = fTrue;
		}
		
	if ( createOptions & SLV_CREATESLV_CREATE )
		{
		pslvfilehdr = (SLVFILEHDR *)PvOSMemoryPageAlloc( g_cbPage, NULL );
		if ( NULL == pslvfilehdr )
			{
			err = ErrERRCheck( JET_errOutOfMemory );
			return err;
			}

		memset( pslvfilehdr, 0, sizeof(SLVFILEHDR) );

		pslvfilehdr->le_ulMagic = ulDAEMagic;
		pslvfilehdr->le_ulVersion = ulDAEVersion;
		pslvfilehdr->le_ulUpdate = ulDAEUpdate;
		pslvfilehdr->le_ulCreateVersion = ulDAEVersion;
		pslvfilehdr->le_ulCreateUpdate = ulDAEUpdate;
		pslvfilehdr->le_attrib = attribSLV;
		pslvfilehdr->le_cbPageSize = g_cbPage;
		pslvfilehdr->le_dwMajorVersion 		= dwGlobalMajorVersion;
		pslvfilehdr->le_dwMinorVersion 		= dwGlobalMinorVersion;
		pslvfilehdr->le_dwBuildNumber 		= dwGlobalBuildNumber;
		pslvfilehdr->le_lSPNumber 			= lGlobalSPNumber;

		pslvfilehdr->le_filetype 			= filetypeSLV;

		memcpy( &pslvfilehdr->signSLV, &rgfmp[ifmp].Pdbfilehdr()->signSLV, sizeof(SIGNATURE) );
		Call( ErrSLVSyncHeader(	pfsapi, 
								rgfmp[ifmp].FReadOnlyAttach(),
								rgfmp[ifmp].SzSLVName(),
								rgfmp[ifmp].Pdbfilehdr(),
								pslvfilehdr ) );

		Call( pfsapi->ErrFileOpen( rgfmp[ifmp].SzSLVName(), &pfapiSLV ) );
		Assert( pfapiSLV );
		Call( pfapiSLV->ErrSetSize( cbSize ) );
		delete pfapiSLV;
		pfapiSLV = NULL;
		}

	if ( createOptions & SLV_CREATESLV_ATTACH )
		{
		Call( ErrSLVCreate( ppib, ifmp ) );
		}

	if ( createOptions & SLV_CREATESLV_OPEN )
		{
		Call( ErrFILEOpenSLV( pfsapi, fRecovering ? ppibNil : ppib, ifmp ) );
		Assert ( JET_errSuccess >= err );
		}

	if ( pfmp->FLogOn() )
		{
		Call( ErrDIRCommitTransaction( ppib, 0 ) );
		fInTransaction = fFalse;
		}
HandleError:	
	delete pfapiSLV;

	if ( fInTransaction )
		{
		(VOID)ErrDIRRollback( ppib );
		}

	if ( err < 0 )
		{
		// clear, if needed after the ErrSLVCreate call
		SLVOwnerMapTerm( ifmp, fFalse );
		
		//	on error, try to delete SLV file
		(VOID)pfsapi->ErrFileDelete( rgfmp[ifmp].SzSLVName() );
		}

	if ( pslvfilehdr != NULL )
		{
		OSMemoryPageFree( (VOID *)pslvfilehdr );
		}

	return err;
	}
	
//  ================================================================
ERR ErrSLVAvailMapInit( PIB *ppib, const IFMP ifmp, const PGNO pgnoSLV, FCB **ppfcbSLV )
//  ================================================================
	{
	ERR		err;
	FUCB	*pfucbSLV;
	FCB		*pfcbSLV;
	
	Assert( ppibNil != ppib );

	//	if recovering, then must be at the end of hard restore where we re-attach
	//	to the db because it moved (in ErrLGRIEndAllSessions())
	Assert( !PinstFromIfmp( ifmp )->FRecovering()
		|| PinstFromIfmp( ifmp )->m_plog->m_fHardRestore );

	// Link LV FCB into table.
	CallR( ErrDIROpen( ppib, pgnoSLV, ifmp, &pfucbSLV ) );
	Assert( pfucbNil != pfucbSLV );
	Assert( !FFUCBVersioned( pfucbSLV ) );	// Verify won't be deferred closed.
	pfcbSLV = pfucbSLV->u.pfcb;
	Assert( !pfcbSLV->FInitialized() );

	Assert( pfcbSLV->Ifmp() == ifmp );
	Assert( pfcbSLV->PgnoFDP() == pgnoSLV );
	Assert( pfcbSLV->Ptdb() == ptdbNil );
	Assert( pfcbSLV->CbDensityFree() == 0 );

	Assert( pfcbSLV->FTypeNull() );
	pfcbSLV->SetTypeSLVAvail();

	Assert( pfcbSLV->PfcbTable() == pfcbNil );

	//	finish the initialization of this SLV FCB

	pfcbSLV->CreateComplete();

	DIRClose( pfucbSLV );

	rgfmp[ifmp].SetPfcbSLVAvail( pfcbSLV );
	*ppfcbSLV = pfcbSLV;

	return err;
	}


VOID SLVAvailMapTerm( const IFMP ifmp, const BOOL fTerminating )
	{
	FMP	* const pfmp				= rgfmp + ifmp;
	Assert( pfmp );

	FCB	* const pfcbSLVAvailMap 	= pfmp->PfcbSLVAvail();
	
	if ( pfcbNil != pfcbSLVAvailMap )
		{
		//	synchronously purge the FCB
		Assert( pfcbSLVAvailMap->FTypeSLVAvail() );
		pfcbSLVAvailMap->PrepareForPurge();

		//	there should only be stray cursors if we're in the middle of terminating,
		//	in all other cases, all cursors should have been properly closed first
		Assert( 0 == pfcbSLVAvailMap->WRefCount() || fTerminating );
		if ( fTerminating )
			pfcbSLVAvailMap->CloseAllCursors( fTrue );

		pfcbSLVAvailMap->Purge();
		pfmp->SetPfcbSLVAvail( pfcbNil );
		}
	}


LOCAL ERR ErrSLVNewSize( const IFMP ifmp, const CPG cpg )
	{
	ERR			err;
	CPG			cpgCacheSizeOld = 0;

	/*	set new EOF pointer
	/**/
	QWORD cbSize = OffsetOfPgno( cpg + 1 );

	//	if recovering, then must be at the end of hard restore where we re-attach
	//	to the db because it moved (in ErrLGRIEndAllSessions())
	Assert( !PinstFromIfmp( ifmp )->FRecovering()
		|| PinstFromIfmp( ifmp )->m_plog->m_fHardRestore );

	SLVSPACENODECACHE * const pslvspacenodecache = rgfmp[ifmp].Pslvspacenodecache();
	Assert( NULL != pslvspacenodecache 
			|| PinstFromIfmp( ifmp )->FRecovering() );

	if( pslvspacenodecache )
		{
		CallR( pslvspacenodecache->ErrReserve( cpg ) );
		}

	rgfmp[ifmp].SemIOExtendDB().Acquire();
	err = rgfmp[ifmp].PfapiSLV()->ErrSetSize( cbSize );
	rgfmp[ifmp].SemIOExtendDB().Release();

	Assert( err < 0 || err == JET_errSuccess );
	if ( JET_errSuccess == err )
		{
		/*	set database size in FMP -- this value should NOT include the reserved pages
		/**/
		cbSize = QWORD( cpg ) * g_cbPage;
		QWORD cbGrow = cbSize - rgfmp[ ifmp ].CbSLVFileSize();
		rgfmp[ifmp].SetSLVFileSize( cbSize );

		rgfmp[ ifmp ].IncSLVSpaceCount( SLVSPACENODE::sFree, CPG( cbGrow / SLVPAGE_SIZE ) );

		CallS( pslvspacenodecache->ErrGrowCache( cpg ) );		
		}

	return err;
	}


LOCAL ERR ErrSLVInit( PIB *ppib, const IFMP ifmp )
	{
	ERR			err;
	FMP			*pfmp			= rgfmp + ifmp;
	INST		*pinst 			= PinstFromIfmp( ifmp );
	DIB			dib;
	PGNO		pgnoLast;
	PGNO		pgnoSLVAvail;
	OBJID		objidSLVAvail;
	FUCB		*pfucbSLVAvail	= pfucbNil;
	FCB			*pfcbSLVAvail	= pfcbNil;
	const BOOL	fTempDb			= ( dbidTemp == pfmp->Dbid() );

	BOOL 		fInTransaction 	= fFalse;
	
	Assert( ppibNil != ppib );

	//	if recovering, then must be at the end of hard restore where we re-attach
	//	to the db because it moved (in ErrLGRIEndAllSessions())
	Assert( !pinst->FRecovering()
			|| pinst->m_plog->m_fHardRestore );

#ifndef RTM
	SLVSPACENODE::Test();
	Call( SLVSPACENODECACHE::ErrUnitTest() );
#endif	//	!RTM

	if ( fTempDb )
		{
		pgnoSLVAvail = pgnoTempDbSLVAvail;
		}
	else
		{
		Call( ErrCATAccessDbSLVAvail( ppib, ifmp, szSLVAvail, &pgnoSLVAvail, &objidSLVAvail ) );
		}

	if ( pfcbNil == pfmp->PfcbSLVOwnerMap() )
		{
		Call ( ErrSLVOwnerMapInit( ppib, ifmp ) );
		}
	Assert( pfcbNil != pfmp->PfcbSLVOwnerMap() );

	Assert( pfcbNil == pfmp->PfcbSLVAvail() );
	Assert( pgnoNull != pgnoSLVAvail );
	Call( ErrSLVAvailMapInit( ppib, ifmp, pgnoSLVAvail, &pfcbSLVAvail ) );
	Assert( pfcbNil != pfcbSLVAvail );
	Assert( pfmp->PfcbSLVAvail() == pfcbSLVAvail );

	Call( ErrBTOpen( ppib, pgnoSLVAvail, ifmp, &pfucbSLVAvail ) );
	Assert( pfucbNil != pfucbSLVAvail );

	Assert( pfucbSLVAvail->u.pfcb == pfcbSLVAvail );
	
	dib.dirflag = fDIRNull;
	dib.pos 	= posLast;
	Call( ErrBTDown( pfucbSLVAvail, &dib, latchReadTouch ) );

	Assert( pfucbSLVAvail->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( &pgnoLast, pfucbSLVAvail->kdfCurr.key );
	Assert( 0 == (CPG)pgnoLast % cpgSLVExtent );

	BTUp( pfucbSLVAvail );

	Assert( 0 == pfmp->CbSLVFileSize() );
	if ( FFUCBUpdatable( pfucbSLVAvail )
		&& ppib->FSetAttachDB()
		// don't reclaim the SLV space at the end of recovery
		// it will be done at restart time
		&& !ppib->FRecoveringEndAllSessions() )
		{		
		//  allocate the SLVSPACENODECACHE in the FMP
		SLVSPACENODECACHE * const pslvspacenodecache = new SLVSPACENODECACHE(
															pinst->m_lSLVDefragFreeThreshold,
															pinst->m_lSLVDefragFreeThreshold,
															pinst->m_lSLVDefragMoveThreshold );
		if( NULL == pslvspacenodecache )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		Assert( NULL == pfmp->Pslvspacenodecache() );
		pfmp->SetPslvspacenodecache( pslvspacenodecache );

		Call( pslvspacenodecache->ErrGrowCache( pgnoLast ) );

		//	move all reserved/deleted SLV space to the Free state
		ULONG cpgSeen;
		ULONG cpgReset;
		ULONG cpgFree;
		Call( ErrSLVResetAllReservedOrDeleted( pfucbSLVAvail, &cpgSeen, &cpgReset, &cpgFree ) );
		Assert( cpgSeen == pgnoLast );

#ifndef RTM
		//  compare the cache against the actual CPG avails
		CallS( pslvspacenodecache->ErrValidate( pfucbSLVAvail ) );
#endif	//	!RTM		

		//	after recovery, SLV file size may not be the correct size
		//	so ensure that the file size matches what the SLV space tree
		//	thinks it should be
		Call( ErrSLVNewSize( ifmp, (CPG)pgnoLast ) );
		Assert( pfmp->CbSLVFileSize() == QWORD( pgnoLast ) * QWORD( g_cbPage ) );

		//  init SLV Space stats

		pfmp->ResetSLVSpaceOperCount();
		pfmp->ResetSLVSpaceCount();
		pfmp->IncSLVSpaceCount( SLVSPACENODE::sFree, CPG( cpgFree ) );
		pfmp->IncSLVSpaceCount( SLVSPACENODE::sCommitted, CPG( cpgSeen - cpgFree ) );
		}
	else
		{
		pfmp->SetSLVFileSize( QWORD( pgnoLast ) * QWORD( g_cbPage ) );
		}

	if ( FFUCBUpdatable( pfucbSLVAvail ) )
		{
		Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
		fInTransaction = fTrue;

		Call( ErrSLVOwnerMapNewSize( ppib, ifmp, pfmp->PgnoSLVLast(), fSLVOWNERMAPNewSLVInit ) );
		
		Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
		fInTransaction = fFalse;
		}

	BTClose( pfucbSLVAvail );

	return JET_errSuccess;

HandleError:
	Assert( err < 0 );

	if ( fInTransaction )
		{
		(VOID) ErrDIRRollback( ppib );
		fInTransaction = fFalse;
		}

	if ( pfcbSLVAvail != pfcbNil )
		{

		//	we need to purge the FCB

		if ( pfucbSLVAvail != pfucbNil )
			{
			//	close the FUCB
			Assert( pfcbSLVAvail->WRefCount() == 1 );
			BTClose( pfucbSLVAvail );
			}

		SLVAvailMapTerm( ifmp, fFalse );
		}
	else
		{
		//	impossible to have an FUCB open without an underlying FCB
		Assert( pfucbSLVAvail == pfucbNil );
		Assert( pfcbNil == pfmp->PfcbSLVAvail() );
		}

	//	reset all SLV variables
	SLVOwnerMapTerm( ifmp, fFalse );
	
	pfmp->SetSLVFileSize( 0 );

	SLVSPACENODECACHE * const pslvspacenodecacheT = pfmp->Pslvspacenodecache();
	pfmp->SetPslvspacenodecache( NULL );

	//  free the SLVSPACENODECACHE
	delete pslvspacenodecacheT;

	return err;
	}


ERR ErrSLVReadHeader(	IFileSystemAPI * const	pfsapi, 
						const BOOL				fReadOnly,
						const CHAR * const		szSLVName,
						DBFILEHDR * const		pdbfilehdr,
						SLVFILEHDR * const		pslvfilehdr,
						IFileAPI * const		pfapi )
	{
	ERR err;

	Assert( NULL != pslvfilehdr );
	Assert( NULL != szSLVName );

	err = ( fReadOnly ? ErrUtilReadShadowedHeader : ErrUtilReadAndFixShadowedHeader )(
								pfsapi, 
								szSLVName, 
								(BYTE *)pslvfilehdr, 
								g_cbPage, 
								OffsetOf( SLVFILEHDR, le_cbPageSize ),
								pfapi );
	if ( err < 0 )
		{
		if ( JET_errFileNotFound == err )
			{
			err = ErrERRCheck( JET_errSLVStreamingFileMissing );
			}
		else if ( JET_errDiskIO == err )
			{
			err = ErrERRCheck( JET_errSLVHeaderBadChecksum );
			}
		}
	else if ( attribSLV != pslvfilehdr->le_attrib )
		{
		err = ErrERRCheck( JET_errSLVHeaderCorrupted );
		}
	else
		{
		err = ErrSLVCheckDBSLVMatch( pdbfilehdr, pslvfilehdr );
		}

	return err;
	}


VOID SLVSpaceRequest( const IFMP ifmpDb, const QWORD cbRecommended );
ERR ErrSLVSpaceFree( const IFMP ifmpDb, const QWORD ibLogical, const QWORD cbSize, const BOOL fDeleted );

//  ================================================================
ERR ErrFILEOpenSLV( IFileSystemAPI *const pfsapi, PIB *ppib, const IFMP ifmp )
//  ================================================================
	{
	ERR				err;
	FMP				*pfmp			= rgfmp + ifmp;
	INST			*pinst			= PinstFromIfmp( ifmp );
	IFileAPI		*pfapiSLV		= NULL;
	SLVFILEHDR		*pslvfilehdr	= NULL;
	SLVROOT			slvroot			= slvrootNil;
	BOOL			fInitSLV		= fTrue;

	Assert( NULL != pfmp->SzSLVName() );
	Assert( !pfmp->FSLVAttached() );
	Assert( 0 == pfmp->CbSLVFileSize() );
	Assert( pfcbNil == pfmp->PfcbSLVAvail() );

	if ( ppibNil != ppib )
		{
		if ( pinst->FRecovering() )
			{
			if ( ppib->FRecoveringEndAllSessions() )
				{
				//	at the end of hard restore where we re-attach
				//	to the db because it moved (in ErrLGRIEndAllSessions())
				Assert( pinst->m_plog->m_fHardRestore );
				}
			else
				{
				//	redoing a CreateDb
				Assert( pfmp->FCreatingDB() );
				fInitSLV = fFalse;
				}
			}

		Assert( pfmp->FAttachingDB()
			|| pfmp->FCreatingDB()
			|| ppib->FSetAttachDB() );
		}
	else
		{
		Assert( pinst->FRecovering() );
		fInitSLV = fFalse;
		}

	Assert( !pfmp->FSLVAttached() );

	pslvfilehdr	= (SLVFILEHDR *)PvOSMemoryPageAlloc( g_cbPage, NULL );
	if ( NULL == pslvfilehdr )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	err = ErrSLVReadHeader(	pfsapi, 
							pfmp->FReadOnlyAttach(),
							pfmp->SzSLVName(),
							pfmp->Pdbfilehdr(),
							pslvfilehdr,
							NULL );

	OSMemoryPageFree( (VOID *)pslvfilehdr );
	pslvfilehdr = NULL;

	Call( err );
	CallS( err );

	//  open the SLV streaming file

	Call( pfsapi->ErrFileOpen( pfmp->SzSLVName(), &pfapiSLV, pfmp->FReadOnlyAttach() ) );
	Assert( !pfmp->PfapiSLV() );
	pfmp->SetPfapiSLV( pfapiSLV );
	pfapiSLV = NULL;

	if ( fInitSLV )
		{
		Assert( ppibNil != ppib );

		// For temp. tables, if initialisation fails here, the space
		// for the LV will be lost, because it's not persisted in
		// the catalog.
		Call( ErrSLVInit( ppib, ifmp ) );

		Assert( pfmp->PgnoSLVLast() >= cpgSLVFileMin );
		Assert( pfcbNil != pfmp->PfcbSLVAvail() );
		}
	else
		{
		Assert( !pinst->FSLVProviderEnabled() );

		Assert( 0 == pfmp->CbSLVFileSize() );
		Assert( pfcbNil == pfmp->PfcbSLVAvail() );
		}

	//  the SLV Provider is enabled

	if ( pinst->FSLVProviderEnabled() )
		{
		Assert( !pinst->FRecovering() );

		//  close the SLV Streaming File
		
		delete pfmp->PfapiSLV();
		pfmp->SetPfapiSLV( NULL );

		//  build the root path and backing file path for the SLV Root

		wchar_t wszRootPath[IFileSystemAPI::cchPathMax];
		Call( ErrOSSTRAsciiToUnicode(	rgfmp[ ifmp ].SzSLVRoot(),
										wszRootPath,
										IFileSystemAPI::cchPathMax ) );
		
		wchar_t wszBackingFile[IFileSystemAPI::cchPathMax];
		Call( ErrOSSTRAsciiToUnicode(	rgfmp[ ifmp ].SzSLVName(),
										wszBackingFile,
										IFileSystemAPI::cchPathMax ) );

		//  open the SLV Root and SLV streaming file via the SLV Provider
		//
		//  NOTE:  as soon as the root is created, the below callbacks can fire

		Call( ErrOSSLVRootCreate(	wszRootPath,
									pinst->m_pfsapi,
									wszBackingFile,
									PSLVROOT_SPACEREQ( SLVSpaceRequest ),
									DWORD_PTR( ifmp ),
									PSLVROOT_SPACEFREE( ErrSLVSpaceFree ),
									DWORD_PTR( ifmp ),
									&slvroot,
									&pfapiSLV ) );

		Assert( pfmp->SlvrootSLV() == slvrootNil );
		pfmp->SetSlvrootSLV( slvroot );
		slvroot = NULL;

		Assert( !pfmp->PfapiSLV() );
		pfmp->SetPfapiSLV( pfapiSLV );
		pfapiSLV = NULL;
		}

	Assert( pfmp->FSLVAttached() );
	return JET_errSuccess;


HandleError:
	SLVClose( ifmp );
	return err;
	}


VOID SLVClose( const IFMP ifmp )
	{
	FMP::AssertVALIDIFMP( ifmp );
	FMP 	*pfmp	= rgfmp + ifmp;
	Assert( pfmp );

	//  close the SLV Root
	//
	//  NOTE:  as soon as the SLV Root is closed we no longer need to worry
	//  about our callbacks firing
		
	if ( pfmp->SlvrootSLV() != slvrootNil )
		{
		OSSLVRootClose( pfmp->SlvrootSLV() );
		pfmp->SetSlvrootSLV( slvrootNil );
		pfmp->SetPfapiSLV( NULL );  //  do not close this!
		}

	//  close the SLV Streaming File
	
	delete pfmp->PfapiSLV();
	pfmp->SetPfapiSLV( NULL );

	//	synchronously purge the FCB
	SLVAvailMapTerm( ifmp, fFalse );
	SLVOwnerMapTerm( ifmp, fFalse );
	
	pfmp->SetSLVFileSize( 0 );

	SLVSPACENODECACHE * const pslvspacenodecache = pfmp->Pslvspacenodecache();
	pfmp->SetPslvspacenodecache( NULL );

	//  free the SLVSPACENODECACHE
	delete pslvspacenodecache;
	}

LOCAL ERR ErrSLVExtendSLV( FUCB *pfucbSLVAvail, CPG cpgReq, PGNO *ppgnoAfterAllExtend )
	{
	ERR			err;
	const IFMP	ifmp			= pfucbSLVAvail->ifmp;
	PGNO		pgnoLast;
	DIB			dib;
	PGNO		cpgEDBFile;
	PGNO		cpgSTMFile;
	PGNO		cpgQuota;
	PGNO		cpgSTMFileRemaining;
	PGNO		cpgQuotaRemaining;
	PGNO		cpgRemaining;
	PGNO		cpgGrow;

	//  get the last pgno used in the streaming file

	dib.pos = posLast;
	dib.dirflag = fDIRNull;

	CallR( ErrBTDown( pfucbSLVAvail, &dib, latchReadTouch ) );
	Assert( pfucbSLVAvail->kdfCurr.key.Cb() == sizeof( PGNO ) );
	LongFromKey( &pgnoLast, pfucbSLVAvail->kdfCurr.key );
	Assert( 0 == (CPG)pgnoLast % cpgSLVExtent );
	BTUp( pfucbSLVAvail );
	
	//	verify pages allocated in fixed chunks
	
	Assert( 0 == pgnoLast % cpgSLVExtent );

	//  compute the remaining pages available for allocation taking into account
	//  the remaining space in the streaming file as well as any database size
	//  quotas that may exist

	cpgEDBFile			= rgfmp[ifmp].PgnoLast();
	cpgSTMFile			= pgnoLast;
	cpgQuota			= rgfmp[ifmp].CpgDatabaseSizeMax();

	cpgSTMFileRemaining	= 0 - cpgSTMFile;
	cpgQuotaRemaining	= (	cpgQuota
								? (	cpgEDBFile + cpgSTMFile > cpgQuota
									? 0
									: cpgQuota - cpgEDBFile - cpgSTMFile )
								: cpgSTMFileRemaining );
	cpgRemaining		= min( cpgSTMFileRemaining, cpgQuotaRemaining );
	cpgRemaining		= cpgRemaining - cpgRemaining % cpgSLVExtent;

	//  compute how much we can grow the file

	//	round up to system parameter
	
	cpgGrow				= cpgReq + PinstFromPfucb( pfucbSLVAvail )->m_cpgSESysMin - 1;
	cpgGrow				= cpgGrow - cpgGrow % PinstFromPfucb( pfucbSLVAvail )->m_cpgSESysMin;

	//	round up to required SLV Extent sized chunks
	
	cpgGrow				= cpgGrow + cpgSLVExtent - 1;
	cpgGrow				= cpgGrow - cpgGrow % cpgSLVExtent;
	
	cpgGrow				= min( cpgGrow, cpgRemaining );

	//  we should be growing the file in SLV Extent sized chunks

	Assert( !( cpgGrow % cpgSLVExtent ) );

	//  we cannot grow the file

	if ( !cpgGrow )
		{
		//  report that we are out of space and fail

		char		szT[16];
		const char	*rgszT[2]	= { rgfmp[ifmp].SzDatabaseName(), szT };

		sprintf( szT, "%d", (ULONG)( ( (QWORD)( cpgEDBFile + cpgSTMFile ) * (QWORD)g_cbPage ) >> 20 ) );
					
		UtilReportEvent(
				eventWarning,
				SPACE_MANAGER_CATEGORY,
				SPACE_MAX_DB_SIZE_REACHED_ID,
				2,
				rgszT,
				0,
				NULL,
				PinstFromIfmp( ifmp ) );

		CallR( ErrERRCheck( JET_errOutOfDatabaseSpace ) );
		}

	//	try to allocate more space from the device.  if we can't get what we
	//  want, try growing by the minimum size before giving up

	err = ErrSLVNewSize( ifmp, pgnoLast + cpgGrow );
	if ( err < JET_errSuccess )
		{
		CallR( ErrSLVNewSize( ifmp, pgnoLast + cpgSLVExtent ) );
		cpgGrow = cpgSLVExtent;
		}

	//	insert the space map for the newly allocated space

	const PGNO	pgnoLastAfterOneExtend	= pgnoLast + cpgSLVExtent;
	const PGNO	pgnoLastAfterAllExtend	= pgnoLast + cpgGrow;

	Assert( 0 == cpgGrow % cpgSLVExtent );
	Assert( 0 == pgnoLast % cpgSLVExtent );
	Assert( 0 == pgnoLastAfterOneExtend % cpgSLVExtent );
	Assert( 0 == pgnoLastAfterAllExtend % cpgSLVExtent );
	Assert( pgnoLastAfterOneExtend <= pgnoLastAfterAllExtend );
	
	do
		{
		pgnoLast += cpgSLVExtent;
		CallR( ErrSLVInsertSpaceNode( pfucbSLVAvail, pgnoLast ) );
		Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
		}
	while ( pgnoLast < pgnoLastAfterAllExtend );
	Assert( pgnoLastAfterAllExtend == pgnoLast );

	Assert ( NULL != ppgnoAfterAllExtend );
	*ppgnoAfterAllExtend = pgnoLastAfterAllExtend;
	if ( pgnoLastAfterOneExtend == pgnoLastAfterAllExtend )
		{
		//	the correct node is write latched -- just exit
		}
	else
		{
		BOOKMARK	bm;
		BYTE		rgbKey[sizeof(PGNO)];

		KeyFromLong( rgbKey, pgnoLastAfterOneExtend );
		bm.key.prefix.Nullify();
		bm.key.suffix.SetCb( sizeof(PGNO) );
		bm.key.suffix.SetPv( rgbKey );
		bm.data.Nullify();
		
		//	must go back to the first node inserted and write latch that node
		//	WriteLatch will ensure no one consumed it

		BTUp( pfucbSLVAvail );
		CallR( ErrBTGotoBookmark( pfucbSLVAvail, bm, latchRIW, fTrue  ) );
		CallS( err );

		CallS( Pcsr( pfucbSLVAvail )->ErrUpgrade() );
		}

	Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
	return err;
	}


LOCAL ERR ErrSLVFindNextExtentWithFreePage( FUCB *pfucbSLVAvail, const BOOL fForDefrag )
	{
	ERR			err;
	CPG			cpgAvail;
	BOOKMARK	bm;
	DIB		 	dib;
	BYTE		rgbKey[sizeof(PGNO)];
	const LATCH	latch			= latchRIW;//latchReadTouch;

	SLVSPACENODECACHE * const pslvspacenodecache = rgfmp[pfucbSLVAvail->ifmp].Pslvspacenodecache();
	Assert( NULL != pslvspacenodecache );

	PGNO		pgnoCache;
	BOOL		f;

Start:
	pgnoCache = pgnoNull;
	if( !fForDefrag )
		{
		f = pslvspacenodecache->FGetCachedLocationForNewInsertion( &pgnoCache );
		if( f )
			{
///			printf( "Cached insertion: %d\n", pgnoCache );
			}
		else
			{
///			printf( "Out of cached insertion space\n" );
			}
		}
	else
		{
		f = pslvspacenodecache->FGetCachedLocationForDefragInsertion( &pgnoCache );
		if( f )
			{
///			printf( "Cached defrag: %d\n", pgnoCache );
			}
		else
			{
///			printf( "Out of cached defrag space\n" );
			}
		}
		
	if( !f )
		{
		err = ErrERRCheck( wrnSLVNoFreePages );
		return err;
		}
		
	KeyFromLong( rgbKey, pgnoCache );
	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	
	dib.pos 	= posDown;
	dib.pbm 	= &bm;
	dib.dirflag = fDIRNull;

	err = ErrBTDown( pfucbSLVAvail, &dib, latch );
	
	if ( err >= JET_errSuccess )
		{
		Assert( pfucbSLVAvail->kdfCurr.key.Cb() == sizeof(PGNO) );
		Assert( pfucbSLVAvail->kdfCurr.data.Cb() == sizeof(SLVSPACENODE) );
		
		cpgAvail = ( (SLVSPACENODE*)pfucbSLVAvail->kdfCurr.data.Pv() )->CpgAvail();
		Assert( cpgAvail >= 0 );

		if ( cpgAvail > 0 )
			{
			//	upgrade to write latch so that we can use this node
			err = Pcsr( pfucbSLVAvail )->ErrUpgrade();
			if ( errBFLatchConflict == err )
				{
				Assert( !Pcsr( pfucbSLVAvail )->FLatched() );
				goto Start;
				}
			CallR( err );

			//	verify no change
			Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
			Assert( pfucbSLVAvail->kdfCurr.data.Cb() == sizeof(SLVSPACENODE) );
			Assert( ( (SLVSPACENODE*)pfucbSLVAvail->kdfCurr.data.Pv() )->CpgAvail() == cpgAvail );

			return JET_errSuccess;
			}
		else
			{
			BTUp( pfucbSLVAvail );
			goto Start;
			}
		}
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrSLVGetFreePageRange(
	FUCB		*pfucbSLVAvail,
	PGNO		*ppgnoFirst,			//	initially pass in first page of extent
	CPG			*pcpgReq,
	const BOOL	fReserveOnly )
//  ================================================================
//
//	first the range of free pages within this extent, and continue searching subsequent
//	extents if necessary
//
//-
	{
	ERR			err;
	CPG			cpgReq			= *pcpgReq;
	PGNO		pgnoLast		= *ppgnoFirst + cpgSLVExtent - 1;

	//	must be in transaction because caller will be required to rollback on failure
	Assert( pfucbSLVAvail->ppib->level > 0 );

	Assert( *ppgnoFirst > pgnoNull );
	Assert( cpgReq > 0 );

	Assert( 0 == pgnoLast % cpgSLVExtent );

	Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
	Call( ( (SLVSPACENODE*)pfucbSLVAvail->kdfCurr.data.Pv() )->ErrGetFreePagesFromExtent(
				pfucbSLVAvail,
				ppgnoFirst,
				pcpgReq,
				fReserveOnly ) );
	Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );

	Assert( *pcpgReq > 0 );
	Assert( *pcpgReq <= cpgReq );
	Assert( *ppgnoFirst + *pcpgReq - 1 <= pgnoLast );

	//	if we hit the end of this extent and we still want more space,
	//	keep going
	while ( *ppgnoFirst + *pcpgReq - 1 == pgnoLast
		&& *pcpgReq < cpgReq )
		{
		PGNO	pgnoLastT;
		CPG		cpgT;

		Pcsr( pfucbSLVAvail )->Downgrade( latchReadTouch );

		err = ErrBTNext( pfucbSLVAvail, fDIRNull );
		Assert( JET_errRecordNotFound != err );
		if ( JET_errNoCurrentRecord == err )
			{
			break;
			}
		Call( err );
		
		err = Pcsr( pfucbSLVAvail )->ErrUpgrade();
		if ( errBFLatchConflict == err )
			{
			//	next page latched by someone else, just bail out now and be
			//	happy with what we have
			Assert( !Pcsr( pfucbSLVAvail )->FLatched() );
			break;
			}
		Call( err );
		
		Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
		Assert( pfucbSLVAvail->kdfCurr.key.Cb() == sizeof(PGNO) );
		Assert( pfucbSLVAvail->kdfCurr.data.Cb() == sizeof(SLVSPACENODE) );

		LongFromKey( &pgnoLastT, pfucbSLVAvail->kdfCurr.key );
		Assert( 0 == (CPG)pgnoLastT % cpgSLVExtent );
		Assert( pgnoLastT > pgnoLast );
		if ( pgnoLastT != pgnoLast + cpgSLVExtent )
			{
			Assert( fFalse );		//	UNDONE: currently impossible because we don't delete SLV space nodes
			break;
			}
		pgnoLast = pgnoLastT;

		Assert( *pcpgReq < cpgReq );
		cpgT = ( cpgReq - *pcpgReq );
		Assert( cpgT > 0 );

		Call( ( (SLVSPACENODE*)pfucbSLVAvail->kdfCurr.data.Pv() )->ErrGetFreePagesFromExtent(
					pfucbSLVAvail,
					NULL,
					&cpgT,
					fReserveOnly ) );
		Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );

		Assert( cpgT <= ( cpgReq - *pcpgReq ) );
		*pcpgReq += cpgT;

		Assert( *pcpgReq <= cpgReq );
		Assert( *ppgnoFirst + *pcpgReq - 1 <= pgnoLast );
		}

	err = JET_errSuccess;

HandleError:
	BTUp( pfucbSLVAvail );
	return err;
	}


LOCAL ERR ErrSLVIGetPages(
	FUCB		*pfucbSLVAvail,
	PGNO		*ppgnoFirst,
	CPG			*pcpgReq,
	const BOOL	fReserveOnly,		//	set to TRUE of Free->Reserved, else Free->Committed
	const BOOL	fForDefrag )		//  set to TRUE if we want space to move an existing run into
	{
	ERR			err;
	FCB			*pfcbSLVAvail		= pfucbSLVAvail->u.pfcb;
	BOOL		fMayExtend			= fFalse;

	PGNO 		pgnoLastAfterAllExtend 	= pgnoNull;

	FMP * 		pfmp 				= &rgfmp[ pfucbSLVAvail->ifmp ];
	BOOL 		fExtended 			= fFalse;

	Assert( pfucbNil != pfucbSLVAvail );
	Assert( pfcbNil != pfucbSLVAvail->u.pfcb );

	//	must ask for at least one page
	Assert( *pcpgReq > 0 );
	
Start:
	if ( fMayExtend )
		{
		pfmp->RwlSLVSpace().EnterAsWriter();
		}
	else
		{
		pfmp->RwlSLVSpace().EnterAsWriter();
		}

	Call( ErrSLVFindNextExtentWithFreePage( pfucbSLVAvail, fForDefrag ) );

	if ( wrnSLVNoFreePages == err )
		{
		BTUp( pfucbSLVAvail );

		if ( !fMayExtend )
			{
			pfmp->RwlSLVSpace().LeaveAsWriter();
			fMayExtend = fTrue;
			goto Start;
			}

		Call( ErrSLVExtendSLV( pfucbSLVAvail, *pcpgReq, &pgnoLastAfterAllExtend ) );
		fExtended = fTrue;
		BTUp( pfucbSLVAvail );
		Call( ErrSLVFindNextExtentWithFreePage( pfucbSLVAvail, fForDefrag ) );			
		Enforce( wrnSLVNoFreePages != err );
		}

	Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
	Assert( pfucbSLVAvail->kdfCurr.key.Cb() == sizeof(PGNO) );
	Assert( pfucbSLVAvail->kdfCurr.data.Cb() == sizeof(SLVSPACENODE) );

	PGNO	pgnoLast;
	LongFromKey( &pgnoLast, pfucbSLVAvail->kdfCurr.key );
	Assert( 0 == (CPG)pgnoLast % cpgSLVExtent );
	*ppgnoFirst = pgnoLast - cpgSLVExtent + 1;
	Call( ErrSLVGetFreePageRange(
				pfucbSLVAvail,
				ppgnoFirst,
				pcpgReq,
				fReserveOnly ) );

	Assert( *pcpgReq > 0 );
	Assert( pgnoNull != *ppgnoFirst );
	Assert( *ppgnoFirst >= pgnoLast - cpgSLVExtent + 1 );
	Assert( *ppgnoFirst <= pgnoLast );

	// check if we have to extend the SLVSpace Map if new space is allocated
	if ( fExtended )
		{
		Assert ( fMayExtend );
		Assert ( pgnoLastAfterAllExtend != pgnoNull );
		Call( ErrSLVOwnerMapNewSize(
					pfucbSLVAvail->ppib,
					pfucbSLVAvail->ifmp,
					pgnoLastAfterAllExtend,
					fSLVOWNERMAPNewSLVGetPages ) );
		}

HandleError:
	Assert( pfcbNil != pfcbSLVAvail );

	if ( fMayExtend )
		{
		pfmp->RwlSLVSpace().LeaveAsWriter();
		}
	else
		{
		pfmp->RwlSLVSpace().LeaveAsWriter();
		}

	return err;
	}
	
ERR ErrSLVGetPages(
	PIB			*ppib,
	const IFMP	ifmp,
	PGNO		*ppgnoFirst,
	CPG			*pcpgReq,
	const BOOL	fReserveOnly,		//	set to TRUE of Free->Reserved, else Free->Committed
	const BOOL  fForDefrag )		//  set to TRUE if we want space to move an existing run into
	{
	ERR			err;
	FUCB		*pfucbSLVAvail			= pfucbNil;
	BOOL		fInTransaction			= fFalse;

	//	must ask for at least one page
	Assert( *pcpgReq > 0 );
	
	Assert( rgfmp[ifmp].FSLVAttached() );
	Assert( pfcbNil != rgfmp[ifmp].PfcbSLVAvail() );
	
	CallR( ErrDIROpen( ppib, rgfmp[ifmp].PfcbSLVAvail(), &pfucbSLVAvail ) );
	Assert( pfucbNil != pfucbSLVAvail );

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTransaction = fTrue;

	Assert( rgfmp[ifmp].PfcbSLVAvail() == pfucbSLVAvail->u.pfcb );
	Call( ErrSLVIGetPages( pfucbSLVAvail, ppgnoFirst, pcpgReq, fReserveOnly, fForDefrag ) );

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	fInTransaction = fFalse;

	//	must return at least one page
	Assert( *pcpgReq > 0 );
	Assert( pgnoNull != *ppgnoFirst );

HandleError:
	if ( fInTransaction )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}

	Assert( pfucbNil != pfucbSLVAvail );
	DIRClose( pfucbSLVAvail );
		
	return err;
	}


LOCAL ERR ErrSLVMapUnexpectedStateError( const SLVSPACEOPER slvspaceoper )
	{
	ERR		err;
	switch ( slvspaceoper )
		{
		case slvspaceoperReservedToCommitted:
		case slvspaceoperFreeReserved:
			err = ErrERRCheck( JET_errSLVPagesNotReserved );
			break;
		case slvspaceoperCommittedToDeleted:
			err = ErrERRCheck( JET_errSLVPagesNotCommitted );
			break;
		case slvspaceoperDeletedToFree:
			err = ErrERRCheck( JET_errSLVPagesNotDeleted );
			break;
		default:
			Assert( fFalse );	//	unexpected case
			err = ErrERRCheck( JET_errSLVSpaceCorrupted );
			break;
		}

	return err;
	}


LOCAL ERR ErrSLVChangePageState(
	PIB						*ppib,
	const IFMP				ifmp,
	const SLVSPACEOPER		slvspaceoper,
	PGNO					pgnoFirst,
	CPG						cpgToChange,
	CSLVInfo::FILEID		fileid			= CSLVInfo::fileidNil,
	QWORD					cbAlloc			= 0,
	const wchar_t* const	wszFileName		= L"" )
	{
	ERR					err;
	FCB					*pfcbSLVAvail		= rgfmp[ifmp].PfcbSLVAvail();
	FUCB				*pfucbSLVAvail		= pfucbNil;
	PGNO				pgnoLastInExtent;
	BOOKMARK			bm;
	DIB		 			dib;
	BYTE				rgbKey[sizeof(PGNO)];
	BOOL				fInTransaction		= fFalse;
	CSLVInfo			slvinfo;
	LATCH				latch				= latchReadTouch;
	DIRFLAG				fDIR;

	PGNO				pgnoFirstCopy 		= pgnoFirst;
	CPG					cpgToChangeCopy 	= cpgToChange;

	Assert( rgfmp[ifmp].FSLVAttached() );
	Assert( pfcbNil != pfcbSLVAvail );

	//	this function only supports certain transitions
	
	switch ( slvspaceoper )
		{
		case slvspaceoperReservedToCommitted:
		case slvspaceoperCommittedToDeleted:
			fDIR = fDIRNull;
			break;
		case slvspaceoperFreeReserved:
		case slvspaceoperDeletedToFree:
			fDIR = fDIRNoVersion;
			break;
		default:
			Assert( fFalse );
			break;
		}

	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;
	dib.dirflag = fDIRFavourNext;

	CallR( ErrDIROpen( ppib, pfcbSLVAvail, &pfucbSLVAvail ) );
	Assert( pfucbNil != pfucbSLVAvail );

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTransaction = fTrue;

Start:
	//	find the chunk containing the first page of our range
	KeyFromLong( rgbKey, ( ( pgnoFirst + cpgSLVExtent - 1 ) / cpgSLVExtent ) * cpgSLVExtent );
	Assert( bm.key.prefix.FNull() );
	Assert( bm.key.suffix.Pv() == rgbKey );
	Assert( bm.key.Cb() == sizeof(PGNO) );
	Assert( posDown == dib.pos );
	Assert( &bm == dib.pbm );
	Assert( fDIRFavourNext == dib.dirflag );
	
	err = ErrBTDown( pfucbSLVAvail, &dib, latch );
	if ( JET_errSuccess != err )
		{
		//	shouldn't get warnings, because we fixed up the seek key to seek exactly to a chunk
		Assert( err < 0 );
		if ( JET_errRecordNotFound == err || err > 0 )
			{
			err = ErrSLVMapUnexpectedStateError( slvspaceoper );
			}

		goto HandleError;
		}

	Assert( Pcsr( pfucbSLVAvail )->FLatched() );
	LongFromKey( &pgnoLastInExtent, pfucbSLVAvail->kdfCurr.key );
	Assert( 0 == (CPG)pgnoLastInExtent % cpgSLVExtent );

	if ( pgnoFirst > pgnoLastInExtent
		|| pgnoFirst <= pgnoLastInExtent - cpgSLVExtent )
		{
		//	should be impossible
		Assert( fFalse );
		Call( ErrERRCheck( JET_errSLVSpaceCorrupted ) );
		}

	LONG	ipage;
	LONG	cpages;

	Assert( pgnoFirst >= ( pgnoLastInExtent - cpgSLVExtent + 1 ) );
	ipage = pgnoFirst - ( pgnoLastInExtent - cpgSLVExtent + 1 );

	Assert( ipage >= 0 );
	Assert( ipage < cpgSLVExtent );

	forever
		{
		//	upgrade to write latch so that we can use this node
		err = Pcsr( pfucbSLVAvail )->ErrUpgrade();
		if ( errBFLatchConflict == err )
			{
			Assert( !Pcsr( pfucbSLVAvail )->FLatched() );
			latch = latchRIW;
			goto Start;
			}
		Call( err );

		//	verify no change
		Assert( latchWrite == Pcsr( pfucbSLVAvail )->Latch() );
		Assert( pfucbSLVAvail->kdfCurr.data.Cb() == sizeof(SLVSPACENODE) );

		Assert( cpgToChange > 0 );
		Assert( ipage >= 0 );
		Assert( ipage < cpgSLVExtent );
		cpages = min( cpgToChange, cpgSLVExtent - ipage );
		
		Call( ErrBTMungeSLVSpace(
					pfucbSLVAvail,
					slvspaceoper,
					ipage,
					cpages,
					fDIR,
					fileid,
					cbAlloc,
					wszFileName ) );

		//  if we are committing reserved space, mark the space as committed to
		//  the SLV Provider.  we do this if and only if the space operation
		//  succeeds because if it does, we are guaranteed to either commit or
		//  free the space via the version store.  if the space operation fails,
		//  it is guaranteed that the SLV Provider will free the space
		//
		//  NOTE:  we had better be able to mark this space as committed or a
		//  double free of this space can occur later!!!!

		if (	slvspaceoperReservedToCommitted == slvspaceoper &&
				PinstFromIfmp( ifmp )->FSLVProviderEnabled() )
			{
			BOOKMARK bm = pfucbSLVAvail->bmCurr;
			BTUp( pfucbSLVAvail );
			
			QWORD ibLogical = OffsetOfPgno( pgnoFirst );
			QWORD cbSize = QWORD( cpages ) * SLVPAGE_SIZE;
			
			CallS( slvinfo.ErrCreateVolatile() );

			CallS( slvinfo.ErrSetFileID( fileid ) );
			CallS( slvinfo.ErrSetFileAlloc( cbAlloc ) );
			CallS( slvinfo.ErrSetFileName( wszFileName ) );

			CSLVInfo::HEADER header;
			header.cbSize			= cbSize;
			header.cRun				= 1;
			header.fDataRecoverable	= fFalse;
			header.rgbitReserved_31	= 0;
			header.rgbitReserved_32	= 0;
			CallS( slvinfo.ErrSetHeader( header ) );

			CSLVInfo::RUN run;
			run.ibVirtualNext	= cbSize;
			run.ibLogical		= ibLogical;
			run.qwReserved		= 0;
			run.ibVirtual		= 0;
			run.cbSize			= cbSize;
			run.ibLogicalNext	= ibLogical + cbSize;
			CallS( slvinfo.ErrMoveAfterLast() );
			CallS( slvinfo.ErrSetCurrentRun( run ) );

			err = ErrOSSLVRootSpaceCommit( rgfmp[ ifmp ].SlvrootSLV(), slvinfo );
			Enforce( err >= JET_errSuccess );

			slvinfo.Unload();

			Call( ErrBTGotoBookmark( pfucbSLVAvail, bm, latchRIW, fTrue ) );
			CallS( Pcsr( pfucbSLVAvail )->ErrUpgrade() );
			}

		//	subsequent iterations will always start with first page in extent
		ipage = 0;
		cpgToChange -= cpages;
		pgnoFirst += cpages;

		if ( cpgToChange > 0 )
			{
			Pcsr( pfucbSLVAvail )->Downgrade( latchReadTouch );
			err = ErrBTNext( pfucbSLVAvail, fDIRNull );
			Assert( JET_errRecordNotFound != err );
			if ( JET_errNoCurrentRecord == err )
				{
				err = ErrSLVMapUnexpectedStateError( slvspaceoper );
				}
			Call( err );
			}
		else
			{
			break;
			}
		}
		
	// We need to mark the pages as unused in the OwnerMap
	// if the operation is moving pages out from the commited state
	// (transition into commited state are not marked in OwnerMap at this level)
	Assert ( 	slvspaceoperReservedToCommitted == slvspaceoper ||
				slvspaceoperCommittedToDeleted == slvspaceoper ||
				slvspaceoperFreeReserved == slvspaceoper ||
				slvspaceoperDeletedToFree == slvspaceoper );
				
	if ( slvspaceoperCommittedToDeleted == slvspaceoper )
		{
		Call ( ErrSLVOwnerMapResetUsageRange(
					ppib,
					ifmp,
					pgnoFirstCopy,
					cpgToChangeCopy,
					fSLVOWNERMAPResetChgPgStatus ) );
		}

	BTUp( pfucbSLVAvail );

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	fInTransaction = fFalse;

HandleError:
	if ( fInTransaction )
		{
		BTUp( pfucbSLVAvail );
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}
	Assert( pfucbNil != pfucbSLVAvail );
	DIRClose( pfucbSLVAvail );
	slvinfo.Unload();

	return err;
	}


// ErrSLVGetRange				- reserves/commits a range of free space from the SLV file
//								  using the requested size as a hint.  more or
//								  less space may be returned
//
// IN:
//		ppib					- session
//		ifmp					- database ifmp
//		cbRequested				- requested range size
//		pibLogical				- buffer for receiving the range's starting offset
//		pcb						- buffer for receiving the range's size
//		fReserveOnly			- if TRUE, move space from Free to Rserved state,
//								  else move from Free to Committed state
//
// RESULT:						ERR
//
// OUT:	
//		pibLogical				- starting offset of the reserved range
//		pcb						- size of the reserved range

ERR ErrSLVGetRange(
	PIB			*ppib,
	const IFMP	ifmp,
	const QWORD	cbRequested,
	QWORD		*pibLogical,
	QWORD		*pcb,
	const BOOL	fReserveOnly,
	const BOOL	fForDefrag )
	{
	CPG			cpgReq		= CPG( ( cbRequested + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
	PGNO		pgnoFirst	= pgnoNull;

	const ERR	err			= ErrSLVGetPages( ppib, ifmp, &pgnoFirst, &cpgReq, fReserveOnly, fForDefrag );

	*pibLogical	= OffsetOfPgno( pgnoFirst );
	*pcb		= QWORD( cpgReq ) * SLVPAGE_SIZE;

	return err;
	}

// ErrSLVCommitReservedRange	- changes the state of the specified range of space
//								  in the SLV file from Reserved to Committed
//
// IN:
//		ppib					- session
//		ifmp					- database ifmp
//		ibLogical				- starting offset of range
//		cbSize					- size of range
//		fileid					- associated SLV File ID
//		cbAlloc					- associated SLV File allocation size
//		wszFileName				- associated SLV File Name
//
// RESULT:						ERR

LOCAL INLINE ERR ErrSLVCommitReservedRange(	PIB* const				ppib,
											const IFMP				ifmp,
											const QWORD				ibLogical,
											const QWORD				cbSize,
											const CSLVInfo::FILEID	fileid,
											const QWORD				cbAlloc,
											const wchar_t* const	wszFileName )
	{
	const CPG	cpgToCommit		= CPG( ( cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
	const PGNO	pgnoFirst		= PgnoOfOffset( ibLogical );

	const ERR	err				= ErrSLVChangePageState(	ppib,
															ifmp,
															slvspaceoperReservedToCommitted,
															pgnoFirst,
															cpgToCommit,
															fileid,
															cbAlloc,
															wszFileName );

	return err;
	}

// ErrSLVDeleteCommittedRange	- changes the state of the specified range of space
//								  in the SLV file from Committed to Deleted
//
// IN:
//		ppib					- session
//		ifmp					- database ifmp
//		ibLogical				- starting offset of range
//		cbSize					- size of range
//		fileid					- associated SLV File ID
//		cbAlloc					- associated SLV File allocation size
//		wszFileName				- associated SLV File Name
//
// RESULT:						ERR

ERR ErrSLVDeleteCommittedRange(	PIB* const				ppib,
								const IFMP				ifmp,
								const QWORD				ibLogical,
								const QWORD				cbSize,
								const CSLVInfo::FILEID	fileid,
								const QWORD				cbAlloc,
								const wchar_t* const	wszFileName )
	{
	Assert( ibLogical % SLVPAGE_SIZE == 0 );
	Assert( cbSize % SLVPAGE_SIZE == 0 );
	Assert( cbSize > 0 );

	const CPG	cpgToRelease	= CPG( cbSize / SLVPAGE_SIZE );
	const PGNO	pgnoFirst		= PgnoOfOffset( ibLogical );

///	printf( "freeing %d pages at %d\n", cpgToRelease, pgnoFirst );
	
	const ERR	err				= ErrSLVChangePageState(	ppib,
															ifmp,
															slvspaceoperCommittedToDeleted,
															pgnoFirst,
															cpgToRelease,
															fileid,
															cbAlloc,
															wszFileName );

	return err;
	}

// ErrSLVFreeReservedRange		- changes the state of the specified range of space
//								  in the SLV file from Reserved to Free
//
// IN:
//		ppib					- session
//		ifmp					- database ifmp
//		ibLogical				- starting offset of range
//		cbSize					- size of range
//
// RESULT:						ERR

LOCAL INLINE ERR ErrSLVFreeReservedRange(	PIB* const	ppib,
											const IFMP	ifmp,
											const QWORD	ibLogical,
											const QWORD	cbSize )
	{
	Assert( ibLogical % SLVPAGE_SIZE == 0 );
	Assert( cbSize % SLVPAGE_SIZE == 0 );
	Assert( cbSize > 0 );

	const CPG	cpgToRelease	= CPG( cbSize / SLVPAGE_SIZE );
	const PGNO	pgnoFirst		= PgnoOfOffset( ibLogical );

	const ERR	err				= ErrSLVChangePageState(	ppib,
															ifmp,
															slvspaceoperFreeReserved,
															pgnoFirst,
															cpgToRelease );

	return err;
	}

// ErrSLVFreeDeletedRange		- changes the state of the specified range of space
//								  in the SLV file from Deleted to Free
//
// IN:
//		ppib					- session
//		ifmp					- database ifmp
//		ibLogical				- starting offset of range
//		cbSize					- size of range
//
// RESULT:						ERR

LOCAL INLINE ERR ErrSLVFreeDeletedRange(	PIB* const	ppib,
											const IFMP	ifmp,
											const QWORD	ibLogical,
											const QWORD	cbSize )
	{
	Assert( ibLogical % SLVPAGE_SIZE == 0 );
	Assert( cbSize % SLVPAGE_SIZE == 0 );
	Assert( cbSize > 0 );

	const CPG	cpgToRelease	= CPG( cbSize / SLVPAGE_SIZE );
	const PGNO	pgnoFirst		= PgnoOfOffset( ibLogical );

	const ERR	err				= ErrSLVChangePageState(	ppib,
															ifmp,
															slvspaceoperDeletedToFree,
															pgnoFirst,
															cpgToRelease );

	return err;
	}


//  ================================================================
class SLVRESERVETASK : public DBTASK
//  ================================================================	
//
//  Reserves space for the SLV Provider when it is enabled
//
//-
	{
	public:
		SLVRESERVETASK( const IFMP ifmp, const QWORD cbRecommended );
		
		ERR ErrExecute( PIB * const ppib );
		VOID HandleError( const ERR err );

	private:
		SLVRESERVETASK( const SLVRESERVETASK& );
		SLVRESERVETASK& operator=( const SLVRESERVETASK& );

		QWORD m_cbRecommended;		//  recommended amount of space to reserve
		BOOL  m_fCalledSLVProvider;

	};


//  ****************************************************************
//	SLVRESERVETASK
//  ****************************************************************

SLVRESERVETASK::SLVRESERVETASK( const IFMP ifmp, const QWORD cbRecommended )
	:	DBTASK( ifmp ),
		m_cbRecommended( cbRecommended ),
		m_fCalledSLVProvider( fFalse )
	{
	//	don't fire off async tasks on the temp. database because the
	//	temp. database is simply not equipped to deal with concurrent access
	AssertRTL( dbidTemp != rgfmp[ifmp].Dbid() );
	}
		
ERR SLVRESERVETASK::ErrExecute( PIB * const ppib )
	{
	ERR			err					= JET_errSuccess;
	BOOL		fInTrx				= fFalse;
	CSLVInfo	slvinfo;
	QWORD		ibVirtual			= 0;

	CSLVInfo::HEADER header;
	CSLVInfo::RUN run;

	//  if we are detaching from the database then do not reserve space

	if ( rgfmp[ m_ifmp ].FDetachingDB() )
		{
		Call( ErrERRCheck( JET_errInvalidDatabase ) );
		}
	
	//  reserve our space in a transaction so that we can rollback on an
	//  error

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTrx = fTrue;

	//  reserve up to m_cbRecommended bytes of space for use by
	//  the SLV Provider, allowing failure for JET_errDiskFull

	CallS( slvinfo.ErrCreateVolatile() );

	header.cbSize			= 0;
	header.cRun				= 0;
	header.fDataRecoverable	= fFalse;
	header.rgbitReserved_31	= 0;
	header.rgbitReserved_32	= 0;

	run.ibVirtualNext		= 0;
	run.ibLogical			= 0;
	run.qwReserved			= 0;
	run.ibVirtual			= 0;
	run.cbSize				= 0;
	run.ibLogicalNext		= 0;

	while ( ibVirtual < m_cbRecommended )
		{
		//  reserve this chunk of space in a transaction so that if we run out
		//  of space in the SLV Info, we can undo the space reserve

		Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
		
		//  try to reserve a chunk of space large enough to cover the remaining
		//  space that we need

		QWORD ibLogical;
		QWORD cbSize;
		CallJ(	ErrSLVGetRange(	ppib,
								m_ifmp,
								m_cbRecommended - ibVirtual,
								&ibLogical,
								&cbSize,
								fTrue,
								fFalse ),
				HandleError2 );

		//  this space can be appended to the current run

		if ( ibLogical == run.ibLogicalNext )
			{
			//  increase the ibVirtualNext of this run to reflect the newly
			//  allocated space

			run.ibVirtualNext	+= cbSize;
			run.cbSize			+= cbSize;
			run.ibLogicalNext	+= cbSize;

			//  increase the size of the SLV File

			header.cbSize += cbSize;
			}

		//  this space cannot be appended to the current run

		else
			{
			//  move to after the last run to append a new run

			CallS( slvinfo.ErrMoveAfterLast() );

			//  set the run to include this data

			run.ibVirtualNext	= ibVirtual + cbSize;
			run.ibLogical		= ibLogical;
			run.qwReserved		= 0;
			run.ibVirtual		= ibVirtual;
			run.cbSize			= cbSize;
			run.ibLogicalNext	= ibLogical + cbSize;

			//  add a run to the header

			header.cbSize += cbSize;
			header.cRun++;
			}

		//  save our changes to the run

		CallJ( slvinfo.ErrSetCurrentRun( run ), HandleError2 );

		//  save our changes to the header
		
		CallS( slvinfo.ErrSetHeader( header ) );

		//  commit our transaction on success

		CallJ( ErrDIRCommitTransaction( ppib, NO_GRBIT ), HandleError2 );

		//  advance our reserve pointer

		ibVirtual += cbSize;

		//  handle errors for this space allocation

HandleError2:
		if ( err < JET_errSuccess )
			{
			CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );

			if ( err == JET_errDiskFull )
				{
				err = JET_errSuccess;
				break;
				}

			Call( err );
			}
		}

	//  commit our transaction on success, but use lazy flush as we will
	//  recover this space either way on a crash

	Call( ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush ) );
	fInTrx = fFalse;

	//  try to grant the reserved space to the SLV Provider

	err = ErrOSSLVRootSpaceReserve( rgfmp[ m_ifmp ].SlvrootSLV(), slvinfo );
	m_fCalledSLVProvider = fTrue;

	//  there was an error granting the space

	if ( err < JET_errSuccess )
		{
		//  try to free the space seen in all runs of this SLV Info, but don't
		//  worry if some of the space is not freed

		CallS( slvinfo.ErrMoveBeforeFirst() );
		while ( slvinfo.ErrMoveNext() != JET_errNoCurrentRecord )
			{
			CSLVInfo::RUN run;
			CallS( slvinfo.ErrGetCurrentRun( &run ) );

			(void)ErrSLVSpaceFree( m_ifmp, run.ibLogical, run.cbSize, fFalse );
			}
		
		//  fail with the original error

		Call( err );
		}

	//  rollback on an error

HandleError:
	if ( err < JET_errSuccess )
		{
		if ( fInTrx )
			{
			CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
			}
		if ( !m_fCalledSLVProvider )
			{
			slvinfo.Unload();
			CallS( slvinfo.ErrCreateVolatile() );
			CallS( ErrOSSLVRootSpaceReserve( rgfmp[ m_ifmp ].SlvrootSLV(), slvinfo ) );
			m_fCalledSLVProvider = fTrue;
			}
		}
	slvinfo.Unload();
	return err;
	}


//	X5:199689
//	this is called if we can't allocate enough resources (e.g. system PIB) to run the task
//	at a minimum we need to return 0 pages to IFS

VOID SLVRESERVETASK::HandleError( const ERR err )
	{
	if( !m_fCalledSLVProvider )
		{
		CSLVInfo slvinfo;
		CallS( slvinfo.ErrCreateVolatile() );
		CallS( ErrOSSLVRootSpaceReserve( rgfmp[ m_ifmp ].SlvrootSLV(), slvinfo ) );
		m_fCalledSLVProvider = fTrue;
		slvinfo.Unload();
		}
	}


// SLVSpaceRequest				- satisfies a synchronous request by the SLV Provider
//								  for more reserved space for an SLV Root.  we satisfy
//								  this request by posting a space request task to the
//								  task pool
//
// IN:
//		ifmpDb					- database ifmp that needs more space
//		cbRecommended			- recommended amount of space to reserve

void SLVSpaceRequest( const IFMP ifmpDb, const QWORD cbRecommended )
	{
	//  attempt to issue a task to reserve SLV space

	SLVRESERVETASK * const ptask = new SLVRESERVETASK( ifmpDb, cbRecommended );
	if ( ptask )
		{
		if ( PinstFromIfmp( ifmpDb )->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask ) >= JET_errSuccess )
			{
			return;
			}
		delete ptask;
		}

	//  we failed to issue the task, so we will tell the SLV Provider that we
	//  have no free space

	CSLVInfo slvinfo;
	CallS( slvinfo.ErrCreateVolatile() );
	CallS( ErrOSSLVRootSpaceReserve( rgfmp[ ifmpDb ].SlvrootSLV(), slvinfo ) );
	slvinfo.Unload();
	}


//  ================================================================
class SLVFREETASK : public DBTASK
//  ================================================================	
//
//  Frees space for the SLV Provider when it is enabled
//
//-
	{
	public:
		SLVFREETASK( const IFMP ifmpDb, const QWORD ibLogical, const QWORD cbSize, const BOOL fDeleted );
		
		ERR ErrExecute( PIB * const ppib );
		VOID HandleError( const ERR err );

	private:
		SLVFREETASK( const SLVFREETASK& );
		SLVFREETASK& operator=( const SLVFREETASK& );

		const QWORD	m_ibLogical;
		const QWORD	m_cbSize;
		const BOOL	m_fDeleted;
	};


//  ****************************************************************
//	SLVFREETASK
//  ****************************************************************

SLVFREETASK::SLVFREETASK( const IFMP ifmpDb, const QWORD ibLogical, const QWORD cbSize, const BOOL fDeleted )
	:	DBTASK( ifmpDb ),
		m_ibLogical( ibLogical ),
		m_cbSize( cbSize ),
		m_fDeleted( fDeleted )
	{
	//	don't fire off async tasks on the temp. database because the
	//	temp. database is simply not equipped to deal with concurrent access
	AssertRTL( dbidTemp != rgfmp[ifmpDb].Dbid() );
	}

ERR SLVFREETASK::ErrExecute( PIB * const ppib )
	{
	ERR		err			= JET_errSuccess;
	BOOL	fInTrx		= fFalse;

	//  free our space in a transaction so that we can rollback on an
	//  error

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTrx = fTrue;

	//  free the specified range

	if ( m_fDeleted )
		{
		Call( ErrSLVFreeDeletedRange( ppib, m_ifmp, m_ibLogical, m_cbSize ) );
		}
	else
		{
		Call( ErrSLVFreeReservedRange( ppib, m_ifmp, m_ibLogical, m_cbSize ) );
		}

	//  commit our transaction on success, but use lazy flush as we will
	//  recover this space either way on a crash

	Call( ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush ) );
	fInTrx = fFalse;

	//  rollback on an error

HandleError:
	if ( err < JET_errSuccess )
		{
		AssertTracking();
		if ( fInTrx )
			{
			CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
			}
		}
	return err;
	}


VOID SLVFREETASK::HandleError( const ERR err )
	{
	//	the space will be recovered when the database is next attached
	}


// ErrSLVSpaceFree				- satisfies a synchronous request by the SLV Provider
//								  to free space for an SLV Root.  we satisfy this
//								  request by posting a space free task to the task
//								  pool
//
// IN:
//		ifmpDb					- database ifmp that contains the space to free
//		ibLogical				- starting offset of range to free
//		cbSize					- size of range to free
//		fDeleted				- indicates that the space to be freed is Deleted
//
// RESULT:						ERR

ERR ErrSLVSpaceFree( const IFMP ifmpDb, const QWORD ibLogical, const QWORD cbSize, const BOOL fDeleted )
	{
	ERR				err			= JET_errSuccess;
	SLVFREETASK*	ptask		= NULL;

	//  issue a task to free SLV space

	if ( !( ptask = new SLVFREETASK( ifmpDb, ibLogical, cbSize, fDeleted ) ) )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	Call( PinstFromIfmp( ifmpDb )->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask ) );

	return JET_errSuccess;

HandleError:
	if ( ptask )
		{
		delete ptask;
		}
	return err;
	}


// ErrSLVDelete					- changes the state of the space owned by the given SLV
//								  from Committed to Deleted.  the space will later be
//								  moved from Deleted to Free when it is known that
//								  no one can possibly see the SLV any longer or when the
//								  system restarts
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		fCopyBuffer				- record in copy buffer
//
// RESULT:						ERR

ERR ErrSLVDelete(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const BOOL		fCopyBuffer )
	{
	ERR 			err;
	CSLVInfo		slvinfo;
	
	//  validate IN args

	ASSERT_VALID( pfucb );

	//  walk all runs in the SLVInfo and decommit their space
	//
	//  NOTE:  we do this backwards so that the space is cleaned up in reverse
	//  order.  this is so that if the SLV Provider is enabled, cleanup will
	//  work properly.  we must cleanup the first run last because it determines
	//  the File ID.  if this space is freed before the rest of the space for
	//  a given file, it is possible that another file could use this space and
	//  thereby cause a File ID collision.  this, of course, would be bad

	Call( slvinfo.ErrLoad( pfucb, columnid, itagSequence, fCopyBuffer ) );

	CSLVInfo::FILEID fileid;
	Call( slvinfo.ErrGetFileID( &fileid ) );

	QWORD cbAlloc;
	Call( slvinfo.ErrGetFileAlloc( &cbAlloc ) );

	wchar_t wszFileName[ IFileSystemAPI::cchPathMax ];
	Call( slvinfo.ErrGetFileName( wszFileName ) );

	CallS( slvinfo.ErrMoveAfterLast() );
	while ( ( err = slvinfo.ErrMovePrev() ) >= JET_errSuccess )
		{
		CSLVInfo::RUN run;
		Call( slvinfo.ErrGetCurrentRun( &run ) );
		
		Call( ErrSLVDeleteCommittedRange(	pfucb->ppib,
											pfucb->ifmp,
											run.ibLogical,
											run.cbSize,
											fileid,
											cbAlloc,
											wszFileName ) );
		}
	if ( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	slvinfo.Unload();
	return err;
	}

// ErrSLVLogDataFromFile		- logs the fact that the an SLV File with the
//								  given SLV Info has been inserted into the SLV
//								  streaming file, logging the actual data if
//								  requested
//
// IN:
//		ppib					- session
//      ifmpDb					- database ifmp
//		slvinfo					- SLV Info
//		pfapiSrc				- SLV File (optional)
//
// RESULT:						ERR

LOCAL ERR ErrSLVLogDataFromFile(	PIB*			ppib,
									IFMP			ifmpDb,
									CSLVInfo&		slvinfo,
									IFileAPI* const	pfapiSrc	= NULL )
	{
	ERR			err				= JET_errSuccess;
	IFileAPI*	pfapi			= pfapiSrc;
	void*		pvView			= NULL;
	void*		pvTempBuffer	= NULL;
	DATA		dataTemp;

	SLVOWNERMAPNODE	slvownermapNode;

#ifdef SLV_USE_VIEWS
	QWORD	cbLog			= SLVPAGE_SIZE;
#else  //  !SLV_USE_VIEWS
	QWORD	cbLog			= g_cbPage;
#endif  //  SLV_USE_VIEWS
	QWORD	cbView			= OSMemoryPageReserveGranularity();

	CSLVInfo::RUN run;
	
	//  validate IN args

	ASSERT_VALID( ppib );
	FMP::AssertVALIDIFMP( ifmpDb );
	Assert( rgfmp[ ifmpDb ].FSLVAttached() );

	//  get the header, file id, file allocation, and file name for the SLV

	CSLVInfo::HEADER header;
	CallS( slvinfo.ErrGetHeader( &header ) );

	CSLVInfo::FILEID fileid;
	Call( slvinfo.ErrGetFileID( &fileid ) );

	QWORD cbAlloc;
	Call( slvinfo.ErrGetFileAlloc( &cbAlloc ) );

	wchar_t wszFileName[ IFileSystemAPI::cchPathMax ];
	Call( slvinfo.ErrGetFileName( wszFileName ) );

	//  we will be logging the actual data for this SLV

// with checksum enabled, we always read the data as we need to checksum it
	if ( header.cbSize )
		{
#ifdef SLV_USE_VIEWS

		//  init the temp buffer such that we will log only one SLVPAGE_SIZE
		//  chunk of data at a time and so that we have no data mapped into a
		//  view initially

		dataTemp.SetPv( (BYTE*)pvView + cbView );
		dataTemp.SetCb( ULONG( cbLog ) );

		//  create a view of the source file, opening it if necessary

		if ( !pfapi )
			{
			Call( ErrOSSLVFileOpen(	rgfmp[ ifmpDb ].SlvrootSLV(),
									slvinfo,
									&pfapi,
									fTrue,
									fTrue ) );
			}
		
#else  //  !SLV_USE_VIEWS

		//  allocate a temporary buffer for use in reading the data
		
		BFAlloc( &pvTempBuffer );
		dataTemp.SetPv( pvTempBuffer );
		dataTemp.SetCb( cbLog );

#ifdef SLV_USE_RAW_FILE

		//  use the backing file handle for our reads

		pfapi = rgfmp[ ifmpDb ].PfapiSLV();

#else  //  !SLV_USE_RAW_FILE

		//  open the SLV as a file

		if ( !pfapi )
			{
			Call( ErrOSSLVFileOpen(	rgfmp[ ifmpDb ].SlvrootSLV(),
									slvinfo,
									&pfapi,
									fTrue,
									fTrue ) );
			}

#endif  //  SLV_USE_RAW_FILE
#endif  //  SLV_USE_VIEWS
		}
		
	//  log all data from the SLV File

	QWORD ibVirtual;
	ibVirtual = 0;
	
	memset( &run, 0, sizeof( run ) );

	CallS( slvinfo.ErrMoveBeforeFirst() );

	while ( ibVirtual < header.cbSize )
		{
		//  we need to retrieve another run from the SLV

		Assert( ibVirtual <= run.ibVirtualNext );
		if ( ibVirtual == run.ibVirtualNext )
			{
			//  retrieve the next run from the SLV

			Call( slvinfo.ErrMoveNext() );
			Call( slvinfo.ErrGetCurrentRun( &run ) );

			//  mark the space used for this SLV

			Call( ErrSLVCommitReservedRange(	ppib,
												ifmpDb,
												run.ibLogical,
												run.cbSize,
												fileid,
												cbAlloc,
												wszFileName ) );
			Call( slvownermapNode.ErrCreateForSearch( ifmpDb, PgnoOfOffset( ibVirtual - run.ibVirtual + run.ibLogical ) ) );
			}

		//  make sure that we don't read past the end of the valid data region

		dataTemp.SetCb( ULONG( min( cbLog, header.cbSize - ibVirtual ) ) );

		//  read data from the SLV if we are going to log it

// with checksum enabled, we always read the data as we need to checksum it
#ifdef SLV_USE_VIEWS

		//  advance to the next chunk of data that we need from this view

		dataTemp.SetPv( (BYTE*)dataTemp.Pv() + cbLog );

		//  the next chunk of data that we need is not in the this view

		if ( (BYTE*)dataTemp.Pv() >= (BYTE*)pvView + cbView )
			{
			//  unmap this view

			Call( pfapi->ErrMMFree( pvView ) );

			//  map a new view containing this offset

			Call( pfapi->ErrMMRead(	ibVirtual,
									min( cbView, header.cbSize - ibVirtual ),
									&pvView ) );

			//  set our buffer to point to this new view

			dataTemp.SetPv( pvView );
			}

#else  //  !SLV_USE_VIEWS
#ifdef SLV_USE_RAW_FILE

		//  read the next chunk of data to log from the backing file
		
		Call( pfapi->ErrIORead(	ibVirtual - run.ibVirtual + run.ibLogical,
								dataTemp.Cb(),
								(BYTE*)dataTemp.Pv() ) );

#else  //  !SLV_USE_RAW_FILE

		//  read the next chunk of data to log from the SLV File

		Call( pfapi->ErrIORead(	ibVirtual,
								dataTemp.Cb(),
								(BYTE*)dataTemp.Pv() ) );

#endif  //  SLV_USE_RAW_FILE
#endif  //  SLV_USE_VIEWS

		//  log an append with or without data, depending on fDataRecoverable.
		//  if data is not logged, redo will assume the data is all zeroes

		LGPOS lgposAppend;
		Call( ErrLGSLVPageAppend(	ppib,
									ifmpDb | ifmpSLV,
									ibVirtual - run.ibVirtual + run.ibLogical,
									dataTemp.Cb(),
									dataTemp.Pv(),
									header.fDataRecoverable,
									&slvownermapNode,
									&lgposAppend ) );
		slvownermapNode.NextPage();
		//  advance our read pointer

		ibVirtual += dataTemp.Cb();
		}

	//  if there is any space in this file beyond the valid data, commit it
	//
	//  NOTE:  if this is a zero length file, we must have at least one
	//  block of space to uniquely identify this file

	while ( slvinfo.ErrMoveNext() != JET_errNoCurrentRecord )
		{
		//  retrieve the next run from the SLV

		Call( slvinfo.ErrGetCurrentRun( &run ) );

		//  mark the space used for this SLV

		Call( ErrSLVCommitReservedRange(	ppib,
											ifmpDb,
											run.ibLogical,
											run.cbSize,
											fileid,
											cbAlloc,
											wszFileName ) );
		}

HandleError:

#ifdef SLV_USE_VIEWS

	if ( err == errLGRecordDataInaccessible )
		{
		err = ErrERRCheck( JET_errSLVFileIO );
		}

	(void)pfapi->ErrMMFree( pvView );

	if ( pfapi != pfapiSrc )
		{
		delete pfapi;
		}

#else  //  !SLV_USE_VIEWS
#ifdef SLV_USE_RAW_FILE
#else  //  !SLV_USE_RAW_FILE

	if ( pfapi != pfapiSrc )
		{
		delete pfapi;
		}

#endif  //  SLV_USE_RAW_FILE

	if ( pvTempBuffer )
		{
		BFFree( pvTempBuffer );
		}

#endif  //  SLV_USE_VIEWS

	return err;
	}

class SLVCHUNK
	{
	public:
		SLVCHUNK()
			:	m_msigReadComplete( CSyncBasicInfo( _T( "SLVCHUNK::m_msigReadComplete" ) ) ),
				m_msigWriteComplete( CSyncBasicInfo( _T( "SLVCHUNK::m_msigWriteComplete" ) ) )
			{
			m_pfapiDest					= NULL;
			m_msigReadComplete.Set();
			m_msigWriteComplete.Set();
			m_err						= JET_errSuccess;
			}

		~SLVCHUNK()
			{
			Assert( m_msigReadComplete.FTryWait() );
			Assert( m_msigWriteComplete.FTryWait() );
			}
	
	public:
		IFileAPI*			m_pfapiDest;
		CManualResetSignal	m_msigReadComplete;
		CManualResetSignal	m_msigWriteComplete;
		ERR					m_err;
	};

void SLVICopyFileIWriteComplete(	const ERR			err,
									IFileAPI* const		pfapiDest,
									const QWORD			ibVirtual,
									const DWORD			cbData,
									const BYTE* const	pbData,
									SLVCHUNK* const		pslvchunk )
	{
	//  save the error code for the write

	pslvchunk->m_err = err;

	//  signal that the write has completed

	pslvchunk->m_msigWriteComplete.Set();
	}

void SLVICopyFileIReadComplete(	const ERR			err,
								IFileAPI* const		pfapiSrc,
								const QWORD			ibVirtual,
								const DWORD			cbData,
								const BYTE* const	pbData,
								SLVCHUNK* const		pslvchunk )
	{
	//  save the error code for the read

	pslvchunk->m_err = err;

	//  signal that the read has completed

	pslvchunk->m_msigReadComplete.Set();

	//  the read was successful

	if ( err >= JET_errSuccess )
		{		
		//  write the data to the destination file immediately
		//
		//  NOTE:  round up the write size to the nearest page boundary so that
		//  we can do uncached writes to files of odd sizes.  we know that the
		//  source buffer will be large enough for this

		CallS( pslvchunk->m_pfapiDest->ErrIOWrite(	ibVirtual,
													( ( cbData + g_cbPage - 1 ) / g_cbPage ) * g_cbPage, 
													pbData,
													IFileAPI::PfnIOComplete( SLVICopyFileIWriteComplete ),
													DWORD_PTR( pslvchunk ) ) );
		CallS( pslvchunk->m_pfapiDest->ErrIOIssue() );
		}

	//  the read was not successful

	else
		{
		//  complete the write callback with the same error

		SLVICopyFileIWriteComplete(	err,
									pslvchunk->m_pfapiDest,
									ibVirtual,
									( ( cbData + g_cbPage - 1 ) / g_cbPage ) * g_cbPage,
									pbData,
									pslvchunk );
		}
	}

// ErrSLVCopyFile				- copies the data from the given SLV File into the
//								  specifed record and field.  the SLV is copied via
//								  the SLV Provider
//
// IN:
//		slvinfo					- source SLV Info (empty indicates unknown)
//		pfapi					- source SLV File
//		cbSize					- source SLV File valid data length
//		fDataRecoverable		- data is logged for recovery
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//
// RESULT:						ERR

LOCAL ERR ErrSLVCopyFile(
	CSLVInfo&		slvinfo,
	IFileAPI* const	pfapi,
	const QWORD		cbSize,
	const BOOL		fDataRecoverable,
	FUCB*			pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence)
	{
	ERR				err				= JET_errSuccess;
	IFileAPI*		pfapiDestSize	= NULL;
	IFileAPI*		pfapiDest		= NULL;
	CSLVInfo		slvinfoDest;
	size_t			cChunk			= 0;
	size_t			cbChunk			= 0;
	SLVCHUNK*		rgslvchunk		= NULL;
	BYTE*			rgbChunk		= NULL;
	
	CSLVInfo::RUN runDest;
	CSLVInfo::HEADER headerDest;
	
	SLVOWNERMAPNODE	slvownermapNode;
	
	//  validate IN args

	Assert( pfapi );
	ASSERT_VALID( pfucb );

	//  create the destination SLV File with a temporary name based on the
	//  number of copies done so far.  we will never persist this name so there
	//  is no worry of a name collision in the future
	//
	//  set the size of the destination SLV File first to force the SLV Provider
	//  to allocate its space optimally, to ensure we have enough space before
	//  we begin the copy, and to enable us to determine the physical location
	//  of the SLV File before hand so that we can copy it and log it in one pass

	QWORD cbSizeAlign = ( ( cbSize + g_cbPage - 1 ) / g_cbPage ) * g_cbPage;

	wchar_t wszFileName[ IFileSystemAPI::cchPathMax ];
	swprintf( wszFileName, L"$CopyDest%016I64X$", rgfmp[ pfucb->ifmp ].DbtimeIncrementAndGet() );
	Call( ErrOSSLVFileCreate(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
								wszFileName,
								cbSizeAlign,
								&pfapiDestSize ) );

	//  get the SLVInfo for the destination SLV we just created, committing its
	//  space in the SLV Provider

	Call( slvinfoDest.ErrCreate( pfucb, columnid, itagSequence, fTrue ) );
	CallS( slvinfoDest.ErrSetFileNameVolatile() );
	Call( ErrOSSLVFileGetSLVInfo(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
									pfapiDestSize,
									&slvinfoDest ) );

	//  get the header, file id, and file allocation for the destination SLV

	CallS( slvinfoDest.ErrGetHeader( &headerDest ) );

	CSLVInfo::FILEID fileid;
	Call( slvinfoDest.ErrGetFileID( &fileid ) );

	QWORD cbAlloc;
	Call( slvinfoDest.ErrGetFileAlloc( &cbAlloc ) );

	//  the destination SLV had better be setup properly

	if (	headerDest.cbSize < cbSizeAlign ||
			cbAlloc < cbSizeAlign )
		{
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

	//  open the destination file with copy-on-write disabled so that we can copy
	//  and log the file data at the same time using the SLV Info acquired above,
	//  the acquisition of which turns on copy-on-write for the file

	Call( ErrOSSLVFileOpen(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
							slvinfoDest,
							&pfapiDest,
							fFalse,
							fFalse ) );

	//  close the old destination file handle.  this must be done after opening
	//  the second handle so that we always have a reference on the file so its
	//  uncommitted space doesn't get freed by the SLV File Table

	delete pfapiDestSize;
	pfapiDestSize = NULL;

	//  set the true size and data recoverability for the destination SLV

	headerDest.cbSize			= cbSize;
	headerDest.fDataRecoverable	= fDataRecoverable;
	CallS( slvinfoDest.ErrSetHeader( headerDest ) );

	//  allocate resources for the copy
	//  CONSIDER:  expose these settings

	cbChunk	= size_t( min( 64 * 1024, cbSizeAlign ) );

	if ( cbChunk == 0 )
		{
		Assert( 0 == cbSize );
		Assert( 0 == cChunk);	
		Assert( NULL == rgslvchunk);	
		Assert( NULL == rgbChunk);	
		}
	else
		{
		Assert( cbChunk );	
		Assert( cbSize );
		
		cChunk = size_t( min( 16, cbSizeAlign / cbChunk ) );

		if ( !( rgslvchunk = new SLVCHUNK[ cChunk ] ) )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}

		//	this memory must be page aligned for I/O
		
		if ( !( rgbChunk = (BYTE*)PvOSMemoryPageAlloc( cChunk * cbChunk, NULL ) ) )	
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		}

	//  pre-issue reads to the source SLV File for the first n chunks of the data
	//  to be copied to prime the pump for the copy operation

	QWORD ibVirtualMac;
	ibVirtualMac = min( headerDest.cbSize, cChunk * cbChunk );
	
	QWORD ibVirtual;
	ibVirtual = 0;

	// the function that writes the destination data will zero out the unused part of the last page
	// this will allow to compute the checksum only over the valid data
	// Assert( buffer that will be used to zero out will be large enough );

	while ( ibVirtual < ibVirtualMac )
		{
		Assert ( cbChunk );	
		Assert ( cChunk );	
		Assert ( rgslvchunk );	
		Assert ( rgbChunk );	
		
		const size_t iChunk = size_t( ( ibVirtual / cbChunk ) % cChunk );

		rgslvchunk[ iChunk ].m_pfapiDest					= pfapiDest;
		rgslvchunk[ iChunk ].m_msigReadComplete.Reset();
		rgslvchunk[ iChunk ].m_msigWriteComplete.Reset();
		rgslvchunk[ iChunk ].m_err							= JET_errSuccess;

		CallS( pfapi->ErrIORead(	ibVirtual,
									DWORD( min( cbChunk, ibVirtualMac - ibVirtual ) ),
									rgbChunk + iChunk * cbChunk,
									IFileAPI::PfnIOComplete( SLVICopyFileIReadComplete ),
									DWORD_PTR( &rgslvchunk[ iChunk ] ) ) );

		ibVirtual += cbChunk;
		}
	CallS( pfapi->ErrIOIssue() );

	//  copy all data from the source SLV File to the destination SLV File,
	//  logging that data and committing that space as we go

	memset( &runDest, 0, sizeof( runDest ) );
	CallS( slvinfoDest.ErrMoveBeforeFirst() );

	ibVirtual = 0;

	while ( ibVirtual < headerDest.cbSize )
		{

		Assert ( cbChunk );	
		Assert ( cChunk );	
		Assert ( rgslvchunk );	
		Assert ( rgbChunk );	

		//  we need to retrieve another run from the destination SLV

		Assert( ibVirtual <= runDest.ibVirtualNext );
		if ( ibVirtual == runDest.ibVirtualNext )
			{
			//  retrieve the next run from the destination SLV

			Call( slvinfoDest.ErrMoveNext() );
			Call( slvinfoDest.ErrGetCurrentRun( &runDest ) );

			//  mark the space used by this run in the destination SLV

			Call( ErrSLVCommitReservedRange(	pfucb->ppib,
												pfucb->ifmp,
												runDest.ibLogical,
												runDest.cbSize,
												fileid,
												cbAlloc,
												wszFileName ) );
			Call( slvownermapNode.ErrCreateForSearch( pfucb->ifmp, PgnoOfOffset( ibVirtual - runDest.ibVirtual + runDest.ibLogical ) ) );
			}

		//  wait for the read for the current chunk to complete

		const size_t iChunk = size_t( ( ibVirtual / cbChunk ) % cChunk );
		const size_t cbOffsetInChunk = size_t( ibVirtual % cbChunk );

		rgslvchunk[ iChunk ].m_msigReadComplete.Wait();
		Call( rgslvchunk[ iChunk ].m_err );

		//  log an append with or without data, depending on fDataRecoverable.
		//  if data is not logged, redo will assume the data is all zeroes

		LGPOS lgposAppend;
		Call( ErrLGSLVPageAppend(	pfucb->ppib,
									pfucb->ifmp | ifmpSLV,
									ibVirtual - runDest.ibVirtual + runDest.ibLogical,
									ULONG( min( SLVPAGE_SIZE, headerDest.cbSize - ibVirtual ) ),
									rgbChunk + ( iChunk * cbChunk ) + cbOffsetInChunk,
									headerDest.fDataRecoverable,
									&slvownermapNode, 
									&lgposAppend ) );
		slvownermapNode.NextPage();

		//  advance our copy pointer

		ibVirtual += SLVPAGE_SIZE;

		//  we are done logging this chunk of the destination SLV File

		if ( ibVirtual % cbChunk == 0 )
			{
			//  wait for the write for the current chunk to complete

			rgslvchunk[ iChunk ].m_msigWriteComplete.Wait();
			Call( rgslvchunk[ iChunk ].m_err );

			//  there is more data to read from the source SLV File

			QWORD ibVirtualChunkNext;
			ibVirtualChunkNext = ibVirtual + ( cChunk - 1 ) * cbChunk;
			
			if ( ibVirtualChunkNext < headerDest.cbSize )
				{
				//  issue a read to the source SLV File for the next chunk
				
				rgslvchunk[ iChunk ].m_pfapiDest					= pfapiDest;
				rgslvchunk[ iChunk ].m_msigReadComplete.Reset();
				rgslvchunk[ iChunk ].m_msigWriteComplete.Reset();
				rgslvchunk[ iChunk ].m_err							= JET_errSuccess;

				CallS( pfapi->ErrIORead(	ibVirtualChunkNext,
											DWORD( min( cbChunk, headerDest.cbSize - ibVirtualChunkNext ) ),
											rgbChunk + iChunk * cbChunk,
											IFileAPI::PfnIOComplete( SLVICopyFileIReadComplete ),
											DWORD_PTR( &rgslvchunk[ iChunk ] ) ) );
				CallS( pfapi->ErrIOIssue() );
				}
			}
		}

	//  if there is any space in this file beyond the valid data, commit it
	//
	//  NOTE:  if this is a zero length file, we must have at least one
	//  block of space to uniquely identify this file

	while ( slvinfoDest.ErrMoveNext() != JET_errNoCurrentRecord )
		{
		//  retrieve the next run from the SLV

		Call( slvinfoDest.ErrGetCurrentRun( &runDest ) );

		//  mark the space used for this SLV

		Call( ErrSLVCommitReservedRange(	pfucb->ppib,
											pfucb->ifmp,
											runDest.ibLogical,
											runDest.cbSize,
											fileid,
											cbAlloc,
											wszFileName ) );
		}

	if ( FFUCBReplacePrepared(pfucb) )
		{
		ERR errT = slvinfoDest.ErrMoveBeforeFirst();
				
		Assert ( JET_errSuccess == errT );
		
		errT = slvinfoDest.ErrMoveNext();
		while ( JET_errSuccess <= errT )
			{
			CSLVInfo::RUN run;
		
			Call( slvinfoDest.ErrGetCurrentRun( &run ) );

			const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
			const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );

			Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
			Call( ErrSLVOwnerMapSetUsageRange(
						pfucb->ppib,
						pfucb->ifmp,
						pgnoFirst,
						cpg,
						pfucb->u.pfcb->ObjidFDP(),
						columnid,
						&pfucb->bmCurr,
						fSLVOWNERMAPSetSLVCopyFile ) );

			errT = slvinfoDest.ErrMoveNext();
			}
		Assert ( JET_errNoCurrentRecord == errT );

		}
	else
		{
		Assert ( FFUCBInsertPrepared( pfucb ) );
		FUCBSetSLVOwnerMapNeedUpdate( pfucb );
		}		

	//  save our changes to the destination SLV
	Call( slvinfoDest.ErrSave() );

	//  register the copy with the SLV Provider
	
	Call( ErrOSSLVRootCopyFile(		rgfmp[ pfucb->ifmp ].SlvrootSLV(),
															pfapi,
															slvinfo,
															pfapiDest,
															slvinfoDest,
															QWORD( rgfmp[ pfucb->ifmp ].DbtimeIncrementAndGet() ) ) );


HandleError:
	if ( rgslvchunk )
		{
		for ( DWORD iChunk = 0; iChunk < cChunk; iChunk++ )
			{
			rgslvchunk[ iChunk ].m_msigReadComplete.Wait();
			rgslvchunk[ iChunk ].m_msigWriteComplete.Wait();
			}
		}
	OSMemoryPageFree( rgbChunk );
	delete [] rgslvchunk;
	delete pfapiDest;
	slvinfoDest.Unload();
	delete pfapiDestSize;
	return err;
	}

// ErrSLVSetColumnFromEA		- set a SLV column from an SLV Provider EA List
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		pdata					- source buffer
//		grbit					- grbit
//
// RESULT:						ERR

LOCAL ERR ErrSLVSetColumnFromEA(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	DATA			* pdata,
	const ULONG		grbit )
	{
	ERR				err		= JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( pfucb );
	Assert( PinstFromPfucb( pfucb )->FSLVProviderEnabled() );
	Assert( FCOLUMNIDTagged( columnid ) );
	Assert( itagSequence );
	Assert( pdata );
	ASSERT_VALID( pdata );
	Assert( !( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
							JET_bitSetSLVFromSLVEA ) ) );
	
	//  create a new iterator to contain the SLV Info for this SLV

	CSLVInfo slvinfo;
	Call( slvinfo.ErrCreate( pfucb, columnid, itagSequence, fTrue ) );
	
	//  if this SLV's data is recoverable, mark the SLV's header as such

	BOOL fDataRecoverable;
	fDataRecoverable = fTrue;//!( grbit & JET_bitSetSLVDataNotRecoverable );

	if ( fDataRecoverable )
		{
		CSLVInfo::HEADER header;
		CallS( slvinfo.ErrGetHeader( &header ) );
		header.fDataRecoverable = fTrue;
		CallS( slvinfo.ErrSetHeader( header ) );
		}

	//  retrieve the SLV Info for this SLV EA List

	Call( ErrOSSLVFileConvertEAToSLVInfo(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
											pdata->Pv(),
											pdata->Cb(),
											&slvinfo ) );

	// mark the space usage in SLVOWNERMAP on update operations
	if ( FFUCBReplacePrepared( pfucb ) )
		{
		ERR errT = slvinfo.ErrMoveBeforeFirst();
		
		if ( JET_errSuccess <= errT )
			errT = slvinfo.ErrMoveNext();

		while ( JET_errSuccess <= errT )
			{
			CSLVInfo::RUN run;
			
			Call( slvinfo.ErrGetCurrentRun( &run ) );

			const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
			const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );

			Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
			Call( ErrSLVOwnerMapSetUsageRange(
						pfucb->ppib,
						pfucb->ifmp,
						pgnoFirst,
						cpg,
						pfucb->u.pfcb->ObjidFDP(),
						columnid,
						&pfucb->bmCurr,
						fSLVOWNERMAPSetSLVFromEA ) );

			errT = slvinfo.ErrMoveNext();
			}
		}
	else
		{
		Assert ( FFUCBInsertPrepared( pfucb ) );
		FUCBSetSLVOwnerMapNeedUpdate( pfucb );
		}
	
	//  log this SLV for recovery

	Call( ErrSLVLogDataFromFile(	pfucb->ppib,
									pfucb->ifmp,
									slvinfo ) );

	//  save our changes to the SLV Info

	Call( slvinfo.ErrSave() );

HandleError:
	slvinfo.Unload();
	return err;
	}

// ErrSLVSetColumnFromFile		- set a SLV column from an SLV Provider File Handle
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		pdata					- source buffer
//		grbit					- grbit
//
// RESULT:						ERR

LOCAL ERR ErrSLVSetColumnFromFile(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	DATA			* pdata,
	const ULONG		grbit )
	{
	ERR				err			= JET_errSuccess;
	IFileAPI*		pfapiSrc	= NULL;
	CSLVInfo		slvinfo;

	//  validate IN args

	ASSERT_VALID( pfucb );
	Assert( PinstFromPfucb( pfucb )->FSLVProviderEnabled() );
	Assert( FCOLUMNIDTagged( columnid ) );
	Assert( itagSequence );
	Assert( pdata );
	ASSERT_VALID( pdata );
	Assert( !( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
							JET_bitSetSLVFromSLVFile ) ) );

	//  open the SLV File

	Call( ErrOSSLVFileOpen(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
							pdata->Pv(),
							pdata->Cb(),
							&pfapiSrc,
							fTrue ) );

	//  create a new iterator to contain the SLV Info for this SLV

	Call( slvinfo.ErrCreate( pfucb, columnid, itagSequence, fTrue ) );
	
	//  if this SLV's data is recoverable, mark the SLV's header as such

	BOOL fDataRecoverable;
	fDataRecoverable = fTrue;//!( grbit & JET_bitSetSLVDataNotRecoverable );

	if ( fDataRecoverable )
		{
		CSLVInfo::HEADER header;
		CallS( slvinfo.ErrGetHeader( &header ) );
		header.fDataRecoverable = fTrue;
		CallS( slvinfo.ErrSetHeader( header ) );
		}

	//  retrieve the SLV Info for this SLV File

	err = ErrOSSLVFileGetSLVInfo(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
									pfapiSrc,
									&slvinfo );
	if ( err != JET_errSLVEAListCorrupt )
		{
		Call( err );
		}

	//  the SLV EA List was corrupt

	if ( err == JET_errSLVEAListCorrupt )
		{
		//  if we are here, we assume that this SLV File is not from the local
		//  store or there is some other reason why this SLV File cannot be
		//  committed.  to resolve this, we will make a new copy of the file

		//  get the SLV File's size

		QWORD cbSizeSrc;
		Call( pfapiSrc->ErrSize( &cbSizeSrc ) );

		//  copy the SLV File into the local store

		CSLVInfo slvinfoSrc;
		CallS( slvinfo.ErrCreateVolatile() );
		Call( ErrSLVCopyFile(	slvinfoSrc,
								pfapiSrc,
								cbSizeSrc,
								fDataRecoverable,
								pfucb,
								columnid,
								itagSequence) );
		}

	//  the SLV EA List was not corrupt

	else
		{
		// mark the space usage in SLVOWNERMAP on update operations
		if ( FFUCBReplacePrepared( pfucb ) )
			{
			ERR errT = slvinfo.ErrMoveBeforeFirst();
			
			if ( JET_errSuccess <= errT )
				errT = slvinfo.ErrMoveNext();

			while ( JET_errSuccess <= errT )
				{
				CSLVInfo::RUN run;
				
				Call( slvinfo.ErrGetCurrentRun( &run ) );

				const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
				const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );

				Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
				Call( ErrSLVOwnerMapSetUsageRange(
							pfucb->ppib,
							pfucb->ifmp,
							pgnoFirst,
							cpg,
							pfucb->u.pfcb->ObjidFDP(),
							columnid,
							&pfucb->bmCurr,
							fSLVOWNERMAPSetSLVFromFile));

				errT = slvinfo.ErrMoveNext();
				}
			}
		else
			{
			Assert ( FFUCBInsertPrepared( pfucb ) );
			FUCBSetSLVOwnerMapNeedUpdate( pfucb );
			}
		
		//  log this SLV for recovery

		Call( ErrSLVLogDataFromFile(	pfucb->ppib,
										pfucb->ifmp,
										slvinfo,
										pfapiSrc ) );

		//  save our changes to the SLV Info

		Call( slvinfo.ErrSave() );
		}

HandleError:
	delete pfapiSrc;
	slvinfo.Unload();
	return err;
	}


//  ================================================================
ULONG32 UlChecksumSLV( const BYTE * const pbMin, const BYTE * const pbMax )
//  ================================================================
	{	
	const ULONG32 		ulSeed 					= 0x89abcdef;
	const ULONG32 		cBitsInByte 			= 8;
	const ULONG32		ulResult				= LOG::UlChecksumBytes( pbMin, pbMax, ulSeed );

	return ulResult;
	}



// ErrSLVWriteRun				- writes SLV Data to an SLV Run
//
// IN:
//		ppib					- session
//      ifmpDb					- database ifmp
//		run						- SLV Run
//		ibOffset				- offset to start writing
//		pdata					- source buffer
//		pcbRead					- ULONG where to return the written data size
//		fDataRecoverable		- data is logged for recovery
//
// RESULT:						ERR
//
// OUT:	
//		pdata					- SLV Data
//		pcbWritten				- SLV Data bytes written
LOCAL ERR ErrSLVWriteRun(	FUCB * 			pfucb,
							PIB*			ppib,
							IFMP			ifmpDb,
							CSLVInfo::RUN&	run,
							QWORD			ibVirtualStart,
							DATA*			pdata,
							QWORD*			pcbWritten,
							BOOL			fDataRecoverable )
	{

	ERR				err		= JET_errSuccess;
	INST			*pinst	= PinstFromIfmp( ifmpDb );
	QWORD			ibData;
	QWORD			ibLogical;
	SLVOWNERMAPNODE	slvownermapNode;

	
	//  validate IN args
	Assert ( pinst );
	Assert ( pfucbNil == pfucb || (pfucb->ifmp == ifmpDb && pfucb->ppib == ppib) );

	ASSERT_VALID( ppib );
	FMP::AssertVALIDIFMP( ifmpDb );
	Assert( rgfmp[ ifmpDb ].FSLVAttached() );
	Assert( run.ibVirtual <= ibVirtualStart );
	Assert( ibVirtualStart < run.ibVirtualNext );
	Assert( pdata );
	ASSERT_VALID( pdata );
	Assert( pcbWritten );

	//  write data from pages in run until we have run out of data or space

	ibData		= 0;
	ibLogical	= run.ibLogical + ibVirtualStart - run.ibVirtual;

	const BOOL  fUpdateOwnerMap		= (BOOL) ( NULL != rgfmp[ifmpDb].PfcbSLVOwnerMap() );

#ifdef SLV_USE_CHECKSUM_FRONTDOOR
#else
	BOOL		fInvalidateChecksum	= fFalse;
#endif

	Assert( ( fUpdateOwnerMap && !pinst->FRecovering() ) ||
			( !fUpdateOwnerMap && pinst->FRecovering() ) ); // = ( fUpdateOwnerMap ^ pinst->m_plog->m_fRecovering )

	Call( slvownermapNode.ErrCreateForSearch( ifmpDb, PgnoOfOffset( ibLogical ) ) );

#ifdef SLV_USE_CHECKSUM_FRONTDOOR
#else
	if ( fUpdateOwnerMap )
		{
		ULONG	dwT1, dwT2;
		err = slvownermapNode.ErrGetChecksum( ppib, &dwT1, &dwT2 );
		if ( err < 0 )
			{
			if ( errSLVInvalidOwnerMapChecksum != err )
				{
				goto HandleError;
				}
			err = JET_errSuccess;
			}
		else
			{
			fInvalidateChecksum = fTrue;
			}
		}
#endif // SLV_USE_CHECKSUM_FRONTDOOR
	
	while ( ibData < pdata->Cb() && ibLogical < run.ibLogicalNext )
		{
		//  determine the ifmp / pgno / ib / cb where we will write data

		IFMP	ifmp	= ifmpDb | ifmpSLV;
		PGNO	pgno	= PgnoOfOffset( ibLogical );
		QWORD	ib		= ibLogical % SLVPAGE_SIZE;
		QWORD	cb		= min( pdata->Cb() - ibData, SLVPAGE_SIZE - ib );

		Assert( fUpdateOwnerMap == (BOOL) ( NULL != rgfmp[ifmpDb].PfcbSLVOwnerMap() ) );

		Assert ( ( fUpdateOwnerMap && !pinst->FRecovering() )
			  || ( !fUpdateOwnerMap && pinst->FRecovering() ) ); // = ( fUpdateOwnerMap ^ pinst->m_plog->m_fRecovering )

		Assert( !pinst->FSLVProviderEnabled() );
		
		DWORD ulChecksum;
		
		if ( fUpdateOwnerMap )
			{
			Assert ( pfucbNil != pfucb);
			RCE::SLVPAGEAPPEND data;

			data.ibLogical	= OffsetOfPgno( pgno ) + ib;
			data.cbData		= DWORD( cb );

			Call ( pinst->m_pver->ErrVERFlag( pfucb, operSLVPageAppend, &data, sizeof(RCE::SLVPAGEAPPEND) ) );
			}

		//  log the append with or without data, depending on fDataRecoverable.
		//  if data is not logged, redo will assume the data is all zeroes

		LGPOS lgposAppend;
		// We never update space map here because SLVProvider is disabled.
		Call( ErrLGSLVPageAppend(	ppib,
									ifmp,
									ibLogical,
									ULONG( cb ),
									(BYTE*)pdata->Pv() + ibData,
									fDataRecoverable,
									NULL,
									&lgposAppend ) );

		//  write latch the current ifmp / pgno.  we will write latch a new page
		//  when the starting offset is on a page boundary because we are only
		//  appending data to the SLV and no valid data could already exist on
		//  that page anyway

		BFLatch bfl;
		Call( ErrBFWriteLatchPage( &bfl, ifmp, pgno, ib ? bflfDefault : bflfNew ) );

		//  dirty and setup log dependencies on the page

		BFDirty( &bfl );
		Assert( !rgfmp[ ifmpDb ].FLogOn() || !PinstFromIfmp( ifmpDb )->m_plog->m_fLogDisabled );
		if ( rgfmp[ ifmpDb ].FLogOn() )
			{
			BFSetLgposBegin0( &bfl, ppib->lgposStart );
			BFSetLgposModify( &bfl, lgposAppend );
			}

		//  copy the desired data from the buffer onto the page

		UtilMemCpy( (BYTE*)bfl.pv + ib, (BYTE*)pdata->Pv() + ibData, size_t( cb ) );

		// we are not overwriting data just appending 
		// so we set to 0 all not used data into the page
		// we need to do this before the checksum calculation
		//	==> we now store cbChecksum, so no need to zero-extend page
		Assert( SLVPAGE_SIZE >= ib + cb );
//		memset( (BYTE*)bfl.pv + ib + cb , 0, size_t( SLVPAGE_SIZE - (ib + cb) ) );

		if ( fUpdateOwnerMap )
			{
			ulChecksum = UlChecksumSLV( (BYTE*)bfl.pv, (BYTE*)bfl.pv + ib + cb );
			}

			//  unlatch the current ifmp / pgno

		BFWriteUnlatch( &bfl );

		if ( fUpdateOwnerMap )
			{
			// if we DON'T set frontdoor checksum and frontdoor is enabled
			// (ValidChecksum flag will remaing unset)
#ifdef SLV_USE_CHECKSUM_FRONTDOOR
			slvownermapNode.AssertOnPage( pgno );
			Call( slvownermapNode.ErrSetChecksum( ppib, ulChecksum, ULONG( cb ) ) );
			slvownermapNode.NextPage();
#else // SLV_USE_CHECKSUM_FRONTDOOR
			if ( rgfmp[ ifmpDb ].FDefragSLVCopy() )
				{
				slvownermapNode.AssertOnPage( pgno );
				Call( slvownermapNode.ErrSetChecksum( ppib, ulChecksum, ULONG( cb ) ) );
				slvownermapNode.NextPage();
				}
			else
				{
				if ( fInvalidateChecksum )
					{
					slvownermapNode.AssertOnPage( pgno );
					Call( slvownermapNode.ErrResetChecksum( ppib ) );
					slvownermapNode.NextPage();
					}
#ifdef DEBUG
				else
					{
					slvownermapNode.AssertOnPage( pgno );
					ERR errT;
					ULONG dwT1, dwT2;
					errT = slvownermapNode.ErrGetChecksum( ppib, &dwT1, &dwT2 );
					Assert( errT < 0 );
					slvownermapNode.NextPage();
					}
#endif // DEBUG
				}
#endif // SLV_USE_CHECKSUM_FRONTDOOR
			}
		//  advance copy pointers

		ibData		+= cb;
		ibLogical	+= cb;
		}

	//  set written bytes for return

	*pcbWritten = ibData;

HandleError:
	return err;
	}

// ErrSLVSetColumnFromData		- set a SLV column from actual data
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		pdata					- source buffer
//		grbit					- grbit
//
// RESULT:						ERR

LOCAL ERR ErrSLVSetColumnFromData(
	FUCB				* pfucb,
	const COLUMNID		columnid,
	const ULONG			itagSequence,
	DATA				* pdata,
	const ULONG			grbit )
	{
	ERR					err			= JET_errSuccess;
	DATA				dataAppend;
	CSLVInfo			slvinfo;
	CSLVInfo::HEADER	header;
		

	//  validate IN args

	ASSERT_VALID( pfucb );
	Assert( !PinstFromPfucb( pfucb )->FSLVProviderEnabled() );
	Assert( FCOLUMNIDTagged( columnid ) );
	Assert( itagSequence );
	Assert( pdata );
	ASSERT_VALID( pdata );
	Assert( !( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
							JET_bitSetAppendLV ) ) );
	
	//  get parameters for setting the SLV

	BOOL fAppend			= ( grbit & JET_bitSetAppendLV ) != 0;
	BOOL fDataRecoverable	= fTrue;//!( grbit & JET_bitSetSLVDataNotRecoverable );

	//  we are appending

	if ( fAppend )
		{
		//  get the SLVInfo for this SLV

		Call( slvinfo.ErrLoad( pfucb, columnid, itagSequence, fTrue ) );
		CallS( slvinfo.ErrGetHeader( &header ) );

		//  retrieve the data recoverability state for this SLV, ignoring
		//  whatever the user is currently requesting.  we do this to ensure
		//  consistent behavior per SLV so that we don't see half the data
		//  logged and the other half not logged, for example

		fDataRecoverable = header.fDataRecoverable;
		}

	//  we are not appending

	else
		{
		//  create the SLVInfo for this SLV

		Call( slvinfo.ErrCreate( pfucb, columnid, itagSequence, fTrue ) );
		CallS( slvinfo.ErrGetHeader( &header ) );

		//  this SLV had better be empty

		Assert( !header.cbSize );
		Assert( !header.cRun );

		//  allocate an initial run for the SLV.  we do this even if there is
		//  no data to append because all SLVs must have at least one run.  we
		//  also set the data recoverability state for this SLV

		QWORD ibLogical;
		QWORD cbSize;
		Call( ErrSLVGetRange(	pfucb->ppib,
								pfucb->ifmp,
								max( pdata->Cb(), 1 ),
								&ibLogical,
								&cbSize,
								fFalse,
								fFalse ) );
								
		CSLVInfo::RUN run;
		run.ibVirtualNext	= cbSize;
		run.ibLogical		= ibLogical;
		run.qwReserved		= 0;
		run.ibVirtual		= 0;
		run.cbSize			= cbSize;
		run.ibLogicalNext	= ibLogical + cbSize;

		CallS( slvinfo.ErrMoveAfterLast() );
		Call( slvinfo.ErrSetCurrentRun( run ) );

		// mark the space usage in SLVOWNERMAP on update operations
		if ( FFUCBReplacePrepared( pfucb ) )
			{
			const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
			const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );
			Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
			Call( ErrSLVOwnerMapSetUsageRange(
						pfucb->ppib,
						pfucb->ifmp,
						pgnoFirst,
						cpg,
						pfucb->u.pfcb->ObjidFDP(),
						columnid,
						&pfucb->bmCurr,
						fSLVOWNERMAPSetSLVFromData ) );
			}
		else
			{
			Assert ( FFUCBInsertPrepared( pfucb ) );
			FUCBSetSLVOwnerMapNeedUpdate( pfucb );
			}
		

		header.cRun++;
		header.fDataRecoverable = fDataRecoverable;

		CallS( slvinfo.ErrSetHeader( header ) );
		}

	//  move to the last run in this SLV

	CallS( slvinfo.ErrMoveAfterLast() );
	CallS( slvinfo.ErrMovePrev() );

	//  append the given data to the SLV

	dataAppend = *pdata;

	while ( dataAppend.Cb() )
		{
		//  there is space available in the current run for appending data

		CSLVInfo::RUN run;
		Call( slvinfo.ErrGetCurrentRun( &run ) );
		
		if ( (QWORD) run.ibVirtualNext > (QWORD) header.cbSize )
			{
			//  write as many bytes as we can into this run

			QWORD cbWritten;
			Call( ErrSLVWriteRun(	pfucb,
									pfucb->ppib,
									pfucb->ifmp,
									run,
									header.cbSize,
									&dataAppend,
									&cbWritten,
									fDataRecoverable ) );

			//  adjust header and data to reflect bytes written

			header.cbSize += cbWritten;
			CallS( slvinfo.ErrSetHeader( header ) );

			dataAppend.DeltaCb( INT( -cbWritten ) );
			dataAppend.DeltaPv( INT_PTR( cbWritten ) );
			}

		//  there is still data to be appended to the SLV

		if ( dataAppend.Cb() )
			{
			//  try to allocate a chunk of space large enough to hold this data

			QWORD ibLogical;
			QWORD cbSize;
			Call( ErrSLVGetRange(	pfucb->ppib,
									pfucb->ifmp,
									dataAppend.Cb(),
									&ibLogical,
									&cbSize,
									fFalse,
									fFalse ) );

			//  this space can be appended to the current run

			if ( ibLogical == run.ibLogicalNext )
				{
				//  increase the ibVirtualNext of this run to reflect the newly
				//  allocated space

				run.ibVirtualNext	+= cbSize;
				run.cbSize			+= cbSize;
				run.ibLogicalNext	+= cbSize;
				}

			//  this space cannot be appended to the current run

			else
				{
				//  remember the current run's ibVirtualNext for the new run's
				//  ibVirtual

				const QWORD ibVirtual = run.ibVirtualNext;

				//  move to after the last run to append a new run

				CallS( slvinfo.ErrMoveAfterLast() );

				//  set the run to include this data

				run.ibVirtualNext	= ibVirtual + cbSize;
				run.ibLogical		= ibLogical;
				run.qwReserved		= 0;
				run.ibVirtual		= ibVirtual;
				run.cbSize			= cbSize;
				run.ibLogicalNext	= ibLogical + cbSize;

				//  add a run to the header

				header.cRun++;
				CallS( slvinfo.ErrSetHeader( header ) );
				}

			// mark the space usage in SLVOWNERMAP on update operations
			if ( FFUCBReplacePrepared( pfucb ) )
				{
				const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
				const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );
				Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
				Call( ErrSLVOwnerMapSetUsageRange(
							pfucb->ppib,
							pfucb->ifmp,
							pgnoFirst,
							cpg,
							pfucb->u.pfcb->ObjidFDP(),
							columnid,
							&pfucb->bmCurr,
							fSLVOWNERMAPSetSLVFromData ) );
				}
			else
				{
				Assert ( FFUCBInsertPrepared( pfucb ) );
				FUCBSetSLVOwnerMapNeedUpdate( pfucb );
				}

			//  save our changes to this run

			Call( slvinfo.ErrSetCurrentRun( run ) );
			}
		}

	//  save our changes to the SLVInfo for this SLV

	Call( slvinfo.ErrSave() );

HandleError:
	slvinfo.Unload();
	return err;
	}


// ErrSLVSetColumn 				- set a SLV column
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		ibOffset				- offset
//		grbit					- grbit
//		pdata					- source buffer
//
// RESULT:						ERR

ERR ErrSLVSetColumn(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const ULONG		ibOffset,
	ULONG			grbit,
	DATA			* pdata )
	{
	ERR				err		= JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( pfucb );
	ASSERT_VALID( pfucb->ppib );
	if ( !pfucb->ppib->level )
		{
		CallR( ErrERRCheck( JET_errNotInTransaction ) );
		}

	if ( FFUCBReplaceNoLockPrepared( pfucb ) )
		{
		CallR( ErrRECUpgradeReplaceNoLock( pfucb ) );
		}
		
	Assert( FCOLUMNIDTagged( columnid ) );
	if ( ibOffset )
		{
		CallR( ErrERRCheck( JET_errInvalidParameter ) );
		}
	Assert( pdata );
	ASSERT_VALID( pdata );

	//  normalize the sequence number we are going to use to set the field so
	//  that if a non-existent sequence number was specified we use the next
	//  available sequence number

	TAGFIELDS	tagfields( pfucb->dataWorkBuf );
	FCB			* const pfcb		= pfucb->u.pfcb;
	const ULONG	itagSequenceMost	= tagfields.UlColumnInstances(
											pfcb,
											columnid,
											FRECUseDerivedBit( columnid, pfcb->Ptdb() ) );
	ULONG itagSequenceSet			= itagSequence && itagSequence <= itagSequenceMost ?
											itagSequence :
											itagSequenceMost + 1;

	//  get parameters for setting the SLV

	const BOOL	fExisting	= ( itagSequenceSet <= itagSequenceMost );

	if ( !fExisting )
		{
		//	strip off AppendLV flag if SLV doesn't exist
		grbit &= ~( grbit & JET_bitSetAppendLV );
		}
		
	const BOOL	fAppend		= ( grbit & JET_bitSetAppendLV );

	//  perform the set column in a transaction to allow rollback on an error
	
	CallR( ErrDIRBeginTransaction( pfucb->ppib, NO_GRBIT ) );

	//  we are replacing an existing SLV

	if ( fExisting && !fAppend )
		{
		//  decommit any space owned by the existing SLV

		Call( ErrSLVDelete( pfucb, columnid, itagSequenceSet, fTrue ) );

		//  delete all SLVInfo for this field
		
		DATA dataNull;
		dataNull.Nullify();
		Call( ErrRECSetLongField(	pfucb,
									columnid,
									itagSequenceSet,
									&dataNull,
									NO_GRBIT,
									0,
									0 ) );
		}

	//  we have data to set for this SLV or we are appending

	if ( pdata->Cb() || fAppend )
		{
		//  we are being asked to set the SLV from an EA list

		if ( grbit & JET_bitSetSLVFromSLVEA )
			{
			//  the only allowed grbits are JET_bitSetSLVDataNotRecoverable and
			//  JET_bitSetSLVFromSLVEA and the SLV Provider must be enabled

			if (	( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
									JET_bitSetSLVFromSLVEA ) ) ||
					!PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
				{
				Call( ErrERRCheck( JET_errInvalidGrbit ) );
				}

			//  set the SLV from an EA list

			Call( ErrSLVSetColumnFromEA(	pfucb,
											columnid,
											itagSequenceSet,
											pdata,
											grbit ) );
			}

		//  we are being asked to set the SLV from a file handle

		else if ( grbit & JET_bitSetSLVFromSLVFile )
			{
			//  the only allowed grbits are JET_bitSetSLVDataNotRecoverable and
			//  JET_bitSetSLVFromSLVFile and the SLV Provider must be enabled

			if (	( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
									JET_bitSetSLVFromSLVFile ) ) ||
					!PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
				{
				Call( ErrERRCheck( JET_errInvalidGrbit ) );
				}
			
			//  set the SLV from a file handle

			Call( ErrSLVSetColumnFromFile(	pfucb,
											columnid,
											itagSequenceSet,
											pdata,
											grbit ) );
			}

		//  we are being asked to return the actual SLV data

		else
			{
			//  the only allowed grbits are JET_bitSetSLVDataNotRecoverable and
			//  JET_bitSetAppendLV and the SLV Provider must be disabled

			if (	( grbit & ~(	JET_bitSetSLVDataNotRecoverable |
									JET_bitSetAppendLV ) ) ||
					PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
				{
				Call( ErrERRCheck( JET_errInvalidGrbit ) );
				}

			//  set the SLV from actual data

			Call( ErrSLVSetColumnFromData(	pfucb,
											columnid,
											itagSequenceSet,
											pdata,
											grbit ) );
			}
		}

	//  commit on success

	Call( ErrDIRCommitTransaction( pfucb->ppib, NO_GRBIT ) );

	return JET_errSuccess;

	//  rollback on an error

HandleError:
	CallSx( ErrDIRRollback( pfucb->ppib ), JET_errRollbackError );
	return err;
	}


//  ================================================================
LOCAL ERR ErrSLVPrereadRun(	PIB const *	ppib,
							const IFMP ifmpDb,
							const CSLVInfo::RUN& run,
							const QWORD ibVirtualStart,
							const QWORD cbData )
//  ================================================================
//
//  Preread the pages in the run. All pages will be preread in 64K chunks
//
//-
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( ppib );
	FMP::AssertVALIDIFMP( ifmpDb );
	Assert( rgfmp[ ifmpDb ].FSLVAttached() );
	Assert( run.ibVirtual <= ibVirtualStart );
	Assert( ibVirtualStart < run.ibVirtualNext );

	const IFMP	ifmp	= ifmpDb | ifmpSLV;

	QWORD ibData;
	QWORD ibLogical;
	ibData		= 0;
	ibLogical	= run.ibLogical + ibVirtualStart - run.ibVirtual;

	const INT 	cpgnoMax 	= 16;	//	64K I/Os
	INT			ipgno 		= 0;
	PGNO		rgpgno[cpgnoMax+1];	//	NULL terminated
	
	//  preread pages in run until we have run out of data or read as much as we want

	while ( ibData < cbData
			&& ibLogical < run.ibLogicalNext )
		{
		//  determine the pgno / ib / cb where we will read data

		const PGNO	pgno	= PgnoOfOffset( ibLogical );
		const QWORD	ib		= ibLogical % SLVPAGE_SIZE;
		const QWORD	cb		= min( cbData - ibData, SLVPAGE_SIZE - ib );

		rgpgno[ipgno++] = pgno;

		if( cpgnoMax == ipgno )
			{
			rgpgno[ipgno] = pgnoNull;
			BFPrereadPageList( ifmp, rgpgno );

			ipgno = 0;
			}

		
		//  advance copy pointers

		ibData		+= cb;
		ibLogical	+= cb;
		}

	rgpgno[ipgno] = pgnoNull;
	BFPrereadPageList( ifmp, rgpgno );
	
	return err;
	}


// ErrSLVReadRun				- reads SLV Data from an SLV Run
//
// IN:
//		ppib					- session
//      ifmpDb					- database ifmp
//		run						- SLV Run
//		ibOffset				- offset to start reading
//		pdata					- destination buffer
//		pcbRead					- ULONG where to return the read data size
//
// RESULT:						ERR
//
// OUT:	
//		pdata					- SLV Data
//		pcbRead					- SLV Data bytes read

LOCAL ERR ErrSLVReadRun(	PIB*			ppib,
							IFMP			ifmpDb,
							CSLVInfo::RUN&	run,
							QWORD			ibVirtualStart,
							DATA*			pdata,
							QWORD*			pcbRead )
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( ppib );
	FMP::AssertVALIDIFMP( ifmpDb );
	Assert( rgfmp[ ifmpDb ].FSLVAttached() );
	Assert( run.ibVirtual <= ibVirtualStart );
	Assert( ibVirtualStart < run.ibVirtualNext );
	Assert( pdata );
	ASSERT_VALID( pdata );
	Assert( pcbRead );

	//  read data from pages in run until we have run out of data or buffer

	QWORD ibData;
	QWORD ibLogical;
	ibData		= 0;
	ibLogical	= run.ibLogical + ibVirtualStart - run.ibVirtual;
	
	while ( ibData < pdata->Cb() && ibLogical < run.ibLogicalNext )
		{
		//  determine the ifmp / pgno / ib / cb where we will read data

		IFMP	ifmp	= ifmpDb | ifmpSLV;
		PGNO	pgno	= PgnoOfOffset( ibLogical );
		QWORD	ib		= ibLogical % SLVPAGE_SIZE;
		QWORD	cb		= min( pdata->Cb() - ibData, SLVPAGE_SIZE - ib );
		
		//  latch the current ifmp / pgno

		BFLatch bfl;
		Call( ErrBFReadLatchPage( &bfl, ifmp, pgno ) );

		//  copy the desired data from the page into the buffer

		UtilMemCpy( (BYTE*)pdata->Pv() + ibData, (BYTE*)bfl.pv + ib, size_t( cb ) );

		//  unlatch the current ifmp / pgno

		BFReadUnlatch( &bfl );

		//  advance copy pointers

		ibData		+= cb;
		ibLogical	+= cb;
		}

	// we expect only this warning from BF and it must not be propagated
	Assert(	JET_errSuccess == err ||
			wrnBFPageFault == err ||
			wrnBFPageFlushPending == err );
	err = JET_errSuccess;
	
	//  set read bytes for return

	*pcbRead = ibData;

HandleError:
	return err;
	}


// ErrSLVRetrieveColumnAsData	- retrieve a SLV column as SLV Data
//
// IN:
//		ppib					- session
//      ifmpDb					- database ifmp
//		slvinfo					- SLV Info
//		ibOffset				- offset to start reading
//		pdata					- destination buffer
//		pcbActual				- ULONG where to return the data size
//
// RESULT:						ERR
//
// OUT:	
//		pdata					- SLV Data
//		pcbActual				- SLV Data bytes remaining after ibOffset
//
	
LOCAL ERR ErrSLVRetrieveColumnAsData(	PIB*		ppib,
										IFMP		ifmpDb,
										CSLVInfo&	slvinfo,
										ULONG		ibOffset,
										DATA*		pdata,
										ULONG*		pcbActual )
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( ppib );
	Assert( !PinstFromIfmp( ifmpDb )->FSLVProviderEnabled() );
	Assert( pdata );
	ASSERT_VALID( pdata );
	
	//  get the header for this SLV

	CSLVInfo::HEADER header;
	CallS( slvinfo.ErrGetHeader( &header ) );

	//  we are being asked to read beyond the end of the SLV

	if ( ibOffset >= header.cbSize )
		{
		//  don't read data and return 0 bytes read

		if ( pcbActual )
			{
			*pcbActual = 0;
			}
		}

	//  we are not being asked to read beyond the end of the SLV

	else
		{
		//  the provided buffer is not zero-length (fast path getting the SLV size)

		if ( pdata->Cb() )
			{
			//  seek to the run before the run that contains the requested offset

			Call( slvinfo.ErrSeek( ibOffset ) );
			CallSx( slvinfo.ErrMovePrev(), JET_errNoCurrentRecord );
			
			//  retrieve data from the SLV until we run out of data or buffer
			
			QWORD ibVirtual	= ibOffset;
			QWORD ibData	= 0;

			while (	ibData < pdata->Cb() &&
					( err = slvinfo.ErrMoveNext() ) >= JET_errSuccess )
				{
				//  get the current run

				CSLVInfo::RUN run;
				Call( slvinfo.ErrGetCurrentRun( &run ) );
				
				//  read data from this run into the SLV

				DATA dataRun;
				dataRun.SetPv( (BYTE*)pdata->Pv() + ibData );
				dataRun.SetCb( ULONG( pdata->Cb() - ibData ) );

				Call( ErrSLVPrereadRun(	ppib,
										ifmpDb,
										run,
										ibVirtual,
										dataRun.Cb() ) );
				
				QWORD cbRead;
				Call( ErrSLVReadRun(	ppib,
										ifmpDb,
										run,
										ibVirtual,
										&dataRun,
										&cbRead ) );
				ibData 		+= cbRead;
				ibVirtual 	+= cbRead;
				}
			if ( err == JET_errNoCurrentRecord )
				{
				err = JET_errSuccess;
				}
			}

		//  return the amount of data in the SLV after the given offset

		if ( pcbActual )
			{
			*pcbActual = ULONG( header.cbSize - ibOffset );
			}
		if ( err >= JET_errSuccess && header.cbSize - ibOffset > pdata->Cb() )
			{
			err = JET_wrnBufferTruncated;
			}
		}

HandleError:
	return err;
	}

// ErrSLVRetrieveColumn			- retrieve a SLV column
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//		ibOffset				- offset to start reading
//		grbit					- grbit
//		dataSLVInfo				- field from record
//		pdata					- destination buffer
//		pcbActual				- ULONG where to return the data size
//		pfLatched				- BOOL where to return record latch status 
//
// RESULT:						ERR
//
// OUT:	
//		pdata					- retrieved data
//		pcbActual				- SLV bytes remaining after ibOffset
//		pfLatched				- record latch status

ERR ErrSLVRetrieveColumn(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const BOOL		fSeparatedSLV,
	const ULONG		ibOffset,
	ULONG			grbit,
	DATA&			dataSLVInfo,
	DATA			* pdata,
	ULONG			* pcbActual )
	{
	ERR 			err			= JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( pfucb );
	ASSERT_VALID( &dataSLVInfo );
	Assert( pdata );
	ASSERT_VALID( pdata );

	//  strip off copy buffer flag if present

	const BOOL	fCopyBuffer = ( grbit & JET_bitRetrieveCopy ) && FFUCBUpdatePrepared( pfucb );
	grbit = grbit & ~JET_bitRetrieveCopy;

	//  ignore JET_bitRetrieveNull as default values are not supported for SLVs

	grbit = grbit & ~JET_bitRetrieveNull;

	//  get the SLV Info for this SLV
	
	CSLVInfo slvinfo;
	Call( slvinfo.ErrLoad( pfucb, columnid, itagSequence, fCopyBuffer, &dataSLVInfo, fSeparatedSLV ) );

// OwnerMap checking on SLV retrieve will fail
// if OLDSLV is running. Just disabled until a proper 
// check is found or ... 
#ifdef NEVER 

#ifdef DEBUG
	// the space map is not updated if the retrieve column is done
	// from inside an on going transaction and the record is prep insert
	// so don't check in this case
	if ( !FFUCBSLVOwnerMapNeedUpdate( pfucb ) )
		{
		ERR errT = slvinfo.ErrMoveBeforeFirst();
		
		if ( JET_errSuccess <= errT )
			errT = slvinfo.ErrMoveNext();

		while ( JET_errSuccess <= errT )
			{
			CSLVInfo::RUN run;
			
			Call( slvinfo.ErrGetCurrentRun( &run ) );

			const CPG	cpg			= ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE;
			const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );

			Assert ( pfucb->u.pfcb->FPrimaryIndex() && pfucb->u.pfcb->FTypeTable() );
			
			Call( ErrSLVOwnerMapCheckUsageRange(
						pfucb->ppib,
						pfucb->ifmp,
						pgnoFirst,
						cpg,
						pfucb->u.pfcb->ObjidFDP(),
						columnid,
						&pfucb->bmCurr));
			
			errT = slvinfo.ErrMoveNext();
			}
		}
	
#endif // DEBUG
#endif // NEVER

	//  we are being asked to return an EA list

	if ( grbit & JET_bitRetrieveSLVAsSLVEA )
		{
		//  no other grbits may be selected and the SLV Provider must be enabled

		if (	grbit != JET_bitRetrieveSLVAsSLVEA ||
				!PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
			{
			Call( ErrERRCheck( JET_errInvalidGrbit ) );
			}

		//  convert the SLV Info into an EA List

		BTUp( pfucb );

		Call( ErrOSSLVFileConvertSLVInfoToEA(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
												slvinfo,
												ibOffset,
												pdata->Pv(),
												pdata->Cb(),
												pcbActual ) );
		}

	//  we are being asked to return a file handle

	else if ( grbit & JET_bitRetrieveSLVAsSLVFile )
		{
		//  no other grbits may be selected and the SLV Provider must be enabled

		if (	grbit != JET_bitRetrieveSLVAsSLVFile ||
				!PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
			{
			Call( ErrERRCheck( JET_errInvalidGrbit ) );
			}

		//  convert the SLV Info into an SLV File

		BTUp( pfucb );

		Call( ErrOSSLVFileConvertSLVInfoToFile(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
												slvinfo,
												ibOffset,
												pdata->Pv(),
												pdata->Cb(),
												pcbActual ) );
		}

	//  we are being asked to return the actual SLV data

	else
		{
		//  no other grbits may be selected and the SLV Provider must be disabled

		if (	grbit ||
				PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
			{
			Call( ErrERRCheck( JET_errInvalidGrbit ) );
			}

		//  return the actual data

		Call( ErrSLVRetrieveColumnAsData(	pfucb->ppib,
											pfucb->ifmp,
											slvinfo,
											ibOffset,
											pdata,
											pcbActual ) );
		}

HandleError:
	slvinfo.Unload();
	return err;
	}

// ErrSLVCopyUsingData			- copies an SLV from the given source field to
//								  the copy buffer containing the given destination
//								  field.  the destination must not already exist.
//								  the SLV is copied via the Buffer Manager
//
// IN:
//		pfucbSrc				- source cursor
//		columnidSrc				- source field ID
//		itagSequenceSrc			- source sequence
//		pfucbDest				- destination cursor
//		columnidDest			- destination field ID
//		itagSequenceDest		- destination sequence
//
// RESULT:						ERR

LOCAL ERR ErrSLVCopyUsingData(
	FUCB			 * pfucbSrc,
	const COLUMNID	columnidSrc,
	const ULONG		itagSequenceSrc,
	FUCB			* pfucbDest, 
	COLUMNID		columnidDest,
	ULONG			itagSequenceDest )
	{
	ERR				err				= JET_errSuccess;
	CSLVInfo		slvinfoSrc;
	CSLVInfo		slvinfoDest;
	void*			pvTempBuffer	= NULL;
	DATA			dataTemp;
	
	CSLVInfo::RUN runSrc;
	CSLVInfo::RUN runDest;
	CSLVInfo::HEADER headerSrc;
	CSLVInfo::HEADER headerDest;
	
	//  validate IN args

	ASSERT_VALID( pfucbSrc );
	Assert( columnidSrc > 0 );
	Assert( itagSequenceSrc > 0 );

	ASSERT_VALID( pfucbDest );
	Assert( columnidDest > 0 );
	Assert( itagSequenceDest > 0 );
	
	//  load the source and destination SLVs

	Call( slvinfoSrc.ErrLoad( pfucbSrc, columnidSrc, itagSequenceSrc, fFalse ) );

	Call( slvinfoDest.ErrCreate( pfucbDest, columnidDest, itagSequenceDest, fTrue ) );

	//  get the header for the source and destination SLVs

	CallS( slvinfoSrc.ErrGetHeader( &headerSrc ) );

	CallS( slvinfoDest.ErrGetHeader( &headerDest ) );

	//  the destination SLV had better be empty

	Assert( !headerDest.cbSize );
	Assert( !headerDest.cRun );

	//  allocate an initial run for the destination SLV.  we do this even if
	//  there is no data to copy because all SLVs must have at least one run.
	//  we also set the data recoverability state for the destination SLV to
	//  be the same as for the source SLV

	QWORD ibLogical;
	QWORD cbSize;
	Call( ErrSLVGetRange(	pfucbDest->ppib,
							pfucbDest->ifmp,
							max( (ULONG) headerSrc.cbSize, 1 ),
							&ibLogical,
							&cbSize,
							fFalse,
							fFalse ) );
							
	runDest.ibVirtualNext	= cbSize;
	runDest.ibLogical		= ibLogical;
	runDest.qwReserved		= 0;
	runDest.ibVirtual		= 0;
	runDest.cbSize			= cbSize;
	runDest.ibLogicalNext	= ibLogical + cbSize;

	CallS( slvinfoDest.ErrMoveAfterLast() );
	Call( slvinfoDest.ErrSetCurrentRun( runDest ) );

	headerDest.cRun++;
	headerDest.fDataRecoverable = headerSrc.fDataRecoverable;

	CallS( slvinfoDest.ErrSetHeader( headerDest ) );
	
	//  copy all data from the source SLV to the destination SLV

	QWORD ibVirtual;
	ibVirtual = 0;
	
	memset( &runSrc, 0, sizeof( runSrc ) );
	
	BFAlloc( &pvTempBuffer );
	dataTemp.SetPv( pvTempBuffer );
	dataTemp.SetCb( g_cbPage );

	while ( ibVirtual < headerSrc.cbSize )
		{
		//  we need to retrieve another run from the source SLV

		Assert( ibVirtual <= runSrc.ibVirtualNext );
		if ( ibVirtual == runSrc.ibVirtualNext )
			{
			//  retrieve the next run from the source SLV

			Call( slvinfoSrc.ErrMoveNext() );
			Call( slvinfoSrc.ErrGetCurrentRun( &runSrc ) );
			}

		//  we need to allocate more space for the destination SLV

		Assert( ibVirtual <= runDest.ibVirtualNext );
		if ( ibVirtual == runDest.ibVirtualNext )
			{
			//  try to allocate a chunk of space large enough to hold this data

			Call( ErrSLVGetRange(	pfucbDest->ppib,
									pfucbDest->ifmp,
									headerSrc.cbSize - ibVirtual,
									&ibLogical,
									&cbSize,
									fFalse,
									fFalse ) );

			//  this space can be appended to the current run

			if ( headerDest.cRun && ibLogical == runDest.ibLogicalNext )
				{
				//  change the run to reflect the newly allocated space

				runDest.ibVirtualNext	+= cbSize;
				runDest.cbSize			+= cbSize;
				runDest.ibLogicalNext	+= cbSize;
				}

			//  this space cannot be appended to the current run

			else
				{
				//  move to after the last run to append a new run

				CallS( slvinfoDest.ErrMoveAfterLast() );

				//  set the run to include this data

				runDest.ibVirtualNext	= ibVirtual + cbSize;
				runDest.ibLogical		= ibLogical;
				runDest.qwReserved		= 0;
				runDest.ibVirtual		= ibVirtual;
				runDest.cbSize			= cbSize;
				runDest.ibLogicalNext	= ibLogical + cbSize;

				//  add a run to the header

				headerDest.cRun++;
				CallS( slvinfoDest.ErrSetHeader( headerDest ) );
				}

			//  save our changes to this run

			Call( slvinfoDest.ErrSetCurrentRun( runDest ) );
			}

		//  make sure that we don't read past the end of the valid data region

		dataTemp.SetCb( ULONG( min( dataTemp.Cb(), headerSrc.cbSize - ibVirtual ) ) );

		//  read data from the source SLV

		Call( ErrSLVPrereadRun(	pfucbSrc->ppib,
								pfucbSrc->ifmp,
								runSrc,
								ibVirtual,
								dataTemp.Cb() ) );

		QWORD cbRead;
		Call( ErrSLVReadRun(	pfucbSrc->ppib,
								pfucbSrc->ifmp,
								runSrc,
								ibVirtual,
								&dataTemp,
								&cbRead ) );
		Assert( cbRead == dataTemp.Cb() );

		//  write data to the destination SLV, logging the data if the source
		//  SLV's data is recoverable

		QWORD cbWritten;
		Call( ErrSLVWriteRun(	pfucbDest,
								pfucbDest->ppib,
								pfucbDest->ifmp,
								runDest,
								ibVirtual,
								&dataTemp,
								&cbWritten,
								headerDest.fDataRecoverable ) );
		Assert( cbWritten == dataTemp.Cb() );

		//  adjust header to reflect bytes copied

		headerDest.cbSize = headerDest.cbSize + dataTemp.Cb();
		CallS( slvinfoDest.ErrSetHeader( headerDest ) );

		//  advance our copy pointer

		ibVirtual += dataTemp.Cb();
		}

	if ( FFUCBReplacePrepared( pfucbDest ) )
		{
		ERR errT = slvinfoDest.ErrMoveBeforeFirst();
				
		Assert ( JET_errSuccess == errT );
		
		errT = slvinfoDest.ErrMoveNext();
		while ( JET_errSuccess <= errT )
			{
			CSLVInfo::RUN run;
		
			Call( slvinfoDest.ErrGetCurrentRun( &run ) );

			const CPG	cpg			= CPG( ( run.cbSize + SLVPAGE_SIZE - 1 ) / SLVPAGE_SIZE );
			const PGNO	pgnoFirst	= PgnoOfOffset( run.ibLogical );

			Assert ( pfucbDest->u.pfcb->FPrimaryIndex() && pfucbDest->u.pfcb->FTypeTable() );
			Call( ErrSLVOwnerMapSetUsageRange(
						pfucbDest->ppib,
						pfucbDest->ifmp,
						pgnoFirst,
						cpg,
						pfucbDest->u.pfcb->ObjidFDP(),
						columnidDest,
						&pfucbDest->bmCurr,
						fSLVOWNERMAPSetSLVCopyData ) );

			errT = slvinfoDest.ErrMoveNext();
			}
		Assert ( JET_errNoCurrentRecord == errT );

		}
	else
		{
		Assert ( FFUCBInsertPrepared( pfucbDest ) );
		FUCBSetSLVOwnerMapNeedUpdate( pfucbDest );
		}		

	//  save our changes to the destination SLV

	Call( slvinfoDest.ErrSave() );

HandleError:
	if ( pvTempBuffer )
		{
		BFFree( pvTempBuffer );
		}
	slvinfoDest.Unload();
	slvinfoSrc.Unload();
	return err;
	}

// ErrSLVCopyUsingFiles			- copies an SLV from the field in the record to
//								  the same field in the record in the copy buffer.
//								  that SLV must not already exist.  the SLV is
//								  copied via the SLV Provider
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//
// RESULT:						ERR

LOCAL ERR ErrSLVCopyUsingFiles(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence )
	{
	ERR				err			= JET_errSuccess;
	CSLVInfo		slvinfoSrc;
	IFileAPI*		pfapiSrc	= NULL;
	
	CSLVInfo::HEADER headerSrc;
	
	//  validate IN args

	ASSERT_VALID( pfucb );


	//  open the source SLV File and read its header.  use a temporary file
	//  name so that we are sure we copy the original data and not whatever
	//  the file currently contains

	Call( slvinfoSrc.ErrLoad( pfucb, columnid, itagSequence, fFalse ) );

	wchar_t wszFileName[ IFileSystemAPI::cchPathMax ];
	swprintf( wszFileName, L"$CopySrc%016I64X$", rgfmp[ pfucb->ifmp ].DbtimeIncrementAndGet() );
	Call( slvinfoSrc.ErrSetFileNameVolatile() );
	Call( slvinfoSrc.ErrSetFileName( wszFileName ) );

	Call( ErrOSSLVFileOpen(	rgfmp[ pfucb->ifmp ].SlvrootSLV(),
							slvinfoSrc,
							&pfapiSrc,
							fTrue,
							fTrue ) );

	CallS( slvinfoSrc.ErrGetHeader( &headerSrc ) );

	//  copy the source SLV File into a new destination SLV File at the same
	//  field in the copy buffer with the same data recoverability as the source

	Call( ErrSLVCopyFile(	slvinfoSrc,
							pfapiSrc,
							headerSrc.cbSize,
							headerSrc.fDataRecoverable,
							pfucb,
							columnid,
							itagSequence ) );
 
HandleError:
	delete pfapiSrc;
	slvinfoSrc.Unload();
	return err;
	}

// ErrSLVCopy					- copies an SLV from the field in the record to
//								  the same field in the record in the copy buffer.
//								  that SLV must not already exist
//
// IN:
//		pfucb					- cursor 
//		columnid				- field ID
//		itagSequence			- sequence
//
// RESULT:						ERR

ERR ErrSLVCopy(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence )
	{
	ERR				err			= JET_errSuccess;
	
	//  validate IN args

	ASSERT_VALID( pfucb );

	//  the SLV Provider is enabled

	if ( PinstFromPfucb( pfucb )->FSLVProviderEnabled() )
		{
		//  copy the SLV using the SLV Provider

		Call( ErrSLVCopyUsingFiles( pfucb, columnid, itagSequence ) );
		}

	//  the SLV Provider is not enabled

	else
		{
		//  copy the SLV using data
		Call( ErrSLVCopyUsingData(	pfucb,
									columnid,
									itagSequence,
									pfucb,
									columnid,
									itagSequence ) );
		}

HandleError:
	return err;
	}


// ErrRECCopySLVsInRecord		- given a record and a copy of that record with
//								  all SLVs removed, this function will copy the
//								  SLVs from the record to the record in the copy
//								  buffer.  the effect of this is to make a new
//								  instance of each SLV for the record in the copy
//								  buffer.  this call is intended for use by
//								  JET_prepInsertCopy
//
// IN:
//		pfucb					- cursor pointing at a record to be copied
//
// RESULT:						ERR

ERR ErrRECCopySLVsInRecord( FUCB *pfucb )
	{
	ERR		err;
	
	//  validate IN args
	
	ASSERT_VALID( pfucb );
	ASSERT_VALID( pfucb->ppib );
	if ( !pfucb->ppib->level )
		{
		CallR( ErrERRCheck( JET_errNotInTransaction ) );
		}
	AssertDIRNoLatch( pfucb->ppib );
	Assert( !pfucb->kdfCurr.data.FNull() );
	Assert( pfcbNil != pfucb->u.pfcb );


	//  perform the copies in a transaction to allow rollback on an error
	
	CallR( ErrDIRBeginTransaction( pfucb->ppib, NO_GRBIT ) );

	//  get the record to copy

	Call( ErrDIRGet( pfucb ) );

	//  copy all SLVs from original record to the record in the copy buffer
	//	(all the SLVs in the copy buffer should already have been removed
	//	by ErrRECAffectLongFieldsInWorkBuf())
	{
	TAGFIELDS	tagfields( pfucb->kdfCurr.data );
	Call( tagfields.ErrCopySLVColumns( pfucb ) );
	}
	
	//  release our latch

	if ( Pcsr( pfucb )->FLatched() )
		{
		CallS( ErrDIRRelease( pfucb ) )
		}
	
	//  commit on success

	Call( ErrDIRCommitTransaction( pfucb->ppib, NO_GRBIT ) );

	return JET_errSuccess;

	//  rollback on an error

HandleError:
	if ( Pcsr( pfucb )->FLatched() )
		{
		CallS( ErrDIRRelease( pfucb ) );
		}

	CallSx( ErrDIRRollback( pfucb->ppib ), JET_errRollbackError );
	AssertDIRNoLatch( pfucb->ppib );	// Guaranteed not to fail while we have a latch.
	return err;
	}


// ErrLGRIRedoSLVPageAppend		- redoes a physical SLV append
//
// IN:
//		ppib					- session
//		plrSLVPageAppend		- log record detailing the physical operation
//
// RESULT:						ERR

ERR LOG::ErrLGRIRedoSLVPageAppend( PIB* ppib, LRSLVPAGEAPPEND* plrSLVPageAppend )
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	ASSERT_VALID( ppib );
	Assert( plrSLVPageAppend );
	Assert( plrSLVPageAppend->FDataLogged() );

	//  extract parameters from the log record

	INST*			pinst		= PinstFromPpib( ppib );
	DBID			dbid		= DBID( plrSLVPageAppend->dbid & dbidMask );
	IFMP			ifmp		= pinst->m_mpdbidifmp[ dbid ];
	QWORD			ibOffset	= plrSLVPageAppend->le_ibLogical % SLVPAGE_SIZE;

	CSLVInfo::RUN	run;
	run.ibVirtualNext	= SLVPAGE_SIZE;
	run.ibLogical		= plrSLVPageAppend->le_ibLogical - ibOffset;
	run.qwReserved		= 0;
	run.ibVirtual		= 0;
	run.cbSize			= SLVPAGE_SIZE;
	run.ibLogicalNext	= plrSLVPageAppend->le_ibLogical - ibOffset + SLVPAGE_SIZE;

	DATA			dataAppend;
	dataAppend.SetPv( (void*)plrSLVPageAppend->szData );
	dataAppend.SetCb( plrSLVPageAppend->le_cbData );

	//	UNDONE: if we ever support JET_bitSetSLVDataNotRecoverable, we'll need some
	//	smart mechanism to zero-out out-of-date data

	//  write the data to the SLV file

	QWORD cbWritten;
	Call( ErrSLVWriteRun(	pfucbNil,
							ppib,
							ifmp,
							run,
							ibOffset,
							&dataAppend,
							&cbWritten,
							fTrue ) );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrSLVPrereadOffset(	PIB * const ppib,
								const IFMP ifmpDb,
								const QWORD ibLogical,
								const QWORD	cbPreread
								)
//  ================================================================
	{
	const IFMP	ifmp	= ifmpDb | ifmpSLV;
	
	QWORD 			cbPrereadRemaining	= cbPreread;

	const INT		cpgnoMax		= 16;	//	64K I/O's
	PGNO			rgpgno[cpgnoMax+1];
	INT				ipgno			= 0;
	
	//  determine the pgno to read from

	PGNO		pgno				= PgnoOfOffset( ibLogical );
	
	while( cbPrereadRemaining > 0 )
		{
		const QWORD cbPrereadThisPage	= min( cbPrereadRemaining, g_cbPage );
		
		rgpgno[ipgno++] = pgno;

		if( cpgnoMax == ipgno )
			{
			rgpgno[ipgno] = pgnoNull;
			BFPrereadPageList( ifmp, rgpgno );

			ipgno = 0;
			}
		
		cbPrereadRemaining -= cbPrereadThisPage;			
		++pgno;
		}

	rgpgno[ipgno] = pgnoNull;
	BFPrereadPageList( ifmp, rgpgno );
		
	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrSLVChecksumOffset(	PIB * const ppib,
								const IFMP ifmpDb,
								const QWORD ibLogical,
								const QWORD	cb,
								DWORD * const pdwChecksum		
								)
//  ================================================================
	{
	ERR			err				= JET_errSuccess;

	const IFMP	ifmp	= ifmpDb | ifmpSLV;
	
	QWORD 			cbRemaining		= cb;
	
	//  determine the pgno to read from

	PGNO		pgno				= PgnoOfOffset( ibLogical );

	*pdwChecksum = 0;
	
	while( cbRemaining > 0 )
		{
		const QWORD cbThisPage	= min( cbRemaining, g_cbPage );

		BFLatch	bfl;
		Call( ErrBFReadLatchPage( &bfl, ifmp, pgno, bflfNoTouch ) );

		*pdwChecksum = LOG::UlChecksumBytes( (BYTE *)bfl.pv, (BYTE *)bfl.pv+cbThisPage, *pdwChecksum );

		BFReadUnlatch( &bfl );
							
		cbRemaining -= cbThisPage;			
		++pgno;
		}

HandleError:
	return err;
	}


//  SLV Information				- manages an LV containing an SLV scatter list

//  fileidNil

const CSLVInfo::FILEID CSLVInfo::fileidNil = -1;

//  constructor

CSLVInfo::CSLVInfo()
	{
	//  init object

	m_pvCache	= m_rgbSmallCache;
	m_cbCache	= sizeof( m_rgbSmallCache );
	}

//  destructor

CSLVInfo::~CSLVInfo()
	{
	//  we should not be holding resources

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );
	}

// ErrCreate					- creates SLV Information in the specified NULL
//								  record and field
// IN:
//		pfucb					- cursor
//      columnid				- field ID
//		itagSequence			- sequence
//		fCopyBuffer				- record is stored in the copy buffer
//
// RESULT:						ERR

ERR CSLVInfo::ErrCreate(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const BOOL		fCopyBuffer )
	{
	//  validate IN args

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );
	
	ASSERT_VALID( pfucb );
	Assert( FCOLUMNIDTagged( columnid ) );
	Assert( itagSequence );
	
	//  save our currency and field

	m_pfucb			= pfucb;
	m_columnid		= columnid;
	m_itagSequence	= itagSequence;
	m_fCopyBuffer	= fCopyBuffer;

	//  init our SLVInfo to appear to be a zero length SLV

	m_ibOffsetChunkMic			= 0;
	m_ibOffsetChunkMac			= sizeof( HEADER );
	m_ibOffsetRunMic			= sizeof( HEADER );
	m_ibOffsetRunMac			= sizeof( HEADER );
	m_ibOffsetRun				= m_ibOffsetRunMic - sizeof( _RUN );
	m_fCacheDirty				= fFalse;
	m_fHeaderDirty				= fTrue;
	m_header.cbSize				= 0;
	m_header.cRun				= 0;
	m_header.fDataRecoverable	= fFalse;
	m_header.rgbitReserved_31	= 0;
	m_header.rgbitReserved_32	= 0;
	m_fFileNameVolatile			= fFalse;
	m_fileid					= fileidNil;
	m_cbAlloc					= 0;
		
	return JET_errSuccess;
	}

// ErrLoad						- loads SLV Information from the specifed record
//								  and field
//
// IN:
//		pfucb					- cursor
//      columnid				- field ID
//		itagSequence			- sequence
//		fCopyBuffer				- record is stored in the copy buffer
//		pdataSLVInfo			- optional buffer containing raw SLV Info data
//
// RESULT:						ERR

ERR CSLVInfo::ErrLoad(
	FUCB			* pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const BOOL		fCopyBuffer,
	DATA			* pdataSLVInfo,
	const BOOL		fSeparatedSLV )
	{
	ERR				err			= JET_errSuccess;
	DWORD			cbSLVInfo	= 0;

	//  validate IN args

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );
	
	ASSERT_VALID( pfucb );
	Assert( FCOLUMNIDTagged( columnid ) );
	Assert( itagSequence );

	//  save our currency and field

	m_pfucb			= pfucb;
	m_columnid		= columnid;
	m_itagSequence	= itagSequence;
	m_fCopyBuffer	= fCopyBuffer;

	//  load the cache with as much data as possible

	if ( pdataSLVInfo )
		{
		//	if retrieving from copy buffer, should not have anything latched
		//	if not retrieving from copy buffer, should have page latched
		Assert( ( fCopyBuffer && !Pcsr( m_pfucb )->FLatched() )
			|| ( !fCopyBuffer && Pcsr( m_pfucb )->FLatched() ) );

		if ( fSeparatedSLV )
			{
			Call( ErrRECIRetrieveSeparatedLongValue(
						m_pfucb,
						*pdataSLVInfo,
						fTrue,
						0,
						(BYTE *)m_pvCache,
						m_cbCache,
						&cbSLVInfo,
						NO_GRBIT ) );
			Assert( !Pcsr( m_pfucb )->FLatched() );
			}
		else
			{
			UtilMemCpy(	m_pvCache,
						pdataSLVInfo->Pv(),
						min( m_cbCache, pdataSLVInfo->Cb() ) );
			cbSLVInfo = pdataSLVInfo->Cb();
			}
		}
	else
		{
		Call( ErrReadSLVInfo( 0, (BYTE*)m_pvCache, m_cbCache, &cbSLVInfo ) );
		}

	//  there is no SLVInfo for this field

	if ( !cbSLVInfo )
		{
		//  init our SLVInfo to appear to be a zero length SLV

		m_ibOffsetChunkMic			= 0;
		m_ibOffsetChunkMac			= sizeof( HEADER );
		m_ibOffsetRunMic			= sizeof( HEADER );
		m_ibOffsetRunMac			= sizeof( HEADER );
		m_ibOffsetRun				= m_ibOffsetRunMic - sizeof( _RUN );
		m_fCacheDirty				= fFalse;
		m_fHeaderDirty				= fTrue;
		m_header.cbSize				= 0;
		m_header.cRun				= 0;
		m_header.fDataRecoverable	= fFalse;
		m_header.rgbitReserved_31	= 0;
		m_header.rgbitReserved_32	= 0;
		m_fFileNameVolatile			= fFalse;
		m_fileid					= fileidNil;
		m_cbAlloc					= 0;
		}

	//  there is SLVInfo for this field

	else
		{
		//  retrieve the header from the SLVInfo

		UtilMemCpy( &m_header, m_pvCache, sizeof( HEADER ) );
		
		//  validate SLVInfo header

		if ( cbSLVInfo < sizeof( HEADER ) )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		if ( m_header.cbSize && !m_header.cRun )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		if ( m_header.rgbitReserved_31 || m_header.rgbitReserved_32 )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		DWORD cbSLVInfoBasic;
		cbSLVInfoBasic = DWORD( sizeof( HEADER ) + m_header.cRun * sizeof( _RUN ) );
		if ( cbSLVInfo < cbSLVInfoBasic )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		DWORD cbSLVInfoFileName;
		cbSLVInfoFileName = cbSLVInfo - cbSLVInfoBasic;
		if (	cbSLVInfoFileName % sizeof( wchar_t ) ||
				cbSLVInfoFileName / sizeof( wchar_t ) >= IFileSystemAPI::cchPathMax )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		//  init our SLVInfo from the retrieved data
		
		m_ibOffsetChunkMic	= 0;
		m_ibOffsetChunkMac	= min( m_cbCache, cbSLVInfo );
		m_ibOffsetRunMic	= DWORD( cbSLVInfo - m_header.cRun * sizeof( _RUN ) );
		m_ibOffsetRunMac	= cbSLVInfo;
		m_ibOffsetRun		= m_ibOffsetRunMic - sizeof( _RUN );
		m_fCacheDirty		= fFalse;
		m_fHeaderDirty		= fFalse;
		m_fFileNameVolatile	= fFalse;
		m_fileid			= fileidNil;
		m_cbAlloc			= 0;
		}

	return JET_errSuccess;

HandleError:
	Unload();
	return err;
	}


// ErrLoadFromData				- loads SLV Information from the specifed record
//								  and field
//
// IN:
//		pfucb					- cursor
//      fid						- field ID
//		itagSequence			- sequence
//		fCopyBuffer				- record is stored in the copy buffer
//		pdataSLVInfo			- optional buffer containing raw SLV Info data
//
// RESULT:						ERR

ERR CSLVInfo::ErrLoadFromData(
	FUCB		* const pfucb,
	const DATA&	dataSLVInfo,
	const BOOL	fSeparatedSLV )
	{
	ERR		err			= JET_errSuccess;
	DWORD	cbSLVInfo	= 0;

	//  validate IN args

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );

	//  save our currency and field

	m_pfucb			= pfucb;
	m_columnid		= 0;
	m_itagSequence	= 1;
	m_fCopyBuffer	= fFalse;

	//  load the cache with as much data as possible

	if ( fSeparatedSLV )
		{
		
		//	get the size
		
		Call( ErrRECIRetrieveSeparatedLongValue(
					m_pfucb,
					dataSLVInfo,
					fTrue,
					0,
					NULL,
					0,
					&cbSLVInfo,
					NO_GRBIT ) );

		//	

		VOID * const pvT = PvOSMemoryHeapAlloc( cbSLVInfo );

		if( NULL == pvT )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		m_pvCache = pvT;
		m_cbCache = cbSLVInfo;

		Call( ErrRECIRetrieveSeparatedLongValue(
					m_pfucb,
					dataSLVInfo,
					fTrue,
					0,
					(BYTE *)m_pvCache,
					m_cbCache,
					&cbSLVInfo,
					NO_GRBIT ) );
					
		}
	else
		{		
		//  An entire byte too big, but better safe than sorry!
		
		VOID * const pvT = PvOSMemoryHeapAlloc( dataSLVInfo.Cb() );

		if( NULL == pvT )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		m_pvCache = pvT;
		m_cbCache = dataSLVInfo.Cb();
	
		UtilMemCpy(
			m_pvCache,
			dataSLVInfo.Pv(),
			dataSLVInfo.Cb() );
		cbSLVInfo = dataSLVInfo.Cb();
		}

	//  there is no SLVInfo for this field

	if ( !cbSLVInfo )
		{
		//  init our SLVInfo to appear to be a zero length SLV

		m_ibOffsetChunkMic			= 0;
		m_ibOffsetChunkMac			= sizeof( HEADER );
		m_ibOffsetRunMic			= sizeof( HEADER );
		m_ibOffsetRunMac			= sizeof( HEADER );
		m_ibOffsetRun				= m_ibOffsetRunMic - sizeof( _RUN );
		m_fCacheDirty				= fFalse;
		m_fHeaderDirty				= fTrue;
		m_header.cbSize				= 0;
		m_header.cRun				= 0;
		m_header.fDataRecoverable	= fFalse;
		m_header.rgbitReserved_31	= 0;
		m_header.rgbitReserved_32	= 0;
		m_fFileNameVolatile			= fFalse;
		m_fileid					= fileidNil;
		m_cbAlloc					= 0;
		}

	//  there is SLVInfo for this field

	else
		{
		//  retrieve the header from the SLVInfo

		UtilMemCpy( &m_header, m_pvCache, sizeof( HEADER ) );
		
		//  validate SLVInfo header

		if ( cbSLVInfo < sizeof( HEADER ) )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		if ( m_header.cbSize && !m_header.cRun )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		if ( m_header.rgbitReserved_31 || m_header.rgbitReserved_32 )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		DWORD cbSLVInfoBasic;
		cbSLVInfoBasic = DWORD( sizeof( HEADER ) + m_header.cRun * sizeof( _RUN ) );
		if ( cbSLVInfo < cbSLVInfoBasic )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		DWORD cbSLVInfoFileName;
		cbSLVInfoFileName = cbSLVInfo - cbSLVInfoBasic;
		if (	cbSLVInfoFileName % sizeof( wchar_t ) ||
				cbSLVInfoFileName / sizeof( wchar_t ) >= IFileSystemAPI::cchPathMax )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		//  init our SLVInfo from the retrieved data
		
		m_ibOffsetChunkMic	= 0;
		m_ibOffsetChunkMac	= min( m_cbCache, cbSLVInfo );
		m_ibOffsetRunMic	= DWORD( cbSLVInfo - m_header.cRun * sizeof( _RUN ) );
		m_ibOffsetRunMac	= cbSLVInfo;
		m_ibOffsetRun		= m_ibOffsetRunMic - sizeof( _RUN );
		m_fCacheDirty		= fFalse;
		m_fHeaderDirty		= fFalse;
		m_fFileNameVolatile	= fFalse;
		m_fileid			= fileidNil;
		m_cbAlloc					= 0;
		}

	return JET_errSuccess;

HandleError:
	Unload();
	return err;
	}

// ErrCopy						- loads a duplicate of existing SLV Information
//
// IN:
//		slvinfo					- existing SLV Information
//
// RESULT:						ERR
//
// NOTE:  Double caching of persisted data is possible!  Use with caution!

ERR CSLVInfo::ErrCopy( CSLVInfo& slvinfo )
	{
	ERR		err			= JET_errSuccess;

	//  validate IN args

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );

	//  the source has a large cache

	if ( slvinfo.m_pvCache != slvinfo.m_rgbSmallCache )
		{
		//  get a large cache of our own

		if ( !( m_pvCache = PvOSMemoryHeapAlloc( slvinfo.m_cbCache ) ) )
			{
			m_pvCache = m_rgbSmallCache;
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		m_cbCache = slvinfo.m_cbCache;
		}

	//  copy the state of the source

	m_pfucb				= slvinfo.m_pfucb;
	m_columnid			= slvinfo.m_columnid;
	m_itagSequence		= slvinfo.m_itagSequence;
	m_fCopyBuffer		= slvinfo.m_fCopyBuffer;

	m_ibOffsetChunkMic	= slvinfo.m_ibOffsetChunkMic;
	m_ibOffsetChunkMac	= slvinfo.m_ibOffsetChunkMac;

	m_ibOffsetRunMic	= slvinfo.m_ibOffsetRunMic;
	m_ibOffsetRunMac	= slvinfo.m_ibOffsetRunMac;

	m_ibOffsetRun		= slvinfo.m_ibOffsetRun;

	m_fCacheDirty		= slvinfo.m_fCacheDirty;
	UtilMemCpy( m_pvCache, slvinfo.m_pvCache, slvinfo.m_ibOffsetRunMac );

	m_fHeaderDirty		= slvinfo.m_fHeaderDirty;
	m_header			= slvinfo.m_header;

	m_fFileNameVolatile	= slvinfo.m_fFileNameVolatile;
	if ( slvinfo.m_fFileNameVolatile )
		{
		wcscpy( m_wszFileName, slvinfo.m_wszFileName );
		}

	m_fileid			= slvinfo.m_fileid;
	m_cbAlloc			= slvinfo.m_cbAlloc;

	return JET_errSuccess;

HandleError:
	Unload();
	return err;
	}

// ErrSave						- saves any changes made
//
// RESULT:						ERR

ERR CSLVInfo::ErrSave()
	{
	ERR err = JET_errSuccess;

	//  save any changes we may have made to the SLVInfo cache

	Call( ErrFlushCache() );

	//  the header is still dirty

	if ( m_fHeaderDirty )
		{
		//  write the header to the SLVInfo

		Call( ErrWriteSLVInfo( 0, (BYTE*)&m_header, sizeof( HEADER ) ) );
		m_fHeaderDirty = fFalse;
		}

HandleError:
	return err;
	}

// ErrCreateVolatile			- creates a container for volatile SLV Information.
//								  this information cannot be saved and any changes
//								  that would cause an overflow of the SLVInfo cache
//								  will return JET_errDiskFull
//
// RESULT:						ERR

ERR CSLVInfo::ErrCreateVolatile()
	{
	//  validate IN args

	Assert( m_pvCache == m_rgbSmallCache );
	Assert( m_cbCache == sizeof( m_rgbSmallCache ) );
	
	//  set our currency and field to indicate volatile SLVInfo

	m_pfucb			= pfucbNil;
	m_columnid		= 0;
	m_itagSequence	= 0;
	m_fCopyBuffer	= fFalse;

	//  init our SLVInfo to appear to be a zero length SLV

	m_ibOffsetChunkMic			= 0;
	m_ibOffsetChunkMac			= sizeof( HEADER );
	m_ibOffsetRunMic			= sizeof( HEADER );
	m_ibOffsetRunMac			= sizeof( HEADER );
	m_ibOffsetRun				= m_ibOffsetRunMic - sizeof( _RUN );
	m_fCacheDirty				= fFalse;
	m_fHeaderDirty				= fTrue;
	m_header.cbSize				= 0;
	m_header.cRun				= 0;
	m_header.fDataRecoverable	= fFalse;
	m_header.rgbitReserved_31		= 0;
	m_header.rgbitReserved_32	= 0;
	m_fFileNameVolatile			= fTrue;
	m_wszFileName[0]			= L'\0';
	m_fileid					= fileidNil;
	m_cbAlloc					= 0;
		
	return JET_errSuccess;
	}

// Unload						- unloads all SLV Information throwing away any
//								  changes made
//
// RESULT:						ERR

void CSLVInfo::Unload()
	{
	//  blow cache if allocated

	if ( m_pvCache != m_rgbSmallCache )
		{
		OSMemoryHeapFree( m_pvCache );
		m_pvCache = m_rgbSmallCache;
		m_cbCache = sizeof( m_rgbSmallCache );
		}
	}

// ErrMoveBeforeFirst			- moves the run cursor before the first run in
//								  the scatter list
//
// RESULT:						ERR

ERR CSLVInfo::ErrMoveBeforeFirst()
	{
	//  reset currency to be before the first run

	m_ibOffsetRun = m_ibOffsetRunMic - sizeof( _RUN );

	return JET_errSuccess;
	}

// ErrMoveNext					- moves the run cursor to the next run in the
//								  scatter list
//
// RESULT:						ERR
//
//		JET_errNoCurrentRecord	- there are no subsequent runs in the scatter
//								  list.  the run currency will be after the last
//								  run in the scatter list

ERR CSLVInfo::ErrMoveNext()
	{
	ERR err = JET_errSuccess;

	//  move to the next run

	m_ibOffsetRun += sizeof( _RUN );
	
	//  we are beyond the last run

	if ( m_ibOffsetRun == m_ibOffsetRunMac )
		{
		//  keep currency at the after last position

		m_ibOffsetRun = m_ibOffsetRunMac;
		
		//  return no current record

		Call( ErrERRCheck( JET_errNoCurrentRecord ) );
		}

HandleError:
	return err;
	}

// ErrMovePrev					- moves the run cursor to the previous run in the
//								  scatter list
//
// RESULT:						ERR
//
//		JET_errNoCurrentRecord	- there are no previous runs in the scatter list.
//								  the run currency will be before the first run in
//								  the scatter list

ERR CSLVInfo::ErrMovePrev()
	{
	ERR err = JET_errSuccess;

	//  move to the next run

	m_ibOffsetRun -= sizeof( _RUN );
	
	//  we are beyond the last run

	if ( m_ibOffsetRun == m_ibOffsetRunMic - sizeof( _RUN ) )
		{
		//  keep currency at the before first position

		m_ibOffsetRun = m_ibOffsetRunMic - sizeof( _RUN );
		
		//  return no current record

		Call( ErrERRCheck( JET_errNoCurrentRecord ) );
		}

HandleError:
	return err;
	}

// ErrMoveAfterLast				- moves the run cursor after the last run in
//								  the scatter list
//
// RESULT:						ERR

ERR CSLVInfo::ErrMoveAfterLast()
	{
	//  reset currency to be after the last run

	m_ibOffsetRun = m_ibOffsetRunMac;

	return JET_errSuccess;
	}

// ErrSeek						- moves the run cursor to the run containing the
//								  specified SLV offset.  the specified offset is
//								  only valid if it is smaller than the SLV size
//								  as indicated by the current header information
//
// RESULT:						ERR
//
//		JET_errNoCurrentRecord	- this SLV offset is not contained by this SLV.
//								  the run currency will be after the last run in
//								  the scatter list

ERR CSLVInfo::ErrSeek( QWORD ibVirtual )
	{
	ERR err = JET_errSuccess;

	//  the specified offset is beyond the end of the valid data for the SLV or
	//  we have no runs

	if ( ibVirtual >= m_header.cbSize || !m_header.cRun )
		{
		//  move to after last and return no current record

		CallS( ErrMoveAfterLast() );
		Call( ErrERRCheck( JET_errNoCurrentRecord ) );
		}

	//  we are being asked to seek to the beginning of the SLV (a common case)

	if ( !ibVirtual )
		{
		//  move to the first run

		CallS( ErrMoveBeforeFirst() );
		Call( ErrMoveNext() );
		}

	//  we are not being asked to seek to the beginning of the SLV

	else
		{
		//  determine what runs are currently cached

		Assert( m_ibOffsetChunkMic <= m_ibOffsetChunkMac );
		Assert( m_ibOffsetRunMic <= m_ibOffsetChunkMac );

		QWORD ibOffsetMic;
		QWORD ibOffsetMac;

		ibOffsetMic = max( m_ibOffsetChunkMic, m_ibOffsetRunMic );
		ibOffsetMac = max( m_ibOffsetChunkMac, m_ibOffsetRunMic );

		ibOffsetMic = min( ibOffsetMic, m_ibOffsetRunMac );
		ibOffsetMac = min( ibOffsetMac, m_ibOffsetRunMac );

		ibOffsetMic = ibOffsetMic + sizeof( _RUN ) - 1;
		ibOffsetMic = ibOffsetMic - ( ibOffsetMic - m_ibOffsetRunMic ) % sizeof( _RUN );
		ibOffsetMac = ibOffsetMac - ( ibOffsetMac - m_ibOffsetRunMic ) % sizeof( _RUN );

		//  determine the limits for the binary search based on what we see in the
		//  cached runs, starting with all the runs.  we go through all this pain
		//  and suffering because loading the cache is VERY expensive and seeks
		//  usually are somewhat near each other with respect to the cache size giving
		//  a high probability that the run we want is already cached

		QWORD ibOffsetFirst;
		QWORD ibOffsetLast;

		ibOffsetFirst	= m_ibOffsetRunMic;
		ibOffsetLast	= m_ibOffsetRunMac - sizeof( _RUN );

		if ( ibOffsetMic + sizeof( _RUN ) <= ibOffsetMac )
			{
			_RUN run;
			UtilMemCpy(	&run,
						(BYTE*)m_pvCache + ibOffsetMic - m_ibOffsetChunkMic,
						sizeof( _RUN ) );

			if ( run.ibVirtualNext < ibVirtual )
				{
				ibOffsetFirst	= max( ibOffsetFirst, ibOffsetMic + sizeof( _RUN ) );
				}
			else if ( run.ibVirtualNext > ibVirtual )
				{
				ibOffsetLast	= min( ibOffsetLast, ibOffsetMic );
				}
			else
				{
				ibOffsetFirst	= max( ibOffsetFirst, ibOffsetMic + sizeof( _RUN ) );
				ibOffsetLast	= min( ibOffsetLast, ibOffsetMic + sizeof( _RUN ) );
				}
			}

		if ( ibOffsetMic < ibOffsetMac - sizeof( _RUN ) )
			{
			_RUN run;
			UtilMemCpy(	&run,
						(BYTE*)m_pvCache + ibOffsetMac - sizeof( _RUN ) - m_ibOffsetChunkMic,
						sizeof( _RUN ) );

			if ( run.ibVirtualNext < ibVirtual )
				{
				ibOffsetFirst	= max( ibOffsetFirst, ibOffsetMac );
				}
			else if ( run.ibVirtualNext > ibVirtual )
				{
				ibOffsetLast	= min( ibOffsetLast, ibOffsetMac - sizeof( _RUN ) );
				}
			else
				{
				ibOffsetFirst	= max( ibOffsetFirst, ibOffsetMac );
				ibOffsetLast	= min( ibOffsetLast, ibOffsetMac );
				}
			}

		Assert( ibOffsetFirst <= ibOffsetLast );

		//  binary search the remaining range of the scatter list for the run whose
		//  ibVirtualLast is greater than or equal to the one we desire

		while ( ibOffsetFirst < ibOffsetLast )
			{
			//  compute a new midpoint

			QWORD ibOffsetMid;
			ibOffsetMid = ( ibOffsetFirst + ibOffsetLast ) / 2;
			ibOffsetMid = ibOffsetMid - ( ibOffsetMid - m_ibOffsetRunMic ) % sizeof( _RUN );

			//  load the new data into the SLVInfo cache

			Assert( ibOffsetMid == ULONG( ibOffsetMid ) );
			Call( ErrLoadCache( ULONG( ibOffsetMid ), sizeof( _RUN ) ) );

			//  copy the data from the SLVInfo cache

			_RUN run;
			UtilMemCpy(	&run,
						(BYTE*)m_pvCache + ibOffsetMid - m_ibOffsetChunkMic,
						sizeof( _RUN ) );

			//  the midpoint is less than or equal to our target.  look in the top half

			if ( run.ibVirtualNext <= ibVirtual )
				{
				ibOffsetFirst = ibOffsetMid + sizeof( _RUN );
				}

			//  the midpoint is greater than our target.  look in the bottom half

			else
				{
				ibOffsetLast = ibOffsetMid;
				}
			}

		//  set the current run to the located run

		Assert( ibOffsetFirst == ULONG( ibOffsetFirst ) );
		m_ibOffsetRun = ULONG( ibOffsetFirst );
		}

#ifdef DEBUG

	//  verify that the run we found is the correct run
	{
	CSLVInfo::RUN run;
	Call( ErrGetCurrentRun( &run ) );

	Assert( run.ibVirtual <= ibVirtual );
	Assert( ibVirtual < run.ibVirtualNext );
	}
#endif  //  DEBUG

HandleError:
	return err;
	}

// ErrGetHeader					- retrieves the header of the SLV scatter list
//
// IN:
//		pheader					- buffer to receive the header
//
// RESULT:						ERR
//
// OUT:	
//		pheader					- header of the SLV scatter list

ERR CSLVInfo::ErrGetHeader( HEADER* pheader )
	{
	//  return a copy of the header

	UtilMemCpy( pheader, &m_header, sizeof( HEADER ) );

	return JET_errSuccess;
	}

// ErrSetHeader					- sets the header of the SLV scatter list
//
// IN:
//		header					- the new header for the SLV scatter list
//
// RESULT:						ERR

ERR CSLVInfo::ErrSetHeader( HEADER& header )
	{
	//  modify our copy of the header

	UtilMemCpy( &m_header, &header, sizeof( HEADER ) );
	m_fHeaderDirty = fTrue;

	return JET_errSuccess;
	}

// ErrGetFileID					- retrieves the file ID of the SLV scatter list
//
// IN:
//		pfileid					- buffer to receive the file ID
//
// RESULT:						ERR
//
// OUT:	
//		pfileid					- file ID of the SLV scatter list

ERR CSLVInfo::ErrGetFileID( FILEID* pfileid )
	{
	ERR err = JET_errSuccess;

	//  the file ID is currently unknown

	if ( m_fileid == fileidNil )
		{
		//  there had better be at least one run in this SLV Info

		if ( !m_header.cRun )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}
			
		//  compute the file ID from the ibLogical of the first run

		Call( ErrLoadCache( m_ibOffsetRunMic, sizeof( _RUN ) ) );

		_RUN run;
		UtilMemCpy(	&run,
					(BYTE*)m_pvCache + m_ibOffsetRunMic - m_ibOffsetChunkMic,
					sizeof( _RUN ) );

		m_fileid = run.ibLogical;
		}

	//  return the file ID

	*pfileid = m_fileid;

HandleError:
	return err;
	}

// ErrSetFileID					- sets the file ID of the SLV scatter list
//
// IN:
//		fileid					- the new file ID for the SLV scatter list
//
// RESULT:						ERR
//

ERR CSLVInfo::ErrSetFileID( FILEID& fileid )
	{
	//  set the file ID

	m_fileid = fileid;

	return JET_errSuccess;
	}

// ErrGetFileAlloc				- retrieves the file allocation size of the
//								  SLV scatter list
//
// IN:
//		pcbAlloc				- buffer to receive the file allocation size
//
// RESULT:						ERR
//
// OUT:	
//		pcbAlloc				- file allocation size of the SLV scatter list

ERR CSLVInfo::ErrGetFileAlloc( QWORD* pcbAlloc )
	{
	ERR err = JET_errSuccess;

	//  the file allocation size is currently unknown

	if ( m_cbAlloc == 0 )
		{
		//  there had better be at least one run in this SLV Info

		if ( !m_header.cRun )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}
			
		//  compute the file allocation size from the ibVirtualNext of the last
		//  run

		Call( ErrLoadCache( m_ibOffsetRunMac - sizeof( _RUN ), sizeof( _RUN ) ) );

		_RUN run;
		UtilMemCpy(	&run,
					(BYTE*)m_pvCache + m_ibOffsetRunMac - sizeof( _RUN ) - m_ibOffsetChunkMic,
					sizeof( _RUN ) );

		m_cbAlloc = run.ibVirtualNext;
		}

	//  return the file allocation size

	*pcbAlloc = m_cbAlloc;

HandleError:
	return err;
	}

// ErrSetFileAlloc				- sets the file allocation size of the SLV
//								  scatter list
//
// IN:
//		cbAlloc					- the new file allocation size for the SLV
//								  scatter list
//
// RESULT:						ERR
//

ERR CSLVInfo::ErrSetFileAlloc( QWORD& cbAlloc )
	{
	//  set the file allocation size

	m_cbAlloc = cbAlloc;

	return JET_errSuccess;
	}

// ErrGetFileName				- retrieves the file name of the SLV scatter list
//
// IN:
//		wszFileName				- buffer to receive the file name (must be at least
//								  IFileSystemAPI::cchPathMax wchar_t's in size)
//
// RESULT:						ERR
//
// OUT:	
//		wszFileName				- file name of the SLV scatter list

ERR CSLVInfo::ErrGetFileName( wchar_t* wszFileName )
	{
	ERR err = JET_errSuccess;

	//  validate IN args
	//
	Assert( wszFileName );
	
	//  the file name is volatile
	//
	if ( m_fFileNameVolatile )
		{
		//  copy the file name from the volatile file name cache
		//
		wcscpy( wszFileName, m_wszFileName );
		}
	//  the file name is not volatile
	//
	else
		{
		//  determine offset and size of file name in the SLV Info.  the file
		//  name is currently the entire extent of the SLV Info between the
		//  header and the runs

		QWORD ibFileName	= sizeof( HEADER );
		DWORD cbFileName	= m_ibOffsetRunMic - sizeof( HEADER );

		//  there is a stored file name
		Assert( cbFileName <= IFileSystemAPI::cchPathMax );
		if ( cbFileName > 0 )
			{
			//  load the new data into the SLVInfo cache
			//
			Assert( ibFileName == ULONG( ibFileName ) );
			Call( ErrLoadCache( ULONG( ibFileName ), cbFileName ) );

			//  copy the data from the SLVInfo cache into the user buffer
			//
			UtilMemCpy(	wszFileName, (BYTE*)m_pvCache + ibFileName - m_ibOffsetChunkMic, cbFileName );
			Assert( wcslen( wszFileName ) < IFileSystemAPI::cchPathMax );
			}

		//  null terminate the string (even if no data was copied)
		//		
		memset( (BYTE*)wszFileName + cbFileName, 0, sizeof( wchar_t ) );
		}

	//  there is no stored file name
	//
	if ( wszFileName[0] == L'\0' )
		{
		//  name the SLV by its file ID

		FILEID fileid;
		Call( ErrGetFileID( &fileid ) );
		swprintf( wszFileName, L"$SLV%016I64X$", fileid );
		}

HandleError:
	return err;
	}

// ErrGetFileNameLength			- retrieves the length of the file name of the SLV
//								  scatter list
//
// IN:
//		pcwchFileName			- buffer to receive the length of the file name
//
// RESULT:						ERR
//
// OUT:	
//		pcwchFileName			- length of the file name of the SLV scatter list

ERR CSLVInfo::ErrGetFileNameLength( size_t* const pcwchFileName )
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	Assert( pcwchFileName );

	//  the file name is volatile

	if ( m_fFileNameVolatile )
		{
		//  return the length of the cached volatile file name

		*pcwchFileName = wcslen( m_wszFileName );
		}

	//  the file name is not volatile

	else
		{
		//  get the current length of the file name

		DWORD cbFileName = m_ibOffsetRunMic - sizeof( HEADER );

		//  validate SLV Info

		if (	( cbFileName % 2 ) != 0 ||
				cbFileName / sizeof( wchar_t ) >= IFileSystemAPI::cchPathMax )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		//  return the length of the file name

		*pcwchFileName = cbFileName / sizeof( wchar_t );
		}

	//  there is no stored file name

	if ( *pcwchFileName == 0 )
		{
		//  we will return a computed file name whose length is always 21 chars

		*pcwchFileName = 21;
		}

HandleError:
	return err;
	}

// ErrSetFileNameVolatile		- marks the file name of the SLV scatter list
//								  as volatile.  the file name will not be
//								  persisted but can still be manipulated
//								  normally
//
// RESULT:						ERR
//
// NOTES:
//
//   Currently, the file name must be marked as volatile before it is set
//   for the first time.  you will receive JET_errSLVCorrupted if you do not
//   observe this limitation

ERR CSLVInfo::ErrSetFileNameVolatile()
	{
	ERR err = JET_errSuccess;

	//  we currently do not support marking the file name as volatile after
	//  it has been set

	if ( m_ibOffsetRunMic - sizeof( HEADER ) > 0 )
		{
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

	//  mark the file name as volatile

	m_fFileNameVolatile = fTrue;

	//  initialize the file name to be an empty string

	m_wszFileName[0] = L'\0';

HandleError:
	return err;
	}

// ErrSetFileName				- sets the file name of the SLV scatter list
//
// IN:
//		wszFileName				- the new file name for the SLV scatter list
//
// RESULT:						ERR
//
//		JET_errSLVCorrupted		- an attempt was made to set the file name of a
//								  scatter list that already has a file name or
//								  already has runs
//
// NOTES:
//
//   Currently, the file name length can only be changed if there are no runs in
//   the scatter list.  you will receive JET_errSLVCorrupted if you attempt to do
//   this.

ERR CSLVInfo::ErrSetFileName( const wchar_t* wszFileName )
	{
	ERR err = JET_errSuccess;

	//  validate IN args

	Assert( wszFileName );
	Assert( wcslen( wszFileName ) );
	Assert( wcslen( wszFileName ) < IFileSystemAPI::cchPathMax );

	//  the file name is volatile

	if ( m_fFileNameVolatile )
		{
		//  copy the file name into the volatile file name cache

		wcscpy( m_wszFileName, wszFileName );
		}

	//  the file name is not volatile

	else
		{
		//  get the offset and size of the file name to insert

		QWORD ibFileName;
		DWORD cbFileName;
		ibFileName = sizeof( HEADER );
		cbFileName = (ULONG)wcslen( wszFileName ) * sizeof( wchar_t );

		//  we currently do not support changing the file name length if the SLV Info
		//  already has runs in its scatter list as it would require moving the rest
		//  of the scatter list in the LV

		if (	cbFileName != m_ibOffsetRunMic - sizeof( HEADER ) &&
				m_ibOffsetRunMic != m_ibOffsetRunMac )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		//  we also do not support shrinking the file name once it has been saved

		if (	cbFileName < m_ibOffsetRunMic - sizeof( HEADER ) &&
				!m_fCacheDirty )
			{
			Call( ErrERRCheck( JET_errSLVCorrupted ) );
			}

		//  load the new data into the SLVInfo cache

		Assert( ibFileName == ULONG( ibFileName ) );
		Call( ErrLoadCache( ULONG( ibFileName ), cbFileName ) );

		//  ensure that the chunk and run limits account for the filename (simple append)

		Assert( ibFileName + cbFileName == ULONG( ibFileName + cbFileName ) );
		
		m_ibOffsetChunkMac	= max( m_ibOffsetChunkMac, ULONG( ibFileName + cbFileName ) );
		m_ibOffsetRunMic	= max( m_ibOffsetRunMic, ULONG( ibFileName + cbFileName ) );
		m_ibOffsetRunMac	= max( m_ibOffsetRunMac, ULONG( ibFileName + cbFileName ) );
		m_ibOffsetRun		= ULONG( ibFileName + cbFileName );

		//  modify the file name with the user supplied data
		
		UtilMemCpy(	(BYTE*)m_pvCache + ibFileName - m_ibOffsetChunkMic,
					(BYTE*)wszFileName,
					cbFileName );
		m_fCacheDirty = fTrue;
		}

HandleError:
	return err;
	}

// ErrGetCurrentRun				- retrieves the current run in the SLV scatter
//
// IN:
//		prun					- buffer to receive the run
//
// RESULT:						ERR
//
// OUT:	
//		prun					- run from the SLV scatter list

ERR CSLVInfo::ErrGetCurrentRun( RUN* prun )
	{
	ERR err = JET_errSuccess;
	_RUN rgrun[2];

	//  we are not currently on a run

	if ( m_ibOffsetRun < m_ibOffsetRunMic || m_ibOffsetRun >= m_ibOffsetRunMac )
		{
		//  fail with no current record

		Call( ErrERRCheck( JET_errNoCurrentRecord ) );
		}

	//  compute the amount of data to read.  normally, we need to read two runs,
	//  the run preceding the current run for its ibVirtualNext (to become the
	//  current run's ibVirtual) and the current run.  we do not need to read
	//  the preceding run if this is the first run

	BYTE* pbRead;
	DWORD cbRead;
	DWORD ibOffsetRead;

	if ( m_ibOffsetRun == m_ibOffsetRunMic )
		{
		rgrun[0].ibVirtualNext	= 0;
		rgrun[0].ibLogical		= -1;
		rgrun[0].qwReserved		= 0;
		
		pbRead			= (BYTE*)&rgrun[1];
		cbRead			= sizeof( rgrun[1] );
		ibOffsetRead	= m_ibOffsetRun;
		}
	else
		{
		pbRead			= (BYTE*)rgrun;
		cbRead			= sizeof( rgrun );
		ibOffsetRead	= m_ibOffsetRun - sizeof( _RUN );
		}

	//  load the new data into the SLVInfo cache

	Call( ErrLoadCache( ibOffsetRead, cbRead ) );

	//  copy the data from the SLVInfo cache

	UtilMemCpy(	pbRead,
				(BYTE*)m_pvCache + ibOffsetRead - m_ibOffsetChunkMic,
				cbRead );

	//  move the read data into the user buffer

	prun->ibVirtualNext	= rgrun[1].ibVirtualNext;
	prun->ibLogical		= rgrun[1].ibLogical;
	prun->qwReserved	= rgrun[1].qwReserved;
	prun->ibVirtual		= rgrun[0].ibVirtualNext;
	prun->cbSize		= prun->ibVirtualNext - prun->ibVirtual;
	prun->ibLogicalNext	= prun->ibLogical + prun->cbSize;

	//  validate SLV run

	if (	prun->ibVirtualNext % SLVPAGE_SIZE ||
			prun->ibLogical % SLVPAGE_SIZE ||
			prun->cbSize % SLVPAGE_SIZE )
		{
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

	if (	m_pfucb != pfucbNil &&
			prun->ibLogicalNext > rgfmp[ m_pfucb->ifmp ].CbTrueSLVFileSize() )
		{
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

	if ( prun->qwReserved )
		{
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

HandleError:
	return err;
	}

// ErrSetCurrentRun				- sets the current run in the SLV scatter list.
//								  new runs may be appended to the scatter list
//								  by setting the current run when we are after
//								  the last run
//
// IN:
//		run						- the new run for the SLV scatter list
//
// RESULT:						ERR
//
//		JET_errNoCurrentRecord	- the current run is before the first run

ERR CSLVInfo::ErrSetCurrentRun( RUN& run )
	{
	ERR err = JET_errSuccess;

	//  if we are before the first run, return no current record

	if ( m_ibOffsetRun < m_ibOffsetRunMic )
		{
		Call( ErrERRCheck( JET_errNoCurrentRecord ) )
		}

	//  load the new data into the SLVInfo cache

	Call( ErrLoadCache( m_ibOffsetRun, sizeof( _RUN ) ) );

	//  ensure that the chunk and run limits include this run (for simple append)

	m_ibOffsetChunkMac	= max( m_ibOffsetChunkMac, m_ibOffsetRun + sizeof( _RUN ) );
	m_ibOffsetRunMac	= max( m_ibOffsetRunMac, m_ibOffsetRun + sizeof( _RUN ) );

	//  modify this run with the user supplied data
	
	UtilMemCpy(	(BYTE*)m_pvCache + m_ibOffsetRun - m_ibOffsetChunkMic,
				(BYTE*)(_RUN*)&run,
				sizeof( _RUN ) );
	m_fCacheDirty = fTrue;

HandleError:
	return err;
	}

// ErrReadSLVInfo				- reads raw LV data from the SLV Information
//
// IN:
//		ibOffsetRead			- LV offset for the range to be retrieved
//		pbRead					- buffer to receive the data
//		cbRead					- size of the range to be retrieved
//		pcbActual				- buffer to receive the actual bytes read
//
// RESULT:						ERR
//
// OUT:	
//		pbRead					- retrieved data
//		pcbActual				- actual bytes beyond the given offset

ERR CSLVInfo::ErrReadSLVInfo(	ULONG ibOffsetRead,
								BYTE* pbRead,
								ULONG cbRead,
								DWORD* pcbActual )
	{
	ERR			err;
	BOOL		fNeedToReleaseLatch	= fFalse;

	//  we should never be here if this is volatile SLVInfo

	Assert( m_pfucb != pfucbNil );

	Assert( m_pfucb->ppib->level > 0 );
	const BOOL	fUseCopyBuffer		= ( ( m_fCopyBuffer && FFUCBUpdatePrepared( m_pfucb ) && !FFUCBNeverRetrieveCopy( m_pfucb ) )
										|| FFUCBAlwaysRetrieveCopy( m_pfucb ) );
	const DATA	* pdataRec;
	DATA		dataRetrieved;

	if ( fUseCopyBuffer )
		{
		pdataRec = &m_pfucb->dataWorkBuf;
		}
	else
		{
		if ( !Pcsr( m_pfucb )->FLatched() )
			{
			Call( ErrDIRGet( m_pfucb ) );
			fNeedToReleaseLatch = fTrue;
			}
		pdataRec = &m_pfucb->kdfCurr.data;
		}

	Assert( FCOLUMNIDTagged( m_columnid ) );
	Assert( 0 != m_itagSequence );
	Assert( pfcbNil != m_pfucb->u.pfcb );
	Assert( ptdbNil != m_pfucb->u.pfcb->Ptdb() );

	Call( ErrRECIRetrieveTaggedColumn(
			m_pfucb->u.pfcb,
			m_columnid,
			m_itagSequence,
			*pdataRec,
			&dataRetrieved,
			grbitRetrieveColumnReadSLVInfo | grbitRetrieveColumnDDLNotLocked | JET_bitRetrieveIgnoreDefault ) );
	if ( wrnRECIntrinsicSLV == err )
		{
		if ( ibOffsetRead >= dataRetrieved.Cb() )
			dataRetrieved.SetCb( 0 );
		else
			{
			dataRetrieved.DeltaPv( ibOffsetRead );
			dataRetrieved.DeltaCb( -ibOffsetRead );
			}

		*pcbActual = dataRetrieved.Cb();

		ULONG	cbCopy;
		if ( dataRetrieved.Cb() <= cbRead )
			{
			cbCopy = dataRetrieved.Cb();
			err = JET_errSuccess;
			}
		else
			{
			cbCopy = cbRead;
			err = ErrERRCheck( JET_wrnBufferTruncated );
			}

		if ( cbCopy > 0 )
			UtilMemCpy( pbRead, dataRetrieved.Pv(), cbCopy );
		}
	else if ( wrnRECSeparatedSLV == err )
		{
		//  If we are retrieving an after-image or
		//	haven't replaced a LV we can simply go
		//	to the LV tree. Otherwise we have to
		//	perform a more detailed consultation of
		//	the version store with ErrRECGetLVImage
		const BOOL fAfterImage = fUseCopyBuffer
									|| !FFUCBUpdateSeparateLV( m_pfucb )
									|| !FFUCBReplacePrepared( m_pfucb );
		Call( ErrRECIRetrieveSeparatedLongValue(
					m_pfucb,
					dataRetrieved,
					fAfterImage,
					ibOffsetRead,
					pbRead,
					cbRead,
					pcbActual,
					NO_GRBIT ) );
		}
	else
		{
		Assert( fFalse );
		err = ErrERRCheck( JET_errSLVCorrupted );
		}

HandleError:
	if ( fNeedToReleaseLatch )
		{
		CallS( ErrDIRRelease( m_pfucb ) );
		}
	return err;
	}

// ErrWriteSLVInfo				- writes raw LV data to the SLV Information
//
// IN:
//		ibOffsetWrite			- LV offset for the range to be written
//		pbWrite					- buffer containing the data to write
//		cbWrite					- size of the range to be written
//
// RESULT:						ERR

ERR CSLVInfo::ErrWriteSLVInfo( ULONG ibOffsetWrite, BYTE* pbWrite, ULONG cbWrite )
	{
	ERR		err			= JET_errSuccess;
	DATA	dataWrite;

	//  if this is volatile SLVInfo, bail with JET_errDiskFull

	if ( m_pfucb == pfucbNil )
		{
		Call( ErrERRCheck( JET_errDiskFull ) );
		}

	//  setup to write the given data to the SLVInfo

	dataWrite.SetPv( pbWrite );
	dataWrite.SetCb( cbWrite );

	//  if we have a latch at this point, we must release it

	if ( m_pfucb->csr.FLatched() )
		{
		CallS( ErrDIRRelease( m_pfucb ) );
		}
			
	//  try to update the SLVInfo with the given data
	
	err = ErrRECSetLongField(	m_pfucb,
								m_columnid,
								m_itagSequence,
								&dataWrite,
								JET_bitSetSLVFromSLVInfo | JET_bitSetOverwriteLV,
									ibOffsetWrite,
								0 );

	//  this update would result in a record that is too big to fit on the page

	if ( err == JET_errRecordTooBig )
		{
		//  attempt to separate any LVs in the record to get more space
		
		Call( ErrRECAffectLongFieldsInWorkBuf( m_pfucb, lvaffectSeparateAll ) );

		//  try once more to update the SLVInfo with the given data.  if it fails
		//  this time because the record is too big, there's nothing we can do
		
		Call( ErrRECSetLongField(	m_pfucb,
									m_columnid,
									m_itagSequence,
									&dataWrite,
									JET_bitSetSLVFromSLVInfo | JET_bitSetOverwriteLV,
									ibOffsetWrite,
									0 ) );
		}

HandleError:
	Assert( err != JET_errColumnNoChunk );
	return err;
	}

// ErrLoadCache					- caches SLV Information from the LV containing
//								  at least the specified offset range.  if there
//								  is no SLVInfo for part of the specified byte
//								  range, sufficient buffer will be reserved to
//								  place new information in that sub-range
//
// IN:
//		ibOffsetRequired		- LV offset for the range to be cached
//		cbRequired				- size of the range to be cached
//
// RESULT:						ERR

ERR CSLVInfo::ErrLoadCache( ULONG ibOffsetRequired, ULONG cbRequired )
	{
	ERR err = JET_errSuccess;

	//  this is the largest cache size we will ever use

	const DWORD cbCacheMax = g_cbPage;

	//  validate IN args

	Assert( cbRequired <= cbCacheMax );

	//  requested range is not already entirely cached

	if (	ibOffsetRequired < m_ibOffsetChunkMic ||
			ibOffsetRequired + cbRequired > m_ibOffsetChunkMac )
		{
		//  we cannot grow the SLVInfo cache's size for a simple append

		if (	ibOffsetRequired < m_ibOffsetChunkMic ||
				ibOffsetRequired > m_ibOffsetChunkMac ||
				ibOffsetRequired + cbRequired > m_ibOffsetChunkMic + cbCacheMax ||
				m_ibOffsetChunkMac < m_ibOffsetRunMac )
			{
			//  save any changes we may have made to the local cache

			Call( ErrFlushCache() );

			//  if we are still using the small cache, grow to the large cache

			if ( m_pvCache == m_rgbSmallCache )
				{
				if ( !( m_pvCache = PvOSMemoryHeapAlloc( cbCacheMax ) ) )
					{
					m_pvCache = m_rgbSmallCache;
					Call( ErrERRCheck( JET_errOutOfMemory ) );
					}
				m_cbCache = cbCacheMax;
				}

			//  choose to load the cache with either all data or as much data as
			//  possible including the current run biased for our traversal, depending
			//  on the current SLVInfo size

			DWORD ibOffsetChunk;
			if ( ibOffsetRequired + cbRequired < m_cbCache )
				{
				ibOffsetChunk = 0;
				}
			else
				{
				if ( ibOffsetRequired < m_ibOffsetChunkMic )
					{
					ibOffsetChunk = ibOffsetRequired + cbRequired - m_cbCache;
					}
				else
					{
					ibOffsetChunk = ibOffsetRequired;
					}
				}

			//  invalidate the current chunk in preparation for reading a new chunk

			m_ibOffsetChunkMic = m_ibOffsetChunkMac;
			
			//  load the cache with the decided data

			DWORD cbActual;
			Call( ErrReadSLVInfo( ibOffsetChunk, (BYTE*)m_pvCache, m_cbCache, &cbActual ) );

			//  reset the current chunk offsets to represent the cached data

			m_ibOffsetChunkMic	= ibOffsetChunk;
			m_ibOffsetChunkMac	= ibOffsetChunk + min( m_cbCache, cbActual );
			}

		//  we can grow the SLVInfo cache's size for a simple append but we are
		//  currently using the small cache

		else if ( ibOffsetRequired + cbRequired > m_ibOffsetChunkMic + m_cbCache )
			{
			//  grow to the large cache, copying the old data over

			Assert( m_pvCache == m_rgbSmallCache );
			
			if ( !( m_pvCache = PvOSMemoryHeapAlloc( cbCacheMax ) ) )
				{
				m_pvCache = m_rgbSmallCache;
				Call( ErrERRCheck( JET_errOutOfMemory ) );
				}
			m_cbCache = cbCacheMax;

			UtilMemCpy( m_pvCache, m_rgbSmallCache, sizeof( m_rgbSmallCache ) );
			}
		}

	//  we had better have cached or left room to create the byte range that was
	//  requested

	Assert( m_ibOffsetChunkMic <= ibOffsetRequired );
	Assert( ibOffsetRequired + cbRequired <= m_ibOffsetChunkMac ||
			m_ibOffsetChunkMac == m_ibOffsetRunMac );
	Assert( ibOffsetRequired + cbRequired <= m_ibOffsetChunkMic + m_cbCache );

	return JET_errSuccess;

HandleError:
	return err;
	}

// ErrFlushCache				- saves any changes made to any SLV Information
//								  from the LV that is currently cached
//
// RESULT:						ERR

ERR CSLVInfo::ErrFlushCache()
	{
	ERR err = JET_errSuccess;

	//  we have a SLVInfo cache and it is dirty

	if ( m_pvCache && m_fCacheDirty )
		{
		//  the header can fit in the current chunk

		if ( !m_ibOffsetChunkMic )
			{
			//  if the cache is dirty, it should include the space for the
			//  header already

			Assert( m_ibOffsetChunkMac >= sizeof( HEADER ) );
			
			//  copy the header into the cache

			UtilMemCpy( m_pvCache, &m_header, sizeof( HEADER ) );
			m_fHeaderDirty = fFalse;
			}

		//  write out the information in the local cache

		Call( ErrWriteSLVInfo(	m_ibOffsetChunkMic,
								(BYTE*)m_pvCache,
								m_ibOffsetChunkMac - m_ibOffsetChunkMic ) );
		m_fCacheDirty = fFalse;
		}

HandleError:
	return err;
	}


// UNDONE SLVOWNERMAP: duplicate code (except SetTypeSLVOwnerMap)  !
//  ================================================================
ERR ErrSLVFCBOwnerMap( PIB *ppib, const IFMP ifmp, const PGNO pgno, FCB **ppfcb )
//  ================================================================
	{
	ERR		err 		= JET_errSuccess;
	FUCB	*pfucb 		= pfucbNil;
	FCB		*pfcb 		= pfcbNil;
	
	Assert( ppibNil != ppib );

	CallR( ErrDIROpen( ppib, pgno, ifmp, &pfucb ) );
	Assert( pfucbNil != pfucb );
	Assert( !FFUCBVersioned( pfucb ) );	// Verify won't be deferred closed.
	pfcb = pfucb->u.pfcb;
	
	Assert( pfcb->Ifmp() == ifmp );
	Assert( pfcb->PgnoFDP() == pgno );
	Assert( pfcb->Ptdb() == ptdbNil );
	Assert( pfcb->CbDensityFree() == 0 );
	
	Assert( pfcb->FTypeNull() );
	pfcb->SetTypeSLVOwnerMap();

	Assert( pfcb->PfcbTable() == pfcbNil );

	//	finish the initialization of this SLV FCB

	pfcb->CreateComplete();

	DIRClose( pfucb );
	*ppfcb = pfcb;
	return err;
	}


ERR ErrSLVOwnerMapInit( PIB *ppib, const IFMP ifmp, PGNO pgnoSLVOwnerMap )
	{
	ERR			err 				= JET_errSuccess;
	FMP			*pfmp				= rgfmp + ifmp;
	INST		*pinst 				= PinstFromIfmp( ifmp );
	FUCB		*pfucbSLVOwnerMap	= pfucbNil;
	FCB			*pfcbSLVOwnerMap	= pfcbNil;
	const BOOL	fTempDb				= ( dbidTemp == pfmp->Dbid() );

	Assert( ppibNil != ppib );

	//	if recovering, then must be at the end of hard restore where we re-attach
	//	to the db because it moved (in ErrLGRIEndAllSessions())
	// or we are during recovery in CreateDB when pgnoSLVOwnerMap is provided
	Assert( !pinst->FRecovering()
			|| pinst->m_plog->m_fHardRestore ||
			pgnoNull != pgnoSLVOwnerMap );

	if ( pgnoNull == pgnoSLVOwnerMap )
		{
		if ( fTempDb )
			{
			pgnoSLVOwnerMap = pgnoTempDbSLVOwnerMap;
			}
		else
			{		
			OBJID objid;
			Call( ErrCATAccessDbSLVOwnerMap( ppib, ifmp, szSLVOwnerMap, &pgnoSLVOwnerMap, &objid ) );
			}
		}
	else
		{
		Assert( pfmp->FCreatingDB() || fGlobalRepair );
		Assert ( !fTempDb || pgnoSLVOwnerMap == pgnoTempDbSLVOwnerMap );
		}

	Assert( pgnoNull != pgnoSLVOwnerMap );

	Call( ErrSLVFCBOwnerMap( ppib, ifmp, pgnoSLVOwnerMap, &pfcbSLVOwnerMap ) );
	Assert( pfcbNil != pfcbSLVOwnerMap );
	Assert ( pfcbSLVOwnerMap->FTypeSLVOwnerMap() );
	
	pfmp->SetPfcbSLVOwnerMap( pfcbSLVOwnerMap );
	
	return JET_errSuccess;

HandleError:
	Assert( err < 0 );

	SLVOwnerMapTerm( ifmp, fFalse );
	return err;
	}


VOID SLVOwnerMapTerm( const IFMP ifmp, const BOOL fTerminating )
	{
	FMP	* const pfmp				= rgfmp + ifmp;
	Assert( pfmp );

	FCB	* const pfcbSLVOwnerMap 	= pfmp->PfcbSLVOwnerMap();

	if ( pfcbNil != pfcbSLVOwnerMap )
		{
		//	synchronously purge the FCB
		Assert( pfcbSLVOwnerMap->FTypeSLVOwnerMap() );
		pfcbSLVOwnerMap->PrepareForPurge();

		//	there should only be stray cursors if we're in the middle of terminating,
		//	in all other cases, all cursors should have been properly closed first
		Assert( 0 == pfcbSLVOwnerMap->WRefCount() || fTerminating );
		if ( fTerminating )
			pfcbSLVOwnerMap->CloseAllCursors( fTrue );

		pfcbSLVOwnerMap->Purge();
		pfmp->SetPfcbSLVOwnerMap( pfcbNil );
		}
	}


//	get the count of all space in the streaming file (owned and available)

ERR ErrSLVGetSpaceInformation( 
	PIB		*ppib,
	IFMP	ifmp,
	CPG		*pcpgOwned,
	CPG		*pcpgAvail )
	{
	ERR		err					= JET_errSuccess;
	FCB		*pfcb				= pfcbNil;
	FUCB	*pfucb				= pfucbNil;
	CPG		cpgAvail			= 0;
	CPG		cpgTotal			= 0;
	BOOL	fInTxn				= fFalse;

	//	make sure a streaming file is present

	if ( !rgfmp[ifmp].Pdbfilehdr()->FSLVExists() )
		{
		Call( ErrERRCheck( JET_errSLVStreamingFileNotCreated ) );
		}

	//	open the SLV avail tree

	pfcb = rgfmp[ifmp].PfcbSLVAvail();
	Assert( pfcbNil != pfcb );
	
	Call( ErrBTOpen( ppib, pfcb, &pfucb, fFalse ) );
	Assert( pfucbNil != pfucb );

	//	start a transaction

	Call( ErrDIRBeginTransaction( ppib, JET_bitTransactionReadOnly ) );
	fInTxn = fTrue;

	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm 	= NULL;
	dib.dirflag = fDIRNull;

	FUCBSetPrereadForward( pfucb, cpgPrereadSequential );
	err = ErrBTDown( pfucb, &dib, latchReadTouch );
	if ( JET_errNoCurrentRecord == err )
		{

		//  the tree is empty

		err = JET_errSuccess;
		goto HandleError;
		}
	Call( err );

	do
		{

		//	check the current node

		if ( sizeof( SLVSPACENODE ) != pfucb->kdfCurr.data.Cb() )
			{
			AssertSz( fFalse, "SLV space tree corruption. Data is wrong size" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		if ( sizeof( PGNO ) != pfucb->kdfCurr.key.Cb() )
			{
			AssertSz( fFalse, "SLV space tree corruption. Key is wrong size" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}

		//	add to the total number of pages seen

		cpgTotal += SLVSPACENODE::cpageMap;

		//	verify the total number of pages

		ULONG pgnoCurr;
		LongFromKey( &pgnoCurr, pfucb->kdfCurr.key );
		if ( cpgTotal != pgnoCurr )
			{
			AssertSz( fFalse, "SLV space tree corruption. Nodes out of order" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}

		const SLVSPACENODE * pspacenode = (SLVSPACENODE *)pfucb->kdfCurr.data.Pv();

#ifndef RTM
		Call( pspacenode->ErrCheckNode( CPRINTFDBGOUT::PcprintfInstance() ) );
#endif	//	RTM

		//	add to the number of available pages

		Assert( pspacenode->CpgAvail() <= SLVSPACENODE::cpageMap );
		cpgAvail += pspacenode->CpgAvail();
		}
	while ( JET_errSuccess == ( err = ErrBTNext( pfucb, fDIRNull ) ) );

	if ( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	fInTxn = fFalse;

HandleError:

	Assert( cpgAvail <= cpgTotal );
	if ( pcpgOwned )
		{
		*pcpgOwned = cpgTotal;
		}
	if ( pcpgAvail )
		{
		*pcpgAvail = cpgAvail;
		}

	if ( pfucb )
		{
		BTClose( pfucb );
		}

	if ( fInTxn )
		{
		Assert( err < JET_errSuccess );
		CallS( ErrDIRRollback( ppib ) );
		}

	return err;
	}

#endif	//	DISABLE_SLV
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\space.cxx ===
#include "std.hxx"
#include "_space.hxx"

PERFInstanceGlobal<> cSPCreate;
PM_CEF_PROC LSPCreateCEFLPv;
LONG LSPCreateCEFLPv( LONG iInstance, VOID *pvBuf )
	{
	cSPCreate.PassTo( iInstance, pvBuf );
	return 0;
	}

PERFInstanceGlobal<> cSPDelete;
PM_CEF_PROC LSPDeleteCEFLPv;
LONG LSPDeleteCEFLPv( LONG iInstance, VOID *pvBuf )
	{
	cSPDelete.PassTo( iInstance, pvBuf );
	return 0;
	}


#include "_bt.hxx"


const CHAR * SzNameOfTable( const FUCB * const pfucb )
	{
	if( pfucb->u.pfcb->FTypeTable() )
		{
		return pfucb->u.pfcb->Ptdb()->SzTableName();
		}
	else if( pfucb->u.pfcb->FTypeLV() )
		{
		return pfucb->u.pfcb->PfcbTable()->Ptdb()->SzTableName();
		}
	else if( pfucb->u.pfcb->FTypeSecondaryIndex() )
		{
		return pfucb->u.pfcb->PfcbTable()->Ptdb()->SzTableName();
///		return pfucb->u.pfcb->PfcbTable()->Ptdb()->SzIndexName(
///					pfucb->u.pfcb->Pidb()->ItagIndexName(),
///					pfucb->u.pfcb->FDerivedIndex() );
		}
	return "";
	}


#define COALESCE_OE


#ifdef DEBUG
///#define SPACECHECK
///#define SPACE_TRACE
///#define DEBUG_DUMP_SPACE_INFO
///#define REPORT_FREED_SORT_FDP
#endif

///#define CONVERT_VERBOSE
#ifdef CONVERT_VERBOSE
extern void VARARG CONVPrintF2(const CHAR * fmt, ...);
#endif


//	prototypes of internal functions
//
LOCAL ERR ErrSPIAddFreedExtent( FUCB *pfucbAE, const PGNO pgnoParentFDP, const PGNO pgnoLast, const CPG cpgSize );
LOCAL ERR ErrSPIGetSE(
	FUCB		* pfucb,
	FUCB		* pfucbAE,
	const CPG	cpgReq,
	const CPG	cpgMin,
	const BOOL	fSplitting,
	SPCACHE		*** pppspcache = NULL );
LOCAL ERR ErrSPIWasAlloc( FUCB *pfucb, PGNO pgnoFirst, CPG cpgSize );
LOCAL ERR ErrSPIValidFDP( PIB *ppib, IFMP ifmp, PGNO pgnoFDP );


//	types of extents in SPBUF
//
enum SPEXT	{ spextFreed, spextSecondary };

LOCAL ErrSPIReservePages( FUCB *pfucb, FUCB *pfucbParent, const SPEXT spext );
LOCAL ERR ErrSPIAddToAvailExt(
	FUCB		*pfucbAE,
	const PGNO	pgnoAELast,
	const CPG	cpgAESize,
	SPCACHE		***pppspcache = NULL );



INLINE VOID AssertSPIPfucbOnRoot( FUCB *pfucb )
	{
#ifdef	DEBUG
	//	check to make sure that FUCB
	//	passed in is on root page
	//	and has page RIW latched
	//
	Assert( pfucb->pcsrRoot != pcsrNil );
	Assert( pfucb->pcsrRoot->Pgno() == PgnoFDP( pfucb ) );
	Assert( pfucb->pcsrRoot->Latch() == latchRIW 
		|| pfucb->pcsrRoot->Latch() == latchWrite );
	Assert( !FFUCBSpace( pfucb ) );
#endif
	}

INLINE VOID AssertSPIPfucbOnSpaceTreeRoot( FUCB *pfucb, CSR *pcsr )
	{
#ifdef	DEBUG
	Assert( FFUCBSpace( pfucb ) );
	Assert( pcsr->FLatched() );
	Assert( pcsr->Pgno() == PgnoRoot( pfucb ) );
	Assert( pcsr->Cpage().FRootPage() );
	Assert( pcsr->Cpage().FSpaceTree() );
#endif
	}

//  write latch page before update
//
LOCAL VOID SPIUpgradeToWriteLatch( FUCB *pfucb )
	{
	CSR		*pcsrT;

	if ( Pcsr( pfucb )->Latch() == latchNone )
		{
		//	latch upgrades only work on root
		//
		Assert( pfucb->pcsrRoot->Pgno() == PgnoFDP( pfucb ) );

		//	must want to upgrade latch in pcsrRoot
		//
		pcsrT = pfucb->pcsrRoot;
		Assert( latchRIW == pcsrT->Latch() );
		}
	else
		{
		//	latch upgrades only work on root
		//
		Assert( Pcsr( pfucb ) == pfucb->pcsrRoot );
		Assert( Pcsr(pfucb)->Pgno() == PgnoFDP( pfucb ) );
		pcsrT = &pfucb->csr;
		}

	if ( pcsrT->Latch() != latchWrite )
		{
		Assert( pcsrT->Latch() == latchRIW );
		pcsrT->UpgradeFromRIWLatch();
		}
	else
		{
		Assert( fFalse );
		}
	}


//	opens a cursor on Avail/owned extent of given tree
//	subsequent BT operations on cursor will place 
//	cursor on root page of available extent
//	this logic is embedded in ErrBTIGotoRootPage
//
ERR ErrSPIOpenAvailExt( PIB *ppib, FCB *pfcb, FUCB **ppfucbAE )
	{
	ERR		err;

	//	open cursor on FCB
	//	label it as a space cursor
	//
	CallR( ErrBTOpen( ppib, pfcb, ppfucbAE, fFalse ) );
	FUCBSetAvailExt( *ppfucbAE );
	FUCBSetIndex( *ppfucbAE );
	Assert( pfcb->FSpaceInitialized() );
	Assert( pfcb->PgnoAE() != pgnoNull );

	return err;
	}


ERR ErrSPIOpenOwnExt( PIB *ppib, FCB *pfcb, FUCB **ppfucbOE )
	{
	ERR	err;

	//	open cursor on FCB
	//	label it as a space cursor
	//
	CallR( ErrBTOpen( ppib, pfcb, ppfucbOE, fFalse ) );
	FUCBSetOwnExt( *ppfucbOE );
	FUCBSetIndex( *ppfucbOE );
	Assert( pfcb->FSpaceInitialized() );
	Assert( pfcb->PgnoOE() != pgnoNull );

	return err;
	}


//	gets pgno of last page owned by database
//
ERR	ErrSPGetLastPgno( PIB *ppib, IFMP ifmp, PGNO *ppgno )
	{
	ERR		err;
	FUCB	*pfucb = pfucbNil;
	FUCB	*pfucbOE = pfucbNil;
	DIB		dib;
	
	CallR( ErrBTOpen( ppib, pgnoSystemRoot, ifmp, &pfucb ) );
	Assert( pfucbNil != pfucb );

	if ( PinstFromPpib( ppib )->m_plog->m_fRecovering && !pfucb->u.pfcb->FSpaceInitialized() )
		{
		//	pgnoOE and pgnoAE need to be obtained
		//
		Call( ErrSPInitFCB( pfucb ) );
		}
	Assert( pfucb->u.pfcb->FSpaceInitialized() );

	Call( ErrSPIOpenOwnExt( ppib, pfucb->u.pfcb, &pfucbOE ) );
	
	dib.dirflag = fDIRNull;
	dib.pos 	= posLast;
	Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );

	Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( ppgno, pfucbOE->kdfCurr.key );

HandleError:
	if ( pfucbOE != pfucbNil )
		{
		BTClose( pfucbOE );
		}
		
	Assert ( pfucb != pfucbNil );
	BTClose( pfucb );
		
	return err;
	}


LOCAL VOID SPIInitFCB( FUCB *pfucb, const BOOL fDeferredInit )
	{
	SPACE_HEADER	* psph;
	CSR				* pcsr	= ( fDeferredInit ? pfucb->pcsrRoot : Pcsr( pfucb ) );
	FCB				* pfcb	= pfucb->u.pfcb;

	Assert( pcsr->FLatched() );

	//	need to acquire FCB lock because that's what protects the Flags
	pfcb->Lock();

	if ( !pfcb->FSpaceInitialized() )
		{
		//	get external header
		//
		NDGetExternalHeader ( pfucb, pcsr );
		Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );
		psph = reinterpret_cast <SPACE_HEADER *> ( pfucb->kdfCurr.data.Pv() );

		if ( psph->FSingleExtent() )
			{
			pfcb->SetPgnoOE( pgnoNull );
			pfcb->SetPgnoAE( pgnoNull );
			}
		else
			{
			pfcb->SetPgnoOE( psph->PgnoOE() );
			pfcb->SetPgnoAE( psph->PgnoAE() );
			}

		if ( !fDeferredInit )
			{
			Assert( pfcb->FUnique() );		// FCB always initialised as unique
			if ( psph->FNonUnique() )
				pfcb->SetNonUnique();
			}

		pfcb->SetSpaceInitialized();
		}

	pfcb->Unlock();

	return;
	}

	
//	initializes FCB with pgnoAE and pgnoOE
//
ERR	ErrSPInitFCB( FUCB * const pfucb )
	{
	ERR				err;
	FCB				*pfcb	= pfucb->u.pfcb;

	Assert( !Pcsr( pfucb )->FLatched() );
	Assert( !FFUCBSpace( pfucb ) );
	
	//	goto root page of tree
	//
	err = ErrBTIGotoRoot( pfucb, latchReadTouch );
	if ( err < 0 )
		{
		if ( fGlobalRepair )
			{
			//  ignore the error
			pfcb->SetPgnoOE( pgnoNull );
			pfcb->SetPgnoAE( pgnoNull );

			Assert( objidNil == pfcb->ObjidFDP() );
		
			err = JET_errSuccess;
			}
		}
	else
		{
		//	get objidFDP from root page, FCB can only be set once

		Assert( objidNil == pfcb->ObjidFDP()
			|| ( PinstFromIfmp( pfucb->ifmp )->FRecovering() && pfcb->ObjidFDP() == Pcsr( pfucb )->Cpage().ObjidFDP() ) );
		pfcb->SetObjidFDP( Pcsr( pfucb )->Cpage().ObjidFDP() );

		SPIInitFCB( pfucb, fFalse );

		BTUp( pfucb );
		}

	return err;
	}


ERR ErrSPDeferredInitFCB( FUCB * const pfucb )
	{
	ERR				err;
	FCB				* pfcb	= pfucb->u.pfcb;
	FUCB			* pfucbT	= pfucbNil;

	Assert( !Pcsr( pfucb )->FLatched() );
	Assert( !FFUCBSpace( pfucb ) );
	
	//	goto root page of tree
	//
	CallR( ErrBTIOpenAndGotoRoot(
				pfucb->ppib,
				pfcb->PgnoFDP(),
				pfucb->ifmp,
				&pfucbT ) );
	Assert( pfucbNil != pfucbT );
	Assert( pfucbT->u.pfcb == pfcb );
	Assert( pcsrNil != pfucbT->pcsrRoot );

	if ( !pfcb->FSpaceInitialized() )
		{
		SPIInitFCB( pfucbT, fTrue );
		}

	pfucbT->pcsrRoot->ReleasePage();
	pfucbT->pcsrRoot = pcsrNil;

	Assert( pfucbNil != pfucbT );
	BTClose( pfucbT );

	return JET_errSuccess;
	}


INLINE SPACE_HEADER *PsphSPIRootPage( FUCB *pfucb )
	{
	SPACE_HEADER	*psph;
	
	AssertSPIPfucbOnRoot( pfucb );
	
	NDGetExternalHeader( pfucb, pfucb->pcsrRoot );
	Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );
	psph = reinterpret_cast <SPACE_HEADER *> ( pfucb->kdfCurr.data.Pv() );

	return psph;
	}

//	get objid of parentFDP of this tree
//
INLINE PgnoSPIParentFDP( FUCB *pfucb )
	{
	return PsphSPIRootPage( pfucb )->PgnoParent();
	}
	
//	get cpgPrimary of this tree
//
INLINE CPG CpgSPIPrimary( FUCB *pfucb )
	{
	return PsphSPIRootPage( pfucb )->CpgPrimary();
	}


INLINE SPLIT_BUFFER *PspbufSPISpaceTreeRootPage( FUCB *pfucb, CSR *pcsr )
	{
	SPLIT_BUFFER	*pspbuf;

	AssertSPIPfucbOnSpaceTreeRoot( pfucb, pcsr );
	
	NDGetExternalHeader( pfucb, pcsr );
	Assert( sizeof( SPLIT_BUFFER ) == pfucb->kdfCurr.data.Cb() );
	pspbuf = reinterpret_cast <SPLIT_BUFFER *> ( pfucb->kdfCurr.data.Pv() );

	return pspbuf;
	}


//#define REPORT_SPBUF

INLINE VOID SPIReportSplitBufferMsg_( const FUCB * const pfucb, const CHAR * const szMsg )
	{
#ifdef REPORT_SPBUF
	OSTrace( ostlLow, OSFormat( 
						"Database '%s': %s SplitBuffer for a Btree (objidFDP %d, pgnoFDP %d).",
						rgfmp[pfucb->ifmp].SzDatabaseName(),
						szMsg,
						pfucb->u.pfcb->ObjidFDP(),
						pfucb->u.pfcb->PgnoFDP() ) );
#endif
	}

INLINE VOID SPIReportAllocatedSplitBuffer( const FUCB * const pfucb )
	{
	SPIReportSplitBufferMsg_( pfucb, "Allocated" );
	}

INLINE VOID SPIReportPersistedSplitBuffer( const FUCB * const pfucb )
	{
	SPIReportSplitBufferMsg_( pfucb, "Persisted" );
	}

INLINE VOID SPIReportGetPageFromSplitBuffer( const FUCB * const pfucb )
	{
	SPIReportSplitBufferMsg_( pfucb, "Retrieved page from" );
	}

INLINE VOID SPIReportAddedPagesToSplitBuffer( const FUCB * const pfucb )
	{
	SPIReportSplitBufferMsg_( pfucb, "Added pages to" );
	}


LOCAL ERR ErrSPIFixSpaceTreeRootPage( FUCB *pfucb, SPLIT_BUFFER **ppspbuf )
	{
	ERR			err			= JET_errSuccess;
	const BOOL	fAvailExt	= FFUCBAvailExt( pfucb );

	AssertSPIPfucbOnSpaceTreeRoot( pfucb, Pcsr( pfucb ) );
	Assert( latchRIW == Pcsr( pfucb )->Latch() );

#ifdef DEBUG
	const BOOL	fNotEnoughPageSpace	= ( Pcsr( pfucb )->Cpage().CbFree() < ( g_cbPage * 3 / 4 ) );
#else
	const BOOL	fNotEnoughPageSpace	= ( Pcsr( pfucb )->Cpage().CbFree() < sizeof(SPLIT_BUFFER) );
#endif

	if ( fNotEnoughPageSpace )
		{
		if ( NULL == pfucb->u.pfcb->Psplitbuf( fAvailExt ) )
			{
			CallR( pfucb->u.pfcb->ErrEnableSplitbuf( fAvailExt ) );
			SPIReportAllocatedSplitBuffer( pfucb );
			}
		*ppspbuf = pfucb->u.pfcb->Psplitbuf( fAvailExt );
		Assert( NULL != *ppspbuf );
		}
	else
		{
		const BOOL		fSplitbufDangling	= ( NULL != pfucb->u.pfcb->Psplitbuf( fAvailExt ) );
		SPLIT_BUFFER	spbuf;
		DATA			data;
		Assert( 0 == pfucb->kdfCurr.data.Cb() );

		//	if in-memory copy of split buffer exists, move it to the page
		if ( fSplitbufDangling )
			{
			memcpy( &spbuf, pfucb->u.pfcb->Psplitbuf( fAvailExt ), sizeof(SPLIT_BUFFER) );
			}
		else
			{
			memset( &spbuf, 0, sizeof(SPLIT_BUFFER) );
			}

		data.SetPv( &spbuf );
		data.SetCb( sizeof(spbuf) );

		Pcsr( pfucb )->UpgradeFromRIWLatch();
		err = ErrNDSetExternalHeader(
					pfucb,
					&data,
					( pfucb->u.pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) );
		Pcsr( pfucb )->Downgrade( latchRIW );
		CallR( err );

		if ( fSplitbufDangling )
			{
			//	split buffer successfully moved to page, destroy in-memory copy
			pfucb->u.pfcb->DisableSplitbuf( fAvailExt );
			SPIReportPersistedSplitBuffer( pfucb );
			}

		//	re-cache external header
		NDGetExternalHeader( pfucb, Pcsr( pfucb ) );

		*ppspbuf = reinterpret_cast <SPLIT_BUFFER *> ( pfucb->kdfCurr.data.Pv() );
		}

	return err;
	}

INLINE ERR ErrSPIGetSPBuf( FUCB *pfucb, SPLIT_BUFFER **ppspbuf )
	{
	ERR	err;
	
	AssertSPIPfucbOnSpaceTreeRoot( pfucb, Pcsr( pfucb ) );
	
	NDGetExternalHeader( pfucb, Pcsr( pfucb ) );
	
	if ( sizeof( SPLIT_BUFFER ) != pfucb->kdfCurr.data.Cb() )
		{
		err = ErrSPIFixSpaceTreeRootPage( pfucb, ppspbuf );
		}
	else
		{
		Assert( NULL == pfucb->u.pfcb->Psplitbuf( FFUCBAvailExt( pfucb ) ) );
		*ppspbuf = reinterpret_cast <SPLIT_BUFFER *> ( pfucb->kdfCurr.data.Pv() );
		err = JET_errSuccess;
		}

	return err;
	}

INLINE VOID SPIDirtyAndSetMaxDbtime( CSR *pcsr1, CSR *pcsr2, CSR *pcsr3 )
	{ 
	Assert( pcsr1->Latch() == latchWrite );
	Assert( pcsr2->Latch() == latchWrite );
	Assert( pcsr3->Latch() == latchWrite );

	pcsr1->Dirty();
	pcsr2->Dirty();
	pcsr3->Dirty();
	DBTIME	dbtimeMax = pcsr1->Dbtime();

	if ( dbtimeMax < pcsr2->Dbtime() )
		{
		dbtimeMax = pcsr2->Dbtime();
		}

	if ( dbtimeMax < pcsr3->Dbtime() )
		{
		dbtimeMax = pcsr3->Dbtime();
		}

	if ( dbtimeMax < 3 )
		{
		Assert( fRecoveringRedo == PinstFromIfmp( pcsr1->Cpage().Ifmp() )->m_plog->m_fRecoveringMode );

		//	CPAGE::ErrGetNewPage() initialises the dbtime to 1.
		Assert( 1 == pcsr1->Dbtime() );
		Assert( 1 == pcsr2->Dbtime() );
		Assert( 1 == pcsr3->Dbtime() );
		}

	//	set dbtime in the three pages
	//
	pcsr1->SetDbtime( dbtimeMax );
	pcsr2->SetDbtime( dbtimeMax );
	pcsr3->SetDbtime( dbtimeMax );
	}


INLINE VOID SPIInitSplitBuffer( FUCB *pfucb, CSR *pcsr )
	{
	//	copy dummy split buffer into external header
	//
	DATA 			data;
	SPLIT_BUFFER	spbuf;

	memset( &spbuf, 0, sizeof(SPLIT_BUFFER) );
	
	Assert( FBTIUpdatablePage( *pcsr ) );		//	check is already performed by caller
	Assert( latchWrite == pcsr->Latch() );
	Assert( pcsr->FDirty() );
	Assert( pgnoNull != PgnoAE( pfucb ) || fGlobalRepair );
	Assert( pgnoNull != PgnoOE( pfucb ) || fGlobalRepair );
	Assert( ( FFUCBAvailExt( pfucb ) && pcsr->Pgno() == PgnoAE( pfucb ) )
		|| ( FFUCBOwnExt( pfucb ) && pcsr->Pgno() == PgnoOE( pfucb ) )
		|| fGlobalRepair );

	data.SetPv( (VOID *)&spbuf );
	data.SetCb( sizeof(spbuf) );
	NDSetExternalHeader( pfucb, pcsr, &data );
	}

	
//	creates extent tree for either owned or available extents.
//
VOID SPICreateExtentTree( FUCB *pfucb, CSR *pcsr, PGNO pgnoLast, CPG cpgExtent, BOOL fAvail )
	{
	if ( !FBTIUpdatablePage( *pcsr ) )
		{
		//	page does not redo
		//
		return;
		}
	
	//	cannot reuse a deferred closed cursor
	//
	Assert( !FFUCBVersioned( pfucb ) );
	Assert( !FFUCBSpace( pfucb ) );
	Assert( pgnoNull != PgnoAE( pfucb ) || fGlobalRepair );
	Assert( pgnoNull != PgnoOE( pfucb ) || fGlobalRepair );
	Assert( latchWrite == pcsr->Latch() );
	Assert( pcsr->FDirty() );
	
	if ( fAvail )
		{
		FUCBSetAvailExt( pfucb );
		}
	else
		{
		FUCBSetOwnExt( pfucb );
		}
	Assert( pcsr->FDirty() );

	SPIInitSplitBuffer( pfucb, pcsr );

	Assert( 0 == pcsr->Cpage().Clines() );
	pcsr->SetILine( 0 );
	
	//	goto Root before insert would place
	//	cursor on appropriate space tree
	//
	if ( cpgExtent != 0 )
		{
		BYTE			rgbKey[sizeof(PGNO)];
		KEYDATAFLAGS	kdf;
		
		KeyFromLong( rgbKey, pgnoLast );
		kdf.key.prefix.Nullify();
		kdf.key.suffix.SetCb( sizeof( PGNO ) );
		kdf.key.suffix.SetPv( rgbKey );

		LittleEndian<CPG> le_cpgExtent = cpgExtent;
		kdf.data.SetCb( sizeof( CPG ) );
		kdf.data.SetPv( &le_cpgExtent );

		kdf.fFlags = 0;

		NDInsert( pfucb, pcsr, &kdf );
		}
	else
		{
		//	avail has already been set up as an empty tree
		//
		Assert( FFUCBAvailExt( pfucb ) );
		}

	if ( fAvail )
		{
		FUCBResetAvailExt( pfucb );
		}
	else
		{
		FUCBResetOwnExt( pfucb );
		}
		
	return;
	}


VOID SPIInitPgnoFDP( FUCB *pfucb, CSR *pcsr, const SPACE_HEADER& sph )
	{
	if ( !FBTIUpdatablePage( *pcsr ) )
		{
		//	page does not redo
		//
		return;
		}
		
	Assert( latchWrite == pcsr->Latch() );
	Assert( pcsr->FDirty() );
	Assert( pcsr->Pgno() == PgnoFDP( pfucb ) || fGlobalRepair );

	PERFIncCounterTable( cSPCreate, PinstFromPfucb( pfucb ), pfucb->u.pfcb->Tableclass() );

	//	copy space information into external header
	//
	DATA 	data;
	
	data.SetPv( (VOID *)&sph );
	data.SetCb( sizeof(sph) );
	NDSetExternalHeader( pfucb, pcsr, &data );
	}


//	performs creation of multiple extent FDP 
//	
VOID SPIPerformCreateMultiple( FUCB *pfucb, CSR *pcsrFDP, CSR *pcsrOE, CSR *pcsrAE, PGNO pgnoPrimary, const SPACE_HEADER& sph )
	{
	const CPG	cpgPrimary = sph.CpgPrimary();
	const PGNO	pgnoLast = pgnoPrimary;
	Assert( fGlobalRepair || pgnoLast == PgnoFDP( pfucb ) + cpgPrimary - 1 );
	
	//	insert space header into FDP
	//
	SPIInitPgnoFDP( pfucb, pcsrFDP, sph );

	//	add allocated extents to OwnExt
	//
	SPICreateExtentTree( pfucb, pcsrOE, pgnoLast, cpgPrimary, fFalse );

	//	add extent minus pages allocated to AvailExt
	//
	SPICreateExtentTree( pfucb, pcsrAE, pgnoLast, cpgPrimary - 1 - 1 - 1, fTrue );
	
	return;
	}

//  ================================================================
ERR ErrSPCreateMultiple(
	FUCB		*pfucb,
	const PGNO	pgnoParent,
	const PGNO	pgnoFDP,
	const OBJID	objidFDP,
	const PGNO	pgnoOE,
	const PGNO	pgnoAE,
	const PGNO	pgnoPrimary,
	const CPG	cpgPrimary,
	const BOOL	fUnique,
	const ULONG	fPageFlags )
//  ================================================================
	{
	ERR				err;
	CSR				csrOE;
	CSR				csrAE;
	CSR				csrFDP;
	SPACE_HEADER	sph;
	LGPOS			lgpos;

	Assert( objidFDP == ObjidFDP( pfucb ) || fGlobalRepair );
	Assert( objidFDP != objidNil );

	Assert( !( fPageFlags & CPAGE::fPageRoot ) );
	Assert( !( fPageFlags & CPAGE::fPageLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageParentOfLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageEmpty ) );
	Assert( !( fPageFlags & CPAGE::fPageSpaceTree) );

	//	init space trees and root page
	//
	Call( csrOE.ErrGetNewPage( pfucb->ppib, 
							   pfucb->ifmp,
							   pgnoOE, 
							   objidFDP,
							   ( fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf | CPAGE::fPageSpaceTree ) & ~CPAGE::fPageRepair,
							   pfucb->u.pfcb->Tableclass() ) );
	Assert( csrOE.Latch() == latchWrite );

	Call( csrAE.ErrGetNewPage( pfucb->ppib, 
							   pfucb->ifmp,
							   pgnoAE, 
							   objidFDP,
							   ( fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf | CPAGE::fPageSpaceTree ) & ~CPAGE::fPageRepair,
							   pfucb->u.pfcb->Tableclass() ) );
	Assert( csrAE.Latch() == latchWrite );
	
	Call( csrFDP.ErrGetNewPage( pfucb->ppib,
								pfucb->ifmp,
								pgnoFDP,
								objidFDP,
								fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf,
								pfucb->u.pfcb->Tableclass() ) );
										
	//	set max dbtime for the three pages in all the pages
	//
	SPIDirtyAndSetMaxDbtime( &csrFDP, &csrOE, &csrAE );
		
	sph.SetCpgPrimary( cpgPrimary );
	sph.SetPgnoParent( pgnoParent );
	
	Assert( sph.FSingleExtent() );		// initialised with these defaults
	Assert( sph.FUnique() );

	sph.SetMultipleExtent();
	
	if ( !fUnique )
		sph.SetNonUnique();
		
	sph.SetPgnoOE( pgnoOE );

	//	log operation
	//
	Call( ErrLGCreateMultipleExtentFDP( pfucb, &csrFDP, &sph, fPageFlags, &lgpos ) );
	csrFDP.Cpage().SetLgposModify( lgpos );
	csrOE.Cpage().SetLgposModify( lgpos );
	csrAE.Cpage().SetLgposModify( lgpos );

	//	perform operation on all three pages
	//
	SPIPerformCreateMultiple( pfucb, &csrFDP, &csrOE, &csrAE, pgnoPrimary, sph );
	
HandleError:
	csrAE.ReleasePage();
	csrOE.ReleasePage();
	csrFDP.ReleasePage();	
	return err;
	}

	
//	initializes a FDP page with external headers for space.
//	Also initializes external space trees appropriately.
//	This operation is logged as an aggregate.
//
INLINE ERR ErrSPICreateMultiple(
	FUCB		*pfucb,
	const PGNO	pgnoParent,
	const PGNO	pgnoFDP,
	const OBJID	objidFDP,
	const CPG	cpgPrimary,
	const ULONG	fPageFlags,
	const BOOL	fUnique )
	{
	return ErrSPCreateMultiple(
			pfucb,
			pgnoParent,
			pgnoFDP,
			objidFDP,
			pgnoFDP + 1,
			pgnoFDP + 2,
			pgnoFDP + cpgPrimary - 1,
			cpgPrimary,
			fUnique,
			fPageFlags );
	}


ERR ErrSPICreateSingle(
	FUCB			*pfucb,
	CSR				*pcsr,
	const PGNO		pgnoParent,
	const PGNO		pgnoFDP,
	const OBJID		objidFDP,
	CPG				cpgPrimary,
	const BOOL		fUnique,
	const ULONG		fPageFlags )
	{
	ERR				err;
	SPACE_HEADER	sph;

	//	copy space information into external header
	//
	sph.SetPgnoParent( pgnoParent );
	sph.SetCpgPrimary( cpgPrimary );

	Assert( sph.FSingleExtent() );		// always initialised to single-extent, unique
	Assert( sph.FUnique() );

	Assert( !( fPageFlags & CPAGE::fPageRoot ) );
	Assert( !( fPageFlags & CPAGE::fPageLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageParentOfLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageEmpty ) );
	Assert( !( fPageFlags & CPAGE::fPageSpaceTree) );

	if ( !fUnique )
		sph.SetNonUnique();
	
	Assert( cpgPrimary > 0 );
	if ( cpgPrimary > cpgSmallSpaceAvailMost )
		{
		sph.SetRgbitAvail( 0xffffffff );
		}
	else
		{
		sph.SetRgbitAvail( 0 );
		while ( --cpgPrimary > 0 )
			{
			sph.SetRgbitAvail( ( sph.RgbitAvail() << 1 ) + 1 );
			}
		}

	Assert( objidFDP == ObjidFDP( pfucb ) );
	Assert( objidFDP != 0 );
	
	//	get pgnoFDP to initialize in current CSR pgno
	//
	Call( pcsr->ErrGetNewPage(
		pfucb->ppib,
		pfucb->ifmp,
		pgnoFDP,
		objidFDP,
		fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf,
		pfucb->u.pfcb->Tableclass() ) );
	pcsr->Dirty();

	if ( !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering )
		{
		LGPOS	lgpos;
		
		Call( ErrLGCreateSingleExtentFDP( pfucb, pcsr, &sph, fPageFlags, &lgpos ) );
		pcsr->Cpage().SetLgposModify( lgpos );
		}

	SPIInitPgnoFDP( pfucb, pcsr, sph );

	BTUp( pfucb );

HandleError:
	return err;
	}


//	opens a cursor on tree to be created, and uses cursor to 
//	initialize space data structures.
//
ERR ErrSPCreate(
	PIB 			*ppib, 
	const IFMP		ifmp,
	const PGNO		pgnoParent,
	const PGNO		pgnoFDP,
	const CPG		cpgPrimary,
	const BOOL		fSPFlags,
	const ULONG		fPageFlags,
	OBJID			*pobjidFDP )
	{
	ERR				err;
	FUCB			*pfucb = pfucbNil;
	const BOOL		fUnique = !( fSPFlags & fSPNonUnique );

	Assert( NULL != pobjidFDP );
	Assert( !( fPageFlags & CPAGE::fPageRoot ) );
	Assert( !( fPageFlags & CPAGE::fPageLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageParentOfLeaf ) );
	Assert( !( fPageFlags & CPAGE::fPageEmpty ) );
	Assert( !( fPageFlags & CPAGE::fPageSpaceTree ) );

	//	UNDONE: an FCB will be allocated here, but it will be uninitialized,
	//	unless this is the special case where we are allocating the DB root,
	//	in which case it is initialized. Hence, the subsequent BTClose
	//	will free it.  It will have to be reallocated on a subsequent
	//	DIR/BTOpen, at which point it will get put into the FCB hash
	//	table.  Implement a fix to allow leaving the FCB in an uninitialized
	//	state, then have it initialized by the subsequent DIR/BTOpen.
	//
	CallR( ErrBTOpen( ppib, pgnoFDP, ifmp, &pfucb, openNew ) );

	FCB	*pfcb	= pfucb->u.pfcb;
	Assert( pfcbNil != pfcb );

	if ( pgnoSystemRoot == pgnoFDP )
		{
		Assert( pfcb->FInitialized() );
		Assert( pfcb->FTypeDatabase() );
		}
	else
		{
		Assert( !pfcb->FInitialized() );
		Assert( pfcb->FTypeNull() );
		}

	Assert( pfcb->FUnique() );		// FCB always initialised as unique
	if ( !fUnique )
		pfcb->SetNonUnique();

	//	get objid for this new FDP. This objid is then stored in catalog table.

	if ( fGlobalRepair && pgnoSystemRoot == pgnoFDP )
		{
		*pobjidFDP = objidSystemRoot;
		}
	else
		{
		Call( rgfmp[ pfucb->ifmp ].ErrObjidLastIncrementAndGet( pobjidFDP ) );
		}
	Assert( pgnoSystemRoot != pgnoFDP || objidSystemRoot == *pobjidFDP );

	//	objidFDP should be initialised to NULL

	Assert( objidNil == pfcb->ObjidFDP() );
	pfcb->SetObjidFDP( *pobjidFDP );

	Assert( !pfcb->FSpaceInitialized() );
	pfcb->SetSpaceInitialized();

	if ( fSPFlags & fSPMultipleExtent )
		{
		Assert( PgnoFDP( pfucb ) == pgnoFDP );
		pfcb->SetPgnoOE( pgnoFDP + 1 );
		pfcb->SetPgnoAE( pgnoFDP + 2 );
		err = ErrSPICreateMultiple(
					pfucb,
					pgnoParent,
					pgnoFDP,
					*pobjidFDP,
					cpgPrimary,
					fPageFlags,
					fUnique );
		}
	else
		{
		Assert( PgnoFDP( pfucb ) == pgnoFDP );
		pfcb->SetPgnoOE( pgnoNull );
		pfcb->SetPgnoAE( pgnoNull );
		err = ErrSPICreateSingle(
					pfucb, 
					Pcsr( pfucb ), 
					pgnoParent,
					pgnoFDP,
					*pobjidFDP,
					cpgPrimary,
					fUnique,
					fPageFlags );
		}

	Assert( !FFUCBSpace( pfucb ) );
	Assert( !FFUCBVersioned( pfucb ) );

HandleError:
	Assert( pfucb != pfucbNil );
	BTClose( pfucb );

	return err;
	}


//	calculates extent info in rgext 
//		number of extents in *pcextMac
LOCAL VOID SPIConvertCalcExtents(
	const SPACE_HEADER&	sph, 
	const PGNO			pgnoFDP,
	EXTENTINFO 			*rgext, 
	INT 				*pcext )
	{
	PGNO				pgno;
	UINT				rgbit		= 0x80000000;
	INT					iextMac		= 0;

	//	if available extent for space after rgbitAvail, then
	//	set rgextT[0] to extent, otherwise set rgextT[0] to
	//	last available extent.
	//
	if ( sph.CpgPrimary() - 1 > cpgSmallSpaceAvailMost )
		{
		pgno = pgnoFDP + sph.CpgPrimary() - 1; 
		rgext[iextMac].pgnoLastInExtent = pgno;
		rgext[iextMac].cpgExtent = sph.CpgPrimary() - cpgSmallSpaceAvailMost - 1;
		pgno -= rgext[iextMac].cpgExtent;
		iextMac++;

		Assert( pgnoFDP + cpgSmallSpaceAvailMost == pgno );
		Assert( 0x80000000 == rgbit );
		}
	else
		{
		pgno = pgnoFDP + cpgSmallSpaceAvailMost;
		while ( 0 != rgbit && 0 == iextMac )
			{
			Assert( pgno > pgnoFDP );
			if ( rgbit & sph.RgbitAvail() ) 
				{
				Assert( pgno <= pgnoFDP + sph.CpgPrimary() - 1 );
				rgext[iextMac].pgnoLastInExtent = pgno;
				rgext[iextMac].cpgExtent = 1;
				iextMac++;
				}

			//	even if we found an available extent, we still need
			//	to update the loop variables in preparation for the
			//	next loop below
			pgno--;
			rgbit >>= 1;
			}
		}

	//	continue through rgbitAvail finding all available extents.
	//	if iextMac == 0 then there was not even one available extent.
	//
	Assert( ( 0 == iextMac && 0 == rgbit )
		|| 1 == iextMac );
		
	//	find additional available extents
	//
	for ( ; rgbit != 0; pgno--, rgbit >>= 1 )
		{
		Assert( pgno > pgnoFDP );
		if ( rgbit & sph.RgbitAvail() )
			{
			const INT	iextPrev	= iextMac - 1;
			Assert( rgext[iextPrev].cpgExtent > 0 );
			Assert( rgext[iextPrev].cpgExtent <= rgext[iextPrev].pgnoLastInExtent );
			Assert( rgext[iextPrev].pgnoLastInExtent <= pgnoFDP + sph.CpgPrimary() - 1 );
			
			const PGNO	pgnoFirst = rgext[iextPrev].pgnoLastInExtent - rgext[iextPrev].cpgExtent + 1;
			Assert( pgnoFirst > pgnoFDP );
			
			if ( pgnoFirst - 1 == pgno )
				{
				Assert( pgnoFirst - 1 > pgnoFDP );
				rgext[iextPrev].cpgExtent++;
				}
			else
				{
				rgext[iextMac].pgnoLastInExtent = pgno;
				rgext[iextMac].cpgExtent = 1;
				iextMac++;
				}
			}
		}

	*pcext = iextMac;
	
	return;
	}


//	update OwnExt root page for a convert
//
LOCAL VOID SPIConvertUpdateOE( FUCB *pfucb, CSR *pcsrOE, const SPACE_HEADER& sph, PGNO pgnoSecondaryFirst, CPG cpgSecondary )
	{
	if ( !FBTIUpdatablePage( *pcsrOE ) )
		{
		Assert( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );
		return;
		}

	Assert( pcsrOE->Latch() == latchWrite );
	Assert( !FFUCBOwnExt( pfucb ) );
	FUCBSetOwnExt( pfucb );

	SPIInitSplitBuffer( pfucb, pcsrOE );

	Assert( 0 == pcsrOE->Cpage().Clines() );
	Assert( cpgSecondary != 0 );
	pcsrOE->SetILine( 0 );

	//	insert secondary and primary extents
	//
	KEYDATAFLAGS kdf;
	LittleEndian<CPG> le_cpgSecondary = cpgSecondary;
	BYTE		rgbKey[sizeof(PGNO)];
	
	KeyFromLong( rgbKey, pgnoSecondaryFirst + cpgSecondary - 1 );
	kdf.Nullify();
	kdf.key.suffix.SetCb( sizeof( PGNO ) );
	kdf.key.suffix.SetPv( rgbKey );
	kdf.data.SetCb( sizeof( CPG ) );
	kdf.data.SetPv( &le_cpgSecondary );
	NDInsert( pfucb, pcsrOE, &kdf );

	CPG						cpgPrimary = sph.CpgPrimary();
	LittleEndian<CPG>		le_cpgPrimary = cpgPrimary;

	KeyFromLong( rgbKey, PgnoFDP( pfucb ) + cpgPrimary - 1 );
	kdf.key.suffix.SetCb( sizeof(PGNO) );
	kdf.key.suffix.SetPv( rgbKey );
	
	kdf.data.SetCb( sizeof(le_cpgPrimary) );
	kdf.data.SetPv( &le_cpgPrimary );

	ERR			err;
	BOOKMARK	bm;
	NDGetBookmarkFromKDF( pfucb, kdf, &bm );
	err = ErrNDSeek( pfucb, pcsrOE, bm );
	Assert( wrnNDFoundLess == err || wrnNDFoundGreater == err );
	if ( wrnNDFoundLess == err )
		{
		Assert( pcsrOE->Cpage().Clines() - 1 == pcsrOE->ILine() );
		pcsrOE->IncrementILine();
		}
	NDInsert( pfucb, pcsrOE, &kdf );
	FUCBResetOwnExt( pfucb );
	
	return;
	}


LOCAL VOID SPIConvertUpdateAE(
	FUCB			*pfucb, 
	CSR 			*pcsrAE, 
	EXTENTINFO		*rgext, 
	INT				iextMac, 
	PGNO			pgnoSecondaryFirst, 
	CPG				cpgSecondary )
	{
	if ( !FBTIUpdatablePage( *pcsrAE ) )
		{
		Assert( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );
		return;
		}

	KEYDATAFLAGS	kdf;
	BYTE			rgbKey[sizeof(PGNO)];
	
	//	create available extent tree and insert avail extents
	//
	Assert( pcsrAE->Latch() == latchWrite );
	Assert( !FFUCBAvailExt( pfucb ) );
	FUCBSetAvailExt( pfucb );

	SPIInitSplitBuffer( pfucb, pcsrAE );

	Assert( cpgSecondary >= 2 );
	Assert( 0 == pcsrAE->Cpage().Clines() );
	pcsrAE->SetILine( 0 );
	
	//	insert secondary extent in AvailExt
	//
	CPG		cpgExtent = cpgSecondary - 1 - 1;
	PGNO	pgnoLast = pgnoSecondaryFirst + cpgSecondary - 1;
	if ( cpgExtent != 0 )
		{
		KeyFromLong( rgbKey, pgnoLast );
		kdf.Nullify();
		kdf.key.suffix.SetCb( sizeof( PGNO ) );
		kdf.key.suffix.SetPv( rgbKey );

		LittleEndian<CPG> le_cpgExtent = cpgExtent;
		kdf.data.SetCb( sizeof( CPG ) );
		kdf.data.SetPv( &le_cpgExtent );

		NDInsert( pfucb, pcsrAE, &kdf );
		}

	Assert( latchWrite == pcsrAE->Latch() );
	
	//	rgext contains list of available pages, with highest pages
	//	first, so traverse the list in reverse order to force append
	for ( INT iext = iextMac - 1; iext >= 0; iext-- )
		{
		
		//	extent may have been fully allocated for OE and AE trees
		//
//		if ( 0 == rgext[iext].cpgExtent )
//			continue;

		//	NO!!  Above code is wrong!  OE and AE trees are allocated
		//	from a secondary extent when the FDP is converted from
		//	single to multiple. -- JLIEM
		Assert( rgext[iext].cpgExtent > 0 );
		
		KeyFromLong( rgbKey, rgext[iext].pgnoLastInExtent );
		kdf.Nullify();
		kdf.key.suffix.SetCb( sizeof(PGNO) );
		kdf.key.suffix.SetPv( rgbKey );
		kdf.data.SetCb( sizeof(PGNO) );
		LittleEndian<CPG> le_cpgExtent = rgext[iext].cpgExtent;
		kdf.data.SetPv( &le_cpgExtent );
		kdf.data.SetCb( sizeof(rgext[iext].cpgExtent) );

		//	seek to point of insert
		//
		if ( 0 < pcsrAE->Cpage().Clines() )
			{
			ERR			err;
			BOOKMARK	bm;
			NDGetBookmarkFromKDF( pfucb, kdf, &bm );
			err = ErrNDSeek( pfucb, pcsrAE, bm );
			Assert( wrnNDFoundLess == err || wrnNDFoundGreater == err );
			if ( wrnNDFoundLess == err )
				pcsrAE->IncrementILine();
			}
			
		NDInsert( pfucb, pcsrAE, &kdf );
		}

	FUCBResetAvailExt( pfucb );
	Assert( pcsrAE->Latch() == latchWrite );
	
	return;
	}


//	update FDP page for convert
//
INLINE VOID SPIConvertUpdateFDP( FUCB *pfucb, CSR *pcsrRoot, SPACE_HEADER *psph )
	{
	Assert( latchWrite == pcsrRoot->Latch() );
	Assert( pcsrRoot->FDirty() );
	DATA	data;
	
	//	update external header to multiple extent space
	//
	data.SetPv( psph );
	data.SetCb( sizeof( *psph ) );
	NDSetExternalHeader( pfucb, pcsrRoot, &data );
	
	return;
	}


//	perform convert operation
//
VOID SPIPerformConvert( FUCB			*pfucb, 
						CSR				*pcsrRoot, 
						CSR				*pcsrAE, 
						CSR				*pcsrOE, 
						SPACE_HEADER	*psph, 
						PGNO			pgnoSecondaryFirst, 
						CPG				cpgSecondary,
						EXTENTINFO		*rgext,
						INT				iextMac )
	{
	SPIConvertUpdateOE( pfucb, pcsrOE, *psph, pgnoSecondaryFirst, cpgSecondary );

	SPIConvertUpdateAE( pfucb, pcsrAE, rgext, iextMac, pgnoSecondaryFirst, cpgSecondary );

	SPIConvertUpdateFDP( pfucb, pcsrRoot, psph );
	}


//	gets space header from root page
//	calcualtes extent info and places in rgext
//
VOID SPIConvertGetExtentinfo( FUCB			*pfucb, 
							  CSR			*pcsrRoot, 
							  SPACE_HEADER	*psph,
							  EXTENTINFO	*rgext, 
							  INT			*piextMac )
	{
	//	determine all available extents.  Maximum number of
	//	extents is (cpgSmallSpaceAvailMost+1)/2 + 1.
	//
	NDGetExternalHeader( pfucb, pcsrRoot );
	Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );

	//	get single extent allocation information
	//
	UtilMemCpy( psph, pfucb->kdfCurr.data.Pv(), sizeof(SPACE_HEADER) );
	
	//	if available extent for space after rgbitAvail, then
	//	set rgextT[0] to extent, otherwise set rgextT[0] to
	//	last available extent.
	//
	SPIConvertCalcExtents( *psph, PgnoFDP( pfucb ), rgext, piextMac );

	return;
	}

	
//	convert single extent root page external header to 
//	multiple extent space tree.
//
LOCAL ERR ErrSPIConvertToMultipleExtent( FUCB *pfucb, CPG cpgReq, CPG cpgMin )
	{
	ERR				err;
	SPACE_HEADER	sph;
	PGNO			pgnoSecondaryFirst	= pgnoNull;
	INT				iextMac				= 0;
	FUCB			*pfucbT;
	CSR				csrOE;
	CSR				csrAE;
	CSR				*pcsrRoot			= pfucb->pcsrRoot;
	DBTIME 			dbtimeBefore 		= dbtimeNil;
	LGPOS			lgpos;
	EXTENTINFO		rgextT[(cpgSmallSpaceAvailMost+1)/2 + 1];

	Assert( NULL != pcsrRoot );
	Assert( latchRIW == pcsrRoot->Latch() );
	AssertSPIPfucbOnRoot( pfucb );
	Assert( !FFUCBSpace( pfucb ) );
	Assert( ObjidFDP( pfucb ) != 0 );
	Assert( pfucb->u.pfcb->FSpaceInitialized() );

	const ULONG fPageFlags = pcsrRoot->Cpage().FFlags()
								& ~CPAGE::fPageRepair
								& ~CPAGE::fPageRoot
								& ~CPAGE::fPageLeaf
								& ~CPAGE::fPageParentOfLeaf;

	//	get extent info from space header of root page
	//
	SPIConvertGetExtentinfo( pfucb, pcsrRoot, &sph, rgextT, &iextMac );
	Assert( sph.FSingleExtent() );
	
	//	allocate secondary extent for space trees
	//
	CallR( ErrBTOpen( pfucb->ppib, pfucb->u.pfcb, &pfucbT, fFalse ) );

	//	account for OE/AE root pages
	cpgMin += 2;

	//  allocate enough pages for OE/AE root paages, plus enough
	//	to satisfy the space request that caused us to
	//	burst to multiple extent in the first place
	cpgMin = max( cpgMultipleExtentConvert, cpgMin );
	cpgReq = max( cpgMin, cpgReq );

	//	database always uses multiple extent
	//
	Assert( sph.PgnoParent() != pgnoNull );
	Call( ErrSPGetExt( pfucbT, sph.PgnoParent(), &cpgReq, cpgMin, &pgnoSecondaryFirst ) );
	Assert( cpgReq >= cpgMin );
	Assert( pgnoSecondaryFirst != pgnoNull );

	//	set pgnoOE and pgnoAE in B-tree
	//
	Assert( pgnoSecondaryFirst != pgnoNull );
	sph.SetMultipleExtent();
	sph.SetPgnoOE( pgnoSecondaryFirst );
	pfucb->u.pfcb->SetPgnoOE( sph.PgnoOE() );
	pfucb->u.pfcb->SetPgnoAE( sph.PgnoAE() );

	//	represent all space in space trees.  Note that maximum
	//	space allowed to accumulate before conversion to 
	//	large space format, should be representable in single
	//	page space trees.
	//
	//	create OwnExt and insert primary and secondary owned extents
	//
	Assert( !FFUCBVersioned( pfucbT ) );
	Assert( !FFUCBSpace( pfucbT ) );
		
	Assert( pgnoNull != PgnoAE( pfucb ) );
	Assert( pgnoNull != PgnoOE( pfucb ) );
	
	//	get pgnoOE and depend on pgnoRoot
	//
	FUCBSetOwnExt( pfucbT );
	Call( csrOE.ErrGetNewPage( pfucbT->ppib, 
							   pfucbT->ifmp,
							   PgnoRoot( pfucbT ), 
							   ObjidFDP( pfucbT ), 
							   fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf | CPAGE::fPageSpaceTree,
							   pfucbT->u.pfcb->Tableclass() ) );
	Assert( latchWrite == csrOE.Latch() );
	FUCBResetOwnExt( pfucbT );

	Call( ErrBFDepend( csrOE.Cpage().PBFLatch(), pfucb->pcsrRoot->Cpage().PBFLatch() ) );

	//	get pgnoAE and depend on pgnoRoot
	//
	FUCBSetAvailExt( pfucbT );
	Call( csrAE.ErrGetNewPage( pfucbT->ppib, 
							   pfucbT->ifmp,
							   PgnoRoot( pfucbT ), 
							   ObjidFDP( pfucbT ),
							   fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf | CPAGE::fPageSpaceTree,
							   pfucbT->u.pfcb->Tableclass() ) );
	Assert( latchWrite == csrAE.Latch() );
	FUCBResetAvailExt( pfucbT );

	Call( ErrBFDepend( csrAE.Cpage().PBFLatch(), pfucb->pcsrRoot->Cpage().PBFLatch() ) );

	SPIUpgradeToWriteLatch( pfucb );

	dbtimeBefore = pfucb->pcsrRoot->Dbtime();
	Assert ( dbtimeNil != dbtimeBefore );
	//	set max dbtime for the three pages in all the pages
	//
	SPIDirtyAndSetMaxDbtime( pfucb->pcsrRoot, &csrOE, &csrAE );

	if ( pfucb->pcsrRoot->Dbtime() > dbtimeBefore )
		{
		//	log convert 
		//	convert can not fail after this operation
		if ( pfucb->u.pfcb->FDontLogSpaceOps() )
			{
			lgpos = lgposMin;
			err = JET_errSuccess;
			}
		else
			{
			err = ErrLGConvertFDP( pfucb, pcsrRoot, &sph, pgnoSecondaryFirst, cpgReq, dbtimeBefore, &lgpos );
			}
		}
	else
		{
		FireWall();
		err = ErrERRCheck( JET_errDbTimeCorrupted );
		}

	// see if we have to revert the time on the page
	if ( err < JET_errSuccess )
		{
		pfucb->pcsrRoot->RevertDbtime( dbtimeBefore );
		}
	Call ( err );
	
	pcsrRoot->Cpage().SetLgposModify( lgpos );
	csrOE.Cpage().SetLgposModify( lgpos );
	csrAE.Cpage().SetLgposModify( lgpos );

	//	perform convert operation
	//
	SPIPerformConvert(
			pfucbT,
			pcsrRoot,
			&csrAE,
			&csrOE,
			&sph,
			pgnoSecondaryFirst,
			cpgReq,
			rgextT,
			iextMac );

	AssertSPIPfucbOnRoot( pfucb );

HandleError:
	Assert( err != JET_errKeyDuplicate );
	csrAE.ReleasePage();
	csrOE.ReleasePage();
	Assert( pfucbT != pfucbNil );
	BTClose( pfucbT );
	return err;
	}

 
INLINE BOOL FSPIAllocateAllAvail(
	const CPG	cpgAvailExt,
	const CPG	cpgReq,
	const LONG	lPageFragment,
	const PGNO	pgnoFDP )
	{
	BOOL fAllocateAllAvail	= fTrue;

	// Use up all available pages if less than or equal to
	// requested amount.

	if ( cpgAvailExt > cpgReq )
		{
		if ( pgnoSystemRoot == pgnoFDP )
			{
			// If allocating extent from database, ensure we don't
			// leave behind an extent smaller than cpgSmallGrow,
			// because that's the smallest size a given table
			// will grow
			if ( cpgAvailExt >= cpgReq + cpgSmallGrow )
				fAllocateAllAvail = fFalse;
			}
		else if ( cpgReq < lPageFragment || cpgAvailExt >= cpgReq + lPageFragment )
			{
			// Don't use all if only requested a small amount
			// (ie. cpgReq < lPageFragment) or there is way more available
			// than we requested (ie. cpgAvailExt >= cpgReq+lPageFragment)
			fAllocateAllAvail = fFalse;
			}
		} 

	return fAllocateAllAvail;
	}


//	gets an extent from tree
//	if space not available in given tree, get from its parent
//
//	pfucbParent cursor placed on root of tree to get the extent from
//		root page should be RIW latched
//	
//	*pcpgReq input is number of pages requested; 
//			 output is number of pages granted
//	cpgMin is minimum neeeded
//	*ppgnoFirst input gives locality of extent needed
//				output is first page of allocated extent
//
LOCAL ERR ErrSPIGetExt(
	FUCB		*pfucbParent,
	CPG			*pcpgReq,
	CPG			cpgMin,
	PGNO		*ppgnoFirst,
	BOOL		fSPFlags = 0,
	UINT		fPageFlags = 0,
	OBJID		*pobjidFDP = NULL )
	{
	ERR			err;
	PIB			* const ppib			= pfucbParent->ppib;
	FCB			* const pfcb			= pfucbParent->u.pfcb;
	CPG 		cpgReq					= *pcpgReq;
	FUCB		*pfucbAE				= pfucbNil;
	SPCACHE		**ppspcache				= NULL;
	BOOL		fFoundNextAvailSE		= fFalse;
	DIB		 	dib;
	CPG			cpgAvailExt;
	PGNO		pgnoAELast;
	BYTE		rgbKey[sizeof(PGNO)];
	BOOKMARK	bm;

	//	check parameters.  If setting up new FDP, increment requested number of
	//	pages to account for consumption of first page to make FDP.
	//
	Assert( *pcpgReq > 0 || ( (fSPFlags & fSPNewFDP) && *pcpgReq == 0 ) );
	Assert( *pcpgReq >= cpgMin );
	AssertSPIPfucbOnRoot( pfucbParent );
	
#ifdef SPACECHECK
	Assert( !( ErrSPIValidFDP( ppib, pfucbParent->ifmp, PgnoFDP( pfucbParent ) ) < 0 ) );
#endif

	//	if a new FDP is requested, increment request count by FDP overhead,
	//	unless the count was smaller than the FDP overhead, in which case
	//	just allocate enough to satisfy the overhead (because the FDP will
	//	likely be small).
	//
	if ( fSPFlags & fSPNewFDP )
		{
		const CPG	cpgFDPMin = ( fSPFlags & fSPMultipleExtent ?
										cpgMultipleExtentMin :
										cpgSingleExtentMin );
		cpgMin = max( cpgMin, cpgFDPMin );
		*pcpgReq = max( *pcpgReq, cpgMin );

		Assert( NULL != pobjidFDP );
		}

	Assert( cpgMin > 0 );

	if ( !pfcb->FSpaceInitialized() )
		{
		SPIInitFCB( pfucbParent, fTrue );
		}

	//	if single extent optimization, then try to allocate from
	//	root page space map.  If cannot satisfy allcation, convert
	//	to multiple extent representation.
	if ( pfcb->PgnoOE() == pgnoNull )
		{
		AssertSPIPfucbOnRoot( pfucbParent );

		//	if any chance in satisfying request from extent
		//	then try to allcate requested extent, or minimum
		//	extent from first available extent.  Only try to 
		//	allocate the minimum request, to facillitate
		//	efficient space usage.
		//
		if ( cpgMin <= cpgSmallSpaceAvailMost )
			{
			SPACE_HEADER	sph;
			UINT			rgbitT;
			PGNO			pgnoAvail;
			DATA			data;
			INT				iT;

			//	get external header
			//
			NDGetExternalHeader( pfucbParent, pfucbParent->pcsrRoot );
			Assert( sizeof( SPACE_HEADER ) == pfucbParent->kdfCurr.data.Cb() );

			//	get single extent allocation information
			//
			UtilMemCpy( &sph, pfucbParent->kdfCurr.data.Pv(), sizeof(sph) );

			const PGNO	pgnoFDP			= PgnoFDP( pfucbParent );
			const CPG	cpgSingleExtent	= min( sph.CpgPrimary(), cpgSmallSpaceAvailMost+1 );	//	+1 because pgnoFDP is not represented in the single extent space map
			Assert( cpgSingleExtent > 0 );
			Assert( cpgSingleExtent <= cpgSmallSpaceAvailMost+1 );

			//	find first fit
			//
			//	make mask for minimum request
			//
			Assert( cpgMin > 0 );
			Assert( cpgMin <= 32 );
			for ( rgbitT = 1, iT = 1; iT < cpgMin; iT++ )
				{
				rgbitT = (rgbitT<<1) + 1;
				}

			for( pgnoAvail = pgnoFDP + 1;
				pgnoAvail + cpgMin <= pgnoFDP + cpgSingleExtent;
				pgnoAvail++, rgbitT <<= 1 )
				{
				Assert( rgbitT != 0 );
				if ( ( rgbitT & sph.RgbitAvail() ) == rgbitT ) 
					{
					SPIUpgradeToWriteLatch( pfucbParent );

					sph.SetRgbitAvail( sph.RgbitAvail() ^ rgbitT );
					data.SetPv( &sph );
					data.SetCb( sizeof(sph) );
					CallR( ErrNDSetExternalHeader(
									pfucbParent,
									pfucbParent->pcsrRoot,
									&data,
									( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );

					//	set up allocated extent as FDP if requested
					//
					Assert( pgnoAvail != pgnoNull );
					*ppgnoFirst = pgnoAvail;
					*pcpgReq = cpgMin;
					goto NewFDP;
					}
				}
			}

		CallR( ErrSPIConvertToMultipleExtent( pfucbParent, *pcpgReq, cpgMin ) ); 
		}

	//	for secondary extent allocation, only the normal Btree operations
	//	are logged. For allocating a new FDP, a special create FDP
	//	record is logged instead, since the new FDP and space pages need
	//	to be initialized as part of recovery.
	//
	//	move to available extents
	//
	CallR( ErrSPIOpenAvailExt( ppib, pfcb, &pfucbAE ) );
	Assert( pfcb == pfucbAE->u.pfcb );

#ifdef CONVERT_VERBOSE
		CONVPrintF2( "\n(a)Dest DB (pgnoFDP %d): Looking for %d pages...", 
			PgnoFDP( pfucbAE ), cpgMin );
#endif
		

	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;

	//	begin search for first extent with size greater than request.
	//	Allocate secondary extent recursively until satisfactory extent found
	//
	//	most of the time, we simply start searching from the beginning of the AvailExt tree,
	//	but if this is the db root, optimise for the common case (request for an SE) by skipping
	//	small AvailExt nodes
	PGNO	pgnoSeek;
	pgnoSeek = ( cpgMin >= cpageSEDefault ? pfcb->PgnoNextAvailSE() : pgnoNull );
	KeyFromLong( rgbKey, pgnoSeek );

	dib.dirflag = fDIRFavourNext;

	if ( ( err = ErrBTDown( pfucbAE, &dib, latchReadTouch ) ) < 0 )
		{
		Assert( err != JET_errNoCurrentRecord );
		if ( err == JET_errRecordNotFound )
			{
			//	no record in availExt tree
			//
			goto GetFromSecondaryExtent;
			}
		#ifdef DEBUG
			DBGprintf( "ErrSPGetExt could not down into available extent tree.\n" );
		#endif
		goto HandleError;
		}


	//	loop through extents looking for one large enough for allocation
	//
	do
		{
		Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
		cpgAvailExt = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
		Assert( cpgAvailExt >= 0 );

		Assert( pfucbAE->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );

		if ( 0 == cpgAvailExt )
			{
			//	We might have zero-sized extents if we crashed in ErrSPIAddToAvailExt().
			//	Simply delete such extents.
			Call( ErrBTFlagDelete(		// UNDONE: Synchronously remove the node
						pfucbAE,
						fDIRNoVersion | ( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
			Pcsr( pfucbAE )->Downgrade( latchReadTouch );
			}
		else
			{
			if ( !fFoundNextAvailSE
				&& cpgAvailExt >= cpageSEDefault )
				{
				pfcb->SetPgnoNextAvailSE( pgnoAELast - cpgAvailExt + 1 );
				fFoundNextAvailSE = fTrue;
				}

			if ( cpgAvailExt >= cpgMin )
				{
				//	if no extent with cpg >= cpageSEDefault, then ensure NextAvailSE
				//	pointer is at least as far as we've scanned
				if ( !fFoundNextAvailSE
					&& pfcb->PgnoNextAvailSE() <= pgnoAELast )
					{
					pfcb->SetPgnoNextAvailSE( pgnoAELast + 1 );
					}
				goto AllocateCurrent;
				}
			}

		err = ErrBTNext( pfucbAE, fDIRNull );
		}
	while ( err >= 0 );

	if ( err != JET_errNoCurrentRecord )
		{
		#ifdef DEBUG
			DBGprintf( "ErrSPGetExt could not scan available extent tree.\n" );
		#endif
		Assert( err < 0 );
		goto HandleError;
		}

	if ( !fFoundNextAvailSE )
		{
		//	didn't find any extents with at least cpageSEDefault pages, so
		//	set pointer to beyond last extent
		pfcb->SetPgnoNextAvailSE( pgnoAELast + 1 );
		}

GetFromSecondaryExtent:
	BTUp( pfucbAE );
	Call( ErrSPIGetSE(
			pfucbParent,
			pfucbAE,
			*pcpgReq, 
			cpgMin,
			fSPFlags & fSPSplitting ) );
	Assert( Pcsr( pfucbAE )->FLatched() );
	Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(CPG) );
	cpgAvailExt = *(UnalignedLittleEndian< CPG > *) pfucbAE->kdfCurr.data.Pv();
	Assert( cpgAvailExt > 0 );
	Assert( cpgAvailExt >= cpgMin );

	Assert( pfucbAE->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );

AllocateCurrent:
	*ppgnoFirst = pgnoAELast - cpgAvailExt + 1;

	if ( FSPIAllocateAllAvail( cpgAvailExt, *pcpgReq, PinstFromPpib( ppib )->m_lPageFragment, pfcb->PgnoFDP() ) )
		{
		*pcpgReq = cpgAvailExt;

		// UNDONE: *pcpgReq may actually be less than cpgMin, because
		// some pages may have been used up if split occurred while
		// updating AvailExt.  However, as long as there's at least
		// one page, this is okay, because the only ones to call this
		// function are GetPage() (for split) and CreateDirectory()
		// (for CreateTable/Index).  The former only ever asks for
		// one page at a time, and the latter can deal with getting
		// only one page even though it asked for more.
		// Assert( *pcpgReq >= cpgMin );
		Assert( cpgAvailExt > 0 );
		
		Call( ErrBTFlagDelete(		// UNDONE: Synchronously remove the node
					pfucbAE,
					fDIRNoVersion | ( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
		}
	else
		{
		DATA	data;
		
		//	*pcpgReq is already set to the return value
		//
		Assert( cpgAvailExt > *pcpgReq );
		
		LittleEndian<CPG>	le_cpg;
		le_cpg = cpgAvailExt - *pcpgReq;
		data.SetCb( sizeof(PGNO) );
		data.SetPv( &le_cpg );
		Call( ErrBTReplace(
					pfucbAE,
					data,
					fDIRNoVersion | ( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
		CallS( err );		// do we need the following stmt? 
		err = JET_errSuccess;
		}

	BTUp( pfucbAE );


NewFDP:
	//	initialize extent as new tree, including support for
	//	localized space allocation.
	//
	if ( fSPFlags & fSPNewFDP )
		{
		Assert( PgnoFDP( pfucbParent ) != *ppgnoFirst );
		if ( fSPFlags & fSPMultipleExtent )
			{
			Assert( *pcpgReq >= cpgMultipleExtentMin );
			}
		else
			{
			Assert( *pcpgReq >= cpgSingleExtentMin );
			}

		//	database root is allocated by DBInitDatabase
		//
		Assert( pgnoSystemRoot != *ppgnoFirst );

		VEREXT	verext;
		verext.pgnoFDP = PgnoFDP( pfucbParent );
		verext.pgnoChildFDP = *ppgnoFirst;
		verext.pgnoFirst = *ppgnoFirst;
		verext.cpgSize = *pcpgReq;

		if ( !( fSPFlags & fSPUnversionedExtent ) )
			{
			VER *pver = PverFromIfmp( pfucbParent->ifmp );
			Call( pver->ErrVERFlag( pfucbParent, operAllocExt, &verext, sizeof(verext) ) );
			}
		
		Call( ErrSPCreate(
					ppib,
					pfucbParent->ifmp,
					PgnoFDP( pfucbParent ),
					*ppgnoFirst,
					*pcpgReq,
					fSPFlags,
					fPageFlags,
					pobjidFDP ) );
		Assert( *pobjidFDP > objidSystemRoot );
						
		//	reduce *pcpgReq by pages allocated for tree root
		//
		if ( fSPFlags & fSPMultipleExtent )
			{
			(*pcpgReq) -= cpgMultipleExtentMin;
			}
		else
			{
			(*pcpgReq) -= cpgSingleExtentMin;
			}

#ifdef DEBUG
		if ( 0 == ppib->level )
			{
			FMP		*pfmp	= &rgfmp[pfucbParent->ifmp];
			Assert( fSPFlags & fSPUnversionedExtent );
			Assert( dbidTemp == pfmp->Dbid()
				|| pfmp->FCreatingDB() );
			}
#endif			
		}

	//	assign error
	//
	err = JET_errSuccess;

#ifdef SPACE_TRACE
	if ( (fSPFlags & fSPNewFDP) )
		{
		DBGprintf( "get space %lu at %lu for FDP from %d.%lu\n", 
		  *pcpgReq + ( fSPFlags & fSPMultipleExtent ? cpgMultipleExtentMin : cpgSingleExtentMin ),
		  *ppgnoFirst, 
		  pfucbParent->ifmp, 
		  PgnoFDP( pfucbParent ) );
		}
	else
		{
		DBGprintf( "get space %lu at %lu from %d.%lu\n", 
			*pcpgReq, 
			*ppgnoFirst, 
			pfucbParent->ifmp, 
			PgnoFDP( pfucbParent ) );
		}
#endif

#ifdef DEBUG
	if ( PinstFromPpib( ppib )->m_plog->m_fDBGTraceBR )
		{
		INT cpg = 0;
		for ( ; 
				cpg < ( (fSPFlags & fSPNewFDP) ? 
						*pcpgReq + ( (fSPFlags & fSPMultipleExtent) ? 3 : 1 ) : *pcpgReq ); 
				cpg++ )
			{
			char sz[256];
			sprintf( sz, "ALLOC ONE PAGE (%d:%ld) %d:%ld",
				pfucbParent->ifmp, PgnoFDP( pfucbParent ),
				pfucbParent->ifmp, *ppgnoFirst + cpg );
			CallS( PinstFromPpib( ppib )->m_plog->ErrLGTrace( ppib, sz ) );
			}
		}
#endif

HandleError:
	if ( pfucbAE != pfucbNil )
		BTClose( pfucbAE );
	return err;
	}


ERR ErrSPGetExt(
	FUCB	*pfucb,
	PGNO	pgnoFDP,
	CPG		*pcpgReq,
	CPG		cpgMin,
	PGNO	*ppgnoFirst,
	BOOL	fSPFlags,
	UINT	fPageFlags,
	OBJID	*pobjidFDP )
	{
	ERR 	err;
	FUCB	*pfucbParent = pfucbNil;

	Assert( !Pcsr( pfucb )->FLatched() );

	//	open cursor on Parent and RIW latch root page
	//
	CallR( ErrBTIOpenAndGotoRoot( pfucb->ppib, pgnoFDP, pfucb->ifmp, &pfucbParent ) );
					 
	//  allocate an extent
	//
	err = ErrSPIGetExt( pfucbParent, pcpgReq, cpgMin, ppgnoFirst, fSPFlags, fPageFlags, pobjidFDP );
	Assert( Pcsr( pfucbParent ) == pfucbParent->pcsrRoot );
	
	//	latch may have been upgraded to write latch
	//	by single extent space allocation operation.
	//
	Assert( pfucbParent->pcsrRoot->Latch() == latchRIW 
		|| pfucbParent->pcsrRoot->Latch() == latchWrite );
	pfucbParent->pcsrRoot->ReleasePage();
	pfucbParent->pcsrRoot = pcsrNil;

	Assert( pfucbParent != pfucbNil );
	BTClose( pfucbParent );
	return err;
	}


#ifdef DEBUG	
INLINE ERR ErrSPIFindOE( PIB *ppib, FCB *pfcb, const PGNO pgnoFirst, const PGNO pgnoLast )
	{
	ERR			err;
	FUCB		*pfucbOE;
	PGNO		pgnoOELast;
	CPG			cpgOESize;
	BOOKMARK	bm;
	DIB			dib;
	BYTE		rgbKey[sizeof(PGNO)];
	
	CallR( ErrSPIOpenOwnExt( ppib, pfcb, &pfucbOE ) );
	
	//	find bounds of owned extent which contains extent to be freed
	//
	KeyFromLong( rgbKey, pgnoFirst );
	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;
	dib.dirflag = fDIRFavourNext;
	Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );
	if ( wrnNDFoundLess == err )
		{
		//	landed on node previous to one we want
		//	move next
		//
		Assert( Pcsr( pfucbOE )->Cpage().Clines() - 1 ==
					Pcsr( pfucbOE )->ILine() );
		Assert( pgnoNull != Pcsr( pfucbOE )->Cpage().PgnoNext() );

		Call( ErrBTNext( pfucbOE, fDIRNull ) );
		}

	Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( &pgnoOELast, pfucbOE->kdfCurr.key );
	Assert( pfucbOE->kdfCurr.data.Cb() == sizeof(CPG) );
	cpgOESize = *(UnalignedLittleEndian< CPG > *)pfucbOE->kdfCurr.data.Pv();

	// Verify that the extent to be freed is contained entirely within
	// this OwnExt node (since we don't allow coalescing of AvailExt
	// nodes across OwnExt boundaries).
	Assert( pgnoOELast - cpgOESize + 1 <= pgnoFirst );
	Assert( pgnoOELast >= pgnoLast );

	err = JET_errSuccess;
	
HandleError:
	BTClose( pfucbOE );
	
	return err;
	}
#endif	// DEBUG	
	

//	ErrSPGetPage
//	========================================================================
//	ERR ErrSPGetPage( FUCB *pfucb, PGNO *ppgnoLast )
//
//	Allocates page from FUCB cache. If cache is nil,
//	allocate from available extents. If availalbe extent tree is empty, 
//	a secondary extent is allocated from the parent FDP to 
//	satisfy the page request. A page closest to 
//	*ppgnoLast is allocated. If *ppgnoLast is pgnoNull, 
//	first free page is allocated
//
//	PARAMETERS	
//		pfucb  		FUCB providing FDP page number and process identifier block
//					cursor should be on root page RIW latched
//		ppgnoLast   may contain page number of last allocated page on
//		   			input, on output contains the page number of the allocated page
//
//-
ERR ErrSPGetPage( FUCB *pfucb, PGNO *ppgnoLast )
	{
	ERR			err;
	PIB			* const ppib			= pfucb->ppib;
	FCB			* const pfcb			= pfucb->u.pfcb;
	FUCB 		*pfucbAE				= pfucbNil;
	CPG			cpgAvailExt;
	PGNO		pgnoAELast;
	SPCACHE		**ppspcache				= NULL;
	BYTE		rgbKey[sizeof(PGNO)];
	BOOKMARK	bm;
	DIB			dib;
#ifdef DEBUG
	PGNO		pgnoSave				= *ppgnoLast;
#endif
	
	//	check for valid input
	//
	Assert( ppgnoLast != NULL );
	Assert( *ppgnoLast != pgnoNull );
	
	//	check FUCB work area for active extent and allocate first available
	//	page of active extent
	//
	if ( FFUCBSpace( pfucb ) )
		{
		const BOOL	fAvailExt	= FFUCBAvailExt( pfucb );
		if ( NULL == pfucb->u.pfcb->Psplitbuf( fAvailExt ) )
			{
			CSR				*pcsrRoot	= pfucb->pcsrRoot;
			SPLIT_BUFFER	spbuf;
			DATA			data;

			AssertSPIPfucbOnSpaceTreeRoot( pfucb, pcsrRoot );

			UtilMemCpy( &spbuf, PspbufSPISpaceTreeRootPage( pfucb, pcsrRoot ), sizeof(SPLIT_BUFFER) );

			CallR( spbuf.ErrGetPage( ppgnoLast, fAvailExt ) );

			Assert( latchRIW == pcsrRoot->Latch() );
			pcsrRoot->UpgradeFromRIWLatch();

			data.SetPv( &spbuf );
			data.SetCb( sizeof(spbuf) );
			err = ErrNDSetExternalHeader(
						pfucb,
						pcsrRoot,
						&data,
						( pfucb->u.pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) );

			//	reset to RIW latch
			pcsrRoot->Downgrade( latchRIW );

			return err;
			}
		else
			{
			AssertSPIPfucbOnSpaceTreeRoot( pfucb, pfucb->pcsrRoot );
			SPIReportGetPageFromSplitBuffer( pfucb );
			return pfucb->u.pfcb->Psplitbuf( fAvailExt )->ErrGetPage( ppgnoLast, fAvailExt );
			}
		}

	//	check for valid input when alocating page from FDP
	//
#ifdef SPACECHECK
	Assert( !( ErrSPIValidFDP( pfucb->ppib, pfucb->ifmp, PgnoFDP( pfucb ) ) < 0 ) );
	if ( 0 != *ppgnoLast )
		{
		CallS( ErrSPIWasAlloc( pfucb, *ppgnoLast, (CPG)1 ) );
		}
#endif

	if ( !pfcb->FSpaceInitialized() )
		{
		SPIInitFCB( pfucb, fTrue );
		}

	//	if single extent optimization, then try to allocate from
	//	root page space map.  If cannot satisfy allcation, convert
	//	to multiple extent representation.
	//
	if ( pfcb->PgnoOE() == pgnoNull )
		{
		SPACE_HEADER	sph;
		UINT			rgbitT;
		PGNO			pgnoAvail;
		DATA			data;

		AssertSPIPfucbOnRoot( pfucb );

		//	if any chance in satisfying request from extent
		//	then try to allcate requested extent, or minimum
		//	extent from first available extent.  Only try to 
		//	allocate the minimum request, to facillitate
		//	efficient space usage.
		//
		//	get external header
		//
		NDGetExternalHeader( pfucb, pfucb->pcsrRoot );
		Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );

		//	get single extent allocation information
		//
		UtilMemCpy( &sph, pfucb->kdfCurr.data.Pv(), sizeof(sph) );

		const PGNO	pgnoFDP			= PgnoFDP( pfucb );
		const CPG	cpgSingleExtent	= min( sph.CpgPrimary(), cpgSmallSpaceAvailMost+1 );	//	+1 because pgnoFDP is not represented in the single extent space map
		Assert( cpgSingleExtent > 0 );
		Assert( cpgSingleExtent <= cpgSmallSpaceAvailMost+1 );

		//	allocate first page
		//
		for( pgnoAvail = pgnoFDP + 1, rgbitT = 1;
			pgnoAvail <= pgnoFDP + cpgSingleExtent - 1;
			pgnoAvail++, rgbitT <<= 1 )
			{
			Assert( rgbitT != 0 );
			if ( rgbitT & sph.RgbitAvail() ) 
				{
				Assert( ( rgbitT & sph.RgbitAvail() ) == rgbitT );

				//  write latch page before update
				//
				SPIUpgradeToWriteLatch( pfucb );

				sph.SetRgbitAvail( sph.RgbitAvail() ^ rgbitT );
				data.SetPv( &sph );
				data.SetCb( sizeof(sph) );
				CallR( ErrNDSetExternalHeader(
							pfucb,
							pfucb->pcsrRoot,
							&data,
							( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );

				//	set output parameter and done
				//
				*ppgnoLast = pgnoAvail;
				goto Done;
				}
			}

		CallR( ErrSPIConvertToMultipleExtent( pfucb, 1, 1 ) );
		}

	//	open cursor on available extent tree
	//
	AssertSPIPfucbOnRoot( pfucb );
	CallR( ErrSPIOpenAvailExt( pfucb->ppib, pfcb, &pfucbAE ) );
	Assert( pfcb == pfucbAE->u.pfcb );

	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;

	//	get node of next contiguous page
	//
FindPage:
	KeyFromLong( rgbKey, *ppgnoLast );

	dib.dirflag = fDIRNull;

	if ( ( err = ErrBTDown( pfucbAE, &dib, latchReadTouch ) ) < 0 )
		{
		Assert( err != JET_errNoCurrentRecord );
		if ( JET_errRecordNotFound == err )
			{
			//	no node in available extent tree,
			//	get a secondary extent
			//
			Assert( pgnoSave == *ppgnoLast );
			BTUp( pfucbAE );
			Call( ErrSPIGetSE( pfucb, pfucbAE, (CPG)1, (CPG)1, fTrue ) );
			Assert( Pcsr( pfucbAE )->FLatched() );
			}
		else
			{
			#ifdef DEBUG
				DBGprintf( "ErrSPGetPage could not go down into available extent tree.\n" );
			#endif
			Call( err );
			}
		}

	else if ( JET_errSuccess == err )
		{
		//	page in use is also in AvailExt
		AssertSz( fFalse, "AvailExt corrupted." );
		Call( ErrERRCheck( JET_errSPAvailExtCorrupted ) );
		}
	else
		{
		//	keep locality of reference
		//	get page closest to *ppgnoLast
		//
		
		//	we always favour FoundGreater, except in the pathological case of no more nodes
		//	greater, in which case we return FoundLess
		Assert( wrnNDFoundGreater == err || wrnNDFoundLess == err );

		Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
		cpgAvailExt = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
		Assert ( cpgAvailExt >= 0 );
		if ( 0 == cpgAvailExt )
			{
			//	We might have zero-sized extents if we crashed in ErrSPIAddToAvailExt().
			//	Simply delete such extents and retry.
			Call( ErrBTFlagDelete(		// UNDONE: Synchronously remove the node
						pfucbAE,
						fDIRNoVersion | ( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
			BTUp( pfucbAE );
			goto FindPage;
			}
		}


	//	allocate first page in node and return code
	//
	Assert( err >= 0 );
	Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
	cpgAvailExt = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
	Assert( cpgAvailExt > 0 );

	Assert( pfucbAE->kdfCurr.key.Cb() == sizeof( PGNO ) );
	LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );

	if ( *ppgnoLast >= pgnoAELast - cpgAvailExt + 1
		&& *ppgnoLast <= pgnoAELast )
		{
		//	page in use is also in AvailExt
		AssertSz( fFalse, "AvailExt corrupted." );
		Call( ErrERRCheck( JET_errSPAvailExtCorrupted ) );
		}

	*ppgnoLast = pgnoAELast - cpgAvailExt + 1;

	//	do not return the same page
	//
	Assert( *ppgnoLast != pgnoSave );

	if ( --cpgAvailExt == 0 )
		{
		Call( ErrBTFlagDelete(		// UNDONE: Synchronously remove the node
					pfucbAE,
					fDIRNoVersion | ( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
		}
	else
		{
		DATA	data;
		LittleEndian<CPG> le_cpgAvailExt;

		le_cpgAvailExt = cpgAvailExt;
		data.SetPv( &le_cpgAvailExt );
		data.SetCb( sizeof(PGNO) );
		Call( ErrBTReplace(
					pfucbAE,
					data,
					fDIRNoVersion | ( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
		}

	BTUp( pfucbAE );
	err = JET_errSuccess;


Done:

#ifdef SPACE_TRACE
	DBGprintf( "get space 1 at %lu from %d.%lu\n", *ppgnoLast, pfucb->ifmp, PgnoFDP( pfucb ) );
#endif

#ifdef DEBUG
	if ( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fDBGTraceBR )
		{
		char sz[256];
		sprintf( sz, "ALLOC ONE PAGE (%d:%ld) %d:%ld",
				pfucb->ifmp, pfcb->PgnoFDP(),
				pfucb->ifmp, *ppgnoLast );
		CallS( PinstFromIfmp( pfucb->ifmp )->m_plog->ErrLGTrace( pfucb->ppib, sz ) );
		}
#endif

HandleError:
	if ( pfucbAE != pfucbNil )
		BTClose( pfucbAE );
	return err;
	}


LOCAL ERR ErrSPIFreeSEToParent(
	FUCB		*pfucb,
	FUCB		*pfucbOE,
	FUCB		*pfucbAE,
	const PGNO	pgnoLast,
	const CPG	cpgSize )
	{
	ERR			err;
	FCB			*pfcb = pfucb->u.pfcb;
	FUCB		*pfucbParent = pfucbNil;
	FCB			*pfcbParent;
	BOOL		fState;
	BOOKMARK	bm;
	DIB 		dib;
	BYTE		rgbKey[sizeof(PGNO)];
			
	Assert( pfcbNil != pfcb );

	//	get parentFDP's root pgno
	//	cursor passed in should be at root of tree 
	//	so we can access pgnoParentFDP from the external header
	const PGNO	pgnoParentFDP = PgnoSPIParentFDP( pfucb );
	if ( pgnoParentFDP == pgnoNull )
		{
		//	UNDONE:	free secondary extents to device
		return JET_errSuccess;
		}
	
	//	parent must always be in memory
	//
	pfcbParent = FCB::PfcbFCBGet( pfucb->ifmp, pgnoParentFDP, &fState );
	Assert( pfcbParent != pfcbNil );
	Assert( fFCBStateInitialized == fState );
	Assert( !pfcb->FTypeNull() );
			
	if ( pfcb->FTypeSecondaryIndex() || pfcb->FTypeLV() )
		{
		Assert( pfcbParent->FTypeTable() );
		}
	else
		{
		Assert( pfcbParent->FTypeDatabase() );
		Assert( !pfcbParent->FDeletePending() );
		Assert( !pfcbParent->FDeleteCommitted() );
		}

	//	delete available extent node
	//
	Call( ErrBTFlagDelete(		// UNDONE: Synchronously remove the node
				pfucbAE,
				fDIRNoVersion | ( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
	BTUp( pfucbAE );

	//	seek to owned extent node and delete it
	//
	KeyFromLong( rgbKey, pgnoLast );
	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;
	dib.dirflag = fDIRNull;
	Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );
				
	Call( ErrBTFlagDelete(		// UNDONE: Synchronously remove the node
				pfucbOE,
				fDIRNoVersion | ( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
	BTUp( pfucbOE );

	//	free extent to parent FDP
	//
	//	open cursor on parent
	//	access root page with RIW latch
	//
	Call( ErrBTOpen( pfucb->ppib, pfcbParent, &pfucbParent ) );
	Call( ErrBTIGotoRoot( pfucbParent, latchRIW ) );
	
	pfucbParent->pcsrRoot = Pcsr( pfucbParent );
	Assert( !FFUCBSpace( pfucbParent ) );
	err = ErrSPFreeExt( pfucbParent, pgnoLast - cpgSize + 1, cpgSize );
	pfucbParent->pcsrRoot = pcsrNil;

HandleError:
	if ( pfucbNil != pfucbParent )
		{
		BTClose( pfucbParent );
		}

	pfcbParent->Release();

	return err;
	}


LOCAL VOID SPIReportLostPages(
	const IFMP	ifmp,
	const OBJID	objidFDP,
	const PGNO	pgnoLast,
	const CPG	cpgLost )
	{
	CHAR		szStartPagesLost[16];
	CHAR		szEndPagesLost[16];
	CHAR		szObjidFDP[16];
	const CHAR	*rgszT[4];

	sprintf( szStartPagesLost, "%d", pgnoLast - cpgLost + 1 );
	sprintf( szEndPagesLost, "%d", pgnoLast );
	sprintf( szObjidFDP, "%d", objidFDP );
		
	rgszT[0] = rgfmp[ifmp].SzDatabaseName();
	rgszT[1] = szStartPagesLost;
	rgszT[2] = szEndPagesLost;
	rgszT[3] = szObjidFDP;
	
	UtilReportEvent(
			eventWarning,
			SPACE_MANAGER_CATEGORY,
			SPACE_LOST_ON_FREE_ID,
			4,
			rgszT,
			0,
			NULL,
			PinstFromIfmp( ifmp ) );
	}


//	ErrSPFreeExt
//	========================================================================
//	ERR ErrSPFreeExt( FUCB *pfucb, PGNO pgnoFirst, CPG cpgSize )
//
//	Frees an extent to an FDP.	The extent, starting at page pgnoFirst
//	and cpgSize pages long, is added to available extent of the FDP.  If the
//	extent freed is a complete secondary extent of the FDP, or can be
//	coalesced with other available extents to form a complete secondary
//	extent, the complete secondary extent is freed to the parent FDP.
//
//	Besides, if the freed extent is contiguous with the FUCB space cache 
//	in pspbuf, the freed extent is added to the FUCB cache. Also, when 
//	an extent is freed recursively to the parentFDP, the FUCB on the parent 
//	shares the same FUCB cache.
//
//	PARAMETERS	pfucb			tree to which the extent is freed,
//								cursor should have currency on root page [RIW latched]
// 				pgnoFirst  		page number of first page in extent to be freed
// 				cpgSize			number of pages in extent to be freed
//
//
//	SIDE EFFECTS
//	COMMENTS
//-
ERR ErrSPFreeExt( FUCB *pfucb, PGNO pgnoFirst, CPG cpgSize )
	{
	ERR			err;
	PIB			* const ppib	= pfucb->ppib;
	FCB			* const pfcb	= pfucb->u.pfcb;
	PGNO  		pgnoLast		= pgnoFirst + cpgSize - 1;
	BOOL		fRootLatched	= fFalse;
	BOOL		fCoalesced		= fFalse;

	// FDP available extent and owned extent operation variables
	//
	BOOKMARK	bm;
	DIB 		dib;
	BYTE		rgbKey[sizeof(PGNO)];

	// owned extent and avail extent variables
	//
	FUCB 		*pfucbAE		= pfucbNil;
	FUCB		*pfucbOE		= pfucbNil;
	PGNO		pgnoOELast;
	CPG			cpgOESize;
	PGNO		pgnoAELast;
	CPG			cpgAESize;

	// check for valid input
	//
	Assert( cpgSize > 0 );

#ifdef SPACECHECK
	CallS( ErrSPIValidFDP( ppib, pfucb->ifmp, PgnoFDP( pfucb ) ) );
#endif

	//	if in space tree, return page back to split buffer
	if ( FFUCBSpace( pfucb ) ) 
		{
		const BOOL	fAvailExt	= FFUCBAvailExt( pfucb );

		//	must be returning space due to split failure
		Assert( 1 == cpgSize );

		if ( NULL == pfucb->u.pfcb->Psplitbuf( fAvailExt ) )
			{
			CSR				*pcsrRoot	= pfucb->pcsrRoot;
			SPLIT_BUFFER	spbuf;
			DATA			data;

			AssertSPIPfucbOnSpaceTreeRoot( pfucb, pcsrRoot );

			UtilMemCpy( &spbuf, PspbufSPISpaceTreeRootPage( pfucb, pcsrRoot ), sizeof(SPLIT_BUFFER) );

			spbuf.ReturnPage( pgnoFirst );
		
			Assert( latchRIW == pcsrRoot->Latch() );
			pcsrRoot->UpgradeFromRIWLatch();

			data.SetPv( &spbuf );
			data.SetCb( sizeof(spbuf) );
			err = ErrNDSetExternalHeader(
						pfucb,
						pcsrRoot,
						&data,
						( pfucb->u.pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) );

			//	reset to RIW latch
			pcsrRoot->Downgrade( latchRIW );
			}
		else
			{
			AssertSPIPfucbOnSpaceTreeRoot( pfucb, pfucb->pcsrRoot );
			pfucb->u.pfcb->Psplitbuf( fAvailExt )->ReturnPage( pgnoFirst );
			err = JET_errSuccess;
			}

		return err;
		}

	//	if caller did not have root page latched, get root page
	//
	if ( pfucb->pcsrRoot == pcsrNil )
		{
		Assert( !Pcsr( pfucb )->FLatched() );
		CallR( ErrBTIGotoRoot( pfucb, latchRIW ) );
		pfucb->pcsrRoot = Pcsr( pfucb );
		fRootLatched = fTrue;
		}
	else
		{
		Assert( pfucb->pcsrRoot->Pgno() == PgnoRoot( pfucb ) );
		Assert( pfucb->pcsrRoot->Latch() == latchRIW
			|| ( pfucb->pcsrRoot->Latch() == latchWrite && pgnoNull == pfcb->PgnoOE() ) ); // SINGLE EXTEND
		}

	if ( !pfcb->FSpaceInitialized() )
		{
		SPIInitFCB( pfucb, fTrue );
		}

#ifdef SPACECHECK
	CallS( ErrSPIWasAlloc( pfucb, pgnoFirst, cpgSize ) == JET_errSuccess );
#endif

	//	if single extent format, then free extent in external header
	//
	if ( pfcb->PgnoOE() == pgnoNull )
		{
		SPACE_HEADER	sph;
		UINT			rgbitT;
		INT				iT;
		DATA			data;
		
		AssertSPIPfucbOnRoot( pfucb );
		Assert( cpgSize <= cpgSmallSpaceAvailMost );
		Assert( pgnoFirst > PgnoFDP( pfucb ) );								//	can't be equal, because then you'd be freeing root page to itself
		Assert( pgnoFirst - PgnoFDP( pfucb ) <= cpgSmallSpaceAvailMost );	//	extent must start and end within single-extent range
		Assert( pgnoFirst + cpgSize - 1 - PgnoFDP( pfucb ) <= cpgSmallSpaceAvailMost );

		//  write latch page before update
		//
		if ( latchWrite != pfucb->pcsrRoot->Latch() )
			{
			SPIUpgradeToWriteLatch( pfucb );
			}

		//	get external header
		//
		NDGetExternalHeader( pfucb, pfucb->pcsrRoot );
		Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );
		
		//	get single extent allocation information
		//
		UtilMemCpy( &sph, pfucb->kdfCurr.data.Pv(), sizeof(sph) );

		//	make mask for extent to free
		//
		for ( rgbitT = 1, iT = 1; iT < cpgSize; iT++ )
			{
			rgbitT = ( rgbitT << 1 ) + 1;
			}
		rgbitT <<= ( pgnoFirst - PgnoFDP( pfucb ) - 1 );
		sph.SetRgbitAvail( sph.RgbitAvail() | rgbitT );
		data.SetPv( &sph );
		data.SetCb( sizeof(sph) );
		Call( ErrNDSetExternalHeader(
					pfucb,
					pfucb->pcsrRoot,
					&data,
					( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );

		goto HandleError;  //  done
		}

	AssertSPIPfucbOnRoot( pfucb );

	//	open owned extent tree
	//
	Call( ErrSPIOpenOwnExt( ppib, pfcb, &pfucbOE ) );
	Assert( pfcb == pfucbOE->u.pfcb );

	//	find bounds of owned extent which contains extent to be freed
	//
	KeyFromLong( rgbKey, pgnoFirst );
	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;
	dib.dirflag = fDIRFavourNext;
	Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );
	if ( err == wrnNDFoundLess )
		{
		//	landed on node previous to one we want
		//	move next
		//
		Assert( Pcsr( pfucbOE )->Cpage().Clines() - 1 ==
					Pcsr( pfucbOE )->ILine() );
		Assert( pgnoNull != Pcsr( pfucbOE )->Cpage().PgnoNext() );

		Call( ErrBTNext( pfucbOE, fDIRNull ) );
		}

	Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( &pgnoOELast, pfucbOE->kdfCurr.key );
	Assert( pfucbOE->kdfCurr.data.Cb() == sizeof(PGNO) );
	cpgOESize = *(UnalignedLittleEndian< CPG > *)pfucbOE->kdfCurr.data.Pv();

	// Verify that the extent to be freed is contained entirely within
	// this OwnExt node (since we don't allow coalescing of AvailExt
	// nodes across OwnExt boundaries).
	if ( pgnoFirst > pgnoOELast
		|| pgnoLast < pgnoOELast - cpgOESize + 1 )
		{
		//	SPACE CORRUPTION!! One possibility is a crash in ErrSPIReservePagesForSplit()
		//	after inserting into the SPLIT_BUFFER but before being able to insert into the OwnExt.
		//	This space is now likely gone forever.  Log an event, but allow to continue.
		err = JET_errSuccess;
		goto HandleError;
		}
	else if ( pgnoOELast - cpgOESize + 1 > pgnoFirst
		|| pgnoOELast < pgnoLast )
		{
		FireWall();
		Call( ErrERRCheck( JET_errSPOwnExtCorrupted ) );
		}
	
	BTUp( pfucbOE );

	//	if available extent empty, add extent to be freed.	
	//	Otherwise, coalesce with left extents by deleting left extents 
	//	and augmenting size. 
	//	Coalesce right extent replacing size of right extent. 
	//
	Call( ErrSPIOpenAvailExt( ppib, pfcb, &pfucbAE ) );
	Assert( pfcb == pfucbAE->u.pfcb );

	if ( pgnoLast == pgnoOELast
		&& cpgSize == cpgOESize )
		{
		//	we're freeing an entire extent, so no point
		//	trying to coalesce
		goto InsertExtent;
		}

FindPage:
	KeyFromLong( rgbKey, pgnoFirst - 1 );
	Assert( bm.key.Cb() == sizeof(PGNO) );
	Assert( bm.key.suffix.Pv() == rgbKey );
	Assert( bm.data.Pv() == NULL );
	Assert( bm.data.Cb() == 0 );
	Assert( dib.pos == posDown );
	Assert( dib.pbm == &bm );
	dib.dirflag = fDIRFavourNext;

	err = ErrBTDown( pfucbAE, &dib, latchReadTouch );
	if ( JET_errRecordNotFound != err )
		{
		BOOL	fOnNextExtent	= fFalse;

		Assert( err != JET_errNoCurrentRecord );
		
#ifdef DEBUG
		if ( err < 0 )
			{
			DBGprintf( "ErrSPFreeExt could not go down into nonempty available extent tree.\n" );
			}
#endif
		Call( err );

		//	found an available extent node 
		//
		cpgAESize = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
		Assert( cpgAESize >= 0 );
		if ( 0 == cpgAESize )
			{
			//	We might have zero-sized extents if we crashed in ErrSPIAddToAvailExt().
			//	Simply delete such extents and retry.
			Call( ErrBTFlagDelete(		// UNDONE: Synchronously remove the node
						pfucbAE,
						fDIRNoVersion | ( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
			BTUp( pfucbAE );
			goto FindPage;
			}
		
		LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );

		//	assert no page is common between available extent node
		//	and freed extent
		//
		Assert( pgnoFirst > pgnoAELast ||
				pgnoLast < pgnoAELast - cpgAESize + 1 );
				
		if ( wrnNDFoundGreater == err )
			{
			Assert( pgnoAELast > pgnoFirst - 1 );

			//	already on the next node, no need to move there
			fOnNextExtent = fTrue;
			}
		else if ( wrnNDFoundLess == err )
			{
			//	available extent nodes last page < pgnoFirst - 1
			//	no possible coalescing on the left
			//	(this is the last node in available extent tree)
			//
			Assert( pgnoAELast < pgnoFirst - 1 );
			}
		else
			{
			Assert( pgnoAELast == pgnoFirst - 1 );
			CallS( err );
			}

		if ( JET_errSuccess == err
			&& pgnoFirst > pgnoOELast - cpgOESize + 1 )
			{
			//	found available extent node whose last page == pgnoFirst - 1
			//	can coalesce freed extent with this node after re-keying
			//
			//	the second condition is to ensure that we do not coalesce
			//		two available extent nodes that belong to different owned extent nodes
			//
			Assert( pgnoAELast - cpgAESize + 1 >= pgnoOELast - cpgOESize + 1 );
			Assert( cpgAESize == 
						*(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv() );
			Assert( pgnoAELast == pgnoFirst - 1 );
						
			cpgSize += cpgAESize;
			pgnoFirst -= cpgAESize;
			Assert( pgnoLast == pgnoFirst + cpgSize - 1 );
			Call( ErrBTFlagDelete(		// UNDONE: Synchronously remove the node
						pfucbAE,
						fDIRNoVersion | ( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );

			//	successfully coalesced on the left, now
			//	attempt coalescing on the right
			//	if we haven't formed a full extent
			Assert( !fOnNextExtent );
			if ( pgnoLast == pgnoOELast
				&& cpgSize == cpgOESize )
				{
				goto InsertExtent;
				}

			Pcsr( pfucbAE )->Downgrade( latchReadTouch );

			//	verify we're still within the boundaries of OwnExt
			Assert( pgnoOELast - cpgOESize + 1 <= pgnoFirst );
			Assert( pgnoOELast >= pgnoLast );
			}

		//	now see if we can coalesce with next node, first moving there if necessary
		//
		if ( !fOnNextExtent )
			{
			err = ErrBTNext( pfucbAE, fDIRNull );
			if ( err < 0 )
				{
				if ( JET_errNoCurrentRecord == err )
					{
					err = JET_errSuccess;
					}
				else
					{
					Call( err );
					}
				}
			else
				{
				//	successfully moved to next extent,
				//	so we can try to coalesce
				fOnNextExtent = fTrue;
				}
			}

		if ( fOnNextExtent )
			{
			cpgAESize = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
			Assert( cpgAESize != 0 );

			LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );
			
			//	verify no page is common between available extent node
			//	and freed extent
			//
			if ( pgnoLast >= pgnoAELast - cpgAESize + 1 )
				{
				AssertSz( fFalse, "AvailExt corrupted." );
				Call( ErrERRCheck( JET_errSPAvailExtCorrupted ) );
				}
				
			if ( pgnoLast == pgnoAELast - cpgAESize && 
				 pgnoAELast <= pgnoOELast )
				{
				//	freed extent falls exactly in front of available extent node 
				//			-- coalesce freed extent with current node
				//
				//	the second condition ensures that we do not coalesce
				//		two available extent nodes that are from different
				//		owned extent nodes
				//
				DATA	data;

				cpgSize += cpgAESize;

				LittleEndian<CPG> le_cpgSize;
				le_cpgSize = cpgSize;
				data.SetPv( &le_cpgSize );
				data.SetCb( sizeof(CPG) );
				Call( ErrBTReplace(
							pfucbAE,
							data,
							fDIRNoVersion | ( pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
				Assert( Pcsr( pfucbAE )->FLatched() );

				pgnoLast = pgnoAELast;
				fCoalesced = fTrue;

				if ( cpgSize >= cpageSEDefault
					&& pgnoNull != pfcb->PgnoNextAvailSE()
					&& pgnoFirst < pfcb->PgnoNextAvailSE() )
					{
					pfcb->SetPgnoNextAvailSE( pgnoFirst );
					}
				}
			}
		}
		
	//	add new node to available extent tree
	//
	if ( !fCoalesced )
		{
InsertExtent:
		BTUp( pfucbAE );
		AssertSPIPfucbOnRoot( pfucb );
		Call( ErrSPIAddFreedExtent(
					pfucbAE,
					PsphSPIRootPage( pfucb )->PgnoParent(),
					pgnoLast,
					cpgSize ) );
		}
	Assert( Pcsr( pfucbAE )->FLatched() );
	

	//	if extent freed coalesced with available extents 
	//	form a complete secondary extent, remove the secondary extent
	//	from the FDP and free it to the parent FDP.	
	//	Since FDP is first page of primary extent, 
	//	we do not have to guard against freeing
	//	primary extents.  
	//	UNDONE: If parent FDP is NULL, FDP is device level and
	//			complete, free secondary extents to device.
	//
	Assert( pgnoLast != pgnoOELast || cpgSize <= cpgOESize );
	if ( pgnoLast == pgnoOELast && cpgSize == cpgOESize )
		{
		Assert( cpgSize > 0 );
		
#ifdef DEBUG		
		Assert( Pcsr( pfucbAE )->FLatched() );
		LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );
		cpgAESize = *(UnalignedLittleEndian< CPG > *) pfucbAE->kdfCurr.data.Pv();
		Assert( pgnoAELast == pgnoLast );
		Assert( cpgAESize == cpgSize );
#endif

		//	owned extent node is same as available extent node
		Call( ErrSPIFreeSEToParent( pfucb, pfucbOE, pfucbAE, pgnoOELast, cpgOESize ) );
		}


HandleError:
	if ( pfucbAE != pfucbNil )
		BTClose( pfucbAE );
	if ( pfucbOE != pfucbNil )
		BTClose( pfucbOE );
	Assert( pfucb->pcsrRoot != pcsrNil );
	if ( fRootLatched )
		{
		Assert( Pcsr( pfucb ) == pfucb->pcsrRoot );
		Assert( pfucb->pcsrRoot->FLatched() );
		pfucb->pcsrRoot->ReleasePage();
		pfucb->pcsrRoot = pcsrNil;
		}
		
#ifdef SPACE_TRACE
	DBGprintf( "free space %lu at %lu to FDP %d.%lu\n", cpgSize, pgnoFirst, pfucb->ifmp, PgnoFDP( pfucb ) );
#endif

#ifdef DEBUG
	if ( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fDBGTraceBR )
		{
		INT cpg = 0;

		Assert( err >= 0 );
		for ( ; cpg < cpgSize; cpg++ )
			{
			char sz[256];
			sprintf( sz, "FREE (%d:%ld) %d:%ld",
					pfucb->ifmp, PgnoFDP( pfucb ),
					pfucb->ifmp, pgnoFirst + cpg );
			CallS( PinstFromIfmp( pfucb->ifmp )->m_plog->ErrLGTrace( ppib, sz ) );
			}
		}
#endif

	Assert( err != JET_errKeyDuplicate );

	return err;
	}


const ULONG cOEListEntriesInit	= 32;
const ULONG cOEListEntriesMax	= 127;

class OWNEXT_LIST
	{
	public:
		OWNEXT_LIST( OWNEXT_LIST **ppOEListHead );
		~OWNEXT_LIST();

	public:
		EXTENTINFO		*RgExtentInfo()			{ return m_extentinfo; }
		ULONG			CEntries() const;
		OWNEXT_LIST		*POEListNext() const	{ return m_pOEListNext; }
		VOID			AddExtentInfoEntry( const PGNO pgnoLast, const CPG cpgSize );

	private:
		EXTENTINFO		m_extentinfo[cOEListEntriesMax];
		ULONG			m_centries;
		OWNEXT_LIST		*m_pOEListNext;
	};

INLINE OWNEXT_LIST::OWNEXT_LIST( OWNEXT_LIST **ppOEListHead )
	{
	m_centries = 0;
	m_pOEListNext = *ppOEListHead;
	*ppOEListHead = this;
	}

INLINE ULONG OWNEXT_LIST::CEntries() const
	{
	Assert( m_centries <= cOEListEntriesMax );
	return m_centries;
	}

INLINE VOID OWNEXT_LIST::AddExtentInfoEntry(
	const PGNO	pgnoLast,
	const CPG	cpgSize )
	{
	Assert( m_centries < cOEListEntriesMax );
	m_extentinfo[m_centries].pgnoLastInExtent = pgnoLast;
	m_extentinfo[m_centries].cpgExtent = cpgSize;
	m_centries++;
	}

INLINE ERR ErrSPIFreeOwnedExtentsInList(
	FUCB		*pfucbParent,
	EXTENTINFO	*rgextinfo,
	const ULONG	cExtents )
	{
	ERR			err;
	INT			i;

	for ( i = 0; i < cExtents; i++ )
		{
		const CPG	cpgSize = rgextinfo[i].cpgExtent;
		const PGNO	pgnoFirst = rgextinfo[i].pgnoLastInExtent - cpgSize + 1;

		Assert( !FFUCBSpace( pfucbParent ) );
		CallR( ErrSPFreeExt( pfucbParent, pgnoFirst, cpgSize ) );
		}

	return JET_errSuccess;
	}


LOCAL ERR ErrSPIFreeAllOwnedExtents( FUCB *pfucbParent, FCB *pfcb, const BOOL fPreservePrimaryExtent )
	{
	ERR			err;
	FUCB  		*pfucbOE;
	DIB			dib;
	CPG			cpgSize;
	PGNO		pgnoLast;
	PGNO		pgnoLastPrev;

#ifdef REPORT_FREED_SORT_FDP
	ULONG		cExtents	= 0;
	CPG			cpgOwned	= 0;
#endif

	Assert( pfcb != pfcbNil );

	//	open owned extent tree of freed FDP
	//	free each extent in owned extent to parent FDP.
	//
	CallR( ErrSPIOpenOwnExt( pfucbParent->ppib, pfcb, &pfucbOE ) );
	
	dib.pos = posFirst;
	dib.dirflag = fDIRNull;
	if ( ( err = ErrBTDown( pfucbOE, &dib, latchReadTouch ) ) < 0 )
		{
		BTClose( pfucbOE );
		return err;
		}
	Assert( wrnNDFoundLess != err );
	Assert( wrnNDFoundGreater != err );

	EXTENTINFO	extinfo[cOEListEntriesInit];
	OWNEXT_LIST	*pOEList		= NULL;
	OWNEXT_LIST	*pOEListCurr	= NULL;
	ULONG		cOEListEntries	= 0;

	//	Collect all Own extent and free them all at once.
	//	Note that the pages kept tracked by own extent tree contains own
	//	extend tree itself. We can not free it while scanning own extent
	//	tree since we could free the pages used by own extend let other
	//	thread to use the pages.

	//	UNDONE: Because we free it all at once, if we crash, we may lose space.
	//	UNDONE: we need logical logging to log the remove all extent is going on
	//	UNDONE: and remember its state so that during recovery it will be able
	//	UNDONE: to redo the clean up.

	pgnoLastPrev = 0;

	do
		{
		cpgSize = *(UnalignedLittleEndian< CPG > *)pfucbOE->kdfCurr.data.Pv();
		Assert( cpgSize > 0 );
		LongFromKey( &pgnoLast, pfucbOE->kdfCurr.key );

		if ( pgnoLastPrev > pgnoLast - cpgSize )
			{
			//	nodes not in ascending order, something is gravely wrong
			//
			AssertSzRTL( fFalse, "OwnExt nodes are not in monotonically-increasing key order." );
			Call( ErrERRCheck( JET_errSPOwnExtCorrupted ) );
			}

		pgnoLastPrev = pgnoLast;

#ifdef REPORT_FREED_SORT_FDP
		cExtents++;
		cpgOwned += cpgSize;
#endif

		if ( !fPreservePrimaryExtent
			|| ( pfcb->PgnoFDP() != pgnoLast + 1 - cpgSize ) )
			{
			// Can't coalesce this OwnExt with previous OwnExt because
			// we may cross OwnExt boundaries in the parent.

			if ( cOEListEntries < cOEListEntriesInit )
				{
				// This entry can fit in the initial EXTENTINFO structure
				// (the one that was allocated on the stack).
				extinfo[cOEListEntries].pgnoLastInExtent = pgnoLast;
				extinfo[cOEListEntries].cpgExtent = cpgSize;
				}
			else 
				{
				Assert( ( NULL == pOEListCurr && NULL == pOEList )
					|| ( NULL != pOEListCurr && NULL != pOEList ) );
				if ( NULL == pOEListCurr || pOEListCurr->CEntries() == cOEListEntriesMax )
					{
					pOEListCurr = (OWNEXT_LIST *)PvOSMemoryHeapAlloc( sizeof( OWNEXT_LIST ) );
					if ( NULL == pOEListCurr )
						{
						Assert( pfucbNil != pfucbOE );
						BTClose( pfucbOE );
						err = ErrERRCheck( JET_errOutOfMemory );
						goto HandleError;
						}
					new( pOEListCurr ) OWNEXT_LIST( &pOEList );

					Assert( pOEList == pOEListCurr );
					}

				pOEListCurr->AddExtentInfoEntry( pgnoLast, cpgSize );
				}

			cOEListEntries++;
			}

		err = ErrBTNext( pfucbOE, fDIRNull );
		}
	while ( err >= 0 );

#ifdef REPORT_FREED_FDP
	OSTrace( ostlLow, OSFormat(
							"Free FDP with %08d owned pages and %08d owned extents [objid:0x%x,pgnoFDP:0x%x]\n",
							cpgOwned,
							cExtents,
							pfcb->ObjidFDP(),
							pfcb->PgnoFDP() ) );

#endif

	//	Close the pfucbOE right away to release any latch on the pages that
	//	are going to be freed and used by others.

	Assert( pfucbNil != pfucbOE );
	BTClose( pfucbOE );

	//	Check the error code.

	if ( err != JET_errNoCurrentRecord )
		{
		Assert( err < 0 );
		goto HandleError;
		}

	Call( ErrSPIFreeOwnedExtentsInList(
			pfucbParent,
			extinfo,
			min( cOEListEntries, cOEListEntriesInit ) ) );

	for ( pOEListCurr = pOEList;
		pOEListCurr != NULL;
		pOEListCurr = pOEListCurr->POEListNext() )
		{
		Assert( cOEListEntries > cOEListEntriesInit );
		Call( ErrSPIFreeOwnedExtentsInList(
				pfucbParent,
				pOEListCurr->RgExtentInfo(),
				pOEListCurr->CEntries() ) );
		}
		
HandleError:
	pOEListCurr = pOEList;
	while ( pOEListCurr != NULL )
		{
		OWNEXT_LIST	*pOEListKill = pOEListCurr;

#ifdef DEBUG
		//	this variable is no longer used, so we can
		//	re-use it for DEBUG-only purposes
		Assert( cOEListEntries > cOEListEntriesInit );
		Assert( cOEListEntries > pOEListCurr->CEntries() );
		cOEListEntries -= pOEListCurr->CEntries();
#endif		
		
		pOEListCurr = pOEListCurr->POEListNext();

		OSMemoryHeapFree( pOEListKill );
		}
	Assert( cOEListEntries <= cOEListEntriesInit );
	
	return err;
	}
	

#ifdef REPORT_FREED_FDP

LOCAL ERR ErrSPIAddAvailExts( PIB * const ppib, FCB * const pfcb )
	{
	ERR			err;
	FUCB  		*pfucbAE;
	DIB			dib;
	ULONG		cExtents	= 0;
	CPG			cpgFree		= 0;

	Assert( pfcb != pfcbNil );

	//	open owned extent tree of freed FDP
	//	free each extent in owned extent to parent FDP.
	//
	CallR( ErrSPIOpenAvailExt( ppib, pfcb, &pfucbAE ) );
	
	dib.pos = posFirst;
	dib.dirflag = fDIRNull;
	err = ErrBTDown( pfucbAE, &dib, latchReadTouch );
	Assert( wrnNDFoundLess != err );
	Assert( wrnNDFoundGreater != err );

	do
		{
		Call( err );

		const CPG	cpgSize	= *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();

		cExtents++;
		cpgFree += cpgSize;

		err = ErrBTNext( pfucbAE, fDIRNull );
		}
	while ( JET_errNoCurrentRecord != err );

	err = JET_errSuccess;

	OSTrace( ostlLow, OSFormat(
							"Free FDP with %08d avail pages and %08d avail extents [objid:0x%x,pgnoFDP:0x%x]\n",
							cpgFree,
							cExtents,
							pfcb->ObjidFDP(),
							pfcb->PgnoFDP() ) );

HandleError:
	Assert( pfucbNil != pfucbAE );
	BTClose( pfucbAE );

	return err;
	}

#endif	//	REPORT_FREED_FDP


//	ErrSPFreeFDP
//	========================================================================
//	ERR ErrSPFreeFDP( FUCB *pfucbParent, PGNO pgnoFDPFreed )
//
//	Frees all owned extents of an FDP to its parent FDP.  The FDP page is freed
//	with the owned extents to the parent FDP.
//
//	PARAMETERS	pfucbParent		cursor on tree space is freed to
//				pgnoFDPFreed	pgnoFDP of FDP to be freed
//
ERR ErrSPFreeFDP(
	PIB			*ppib,
	FCB			*pfcbFDPToFree,
	const PGNO	pgnoFDPParent,
	const BOOL	fPreservePrimaryExtent )
	{
	ERR			err;
	const IFMP	ifmp			= pfcbFDPToFree->Ifmp();
	const PGNO	pgnoFDPFree		= pfcbFDPToFree->PgnoFDP();
	FUCB		*pfucbParent	= pfucbNil;
	FUCB		*pfucb			= pfucbNil;

	PERFIncCounterTable( cSPDelete, PinstFromIfmp( pfcbFDPToFree->Ifmp() ), pfcbFDPToFree->Tableclass() );

	//	begin transaction if one is already not begun
	//
	BOOL	fBeginTrx	= fFalse;
	if ( ppib->level == 0 )
		{
		CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
		fBeginTrx	= fTrue;
		}

	Assert( pgnoNull != pgnoFDPParent );
	Assert( pgnoNull != pgnoFDPFree );
	
	Assert( dbidTemp != rgfmp[ ifmp ].Dbid() || pgnoSystemRoot == pgnoFDPParent );
	
	Call( ErrBTOpen( ppib, pgnoFDPParent, ifmp, &pfucbParent ) );
	Assert( pfucbNil != pfucbParent );
	Assert( pfucbParent->u.pfcb->FInitialized() );
	
	//	check for valid parameters.
	//
#ifdef SPACECHECK
	CallS( ErrSPIValidFDP( ppib, pfucbParent->ifmp, pgnoFDPFree ) );
#endif

#ifdef SPACE_TRACE
	DBGprintf( "free space FDP at %d.%lu\n", pfucbParent->ifmp, pgnoFDPFree );
#endif
	
#ifdef DEBUG
	if ( PinstFromPpib( ppib )->m_plog->m_fDBGTraceBR )
		{
		char sz[256];

		sprintf( sz, "FREE FDP (%d:%ld)", ifmp, pgnoFDPFree );
		CallS( PinstFromPpib( ppib )->m_plog->ErrLGTrace( ppib, sz ) );
		}
#endif

	//	get temporary FUCB
	//
	Call( ErrBTOpen( ppib, pfcbFDPToFree, &pfucb ) );
	Assert( pfucbNil != pfucb );
	Assert( pfucb->u.pfcb->FInitialized() );
	Assert( pfucb->u.pfcb->FDeleteCommitted() );
	FUCBSetIndex( pfucb );

	Call( ErrBTIGotoRoot( pfucb, latchRIW ) );
	pfucb->pcsrRoot = Pcsr( pfucb );

#ifdef SPACECHECK
	CallS( ErrSPIWasAlloc( pfucb, pgnoFDPFree, 1 ) );
#endif

	//	get parent FDP pgno
	//
	Assert( pgnoFDPParent == PgnoSPIParentFDP( pfucb ) );
	Assert( pgnoFDPParent == PgnoFDP( pfucbParent ) );

	if ( !pfucb->u.pfcb->FSpaceInitialized() )
		{
		SPIInitFCB( pfucb, fTrue );
		}

	//	if single extent format, then free extent in external header
	//
	if ( pfucb->u.pfcb->PgnoOE() == pgnoNull )
		{
		if ( !fPreservePrimaryExtent )
			{
			SPACE_HEADER 	*psph;
			
			AssertSPIPfucbOnRoot( pfucb );

			//	get external header
			//
			NDGetExternalHeader( pfucb, pfucb->pcsrRoot );
			Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );
			psph = reinterpret_cast <SPACE_HEADER *> ( pfucb->kdfCurr.data.Pv() );

			ULONG cpgPrimary = psph->CpgPrimary();
			Assert( psph->CpgPrimary() != 0 );

			//	Close the cursor to make sure it latches no buffer whose page
			//	is going to be freed.

			pfucb->pcsrRoot = pcsrNil;
			BTClose( pfucb );
			pfucb = pfucbNil;

			Assert( !FFUCBSpace( pfucbParent ) );
			err = ErrSPFreeExt( pfucbParent, pgnoFDPFree, cpgPrimary );
			}
		}
	else
		{
		//	Close the cursor to make sure it latches no buffer whose page
		//	is going to be freed.

		FCB *pfcb = pfucb->u.pfcb;
		pfucb->pcsrRoot = pcsrNil;
		BTClose( pfucb );
		pfucb = pfucbNil;

#ifdef REPORT_FREED_SORT_FDP
		if ( dbidTemp == rgfmp[pfucbParent->ifmp].Dbid() )
			{
			Call( ErrSPIAddAvailExts( ppib, pfcb ) );
			}
#endif

		Call( ErrSPIFreeAllOwnedExtents( pfucbParent, pfcb, fPreservePrimaryExtent ) );
		Assert( !Pcsr( pfucbParent )->FLatched() );
		}

HandleError:
	if ( pfucbNil != pfucb )
		{
		pfucb->pcsrRoot = pcsrNil;
		BTClose( pfucb );
		}
		
	if ( pfucbNil != pfucbParent )
		{
		BTClose( pfucbParent );
		}

	if ( fBeginTrx )
		{
		if ( err >= 0 )
			{
			err = ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush );
			}
		if ( err < 0 )
			{
			CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
			}
		}

#ifdef DEBUG
	if ( !FSPExpectedError( err ) )
		{
		CallS( err );
		AssertSz( fFalse, ( dbidTemp != rgfmp[ ifmp ].Dbid() ?
								"Space potentially lost permanently in user database." :
								"Space potentially lost in temporary database." ) );
		}
#endif

	return err;
	}

INLINE ERR ErrSPIAddExtent(
	FUCB		*pfucb,
	const PGNO	pgnoLast,
	const CPG	cpgSize )
	{
	ERR			err;
	KEY			key;
	DATA 		data;
	BYTE		rgbKey[sizeof(PGNO)];

	Assert( FFUCBSpace( pfucb ) );
	Assert( !Pcsr( pfucb )->FLatched() );
	Assert( cpgSize > 0 );

	// begin log macro
	//
	KeyFromLong( rgbKey, pgnoLast );
	key.prefix.Nullify();
	key.suffix.SetCb( sizeof(PGNO) );
	key.suffix.SetPv( rgbKey );

	LittleEndian<CPG> le_cpgSize = cpgSize;
	data.SetPv( (VOID *)&le_cpgSize );
	data.SetCb( sizeof(CPG) );

	BTUp( pfucb );
	Call( ErrBTInsert(
				pfucb,
				key,
				data,
				fDIRNoVersion | ( pfucb->u.pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
	Assert( Pcsr( pfucb )->FLatched() );

HandleError:	
	Assert( errSPOutOfOwnExtCacheSpace != err );
	Assert( errSPOutOfAvailExtCacheSpace != err );
	return err;
	}

LOCAL ERR ErrSPIAddToAvailExt(
	FUCB		*pfucbAE,
	const PGNO	pgnoAELast,
	const CPG	cpgAESize,
	SPCACHE		***pppspcache )
	{
	ERR			err;
	FCB			* const pfcb	= pfucbAE->u.pfcb;

	Assert( FFUCBAvailExt( pfucbAE ) );
	err = ErrSPIAddExtent( pfucbAE, pgnoAELast, cpgAESize );
	if ( err < 0 )
		{
		if ( JET_errKeyDuplicate == err )
			{
			err = ErrERRCheck( JET_errSPAvailExtCorrupted );
			}
		}

	else if ( cpgAESize >= cpageSEDefault
		&& pgnoNull != pfcb->PgnoNextAvailSE() )
		{
		const PGNO	pgnoFirst	= pgnoAELast - cpgAESize + 1;
		if ( pgnoFirst < pfcb->PgnoNextAvailSE() )
			{
			pfcb->SetPgnoNextAvailSE( pgnoFirst );
			}
		}

	return err;
	}
	
LOCAL ERR ErrSPIAddToOwnExt(
	FUCB		*pfucb,
	const PGNO	pgnoOELast,
	CPG			cpgOESize,
	CPG			*pcpgCoalesced )
	{
	ERR			err;
	FUCB		*pfucbOE;
	

	//	open cursor on owned extent
	//
	CallR( ErrSPIOpenOwnExt( pfucb->ppib, pfucb->u.pfcb, &pfucbOE ) );
	Assert( FFUCBOwnExt( pfucbOE ) );
	
#ifdef COALESCE_OE
	//	coalescing OWNEXT is done only for temp database or top-level OE,
	//	otherwise we end up creating huge OwnExt's in which space never
	//	gets freed to the parent
	//	
	if ( NULL != pcpgCoalesced
		&& ( objidSystemRoot == pfucb->u.pfcb->ObjidFDP()
			|| dbidTemp == rgfmp[pfucb->ifmp].Dbid() ) )
		{
		CPG			cpgOECoalesced	= 0;
		BOOKMARK	bmSeek;
		DIB			dib;
		BYTE		rgbKey[sizeof(PGNO)];
		
		//	set up variables for coalescing
		//
		KeyFromLong( rgbKey, pgnoOELast - cpgOESize );
		bmSeek.key.prefix.Nullify();
		bmSeek.key.suffix.SetCb( sizeof(PGNO) );
		bmSeek.key.suffix.SetPv( rgbKey );
		bmSeek.data.Nullify();

		//	look for extent that ends at pgnoOELast - cpgOESize, 
		//	the only extent we can coalesce with
		//
		dib.pos		= posDown;
		dib.pbm		= &bmSeek;
		dib.dirflag	= fDIRExact;
		err = ErrBTDown( pfucbOE, &dib, latchReadTouch );
		if ( JET_errRecordNotFound == err )
			{
			err = JET_errSuccess;
			}
		else if ( JET_errSuccess == err )
			{
			//  we found a match, so get the old extent's size, delete the old extent,
			//  and add it's size to the new extent to insert
			//
			Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
			
#ifdef DEBUG
			PGNO	pgnoOELastPrev;
			LongFromKey( &pgnoOELastPrev, pfucbOE->kdfCurr.key );
			Assert( pgnoOELastPrev == pgnoOELast - cpgOESize );
			Assert( pfucbOE->kdfCurr.data.Cb() == sizeof(CPG) );
#endif
			
			cpgOECoalesced = *(UnalignedLittleEndian< CPG > *)pfucbOE->kdfCurr.data.Pv();
			Assert( cpgOECoalesced > 0 );
			Call( ErrBTFlagDelete(		// UNDONE: Synchronously remove the node
						pfucbOE,
						fDIRNoVersion | ( pfucbOE->u.pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );

			Assert( NULL != pcpgCoalesced );
			*pcpgCoalesced = cpgOECoalesced;

			cpgOESize += cpgOECoalesced;
			}
		else
			{
			Call( err );
			}

		BTUp( pfucbOE );
		}
#endif	//	COALESCE_OE

	Call( ErrSPIAddExtent( pfucbOE, pgnoOELast, cpgOESize ) );

HandleError:
	Assert( errSPOutOfOwnExtCacheSpace != err );
	Assert( errSPOutOfAvailExtCacheSpace != err );
	pfucbOE->pcsrRoot = pcsrNil;
	BTClose( pfucbOE );
	return err;
	}

#ifdef COALESCE_OE
LOCAL ERR ErrSPICoalesceAvailExt(
	FUCB		*pfucbAE,
	const PGNO	pgnoLast,
	const CPG	cpgSize,
	CPG			*pcpgCoalesce )
	{
	ERR			err;
	BOOKMARK	bmSeek;
	DIB			dib;
	BYTE		rgbKey[sizeof(PGNO)];

	//	coalescing AVAILEXT is done only for temp database
	//	or top-level OE
	//	
	Assert( objidSystemRoot == pfucbAE->u.pfcb->ObjidFDP()
			|| dbidTemp == rgfmp[pfucbAE->ifmp].Dbid() );

	*pcpgCoalesce = 0;
		
	//	Set up seek key to Avail Size
	//
	KeyFromLong( rgbKey, pgnoLast - cpgSize );
		
	//	set up variables for coalescing
	//
	bmSeek.key.prefix.Nullify();
	bmSeek.key.suffix.SetCb( sizeof(PGNO) );
	bmSeek.key.suffix.SetPv( rgbKey );
	bmSeek.data.Nullify();

	//	look for extent that ends at pgnoLast - cpgSize, 
	//	the only extent we can coalesce with
	//
	dib.pos		= posDown;
	dib.pbm		= &bmSeek;
	dib.dirflag	= fDIRNull;
	err = ErrBTDown( pfucbAE, &dib, latchReadTouch );
	if ( JET_errRecordNotFound == err )
		{
		//	no record in available extent
		//	mask error
		err = JET_errSuccess;
		}
	else if ( JET_errSuccess == err )
		{
		//  we found a match, so get the old extent's size, delete the old extent,
		//  and add it's size to the new extent to insert
		//
#ifdef DEBUG
		PGNO	pgnoAELast;
		Assert( pfucbAE->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoAELast, pfucbAE->kdfCurr.key );
		Assert( pgnoAELast == pgnoLast - cpgSize );
#endif
		Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
		*pcpgCoalesce = *(UnalignedLittleEndian< CPG > *)pfucbAE->kdfCurr.data.Pv();
		err = ErrBTFlagDelete(		// UNDONE: Synchronously remove the node
					pfucbAE,
					fDIRNoVersion | ( pfucbAE->u.pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) );
		}

	BTUp( pfucbAE );
		
	return err;
	}
#endif	//	COALESCE_OE


//	if Secondary extent, add given extent owned extent and available extent
//	if Freed extent, add extent to available extent
//	splits caused during insertion into owned extent and available extent will
//		use space from FUCB space cache, which is initialized here
//	pufcbAE is cursor on available extent tree, should be positioned on 
//		added available extent node
//
LOCAL ERR ErrSPIAddSecondaryExtent(
	FUCB		*pfucb,
	FUCB		*pfucbAE,
	const PGNO	pgnoLast, 
	CPG			cpgSize,
	SPCACHE		***pppspcache )
	{
	ERR			err;
	CPG			cpgOECoalesced	= 0;

	//	if this is a secondary extent, insert new extent into OWNEXT and
	//	AVAILEXT, coalescing with an existing extent to the left, if possible.
	//
	Call( ErrSPIAddToOwnExt(
				pfucb,
				pgnoLast,
				cpgSize,
				&cpgOECoalesced ) );


#ifdef COALESCE_OE
	//	We shouldn't even try coalescing AvailExt if no coalescing of OwnExt was done
	//	(since we cannot coalesce AvailExt across OwnExt boundaries)
	if ( cpgOECoalesced > 0 )
		{
		CPG	cpgAECoalesced;
		
		Call( ErrSPICoalesceAvailExt( pfucbAE, pgnoLast, cpgSize, &cpgAECoalesced ) );
		
		// Ensure AvailExt wasn't coalesced across OwnExt boundaries.
		Assert( cpgAECoalesced <= cpgOECoalesced );
		cpgSize += cpgAECoalesced;
		}
#else
	Assert( 0 == cpgOECoalesced );
#endif	//	COALESCE_OE		


	Call( ErrSPIAddToAvailExt( pfucbAE, pgnoLast, cpgSize, pppspcache ) );
	Assert( Pcsr( pfucbAE )->FLatched() );
	
HandleError:
	return err;
	}


INLINE ERR ErrSPICheckSmallFDP( FUCB *pfucb, BOOL *pfSmallFDP )
	{
	ERR		err;
	FUCB	*pfucbOE	= pfucbNil;
	CPG		cpgOwned	= 0;
	DIB		dib;

	CallR( ErrSPIOpenOwnExt( pfucb->ppib, pfucb->u.pfcb, &pfucbOE ) );
	Assert( pfucbNil != pfucbOE );
	
	//  determine if this FDP owns a lot of space [> cpgSmallFDP]
	//	
	dib.pos = posFirst;
	dib.dirflag = fDIRNull;
	err = ErrBTDown( pfucbOE, &dib, latchReadTouch );
	Assert( err != JET_errNoCurrentRecord );
	Assert( err != JET_errRecordNotFound );

	//	assume small FDP unless proven otherwise
	//
	*pfSmallFDP = fTrue;

	// Count pages until we reach the end or hit short-circuit value (cpgSmallFDP).
	do
		{
		//	process navigation error
		//
		Call( err );

		const CPG	cpgOwnedCurr = *(UnalignedLittleEndian< CPG > *)pfucbOE->kdfCurr.data.Pv();

		Assert( pfucbOE->kdfCurr.data.Cb() == sizeof( PGNO ) );
		Assert( cpgOwnedCurr != 0 );
			
		cpgOwned += cpgOwnedCurr;
		if ( cpgOwned > cpgSmallFDP )
			{
			*pfSmallFDP = fFalse;
			break;
			}

		err = ErrBTNext( pfucbOE, fDIRNull );
		}
	while ( JET_errNoCurrentRecord != err );

	err = JET_errSuccess;

HandleError:
	Assert( pfucbNil != pfucbOE );
	BTClose( pfucbOE );
	return err;
	}


LOCAL VOID SPReportMaxDbSizeExceeded( const IFMP ifmp, const CPG cpg )
	{
	//	Log event to tell user that it reaches the database size limit.

	CHAR		szCurrentSizeMb[16];
	const CHAR	* rgszT[2]			= { rgfmp[ifmp].SzDatabaseName(), szCurrentSizeMb };

	sprintf( szCurrentSizeMb, "%d", (ULONG)(( (QWORD)cpg * (QWORD)( g_cbPage >> 10 /* Kb */ ) ) >> 10 /* Mb */) );
				
	UtilReportEvent(
			eventWarning,
			SPACE_MANAGER_CATEGORY,
			SPACE_MAX_DB_SIZE_REACHED_ID,
			2,
			rgszT,
			0,
			NULL,
			PinstFromIfmp( ifmp ) );
	}


LOCAL ERR ErrSPIExtendDB(
	FUCB		*pfucb,
	const CPG	cpgSEMin,
	CPG			*pcpgSEReq,
	PGNO		*ppgnoSELast,
	const BOOL	fPermitAsyncExtension )
	{
	ERR			err;
	CPG			cpgSEReq			= *pcpgSEReq;
	FUCB		*pfucbOE			= pfucbNil;
	PGNO		pgnoSELast			= pgnoNull;
	BOOL		fAllocAE			= fFalse;
	CPG			cpgAsyncExtension	= 0;
	DIB			dib;

	Assert( pgnoSystemRoot == pfucb->u.pfcb->PgnoFDP() );

	Call( ErrSPIOpenOwnExt( pfucb->ppib, pfucb->u.pfcb, &pfucbOE ) );

	dib.pos = posLast;
	dib.dirflag = fDIRNull;

	Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );
	Assert( pfucbOE->kdfCurr.key.Cb() == sizeof( PGNO ) );
	LongFromKey( &pgnoSELast, pfucbOE->kdfCurr.key );
	BTUp( pfucbOE );

	//	allocate more space from device
	//
	Assert( pgnoSysMax >= pgnoSELast );
	Assert( cpgSEMin > 0 );
	if ( pgnoSysMax - pgnoSELast < (PGNO)cpgSEMin )
		{
		SPReportMaxDbSizeExceeded( pfucb->ifmp, (CPG)pgnoSELast );
		err = ErrERRCheck( JET_errOutOfDatabaseSpace );
		goto HandleError;
		}

	//	NOTE: casting below means that requests of >= 8TB chunks will cause problems
	//
	Assert( cpgSEReq > 0 );
	Assert( pgnoSysMax > pgnoSELast );
	cpgSEReq = (CPG)min( (PGNO)cpgSEReq, (PGNO)( pgnoSysMax - pgnoSELast ) );
	Assert( cpgSEReq > 0 );
	Assert( cpgSEMin <= cpgSEReq );

#ifdef ASYNC_DB_EXTENSION
	if ( fPermitAsyncExtension && pgnoSELast + cpgSEReq + cpgSEReq < pgnoSysMax )
		{
		cpgAsyncExtension = cpgSEReq;
		}
#endif

	err = ErrIONewSize( pfucb->ifmp, pgnoSELast + cpgSEReq, cpgAsyncExtension );
	if ( err < 0 )
		{
		Call( ErrIONewSize( pfucb->ifmp, pgnoSELast + cpgSEMin ) );
		cpgSEReq = cpgSEMin;
		}

	//	calculate last page of device level secondary extent
	//
	pgnoSELast += cpgSEReq;

	Assert( cpgSEReq >= cpgSEMin );
	*pcpgSEReq = cpgSEReq;
	*ppgnoSELast = pgnoSELast;

HandleError:
	if ( pfucbNil != pfucbOE )
		BTClose( pfucbOE );

	return err;
	}

LOCAL ERR ErrSPIReservePagesForSplit( FUCB *pfucb, FUCB *pfucbParent )
	{
	ERR				err;
	ERR				wrn		= JET_errSuccess;
	SPLIT_BUFFER	*pspbuf;
	CPG				cpgMin;
	CPG				cpgReq;
#ifdef DEBUG
	ULONG			crepeat	= 0;
#endif	

	Assert( FFUCBSpace( pfucb ) );
	Assert( !Pcsr( pfucb )->FLatched() );
	Assert( pfucb->u.pfcb->FSpaceInitialized() );

	forever
		{
#ifdef DEBUG
		Assert( crepeat < 3 );		//	shouldn't have to make more than 2 iterations in common case
									//	and 3 when we crash in the middle of operation. Consequential
									//	recovery will not reclaim the space.
		crepeat++;
#endif
		Call( ErrBTIGotoRoot( pfucb, latchRIW ) );

		AssertSPIPfucbOnSpaceTreeRoot( pfucb, Pcsr( pfucb ) );

		Call( ErrSPIGetSPBuf( pfucb, &pspbuf ) );

		if ( pfucb->csr.Cpage().FLeafPage() )
			{
			//	root page is also leaf page,
			//	see if we're close to splitting
			if ( pfucb->csr.Cpage().CbFree() < 100 )
				{
				cpgMin = cpgMaxRootPageSplit;
				cpgReq = cpgReqRootPageSplit;
				}
			else
				{
				cpgMin = 0;
				cpgReq = 0;
				}
			}
		else if ( pfucb->csr.Cpage().FParentOfLeaf() )
			{
			cpgMin = cpgMaxParentOfLeafRootSplit;
			cpgReq = cpgReqParentOfLeafRootSplit;
			}
		else
			{
			cpgMin = cpgMaxSpaceTreeSplit;
			cpgReq = cpgReqSpaceTreeSplit;
			}

		if ( pspbuf->CpgBuffer1() + pspbuf->CpgBuffer2() >= cpgMin )
			{
			break;
			}
		else
			{
			PGNO			pgnoLast;
			SPLIT_BUFFER	spbuf;
			DATA			data;

			UtilMemCpy( &spbuf, pspbuf, sizeof(SPLIT_BUFFER) );

			//	one of the buffers must have dropped to zero -- that's the
			//	one we'll be replacing
			Assert( 0 == spbuf.CpgBuffer1() || 0 == spbuf.CpgBuffer2() );
			
			if ( pfucbNil != pfucbParent )
				{
				PGNO	pgnoFirst	= pgnoNull;
				err = ErrSPIGetExt(
							pfucbParent,
							&cpgReq,
							cpgMin,
							&pgnoFirst );
				AssertSPIPfucbOnRoot( pfucbParent );
				AssertSPIPfucbOnSpaceTreeRoot( pfucb, Pcsr( pfucb ) );
				Call( err );

				pgnoLast = pgnoFirst + cpgReq - 1;
				}
			else
				{
				Assert( pgnoSystemRoot == pfucb->u.pfcb->PgnoFDP() );

				//	don't want to grow database by only 1 or 2 pages at a time, so
				//	choose largest value to satisfy max. theoretical split requirements
				cpgMin = cpgMaxSpaceTreeSplit;
				cpgReq = cpgReqSpaceTreeSplit;

				BTUp( pfucb );
				Call( ErrSPIExtendDB( pfucb, cpgMin, &cpgReq, &pgnoLast, fFalse ) );
				
				Call( ErrBTIGotoRoot( pfucb, latchRIW ) );
				AssertSPIPfucbOnSpaceTreeRoot( pfucb, Pcsr( pfucb ) );
				}
			Assert( cpgReq >= cpgMin );

			if ( NULL == pfucb->u.pfcb->Psplitbuf( FFUCBAvailExt( pfucb ) ) )
				{
				pfucb->csr.UpgradeFromRIWLatch();

				//	verify no one snuck in underneath us
				Assert( 0 == memcmp( &spbuf,
										PspbufSPISpaceTreeRootPage( pfucb, Pcsr( pfucb ) ),
										sizeof(SPLIT_BUFFER) ) );
				Assert( sizeof( SPLIT_BUFFER ) == pfucb->kdfCurr.data.Cb() );	//	WARNING: relies on NDGetExternalHeader() in the previous assert

				spbuf.AddPages( pgnoLast, cpgReq );
				data.SetPv( &spbuf );
				data.SetCb( sizeof(spbuf) );
				Call( ErrNDSetExternalHeader(
							pfucb,
							&data,
							( pfucb->u.pfcb->FDontLogSpaceOps() ? fDIRNoLog : fDIRNull ) ) );
				}
			else
				{
				//	verify no one snuck in underneath us
				Assert( pspbuf == pfucb->u.pfcb->Psplitbuf( FFUCBAvailExt( pfucb ) ) );
				Assert( 0 == memcmp( &spbuf, pspbuf, sizeof(SPLIT_BUFFER) ) );
				pspbuf->AddPages( pgnoLast, cpgReq );
				SPIReportAddedPagesToSplitBuffer( pfucb );
				}

			BTUp( pfucb );

			Call( ErrSPIAddToOwnExt( pfucb, pgnoLast, cpgReq, NULL ) );

			wrn = ErrERRCheck( wrnSPReservedPages );				//	indicate that we reserved pages

			if ( FFUCBAvailExt( pfucb ) )
				{
				//	if we reserved pages for AE, then the reserved pages didn't
				//	get used to satisfy any splits in the OE, so we can just exit
				break;
				}
			else
				{
				//	if we reserved pages for OE, we have to check again in case
				//	the insertion into OE for the reserved pages caused a split
				//	and consumed some of those pages.
				Assert( FFUCBOwnExt( pfucb ) );
				}
			}

		}	//	forever

	CallS( err );
	err = wrn;

HandleError:
	BTUp( pfucb );
	return err;
	}

ERR ErrSPReservePagesForSplit( FUCB *pfucb, FUCB *pfucbParent )
	{
	ERR		err;

	Assert( !Pcsr( pfucbParent )->FLatched() );
	Assert( pcsrNil != pfucbParent->pcsrRoot );
	CallR( ErrBTIGotoRoot( pfucbParent, latchRIW ) );

	err = ErrSPIReservePagesForSplit( pfucb, pfucbParent );

	BTUp( pfucbParent );

	return err;
	}



LOCAL ErrSPIReservePages( FUCB *pfucb, FUCB *pfucbParent, const SPEXT spext )
	{
	ERR		err;
	FCB		* const pfcb	= pfucb->u.pfcb;
	FUCB	* pfucbOE		= pfucbNil;
	FUCB	* pfucbAE		= pfucbNil;

	Call( ErrSPIOpenOwnExt( pfucb->ppib, pfcb, &pfucbOE ) );
	Call( ErrSPIReservePagesForSplit( pfucbOE, pfucbParent ) );

	Call( ErrSPIOpenAvailExt( pfucb->ppib, pfcb, &pfucbAE ) );
	Call( ErrSPIReservePagesForSplit( pfucbAE, pfucbParent ) );

	if ( spextSecondary == spext && wrnSPReservedPages == err )
		{
		//	if reserving for a secondary extent, must check OE again
		//	in case allocation of reserved pages in AE used up some of
		//	the OE reserved pages when inserting the extent into OE;
		//	if reserving for a freed extent, this is unnecessary because
		//	we won't be updating OE
		Call( ErrSPIReservePagesForSplit( pfucbOE, pfucbParent ) );
		}
	
HandleError:
	if ( pfucbNil != pfucbOE )
		BTClose( pfucbOE );
	if ( pfucbNil != pfucbAE )
		BTClose( pfucbAE );

	return err;
	}



//	gets secondary extent from parentFDP, and adds to available extent
//	tree if pfucbAE is non-Nil.
//
//	INPUT:	pfucb		cursor on FDP root page with RIW latch
//			pfucbAE		cursor on available extent tree
//			cpgReq		requested number of pages
//			cpgMin		minimum number of pages required
//
//	
LOCAL ERR ErrSPIGetSE(
	FUCB 			*pfucb, 
	FUCB 			*pfucbAE,
	const CPG		cpgReq, 
	const CPG		cpgMin,
	const BOOL		fSplitting,
	SPCACHE			***pppspcache )
	{
	ERR				err;
	PIB 			*ppib					= pfucb->ppib;
	FUCB 			*pfucbParent			= pfucbNil;
	SPACE_HEADER	*psph;
	PGNO			pgnoParentFDP;
	CPG				cpgPrimary;
	CPG				cpgSEReq;
	CPG				cpgSEMin;
	PGNO			pgnoSELast;
	BOOL			fHoldExtendingDBLatch	= fFalse;
	FMP				*pfmp					= &rgfmp[ pfucb->ifmp ];

	//	check validity of input parameters
	//
	AssertSPIPfucbOnRoot( pfucb );
	Assert( pfucbNil != pfucbAE );
	Assert( !Pcsr( pfucbAE )->FLatched() );
	Assert( pfucb->u.pfcb->FSpaceInitialized() );
	
	//	get parent FDP page number
	//	and primary extent size
	//
	psph = PsphSPIRootPage( pfucb );
	pgnoParentFDP = psph->PgnoParent();
	cpgPrimary = psph->CpgPrimary();

///	pfucb->u.pfcb->CritSpaceTrees().Enter();

	//	pages of allocated extent may be used to split Owned extents and
	//	AVAILEXT trees.  If this happens, then subsequent added
	//	extent will not have to split and will be able to satisfy
	//	requested allocation.
	//
	if ( pgnoNull != pgnoParentFDP )
		{
		PGNO	pgnoSEFirst	= pgnoNull;
		BOOL	fSmallFDP	= fFalse;

		cpgSEMin = cpgMin;

		if ( cpgPrimary < cpgSmallFDP )
			{
			const CPAGE& 	cpage	= pfucb->pcsrRoot->Cpage();

			//	if root page is also a leaf page or a
			//	parent-of-leaf page with just a few nodes,
			//	then this is likely a small FDP, otherwise
			//	it's almost certainly not, so don't bother
			//	checking
			//
			if ( cpage.FLeafPage()
				|| ( cpage.FParentOfLeaf() && cpage.Clines() < cpgSmallFDP ) )
				{
				Call( ErrSPICheckSmallFDP( pfucb, &fSmallFDP ) );
				}
			}

		if ( fSmallFDP )
			{
			//  if this FDP owns just a little space, 
			//	add a very small constant amount of space, 
			//	unless more is requested, 
			//	in order to optimize space allocation
			//  for small tables, indexes, etc.
			//
			cpgSEReq = max( cpgReq, cpgSmallGrow );
			}
		else
			{
			const CPG	cpgSEReqDefault		= ( dbidTemp == pfmp->Dbid()
												&& ppib->FBatchIndexCreation() ?
															cpageDbExtensionDefault :
															cpageSEDefault );

			Assert( cpageDbExtensionDefault != cpgSEReqDefault
				|| pgnoSystemRoot == pgnoParentFDP );

			//  if this FDP owns a lot of space, allocate a fraction of the primary
			//  extent (or more if requested), but at least a given minimum amount
			//
			cpgSEReq = max( cpgReq, max( cpgPrimary/cSecFrac, cpgSEReqDefault ) );
			}

		//	open cursor on parent FDP to get space from
		//
		Call( ErrBTIOpenAndGotoRoot( pfucb->ppib, pgnoParentFDP, pfucb->ifmp, &pfucbParent ) );
		
		Call( ErrSPIReservePages( pfucb, pfucbParent, spextSecondary ) );

		//  allocate extent
		//
		err = ErrSPIGetExt(
					pfucbParent,
					&cpgSEReq,
					cpgSEMin,
					&pgnoSEFirst,
					fSplitting ? fSPSplitting : 0 );
		AssertSPIPfucbOnRoot( pfucbParent );
		AssertSPIPfucbOnRoot( pfucb );
		Call( err );

		Assert( cpgSEReq >= cpgSEMin );
		pgnoSELast = pgnoSEFirst + cpgSEReq - 1;
			
#ifdef CONVERT_VERBOSE
		CONVPrintF2( "\n  (p)PgnoFDP %d: Attempt to add SE of %d pages ending at page %d.",
				PgnoFDP( pfucb ), cpgSEReq, pgnoSELast );
#endif

		BTUp( pfucbAE );
		err = ErrSPIAddSecondaryExtent(
						pfucb,
						pfucbAE,
						pgnoSELast,
						cpgSEReq,
						pppspcache );
		Assert( errSPOutOfOwnExtCacheSpace != err );
		Assert( errSPOutOfAvailExtCacheSpace != err );
		Call( err );
		}
		
	else
		{
		//	allocate a secondary extent from the operating system
		//	by getting page number of last owned page, extending the
		//	file as possible and adding the sized secondary extent
		//	NOTE: only one user can do this at one time.
		//
		if ( dbidTemp == pfmp->Dbid() && !ppib->FBatchIndexCreation() )
			cpgSEMin = max( cpgMin, cpageSEDefault );
		else			
			cpgSEMin = max( cpgMin, PinstFromPpib( ppib )->m_cpgSESysMin );

		cpgSEReq = max( cpgReq, max( cpgPrimary/cSecFrac, cpgSEMin ) );
		
		if ( dbidTemp != pfmp->Dbid() )
			{
			//	Round up to multiple of system parameter.

			cpgSEReq += PinstFromPpib( ppib )->m_cpgSESysMin - 1;
			cpgSEReq -= cpgSEReq % PinstFromPpib( ppib )->m_cpgSESysMin;
		
			//	Round up to multiple of reasonable constant.

			cpgSEReq += cpageDbExtensionDefault - 1;
			cpgSEReq -= cpgSEReq % cpageDbExtensionDefault;
			}

		//	only permit async extension on batch index creation,
		//	or if the growth wouldn't be noticeable compared to
		//	the existing database size
		//
		const BOOL	fPermitAsyncExtension	= ( ppib->FBatchIndexCreation()
												|| cpgSEReq < pfmp->PgnoLast() / 100 );

		//	get extendingDB latch in order to check/update fExtendingDB
		//
		pfmp->GetExtendingDBLatch();
		fHoldExtendingDBLatch = fTrue;

		if ( pfmp->CpgDatabaseSizeMax() )
			{
			//	Check if it reaches the max size
			const ULONG		cpgNow	= pfmp->PgnoLast();

			Assert( 0 == pfmp->PgnoSLVLast() || pfmp->FSLVAttached() );

			if ( cpgNow + cpgSEReq + (CPG)pfmp->PgnoSLVLast() > pfmp->CpgDatabaseSizeMax() )
				{
				SPReportMaxDbSizeExceeded( pfucb->ifmp, cpgNow + (CPG)pfmp->PgnoSLVLast() );

				err = ErrERRCheck( JET_errOutOfDatabaseSpace );
				goto HandleError;
				}
			}

		Call( ErrSPIReservePages( pfucb, pfucbNil, spextSecondary ) );

		Call( ErrSPIExtendDB( pfucb, cpgSEMin, &cpgSEReq, &pgnoSELast, fPermitAsyncExtension ) );

		BTUp( pfucbAE );
		err = ErrSPIAddSecondaryExtent( pfucb, pfucbAE, pgnoSELast, cpgSEReq, pppspcache );
		Assert( errSPOutOfOwnExtCacheSpace != err );
		Assert( errSPOutOfAvailExtCacheSpace != err );
		Call( err );
		}
		
	AssertSPIPfucbOnRoot( pfucb );
	Assert( Pcsr( pfucbAE )->FLatched() );
	Assert( cpgSEReq >= cpgSEMin );

HandleError:
	if ( pfucbNil != pfucbParent )
		{
		Assert( pgnoNull != pgnoParentFDP );
		if ( pcsrNil != pfucbParent->pcsrRoot )
			{
			pfucbParent->pcsrRoot->ReleasePage();
			pfucbParent->pcsrRoot = pcsrNil;
			}
			
		Assert( !Pcsr( pfucbParent )->FLatched() );
		BTClose( pfucbParent );
		}
		
	if ( fHoldExtendingDBLatch )
		{
		Assert( pgnoNull == pgnoParentFDP );
		pfmp->ReleaseExtendingDBLatch();
		}

///	pfucb->u.pfcb->CritSpaceTrees().Leave();

	return err;
	}


LOCAL ERR ErrSPIAddFreedExtent(
	FUCB		*pfucbAE,
	const PGNO	pgnoParentFDP,
	const PGNO	pgnoLast, 
	const CPG	cpgSize )
	{
	ERR			err;
	FUCB		*pfucbParent	= pfucbNil;

	Assert( !Pcsr( pfucbAE )->FLatched() );

///	pfucbAE->u.pfcb->CritSpaceTrees().Enter();

	//	if pgnoNull == pgnoParentFDP, then we're in the database's space tree
	if ( pgnoNull != pgnoParentFDP )
		{
		Call( ErrBTIOpenAndGotoRoot( pfucbAE->ppib, pgnoParentFDP, pfucbAE->ifmp, &pfucbParent ) );
		}
		
	Call( ErrSPIReservePages( pfucbAE, pfucbParent, spextFreed ) );

	Call( ErrSPIAddToAvailExt( pfucbAE, pgnoLast, cpgSize ) );
	Assert( Pcsr( pfucbAE )->FLatched() );

HandleError:
	if ( pfucbNil != pfucbParent )
		{
		if ( pcsrNil != pfucbParent->pcsrRoot )
			{
			pfucbParent->pcsrRoot->ReleasePage();
			pfucbParent->pcsrRoot = pcsrNil;
			}
		BTClose( pfucbParent );
		}

///	pfucbAE->u.pfcb->CritSpaceTrees().Leave();

	return err;
	}


//	Check that the buffer passed to ErrSPGetInfo() is big enough to accommodate
//	the information requested.
//
INLINE ERR ErrSPCheckInfoBuf( const ULONG cbBufSize, const ULONG fSPExtents )
	{
	ULONG	cbUnchecked		= cbBufSize;

	if ( FSPOwnedExtent( fSPExtents ) )
		{
		if ( cbUnchecked < sizeof(CPG) )
			{
			return ErrERRCheck( JET_errBufferTooSmall );
			}
		cbUnchecked -= sizeof(CPG);

		//	if list needed, ensure enough space for list sentinel
		//
		if ( FSPExtentList( fSPExtents ) )
			{
			if ( cbUnchecked < sizeof(EXTENTINFO) )
				{
				return ErrERRCheck( JET_errBufferTooSmall );
				}
			cbUnchecked -= sizeof(EXTENTINFO);
			}
		}

	if ( FSPAvailExtent( fSPExtents ) )
		{
		if ( cbUnchecked < sizeof(CPG) )
			{
			return ErrERRCheck( JET_errBufferTooSmall );
			}
		cbUnchecked -= sizeof(CPG);

		//	if list needed, ensure enough space for list sentinel
		//
		if ( FSPExtentList( fSPExtents ) )
			{
			if ( cbUnchecked < sizeof(EXTENTINFO) )
				{
				return ErrERRCheck( JET_errBufferTooSmall );
				}
			cbUnchecked -= sizeof(EXTENTINFO);
			}
		}

	return JET_errSuccess;
	}
						  

LOCAL ERR ErrSPIGetInfo(
	FUCB		*pfucb,
	INT			*pcpgTotal,
	INT			*piext,
	INT			cext,
	EXTENTINFO	*rgext,
	INT			*pcextSentinelsRemaining,
	CPRINTF		* const pcprintf )
	{
	ERR			err;
	DIB			dib;
	INT			iext			= *piext;
	const BOOL	fExtentList		= ( cext > 0 );

	PGNO		pgnoLastSeen	= pgnoNull;
	CPG			cpgSeen 		= 0;
	ULONG		cRecords 		= 0;
	ULONG		cRecordsDeleted = 0;
	
	Assert( !fExtentList || NULL != pcextSentinelsRemaining );

	*pcpgTotal = 0;

	//  we will be traversing the entire tree in order, preread all the pages
	FUCBSetSequential( pfucb );
	FUCBSetPrereadForward( pfucb, cpgPrereadSequential );
	
	dib.dirflag = fDIRNull;
	dib.pos = posFirst;
	Assert( FFUCBSpace( pfucb ) );
	err = ErrBTDown( pfucb, &dib, latchReadNoTouch );

	if ( err != JET_errRecordNotFound )
		{
		Call( err );

		forever
			{
#ifdef DEBUG_DUMP_SPACE_INFO
			PGNO	pgnoLast;
			CPG		cpg;
			
			LongFromKey( &pgnoLast, pfucb->kdfCurr.key );
			cpg = *(UnalignedLittleEndian< CPG > *)pfucb->kdfCurr.data.Pv();
			printf( "\n    %d-%d %d", pgnoLast-cpg+1, pgnoLast, Pcsr( pfucb )->Pgno() );
			if ( FNDDeleted( pfucb->kdfCurr ) )
				printf( " (Del)" );
#endif

			if( pcprintf )
				{
				PGNO	pgnoLast;
				LongFromKey( &pgnoLast, pfucb->kdfCurr.key );
				
				const CPG cpg = *(UnalignedLittleEndian< CPG > *)pfucb->kdfCurr.data.Pv();	

				if( pgnoLastSeen != Pcsr( pfucb )->Pgno() )
					{
					pgnoLastSeen = Pcsr( pfucb )->Pgno();
					++cpgSeen;
					}
					
				(*pcprintf)( "%s:\t%d-%d %d [%d]%s\n",
								SzNameOfTable( pfucb ),
								pgnoLast-cpg+1,
								pgnoLast,
								cpg,
								Pcsr( pfucb )->Pgno(),
								FNDDeleted( pfucb->kdfCurr ) ? " (DEL)" : "" );

				++cRecords;
				if( FNDDeleted( pfucb->kdfCurr ) )
					{
					++cRecordsDeleted;
					}
				}				
				
			Assert( pfucb->kdfCurr.data.Cb() == sizeof(PGNO) );
			*pcpgTotal += *(UnalignedLittleEndian< PGNO > *)pfucb->kdfCurr.data.Pv();

			if ( fExtentList )
				{
				Assert( iext < cext );
				Assert( pfucb->kdfCurr.key.Cb() == sizeof(PGNO) );

				//	be sure to leave space for the sentinels
				//	(if no more room, we still want to keep
				//	calculating page count - we just can't
				//	keep track of individual extents anymore
				//
				Assert( iext + *pcextSentinelsRemaining <= cext );
				if ( iext + *pcextSentinelsRemaining < cext )
					{
					PGNO pgno;
					LongFromKey( &pgno, pfucb->kdfCurr.key );
					rgext[iext].pgnoLastInExtent = pgno;
					rgext[iext].cpgExtent = *(UnalignedLittleEndian< CPG > *)pfucb->kdfCurr.data.Pv();
					iext++;
					}
				}

			err = ErrBTNext( pfucb, fDIRNull );
			if ( err < 0 )
				{
				if ( err != JET_errNoCurrentRecord )
					goto HandleError;
				break;
				}
			}
		}

	if ( fExtentList )
		{
		Assert( iext < cext );

		rgext[iext].pgnoLastInExtent = pgnoNull;
		rgext[iext].cpgExtent = 0;
		iext++;

		Assert( NULL != pcextSentinelsRemaining );
		Assert( *pcextSentinelsRemaining > 0 );
		(*pcextSentinelsRemaining)--;
		}
		
	*piext = iext;
	Assert( *piext + *pcextSentinelsRemaining <= cext );

	if( pcprintf )
		{
		(*pcprintf)( "%s:\t%d pages, %d nodes, %d deleted nodes\n", 
						SzNameOfTable( pfucb ),
						cpgSeen,
						cRecords,
						cRecordsDeleted );
		}

	err = JET_errSuccess;

HandleError:
	return err;
	}


ERR ErrSPGetInfo( 
	PIB			*ppib, 
	const IFMP	ifmp, 
	FUCB		*pfucb,
	BYTE		*pbResult, 
	const ULONG	cbMax, 
	const ULONG	fSPExtents,
	CPRINTF * const pcprintf )
	{
	ERR			err;
	CPG			*pcpgOwnExtTotal;
	CPG			*pcpgAvailExtTotal;
	EXTENTINFO	*rgext;
	FUCB 		*pfucbT				= pfucbNil;
	INT			iext;

	//	must specify either owned extent or available extent (or both) to retrieve
	//
	if ( !( FSPOwnedExtent( fSPExtents ) || FSPAvailExtent( fSPExtents ) ) )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}

	CallR( ErrSPCheckInfoBuf( cbMax, fSPExtents ) );

	memset( pbResult, '\0', cbMax );

	//	setup up return information.  owned extent is followed by available extent.  
	//	This is followed by extent list for both trees
	//
	if ( FSPOwnedExtent( fSPExtents ) )
		{
		pcpgOwnExtTotal = (CPG *)pbResult;
		if ( FSPAvailExtent( fSPExtents ) )
			{
			pcpgAvailExtTotal = pcpgOwnExtTotal + 1;
			rgext = (EXTENTINFO *)( pcpgAvailExtTotal + 1 );
			}
		else
			{
			pcpgAvailExtTotal = NULL;
			rgext = (EXTENTINFO *)( pcpgOwnExtTotal + 1 );
			}
		}
	else
		{
		Assert( FSPAvailExtent( fSPExtents ) );
		pcpgOwnExtTotal = NULL;
		pcpgAvailExtTotal = (CPG *)pbResult;
		}

	const BOOL	fExtentList		= FSPExtentList( fSPExtents );
	const INT	cext			= fExtentList ? ( ( pbResult + cbMax ) - ( (BYTE *)rgext ) ) / sizeof(EXTENTINFO) : 0;
	INT			cextSentinelsRemaining;
	if ( fExtentList )
		{
		if ( FSPOwnedExtent( fSPExtents ) && FSPAvailExtent( fSPExtents ) )
			{
			Assert( cext >= 2 );
			cextSentinelsRemaining = 2;
			}
		else
			{
			Assert( cext >= 1 );
			cextSentinelsRemaining = 1;
			}
		}
	else
		{
		cextSentinelsRemaining = 0;
		}

	if ( pfucbNil == pfucb )
		{
		err = ErrBTOpen( ppib, pgnoSystemRoot, ifmp, &pfucbT );
		}
	else
		{
		err = ErrBTOpen( ppib, pfucb->u.pfcb, &pfucbT );
		}
	CallR( err );
	Assert( pfucbNil != pfucbT );

	Call( ErrBTIGotoRoot( pfucbT, latchReadTouch ) );
	Assert( pcsrNil == pfucbT->pcsrRoot );
	pfucbT->pcsrRoot = Pcsr( pfucbT );

	if ( !pfucbT->u.pfcb->FSpaceInitialized() )
		{
		//	UNDONE: Are there cuncurrency issues with updating the FCB
		//	while we only have a read latch?
		SPIInitFCB( pfucbT, fTrue );
		if( pgnoNull != pfucbT->u.pfcb->PgnoOE() )
			{
			BFPrereadPageRange( pfucbT->ifmp, pfucbT->u.pfcb->PgnoOE(), 2 );
			}
		}

	//	initialize number of extent list entries
	//
	iext = 0;

	if ( FSPOwnedExtent( fSPExtents ) )
		{
		Assert( NULL != pcpgOwnExtTotal );

		//	if single extent format, then free extent in external header
		//
		if ( pfucbT->u.pfcb->PgnoOE() == pgnoNull )
			{
			SPACE_HEADER 	*psph;

			if( pcprintf )
				{
				(*pcprintf)( "\n%s: OWNEXT: single extent\n", SzNameOfTable( pfucbT ) );
				}
				
			Assert( pfucbT->pcsrRoot != pcsrNil );
			Assert( pfucbT->pcsrRoot->Pgno() == PgnoFDP( pfucb ) );
			Assert( pfucbT->pcsrRoot->FLatched() );
			Assert( !FFUCBSpace( pfucbT ) );

			//	get external header
			//
			NDGetExternalHeader( pfucbT, pfucbT->pcsrRoot );
			Assert( sizeof( SPACE_HEADER ) == pfucbT->kdfCurr.data.Cb() );
			psph = reinterpret_cast <SPACE_HEADER *> ( pfucbT->kdfCurr.data.Pv() );

			*pcpgOwnExtTotal = psph->CpgPrimary();
			if ( fExtentList )
				{
				Assert( iext + cextSentinelsRemaining <= cext );
				if ( iext + cextSentinelsRemaining < cext )
					{
					rgext[iext].pgnoLastInExtent = PgnoFDP( pfucbT ) + psph->CpgPrimary() - 1;
					rgext[iext].cpgExtent = psph->CpgPrimary();
					iext++;
					}
					
				Assert( iext + cextSentinelsRemaining <= cext );
				rgext[iext].pgnoLastInExtent = pgnoNull;
				rgext[iext].cpgExtent = 0;
				iext++;
				
				Assert( cextSentinelsRemaining > 0 );
				cextSentinelsRemaining--;

				if ( iext == cext )
					{
					Assert( !FSPAvailExtent( fSPExtents ) );
					Assert( 0 == cextSentinelsRemaining );
					}
				else
					{
					Assert( iext < cext );
					Assert( iext + cextSentinelsRemaining <= cext );
					}
				}
			}
			
		else
			{
			//	open cursor on owned extent tree
			//
			FUCB	*pfucbOE = pfucbNil;
		
			Call( ErrSPIOpenOwnExt( ppib, pfucbT->u.pfcb, &pfucbOE ) );

			if( pcprintf )
				{
				(*pcprintf)( "\n%s: OWNEXT\n", SzNameOfTable( pfucbT ) );
				}

			err = ErrSPIGetInfo(
				pfucbOE,
				reinterpret_cast<INT *>( pcpgOwnExtTotal ),
				&iext,
				cext,
				rgext,
				&cextSentinelsRemaining,
				pcprintf );

			Assert( pfucbOE != pfucbNil );
			BTClose( pfucbOE );
			Call( err );
			}
		}

	if ( FSPAvailExtent( fSPExtents ) )
		{
		Assert( NULL != pcpgAvailExtTotal );
		Assert( !fExtentList || 1 == cextSentinelsRemaining );
		
		//	if single extent format, then free extent in external header
		//
		if ( pfucbT->u.pfcb->PgnoOE() == pgnoNull )
			{
			SPACE_HEADER 	*psph;

			if( pcprintf )
				{
				(*pcprintf)( "\n%s: AVAILEXT: single extent\n", SzNameOfTable( pfucbT ) );
				}

			Assert( pfucbT->pcsrRoot != pcsrNil );
			Assert( pfucbT->pcsrRoot->Pgno() == PgnoFDP( pfucb ) );
			Assert( pfucbT->pcsrRoot->FLatched() );
			Assert( !FFUCBSpace( pfucbT ) );

			//	get external header
			//
			NDGetExternalHeader( pfucbT, pfucbT->pcsrRoot );
			Assert( sizeof( SPACE_HEADER ) == pfucbT->kdfCurr.data.Cb() );
			psph = reinterpret_cast <SPACE_HEADER *> ( pfucbT->kdfCurr.data.Pv() );

			*pcpgAvailExtTotal = 0;
			
			//	continue through rgbitAvail finding all available extents
			//
			PGNO	pgnoT			= PgnoFDP( pfucbT ) + 1;
			CPG		cpgPrimarySeen	= 1;		//	account for pgnoFDP
			PGNO	pgnoPrevAvail	= pgnoNull;
			UINT	rgbitT;

			for ( rgbitT = 0x00000001;
				rgbitT != 0 && cpgPrimarySeen < psph->CpgPrimary();
				cpgPrimarySeen++, pgnoT++, rgbitT <<= 1 )
				{
				Assert( pgnoT <= PgnoFDP( pfucbT ) + cpgSmallSpaceAvailMost );
				
				if ( rgbitT & psph->RgbitAvail() ) 
					{
					(*pcpgAvailExtTotal)++;

					if ( fExtentList )
						{
						Assert( iext + cextSentinelsRemaining <= cext );
						if ( pgnoT == pgnoPrevAvail + 1 )
							{
							Assert( iext > 0 );
							const INT	iextPrev	= iext - 1;
							Assert( pgnoNull != pgnoPrevAvail );
							Assert( pgnoPrevAvail == rgext[iextPrev].pgnoLastInExtent );
							rgext[iextPrev].pgnoLastInExtent = pgnoT;
							rgext[iextPrev].cpgExtent++;
							
							Assert( rgext[iextPrev].pgnoLastInExtent - rgext[iextPrev].cpgExtent
									>= PgnoFDP( pfucbT ) );
									
							pgnoPrevAvail = pgnoT;
							}
						else if ( iext + cextSentinelsRemaining < cext )
							{
							rgext[iext].pgnoLastInExtent = pgnoT;
							rgext[iext].cpgExtent = 1;
							iext++;

							Assert( iext + cextSentinelsRemaining <= cext );
							
							pgnoPrevAvail = pgnoT;
							}
						}
					}
				}


			//	must also account for any pages that were not present in
			//	the space bitmap
			if ( psph->CpgPrimary() > cpgSmallSpaceAvailMost + 1 )
				{
				Assert( cpgSmallSpaceAvailMost + 1 == cpgPrimarySeen );
				const CPG	cpgRemaining		= psph->CpgPrimary() - ( cpgSmallSpaceAvailMost + 1 );

				(*pcpgAvailExtTotal) += cpgRemaining;

				if ( fExtentList )
					{
					Assert( iext + cextSentinelsRemaining <= cext );
					
					const PGNO	pgnoLastOfRemaining	= PgnoFDP( pfucbT ) + psph->CpgPrimary() - 1;
					if ( pgnoLastOfRemaining - cpgRemaining == pgnoPrevAvail )
						{
						Assert( iext > 0 );
						const INT	iextPrev	= iext - 1;
						Assert( pgnoNull != pgnoPrevAvail );
						Assert( pgnoPrevAvail == rgext[iextPrev].pgnoLastInExtent );
						rgext[iextPrev].pgnoLastInExtent = pgnoLastOfRemaining;
						rgext[iextPrev].cpgExtent += cpgRemaining;
						
						Assert( rgext[iextPrev].pgnoLastInExtent - rgext[iextPrev].cpgExtent
								>= PgnoFDP( pfucbT ) );
						}
					else if ( iext + cextSentinelsRemaining < cext )
						{
						rgext[iext].pgnoLastInExtent = pgnoLastOfRemaining;
						rgext[iext].cpgExtent = cpgRemaining;
						iext++;

						Assert( iext + cextSentinelsRemaining <= cext );
						}
					}
				
				}
				
			if ( fExtentList )
				{
				Assert( iext < cext );	//  otherwise ErrSPCheckInfoBuf would fail
				rgext[iext].pgnoLastInExtent = pgnoNull;
				rgext[iext].cpgExtent = 0;
				iext++;

				Assert( cextSentinelsRemaining > 0 );
				cextSentinelsRemaining--;
				
				Assert( iext + cextSentinelsRemaining <= cext );
				}
			}
			
		else
			{
			//	open cursor on available extent tree
			//
			FUCB	*pfucbAE = pfucbNil;

			Call( ErrSPIOpenAvailExt( ppib, pfucbT->u.pfcb, &pfucbAE ) );

			if( pcprintf )
				{
				(*pcprintf)( "\n%s: AVAILEXT\n", SzNameOfTable( pfucbT ) );
				}

			err = ErrSPIGetInfo(
				pfucbAE,
				reinterpret_cast<INT *>( pcpgAvailExtTotal ),
				&iext,
				cext,
				rgext,
				&cextSentinelsRemaining,
				pcprintf );
				
#ifdef DEBUG_DUMP_SPACE_INFO
printf( "\n\n" );
#endif

			Assert( pfucbAE != pfucbNil );
			BTClose( pfucbAE );
			Call( err );
			}

		//	if possible, verify AvailExt total against OwnExt total
		Assert( !FSPOwnedExtent( fSPExtents )
			|| ( *pcpgAvailExtTotal <= *pcpgOwnExtTotal ) );
		}
		
	Assert( 0 == cextSentinelsRemaining );
		

HandleError:
	Assert( pfucbNil != pfucbT );
	pfucbT->pcsrRoot = pcsrNil;
	BTClose( pfucbT );

	return err;
	}


//	HACK: this is a hack function to force a dummy logged update
//	in the database to ensure that after recovery, the dbtime in the
//	db header is greater than any of the dbtimes in pages belonging
//	to non-logged indexes
//
ERR ErrSPDummyUpdate( FUCB * pfucb )
	{
	ERR				err;
	SPACE_HEADER	sph;
	DATA			data;

	Assert( !Pcsr( pfucb )->FLatched() );

	CallR( ErrBTIGotoRoot( pfucb, latchRIW ) );

	//	get external header
	//
	NDGetExternalHeader( pfucb );
	Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );

	//	make copy of header
	//
	UtilMemCpy( &sph, pfucb->kdfCurr.data.Pv(), sizeof(sph) );

	//	upgrade to write latch
	//
	Pcsr( pfucb )->UpgradeFromRIWLatch();

	//	perform dummy logged update to force the dbtime
	//	counter to be incremented over any non-logged
	//	updates
	//
	data.SetPv( &sph );
	data.SetCb( sizeof(sph) );
	err = ErrNDSetExternalHeader( pfucb, &data, fDIRNull );

	//	release latch regardless of error
	//
	BTUp( pfucb );

	return err;
	}


#ifdef SPACECHECK

LOCAL ERR ErrSPIValidFDP( PIB *ppib, IFMP ifmp, PGNO pgnoFDP )
	{
	ERR			err;
	FUCB		*pfucb = pfucbNil;
	FUCB		*pfucbOE = pfucbNil;
	DIB			dib;
	BOOKMARK	bm;
	PGNO		pgnoOELast;
	CPG			cpgOESize;
	BYTE		rgbKey[sizeof(PGNO)];

	Assert( pgnoFDP != pgnoNull );

	//	get temporary FUCB
	//
	Call( ErrBTOpen( ppib, pgnoFDP, ifmp, &pfucb ) );
	Assert( pfucbNil != pfucb );
	Assert( pfucb->u.pfcb->FInitialized() );

	if ( pfucb->u.pfcb->PgnoOE() != pgnoNull )
		{
		//	open cursor on owned extent
		//
		Call( ErrSPIOpenOwnExt( ppib, pfucb->u.pfcb, &pfucbOE ) );

		//	search for pgnoFDP in owned extent tree
		//
		KeyFromLong( rgbKey, pgnoFDP );
		bm.Nullify();
		bm.key.suffix.SetPv( rgbKey );
		bm.key.suffix.SetCb( sizeof(PGNO) );
		dib.pos = posDown;
		dib.pbm = &bm;
		dib.dirflag = fDIRNull;
		Call( ErrBTDown( pfucbOE, &dib, latchReadTouch ) );
		if ( err == wrnNDFoundGreater )
			{
			Call( ErrBTGet( pfucbOE ) );
			}

		Assert( pfucbOE->kdfCurr.key.Cb() == sizeof( PGNO ) );
		LongFromKey( &pgnoOELast, pfucbOE->kdfCurr.key );

		Assert( pfucbOE->kdfCurr.data.Cb() == sizeof(PGNO) );
		cpgOESize = *(UnalignedLittleEndian< PGNO > *)pfucbOE->kdfCurr.data.Pv();

		//	FDP page should be first page of primary extent
		//
		Assert( pgnoFDP == pgnoOELast - cpgOESize + 1 );
		}

HandleError:
	if ( pfucbOE != pfucbNil )
		BTClose( pfucbOE );
	if ( pfucb != pfucbNil )
		BTClose( pfucb );
	return err;
	}


//	checks if an extent described by pgnoFirst, cpgSize was allocated
//
LOCAL ERR ErrSPIWasAlloc(
	FUCB	*pfucb,
	PGNO	pgnoFirst, 
	CPG		cpgSize )
	{
	ERR		err;
	FUCB	*pfucbOE = pfucbNil;
	FUCB	*pfucbAE = pfucbNil;
	DIB		dib;
	BOOKMARK bm;
	PGNO	pgnoOwnLast;
	CPG		cpgOwnExt;
	PGNO	pgnoAvailLast;
	PGNO	pgnoLast = pgnoFirst + cpgSize - 1;
	CPG  	cpgAvailExt;
	BYTE	rgbKey[sizeof(PGNO)];

	if ( pfucb->u.pfcb->PgnoOE() == pgnoNull )
		{
		SPACE_HEADER 	*psph;
		UINT			rgbitT;
		INT				iT;

		AssertSPIPfucbOnRoot( pfucb );

		//	get external header
		//
		NDGetExternalHeader( pfucb, pfucb->pcsrRoot );
		Assert( sizeof( SPACE_HEADER ) == pfucb->kdfCurr.data.Cb() );
		psph = reinterpret_cast <SPACE_HEADER *> ( pfucb->kdfCurr.data.Pv() );
		//	make mask for extent to check
		//
		for ( rgbitT = 1, iT = 1; 
			iT < cpgSize && iT < cpgSmallSpaceAvailMost; 
			iT++ )
			rgbitT = (rgbitT<<1) + 1;
		Assert( pgnoFirst - PgnoFDP( pfucb ) < cpgSmallSpaceAvailMost );
		if ( pgnoFirst != PgnoFDP( pfucb ) )
			{
			rgbitT<<(pgnoFirst - PgnoFDP( pfucb ) - 1);
			Assert( ( psph->RgbitAvail() & rgbitT ) == 0 );
			}

		goto HandleError;
		}

	//	open cursor on owned extent
	//
	Call( ErrSPIOpenOwnExt( pfucb->ppib, pfucb->u.pfcb, &pfucbOE ) );

	//	check that the given extent is owned by the given FDP but not
	//	available in the FDP available extent.
	//
	KeyFromLong( rgbKey, pgnoLast );
	bm.key.prefix.Nullify();
	bm.key.suffix.SetCb( sizeof(PGNO) );
	bm.key.suffix.SetPv( rgbKey );
	bm.data.Nullify();
	dib.pos = posDown;
	dib.pbm = &bm;
	dib.dirflag = fDIRNull;
	Call( ErrBTDown( pfucbOE, &dib, latchReadNoTouch ) );
	if ( err == wrnNDFoundLess )
		{
		Assert( fFalse );
		Assert( Pcsr( pfucbOE )->Cpage().Clines() - 1 == 
					Pcsr( pfucbOE )->ILine() );
		Assert( pgnoNull != Pcsr( pfucbOE )->Cpage().PgnoNext() );

		Call( ErrBTNext( pfucbOE, fDIRNull ) );
		err = ErrERRCheck( wrnNDFoundGreater );

		#ifdef DEBUG
		PGNO	pgnoT;
		Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoT, pfucbOE->kdfCurr.key );
		Assert( pgnoT > pgnoLast );
		#endif
		}
		
	if ( err == wrnNDFoundGreater )
		{
		Call( ErrBTGet( pfucbOE ) );
		}
	
	Assert( pfucbOE->kdfCurr.key.Cb() == sizeof(PGNO) );
	LongFromKey( &pgnoOwnLast, pfucbOE->kdfCurr.key );
	Assert( pfucbOE->kdfCurr.data.Cb() == sizeof(PGNO) );
	cpgOwnExt = *(UnalignedLittleEndian< PGNO > *) pfucbOE->kdfCurr.data.Pv();
	Assert( pgnoFirst >= pgnoOwnLast - cpgOwnExt + 1 );
	BTUp( pfucbOE );

	//	check that the extent is not in available extent.  Since the BT search
	//	is keyed with the last page of the extent to be freed, it is sufficient
	//	to check that the last page of the extent to be freed is in the found
	//	extent to determine the full extent has not been allocated.  
	//	If available extent is empty then the extent cannot be in available extent
	//	and has been allocated.
	//
	Call( ErrSPIOpenAvailExt( pfucb->ppib, pfucb->u.pfcb, &pfucbAE ) );
	
	if ( ( err = ErrBTDown( pfucbAE, &dib, latchReadNoTouch ) ) < 0 )
		{
		if ( err == JET_errNoCurrentRecord )
			{
			err = JET_errSuccess;
			goto HandleError;
			}
		goto HandleError;
		}

	//	extent should not be found in available extent tree
	//
	Assert( err != JET_errSuccess );
		
	if ( err == wrnNDFoundGreater )
		{
		Call( ErrBTNext( pfucbAE, fDIRNull ) );
		Assert( pfucbAE->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoAvailLast, pfucbAE->kdfCurr.key );
		Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
		cpgAvailExt = *(UnalignedLittleEndian< PGNO > *)pfucbAE->kdfCurr.data.Pv();
		Assert( cpgAvailExt != 0 );
		//	last page of extent should be < first page in available extent node
		//	 
		Assert( pgnoLast < pgnoAvailLast - cpgAvailExt + 1 );
		}
	else
		{
		Assert( err == wrnNDFoundLess );
		Call( ErrBTPrev( pfucbAE, fDIRNull ) );
		Assert( pfucbAE->kdfCurr.key.Cb() == sizeof(PGNO) );
		LongFromKey( &pgnoAvailLast, pfucbAE->kdfCurr.key );
		Assert( pfucbAE->kdfCurr.data.Cb() == sizeof(PGNO) );
		cpgAvailExt = *(UnalignedLittleEndian< PGNO > *)pfucbAE->kdfCurr.data.Pv();

		//	first page of extent > last page in available extent node
		//
		Assert( pgnoFirst > pgnoAvailLast );
		}
		
HandleError:
	if ( pfucbOE != pfucbNil )
		BTClose( pfucbOE );
	if ( pfucbAE != pfucbNil )
		BTClose( pfucbAE );
	
	return JET_errSuccess;
	}

#endif	//	SPACE_CHECK
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\sortapi.cxx ===
#include "std.hxx"
#include "_comp.hxx"


LOCAL LIDMAP	lidmapDefrag;

const INT fCOLSDELETEDNone		= 0;		//	Flags to determine if any columns have been deleted.
const INT fCOLSDELETEDFixedVar	= (1<<0);
const INT fCOLSDELETEDTagged	= (1<<1);

INLINE BOOL FCOLSDELETEDNone	( INT fColumnsDeleted )		{ return ( fColumnsDeleted == fCOLSDELETEDNone ); }
INLINE BOOL FCOLSDELETEDFixedVar( INT fColumnsDeleted )		{ return ( fColumnsDeleted & fCOLSDELETEDFixedVar ); }
INLINE BOOL FCOLSDELETEDTagged	( INT fColumnsDeleted )		{ return ( fColumnsDeleted & fCOLSDELETEDTagged ); }

INLINE VOID FCOLSDELETEDSetNone		( INT& fColumnsDeleted )
	{
	fColumnsDeleted = fCOLSDELETEDNone;
	Assert( FCOLSDELETEDNone( fColumnsDeleted ) );
	}
INLINE VOID FCOLSDELETEDSetFixedVar	( INT& fColumnsDeleted )
	{
	fColumnsDeleted |= fCOLSDELETEDFixedVar;
	Assert( FCOLSDELETEDFixedVar( fColumnsDeleted ) );
	}
INLINE VOID FCOLSDELETEDSetTagged	( INT& fColumnsDeleted )
	{
	fColumnsDeleted |= fCOLSDELETEDTagged;
	Assert( FCOLSDELETEDTagged( fColumnsDeleted ) );
	}


LOCAL ERR ErrSORTTableOpen(
	PIB				*ppib,
	JET_COLUMNDEF	*rgcolumndef,
	ULONG			ccolumndef,
	IDXUNICODE		*pidxunicode,
	JET_GRBIT		grbit,
	FUCB			**ppfucb,
	JET_COLUMNID	*rgcolumnid )
	{
	ERR				err						= JET_errSuccess;
	INT				icolumndefMax			= (INT)ccolumndef;
	FUCB  			*pfucb					= pfucbNil;
	TDB				*ptdb					= ptdbNil;
	JET_COLUMNDEF	*pcolumndef				= NULL;
	JET_COLUMNID	*pcolumnid				= NULL;
	JET_COLUMNDEF	*pcolumndefMax			= rgcolumndef+icolumndefMax;
	TCIB			tcib					= { fidFixedLeast-1, fidVarLeast-1, fidTaggedLeast-1 };
	WORD			ibRec;
	BOOL			fTruncate;
	BOOL			fIndexOnLocalizedText	= fFalse;
	IDXSEG			rgidxseg[JET_ccolKeyMost];
	ULONG			iidxsegMac;
	IDB				idb;
	idb.SetCidxsegConditional( 0 );

	CheckPIB( ppib );

	INST			*pinst = PinstFromPpib( ppib );

	//  always remove duplicates unless this is a forward only sort and the
	//  caller didn't ask for duplicate removal
	BOOL fRemoveDuplicates = !( ( grbit & JET_bitTTForwardOnly ) && !( grbit & JET_bitTTUnique ) );

	CallJ( ErrSORTOpen( ppib, &pfucb, fRemoveDuplicates, fFalse ), SimpleError );
	*ppfucb = pfucb;

	//	save open flags
	//
	pfucb->u.pscb->grbit = grbit;

	//	determine max field ids and fix up lengths
	//

	//====================================================
	// Determine field "mode" as follows:
	// if ( JET_bitColumnTagged given ) or "long" ==> TAGGED
	// else if ( numeric type || JET_bitColumnFixed given ) ==> FIXED
	// else ==> VARIABLE
	//====================================================
	// Determine maximum field length as follows:
	// switch ( field type )
	//	   case numeric:
	//		   max = <exact length of specified type>;
	//	   case "short" textual:
	//		   if ( specified max == 0 ) max = JET_cbColumnMost
	//		   else max = MIN( JET_cbColumnMost, specified max )
	//====================================================
	for ( pcolumndef = rgcolumndef, pcolumnid = rgcolumnid; pcolumndef < pcolumndefMax; pcolumndef++, pcolumnid++ )
		{
		if ( FRECSLV( pcolumndef->coltyp ) )
			{
#ifdef TEMP_SLV
			if ( ( *pcolumnid = ++tcib.fidTaggedLast ) > fidTaggedMost )
				{
				Error( ErrERRCheck( JET_errTooManyColumns ), HandleError );
				}
#else
			Assert( fFalse );	//	streaming file on temp db currently not supported
			Error( ErrERRCheck( JET_errSLVStreamingFileNotCreated ), HandleError );
#endif
			}
		else if ( ( pcolumndef->grbit & JET_bitColumnTagged )
			|| FRECLongValue( pcolumndef->coltyp ) )
			{
			if ( ( *pcolumnid = ++tcib.fidTaggedLast ) > fidTaggedMost )
				{
				Error( ErrERRCheck( JET_errTooManyColumns ), HandleError );
				}
			}
		else if ( pcolumndef->coltyp == JET_coltypBit ||
			pcolumndef->coltyp == JET_coltypUnsignedByte ||
			pcolumndef->coltyp == JET_coltypShort ||
			pcolumndef->coltyp == JET_coltypLong ||
			pcolumndef->coltyp == JET_coltypCurrency ||
			pcolumndef->coltyp == JET_coltypIEEESingle ||
			pcolumndef->coltyp == JET_coltypIEEEDouble ||
			pcolumndef->coltyp == JET_coltypDateTime ||
			( pcolumndef->grbit & JET_bitColumnFixed ) )
			{
			if ( ( *pcolumnid = ++tcib.fidFixedLast ) > fidFixedMost )
				{
				Error( ErrERRCheck( JET_errTooManyColumns ), HandleError );
				}
			}
		else
			{
			if ( ( *pcolumnid = ++tcib.fidVarLast ) > fidVarMost )
				Error( ErrERRCheck( JET_errTooManyColumns ), HandleError );
			}
		}

	Assert( pfucb->u.pscb->fcb.FTypeSort() );
	Assert( pfucb->u.pscb->fcb.FPrimaryIndex() );
	Assert( pfucb->u.pscb->fcb.FSequentialIndex() );

	Call( ErrTDBCreate( pinst, &ptdb, &tcib ) );

	Assert( ptdb->ItagTableName() == 0 );		// No name associated with temp. tables

	pfucb->u.pscb->fcb.SetPtdb( ptdb );
	Assert( ptdbNil != pfucb->u.pscb->fcb.Ptdb() );
	Assert( pidbNil == pfucb->u.pscb->fcb.Pidb() );

	ibRec = ibRECStartFixedColumns;

	iidxsegMac = 0;
	for ( pcolumndef = rgcolumndef, pcolumnid = rgcolumnid; pcolumndef < pcolumndefMax; pcolumndef++, pcolumnid++ )
		{
		FIELD	field;
		BOOL	fLocalizedText	= fFalse;

		memset( &field, 0, sizeof(FIELD) );
		field.coltyp = FIELD_COLTYP( pcolumndef->coltyp );
		if ( FRECTextColumn( field.coltyp ) )
			{
			//	check code page
			//
			switch( pcolumndef->cp )
				{
				case usUniCodePage:
					fLocalizedText = fTrue;
					field.cp = usUniCodePage;
					break;

				case 0:
				case usEnglishCodePage:
					field.cp = usEnglishCodePage;
					break;

				default:
					Error( ErrERRCheck( JET_errInvalidCodePage ), HandleError );
					break;
				}
			}

		Assert( field.coltyp != JET_coltypNil );
		field.cbMaxLen = UlCATColumnSize( field.coltyp, pcolumndef->cbMax, &fTruncate );

		//	ibRecordOffset is only relevant for fixed fields.  It will be ignored by
		//	RECAddFieldDef(), so do not set it.
		//
		if ( FCOLUMNIDFixed( *pcolumnid ) )
			{
			field.ibRecordOffset = ibRec;
			ibRec = WORD( ibRec + field.cbMaxLen );
			}

		Assert( !FCOLUMNIDTemplateColumn( *pcolumnid ) );
		Call( ErrRECAddFieldDef( ptdb, *pcolumnid, &field ) );

		if ( ( pcolumndef->grbit & JET_bitColumnTTKey ) && iidxsegMac < JET_ccolKeyMost )
			{
			rgidxseg[iidxsegMac].ResetFlags();
			rgidxseg[iidxsegMac].SetFid( FidOfColumnid( *pcolumnid ) );

			if ( pcolumndef->grbit & JET_bitColumnTTDescending )
				rgidxseg[iidxsegMac].SetFDescending();

			if ( fLocalizedText )
				fIndexOnLocalizedText = fTrue;

			iidxsegMac++;
			}
		}
	Assert( ibRec <= cbRECRecordMost );
	ptdb->SetIbEndFixedColumns( ibRec, ptdb->FidFixedLast() );

	//	set up the IDB and index definition if necessary
	//
	Assert( iidxsegMac <= JET_ccolKeyMost );
	idb.SetCidxseg( (BYTE)iidxsegMac );
	if ( iidxsegMac > 0 )
		{
		idb.SetCbVarSegMac( JET_cbPrimaryKeyMost );

		idb.ResetFlags();

		if ( NULL != pidxunicode )
			{
			*( idb.Pidxunicode() ) = *pidxunicode;
			Call( ErrFILEICheckUserDefinedUnicode( pinst, idb.Pidxunicode() ) );

			idb.SetFLocaleId();
			}
		else
			{
			Assert( !idb.FLocaleId() );
			*( idb.Pidxunicode() ) = pinst->m_idxunicodeDefault;
			}

		if ( fIndexOnLocalizedText )
			idb.SetFLocalizedText();
		if ( grbit & JET_bitTTSortNullsHigh )
			idb.SetFSortNullsHigh();

		idb.SetFPrimary();
		idb.SetFAllowAllNulls();
		idb.SetFAllowFirstNull();
		idb.SetFAllowSomeNulls();
		idb.SetFUnique();
		idb.SetItagIndexName( 0 );	// Sorts and temp tables don't store index name

		Call( ErrIDBSetIdxSeg( &idb, ptdb, rgidxseg ) );

		Call( ErrFILEIGenerateIDB( &( pfucb->u.pscb->fcb ), ptdb, &idb ) );

		Assert( pidbNil != pfucb->u.pscb->fcb.Pidb() );

		pfucb->u.pscb->fcb.ResetSequentialIndex();
		}

	Assert( ptdbNil != pfucb->u.pscb->fcb.Ptdb() );

	//	reset copy buffer
	//
	pfucb->pvWorkBuf = NULL;
	pfucb->dataWorkBuf.SetPv( NULL );
	FUCBResetUpdateFlags( pfucb );

	//	reset key buffer
	//
	pfucb->dataSearchKey.Nullify();
	pfucb->cColumnsInSearchKey = 0;
	KSReset( pfucb );

	return JET_errSuccess;

HandleError:
	SORTClose( pfucb );
SimpleError:
	*ppfucb = pfucbNil;
	return err;
	}


ERR VTAPI ErrIsamSortOpen(
	PIB					*ppib,
	JET_COLUMNDEF		*rgcolumndef,
	ULONG				ccolumndef,
	JET_UNICODEINDEX	*pidxunicode,
	JET_GRBIT			grbit,
	FUCB				**ppfucb,
	JET_COLUMNID		*rgcolumnid )
	{
	ERR				err;
	FUCB 			*pfucb;

	CallR( ErrSORTTableOpen( ppib, rgcolumndef, ccolumndef, (IDXUNICODE *)pidxunicode, grbit, &pfucb, rgcolumnid ) );
	Assert( pfucb->u.pscb->fcb.WRefCount() == 1 );
	SORTBeforeFirst( pfucb );

	pfucb->pvtfndef = &vtfndefTTSortIns;

	//	sort is done on the temp database which is always updatable
	//
	FUCBSetUpdatable( pfucb );
	*ppfucb = pfucb;

	return err;
	}



ERR VTAPI ErrIsamSortSetIndexRange( JET_SESID sesid, JET_VTID vtid, JET_GRBIT grbit )
	{
	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );
	ERR		err = JET_errSuccess;

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );
	Assert( pfucb->u.pscb->grbit & JET_bitTTScrollable|JET_bitTTIndexed );

	if ( !FKSPrepared( pfucb ) )
		{
		return ErrERRCheck( JET_errKeyNotMade );
		}

	FUCBSetIndexRange( pfucb, grbit );
	err =  ErrSORTCheckIndexRange( pfucb );

	//	reset key status
	//
	KSReset( pfucb );

	//	if instant duration index range, then reset index range.
	//
	if ( grbit & JET_bitRangeInstantDuration )
		{
		DIRResetIndexRange( pfucb );
		}

	return err;
	}


ERR VTAPI ErrIsamSortMove( JET_SESID sesid, JET_VTID vtid, LONG csrid, JET_GRBIT grbit )
	{
	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );
	ERR		err = JET_errSuccess;

	Assert( !FSCBInsert( pfucb->u.pscb ) );

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );

	//	assert reset copy buffer status
	//
	Assert( !FFUCBUpdatePrepared( pfucb ) );

	//	move forward csrid records
	//
	if ( csrid > 0 )
		{
		if ( csrid == JET_MoveLast )
			{
			err = ErrSORTLast( pfucb );
			}
		else
			{
			while ( csrid-- > 0 )
				{
				if ( ( err = ErrSORTNext( pfucb ) ) < 0 )
					break;
				}
			}
		}
	else if ( csrid < 0 )
		{
		Assert( ( pfucb->u.pscb->grbit & ( JET_bitTTScrollable | JET_bitTTIndexed ) ) );
		if ( csrid == JET_MoveFirst )
			{
			err = ErrSORTFirst( pfucb );
			}
		else
			{
			while ( csrid++ < 0 )
				{
				if ( ( err = ErrSORTPrev( pfucb ) ) < 0 )
					break;
				}
			}
		}
	else
		{
		//	return currency status for move 0
		//
		Assert( csrid == 0 );
		if ( pfucb->u.pscb->ispairMac > 0
			&& pfucb->ispairCurr < pfucb->u.pscb->ispairMac
			&& pfucb->ispairCurr >= 0 )
			{
			err = JET_errSuccess;
			}
		else
			{
			err = ErrERRCheck( JET_errNoCurrentRecord );
			}
		}

	return err;
	}


ERR VTAPI ErrIsamSortSeek( JET_SESID sesid, JET_VTID vtid, JET_GRBIT grbit )
	{
	const BOOL 	fGT = ( grbit & ( JET_bitSeekGT | JET_bitSeekGE ) );
	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );
	ERR		err = JET_errSuccess;
	KEY		key;

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );

	//	assert reset copy buffer status
	//
	Assert( !FFUCBUpdatePrepared( pfucb ) );
	Assert( pfucb->u.pscb->grbit & JET_bitTTIndexed );

	if ( !( FKSPrepared( pfucb ) ) )
		{
		return ErrERRCheck( JET_errKeyNotMade );
		}

	FUCBAssertValidSearchKey( pfucb );

	//	ignore segment counter
	key.prefix.Nullify();
	key.suffix.SetPv( pfucb->dataSearchKey.Pv() );
	key.suffix.SetCb( pfucb->dataSearchKey.Cb() );

	//	perform seek for equal to or greater than
	//
	err = ErrSORTSeek( pfucb, &key, fGT );
	if ( err >= 0 )
		{
		KSReset( pfucb );
		}

	Assert( err == JET_errSuccess
		|| err == JET_errRecordNotFound
		|| err == JET_wrnSeekNotEqual );

	const INT bitSeekAll = ( JET_bitSeekEQ
							| JET_bitSeekGE
							| JET_bitSeekGT
							| JET_bitSeekLE
							| JET_bitSeekLT );

	//	take additional action if necessary or polymorph error return
	//	based on grbit
	//
	switch ( grbit & bitSeekAll )
		{
		case JET_bitSeekEQ:
			if ( JET_wrnSeekNotEqual == err )
				err = ErrERRCheck( JET_errRecordNotFound );
			case JET_bitSeekGE:
			case JET_bitSeekLE:
				break;
		case JET_bitSeekLT:
			if ( JET_wrnSeekNotEqual == err )
				err = JET_errSuccess;
			else if ( JET_errSuccess == err )
				{
				err = ErrIsamSortMove( ppib, pfucb, JET_MovePrevious, NO_GRBIT );
				if ( JET_errNoCurrentRecord == err )
					err = ErrERRCheck( JET_errRecordNotFound );
				}
			break;
		default:
			Assert( grbit == JET_bitSeekGT );
			if ( JET_wrnSeekNotEqual == err )
				err = JET_errSuccess;
			else if ( JET_errSuccess == err )
				{
				err = ErrIsamSortMove( ppib, pfucb, JET_MoveNext, NO_GRBIT );
				if ( JET_errNoCurrentRecord == err )
					err = ErrERRCheck( JET_errRecordNotFound );
				}
			break;
		}

	return err;
	}


ERR VTAPI ErrIsamSortGetBookmark(
	JET_SESID		sesid,
	JET_VTID		vtid,
	VOID * const	pvBookmark,
	const ULONG		cbMax,
	ULONG * const	pcbActual )
	{
	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );
	SCB		* const pscb	= pfucb->u.pscb;
	ERR		err = JET_errSuccess;
	LONG	ipb;

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );
	Assert( pvBookmark != NULL );
	Assert( pscb->crun == 0 );

	if ( cbMax < sizeof( ipb ) )
		{
		return ErrERRCheck( JET_errBufferTooSmall );
		}

	//	bookmark on sort is index to pointer to byte
	//
	ipb = pfucb->ispairCurr;
	if ( ipb < 0 || ipb >= pfucb->u.pscb->ispairMac )
		return ErrERRCheck( JET_errNoCurrentRecord );

	*(long *)pvBookmark = ipb;

	if ( pcbActual )
		{
		*pcbActual = sizeof(ipb);
		}

	Assert( err == JET_errSuccess );
	return err;
	}


ERR VTAPI ErrIsamSortGotoBookmark(
	JET_SESID			sesid,
	JET_VTID			vtid,
	const VOID * const	pvBookmark,
	const ULONG			cbBookmark )
	{
	ERR					err;
	PIB * const			ppib	= ( PIB * )( sesid );
	FUCB * const 		pfucb	= ( FUCB * )( vtid );

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );
	Assert( pfucb->u.pscb->crun == 0 );

	//	assert reset copy buffer status
	//
	Assert( !FFUCBUpdatePrepared( pfucb ) );

	if ( cbBookmark != sizeof( long )
		|| NULL == pvBookmark )
		{
		return ErrERRCheck( JET_errInvalidBookmark );
		}

	Assert( *( long *)pvBookmark < pfucb->u.pscb->ispairMac );
	Assert( *( long *)pvBookmark >= 0 );

	pfucb->ispairCurr = *(LONG *)pvBookmark;

	Assert( err == JET_errSuccess );
	return err;
	}


#ifdef DEBUG

ERR VTAPI ErrIsamSortRetrieveKey(
	JET_SESID		sesid,
	JET_VTID		vtid,
	VOID*			pv,
	const ULONG		cbMax,
	ULONG*			pcbActual,
	JET_GRBIT		grbit )
	{
 	PIB* const		ppib	= ( PIB * )( sesid );
	FUCB* const		pfucb	= ( FUCB * )( vtid );

	return ErrIsamRetrieveKey( ppib, pfucb, (BYTE *)pv, cbMax, pcbActual, NO_GRBIT );
	}

#endif	// DEBUG


/*	update only supports insert
/**/
ERR VTAPI ErrIsamSortUpdate(
	JET_SESID		sesid,
	JET_VTID		vtid,
	VOID			* pv,
	ULONG			cbMax,
	ULONG			* pcbActual,
	JET_GRBIT		grbit )
	{
 	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );

	BYTE  	rgbKeyBuf[ JET_cbPrimaryKeyMost ];
	KEY		key;
	ERR		err = JET_errSuccess;

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucb );

	Assert( FFUCBSort( pfucb ) );
	Assert( pfucb->u.pscb->fcb.FTypeSort() );
	if ( !( FFUCBInsertPrepared( pfucb ) ) )
		{
		return ErrERRCheck( JET_errUpdateNotPrepared );
		}
	Assert( pfucb->u.pscb != pscbNil );
	Assert( pfucb->u.pscb->fcb.Ptdb() != ptdbNil );
	/*	cannot get bookmark before sorting.
	/**/

	/*	record to use for put
	/**/
	if ( pfucb->dataWorkBuf.FNull() )
		{
		return ErrERRCheck( JET_errRecordNoCopy );
		}
	else
		{
		BOOL fIllegalNulls;
		CallR( ErrRECIIllegalNulls( pfucb, pfucb->dataWorkBuf, &fIllegalNulls ) )
		if ( fIllegalNulls )
			return ErrERRCheck( JET_errNullInvalid );
		}

	key.prefix.Nullify();
	key.suffix.SetPv( rgbKeyBuf );
	key.suffix.SetCb( sizeof( rgbKeyBuf ) );

	//	UNDONE:	sort to support tagged columns
	Assert( pfucb->u.pscb->fcb.Pidb() != pidbNil );
	CallR( ErrRECRetrieveKeyFromCopyBuffer(
		pfucb,
		pfucb->u.pscb->fcb.Pidb(),
		&key,
		1,
		0,
		NULL,
		prceNil ) );

	CallS( ErrRECValidIndexKeyWarning( err ) );
	Assert( wrnFLDOutOfKeys != err );
	Assert( wrnFLDOutOfTuples != err );
	Assert( wrnFLDNotPresentInIndex != err );

	/*	return err if sort requires no NULL segment and segment NULL
	/**/
	if ( pfucb->u.pscb->fcb.Pidb()->FNoNullSeg()
		&& ( wrnFLDNullSeg == err || wrnFLDNullFirstSeg == err || wrnFLDNullKey == err ) )
		{
		return ErrERRCheck( JET_errNullKeyDisallowed );
		}

	/*	add if sort allows
	/**/
	if ( JET_errSuccess == err
		|| ( wrnFLDNullKey == err && pfucb->u.pscb->fcb.Pidb()->FAllowAllNulls() )
		|| ( wrnFLDNullFirstSeg == err && pfucb->u.pscb->fcb.Pidb()->FAllowFirstNull() )
		|| ( wrnFLDNullSeg == err && pfucb->u.pscb->fcb.Pidb()->FAllowFirstNull() ) )
		{
		Assert( key.Cb() <= JET_cbPrimaryKeyMost );
		CallR( ErrSORTInsert( pfucb, key, pfucb->dataWorkBuf ) );
		}

	RECIFreeCopyBuffer( pfucb );
	FUCBResetUpdateFlags( pfucb );

	return err;
	}


ERR VTAPI ErrIsamSortDupCursor(
	JET_SESID		sesid,
	JET_VTID		vtid,
	JET_TABLEID		*ptableid,
	JET_GRBIT		grbit )
	{
 	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );

	FUCB   			**ppfucbDup	= (FUCB **)ptableid;
	FUCB   			*pfucbDup = pfucbNil;
	ERR				err = JET_errSuccess;

	if ( FFUCBIndex( pfucb ) )
		{
		err = ErrIsamDupCursor( ppib, pfucb, ppfucbDup, grbit );
		return err;
		}

	INST *pinst = PinstFromPpib( ppib );
	Call( ErrFUCBOpen( ppib, pinst->m_mpdbidifmp[ dbidTemp ], &pfucbDup ) );
  	pfucb->u.pscb->fcb.Link( pfucbDup );
	pfucbDup->pvtfndef = &vtfndefTTSortIns;

	pfucbDup->ulFlags = pfucb->ulFlags;

	pfucbDup->dataSearchKey.Nullify();
	pfucbDup->cColumnsInSearchKey = 0;
	KSReset( pfucbDup );

	/*	initialize working buffer to unallocated
	/**/
	pfucbDup->pvWorkBuf = NULL;
	pfucbDup->dataWorkBuf.SetPv( NULL );
	FUCBResetUpdateFlags( pfucbDup );

	/*	move currency to the first record and ignore error if no records
	/**/
	err = ErrIsamSortMove( ppib, pfucbDup, (ULONG)JET_MoveFirst, 0 );
	if ( err < 0  )
		{
		if ( err != JET_errNoCurrentRecord )
			goto HandleError;
		err = JET_errSuccess;
		}

	*ppfucbDup = pfucbDup;

	return JET_errSuccess;

HandleError:
	if ( pfucbDup != pfucbNil )
		{
		FUCBClose( pfucbDup );
		}
	return err;
	}


ERR VTAPI ErrIsamSortClose(
	JET_SESID		sesid,
	JET_VTID		vtid
	)
	{
 	PIB		* const ppib	= ( PIB * )( sesid );
	FUCB	* const pfucb	= ( FUCB * )( vtid );
	ERR		err = JET_errSuccess;

	CallR( ErrPIBCheck( ppib ) );
	Assert( pfucb->pvtfndef != &vtfndefInvalidTableid );
	pfucb->pvtfndef = &vtfndefInvalidTableid;

	if ( FFUCBIndex( pfucb ) )
		{
		CheckTable( ppib, pfucb );
		CallS( ErrFILECloseTable( ppib, pfucb ) );
		}
	else
		{
		CheckSort( ppib, pfucb );
		Assert( FFUCBSort( pfucb ) );

		/*	release key buffer
		/**/
		RECReleaseKeySearchBuffer( pfucb );

		/*	release working buffer
		/**/
		RECIFreeCopyBuffer( pfucb );

		SORTClose( pfucb );
		}

	return err;
	}


ERR VTAPI ErrIsamSortGetTableInfo(
	JET_SESID		sesid,
	JET_VTID		vtid,
	VOID			* pv,
	ULONG			cbOutMax,
	ULONG			lInfoLevel )
	{
	FUCB	* const pfucb	= ( FUCB * )( vtid );

	if ( lInfoLevel != JET_TblInfo )
		{
		return ErrERRCheck( JET_errInvalidOperation );
		}

	/*	check buffer size
	/**/
	if ( cbOutMax < sizeof(JET_OBJECTINFO) )
		{
		return ErrERRCheck( JET_errInvalidParameter );
		}

	memset( (BYTE *)pv, 0x00, (SHORT)cbOutMax );
	( (JET_OBJECTINFO *)pv )->cbStruct = sizeof(JET_OBJECTINFO);
	( (JET_OBJECTINFO *)pv )->objtyp   = JET_objtypTable;

	// Get maximum number of retrievable records.  If sort is totally
	// in-memory, then this number is exact, because we can predetermine
	// how many dupes there are (see CspairSORTIUnique()).  However,
	// if the sort had to be moved to disk, we cannot predetermine
	// how many dupes will be filtered out.  In this case, this number
	// may be larger than the number of retrievable records (it is
	// equivalent to the number of records pumped into the sort
	// in the first place).
	//
	// UNDONE: can't handle more than ULONG_MAX records
	//
	( (JET_OBJECTINFO *)pv )->cRecord  = ULONG( min( pfucb->u.pscb->cRecords, ULONG_MAX ) );

	return JET_errSuccess;
	}


// Advances the copy progress meter.
INLINE ERR ErrSORTCopyProgress(
	STATUSINFO	* pstatus,
	const ULONG	cPagesTraversed )
	{
	JET_SNPROG	snprog;

	Assert( pstatus->pfnStatus );
	Assert( pstatus->snt == JET_sntProgress );

	pstatus->cunitDone += ( cPagesTraversed * pstatus->cunitPerProgression );

	Assert( pstatus->cunitProjected <= pstatus->cunitTotal );
	if ( pstatus->cunitDone > pstatus->cunitProjected )
		{
		Assert( fGlobalRepair );
		pstatus->cunitPerProgression = 0;
		pstatus->cunitDone = pstatus->cunitProjected;
		}

	Assert( pstatus->cunitDone <= pstatus->cunitTotal );

	snprog.cbStruct = sizeof( JET_SNPROG );
	snprog.cunitDone = pstatus->cunitDone;
	snprog.cunitTotal = pstatus->cunitTotal;

	return ( ERR )( *pstatus->pfnStatus )(
		pstatus->sesid,
		pstatus->snp,
		pstatus->snt,
		&snprog );
	}


ERR ErrSORTIncrementLVRefcountDest(
	PIB				* ppib,
	const LID		lidSrc,
	LID				* const plidDest )
	{
	Assert( JET_tableidNil != lidmapDefrag.Tableid() );
	return lidmapDefrag.ErrIncrementLVRefcountDest(
				ppib,
				lidSrc,
				plidDest );
	}

// This function assumes that the source record has already been completely copied
// over to the destination record.  The only thing left to do is rescan the tagged
// portion of the record looking for separated long values.  If we find any,
// copy them over and update the record's LID accordingly.
INLINE ERR ErrSORTUpdateSeparatedLVs(
	FUCB				* pfucbSrc,
	FUCB				* pfucbDest,
	JET_COLUMNID		* mpcolumnidcolumnidTagged,
	STATUSINFO			* pstatus )
	{
	TAGFIELDS			tagfields( pfucbDest->dataWorkBuf );
	return tagfields.ErrUpdateSeparatedLongValuesAfterCopy(
						pfucbSrc,
						pfucbDest,
						mpcolumnidcolumnidTagged,
						pstatus );
	}

INLINE ERR ErrSORTCopyTaggedColumns(
	FUCB				* pfucbSrc,
	FUCB				* pfucbDest,
	BYTE				* pbRecBuf,
	JET_COLUMNID		* mpcolumnidcolumnidTagged,
	STATUSINFO			* pstatus )
	{
	Assert( Pcsr( pfucbSrc )->FLatched() );

	TAGFIELDS			tagfields( pfucbSrc->kdfCurr.data );

	//	Copy the tagged columns into the record buffer,
	//	to avoid complex latching/unlatching/refreshing
	//	while copying separated long values
	tagfields.Migrate( pbRecBuf );
	CallS( ErrDIRRelease( pfucbSrc ) );

	tagfields.CopyTaggedColumns(
						pfucbSrc,
						pfucbDest,
						mpcolumnidcolumnidTagged );

 	return ErrSORTUpdateSeparatedLVs(
						pfucbSrc,
						pfucbDest,
						mpcolumnidcolumnidTagged,
						pstatus );
	}


// Returns a count of the bytes copied.
INLINE SIZE_T CbSORTCopyFixedVarColumns(
	TDB				*ptdbSrc,
	TDB				*ptdbDest,
   	CPCOL			*rgcpcol,			// Only used for DEBUG
	ULONG			ccpcolMax,			// Only used for DEBUG
	BYTE			* const pbRecSrc,
	BYTE			* const pbRecDest )
	{
	REC										* const precSrc = (REC *)pbRecSrc;
	REC										* const precDest = (REC *)pbRecDest;
	UnalignedLittleEndian< REC::VAROFFSET > *pibVarOffsSrc;
	UnalignedLittleEndian< REC::VAROFFSET > *pibVarOffsDest;
	BYTE									*prgbitNullSrc;
	BYTE									*prgbitNullDest;
	BYTE									*prgbitNullT;
	FID										fidFixedLastDest;
	FID										fidVarLastDest;
	FID										fidT;
	BYTE									*pbChunkSrc = 0;
	BYTE									*pbChunkDest = 0;
	INT										cbChunk;

	const FID		fidFixedLastSrc	= precSrc->FidFixedLastInRec();
	const FID		fidVarLastSrc	= precSrc->FidVarLastInRec();

	prgbitNullSrc = precSrc->PbFixedNullBitMap();
	pibVarOffsSrc = precSrc->PibVarOffsets();

	Assert( (BYTE *)pibVarOffsSrc > pbRecSrc );
	Assert( (BYTE *)pibVarOffsSrc < pbRecSrc + REC::CbRecordMax() );

	// Need some space for the null-bit array.  Use the space after the
	// theoretical maximum space for fixed columns (ie. if all fixed columns
	// were set).  Assert that the null-bit array will fit in the pathological case.
	const INT	cFixedColumnsDest = ( ptdbDest->FidFixedLast() - fidFixedLeast + 1 );
	Assert( ptdbSrc->IbEndFixedColumns() < REC::CbRecordMax() );
	Assert( ptdbDest->IbEndFixedColumns() <= ptdbSrc->IbEndFixedColumns() );
	Assert( ptdbDest->IbEndFixedColumns() + ( ( cFixedColumnsDest + 7 ) / 8 )
			<= REC::CbRecordMax() );
	prgbitNullT = pbRecDest + ptdbDest->IbEndFixedColumns();
	memset( prgbitNullT, 0, ( cFixedColumnsDest + 7 ) / 8 );

	pbChunkSrc = pbRecSrc + ibRECStartFixedColumns;
	pbChunkDest = pbRecDest + ibRECStartFixedColumns;
	cbChunk = 0;

#ifdef DEBUG
	BOOL			fSawPlaceholderColumn	= fFalse;
#endif

	Assert( !ptdbSrc->FInitialisingDefaultRecord() );

	fidFixedLastDest = fidFixedLeast-1;
	for ( fidT = fidFixedLeast; fidT <= fidFixedLastSrc; fidT++ )
		{
		const BOOL	fTemplateColumn	= ptdbSrc->FFixedTemplateColumn( fidT );
		const WORD	ibNextOffset	= ptdbSrc->IbOffsetOfNextColumn( fidT );
		const FIELD	*pfieldFixedSrc	= ptdbSrc->PfieldFixed( ColumnidOfFid( fidT, fTemplateColumn ) );

		// Copy only undeleted columns
		if ( JET_coltypNil == pfieldFixedSrc->coltyp
			|| FFIELDPrimaryIndexPlaceholder( pfieldFixedSrc->ffield ) )
			{
#ifdef DEBUG
			if ( FFIELDPrimaryIndexPlaceholder( pfieldFixedSrc->ffield ) )
				fSawPlaceholderColumn = fTrue;
			else
				Assert( !fTemplateColumn );
#endif
			if ( cbChunk > 0 )
				{
				UtilMemCpy( pbChunkDest, pbChunkSrc, cbChunk );
				pbChunkDest += cbChunk;
				}

			pbChunkSrc = pbRecSrc + ibNextOffset;
			cbChunk = 0;
			}
		else
			{
#ifdef DEBUG		// Assert that the fids match what the columnid map says
			BOOL	fFound = fFalse;
			INT		i;

			for ( i = 0; i < ccpcolMax; i++ )
				{
				if ( rgcpcol[i].columnidSrc == ColumnidOfFid( fidT, fTemplateColumn )  )
					{
					const COLUMNID	columnidT	= ColumnidOfFid(
														FID( fidFixedLastDest+1 ),
														ptdbDest->FFixedTemplateColumn( FID( fidFixedLastDest+1 ) ) );
					Assert( rgcpcol[i].columnidDest == columnidT );
					fFound = fTrue;
					break;
					}
				}
			if ( fidT >= ptdbSrc->FidFixedFirst() )
				{
				// Only columns in the derived table are in the columnid map.
				Assert( fFound );
				}
			else
				{
				// Base table columns are not in the columnid map.  Since base
				// tables have fixed DDL, the columnid in the source and destination
				// table should not have changed.
				Assert( !fFound );
				Assert( fidT == fidFixedLastDest+1
					|| ( fSawPlaceholderColumn && fidT == fidFixedLastDest+1+1 ) );		//	should only be one placeholder column
				}
#endif

			// If the source field is null, assert that the destination column
			// has also been flagged as such.
			const UINT	ifid	= fidT - fidFixedLeast;

			Assert( FFixedNullBit( prgbitNullSrc + ( ifid/8 ), ifid )
				|| !FFixedNullBit( prgbitNullT + ( fidFixedLastDest / 8 ), fidFixedLastDest ) );
			if ( FFixedNullBit( prgbitNullSrc + ( ifid/8 ), ifid ) )
				{
				SetFixedNullBit(
					prgbitNullT + ( fidFixedLastDest / 8 ),
					fidFixedLastDest );
 				}

			// Don't increment till here, because the code above requires
			// the fid as an index.

			fidFixedLastDest++;

#ifdef DEBUG
			const COLUMNID	columnidT		= ColumnidOfFid(
													fidFixedLastDest,
													ptdbDest->FFixedTemplateColumn( fidFixedLastDest ) );
			const FIELD		* const pfieldT	= ptdbDest->PfieldFixed( columnidT );
			Assert( ibNextOffset > pfieldFixedSrc->ibRecordOffset );
			Assert( pfieldT->ibRecordOffset == pbChunkDest + cbChunk - pbRecDest );
			Assert( pfieldT->ibRecordOffset >= ibRECStartFixedColumns );
			Assert( pfieldT->ibRecordOffset < REC::CbRecordMax() );
			Assert( pfieldT->ibRecordOffset < ptdbDest->IbEndFixedColumns() );
#endif

			Assert( pfieldFixedSrc->cbMaxLen ==
						(ULONG)( ibNextOffset - pfieldFixedSrc->ibRecordOffset ) );
			cbChunk += pfieldFixedSrc->cbMaxLen;
			}
		}

	Assert( fidFixedLastDest <= ptdbDest->FidFixedLast() );

	if ( cbChunk > 0 )
		{
		UtilMemCpy( pbChunkDest, pbChunkSrc, cbChunk );
		}

	Assert( prgbitNullT == pbRecDest + ptdbDest->IbEndFixedColumns() );
	if ( fidFixedLastDest < ptdbDest->FidFixedLast() )
		{
		const COLUMNID	columnidT		= ColumnidOfFid(
												FID( fidFixedLastDest+1 ),
												ptdbDest->FFixedTemplateColumn( FID( fidFixedLastDest+1 ) ) );
		const FIELD		* const pfieldT	= ptdbDest->PfieldFixed( columnidT );
		prgbitNullDest = pbRecDest + pfieldT->ibRecordOffset;

		// Shift the null-bit array into place.
		memmove( prgbitNullDest, prgbitNullT, ( fidFixedLastDest + 7 ) / 8 );
		}
	else
		{
		prgbitNullDest = prgbitNullT;
		}

	// Should end up at the start of the null-bit array.
	Assert( pbChunkDest + cbChunk == prgbitNullDest );


	// The variable columns must be done in two passes.  The first pass
	// just determines the highest variable columnid in the record.
	// The second pass does the work.
	pibVarOffsDest = (UnalignedLittleEndian<REC::VAROFFSET> *)(
						prgbitNullDest + ( ( fidFixedLastDest + 7 ) / 8 ) );

	fidVarLastDest = fidVarLeast-1;
	for ( fidT = fidVarLeast; fidT <= fidVarLastSrc; fidT++ )
		{
		const COLUMNID	columnidVarSrc			= ColumnidOfFid( fidT, ptdbSrc->FVarTemplateColumn( fidT ) );
		const FIELD		* const pfieldVarSrc	= ptdbSrc->PfieldVar( columnidVarSrc );

		// Only care about undeleted columns
		Assert( pfieldVarSrc->coltyp != JET_coltypNil
			|| !FCOLUMNIDTemplateColumn( columnidVarSrc ) );
		if ( pfieldVarSrc->coltyp != JET_coltypNil )
			{
#ifdef DEBUG		// Assert that the fids match what the columnid map says
			BOOL		fFound = fFalse;
			INT			i;

			for ( i = 0; i < ccpcolMax; i++ )
				{
				if ( rgcpcol[i].columnidSrc == columnidVarSrc )
					{
					const COLUMNID	columnidT		= ColumnidOfFid(
															FID( fidVarLastDest+1 ),
															ptdbDest->FVarTemplateColumn( FID( fidVarLastDest+1 ) ) );
					Assert( rgcpcol[i].columnidDest == columnidT );
					fFound = fTrue;
					break;
					}
				}
			if ( fidT >= ptdbSrc->FidVarFirst() )
				{
				// Only columns in the derived table are in the columnid map.
				Assert( fFound );
				}
			else
				{
				// Base table columns are not in the columnid map.  Since base
				// tables have fixed DDL, the columnid in the source and destination
				// table should not have changed.
				Assert( !fFound );
				Assert( fidT == fidVarLastDest+1 );
				}
#endif

			fidVarLastDest++;
			}
		}
	Assert( fidVarLastDest <= ptdbDest->FidVarLast() );

	// The second iteration through the variable columns, we copy the column data
	// and update the offsets and nullity.
	pbChunkSrc = (BYTE *)( pibVarOffsSrc + ( fidVarLastSrc - fidVarLeast + 1 ) );
	Assert( pbChunkSrc == precSrc->PbVarData() );
	pbChunkDest = (BYTE *)( pibVarOffsDest + ( fidVarLastDest - fidVarLeast + 1 ) );
	cbChunk = 0;

#ifdef DEBUG
	const FID	fidVarLastSave = fidVarLastDest;
#endif

	fidVarLastDest = fidVarLeast-1;
	for ( fidT = fidVarLeast; fidT <= fidVarLastSrc; fidT++ )
		{
		const COLUMNID	columnidVarSrc			= ColumnidOfFid( fidT, ptdbSrc->FVarTemplateColumn( fidT ) );
		const FIELD		* const pfieldVarSrc	= ptdbSrc->PfieldVar( columnidVarSrc );
		const UINT		ifid					= fidT - fidVarLeast;

		// Only care about undeleted columns
		Assert( pfieldVarSrc->coltyp != JET_coltypNil
			|| !FCOLUMNIDTemplateColumn( columnidVarSrc ) );
		if ( pfieldVarSrc->coltyp == JET_coltypNil )
			{
			if ( cbChunk > 0 )
				{
				UtilMemCpy( pbChunkDest, pbChunkSrc, cbChunk );
				pbChunkDest += cbChunk;
				}

			pbChunkSrc = precSrc->PbVarData() + IbVarOffset( pibVarOffsSrc[ifid] );
			cbChunk = 0;
			}
		else
			{
			const REC::VAROFFSET	ibStart = ( fidVarLeast-1 == fidVarLastDest ?
													REC::VAROFFSET( 0 ) :
													IbVarOffset( pibVarOffsDest[fidVarLastDest-fidVarLeast] ) );
			INT	cb;

			fidVarLastDest++;
			if ( FVarNullBit( pibVarOffsSrc[ifid] ) )
				{
				pibVarOffsDest[fidVarLastDest-fidVarLeast] = ibStart;
				SetVarNullBit( *( UnalignedLittleEndian< WORD >*)(&pibVarOffsDest[fidVarLastDest-fidVarLeast]) );
				cb = 0;
				}
			else
				{
				if ( ifid > 0 )
					{
					Assert( IbVarOffset( pibVarOffsSrc[ifid] ) >=
								IbVarOffset( pibVarOffsSrc[ifid-1] ) );
					cb = IbVarOffset( pibVarOffsSrc[ifid] )
							- IbVarOffset( pibVarOffsSrc[ifid-1] );
					}
				else
					{
					Assert( IbVarOffset( pibVarOffsSrc[ifid] ) >= 0 );
					cb = IbVarOffset( pibVarOffsSrc[ifid] );
					}

				pibVarOffsDest[fidVarLastDest-fidVarLeast] = REC::VAROFFSET( ibStart + cb );
				Assert( !FVarNullBit( pibVarOffsDest[fidVarLastDest-fidVarLeast] ) );
				}

			cbChunk += cb;
			}
		}

	Assert( fidVarLastDest == fidVarLastSave );

	if ( cbChunk > 0 )
		{
		UtilMemCpy( pbChunkDest, pbChunkSrc, cbChunk );
		}

	precDest->SetFidFixedLastInRec( fidFixedLastDest );
	precDest->SetFidVarLastInRec( fidVarLastDest );
	precDest->SetIbEndOfFixedData( REC::RECOFFSET( (BYTE *)pibVarOffsDest - pbRecDest ) );

	// Should end up at the start of the tagflds.
	Assert( precDest->PbTaggedData() == pbChunkDest + cbChunk );

	Assert( precDest->IbEndOfVarData() <= precSrc->IbEndOfVarData() );

	return precDest->PbVarData() + precDest->IbEndOfVarData() - pbRecDest;
	}


INLINE VOID SORTCheckVarTaggedCols( const REC *prec, const ULONG cbRec, const TDB *ptdb )
	{
#if 0	//	enable only to fix corruption
	const BYTE			*pbRecMax			= (BYTE *)prec + cbRec;
	TAGFLD				*ptagfld;
	TAGFLD				*ptagfldPrev;
	FID					fid;
	const WORD			wCorruptBit			= 0x0400;

	Assert( prec->FidFixedLastInRec() <= ptdb->FidFixedLast() );
	Assert( prec->FidVarLastInRec() <= ptdb->FidVarLast() );


	UnalignedLittleEndian<REC::VAROFFSET>	*pibVarOffs		= prec->PibVarOffsets();

	Assert( (BYTE *)pibVarOffs <= pbRecMax );

	Assert( !FVarNullBit( pibVarOffs[fidVarLastInRec+1-fidVarLeast] ) );
	Assert( ibVarOffset( pibVarOffs[fidVarLastInRec+1-fidVarLeast] ) ==
		pibVarOffs[fidVarLastInRec+1-fidVarLeast] );
	Assert( pibVarOffs[fidVarLastInRec+1-fidVarLeast] > sizeof(RECHDR) );
	Assert( pibVarOffs[fidVarLastInRec+1-fidVarLeast] <= cbRec );

	for ( fid = fidVarLeast; fid <= fidVarLastInRec; fid++ )
		{
		WORD	db;
		Assert( ibVarOffset( pibVarOffs[fid-fidVarLeast] ) > sizeof(RECHDR) );
		Assert( (ULONG)ibVarOffset( pibVarOffs[fid-fidVarLeast] ) <= cbRec );
		Assert( ibVarOffset( pibVarOffs[fid+1-fidVarLeast] ) > sizeof(RECHDR) );
		Assert( (ULONG)ibVarOffset( pibVarOffs[fid+1-fidVarLeast] ) <= cbRec );
		Assert( ibVarOffset( pibVarOffs[fid-fidVarLeast] ) <= ibVarOffset( pibVarOffs[fid+1-fidVarLeast] ) );

		db = ibVarOffset( pibVarOffs[fid+1-fidVarLeast] ) - ibVarOffset( pibVarOffs[ fid-fidVarLeast] );
		Assert( db >= 0 );
		if ( db > JET_cbColumnMost && ( pibVarOffs[fid+1-fidVarLeast] & wCorruptBit ) )
			{
			pibVarOffs[fid+1-fidVarLeast] &= ~wCorruptBit;
			db = ibVarOffset( pibVarOffs[fid+1-fidVarLeast] ) - ibVarOffset( pibVarOffs [fid-fidVarLeast] );
			printf( "\nReset corrupt bit in VarOffset entry." );
			}
		Assert( db <= JET_cbColumnMost );
		}

CheckTagged:
	ptagfldPrev = (TAGFLD*)( pbRec + pibVarOffs[fidVarLastInRec+1-fidVarLeast] );
	for ( ptagfld = (TAGFLD*)( pbRec + pibVarOffs[fidVarLastInRec+1-fidVarLeast] );
		(BYTE *)ptagfld < pbRecMax;
		ptagfld = PtagfldNext( ptagfld ) )
		{
		if ( ptagfld->fid > pfdb->fidTaggedLast && ( ptagfld->fid & wCorruptBit ) )
			{
			ptagfld->fid &= ~wCorruptBit;
			printf( "\nReset corrupt bit in Fid of TAGFLD." );
			}
		Assert( ptagfld->fid <= pfdb->fidTaggedLast );
		Assert( FTaggedFid( ptagfld->fid ) );
		if ( (BYTE *)PtagfldNext( ptagfld ) > pbRecMax
			&& ( ptagfld->cbData & wCorruptBit ) )
			{
			ptagfld->cbData &= ~wCorruptBit;
			printf( "\nReset corrupt bit in Cb of TAGFLD." );
			}

		if ( ptagfld->fid < ptagfldPrev->fid )
			{
#ifdef INTRINSIC_LV
			BYTE	rgb[g_cbPageMax];
#else // INTRINSIC_LV
			BYTE	rgb[sizeof(TAGFLD)+cbLVIntrinsicMost];
#endif // INTRINSIC_LV
			ULONG	cbCopy			= sizeof(TAGFLD)+ptagfldPrev->cb;
			BYTE	*pbNext			= (BYTE *)PtagfldNext( ptagfld );

			Assert( cbCopy-sizeof(TAGFLD) <= cbLVIntrinsicMost );
			UtilMemCpy( rgb, ptagfldPrev, cbCopy );
			memmove( ptagfldPrev, ptagfld, sizeof(TAGFLD)+ptagfld->cb );

			ptagfld = PtagfldNext( ptagfldPrev );
			UtilMemCpy( ptagfld, rgb, cbCopy );
			Assert( PtagfldNext( ptagfld ) == m_pbNext );
			printf( "\nFixed TAGFLD out of order." );

			//	restart from beginning to verify order
			goto CheckTagged;
			}
		ptagfldPrev = ptagfld;

		Assert( (BYTE *)PtagfldNext( ptagfld ) <= pbRecMax );
		}
	Assert( (BYTE *)ptagfld == pbRecMax );
#endif	//	0
	}


INLINE ERR ErrSORTCopyOneRecord(
	FUCB			* pfucbSrc,
	FUCB			* pfucbDest,
	BOOKMARK		* const pbmPrimary,
	INT				fColumnsDeleted,
	BYTE			* pbRecBuf,
	CPCOL			* rgcpcol,
	ULONG			ccpcolMax,
	JET_COLUMNID	* mpcolumnidcolumnidTagged,
	STATUSINFO		* pstatus )
	{
	ERR				err				= JET_errSuccess;
	BYTE			* pbRecSrc		= 0;
	BYTE			* pbRecDest		= 0;
	ULONG			cbRecSrc;
	SIZE_T			cbRecSrcFixedVar;
	SIZE_T			cbRecDestFixedVar;

	Assert( Pcsr( pfucbSrc )->FLatched() );
	CallS( ErrDIRRelease( pfucbSrc ) );		// Must release latch in order to call prepare update.

	// Work buffer pre-allocated IsamPrepareUpdate won't continually
	// allocate one.
	Assert( NULL != pfucbDest->pvWorkBuf );
	Assert( pfucbDest->dataWorkBuf.Pv() == pfucbDest->pvWorkBuf );

	//	setup pfucbDest for insert
	//
	Call( ErrIsamPrepareUpdate( pfucbDest->ppib, pfucbDest, JET_prepInsert ) );

	//	re-access source record
	//
	Assert( !Pcsr( pfucbSrc )->FLatched() );
	Call( ErrDIRGet( pfucbSrc ) );

	pbRecSrc = (BYTE *)pfucbSrc->kdfCurr.data.Pv();
	cbRecSrc = pfucbSrc->kdfCurr.data.Cb();
	pbRecDest = (BYTE *)pfucbDest->dataWorkBuf.Pv();

	Assert( cbRecSrc >= REC::cbRecordMin );
	Assert( cbRecSrc <= REC::CbRecordMax() );

	Assert( ( (REC *)pbRecSrc )->FidFixedLastInRec() <= pfucbSrc->u.pfcb->Ptdb()->FidFixedLast() );
	Assert( ( (REC *)pbRecSrc )->FidVarLastInRec() <= pfucbSrc->u.pfcb->Ptdb()->FidVarLast() );

	cbRecSrcFixedVar = ( (REC *)pbRecSrc )->PbTaggedData() - pbRecSrc;
	Assert( cbRecSrcFixedVar >= REC::cbRecordMin );
	Assert( cbRecSrcFixedVar <= cbRecSrc );

	SORTCheckVarTaggedCols( (REC *)pbRecSrc, cbRecSrc, pfucbSrc->u.pfcb->Ptdb() );

	if ( FCOLSDELETEDNone( fColumnsDeleted ) )
		{
		// Do the copy as one big chunk.
		UtilMemCpy( pbRecDest, pbRecSrc, cbRecSrc );
		pfucbDest->dataWorkBuf.SetCb( cbRecSrc );
		cbRecDestFixedVar = cbRecSrcFixedVar;
		}
	else
		{
		if ( FCOLSDELETEDFixedVar( fColumnsDeleted ) )
			{
			cbRecDestFixedVar = CbSORTCopyFixedVarColumns(
										pfucbSrc->u.pfcb->Ptdb(),
										pfucbDest->u.pfcb->Ptdb(),
										rgcpcol,
										ccpcolMax,
										pbRecSrc,
										pbRecDest );
			}

		else
			{
			UtilMemCpy( pbRecDest, pbRecSrc, cbRecSrcFixedVar );
			cbRecDestFixedVar = cbRecSrcFixedVar;
			}

		Assert( cbRecDestFixedVar >= REC::cbRecordMin );
		Assert( cbRecDestFixedVar <= cbRecSrcFixedVar );

		if ( FCOLSDELETEDTagged( fColumnsDeleted ) )
			{
			pfucbDest->dataWorkBuf.SetCb( cbRecDestFixedVar );
			Call( ErrSORTCopyTaggedColumns(
				pfucbSrc,
				pfucbDest,
				pbRecBuf,
				mpcolumnidcolumnidTagged,
				pstatus ) );
			AssertDIRNoLatch( pfucbSrc->ppib );

			Assert( pfucbDest->dataWorkBuf.Cb() >= cbRecDestFixedVar );
			Assert( pfucbDest->dataWorkBuf.Cb() <= cbRecSrc );

			// When we copied the tagged columns, we also took care of
			// copying the separated LV's.  We're done now, so go ahead and
			// insert the record.
			goto InsertRecord;
			}
		else
			{
			UtilMemCpy(
				pbRecDest+cbRecDestFixedVar,
				pbRecSrc+cbRecSrcFixedVar,
				cbRecSrc - cbRecSrcFixedVar );
			pfucbDest->dataWorkBuf.SetCb( cbRecDestFixedVar + ( cbRecSrc - cbRecSrcFixedVar ) );

			Assert( pfucbDest->dataWorkBuf.Cb() >= cbRecDestFixedVar );
			Assert( pfucbDest->dataWorkBuf.Cb() <= cbRecSrc );
			}
		}

	Assert( Pcsr( pfucbSrc )->FLatched() );
	CallS( ErrDIRRelease( pfucbSrc ) );

	// Now fix up the LIDs for separated long values, if any.
	Call( ErrSORTUpdateSeparatedLVs(
		pfucbSrc,
		pfucbDest,
		mpcolumnidcolumnidTagged,
		pstatus ) );

InsertRecord:
	if ( pstatus != NULL )
		{
		const FID	fidFixedLast	= ( (REC *)pbRecDest )->FidFixedLastInRec();
		const FID	fidVarLast		= ( (REC *)pbRecDest )->FidVarLastInRec();

		Assert( fidFixedLast >= fidFixedLeast-1 );
		Assert(	fidFixedLast <= pfucbDest->u.pfcb->Ptdb()->FidFixedLast() );
		Assert( fidVarLast >= fidVarLeast-1 );
		Assert( fidVarLast <= pfucbDest->u.pfcb->Ptdb()->FidVarLast() );

		// Do not count record header.
		const INT	cbOverhead =
						ibRECStartFixedColumns								// Record header + offset to tagged fields
						+ ( ( fidFixedLast + 1 - fidFixedLeast ) + 7 ) / 8	// Null array for fixed columns
						+ ( fidVarLast + 1 - fidVarLeast ) * sizeof(WORD);	// Variable offsets array
		Assert( cbRecDestFixedVar >= cbOverhead );

		// Do not count offsets tables or null arrays.
		pstatus->cbRawData += ( cbRecDestFixedVar - cbOverhead );
		}

	// for the moment we try to preserve de sequencial index in order to don't change the bookmark of the record
	// its' a temporary hack for SLV SPACEMAP
	if ( pidbNil == pfucbSrc->u.pfcb->Pidb() )
		{
		//	file is sequential
		//
		DBK	dbk;

		Assert ( pfucbSrc->bmCurr.key.Cb() == sizeof(DBK) );
		LongFromKey( &dbk, pfucbSrc->bmCurr.key );
		Assert( dbk > 0 );

		pfucbDest->u.pfcb->Ptdb()->SetDbkMost( dbk );
		}

#ifdef DEBUG
	FID	fidAutoInc;
	BOOL f8BytesAutoInc;
	fidAutoInc = pfucbDest->u.pfcb->Ptdb()->FidAutoincrement();
	f8BytesAutoInc = pfucbDest->u.pfcb->Ptdb()->F8BytesAutoInc();
	if ( fidAutoInc != 0 )
		{
		Assert( FFixedFid( fidAutoInc ) );
		Assert(	fidAutoInc <= pfucbDest->u.pfcb->Ptdb()->FidFixedLast() );
		Assert( FFUCBColumnSet( pfucbDest, fidAutoInc ) );

		// Need to set fid to zero to short-circuit AutoInc check
		// in ErrRECIInsert().
		pfucbDest->u.pfcb->Ptdb()->ResetFidAutoincrement();
		}

	err = ErrRECInsert( pfucbDest, pbmPrimary );

	if ( fidAutoInc != 0 )
		{
		// Reset AutoInc after update.
		pfucbDest->u.pfcb->Ptdb()->SetFidAutoincrement( fidAutoInc, f8BytesAutoInc );
		}

	Call( err );


#else
	Call( ErrRECInsert( pfucbDest, pbmPrimary ) );
#endif	//	DEBUG

HandleError:
	// Work buffer preserved for next record.
	Assert( NULL != pfucbDest->pvWorkBuf || err < 0 );

	return err;
	}


// Verify integrity of columnid maps.
INLINE VOID SORTAssertColumnidMaps(
	TDB				*ptdb,
	CPCOL			*rgcpcol,
	ULONG			ccpcolMax,
	JET_COLUMNID	*mpcolumnidcolumnidTagged,
	INT				fColumnsDeleted,
	const BOOL		fTemplateTable )
	{
#ifdef DEBUG

	INT	i;
	if ( FCOLSDELETEDFixedVar( fColumnsDeleted ) )
		{
		// Ensure columnids are monotonically increasing.
		for ( i = 0; i < (INT)ccpcolMax; i++ )
			{
			if ( FCOLUMNIDTemplateColumn( rgcpcol[i].columnidSrc ) )
				{
				Assert( fTemplateTable );
				Assert( FCOLUMNIDTemplateColumn( rgcpcol[i].columnidDest ) );
				}
			else
				{
				Assert( !fTemplateTable );
				Assert( !FCOLUMNIDTemplateColumn( rgcpcol[i].columnidDest ) );
				}

			Assert( FidOfColumnid( rgcpcol[i].columnidDest ) <= FidOfColumnid( rgcpcol[i].columnidSrc ) );
			if ( FCOLUMNIDFixed( rgcpcol[i].columnidSrc ) )
				{
				Assert( FCOLUMNIDFixed( rgcpcol[i].columnidDest ) );
				if ( i > 0 )
					{
					Assert( rgcpcol[i].columnidDest == rgcpcol[i-1].columnidDest + 1 );
					}
				}
			else
				{
				Assert( FCOLUMNIDVar( rgcpcol[i].columnidSrc ) );
				Assert( FCOLUMNIDVar( rgcpcol[i].columnidDest ) );
				if ( i > 0 )
					{
					if ( FCOLUMNIDVar( rgcpcol[i-1].columnidDest ) )
						{
						Assert( rgcpcol[i].columnidDest == rgcpcol[i-1].columnidDest + 1 );
						}
					else
						{
						Assert( FCOLUMNIDFixed( rgcpcol[i-1].columnidDest ) );
						}
					}
				}
			}
		}
	else
		{
		// No deleted columns, so ensure columnids didn't change.  Additionally,
		// columnids should be monotonically increasing.
		for ( i = 0; i < (INT)ccpcolMax; i++ )
			{
			Assert( rgcpcol[i].columnidDest == rgcpcol[i].columnidSrc );

			if ( FCOLUMNIDFixed( rgcpcol[i].columnidSrc ) )
				{
				Assert( i == 0 ?
					FidOfColumnid( rgcpcol[i].columnidDest ) == ptdb->FidFixedFirst() :
					rgcpcol[i].columnidDest == rgcpcol[i-1].columnidDest + 1 );
				}
			else
				{
				Assert( FCOLUMNIDVar( rgcpcol[i].columnidSrc ) );
				if ( i == 0 )
					{
					// If we get here, there's no fixed columns.
					Assert( FidOfColumnid( rgcpcol[i].columnidDest ) == ptdb->FidVarFirst() );
					Assert( ptdb->FidFixedLast() == ptdb->FidFixedFirst() - 1 );
					}
				else if ( FCOLUMNIDVar( rgcpcol[i-1].columnidDest ) )
					{
					Assert( rgcpcol[i].columnidDest == rgcpcol[i-1].columnidDest + 1 );
					}
				else
					{
					// Must be the beginning of the variable columns.
					Assert( FidOfColumnid( rgcpcol[i].columnidDest ) == ptdb->FidVarFirst() );
					Assert( FidOfColumnid( rgcpcol[i-1].columnidDest ) == ptdb->FidFixedLast() );
					}
				}
			}
		}


	// Check tagged columns.  Note that base table columns do not appear in
	// the columnid map.
	FID			fidT		= ptdb->FidTaggedFirst();
	const FID	fidLast		= ptdb->FidTaggedLast();
	if ( FCOLSDELETEDTagged( fColumnsDeleted ) )
		{
		for ( ; fidT <= fidLast; fidT++ )
			{
			const FIELD	*pfieldTagged = ptdb->PfieldTagged( ColumnidOfFid( fidT, fTemplateTable ) );
			if ( pfieldTagged->coltyp != JET_coltypNil )
				{
				Assert( FCOLUMNIDTagged( mpcolumnidcolumnidTagged[fidT-fidTaggedLeast] ) );
				Assert(	FidOfColumnid( mpcolumnidcolumnidTagged[fidT-fidTaggedLeast] ) <= fidT );
				}
			}
		}
	else
		{
		// No deleted columns, so ensure columnids didn't change.
		for ( ; fidT <= fidLast; fidT++ )
			{
			Assert( ptdb->PfieldTagged( ColumnidOfFid( fidT, fTemplateTable ) )->coltyp != JET_coltypNil );
			if ( fidT > ptdb->FidTaggedFirst() )
				{
				Assert( mpcolumnidcolumnidTagged[fidT-fidTaggedLeast]
						== mpcolumnidcolumnidTagged[fidT-fidTaggedLeast-1] + 1 );
				}
			Assert( FidOfColumnid( mpcolumnidcolumnidTagged[fidT-fidTaggedLeast] ) == fidT );
			}
		}

#endif	// DEBUG
	}


ERR ErrSORTCopyRecords(
	PIB				*ppib,
	FUCB			*pfucbSrc,
	FUCB			*pfucbDest,
	CPCOL			*rgcpcol,
	ULONG			ccpcolMax,
	LONG			crecMax,
	ULONG			*pcsridCopied,
	ULONG			*precidLast,
	BYTE			*pbLVBuf,
	JET_COLUMNID	*mpcolumnidcolumnidTagged,
	STATUSINFO		*pstatus )
	{
	ERR				err						= JET_errSuccess;
	TDB				*ptdb					= ptdbNil;
	INT				fColumnsDeleted			= 0;
	LONG			dsrid					= 0;
	BYTE			*pbRecBuf				= NULL;		// buffer for source record
	VOID			*pvWorkBuf				= NULL;		// buffer for destination record
	BOOL			fDoAll					= ( 0 == crecMax );
	PGNO			pgnoCurrPage;
	FCB				*pfcbSecondaryIndexes	= pfcbNil;
	const BOOL		fOldSeq					= FFUCBSequential( pfucbSrc );
	BOOL			fInTrx					= fFalse;

	FCB				*pfcbNextIndexBatch		= pfcbNil;
	BOOL			fBuildIndexWhileCopying	= fFalse;
	FUCB			*rgpfucbSort[cFILEIndexBatchSizeDefault];
	ULONG			rgcRecInput[cFILEIndexBatchSizeDefault];
	BOOKMARK		*pbmPrimary				= NULL;
	BOOKMARK		bmPrimary;
	KEY				keyBuffer;
	BYTE			rgbPrimaryBM[JET_cbBookmarkMost];
	BYTE  	 		rgbSecondaryKey[JET_cbSecondaryKeyMost];
	INT				cIndexesToBuild;
	INT				iindex;

	//	Copy LV tree before copying data records.
	Assert( JET_tableidNil == lidmapDefrag.Tableid() );
	err = ErrCMPCopyLVTree(
				ppib,
				pfucbSrc,
				pfucbDest,
				pbLVBuf,
				g_cbLVBuf,
				pstatus );
	if ( err < 0 )
		{
		if ( JET_tableidNil != lidmapDefrag.Tableid() )
			{
			CallS( lidmapDefrag.ErrTerm( ppib ) );
			Assert( JET_tableidNil == lidmapDefrag.Tableid() );
			}
		return err;
		}

	//  set FUCB to sequential mode for a more efficient scan
	FUCBSetSequential( pfucbSrc );

//	tableidGlobal may be set in copy long value tree function.
//	Assert( JET_tableidNil == tableidGlobalLIDMap );

	BFAlloc( (VOID **)&pbRecBuf );
	Assert ( NULL != pbRecBuf );
	Assert( NULL != pbLVBuf );

	// Preallocate work buffer so ErrIsamPrepareUpdate() doesn't continually
	// allocate one.
	Assert( NULL == pfucbDest->pvWorkBuf );
	RECIAllocCopyBuffer( pfucbDest );
	pvWorkBuf = pfucbDest->pvWorkBuf;
	Assert ( NULL != pvWorkBuf );

	Assert( ppib == pfucbSrc->ppib );
	Assert( ppib == pfucbDest->ppib );


	ptdb = pfucbSrc->u.pfcb->Ptdb();

	// Need to determine if there were any columns deleted.
	FCOLSDELETEDSetNone( fColumnsDeleted );

	// The fixed/variable columnid map already filters out deleted columns.
	// If the size of the map is not equal to the number of fixed and variable
	// columns in the source table, then we know some have been deleted.
	// Note that for derived tables, we don't have to bother checking deleted
	// columns in the base table, since the DDL of base tables is fixed.
	Assert( ccpcolMax <=
		(ULONG)( ( ptdb->FidFixedLast() + 1 - ptdb->FidFixedFirst() )
					+ ( ptdb->FidVarLast() + 1 - ptdb->FidVarFirst() ) ) );
	if ( ccpcolMax < (ULONG)( ( ptdb->FidFixedLast() + 1 - ptdb->FidFixedFirst() )
								+ ( ptdb->FidVarLast() + 1 - ptdb->FidVarFirst() ) ) )
		{
		FCOLSDELETEDSetFixedVar( fColumnsDeleted );
		}

	//	LAURIONB_HACK
	extern BOOL g_fCompactTemplateTableColumnDropped;
	if( g_fCompactTemplateTableColumnDropped && pfcbNil != ptdb->PfcbTemplateTable() )
		{
		FCOLSDELETEDSetFixedVar( fColumnsDeleted );
		}

	/*	tagged columnid map works differently than the fixed/variable columnid
	/*	map; deleted columns are not filtered out (they have an entry of 0).  So we
	/*	have to consult the source table's TDB.
	/**/
	FID			fidT		= ptdb->FidTaggedFirst();
	const FID	fidLast		= ptdb->FidTaggedLast();
	Assert( fidLast == fidT-1 || FTaggedFid( fidLast ) );

	if ( ptdb->FESE97DerivedTable() )
		{
		//	columnids will be renumbered - set Deleted flag to
		//	force FIDs in TAGFLDs to be recalculated
		FCOLSDELETEDSetTagged( fColumnsDeleted );
		}
	else
		{
		for ( ; fidT <= fidLast; fidT++ )
			{
			const FIELD	*pfieldTagged = ptdb->PfieldTagged( ColumnidOfFid( fidT, ptdb->FTemplateTable() ) );
			if ( pfieldTagged->coltyp == JET_coltypNil )
				{
				FCOLSDELETEDSetTagged( fColumnsDeleted );
				break;
				}
			}
		}

	SORTAssertColumnidMaps(
		ptdb,
		rgcpcol,
		ccpcolMax,
		mpcolumnidcolumnidTagged,
		fColumnsDeleted,
		ptdb->FTemplateTable() );

	Assert( crecMax >= 0 );

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	fInTrx = fTrue;

	DIRBeforeFirst( pfucbSrc );
	err = ErrDIRNext( pfucbSrc, fDIRNull );
	if ( err < 0 )
		{
		Assert( JET_errRecordNotFound != err );
		if ( JET_errNoCurrentRecord == err )
			err = JET_errSuccess;		// empty table
		goto HandleError;
		}

	// Disconnect secondary indexes to prevent Update from attempting
	// to update secondary indexes.
	pfcbSecondaryIndexes = pfucbDest->u.pfcb->PfcbNextIndex();
	pfucbDest->u.pfcb->SetPfcbNextIndex( pfcbNil );


	//	NOTE: do not update FAboveThreshold() because we will be putting
	//		  the secondary indexes back before the FCBs refcnt can go
	//		  to 0 (forcing the FCB into the wrong avail list)

	if ( pfcbNil != pfcbSecondaryIndexes )
		{
		for ( iindex = 0; iindex < cFILEIndexBatchSizeDefault; iindex++ )
			rgpfucbSort[iindex] = pfucbNil;

		Call( ErrFILEIndexBatchInit(
					ppib,
					rgpfucbSort,
					pfcbSecondaryIndexes,
					&cIndexesToBuild,
					rgcRecInput,
					&pfcbNextIndexBatch,
					cFILEIndexBatchSizeDefault ) );

#ifdef PARALLEL_BATCH_INDEX_BUILD
		//	only build indexes while copying data records if
		//	there are not many indexes -- if there are too many,
		//	then just create all the indexes in one big pass
		//	after we're done copying the records
		fBuildIndexWhileCopying = ( pfcbNil == pfcbNextIndexBatch );
#else
		fBuildIndexWhileCopying = fTrue;
#endif

		if ( fBuildIndexWhileCopying )
			{
			bmPrimary.key.prefix.Nullify();
			bmPrimary.key.suffix.SetCb( sizeof( rgbPrimaryBM ) );
			bmPrimary.key.suffix.SetPv( rgbPrimaryBM );
			bmPrimary.data.Nullify();
			pbmPrimary = &bmPrimary;

			keyBuffer.prefix.Nullify();
			keyBuffer.suffix.SetCb( sizeof( rgbSecondaryKey ) );
			keyBuffer.suffix.SetPv( rgbSecondaryKey );
			}
		else
			{
#ifdef PARALLEL_BATCH_INDEX_BUILD
			for ( iindex = 0; iindex < cFILEIndexBatchSizeDefault; iindex++ )
				{
				Assert( pfucbNil != rgpfucbSort[iindex] );
				SORTClose( rgpfucbSort[iindex] );
				rgpfucbSort[iindex] = pfucbNil;
				}
#endif
			}
		}

	pgnoCurrPage = Pcsr( pfucbSrc )->Pgno();
	forever
		{
		Assert( Pcsr( pfucbSrc )->FLatched() );
		err = ErrSORTCopyOneRecord(
			pfucbSrc,
			pfucbDest,
			pbmPrimary,
			fColumnsDeleted,
			pbRecBuf,
			rgcpcol,						// Only used for DEBUG
			ccpcolMax,						// Only used for DEBUG
			mpcolumnidcolumnidTagged,
			pstatus );
		if ( err < 0 )
			{
			if ( fGlobalRepair )
				{
				UtilReportEvent( eventWarning, REPAIR_CATEGORY, REPAIR_BAD_RECORD_ID, 0, NULL );
				}
			else
				goto HandleError;
			}
		else
			{
			// Latch released in order to copy record.
			Assert( !Pcsr( pfucbSrc )->FLatched() );
			Assert( !Pcsr( pfucbDest )->FLatched() );

			// Work buffer preserved.
			Assert( pfucbDest->dataWorkBuf.Pv() == pvWorkBuf );
			Assert( pfucbDest->dataWorkBuf.Cb() >= REC::cbRecordMin );
			Assert( pfucbDest->dataWorkBuf.Cb() <= REC::CbRecordMax() );
			}

		dsrid++;

		if ( err >= 0 && fBuildIndexWhileCopying )
			{
			Assert( pfcbNil != pfcbSecondaryIndexes );
			Assert( cIndexesToBuild > 0 );
			Assert( cIndexesToBuild <= cFILEIndexBatchSizeDefault );
			Assert( pbmPrimary == &bmPrimary );
			Call( ErrFILEIndexBatchAddEntry(
						rgpfucbSort,
						pfucbDest,
						pbmPrimary,
						pfucbDest->dataWorkBuf,
						pfcbSecondaryIndexes,
						cIndexesToBuild,
						rgcRecInput,
						keyBuffer ) );	//lint !e644
			}

		/*	break if copied required records or if no next/prev record
		/**/

		if ( !fDoAll  &&  --crecMax == 0 )
			break;

		err = ErrDIRNext( pfucbSrc, fDIRNull );
		if ( err < 0 )
			{
			Assert( JET_errRecordNotFound != err );
			if ( err != JET_errNoCurrentRecord  )
				goto HandleError;

			if ( pstatus != NULL )
				{
				pstatus->cLeafPagesTraversed++;
				Call( ErrSORTCopyProgress( pstatus, 1 ) );
				}
			err = JET_errSuccess;
			break;
			}

		else if ( pstatus != NULL && pgnoCurrPage != Pcsr( pfucbSrc )->Pgno() )
			{
			pgnoCurrPage = Pcsr( pfucbSrc )->Pgno();
			pstatus->cLeafPagesTraversed++;
			Call( ErrSORTCopyProgress( pstatus, 1 ) );
			}
		}

	Assert( fInTrx );
	Call( ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush ) );
	fInTrx = fFalse;

	if ( JET_tableidNil != lidmapDefrag.Tableid() )
		{
		// Validate LV refcounts
		Call( lidmapDefrag.ErrUpdateLVRefcounts( ppib, pfucbDest ) );
		}


	// Reattach secondary indexes.
	Assert( pfucbDest->u.pfcb->PfcbNextIndex() == pfcbNil );
	pfucbDest->u.pfcb->SetPfcbNextIndex( pfcbSecondaryIndexes );

	if ( pfcbNil != pfcbSecondaryIndexes )
		{
		if ( pstatus )
			{
			Assert( pstatus->cSecondaryIndexes > 0 );

			// if cunitPerProgression is 0, then corruption was detected
			// during GlobalRepair
			Assert( pstatus->cunitPerProgression > 0 || fGlobalRepair );
			if ( pstatus->cunitPerProgression > 0 )
				{
				Assert( pstatus->cunitDone <= pstatus->cunitProjected );
				Assert( pstatus->cunitProjected <= pstatus->cunitTotal );
				const ULONG	cpgRemaining = pstatus->cunitProjected - pstatus->cunitDone;

				// Each secondary index has at least an FDP.
				Assert( cpgRemaining >= pstatus->cSecondaryIndexes );

				pstatus->cunitPerProgression = cpgRemaining / pstatus->cSecondaryIndexes;
				Assert( pstatus->cunitPerProgression >= 1 );
				Assert( pstatus->cunitPerProgression * pstatus->cSecondaryIndexes <= cpgRemaining );
				}
			}

#ifdef PARALLEL_BATCH_INDEX_BUILD
		if ( fBuildIndexWhileCopying )
			{
			//	this should be all the indexes for this table
			//
			Call( ErrFILEIndexBatchTerm(
						ppib,
						rgpfucbSort,
						pfcbSecondaryIndexes,
						cIndexesToBuild,
						rgcRecInput,
						pstatus ) );
			}
		else
			{
			ULONG	cIndexes	= 0;
			for ( FCB * pfcbT = pfcbSecondaryIndexes; pfcbNil != pfcbT; pfcbT = pfcbT->PfcbNextIndex() )
				{
				cIndexes++;
				}
			Assert( cIndexes > cFILEIndexBatchSizeDefault );

			Call( ErrFILEBuildAllIndexes(
						ppib,
						pfucbDest,
						pfcbSecondaryIndexes,
						pstatus,
						cIndexes ) );
			}
#else
		// Finish first batch of indexes, then make the rest.
		Call( ErrFILEIndexBatchTerm(
					ppib,
					rgpfucbSort,
					pfcbSecondaryIndexes,
					cIndexesToBuild,
					rgcRecInput,
					pstatus ) );

		if ( pfcbNil != pfcbNextIndexBatch )
			{
			Assert( cFILEIndexBatchSizeDefault == cIndexesToBuild );
			Call( ErrFILEBuildAllIndexes(
							ppib,
							pfucbDest,
							pfcbNextIndexBatch,
							pstatus,
							cFILEIndexBatchSizeDefault ) );
			}
#endif	//	PARALLEL_BATCH_INDEX_BUILD
		}

HandleError:
	if ( pcsridCopied )
		*pcsridCopied = dsrid;
	if ( precidLast )
		*precidLast = 0xffffffff;

	if ( err < 0 && pfcbNil != pfcbSecondaryIndexes )
		{
		for ( iindex = 0; iindex < cFILEIndexBatchSizeDefault; iindex++ )
			{
			if ( pfucbNil != rgpfucbSort[iindex] )	//lint !e644
				{
				SORTClose( rgpfucbSort[iindex] );
				rgpfucbSort[iindex] = pfucbNil;
				}
			}

		// Ensure secondary indexes reattached.
		pfucbDest->u.pfcb->SetPfcbNextIndex( pfcbSecondaryIndexes );
		}
#ifdef DEBUG
	else if ( pfcbNil != pfcbSecondaryIndexes )
		{
		for ( iindex = 0; iindex < cFILEIndexBatchSizeDefault; iindex++ )
			{
			Assert( pfucbNil == rgpfucbSort[iindex] );
			}
		}
#endif

	if ( fInTrx )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}

	// This gets allocated by CopyLVTree()
	if ( JET_tableidNil != lidmapDefrag.Tableid() )
		{
		CallS( lidmapDefrag.ErrTerm( ppib ) );
		Assert( JET_tableidNil == lidmapDefrag.Tableid() );
		}

	Assert( NULL != pvWorkBuf );
	Assert( pvWorkBuf == pfucbDest->pvWorkBuf
		|| ( err < 0 && NULL == pfucbDest->pvWorkBuf ) );	//	work buffer may have been deallocated on error
	Assert( pfucbDest->dataWorkBuf.Pv() == pfucbDest->pvWorkBuf );
	RECIFreeCopyBuffer( pfucbDest );

	Assert ( NULL != pbRecBuf );
	BFFree( pbRecBuf );

	if ( !fOldSeq )
		FUCBResetSequential( pfucbSrc );

	return err;
	}

ERR ISAMAPI ErrIsamCopyRecords(
	JET_SESID		sesid,
	JET_TABLEID		tableidSrc,
	JET_TABLEID		tableidDest,
	CPCOL			*rgcpcol,
	ULONG			ccpcolMax,
	LONG			crecMax,
	ULONG			*pcsridCopied,
	ULONG			*precidLast,
	BYTE			*pbLVBuf,
	JET_COLUMNID	*mpcolumnidcolumnidTagged,
	STATUSINFO		*pstatus )
	{
	ERR				err;
	PIB				*ppib;
	FUCB			*pfucbSrc;
	FUCB			*pfucbDest;

	ppib = reinterpret_cast<PIB *>( sesid );
	pfucbSrc = reinterpret_cast<FUCB *>( tableidSrc );
	pfucbDest = reinterpret_cast<FUCB *>( tableidDest );

	/*	ensure tableidSrc and tableidDest are system ISAM
	/**/
	Assert( pfucbSrc->pvtfndef == (VTFNDEF *)&vtfndefIsam
			|| pfucbSrc->pvtfndef == (VTFNDEF *)&vtfndefTTBase );
	Assert( pfucbDest->pvtfndef == (VTFNDEF *)&vtfndefIsam
			|| pfucbDest->pvtfndef == (VTFNDEF *)&vtfndefTTBase );

	err = ErrSORTCopyRecords(
				ppib,
				pfucbSrc,
				pfucbDest,
				rgcpcol,
				ccpcolMax,
				crecMax,
				pcsridCopied,
				precidLast,
				pbLVBuf,
				mpcolumnidcolumnidTagged,
				pstatus );

	return err;
	}


//	UNDONE:  use GetTempFileName()
INLINE ULONG ulSORTTempNameGen( VOID )
	{
	static ULONG ulTempNum = 0;
	return ulTempNum++;
	}

LOCAL ERR ErrIsamSortMaterialize( PIB * const ppib, FUCB * const pfucbSort, const BOOL fIndex )
	{
	ERR		err;
	FUCB   	*pfucbTable			= pfucbNil;
	FCB		*pfcbTable			= pfcbNil;
	TDB		*ptdbTable			= ptdbNil;
	FCB		*pfcbSort			= pfcbNil;
	TDB		*ptdbSort			= ptdbNil;
	MEMPOOL	mempoolSave;
	CHAR   	szName[JET_cbNameMost + 1];
	BOOL	fBeginTransaction	= fFalse;
	JET_TABLECREATE2 tablecreate	= {
		sizeof(JET_TABLECREATE2),
		szName,
		NULL,
	   	16, 100, 			// Pages and density
	   	NULL, 0, NULL, 0, NULL, 0,	// Columns and indexes and callbacks
	   	NO_GRBIT,			// grbit
	   	0,					// returned tableid
	   	0 };				// returned count of objects created

	CallR( ErrPIBCheck( ppib ) );
	CheckSort( ppib, pfucbSort );

	Assert( ppib->level < levelMax );
	Assert( pfucbSort->ppib == ppib );
	Assert( !( FFUCBIndex( pfucbSort ) ) );

	/*	causes remaining runs to be flushed to disk
	/**/
	if ( FSCBInsert( pfucbSort->u.pscb ) )
		{
		// If fSCBInsert is set, must have been called from ErrIsamOpenTempTable(),
		// in which case there should be no records.
		Assert( 0 == pfucbSort->u.pscb->cRecords );
		CallR( ErrSORTEndInsert( pfucbSort ) );
		}


	/*	generate temporary file name
	/**/
	sprintf( szName, "TEMP%lu", ulSORTTempNameGen() );

	/*	create table
	/**/
	INST *pinst = PinstFromPpib( ppib );
	CallR( ErrFILECreateTable( ppib, pinst->m_mpdbidifmp[ dbidTemp ], &tablecreate ) );
	pfucbTable = (FUCB *)( tablecreate.tableid );
	Assert( pfucbNil != pfucbTable );
	/*	only one table created
	/**/
	Assert( tablecreate.cCreated == 1 );

	/*	move to DATA root
	/**/
	const INT	fDIRFlags = ( fDIRNoVersion|fDIRAppend );	// No versioning -- on error, entire FDP is freed
	DIRGotoRoot( pfucbTable );
	Call( ErrDIRInitAppend( pfucbTable ) );

	pfcbSort = &pfucbSort->u.pscb->fcb;
	Assert( pfcbSort->FTypeSort() );

	ptdbSort = pfcbSort->Ptdb();
	Assert( ptdbNil != ptdbSort );

	pfcbTable = pfucbTable->u.pfcb;
	Assert( pfcbTable->FTypeTemporaryTable() );

	ptdbTable = pfcbTable->Ptdb();
	Assert( ptdbNil != ptdbTable );

	err = ErrSORTFirst( pfucbSort );

	if ( fIndex )
		{
		while ( err >= 0 )
			{
			Call( ErrDIRAppend(
						pfucbTable,
						pfucbSort->kdfCurr.key,
						pfucbSort->kdfCurr.data,
						fDIRFlags ) );

			Assert( Pcsr( pfucbTable )->FLatched() );

			err = ErrSORTNext( pfucbSort );
			}
		}
	else
		{
		KEY		key;
		DBK		dbk = 0;
		BYTE  	rgb[4];

		key.prefix.Nullify();
		key.suffix.SetPv( rgb );
		key.suffix.SetCb( sizeof(DBK) );

		while ( err >= 0 )
			{
			rgb[0] = (BYTE)(dbk >> 24);
			rgb[1] = (BYTE)((dbk >> 16) & 0xff);
			rgb[2] = (BYTE)((dbk >> 8) & 0xff);
			rgb[3] = (BYTE)(dbk & 0xff);
			dbk++;

			Call( ErrDIRAppend(
				pfucbTable,
				key,
				pfucbSort->kdfCurr.data,
				fDIRFlags ) );
			err = ErrSORTNext( pfucbSort );
			}

		dbk++;			//	add one to set to next available dbk

		Assert( ptdbTable->DbkMost() == 0 );
		ptdbTable->InitDbkMost( dbk );			//	should not conflict with anyone, since we have exclusive use
		Assert( ptdbTable->DbkMost() == dbk );
		}

	Assert( err < 0 );
	if ( err != JET_errNoCurrentRecord )
		goto HandleError;

	Call( ErrDIRTermAppend( pfucbTable ) );

	//	convert sort cursor into table cursor by changing flags.
	//
	Assert( pfcbTable->PfcbNextIndex() == pfcbNil );
	Assert( rgfmp[ pfcbTable->Ifmp() ].Dbid() == dbidTemp );
	pfcbTable->SetCbDensityFree( 0 );

	// UNDONE: This strategy of swapping the sort and table FCB's is a real
	// hack with the potential to cause future problems.  I've already been
	// bitten several times because ulFCBFlags is forcefully cleared.
	// Is there a better way to do this?

	// UNDONE:	clean up flag reset
	//
	Assert( pfcbTable->FDomainDenyReadByUs( ppib ) );
	Assert( pfcbTable->FTypeTemporaryTable() );
	Assert( pfcbTable->FPrimaryIndex() );
	Assert( pfcbTable->FInitialized() );
	Assert( pfcbTable->ErrErrInit() == JET_errSuccess );
	Assert( pfcbTable->FInList() );

	pfcbTable->ResetFlags();

	pfcbTable->SetDomainDenyRead( ppib );
	pfcbTable->SetTypeTemporaryTable();
	pfcbTable->SetFixedDDL();
	pfcbTable->SetPrimaryIndex();
	pfcbTable->CreateComplete();	//	FInitialized() = fTrue, m_errInit = JET_errSuccess
	pfcbTable->SetInList();		// Already placed in list by FILEOpenTable()

	// Swap field info in the sort and table TDB's.  The TDB for the sort
	// will then be released when the sort is closed.
	Assert( ptdbSort != ptdbNil );
	Assert( ptdbSort->PfcbTemplateTable() == pfcbNil );
	Assert( ptdbSort->IbEndFixedColumns() >= ibRECStartFixedColumns );
	Assert( ptdbTable != ptdbNil );
	Assert( ptdbTable->FidFixedLast() == fidFixedLeast-1 );
	Assert( ptdbTable->FidVarLast() == fidVarLeast-1 );
	Assert( ptdbTable->FidTaggedLast() == fidTaggedLeast-1 );
	Assert( ptdbTable->IbEndFixedColumns() == ibRECStartFixedColumns );
	Assert( ptdbTable->FidVersion() == 0 );
	Assert( ptdbTable->FidAutoincrement() == 0 );
	Assert( pfieldNil == ptdbTable->PfieldsInitial() );
	Assert( ptdbTable->PfcbTemplateTable() == pfcbNil );

	//	copy FIELD structures into byte pool (note that
	//	although it would be dumb, it is theoretically
	//	possible to create a sort without any columns)
	//	UNDONE: it would be nicer to copy the FIELD
	//	structure to the temp. table's PfieldsInitial(),
	//	but can't modify the fidLastInitial constants
	//	anymore
	Assert( 0 == ptdbSort->CDynamicColumns() );
	Assert( 0 == ptdbTable->CInitialColumns() );
	Assert( 0 == ptdbTable->CDynamicColumns() );
	const ULONG		cCols	= ptdbSort->CInitialColumns();
	if ( cCols > 0 )
		{
		//	Add the FIELD structures to the sort's byte pool
		//	so that it will be a simple matter to swap byte
		//	pools between the sort and the table
		Call( ptdbSort->MemPool().ErrReplaceEntry(
					itagTDBFields,
					(BYTE *)ptdbSort->PfieldsInitial(),
					cCols * sizeof(FIELD) ) );
		}


	// Add the table name to the sort's byte pool so that it will be a simple
	// matter to swap byte pools between the sort and the table.
	Assert( ptdbSort->ItagTableName() == 0 );
	MEMPOOL::ITAG	itagNew;
	Call( ptdbSort->MemPool().ErrAddEntry(
				(BYTE *)szName,
				(ULONG)strlen( szName ) + 1,
				&itagNew ) );
	if ( fIndex
		|| pfcbSort->Pidb() != pidbNil )	// UNDONE: Temporarily add second clause to silence asserts. -- JL
		{
		Assert( pfcbSort->Pidb() != pidbNil );
		if ( pfcbSort->Pidb()->FIsRgidxsegInMempool() )
			{
			Assert( pfcbSort->Pidb()->ItagRgidxseg() == itagTDBTempTableIdxSeg );
			Assert( itagNew == itagTDBTempTableNameWithIdxSeg );
			}
		else
			{
			Assert( pfcbSort->Pidb()->Cidxseg() > 0 );
			Assert( itagNew == itagTDBTableName );
			}
		}
	else
		{
		Assert( pfcbSort->Pidb() == pidbNil );
		Assert( itagNew == itagTDBTableName );
		}
	ptdbSort->SetItagTableName( itagNew );

	// Try to compact the byte pool.  If it fails, don't worry about it.
	// It just means the byte pool may contain unused space.
	ptdbSort->MemPool().FCompact();

	// Since sort is about to be closed, everything can be cannibalised,
	// except the byte pool, which must be explicitly freed.
	mempoolSave = ptdbSort->MemPool();
	ptdbSort->SetMemPool( ptdbTable->MemPool() );

	// WARNING: From this point on pfcbTable will be irrevocably
	// cannibalised before being transferred to the sort cursor.
	// Thus, we must ensure success all the way up to the
	// DIRClose( pfucbTable ), because we will no longer be
	// able to close the table properly via FILECloseTable()
	// in HandleError.

	ptdbTable->SetMemPool( mempoolSave );
	ptdbTable->MaterializeFids( ptdbSort );
	ptdbTable->SetItagTableName( ptdbSort->ItagTableName() );

	//	shouldn't have a default record, but propagate just to be safe
	Assert( NULL == ptdbSort->PdataDefaultRecord() );
	Assert( NULL == ptdbTable->PdataDefaultRecord() );
	ptdbTable->SetPdataDefaultRecord( ptdbSort->PdataDefaultRecord() );
	ptdbSort->SetPdataDefaultRecord( NULL );

	//	switch sort and table IDB
	Assert( pfcbTable->Pidb() == pidbNil );
	if ( fIndex )
		{
		Assert( pfcbSort->Pidb() != pidbNil );
		Assert( pfcbSort->Pidb()->ItagIndexName() == 0 );	// Sort and temp table indexes have no name.
		pfcbTable->SetPidb( pfcbSort->Pidb() );
		pfcbSort->SetPidb( pidbNil );
		}

	//	convert sort cursor flags to table flags
	//
	Assert( rgfmp[ pfucbSort->ifmp ].Dbid() == dbidTemp );
	Assert( pfucbSort->pfucbCurIndex == pfucbNil );
	FUCBSetIndex( pfucbSort );
	FUCBResetSort( pfucbSort );
	FUCBSetMayCacheLVCursor( pfucbSort );

	// release SCB, upgrade sort cursor to table cursor,
	// then close original table cursor
	//
	Assert( pfucbSort->u.pscb->fcb.WRefCount() == 1 );
	Assert( pfucbSort->u.pscb->fcb.Pfucb() == pfucbSort );
	if ( pfucbSort->u.pscb->fcb.PgnoFDP() != pgnoNull )
		SORTICloseRun( ppib, pfucbSort->u.pscb );
	SORTClosePscb( pfucbSort->u.pscb );
	pfcbTable->Link( pfucbSort );
	DIRBeforeFirst( pfucbSort );

	CheckTable( ppib, pfucbTable );
	Assert( pfucbTable->pvtfndef == &vtfndefInvalidTableid );
	Assert( !FFUCBUpdatePrepared( pfucbTable ) );
	Assert( NULL == pfucbTable->pvWorkBuf );
	FUCBAssertNoSearchKey( pfucbTable );
	Assert( pfucbNil == pfucbTable->pfucbCurIndex );
	Assert( pfucbTable->u.pfcb->FTypeTemporaryTable() );
	DIRClose( pfucbTable );
	pfucbTable = pfucbNil;

	// WARNING: Materialisation complete.  Sort has been transformed
	// into a temp table.  Must return success so dispatch table will
	// be updated accordingly.
	pfucbSort->pvtfndef = &vtfndefTTBase;
	return JET_errSuccess;


HandleError:
	if ( pfucbNil != pfucbTable )
		{
		// On error, release temporary table.
		Assert( err < JET_errSuccess );
		Assert( pfucbTable->u.pfcb->FTypeTemporaryTable() );
		CallS( ErrFILECloseTable( ppib, pfucbTable ) );
		}

	return err;
	}


#pragma warning(disable:4028 4030)	//  parameter mismatch errors

#ifndef DEBUG
#define ErrIsamSortRetrieveKey		ErrIsamRetrieveKey
#endif

#ifdef DB_DISPATCHING
extern VDBFNCapability				ErrIsamCapability;
extern VDBFNCloseDatabase			ErrIsamCloseDatabase;
extern VDBFNCreateObject			ErrIsamCreateObject;
extern VDBFNCreateTable 			ErrIsamCreateTable;
extern VDBFNDeleteObject			ErrIsamDeleteObject;
extern VDBFNDeleteTable 			ErrIsamDeleteTable;
extern VDBFNGetColumnInfo			ErrIsamGetColumnInfo;
extern VDBFNGetDatabaseInfo 		ErrIsamGetDatabaseInfo;
extern VDBFNGetIndexInfo			ErrIsamGetIndexInfo;
extern VDBFNGetObjectInfo			ErrIsamGetObjectInfo;
extern VDBFNOpenTable				ErrIsamOpenTable;
extern VDBFNRenameTable 			ErrIsamRenameTable;
extern VDBFNGetObjidFromName		ErrIsamGetObjidFromName;
extern VDBFNRenameObject			ErrIsamRenameObject;


CODECONST(VDBFNDEF) vdbfndefIsam =
	{
	sizeof(VDBFNDEF),
	0,
	NULL,
	ErrIsamCapability,
	ErrIsamCloseDatabase,
	ErrIsamCreateObject,
	ErrIsamCreateTable,
	ErrIsamDeleteObject,
	ErrIsamDeleteTable,
	ErrIllegalExecuteSql,
	ErrIsamGetColumnInfo,
	ErrIsamGetDatabaseInfo,
	ErrIsamGetIndexInfo,
	ErrIsamGetObjectInfo,
	ErrIsamOpenTable,
	ErrIsamRenameObject,
	ErrIsamRenameTable,
	ErrIsamGetObjidFromName,
	};
#endif


extern VTFNAddColumn				ErrIsamAddColumn;
extern VTFNCloseTable				ErrIsamCloseTable;
extern VTFNComputeStats 			ErrIsamComputeStats;
extern VTFNCreateIndex				ErrIsamCreateIndex;
extern VTFNDelete					ErrIsamDelete;
extern VTFNDeleteColumn 			ErrIsamDeleteColumn;
extern VTFNDeleteIndex				ErrIsamDeleteIndex;
extern VTFNDupCursor				ErrIsamDupCursor;
extern VTFNEscrowUpdate		  		ErrIsamEscrowUpdate;
extern VTFNGetBookmark				ErrIsamGetBookmark;
extern VTFNGetIndexBookmark			ErrIsamGetIndexBookmark;
extern VTFNGetChecksum				ErrIsamGetChecksum;
extern VTFNGetCurrentIndex			ErrIsamGetCurrentIndex;
extern VTFNGetCursorInfo			ErrIsamGetCursorInfo;
extern VTFNGetRecordPosition		ErrIsamGetRecordPosition;
extern VTFNGetTableColumnInfo		ErrIsamGetTableColumnInfo;
extern VTFNGetTableIndexInfo		ErrIsamGetTableIndexInfo;
extern VTFNGetTableInfo 			ErrIsamGetTableInfo;
extern VTFNGotoBookmark 			ErrIsamGotoBookmark;
extern VTFNGotoIndexBookmark		ErrIsamGotoIndexBookmark;
extern VTFNGotoPosition 			ErrIsamGotoPosition;
extern VTFNMakeKey					ErrIsamMakeKey;
extern VTFNMove 					ErrIsamMove;
extern VTFNPrepareUpdate			ErrIsamPrepareUpdate;
extern VTFNRenameColumn 			ErrIsamRenameColumn;
extern VTFNRenameIndex				ErrIsamRenameIndex;
extern VTFNRetrieveColumn			ErrIsamRetrieveColumn;
extern VTFNRetrieveColumns			ErrIsamRetrieveColumns;
extern VTFNRetrieveKey				ErrIsamRetrieveKey;
extern VTFNSeek 					ErrIsamSeek;
extern VTFNSeek 					ErrIsamSortSeek;
extern VTFNSetCurrentIndex			ErrIsamSetCurrentIndex;
extern VTFNSetColumn				ErrIsamSetColumn;
extern VTFNSetColumns				ErrIsamSetColumns;
extern VTFNSetIndexRange			ErrIsamSetIndexRange;
extern VTFNSetIndexRange			ErrIsamSortSetIndexRange;
extern VTFNUpdate					ErrIsamUpdate;
extern VTFNGetLock 					ErrIsamGetLock;
extern VTFNEnumerateColumns			ErrIsamEnumerateColumns;

extern VTFNDupCursor				ErrIsamSortDupCursor;
extern VTFNGetTableInfo		 		ErrIsamSortGetTableInfo;
extern VTFNCloseTable				ErrIsamSortClose;
extern VTFNMove 					ErrIsamSortMove;
extern VTFNGetBookmark				ErrIsamSortGetBookmark;
extern VTFNGotoBookmark 			ErrIsamSortGotoBookmark;
extern VTFNRetrieveKey				ErrIsamSortRetrieveKey;
extern VTFNUpdate					ErrIsamSortUpdate;

extern VTFNDupCursor				ErrTTSortRetDupCursor;

extern VTFNDupCursor				ErrTTBaseDupCursor;
extern VTFNMove 					ErrTTSortInsMove;
extern VTFNSeek 					ErrTTSortInsSeek;


CODECONST(VTFNDEF) vtfndefIsam =
	{
	sizeof(VTFNDEF),
	0,
	NULL,
	ErrIsamAddColumn,
	ErrIsamCloseTable,
	ErrIsamComputeStats,
	ErrIsamCreateIndex,
	ErrIsamDelete,
	ErrIsamDeleteColumn,
	ErrIsamDeleteIndex,
	ErrIsamDupCursor,
	ErrIsamEscrowUpdate,
	ErrIsamGetBookmark,
	ErrIsamGetIndexBookmark,
	ErrIsamGetChecksum,
	ErrIsamGetCurrentIndex,
	ErrIsamGetCursorInfo,
	ErrIsamGetRecordPosition,
	ErrIsamGetTableColumnInfo,
	ErrIsamGetTableIndexInfo,
	ErrIsamGetTableInfo,
	ErrIsamGotoBookmark,
	ErrIsamGotoIndexBookmark,
	ErrIsamGotoPosition,
	ErrIsamMakeKey,
	ErrIsamMove,
	ErrIsamPrepareUpdate,
	ErrIsamRenameColumn,
	ErrIsamRenameIndex,
	ErrIsamRetrieveColumn,
	ErrIsamRetrieveColumns,
	ErrIsamRetrieveKey,
	ErrIsamSeek,
	ErrIsamSetCurrentIndex,
	ErrIsamSetColumn,
	ErrIsamSetColumns,
	ErrIsamSetIndexRange,
	ErrIsamUpdate,
	ErrIsamGetLock,
	ErrIsamRegisterCallback,
	ErrIsamUnregisterCallback,
	ErrIsamSetLS,
	ErrIsamGetLS,
	ErrIsamIndexRecordCount,
	ErrIsamRetrieveTaggedColumnList,
	ErrIsamSetSequential,
	ErrIsamResetSequential,
	ErrIsamEnumerateColumns
	};


CODECONST(VTFNDEF) vtfndefTTSortIns =
	{
	sizeof(VTFNDEF),
	0,
	NULL,
	ErrIllegalAddColumn,
	ErrIsamSortClose,			// WARNING: Must be same as vtfndefTTSortClose
	ErrIllegalComputeStats,
	ErrIllegalCreateIndex,
	ErrIllegalDelete,
	ErrIllegalDeleteColumn,
	ErrIllegalDeleteIndex,
	ErrIllegalDupCursor,
	ErrIsamEscrowUpdate,
	ErrIllegalGetBookmark,
	ErrIllegalGetIndexBookmark,
	ErrIllegalGetChecksum,
	ErrIllegalGetCurrentIndex,
	ErrIllegalGetCursorInfo,
	ErrIllegalGetRecordPosition,
	ErrIllegalGetTableColumnInfo,
	ErrIllegalGetTableIndexInfo,
	ErrIllegalGetTableInfo,
	ErrIllegalGotoBookmark,
	ErrIllegalGotoIndexBookmark,
	ErrIllegalGotoPosition,
	ErrIsamMakeKey,
	ErrTTSortInsMove,
	ErrIsamPrepareUpdate,
	ErrIllegalRenameColumn,
	ErrIllegalRenameIndex,
	ErrIllegalRetrieveColumn,
	ErrIllegalRetrieveColumns,
	ErrIsamSortRetrieveKey,
	ErrTTSortInsSeek,
	ErrIllegalSetCurrentIndex,
	ErrIsamSetColumn,
	ErrIsamSetColumns,
	ErrIllegalSetIndexRange,
	ErrIsamSortUpdate,
	ErrIllegalGetLock,
	ErrIllegalRegisterCallback,
	ErrIllegalUnregisterCallback,
	ErrIllegalSetLS,
	ErrIllegalGetLS,
	ErrIllegalIndexRecordCount,
	ErrIllegalRetrieveTaggedColumnList,
	ErrIllegalSetSequential,
	ErrIllegalResetSequential,
	ErrIllegalEnumerateColumns
	};


CODECONST(VTFNDEF) vtfndefTTSortRet =
	{
	sizeof(VTFNDEF),
	0,
	NULL,
	ErrIllegalAddColumn,
	ErrIsamSortClose,			// WARNING: Must be same as vtfndefTTSortClose
	ErrIllegalComputeStats,
	ErrIllegalCreateIndex,
	ErrIllegalDelete,
	ErrIllegalDeleteColumn,
	ErrIllegalDeleteIndex,
	ErrTTSortRetDupCursor,
	ErrIllegalEscrowUpdate,
	ErrIsamSortGetBookmark,
	ErrIllegalGetIndexBookmark,
	ErrIllegalGetChecksum,
	ErrIllegalGetCurrentIndex,
	ErrIllegalGetCursorInfo,
	ErrIllegalGetRecordPosition,
	ErrIllegalGetTableColumnInfo,
	ErrIllegalGetTableIndexInfo,
	ErrIsamSortGetTableInfo,
	ErrIsamSortGotoBookmark,
	ErrIllegalGotoIndexBookmark,
	ErrIllegalGotoPosition,
	ErrIsamMakeKey,
	ErrIsamSortMove,
	ErrIllegalPrepareUpdate,
	ErrIllegalRenameColumn,
	ErrIllegalRenameIndex,
	ErrIsamRetrieveColumn,
	ErrIsamRetrieveColumns,
	ErrIsamSortRetrieveKey,
	ErrIsamSortSeek,
	ErrIllegalSetCurrentIndex,
	ErrIllegalSetColumn,
	ErrIllegalSetColumns,
	ErrIsamSortSetIndexRange,
	ErrIllegalUpdate,
	ErrIllegalGetLock,
	ErrIllegalRegisterCallback,
	ErrIllegalUnregisterCallback,
	ErrIsamSetLS,
	ErrIsamGetLS,
	ErrIllegalIndexRecordCount,
	ErrIllegalRetrieveTaggedColumnList,
	ErrIllegalSetSequential,
	ErrIllegalResetSequential,
	ErrIsamEnumerateColumns
	};

// for temp table (ie. a sort that's been materialized)
CODECONST(VTFNDEF) vtfndefTTBase =
	{
	sizeof(VTFNDEF),
	0,
	NULL,
	ErrIllegalAddColumn,
	ErrIsamSortClose,
	ErrIllegalComputeStats,
	ErrIllegalCreateIndex,
	ErrIsamDelete,
	ErrIllegalDeleteColumn,
	ErrIllegalDeleteIndex,
	ErrTTBaseDupCursor,
	ErrIsamEscrowUpdate,
	ErrIsamGetBookmark,
	ErrIllegalGetIndexBookmark,
	ErrIsamGetChecksum,
	ErrIllegalGetCurrentIndex,
	ErrIsamGetCursorInfo,
	ErrIllegalGetRecordPosition,
	ErrIllegalGetTableColumnInfo,
	ErrIllegalGetTableIndexInfo,
	ErrIsamSortGetTableInfo,
	ErrIsamGotoBookmark,
	ErrIllegalGotoIndexBookmark,
	ErrIllegalGotoPosition,
	ErrIsamMakeKey,
	ErrIsamMove,
	ErrIsamPrepareUpdate,
	ErrIllegalRenameColumn,
	ErrIllegalRenameIndex,
	ErrIsamRetrieveColumn,
	ErrIsamRetrieveColumns,
	ErrIsamRetrieveKey,
	ErrIsamSeek,
	ErrIllegalSetCurrentIndex,
	ErrIsamSetColumn,
	ErrIsamSetColumns,
	ErrIsamSetIndexRange,
	ErrIsamUpdate,
	ErrIllegalGetLock,
	ErrIllegalRegisterCallback,
	ErrIllegalUnregisterCallback,
	ErrIsamSetLS,
	ErrIsamGetLS,
	ErrIllegalIndexRecordCount,
	ErrIllegalRetrieveTaggedColumnList,
	ErrIllegalSetSequential,
	ErrIllegalResetSequential,
	ErrIsamEnumerateColumns
	};

// Inconsistent sort -- must be closed.
LOCAL CODECONST(VTFNDEF) vtfndefTTSortClose =
	{
	sizeof(VTFNDEF),
	0,
	NULL,
	ErrIllegalAddColumn,
	ErrIsamSortClose,
	ErrIllegalComputeStats,
	ErrIllegalCreateIndex,
	ErrIllegalDelete,
	ErrIllegalDeleteColumn,
	ErrIllegalDeleteIndex,
	ErrIllegalDupCursor,
	ErrIllegalEscrowUpdate,
	ErrIllegalGetBookmark,
	ErrIllegalGetIndexBookmark,
	ErrIllegalGetChecksum,
	ErrIllegalGetCurrentIndex,
	ErrIllegalGetCursorInfo,
	ErrIllegalGetRecordPosition,
	ErrIllegalGetTableColumnInfo,
	ErrIllegalGetTableIndexInfo,
	ErrIllegalGetTableInfo,
	ErrIllegalGotoBookmark,
	ErrIllegalGotoIndexBookmark,
	ErrIllegalGotoPosition,
	ErrIllegalMakeKey,
	ErrIllegalMove,
	ErrIllegalPrepareUpdate,
	ErrIllegalRenameColumn,
	ErrIllegalRenameIndex,
	ErrIllegalRetrieveColumn,
	ErrIllegalRetrieveColumns,
	ErrIllegalRetrieveKey,
	ErrIllegalSeek,
	ErrIllegalSetCurrentIndex,
	ErrIllegalSetColumn,
	ErrIllegalSetColumns,
	ErrIllegalSetIndexRange,
	ErrIllegalUpdate,
	ErrIllegalGetLock,
	ErrIllegalRegisterCallback,
	ErrIllegalUnregisterCallback,
	ErrIllegalSetLS,
	ErrIllegalGetLS,
	ErrIllegalIndexRecordCount,
	ErrIllegalRetrieveTaggedColumnList,
	ErrIllegalSetSequential,
	ErrIllegalResetSequential,
	ErrIllegalEnumerateColumns
	};


/*=================================================================
// ErrIsamOpenTempTable
//
// Description:
//
//	Returns a tableid for a temporary (lightweight) table.	The data
//	definitions for the table are specified at open time.
//
// Parameters:
//	JET_SESID			sesid				user session id
//	JET_TABLEID			*ptableid			new JET (dispatchable) tableid
//	ULONG				csinfo				count of JET_COLUMNDEF structures
//											(==number of columns in table)
//	JET_COLUMNDEF		*rgcolumndef		An array of column and key defintions
//											Note that TT's do require that a key be
//											defined. (see jet.h for JET_COLUMNDEF)
//	JET_GRBIT			grbit				valid values
//											JET_bitTTUpdatable (for insert and update)
//											JET_bitTTScrollable (for movement other then movenext)
//
// Return Value:
//	err			jet error code or JET_errSuccess.
//	*ptableid	a dispatchable tableid
//
// Errors/Warnings:
//
// Side Effects:
//
=================================================================*/
ERR VDBAPI ErrIsamOpenTempTable(
	JET_SESID				sesid,
	const JET_COLUMNDEF		*rgcolumndef,
	ULONG					ccolumndef,
	JET_UNICODEINDEX		*pidxunicode,
	JET_GRBIT				grbit,
	JET_TABLEID				*ptableid,
	JET_COLUMNID			*rgcolumnid )
	{
	ERR						err;
	FUCB					*pfucb;

	Assert( ptableid );
	*ptableid = JET_tableidNil;

	//  if temp tables are disabled then fail
	if ( !PinstFromPpib( (PIB*)sesid )->m_lTemporaryTablesMax )
		{
		CallR( ErrERRCheck( JET_errTooManySorts ) );
		}

	//  if the caller wants a forward only sort then they cannot ask for any
	//  functionality that will result in a table
	if (	( grbit & JET_bitTTForwardOnly ) &&
			( grbit & ( JET_bitTTIndexed | JET_bitTTUpdatable | JET_bitTTScrollable | JET_bitTTForceMaterialization ) ) )
		{
		CallR( ErrERRCheck( JET_errCannotMaterializeForwardOnlySort ) );
		}

	CallR( ErrIsamSortOpen(
				(PIB *)sesid,
				(JET_COLUMNDEF *)rgcolumndef,
				ccolumndef,
				pidxunicode,
				grbit,
				&pfucb,
				rgcolumnid ) );
	Assert( pfucbNil != pfucb );
	Assert( &vtfndefTTSortIns == pfucb->pvtfndef );
	Assert( ptdbNil != pfucb->u.pscb->fcb.Ptdb() );

	BOOL	fIndexed = fFalse;
	BOOL	fLongValues = fFalse;
	for ( UINT iColumndef = 0; iColumndef < (INT)ccolumndef; iColumndef++ )
		{
		fIndexed |= ( ( rgcolumndef[iColumndef].grbit & JET_bitColumnTTKey ) != 0);
		fLongValues |= FRECLongValue( rgcolumndef[iColumndef].coltyp ) | FRECSLV( rgcolumndef[iColumndef].coltyp );
		}

	//	if no index, force materialisation to avoid unnecessarily sorting
	//	if long values exist, must materialise because sorts don't support LV's
	//	if user wants error to be returned on insertion of duplicates, then must
	//		must materialise because sorts remove dupes silently
	if ( !fIndexed || fLongValues || ( grbit & JET_bitTTForceMaterialization ) )
		{
		//	if the caller wants a forward only sort then they cannot ask for any
		//	functionality that will result in a table
		if ( grbit & JET_bitTTForwardOnly )
			{
			CallS( ErrIsamSortClose( sesid, (JET_VTID)pfucb ) );
			return ErrERRCheck( JET_errCannotMaterializeForwardOnlySort );
			}
		err = ErrIsamSortMaterialize( (PIB *)sesid, pfucb, fIndexed );
		Assert( err <= 0 );
		if ( err < 0 )
			{
			CallS( ErrIsamSortClose( sesid, (JET_VTID)pfucb ) );
			return err;
			}
		}

	*ptableid = (JET_TABLEID)pfucb;
	return err;
	}


ERR ErrTTEndInsert( JET_SESID sesid, JET_TABLEID tableid, BOOL *pfMovedToFirst )
	{
	ERR				err;
	FUCB			*pfucb			= (FUCB *)tableid;
	BOOL			fOverflow;
	BOOL			fMaterialize;
	const JET_GRBIT	grbitOpen		= pfucb->u.pscb->grbit;

	Assert( &vtfndefTTSortIns == pfucb->pvtfndef );

	Assert( pfMovedToFirst );
	*pfMovedToFirst = fFalse;

	Call( ErrSORTEndInsert( pfucb ) );

	fOverflow = ( JET_wrnSortOverflow == err );
	Assert( JET_errSuccess == err || fOverflow );

	fMaterialize = ( grbitOpen & JET_bitTTUpdatable )
					|| ( ( grbitOpen & ( JET_bitTTScrollable|JET_bitTTIndexed ) )
						&& fOverflow );
	if ( fMaterialize )
		{
		Assert( !( grbitOpen & JET_bitTTForwardOnly ) );
		Call( ErrIsamSortMaterialize( (PIB *)sesid, pfucb, ( grbitOpen & JET_bitTTIndexed ) != 0 ) );
		Assert( JET_errSuccess == err );
		pfucb->pvtfndef = &vtfndefTTBase;
		}
	else
		{
		// In case we have runs, we must call SORTFirst() to
		// start last merge and get first record
		err = ErrSORTFirst( pfucb );
		Assert( err <= JET_errSuccess );
		if ( err < 0 )
			{
			if ( JET_errNoCurrentRecord != err )
				goto HandleError;
			}

		*pfMovedToFirst = fTrue;
		pfucb->pvtfndef = &vtfndefTTSortRet;
		}

	Assert( JET_errSuccess == err
		|| ( JET_errNoCurrentRecord == err && *pfMovedToFirst ) );
	return err;

HandleError:
	Assert( err < 0 );
	Assert( JET_errNoCurrentRecord != err );

	// On failure, sort is no longer consistent.  It must be
	// invalidated.  The only legal operation left is to close it.
	Assert( &vtfndefTTSortIns == pfucb->pvtfndef );
	pfucb->pvtfndef = &vtfndefTTSortClose;

	return err;
	}


/*=================================================================
// ErrTTSortInsMove
//
//	Functionally the same as JetMove().  This routine traps the first
//	move call on a TT, to perform any necessary transformations.
//	Routine should only be used by ttapi.c via disp.asm.
//
//	May cause a sort to be materialized
=================================================================*/
ERR VTAPI ErrTTSortInsMove( JET_SESID sesid, JET_TABLEID tableid, long crow, JET_GRBIT grbit )
	{
	ERR		err;
	BOOL	fMovedToFirst;

	if ( FFUCBUpdatePrepared( (FUCB *)tableid ) )
		{
		CallR( ErrIsamPrepareUpdate( (PIB *)sesid, (FUCB *)tableid, JET_prepCancel ) );
		}

	err = ErrTTEndInsert( sesid, tableid, &fMovedToFirst );
	Assert( JET_errNoCurrentRecord != err || fMovedToFirst );
	CallR( err );
	Assert( JET_errSuccess == err );

	if ( fMovedToFirst )
		{
		// May have already moved to first record if we had an on-disk sort
		// that wasn't materialised (because it's not updatable or
		// backwards-scrollable).
		if ( crow > 0 && crow < JET_MoveLast )
			crow--;

		if ( JET_MoveFirst == crow || 0 == crow )
			return JET_errSuccess;
		}

	err = ErrDispMove( sesid, tableid, crow, grbit );
	return err;
	}


/*=================================================================
// ErrTTSortInsSeek
//
//	Functionally the same as JetSeek().  This routine traps the first
//	seek call on a TT, to perform any necessary transformations.
//	Routine should only be used by ttapi.c via disp.asm.
//
//	May cause a sort to be materialized
=================================================================*/
ERR VTAPI ErrTTSortInsSeek( JET_SESID sesid, JET_TABLEID tableid, JET_GRBIT grbit )
	{
	ERR		err;
	BOOL	fMovedToFirst;

	if ( FFUCBUpdatePrepared( (FUCB *)tableid ) )
		{
		CallR( ErrIsamPrepareUpdate( (PIB *)sesid, (FUCB *)tableid, JET_prepCancel ) );
		}

	err = ErrTTEndInsert( sesid, tableid, &fMovedToFirst );
	Assert( err <= JET_errSuccess );
	if ( err < 0 )
		{
		if ( JET_errNoCurrentRecord == err )
			{
			Assert( fMovedToFirst );
			err = ErrERRCheck( JET_errRecordNotFound );
			}
		}
	else
		{
		err = ErrDispSeek( sesid, tableid, grbit );
		}

	Assert( JET_errNoCurrentRecord != err );
	return err;
	}


ERR VTAPI ErrTTSortRetDupCursor( JET_SESID sesid, JET_TABLEID tableid, JET_TABLEID *ptableidDup, JET_GRBIT grbit )
	{
	ERR err = ErrIsamSortDupCursor( sesid, tableid, ptableidDup, grbit );
	if ( err >= 0 )
		{
		*(const VTFNDEF **)(*ptableidDup) = &vtfndefTTSortRet;
		}

	return err;
	}


ERR VTAPI ErrTTBaseDupCursor( JET_SESID sesid, JET_TABLEID tableid, JET_TABLEID *ptableidDup, JET_GRBIT grbit )
	{
	ERR err = ErrIsamSortDupCursor( sesid, tableid, ptableidDup, grbit );
	if ( err >= 0 )
		{
		*(const VTFNDEF **)(*ptableidDup) = &vtfndefTTBase;
		}

	return err;
	}




//++++++++++++++++++++++++++++++++++++++++++++++++++++++
//	Special hack - copy LV a tree of a table.
//++++++++++++++++++++++++++++++++++++++++++++++++++++++

ERR	ErrCMPCopyLVTree(
	PIB			*ppib,
	FUCB		*pfucbSrc,
	FUCB		*pfucbDest,
	BYTE		*pbBuf,
	ULONG		cbBufSize,
	STATUSINFO	*pstatus )
	{
	ERR			err;
	JET_TABLEID	tableidLIDMap	= JET_tableidNil;
	FUCB		*pfucbGetLV		= pfucbNil;
	FUCB		*pfucbCopyLV	= pfucbNil;
	LID			lidSrc			= 0;
	LVROOT		lvroot;
	DATA		dataNull;
	PGNO		pgnoLastLV		= pgnoNull;

	dataNull.Nullify();

	//	for efficiency, ensure buffer is a multiple of chunk size
	Assert( cbBufSize % g_cbColumnLVChunkMost == 0 );

	Assert( JET_tableidNil == lidmapDefrag.Tableid() );

	err =  ErrCMPGetSLongFieldFirst(
					pfucbSrc,
					&pfucbGetLV,
					&lidSrc,
					&lvroot );
	if ( err < 0 )
		{
		Assert( pfucbNil == pfucbGetLV );
		if ( JET_errRecordNotFound == err )
			err = JET_errSuccess;
		return err;
		}

	Assert( pfucbNil != pfucbGetLV );

	Call( lidmapDefrag.ErrLIDMAPInit( ppib ) );
	Assert( JET_tableidNil != lidmapDefrag.Tableid() );

	do
		{
		LID		lidDest;
		ULONG	ibLongValue = 0;

		Assert( pgnoNull !=  Pcsr( pfucbGetLV )->Pgno() );

		// Create destination LV with same refcount as source LV.  Update
		// later if correction is needed.
		Assert( lvroot.ulReference > 0 );
		Assert( pfucbNil == pfucbCopyLV );
		Call( ErrRECSeparateLV(
					pfucbDest,
					&dataNull,
					&lidDest,
					&pfucbCopyLV,
					&lvroot ) );
		Assert( pfucbNil != pfucbCopyLV );

		do {
			ULONG cbReturned;

			// On each iteration, retrieve as many LV chunks as can fit
			// into our LV buffer.  For this to work optimally,
			// ibLongValue must always point to the beginning of a chunk.
			Assert( ibLongValue % g_cbColumnLVChunkMost == 0 );
			Call( ErrCMPRetrieveSLongFieldValueByChunks(
						pfucbGetLV,		// pfucb must start on a LVROOT node
						lidSrc,
						lvroot.ulSize,
						ibLongValue,
						pbBuf,
						cbBufSize,
						&cbReturned ) );

			Assert( cbReturned > 0 );
			Assert( cbReturned <= cbBufSize );

			Call( ErrRECAppendLVChunks(
						pfucbCopyLV,
						lidDest,
						ibLongValue,
						pbBuf,
						cbReturned ) );

			Assert( err != JET_wrnCopyLongValue );

			ibLongValue += cbReturned;					// Prepare for next chunk.
			}
		while ( lvroot.ulSize > ibLongValue );

		Assert( pfucbNil != pfucbCopyLV );
		DIRClose( pfucbCopyLV );
		pfucbCopyLV = pfucbNil;

		Assert( lvroot.ulSize == ibLongValue );

		// insert src LID and dest LID into the global LID map table
		Call( lidmapDefrag.ErrInsert( ppib, lidSrc, lidDest, lvroot.ulReference ) );

		Assert( pgnoNull !=  Pcsr( pfucbGetLV )->Pgno() );

		if ( pstatus != NULL )
			{
			ULONG	cLVPagesTraversed;

			if ( lvroot.ulSize > g_cbColumnLVChunkMost )
				{
				Assert( Pcsr( pfucbGetLV )->Pgno() != pgnoLastLV );
				cLVPagesTraversed = lvroot.ulSize / g_cbColumnLVChunkMost;
				}
			else if ( Pcsr( pfucbGetLV )->Pgno() != pgnoLastLV )
				{
				cLVPagesTraversed = 1;
				}
			else
				{
				cLVPagesTraversed = 0;
				}

			pgnoLastLV = Pcsr( pfucbGetLV )->Pgno();

			pstatus->cbRawDataLV += lvroot.ulSize;
			pstatus->cLVPagesTraversed += cLVPagesTraversed;
			Call( ErrSORTCopyProgress( pstatus, cLVPagesTraversed ) );
			}

		err = ErrCMPGetSLongFieldNext( pfucbGetLV, &lidSrc, &lvroot );
		}
	while ( err >= JET_errSuccess );

	if ( JET_errNoCurrentRecord == err )
		err = JET_errSuccess;

HandleError:
	if ( pfucbNil != pfucbCopyLV )
		DIRClose( pfucbCopyLV );

	Assert( pfucbNil != pfucbGetLV );
	CMPGetSLongFieldClose( pfucbGetLV );

	return err;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\std.cxx ===
#include "std.hxx"
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\repair.cxx ===
#include "std.hxx"

#ifdef MINIMAL_FUNCTIONALITY

//  ================================================================
RECCHECK::RECCHECK()
//  ================================================================
	{
	}

	
//  ================================================================
RECCHECK::~RECCHECK()
//  ================================================================
	{
	}

#else

#ifdef RTM
#else
//	UNDONE: consider turning these off when repair stabilizes
#define REPAIR_DEBUG_VERBOSE_SPACE
#define REPAIR_DEBUG_VERBOSE_STREAMING
#define REPAIR_DEBUG_CALLS
#endif	//	!RTM

#ifdef REPAIR_DEBUG_CALLS

CPRINTF * pcprintfRepairDebugCalls = NULL;

void ReportErr( const long err, const unsigned long ulLine, const char * const szFileName )
	{
	if( pcprintfRepairDebugCalls )
		{
		(*pcprintfRepairDebugCalls)( "error %d at line %d of %s\r\n", err, ulLine, szFileName );
		}
	}

#undef CallJ
#undef Call

#define CallJ( fn, label )						\
	{											\
	if ( ( err = fn ) < 0 )						\
		{										\
		ReportErr( err, __LINE__, __FILE__ );	\
		goto label;								\
		}										\
	}

#define Call( fn )		CallJ( fn, HandleError )

#endif	//	REPAIR_DEBUG_CALLS

#define KeyLengthMax	16  // the max key length we currently use

//  ****************************************************************
//  STRUCTS/CLASSES
//  ****************************************************************

//  ================================================================
struct REPAIRTT
//  ================================================================
	{
	JET_TABLEID		tableidBadPages;
	INT				crecordsBadPages;
	JET_COLUMNID	rgcolumnidBadPages[1];
	
	JET_TABLEID		tableidAvailable;
	INT				crecordsAvailable;
	JET_COLUMNID	rgcolumnidAvailable[2];

	JET_TABLEID		tableidOwned;
	INT				crecordsOwned;
	JET_COLUMNID	rgcolumnidOwned[2];

	JET_TABLEID		tableidUsed;
	INT				crecordsUsed;
	JET_COLUMNID	rgcolumnidUsed[3];
	};


//  ================================================================
struct REPAIRTABLE
//  ================================================================
	{
	OBJID		objidFDP;
	OBJID		objidLV;

	PGNO		pgnoFDP;
	PGNO		pgnoLV;

	BOOL		fHasPrimaryIndex;
	
	OBJIDLIST	objidlistIndexes;
	REPAIRTABLE	*prepairtableNext;
	CHAR		szTableName[JET_cbNameMost+1];

	FID			fidFixedLast;
	FID			fidVarLast;
	FID			fidTaggedLast;

	USHORT		ibEndOfFixedData;

	BOOL		fRepairTable;
	BOOL		fRepairLV;
	BOOL		fRepairIndexes;
	BOOL		fRepairLVRefcounts;
	BOOL		fTableHasSLV;
	BOOL		fTemplateTable;
	BOOL		fDerivedTable;
	};


//  ================================================================
struct INTEGGLOBALS
//  ================================================================
//
//  Values shared between the different threads for multi-threaded
//  integrity
//
//-
	{
	INTEGGLOBALS() : crit( CLockBasicInfo( CSyncBasicInfo( szIntegGlobals ), rankIntegGlobals, 0 ) ) {}
	~INTEGGLOBALS() {}
	
	CCriticalSection crit;

	BOOL				fCorruptionSeen;	//	did we encounter a corrupted table?
	ERR					err;				//	used for runtime failures (i.e. not -1206)
	
	REPAIRTABLE 		** pprepairtable;
	TTARRAY 			* pttarrayOwnedSpace;
	TTARRAY 			* pttarrayAvailSpace;
	TTARRAY 			* pttarraySLVAvail;	
	TTARRAY 			* pttarraySLVOwnerMapColumnid;	
	TTARRAY 			* pttarraySLVOwnerMapKey;	
	TTARRAY				* pttarraySLVChecksumLengths;
	BOOL				* pfDbtimeTooLarge;
	const REPAIROPTS 	* popts;
	};


//  ================================================================
struct CHECKTABLE
//  ================================================================
//
//  Tells a given task thread which table to check
//
//-
	{
	IFMP 				ifmp;
	char 				szTable[JET_cbNameMost+1];
	char 				szIndex[JET_cbNameMost+1];
	
	OBJID 				objidFDP;
	PGNO 				pgnoFDP;
	OBJID 				objidParent;
	PGNO				pgnoFDPParent;
	ULONG 				fPageFlags;
	BOOL 				fUnique;
	RECCHECK *  		preccheck;
	
	CPG					cpgPrimaryExtent;	//  used to preread the table
	INTEGGLOBALS		*pglobals;

	BOOL				fDeleteWhenDone;	//	if set to true the structure will be 'delete'd
	CManualResetSignal	signal;				//	set when the table has been checked if fDelete is not set

	//  need a constructor to initialize the signal
	
	CHECKTABLE() : signal( CSyncBasicInfo( _T( "CHECKTABLE::signal" ) ) ) {}
	};


#ifdef DISABLE_SLV
#else

//  ================================================================
struct CHECKSUMSLVCHUNK
//  ================================================================
//
//  Used to read and checksum information from a streaming files
//
//-
	{
	
	//	this signal will be set when I/O completes

	BOOL				fIOIssued;
	CManualResetSignal 	signal;

	//	errors from I/O will be returned in this
	
	ERR err;

	//	data, starting at pgnoFirst will be read into the given buffer
	
	VOID 	* pvBuffer;
	ULONG	cbBuffer;
	PGNO	pgnoFirst;
	ULONG	cbRead;

	//	verify checksums with non-zero lengths against the expected checksums
	//	store the real checksums in the checksums array
	
	ULONG 	* pulChecksumsExpected;
	ULONG	* pulChecksumLengths;
	ULONG 	* pulChecksums;
	OBJID 	* pobjid;
	COLUMNID* pcolumnid;
	USHORT	* pcbKey;
	BYTE   ** pprgbKey;

	//  need a constructor to initialize the signal
	
	CHECKSUMSLVCHUNK()
		:	signal( CSyncBasicInfo( _T( "CHECKSUMSLVCHUNK::signal" ) ) )
		{
		signal.Set();
		}
	};
	

//  ================================================================
struct CHECKSUMSLV
//  ================================================================
//
//  Tells a given task thread which table to check
//
//-
	{
	IFMP				ifmp;
	IFileAPI		*pfapiSLV;
	const CHAR 			*szSLV;
	CPG					cpgSLV;
	ERR 				*perr;
	const REPAIROPTS 	*popts;
	
	CHECKSUMSLVCHUNK	*rgchecksumchunk;	//  array of CHECKSUMSLVCHUNK's
	INT					cchecksumchunk;		//  number of chunks
	INT					cbchecksumchunk;	//  number of bytes per read

	TTARRAY * pttarraySLVChecksumsFromFile;
	TTARRAY * pttarraySLVChecksumLengthsFromSpaceMap;
	
	};


//  ================================================================
class CSLVAvailIterator
//  ================================================================
//
//  Walk through the SLVAvailTree
//
//-
	{
	public:
		CSLVAvailIterator();
		~CSLVAvailIterator();

		ERR ErrInit( PIB * const ppib, const IFMP ifmp );
		ERR ErrTerm();
		ERR ErrMoveFirst();
		ERR ErrMoveNext();

		BOOL FCommitted() const;

	private:
		FUCB * 	m_pfucb;
		PGNO	m_pgnoCurr;
		INT		m_ipage;
		const SLVSPACENODE * m_pspacenode;
		
	private:
		CSLVAvailIterator( const CSLVAvailIterator& );
		CSLVAvailIterator& operator= ( const CSLVAvailIterator& );
	};


//  ================================================================
class CSLVOwnerMapIterator
//  ================================================================
	{
	public:
		CSLVOwnerMapIterator();
		~CSLVOwnerMapIterator();

		ERR ErrInit( PIB * const ppib, const IFMP ifmp );
		ERR ErrTerm();
		ERR ErrMoveFirst();
		ERR ErrMoveNext();

		BOOL FNull() const;
		OBJID Objid() const;
		COLUMNID Columnid() const;
		const VOID * PvKey() const;
		INT CbKey() const;
		ULONG UlChecksum() const;
		USHORT CbDataChecksummed() const;
		BOOL FChecksumIsValid() const;

	private:
		FUCB * m_pfucb;
		SLVOWNERMAP m_slvownermapnode;

	private:
		CSLVOwnerMapIterator( const CSLVOwnerMapIterator& );
		CSLVOwnerMapIterator& operator= ( const CSLVOwnerMapIterator& );
	};


//  ================================================================
class RECCHECKSLVOWNERMAP : public RECCHECK
//  ================================================================
//
//  For the SLVOwnerMap
//
//-
	{
	public:
		RECCHECKSLVOWNERMAP(
			const REPAIROPTS * const popts );
		~RECCHECKSLVOWNERMAP();

		ERR operator()( const KEYDATAFLAGS& kdf );

	private:
		const REPAIROPTS * const m_popts;
	};


//  ================================================================
class RECCHECKSLVSPACE : public RECCHECK
//  ================================================================
//
//  Checks the SLVSpace bitmap
//
//-
	{
	public:
		RECCHECKSLVSPACE( const IFMP ifmp, const REPAIROPTS * const popts );
		~RECCHECKSLVSPACE();

		ERR operator()( const KEYDATAFLAGS& kdf );

	private:
		const REPAIROPTS * const 	m_popts;
		const IFMP					m_ifmp;
		LONG						m_cpagesSeen;
	};

#endif	//	DISABLE_SLV


//  ================================================================
class RECCHECKMACRO : public RECCHECK
//  ================================================================
//
//  Used to string together multiple RECCHECKs
//
//-
	{
	public:
		RECCHECKMACRO();
		~RECCHECKMACRO();

		ERR operator()( const KEYDATAFLAGS& kdf );

		VOID Add( RECCHECK * const preccheck );

	private:
		INT	m_creccheck;
		RECCHECK * m_rgpreccheck[16];
	};


//  ================================================================
class RECCHECKNULL : public RECCHECK
//  ================================================================
//
//  No-op
//
//-
	{
	public:
		RECCHECKNULL() {}
		~RECCHECKNULL() {}

		ERR operator()( const KEYDATAFLAGS& kdf ) { return JET_errSuccess; }
	};


//  ================================================================
class RECCHECKSPACE : public RECCHECK
//  ================================================================
//
//  Checks OE/AE trees
//
//-
	{
	public:
		RECCHECKSPACE( PIB * const ppib, const REPAIROPTS * const popts );
		~RECCHECKSPACE();

	public:
		ERR operator()( const KEYDATAFLAGS& kdf );
		CPG CpgSeen() const { return m_cpgSeen; }
		CPG CpgLast() const { return m_cpgLast; }
		PGNO PgnoLast() const { return m_pgnoLast; }

	protected:
		PIB		* const m_ppib;
		const REPAIROPTS * const m_popts;

	private:	
		PGNO 	m_pgnoLast;
		CPG		m_cpgLast;
		CPG		m_cpgSeen;
	};


//  ================================================================
class RECCHECKSPACEOE : public RECCHECKSPACE
//  ================================================================
//
//-
	{
	public:
		RECCHECKSPACEOE(
			PIB * const ppib,
			TTARRAY * const pttarrayOE,
			const OBJID objid,
			const OBJID objidParent,
			const REPAIROPTS * const popts );
		~RECCHECKSPACEOE();

		ERR operator()( const KEYDATAFLAGS& kdf );

	private:
		const OBJID		m_objid;
		const OBJID		m_objidParent;
	
		TTARRAY 	* const m_pttarrayOE;	
	};


//  ================================================================
class RECCHECKSPACEAE : public RECCHECKSPACE
//  ================================================================
//
//-
	{
	public:
		RECCHECKSPACEAE(
			PIB * const ppib,
			TTARRAY * const pttarrayOE,
			TTARRAY * const pttarrayAE,
			const OBJID objid,
			const OBJID objidParent,
			const REPAIROPTS * const popts );
		~RECCHECKSPACEAE();

		ERR operator()( const KEYDATAFLAGS& kdf );

	private:
		const OBJID		m_objid;
		const OBJID		m_objidParent;
		
		TTARRAY * const m_pttarrayOE;
		TTARRAY * const m_pttarrayAE;
	};


//  ================================================================
struct ENTRYINFO
//  ================================================================
	{
	ULONG			objidTable;
	SHORT			objType;
	ULONG			objidFDP;
	CHAR			szName[JET_cbNameMost + 1];
	ULONG			pgnoFDPORColType;
	ULONG 			dwFlags;
	CHAR			szTemplateTblORCallback[JET_cbNameMost + 1];
	ULONG			ibRecordOffset;		//  offset of record in fixed column
	BYTE			rgbIdxseg[JET_ccolKeyMost*sizeof(IDXSEG)];
	};

//  ================================================================
struct ENTRYTOCHECK
//  ================================================================
	{
	ULONG			objidTable;
	SHORT			objType;
	ULONG			objidFDP;
	};

//  ================================================================
struct INFOLIST
//  ================================================================
	{
	ENTRYINFO		info;
	INFOLIST	*	pInfoListNext;
	};

//  ================================================================
struct TEMPLATEINFOLIST
//  ================================================================
	{
	CHAR					szTemplateTableName[JET_cbNameMost + 1];
	INFOLIST			*	pColInfoList;
	TEMPLATEINFOLIST	*	pTemplateInfoListNext;
	};


	
//  ****************************************************************
//  PROTOTYPES
//  ****************************************************************

extern VOID NDIGetKeydataflags( const CPAGE& cpage, INT iline, KEYDATAFLAGS * pkdf );

LOCAL VOID REPAIRIPrereadIndexesOfFCB( const FCB * const pfcb );

LOCAL PGNO PgnoLast( const IFMP ifmp );

LOCAL VOID REPAIRDumpHex( CHAR * const szDest, const INT cchDest, const BYTE * const pb, const INT cb );

LOCAL VOID REPAIRDumpStats(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const BTSTATS * const pbtstats,
	const REPAIROPTS * const popts );
LOCAL VOID REPAIRPrintSig( const SIGNATURE * const psig, CPRINTF * const pcprintf );

//

LOCAL JET_ERR __stdcall ErrREPAIRNullStatusFN( JET_SESID, JET_SNP, JET_SNT, void * );

//  

LOCAL VOID REPAIRSetCacheSize( const REPAIROPTS * const popts );
LOCAL VOID REPAIRResetCacheSize( const REPAIROPTS * const popts );
LOCAL VOID REPAIRPrintStartMessages( const CHAR * const szDatabase, const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckDatabaseSize( const CHAR * const szDatabase, const IFMP ifmp, const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRFixDatabaseSize( const CHAR * const szDatabase, const IFMP ifmp, const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRGetStreamingFileSize(
	const CHAR * const szSLV,
	const IFMP ifmp,
	IFileAPI *const pfapiSLV,
	CPG * const pcpgSLV,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckStreamingFileHeader(
	INST *pinst,
	const CHAR * const szDatabase,
	const CHAR * const szSLV,
	const IFMP ifmp,
	const REPAIROPTS * const popts );
LOCAL VOID REPAIRCheckSLVAvailTreeTask( PIB * const ppib, const ULONG_PTR ul );
LOCAL VOID REPAIRCheckSLVOwnerMapTreeTask( PIB * const ppib, const ULONG_PTR ul );
LOCAL ERR ErrREPAIRStartCheckSLVTrees(
	PIB * const ppib,
	const IFMP ifmp,
	const CHAR * const szSLV,
	IFileAPI *const pfapiSLV,
	const ULONG cpgSLV,
	TASKMGR * const ptaskmgr,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVChecksumsFromFile,
	TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,
	BOOL * const pfDbtimeTooLarge,
	INTEGGLOBALS * const pintegglobals,		
	ERR * const perrSLVChecksum,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRStopCheckSLVTrees( const INTEGGLOBALS * const pintegglobals, BOOL * const pfCorrupt );
LOCAL BOOL FREPAIRRepairtableHasSLV( const REPAIRTABLE * const prepairtable );
LOCAL VOID REPAIRFreeRepairtables( REPAIRTABLE * const prepairtable );
LOCAL VOID REPAIRPrintEndMessages(
	const CHAR * const szDatabase,
	const ULONG timerStart,
	const REPAIROPTS * const popts );
LOCAL VOID INTEGRITYPrintEndErrorMessages(
	const LOGTIME logtimeLastFullBackup,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRAttachForIntegrity(
	const JET_SESID sesid,
	const CHAR * const szDatabase,
	IFMP * const pifmp,
	const REPAIROPTS * const popts );

// Routines to check the database header, global space tree and catalogs

LOCAL ERR ErrREPAIRCheckHeader(
	INST * const pinst,
	const char * const szDatabase,
	const char * const szStreamingFile,
	LOGTIME * const plogtimeLastFullBackup,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSystemTables(
	PIB * const ppib,
	const IFMP ifmp,
	TASKMGR * const ptaskmgr,
	BOOL * const pfCatalogCorrupt,
	BOOL * const pfShadowCatalogCorrupt,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts );
	
// check the logical consistency of catalogs
LOCAL ERR ErrREPAIRRetrieveCatalogColumns(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * pfucbCatalog, 
	ENTRYINFO * const pentryinfo,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertIntoTemplateInfoList(
	TEMPLATEINFOLIST ** ppTemplateInfoList, 
	const CHAR * szTemplateTable,
	INFOLIST * const pInfoList,
	const REPAIROPTS * const popts );
LOCAL VOID REPAIRUtilCleanInfoList( INFOLIST **ppInfo );
LOCAL VOID REPAIRUtilCleanTemplateInfoList( TEMPLATEINFOLIST **ppTemplateInfoList ); 
LOCAL ERR ErrREPAIRCheckOneIndexLogical(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucbCatalog,
	const ENTRYINFO entryinfo, 
	const ULONG objidTable,
	const ULONG pgnoFDPTable,
	const ULONG objidLV,
	const ULONG pgnoFDPLV,
	const INFOLIST * pColInfoList,
	const TEMPLATEINFOLIST * pTemplateInfoList, 
	const BOOL fDerivedTable,
	const CHAR * pszTemplateTableName,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckOneTableLogical( 
	INFOLIST **ppInfo, 
	const ENTRYINFO entryinfo, 
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRICheckCatalogEntryPgnoFDPs(
	PIB * const ppib,
	const IFMP ifmp, 
	PGNO  *prgpgno, 
	const ENTRYTOCHECK * const prgentryToCheck,
	INFOLIST **ppEntriesToDelete,
	const BOOL	fFix,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckFixCatalogLogical(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	const BOOL  fShadow,
	const BOOL	fFix,
	QWORD * const pqwRecords,	
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSystemTablesLogical(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	BOOL * const pfCatalogCorrupt,
	BOOL * const pfShadowCatalogCorrupt,
	const REPAIROPTS * const popts );

LOCAL ERR ErrREPAIRCheckSpaceTree(
	PIB * const ppib,
	const IFMP ifmp, 
	BOOL * const pfSpaceTreeCorrupt,
	PGNO * const ppgnoLastOwned,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts );
LOCAL ErrREPAIRCheckRange(
	PIB * const ppib,
	TTARRAY * const pttarray,
	const ULONG ulFirst,
	const ULONG ulLast,
	const ULONG ulValue,
	BOOL * const pfMismatch,
	const REPAIROPTS * const popts );

//  Checksumming the streaming files

LOCAL VOID REPAIRStreamingFileReadComplete(
		const ERR			err,
		IFileAPI *const	pfapi,
		const QWORD			ibOffset,
		const DWORD			cbData,
		const BYTE* const	pbData,
		const DWORD_PTR		dwCompletionKey );

#ifdef DISABLE_SLV
#else

LOCAL ERR ErrREPAIRAllocChecksumslvchunk(
	CHECKSUMSLVCHUNK * const pchecksumslvchunk,
	const INT cbBuffer,
	const INT cpages );
LOCAL ERR ErrREPAIRAllocChecksumslv( CHECKSUMSLV * const pchecksumslv );
LOCAL VOID REPAIRFreeChecksumslvchunk( 
	CHECKSUMSLVCHUNK * const pchecksumslvchunk, 
	const INT cpages );		
LOCAL VOID REPAIRFreeChecksumslv( CHECKSUMSLV * const pchecksumslv );

LOCAL ERR ErrREPAIRChecksumSLVChunk(
	PIB * const ppib,
	const CHECKSUMSLV * const pchecksumslv,
	const INT ichunk,
	PGNO * const ppgno,
	const PGNO pgnoMax );

LOCAL ERR ErrREPAIRSetSequentialMoveFirst(
	PIB * const ppib,
	FUCB * const pfucbSLVSpaceMap,
	const REPAIROPTS * const popts );

LOCAL ERR ErrREPAIRISetupChecksumchunk(
	FUCB * const pfucbSLVSpaceMap,
	CHECKSUMSLV * const pchecksumslv,
	BOOL * const pfDone,
	BOOL * const pfChunkHasChecksums,
	const INT ichunk );
LOCAL ERR ErrREPAIRCheckSLVChecksums(
	PIB * const ppib,
	FUCB * const pfucbSLVOwnerMap,
	CHECKSUMSLV * const pchecksumslv );

LOCAL VOID REPAIRSLVChecksumTask( PIB * const ppib, const ULONG_PTR ul );
LOCAL ERR ErrREPAIRChecksumSLV(
	const IFMP ifmp,
	IFileAPI *const pfapiSLV,
	const CHAR * const szSLV,
	const CPG cpgSLV,
	TASKMGR * const ptaskmgr,
	ERR * const perr,
	TTARRAY * const pttarraySLVChecksumsFromFile,
	TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,			
	const REPAIROPTS * const popts );

//  Routines to check the SLV avail and space-map trees	

LOCAL ERR ErrREPAIRCheckSLVAvailTree(
	PIB * const ppib,
	const IFMP ifmp, 
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSLVOwnerMapTree(
	PIB * const ppib,
	const IFMP ifmp, 
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRVerifySLVTrees(
	PIB * const ppib, 
	const IFMP ifmp,
	BOOL * const pfSLVSpaceTreesCorrupt,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,
	TTARRAY	* const pttarraySLVOwnerMapKey,	
	TTARRAY	* const pttarraySLVChecksumLengths,		
	const REPAIROPTS * const popts );

#endif	//	DISABLE_SLV

//	Routines to check the space allocation of a table

LOCAL ERR ErrREPAIRGetPgnoOEAE( 
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	PGNO * const ppgnoOE,
	PGNO * const ppgnoAE,
	PGNO * const ppgnoParent,	
	const BOOL fUnique,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertSEInfo(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertOERunIntoTT(
	PIB * const ppib,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertAERunIntoTT(
	PIB * const ppib,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );

//  Routines to check tables

LOCAL ERR ErrREPAIRStartCheckAllTables(
	PIB * const ppib,
	const IFMP ifmp,
	TASKMGR * const ptaskmgr,
	REPAIRTABLE ** const pprepairtable,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,	
	TTARRAY	* const pttarraySLVOwnerMapKey,
	TTARRAY * const pttarraySLVChecksumLengths,
	BOOL * const pfDbtimeTooLarge,
	INTEGGLOBALS * const pintegglobals,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRStopCheckTables( const INTEGGLOBALS * const pintegglobals, BOOL * const pfCorrupt );
LOCAL ERR ErrREPAIRPostTableTask(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucbCatalog,
	const CHAR * const szTable,
	REPAIRTABLE ** const pprepairtable,
	INTEGGLOBALS * const pintegglobals,
	TASKMGR * const ptaskmgr,
	BOOL * const pfCorrupted,
	const REPAIROPTS * const popts );
LOCAL VOID REPAIRCheckOneTableTask( PIB * const ppib, const ULONG_PTR ul );
LOCAL VOID REPAIRCheckTreeAndSpaceTask( PIB * const ppib, const ULONG_PTR ul );
LOCAL BOOL FREPAIRTableHasSLVColumn( const FCB * const pfcb );
LOCAL ERR ErrREPAIRCheckOneTable(
	PIB * const ppib,
	const IFMP ifmp,
	const char * const szTable,
	const OBJID objidTable,
	const PGNO pgnoFDP,
	const CPG cpgPrimaryExtent,
	REPAIRTABLE ** const pprepairtable,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,
	TTARRAY	* const pttarraySLVOwnerMapKey,
	TTARRAY * const pttarraySLVChecksumLengths,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCompareLVRefcounts(
	PIB * const ppib,
	const IFMP ifmp,
	TTMAP& ttmapLVRefcountsFromTable,
	TTMAP& ttmapLVRefcountsFromLV,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSplitBuf( 
	PIB * const ppib,
	const PGNO pgnoLastBuffer, 
	const CPG cpgBuffer,	
	const OBJID objidCurrent,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSPLITBUFFERInSpaceTree(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const OBJID objidCurrent,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckTree(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts );	
LOCAL ERR ErrREPAIRCheckTreeAndSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRecheckSpaceTreeAndSystemTablesSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const CPG cpgDatabase,
	BOOL * const pfSpaceTreeCorrupt,
	TTARRAY ** const ppttarrayOwnedSpace,
	TTARRAY ** const ppttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckTree(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoRoot,
	const OBJID objidFDP,
	const ULONG fPageFlags,
	RECCHECK * const preccheck,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const BOOL fNonUnique,
	BOOL * const pfDbtimeTooLarge,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRICheck(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objidFDP,
	const ULONG fFlagsFDP,
	CSR& csr,
	const BOOL fPrereadSibling,
	RECCHECK * const preccheck,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const BOOL fNonUnique,
	BOOL * const pfDbtimeTooLarge,
	BTSTATS *const  btstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRICheckNode(
	const PGNO pgno,
	const INT iline,
	const BYTE * pbPage,
	const KEYDATAFLAGS& kdf,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRICheckRecord(
	const PGNO pgno,
	const INT iline,
	const BYTE * const pbPage,
	const KEYDATAFLAGS& kdf,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRICheckInternalLine(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts,
	KEYDATAFLAGS& kdfCurr,
	const KEYDATAFLAGS& kdfPrev );
LOCAL ERR ErrREPAIRICheckLeafLine(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	RECCHECK * const preccheck,
	const BOOL fNonUnique,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts,
	KEYDATAFLAGS& kdfCurr,
	const KEYDATAFLAGS& kdfPrev,
	BOOL * const pfEmpty );
LOCAL ERR ErrREPAIRCheckInternal(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	BTSTATS * const pbtstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckLeaf(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	RECCHECK * const preccheck,
	const BOOL fNonUnique,
	BTSTATS * const pbtstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts );

//  Preparing to repair

LOCAL ERR ErrREPAIRCreateTempTables(
	PIB * const ppib,
	const BOOL fRepairGlobalSpace,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts );

//  Scanning all the pages in the database

LOCAL ERR ErrREPAIRScanDB(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTT * const prepairtt,
	DBTIME * const pdbtimeLast,
	OBJID  * const pobjidLast,
	PGNO   * const ppgnoLastOESeen,
	const REPAIRTABLE * const prepairtable,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertPageIntoTables(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	REPAIRTT * const prepairtt,
	const REPAIRTABLE * const prepairtable,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertBadPageIntoTables(
	PIB * const ppib,
	const PGNO pgno,
	REPAIRTT * const prepairtt,
	const REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );

//	Attach the database to repair it, changing the header

LOCAL ERR ErrREPAIRAttachForRepair(
	const JET_SESID sesid,
	const CHAR * const szDatabase,
	const CHAR * const szSLV,
	IFMP * const pifmp,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRChangeDBSignature(
	INST *pinst,
	const char * const szDatabase,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	SIGNATURE * const psignDb,
	SIGNATURE * const psignSLV,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRChangeSLVSignature(
	INST *pinst,
	const char * const szSLV,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const SIGNATURE * const psignDb,
	const SIGNATURE * const psignSLV,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRChangeSignature(
	INST *pinst,
	const char * const szDatabase,
	const char * const szSLV,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const REPAIROPTS * const popts );

//  Repair the global space tree

LOCAL ERR ErrREPAIRRepairGlobalSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts );

//  Fix the catalogs, copying from the shadow if necessary

LOCAL ERR ErrREPAIRBuildCatalogEntryToDeleteList( 
	INFOLIST **ppDeleteList, 
	const ENTRYINFO entryinfo );
LOCAL ERR ErrREPAIRDeleteCorruptedEntriesFromCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const INFOLIST *pTablesToDelete,
	const INFOLIST *pEntriesToDelete,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertMSOEntriesToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRepairCatalogs(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	const BOOL fCatalogCorrupt,
	const BOOL fShadowCatalogCorrupt, 
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRScanDBAndRepairCatalogs(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertCatalogRecordIntoTempTable(
	PIB * const ppib,
	const IFMP ifmp,
	const KEYDATAFLAGS& kdf,
	const JET_TABLEID tableid,
	const JET_COLUMNID columnidKey,
	const JET_COLUMNID columnidData,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCopyTempTableToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const JET_TABLEID tableid,
	const JET_COLUMNID columnidKey,
	const JET_COLUMNID columnidData,
	const REPAIROPTS * const popts );

//	Repair ordinary tables

LOCAL ERR ErrREPAIRRepairDatabase(
	PIB * const ppib,
	const CHAR * const szDatabase,
	const CHAR * const szSLV,
	const CPG cpgSLV,
	IFMP * const pifmp,
	const OBJID objidLast,
	const PGNO pgnoLastOE,
	REPAIRTABLE * const prepairtable,
	const BOOL fRepairedCatalog,
	BOOL fRepairGlobalSpace,
	const BOOL fRepairSLVSpace,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY * const pttarraySLVChecksumLengths,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,
	TTARRAY	* const pttarraySLVOwnerMapKey,		
	TTARRAY * const pttarraySLVChecksumsFromFile,
	TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,
	const REPAIROPTS * const popts );

#ifdef DISABLE_SLV
#else

LOCAL ERR ErrREPAIRDeleteSLVSpaceTrees(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts );	
LOCAL ERR ErrREPAIRCreateSLVSpaceTrees(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRebuildSLVSpaceTrees(
	PIB * const ppib,
	const IFMP ifmp,
	const CHAR * const szSLV,
	const CPG cpgSLV,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY * const pttarraySLVChecksumLengths,
	TTARRAY * const pttarraySLVOwnerMapColumnid,
	TTARRAY * const pttarraySLVOwnerMapKey,
	TTARRAY * const pttarraySLVChecksumsFromFile,
	TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,	
	const OBJIDLIST * const pobjidlist,
	const REPAIROPTS * const popts );

#endif	//	DISABLE_SLV


LOCAL ERR ErrREPAIRRepairTable(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTT * const prepairtt,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRebuildBT(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTABLE * const prepairtable,
	FUCB * const pfucbTable,
	PGNO * const ppgnoFDP,
	const ULONG fPageFlags,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCreateEmptyFDP(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoParent,
	PGNO * const pgnoFDPNew,
	const ULONG fPageFlags,
	const BOOL fUnique,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRebuildSpace(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	const PGNO pgnoParent,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertRunIntoSpaceTree(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRRebuildInternalBT(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts );

LOCAL ERR ErrREPAIRFixLVs(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoLV,
	TTMAP * const pttmapLVTree,
	const BOOL fFixMissingLVROOT,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );

#ifdef DISABLE_SLV
#else

LOCAL ERR ErrREPAIRCheckSLVInfoForUpdateTrees( 
	PIB * const ppib,
	CSLVInfo * const pslvinfo,
	const OBJID objidFDP,
	TTARRAY * const pttarraySLVAvail,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRUpdateSLVAvailFromSLVRun( 
	PIB * const ppib,
	const IFMP ifmp,
	const CSLVInfo::RUN& slvRun,
	FUCB * const pfucbSLVAvail,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRUpdateSLVOwnerMapFromSLVRun( 
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objidFDP,
	const COLUMNID columnid,
	const BOOKMARK& bm,
	const CSLVInfo::RUN& slvRun,
	SLVOWNERMAPNODE * const pslvownermapNode,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRUpdateSLVTablesFromSLVInfo( 
	PIB * const ppib,
	const IFMP ifmp,
	CSLVInfo * const pslvinfo,
	const BOOKMARK& bm,
	const OBJID objidFDP,
	const COLUMNID columnid,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,	
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckUpdateLVsFromColumn( 
	FUCB * const pfucb,
	TAGFLD_ITERATOR& tagfldIterator,
	const TTMAP * const pttmapLVTree,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRUpdateLVsFromColumn( 
	FUCB * const pfucb,
	TAGFLD_ITERATOR& tagfldIterator,
	TTMAP * const pttmapRecords,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckUpdateSLVsFromColumn( 
	FUCB * const pfucb,
	const COLUMNID columnid,
	const TAGFIELDS_ITERATOR& tagfieldsIterator,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRUpdateSLVsFromColumn( 
	FUCB * const pfucb,
	const COLUMNID columnid,
	const INT itagSequence,
	const TAGFLD * const ptagfld,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const REPAIROPTS * const popts );

#endif	//	DISABLE_SLV


LOCAL ERR ErrREPAIRAddOneCatalogRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const ULONG	objidTable,
	const COLUMNID	fidColumnLastInRec,
	const USHORT ibRecordOffset, 
	const ULONG	cbMaxLen,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRInsertDummyRecordsToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRCheckFixOneRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const DATA& dataRec,
	FUCB * const pfucb,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const TTMAP * const pttmapLVTree,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRFixOneRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const DATA& dataRec,
	FUCB * const pfucb,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	TTMAP * const pttmapRecords,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRFixRecords(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	TTMAP * const pttmapRecords,
	const TTMAP * const pttmapLVTree,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRFixLVRefcounts(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoLV,
	TTMAP * const pttmapRecords,
	TTMAP * const pttmapLVTree,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRFixupTable(
	PIB * const ppib,
	const IFMP ifmp,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );
LOCAL ERR ErrREPAIRBuildAllIndexes(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB ** const ppfucb,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts );


//  ****************************************************************
//  GLOBALS
//  ****************************************************************

//  These are typically very large tables. Start integrity checking
//  them first for maximum overlap
//
//  THE TABLENAMES MUST BE IN ASCENDING ALPHABETICAL ORDER
//  if they are not FIsLargeTable will not work properly!
//

const CHAR * rgszLargeTables[] = {
	"1-24",
	"1-2F",
	"1-A",
	"1-D",
	"Folders",
	"Msg"
};
const INT cszLargeTables = sizeof( rgszLargeTables ) / sizeof( rgszLargeTables[0] );

//  To take advantage of sequential NT I/O round up sequential prereads to this many pages (64K)

const INT g_cpgMinRepairSequentialPreread = ( 64 * 1024 ) / g_cbPage;

//  When examing the SLV space maps we don't store the entire key. This determines
//  how many bytes to store (rounded up to the nearest ULONG
//  the first BYTE stores the length of the key

static const cbSLVKeyToStore = 15;
static const culSLVKeyToStore = cbSLVKeyToStore + sizeof( ULONG ) / sizeof( ULONG );

//  Checksumming the SLV files
//  These must come out to a multiple of the SLV file size

static const cbSLVChecksumChunk = 64 * 1024;
static const cSLVChecksumChunk	= 32;

//	Any SLV pages owned by this objid have invalid checksums

const OBJID objidInvalid = 0xfffffffe;


//  ================================================================
ERR ErrDBUTLRepair( JET_SESID sesid, const JET_DBUTIL *pdbutil, CPRINTF* const pcprintf )
//  ================================================================
	{
	Assert( NULL != pdbutil->szDatabase );
	
	ERR 		err 	= JET_errSuccess;
	PIB * const ppib 	= reinterpret_cast<PIB *>( sesid );
	INST * const pinst	= PinstFromPpib( ppib );

	const CHAR * const szDatabase		= pdbutil->szDatabase;
	const CHAR * const szSLV			= pdbutil->szSLV;

	_TCHAR szFolder[ IFileSystemAPI::cchPathMax ];
	_TCHAR szPrefix[ IFileSystemAPI::cchPathMax ];
	_TCHAR szFileExt[ IFileSystemAPI::cchPathMax ];
	_TCHAR szFile[ IFileSystemAPI::cchPathMax ];

	const	INT cThreads 			= ( CUtilProcessProcessor() * 4 );
		
	CPRINTFFN	cprintfStdout( printf );
	
	if ( pdbutil->szIntegPrefix )
		{
		_tcscpy( szPrefix, pdbutil->szIntegPrefix );
		}
	else
		{
		CallR( pinst->m_pfsapi->ErrPathParse(	pdbutil->szDatabase,
												szFolder,
												szPrefix,
												szFileExt ) );
		}
	_tcscpy( szFile, szPrefix );
	_tcscat( szFile, ".INTEG.RAW" );
	CPRINTFFILE cprintfFile( szFile );
	
	_tcscpy( szFile, szPrefix );
	_tcscat( szFile, ".INTGINFO.TXT" );
	CPRINTF * const pcprintfStatsInternal = ( pdbutil->grbitOptions & JET_bitDBUtilOptionStats ) ?
									new CPRINTFFILE( szFile ) :
									CPRINTFNULL::PcprintfInstance();
	if( NULL == pcprintfStatsInternal )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	CPRINTFTLSPREFIX cprintf( pcprintf );
	CPRINTFTLSPREFIX cprintfVerbose( &cprintfFile );
	CPRINTFTLSPREFIX cprintfDebug( &cprintfFile );
	CPRINTFTLSPREFIX cprintfError( &cprintfFile, "ERROR: " );
	CPRINTFTLSPREFIX cprintfWarning( &cprintfFile, "WARNING: " );
	CPRINTFTLSPREFIX cprintfStats( pcprintfStatsInternal );

#ifdef REPAIR_DEBUG_CALLS
	pcprintfRepairDebugCalls = &cprintfError;
#endif	//	REPAIR_DEBUG_CALLS

	JET_SNPROG	snprog;
	memset( &snprog, 0, sizeof( JET_SNPROG ) );
	snprog.cbStruct = sizeof( JET_SNPROG );

	REPAIROPTS	repairopts;	
	repairopts.grbit			= pdbutil->grbitOptions;
	repairopts.pcprintf			= &cprintf;
	repairopts.pcprintfVerbose 	= &cprintfVerbose;
	repairopts.pcprintfError 	= &cprintfError;
	repairopts.pcprintfWarning	= &cprintfWarning;
	repairopts.pcprintfDebug	= &cprintfDebug;
	repairopts.pcprintfStats	= &cprintfStats;
	repairopts.pfnStatus		= pdbutil->pfnCallback ? (JET_PFNSTATUS)(pdbutil->pfnCallback) : ErrREPAIRNullStatusFN;
	repairopts.psnprog			= &snprog;

	const REPAIROPTS * const popts = &repairopts;

	//	startup messages
	
	REPAIRPrintStartMessages( szDatabase, popts );

	//	first, set the cache size. the BF clean thread will see the change
	//	and grow the cache
	
	REPAIRSetCacheSize( popts );

	//
	
	ERR				errSLVChecksum				= JET_errSuccess;
	
	BOOL			fGlobalSpaceCorrupt			= fFalse;
	BOOL 			fCatalogCorrupt				= fFalse;
	BOOL 			fShadowCatalogCorrupt		= fFalse;
	BOOL			fSLVSpaceTreesCorrupt		= fFalse;
	BOOL			fTablesCorrupt				= fFalse;
	BOOL			fStreamingFileCorrupt		= fFalse;
	BOOL			fUnicodeFixupTableCorrupt	= fFalse;
	BOOL			fRepairedCatalog			= fFalse;

	TTARRAY * pttarrayOwnedSpace 			= NULL;
	TTARRAY * pttarrayAvailSpace 			= NULL;
	TTARRAY * pttarraySLVAvail 				= NULL;
	TTARRAY * pttarraySLVOwnerMapColumnid	= NULL;
	TTARRAY * pttarraySLVOwnerMapKey		= NULL;
	TTARRAY * pttarraySLVChecksumLengths	= NULL;

	TTARRAY * pttarraySLVChecksumsFromFile				= NULL;
	TTARRAY * pttarraySLVChecksumLengthsFromSpaceMap	= NULL;
	
	REPAIRTABLE *	prepairtable 				= NULL;
	IFileAPI *		pfapiSLV 					= NULL;
	IFMP			ifmp						= 0xffffffff;	
	CPG 			cpgDatabase					= 0;
	CPG 			cpgSLV						= 0;
	OBJID 			objidLast					= objidNil;
	PGNO			pgnoLastOE					= pgnoNull;
	TASKMGR			taskmgr;
	
	INTEGGLOBALS	integglobalsTables;
	INTEGGLOBALS	integglobalsSLVSpaceTrees;

	BOOL			fDbtimeTooLarge				= fFalse;

	LOGTIME			logtimeLastFullBackup;
	memset( &logtimeLastFullBackup, 0, sizeof( LOGTIME ) );

	
	const ULONG		timerStart					= TickOSTimeCurrent();

	BOOL			fGlobalRepairSave			= fGlobalRepair;

	//  unless fDontRepair is set this will set the consistency bit so we can attach
	
	Call( ErrREPAIRCheckHeader(
			pinst,
			szDatabase,
			szSLV,
			&logtimeLastFullBackup,
			popts ) );

	//  set the magic switch!
	
	fGlobalRepair = fTrue;

	//	make sure the SLV matches the database
	
	if( szSLV )
		{		
#ifdef DISABLE_SLV
		Call( ErrERRCheck( JET_wrnNyi ) );
#else
		Call( ErrREPAIRCheckStreamingFileHeader(
				pinst,
				szDatabase,
				szSLV,
				ifmp,
				popts ) );				
#endif
		}

	//  attach to the database
	
	Call( ErrREPAIRAttachForIntegrity( sesid, szDatabase, &ifmp, popts ) );
	cpgDatabase = PgnoLast( ifmp );

	//	make sure the database is a multiple of the page size

	err = ErrREPAIRCheckDatabaseSize( szDatabase, ifmp, popts );
	if( JET_errDatabaseCorrupted == err )
		{		
		if ( popts->grbit & JET_bitDBUtilOptionDontRepair )
			{
			(*popts->pcprintfVerbose)( "The database is the wrong size. Integrity check cannot continue\r\n"  );
			(VOID)INTEGRITYPrintEndErrorMessages( logtimeLastFullBackup, popts );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		else
			{
			Call( ErrREPAIRFixDatabaseSize( szDatabase, ifmp, popts ) );
			}
		}
	Call( err );

	//  preread the first 2048 pages (8/16 megs)
	
	BFPrereadPageRange( ifmp, pgnoSystemRoot, min( 2048, cpgDatabase ) );

	//	reset the (repair-only) count of undefined unicode entries in the database
	//	at the end of repair we can compare this against the actual number of records in
	//	the fixup table

	rgfmp[ifmp].ResetCundefinedUnicodeEntries();
	
	//	open the SLV file and get its size

	if( szSLV )
		{		
#ifdef DISABLE_SLV
		Call( ErrERRCheck( JET_wrnNyi ) );
#else
		Call( pinst->m_pfsapi->ErrFileOpen( szSLV, &pfapiSLV, fTrue ) );
		Call( ErrREPAIRGetStreamingFileSize(
				szSLV,
				ifmp,
				pfapiSLV,
				&cpgSLV,
				popts ) );
		Assert( rgfmp[ifmp].CbSLVFileSize() == ( (QWORD)cpgSLV << (QWORD)g_shfCbPage ) );		

		//	we can't have a file open read-only and read-write at the same time
		//	close the file in case we need to open it to update the header
		//	during a catalog repair
		
		delete pfapiSLV;
		pfapiSLV = NULL;
#endif
		}
		
	//	init the TTARRAY's
	
	pttarrayOwnedSpace 				= new TTARRAY( cpgDatabase + 1, objidSystemRoot );
	pttarrayAvailSpace 				= new TTARRAY( cpgDatabase + 1, objidNil );
	pttarraySLVAvail 				= new TTARRAY( cpgSLV + 1, objidNil );
	pttarraySLVOwnerMapColumnid 	= new TTARRAY( cpgSLV + 1, 0 );
	pttarraySLVOwnerMapKey 			= new TTARRAY( ( cpgSLV + 1 ) * culSLVKeyToStore, 0 );
	pttarraySLVChecksumLengths 		= new TTARRAY( cpgSLV + 1, 0xFFFFFFFF );
	pttarraySLVChecksumsFromFile			= new TTARRAY( cpgSLV + 1, 0xFFFFFFFF );
	pttarraySLVChecksumLengthsFromSpaceMap 	= new TTARRAY( cpgSLV + 1, 0x0 );	
	if( NULL == pttarrayOwnedSpace
		|| NULL == pttarrayAvailSpace
		|| NULL == pttarraySLVAvail
		|| NULL == pttarraySLVOwnerMapColumnid
		|| NULL == pttarraySLVOwnerMapKey
		|| NULL == pttarraySLVChecksumLengths
		|| NULL == pttarraySLVChecksumsFromFile
		|| NULL == pttarraySLVChecksumLengthsFromSpaceMap
		)
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
		
	Call( pttarrayOwnedSpace->ErrInit( pinst ) );
	Call( pttarrayAvailSpace->ErrInit( pinst ) );
	Call( pttarraySLVAvail->ErrInit( pinst ) );
	Call( pttarraySLVOwnerMapColumnid->ErrInit( pinst ) );
	Call( pttarraySLVOwnerMapKey->ErrInit( pinst ) );
	Call( pttarraySLVChecksumLengths->ErrInit( pinst ) );
	Call( pttarraySLVChecksumsFromFile->ErrInit( pinst ) );
	Call( pttarraySLVChecksumLengthsFromSpaceMap->ErrInit( pinst ) );

	//	init the TASKMGR

	(*popts->pcprintfVerbose)( "Creating %d threads\r\n", cThreads );
	Call( taskmgr.ErrInit( pinst, cThreads ) );

	//	init the status bar

	snprog.cunitTotal 	= cpgDatabase;
	snprog.cunitDone 	= 0;
	(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntBegin, NULL );	

	//	Set the cache size to 0 to allow DBA to do its thing

	REPAIRResetCacheSize( popts );

	//	check the global space trees
	
	Call( ErrREPAIRCheckSpaceTree(
			ppib,
			ifmp,
			&fGlobalSpaceCorrupt,
			&pgnoLastOE,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			&fDbtimeTooLarge,
			popts ) );

	//	check the catalog and shadow catalog
	
	Call( ErrREPAIRCheckSystemTables(
			ppib,
			ifmp,
			&taskmgr,
			&fCatalogCorrupt,
			&fShadowCatalogCorrupt,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			&fDbtimeTooLarge,
			popts ) );

	// IF 	at least one of catalogs is physically consistent
	// THEN Check the logical consistency of catalogs
	if ( !fCatalogCorrupt || !fShadowCatalogCorrupt )
		{
		Call( ErrREPAIRCheckSystemTablesLogical( 
			ppib, 
			ifmp, 
			&objidLast,
			&fCatalogCorrupt, 
			&fShadowCatalogCorrupt,
			popts ) );
		}


	//  if needed we have to repair the catalog right now so that we can access the other tables
			
	if( fCatalogCorrupt || fShadowCatalogCorrupt )
		{
		if ( popts->grbit & JET_bitDBUtilOptionDontRepair )
			{
			(*popts->pcprintfVerbose)( "The catalog is corrupted. not all tables could be checked\r\n"  );
			(VOID)INTEGRITYPrintEndErrorMessages( logtimeLastFullBackup, popts );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );

			}
		else
			{
					
			//  We don't know the dbtimeLast yet. We'll find it when we scan the database
			//  and attach for repair again. we set the dbtimeLast to 1. This will be
			//  fine as we will set the dbtime in the header later and when these pages are updated
			//  later they will get a good dbtime
			
			Call( ErrREPAIRAttachForRepair( sesid, szDatabase, szSLV, &ifmp, 1, objidNil, popts ) );
			if( fGlobalSpaceCorrupt 
				|| cpgDatabase > pgnoLastOE )
				{
				Call( ErrREPAIRRepairGlobalSpace( ppib, ifmp, popts ) );
				fGlobalSpaceCorrupt 	= fFalse;
				}

			(*popts->pcprintfVerbose)( "rebuilding catalogs\r\n"  );
			(*popts->pcprintfVerbose).Indent();
			Call( ErrREPAIRRepairCatalogs( 
						ppib, 
						ifmp, 
						&objidLast, 
						fCatalogCorrupt, 
						fShadowCatalogCorrupt, 
						popts ) );
			(*popts->pcprintfVerbose).Unindent();

			//  Flush the entire database so that if we crash here we don't have to repair the catalogs again
			
			(VOID)ErrBFFlush( ifmp );

			//Space tree may change after repairing catalog
			//Re-build pttarrayOwnedSpace and pttarrayAvailSpace
			Call( ErrREPAIRRecheckSpaceTreeAndSystemTablesSpace(
						ppib,
						ifmp,
						cpgDatabase,
						&fGlobalSpaceCorrupt,
						&pttarrayOwnedSpace,
						&pttarrayAvailSpace,
						&fDbtimeTooLarge,
						popts ) );

			fCatalogCorrupt 		= fFalse;
			fShadowCatalogCorrupt 	= fFalse;
			fRepairedCatalog		= fTrue;

			// done repairing catalogs
			(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntComplete, NULL );

			// continue integrity check 
			(*popts->pcprintf)( "\r\nChecking the database.\r\n"  );
			popts->psnprog->cunitTotal 	= cpgDatabase;
			popts->psnprog->cunitDone	= 0;
			(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntBegin, NULL );
			}
		}

	//	start checking the SLV space trees

	if( szSLV )
		{		
#ifdef DISABLE_SLV
		Call( ErrERRCheck( JET_wrnNyi ) );
#else
		Call( pinst->m_pfsapi->ErrFileOpen( szSLV, &pfapiSLV, fTrue ) );		
		Call( ErrREPAIRStartCheckSLVTrees(
				ppib,
				ifmp,
				szSLV,
				pfapiSLV,
				cpgSLV,
				&taskmgr,
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				pttarraySLVChecksumsFromFile,
				pttarraySLVChecksumLengthsFromSpaceMap,
				&fDbtimeTooLarge,
				&integglobalsSLVSpaceTrees,
				&errSLVChecksum,
				popts ) );
#endif
		}

	//  check all the normal tables in the system 
	//  if this returns JET_errDatabaseCorrupted it means there is a corruption in the catalog
	
	Call( ErrREPAIRStartCheckAllTables(
			ppib,
			ifmp,
			&taskmgr,
			&prepairtable, 
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			pttarraySLVAvail,
			pttarraySLVOwnerMapColumnid,
			pttarraySLVOwnerMapKey,
			pttarraySLVChecksumLengths,
			&fDbtimeTooLarge,
			&integglobalsTables,
			popts ) );

	//  terminate the taskmgr. once all the threads have stopped all the checks will
	//  have been done and we can collect the results
	
	Call( taskmgr.ErrTerm() );

	//	are the SLV space trees corrupt?

	if( szSLV )
		{		
#ifdef DISABLE_SLV
		Call( ErrERRCheck( JET_wrnNyi ) );
#else
		Call( ErrREPAIRStopCheckSLVTrees(
					&integglobalsSLVSpaceTrees,
					&fSLVSpaceTreesCorrupt ) );
		delete pfapiSLV;
		pfapiSLV = NULL;
#endif
		}

	//	was the streaming file corrupt
	if( !fSLVSpaceTreesCorrupt )
		{
		if( JET_errSLVReadVerifyFailure == errSLVChecksum )
			{
			fStreamingFileCorrupt = fTrue;
			}
		else
			{
			Call( errSLVChecksum );
			}
		}
	
	//	were any tables corrupt?
	
	Call( ErrREPAIRStopCheckTables(
				&integglobalsTables,
				&fTablesCorrupt ) );
	if( objidNil == objidLast )
		{
		(*popts->pcprintfWarning)( "objidLast is objidNil (%d)\r\n", objidNil );
		objidLast = objidNil + 1;
		}

	//	if we don't think the global space tree is corrupt, check to see
	//	if any of the pages beyond the end of the global OwnExt are actually
	//	being used by other tables

	if( !fGlobalSpaceCorrupt )
		{
		const PGNO pgnoLastPhysical 	= cpgDatabase;
		const PGNO pgnoLastLogical	 	= pgnoLastOE;
		
		if( pgnoLastPhysical < pgnoLastLogical )
			{
			(*popts->pcprintfError)( "Database file is too small (expected %d pages, file is %d pages)\r\n",
										pgnoLastLogical, pgnoLastPhysical );
			fGlobalSpaceCorrupt = fTrue;
			}
		else if( pgnoLastPhysical > pgnoLastLogical )
			{
			
			//	make sure that every page from the logical last to the 
			//	physical last is owned by the database

			(*popts->pcprintfWarning)( "Database file is too big (expected %d pages, file is %d pages)\r\n",
										pgnoLastLogical, pgnoLastPhysical );
			
			Call( ErrREPAIRCheckRange(
					ppib,
					pttarrayOwnedSpace,
					pgnoLastLogical + 1,
					pgnoLastPhysical,
					objidSystemRoot,
					&fGlobalSpaceCorrupt,
					popts ) );

			if( fGlobalSpaceCorrupt )
				{
				(*popts->pcprintfError)( "Global space tree is too small (has %d pages, file is %d pages). "
										 "The space tree will be rebuilt\r\n",
										pgnoLastLogical, pgnoLastPhysical );
				}
					
			}
		}

	if( !fTablesCorrupt && FNORMStringHasUndefinedCharsIsSupported() )
		{
		//	none of the tables are corrupt. that means we can check the unicode fixup table
		//	there is a count of the number of invalid unicode entries (in indexes that are flagged for fixup)
		//	in the FMP. verify the fixup table and compare the number of entries in it against
		//	the number of entries in the fixup table

		Assert( !fUnicodeFixupTableCorrupt );
		(*popts->pcprintfVerbose)( "checking unicode fixup table\r\n"  );

		QWORD qwRecordsSeen = 0;
		err = ErrCATVerifyMSU( ppib, ifmp, 0x7fffffffffffffff, &qwRecordsSeen, popts->pcprintfError );
		if( err < JET_errSuccess )
			{
			(*popts->pcprintfError)( "error %d checking the unicode fixup table\r\n", err );
			fUnicodeFixupTableCorrupt = fTrue;
			err = JET_errSuccess;
			}
		else if( qwRecordsSeen != rgfmp[ifmp].CundefinedUnicodeEntries() )
			{
			(*popts->pcprintfError)( "unicode fixup table has %I64d entries, but %I64d undefined entries were seen in the database\r\n",
				qwRecordsSeen,
				rgfmp[ifmp].CundefinedUnicodeEntries() );
			fUnicodeFixupTableCorrupt = fTrue;			
			}
		}
	
#ifdef DISABLE_SLV
#else

	//  see if any of the corrupted tables had SLV columns
	//  if so, we will need to rebuild the SLV space maps
	
	if ( fTablesCorrupt
		&& !fSLVSpaceTreesCorrupt
		&& szSLV )
		{		
		fSLVSpaceTreesCorrupt = FREPAIRRepairtableHasSLV( prepairtable );
		}

	//  Now all the tables have been checked we can verify the SLV space structures
	
	if ( !fSLVSpaceTreesCorrupt
		&& szSLV )
		{
		err = ErrREPAIRVerifySLVTrees(
					ppib,
					ifmp,
					&fSLVSpaceTreesCorrupt,
					pttarraySLVAvail,
					pttarraySLVOwnerMapColumnid,
					pttarraySLVOwnerMapKey,
					pttarraySLVChecksumLengths,
					popts );	
		if( JET_errDatabaseCorrupted == err )
			{
			err = JET_errSuccess;
			}
		Call( err );
		}

#endif	//	DISABLE_SLV

	//  finished with the integrity checking phase
	
	(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntComplete, NULL );	
	(*popts->pcprintfVerbose).Unindent();
				
	//  repair the database, or exit if we are just doing an integrity check 

	Assert( !fCatalogCorrupt );
	Assert( !fShadowCatalogCorrupt );

	//	bugfix (X5:121062, X5:121266): need to scan the database if we repaired the catalog and
	//	no tables are corrupt. the dbtime, objid must be set and bad pages must be zeroed out
	
	if( fTablesCorrupt || 
		fGlobalSpaceCorrupt || 
		fSLVSpaceTreesCorrupt || 
		fRepairedCatalog || 
		fStreamingFileCorrupt ||
		fUnicodeFixupTableCorrupt ||
		fDbtimeTooLarge )
		{				
		if( !( popts->grbit & JET_bitDBUtilOptionDontRepair ) )
			{
			Call( ErrREPAIRRepairDatabase( 
					ppib,
					szDatabase,
					szSLV,
					cpgSLV,
					&ifmp,
					objidLast,
					pgnoLastOE,
					prepairtable,
					fRepairedCatalog,
					fGlobalSpaceCorrupt,
					fSLVSpaceTreesCorrupt | fStreamingFileCorrupt,
					pttarrayOwnedSpace,
					pttarrayAvailSpace,
					pttarraySLVAvail,
					pttarraySLVChecksumLengths,
					pttarraySLVOwnerMapColumnid,
					pttarraySLVOwnerMapKey,
					pttarraySLVChecksumsFromFile,
					pttarraySLVChecksumLengthsFromSpaceMap,
					popts ) );
			(*popts->pcprintfVerbose)( "\r\nRepair completed. Database corruption has been repaired!\r\n\n" );
			(*popts->pcprintf)( "\r\nRepair completed. Database corruption has been repaired!\r\n\n" );
			err = ErrERRCheck( JET_wrnDatabaseRepaired );
			}
		else
			{
			(*popts->pcprintfVerbose)( "\r\nIntegrity check completed. Database is CORRUPTED!\r\n" );
			(VOID)INTEGRITYPrintEndErrorMessages( logtimeLastFullBackup, popts );
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			}
		}
	else
		{
		(*popts->pcprintfVerbose)( "\r\nIntegrity check successful.\r\n\r\n" );
		(*popts->pcprintf)( "\r\nIntegrity check successful.\r\n\r\n" );
		}

	(VOID)ErrIsamCloseDatabase( sesid, (JET_DBID)ifmp, 0 );
	(VOID)ErrIsamDetachDatabase( sesid, NULL, szDatabase );

HandleError:

	CallS( taskmgr.ErrTerm() );
	
	(*popts->pcprintfVerbose).Unindent();

	REPAIRPrintEndMessages( szDatabase, timerStart, popts );
	
	REPAIRFreeRepairtables( prepairtable );

	delete pttarrayOwnedSpace;
	delete pttarrayAvailSpace;
	delete pttarraySLVAvail;
	delete pttarraySLVOwnerMapColumnid;
	delete pttarraySLVOwnerMapKey;
	delete pttarraySLVChecksumLengths;

	delete pttarraySLVChecksumsFromFile;
	delete pttarraySLVChecksumLengthsFromSpaceMap;
			
	if( pdbutil->grbitOptions & JET_bitDBUtilOptionStats )
		{
		delete pcprintfStatsInternal;
		}

	delete pfapiSLV;

	fGlobalRepair = fGlobalRepairSave;
	return err;
	}


//  ================================================================
BOOL FIsLargeTable( const CHAR * const szTable )
//  ================================================================
	{
#ifdef DEBUG

	//  check to make sure the rgszLargeTables is in order
	
	static INT fChecked = fFalse;
	if( !fChecked )
		{
		INT iszT;
		for( iszT = 0; iszT < cszLargeTables - 1; ++iszT )
			{
			AssertSz( _stricmp( rgszLargeTables[iszT], rgszLargeTables[iszT+1] ) < 0, "rgszLargeTables is out of order" );
			}
		fChecked = fTrue;
		}
#endif	//	DEBUG

	INT isz;
	for( isz = 0; isz < cszLargeTables; ++isz )
		{
		const INT cmp = _stricmp( szTable, rgszLargeTables[isz] );
		if( 0 == cmp )
			{
			return fTrue;
			}
		if( cmp < 0 )
			{
			//  all other table names in the array will be greater than this as
			//  well. we can stop checking here
			return fFalse;
			}
		}
	return fFalse;
	}


//  ================================================================
LOCAL PGNO PgnoLast( const IFMP ifmp )
//  ================================================================
	{
	return rgfmp[ifmp].PgnoLast();
	}


//  ================================================================
LOCAL VOID REPAIRIPrereadIndexesOfFCB( const FCB * const pfcb )
//  ================================================================
	{
	const INT cSecondaryIndexesToPreread = 64;
	
	PGNO rgpgno[cSecondaryIndexesToPreread + 1];	//  NULL-terminated
	INT	ipgno = 0;
	const FCB *	pfcbT = pfcb->PfcbNextIndex();

	while( pfcbNil != pfcbT && ipgno < cSecondaryIndexesToPreread )
		{
		rgpgno[ipgno++] = pfcbT->PgnoFDP();
		pfcbT = pfcbT->PfcbNextIndex();
		}
	rgpgno[ipgno] = pgnoNull;
	
	BFPrereadPageList( pfcb->Ifmp(), rgpgno );
	}


//  ================================================================
LOCAL VOID REPAIRDumpStats(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const BTSTATS * const pbtstats,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	if( !(popts->grbit & JET_bitDBUtilOptionStats ) )
		{
		return;
		}
		
	(*popts->pcprintfStats)( "total pages: %d\r\n", pbtstats->cpageInternal+pbtstats->cpageLeaf );
	(*popts->pcprintfStats).Indent();
	(*popts->pcprintfStats)( "internal pages: %d\r\n", pbtstats->cpageInternal );
	(*popts->pcprintfStats)( "leaf pages: %d\r\n", pbtstats->cpageLeaf );
	(*popts->pcprintfStats).Unindent();
	(*popts->pcprintfStats)( "tree height: %d\r\n", pbtstats->cpageDepth );
	(*popts->pcprintfStats)( "empty pages: %d\r\n", pbtstats->cpageEmpty );

	LONG cnodeInternal 	= 0;
	LONG cnodeLeaf		= 0;

	QWORD cbKeyTotal	= 0;
	QWORD cbKeySuffixes = 0;
	QWORD cbKeyPrefixes	= 0;
	
	INT ikey;
	for( ikey = 0; ikey < JET_cbKeyMost; ikey++ )
		{
		cnodeInternal += pbtstats->rgckeyInternal[ikey];
		cnodeLeaf	  += pbtstats->rgckeyLeaf[ikey];

		cbKeyTotal	+= pbtstats->rgckeyInternal[ikey] * ikey;
		cbKeyTotal	+= pbtstats->rgckeyLeaf[ikey] * ikey;

		cbKeySuffixes	+= pbtstats->rgckeySuffixInternal[ikey] * ikey;
		cbKeySuffixes	+= pbtstats->rgckeySuffixLeaf[ikey] * ikey;

		cbKeyPrefixes	+= pbtstats->rgckeyPrefixInternal[ikey] * ikey;
		cbKeyPrefixes	+= pbtstats->rgckeyPrefixLeaf[ikey] * ikey;
		}

	const QWORD cbSavings = cbKeyTotal - ( cbKeySuffixes + cbKeyPrefixes + ( pbtstats->cnodeCompressed * 2 ) );
	
	(*popts->pcprintfStats)( "total nodes: %d\r\n", cnodeInternal+cnodeLeaf );	
	(*popts->pcprintfStats)( "compressed nodes: %d\r\n", pbtstats->cnodeCompressed );
	(*popts->pcprintfStats)( "space saved: %d\r\n", cbSavings );
	(*popts->pcprintfStats)( "internal nodes: %d\r\n", cnodeInternal );	
	(*popts->pcprintfStats)( "leaf nodes: %d\r\n", cnodeLeaf );	
	(*popts->pcprintfStats).Indent();
	(*popts->pcprintfStats)( "versioned: %d\r\n", pbtstats->cnodeVersioned );
	(*popts->pcprintfStats)( "deleted: %d\r\n", pbtstats->cnodeDeleted );
	(*popts->pcprintfStats).Unindent();
	}


//  ================================================================
LOCAL VOID REPAIRDumpHex( CHAR * const szDest, const INT cchDest, const BYTE * const pb, const INT cb )
//  ================================================================
	{
	const BYTE * const pbMax = pb + cb;
	const BYTE * pbT = pb;
	
	CHAR * sz = szDest;
	
	while( pbT < pbMax )
		{
		sz += sprintf( sz, "%2.2x", *pbT++ );
		if( pbT != pbMax )
			{
			sz += sprintf( sz, " " );
			}
		}
	}


//  ================================================================
LOCAL VOID REPAIRPrintSig( const SIGNATURE * const psig, CPRINTF * const pcprintf )
//  ================================================================
	{
	const LOGTIME tm = psig->logtimeCreate;
	(*pcprintf)( "Create time:%02d/%02d/%04d %02d:%02d:%02d Rand:%lu Computer:%s\r\n",
						(short) tm.bMonth, (short) tm.bDay,	(short) tm.bYear + 1900,
						(short) tm.bHours, (short) tm.bMinutes, (short) tm.bSeconds,
						ULONG(psig->le_ulRandom),
						psig->szComputerName );
	}


//  ================================================================
LOCAL VOID REPAIRSetCacheSize( const REPAIROPTS * const popts )
//  ================================================================
//
//  try to maximize our cache. ignore any errors
//  because of some weirdness with DBA we cap the maximum cache size at 1.5GB
//
//-
	{
	const ULONG cpgBuffersT	= (ULONG)max( 1536, ( ( OSMemoryAvailable() - ( 16 * 1024 * 1024 ) ) / ( g_cbPage + 128 ) ) );
	const ULONG	cpgBuffers	= (ULONG)min( cpgBuffersT, ( 1536 * 1024 * 1024 ) / ( g_cbPage + 128 ) );
	(*popts->pcprintfVerbose)( "trying for %d buffers\r\n", cpgBuffers );
	CallS( ErrBFSetCacheSize( cpgBuffers ) );
	}


//  ================================================================
LOCAL VOID REPAIRResetCacheSize( const REPAIROPTS * const popts )
//  ================================================================
	{
	CallS( ErrBFSetCacheSize( 0 ) );
	}


//  ================================================================
LOCAL VOID REPAIRPrintStartMessages( const CHAR * const szDatabase, const REPAIROPTS * const popts )
//  ================================================================
	{
	(*popts->pcprintfStats)( "***** %s of database '%s' started [%s version %02d.%02d.%04d.%04d, (%s)]\r\n\r\n",
				( popts->grbit & JET_bitDBUtilOptionDontRepair ) ? "Integrity check" : "Repair",
				szDatabase,
				SzUtilImageVersionName(),
				DwUtilImageVersionMajor(),
				DwUtilImageVersionMinor(),
				DwUtilImageBuildNumberMajor(),
				DwUtilImageBuildNumberMinor(),
				SzUtilImageBuildClass()
				);
				
	(*popts->pcprintfVerbose)( "***** %s of database '%s' started [%s version %02d.%02d.%04d.%04d, (%s)]\r\n\r\n",
				( popts->grbit & JET_bitDBUtilOptionDontRepair ) ? "Integrity check" : "Repair",
				szDatabase,
				SzUtilImageVersionName(),
				DwUtilImageVersionMajor(),
				DwUtilImageVersionMinor(),
				DwUtilImageBuildNumberMajor(),
				DwUtilImageBuildNumberMinor(),
				SzUtilImageBuildClass()
				);

	(*popts->pcprintfVerbose)( "search for \'ERROR:\' to find errors\r\n" );
	(*popts->pcprintfVerbose)( "search for \'WARNING:\' to find warnings\r\n" );
				
	(*popts->pcprintfVerbose).Indent();				

	(*popts->pcprintf)( "\r\nChecking database integrity.\r\n"  );
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckDatabaseSize(
	const CHAR * const szDatabase,
	const IFMP ifmp,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	QWORD cbDatabase;
	Call( rgfmp[ifmp].Pfapi()->ErrSize( &cbDatabase ) );
	if( 0 != cbDatabase % g_cbPage )
		{
		(*popts->pcprintfError)( "database file \"%s\" is has the wrong size (%I64d bytes, must be a multiple of %d bytes)\r\n",
										szDatabase, cbDatabase, g_cbPage );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	else
		{
		(*popts->pcprintfVerbose)( "database file \"%s\" is %I64d bytes\r\n",
										szDatabase, cbDatabase );
		}

HandleError:	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRFixDatabaseSize(
	const CHAR * const szDatabase,
	const IFMP ifmp,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	QWORD cbDatabase;
	Call( rgfmp[ifmp].Pfapi()->ErrSize( &cbDatabase ) );
	const CPG	cpgNew			= CPG( cbDatabase / (QWORD)g_cbPage );
	const QWORD	cbDatabaseNew	= (QWORD)g_cbPage * (QWORD)cpgNew;

	(*popts->pcprintfVerbose)( "setting database file \"%s\" size (was %I64d bytes, will be %I64d bytes -- %d pages)\r\n",
							szDatabase, cbDatabase, cbDatabaseNew, cpgNew );
	
	Call( ErrIONewSize( ifmp, cpgNew - cpgDBReserved ) );

	if( JET_errSuccess == err )
		{
		Assert( JET_errSuccess == ErrREPAIRCheckDatabaseSize( szDatabase, ifmp, popts ) );
		}

HandleError:	
	return err;
	}



#ifdef DISABLE_SLV
#else

//  ================================================================
LOCAL ERR ErrREPAIRGetStreamingFileSize(
			const CHAR * const szSLV,
			const IFMP ifmp,
			IFileAPI *const pfapiSLV,
			CPG * const pcpgSLV,
			const REPAIROPTS * const popts )
//  ================================================================
	{			
	ERR err = JET_errSuccess;
	
	QWORD 	cbSLV;
		
	Call( pfapiSLV->ErrSize( &cbSLV ) );
	*pcpgSLV = (LONG)( cbSLV >> (QWORD)g_shfCbPage ) - cpgDBReserved;

	if( cbSLV < ( SLVSPACENODE::cpageMap << g_shfCbPage ) )
		{
		(*popts->pcprintfError)( "streaming file \"%s\" is too small (%I64d bytes, must be at least %d bytes)\r\n",
										szSLV, cbSLV, SLVSPACENODE::cpageMap << g_shfCbPage );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
#ifdef NEVER	

	//	this can be the case because the file is not extended at the end of recovery
	//	instead we wait until the next attach to fix the streaming file size
	//	(this way we avoid opening the catalog during recovery)
	
	else if( 0 != ( ( cbSLV - ( (QWORD)cpgDBReserved << g_shfCbPage ) ) % ( SLVSPACENODE::cpageMap << g_shfCbPage ) ) )
		{
		(*popts->pcprintfError)( "streaming file \"%s\" is a weird size (%I64d bytes, expected a multiple of %d bytes)\r\n",
										szSLV, cbSLV, SLVSPACENODE::cpageMap << g_shfCbPage );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}	
		
#endif	//	NEVER		

	else
		{
			
		(*popts->pcprintfVerbose)( "streaming file has %d pages\r\n", *pcpgSLV );

		//  several things depend on this being set
	
		rgfmp[ifmp].SetSLVFileSize( (QWORD)*pcpgSLV << (QWORD)g_shfCbPage );
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckStreamingFileHeader(
	INST *pinst,
	const CHAR * const szDatabase,
	const CHAR * const szSLV,
	const IFMP ifmp,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	DBFILEHDR * const pdbfilehdr = (DBFILEHDR *)PvOSMemoryPageAlloc( g_cbPage, NULL );
	SLVFILEHDR * const pslvfilehdr = (SLVFILEHDR *)PvOSMemoryPageAlloc( g_cbPage, NULL );
	
	if ( NULL == pslvfilehdr
		|| NULL == pdbfilehdr )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	Call( ErrUtilReadShadowedHeader( pinst->m_pfsapi, szDatabase, (BYTE *)pdbfilehdr, g_cbPage, OffsetOf( DBFILEHDR, le_cbPageSize ) ) );
	Call( ErrUtilReadShadowedHeader( pinst->m_pfsapi, szSLV, (BYTE *)pslvfilehdr, g_cbPage, OffsetOf( SLVFILEHDR, le_cbPageSize ) ) );
	
	if ( attribSLV != pslvfilehdr->le_attrib )
		{
		(*popts->pcprintfError)( "'%s' is not a streaming file (attrib is %d, expected %d)\r\n",
									szSLV, pslvfilehdr->le_attrib, attribSLV );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	if ( ( err = ErrSLVCheckDBSLVMatch( pdbfilehdr, pslvfilehdr ) ) < 0 )
		{
		(*popts->pcprintfError)( "file mismatch between database '%s' and streaming file '%s'\r\n",
									szDatabase, szSLV );
		(*popts->pcprintfError)( "database signatures:\r\n" );
		REPAIRPrintSig( &pdbfilehdr->signDb, popts->pcprintfError );
		REPAIRPrintSig( &pdbfilehdr->signSLV, popts->pcprintfError );
		(*popts->pcprintfError)( "streaming file signatures:\r\n" );
		REPAIRPrintSig( &pslvfilehdr->signDb, popts->pcprintfError );		
		REPAIRPrintSig( &pslvfilehdr->signSLV, popts->pcprintfError );
		// UNDONE STRICT_DB_SLV_CHECK have to print lgpos and logtime
		}

HandleError:
	OSMemoryPageFree( pslvfilehdr );
	OSMemoryPageFree( pdbfilehdr );

	if( JET_errFileNotFound == err )
		{(*popts->pcprintfError)( "streaming file '%s' is missing\r\n", szSLV );
		err = JET_errSLVStreamingFileMissing;
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRStartCheckSLVTrees(
		PIB * const ppib,
		const IFMP ifmp,
		const CHAR * const szSLV,
		IFileAPI *const pfapiSLV,	
		const ULONG cpgSLV,
		TASKMGR * const ptaskmgr,
		TTARRAY * const pttarrayOwnedSpace,
		TTARRAY * const pttarrayAvailSpace,
		TTARRAY * const pttarraySLVChecksumsFromFile,
		TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,		
		BOOL * const pfDbtimeTooLarge,
		INTEGGLOBALS * const pintegglobals,		
		ERR * const perrSLVChecksum,
		const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	pintegglobals->fCorruptionSeen 				= fFalse;
	pintegglobals->err 							= JET_errSuccess;
	pintegglobals->pprepairtable 				= NULL;
	pintegglobals->pttarrayOwnedSpace 			= pttarrayOwnedSpace;
	pintegglobals->pttarrayAvailSpace 			= pttarrayAvailSpace;
	pintegglobals->pttarraySLVAvail 			= NULL;
	pintegglobals->pttarraySLVOwnerMapColumnid 	= NULL;
	pintegglobals->pttarraySLVOwnerMapKey 		= NULL;
	pintegglobals->pttarraySLVChecksumLengths	= NULL;	
	pintegglobals->pfDbtimeTooLarge				= pfDbtimeTooLarge;
	pintegglobals->popts = popts;

	CHECKTABLE 			* pchecktableSLVAvail		= NULL;
	CHECKTABLE 			* pchecktableSLVOwnerMap	= NULL;

	//  SLVAvail (async)
	
	pchecktableSLVAvail = new CHECKTABLE;
	if( NULL == pchecktableSLVAvail )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	pchecktableSLVAvail->ifmp 				= ifmp;
	strcpy(pchecktableSLVAvail->szTable, szSLVAvail );
	pchecktableSLVAvail->szIndex[0]			= 0;
	pchecktableSLVAvail->objidFDP 			= objidNil;
	pchecktableSLVAvail->pgnoFDP 			= pgnoNull;
	pchecktableSLVAvail->objidParent		= objidSystemRoot;
	pchecktableSLVAvail->pgnoFDPParent		= pgnoSystemRoot;
	pchecktableSLVAvail->fPageFlags			= 0;
	pchecktableSLVAvail->fUnique			= fTrue;
	pchecktableSLVAvail->preccheck			= NULL;
	pchecktableSLVAvail->cpgPrimaryExtent	= 0;
	pchecktableSLVAvail->pglobals			= pintegglobals;
	pchecktableSLVAvail->fDeleteWhenDone	= fTrue;

	err = ptaskmgr->ErrPostTask( REPAIRCheckSLVAvailTreeTask, (ULONG_PTR)pchecktableSLVAvail );
	if( err < 0 )
		{
		delete pchecktableSLVAvail;
		Call( err );
		}

	//	SLVOwnerMap (sync)
	
	pchecktableSLVOwnerMap = new CHECKTABLE;
	if( NULL == pchecktableSLVOwnerMap )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	pchecktableSLVOwnerMap->ifmp 				= ifmp;
	strcpy(pchecktableSLVOwnerMap->szTable, szSLVOwnerMap );
	pchecktableSLVOwnerMap->szIndex[0]			= 0;
	pchecktableSLVOwnerMap->objidFDP 			= objidNil;
	pchecktableSLVOwnerMap->pgnoFDP 			= pgnoNull;
	pchecktableSLVOwnerMap->objidParent			= objidSystemRoot;
	pchecktableSLVOwnerMap->pgnoFDPParent		= pgnoSystemRoot;
	pchecktableSLVOwnerMap->fPageFlags			= 0;
	pchecktableSLVOwnerMap->fUnique				= fTrue;
	pchecktableSLVOwnerMap->preccheck			= NULL;
	pchecktableSLVOwnerMap->cpgPrimaryExtent	= 0;
	pchecktableSLVOwnerMap->pglobals			= pintegglobals;
	pchecktableSLVOwnerMap->fDeleteWhenDone		= fTrue;

	REPAIRCheckSLVOwnerMapTreeTask( ppib, (ULONG_PTR)pchecktableSLVOwnerMap );

	//	Checksum the streaming file (async)

	Call( ErrREPAIRChecksumSLV(
			ifmp,
			pfapiSLV,
			szSLV,
			cpgSLV,
			ptaskmgr,
			perrSLVChecksum,
			pttarraySLVChecksumsFromFile,
			pttarraySLVChecksumLengthsFromSpaceMap,				
			popts ) );	
	
HandleError:
	Ptls()->szCprintfPrefix = NULL;
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRStopCheckSLVTrees( const INTEGGLOBALS * const pintegglobals, BOOL * const pfCorrupt )
//  ================================================================
	{
	*pfCorrupt = pintegglobals->fCorruptionSeen;
	return pintegglobals->err;
	}


//  ================================================================
LOCAL VOID REPAIRCheckSLVAvailTreeTask( PIB * const ppib, const ULONG_PTR ul )
//  ================================================================
	{
	TASKMGR::TASK task = REPAIRCheckSLVAvailTreeTask;	// should only compile if the signatures match

	CHECKTABLE * const pchecktable = (CHECKTABLE *)ul;

	CallS( ErrDIRBeginTransaction(ppib, NO_GRBIT ) );
	
	Ptls()->szCprintfPrefix = pchecktable->szTable;

	ERR err = ErrREPAIRCheckSLVAvailTree(
				ppib,
				pchecktable->ifmp,
				pchecktable->pglobals->pttarrayOwnedSpace,
				pchecktable->pglobals->pttarrayAvailSpace,
				pchecktable->pglobals->pfDbtimeTooLarge,
				pchecktable->pglobals->popts );

	switch( err )
		{
		
		//  we should never normally get these errors. morph them into corrupted database errors
		
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVSpaceCorrupted:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}

	if( JET_errDatabaseCorrupted == err )
		{
		
		//  we just need to set this, it will never be unset
		
		pchecktable->pglobals->fCorruptionSeen = fTrue;
		
		}
	else if( err < 0 )
		{
		
		//  we'll just keep the last non-corrupting error
		
		pchecktable->pglobals->err = err;
		
		}

	Ptls()->szCprintfPrefix = "NULL";

	if( pchecktable->fDeleteWhenDone )
		{
		delete pchecktable;
		}
	else
		{
		pchecktable->signal.Set();
		}

	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	}


//  ================================================================
LOCAL VOID REPAIRCheckSLVOwnerMapTreeTask( PIB * const ppib, const ULONG_PTR ul )
//  ================================================================
	{
	TASKMGR::TASK task = REPAIRCheckSLVOwnerMapTreeTask;	// should only compile if the signatures match

	CHECKTABLE * const pchecktable = (CHECKTABLE *)ul;

	CallS( ErrDIRBeginTransaction(ppib, NO_GRBIT ) );
	
	Ptls()->szCprintfPrefix = pchecktable->szTable;

	ERR err = ErrREPAIRCheckSLVOwnerMapTree(
				ppib,
				pchecktable->ifmp,
				pchecktable->pglobals->pttarrayOwnedSpace,
				pchecktable->pglobals->pttarrayAvailSpace,
				pchecktable->pglobals->pfDbtimeTooLarge,
				pchecktable->pglobals->popts );

	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVSpaceCorrupted:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}

	if( JET_errDatabaseCorrupted == err )
		{
		
		//  we just need to set this, it will never be unset
		
		pchecktable->pglobals->fCorruptionSeen = fTrue;
		
		}
	else if( err < 0 )
		{
		
		//  we'll just keep the last non-corrupting error
		
		pchecktable->pglobals->err = err;		
		}

	Ptls()->szCprintfPrefix = "NULL";

	if( pchecktable->fDeleteWhenDone )
		{
		delete pchecktable;
		}
	else
		{
		pchecktable->signal.Set();
		}

	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );	
	}


//  ================================================================
LOCAL BOOL FREPAIRRepairtableHasSLV( const REPAIRTABLE * const prepairtable )
//  ================================================================
	{
	const REPAIRTABLE * prepairtableT = prepairtable;
	while( prepairtableT )
		{
		if( prepairtableT->fTableHasSLV )
			{
			return fTrue;
			}
		prepairtableT = prepairtableT->prepairtableNext;
		}
	return fFalse;
	}

#endif	//	DISABLE_SLV

//  ================================================================
LOCAL VOID REPAIRFreeRepairtables( REPAIRTABLE * const prepairtable )
//  ================================================================
	{
	REPAIRTABLE * prepairtableT = prepairtable;
	
	while( prepairtableT )
		{
		REPAIRTABLE * const prepairtableNext = prepairtableT->prepairtableNext;
		prepairtableT->~REPAIRTABLE();
		OSMemoryHeapFree( prepairtableT );
		prepairtableT = prepairtableNext;
		}
	}


//  ================================================================
LOCAL VOID REPAIRPrintEndMessages(
			const CHAR * const szDatabase,
			const ULONG timerStart,
			const REPAIROPTS * const popts )
//  ================================================================
	{
	const ULONG timerEnd = TickOSTimeCurrent();
	(*popts->pcprintfStats)( "\r\n\r\n" );
	(*popts->pcprintfStats)( "***** Eseutil completed in %d seconds\r\n\r\n",
				( ( timerEnd - timerStart ) / 1000 ) );
	(*popts->pcprintfVerbose)( "\r\n\r\n" );
	(*popts->pcprintfVerbose)( "***** Eseutil completed in %d seconds\r\n\r\n",
				( ( timerEnd - timerStart ) / 1000 ) );
	}


//  ================================================================
LOCAL VOID INTEGRITYPrintEndErrorMessages(
	const LOGTIME logtimeLastFullBackup,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	(*popts->pcprintf)( "\r\nIntegrity check completed.  " );
	if( 0 == logtimeLastFullBackup.bYear && 0 == logtimeLastFullBackup.bMonth && 
		0 == logtimeLastFullBackup.bDay && 0 == logtimeLastFullBackup.bHours && 
		0 == logtimeLastFullBackup.bMinutes && 0 == logtimeLastFullBackup.bSeconds )
		{
		(*popts->pcprintf)( "Database is CORRUPTED!\r\n" );
		}
	else
		{
		(*popts->pcprintfVerbose)( "\r\nThe last full backup of this database was on %02d/%02d/%04d %02d:%02d:%02d\r\n",
						(short) logtimeLastFullBackup.bMonth, 
						(short) logtimeLastFullBackup.bDay,	
						(short) logtimeLastFullBackup.bYear + 1900, 
						(short) logtimeLastFullBackup.bHours, 
						(short) logtimeLastFullBackup.bMinutes, 
						(short) logtimeLastFullBackup.bSeconds );
		(*popts->pcprintf)( "\r\nDatabase is CORRUPTED, the last full backup of this database was on %02d/%02d/%04d %02d:%02d:%02d\r\n",
						(short) logtimeLastFullBackup.bMonth, 
						(short) logtimeLastFullBackup.bDay,	
						(short) logtimeLastFullBackup.bYear + 1900, 
						(short) logtimeLastFullBackup.bHours, 
						(short) logtimeLastFullBackup.bMinutes, 
						(short) logtimeLastFullBackup.bSeconds );
		}
	}


//  ================================================================
LOCAL JET_ERR __stdcall ErrREPAIRNullStatusFN( JET_SESID, JET_SNP, JET_SNT, void * )
//  ================================================================
	{
	JET_PFNSTATUS pfnStatus = ErrREPAIRNullStatusFN;
	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckHeader(
	INST * const pinst,
	const char * const szDatabase,
	const char * const szStreamingFile,
	LOGTIME * const plogtimeLastFullBackup,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Force the database to a consistent state and change the signature
//  so that we will not be able to use the logs against the database
//  again
//
//-
	{
	ERR err = JET_errSuccess;
	DBFILEHDR_FIX * const pdfh = reinterpret_cast<DBFILEHDR_FIX * >( PvOSMemoryPageAlloc( g_cbPage, NULL ) );
	if ( NULL == pdfh )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	(*popts->pcprintfVerbose)( "checking database header\r\n" );
	(*popts->pcprintfVerbose).Unindent();

	err = ( ( popts->grbit & JET_bitDBUtilOptionDontRepair ) ?
			ErrUtilReadShadowedHeader : ErrUtilReadAndFixShadowedHeader )
				( pinst->m_pfsapi,
				const_cast<CHAR *>( szDatabase ),
				reinterpret_cast<BYTE*>( pdfh ),
				g_cbPage,
				OffsetOf( DBFILEHDR_FIX, le_cbPageSize ),
				NULL );
		
	if ( err < 0 )
		{
		if ( JET_errDiskIO == err )
			{
			(*popts->pcprintfError)( "unable to read database header\r\n" );
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			}
		goto HandleError;
		}

	if( pdfh->m_ulDbFlags & fDbSLVExists )
		{
		if( NULL == szStreamingFile )
			{
			(*popts->pcprintfError)( "streaming file not specified, but fDBSLVExists set in database header\r\n" );
			err = ErrERRCheck( JET_errSLVStreamingFileMissing );
			goto HandleError;
			}
		}
	else
		{
		if( NULL != szStreamingFile )
			{
			(*popts->pcprintfError)( "streaming file specified, but fDBSLVExists not set in database header\r\n" );
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			goto HandleError;
			}
		}
		
	if( JET_dbstateCleanShutdown != pdfh->le_dbstate )
		{
		const CHAR * szState;
		switch( pdfh->le_dbstate )
			{
			case JET_dbstateDirtyShutdown:
				szState = "dirty shutdown";
				break;
			case JET_dbstateForceDetach:
				szState = "force detech";
				break;
			case JET_dbstateJustCreated:
				szState = "just created";
				err = ErrERRCheck( JET_errDatabaseCorrupted );
				break;
			case JET_dbstateBeingConverted:
				szState = "being converted";
				err = ErrERRCheck( JET_errDatabaseCorrupted );
				break;
			default:
				szState = "unknown";
				err = ErrERRCheck( JET_errDatabaseCorrupted );
				break;
			}
		(*popts->pcprintfError)( "database was not shutdown cleanly (%s)\r\n", szState );

		if( JET_dbstateDirtyShutdown == pdfh->le_dbstate 	
			|| JET_dbstateForceDetach == pdfh->le_dbstate )
			{
			(*popts->pcprintf)( "\r\nThe database is not up-to-date. This operation may find that\n");
			(*popts->pcprintf)( "this database is corrupt because data from the log files has\n");
			(*popts->pcprintf)( "yet to be placed in the database.\r\n\n" ); 
			(*popts->pcprintf)( "To ensure the database is up-to-date please use the 'Recovery' operation.\r\n\n" );
			}			
		}


	// Get the last full backup info
	if ( NULL != plogtimeLastFullBackup )
		{
		*plogtimeLastFullBackup = pdfh->bkinfoFullPrev.logtimeMark;
		}

HandleError:
	(*popts->pcprintfVerbose).Unindent();
	OSMemoryPageFree( pdfh );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSystemTables(
	PIB * const ppib,
	const IFMP ifmp,
	TASKMGR * const ptaskmgr,
	BOOL * const pfCatalogCorrupt,
	BOOL * const pfShadowCatalogCorrupt,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	FUCB * pfucbCatalog = pfucbNil;

	const FIDLASTINTDB fidLastInTDB = { fidMSO_FixedLast, fidMSO_VarLast, fidMSO_TaggedLast };

	(*popts->pcprintfVerbose)( "checking system tables\r\n" );
	(*popts->pcprintfVerbose).Indent();
	
	RECCHECKNULL 	recchecknull;
	RECCHECKTABLE 	recchecktable( objidNil, pfucbNil, fidLastInTDB, NULL, NULL, NULL, NULL, NULL, popts );
	
	*pfCatalogCorrupt 		= fTrue;
	*pfShadowCatalogCorrupt = fTrue;

	INTEGGLOBALS	integglobalsCatalog;
	INTEGGLOBALS	integglobalsShadowCatalog;

	integglobalsCatalog.fCorruptionSeen 			= integglobalsShadowCatalog.fCorruptionSeen 			= fFalse;
	integglobalsCatalog.err 						= integglobalsShadowCatalog.err 						= JET_errSuccess;
	integglobalsCatalog.pprepairtable 				= integglobalsShadowCatalog.pprepairtable 				= NULL;
	integglobalsCatalog.pttarrayOwnedSpace 			= integglobalsShadowCatalog.pttarrayOwnedSpace 			= pttarrayOwnedSpace;
	integglobalsCatalog.pttarrayAvailSpace 			= integglobalsShadowCatalog.pttarrayAvailSpace 			= pttarrayAvailSpace;
	integglobalsCatalog.pttarraySLVAvail 			= integglobalsShadowCatalog.pttarraySLVAvail 			= NULL;
	integglobalsCatalog.pttarraySLVOwnerMapColumnid = integglobalsShadowCatalog.pttarraySLVOwnerMapColumnid	= NULL;
	integglobalsCatalog.pttarraySLVOwnerMapKey 		= integglobalsShadowCatalog.pttarraySLVOwnerMapKey 		= NULL;
	integglobalsCatalog.pttarraySLVChecksumLengths	= integglobalsShadowCatalog.pttarraySLVChecksumLengths	= NULL;
	integglobalsCatalog.pfDbtimeTooLarge 			= integglobalsShadowCatalog.pfDbtimeTooLarge			= pfDbtimeTooLarge;
	integglobalsCatalog.popts 						= integglobalsShadowCatalog.popts 						= popts;

	CHECKTABLE checktableMSO;
	CHECKTABLE checktableMSOShadow;
	CHECKTABLE checktableMSO_NameIndex;
	CHECKTABLE checktableMSO_RootObjectIndex;	

	//  MSO
	
	checktableMSO.ifmp 							= ifmp;
	strcpy(checktableMSO.szTable, szMSO );
	checktableMSO.szIndex[0]					= 0;
	checktableMSO.objidFDP	 					= objidFDPMSO;
	checktableMSO.pgnoFDP 						= pgnoFDPMSO;
	checktableMSO.objidParent					= objidSystemRoot;
	checktableMSO.pgnoFDPParent					= pgnoSystemRoot;
	checktableMSO.fPageFlags					= CPAGE::fPagePrimary;
	checktableMSO.fUnique						= fTrue;
	checktableMSO.preccheck						= &recchecktable;
	checktableMSO.cpgPrimaryExtent				= 16;
	checktableMSO.pglobals						= &integglobalsCatalog;
	checktableMSO.fDeleteWhenDone				= fFalse;

	Call( ptaskmgr->ErrPostTask( REPAIRCheckTreeAndSpaceTask, (ULONG_PTR)&checktableMSO ) );
	(*popts->pcprintfVerbose)( "%s %s\r\n", checktableMSO.szTable, checktableMSO.szIndex  );

	//	MSOShadow
	
	checktableMSOShadow.ifmp 					= ifmp;
	strcpy(checktableMSOShadow.szTable, szMSOShadow );
	checktableMSOShadow.szIndex[0]				= 0;
	checktableMSOShadow.objidFDP 				= objidFDPMSOShadow;
	checktableMSOShadow.pgnoFDP 				= pgnoFDPMSOShadow;
	checktableMSOShadow.objidParent				= objidSystemRoot;
	checktableMSOShadow.pgnoFDPParent			= pgnoSystemRoot;
	checktableMSOShadow.fPageFlags				= CPAGE::fPagePrimary;
	checktableMSOShadow.fUnique					= fTrue;
	checktableMSOShadow.preccheck				= &recchecktable;
	checktableMSOShadow.cpgPrimaryExtent		= 16;
	checktableMSOShadow.pglobals				= &integglobalsShadowCatalog;
	checktableMSOShadow.fDeleteWhenDone			= fFalse;

	Call( ptaskmgr->ErrPostTask( REPAIRCheckTreeAndSpaceTask, (ULONG_PTR)&checktableMSOShadow ) );
	(*popts->pcprintfVerbose)( "%s %s\r\n", checktableMSOShadow.szTable, checktableMSOShadow.szIndex  );

	//	wait for the check of MSO to finish so the space tables are up to date

	checktableMSO.signal.Wait();
	Call( integglobalsCatalog.err );

	if( !integglobalsCatalog.fCorruptionSeen )
		{

		//	MSO ==> MSO_NameIndex
		
		checktableMSO_NameIndex.ifmp 				= ifmp;
		strcpy(checktableMSO_NameIndex.szTable, szMSO );
		strcpy(checktableMSO_NameIndex.szIndex, szMSONameIndex );
		checktableMSO_NameIndex.objidFDP 			= objidFDPMSO_NameIndex;
		checktableMSO_NameIndex.pgnoFDP 			= pgnoFDPMSO_NameIndex;
		checktableMSO_NameIndex.objidParent			= objidFDPMSO;
		checktableMSO_NameIndex.pgnoFDPParent		= pgnoFDPMSO;
		checktableMSO_NameIndex.fPageFlags			= CPAGE::fPageIndex;
		checktableMSO_NameIndex.fUnique				= fTrue;
		checktableMSO_NameIndex.preccheck			= &recchecknull;
		checktableMSO_NameIndex.cpgPrimaryExtent	= 16;
		checktableMSO_NameIndex.pglobals			= &integglobalsCatalog;
		checktableMSO_NameIndex.fDeleteWhenDone		= fFalse;

		Call( ptaskmgr->ErrPostTask( REPAIRCheckTreeAndSpaceTask, (ULONG_PTR)&checktableMSO_NameIndex ) );
		(*popts->pcprintfVerbose)( "%s %s\r\n", checktableMSO_NameIndex.szTable, checktableMSO_NameIndex.szIndex  );

		//	MSO ==> MSO_RootObjectIndex

		checktableMSO_RootObjectIndex.ifmp 				= ifmp;
		strcpy(checktableMSO_RootObjectIndex.szTable, szMSO );
		strcpy(checktableMSO_RootObjectIndex.szIndex, szMSORootObjectsIndex );
		checktableMSO_RootObjectIndex.objidFDP 			= objidFDPMSO_RootObjectIndex;
		checktableMSO_RootObjectIndex.pgnoFDP 			= pgnoFDPMSO_RootObjectIndex;
		checktableMSO_RootObjectIndex.objidParent		= objidFDPMSO;
		checktableMSO_RootObjectIndex.pgnoFDPParent		= pgnoFDPMSO;
		checktableMSO_RootObjectIndex.fPageFlags		= CPAGE::fPageIndex;
		checktableMSO_RootObjectIndex.fUnique			= fTrue;
		checktableMSO_RootObjectIndex.preccheck			= &recchecknull;
		checktableMSO_RootObjectIndex.cpgPrimaryExtent	= 16;
		checktableMSO_RootObjectIndex.pglobals			= &integglobalsCatalog;
		checktableMSO_RootObjectIndex.fDeleteWhenDone	= fFalse;

		Call( ptaskmgr->ErrPostTask( REPAIRCheckTreeAndSpaceTask, (ULONG_PTR)&checktableMSO_RootObjectIndex ) );
		(*popts->pcprintfVerbose)( "%s %s\r\n", checktableMSO_RootObjectIndex.szTable, checktableMSO_RootObjectIndex.szIndex  );

		//	wait for the index checks

		checktableMSO_NameIndex.signal.Wait();
		checktableMSO_RootObjectIndex.signal.Wait();
		
		Call( integglobalsCatalog.err );
		}

	//	wait for the shadow catalog 
	
	checktableMSOShadow.signal.Wait();
	Call( integglobalsShadowCatalog.err );

	//	were there any corruptions?

	*pfShadowCatalogCorrupt = integglobalsShadowCatalog.fCorruptionSeen;

	//  rebuild the indexes of the catalog

	if( !integglobalsCatalog.fCorruptionSeen )
		{
		(*popts->pcprintfVerbose)( "rebuilding and comparing indexes\r\n" );
		Call( ErrCATOpen( ppib, ifmp, &pfucbCatalog, fFalse ) );
		Assert( pfucbCatalog->u.pfcb->PfcbNextIndex() != pfcbNil );	
		DIRUp( pfucbCatalog );

#ifdef PARALLEL_BATCH_INDEX_BUILD
		ULONG	cIndexes	= 0;
		for ( FCB * pfcbT = pfucbCatalog->u.pfcb->PfcbNextIndex(); pfcbNil != pfcbT; pfcbT = pfcbT->PfcbNextIndex() )
			{
			cIndexes++;
			}
#else
		const ULONG	cIndexes	= cFILEIndexBatchSizeDefault;
#endif

		err = ErrFILEBuildAllIndexes(
				ppib,
				pfucbCatalog,
				pfucbCatalog->u.pfcb->PfcbNextIndex(),
				NULL,
				cIndexes,
				fTrue,
				popts->pcprintfError );

		if( JET_errSuccess != err )
			{
			*pfCatalogCorrupt = fTrue;
			err = JET_errSuccess;
			goto HandleError;
			}

		//	if we made it this far, the catalog must be fine
		
		*pfCatalogCorrupt 		= fFalse;
		}


HandleError:
	(*popts->pcprintfVerbose).Unindent();

	if( pfucbNil != pfucbCatalog )
		{
		DIRClose( pfucbCatalog );
		}

	return err;
	}



//  ================================================================
LOCAL ERR ErrREPAIRRetrieveCatalogColumns(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * pfucbCatalog, 
	ENTRYINFO * const pentryinfo,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	const INT	cColumnsToRetrieve 		= 9;
	JET_RETRIEVECOLUMN	rgretrievecolumn[ cColumnsToRetrieve ];
	ERR					err				= JET_errSuccess;
	INT					iretrievecolumn	= 0;

	memset( rgretrievecolumn, 0, sizeof( rgretrievecolumn ) );

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_ObjidTable;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->objidTable );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->objidTable );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_Type;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->objType );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->objType );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_Id;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->objidFDP );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->objidFDP );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_Name;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)( pentryinfo->szName );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->szName );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_PgnoFDP;  
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->pgnoFDPORColType );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->pgnoFDPORColType );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	


	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_Flags;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->dwFlags );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->dwFlags );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_TemplateTable;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)( pentryinfo->szTemplateTblORCallback );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->szTemplateTblORCallback );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;	

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_RecordOffset;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)&( pentryinfo->ibRecordOffset );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->ibRecordOffset );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;		

	rgretrievecolumn[iretrievecolumn].columnid 		= fidMSO_KeyFldIDs;
	rgretrievecolumn[iretrievecolumn].pvData 		= (BYTE *)( pentryinfo->rgbIdxseg );
	rgretrievecolumn[iretrievecolumn].cbData		= sizeof( pentryinfo->rgbIdxseg );
	rgretrievecolumn[iretrievecolumn].itagSequence	= 1;
	++iretrievecolumn;		

	Call( ErrIsamRetrieveColumns(
				(JET_SESID)ppib,
				(JET_TABLEID)pfucbCatalog,
				rgretrievecolumn,
				iretrievecolumn ) );
				
HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertIntoTemplateInfoList(
	TEMPLATEINFOLIST ** ppTemplateInfoList, 
	const CHAR * szTemplateTable,
	INFOLIST * const pInfoList,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR						err				= JET_errSuccess;
	TEMPLATEINFOLIST 	* 	pTemplateInfo;
	TEMPLATEINFOLIST 	*	pTemp 			= *ppTemplateInfoList;

	Alloc( pTemplateInfo = new TEMPLATEINFOLIST );

	memset( pTemplateInfo, 0, sizeof(TEMPLATEINFOLIST) );
	strcpy( pTemplateInfo->szTemplateTableName, szTemplateTable );
	pTemplateInfo->pColInfoList = pInfoList;
	pTemplateInfo->pTemplateInfoListNext = NULL;

	if (NULL == pTemp ) // empty list
		{
		*ppTemplateInfoList = pTemplateInfo;
		}
	else 
		{
		while ( pTemp->pTemplateInfoListNext )
			{		
			pTemp = pTemp->pTemplateInfoListNext;
			} 
		pTemp->pTemplateInfoListNext = pTemplateInfo;
		}	

HandleError:
	return err;			
	}


//  ================================================================
LOCAL VOID REPAIRUtilCleanInfoList( INFOLIST **ppInfo )
//  ================================================================
	{
	INFOLIST *pTemp = *ppInfo;

	while ( pTemp )
		{
		pTemp = pTemp->pInfoListNext;
		delete *ppInfo;
		*ppInfo = pTemp;
		}
	}


//  ================================================================
LOCAL VOID REPAIRUtilCleanTemplateInfoList( TEMPLATEINFOLIST **ppTemplateInfoList )
//  ================================================================
	{
	TEMPLATEINFOLIST *pTemp = *ppTemplateInfoList;
	
	while ( pTemp )
		{
		pTemp = pTemp->pTemplateInfoListNext;
		REPAIRUtilCleanInfoList( &((*ppTemplateInfoList)->pColInfoList) );
		delete *ppTemplateInfoList;
		*ppTemplateInfoList = pTemp;
		}	
	}


//  ================================================================
LOCAL VOID REPAIRCheckIndexColumnsOldFormat(
	const OBJID objidTable,
	const ENTRYINFO entryinfo, 
	const INFOLIST * const pColInfoList,
	const INFOLIST * const pTemplateColInfo,
	BOOL * const pfCorrupted,
	const REPAIROPTS * const popts )
//  ================================================================
//
//	Without the TDB we can't determine if a column belongs to a 
//	template table or not. We'll look in both places and assume
//	the one we find is correct
//
//-
	{
	*pfCorrupted = fFalse;
	
	IDXSEG_OLD			rgidxseg[JET_ccolKeyMost];
	
	memset( rgidxseg, 0, sizeof( rgidxseg ) );
	memcpy( rgidxseg, entryinfo.rgbIdxseg, min( sizeof( entryinfo.rgbIdxseg ), sizeof( rgidxseg ) ) );

	const INT cidxseg = sizeof( entryinfo.rgbIdxseg ) / sizeof( IDXSEG_OLD );
	
	for( INT iidxseg = 0; iidxseg < JET_ccolKeyMost; iidxseg++ )
		{
		if( 0 == rgidxseg[iidxseg] )
			{
			break;
			}

		const FID fid = ( rgidxseg[iidxseg] < 0 )  ? FID( -rgidxseg[iidxseg] ) : rgidxseg[iidxseg];
		const INFOLIST	* pinfolistT = NULL;	
		
		pinfolistT = pTemplateColInfo;
		while( pinfolistT )
			{
			if( fid == pinfolistT->info.objidFDP )
				{
				break;
				}
			pinfolistT = pinfolistT->pInfoListNext;
			}

		if( !pinfolistT || JET_coltypNil == pinfolistT->info.pgnoFDPORColType )
			{
			pinfolistT = pColInfoList;	
			while( pinfolistT )
				{
				if( fid == pinfolistT->info.objidFDP )
					{
					break;
					}
				pinfolistT = pinfolistT->pInfoListNext;
				}
			}
				
		if ( !pinfolistT )	// not find any table column that matches this column in an index
			{
			(*popts->pcprintfError)( "Column %d from index %s (%d) does not exist in the table (%d) \r\n",
									fid, entryinfo.szName, entryinfo.objidFDP, objidTable );
			*pfCorrupted = fTrue;
			}
		else  if ( JET_coltypNil == pinfolistT->info.pgnoFDPORColType )
			{
			(*popts->pcprintfError)( "Column %d for index %s (%d) has been deleted from table (%d) \r\n",
									fid, entryinfo.szName, entryinfo.objidFDP, objidTable );
			*pfCorrupted = fTrue;
			}
		}
	}


//  ================================================================
LOCAL VOID REPAIRCheckIndexColumnsExtendedFormat(
	const OBJID objidTable,
	const ENTRYINFO entryinfo, 
	const INFOLIST * const pColInfoList,
	const INFOLIST * const pTemplateColInfo,
	BOOL * const pfCorrupted,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	*pfCorrupted = fFalse;
	
	IDXSEG			rgidxseg[JET_ccolKeyMost];
	
	memset( rgidxseg, 0, sizeof( rgidxseg ) );
	memcpy( rgidxseg, entryinfo.rgbIdxseg, sizeof( entryinfo.rgbIdxseg ) );

	const INT cidxseg = sizeof( entryinfo.rgbIdxseg ) / sizeof( IDXSEG );
	
	for( INT iidxseg = 0; iidxseg < cidxseg; iidxseg++ )
		{
		if( 0 == rgidxseg[iidxseg].Fid() )
			{
			break;
			}
		
		const INFOLIST	* pinfolistT = NULL;	
		//	if this is a template table, pTemplateColInfo will be NULL
		//	we should check against our own columns
		if( !rgidxseg[iidxseg].FTemplateColumn() || !pTemplateColInfo )
			{
			pinfolistT = pColInfoList;	
			}
		else
			{
			pinfolistT = pTemplateColInfo;
			}
		
		while( pinfolistT )
			{
			if( rgidxseg[iidxseg].Fid() == pinfolistT->info.objidFDP )
				{
				break;
				}
			pinfolistT = pinfolistT->pInfoListNext;
			}
				
		if ( !pinfolistT )	// not find any table column that matches this column in an index
			{
			(*popts->pcprintfError)( "Column %d (FID = %d) from index %s does not exist in the table (%d) \r\n",
									rgidxseg[iidxseg].Columnid(), rgidxseg[iidxseg].Fid(), entryinfo.szName, objidTable );
			*pfCorrupted = fTrue;
			}
		else  if ( JET_coltypNil == pinfolistT->info.pgnoFDPORColType )
			{
			(*popts->pcprintfError)( "Column %d (FID = %d) for index %s has been deleted from table (%d) \r\n",
									rgidxseg[iidxseg].Columnid(), rgidxseg[iidxseg].Fid(), entryinfo.szName, objidTable );
			*pfCorrupted = fTrue;
			}
		}
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckOneIndexLogical(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucbCatalog,
	const ENTRYINFO entryinfo, 
	const ULONG objidTable,
	const ULONG pgnoFDPTable,
	const ULONG objidLV,
	const ULONG pgnoFDPLV,
	const INFOLIST * pColInfoList,
	const TEMPLATEINFOLIST * pTemplateInfoList, 
	const BOOL fDerivedTable,
	const CHAR * pszTemplateTableName,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err					= JET_errSuccess;
	ULONG 			fPrimaryIndex 		= fidbPrimary & entryinfo.dwFlags;
	BOOL			fCorrupted			= fFalse;

	INFOLIST		*	pTemplateColInfo 	= NULL;
	const INFOLIST	* 	pTemp 				= NULL;
	
	// check index entry itself
	
	if( objidTable == entryinfo.objidFDP && !fPrimaryIndex )
		{
		(*popts->pcprintfError)( "objidFDP (%d) in secondary index %s is the same as tableid (%d)\r\n", 
									entryinfo.objidFDP, entryinfo.szName, objidTable );
		fCorrupted = fTrue;
		}

	if( fPrimaryIndex && objidTable != entryinfo.objidFDP )
		{
		(*popts->pcprintfError)( "objidFDP (%d) in primary index %s is not the same as tableid (%d)\r\n", 
									entryinfo.objidFDP, entryinfo.szName, objidTable );
		fCorrupted = fTrue;
		}

	if( pgnoFDPTable == entryinfo.pgnoFDPORColType && !fPrimaryIndex )
		{
		(*popts->pcprintfError)( "pgnoFDP (%d) in secondary index %s is the same as pgnoFDP table (%d)\r\n", 
									entryinfo.pgnoFDPORColType, entryinfo.szName, pgnoFDPTable );
		fCorrupted = fTrue;
		}

	if( fPrimaryIndex && pgnoFDPTable != entryinfo.pgnoFDPORColType )
		{
		(*popts->pcprintfWarning)( "pgnoFDP (%d) in primary index %s is not the same as pgnoFDP table (%d)\r\n", 
									entryinfo.pgnoFDPORColType, entryinfo.szName, pgnoFDPTable );
		}
				
	if( objidLV == entryinfo.objidFDP )
		{
		(*popts->pcprintfError)( "objidFDP (%d) of index %s is the same as objid LV (%d)\r\n", 
									entryinfo.objidFDP, entryinfo.szName, objidLV );
		fCorrupted = fTrue;
		}

	if( pgnoFDPLV == entryinfo.pgnoFDPORColType )
		{
		(*popts->pcprintfError)( "pgnoFDP (%d) of index %s the same as pgnoFDP LV (%d)\r\n", 
									entryinfo.pgnoFDPORColType, entryinfo.szName, pgnoFDPLV );
		fCorrupted = fTrue;
		}

	// Check if a column in an index is a table column 
	
	if( fDerivedTable )
		{
		while ( pTemplateInfoList )
			{
			if ( !strcmp(pszTemplateTableName, pTemplateInfoList->szTemplateTableName) )
				{
				pTemplateColInfo = pTemplateInfoList->pColInfoList;
				break; 
				}
			else 
				{
				pTemplateInfoList = pTemplateInfoList->pTemplateInfoListNext;
				}
			}

		if( NULL == pTemplateInfoList )
			{
			(*popts->pcprintfError)( "couldn't find the template table for index %s\r\n", 
									entryinfo.szName );
			fCorrupted = fTrue;
			}
		}

	LE_IDXFLAG * const ple_idxflag = (LE_IDXFLAG*)(&entryinfo.dwFlags);
	const IDXFLAG idxflag 		= ple_idxflag->fIDXFlags;
	
	if( !( FIDXExtendedColumns( idxflag ) ) )
		{
		REPAIRCheckIndexColumnsOldFormat(
			objidTable,
			entryinfo,
			pColInfoList,
			pTemplateColInfo,
			&fCorrupted,
			popts );
		}
	else
		{
		REPAIRCheckIndexColumnsExtendedFormat(
			objidTable,
			entryinfo,
			pColInfoList,
			pTemplateColInfo,
			&fCorrupted,
			popts );
		}
	
	if( fCorrupted )
		{
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
	return err;
	}


//  ================================================================ 
ERR ErrREPAIRCheckOneTableLogical(
	INFOLIST **ppInfo, 
	const ENTRYINFO entryinfo, 
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err			= JET_errSuccess;
	INFOLIST 	* 	pInfo;
	INFOLIST 	*	pTemp 		= *ppInfo;
	BOOL			fAddedIntoList 	= fFalse;

	Alloc( pInfo = new INFOLIST );

	memset( pInfo, 0, sizeof(INFOLIST) );
	pInfo->info = entryinfo;
	pInfo->pInfoListNext = NULL;

	if (NULL == pTemp ) // empty list
		{
		*ppInfo = pInfo;
		fAddedIntoList = fTrue;
		}
	else 
		{
		// at least one element in the linked list
		do
			{
			// Check uniqueness of name
			if( !strcmp( pTemp->info.szName, pInfo->info.szName ) )
				{
				(*popts->pcprintfError)( "Dulicated name (%s) in a table\t\n", pInfo->info.szName );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}

			// Check uniqueness of pgnoFDP for Indexes
			if( sysobjIndex == pInfo->info.objType  && 
				pTemp->info.pgnoFDPORColType == pInfo->info.pgnoFDPORColType )
				{
				(*popts->pcprintfError)( "Dulicated pgnoFDP (%d) in a table\t\n", pInfo->info.pgnoFDPORColType );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}

			// Check uniqueness of objid
			if ( pTemp->info.objidFDP == pInfo->info.objidFDP )
				{
				(*popts->pcprintfError)( "Dulicated objid (%d) in a table\t\n", pInfo->info.objidFDP );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			Assert( pTemp->info.objidFDP < pInfo->info.objidFDP );

			if( NULL == pTemp->pInfoListNext ) 
				{
				pTemp->pInfoListNext = pInfo; // always inserted into the end of the list
				fAddedIntoList = fTrue;
				break;
				}
			else
				{
				pTemp = pTemp->pInfoListNext;
				}
			}
		while( pTemp );
		}	

HandleError:
	if ( !fAddedIntoList )
		{
		delete pInfo;
		}

	return err;	
	}


//  ================================================================
LOCAL ERR ErrREPAIRICheckCatalogEntryPgnoFDPs(
	PIB * const ppib,
	const IFMP ifmp, 
	PGNO  * prgpgno, 
	const ENTRYTOCHECK * const prgentryToCheck,
	INFOLIST **ppEntriesToDelete,
	const BOOL	fFix,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR 			err = JET_errSuccess;
	
	ULONG	ulpgno = 0;

	//	preread 
	BFPrereadPageList( ifmp, prgpgno );

	//	check
	for(ulpgno = 0; pgnoNull != prgpgno[ulpgno]; ulpgno++ )
		{
		//	check the root page of an index
		OBJID objidIndexFDP = 0;
		CSR	csr;
	
		err = csr.ErrGetReadPage( 
					ppib, 
					ifmp,
					prgpgno[ulpgno],
					bflfNoTouch );
		if( err < 0 )
			{
			(*popts->pcprintfError)( "error %d trying to read page %d\r\n", err, prgpgno[ulpgno] );
			err = JET_errSuccess;
			}
		else
			{
			objidIndexFDP = csr.Cpage().ObjidFDP();
			}
		csr.ReleasePage( fTrue );

		//	error: the root page of a tree belongs to another objid
		if( prgentryToCheck[ulpgno].objidFDP != objidIndexFDP 
			&& sysobjTable != prgentryToCheck[ulpgno].objType )
			{
			ENTRYINFO entryinfo;
			memset( &entryinfo, 0, sizeof( entryinfo ) );
			entryinfo.objidTable = prgentryToCheck[ulpgno].objidTable;
			entryinfo.objType	 = prgentryToCheck[ulpgno].objType;
			entryinfo.objidFDP	 = prgentryToCheck[ulpgno].objidFDP;

			(*popts->pcprintfError)( "Catalog entry corruption: page %d belongs to %d (expected %d)\t\n", 
									prgpgno[ulpgno], objidIndexFDP, entryinfo.objidFDP );
									
			//	Collect Corrupted catalog entry info for future use
			if ( fFix )
				{
				Call( ErrREPAIRBuildCatalogEntryToDeleteList( ppEntriesToDelete, entryinfo ) );
				}
				else
				{
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			}
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckFixCatalogLogical(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	const BOOL  fShadow,
	const BOOL	fFix,
	QWORD * const pqwRecords,
	const REPAIROPTS * const popts )
//  ================================================================
	{				
	ERR				err				= JET_errSuccess;
	FUCB		* 	pfucbCatalog 	= pfucbNil;

	BOOL			fObjidFDPSeen	= fFalse;
	
	BOOL			fTemplateTable	= fFalse;
	BOOL			fDerivedTable	= fFalse;
	BOOL			fSeenSLVAvail	= fFalse;
	BOOL			fSeenSLVOwnerMap= fFalse;

	ENTRYINFO		entryinfo;
	ULONG			pgnoFDPTableCurr= 0;
	ULONG 			objidTableCurr 	= 0;
	CHAR			szTableName[JET_cbNameMost + 1];
	CHAR			szTemplateTable[JET_cbNameMost + 1];
	ULONG			pgnoFDPLVCurr 	= 0;
	ULONG			objidLVCurr		= 0;
	BOOL 			fSeenLongValue 	= fFalse;
	BOOL 			fSeenCallback 	= fFalse;
	
	BOOL			fSeenCorruptedTable			= fFalse;
	ULONG			objidLastCorruptedTable 	= 0xffffffff;
	BOOL			fSeenCorruptedIndex 		= fFalse;

	INFOLIST	*	pColInfo 		= NULL;
	INFOLIST	*	pIdxInfo		= NULL;
	TEMPLATEINFOLIST	*	pTemplateInfoList = NULL;

	INFOLIST	*	pTablesToDelete = NULL;
	INFOLIST	*	pEntriesToDelete = NULL;

	const INT		cpgnoFDPToPreread		= 64; // max pgnoFDP to pre-read
	
	PGNO			rgpgno[cpgnoFDPToPreread + 1]; //  NULL-terminated
	ENTRYTOCHECK	rgentryToCheck[cpgnoFDPToPreread + 1];
	ULONG			ulCount = 0;
	
	ULONG 			ulpgno;

	//Initialize
	for(ulpgno = 0; cpgnoFDPToPreread >= ulpgno ; ulpgno++ )
		{
		rgpgno[ulpgno] = pgnoNull;
		}


	Call( ErrCATOpen( ppib, ifmp, &pfucbCatalog, fShadow ) ); 
	Assert( pfucbNil != pfucbCatalog );
	Call( ErrIsamSetCurrentIndex( ppib, pfucbCatalog, szMSOIdIndex ) );
	Call( ErrIsamMove( ppib, pfucbCatalog, JET_MoveFirst, NO_GRBIT ) );

	while ( JET_errNoCurrentRecord != err )
		{
		Call( err );

		memset( &entryinfo, 0, sizeof( entryinfo ) );
		Call( ErrREPAIRRetrieveCatalogColumns( ppib, ifmp, pfucbCatalog, &entryinfo, popts) );

		if( objidSystemRoot == entryinfo.objidTable //special table
			&& !fSeenCorruptedTable ) 
			{
			objidTableCurr = entryinfo.objidTable;
			
			if( sysobjSLVAvail == entryinfo.objType )
				{
				//	special case
				if ( fSeenSLVAvail )
					{
					(*popts->pcprintfError)("Multiple SLVAvail trees in catalog\t\n" );
					fSeenCorruptedTable = fTrue;
					}
				else
					{
					fSeenSLVAvail = fTrue;
					fObjidFDPSeen = fTrue;
					}
				}
			else if( sysobjSLVOwnerMap == entryinfo.objType )
				{	
				//	special case
				if( fSeenSLVOwnerMap )
					{
					(*popts->pcprintfError)("Multiple SLVOwnerMap trees in catalog\t\n" );
					fSeenCorruptedTable = fTrue;
					}
				else
					{
					fSeenSLVOwnerMap = fTrue;
					fObjidFDPSeen = fTrue;
					}
				} 
			else 
				{
				// Error
				(*popts->pcprintfError)( "Invalid object type (%d) for table (%d)\t\n", entryinfo.objType, entryinfo.objidTable );
				fSeenCorruptedTable = fTrue;
				}
			}
		else if( objidTableCurr != entryinfo.objidTable )
			{
			//	we are on the first record of a different table

			// Clean the info of previous table 
			if ( fTemplateTable && !fSeenCorruptedTable )
				{
				// We need keep template table column info
				Call( ErrREPAIRInsertIntoTemplateInfoList( 
								&pTemplateInfoList, 
								szTemplateTable,
								pColInfo, 
								popts ) );
				}
			else
				{ 
				REPAIRUtilCleanInfoList ( &pColInfo );
				}
				REPAIRUtilCleanInfoList ( &pIdxInfo );

			// Start checking new table info
			objidTableCurr = entryinfo.objidTable;
			
			if( sysobjTable != entryinfo.objType )
				{
				//	ERROR: the first record must be sysobjTable
				(*popts->pcprintfError)( "Invalid object type (%d, expected %d)\n", entryinfo.objType, sysobjTable );
				fSeenCorruptedTable = fTrue;
				}
			else if( entryinfo.objidTable != entryinfo.objidFDP )
				{
				Assert( sysobjTable == entryinfo.objType );
				(*popts->pcprintfError)( "Invalid tableid (%d, expected %d)\n", entryinfo.objidFDP, objidTableCurr );
				fSeenCorruptedTable = fTrue;
				}
			else
				{
				fObjidFDPSeen = fTrue;

				// set up some info for new table
				fSeenCorruptedTable	= fFalse;
				pgnoFDPLVCurr		= 0;
				objidLVCurr			= 0;
				fSeenLongValue 		= fFalse;
				fSeenCallback 		= fFalse;
				pColInfo 			= NULL;
				pIdxInfo			= NULL;

				pgnoFDPTableCurr 	= entryinfo.pgnoFDPORColType;
				strcpy( szTableName, entryinfo.szName );
				if( JET_bitObjectTableTemplate & entryinfo.dwFlags ) 
					{
					fTemplateTable = fTrue;
					strcpy( szTemplateTable, entryinfo.szName );
					fDerivedTable = fFalse;
					}
				else if( JET_bitObjectTableDerived & entryinfo.dwFlags )
					{
					fDerivedTable = fTrue;
					strcpy( szTemplateTable, entryinfo.szTemplateTblORCallback );
					fTemplateTable = fFalse;

					TEMPLATEINFOLIST *pTempTemplateList = pTemplateInfoList;
					while( pTempTemplateList )
						{
						if ( !strcmp(szTemplateTable, pTempTemplateList->szTemplateTableName) )
							{
 							break; // find it
							}
						else 
							{
							pTempTemplateList = pTempTemplateList->pTemplateInfoListNext;
							}
						}
					if( NULL == pTempTemplateList )
						{
						//did not find the template table
						(*popts->pcprintfError)("Template table (%s) does not exist\n", szTemplateTable );
						fSeenCorruptedTable = fTrue;
						}
					}
				else 
					{
					fTemplateTable = fFalse;
					fDerivedTable = fFalse;
					}
				}
			}
		else if( !fSeenCorruptedTable )
			{
			// check the logical correctness of a table
			Assert( objidTableCurr == entryinfo.objidTable );

			switch ( entryinfo.objType )
				{
				case sysobjTable:
				//	ERROR: multiple table record;
					(*popts->pcprintfError)("Multiple table records for a table (%d) in a catalog\n", entryinfo.objidTable ); 
					fSeenCorruptedTable = fTrue;
					break;
				case sysobjColumn:
					//	X5:196648 -- don't insert the column into the list if it is JET_coltypNil
					//	that means its a deleted column and shouldn't be seen
					if( JET_coltypNil == entryinfo.pgnoFDPORColType )
						{
						err = JET_errSuccess;
						}
					else
						{
						//	check column info and store it into column linked list
						err = ErrREPAIRCheckOneTableLogical( 
								&pColInfo, 
								entryinfo, 
								popts );
						}
					
					if( JET_errDatabaseCorrupted == err )
						{
						fSeenCorruptedTable = fTrue;
						}
						
					break;
				case sysobjIndex:	
					err = ErrREPAIRCheckOneIndexLogical( 
								ppib, 
								ifmp, 
								pfucbCatalog,
								entryinfo, 
								objidTableCurr, 
								pgnoFDPTableCurr, 
								objidLVCurr, 
								pgnoFDPLVCurr, 
								pColInfo, 
								pTemplateInfoList, 
								fDerivedTable, 
								szTemplateTable, 
								popts );
					if( JET_errDatabaseCorrupted == err ) 
						{
						fSeenCorruptedIndex = fTrue;
						}
					else 
						{
						//	check index info and store it into index linked list
						err = ErrREPAIRCheckOneTableLogical( 
									&pIdxInfo, 
									entryinfo, 
									popts );
						if ( JET_errDatabaseCorrupted == err )
							{
							//fSeenCorruptedTable = fTrue;
							fSeenCorruptedIndex = fTrue;
							} 
						else
							{
							fObjidFDPSeen = fTrue;
							}
						}
					break;
				case sysobjLongValue:
					if( fSeenLongValue )
						{
						//	ERROR: multiple long-value records
						(*popts->pcprintfError)("Multiple long-value records for a table (%d) in a catalog\t\n", entryinfo.objidTable );
						fSeenCorruptedTable = fTrue;
						}
					else
						{
						fObjidFDPSeen = fTrue;
						
						pgnoFDPLVCurr = entryinfo.pgnoFDPORColType;
						objidLVCurr = entryinfo.objidFDP;
						fSeenLongValue = fTrue;
						}
					break;
				case sysobjCallback:
					if( fSeenCallback )
						{
						//	ERROR: multiple callbacks
						(*popts->pcprintfError)("Multiple callbacks for a table (%d) in a catalog\t\n", entryinfo.objidTable );
						fSeenCorruptedTable = fTrue;
						}
					else
						{
						fSeenCallback = fTrue;
						}
					break;
				case sysobjSLVAvail:
				case sysobjSLVOwnerMap:
				default:
					//	ERROR
					(*popts->pcprintfError)("Invalid Object Type (%d) for a table (%d) in a catalog\t\n", entryinfo.objType, entryinfo.objidTable );
					fSeenCorruptedTable = fTrue;
					break;
				} //switch
			}
		else
			{
			// Inside a table that has already been found corruption 
			Assert( fSeenCorruptedTable && objidTableCurr == entryinfo.objidTable );
			}
		
		if( fSeenCorruptedIndex )
			{
			//Collect Corrupted index info for future use
			if ( fFix )
				{
				Call( ErrREPAIRBuildCatalogEntryToDeleteList( &pEntriesToDelete, entryinfo ) );
				}
			else
				{
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			fSeenCorruptedIndex = fFalse;
			}

		if( fSeenCorruptedTable && objidLastCorruptedTable != entryinfo.objidTable )
			{
			//Collect Corrupted table info for future use
			if ( fFix )
				{
				Call( ErrREPAIRBuildCatalogEntryToDeleteList( &pTablesToDelete, entryinfo ) );
				}
			else
				{
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			objidLastCorruptedTable = entryinfo.objidTable;
			}

		if( fObjidFDPSeen )
			{
			if( entryinfo.objidFDP > *pobjidLast )
				{
				*pobjidLast = entryinfo.objidFDP;
				}
				
			rgpgno[ulCount] = entryinfo.pgnoFDPORColType;
			rgentryToCheck[ulCount].objidTable = entryinfo.objidTable; 
			rgentryToCheck[ulCount].objType	= entryinfo.objType;
			rgentryToCheck[ulCount].objidFDP = entryinfo.objidFDP;
			ulCount++;
			
			fObjidFDPSeen = fFalse;
			}
			
		err = ErrIsamMove( ppib, pfucbCatalog, JET_MoveNext, NO_GRBIT );

		//Special case: 
		//test if the root page in an index belongs to objid
		if ( 0 == ( ulCount % ( cpgnoFDPToPreread - 1 ) ) 
			 || JET_errNoCurrentRecord == err )
			{
			ERR errCheck = JET_errSuccess;
			
			errCheck = ErrREPAIRICheckCatalogEntryPgnoFDPs(
						ppib,
						ifmp, 
						rgpgno, 
						rgentryToCheck,
						&pEntriesToDelete,
						fFix,
						popts );
			if( errCheck < 0 )
				{
				Call( errCheck );
				}

			if( JET_errNoCurrentRecord != err )
				{
				//reset the array for future use
				for( ulpgno = 0; pgnoNull != rgpgno[ulpgno]; ulpgno++ )
					{
					Assert( cpgnoFDPToPreread > ulpgno ); 
					rgpgno[ulpgno] = pgnoNull;
					}
				//reset the counter
				ulCount = 0;
				}
			}
		
		} // while ( JET_errNoCurrentRecord != err )

	if( JET_errNoCurrentRecord == err )
		{
		//	expected error: we are at the end of catalog
		err = JET_errSuccess;
		}

HandleError:
	(*popts->pcprintfVerbose).Unindent();
	
	// clean		
	REPAIRUtilCleanInfoList( &pColInfo );
	REPAIRUtilCleanInfoList( &pIdxInfo );
	REPAIRUtilCleanTemplateInfoList( &pTemplateInfoList );

	if( pfucbNil != pfucbCatalog )
		{
		CallS( ErrCATClose( ppib, pfucbCatalog ) );
		}

		
	if( fFix && ( pTablesToDelete || pEntriesToDelete ) )
		{
		// Delete corrupted table entries in catalog
		(*popts->pcprintfVerbose)( "Deleting corrupted catalog entries\t\n" );
		err = ErrREPAIRDeleteCorruptedEntriesFromCatalog( 
					ppib, 
					ifmp, 
					pTablesToDelete, 
					pEntriesToDelete, 
					popts );
		if( JET_errSuccess == err )
			{
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			}
		}
		
	// clean
	if( pTablesToDelete )
		{
		REPAIRUtilCleanInfoList( &pTablesToDelete );
		}
	if( pEntriesToDelete )
		{
		REPAIRUtilCleanInfoList( &pEntriesToDelete );
		}
		
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSystemTablesLogical(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	BOOL * const pfCatalogCorrupt,
	BOOL * const pfShadowCatalogCorrupt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err				= JET_errSuccess;
	const BOOL 		fFix			= fFalse;   // Don't fix 

	// Enter this function when we did not find corruption 
	// in at least one catalog from physical check
	Assert( ! ( *pfCatalogCorrupt ) || !( *pfShadowCatalogCorrupt ) );

	(*popts->pcprintfVerbose)( "checking logical consistency of system tables\t\n" );
	(*popts->pcprintfVerbose).Indent();

	QWORD qwObjectsInCatalog 		= 0;
	QWORD qwObjectsInShadowCatalog	= 0;

	if( !( *pfCatalogCorrupt ) )
		{
		(*popts->pcprintfVerbose)( "%s\t\n", szMSO );
		err = ErrREPAIRCheckFixCatalogLogical( ppib, ifmp, pobjidLast, fFalse, fFix, &qwObjectsInCatalog, popts );
		if( JET_errDatabaseCorrupted == err )
			{
			(*popts->pcprintfError)( "%s is logically corrupted\t\n", szMSO );
			*pfCatalogCorrupt = fTrue;
			}
		else
			{
			Call( err );
			}
		}

	if( !( *pfShadowCatalogCorrupt ) )
		{
		(*popts->pcprintfVerbose)( "%s\t\n", szMSOShadow );
		err = ErrREPAIRCheckFixCatalogLogical( ppib, ifmp, pobjidLast, fTrue, fFix, &qwObjectsInShadowCatalog, popts );
		if( JET_errDatabaseCorrupted == err )
			{
			(*popts->pcprintfError)( "%s is logically corrupted\t\n", szMSOShadow );
			*pfShadowCatalogCorrupt = fTrue;
			}
		else
			{
			Call( err );
			}
		}

	if( !*pfCatalogCorrupt
		&& !*pfShadowCatalogCorrupt
		&& ( qwObjectsInCatalog != qwObjectsInShadowCatalog ) )
		{
		(*popts->pcprintfError)( "the catalog has %I64d records, but the shadow catalog has %I64d records\r\n", qwObjectsInCatalog, qwObjectsInShadowCatalog );
		*pfShadowCatalogCorrupt = fTrue;
		}

	Assert( JET_errDatabaseCorrupted == err ||
			JET_errSuccess	== err );
	err = JET_errSuccess;

HandleError:
	(*popts->pcprintfVerbose).Unindent();
	return err;
	}



//  ================================================================
LOCAL ERR ErrREPAIRCheckSpaceTree(
	PIB * const ppib,
	const IFMP ifmp, 
	BOOL * const pfSpaceTreeCorrupt,
	PGNO * const ppgnoLastOwned,	
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	RECCHECKNULL recchecknull;
	RECCHECKSPACE	reccheckSpace( ppib, popts );
	RECCHECKSPACEAE reccheckAE( ppib, pttarrayOwnedSpace, pttarrayAvailSpace, 1, objidNil, popts );

	BTSTATS btstats;
	memset( &btstats, 0, sizeof( btstats ) );

	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

	(*popts->pcprintfVerbose)( "checking SystemRoot\r\n" );
	(*popts->pcprintfVerbose).Indent();

	(*popts->pcprintfStats)( "\r\n\r\n" );
	(*popts->pcprintfStats)( "===== database root =====\r\n" );

	Call( ErrREPAIRCheckTree(
			ppib,
			ifmp,
			pgnoSystemRoot,
			1,
			CPAGE::fPagePrimary,
			&recchecknull,
			NULL,
			NULL,
			fFalse,
			pfDbtimeTooLarge,
			&btstats,
			popts ) );
	(*popts->pcprintfVerbose)( "SystemRoot (OE)\r\n" );
	Call( ErrREPAIRCheckTree(
			ppib,
			ifmp,
			pgnoSystemRoot+1,
			1,
			CPAGE::fPageSpaceTree,
			&reccheckSpace,
			NULL,
			NULL,
			fFalse,
			pfDbtimeTooLarge,
			&btstats,
			popts ) );
	(*popts->pcprintfVerbose)( "SystemRoot (AE)\r\n" );
	Call( ErrREPAIRCheckTree(
			ppib,
			ifmp,
			pgnoSystemRoot+2,
			1,
			CPAGE::fPageSpaceTree,
			&reccheckAE,
			NULL,
			NULL,
			fFalse,
			pfDbtimeTooLarge,
			&btstats,
			popts ) );

	*ppgnoLastOwned = reccheckSpace.PgnoLast();

HandleError:
	(*popts->pcprintfVerbose).Unindent();
	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	
	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVOwnerMapCorrupted:
		case JET_errDatabaseCorrupted:
			*pfSpaceTreeCorrupt = fTrue;
			err = JET_errSuccess;
			break;
		default:
			*pfSpaceTreeCorrupt = fFalse;
			break;
		}

	return err;
	}


//  ================================================================
LOCAL ErrREPAIRCheckRange(
	PIB * const ppib,
	TTARRAY * const pttarray,
	const ULONG ulFirst,
	const ULONG ulLast,
	const ULONG ulValue,
	BOOL * const pfMismatch,
	const REPAIROPTS * const popts )
//  ================================================================
//
//	Make sure all entries in the ttarray are equal to the given value
//
//-
	{
	ERR err = JET_errSuccess;
	TTARRAY::RUN run;

	pttarray->BeginRun( ppib, &run );

	ULONG ul;
	for( ul = ulFirst; ul < ulLast; ul++ )
		{
		ULONG ulActual;

		Call( pttarray->ErrGetValue( ppib, ul, &ulActual, &run ) );

		if( ulActual != ulValue )
			{
			*pfMismatch = fTrue;
			break;
			}
		}


HandleError:
	pttarray->EndRun( ppib, &run );	
	return err;
	}


#ifdef DISABLE_SLV
#else

//  ================================================================
LOCAL ERR ErrREPAIRCheckSLVAvailTree(
	PIB * const ppib,
	const IFMP ifmp, 
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	JET_ERR err = JET_errSuccess;

	RECCHECKSLVSPACE reccheckslvspace( ifmp, popts );

	PGNO 	pgnoSLVAvail;
	OBJID	objidSLVAvail;
	Call( ErrCATAccessDbSLVAvail( ppib, ifmp, szSLVAvail, &pgnoSLVAvail, &objidSLVAvail ) );

	(*popts->pcprintfVerbose)( "checking SLV space (pgno %d, objid %d)\r\n", pgnoSLVAvail, objidSLVAvail );
	(*popts->pcprintfVerbose).Indent();

	Call( ErrREPAIRCheckTreeAndSpace(
			ppib,
			ifmp,
			objidSLVAvail,
			pgnoSLVAvail,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPageSLVAvail,
			fTrue,
			&reccheckslvspace,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			pfDbtimeTooLarge,
			popts ) );
	
HandleError:

	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVSpaceCorrupted:		
		case JET_errSLVSpaceMapCorrupted:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSLVOwnerMapTree(
	PIB * const ppib,
	const IFMP ifmp, 
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	JET_ERR err = JET_errSuccess;
	
	RECCHECKSLVOWNERMAP reccheckslvownermap( popts );

	PGNO 	pgnoSLVOwnerMap;
	OBJID	objidSLVOwnerMap;
	Call( ErrCATAccessDbSLVOwnerMap( ppib, ifmp, szSLVOwnerMap, &pgnoSLVOwnerMap, &objidSLVOwnerMap ) );

	(*popts->pcprintfVerbose)( "checking SLV space map (pgno %d, objid %d)\r\n", pgnoSLVOwnerMap, objidSLVOwnerMap );
	(*popts->pcprintfVerbose).Indent();

	Call( ErrREPAIRCheckTreeAndSpace(
			ppib,
			ifmp,
			objidSLVOwnerMap,
			pgnoSLVOwnerMap,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPageSLVOwnerMap,
			fTrue,
			&reccheckslvownermap,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			pfDbtimeTooLarge,
			popts ) );
	
HandleError:

	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVSpaceCorrupted:  		
		case JET_errSLVSpaceMapCorrupted:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}
		
	return err;
	}


//  ================================================================
LOCAL VOID REPAIRStreamingFileReadComplete(
		const ERR			err,
		IFileAPI *const	pfapi,
		const QWORD			ibOffset,
		const DWORD			cbData,
		const BYTE* const	pbData,
		const DWORD_PTR		dwCompletionKey )
//  ================================================================
//
//  Just set the signal, the checksumming will be done in the main thread
//
//-
	{
	IFileAPI::PfnIOComplete pfn = REPAIRStreamingFileReadComplete;	//	should only compile if signatures match
	
	CHECKSUMSLVCHUNK * pchecksumchunk = (CHECKSUMSLVCHUNK *)dwCompletionKey;
	pchecksumchunk->err = err;
	pchecksumchunk->signal.Set();
	}


//  ================================================================
LOCAL ERR ErrREPAIRAllocChecksumslvchunk( CHECKSUMSLVCHUNK * const pchecksumslvchunk, const INT cbBuffer, const INT cpages )
//  ================================================================
	{
	pchecksumslvchunk->pvBuffer 			= PvOSMemoryPageAlloc( cbBuffer, NULL );
	pchecksumslvchunk->pulChecksums 		= new ULONG[cpages];
	pchecksumslvchunk->pulChecksumsExpected = new ULONG[cpages];
	pchecksumslvchunk->pulChecksumLengths 	= new ULONG[cpages];
	pchecksumslvchunk->pobjid				= new OBJID[cpages];
	pchecksumslvchunk->pcolumnid			= new COLUMNID[cpages];
	pchecksumslvchunk->pcbKey				= new USHORT[cpages];
	pchecksumslvchunk->pprgbKey				= new BYTE*[cpages];

	if(	NULL == pchecksumslvchunk->pvBuffer
		|| NULL == pchecksumslvchunk->pulChecksums
		|| NULL == pchecksumslvchunk->pulChecksumsExpected
		|| NULL == pchecksumslvchunk->pulChecksumLengths
		|| NULL == pchecksumslvchunk->pobjid
		|| NULL == pchecksumslvchunk->pcolumnid
		|| NULL == pchecksumslvchunk->pcbKey
		|| NULL == pchecksumslvchunk->pprgbKey )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	for (int ipage = 0; ipage < cpages; ipage++) 
		{
		pchecksumslvchunk->pprgbKey[ipage] = new BYTE[KeyLengthMax];
		if ( NULL == pchecksumslvchunk->pprgbKey[ipage] )
			{
			return ErrERRCheck( JET_errOutOfMemory );
			}
		}

	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrREPAIRAllocChecksumslv( CHECKSUMSLV * const pchecksumslv )
//  ================================================================
	{
	pchecksumslv->rgchecksumchunk 			= new CHECKSUMSLVCHUNK[pchecksumslv->cchecksumchunk];
	if( NULL == pchecksumslv->rgchecksumchunk )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	const INT cbBuffer 	= pchecksumslv->cbchecksumchunk;
	const INT cpages 	= cbBuffer / g_cbPage;

	INT ichunk;
	for( ichunk = 0; ichunk < pchecksumslv->cchecksumchunk; ++ichunk )
		{
		pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fFalse;
		pchecksumslv->rgchecksumchunk[ichunk].err		= JET_errSuccess;
		
		const ERR err = ErrREPAIRAllocChecksumslvchunk( pchecksumslv->rgchecksumchunk + ichunk, cbBuffer, cpages );

		if( err < 0 )
			{
			while( --ichunk > 0 )
				{
				REPAIRFreeChecksumslvchunk( pchecksumslv->rgchecksumchunk + ichunk, cpages );
				}
			return err;
			}
		}
	return JET_errSuccess;
	}


//  ================================================================
LOCAL VOID REPAIRFreeChecksumslvchunk( CHECKSUMSLVCHUNK * const pchecksumslvchunk, const INT cpages  )
//  ================================================================
	{
	OSMemoryPageFree( pchecksumslvchunk->pvBuffer );
	delete [] pchecksumslvchunk->pulChecksums;
	delete [] pchecksumslvchunk->pulChecksumsExpected;
	delete [] pchecksumslvchunk->pulChecksumLengths;
	delete [] pchecksumslvchunk->pobjid;
	delete [] pchecksumslvchunk->pcolumnid;
	delete [] pchecksumslvchunk->pcbKey;
	for (int ipage = 0; ipage < cpages; ipage++)
		{
		if ( pchecksumslvchunk->pprgbKey[ipage] )
			delete [] pchecksumslvchunk->pprgbKey[ipage];
		pchecksumslvchunk->pprgbKey[ipage] = NULL;
		} 
	delete [] pchecksumslvchunk->pprgbKey;
	
	pchecksumslvchunk->pvBuffer 	= NULL;
	pchecksumslvchunk->pulChecksums	= NULL;
	pchecksumslvchunk->pulChecksumsExpected = NULL;
	pchecksumslvchunk->pulChecksumLengths	= NULL;
	pchecksumslvchunk->pobjid = NULL;
	pchecksumslvchunk->pcolumnid = NULL;
	pchecksumslvchunk->pcbKey = NULL;
	pchecksumslvchunk->pprgbKey = NULL;

	}


//  ================================================================
LOCAL VOID REPAIRFreeChecksumslv( CHECKSUMSLV * const pchecksumslv )
//  ================================================================
	{
	if( pchecksumslv )
		{
		if( pchecksumslv->rgchecksumchunk )
			{
			const INT cpages = pchecksumslv->cbchecksumchunk / g_cbPage;
			INT ichunk;
			for( ichunk = 0; ichunk < pchecksumslv->cchecksumchunk; ++ichunk )
				{
				if( NULL != pchecksumslv->rgchecksumchunk[ichunk].pvBuffer )
					{
					REPAIRFreeChecksumslvchunk( pchecksumslv->rgchecksumchunk + ichunk, cpages );
					}
				}
			delete [] pchecksumslv->rgchecksumchunk;
			}
		delete pchecksumslv;
		}
	}


//  ================================================================
LOCAL ERR ErrREPAIRChecksumSLVChunk(
	PIB * const ppib,
	const CHECKSUMSLV * const pchecksumslv,
	const INT ichunk,
	BOOL * const pfMismatchSeen )
//  ================================================================
	{
	ERR 	err 		= JET_errSuccess;

	const CPG cpgRead 	= pchecksumslv->rgchecksumchunk[ichunk].cbRead / g_cbPage;
	const BYTE * pb 	= (BYTE *)pchecksumslv->rgchecksumchunk[ichunk].pvBuffer;

	PGNO pgno = pchecksumslv->rgchecksumchunk[ichunk].pgnoFirst;

	*pfMismatchSeen 	= fFalse;

	//

	TTARRAY::RUN checksumsRun;
	TTARRAY::RUN checksumLengthsRun;

	pchecksumslv->pttarraySLVChecksumsFromFile->BeginRun( ppib, &checksumsRun );
	pchecksumslv->pttarraySLVChecksumLengthsFromSpaceMap->BeginRun( ppib, &checksumLengthsRun );

	//
	
	INT ipage;
	for( ipage = 0; ipage < cpgRead; ++ipage )
		{
		const ULONG cbChecksum 			= pchecksumslv->rgchecksumchunk[ichunk].pulChecksumLengths[ipage];
		Assert( cbChecksum <= g_cbPage );
		
		if( 0 != cbChecksum )
			{
			const ULONG ulChecksum 			= UlChecksumSLV( pb, pb + cbChecksum );
			const ULONG ulChecksumExpected = pchecksumslv->rgchecksumchunk[ichunk].pulChecksumsExpected[ipage];

			pchecksumslv->rgchecksumchunk[ichunk].pulChecksums[ipage] = ulChecksum;

			if( ulChecksum != ulChecksumExpected )
				{
				CHAR rgbKey[64];
				REPAIRDumpHex(
					rgbKey,
					sizeof( rgbKey ),
					pchecksumslv->rgchecksumchunk[ichunk].pprgbKey[ipage],
					pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage] );
				
				(*pchecksumslv->popts->pcprintfError)(
					"SLV checksum mismatch: Page %d (owned by %d:%d:%s). Checksum length %d bytes. Expected checksum 0x%x. Actual checksum 0x%x.\r\n",
					pgno,   
					pchecksumslv->rgchecksumchunk[ichunk].pobjid[ipage], 
					pchecksumslv->rgchecksumchunk[ichunk].pcolumnid[ipage],
					rgbKey,
					cbChecksum, 
					ulChecksumExpected, 
					ulChecksum);
					
				*pfMismatchSeen = fTrue;
				}
			else
				{
				Call( pchecksumslv->pttarraySLVChecksumsFromFile->ErrSetValue(
					ppib,
					pgno,
					ulChecksum,
					&checksumsRun ) );
				Call( pchecksumslv->pttarraySLVChecksumLengthsFromSpaceMap->ErrSetValue(
					ppib,
					pgno,
					cbChecksum,
					&checksumLengthsRun ) );
				}
			}
		pb += g_cbPage;
		++pgno;
		}

HandleError:

	pchecksumslv->pttarraySLVChecksumsFromFile->EndRun( ppib, &checksumsRun );
	pchecksumslv->pttarraySLVChecksumLengthsFromSpaceMap->EndRun( ppib, &checksumLengthsRun );
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRISetupChecksumchunk(
	FUCB * const pfucbSLVSpaceMap,
	CHECKSUMSLV * const pchecksumslv,
	BOOL * const pfDone,
	BOOL * const pfChunkHasChecksums,
	const INT ichunk )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	INT ipage;

	const INT cpages 	= pchecksumslv->cbchecksumchunk / g_cbPage;	
	const INT cbToZero 	= sizeof( pchecksumslv->rgchecksumchunk[ichunk].pulChecksumsExpected[0] ) * cpages;
	
	memset( pchecksumslv->rgchecksumchunk[ichunk].pulChecksumsExpected, 0, cbToZero );
	memset( pchecksumslv->rgchecksumchunk[ichunk].pulChecksumLengths, 0, cbToZero );
	memset(pchecksumslv->rgchecksumchunk[ichunk].pobjid, 0, sizeof( pchecksumslv->rgchecksumchunk[ichunk].pobjid[0] ) * cpages );
	memset(pchecksumslv->rgchecksumchunk[ichunk].pcolumnid, 0, sizeof( pchecksumslv->rgchecksumchunk[ichunk].pcolumnid[0] ) * cpages );
	memset(pchecksumslv->rgchecksumchunk[ichunk].pcbKey, 0, sizeof( pchecksumslv->rgchecksumchunk[ichunk].pcbKey[0] ) * cpages );
	for(ipage =0; ipage < cpages; ++ipage)
		{
		memset(pchecksumslv->rgchecksumchunk[ichunk].pprgbKey[ipage], 0, KeyLengthMax );
		}
	
	*pfDone 				= fFalse;
	*pfChunkHasChecksums	= fFalse;

	pchecksumslv->rgchecksumchunk[ichunk].signal.Reset();
	pchecksumslv->rgchecksumchunk[ichunk].err 		= JET_errSuccess;
	pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fFalse;
	pchecksumslv->rgchecksumchunk[ichunk].cbRead	= 0;
	
	//	are we at the end of slvspacemap?
	
	if( !Pcsr( pfucbSLVSpaceMap )->FLatched() )
		{
		*pfDone = fTrue;
		return JET_errSuccess;
		}

	//

	PGNO pgnoFirst;
	LongFromKey( (ULONG *)&pgnoFirst, pfucbSLVSpaceMap->kdfCurr.key );
	pchecksumslv->rgchecksumchunk[ichunk].pgnoFirst = pgnoFirst;
	
	//	propagate the checksum lengths into the CHECKSUMCHUNK

	INT ipageMaxWithChecksum = 0;
	for( ipage = 0; ipage < cpages; ++ipage )
		{
		SLVOWNERMAPNODE slvownermapnode;	
		
		slvownermapnode.Retrieve( pfucbSLVSpaceMap->kdfCurr.data );

		if( slvownermapnode.FValidChecksum() )
			{
			ipageMaxWithChecksum = ipage;
			
			pchecksumslv->rgchecksumchunk[ichunk].pulChecksumsExpected[ipage] = slvownermapnode.UlChecksum();
			pchecksumslv->rgchecksumchunk[ichunk].pulChecksumLengths[ipage] 	= slvownermapnode.CbDataChecksummed();		
			pchecksumslv->rgchecksumchunk[ichunk].pobjid[ipage] = slvownermapnode.Objid();
			pchecksumslv->rgchecksumchunk[ichunk].pcolumnid[ipage] = slvownermapnode.Columnid();
			pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage] = slvownermapnode.CbKey();

			//make sure the key Length is not greater than KeyLengthMax
			if (pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage] > KeyLengthMax)
				{
				(*pchecksumslv->popts->pcprintfError)(
					"INTERNAL ERROR: key of SLV-owning record is too large (%d bytes, buffer is %d bytes)\r\n",
					pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage], KeyLengthMax );
				Call( ErrERRCheck( JET_errInternalError ) );				
				}

			memcpy(	pchecksumslv->rgchecksumchunk[ichunk].pprgbKey[ipage], 
			        (BYTE *)slvownermapnode.PvKey(), 
			        pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage] );
					
			*pfChunkHasChecksums = fTrue;
			}
		else
			{
			pchecksumslv->rgchecksumchunk[ichunk].pulChecksumsExpected[ipage] = 0;
			pchecksumslv->rgchecksumchunk[ichunk].pulChecksumLengths[ipage] 	= 0;
			pchecksumslv->rgchecksumchunk[ichunk].pobjid[ipage] = 0;
			pchecksumslv->rgchecksumchunk[ichunk].pcolumnid[ipage] = 0;
			pchecksumslv->rgchecksumchunk[ichunk].pcbKey[ipage] = 0;
			memset(pchecksumslv->rgchecksumchunk[ichunk].pprgbKey[ipage], 0, KeyLengthMax );
			}
			
		err = ErrDIRNext( pfucbSLVSpaceMap, fDIRNull );
		if( JET_errNoCurrentRecord == err )
			{
			err = JET_errSuccess;
			break;
			}
		Call( err );
		}

	pchecksumslv->rgchecksumchunk[ichunk].cbRead = ( ipageMaxWithChecksum + 1 ) * g_cbPage;
	
HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRSetSequentialMoveFirst(
	PIB * const ppib,
	FUCB * const pfucbSLVSpaceMap,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	FUCBSetPrereadForward( pfucbSLVSpaceMap, cpgPrereadSequential );
	
	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm 	= NULL;
	dib.dirflag = fDIRNull;

	Call( ErrDIRDown( pfucbSLVSpaceMap, &dib ) );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSLVChecksums(
	PIB * const ppib,
	FUCB * const pfucbSLVSpaceMap,
	CHECKSUMSLV * const pchecksumslv )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	ERR errSav = JET_errSuccess;

	BOOL fNoMoreReads 		= fFalse;
	BOOL fChunkHasChecksums	= fFalse;

	BOOL fMismatchSeen		= fFalse;
	BOOL fMismatchSeenT		= fFalse;
	
	const QWORD ibOffsetMax = ( ( pchecksumslv->cpgSLV + cpgDBReserved ) << g_shfCbPage );
	QWORD ibOffset = (cpgDBReserved << g_shfCbPage);
	
	INT ichunk;

	//	move to the start of the SLVSpaceMap

	Call( ErrREPAIRSetSequentialMoveFirst( ppib, pfucbSLVSpaceMap, pchecksumslv->popts ) );
	
	//  setup initial reads

	ichunk = 0;
	while( ichunk < pchecksumslv->cchecksumchunk && !fNoMoreReads )
		{
		Call( ErrREPAIRISetupChecksumchunk(
				pfucbSLVSpaceMap,
				pchecksumslv,
				&fNoMoreReads,
				&fChunkHasChecksums,
				ichunk ) );
		if( fChunkHasChecksums )
			{
			Assert( 0 != pchecksumslv->rgchecksumchunk[ichunk].cbRead );
			
			Call( pchecksumslv->pfapiSLV->ErrIORead(
										ibOffset,
										pchecksumslv->rgchecksumchunk[ichunk].cbRead,
										(BYTE *)(pchecksumslv->rgchecksumchunk[ichunk].pvBuffer),
										REPAIRStreamingFileReadComplete,
										(DWORD_PTR)(pchecksumslv->rgchecksumchunk + ichunk) ) );
										
			pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fTrue;
						
			Assert( ibOffset <= ibOffsetMax );
			++ichunk;
			}
		ibOffset += pchecksumslv->cbchecksumchunk;			
		}

	CallS( pchecksumslv->pfapiSLV->ErrIOIssue() );
		
	//

	ichunk = 0;
	while( !fNoMoreReads )
		{
		
		//	collect the I/O for this chunk

		Assert( pchecksumslv->rgchecksumchunk[ichunk].fIOIssued );
		pchecksumslv->rgchecksumchunk[ichunk].signal.Wait();			
		pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fFalse;
		
		Call( pchecksumslv->rgchecksumchunk[ichunk].err );

		Call( ErrREPAIRChecksumSLVChunk(	
				ppib,
				pchecksumslv,
				ichunk,
				&fMismatchSeenT ) );
		fMismatchSeen = ( fMismatchSeen || fMismatchSeenT );

		//

		while (1)
			{
			Call( ErrREPAIRISetupChecksumchunk(
				pfucbSLVSpaceMap,
				pchecksumslv,
				&fNoMoreReads,
				&fChunkHasChecksums,
				ichunk ) );

			if( fChunkHasChecksums || fNoMoreReads )
				{
				break;
				}
			else
				{
				ibOffset += pchecksumslv->cbchecksumchunk;
				}
			}
			
		if( !fChunkHasChecksums )
			{
			Assert( fNoMoreReads );
			}
		else
			{
			Assert( 0 != pchecksumslv->rgchecksumchunk[ichunk].cbRead );
			
			Call( pchecksumslv->pfapiSLV->ErrIORead(
										ibOffset,
										pchecksumslv->rgchecksumchunk[ichunk].cbRead,
										(BYTE *)(pchecksumslv->rgchecksumchunk[ichunk].pvBuffer),
										REPAIRStreamingFileReadComplete,
										(DWORD_PTR)(pchecksumslv->rgchecksumchunk + ichunk) ) );
										
			pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fTrue;
			
			ichunk = ( ichunk + 1 ) % pchecksumslv->cchecksumchunk;
			ibOffset += pchecksumslv->cbchecksumchunk;
			
			Assert( ibOffset <= ibOffsetMax );
			}			

		CallS( pchecksumslv->pfapiSLV->ErrIOIssue() );		
		}

	//  collect all outstanding reads

	for( ichunk = 0; ichunk < pchecksumslv->cchecksumchunk; ++ichunk )
		{
		if( !pchecksumslv->rgchecksumchunk[ichunk].fIOIssued )
			{
			continue;
			}
			
		pchecksumslv->rgchecksumchunk[ichunk].signal.Wait();			
		pchecksumslv->rgchecksumchunk[ichunk].fIOIssued = fFalse;
		
		Call( pchecksumslv->rgchecksumchunk[ichunk].err );

		Call( ErrREPAIRChecksumSLVChunk(	
				ppib,
				pchecksumslv,
				ichunk,
				&fMismatchSeenT ) );
		fMismatchSeen = ( fMismatchSeen || fMismatchSeenT );
		}
		
HandleError:

	BTUp( pfucbSLVSpaceMap );
	CallS( pchecksumslv->pfapiSLV->ErrIOIssue() );

	for ( ichunk = 0; ichunk < pchecksumslv->cchecksumchunk; ichunk++ )
		{
		if( pchecksumslv->rgchecksumchunk[ichunk].fIOIssued )
			{
			Assert( err < JET_errSuccess );
			pchecksumslv->rgchecksumchunk[ichunk].signal.Wait();			
			}
		}

	if( JET_errSuccess == err && fMismatchSeen )
		{
		*(pchecksumslv->perr) = JET_errSLVReadVerifyFailure;
		}
	else
		{
		*(pchecksumslv->perr) = err;		
		}
		
	return err;
	}


//  ================================================================
LOCAL VOID REPAIRSLVChecksumTask( PIB * const ppib, const ULONG_PTR ul )
//  ================================================================
	{
	TASKMGR::TASK task = REPAIRSLVChecksumTask;	// should only compile if the signatures match

	ERR err = JET_errSuccess;

	PGNO 	pgnoSLVOwnerMap;
	OBJID	objidSLVOwnerMap;

	FUCB * pfucbSLVOwnerMap = pfucbNil;
	
	CHECKSUMSLV * const pchecksumslv = (CHECKSUMSLV *)ul;

	Ptls()->szCprintfPrefix = pchecksumslv->szSLV;

	(*pchecksumslv->popts->pcprintfVerbose)( "Checksumming streaming file \"%s\" (%d pages)\r\n",
		pchecksumslv->szSLV,
		pchecksumslv->cpgSLV );

	Call( ErrCATAccessDbSLVOwnerMap( ppib, pchecksumslv->ifmp, szSLVOwnerMap, &pgnoSLVOwnerMap, &objidSLVOwnerMap ) );
	Call( ErrBTOpen( ppib, pgnoSLVOwnerMap, pchecksumslv->ifmp, &pfucbSLVOwnerMap ) );	
	Call( ErrREPAIRCheckSLVChecksums( ppib, pfucbSLVOwnerMap, pchecksumslv ) );
		
HandleError:
	(*pchecksumslv->popts->pcprintfVerbose)( "Checksumming of streaming file \"%s\" finishes with error %d\r\n",
		pchecksumslv->szSLV,
		err );

	if( pfucbNil != pfucbSLVOwnerMap )
		{
		DIRClose( pfucbSLVOwnerMap );
		}
	REPAIRFreeChecksumslv( pchecksumslv );		
	}


//  ================================================================
LOCAL ERR ErrREPAIRChecksumSLV(
	const IFMP ifmp,
	IFileAPI *const pfapiSLV,
	const CHAR * const szSLV,
	const CPG cpgSLV,
	TASKMGR * const ptaskmgr,
	ERR * const perr,
	TTARRAY * const pttarraySLVChecksumsFromFile,
	TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,			
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	CHECKSUMSLV * const pchecksumslv = new CHECKSUMSLV;

	if( NULL == pchecksumslv )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	pchecksumslv->ifmp						= ifmp;
	pchecksumslv->pfapiSLV 					= pfapiSLV;
	pchecksumslv->szSLV 					= szSLV;
	pchecksumslv->cpgSLV 					= cpgSLV;
	pchecksumslv->perr 						= perr;
	pchecksumslv->popts 					= popts;

	pchecksumslv->pttarraySLVChecksumsFromFile				= pttarraySLVChecksumsFromFile;
	pchecksumslv->pttarraySLVChecksumLengthsFromSpaceMap 	= pttarraySLVChecksumLengthsFromSpaceMap;

	pchecksumslv->cchecksumchunk 			= cSLVChecksumChunk;	
	pchecksumslv->cbchecksumchunk 			= cbSLVChecksumChunk;

	Call( ErrREPAIRAllocChecksumslv( pchecksumslv ) );
	
	Call( ptaskmgr->ErrPostTask( REPAIRSLVChecksumTask, (ULONG_PTR)pchecksumslv ) );
	
HandleError:
	if( err < 0 )
		{
		REPAIRFreeChecksumslv( pchecksumslv );
		}
	return err;
	}	

//  ================================================================
LOCAL ERR ErrREPAIRVerifySLVTrees(
	PIB * const ppib, 
	const IFMP ifmp,
	BOOL * const pfSLVSpaceTreesCorrupt,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,	
	TTARRAY	* const pttarraySLVOwnerMapKey,
	TTARRAY	* const pttarraySLVChecksumLengths,	
	const REPAIROPTS * const popts )
//  ================================================================
//
//  At this point all the tables in the database should be good.
//  Check logical consistency.
//
//-
	{
	ERR err = JET_errSuccess;

	BOOL fCorrupted = fFalse;
	
	(*popts->pcprintfVerbose)( "Verifying SLV space maps\r\n" );

	CSLVAvailIterator 		slvavailIterator;
	CSLVOwnerMapIterator 	slvownermapIterator;

	//  no need to worry about deadlock between these TTARRAYs because they are
	//  only being accessed by this thread for the duration of this function call
	TTARRAY::RUN availRun;
	TTARRAY::RUN columnidRun;
	TTARRAY::RUN keyRun;
	TTARRAY::RUN lengthRun;
	
	INT pgno = 1;	

	Call( slvavailIterator.ErrInit( ppib, ifmp ) );
	Call( slvownermapIterator.ErrInit( ppib, ifmp ) );

	Call( slvavailIterator.ErrMoveFirst() );
	Call( slvownermapIterator.ErrMoveFirst() );

	pttarraySLVAvail->BeginRun( ppib, &availRun );
	pttarraySLVOwnerMapColumnid->BeginRun( ppib, &columnidRun );
	pttarraySLVOwnerMapKey->BeginRun( ppib, &keyRun );
	pttarraySLVChecksumLengths->BeginRun( ppib, &lengthRun );
	
	while(1)
		{
		OBJID objidOwning;
		Call( pttarraySLVAvail->ErrGetValue( ppib, pgno, &objidOwning, &availRun ) );

		if( objidNil == objidOwning )
			{
			
			//  unused page
			
			if( slvavailIterator.FCommitted() )
				{
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d is unused but is marked as committed\r\n", pgno );
				fCorrupted = fTrue;
				}

			if( !slvownermapIterator.FNull() )
				{
				CHAR rgbKey[64];
				REPAIRDumpHex(
					rgbKey,
					sizeof( rgbKey ),
					(BYTE *)slvownermapIterator.PvKey(),
					slvownermapIterator.CbKey() );
				
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d is unused but is marked as owned by %d:%d:%s\r\n",
					pgno, slvownermapIterator.Objid(), slvownermapIterator.Columnid(), rgbKey );

				fCorrupted = fTrue;
				}
				
			}
		else
			{
			
			//  used page

			COLUMNID columnid;
			ULONG cbDataChecksummed;
			ULONG rgulKey[culSLVKeyToStore];
			BYTE * const pbKey = (BYTE *)rgulKey;
			
			KEY keyOwning;
			keyOwning.Nullify();
			keyOwning.suffix.SetPv( pbKey + 1 );

			KEY keyExpected;
			keyExpected.Nullify();
			keyExpected.suffix.SetPv( const_cast<VOID *>( slvownermapIterator.PvKey() ) );
			keyExpected.suffix.SetCb( slvownermapIterator.CbKey() );
			
			Call( pttarraySLVOwnerMapColumnid->ErrGetValue( ppib, pgno, &columnid, &columnidRun ) );
			Call( pttarraySLVChecksumLengths->ErrGetValue( ppib, pgno, &cbDataChecksummed, &lengthRun ) );

			INT iul;
			for( iul = 0; iul < culSLVKeyToStore; ++iul )
				{
				Call( pttarraySLVOwnerMapKey->ErrGetValue(
											ppib,
											pgno * culSLVKeyToStore + iul,
											rgulKey + iul,
											&keyRun ) );
				}			

			keyOwning.suffix.SetCb( *pbKey );

			if( !slvavailIterator.FCommitted() )
				{
				CHAR rgbKeyOwner[64];
				REPAIRDumpHex(
					rgbKeyOwner,
					sizeof( rgbKeyOwner ),
					(BYTE *)keyOwning.suffix.Pv(),
					keyOwning.suffix.Cb() );
			
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d is owned by %d:%d:%s but is not marked as committed\r\n",
					pgno,
					objidOwning,
					columnid,
					rgbKeyOwner );
				fCorrupted = fTrue;
				}

			if( slvownermapIterator.FNull() )
				{
				CHAR rgbKeyOwner[64];
				REPAIRDumpHex(
					rgbKeyOwner,
					sizeof( rgbKeyOwner ),
					(BYTE *)keyOwning.suffix.Pv(),
					keyOwning.suffix.Cb() );
			
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d is owned by %d:%d:%s but has no space map entry\r\n",
					pgno,
					objidOwning,
					columnid,
					rgbKeyOwner );
				fCorrupted = fTrue;
				}
			else if( slvownermapIterator.Objid() != objidOwning
					|| slvownermapIterator.Columnid() != columnid
					|| 0 != CmpKeyShortest( keyOwning, keyExpected ) )
				{
				CHAR rgbKeyExpected[64];
				REPAIRDumpHex(
					rgbKeyExpected,
					sizeof( rgbKeyExpected ),
					(BYTE *)slvownermapIterator.PvKey(),
					slvownermapIterator.CbKey() );
					
				CHAR rgbKeyOwner[64];
				REPAIRDumpHex(
					rgbKeyOwner,
					sizeof( rgbKeyOwner ),
					(BYTE *)keyOwning.suffix.Pv(),
					keyOwning.suffix.Cb() );
			
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d is owned by %d:%d:%s (expected %d:%d:%s)\r\n",
					pgno,
					//  the record actually referencing the page is
					objidOwning,
					columnid,
					rgbKeyOwner,
					//  but the SLVOwnerMap says
					slvownermapIterator.Objid(),
					slvownermapIterator.Columnid(),
					rgbKeyExpected
				 );

				fCorrupted = fTrue;
				}			
			else if( slvownermapIterator.FChecksumIsValid()
					&& slvownermapIterator.CbDataChecksummed() != cbDataChecksummed )
				{
				CHAR rgbKeyOwner[64];
				REPAIRDumpHex(
					rgbKeyOwner,
					sizeof( rgbKeyOwner ),
					(BYTE *)keyOwning.suffix.Pv(),
					keyOwning.suffix.Cb() );
				
				(*popts->pcprintfError)( 
					"SLV space corruption: page %d (owned by %d:%d:%s) has a checksum length mismatch (%d, expected %d)\r\n",
					pgno,
					//  the record actually referencing the page is
					objidOwning,
					columnid,
					rgbKeyOwner,
					slvownermapIterator.CbDataChecksummed(),
					cbDataChecksummed );
				fCorrupted = fTrue;
				}
			}
			
		++pgno;
		if( pgno > rgfmp[ifmp].PgnoSLVLast() )
			{
			break;
			}

		err = slvavailIterator.ErrMoveNext();
		if( JET_errNoCurrentRecord == err )
			{
			//  the size may not match, but we are O.K. as long as there are no pages used
			//	beyond this point

			err = JET_errSuccess;
			break;
			}
		Call( err );
			
		err = slvownermapIterator.ErrMoveNext();					
		if( JET_errNoCurrentRecord == err )
			{
			//  the size may not match, but we are O.K. as long as there are no pages used
			//	beyond this point

			err = JET_errSuccess;
			break;			
			}
		Call( err );			
		}

	while( pgno < rgfmp[ifmp].PgnoSLVLast() )
		{
		OBJID objidOwning;
		Call( pttarraySLVAvail->ErrGetValue( ppib, pgno, &objidOwning, &availRun ) );

		if( objidNil != objidOwning )
			{
			(*popts->pcprintfError)( 
				"streaming file corruption: page %d is used by objid %d but has no entry\r\n",
				pgno, objidOwning );
			fCorrupted = fTrue;
			}		

		++pgno;			
		}
	
HandleError:
	pttarraySLVAvail->EndRun( ppib, &availRun );
	pttarraySLVOwnerMapColumnid->EndRun( ppib, &columnidRun );
	pttarraySLVOwnerMapKey->EndRun( ppib, &keyRun );
	pttarraySLVChecksumLengths->EndRun( ppib, &lengthRun );

	CallS( slvavailIterator.ErrTerm() );
	CallS( slvownermapIterator.ErrTerm() );

	*pfSLVSpaceTreesCorrupt = fCorrupted;

	return err;
	}

#endif	//	DISABLE_SLV


//  ================================================================
LOCAL ERR ErrREPAIRStartCheckAllTables(
	PIB * const ppib,
	const IFMP ifmp,
	TASKMGR * const ptaskmgr,
	REPAIRTABLE ** const pprepairtable,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVAvail,	
	TTARRAY	* const pttarraySLVOwnerMapColumnid,	
	TTARRAY	* const pttarraySLVOwnerMapKey,	
	TTARRAY * const pttarraySLVChecksumLengths,	
	BOOL * const pfDbtimeTooLarge,
	INTEGGLOBALS * const pintegglobals,
	const REPAIROPTS * const popts )
//  ================================================================
//
//-
	{
	ERR		err				= JET_errSuccess;
	FUCB	*pfucbCatalog 	= pfucbNil;
	DATA	data;
	BOOL	fCorruptionSeen	= fFalse;	//  true if we have seen corruption in the catalog itself

	pintegglobals->fCorruptionSeen 				= fFalse;
	pintegglobals->err							= JET_errSuccess;
	pintegglobals->pprepairtable				= pprepairtable;
	pintegglobals->pttarrayOwnedSpace			= pttarrayOwnedSpace;
	pintegglobals->pttarrayAvailSpace			= pttarrayAvailSpace;
	pintegglobals->pttarraySLVAvail				= pttarraySLVAvail;
	pintegglobals->pttarraySLVOwnerMapColumnid	= pttarraySLVOwnerMapColumnid;
	pintegglobals->pttarraySLVOwnerMapKey 		= pttarraySLVOwnerMapKey;	
	pintegglobals->pttarraySLVChecksumLengths	= pttarraySLVChecksumLengths;
	pintegglobals->pfDbtimeTooLarge 			= pfDbtimeTooLarge;
	pintegglobals->popts						= popts;

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	
	Call( ErrCATOpen( ppib, ifmp, &pfucbCatalog ) );
	Assert( pfucbNil != pfucbCatalog );
	FUCBSetSequential( pfucbCatalog );
	FUCBSetPrereadForward( pfucbCatalog, cpgPrereadSequential );

	Call( ErrIsamSetCurrentIndex( ppib, pfucbCatalog, szMSORootObjectsIndex ) );

	//  check the large tables first for maximum overlap
	
	INT isz;
	for( isz = 0; isz < cszLargeTables && !fCorruptionSeen; ++isz )
		{
		const BYTE	bTrue		= 0xff;	
		Call( ErrIsamMakeKey(
					ppib,
					pfucbCatalog,
					&bTrue,
					sizeof(bTrue),
					JET_bitNewKey ) );
		Call( ErrIsamMakeKey(
					ppib,
					pfucbCatalog,
					(BYTE *)rgszLargeTables[isz],
					(ULONG)strlen( rgszLargeTables[isz] ),
					NO_GRBIT ) );
		err = ErrIsamSeek( ppib, pfucbCatalog, JET_bitSeekEQ );
		if ( JET_errRecordNotFound == err )
			{
			//  This large table not present in this database
			continue;
			}
		Call( err );
		Call( ErrDIRGet( pfucbCatalog ) );
		Call( ErrREPAIRPostTableTask(
				ppib,
				ifmp,
				pfucbCatalog,
				rgszLargeTables[isz],
				pprepairtable,
				pintegglobals,
				ptaskmgr,
				&fCorruptionSeen,
				popts ) );
		}


	err = ErrIsamMove( ppib, pfucbCatalog, JET_MoveFirst, 0 );
	while ( err >= 0 )
		{
		Call( ErrDIRGet( pfucbCatalog ) );

		CHAR szTable[JET_cbNameMost+1];
		Assert( FVarFid( fidMSO_Name ) );
		Call( ErrRECIRetrieveVarColumn(
					pfcbNil,
					pfucbCatalog->u.pfcb->Ptdb(),
					fidMSO_Name,
					pfucbCatalog->kdfCurr.data,
					&data ) );
		CallS( err );
		if( data.Cb() > JET_cbNameMost  )
			{
			(*popts->pcprintfError)( "catalog corruption (MSO_Name): name too long: expected no more than %d bytes, got %d\r\n",
				JET_cbNameMost, data.Cb() );	
			fCorruptionSeen = fTrue;
			Call( ErrDIRRelease( pfucbCatalog ) );
			}
		else
			{
			UtilMemCpy( szTable, data.Pv(), data.Cb() );
			szTable[data.Cb()] = 0;

			//  if this is a catalog or a large table it will
			//  already have been checked
			
			if( !FCATSystemTable( szTable )
				&& !FIsLargeTable( szTable ) )
				{			
				Call( ErrREPAIRPostTableTask(
						ppib,
						ifmp,
						pfucbCatalog,
						szTable,
						pprepairtable,
						pintegglobals,
						ptaskmgr,
						&fCorruptionSeen,
						popts ) );
				}
			else
				{
				Call( ErrDIRRelease( pfucbCatalog ) );
				}
			}

		err = ErrIsamMove( ppib, pfucbCatalog, JET_MoveNext, 0 );
		}
	if ( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}


HandleError:
	if( pfucbNil != pfucbCatalog )
		{
		CallS( ErrFILECloseTable( ppib, pfucbCatalog ) );
		}
	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );

	if( JET_errSuccess == err
		&& fCorruptionSeen )
		{
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRStopCheckTables( const INTEGGLOBALS * const pintegglobals, BOOL * const pfCorrupt )
//  ================================================================
	{
	*pfCorrupt = pintegglobals->fCorruptionSeen;
	return pintegglobals->err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRPostTableTask(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucbCatalog,
	const CHAR * const szTable,
	REPAIRTABLE ** const pprepairtable,
	INTEGGLOBALS * const pintegglobals,
	TASKMGR * const ptaskmgr,
	BOOL * const pfCorrupted,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Takes a latched FUCB, returns an unlatched FUCB
//
//-
	{
	ERR err = JET_errSuccess;
	DATA data;
	
	PGNO pgnoFDP;
	CPG  cpgExtent;
		
	Assert( FFixedFid( fidMSO_PgnoFDP ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_PgnoFDP,
				pfucbCatalog->kdfCurr.data,
				&data ) );
	CallS( err );
	if( sizeof( PGNO ) != data.Cb() )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_PgnoFDP): unexpected size: expected %d bytes, got %d\r\n",
			sizeof( PGNO ), data.Cb() );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}
	pgnoFDP = *( (UnalignedLittleEndian< PGNO > *)data.Pv() );
	if( pgnoNull == pgnoFDP )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_PgnoFDP): pgnoFDP is NULL\r\n" );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}

	Assert( FFixedFid( fidMSO_Pages ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_Pages,
				pfucbCatalog->kdfCurr.data,
				&data ) );
	CallS( err );
	if( sizeof( CPG ) != data.Cb() )
		{
		(*popts->pcprintfError)( "catalog corruption (fidMSO_Pages): unexpected size: expected %d bytes, got %d\r\n",
			sizeof( CPG ), data.Cb() );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}
	cpgExtent = max( 1, (INT) *( (UnalignedLittleEndian< CPG > *)data.Pv() ) );
	
	Assert( FFixedFid( fidMSO_Type ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_Type,
				pfucbCatalog->kdfCurr.data,
				&data ) );
	CallS( err );
	if( sizeof( SYSOBJ ) != data.Cb() )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_Type): unexpected size: expected %d bytes, got %d\r\n",
			sizeof( SYSOBJ ), data.Cb() );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}
	if( sysobjTable != *( (UnalignedLittleEndian< SYSOBJ > *)data.Pv() ) )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_Type): unexpected type: expected type %d, got type %d\r\n",
			sysobjTable, *( (UnalignedLittleEndian< SYSOBJ > *)data.Pv() ) );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}
			
#ifdef DEBUG
	Assert( FVarFid( fidMSO_Name ) );
	Call( ErrRECIRetrieveVarColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_Name,
				pfucbCatalog->kdfCurr.data,
				&data ) );
	CallS( err );
	Assert( 0 == _strnicmp( szTable, reinterpret_cast<CHAR *>( data.Pv() ), data.Cb() ) );
#endif	//	DEBUG

	OBJID objidTable;
	Assert( FFixedFid( fidMSO_Id ) );
	Call( ErrRECIRetrieveFixedColumn(
				pfcbNil,
				pfucbCatalog->u.pfcb->Ptdb(),
				fidMSO_Id,
				pfucbCatalog->kdfCurr.data,
				&data ) );
	CallS( err );
	if( sizeof( objidTable ) != data.Cb() )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_Id): unexpected size: expected %d bytes, got %d\r\n",
			sizeof( objidTable ), data.Cb() );	
		*pfCorrupted = fTrue;
		goto HandleError;
		}

	objidTable = *(UnalignedLittleEndian< OBJID > *)data.Pv();

	if( objidInvalid == objidTable )
		{
		(*popts->pcprintfError)( "catalog corruption (MSO_Id): objidInvalid (%d) reached.\r\n", objidInvalid );	
		*pfCorrupted = fTrue;
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
	Call( ErrDIRRelease( pfucbCatalog ) );

		{
		CHECKTABLE * const pchecktable 	= new CHECKTABLE;
		if( NULL == pchecktable )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
			
		pchecktable->ifmp 				= ifmp;
		strcpy(pchecktable->szTable, szTable );
		pchecktable->szIndex[0]			= 0;
		pchecktable->objidFDP 			= objidTable;
		pchecktable->pgnoFDP 			= pgnoFDP;
		pchecktable->objidParent		= objidSystemRoot;
		pchecktable->pgnoFDPParent		= pgnoSystemRoot;
		pchecktable->fPageFlags			= 0;
		pchecktable->fUnique			= fTrue;
		pchecktable->preccheck			= NULL;
		pchecktable->cpgPrimaryExtent	= cpgExtent;
		pchecktable->pglobals			= pintegglobals;
		pchecktable->fDeleteWhenDone	= fTrue;

		err = ptaskmgr->ErrPostTask( REPAIRCheckOneTableTask, (ULONG_PTR)pchecktable );
		if( err < 0 )
			{
			Assert( JET_errOutOfMemory == err );
			//  we were not able to post this
			delete pchecktable;
			Call( err );
			}
		}

	return err;

HandleError:
	Call( ErrDIRRelease( pfucbCatalog ) );
	return err;
	}


//  ================================================================
LOCAL VOID REPAIRCheckTreeAndSpaceTask( PIB * const ppib, const ULONG_PTR ul )
//  ================================================================
	{
	TASKMGR::TASK task = REPAIRCheckTreeAndSpaceTask;	// should only compile if the signatures match

	CHECKTABLE * const pchecktable = (CHECKTABLE *)ul;

	CallS( ErrDIRBeginTransaction(ppib, NO_GRBIT ) );
	
	Ptls()->szCprintfPrefix = pchecktable->szTable;
	
	ERR err = ErrREPAIRCheckTreeAndSpace(
						ppib,
						pchecktable->ifmp,
						pchecktable->objidFDP,
						pchecktable->pgnoFDP,
						pchecktable->objidParent,
						pchecktable->pgnoFDPParent,
						pchecktable->fPageFlags,
						pchecktable->fUnique,
						pchecktable->preccheck,
						pchecktable->pglobals->pttarrayOwnedSpace,
						pchecktable->pglobals->pttarrayAvailSpace,
						pchecktable->pglobals->pfDbtimeTooLarge,
						pchecktable->pglobals->popts );

	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
		case JET_errSLVSpaceCorrupted:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}

	if( JET_errDatabaseCorrupted == err )
		{
		//  we just need to set this, it will never be unset
		pchecktable->pglobals->fCorruptionSeen = fTrue;
		}
	else if( err < 0 )
		{
		//  we'll just keep the last non-corrupting error
		pchecktable->pglobals->err = err;
		}

	Ptls()->szCprintfPrefix = "NULL";

	if( pchecktable->fDeleteWhenDone )
		{
		delete pchecktable;
		}
	else
		{
		pchecktable->signal.Set();
		}

	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	}


//  ================================================================
LOCAL VOID REPAIRCheckOneTableTask( PIB * const ppib, const ULONG_PTR ul )
//  ================================================================
	{
	TASKMGR::TASK task = REPAIRCheckOneTableTask;	// should only compile if the signatures match

	REPAIRTABLE * prepairtable = NULL;
	CHECKTABLE * const pchecktable = (CHECKTABLE *)ul;

	CallS( ErrDIRBeginTransaction(ppib, NO_GRBIT ) );
	
	Ptls()->szCprintfPrefix = pchecktable->szTable;
	
	const ERR err = ErrREPAIRCheckOneTable(
						ppib,
						pchecktable->ifmp,
						pchecktable->szTable,
						pchecktable->objidFDP,
						pchecktable->pgnoFDP,
						pchecktable->cpgPrimaryExtent,
						&prepairtable,
						pchecktable->pglobals->pttarrayOwnedSpace,
						pchecktable->pglobals->pttarrayAvailSpace,
						pchecktable->pglobals->pttarraySLVAvail,
						pchecktable->pglobals->pttarraySLVOwnerMapColumnid,
						pchecktable->pglobals->pttarraySLVOwnerMapKey,
						pchecktable->pglobals->pttarraySLVChecksumLengths,
						pchecktable->pglobals->pfDbtimeTooLarge,
						pchecktable->pglobals->popts );

	if( JET_errDatabaseCorrupted == err )
		{
		//  we just need to set this, it will never be unset
		pchecktable->pglobals->fCorruptionSeen = fTrue;
		}
	else if( err < 0 )
		{
		//  we'll just keep the last non-corrupting error
		pchecktable->pglobals->err = err;
		}

	if( NULL != prepairtable )
		{
		pchecktable->pglobals->crit.Enter();
		prepairtable->prepairtableNext = *(pchecktable->pglobals->pprepairtable);
		*(pchecktable->pglobals->pprepairtable) = prepairtable;
		pchecktable->pglobals->crit.Leave();
		}


	Ptls()->szCprintfPrefix = "NULL";

	if( pchecktable->fDeleteWhenDone )
		{
		delete pchecktable;
		}
	else
		{
		pchecktable->signal.Set();
		}

	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	}


//  ================================================================
LOCAL BOOL FREPAIRTableHasSLVColumn( const FCB * const pfcb )
//  ================================================================
	{
	return pfcb->Ptdb()->FTableHasSLVColumn();
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckOneTable(
	PIB * const ppib,
	const IFMP ifmp,
	const char * const szTable,
	const OBJID objidTable,
	const PGNO pgnoFDP,
	const CPG cpgPrimaryExtent,
	REPAIRTABLE ** const pprepairtable,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	TTARRAY * const pttarraySLVAvail,	
	TTARRAY	* const pttarraySLVOwnerMapColumnid,	
	TTARRAY	* const pttarraySLVOwnerMapKey,	
	TTARRAY * const pttarraySLVChecksumLengths,	
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR		err;
	
	FUCB	*pfucbTable = pfucbNil;
	
	FCB		*pfcbTable	= pfcbNil;
	FCB		*pfcbIndex	= pfcbNil;

	PGNO 	pgnoLV 		= pgnoNull;
	OBJID	objidLV		= objidNil;

	BOOL		fRepairTable		= fTrue;
	BOOL		fRepairLV			= fTrue;
	BOOL		fRepairIndexes		= fTrue;
	BOOL		fRepairLVRefcounts	= fTrue;

	const ULONG timerStart = TickOSTimeCurrent();

	//  preread the first extent
	
	const CPG cpgToPreread = min( rgfmp[ifmp].PgnoLast() - pgnoFDP + 1,
								max( cpgPrimaryExtent, g_cpgMinRepairSequentialPreread ) );
	BFPrereadPageRange( ifmp, pgnoFDP, cpgToPreread );

	//  do not pass in the pgnoFDP (forces lookup of the objid)
	
	err = ErrFILEOpenTable( ppib, ifmp, &pfucbTable, szTable, NO_GRBIT );	

	FIDLASTINTDB fidLastInTDB;
	memset( &fidLastInTDB, 0, sizeof(fidLastInTDB) );
	if( err >= 0)
		{
		Assert( pfucbNil != pfucbTable );
		pfcbTable 	= pfucbTable->u.pfcb;	

		fidLastInTDB.fidFixedLastInTDB = pfcbTable->Ptdb()->FidFixedLast(); 
		fidLastInTDB.fidVarLastInTDB = pfcbTable->Ptdb()->FidVarLast();
		fidLastInTDB.fidTaggedLastInTDB = pfcbTable->Ptdb()->FidTaggedLast();
		}

	TTMAP ttmapLVRefcountsFromTable( ppib );
	TTMAP ttmapLVRefcountsFromLV( ppib );
	
	RECCHECKTABLE 	recchecktable(
						objidTable,
						pfucbTable,
						fidLastInTDB,
						&ttmapLVRefcountsFromTable,
						pttarraySLVAvail,
						pttarraySLVOwnerMapColumnid,	
						pttarraySLVOwnerMapKey,	
						pttarraySLVChecksumLengths,
						popts );

	Call( err );
	

	//  preread the indexes of the table
	
	REPAIRIPrereadIndexesOfFCB( pfcbTable );

	//  check for a LV tree
	
	Call( ErrCATAccessTableLV( ppib, ifmp, objidTable, &pgnoLV, &objidLV ) );
	if( pgnoNull != pgnoLV )
		{
		const CPG cpgToPrereadLV = min( rgfmp[ifmp].PgnoLast() - pgnoLV + 1,
								max( cpgLVTree, g_cpgMinRepairSequentialPreread ) );
		BFPrereadPageRange( ifmp, pgnoLV, cpgToPrereadLV );
		Call( ttmapLVRefcountsFromLV.ErrInit( PinstFromPpib( ppib ) ) );
		}

	Call( ttmapLVRefcountsFromTable.ErrInit( PinstFromPpib( ppib ) ) );

	(*popts->pcprintfVerbose)( "checking table \"%s\" (%d)\r\n", szTable, objidTable );
	(*popts->pcprintfVerbose).Indent();

	(*popts->pcprintfStats).Indent();

	Call( ErrREPAIRCheckSpace(
			ppib,
			ifmp,
			objidTable,
			pgnoFDP,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPagePrimary,
			fTrue,
			&recchecktable,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			pfDbtimeTooLarge,
			popts ) );

	//  check the long-value tree if it exists
	//	CONSIDER:  multi-thread this by checking the LV tree and data trees in separate threads
	
	if( pgnoNull != pgnoLV )
		{
		LVSTATS lvstats;
		memset( &lvstats, 0, sizeof( lvstats ) );
		lvstats.cbLVMin = LONG_MAX;
		RECCHECKLV 		recchecklv( ttmapLVRefcountsFromLV, popts );
		RECCHECKLVSTATS	recchecklvstats( &lvstats );
		RECCHECKMACRO	reccheckmacro;
		reccheckmacro.Add( &recchecklvstats );
		reccheckmacro.Add( &recchecklv );	// put this last so that warnings are returned
		(*popts->pcprintfVerbose)( "checking long value tree (%d)\r\n", objidLV );
		(*popts->pcprintfStats)( "\r\n" );
		(*popts->pcprintfStats)( "===== long value tree =====\r\n" );
		Call( ErrREPAIRCheckTreeAndSpace(
				ppib,
				ifmp,
				objidLV,
				pgnoLV,
				objidTable,
				pgnoFDP,
				CPAGE::fPageLongValue,
				fTrue,
				&reccheckmacro, 
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				pfDbtimeTooLarge,
				popts ) );
		}
	else
		{
		fRepairLV = fFalse;
		}
		
	//  check the main table
	
	(*popts->pcprintfStats)( "\r\n\r\n" );
	(*popts->pcprintfStats)( "===== table \"%s\" =====\r\n", szTable );

	(*popts->pcprintfVerbose)( "checking data\r\n" );
	Call( ErrREPAIRCheckTree(
			ppib,
			ifmp,
			objidTable,
			pgnoFDP,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPagePrimary,
			fTrue,
			&recchecktable,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			pfDbtimeTooLarge,
			popts ) );
	fRepairTable = fFalse;

	// Until now, we can safely say that long-Value tree  
	// don't need to be rebuilt 
	fRepairLV = fFalse;

	//  compare LV refcounts found in the table to LV refcounts found in the LV tree

	if( pgnoNull != pgnoLV )
		{
		(*popts->pcprintfVerbose)( "checking LV refcounts\r\n" );
		Call( ErrREPAIRCompareLVRefcounts( ppib, ifmp, ttmapLVRefcountsFromTable, ttmapLVRefcountsFromLV, popts ) ); 
		}
	else
		{
		// LV tree does not exist, LV refcounts found in the table should be 0
		err = ttmapLVRefcountsFromTable.ErrMoveFirst();
		if( JET_errNoCurrentRecord != err ) 
			{
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		else
			{ // expected result from a good table
			err = JET_errSuccess;
			}
		}

	fRepairLVRefcounts	= fFalse;

	//	check all secondary indexes
	
	for(
		pfcbIndex = pfucbTable->u.pfcb->PfcbNextIndex();
		pfcbNil != pfcbIndex;
		pfcbIndex = pfcbIndex->PfcbNextIndex() )
		{
		RECCHECKNULL 	recchecknull;
		const CHAR * const szIndexName 	= pfucbTable->u.pfcb->Ptdb()->SzIndexName( pfcbIndex->Pidb()->ItagIndexName(), pfcbIndex->FDerivedIndex() );
		const OBJID objidIndex = pfcbIndex->ObjidFDP();
		(*popts->pcprintfVerbose)( "checking index \"%s\" (%d)\r\n", szIndexName, objidIndex );
		(*popts->pcprintfStats)( "\r\n" );
		(*popts->pcprintfStats)( "===== index \"%s\" =====\r\n", szIndexName );
			
		Call( ErrREPAIRCheckTreeAndSpace(
				ppib,
				ifmp,
				pfcbIndex->ObjidFDP(),
				pfcbIndex->PgnoFDP(),
				objidTable,
				pgnoFDP,
				CPAGE::fPageIndex,
				pfcbIndex->Pidb()->FUnique(),
				&recchecknull,
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				pfDbtimeTooLarge,
				popts ) );
		}

	if( !( popts->grbit & JET_bitDBUtilOptionDontBuildIndexes ) )
		{
		if ( pfucbTable->u.pfcb->PfcbNextIndex() != pfcbNil )
			{
			(*popts->pcprintfVerbose)( "rebuilding and comparing indexes\r\n" );
			DIRUp( pfucbTable );

#ifdef PARALLEL_BATCH_INDEX_BUILD
			ULONG	cIndexes	= 0;
			for ( FCB * pfcbT = pfucbTable->u.pfcb->PfcbNextIndex(); pfcbNil != pfcbT; pfcbT = pfcbT->PfcbNextIndex() )
				{
				cIndexes++;
				}
#else
			const ULONG	cIndexes	= cFILEIndexBatchSizeDefault;
#endif

			Call( ErrFILEBuildAllIndexes(
					ppib,
					pfucbTable,
					pfucbTable->u.pfcb->PfcbNextIndex(),
					NULL,
					cIndexes,
					fTrue,
					popts->pcprintfError ) );
			}
		}

	fRepairIndexes = fFalse;

HandleError:		
	switch( err )
		{
		//  we should never normally get these errors. morph them into corrupted database errors
		case JET_errNoCurrentRecord:
		case JET_errRecordDeleted:
		case JET_errRecordNotFound:
		case JET_errReadVerifyFailure:
		case JET_errPageNotInitialized:
		case JET_errDiskIO:
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			break;
		default:
			break;
		}

	ERR errSaved = err;

	if( JET_errDatabaseCorrupted == err )
		{
		if( fRepairTable )
			{
			(*popts->pcprintfVerbose)( "\tData table will be rebuilt\r\n" );
			}
		if( fRepairLV	)
			{
			(*popts->pcprintfVerbose)( "\tLong-Value table will be rebuilt\r\n" );
			}
		if( fRepairIndexes )
			{
			(*popts->pcprintfVerbose)( "\tIndexes will be rebuilt\r\n" );
			}
		if( fRepairLVRefcounts )
			{
			(*popts->pcprintfVerbose)( "\tLong-Value refcounts will be rebuilt\r\n" );
			}
		}

	if( JET_errDatabaseCorrupted == err && !( popts->grbit & JET_bitDBUtilOptionDontRepair ) )
		{
		//  store all the pgnos that we are interested in
		//  the error handling here is sloppy (running out of memory will cause a leak)
		
		Assert( pfcbNil != pfcbTable );
		
		(*popts->pcprintfVerbose)( "table \"%s\" is corrupted\r\n", szTable );
		VOID * pv = PvOSMemoryHeapAlloc( sizeof( REPAIRTABLE ) );
		if( NULL == pv )
			{
			err = ErrERRCheck( JET_errOutOfMemory );
			goto Abort;
			}
		memset( pv, 0, sizeof( REPAIRTABLE ) );
		
		REPAIRTABLE * prepairtable = new(pv) REPAIRTABLE;

		strcpy( prepairtable->szTableName, szTable );

		prepairtable->objidFDP 	= objidTable;
		prepairtable->pgnoFDP	= pfcbTable->PgnoFDP();
		(*popts->pcprintfVerbose)( "\tdata: %d (%d)\r\n", prepairtable->objidFDP, prepairtable->pgnoFDP );

		Assert( fRepairTable || fRepairLV || fRepairLVRefcounts || fRepairIndexes );
		prepairtable->fRepairTable			= fRepairTable;
		prepairtable->fRepairLV				= fRepairLV;
		prepairtable->fRepairIndexes		= fRepairIndexes;
		prepairtable->fRepairLVRefcounts	= fRepairLVRefcounts;

		if( pgnoNull != pgnoLV )
			{
			Assert( objidNil != objidLV );
			prepairtable->objidLV 	= objidLV;
			prepairtable->pgnoLV	= pgnoLV;
			(*popts->pcprintfVerbose)( "\tlong values: %d (%d)\r\n", prepairtable->objidLV, prepairtable->pgnoLV );
			}
			
		for( pfcbIndex = pfucbTable->u.pfcb;
			pfcbNil != pfcbIndex;
			pfcbIndex = pfcbIndex->PfcbNextIndex() )
			{
			if( !pfcbIndex->FPrimaryIndex() )
				{
				const CHAR * const szIndexName 	= pfucbTable->u.pfcb->Ptdb()->SzIndexName( pfcbIndex->Pidb()->ItagIndexName(), pfcbIndex->FDerivedIndex() );
				CallJ( prepairtable->objidlistIndexes.ErrAddObjid( pfcbIndex->ObjidFDP() ), Abort );
				(*popts->pcprintfVerbose)( "\tindex \"%s\": %d (%d)\r\n", szIndexName, pfcbIndex->ObjidFDP(),pfcbIndex->PgnoFDP() );
				}
			else if( pfcbIndex->Pidb() != pidbNil )
				{
				prepairtable->fHasPrimaryIndex = fTrue;
				(*popts->pcprintfVerbose)( "\tprimary index\r\n" );
				}			
			}

		prepairtable->fTableHasSLV 		= FREPAIRTableHasSLVColumn( pfcbTable );

		if( prepairtable->fTableHasSLV )
			{
			(*popts->pcprintfVerbose)( "\tSLV column\r\n" );			
			}
			
		prepairtable->fTemplateTable 	= pfcbTable->FTemplateTable();
		prepairtable->fDerivedTable 	= pfcbTable->FDerivedTable();
		
		prepairtable->objidlistIndexes.Sort();
		prepairtable->prepairtableNext = *pprepairtable;
		*pprepairtable = prepairtable;
		}

Abort:
	if ( pfucbNil != pfucbTable )
		{
///		FCB * const pfcbTable = pfucbTable->u.pfcb;
		
		CallS( ErrFILECloseTable( ppib, pfucbTable ) );

		//  BUGFIX: purge the FCB to avoid confusion with other tables/indexes with have
		//  the same pgnoFDP (corrupted catalog)

		//	BUGFIX: callbacks can open this table so we can no longer guarantee that 
		//	the FCB is not referenced.
		//	UNDONE: get a better way to avoid duplicate pgnoFDP problems
		
///		if( pfcbTable && !pfcbTable->FTemplateTable() )
///			{
///			pfcbTable->PrepareForPurge();
///			pfcbTable->Purge();
///			}
		}

	(*popts->pcprintfVerbose).Unindent();
	(*popts->pcprintfStats).Unindent();

	const ULONG timerEnd 	= TickOSTimeCurrent();
	const ULONG csecElapsed = ( ( timerEnd - timerStart ) / 1000 );

	err = ( JET_errSuccess == err ) ? errSaved : err;

	(*popts->pcprintfVerbose)( "integrity check of table \"%s\" (%d) finishes with error %d after %d seconds\r\n",
		szTable,
		objidTable,
		err,
		csecElapsed );

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCompareLVRefcounts(
	PIB * const ppib,
	const IFMP ifmp,
	TTMAP& ttmapLVRefcountsFromTable,
	TTMAP& ttmapLVRefcountsFromLV,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	BOOL fNoMoreRefcountsFromTable 	= fFalse;
	BOOL fNoMoreRefcountsFromLV 	= fFalse;
	BOOL fSawError					= fFalse;

	err = ttmapLVRefcountsFromTable.ErrMoveFirst();
	if( JET_errNoCurrentRecord == err )
		{
		fNoMoreRefcountsFromTable = fTrue;
		err = JET_errSuccess;
		}
	Call( err );

	err = ttmapLVRefcountsFromLV.ErrMoveFirst();
	if( JET_errNoCurrentRecord == err )
		{
		fNoMoreRefcountsFromLV = fTrue;
		err = JET_errSuccess;
		}
	Call( err );

	//  repeat while we have at least one set of refcounts
	
	while( 	!fNoMoreRefcountsFromTable
			|| !fNoMoreRefcountsFromLV )
		{
		ULONG lidFromTable;
		ULONG ulRefcountFromTable;
		if( !fNoMoreRefcountsFromTable )
			{
			Call( ttmapLVRefcountsFromTable.ErrGetCurrentKeyValue( &lidFromTable, &ulRefcountFromTable ) );
			}
		else
			{
			lidFromTable 		= 0xffffffff;
			ulRefcountFromTable = 0;
			}

		ULONG lidFromLV;
		ULONG ulRefcountFromLV;
		if( !fNoMoreRefcountsFromLV )
			{
			Call( ttmapLVRefcountsFromLV.ErrGetCurrentKeyValue( &lidFromLV, &ulRefcountFromLV ) );
			}
		else
			{
			lidFromLV 			= 0xffffffff;
			ulRefcountFromLV	= 0;
			}

		if( lidFromTable > lidFromLV )
			{
			
			//  we see a LID in the LV-tree that is not referenced in the table
			//  its an orphaned LV. WARNING

			(*popts->pcprintfWarning)( "orphaned LV (lid %d, refcount %d)\r\n", lidFromLV, ulRefcountFromLV );
			
			}
		else if( lidFromLV > lidFromTable )
			{
			
			//  we see a LID in the table that doesn't exist in the LV tree. ERROR

			(*popts->pcprintfError)( "record references non-existant LV (lid %d, refcount %d)\r\n",
									lidFromTable, ulRefcountFromTable );
			fSawError = fTrue;
			}
		else if( ulRefcountFromTable > ulRefcountFromLV )
			{
			Assert( lidFromTable == lidFromLV );
			
			//  the refcount in the LV tree is too low. ERROR

			(*popts->pcprintfError)( "LV refcount too low (lid %d, refcount %d, correct refcount %d)\r\n",
										lidFromLV, ulRefcountFromLV, ulRefcountFromTable );

			fSawError = fTrue;
			}
		else if( ulRefcountFromLV > ulRefcountFromTable )
			{
			Assert( lidFromTable == lidFromLV );

			//  the refcount in the LV tree is too high. WARNING

			(*popts->pcprintfWarning)( "LV refcount too high (lid %d, refcount %d, correct refcount %d)\r\n",
										lidFromLV, ulRefcountFromLV, ulRefcountFromTable );
			
			}
		else
			{

			//  perfect match. no error
			
			Assert( lidFromTable == lidFromLV );
			Assert( ulRefcountFromTable == ulRefcountFromLV );
			Assert( 0xffffffff != lidFromLV );
			}

		if( lidFromTable <= lidFromLV )
			{
			err = ttmapLVRefcountsFromTable.ErrMoveNext();
			if( JET_errNoCurrentRecord == err )
				{
				fNoMoreRefcountsFromTable = fTrue;
				err = JET_errSuccess;
				}
			Call( err );
			}
		if( lidFromLV <= lidFromTable )
			{
			err = ttmapLVRefcountsFromLV.ErrMoveNext();
			if( JET_errNoCurrentRecord == err )
				{
				fNoMoreRefcountsFromLV = fTrue;
				err = JET_errSuccess;
				}
			Call( err );
			
			}
		}

HandleError:		
	if( JET_errSuccess == err && fSawError )
		{
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCreateTempTables(
	PIB * const ppib,
	const BOOL fRepairGlobalSpace,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR	err = JET_errSuccess;
		
	JET_COLUMNDEF	rgcolumndef[3] = {
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypNil, 0, 0, 0, 0, JET_cbKeyMost, JET_bitColumnTTKey },
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypNil, 0, 0, 0, 0, JET_cbKeyMost, JET_bitColumnTTKey },
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypNil, 0, 0, 0, 0, JET_cbKeyMost, JET_bitColumnTTKey }
		};	
	
	//  BadPages
	rgcolumndef[0].coltyp = JET_coltypLong;	// Pgno	
	Call( ErrIsamOpenTempTable(
		reinterpret_cast<JET_SESID>( ppib ),
		rgcolumndef,
		1,
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable | JET_bitTTUpdatable,
		&prepairtt->tableidBadPages,
		prepairtt->rgcolumnidBadPages ) );

	//  Owned
	rgcolumndef[0].coltyp = JET_coltypLong;	// ObjidFDP
	rgcolumndef[1].coltyp = JET_coltypLong;	// Pgno
	Call( ErrIsamOpenTempTable(
		reinterpret_cast<JET_SESID>( ppib ),
		rgcolumndef,
		2,
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable| JET_bitTTUpdatable,
		&prepairtt->tableidOwned,
		prepairtt->rgcolumnidOwned ) );

	//  Used
	rgcolumndef[0].coltyp = JET_coltypLong;			// ObjidFDP
	rgcolumndef[1].coltyp = JET_coltypLongBinary;	// Key
	rgcolumndef[2].grbit  = NO_GRBIT;				// allows us to catch duplicates (same objid/key)
	rgcolumndef[2].coltyp = JET_coltypLong;			// Pgno
	Call( ErrIsamOpenTempTable(
		reinterpret_cast<JET_SESID>( ppib ),
		rgcolumndef,
		3,
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable| JET_bitTTUpdatable,
		&prepairtt->tableidUsed,
		prepairtt->rgcolumnidUsed ) );
	rgcolumndef[2].grbit  = rgcolumndef[0].grbit;

	//  Available
	rgcolumndef[0].coltyp = JET_coltypLong;	// ObjidFDP
	rgcolumndef[1].coltyp = JET_coltypLong;	// Pgno
	Call( ErrIsamOpenTempTable(
		reinterpret_cast<JET_SESID>( ppib ),
		rgcolumndef,
		2,
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable| JET_bitTTUpdatable,
		&prepairtt->tableidAvailable,
		prepairtt->rgcolumnidAvailable ) );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRScanDB(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTT * const prepairtt,
	DBTIME * const pdbtimeLast,
	OBJID  * const pobjidLast,
	PGNO   * const ppgnoLastOESeen,
	const REPAIRTABLE * const prepairtable,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	const JET_SESID sesid	= reinterpret_cast<JET_SESID>( ppib );
	ERR err = JET_errSuccess;

	const CPG cpgPreread = 256;
	CPG cpgRemaining;

	CPG	cpgUninitialized 	= 0;
	CPG	cpgBad 				= 0;
	
	const PGNO	pgnoFirst 	= 1;
	const PGNO	pgnoLast	= PgnoLast( ifmp );
	PGNO	pgno			= pgnoFirst;

	(*popts->pcprintfVerbose)( "scanning the database from page %d to page %d\r\n", pgnoFirst, pgnoLast );		

	popts->psnprog->cunitTotal = pgnoLast;
	popts->psnprog->cunitDone = 0;
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntBegin, NULL );	
	
	BFPrereadPageRange( ifmp, pgnoFirst, min(cpgPreread * 2,pgnoLast-1) );
	cpgRemaining = cpgPreread;

	while( pgnoLast	>= pgno )
		{
		if( 0 == --cpgRemaining )
			{
			if( ( pgno + ( cpgPreread * 2 ) ) < pgnoLast )
				{
				BFPrereadPageRange( ifmp, pgno + cpgPreread, cpgPreread );
				}
			popts->psnprog->cunitDone = pgno;
			(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
			cpgRemaining = cpgPreread;
			}
			
		CSR	csr;
		err = csr.ErrGetReadPage( 
						ppib, 
						ifmp,
						pgno,
						bflfNoTouch );
			
		if( JET_errPageNotInitialized == err )
			{
			//  unused page. ignore it
			++cpgUninitialized;
			err = JET_errSuccess;
			}
		else if( JET_errReadVerifyFailure == err || JET_errDiskIO == err )
			{
			++cpgBad;
			err = CPAGE::ErrResetHeader( ppib, ifmp, pgno );
			if( err < 0 )
				{
				(*popts->pcprintfVerbose)( "error %d resetting page %d (online backup may not work)\r\n", err, pgno );		
				}
			else
				{
				(*popts->pcprintfVerbose)( "error %d reading page %d. the page has been zeroed out so online backup will work\r\n", err, pgno );						
				}
			err = ErrREPAIRInsertBadPageIntoTables( ppib, pgno, prepairtt, prepairtable, popts );
			}
		else if( err >= 0 )
			{
			*ppgnoLastOESeen = max( pgno, *ppgnoLastOESeen );
			
			if( csr.Cpage().Dbtime() > *pdbtimeLast )
				{
				*pdbtimeLast = csr.Cpage().Dbtime();
				}
			if( csr.Cpage().ObjidFDP() > *pobjidLast )
				{
				*pobjidLast = csr.Cpage().ObjidFDP();
				}

			err = ErrREPAIRInsertPageIntoTables(
					ppib,
					ifmp,
					csr,
					prepairtt,
					prepairtable,
					pttarrayOwnedSpace,
					pttarrayAvailSpace,					
					popts );
			}
		csr.ReleasePage( fTrue );
		csr.Reset();
		Call( err );
		++pgno;
		}

	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntComplete, NULL );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRIFixLeafPage(
	PIB * const	ppib,
	const IFMP ifmp,
	CSR& csr,
#ifdef SYNC_DEADLOCK_DETECTION
	COwner* const pownerSaved,
#endif  //  SYNC_DEADLOCK_DETECTION
	const REPAIROPTS * const popts)
//  ================================================================
//
//  Called on leaf pages that are part of tables being repaired
//
//  Currently this just checks leaf pages to make sure that they are
//  usable.
//
//  UNDONE: write-latch the page and fix it
//
//-
	{
	ERR err = JET_errSuccess;
	KEYDATAFLAGS 	rgkdf[2];			
	
	Call( csr.Cpage().ErrCheckPage( popts->pcprintfError ) );

Restart:

	//  check to see if the nodes are in key order		
	
	INT iline;
	for( iline = 0; iline < csr.Cpage().Clines(); iline++ )
		{
		csr.SetILine( iline );
		const INT ikdfCurr = iline % 2;
		const INT ikdfPrev = ( iline + 1 ) % 2;

		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), rgkdf + ikdfCurr );
		Call( ErrREPAIRICheckNode(
				csr.Pgno(),
				csr.ILine(),
				(BYTE *)csr.Cpage().PvBuffer(),
				rgkdf[ikdfCurr],
				popts ) );

		if( !csr.Cpage().FLongValuePage()
			&& !csr.Cpage().FIndexPage()
			&& !csr.Cpage().FSLVOwnerMapPage()
			&& !csr.Cpage().FSLVAvailPage() )
			{
			err = ErrREPAIRICheckRecord(
					csr.Pgno(),
					csr.ILine(),
					(BYTE *)csr.Cpage().PvBuffer(),
					rgkdf[ikdfCurr],
					popts );
			if( JET_errDatabaseCorrupted == err )
				{
				
				//	delete this record
				
#ifdef SYNC_DEADLOCK_DETECTION
				if( pownerSaved )
					{
					Pcls()->pownerLockHead = pownerSaved;
					}
#endif  //  SYNC_DEADLOCK_DETECTION

				// the page should have been read/write latched
				Assert( FBFReadLatched( ifmp, csr.Pgno() )
						|| FBFWriteLatched( ifmp, csr.Pgno() ) );

				if( !FBFWriteLatched( ifmp, csr.Pgno() ) )
					{
					err = csr.ErrUpgrade( );
					//Only one thread should latch the page
					Assert( errBFLatchConflict != err );
					Call( err );
					csr.Dirty();
					}

				csr.Cpage().Delete( csr.ILine() );

				goto Restart;
				}
			}
			
		if( rgkdf[ikdfCurr].key.prefix.Cb() == 1 )
			{
			(*popts->pcprintfError)( "node [%d:%d]: incorrectly compressed key\r\n", csr.Pgno(), csr.ILine() );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}

		if( iline > 0 )
			{
			//  this routine should only be called on the clustered index or LV tree
			//  just compare the keys
			
			const INT cmp = CmpKey( rgkdf[ikdfPrev].key, rgkdf[ikdfCurr].key );
			if( cmp > 0 )
				{
				(*popts->pcprintfError)( "node [%d:%d]: nodes out of order on leaf page\r\n", csr.Pgno(), csr.ILine() );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			else if( 0 == cmp )
				{
				(*popts->pcprintfError)( "node [%d:%d]: illegal duplicate key on leaf page\r\n", csr.Pgno(), csr.ILine() );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			}
		}

HandleError:

#ifdef SYNC_DEADLOCK_DETECTION
	if( pownerSaved )
		{
		Pcls()->pownerLockHead = NULL;
		}
#endif  //  SYNC_DEADLOCK_DETECTION

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertOwned(
	const JET_SESID sesid,
	const OBJID objidOwning,
	const PGNO pgno,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

#ifdef REPAIR_DEBUG_VERBOSE_SPACE
	(*popts->pcprintfDebug)( "Inserting page %d (objidOwning: %d) into Owned table\r\n", pgno, objidOwning );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

	Call( ErrDispPrepareUpdate( sesid, prepairtt->tableidOwned, JET_prepInsert ) );
	Call( ErrDispSetColumn(		//  ObjidFDP
		sesid, 
		prepairtt->tableidOwned, 
		prepairtt->rgcolumnidOwned[0],
		(BYTE *)&objidOwning, 
		sizeof( objidOwning ),
		0, 
		NULL ) );
	Call( ErrDispSetColumn(		//  Pgno
		sesid, 
		prepairtt->tableidOwned, 
		prepairtt->rgcolumnidOwned[1],
		(BYTE *)&pgno, 
		sizeof( pgno ),
		0, 
		NULL ) );
	Call( ErrDispUpdate( sesid, prepairtt->tableidOwned, NULL, 0, NULL, 0 ) );
	++(prepairtt->crecordsOwned);

HandleError:
	Assert( err != JET_errKeyDuplicate );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertPageIntoTables(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	REPAIRTT * const prepairtt,
	const REPAIRTABLE * const prepairtable,
	const TTARRAY * const pttarrayOwnedSpace,
	const TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR			err			= JET_errSuccess;
	const PGNO	pgno		= csr.Cpage().Pgno();
	const OBJID	objidFDP	= csr.Cpage().ObjidFDP();
	
	const REPAIRTABLE * prepairtableT = NULL;

	BOOL fOwnedPage		= fFalse;
	BOOL fAvailPage 	= fFalse;
	BOOL fUsedPage		= fFalse;
	OBJID objidInsert		= objidNil;
	OBJID objidInsertParent	= objidNil;

	BOOL	fInTransaction 	= fFalse;

	const JET_SESID sesid	= reinterpret_cast<JET_SESID>( ppib );
	
	prepairtableT = prepairtable;
	while( prepairtableT )
		{
								
		if( csr.Cpage().FLeafPage()
			&& !csr.Cpage().FSpaceTree()
			&& !csr.Cpage().FEmptyPage()
			&& !csr.Cpage().FRepairedPage()
			&& csr.Cpage().Clines() > 0
			&& objidFDP == prepairtableT->objidFDP
			&& csr.Cpage().FPrimaryPage()
			)
			{
			//  leaf page of main tree.  re-used			
			fOwnedPage 		= fTrue;
			fAvailPage		= fFalse;
			fUsedPage		= fTrue;
			
			objidInsert 	= prepairtableT->objidFDP;
			
			break;
			}
			
		else if( csr.Cpage().FLeafPage()
			&& !csr.Cpage().FSpaceTree()
			&& !csr.Cpage().FEmptyPage()
			&& !csr.Cpage().FRepairedPage()
			&& csr.Cpage().Clines() > 0
			&& objidFDP == prepairtableT->objidLV
			&& csr.Cpage().FLongValuePage()
			)
			{
			//  leaf page of LV tree.  re-used			
			fOwnedPage 		= fTrue;
			fAvailPage		= fFalse;
			fUsedPage		= fTrue;
			
			objidInsert			= objidFDP;
			objidInsertParent	= prepairtableT->objidFDP;
			
			break;
			}
			
		else if( prepairtableT->objidlistIndexes.FObjidPresent( objidFDP )
			&& csr.Cpage().FIndexPage())
			{
			//  a secondary index page (non-FDP). discard as we will rebuild the indexes			
			fOwnedPage 		= fTrue;
			fAvailPage		= fTrue;
			fUsedPage		= fFalse;
			
			objidInsert 	= prepairtableT->objidFDP;
			
			break;
			}
			
		else if( objidFDP == prepairtableT->objidFDP
				 || objidFDP == prepairtableT->objidLV )
			{
			//  a internal/space page from the main or LV tree. discard as we rebuild internal pages and the
			//  space tree
			Assert( !csr.Cpage().FLeafPage()
					|| csr.Cpage().FSpaceTree()
					|| csr.Cpage().FEmptyPage()
					|| csr.Cpage().FRepairedPage()
					|| csr.Cpage().Clines() == 0
					);
					
			fOwnedPage 		= fTrue;
			fAvailPage		= fTrue;
			fUsedPage		= fFalse;
			
			objidInsert 	= prepairtableT->objidFDP;
			
			break;
			}
			
		prepairtableT = prepairtableT->prepairtableNext;
		}

	//  Optimization: this is not a page we are interested in
	if( NULL == prepairtableT )
		{
		Assert( !fOwnedPage );
		Assert( !fAvailPage );
		Assert( !fUsedPage );
		return JET_errSuccess;
		}

	Assert( !fUsedPage || fOwnedPage );
	Assert( !fAvailPage || fOwnedPage );
	Assert( !( fUsedPage && fAvailPage ) );
	
#ifdef SYNC_DEADLOCK_DETECTION
	COwner* const pownerSaved = Pcls()->pownerLockHead;
	Pcls()->pownerLockHead = NULL;
#endif  //  SYNC_DEADLOCK_DETECTION

	OBJID objidAvail;
	Call( pttarrayAvailSpace->ErrGetValue( ppib, pgno, &objidAvail ) );
	// getting either objidNil or objidInsert is OK
	if( objidNil != objidAvail 
		&& objidInsert != objidAvail
		&& objidInsertParent != objidAvail
		&& objidSystemRoot != objidAvail )
		{
		(*popts->pcprintfDebug)(
			"page %d (objidFDP: %d, objidInsert: %d) is available to objid %d. ignoring\r\n",
			pgno, objidFDP, objidInsert, objidAvail );			
			
		fOwnedPage	= fFalse;
		fAvailPage	= fFalse;
		fUsedPage	= fFalse;
		err = JET_errSuccess;
		goto HandleError;
		}

	OBJID objidOwning;
	Call( pttarrayOwnedSpace->ErrGetValue( ppib, pgno, &objidOwning ) );
	if( objidInsert != objidOwning
			&& objidFDP != objidOwning )
		{
		(*popts->pcprintfDebug)(
			"page %d (objidFDP: %d, objidInsert: %d) is owned by objid %d. ignoring\r\n",
			pgno, objidFDP, objidInsert, objidOwning );
			
		fOwnedPage	= fFalse;
		fAvailPage	= fFalse;
		fUsedPage	= fFalse;
		err = JET_errSuccess;
		goto HandleError;
		}

	Call( ErrIsamBeginTransaction( sesid, NO_GRBIT ) );
	fInTransaction = fTrue;

	if( fOwnedPage )
		{
		Assert( objidNil != objidInsert );
		Assert( objidInsert != objidInsertParent );
		Assert( fAvailPage || fUsedPage );

		Call( ErrREPAIRInsertOwned( sesid, objidInsert, pgno, prepairtt, popts ) );
		if( objidNil != objidInsertParent )
			{
			Call( ErrREPAIRInsertOwned( sesid, objidInsertParent, pgno, prepairtt, popts ) );
			}
		}
	else
		{
		Assert( !fAvailPage );
		Assert( !fUsedPage );
		}

	Assert( !fAvailPage || objidNil == objidInsertParent );

	if( fAvailPage )
		{
		Assert( !fUsedPage );
		
#ifdef REPAIR_DEBUG_VERBOSE_SPACE
		(*popts->pcprintfDebug)(
			"Inserting page %d (objidFDP: %d, objidInsert: %d) into Available table\r\n",
			pgno, objidFDP, objidInsert );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

		Assert( JET_tableidNil != prepairtt->tableidAvailable );

		Call( ErrDispPrepareUpdate( sesid, prepairtt->tableidAvailable, JET_prepInsert ) );
		Call( ErrDispSetColumn(		//  ObjidInsert
			sesid, 
			prepairtt->tableidAvailable, 
			prepairtt->rgcolumnidAvailable[0],
			(BYTE *)&objidInsert, 
			sizeof( objidInsert ),
			0, 
			NULL ) );
		Call( ErrDispSetColumn(		//  Pgno
			sesid, 
			prepairtt->tableidAvailable, 
			prepairtt->rgcolumnidAvailable[1],
			(BYTE *)&pgno, 
			sizeof( pgno ),
			0, 
			NULL ) );
		Call( ErrDispUpdate( sesid, prepairtt->tableidAvailable, NULL, 0, NULL, 0 ) );
		++(prepairtt->crecordsAvailable);
		}
		
	else if( fUsedPage )
		{
		Assert( csr.Cpage().FLeafPage() );
		Assert( csr.Cpage().Clines() > 0 );
		Assert( objidInsert == objidFDP );

		err = ErrREPAIRIFixLeafPage(
				ppib,
				ifmp,
				csr,
			#ifdef SYNC_DEADLOCK_DETECTION
				pownerSaved,
			#endif  //  SYNC_DEADLOCK_DETECTION
				popts );
		if( JET_errDatabaseCorrupted == err )
			{
			(*popts->pcprintfError)( "page %d: err %d. discarding page\r\n", pgno, err );

			UtilReportEvent(
					eventWarning,
					REPAIR_CATEGORY,
					REPAIR_BAD_PAGE_ID,
					0, NULL );

			//  this page is not usable. skip it
			
			err = JET_errSuccess;
			goto HandleError;
			}
		else if( 0 == csr.Cpage().Clines() )
			{
			(*popts->pcprintfError)( "page %d: all records were bad. discarding page\r\n", pgno );

			UtilReportEvent(
					eventWarning,
					REPAIR_CATEGORY,
					REPAIR_BAD_PAGE_ID,
					0, NULL );

			//  this page is now empty. skip it
			
			err = JET_errSuccess;
			goto HandleError;
			}
		
#ifdef REPAIR_DEBUG_VERBOSE_SPACE
		(*popts->pcprintfDebug)( "Inserting page %d (objidFDP: %d) into Used table\r\n", pgno, objidFDP );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE
		Call( ErrDispPrepareUpdate( sesid, prepairtt->tableidUsed, JET_prepInsert ) );
		Call( ErrDispSetColumn(		//  ObjidFDP
			sesid, 
			prepairtt->tableidUsed, 
			prepairtt->rgcolumnidUsed[0],
			(BYTE *)&objidFDP, 
			sizeof( objidFDP ),
			0, 
			NULL ) );

		//  extract the key of the last node on the page
		BYTE rgbKey[JET_cbKeyMost+1];
		csr.SetILine( csr.Cpage().Clines() - 1 );
		KEYDATAFLAGS kdf;
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdf );
		kdf.key.CopyIntoBuffer( rgbKey, sizeof( rgbKey ) );
		
		Call( ErrDispSetColumn(		//  Key
			sesid, 
			prepairtt->tableidUsed, 
			prepairtt->rgcolumnidUsed[1],
			rgbKey, 
			kdf.key.Cb(),
			0, 
			NULL ) );
		Call( ErrDispSetColumn(		//  Pgno
			sesid, 
			prepairtt->tableidUsed, 
			prepairtt->rgcolumnidUsed[2],
			(BYTE *)&pgno, 
			sizeof( pgno ),
			0, 
			NULL ) );
		err = ErrDispUpdate( sesid, prepairtt->tableidUsed, NULL, 0, NULL, 0 );
		if( JET_errKeyDuplicate == err )
			{
			UtilReportEvent(
				eventWarning,
				REPAIR_CATEGORY,
				REPAIR_BAD_PAGE_ID,
				0, NULL );
			(*popts->pcprintfError)( "page %d: duplicate keys. discarding page\r\n", pgno );
			err = ErrDispPrepareUpdate( sesid, prepairtt->tableidUsed, JET_prepCancel );
			}
		Call( err );
		++(prepairtt->crecordsUsed);
		}

	Call( ErrIsamCommitTransaction( sesid, 0 ) );
	fInTransaction = fFalse;	

HandleError:
	if( fInTransaction )
		{
		CallS( ErrIsamRollback( sesid, 0 ) );
		}

#ifdef SYNC_DEADLOCK_DETECTION
	Pcls()->pownerLockHead = pownerSaved;
#endif  //  SYNC_DEADLOCK_DETECTION

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertBadPageIntoTables(
	PIB * const ppib,
	const PGNO pgno,
	REPAIRTT * const prepairtt,
	const REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	BOOL	fInTransaction 	= fFalse;

	const JET_SESID sesid	= reinterpret_cast<JET_SESID>( ppib );
	
	(*popts->pcprintfDebug)( "Inserting page %d into BadPages table\r\n", pgno );

	UtilReportEvent(
			eventWarning,
			REPAIR_CATEGORY,
			REPAIR_BAD_PAGE_ID,
			0, NULL );

#ifdef SYNC_DEADLOCK_DETECTION
	COwner* const pownerSaved = Pcls()->pownerLockHead;
	Pcls()->pownerLockHead = NULL;
#endif  //  SYNC_DEADLOCK_DETECTION
		
	Call( ErrIsamBeginTransaction( sesid, NO_GRBIT ) );
	fInTransaction = fTrue;
		
	Call( ErrDispPrepareUpdate( sesid, prepairtt->tableidBadPages, JET_prepInsert ) );
	Call( ErrDispSetColumn(		//  pgno
		sesid, 
		prepairtt->tableidBadPages, 
		prepairtt->rgcolumnidBadPages[0],
		(BYTE *)&pgno, 
		sizeof( pgno ),
		0, 
		NULL ) );
	Call( ErrDispUpdate( sesid, prepairtt->tableidBadPages, NULL, 0, NULL, 0 ) );
	++(prepairtt->crecordsBadPages);
		
	Call( ErrIsamCommitTransaction( sesid, 0 ) );
	fInTransaction = fFalse;

HandleError:
	if( fInTransaction )
		{
		CallS( ErrIsamRollback( sesid, 0 ) );
		}

#ifdef SYNC_DEADLOCK_DETECTION
	Pcls()->pownerLockHead = pownerSaved;
#endif  //  SYNC_DEADLOCK_DETECTION

	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrREPAIRGetPgnoOEAE( 
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	PGNO * const ppgnoOE,
	PGNO * const ppgnoAE,
	PGNO * const ppgnoParent,
	const BOOL fUnique,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR	err = JET_errSuccess;

	*ppgnoOE 		= pgnoNull;
	*ppgnoAE 		= pgnoNull;
	*ppgnoParent	= pgnoNull;
	
	CSR	csr;
	CallR( csr.ErrGetReadPage(
			ppib,
			ifmp,
			pgnoFDP,
			bflfNoTouch ) );

	LINE line;
	csr.Cpage().GetPtrExternalHeader( &line );

	if( sizeof( SPACE_HEADER ) != line.cb )
		{
		(*popts->pcprintfError)( "page %d: external header is unexpected size. got %d bytes, expected %d\r\n",
								 pgnoFDP, line.cb, sizeof( SPACE_HEADER ) );		
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
	const SPACE_HEADER * psph;
	psph = reinterpret_cast <SPACE_HEADER *> ( line.pv );

	if( fUnique != psph->FUnique() )
		{
		(*popts->pcprintfError)( "page %d: external header has wrong unique flag. got %s, expected %s\r\n",
									pgnoFDP,
									psph->FUnique() ? "UNIQUE" : "NON-UNIQUE",
									fUnique ? "UNIQUE" : "NON-UNIQUE"
									);		
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	*ppgnoParent = psph->PgnoParent();
	
	if ( psph->FSingleExtent() )
		{
		*ppgnoOE = pgnoNull;
		*ppgnoAE = pgnoNull;
		}
	else
		{
		*ppgnoOE = psph->PgnoOE();
		*ppgnoAE = psph->PgnoAE();
		if( pgnoNull == *ppgnoOE )
			{
			(*popts->pcprintfError)( "page %d: pgnoOE is pgnoNull", pgnoFDP );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		if( pgnoNull == *ppgnoAE )
			{
			(*popts->pcprintfError)( "page %d: pgnoAE is pgnoNull", pgnoFDP );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		if( *ppgnoOE == *ppgnoAE )
			{
			(*popts->pcprintfError)( "page %d: pgnoOE and pgnoAE are the same (%d)", pgnoFDP, *ppgnoOE );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		if( pgnoFDP == *ppgnoOE )
			{
			(*popts->pcprintfError)( "page %d: pgnoOE and pgnoFDP are the same", pgnoFDP );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		if( pgnoFDP == *ppgnoAE )
			{
			(*popts->pcprintfError)( "page %d: pgnoAE and pgnoFDP are the same", pgnoFDP );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}			
		}

HandleError:
	csr.ReleasePage();
	csr.Reset();
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSplitBuf( 
	PIB * const ppib,
	const PGNO pgnoLastBuffer, 
	const CPG cpgBuffer,	
	const OBJID objidCurrent,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR		err						= JET_errSuccess;
	PGNO 	pgnoT;

	Assert( cpgBuffer >= 0 );
	Assert( pgnoLastBuffer <= pgnoSysMax );
	Assert( pgnoLastBuffer >= cpgBuffer );

	//  these TTARRAYs are always accessed in OwnedSpace, AvailSpace order
	TTARRAY::RUN runOwned;
	TTARRAY::RUN runAvail;
	
	pttarrayOwnedSpace->BeginRun( ppib, &runOwned );
	pttarrayAvailSpace->BeginRun( ppib, &runAvail );

	
	for ( pgnoT = pgnoLastBuffer - cpgBuffer + 1; pgnoT <= pgnoLastBuffer; pgnoT++ )
		{
		OBJID objid;
			
		if( pttarrayOwnedSpace )
			{
			Call( pttarrayOwnedSpace->ErrGetValue( ppib, pgnoT, &objid, &runOwned ) );
			
			if( objidParent != objid && objidCurrent != objid )
				{
				(*popts->pcprintfError)( "space allocation error (OE): page %d is already owned by objid %d ( expected parent objid %d or objid %d)\r\n",
												pgnoT, objid, objidParent, objidCurrent );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}
			if( objidCurrent != objid )
				{
				Assert( objidParent == objid );
				Call( pttarrayOwnedSpace->ErrSetValue( ppib, pgnoT, objidCurrent, &runOwned ) );
				}
				
			}
		
		if( pttarrayAvailSpace )
			{
			Call( pttarrayAvailSpace->ErrGetValue( ppib, pgnoT, &objid, &runAvail ) );
			
			if( objidNil != objid )
				{
				(*popts->pcprintfError)( "space allocation error (AE): page %d is available to objid %d (expected 0)\r\n",
										pgnoT, objid );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}

			Call( pttarrayAvailSpace->ErrSetValue( ppib, pgnoT, objidCurrent, &runAvail ) );
			}
		}
HandleError:
	if( pttarrayOwnedSpace )
		{
		pttarrayOwnedSpace->EndRun( ppib, &runOwned );
		}

	if( pttarrayAvailSpace )
		{
		pttarrayAvailSpace->EndRun( ppib, &runAvail );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckSPLITBUFFERInSpaceTree(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const OBJID objidCurrent,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR		err						= JET_errSuccess;
	
	Assert( pgnoNull != pgnoFDP );

	SPLIT_BUFFER 	spb;

	memset( &spb, 0, sizeof( SPLIT_BUFFER ) );


	CSR	csr;
	err = csr.ErrGetReadPage( 
					ppib, 
					ifmp,
					pgnoFDP,
					bflfNoTouch );
	if( err < 0 )
		{
		(*popts->pcprintfError)( "page %d: error %d on read\r\n", pgnoFDP, err );
		Call( err );
		}
		
	// the spacetree and rootpage flags should be checked before this function
	Assert( csr.Cpage().FSpaceTree() );
	Assert( csr.Cpage().FRootPage() );
		
	LINE line;
	csr.Cpage().GetPtrExternalHeader( &line );

	if( 0 == line.cb )
		{
		// no SPLIT_BUFFER
		Call( JET_errSuccess );
		}
	else if(sizeof( SPLIT_BUFFER ) == line.cb)
		{
		UtilMemCpy( &spb, line.pv, sizeof( SPLIT_BUFFER ) );
		}
	else
		{
		(*popts->pcprintfError)( "page %d: split buffer is unexpected size. got %d bytes, expected %d\r\n",
								 pgnoFDP, line.cb, sizeof( SPLIT_BUFFER ) );		
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( 0 != spb.CpgBuffer1() )
		{
		Call( ErrREPAIRCheckSplitBuf( 
					ppib,
					spb.PgnoLastBuffer1(), 
					spb.CpgBuffer1(),	
					objidCurrent,
					objidParent,
					pttarrayOwnedSpace,
					pttarrayAvailSpace,
					popts ) );
		}
		
	if( 0 != spb.CpgBuffer2() )
		{
		Call( ErrREPAIRCheckSplitBuf( 
					ppib,
					spb.PgnoLastBuffer2(), 
					spb.CpgBuffer2(),	
					objidCurrent,
					objidParent,
					pttarrayOwnedSpace,
					pttarrayAvailSpace,
					popts ) );
		}		
		
HandleError:
	csr.ReleasePage();
	csr.Reset();

	return err;
	}



//  ================================================================
LOCAL ERR ErrREPAIRCheckSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	BTSTATS btstats;

	//  don't preread the root of the tree, it should have been preread already 
	
	PGNO pgnoOE;
	PGNO pgnoAE;
	PGNO pgnoParentActual;
	Call( ErrREPAIRGetPgnoOEAE(
			ppib,
			ifmp,
			pgnoFDP,
			&pgnoOE,
			&pgnoAE,
			&pgnoParentActual,
			fUnique,
			popts ) );

	if( pgnoNull != pgnoOE )
		{
		//  preread the roots of the space trees
		PGNO	rgpgno[3];
		rgpgno[0] = pgnoOE;
		rgpgno[1] = pgnoAE;
		rgpgno[2] = pgnoNull;
		BFPrereadPageList( ifmp, rgpgno );
		}

	if( pgnoFDPParent != pgnoParentActual )
		{
		(*popts->pcprintfError)( "page %d (objid %d): space corruption. pgno parent is %d, expected %d\r\n",
			pgnoFDP, objid, pgnoParentActual, pgnoFDPParent );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
				
	if( pgnoNull != pgnoOE )
		{
		RECCHECKSPACEOE reccheckOE( ppib, pttarrayOwnedSpace, objid, objidParent, popts );
		RECCHECKSPACEAE reccheckAE( ppib, pttarrayOwnedSpace, pttarrayAvailSpace, objid, objidParent, popts );

		memset( &btstats, 0, sizeof( BTSTATS ) );
		Call( ErrREPAIRCheckTree(
				ppib,
				ifmp,
				pgnoOE,
				objid,
				fPageFlags | CPAGE::fPageSpaceTree,
				&reccheckOE,
				NULL,
				pttarrayAvailSpace,	//  at least make sure we aren't available to anyone else
				fFalse,
				pfDbtimeTooLarge,
				&btstats,
				popts ) );

		memset( &btstats, 0, sizeof( BTSTATS ) );
		Call( ErrREPAIRCheckTree(
				ppib,
				ifmp,
				pgnoAE,
				objid,
				fPageFlags | CPAGE::fPageSpaceTree,
				&reccheckAE,
				pttarrayOwnedSpace,	//  we now know which pages we own
				pttarrayAvailSpace,	//  at least make sure we aren't available to anyone else
				fFalse,
				pfDbtimeTooLarge,
				&btstats,
				popts ) );

		// check SPLIT_BUFFER 
		Call( ErrREPAIRCheckSPLITBUFFERInSpaceTree(
				ppib,
				ifmp,
				pgnoOE,
				objid, 
				objidParent, 
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				popts ) );
		Call( ErrREPAIRCheckSPLITBUFFERInSpaceTree(
				ppib,
				ifmp,
				pgnoAE,
				objid, 
				objidParent,
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				popts ) );

		}
	else
		{
		Call( ErrREPAIRInsertSEInfo(
				ppib,
				ifmp,
				pgnoFDP,
				objid,
				objidParent,
				pttarrayOwnedSpace,
				pttarrayAvailSpace,
				popts ) );
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckTree(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	BTSTATS btstats;

	memset( &btstats, 0, sizeof( BTSTATS ) );
	Call( ErrREPAIRCheckTree(
			ppib,
			ifmp,
			pgnoFDP,
			objid,
			fPageFlags,
			preccheck,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			!fUnique,
			pfDbtimeTooLarge,
			&btstats,
			popts ) );

	if( popts->grbit & JET_bitDBUtilOptionStats )
		{
		REPAIRDumpStats( ppib, ifmp, pgnoFDP, &btstats, popts );
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckTreeAndSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoFDP,
	const OBJID objidParent,
	const PGNO pgnoFDPParent,
	const ULONG fPageFlags,
	const BOOL fUnique,
	RECCHECK * const preccheck,
	TTARRAY * const pttarrayOwnedSpace,
	TTARRAY * const pttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Call( ErrREPAIRCheckSpace( 
			ppib,
			ifmp,
			objid,
			pgnoFDP,
			objidParent,
			pgnoFDPParent,
			fPageFlags,
			fUnique,
			preccheck,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			pfDbtimeTooLarge,
			popts ) );

	Call( ErrREPAIRCheckTree( 
			ppib,
			ifmp,
			objid,
			pgnoFDP,
			objidParent,
			pgnoFDPParent,
			fPageFlags,
			fUnique,
			preccheck,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			pfDbtimeTooLarge,
			popts ) );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertSEInfo(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts ) 
//  ================================================================
	{
	JET_ERR err = JET_errSuccess;
	CSR csr;
	
	CallR( csr.ErrGetReadPage(
			ppib,
			ifmp,
			pgnoFDP,
			bflfNoTouch ) );

	LINE line;
	csr.Cpage().GetPtrExternalHeader( &line );

	Assert( sizeof( SPACE_HEADER ) == line.cb );	//  checked in ErrREPAIRGetPgnoOEAE
		
	const SPACE_HEADER * const psph = reinterpret_cast <SPACE_HEADER *> ( line.pv );
	Assert( psph->FSingleExtent() );	//	checked in ErrREPAIRGetPgnoOEAE

	const CPG cpgOE = psph->CpgPrimary();
	const PGNO pgnoOELast = pgnoFDP + cpgOE - 1;

	Call( ErrREPAIRInsertOERunIntoTT(
		ppib,
		pgnoOELast,
		cpgOE,
		objid,
		objidParent,
		pttarrayOwnedSpace,
		popts ) );

HandleError:
	csr.ReleasePage();
	csr.Reset();

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertOERunIntoTT(
	PIB * const ppib,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	const REPAIROPTS * const popts ) 
//  ================================================================
	{
	JET_ERR err = JET_errSuccess;

	TTARRAY::RUN run;
	pttarrayOwnedSpace->BeginRun( ppib, &run );
	
	for( PGNO pgno = pgnoLast; pgno > pgnoLast - cpgRun; --pgno )
		{
		if( objidNil != objidParent )
			{
			OBJID objidOwning;
			Call( pttarrayOwnedSpace->ErrGetValue( ppib, pgno, &objidOwning, &run ) );
			
			if( objidParent != objidOwning )
				{
				(*popts->pcprintfError)( "space allocation error (OE): page %d is already owned by objid %d, (expected parent objid %d for objid %d)\r\n",
												pgno, objidOwning, objidParent, objid );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}
			}
		Call( pttarrayOwnedSpace->ErrSetValue( ppib, pgno, objid, &run ) );
		}
		
HandleError:
	pttarrayOwnedSpace->EndRun( ppib, &run );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertAERunIntoTT(
	PIB * const ppib,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const OBJID objid,
	const OBJID objidParent,
	TTARRAY * const pttarrayOwnedSpace, 
	TTARRAY * const pttarrayAvailSpace,
	const REPAIROPTS * const popts ) 
//  ================================================================
	{
	JET_ERR err = JET_errSuccess;

	//  these TTARRAYs are always accessed in OwnedSpace, AvailSpace order
	TTARRAY::RUN runOwned;
	TTARRAY::RUN runAvail;
	
	pttarrayOwnedSpace->BeginRun( ppib, &runOwned );
	pttarrayAvailSpace->BeginRun( ppib, &runAvail );
	
	for( PGNO pgno = pgnoLast; pgno > pgnoLast - cpgRun; --pgno )
		{		
		//  we must own this page
		OBJID objidOwning;
		Call( pttarrayOwnedSpace->ErrGetValue( ppib, pgno, &objidOwning, &runOwned ) );
		
		if( objidOwning != objid )
			{
			(*popts->pcprintfError)( "space allocation error (AE): page %d is owned by objid %d, (expected objid %d)\r\n",
										pgno, objidOwning, objid );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}

		//  the page must not be available to any other table
		Call( pttarrayAvailSpace->ErrGetValue( ppib, pgno, &objidOwning, &runAvail ) );
		if( objidNil != objidOwning )
			{
			(*popts->pcprintfError)( "space allocation error (AE): page %d is available to objid %d\r\n",
										pgno, objidOwning );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}
		Call( pttarrayAvailSpace->ErrSetValue( ppib, pgno, objid, &runAvail ) );
		}

HandleError:
	pttarrayOwnedSpace->EndRun( ppib, &runOwned );
	pttarrayAvailSpace->EndRun( ppib, &runAvail );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRecheckSpaceTreeAndSystemTablesSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const CPG cpgDatabase,
	BOOL * const pfSpaceTreeCorrupt,
	TTARRAY ** const ppttarrayOwnedSpace,
	TTARRAY ** const ppttarrayAvailSpace,
	BOOL * const pfDbtimeTooLarge,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	PGNO pgnoLastOE	= pgnoNull;
	INST * const pinst	= PinstFromPpib( ppib );

	const FIDLASTINTDB fidLastInTDB = { fidMSO_FixedLast, fidMSO_VarLast, fidMSO_TaggedLast };
	
	RECCHECKNULL 	recchecknull;
	RECCHECKTABLE 	recchecktable( objidNil, pfucbNil, fidLastInTDB, NULL, NULL, NULL, NULL, NULL, popts ); 

	delete *ppttarrayOwnedSpace;
	delete *ppttarrayAvailSpace;

	*ppttarrayOwnedSpace 	= new TTARRAY( cpgDatabase + 1, objidSystemRoot );
	*ppttarrayAvailSpace 	= new TTARRAY( cpgDatabase + 1, objidNil );
		
	if( NULL == *ppttarrayOwnedSpace
		|| NULL == *ppttarrayAvailSpace )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	Call( (*ppttarrayOwnedSpace)->ErrInit( pinst ) );
	Call( (*ppttarrayAvailSpace)->ErrInit( pinst ) );
			
	Call( ErrREPAIRCheckSpaceTree(
			ppib,
			ifmp,
			pfSpaceTreeCorrupt,
			&pgnoLastOE,
			*ppttarrayOwnedSpace,
			*ppttarrayAvailSpace,
			pfDbtimeTooLarge,
			popts ) );

	// ignore err 
	err = ErrREPAIRCheckSpace(
			ppib,
			ifmp,
			objidFDPMSO,
			pgnoFDPMSO,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPagePrimary,
			fTrue,
			&recchecktable,
			*ppttarrayOwnedSpace,
			*ppttarrayAvailSpace,
			pfDbtimeTooLarge,
			popts );
			
	err = ErrREPAIRCheckSpace(
			ppib,
			ifmp,
			objidFDPMSO_NameIndex,
			pgnoFDPMSO_NameIndex,
			objidFDPMSO,
			pgnoFDPMSO,
			CPAGE::fPageIndex,
			fTrue,
			&recchecknull,
			*ppttarrayOwnedSpace,
			*ppttarrayAvailSpace,
			pfDbtimeTooLarge,
			popts );
	err = ErrREPAIRCheckSpace(
			ppib,
			ifmp,
			objidFDPMSO_RootObjectIndex,
			pgnoFDPMSO_RootObjectIndex,
			objidFDPMSO,
			pgnoFDPMSO,
			CPAGE::fPageIndex,
			fTrue,
			&recchecknull,
			*ppttarrayOwnedSpace,
			*ppttarrayAvailSpace,
			pfDbtimeTooLarge,
			popts );				
	err = ErrREPAIRCheckSpace(
			ppib,
			ifmp,
			objidFDPMSOShadow,
			pgnoFDPMSOShadow,
			objidSystemRoot,
			pgnoSystemRoot,
			CPAGE::fPagePrimary,
			fTrue,
			&recchecktable,
			*ppttarrayOwnedSpace,
			*ppttarrayAvailSpace,
			pfDbtimeTooLarge,
			popts );
							
	err = JET_errSuccess;

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckTree(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoRoot,
	const OBJID objidFDP,
	const ULONG fPageFlags,
	RECCHECK * const preccheck,
	const TTARRAY * const pttarrayOwnedSpace, 
	const TTARRAY * const pttarrayAvailSpace,
	const BOOL fNonUnique,
	BOOL * const pfDbtimeTooLarge,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR	err	= JET_errSuccess;

	const ULONG	fFlagsFDP = fPageFlags;

	pbtstats->pgnoLastSeen = pgnoNull;
	
	CSR	csr;
	err = csr.ErrGetReadPage( 
					ppib, 
					ifmp,
					pgnoRoot,
					bflfNoTouch );
	if( err < 0 )
		{
		(*popts->pcprintfError)( "page %d: error %d on read\r\n", pgnoRoot, err );
		Call( err );
		}

	if( !csr.Cpage().FRootPage() )
		{
		(*popts->pcprintfError)( "page %d: pgnoRoot is not a root page\r\n", pgnoRoot );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	
	csr.SetILine( 0 );
	
	Call( ErrREPAIRICheck(
			ppib,
			ifmp,
			objidFDP,
			fFlagsFDP,
			csr,
			fFalse,
			preccheck,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			fNonUnique,
			pfDbtimeTooLarge,
			pbtstats,
			NULL,
			NULL,
			popts ) );

	if( pgnoNull != pbtstats->pgnoNextExpected )
		{
		(*popts->pcprintfError)( "page %d: corrupt leaf links\r\n", pbtstats->pgnoLastSeen );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
HandleError:
	csr.ReleasePage( fTrue );
	csr.Reset();
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRICheck(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objidFDP,
	const ULONG fFlagsFDP,
	CSR& csr,
	const BOOL fPrereadSibling,
	RECCHECK * const preccheck,
	const TTARRAY * const pttarrayOwnedSpace,	//  can be null
	const TTARRAY * const pttarrayAvailSpace,	//	can be null
	const BOOL fNonUnique,
	BOOL * const pfDbtimeTooLarge,
	BTSTATS * const pbtstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err		= JET_errSuccess;

	if ( 0 == ( AtomicIncrement( (LONG *)(&popts->psnprog->cunitDone) ) % ( popts->psnprog->cunitTotal / 100 ) )
		&& popts->crit.FTryEnter() )
		{
		if ( 0 == ( popts->psnprog->cunitDone % ( popts->psnprog->cunitTotal / 100 ) ) )	// every 1%
			{
			popts->psnprog->cunitDone = min( popts->psnprog->cunitDone, popts->psnprog->cunitTotal );
			(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntProgress, popts->psnprog );	
			}
		popts->crit.Leave();
		}

	Call( csr.Cpage().ErrCheckPage( popts->pcprintfError ) );
	
	if( csr.Cpage().ObjidFDP() != objidFDP )
		{
		(*popts->pcprintfError)( "page %d: page belongs to different tree (objid is %d, should be %d)\r\n",
									csr.Pgno(),
									csr.Cpage().ObjidFDP(),
									objidFDP );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( ( csr.Cpage().FFlags() | fFlagsFDP ) != csr.Cpage().FFlags() )
		{
		(*popts->pcprintfError)( "page %d: page flag mismatch with FDP (current flags: 0x%x, FDP flags 0x%x)\r\n",
									csr.Pgno(), csr.Cpage().FFlags(), fFlagsFDP );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( csr.Cpage().Dbtime() > rgfmp[ifmp].Pdbfilehdr()->le_dbtimeDirtied )
		{
		(*popts->pcprintfError)( "page %d: dbtime is larger than database dbtime (0x%I64x, 0x%I64x)\n",
								csr.Cpage().Pgno(), csr.Cpage().Dbtime(), rgfmp[ifmp].Pdbfilehdr()->le_dbtimeDirtied );
		*pfDbtimeTooLarge = fTrue;
		}
	
	//  check that this page is owned by this tree and not available to anyone
	
	OBJID objid;
	if( pttarrayOwnedSpace )
		{
		Call( pttarrayOwnedSpace->ErrGetValue( ppib, csr.Pgno(), &objid, NULL ) );
		if( csr.Cpage().ObjidFDP() != objid )
			{
			(*popts->pcprintfError)( "page %d: space allocation error. page is owned by objid %d, expecting %d\r\n",
										csr.Pgno(), objid, csr.Cpage().ObjidFDP() );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}
	if( pttarrayAvailSpace )
		{
		Call( pttarrayAvailSpace->ErrGetValue( ppib, csr.Pgno(), &objid, NULL ) );
		if( objidNil != objid )
			{
			(*popts->pcprintfError)( "page %d: space allocation error. page is available to objid %d\r\n",
										csr.Pgno(), objid );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}

	if( csr.Cpage().FLeafPage() )
		{
		//  if we were the last leaf page page preread we preread our neighbour
		if( fPrereadSibling )
			{
			const PGNO pgnoNext = csr.Cpage().PgnoNext();
			if( pgnoNull != pgnoNext )
				{
				BFPrereadPageRange( ifmp, pgnoNext, g_cpgMinRepairSequentialPreread );
				}
			}

		Call( ErrREPAIRCheckLeaf(
				ppib,
				ifmp,
				csr,
				preccheck,
				fNonUnique,
				pbtstats,
				pbookmarkCurrParent,
				pbookmarkPrevParent,
				popts ) );
		}
	else
		{
		if( !csr.Cpage().FInvisibleSons() )
			{
			(*popts->pcprintfError)( "page %d: not an internal page\r\n" );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}

		//  check the internal page before prereading its children
		
		Call( ErrREPAIRCheckInternal( 
				ppib, 
				ifmp, 
				csr, 
				pbtstats, 
				pbookmarkCurrParent, 
				pbookmarkPrevParent, 
				popts ) );

		PGNO		rgpgno[g_cbPageMax/(sizeof(PGNO) + cbNDInsertionOverhead + cbNDNullKeyData )];
		const INT 	cpgno = csr.Cpage().Clines();

		INT iline;
		for( iline = 0; iline < cpgno; iline++ )
			{
			csr.SetILine( iline );

			KEYDATAFLAGS kdf;			
			NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdf );
				
			rgpgno[iline] = *((UnalignedLittleEndian< PGNO > *)kdf.data.Pv() );
			}
		rgpgno[cpgno] = pgnoNull;

		BFPrereadPageList( ifmp, rgpgno );

		BOOKMARK	rgbookmark[2];
		BOOKMARK	*rgpbookmark[2];
		rgpbookmark[0] = NULL;
		rgpbookmark[1] = NULL;

		BOOL	fChildrenAreLeaf;
		BOOL	fChildrenAreParentOfLeaf;
		BOOL	fChildrenAreInternal;
		
		INT ipgno;
		for( ipgno = 0; ipgno < cpgno; ipgno++ )
			{
			csr.SetILine( ipgno );

			KEYDATAFLAGS kdf;
			NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdf );

			const INT ibookmarkCurr = ipgno % 2;
			const INT ibookmarkPrev = ( ipgno + 1 ) % 2;
			Assert( ibookmarkCurr != ibookmarkPrev );
			Assert( rgpbookmark[ibookmarkPrev] != NULL || 0 == ipgno );
			
			rgbookmark[ibookmarkCurr].key 	= kdf.key;
			rgbookmark[ibookmarkCurr].data 	= kdf.data;
			rgpbookmark[ibookmarkCurr] 		= &rgbookmark[ibookmarkCurr];

			if( rgbookmark[ibookmarkCurr].key.FNull() )
				{
				if( cpgno-1 != ipgno )
					{
					(*popts->pcprintfError)( "node [%d:%d]: NULL key is not last key in page\r\n", csr.Pgno(), csr.ILine() );
//					BFFree( rgpgno );
					Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
					}
				rgpbookmark[ibookmarkCurr] = NULL;
				}
				
			CSR	csrChild;
			err = csrChild.ErrGetReadPage(
								ppib,
								ifmp,
								rgpgno[ipgno],
								bflfNoTouch );
			if( err < 0 )
				{
				(*popts->pcprintfError)( "page %d: error %d on read\r\n", rgpgno[ipgno], err );
//				BFFree( rgpgno );
				goto HandleError;
				}
						
			err = ErrREPAIRICheck(
					ppib,
					ifmp,
					objidFDP,
					fFlagsFDP,
					csrChild,
					( cpgno - 1 == ipgno ),
					preccheck,
					pttarrayOwnedSpace,
					pttarrayAvailSpace,
					fNonUnique,
					pfDbtimeTooLarge,
					pbtstats,
					rgpbookmark[ibookmarkCurr],
					rgpbookmark[ibookmarkPrev],
					popts );
					
			if( err < 0 )
				{
				(*popts->pcprintfError)( "node [%d:%d]: subtree check (page %d) failed with err %d\r\n", csr.Pgno(), csr.ILine(), rgpgno[ipgno], err );
				}
			else if( 0 == ipgno )
				{
				fChildrenAreLeaf 			= csrChild.Cpage().FLeafPage();
				fChildrenAreParentOfLeaf 	= csrChild.Cpage().FParentOfLeaf();
				fChildrenAreInternal 		= !fChildrenAreLeaf && !fChildrenAreParentOfLeaf;

				if( csr.Cpage().FParentOfLeaf() && !fChildrenAreLeaf )
					{
					err = ErrERRCheck( JET_errDatabaseCorrupted );
					(*popts->pcprintfError)( "page %d: child (%d) is not a leaf page but parent is parent-of-leaf\r\n", csr.Pgno(), rgpgno[ipgno] );
					}
				else if( !csr.Cpage().FParentOfLeaf() && fChildrenAreLeaf )
					{
					err = ErrERRCheck( JET_errDatabaseCorrupted );
					(*popts->pcprintfError)( "page %d: child (%d) is a leaf page but parent is not parent-of-leaf\r\n", csr.Pgno(), rgpgno[ipgno] );
					}
				}
			else
				{
				if( csrChild.Cpage().FLeafPage() != fChildrenAreLeaf )
					{
					err = ErrERRCheck( JET_errDatabaseCorrupted );
					(*popts->pcprintfError)( "node [%d:%d]: b-tree depth different (page %d, page %d) expected child (%d) to be leaf\r\n", csr.Pgno(), csr.ILine(), rgpgno[0], rgpgno[ipgno] );
					}
				else if( csrChild.Cpage().FParentOfLeaf() != fChildrenAreParentOfLeaf )
					{
					err = ErrERRCheck( JET_errDatabaseCorrupted );
					(*popts->pcprintfError)( "node [%d:%d]: b-tree depth different (page %d, page %d) expected child (%d) to be parent-of-leaf\r\n", csr.Pgno(), csr.ILine(), rgpgno[0], rgpgno[ipgno] );
					}
				else if( fChildrenAreInternal && ( csrChild.Cpage().FLeafPage() || csrChild.Cpage().FParentOfLeaf() ) )
					{
					err = ErrERRCheck( JET_errDatabaseCorrupted );
					(*popts->pcprintfError)( "node [%d:%d]: b-tree depth different (page %d, page %d) expected child (%d) to be internal\r\n", csr.Pgno(), csr.ILine(), rgpgno[0], rgpgno[ipgno] );
					}
				}
				
			csrChild.ReleasePage( fTrue );
			csrChild.Reset();
//			if ( err < 0 )
//				{
//				BFFree( rgpgno );
//				}
			Call( err );
			}
//		BFFree( rgpgno );
		}
		
HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRICheckNode(
	const PGNO pgno,
	const INT iline,
	const BYTE * const pbPage,
	const KEYDATAFLAGS& kdf,
	const REPAIROPTS * const popts )
//  ================================================================	
	{
	ERR			err = JET_errSuccess;

	if( kdf.key.Cb() > JET_cbKeyMost * 2 )
		{
		(*popts->pcprintfError)( "node [%d:%d]: key is too long (%d bytes)\r\n", pgno, iline, kdf.key.Cb() );
		CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	if( kdf.data.Cb() > g_cbPage )
		{
		(*popts->pcprintfError)( "node [%d:%d]: data is too long (%d bytes)\r\n", pgno, iline, kdf.data.Cb() );
		CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	if( ( kdf.key.Cb() + kdf.data.Cb() ) > g_cbPage )	
		{
		(*popts->pcprintfError)( "node [%d:%d]: node is too big (%d bytes)\r\n", pgno, iline, kdf.key.Cb() + kdf.data.Cb() );
		CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
	const BYTE * const pbKeyPrefix = (BYTE *)kdf.key.prefix.Pv();
	const BYTE * const pbKeySuffix = (BYTE *)kdf.key.suffix.Pv();
	const BYTE * const pbData = (BYTE *)kdf.data.Pv();

	if( FNDCompressed( kdf ) )
		{
		if( pbKeyPrefix < pbPage 
			|| pbKeyPrefix + kdf.key.prefix.Cb() >= pbPage + g_cbPage )
			{
			(*popts->pcprintfError)( "node [%d:%d]: prefix not on page\r\n", pgno, iline );
			CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}

	if( kdf.key.suffix.Cb() > 0 )
		{
		if( pbKeySuffix < pbPage 
			|| pbKeySuffix + kdf.key.suffix.Cb() >= pbPage + g_cbPage )
			{
			(*popts->pcprintfError)( "node [%d:%d]: suffix not on page\r\n", pgno, iline );
			CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}

	if( kdf.data.Cb() > 0 )
		{
		if( pbData < pbPage 
			|| pbData + kdf.data.Cb() >= pbPage + g_cbPage )
			{
			(*popts->pcprintfError)( "node [%d:%d]: data not on page\r\n", pgno, iline );
			CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}
		
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRICheckRecord(
	const PGNO pgno,
	const INT iline,
	const BYTE * const pbPage,
	const KEYDATAFLAGS& kdf,
	const REPAIROPTS * const popts )
//  ================================================================	
	{
	ERR		err = JET_errSuccess;
	const FIDLASTINTDB fidLastInTDB = { fidFixedMost, fidVarMost, fidTaggedMost }; 

	RECCHECKTABLE 	reccheck( objidNil, pfucbNil, fidLastInTDB, NULL, NULL, NULL, NULL, NULL, popts ); 

	Call( reccheck.ErrCheckRecord( kdf ) );
			
HandleError:
	return err;

	}


//  ================================================================
LOCAL int LREPAIRHandleException(
	const PIB * const ppib,
	const IFMP ifmp,
	const CSR& csr,
	const EXCEPTION exception,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	const DWORD dwExceptionId 			= ExceptionId( exception );
	const EExceptionFilterAction efa	= efaExecuteHandler;

	(*popts->pcprintfError)( "node [%d:%d]: caught exception 0x%x\r\n",
				csr.Pgno(),
				csr.ILine(),
				dwExceptionId
				);

	return efa;
	}


#pragma warning( disable : 4509 )
//  ================================================================
LOCAL ERR ErrREPAIRICheckInternalLine(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts,
	KEYDATAFLAGS& kdfCurr,
	const KEYDATAFLAGS& kdfPrev )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	TRY
		{		
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfCurr );
		CallJ( ErrREPAIRICheckNode(
				csr.Pgno(),
				csr.ILine(),
				(BYTE *)csr.Cpage().PvBuffer(),
				kdfCurr,
				popts ), HandleTryError );
		
		if( FNDDeleted( kdfCurr ) )
			{
			(*popts->pcprintfError)( "node [%d:%d]: deleted node on internal page\r\n", csr.Pgno(), csr.ILine() );
			CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
			}
			
		if( FNDVersion( kdfCurr ) )
			{
			(*popts->pcprintfError)( "node [%d:%d]: versioned node on internal page\r\n", csr.Pgno(), csr.ILine() );
			CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
			}

		if( 1 == kdfCurr.key.prefix.Cb() )
			{
			(*popts->pcprintfError)( "node [%d:%d]: incorrectly compressed key\r\n", csr.Pgno(), csr.ILine() );
			CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
			}
				
		if( sizeof( PGNO ) != kdfCurr.data.Cb()  )
			{
			(*popts->pcprintfError)( "node [%d:%d]: bad internal data size\r\n", csr.Pgno(), csr.ILine() );
			CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
			}

		if( csr.ILine() > 0 && !kdfCurr.key.FNull() )
			{
			const INT cmp = CmpKey( kdfPrev.key, kdfCurr.key );
			if( cmp > 0 )
				{
				(*popts->pcprintfError)( "node [%d:%d]: nodes out of order on internal page\r\n", csr.Pgno(), csr.ILine() );
				CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
				}
			else if( 0 == cmp )
				{
				(*popts->pcprintfError)( "node [%d:%d]: illegal duplicate key on internal page\r\n", csr.Pgno(), csr.ILine() );
				CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
				}
			}
		
		if( FNDCompressed( kdfCurr ) )
			{
			++(pbtstats->cnodeCompressed);
			}			
		pbtstats->cbDataInternal += kdfCurr.data.Cb();
		++(pbtstats->rgckeyInternal[kdfCurr.key.Cb()]);
		++(pbtstats->rgckeySuffixInternal[kdfCurr.key.suffix.Cb()]);

HandleTryError:
		;
		}
	EXCEPT( LREPAIRHandleException( ppib, ifmp, csr, ExceptionInfo(), popts ) )
		{
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
	ENDEXCEPT

	return err;
	}
#pragma warning( default : 4509 )


//  ================================================================
LOCAL ERR ErrREPAIRCheckInternal(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	BTSTATS * const pbtstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err		= JET_errSuccess;
	KEYDATAFLAGS 	rgkdf[2];			

	if( csr.Cpage().Clines() <= 0 )
		{
		(*popts->pcprintfError)( "page %d: empty internal page\r\n", csr.Pgno() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( pgnoNull != csr.Cpage().PgnoNext() )
		{
		(*popts->pcprintfError)( "page %d: pgno next is non-NULL (%d)\r\n", csr.Pgno(), csr.Cpage().PgnoNext() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( pgnoNull != csr.Cpage().PgnoPrev() )
		{
		(*popts->pcprintfError)( "page %d: pgno next is non-NULL (%d)\r\n", csr.Pgno(), csr.Cpage().PgnoPrev() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}		

	LINE line;
	csr.Cpage().GetPtrExternalHeader( &line );
	++(pbtstats->cpageInternal);
	if( csr.Cpage().FRootPage() && !csr.Cpage().FSpaceTree() )
		{
		if( sizeof( SPACE_HEADER ) != line.cb )
			{
			(*popts->pcprintfError)( "page %d: space header is wrong size (expected %d bytes, got %d bytes)\r\n",
				csr.Pgno(),
				sizeof( SPACE_HEADER ),
				line.cb
				);
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}
	else
		{
		++(pbtstats->rgckeyPrefixInternal[line.cb]);
		}
	if( pbtstats->cpageDepth <= 0 )
		{
		--(pbtstats->cpageDepth);
		}

	INT iline;
	for( iline = 0; iline < csr.Cpage().Clines(); iline++ )
		{
		csr.SetILine( iline );
		KEYDATAFLAGS& kdfCurr = rgkdf[ iline % 2 ];
		const KEYDATAFLAGS& kdfPrev = rgkdf[ ( iline + 1 ) % 2 ];

		Call( ErrREPAIRICheckInternalLine(
					ppib,
					ifmp,
					csr,
					pbtstats,
					popts,
					kdfCurr,
					kdfPrev ) );
		}

	if( pbookmarkCurrParent )
		{
		csr.SetILine( csr.Cpage().Clines() - 1 );

		KEYDATAFLAGS kdfLast;
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfLast );
		
		if( !FKeysEqual( kdfLast.key, pbookmarkCurrParent->key ) )
			{
			(*popts->pcprintfError)( "page %d: bad page pointer to internal page\r\n", csr.Pgno() );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}

	if( pbookmarkPrevParent )
		{
		csr.SetILine( 0 );

		KEYDATAFLAGS kdfFirst;
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfFirst );
		if( kdfFirst.key.Cb() != 0 )	//  NULL is greater than anything
			{
			const INT cmp = CmpKey( kdfFirst.key, pbookmarkPrevParent->key );
			if( cmp < 0 )
				{
				(*popts->pcprintfError)( "page %d: prev parent pointer > first node on internal page\r\n", csr.Pgno() );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			}
		}

HandleError:
	return err;
	}

	
#pragma warning( disable : 4509 )
//  ================================================================
LOCAL ERR ErrREPAIRICheckLeafLine(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	RECCHECK * const preccheck,
	const BOOL fNonUnique,
	BTSTATS * const pbtstats,
	const REPAIROPTS * const popts,
	KEYDATAFLAGS& kdfCurr,
	const KEYDATAFLAGS& kdfPrev,
	BOOL * const pfEmpty )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	TRY
		{		
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfCurr );
		CallJ( ErrREPAIRICheckNode(
				csr.Pgno(),
				csr.ILine(),
				(BYTE *)csr.Cpage().PvBuffer(),
				kdfCurr,
				popts ), HandleTryError );
		
		if( kdfCurr.key.prefix.Cb() == 1 )
			{
			(*popts->pcprintfError)( "node [%d:%d]: incorrectly compressed key\r\n", csr.Pgno(), csr.ILine() );
			CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
			}

		if( !FNDDeleted( kdfCurr ) )
			{
			*pfEmpty = fFalse;
			err = (*preccheck)( kdfCurr );
			if( err > 0 )
				{
				(*popts->pcprintfWarning)( "node [%d:%d]: leaf node check failed\r\n", csr.Pgno(), csr.ILine() );
				}
			else if( err < 0 )
				{
				(*popts->pcprintfError)( "node [%d:%d]: leaf node check failed\r\n", csr.Pgno(), csr.ILine() );
				CallJ( err, HandleTryError );
				}
			}
		else
			{
			++(pbtstats->cnodeDeleted);
			}

		if( csr.ILine() > 0 )
			{
			Assert( !fNonUnique || csr.Cpage().FIndexPage() );
			
			const INT cmp = fNonUnique ?
								CmpKeyData( kdfPrev, kdfCurr, NULL ) :
								CmpKey( kdfPrev.key, kdfCurr.key );
			if( cmp > 0 )
				{
				(*popts->pcprintfError)( "node [%d:%d]: nodes out of order on leaf page\r\n", csr.Pgno(), csr.ILine() );
				CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
				}
			else if( 0 == cmp )
				{
				(*popts->pcprintfError)( "node [%d:%d]: illegal duplicate key on leaf page\r\n", csr.Pgno(), csr.ILine() );
				CallJ( ErrERRCheck( JET_errDatabaseCorrupted ), HandleTryError );
				}
			}

		if( FNDVersion( kdfCurr ) )
			{
			++(pbtstats->cnodeVersioned);
			}
			
		if( FNDCompressed( kdfCurr ) )
			{
			++(pbtstats->cnodeCompressed);
			}
			
		pbtstats->cbDataLeaf += kdfCurr.data.Cb();
		++(pbtstats->rgckeyLeaf[kdfCurr.key.Cb()]);
		++(pbtstats->rgckeySuffixLeaf[kdfCurr.key.suffix.Cb()]);

HandleTryError:
		;
		}
	EXCEPT( LREPAIRHandleException( ppib, ifmp, csr, ExceptionInfo(), popts ) )
		{
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
	ENDEXCEPT

	return err;
	}
#pragma warning( default : 4509 )


//  ================================================================
LOCAL ERR ErrREPAIRCheckLeaf(
	PIB * const ppib,
	const IFMP ifmp,
	CSR& csr,
	RECCHECK * const preccheck,
	const BOOL fNonUnique,
	BTSTATS * const pbtstats,
	const BOOKMARK * const pbookmarkCurrParent,
	const BOOKMARK * const pbookmarkPrevParent,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err		= JET_errSuccess;
	BOOL			fEmpty	= fTrue;
	KEYDATAFLAGS 	rgkdf[2];			

	if( csr.Cpage().Clines() == 0 && !csr.Cpage().FRootPage() )
		{
		(*popts->pcprintfError)( "page %d: empty leaf page (non-root)\r\n", csr.Pgno() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( csr.Cpage().PgnoPrev() != pbtstats->pgnoLastSeen )
		{
		(*popts->pcprintfError)( "page %d: bad leaf page links\r\n", csr.Pgno() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	if( pgnoNull != pbtstats->pgnoNextExpected && csr.Cpage().Pgno() != pbtstats->pgnoNextExpected )
		{
		(*popts->pcprintfError)( "page %d: bad leaf page links\r\n", csr.Pgno() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	pbtstats->pgnoLastSeen 		= csr.Cpage().Pgno();
	pbtstats->pgnoNextExpected	= csr.Cpage().PgnoNext();	

	LINE line;
	csr.Cpage().GetPtrExternalHeader( &line );
	++(pbtstats->cpageLeaf);
	if( csr.Cpage().FRootPage() && !csr.Cpage().FSpaceTree() )
		{
		if( sizeof( SPACE_HEADER ) != line.cb )
			{
			(*popts->pcprintfError)( "page %d: space header is wrong size (expected %d bytes, got %d bytes)\r\n",
				csr.Pgno(),
				sizeof( SPACE_HEADER ),
				line.cb
				);
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}
	else
		{
		++(pbtstats->rgckeyPrefixLeaf[line.cb]);
		}
		
	if( pbtstats->cpageDepth <= 0 )
		{
		pbtstats->cpageDepth *= -1;
		++(pbtstats->cpageDepth);
		}

	INT iline;
	for( iline = 0; iline < csr.Cpage().Clines(); iline++ )
		{
		csr.SetILine( iline );
		KEYDATAFLAGS& kdfCurr = rgkdf[ iline % 2 ];
		const KEYDATAFLAGS& kdfPrev = rgkdf[ ( iline + 1 ) % 2 ];

		Call( ErrREPAIRICheckLeafLine(
					ppib,
					ifmp,
					csr,
					preccheck,
					fNonUnique,
					pbtstats,
					popts,
					kdfCurr,
					kdfPrev,
					&fEmpty ) );
		}

	if( pbookmarkCurrParent )
		{
		if( pgnoNull == csr.Cpage().PgnoNext() )
			{
			(*popts->pcprintfError)( "page %d: non-NULL page pointer to leaf page with no pgnoNext\r\n", csr.Pgno() );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}
			
		csr.SetILine( csr.Cpage().Clines() - 1 );

		KEYDATAFLAGS kdfLast;
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfLast );

		const INT cmp = CmpKey( kdfLast.key, pbookmarkCurrParent->key );
		if( cmp >= 0 )
			{
			(*popts->pcprintfError)( "page %d: bad page pointer to leaf page\r\n", csr.Pgno() );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
			}
		}
	else if( pgnoNull != csr.Cpage().PgnoNext() )	// NULL parent means we are at the end of the tree
		{
		(*popts->pcprintfError)( "page %d: NULL page pointer to leaf page with pgnoNext\r\n", csr.Pgno() );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
		}

	if( pbookmarkPrevParent )
		{
		csr.SetILine( 0 );

		KEYDATAFLAGS kdfFirst;
		NDIGetKeydataflags( csr.Cpage(), csr.ILine(), &kdfFirst );

		if( kdfFirst.key.Cb() != 0 )
			{
			//  for secondary indexes compare with the primary key that is in the data
			const INT cmp = CmpKeyWithKeyData( pbookmarkPrevParent->key, kdfFirst );
			if( cmp > 0 )
				{
				(*popts->pcprintfError)( "page %d: prev parent pointer > first node on leaf page\r\n", csr.Pgno() );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			}
		}

	if( fEmpty )
		{
		++(pbtstats->cpageEmpty);
		}

HandleError:
	return err;
	}


//  ================================================================
OBJIDLIST::OBJIDLIST() :
//  ================================================================
	m_cobjid( 0 ),
	m_cobjidMax( 0 ),
	m_rgobjid( NULL ),
	m_fSorted( fFalse )
	{
	}


//  ================================================================
OBJIDLIST::~OBJIDLIST()
//  ================================================================
	{
	if( NULL != m_rgobjid )
		{
		Assert( 0 < m_cobjidMax );
		OSMemoryHeapFree( m_rgobjid );
		m_rgobjid = reinterpret_cast<OBJID *>( lBitsAllFlipped );
		}
	else
		{
		Assert( 0 == m_cobjidMax );
		Assert( 0 == m_cobjid );
		}
	}

	
//  ================================================================
ERR OBJIDLIST::ErrAddObjid( const OBJID objid )
//  ================================================================
	{
	if( m_cobjid == m_cobjidMax )
		{		
		//  resize/create the array

		OBJID * const rgobjidOld = m_rgobjid;
		const INT cobjidMaxNew 	 = m_cobjidMax + 16;
		OBJID * const rgobjidNew = reinterpret_cast<OBJID *>( PvOSMemoryHeapAlloc( cobjidMaxNew * sizeof( OBJID ) ) );
		if( NULL == rgobjidNew )
			{
			return ErrERRCheck( JET_errOutOfMemory );
			}
			
		UtilMemCpy( rgobjidNew, m_rgobjid, sizeof( OBJID ) * m_cobjid );
		m_cobjidMax = cobjidMaxNew;
		m_rgobjid 	= rgobjidNew;
		OSMemoryHeapFree( rgobjidOld );
		}
	m_rgobjid[m_cobjid++] = objid;
	m_fSorted = fFalse;
	return JET_errSuccess;
	}
	

//  ================================================================
BOOL OBJIDLIST::FObjidPresent( const OBJID objid ) const
//  ================================================================
	{
	Assert( m_fSorted );
	return binary_search( (OBJID *)m_rgobjid, (OBJID *)m_rgobjid + m_cobjid, objid );
	}


//  ================================================================
VOID OBJIDLIST::Sort()
//  ================================================================
	{
	sort( m_rgobjid, m_rgobjid + m_cobjid );
	m_fSorted = fTrue;
	}


//  ================================================================
LOCAL ERR ErrREPAIRAttachForIntegrity(
	const JET_SESID sesid,
	const CHAR * const szDatabase,
	IFMP * const pifmp,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Attach R/O, without attaching the SLV
//
	{
	ERR			err					= JET_errSuccess;
	JET_DBID	dbid;
	const BOOL	fAttachReadonly		= popts->grbit & JET_bitDBUtilOptionDontRepair;

	Call( ErrIsamAttachDatabase(
		sesid,
		szDatabase,
		NULL,
		NULL,
		0,
		( fAttachReadonly ? JET_bitDbReadOnly : 0 ) | JET_bitDbRecoveryOff) );
	Assert( JET_wrnDatabaseAttached != err );

	Call( ErrIsamOpenDatabase(
		sesid,
		szDatabase,
		NULL,
		&dbid,
		( fAttachReadonly ? JET_bitDbReadOnly : 0 ) | JET_bitDbRecoveryOff ) );
	*pifmp = dbid;

	rgfmp[*pifmp].SetVersioningOff();

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRAttachForRepair(
	const JET_SESID sesid,
	const CHAR * const szDatabase,
	const CHAR * const szSLV,
	IFMP * const pifmp,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Close/Detach a previously attached database, attach R/W and change the signature
//
	{
	ERR			err		= JET_errSuccess;
	JET_DBID	dbid;

	CallR( ErrIsamCloseDatabase( sesid, (JET_DBID)*pifmp, 0 ) );
	CallR( ErrIsamDetachDatabase( sesid, NULL, szDatabase ) );
	CallR( ErrREPAIRChangeSignature( PinstFromPpib( (PIB *)sesid ), szDatabase, szSLV, dbtimeLast, objidLast, popts ) );
	CallR( ErrIsamAttachDatabase( sesid, szDatabase, NULL, NULL, 0, JET_bitDbRecoveryOff) );
	Assert( JET_wrnDatabaseAttached != err );
	CallR( ErrIsamOpenDatabase(
			sesid,
			szDatabase,
			NULL,
			&dbid,
			JET_bitDbRecoveryOff ) );
	*pifmp = dbid;
	rgfmp[*pifmp].SetVersioningOff();
	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrREPAIRChangeSignature(
	INST *pinst,
	const char * const szDatabase,
	const char * const szSLV,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Force the database to a consistent state and change the signature
//  so that we will not be able to use the logs against the database
//  again
//
//-
	{
	ERR err = JET_errSuccess;

	//  the SLV file and the database must have the same signatures
	
	SIGNATURE signDb;
	SIGNATURE signSLV;

	Call( ErrREPAIRChangeDBSignature(
			pinst,
			szDatabase,
			dbtimeLast,
			objidLast,
			&signDb,
			&signSLV,
			popts ) );
			
	if( NULL != szSLV )
		{
		Call( ErrREPAIRChangeSLVSignature(
				pinst,
				szSLV,
				dbtimeLast,
				objidLast,
				&signDb,
				&signSLV,
				popts ) );
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRChangeDBSignature(
	INST *pinst,
	const char * const szDatabase,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	SIGNATURE * const psignDb,
	SIGNATURE * const psignSLV,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Force the database to a consistent state and change the signature
//  so that we will not be able to use the logs against the database
//  again
//
//-
	{
	ERR err = JET_errSuccess;
	DBFILEHDR * const pdfh = reinterpret_cast<DBFILEHDR * >( PvOSMemoryPageAlloc( g_cbPage, NULL ) );
	if ( NULL == pdfh )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	err = ( ( popts->grbit & JET_bitDBUtilOptionDontRepair ) ?
			ErrUtilReadShadowedHeader : ErrUtilReadAndFixShadowedHeader )
				( pinst->m_pfsapi,
				const_cast<CHAR *>( szDatabase ),
				reinterpret_cast<BYTE*>( pdfh ),
				g_cbPage,
				OffsetOf( DBFILEHDR, le_cbPageSize ),
				NULL );
		
	if ( err < 0 )
		{
		if ( JET_errDiskIO == err )
			{
			(*popts->pcprintfError)( "unable to read database header for %s\r\n", szDatabase );
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			}
		goto HandleError;
		}

	if( !( popts->grbit & JET_bitDBUtilOptionDontRepair ) )
		{
		Assert( 0 != dwGlobalMajorVersion );
		
		(*popts->pcprintfVerbose)( "changing signature of %s\r\n", szDatabase );		

		pdfh->le_ulMagic 			= ulDAEMagic;
		pdfh->le_ulVersion 			= ulDAEVersion;
		pdfh->le_ulUpdate 			= ulDAEUpdate;
		if( 0 != dbtimeLast )
			{
			
			//	sometimes we may not have re-calculated the dbtime
			
			pdfh->le_dbtimeDirtied 		= dbtimeLast + 1;
			}
		pdfh->le_objidLast 			= objidLast + 1;
		pdfh->le_attrib 			= attribDb;
		pdfh->le_dwMajorVersion 	= dwGlobalMajorVersion;
		pdfh->le_dwMinorVersion 	= dwGlobalMinorVersion;
		pdfh->le_dwBuildNumber 		= dwGlobalBuildNumber;
		pdfh->le_lSPNumber 			= lGlobalSPNumber;
		pdfh->le_lGenMinRequired 	= 0;
		pdfh->le_lGenMaxRequired 	= 0;
		if( objidNil != objidLast )
			{
			++(pdfh->le_ulRepairCount);
			}
		LGIGetDateTime( &pdfh->logtimeRepair );
		
		pdfh->ResetUpgradeDb();

		memset( &pdfh->signLog, 0, sizeof( SIGNATURE ) );
		
		memset( &pdfh->bkinfoFullPrev, 0, sizeof( BKINFO ) );
		memset( &pdfh->bkinfoIncPrev, 0, sizeof( BKINFO ) );
		memset( &pdfh->bkinfoFullCur, 0, sizeof( BKINFO ) );

		SIGGetSignature( &(pdfh->signDb) );
		SIGGetSignature( &(pdfh->signSLV) );

		*psignDb 	= pdfh->signDb;
		*psignSLV 	= pdfh->signSLV;

		(*popts->pcprintfVerbose)( "new DB signature is:\r\n" );		
		REPAIRPrintSig( &pdfh->signDb, popts->pcprintfVerbose );
		(*popts->pcprintfVerbose)( "new SLV signature is:\r\n" );		
		REPAIRPrintSig( &pdfh->signSLV, popts->pcprintfVerbose );

		Call( ErrUtilWriteShadowedHeader(	pinst->m_pfsapi, 
											szDatabase, 
											fTrue,
											reinterpret_cast<BYTE*>( pdfh ), 
											g_cbPage ) );
		}

HandleError:
	OSMemoryPageFree( pdfh );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRChangeSLVSignature(
	INST *pinst,
	const char * const szSLV,
	const DBTIME dbtimeLast,
	const OBJID objidLast,
	const SIGNATURE * const psignDb,
	const SIGNATURE * const psignSLV,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Force the database to a consistent state and change the signature
//  so that we will not be able to use the logs against the database
//  again
//
//-
	{
	ERR err = JET_errSuccess;
	SLVFILEHDR * const pslvfilehdr = reinterpret_cast<SLVFILEHDR *>( PvOSMemoryPageAlloc( g_cbPage, NULL ) );
	if ( NULL == pslvfilehdr )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	err = ( ( popts->grbit & JET_bitDBUtilOptionDontRepair ) ?
			ErrUtilReadShadowedHeader : ErrUtilReadAndFixShadowedHeader )
				( pinst->m_pfsapi,
				const_cast<CHAR *>( szSLV ),
				reinterpret_cast<BYTE*>( pslvfilehdr ),
				g_cbPage,
				OffsetOf( SLVFILEHDR, le_cbPageSize ),
				NULL );
		
	if ( err < 0 )
		{
		if ( JET_errDiskIO == err )
			{
			(*popts->pcprintfError)( "unable to read SLV header for %s\r\n", szSLV );
			err = ErrERRCheck( JET_errDatabaseCorrupted );
			}
		goto HandleError;
		}

	if( !( popts->grbit & JET_bitDBUtilOptionDontRepair ) )
		{
		Assert( 0 != dwGlobalMajorVersion );
		
		(*popts->pcprintfVerbose)( "changing signature of %s\r\n", szSLV );		

		pslvfilehdr->SetDbstate( JET_dbstateCleanShutdown );

		pslvfilehdr->le_ulMagic 			= ulDAEMagic;
		pslvfilehdr->le_ulVersion 			= ulDAEVersion;
		pslvfilehdr->le_ulUpdate 			= ulDAEUpdate;
		pslvfilehdr->le_dbtimeDirtied 		= dbtimeLast + 1;
		pslvfilehdr->le_objidLast 			= objidLast + 1;
		pslvfilehdr->le_attrib 				= attribSLV;
		pslvfilehdr->le_dwMajorVersion 		= dwGlobalMajorVersion;
		pslvfilehdr->le_dwMinorVersion 		= dwGlobalMinorVersion;
		pslvfilehdr->le_dwBuildNumber 		= dwGlobalBuildNumber;
		pslvfilehdr->le_lSPNumber 			= lGlobalSPNumber;
		pslvfilehdr->le_lGenMinRequired 	= 0;
		pslvfilehdr->le_lGenMaxRequired 	= 0;
		if( objidNil != objidLast )
			{
			++(pslvfilehdr->le_ulRepairCount);
			}
		LGIGetDateTime( &pslvfilehdr->logtimeRepair );
		
		memset( &pslvfilehdr->signLog, 0, sizeof( SIGNATURE ) );

		pslvfilehdr->signDb 	= *psignDb;
		pslvfilehdr->signSLV 	= *psignSLV;

		(*popts->pcprintfVerbose)( "new DB signature is:\r\n" );		
		REPAIRPrintSig( &pslvfilehdr->signDb, popts->pcprintfVerbose );
		(*popts->pcprintfVerbose)( "new SLV signature is:\r\n" );		
		REPAIRPrintSig( &pslvfilehdr->signSLV, popts->pcprintfVerbose );

		Call( ErrUtilWriteShadowedHeader(	pinst->m_pfsapi, 
											szSLV, 
											fFalse,
											reinterpret_cast<BYTE*>( pslvfilehdr ), 
											g_cbPage ) );
		}

HandleError:
	OSMemoryPageFree( pslvfilehdr );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRepairGlobalSpace(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts )
//  ================================================================
	{	
	ERR err = JET_errSuccess;

	const PGNO pgnoLast = PgnoLast( ifmp );
	const CPG  cpgOwned = PgnoLast( ifmp ) - 3;	// we will insert three pages in the ErrSPCreate below

	FUCB 	*pfucb		= pfucbNil;
	FUCB	*pfucbOE	= pfucbNil;

	(*popts->pcprintfVerbose)( "repairing database root\r\n" );				

	OBJID			objidFDP;
	Call( ErrSPCreate(
				ppib,
				ifmp,
				pgnoNull,
				pgnoSystemRoot,
				3,
				fSPMultipleExtent,
				(ULONG)CPAGE::fPagePrimary,
				&objidFDP ) );
	Assert( objidSystemRoot == objidFDP );

	Call( ErrDIROpen( ppib, pgnoSystemRoot, ifmp, &pfucb ) );

	//  The tree has only one node so we can insert ths node without splitting
	Call( ErrSPIOpenOwnExt( ppib, pfucb->u.pfcb, &pfucbOE ) );

	(*popts->pcprintfDebug)( "Global OwnExt:  %d pages ending at %d\r\n", cpgOwned, pgnoLast );
	Call( ErrREPAIRInsertRunIntoSpaceTree(
					ppib,
					ifmp,
					pfucbOE,
					pgnoLast,
					cpgOwned,
					popts ) );

HandleError:
	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		}
	if( pfucbNil != pfucbOE )
		{
		DIRClose( pfucbOE );
		}
	return err;
	}


//  ================================================================ 
LOCAL ERR ErrREPAIRBuildCatalogEntryToDeleteList( 
	INFOLIST **ppDeleteList, 
	const ENTRYINFO entryinfo )
//  ================================================================
	{
	//Insert entryinfo into the list based on its objidTable+objType+objidFDP
	
	ERR				err			= JET_errSuccess;
	INFOLIST 	*	pTemp 		= *ppDeleteList;
	INFOLIST 	* 	pInfo;

	pInfo = new INFOLIST;

	if( NULL == pInfo )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}
	
	memset( pInfo, 0, sizeof(INFOLIST) );
	pInfo->info = entryinfo;
	pInfo->pInfoListNext = NULL;

	if( NULL == pTemp // empty list
		|| pTemp->info.objidTable > entryinfo.objidTable 
		|| ( pTemp->info.objidTable == entryinfo.objidTable
			 && pTemp->info.objType > entryinfo.objType )  
		|| ( pTemp->info.objidTable == entryinfo.objidTable
			 && pTemp->info.objType == entryinfo.objType
			 && pTemp->info.objidFDP > entryinfo.objidFDP ) ) 
		{
		// insert into the first record of the list
		pInfo->pInfoListNext = pTemp;
		*ppDeleteList = pInfo;
		}
	else 
		{	
		while( NULL != pTemp->pInfoListNext 
			   && ( pTemp->pInfoListNext->info.objidTable < entryinfo.objidTable 
			   		|| ( pTemp->pInfoListNext->info.objidTable == entryinfo.objidTable
						 && pTemp->pInfoListNext->info.objType < entryinfo.objType ) 
			   		|| ( pTemp->pInfoListNext->info.objidTable == entryinfo.objidTable
			   			 && pTemp->pInfoListNext->info.objType == entryinfo.objType
			   			 && pTemp->pInfoListNext->info.objidFDP < entryinfo.objidFDP ) ) )
			{
			pTemp = pTemp->pInfoListNext;
			}
		pInfo->pInfoListNext = pTemp->pInfoListNext;
		pTemp->pInfoListNext = pInfo;
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRDeleteCorruptedEntriesFromCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const INFOLIST *pTablesToDelete,
	const INFOLIST *pEntriesToDelete,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err				= JET_errSuccess;
	FUCB 		*	pfucbCatalog 	= pfucbNil;
	ENTRYINFO 		entryinfo;

	BOOL			fEntryToDelete  = fFalse;

	BOOL			fSeenSLVAvail	= fFalse;
	BOOL			fSeenSLVOwnerMap= fFalse;

	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm		= NULL;
	dib.dirflag	= fDIRNull;

	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

	Call( ErrDIROpen( ppib, pgnoFDPMSO, ifmp, &pfucbCatalog ) );
	Assert( pfucbNil != pfucbCatalog );
	
	FUCBSetIndex( pfucbCatalog );
	FUCBSetSequential( pfucbCatalog );
	
	err = ErrDIRDown( pfucbCatalog, &dib );

	// if no more records in catalog or no more records to delete, exit
	while ( JET_errNoCurrentRecord != err  
			&& ( pTablesToDelete || pEntriesToDelete ) )
		{
		Call( err );
			
		Call( ErrDIRRelease( pfucbCatalog ) );
		
		memset( &entryinfo, 0, sizeof( entryinfo ) );
		Call( ErrREPAIRRetrieveCatalogColumns( ppib, ifmp, pfucbCatalog, &entryinfo, popts ) );

		while( pTablesToDelete && pTablesToDelete->info.objidTable < entryinfo.objidTable )
			{
			pTablesToDelete = pTablesToDelete->pInfoListNext;
			}
		while( pEntriesToDelete && pEntriesToDelete->info.objidTable < entryinfo.objidTable )
			{
			pEntriesToDelete = pEntriesToDelete->pInfoListNext;
			}

		if( pTablesToDelete && pTablesToDelete->info.objidTable == entryinfo.objidTable )
			{
			// find the corrupted table entries in catalog
			if( objidSystemRoot == entryinfo.objidTable &&
				sysobjSLVAvail == entryinfo.objType ) //special case
				{
				if ( fSeenSLVAvail )
					{
					// find the multiple SLVAvail tree entry in catalog
					fEntryToDelete = fTrue;
					}
				else
					{
					fSeenSLVAvail = fTrue;
					}
				}
			else if( objidSystemRoot == entryinfo.objidTable &&
					 sysobjSLVOwnerMap == entryinfo.objType ) //special case
				{
				if ( fSeenSLVOwnerMap )
					{
					// find the multiple SLVOwnerMap tree entry in catalog
					fEntryToDelete = fTrue;
					}
				else
					{
					fSeenSLVOwnerMap = fTrue;
					}
				}
			else
				{
				fEntryToDelete = fTrue;
				}
			}	
		else if( pEntriesToDelete
				 && pEntriesToDelete->info.objidTable == entryinfo.objidTable 
				 && pEntriesToDelete->info.objidFDP == entryinfo.objidFDP 
				 && ( sysobjIndex == entryinfo.objType 
				 	  || sysobjLongValue == entryinfo.objType ) )
				{
				// find the corrupted entry in catalog 
				fEntryToDelete = fTrue;
				
				pEntriesToDelete = pEntriesToDelete->pInfoListNext;	
				}
		else
			{
			// good entry in catalog
			}

		if( fEntryToDelete )
			{
			// delete the entry in the catalog
			(*popts->pcprintfVerbose)( "Deleting a catalog entry (%d %s)\t\n", 
										entryinfo.objidTable, entryinfo.szName );
			
			Call( ErrDIRDelete( pfucbCatalog, fDIRNoVersion ) );

			fEntryToDelete = fFalse;
			}

		err = ErrDIRNext( pfucbCatalog, fDIRNull );	
		}

	if( JET_errNoCurrentRecord == err
		|| JET_errRecordNotFound == err )
		{
		err = JET_errSuccess;
		}
		
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	
HandleError:
	
	if( pfucbNil != pfucbCatalog )
		{
		DIRClose( pfucbCatalog );
		}

	if( JET_errSuccess != err )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}
	
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertMSOEntriesToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR			err		= JET_errSuccess;
	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	Call( ErrREPAIRCATCreate( 
					ppib, 
					ifmp, 
					objidFDPMSO_NameIndex, 
					objidFDPMSO_RootObjectIndex,
					fTrue ) );
	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
HandleError:
	if( JET_errSuccess != err )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}
	
	return err;	
	}


//  ================================================================
LOCAL ERR ErrREPAIRRepairCatalogs(
	PIB * const ppib,
	const IFMP ifmp,
	OBJID * const pobjidLast,
	const BOOL fCatalogCorrupt,
	const BOOL fShadowCatalogCorrupt, 
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR		err						= JET_errSuccess;

	FUCB	* pfucbParent 			= pfucbNil;
	FUCB	* pfucbCatalog 			= pfucbNil;
	FUCB	* pfucbShadowCatalog 	= pfucbNil;
	FUCB	* pfucbSpace			= pfucbNil;

	QWORD	qwRecords				= 0;

	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	
	if( fCatalogCorrupt || fShadowCatalogCorrupt )
		{
		//  we'll need this for the space
		Call( ErrDIROpen( ppib, pgnoSystemRoot, ifmp, &pfucbParent ) );
		}
		
	if( fCatalogCorrupt && fShadowCatalogCorrupt )
		{
		Call( ErrREPAIRScanDBAndRepairCatalogs( ppib, ifmp, popts ) );

		// Check New Catalog and Delete all records pertaining to a corrupted table
		err = ErrREPAIRCheckFixCatalogLogical( 
					ppib, 
					ifmp, 
					pobjidLast, 
					fFalse, 
					fTrue, 
					&qwRecords,
					popts );
		if( JET_errDatabaseCorrupted == err )
			{
			(*popts->pcprintfVerbose)( "Repaired the logical inconsistency of the catalog\t\n" );
			}
		}
		
	if( fShadowCatalogCorrupt )
		{
		//  if the catalog was corrupted as well it was repaired above
		const PGNO pgnoLast = 32;
		const PGNO cpgOwned = 8 + 1;
		Assert( cpgOwned == cpgMSOShadowInitial );

		(*popts->pcprintf)( "\r\nRebuilding %s from %s.\r\n", szMSOShadow, szMSO );
		popts->psnprog->cunitTotal 	= PgnoLast( ifmp );
		popts->psnprog->cunitDone	= 0;
		(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntBegin, NULL );

		(*popts->pcprintfVerbose)( "rebuilding %s from %s\r\n", szMSOShadow, szMSO );
		
		//  copy from the catalog to the shadow
		Assert( pfucbNil != pfucbParent );
		Call( ErrSPCreateMultiple(
			pfucbParent,
			pgnoSystemRoot,
			pgnoFDPMSOShadow,
			objidFDPMSOShadow,
			pgnoFDPMSOShadow+1,
			pgnoFDPMSOShadow+2,
			pgnoLast,
			cpgOwned,
			fTrue,
			CPAGE::fPagePrimary ) );

		DIRClose( pfucbParent );
		pfucbParent = pfucbNil;

		Call( ErrFILEOpenTable( ppib, ifmp, &pfucbCatalog, szMSO, NO_GRBIT ) );
		Call( ErrFILEOpenTable( ppib, ifmp, &pfucbShadowCatalog, szMSOShadow, NO_GRBIT ) );
		Call( ErrBTCopyTree( pfucbCatalog, pfucbShadowCatalog, fDIRNoLog | fDIRNoVersion ) );

		DIRClose( pfucbCatalog );
		pfucbCatalog = pfucbNil;
		DIRClose( pfucbShadowCatalog );
		pfucbShadowCatalog = pfucbNil;		
		}
	else if( fCatalogCorrupt )
		{
		const PGNO pgnoLast = 23;
		const PGNO cpgOwned = 23 - 3 - 3;	//  3 for system root, 3 for FDP
		Assert( cpgMSOInitial >= cpgOwned );

		(*popts->pcprintf)( "\r\nRebuilding %s from %s.\r\n", szMSO, szMSOShadow );
		popts->psnprog->cunitTotal 	= PgnoLast( ifmp );
		popts->psnprog->cunitDone	= 0;
		(VOID)popts->pfnStatus( (JET_SESID)ppib, JET_snpRepair, JET_sntBegin, NULL );

		(*popts->pcprintfVerbose)( "rebuilding %s from %s\r\n", szMSO, szMSOShadow );
		
		Assert( pfucbNil != pfucbParent );
		//  when we create this we cannot make all the pages available, some will be needed later
		//  for the index FDP's. The easiest thing to do is not add any pages to the AvailExt
		Call( ErrSPCreateMultiple(
			pfucbParent,
			pgnoSystemRoot,
			pgnoFDPMSO,
			objidFDPMSO,
			pgnoFDPMSO+1,
			pgnoFDPMSO+2,
			pgnoFDPMSO+2,
			3,
			fTrue,
			CPAGE::fPagePrimary ) );

		DIRClose( pfucbParent );
		pfucbParent = pfucbNil;
		
		Call( ErrFILEOpenTable( ppib, ifmp, &pfucbCatalog, szMSO, NO_GRBIT ) );

		if ( !pfucbCatalog->u.pfcb->FSpaceInitialized() )
			{
			pfucbCatalog->u.pfcb->SetPgnoOE( pgnoFDPMSO+1 );
			pfucbCatalog->u.pfcb->SetPgnoAE( pgnoFDPMSO+2 );
			pfucbCatalog->u.pfcb->SetSpaceInitialized();
			}
		Call( ErrSPIOpenOwnExt( ppib, pfucbCatalog->u.pfcb, &pfucbSpace ) );
#ifdef REPAIR_DEBUG_VERBOSE_SPACE
		(*popts->pcprintfDebug)( "%s OwnExt: %d pages ending at %d\r\n", szMSO, cpgOwned, pgnoLast );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

		Call( ErrREPAIRInsertRunIntoSpaceTree(
					ppib,
					ifmp,
					pfucbSpace,
					pgnoLast,
					cpgOwned,
					popts ) );

		Call( ErrFILEOpenTable( ppib, ifmp, &pfucbShadowCatalog, szMSOShadow, NO_GRBIT ) );
		Call( ErrBTCopyTree( pfucbShadowCatalog, pfucbCatalog, fDIRNoLog | fDIRNoVersion ) );

		DIRClose( pfucbSpace );
		pfucbSpace = pfucbNil;
		DIRClose( pfucbCatalog );
		pfucbCatalog = pfucbNil;
		DIRClose( pfucbShadowCatalog );
		pfucbShadowCatalog = pfucbNil;		
		}

	if( fCatalogCorrupt || !fShadowCatalogCorrupt )
		{
		//  we don't need to repair the indexes if just the shadow catalog was corrupt
		//  otherwise (i.e. the catalog was corrupt or neither catalog was corrupt) we
		//  need to rebuild the indexes
		(*popts->pcprintfVerbose)( "rebuilding indexes for %s\r\n", szMSO );

		REPAIRTABLE repairtable;
		memset( &repairtable, 0, sizeof( REPAIRTABLE ) );
		repairtable.objidFDP = objidFDPMSO;
		repairtable.objidLV	 = objidNil;
		repairtable.pgnoFDP  = pgnoFDPMSO;
		repairtable.pgnoLV   = pgnoNull;
		repairtable.fHasPrimaryIndex = fTrue;
		strcpy( repairtable.szTableName, szMSO );
				
		//  we should be able to open the catalog without referring to the catalog
		Call( ErrFILEOpenTable( ppib, ifmp, &pfucbCatalog, szMSO, NO_GRBIT ) );
		FUCBSetSystemTable( pfucbCatalog );
		Call( ErrREPAIRBuildAllIndexes( ppib, ifmp, &pfucbCatalog, &repairtable, popts ) );
		}
	
HandleError:
	if( pfucbNil != pfucbParent )
		{
		DIRClose( pfucbParent );
		}
	if( pfucbNil != pfucbCatalog )
		{
		DIRClose( pfucbCatalog );
		}
	if( pfucbNil != pfucbShadowCatalog )
		{
		DIRClose( pfucbShadowCatalog );
		}
	if( pfucbNil != pfucbSpace )
		{
		DIRClose( pfucbSpace );
		}

	CallS( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRScanDBAndRepairCatalogs(
	PIB * const ppib,
	const IFMP ifmp,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  This is called if both copies of the system catalog are corrupt
//  we extract all the pages belonging to _either_ the catalog or
//  the shadow catalog and then take the union of their records (removing
//  duplicates).
//
//-
	{
	const JET_SESID sesid	= reinterpret_cast<JET_SESID>( ppib );
	const CPG cpgPreread 	= 256;
	const PGNO	pgnoFirst 	= 1;
	const PGNO	pgnoLast	= PgnoLast( ifmp );
	
	ERR	err = JET_errSuccess;

	CPG cpgRemaining;

	CPG	cpgUninitialized 	= 0;
	CPG	cpgBad 				= 0;
	
	PGNO	pgno			= pgnoFirst;
	
	INT		cRecords			= 0;
	INT		cRecordsDuplicate	= 0;

	JET_COLUMNDEF	rgcolumndef[2] = {
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypLongBinary, 0, 0, 0, 0, JET_cbPrimaryKeyMost, JET_bitColumnTTKey },	//	KEY
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypLongBinary, 0, 0, 0, 0, 0, 0 },										//	DATA
		};	

	JET_TABLEID tableid;
	JET_COLUMNID rgcolumnid[2];
	const JET_COLUMNID& columnidKey	 = rgcolumnid[0];
	const JET_COLUMNID& columnidData = rgcolumnid[1];
	
	Call( ErrIsamOpenTempTable(
		reinterpret_cast<JET_SESID>( ppib ),
		rgcolumndef,
		sizeof( rgcolumndef ) / sizeof( rgcolumndef[0] ),
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable | JET_bitTTUpdatable,
		&tableid,
		rgcolumnid ) );

	(*popts->pcprintf)( "\r\nScanning the database catalog.\r\n" );
	(*popts->pcprintfVerbose)( "scanning the database for catalog records from page %d to page %d\r\n", pgnoFirst, pgnoLast );		

	popts->psnprog->cunitTotal = pgnoLast;
	popts->psnprog->cunitDone = 0;
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntBegin, NULL );	
	
	BFPrereadPageRange( ifmp, pgnoFirst, min(cpgPreread * 2,pgnoLast-1) );
	cpgRemaining = cpgPreread;

	while( pgnoLast	!= pgno )
		{
		if( 0 == --cpgRemaining )
			{
			popts->psnprog->cunitDone = pgno;
			(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
			if( ( pgno + ( cpgPreread * 2 ) ) < pgnoLast )
				{
				BFPrereadPageRange( ifmp, pgno + cpgPreread, cpgPreread );
				}
			cpgRemaining = cpgPreread;
			}
			
		CSR	csr;
		err = csr.ErrGetReadPage( 
					ppib, 
					ifmp,
					pgno,
					bflfNoTouch );

		if( JET_errPageNotInitialized == err )
			{
			err = JET_errSuccess;
			}
		else if( JET_errReadVerifyFailure == err || JET_errDiskIO == err )
			{
			err = JET_errSuccess;
			}
		else if( err >= 0 )
			{
			if( ( 	csr.Cpage().ObjidFDP() == objidFDPMSO
					|| csr.Cpage().ObjidFDP() == objidFDPMSOShadow )
				&& csr.Cpage().FLeafPage()
				&& !csr.Cpage().FSpaceTree()
				&& !csr.Cpage().FEmptyPage()
				&& !csr.Cpage().FRepairedPage()
				&& csr.Cpage().Clines() > 0
				&& csr.Cpage().FPrimaryPage() 
				&& !csr.Cpage().FSLVOwnerMapPage() 	
				&& !csr.Cpage().FSLVAvailPage()  	
				&& !csr.Cpage().FLongValuePage() )
				{
				err = ErrREPAIRIFixLeafPage( 
							ppib, 
							ifmp,
							csr, 
					#ifdef SYNC_DEADLOCK_DETECTION
							NULL,
					#endif  //  SYNC_DEADLOCK_DETECTION
							popts );
				if( err < 0 )
					{
					(*popts->pcprintfError)( "page %d: err %d. discarding page\r\n", pgno, err );

					UtilReportEvent(
							eventWarning,
							REPAIR_CATEGORY,
							REPAIR_BAD_PAGE_ID,
							0, NULL );

					//  this page is not usable. skip it
					
					err = JET_errSuccess;
					}
				else if( 0 == csr.Cpage().Clines() )
					{
					(*popts->pcprintfError)( "page %d: all records were bad. discarding page\r\n", pgno );

					UtilReportEvent(
							eventWarning,
							REPAIR_CATEGORY,
							REPAIR_BAD_PAGE_ID,
							0, NULL );

					//  this page is now empty. skip it
					
					err = JET_errSuccess;
					goto HandleError;
					}					
				else
					{
					//  a non-empty leaf page of one of the catalogs. copy the records into the temp table
					INT iline;
					for( iline = 0;
						iline < csr.Cpage().Clines() && err >= 0;
						++iline )
						{
						KEYDATAFLAGS kdf;
						NDIGetKeydataflags( csr.Cpage(), iline, &kdf );
						if( !FNDDeleted( kdf ) )
							{
							++cRecords;

							//	X5:102291
							//
							//	a ranking violation assert occurs if we attempt
							//	to do the insert with a page latched (thanks
							//	to andygo)
							//
							//	copy the information to a separate page before inserting it
							//
							//	UNDONE:	consider skipping this step in retail as it is
							//	only working around an assert
							
							BYTE rgb[g_cbPageMax];
							BYTE * pb = rgb;
							
							memcpy( pb, kdf.key.prefix.Pv(), kdf.key.prefix.Cb() );
							kdf.key.prefix.SetPv( pb );
							pb += kdf.key.prefix.Cb();

							memcpy( pb, kdf.key.suffix.Pv(), kdf.key.suffix.Cb() );
							kdf.key.suffix.SetPv( pb );
							pb += kdf.key.suffix.Cb();
							
							memcpy( pb, kdf.data.Pv(), kdf.data.Cb() );
							kdf.data.SetPv( pb );
							pb += kdf.data.Cb();

							csr.ReleasePage( fFalse );							

							err = ErrREPAIRInsertCatalogRecordIntoTempTable(
									ppib,
									ifmp,
									kdf,
									tableid,
									columnidKey,
									columnidData,
									popts );
									
							if( JET_errKeyDuplicate == err )
								{
								++cRecordsDuplicate;
								err = JET_errSuccess;
								}

							Call( csr.ErrGetReadPage( 
										ppib, 
										ifmp,
										pgno,
										bflfNoTouch ) );
								
							}
						}
					}
				}			
			}
			
		csr.ReleasePage( fTrue );
		csr.Reset();
		Call( err );
		++pgno;
		}

	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntComplete, NULL );
	(*popts->pcprintfVerbose)( "%d catalog records found. %d unique\r\n", cRecords, cRecords - cRecordsDuplicate );		

	//  Now we have to insert the records back into the catalog
	(*popts->pcprintf)( "\r\nRebuilding %s.\r\n", szMSO );

	popts->psnprog->cunitTotal = cRecords - cRecordsDuplicate;
	popts->psnprog->cunitDone = 0;
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntBegin, NULL );	

	Call( ErrREPAIRCopyTempTableToCatalog(
				ppib,
				ifmp,
				tableid,
				columnidKey,
				columnidData,
				popts ) );
		
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntComplete, NULL );

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertCatalogRecordIntoTempTable(
	PIB * const ppib,
	const IFMP ifmp,
	const KEYDATAFLAGS& kdf,
	const JET_TABLEID tableid,
	const JET_COLUMNID columnidKey,
	const JET_COLUMNID columnidData,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	const JET_SESID sesid = (JET_SESID)ppib;
	
	JET_ERR err = JET_errSuccess;
					
	Call( ErrDispPrepareUpdate(
				sesid,
				tableid,
				JET_prepInsert ) );

	BYTE rgbKey[JET_cbPrimaryKeyMost];
	kdf.key.CopyIntoBuffer( rgbKey, sizeof( rgbKey ) );

	Call( ErrDispSetColumn(
				sesid, 
				tableid, 
				columnidKey,
				rgbKey, 
				kdf.key.Cb(),
				0, 
				NULL ) );

	Call( ErrDispSetColumn(
				sesid, 
				tableid, 
				columnidData,
				kdf.data.Pv(), 
				kdf.data.Cb(),
				0, 
				NULL ) );

	err = ErrDispUpdate( sesid, tableid, NULL, 0, NULL,	0 );
	if( err < 0 )
		{
		CallS( ErrDispPrepareUpdate( sesid, tableid, JET_prepCancel ) );
		}
	
HandleError:
	return err;
	}
	

//  ================================================================
LOCAL ERR ErrREPAIRCopyTempTableToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	const JET_TABLEID tableid,
	const JET_COLUMNID columnidKey,
	const JET_COLUMNID columnidData,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Copy from the temp table to the catalog. The progress bar should have been initialized 
//  before and should be terminated afterwards.
//
//-
	{
	const PGNO pgnoLast = 23;
	const PGNO cpgOwned = 23 - 3 - 3;	//  3 for system root, 3 for FDP
	Assert( cpgMSOInitial >= cpgOwned );
	const JET_SESID sesid = reinterpret_cast<JET_SESID>( ppib );
	
	JET_ERR	err				= JET_errSuccess;

	FUCB	* pfucbParent	= pfucbNil;
	FUCB	* pfucbCatalog	= pfucbNil;
	FUCB	* pfucbSpace	= pfucbNil;

	//  UNDONE: this could be done in just one buffer, but this makes it easier
	
	VOID * pvKey = NULL;
	BFAlloc( &pvKey );
	VOID * pvData = NULL;
	BFAlloc( &pvData );
	
	Call( ErrDIROpen( ppib, pgnoSystemRoot, ifmp, &pfucbParent ) );
	Assert( pfucbNil != pfucbParent );

	//  when we create this we cannot make all the pages available, some will be needed later
	//  for the index FDP's. The easiest thing to do is not add any pages to the AvailExt
	Call( ErrSPCreateMultiple(
		pfucbParent,
		pgnoSystemRoot,
		pgnoFDPMSO,
		objidFDPMSO,
		pgnoFDPMSO+1,
		pgnoFDPMSO+2,
		pgnoFDPMSO+2,
		3,
		fTrue,
		CPAGE::fPagePrimary ) );

	DIRClose( pfucbParent );
	pfucbParent = pfucbNil;

	Call( ErrFILEOpenTable( ppib, ifmp, &pfucbCatalog, szMSO, NO_GRBIT ) );
	
	if ( !pfucbCatalog->u.pfcb->FSpaceInitialized() )
		{
		pfucbCatalog->u.pfcb->SetPgnoOE( pgnoFDPMSO+1 );
		pfucbCatalog->u.pfcb->SetPgnoAE( pgnoFDPMSO+2 );
		pfucbCatalog->u.pfcb->SetSpaceInitialized();
		}
	Call( ErrSPIOpenOwnExt( ppib, pfucbCatalog->u.pfcb, &pfucbSpace ) );
#ifdef REPAIR_DEBUG_VERBOSE_SPACE
	(*popts->pcprintfDebug)( "%s OwnExt: %d pages ending at %d\r\n", szMSO, cpgOwned, pgnoLast );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

	Call( ErrREPAIRInsertRunIntoSpaceTree(
				ppib,
				ifmp,
				pfucbSpace,
				pgnoLast,
				cpgOwned,
				popts ) );

	DIRClose( pfucbSpace );
	pfucbSpace = pfucbNil;

	LONG cRow;
	for( cRow = JET_MoveFirst;
		 ( err = ErrDispMove( sesid, tableid, cRow, NO_GRBIT ) ) == JET_errSuccess;
		 cRow = JET_MoveNext )
		{
		KEY key;
		DATA data;

		ULONG cbKey;
		ULONG cbData;

		Call( ErrDispRetrieveColumn( sesid, tableid, columnidKey, pvKey, g_cbPage, &cbKey, NO_GRBIT, NULL ) );
		Call( ErrDispRetrieveColumn( sesid, tableid, columnidData, pvData, g_cbPage, &cbData, NO_GRBIT, NULL ) );

		key.prefix.Nullify();
		key.suffix.SetPv( pvKey );
		key.suffix.SetCb( cbKey );
		data.SetPv( pvData );
		data.SetCb( cbData );
		
		Call( ErrBTInsert( pfucbCatalog, key, data, fDIRNoVersion|fDIRAppend, NULL ) );
		BTUp( pfucbCatalog );

		++(popts->psnprog->cunitDone);
		(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
		}
	if( JET_errNoCurrentRecord == err )
		{
		//  we moved off the end of the table
		err = JET_errSuccess;
		}
		
	DIRClose( pfucbCatalog );
	pfucbCatalog = pfucbNil;
			
HandleError:
	BFFree( pvKey );
	BFFree( pvData );
	if( pfucbNil != pfucbParent )
		{
		DIRClose( pfucbParent );
		}
	if( pfucbNil != pfucbCatalog )
		{
		DIRClose( pfucbCatalog );
		}
	if( pfucbNil != pfucbSpace )
		{
		DIRClose( pfucbSpace );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRepairDatabase(
			PIB * const ppib,
			const CHAR * const szDatabase,
			const CHAR * const szSLV,
			const CPG cpgSLV,
			IFMP * const pifmp,
			const OBJID objidLast,
			const PGNO pgnoLastOE,
			REPAIRTABLE * const prepairtable,
			const BOOL fRepairedCatalog,
			BOOL fRepairGlobalSpace,
			const BOOL fRepairSLVSpace,
			TTARRAY * const pttarrayOwnedSpace,
			TTARRAY * const pttarrayAvailSpace,
			TTARRAY * const pttarraySLVAvail,
			TTARRAY * const pttarraySLVChecksumLengths,
			TTARRAY	* const pttarraySLVOwnerMapColumnid,
			TTARRAY	* const pttarraySLVOwnerMapKey,		
			TTARRAY * const pttarraySLVChecksumsFromFile,
			TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,
			const REPAIROPTS * const popts )
//  ================================================================
	{
	Assert( !(popts->grbit & JET_bitDBUtilOptionDontRepair ) );

	ERR err = JET_errSuccess;

	const JET_SESID sesid = (JET_SESID)ppib;
	
	REPAIRTT repairtt;	
	memset( &repairtt, 0, sizeof( REPAIRTT ) );
	repairtt.tableidBadPages	= JET_tableidNil;
	repairtt.tableidAvailable	= JET_tableidNil;
	repairtt.tableidOwned		= JET_tableidNil;
	repairtt.tableidUsed		= JET_tableidNil;

	REPAIRTABLE * 	prepairtableT			= NULL;
	DBTIME 			dbtimeLast 				= 0;
	const OBJID 	objidFDPMin				= 5; // normal table should have > 5 objidFDP
	OBJID			objidFDPLast			= objidLast; // objidLast from catalog
	PGNO			pgnoLastOESeen			= pgnoLastOE;

	ERR				errRebuildSLVSpaceTrees	= JET_errSuccess;
	INT				cTablesToRepair			= 0;
	
	OBJIDLIST 	objidlist;

	//  get a list of objids we'll be repairing
	
	prepairtableT = prepairtable;
	while( prepairtableT )
		{
		++cTablesToRepair;
		Call( objidlist.ErrAddObjid( prepairtableT->objidFDP ) );
		prepairtableT = prepairtableT->prepairtableNext;
		}
	objidlist.Sort();

	//  scan the database
		
	(*popts->pcprintf)( "\r\nScanning the database.\r\n"  );
	(*popts->pcprintfVerbose).Indent();
	Call( ErrREPAIRAttachForRepair( 
			sesid, 
			szDatabase, 
			szSLV, 
			pifmp, 
			dbtimeLast, 
			objidFDPLast, 
			popts ) );
	Call( ErrREPAIRCreateTempTables( ppib, fRepairGlobalSpace, &repairtt, popts ) );
	Call( ErrREPAIRScanDB(
			ppib,
			*pifmp,
			&repairtt,
			&dbtimeLast,
			&objidFDPLast, 
			&pgnoLastOESeen,
			prepairtable,
			pttarrayOwnedSpace,
			pttarrayAvailSpace,
			popts ) );
	(*popts->pcprintfVerbose).Unindent();

	if( 0 == dbtimeLast )
		{
		(*popts->pcprintfError)( "dbtimeLast is 0!\r\n" );
		Call( ErrERRCheck( JET_errInternalError ) );
		}	

	if( objidFDPMin > objidFDPLast )
		{
		(*popts->pcprintfError)( "objidLast is %d!\r\n", objidFDPLast );
		Call( ErrERRCheck( JET_errInternalError ) );
		}

	if( pgnoLastOESeen > pgnoLastOE )
		{
		(*popts->pcprintfError)( "Global space tree is too small (has %d pages, seen %d pages). "
								 "The space tree will be rebuilt\r\n",
								 pgnoLastOE, pgnoLastOESeen );
		fRepairGlobalSpace = fTrue;
		}

	//  set sequential access for the temp tables that we will be using
	
	FUCBSetSequential( (FUCB *)(repairtt.tableidAvailable) );
	FUCBSetPrereadForward( (FUCB *)(repairtt.tableidAvailable), cpgPrereadSequential );
	FUCBSetSequential( (FUCB *)(repairtt.tableidOwned) );
	FUCBSetPrereadForward( (FUCB *)(repairtt.tableidOwned), cpgPrereadSequential );
	FUCBSetSequential( (FUCB *)(repairtt.tableidUsed) );
	FUCBSetPrereadForward( (FUCB *)(repairtt.tableidUsed), cpgPrereadSequential );
			
	//  attach and set dbtimeLast and objidLast
	
	Call( ErrREPAIRAttachForRepair( 
			sesid,
			szDatabase, 
			szSLV, 
			pifmp, 
			dbtimeLast, 
			objidFDPLast, 
			popts ) );

	// Check new catalog and add system table entries if they did not exist 
	if( fRepairedCatalog )
		{		
		Call( ErrREPAIRInsertMSOEntriesToCatalog( ppib, *pifmp, popts ) );
		}

	(*popts->pcprintf)( "\r\nRepairing damaged tables.\r\n"  );
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntBegin, NULL );	

	//  for the progress baruse the number of things we are going to repair
	//  (a really bad approximation, but better than nothing)
	
	popts->psnprog->cunitTotal 	= 0;
	popts->psnprog->cunitDone 	= 0;
	popts->psnprog->cunitTotal	= cTablesToRepair;

	if( fRepairGlobalSpace )
		{
		++(popts->psnprog->cunitTotal);
		}

	if( fRepairSLVSpace )
		{
		++(popts->psnprog->cunitTotal);
		}

	if( fRepairGlobalSpace )
		{
		Assert( 0 == popts->psnprog->cunitDone );
		Call( ErrREPAIRRepairGlobalSpace( ppib, *pifmp, popts ) );
		++(popts->psnprog->cunitDone);
		(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
		(VOID)ErrBFFlush( *pifmp );
		}

	//  if necessary, rebuild the SLVSpaceTrees using the tables that are not being repaired
	
	if( fRepairSLVSpace )
		{
#ifdef DISABLE_SLV
		Call( ErrERRCheck( JET_wrnNyi ) )
#else
		Call( ErrREPAIRRebuildSLVSpaceTrees(
				ppib,
				*pifmp,
				szSLV,
				cpgSLV,
				pttarraySLVAvail,
				pttarraySLVChecksumLengths,
				pttarraySLVOwnerMapColumnid,
				pttarraySLVOwnerMapKey,
				pttarraySLVChecksumsFromFile,
				pttarraySLVChecksumLengthsFromSpaceMap,
				&objidlist,
				popts ) );
		++(popts->psnprog->cunitDone);
		(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
		(VOID)ErrBFFlush( *pifmp );
#endif
		}

	if( prepairtable )
		{
		//	delete the unicode fixup table (MSU) and reset the fixup flag on all indexes
		(*popts->pcprintf)( "\r\nDeleting unicode fixup table.\r\n"  );
		Call( ErrCATDeleteMSU( ppib, *pifmp ) );
		}

	(*popts->pcprintfVerbose).Indent();
	prepairtableT = prepairtable;
	while( prepairtableT )
		{
		if( FCATUnicodeFixupTable( prepairtableT->szTableName ) )
			{
			(*popts->pcprintfVerbose)( "table \"%s\" is a system table and does not need to be repaired\r\n", prepairtableT->szTableName );		
			}
		else
			{
			//  CONSIDER:  multi-thread the repair code. the main issue to deal with is 
			//  the call to DetachDatabase() which purges the FCBs. A more selective
			//  purge call should suffice. Also, any template tables should probably
			//  be repaired first, to avoid changing the FCB of a template table while
			//  a derived table is being repaired
			
			//  we are going to be changing pgnoFDPs so we need to purge all FCBs
			
			FCB::DetachDatabase( *pifmp, fFalse );
			Call( ErrREPAIRRepairTable( ppib, *pifmp, &repairtt, pttarraySLVAvail, prepairtableT, popts ) );

			//  Flush the entire database so that if we crash here we don't have to repair this
			//  table again
			
			(VOID)ErrBFFlush( *pifmp );
			}

		prepairtableT = prepairtableT->prepairtableNext;
		++(popts->psnprog->cunitDone);
		(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntProgress, popts->psnprog );	
		}
	(*popts->pcprintfVerbose).Unindent();

	//  CONSIDER:  add the pages in the BadPages TT to the OwnExt of a special table
	
	(VOID)popts->pfnStatus( sesid, JET_snpRepair, JET_sntComplete, NULL );	
	
HandleError:
	
	if( JET_tableidNil != repairtt.tableidBadPages )
		{
		CallS( ErrDispCloseTable( sesid, repairtt.tableidBadPages ) );
		repairtt.tableidBadPages = JET_tableidNil;
		}
	if( JET_tableidNil != repairtt.tableidAvailable )
		{
		CallS( ErrDispCloseTable( sesid, repairtt.tableidAvailable ) );
		repairtt.tableidAvailable = JET_tableidNil;
		}
	if( JET_tableidNil != repairtt.tableidOwned )
		{
		CallS( ErrDispCloseTable( sesid, repairtt.tableidOwned ) );
		repairtt.tableidOwned = JET_tableidNil;
		}
	if( JET_tableidNil != repairtt.tableidUsed )
		{
		CallS( ErrDispCloseTable( sesid, repairtt.tableidUsed ) );
		repairtt.tableidUsed = JET_tableidNil;
		}

	if ( *pifmp != ifmpMax )
		{
		SLVClose( *pifmp );	
		}
	
	return err;
	}


#ifdef DISABLE_SLV
#else

//  ================================================================
LOCAL ERR ErrREPAIRDeleteSLVSpaceTrees(
			PIB * const ppib,
			const IFMP ifmp,
			const REPAIROPTS * const popts )
//  ================================================================
//
//  The space used by the old trees is not reclaimed, the catalog
//	entries are simply removed.
//
//-
	{
	ERR err = JET_errSuccess;

	err = ErrCATDeleteDbObject( ppib, ifmp, szSLVAvail, sysobjSLVAvail );
	if( err < 0 )
		{
		(*popts->pcprintfVerbose)( "error %d trying to delete the SLVAvail tree\r\n", err );
		}
		
	err = ErrCATDeleteDbObject( ppib, ifmp, szSLVOwnerMap, sysobjSLVOwnerMap );
	if( err < 0 )
		{
		(*popts->pcprintfVerbose)( "error %d trying to delete the SLVOwnerMap tree\r\n", err );
		}
	
	return JET_errSuccess;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCreateSLVSpaceTrees(
			PIB * const ppib,
			const IFMP ifmp,
			const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	FUCB * pfucbDb = pfucbNil;

	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	
	(*popts->pcprintfVerbose)( "Creating SLV space trees\r\n" );
	
	Call( ErrDIROpen( ppib, pgnoSystemRoot, ifmp, &pfucbDb ) );
	
	Call( ErrSLVCreateAvailMap( ppib, pfucbDb ) );
	Call( ErrSLVCreateOwnerMap( ppib, pfucbDb ) );

	Assert( pfcbNil != rgfmp[ifmp].PfcbSLVAvail() );
	Assert( pfcbNil != rgfmp[ifmp].PfcbSLVOwnerMap() );

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	
HandleError:
	if( NULL != pfucbDb )
		{
		DIRClose( pfucbDb );
		}
	if( err < 0 )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRebuildSLVSpaceTrees(
			PIB * const ppib,
			const IFMP ifmp,
			const CHAR * const szSLV,
			const CPG cpgSLV,
			TTARRAY * const pttarraySLVAvail,
			TTARRAY * const pttarraySLVChecksumLengths,
			TTARRAY * const pttarraySLVOwnerMapColumnid,
			TTARRAY * const pttarraySLVOwnerMapKey,
			TTARRAY * const pttarraySLVChecksumsFromFile,
			TTARRAY * const pttarraySLVChecksumLengthsFromSpaceMap,			
			const OBJIDLIST * const pobjidlist,
			const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	//  no need to worry about deadlock between these TTARRAYs because they are
	//  only being accessed by this thread for the duration of this function call
	
	TTARRAY::RUN availRun;
	TTARRAY::RUN columnidRun;
	TTARRAY::RUN keyRun;
	TTARRAY::RUN lengthRun;
	TTARRAY::RUN checksumsFromFileRun;
	TTARRAY::RUN checksumsLengthsFromSpaceMapRun;
	
	INT pgno 	= 1;	
	INT ipage 	= 0;

	SLVOWNERMAPNODE			slvownermapNode;

	FCB * pfcbSLVAvail 		= pfcbNil;	
	FUCB * pfucbSLVAvail 	= pfucbNil;
	
	pttarraySLVAvail->BeginRun( ppib, &availRun );
	pttarraySLVOwnerMapColumnid->BeginRun( ppib, &columnidRun );
	pttarraySLVOwnerMapKey->BeginRun( ppib, &keyRun );
	pttarraySLVChecksumLengths->BeginRun( ppib, &lengthRun );
	pttarraySLVChecksumsFromFile->BeginRun( ppib, &checksumsFromFileRun );
	pttarraySLVChecksumLengthsFromSpaceMap->BeginRun( ppib, &checksumsLengthsFromSpaceMapRun );

	//	Delete the old SLV trees

	Call( ErrREPAIRDeleteSLVSpaceTrees( ppib, ifmp, popts ) );

	//	re-create the SLV FCBs in the FMP

	Call( ErrREPAIRCreateSLVSpaceTrees( ppib, ifmp, popts ) );

	pfcbSLVAvail = rgfmp[ifmp].PfcbSLVAvail();
	Assert( pfcbNil != pfcbSLVAvail );

	//  Open the SVLAvail Tree
	
	Call( ErrBTOpen( ppib, pfcbSLVAvail, &pfucbSLVAvail, fFalse ) );
	Assert( pfucbNil != pfucbSLVAvail );

	//	These entries were created when we created the SLV trees

	DIB dib;
	dib.pos = posFirst;
	dib.pbm = NULL;
	dib.dirflag = fDIRNull;

	Call( ErrBTDown( pfucbSLVAvail, &dib, latchRIW ) );
	Call( Pcsr( pfucbSLVAvail )->ErrUpgrade() );
	
	while( pgno <= cpgSLVFileMin )
		{
		OBJID objidOwning;
		Call( pttarraySLVAvail->ErrGetValue( ppib, pgno, &objidOwning, &availRun ) );

		if( objidNil == objidOwning
			|| objidInvalid == objidOwning
			|| pobjidlist->FObjidPresent( objidOwning ) )
			{
			//	leave this entry empty
			}
		else
			{
			
			//  used page

			INT 			iul;
			ULONG 			ulChecksum 			= 0;
			ULONG			cbChecksumLength 	= 0;
			ULONG			cbChecksumLengthFromSpaceMap = 0;
			COLUMNID 		columnid;
			ULONG 			rgulKey[culSLVKeyToStore];
			BYTE * const 	pbKey = (BYTE *)rgulKey;

			BOOKMARK bm;			
			bm.key.Nullify();
			bm.key.suffix.SetPv( pbKey + 1 );
			bm.data.Nullify();
			
			Call( pttarraySLVChecksumsFromFile->ErrGetValue( ppib, pgno, &ulChecksum, &checksumsFromFileRun ) );
			Call( pttarraySLVOwnerMapColumnid->ErrGetValue( ppib, pgno, &columnid, &columnidRun ) );
			Call( pttarraySLVChecksumLengths->ErrGetValue( ppib, pgno, &cbChecksumLength, &lengthRun ) );
			Call( pttarraySLVChecksumLengthsFromSpaceMap->ErrGetValue(
					ppib,
					pgno,
					&cbChecksumLengthFromSpaceMap,
					&checksumsLengthsFromSpaceMapRun ) );
			for( iul = 0; iul < culSLVKeyToStore; ++iul )
				{
				Call( pttarraySLVOwnerMapKey->ErrGetValue(
											ppib,
											pgno * culSLVKeyToStore + iul,
											rgulKey + iul,
											&keyRun ) );
				}			

			bm.key.suffix.SetCb( *pbKey );

#ifdef REPAIR_DEBUG_VERBOSE_STREAMING
			Assert( bm.key.prefix.FNull() );
			char szBM[256];
						
			REPAIRDumpHex(
				szBM,
				sizeof( szBM ),
				(BYTE *)bm.key.suffix.Pv(),
				bm.key.suffix.Cb() );
			(*popts->pcprintfVerbose)( "streaming file page %d belongs to objid %d, bookmark %s, columnid %d, checksum %x, checksum length %d\r\n",
											pgno,
											objidOwning,
											szBM,
											columnid,
											ulChecksum,
											cbChecksumLength );	
#endif	//	REPAIR_DEBUG_VERBOSE_STREAMING
			
			//  insert the information into the SLVOwnerMap tree
			//	mark the page as used in the SLVAvail tree

			Call( slvownermapNode.ErrCreateForSet( ifmp, pgno, objidOwning, columnid, &bm ) );

			if( cbChecksumLength != cbChecksumLengthFromSpaceMap )
				{

				//	the checksum was invalid

				slvownermapNode.ResetFValidChecksum();				
				
				}
			else
				{
				if( 0 != cbChecksumLength )
					{
					slvownermapNode.SetUlChecksum( ulChecksum );
					slvownermapNode.SetCbDataChecksummed( static_cast<USHORT>( cbChecksumLength ) );
					slvownermapNode.SetFValidChecksum();
					}
				else
					{
					slvownermapNode.ResetFValidChecksum();
					}
				}

			//	UNDONE:  bundle these into contigous runs to reduce the number of calls
			//	to ErrBTMungeSLVSpace by increasing the cpages count
			
			Call( ErrBTMungeSLVSpace(
				pfucbSLVAvail,
				slvspaceoperFreeToCommitted,
				ipage,
				1,
				fDIRNull ) );			
			Assert( Pcsr( pfucbSLVAvail )->FLatched() );

			Call( slvownermapNode.ErrSetData( ppib, fTrue ) );		
			}
		++pgno;
		++ipage;
		}

	BTUp( pfucbSLVAvail );
	slvownermapNode.ResetCursor();

	//	these have no entries yet
	
	Assert( cpgSLVFileMin + 1 == pgno );
	while( pgno <= cpgSLV )
		{
		if( ( pgno % SLVSPACENODE::cpageMap ) == 1 )
			{
			
			//  insert a new SLVSPACENODE entry

			const PGNO pgnoLast = pgno + SLVSPACENODE::cpageMap - 1;

			Call( ErrSLVInsertSpaceNode( pfucbSLVAvail, pgnoLast ) );
			Assert( Pcsr( pfucbSLVAvail )->FLatched() );
			
			ipage = 0;
			}

		Assert( Pcsr( pfucbSLVAvail )->FLatched() );
			
		OBJID objidOwning;
		Call( pttarraySLVAvail->ErrGetValue( ppib, pgno, &objidOwning, &availRun ) );

		Call( slvownermapNode.ErrCreateForSearch( ifmp, pgno ) );
		Call( slvownermapNode.ErrNew( ppib ) );
		slvownermapNode.ResetCursor();

		if( objidNil == objidOwning )
			{
			
			//  unused page. create an empty space map entry
			
			}
		else if( objidInvalid == objidOwning )
			{

			//	this page has a bad checksum. create an empty space map entry
			
			}
		else if( pobjidlist->FObjidPresent( objidOwning ) )
			{

			//  this page is used by a corrupted table. we'll fix up these entries while repairing the table
			//  create an empty space map entry

			}
		else
			{
			
			//  used page

			INT 			iul;
			ULONG 			ulChecksum = 0;
			COLUMNID 		columnid;
			ULONG 			rgulKey[culSLVKeyToStore];
			BYTE * const 	pbKey = (BYTE *)rgulKey;

			BOOKMARK bm;			
			bm.key.Nullify();
			bm.key.suffix.SetPv( pbKey + 1 );
			bm.data.Nullify();
			
			Call( pttarraySLVOwnerMapColumnid->ErrGetValue( ppib, pgno, &columnid, &columnidRun ) );
			for( iul = 0; iul < culSLVKeyToStore; ++iul )
				{
				Call( pttarraySLVOwnerMapKey->ErrGetValue(
											ppib,
											pgno * culSLVKeyToStore + iul,
											rgulKey + iul,
											&keyRun ) );
				}			

			bm.key.suffix.SetCb( *pbKey );

#ifdef REPAIR_DEBUG_VERBOSE_STREAMING
			Assert( bm.key.prefix.FNull() );
			char szBM[256];
			REPAIRDumpHex(
				szBM,
				sizeof( szBM ),
				(BYTE *)bm.key.suffix.Pv(),
				bm.key.suffix.Cb() );
			(*popts->pcprintfVerbose)( "streaming file page %d belongs to objid %d, bookmark %s, columnid %d\r\n",
											pgno,
											objidOwning,
											szBM,
											columnid );	
#endif	//	REPAIR_DEBUG_VERBOSE_STREAMING
			
			//  insert the information into the SLVOwnerMap tree			
			//	mark the page as used in the SLVAvail tree

			Call( slvownermapNode.ErrCreateForSet( ifmp, pgno, objidOwning, columnid, &bm ) );
			Call( slvownermapNode.ErrSetData( ppib, fTrue ) );
			slvownermapNode.ResetCursor();

			//	UNDONE:  bundle these into contigous runs to reduce the number of calls
			//	to ErrBTMungeSLVSpace by increasing the cpages count
			
			Call( ErrBTMungeSLVSpace(
				pfucbSLVAvail,
				slvspaceoperFreeToCommitted,
				ipage,
				1,
				fDIRNull ) );			
			Assert( Pcsr( pfucbSLVAvail )->FLatched() );
			}

		++pgno;
		++ipage;
		}
		
HandleError:

	pttarraySLVAvail->EndRun( ppib, &availRun );
	pttarraySLVOwnerMapColumnid->EndRun( ppib, &columnidRun );
	pttarraySLVOwnerMapKey->EndRun( ppib, &keyRun );
	pttarraySLVChecksumLengths->EndRun( ppib, &lengthRun );
	pttarraySLVChecksumsFromFile->EndRun( ppib, &checksumsFromFileRun );
	pttarraySLVChecksumLengthsFromSpaceMap->EndRun( ppib, &checksumsLengthsFromSpaceMapRun );

	if( pfucbNil != pfucbSLVAvail )
		{
		BTClose( pfucbSLVAvail );
		}

	return err;
	}

#endif	//	DISABLE_SLV

//  ================================================================
LOCAL ERR ErrREPAIRRepairTable(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTT * const prepairtt,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR 	err 				= JET_errSuccess;
	FUCB	* pfucb				= pfucbNil;
	FUCB	* pfucbSLVAvail		= pfucbNil;
	FUCB	* pfucbSLVOwnerMap	= pfucbNil;
	
	FDPINFO	fdpinfo;

	(*popts->pcprintfVerbose)( "repairing table \"%s\"\r\n", prepairtable->szTableName );
	(*popts->pcprintfVerbose).Indent();
	
	Call( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

	if( prepairtable->fRepairTable )
		{
		(*popts->pcprintfVerbose)( "rebuilding data\r\n" );
		Call( ErrREPAIRRebuildBT(
					ppib,
					ifmp,
					prepairtable,
					pfucbNil,
					&prepairtable->pgnoFDP,
					CPAGE::fPagePrimary | CPAGE::fPageRepair,
					prepairtt,
					popts ) );
		}

	fdpinfo.pgnoFDP 	= prepairtable->pgnoFDP;
	fdpinfo.objidFDP 	= prepairtable->objidFDP;
	Call( ErrFILEOpenTable( ppib, ifmp, &pfucb, prepairtable->szTableName, NO_GRBIT, &fdpinfo ) );
	
	if( prepairtable->fRepairLV 
		&& objidNil != prepairtable->objidLV )
		{
		(*popts->pcprintfVerbose)( "rebuilding long value tree\r\n" );
		Call( ErrREPAIRRebuildBT(
				ppib,
				ifmp,
				prepairtable,
				pfucb,
				&prepairtable->pgnoLV,
				CPAGE::fPageLongValue | CPAGE::fPageRepair,
				prepairtt,
				popts ) );
		}

	if( prepairtable->fTableHasSLV )
		{
		Call( ErrDIROpen( ppib, rgfmp[ifmp].PfcbSLVAvail(), &pfucbSLVAvail ) );
		Call( ErrDIROpen( ppib, rgfmp[ifmp].PfcbSLVOwnerMap(), &pfucbSLVOwnerMap ) );
		}
		
	Call( ErrREPAIRFixupTable( ppib, ifmp, pttarraySLVAvail, pfucbSLVAvail, pfucbSLVOwnerMap, prepairtable, popts ) );

	if( prepairtable->fRepairIndexes )
		{
		(*popts->pcprintfVerbose)( "rebuilding indexes\r\n" );
		Call( ErrREPAIRBuildAllIndexes( ppib, ifmp, &pfucb, prepairtable, popts ) );
		}

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );

HandleError:

	(*popts->pcprintfVerbose).Unindent();

	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		pfucb = pfucbNil;
		}

	if( pfucbNil != pfucbSLVAvail )
		{
		DIRClose( pfucbSLVAvail );
		pfucbSLVAvail = pfucbNil;
		}

	if( pfucbNil != pfucbSLVOwnerMap )
		{
		DIRClose( pfucbSLVOwnerMap );
		pfucbSLVOwnerMap = pfucbNil;
		}
		
	if( err < 0 )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRebuildBT(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTABLE * const prepairtable,
	FUCB * const pfucbTable,
	PGNO * const ppgnoFDP,
	const ULONG fPageFlags,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	FUCB * pfucb = pfucbNil;
	PGNO	pgnoFDPOld = *ppgnoFDP;
	PGNO	pgnoFDPNew = pgnoNull;

	const OBJID	objidTable	= prepairtable->objidFDP;
	const BOOL fRepairLV	= ( pfucbNil != pfucbTable );
	const OBJID objidFDP	= ( fRepairLV ? prepairtable->objidLV : prepairtable->objidFDP );
	const PGNO pgnoParent	= fRepairLV ? prepairtable->pgnoFDP : pgnoSystemRoot;

	//  we change the pgnoFDP so this cannot be called on system tables
	Assert( !FCATSystemTable( prepairtable->szTableName ) );
	
	Call( ErrREPAIRCreateEmptyFDP(
		ppib,
		ifmp,
		objidFDP,
		pgnoParent,
		&pgnoFDPNew,
		fPageFlags,
		fTrue,	//	data and LV trees are always unique
		popts ) );

if( fRepairLV )
		{
		(*popts->pcprintfDebug)( "LV (%d). new pgnoFDP is %d\r\n", objidFDP, pgnoFDPNew );
		}
	else
		{
		(*popts->pcprintfDebug)( "table %s (%d). new pgnoFDP is %d\r\n", prepairtable->szTableName, objidFDP, pgnoFDPNew );
		}

	Assert( FCB::PfcbFCBGet( ifmp, pgnoFDPOld, NULL, fFalse ) == pfcbNil );
	Assert( FCB::PfcbFCBGet( ifmp, pgnoFDPNew, NULL, fFalse ) == pfcbNil );

	Call( ErrCATChangePgnoFDP(
			ppib,
			ifmp,
			objidTable,
			objidFDP,
			( fRepairLV ? sysobjLongValue : sysobjTable ),
			pgnoFDPNew ) );

	if( !fRepairLV && prepairtable->fHasPrimaryIndex )
		{
		Call( ErrCATChangePgnoFDP(
			ppib,
			ifmp,
			objidTable,
			objidFDP,
			sysobjIndex,
			pgnoFDPNew ) );
		}

	//  at this point:
	//     1.  the system tables have been repaired
	//     2.  we have a global space tree
	//     3.  we have a new (empty) pgnoFDP and space trees
	//     4.  the catalogs have been updated with our new pgnoFDP

	if( !fRepairLV )
		{
		FDPINFO	fdpinfo	= { pgnoFDPNew, objidFDP };
		Call( ErrFILEOpenTable( ppib, ifmp, &pfucb, prepairtable->szTableName, NO_GRBIT, &fdpinfo ) );
		}
	else
		{
		Call( ErrFILEOpenLVRoot( pfucbTable, &pfucb, fFalse ) );
		}
	FUCBSetRepair( pfucb );

	Call( ErrREPAIRRebuildSpace( ppib, ifmp, pfucb, pgnoParent, prepairtt, popts ) );
	Call( ErrREPAIRRebuildInternalBT( ppib, ifmp, pfucb, prepairtt, popts ) );

	*ppgnoFDP = pgnoFDPNew;
	
HandleError:
	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCreateEmptyFDP(
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objid,
	const PGNO pgnoParent,
	PGNO * const ppgnoFDPNew,
	const ULONG fPageFlags,
	const BOOL fUnique,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	PGNO pgnoOE = pgnoNull;
	PGNO pgnoAE = pgnoNull;
	
	const CPG cpgMin	= cpgMultipleExtentMin;
	CPG cpgRequest		= cpgMin;
	
	FUCB * pfucb = pfucbNil;

	//  the fucb is just used to get the pib so we open it on the parent
	Call( ErrDIROpen( ppib, pgnoParent, ifmp, &pfucb ) );
	if( pgnoNull == *ppgnoFDPNew )
		{
		Call( ErrSPGetExt(
			pfucb,
			pgnoParent,
			&cpgRequest,
			cpgMin,
			ppgnoFDPNew,
			fSPUnversionedExtent ) );
		}
	pgnoOE = *ppgnoFDPNew + 1;
	pgnoAE = pgnoOE + 1;

	(*popts->pcprintfDebug)( "creating new FDP with %d pages starting at %d\r\n", cpgRequest, *ppgnoFDPNew );

	Call( ErrSPCreateMultiple(
		pfucb,
		pgnoParent,
		*ppgnoFDPNew,
		objid,
		pgnoOE,
		pgnoAE,
		*ppgnoFDPNew + cpgRequest - 1,
		cpgRequest,
		fUnique,			//  always unique
		fPageFlags ) );
	
HandleError:
	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		}
	return err;
	}


//  ================================================================
class REPAIRRUN
//  ================================================================
	{
	public:
		REPAIRRUN(
			const JET_SESID sesid,
			const JET_TABLEID tableid,
			const JET_COLUMNID columnidFDP,
			const JET_COLUMNID columnidPgno,
			const OBJID objidFDP );
		~REPAIRRUN() {}

		ERR ErrREPAIRRUNInit();
		ERR ErrGetRun( PGNO * const ppgnoLast, CPG * const pcpgRun );

	private:
		const JET_SESID 	m_sesid;
		const JET_TABLEID	m_tableid;
		const JET_COLUMNID	m_columnidFDP;
		const JET_COLUMNID	m_columnidPgno;
		const OBJID			m_objidFDP;
	};

	
//  ================================================================
REPAIRRUN::REPAIRRUN(
			const JET_SESID sesid,
			const JET_TABLEID tableid,
			const JET_COLUMNID columnidFDP,
			const JET_COLUMNID columnidPgno,
			const OBJID objidFDP ) :
//  ================================================================
	m_sesid( sesid ),
	m_tableid( tableid ),
	m_columnidFDP( columnidFDP ),
	m_columnidPgno( columnidPgno ),
	m_objidFDP( objidFDP )
	{
	}
	

//  ================================================================
ERR REPAIRRUN::ErrREPAIRRUNInit()
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Call( ErrIsamMakeKey( m_sesid, m_tableid, &m_objidFDP, sizeof( m_objidFDP ), JET_bitNewKey ) );
	Call( ErrDispSeek( m_sesid, m_tableid, JET_bitSeekGE ) );

HandleError:
	if( JET_errRecordNotFound == err )
		{
		err = JET_errSuccess; 
		}
	return err;
	}


//  ================================================================
ERR REPAIRRUN::ErrGetRun( PGNO * const ppgnoLast, CPG * const pcpgRun )
//  ================================================================
	{
	ERR		err			= JET_errSuccess;
	PGNO 	pgnoStart 	= pgnoNull;
	CPG		cpgRun		= 0;

	while ( err >= JET_errSuccess )
		{
		ULONG	cbActual;
		PGNO	objidCurr;
		PGNO	pgnoCurr;

		err = ErrDispRetrieveColumn(
				m_sesid,
				m_tableid,
				m_columnidFDP,
				&objidCurr,
				sizeof( objidCurr ),
				&cbActual,
				NO_GRBIT,
				NULL );
		if ( JET_errNoCurrentRecord == err )
			{
			break;
			}
		Call( err );
		Assert( sizeof( objidCurr ) == cbActual );

		if ( objidCurr != m_objidFDP )
			{
			break;
			}

		Call( ErrDispRetrieveColumn(
				m_sesid,
				m_tableid,
				m_columnidPgno,
				&pgnoCurr,
				sizeof( pgnoCurr ),
				&cbActual,
				NO_GRBIT,
				NULL ) );
		Assert( sizeof( pgnoCurr ) == cbActual );

		if ( pgnoNull == pgnoStart )
			{
			pgnoStart = pgnoCurr;
			cpgRun = 1;
			}
		else if ( pgnoStart + cpgRun == pgnoCurr )
			{
			//  this is part of a contigous chunk
			++cpgRun;
			}
		else
			{
			Assert( pgnoStart + cpgRun < pgnoCurr );
			break;
			}

		err = ErrDispMove( m_sesid, m_tableid, JET_MoveNext, NO_GRBIT );
		}

	if ( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	*ppgnoLast 	= pgnoStart + cpgRun - 1;
	*pcpgRun	= cpgRun;

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRebuildSpace(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	const PGNO pgnoParent,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR				err		= JET_errSuccess;

	const JET_SESID	sesid	= reinterpret_cast<JET_SESID>( ppib );

#ifdef DEBUG
	PGNO	pgnoPrev		= pgnoNull;
	CPG		cpgPrev			= 0;
#endif	//	DEBUG
	
	PGNO 	pgnoLast 		= pgnoNull;
	CPG		cpgRun			= 0;

	FUCB	*pfucbOE 		= pfucbNil;
	FUCB	*pfucbAE 		= pfucbNil;
	FUCB	*pfucbParent	= pfucbNil;

	const OBJID	objidFDP	= pfucb->u.pfcb->ObjidFDP();

	Assert( JET_tableidNil != prepairtt->tableidOwned );
	REPAIRRUN repairrunOwnedExt(
		sesid,
		prepairtt->tableidOwned,
		prepairtt->rgcolumnidOwned[0],
		prepairtt->rgcolumnidOwned[1],
		objidFDP );

	Assert( JET_tableidNil != prepairtt->tableidAvailable );
	REPAIRRUN repairrunAvailExt(
		sesid,
		prepairtt->tableidAvailable,
		prepairtt->rgcolumnidAvailable[0],
		prepairtt->rgcolumnidAvailable[1],
		objidFDP );

	//  The FCB is new so the space should be uninit
	if ( !pfucb->u.pfcb->FSpaceInitialized() )
		{
		pfucb->u.pfcb->SetPgnoOE( pfucb->u.pfcb->PgnoFDP()+1 );
		pfucb->u.pfcb->SetPgnoAE( pfucb->u.pfcb->PgnoFDP()+2 );
		pfucb->u.pfcb->SetSpaceInitialized();
		}
	Call( ErrSPIOpenOwnExt( ppib, pfucb->u.pfcb, &pfucbOE ) );
	Call( ErrSPIOpenAvailExt( ppib, pfucb->u.pfcb, &pfucbAE ) )

	Assert( pgnoNull != pgnoParent );
	if ( pgnoNull != pgnoParent )
		{
		Call( ErrBTOpen( ppib, pgnoParent, ifmp, &pfucbParent ) );
		Assert( pfucbNil != pfucbParent );
		Assert( pfcbNil != pfucbParent->u.pfcb );
		Assert( pfucbParent->u.pfcb->FInitialized() );
		Assert( pcsrNil == pfucbParent->pcsrRoot );
		pfucbParent->pcsrRoot = Pcsr( pfucbParent );
		}

	Call( repairrunOwnedExt.ErrREPAIRRUNInit() );

#ifdef DEBUG
	pgnoPrev = pgnoNull;
	cpgPrev = 0;
#endif	//	DEBUG

	forever 
		{
		Call( repairrunOwnedExt.ErrGetRun( &pgnoLast, &cpgRun ) );
		if( pgnoNull == pgnoLast || 0 == cpgRun )
			{
			break;
			}
		Assert( pgnoLast - cpgRun > pgnoPrev );
#ifdef REPAIR_DEBUG_VERBOSE_SPACE
		(*popts->pcprintfDebug)( "OwnExt:  %d pages ending at %d\r\n", cpgRun, pgnoLast );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

		Call( ErrSPReservePagesForSplit( pfucbOE, pfucbParent ) );
		Call( ErrREPAIRInsertRunIntoSpaceTree(
					ppib,
					ifmp,
					pfucbOE,
					pgnoLast,
					cpgRun,
					popts ) );
		}

HandleError:
	if ( pfucbNil != pfucbParent )
		{
		Assert( pcsrNil != pfucbParent->pcsrRoot );
		pfucbParent->pcsrRoot->ReleasePage();
		pfucbParent->pcsrRoot = pcsrNil;
		BTClose( pfucbParent );
		}
	if( pfucbNil != pfucbOE )
		{
		BTClose( pfucbOE );
		}
	if( pfucbNil != pfucbAE )
		{
		BTClose( pfucbAE );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertRunIntoSpaceTree(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	const PGNO pgnoLast,
	const CPG cpgRun,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	KEY			key;
	DATA 		data;
	BYTE		rgbKey[sizeof(PGNO)];

	Assert( FFUCBSpace( pfucb ) );

	KeyFromLong( rgbKey, pgnoLast );
	key.prefix.Nullify();
	key.suffix.SetPv( rgbKey );
	key.suffix.SetCb( sizeof(pgnoLast) );

	LittleEndian<LONG> le_cpgRun = cpgRun;
	data.SetPv( &le_cpgRun );
	data.SetCb( sizeof(cpgRun) );

	Call( ErrBTInsert( pfucb, key, data, fDIRNoVersion | fDIRNoLog ) );
	BTUp( pfucb );

HandleError:
	Assert( JET_errKeyDuplicate != err );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRResetRepairFlags(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	const PGNO pgnoFDP = pfucb->u.pfcb->PgnoFDP();
	
	CSR csr;
	CallR( csr.ErrGetRIWPage( ppib, ifmp, pgnoFDP ) );

	//  move down to the leaf level
	while( !csr.Cpage().FLeafPage() )
		{
		NDMoveFirstSon( pfucb, &csr );
		const PGNO pgnoChild	= *(UnalignedLittleEndian< PGNO > *) pfucb->kdfCurr.data.Pv();
		Call( csr.ErrSwitchPage( ppib, ifmp, pgnoChild ) );
		}
	Assert( pgnoNull == csr.Cpage().PgnoPrev() );

	for( ; ; )
		{

		ULONG ulFlags = csr.Cpage().FFlags();
		Assert( ulFlags & CPAGE::fPageLeaf );
		Assert( ulFlags & CPAGE::fPageRepair );
		if( 0 != csr.Cpage().Clines() )
			{
			ulFlags &= ~CPAGE::fPageLeaf;
			ulFlags = ulFlags | CPAGE::fPageParentOfLeaf;
			}
		else
			{
			//  This is an empty tree so this should be the root page
			Assert( csr.Cpage().FRootPage() );
			}
		ulFlags &= ~CPAGE::fPageRepair;
		Assert( !( ulFlags & CPAGE::fPageEmpty ) );
		Assert( !( ulFlags & CPAGE::fPageSpaceTree ) );
		Assert( !( ulFlags & CPAGE::fPageIndex ) );

		const PGNO pgnoNext = csr.Cpage().PgnoNext();

		Call( csr.ErrUpgrade( ) );
		csr.Dirty();
		csr.Cpage().SetPgnoPrev( pgnoNull );
		csr.Cpage().SetPgnoNext( pgnoNull );
		csr.Cpage().SetFlags( ulFlags );
		csr.Downgrade( latchReadNoTouch );
		csr.ReleasePage();
		csr.Reset();

		if( pgnoNull != pgnoNext )
			{
			Call( csr.ErrGetRIWPage( ppib, ifmp, pgnoNext ) );
			}
		else
			{
			break;
			}
		}

HandleError:
	csr.ReleasePage();
	csr.Reset();
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRRebuildInternalBT(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB * const pfucb,
	REPAIRTT * const prepairtt,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	//  for every used page
	//		compute the separator key
	//		insert the separator key into the new BT
	//		set pgnoPrev and pgnoNext of the page
	//	traverse the leaf level, unlink all the pages and change them to parent of leaf

	const CPG			cpgDatabase	= PgnoLast( ifmp );
	
	const OBJID			objidFDP	= pfucb->u.pfcb->ObjidFDP();
	const JET_SESID		sesid	= reinterpret_cast<JET_SESID>( ppib );
	const JET_TABLEID	tableid	= prepairtt->tableidUsed;
	const JET_COLUMNID	columnidFDP		= prepairtt->rgcolumnidUsed[0];
	const JET_COLUMNID	columnidKey		= prepairtt->rgcolumnidUsed[1];
	const JET_COLUMNID	columnidPgno	= prepairtt->rgcolumnidUsed[2];

	PGNO pgnoPrev = pgnoNull;
	PGNO pgnoNext = pgnoNull;
	PGNO pgnoCurr = pgnoNull;

	CPG	cpgInserted = 0;
	
	ERR	errMove		= JET_errSuccess;

	DIRUp( pfucb );
	
	Call( ErrIsamMakeKey( sesid, tableid, &objidFDP, sizeof( objidFDP ), JET_bitNewKey ) );
	err = ErrDispSeek( sesid, tableid, JET_bitSeekGE );
	if( JET_errRecordNotFound == err )
		{
		err 	= JET_errSuccess;
		errMove = JET_errNoCurrentRecord;
		}
	Call( err );

	while( JET_errSuccess == errMove )
		{
		ULONG	cbKey1	= 0;
		BYTE 	rgbKey1[JET_cbKeyMost];
		ULONG	cbKey2	= 0;
		BYTE 	rgbKey2[JET_cbKeyMost];
		INT 	ibKeyCommon;

		ULONG	cbActual;

		KEY 	key;
		DATA	data;

		OBJID objidFDPCurr;
		Call( ErrDispRetrieveColumn(
				sesid,
				tableid,
				columnidFDP,
				&objidFDPCurr,
				sizeof( objidFDPCurr ),
				&cbActual,
				NO_GRBIT,
				NULL ) );
		Assert( sizeof( objidFDP ) == cbActual );
		if( objidFDP != objidFDPCurr )
			{
			break;
			}

		Call( ErrDispRetrieveColumn(
				sesid,
				tableid,
				columnidPgno,
				&pgnoCurr,
				sizeof( pgnoCurr ),
				&cbActual,
				NO_GRBIT,
				NULL ) );
		Assert( sizeof( pgnoCurr ) == cbActual );
		Assert( pgnoNull == pgnoNext && pgnoNull == pgnoPrev
				|| pgnoCurr == pgnoNext );

		//  assume that the pages for the tree are found close together. preread them
		if( pgnoCurr > pgnoPrev + g_cpgMinRepairSequentialPreread
			&& pgnoCurr + g_cpgMinRepairSequentialPreread < cpgDatabase )
			{
			BFPrereadPageRange( ifmp, pgnoCurr, g_cpgMinRepairSequentialPreread );
			}
			
		Call( ErrDispRetrieveColumn(
				sesid,
				tableid,
				columnidKey,
				rgbKey1,
				sizeof( rgbKey1 ),
				&cbKey1,
				NO_GRBIT,
				NULL ) );

NextPage:
		pgnoNext = pgnoNull;
		ibKeyCommon = -1;
		errMove = ErrDispMove( sesid, tableid, JET_MoveNext, NO_GRBIT );
		if( JET_errSuccess != errMove 
			&& JET_errNoCurrentRecord != errMove )
			{
			Call( errMove );
			}
		if( JET_errSuccess == errMove )
			{
			Call( ErrDispRetrieveColumn(
					sesid,
					tableid,
					columnidFDP,
					&objidFDPCurr,
					sizeof( objidFDPCurr ),
					&cbActual,
					NO_GRBIT,
					NULL ) );
			Assert( sizeof( objidFDPCurr ) == cbActual );
			if( objidFDP == objidFDPCurr )
				{
				Call( ErrDispRetrieveColumn(
						sesid,
						tableid,
						columnidPgno,
						&pgnoNext,
						sizeof( pgnoNext ),
						&cbActual,
						NO_GRBIT,
						NULL ) );
				Assert( sizeof( pgnoNext ) == cbActual );

				CSR csr;
				Call( csr.ErrGetReadPage( ppib, ifmp, pgnoNext, bflfNoTouch ) );
				csr.SetILine( 0 );
				KEYDATAFLAGS kdf;
				NDIGetKeydataflags( csr.Cpage(), 0, &kdf );
				cbKey2 = kdf.key.Cb();
				kdf.key.CopyIntoBuffer( rgbKey2, sizeof( rgbKey2 ) );
				csr.ReleasePage( fTrue );
				csr.Reset();

				for( ibKeyCommon = 0;
					 ibKeyCommon < cbKey1 && ibKeyCommon < cbKey2 && rgbKey1[ibKeyCommon] == rgbKey2[ibKeyCommon];
					 ++ibKeyCommon )
					 ;
				if( ibKeyCommon >= cbKey2
					|| ibKeyCommon < cbKey1 && rgbKey1[ibKeyCommon] > rgbKey2[ibKeyCommon] )
					{
					//  this removed inter-page duplicates. it won't remove intra-page duplicates
					(*popts->pcprintfVerbose)
						( "page %d and page %d have duplicate/overlapping keys. discarding page %d\r\n",
							pgnoCurr, pgnoNext, pgnoNext );		
					goto NextPage;
					}
				}
			}

		key.prefix.Nullify();
		key.suffix.SetPv( rgbKey2 );
		key.suffix.SetCb( ibKeyCommon + 1 );	//  turn from ib to cb

		LittleEndian<PGNO> le_pgnoCurr = pgnoCurr;
		data.SetPv( &le_pgnoCurr );
		data.SetCb( sizeof( pgnoCurr ) );

#ifdef REPAIR_DEBUG_VERBOSE_SPACE
		(*popts->pcprintfDebug)( "Page %d: pgnoPrev %d, pgnoNext %d, key-length %d\r\n",
			pgnoCurr, pgnoPrev, pgnoNext, key.Cb() );
#endif	//	REPAIR_DEBUG_VERBOSE_SPACE

		++cpgInserted;
		
		if( pgnoNull != pgnoPrev )
			{
			//  we have been through here before

			Call( ErrDIRGet( pfucb ) );
			err = Pcsr( pfucb )->ErrUpgrade();
			
			while( errBFLatchConflict == err )
				{
				//	do a BTUp() so that the append code will do a BTInsert

				CallS( ErrDIRRelease( pfucb ) );
				Call( ErrDIRGet( pfucb ) );
				err = Pcsr( pfucb )->ErrUpgrade();
				}
			Call( err );
			}
		Call( ErrDIRAppend( pfucb, key, data, fDIRNoVersion | fDIRNoLog ) );
		if( pgnoNull != pgnoNext )
			{
			Call( ErrDIRRelease( pfucb ) );
			}
		else
			{
			//  this is our last time through
			DIRUp( pfucb );
			}

		CSR csr;
		Call( csr.ErrGetRIWPage( ppib, ifmp, pgnoCurr ) );
		Call( csr.ErrUpgrade( ) );
		csr.Dirty();
		Assert( csr.Cpage().Pgno() == pgnoCurr );
		Assert( csr.Cpage().ObjidFDP() == objidFDP );
		csr.Cpage().SetPgnoPrev( pgnoPrev );
		csr.Cpage().SetPgnoNext( pgnoNext );
		csr.ReleasePage( fTrue );
		csr.Reset();
		
		pgnoPrev = pgnoCurr;			
		}
		
	CallSx( errMove, JET_errNoCurrentRecord );

	Call( ErrREPAIRResetRepairFlags( ppib, ifmp, pfucb, popts ) );

	(*popts->pcprintfVerbose)( "b-tree rebuilt with %d data pages\r\n", cpgInserted );
	
HandleError:
	Assert( JET_errKeyDuplicate != err );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRFixLVs(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoLV,
	TTMAP * const pttmapLVTree,
	const BOOL fFixMissingLVROOT,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Delete LVs that are not complete
//  Re-insert missing LVROOTs
//  Store the LV refcounts in the ttmap
//
//-
	{
	ERR	err = JET_errSuccess;
	
	FUCB * 	pfucb 		= pfucbNil;
	BOOL	fDone		= fFalse;
	LID		lidCurr		= 0;
	INT 	clvDeleted	= 0;
	
	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm		= NULL;
	dib.dirflag	= fDIRNull;

	Assert( pgnoNull != pgnoLV );

	(*popts->pcprintfVerbose)( "fixing long value tree\r\n" );
	
	Call( ErrDIROpen( ppib, pgnoLV, ifmp, &pfucb ) );
	Assert( pfucbNil != pfucb );
	
	FUCBSetIndex( pfucb );
	FUCBSetSequential( pfucb );
	FUCBSetPrereadForward( pfucb, cpgPrereadSequential );
	
	err = ErrDIRDown( pfucb, &dib );
	if( JET_errRecordNotFound == err )
		{
		
		// no long values
		
		err = JET_errSuccess;
		fDone = fTrue;
		}
	Call( err );
	
	while( !fDone )
		{
		const LID 	lidOld 		= lidCurr;
		BOOL 		fLVComplete = fFalse;
		BOOL		fLVHasRoot	= fFalse;
		ULONG		ulRefcount	= 0;
		ULONG		ulSize 		= 0;
		
		lidCurr = 0;
		
		Call( ErrREPAIRCheckLV( pfucb, &lidCurr, &ulRefcount, &ulSize, &fLVHasRoot, &fLVComplete, &fDone ) );
		if( !fLVComplete || ( !fLVHasRoot && !fFixMissingLVROOT ) )
			{
			Assert( 0 != lidCurr );
			(*popts->pcprintfVerbose)( "long value %d is not complete. deleting\r\n", lidCurr );
			Call( ErrREPAIRDeleteLV( pfucb, lidCurr ) );
			
			++clvDeleted;
			Call( ErrREPAIRNextLV( pfucb, lidCurr, &fDone ) );
			}
		else
			{
			if( !fLVHasRoot )
				{
				Assert( fFixMissingLVROOT );
				Assert( 0 != ulRefcount );
				Assert( 0 != ulSize );
				
				FUCB * pfucbLVRoot = pfucbNil;
				
				if( !fDone )
					{
					Call( ErrDIRRelease( pfucb ) );
					}
				else
					{
					DIRUp( pfucb );
					}
				
				Call( ErrDIROpen( ppib, pgnoLV, ifmp, &pfucbLVRoot ) );
				
				(*popts->pcprintfVerbose)( "long value %d has no root. creating a root with refcount %d and size %d\r\n", lidCurr, ulRefcount, ulSize );
				
				err = ErrREPAIRCreateLVRoot( pfucbLVRoot, lidCurr, ulRefcount, ulSize );

				//	close the cursor before checking the error
				
				DIRClose( pfucbLVRoot );
				pfucbLVRoot = pfucbNil;
				
				Call( err );

				if( !fDone )
					{
					Call( ErrDIRGet( pfucb ) );
					}
				}
				
			//  we are already on the next LV
			
			Assert( lidCurr > lidOld );
			if( !fDone )
				{
				Call( ErrDIRRelease( pfucb ) );
				Call( pttmapLVTree->ErrInsertKeyValue( lidCurr, ulRefcount ) );
				Call( ErrDIRGet( pfucb ) );
				}
			else
				{
				DIRUp( pfucb );
				Call( pttmapLVTree->ErrInsertKeyValue( lidCurr, ulRefcount ) );
				}
			}
		}

HandleError:
	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		}

	return err;
	}


#ifdef DISABLE_SLV
#else

//  ================================================================
LOCAL ERR ErrREPAIRCheckSLVInfoForUpdateTrees( 
	PIB * const ppib,
	CSLVInfo * const pslvinfo,
	const OBJID objidFDP,
	TTARRAY * const pttarraySLVAvail,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	TTARRAY::RUN availRun;
	pttarraySLVAvail->BeginRun( ppib, &availRun );
	
	Call( pslvinfo->ErrMoveBeforeFirst() );
	while( ( err = pslvinfo->ErrMoveNext() ) == JET_errSuccess )
		{
		CSLVInfo::RUN slvRun;

		Call( pslvinfo->ErrGetCurrentRun( &slvRun ) );

		CPG cpg;
		PGNO pgnoSLVFirst;
		PGNO pgnoSLV;

		cpg 			= slvRun.Cpg();
		pgnoSLVFirst 	= slvRun.PgnoFirst();

		for( pgnoSLV = pgnoSLVFirst; pgnoSLV < pgnoSLVFirst + cpg; ++pgnoSLV )
			{
			OBJID objidOwning;
			Call( pttarraySLVAvail->ErrGetValue( ppib, pgnoSLV, &objidOwning, &availRun ) );

			if( objidInvalid == objidOwning )
				{
				(*popts->pcprintfError)(
					"SLV space allocation error page %d has a bad checksum\r\n",
					pgnoSLV );			
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
				
			if( objidFDP != objidOwning
				&& objidNil != objidOwning )
				{
				(*popts->pcprintfError)(
					"SLV space allocation error page %d is already owned by objid %d\r\n",
					pgnoSLV, objidOwning & 0x7fffffff );			
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}

			err = pttarraySLVAvail->ErrSetValue( ppib, pgnoSLV, objidFDP | 0x80000000, &availRun );
			if( JET_errRecordNotFound == err )
				{
				(*popts->pcprintfError)( "SLV space allocation error page %d is not in the streaming file\r\n", pgnoSLV );
				err = ErrERRCheck( JET_errDatabaseCorrupted );				
				}
			Call( err ); 								
			}
		}

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
	
HandleError:
	pttarraySLVAvail->EndRun( ppib, &availRun );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRUpdateSLVAvailFromSLVRun( 
	PIB * const ppib,
	const IFMP ifmp,
	const CSLVInfo::RUN& slvRun,
	FUCB * const pfucbSLVAvail,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	CPG cpgCurr 		= slvRun.Cpg();
	PGNO pgnoSLVCurr	= slvRun.PgnoFirst();
	
	while( cpgCurr > 0 )
		{
		const INT 	iChunk 			= ( pgnoSLVCurr + SLVSPACENODE::cpageMap - 1 ) / SLVSPACENODE::cpageMap;
		const PGNO 	pgnoSLVChunk 	= iChunk * SLVSPACENODE::cpageMap;
		const CPG 	cpg 			= min( cpgCurr, pgnoSLVChunk - pgnoSLVCurr + 1 );
		const INT	ipage			= SLVSPACENODE::cpageMap - ( pgnoSLVChunk - pgnoSLVCurr ) - 1;
		
		//	seek to the location

		BYTE		rgbKey[sizeof(PGNO)];
		BOOKMARK	bm;

		KeyFromLong( rgbKey, pgnoSLVChunk );
		bm.key.prefix.Nullify();
		bm.key.suffix.SetPv( rgbKey );
		bm.key.suffix.SetCb( sizeof( rgbKey ) );
		bm.data.Nullify();

		DIB dib;
		dib.pos = posDown;
		dib.pbm = &bm;
		dib.dirflag = fDIRNull;

		Call( ErrBTDown( pfucbSLVAvail, &dib, latchRIW ) );
		Call( Pcsr( pfucbSLVAvail )->ErrUpgrade() );
		
		//	write latch the page

		Call( ErrBTMungeSLVSpace(
			pfucbSLVAvail,
			slvspaceoperFreeToCommitted,
			ipage,
			cpg,
			fDIRNull ) );			

		//	not terribly efficent if the run spans multiple chunks, but that should be very rare

		BTUp( pfucbSLVAvail );		

		cpgCurr 	-= cpg;
		pgnoSLVCurr += cpg;

		Assert( 0 == cpgCurr || 0 == ( ( pgnoSLVCurr - 1 ) % SLVSPACENODE::cpageMap ) );
		}			

HandleError:
	BTUp( pfucbSLVAvail );
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRUpdateSLVOwnerMapFromSLVRun( 
	PIB * const ppib,
	const IFMP ifmp,
	const OBJID objidFDP,
	const COLUMNID columnid,
	const BOOKMARK& bm,
	const CSLVInfo::RUN& slvRun,
	SLVOWNERMAPNODE * const pslvownermapNode,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	const PGNO pgnoSLVMin 	= slvRun.PgnoFirst();
	const PGNO pgnoSLVMax	= slvRun.PgnoFirst() + slvRun.Cpg();

	ERR err = JET_errSuccess;
	
	PGNO pgnoSLV;
	for( pgnoSLV = pgnoSLVMin; pgnoSLV < pgnoSLVMax; ++pgnoSLV )
		{
#ifdef REPAIR_DEBUG_VERBOSE_STREAMING
		Assert( bm.key.prefix.FNull() );
		char szBM[256];
		REPAIRDumpHex(
			szBM,
			sizeof( szBM ),
			(BYTE *)bm.key.suffix.Pv(),
			bm.key.suffix.Cb() );
		(*popts->pcprintfVerbose)( "streaming file page %d belongs to objid %d, bookmark %s, columnid %d\r\n",
										pgnoSLV,
										objidFDP,
										szBM,
										columnid );	
#endif	//	REPAIR_DEBUG_VERBOSE_STREAMING

		Call( pslvownermapNode->ErrCreateForSet( ifmp, pgnoSLV, objidFDP, columnid, const_cast<BOOKMARK *>( &bm ) ) );			
		Call( pslvownermapNode->ErrSetData( ppib, fTrue ) );
		pslvownermapNode->NextPage();			
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRUpdateSLVTablesFromSLVInfo( 
	PIB * const ppib,
	const IFMP ifmp,
	CSLVInfo * const pslvinfo,
	const BOOKMARK& bm,
	const OBJID objidFDP,
	const COLUMNID columnid,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,	
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR					err			= JET_errSuccess;
	SLVOWNERMAPNODE		slvownermapNode;

	Call( pslvinfo->ErrMoveBeforeFirst() );
	while( ( err = pslvinfo->ErrMoveNext() ) == JET_errSuccess )
		{		
		CSLVInfo::RUN slvRun;
		Call( pslvinfo->ErrGetCurrentRun( &slvRun ) );

		Call( ErrREPAIRUpdateSLVOwnerMapFromSLVRun(
				ppib,
				ifmp,
				objidFDP,
				columnid,
				bm,
				slvRun,
				&slvownermapNode,
				popts ) );

		Call( ErrREPAIRUpdateSLVAvailFromSLVRun(
				ppib,
				ifmp,
				slvRun,
				pfucbSLVAvail,
				popts ) );
		}

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckUpdateSLVsFromColumn( 
	FUCB * const pfucb,
	const COLUMNID columnid,
	const TAGFIELDS_ITERATOR& tagfieldsIterator,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Assert( tagfieldsIterator.FSLV() );

	CSLVInfo slvinfo;
	
	DATA data;
	data.SetPv( const_cast<BYTE *>( tagfieldsIterator.TagfldIterator().PbData() ) );
	data.SetCb( tagfieldsIterator.TagfldIterator().CbData() );

	const BOOKMARK bm = pfucb->bmCurr;

	if( pfucbNil == pfucbSLVAvail
		|| pfucbNil == pfucbSLVOwnerMap )
		{
		(*popts->pcprintfError)( "record (%d:%d) has an unexpected SLV\r\n",
									Pcsr( pfucb )->Pgno(), Pcsr( pfucb )->ILine() );
		Call( ErrERRCheck( JET_errSLVCorrupted ) );
		}

	Call( slvinfo.ErrLoadFromData( pfucb, data, tagfieldsIterator.TagfldIterator().FSeparated() ) );			
		
	//	see if any of the pages in the SLV are already used or out of range

	Call( ErrREPAIRCheckSLVInfoForUpdateTrees( pfucb->ppib, &slvinfo, ObjidFDP( pfucb ), pttarraySLVAvail, popts ) );

HandleError:	
	slvinfo.Unload();

	if( JET_errSuccess != err )
		{
		err = ErrERRCheck( JET_errSLVCorrupted );
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRUpdateSLVsFromColumn( 
	FUCB * const pfucb,
	const COLUMNID columnid,
	const TAGFIELDS_ITERATOR& tagfieldsIterator,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Assert( tagfieldsIterator.FSLV() );

	CSLVInfo slvinfo;
	
	DATA data;
	data.SetPv( const_cast<BYTE *>( tagfieldsIterator.TagfldIterator().PbData() ) );
	data.SetCb( tagfieldsIterator.TagfldIterator().CbData() );

	const BOOKMARK bm = pfucb->bmCurr;

	Call( slvinfo.ErrLoadFromData( pfucb, data, tagfieldsIterator.TagfldIterator().FSeparated() ) );			

	//	set the ownership info
	
	Call( ErrREPAIRUpdateSLVTablesFromSLVInfo(
			pfucb->ppib,
			pfucb->ifmp,
			&slvinfo,
			bm,
			ObjidFDP( pfucb ),
			columnid,
			pfucbSLVAvail,
			pfucbSLVOwnerMap,
			popts ) );

HandleError:	
	slvinfo.Unload();
	return err;
	}

#endif	//	DISABLE_SLV


//  ================================================================
LOCAL ERR ErrREPAIRCheckUpdateLVsFromColumn( 
	FUCB * const pfucb,
	TAGFLD_ITERATOR& tagfldIterator,
	const TTMAP * const pttmapLVTree,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	while( JET_errSuccess == ( err = tagfldIterator.ErrMoveNext() ) )
		{					
		if( tagfldIterator.FSeparated() )
			{
			const LID lid = LidOfSeparatedLV( tagfldIterator.PbData() );
			
			ULONG ulUnused;
			err = pttmapLVTree->ErrGetValue( lid, &ulUnused );
			if( JET_errRecordNotFound == err )
				{						
				(*popts->pcprintfDebug)( "record (%d:%d) references non-existant LID (%d). deleting\r\n",
											Pcsr( pfucb )->Pgno(), Pcsr( pfucb )->ILine(), lid );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			else
				{
				Call( err );
				}
			}
		}
		
	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	return err;
	}

				
//  ================================================================
LOCAL ERR ErrREPAIRUpdateLVsFromColumn( 
	FUCB * const pfucb,
	TAGFLD_ITERATOR& tagfldIterator,
	TTMAP * const pttmapRecords,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	while( JET_errSuccess == ( err = tagfldIterator.ErrMoveNext() ) )
		{					
		if( tagfldIterator.FSeparated() )
			{
			const LID lid = LidOfSeparatedLV( tagfldIterator.PbData() );

			Call( pttmapRecords->ErrIncrementValue( lid ) );
			}
		}

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRAddOneCatalogRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const ULONG	objidTable,
	const COLUMNID	columnid,
	const USHORT ibRecordOffset, 
	const ULONG	cbMaxLen,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR 			err = JET_errSuccess;
	CHAR			szColumnName[32];
	FIELD			field;

	Assert( objidTable > objidSystemRoot );

	(*popts->pcprintfStats).Indent();


	// Set column name to be a bogus name of the form "JetStub_<objidFDP>_<fid>".
	strcpy( szColumnName, "JetStub_" );
	_ultoa( objidTable, szColumnName + strlen( szColumnName ), 10 );
	Assert( strlen( szColumnName ) < 32 );
	strcat( szColumnName, "_" );
	_ultoa( columnid, szColumnName + strlen( szColumnName ), 10 );
	Assert( strlen( szColumnName ) < 32 );

	//	must zero out to ensure unused fields are ignored
	memset( &field, 0, sizeof(field) );
	
	field.coltyp = JET_coltypNil;
	field.cbMaxLen = cbMaxLen;
	//field.ffield = ffieldDeleted;
	field.ffield = 0;
	field.ibRecordOffset = ibRecordOffset;
	field.cp = 0;

	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );
	err = ErrCATAddTableColumn( 
				ppib,
				ifmp,
				objidTable,
				szColumnName,
				columnid,
				&field,
				NULL,
				0,
				NULL,
				NULL,
				0 );

	if( JET_errSuccess == err )
	 	{
	 	(*popts->pcprintfVerbose)( "Inserted a column record (objidTable %d:columnid %d) into catalogs\t\n", objidTable, columnid );
	 	}
	 else
	 	{
	 	if( JET_errColumnDuplicate == err )
	 		{
	 		(*popts->pcprintfVerbose)( "The column record (%d) for table (%d) has already existed in catalogs\t\n", columnid, objidTable );
	 		err = JET_errSuccess;
	 		}
	 	(*popts->pcprintfVerbose)( "Inserting a column record (objidTable %d:columnid %d) into catalogs fails (%d)\t\n", objidTable, columnid, err );
	 	}

	Call( ErrDIRCommitTransaction( ppib, NO_GRBIT ) );
	
HandleError:
	(*popts->pcprintfVerbose).Unindent();

	if( JET_errSuccess != err )
		{
		CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRInsertDummyRecordsToCatalog(
	PIB * const ppib,
	const IFMP ifmp,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR			err		= JET_errSuccess;

	FUCB 	*	pfucbTable 				= pfucbNil;
	
	err = ErrFILEOpenTable( ppib, ifmp, &pfucbTable, prepairtable->szTableName, NO_GRBIT );

	if( JET_errSuccess != err )
		{
		err = JET_errSuccess;
		}
	else
		{

		if( prepairtable->fidFixedLast > pfucbTable->u.pfcb->Ptdb()->FidFixedLast() )
			{
			COLUMNID	columnidFixedLast = ColumnidOfFid( pfucbTable->u.pfcb->Ptdb()->FidFixedLast(), fFalse );
			
			ULONG colTypeLastOld = 0; 	
			USHORT ibRecordOffsetLastOld = 0; 

			// Get values of colType and ibRecordOffset !
			FUCB		* 	pfucbCatalog 	= pfucbNil;
			USHORT		sysobj 	= sysobjColumn;
	
			Call( ErrCATOpen( ppib, ifmp, &pfucbCatalog, fFalse ) );
			Assert( pfucbNil != pfucbCatalog );
			Assert( pfucbNil == pfucbCatalog->pfucbCurIndex );
			Assert( !Pcsr( pfucbCatalog )->FLatched() );

			Call( ErrIsamMakeKey(
						ppib,
						pfucbCatalog,
						(BYTE *)&(prepairtable->objidFDP),
						sizeof(prepairtable->objidFDP),
						JET_bitNewKey ) );
			Call( ErrIsamMakeKey(
						ppib,
						pfucbCatalog,
						(BYTE *)&sysobj,
						sizeof(sysobj),
						NO_GRBIT ) );
			Call( ErrIsamMakeKey(
						ppib,
						pfucbCatalog,
						(BYTE *)&(columnidFixedLast),
						sizeof(COLUMNID),
						NO_GRBIT ) );
			err = ErrIsamSeek( ppib, pfucbCatalog, JET_bitSeekEQ );
			Assert( err <= JET_errSuccess );	//	SeekEQ shouldn't return warnings

			if ( JET_errSuccess == err )
				{
				DATA		dataField;
				
				Assert( !Pcsr( pfucbCatalog )->FLatched() );
				Call( ErrDIRGet( pfucbCatalog ) );
				
				Assert( FFixedFid( fidMSO_Coltyp ) );
				Call( ErrRECIRetrieveFixedColumn(
							pfcbNil,
							pfucbCatalog->u.pfcb->Ptdb(),
							fidMSO_Coltyp,
							pfucbCatalog->kdfCurr.data,
							&dataField ) );
				Assert( dataField.Cb() == sizeof(JET_COLTYP) );
				colTypeLastOld = *( UnalignedLittleEndian< JET_COLTYP > *)dataField.Pv();

				Assert( FFixedFid( fidMSO_RecordOffset ) );
				Call( ErrRECIRetrieveFixedColumn(
							pfcbNil,
							pfucbCatalog->u.pfcb->Ptdb(),
							fidMSO_RecordOffset,
							pfucbCatalog->kdfCurr.data,
							&dataField ) );
				if ( JET_wrnColumnNull == err )
					{
					Assert( dataField.Cb() == 0 );
					ibRecordOffsetLastOld = 0;		// Set to a dummy value.
					}
				else
					{
					Assert( dataField.Cb() == sizeof(REC::RECOFFSET) );
					ibRecordOffsetLastOld = *(UnalignedLittleEndian< REC::RECOFFSET > *)dataField.Pv();
					}				
				}
			
			if( pfucbNil != pfucbCatalog )
				{
				err =  ErrCATClose( ppib, pfucbCatalog );	
				}
			err = JET_errSuccess;
		

			USHORT cbFixedLastOld = 0;
			switch ( colTypeLastOld )
				{
				case JET_coltypBit:
				case JET_coltypUnsignedByte: 	cbFixedLastOld = 1; break;
				case JET_coltypShort: 			cbFixedLastOld = 2; break;
				case JET_coltypLong:
				case JET_coltypIEEESingle:		cbFixedLastOld = 4; break;
				case JET_coltypCurrency:			
				case JET_coltypIEEEDouble:		
				case JET_coltypDateTime:		cbFixedLastOld = 8; break; 
				default:						cbFixedLastOld = 0; break;
				}
			/* Calculate record offset and max column length
						
			 max column length	= ibEndOfFixedData 
									- offset of fidFixedLastInCAT
									- Length of fidFixedLastInCAT
									- sizeof fixed field bit array
									- (one byte for each fid in between these two fids)

			offset = offset of fidFixedLastInCAT + Length of fidFixedLastInCAT 
						+ (one byte for each fid in between these two fids)
			*/

			ULONG cbMaxLen = prepairtable->ibEndOfFixedData 
							- ibRecordOffsetLastOld 
							- cbFixedLastOld 
							- ( prepairtable->fidFixedLast % 8 ? prepairtable->fidFixedLast/8 + 1 : prepairtable->fidFixedLast/8 )
							- (prepairtable->fidFixedLast - FidOfColumnid(columnidFixedLast) - 1 );

			Assert( prepairtable->ibEndOfFixedData >= 
					( ibRecordOffsetLastOld + cbFixedLastOld 
					  + ( prepairtable->fidFixedLast % 8 ? prepairtable->fidFixedLast/8 + 1 : prepairtable->fidFixedLast/8 )
					  + (prepairtable->fidFixedLast - FidOfColumnid(columnidFixedLast) - 1 ) ) );
							
			USHORT ibRecordOffset = (USHORT)(ibRecordOffsetLastOld 
							+ cbFixedLastOld
							+ (prepairtable->fidFixedLast - FidOfColumnid(columnidFixedLast) - 1 ) );

			err = ErrREPAIRAddOneCatalogRecord( 
						ppib, 
						ifmp, 
						prepairtable->objidFDP, 
						prepairtable->fidFixedLast, 
						ibRecordOffset, 
						cbMaxLen, 
						popts );
			}
			
		if( prepairtable->fidVarLast > pfucbTable->u.pfcb->Ptdb()->FidVarLast() )
			{
			err = ErrREPAIRAddOneCatalogRecord( 
						ppib, 
						ifmp, 
						prepairtable->objidFDP, 
						prepairtable->fidVarLast, 
						0,
						0, 
						popts );
			}
	
		if( prepairtable->fidTaggedLast > pfucbTable->u.pfcb->Ptdb()->FidTaggedLast() )
			{
			err = ErrREPAIRAddOneCatalogRecord( 
						ppib, 
						ifmp, 
						prepairtable->objidFDP,  
						prepairtable->fidTaggedLast, 
						0,
						0,
						popts );
			}

		// ignore errors
		err = JET_errSuccess;
		}
		
HandleError:
	if( pfucbNil != pfucbTable )
		{
		Call( ErrFILECloseTable( ppib, pfucbTable ) );
		pfucbTable = pfucbNil;
		}
		
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRCheckFixOneRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const DATA& dataRec,
	FUCB * const pfucb,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	const TTMAP * const pttmapLVTree,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	TAGFIELDS_ITERATOR tagfieldsIterator( dataRec );
	tagfieldsIterator.MoveBeforeFirst();
	
	while( JET_errSuccess == ( err = tagfieldsIterator.ErrMoveNext() ) )
		{
		const COLUMNID columnid 	= tagfieldsIterator.Columnid( pfucb->u.pfcb->Ptdb() );		

		if( !tagfieldsIterator.FDerived() )
			{
			prepairtable->fidTaggedLast = max( tagfieldsIterator.Fid(), prepairtable->fidTaggedLast );
			}

		if( tagfieldsIterator.FNull() )
			{
			continue;
			}
		else
			{
			tagfieldsIterator.TagfldIterator().MoveBeforeFirst();
			
			if( tagfieldsIterator.FSLV() )
				{
#ifdef DISABLE_SLV
				Call( ErrERRCheck( JET_wrnNyi ) );
#else
				CallS( tagfieldsIterator.TagfldIterator().ErrMoveNext() );
						
				Call( ErrREPAIRCheckUpdateSLVsFromColumn(
						pfucb,
						columnid,
						tagfieldsIterator,
						pttarraySLVAvail,
						pfucbSLVAvail,
						pfucbSLVOwnerMap,
						popts ) );
#endif
				}
			
			if ( tagfieldsIterator.FLV() )
				{
				Call( ErrREPAIRCheckUpdateLVsFromColumn(
						pfucb,
						tagfieldsIterator.TagfldIterator(),
						pttmapLVTree,
						popts ) );						
				}				
			}
		}
			
	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
	Call( err );

HandleError:
	if( JET_errSLVCorrupted == err )
		{
		(*popts->pcprintfError)( "record (%d:%d) has SLV corruption\r\n",
									Pcsr( pfucb )->Pgno(), Pcsr( pfucb )->ILine() );
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
		
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRFixOneRecord(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	const DATA& dataRec,
	FUCB * const pfucb,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	TTMAP * const pttmapRecords,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	TAGFIELDS_ITERATOR tagfieldsIterator( dataRec );
	
	tagfieldsIterator.MoveBeforeFirst();
	while( JET_errSuccess == ( err = tagfieldsIterator.ErrMoveNext() ) )
		{
		const COLUMNID columnid 	= tagfieldsIterator.Columnid( pfucb->u.pfcb->Ptdb() );
		
		if( tagfieldsIterator.FNull() )
			{
			continue;
			}
		else
			{
			tagfieldsIterator.TagfldIterator().MoveBeforeFirst();
		
			if( tagfieldsIterator.FSLV() )
				{
#ifdef DISABLE_SLV
				Call( ErrERRCheck( JET_wrnNyi ) );
#else
				Assert( pfucbNil != pfucbSLVAvail );
				Assert( pfucbNil != pfucbSLVOwnerMap );

				CallS( tagfieldsIterator.TagfldIterator().ErrMoveNext() );					
				Call( ErrREPAIRUpdateSLVsFromColumn(
						pfucb,
						columnid,
						tagfieldsIterator,
						pttarraySLVAvail,
						pfucbSLVAvail,
						pfucbSLVOwnerMap,
						popts ) );
#endif
			}
		
			if ( tagfieldsIterator.FLV() )
				{
				Call( ErrREPAIRUpdateLVsFromColumn(
						pfucb,
						tagfieldsIterator.TagfldIterator(),
						pttmapRecords,
						popts ) );
				}	
			}			
		}
		
	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRFixRecords(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoFDP,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	TTMAP * const pttmapRecords,
	const TTMAP * const pttmapLVTree,
	TTARRAY * const pttarraySLVAvail,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
//
// 	Delete records that reference non-existant LVs
//	Delete records whose SLV pages are used elsewhere
//	Update the SLV trees
//	Store the correct refcounts in the TTMAP
//
//-
	{
	ERR	err = JET_errSuccess;

	FUCB * 	pfucb 	= pfucbNil;

	INT	crecordDeleted 	= 0;

	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm		= NULL;
	dib.dirflag	= fDIRNull;

	(*popts->pcprintfVerbose)( "fixing records\r\n" );

	Call( ErrDIROpen( ppib, pgnoFDP, ifmp, &pfucb ) );
	Assert( pfucbNil != pfucb );
	
	FUCBSetIndex( pfucb );
	FUCBSetSequential( pfucb );
	FUCBSetPrereadForward( pfucb, cpgPrereadSequential );

	//	allocate working buffer
	
	Assert( NULL == pfucb->pvWorkBuf );
	RECIAllocCopyBuffer( pfucb );
	
	err = ErrDIRDown( pfucb, &dib );
	
	while( err >= 0 )
		{

		//  CONSIDER:  do we really need to copy the record? Try keeping the page latched
		
		UtilMemCpy( pfucb->dataWorkBuf.Pv(), pfucb->kdfCurr.data.Pv(), pfucb->kdfCurr.data.Cb() );
		
		const REC * const prec 		= (REC *)( pfucb->dataWorkBuf.Pv() );
		const BYTE * const pbRecMax = (BYTE *)( prec ) + pfucb->kdfCurr.data.Cb();

		DATA dataRec;
		dataRec.SetPv( pfucb->dataWorkBuf.Pv() );
		dataRec.SetCb( pfucb->kdfCurr.data.Cb() );
		
		Call( ErrDIRRelease( pfucb ) );
		
		if( prepairtable->fidFixedLast < prec->FidFixedLastInRec() )
			{
			prepairtable->fidFixedLast	= prec->FidFixedLastInRec();
			prepairtable->ibEndOfFixedData 	= prec->IbEndOfFixedData();
			}

		prepairtable->fidVarLast	= max( prec->FidVarLastInRec(), prepairtable->fidVarLast );

		err = ErrREPAIRCheckFixOneRecord(
				ppib,
				ifmp,
				pgnoFDP,
				dataRec,
				pfucb,
				pfucbSLVAvail,
				pfucbSLVOwnerMap,
				pttmapLVTree,
				pttarraySLVAvail,
				prepairtable,
				popts );		
		if( JET_errDatabaseCorrupted == err )
			{
			(*popts->pcprintfError)( "record (%d:%d) is corrupted. Deleting\r\n",
									Pcsr( pfucb )->Pgno(), Pcsr( pfucb )->ILine() );
			UtilReportEvent( eventWarning, REPAIR_CATEGORY, REPAIR_BAD_COLUMN_ID, 0, NULL );
			Call( ErrDIRDelete( pfucb, fDIRNoVersion ) );
			++crecordDeleted;
			}
		else if( JET_errSuccess == err )
			{
			Call( ErrREPAIRFixOneRecord(
				ppib,
				ifmp,
				pgnoFDP,
				dataRec,
				pfucb,
				pfucbSLVAvail,
				pfucbSLVOwnerMap,
				pttmapRecords,
				pttarraySLVAvail,
				prepairtable,
				popts ) );					
			}
		Call( err );		
		err = ErrDIRNext( pfucb, fDIRNull );
		}
		
	if( JET_errNoCurrentRecord == err
		|| JET_errRecordNotFound == err )
		{
		err = JET_errSuccess;
		}

HandleError:
	if( pfucbNil != pfucb )
		{
		Assert( NULL != pfucb->pvWorkBuf );

		//Insert dummy table column records to catalog 
		//if the last column in record is not the last one in TDB
		if( prepairtable->fidFixedLast > pfucb->u.pfcb->Ptdb()->FidFixedLast() 
			|| prepairtable->fidVarLast > pfucb->u.pfcb->Ptdb()->FidVarLast() 
			|| prepairtable->fidTaggedLast > pfucb->u.pfcb->Ptdb()->FidTaggedLast() )
			{
			err = ErrREPAIRInsertDummyRecordsToCatalog( ppib, ifmp, prepairtable, popts );
			}

		RECIFreeCopyBuffer( pfucb ); 
		DIRClose( pfucb );
		}

	return err;
	}		


//  ================================================================
LOCAL ERR ErrREPAIRFixLVRefcounts(
	PIB * const ppib,
	const IFMP ifmp,
	const PGNO pgnoLV,
	TTMAP * const pttmapRecords,
	TTMAP * const pttmapLVTree,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
//
//  Using the two TTMAPs find LVs with the wrong refcount and fix the refcounts
//
//-
	{
	ERR err 		= JET_errSuccess;
	FUCB * pfucb	= pfucbNil;

	(*popts->pcprintfVerbose)( "fixing long value refcounts\r\n" );

	err = pttmapRecords->ErrMoveFirst();
	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		goto HandleError;
		}
	Call( pttmapLVTree->ErrMoveFirst() );

	Call( ErrDIROpen( ppib, pgnoLV, ifmp, &pfucb ) );

	for( ;; )
		{
		LID 	lid;
		LID 	lidRecord;
		ULONG	ulRefcount;
		ULONG	ulRefcountRecord;

		Call( pttmapLVTree->ErrGetCurrentKeyValue( (ULONG *)&lid, &ulRefcount ) );
		Call( pttmapRecords->ErrGetCurrentKeyValue( (ULONG *)&lidRecord, &ulRefcountRecord ) );
		if( lid == lidRecord )
			{
			if( ulRefcount != ulRefcountRecord )
				{
				(*popts->pcprintfVerbose)(
					"lid %d has incorrect refcount (refcount is %d, should be %d). correcting\r\n",
					lid,
					ulRefcount,
					ulRefcountRecord
					);			
				Call( ErrREPAIRUpdateLVRefcount( pfucb, lid, ulRefcount, ulRefcountRecord ) );
				}
			err = pttmapRecords->ErrMoveNext();
			if( JET_errNoCurrentRecord == err )
				{
				err = JET_errSuccess;
				break;
				}
			}
		Call( pttmapLVTree->ErrMoveNext() );
		}
		
HandleError:
	if( pfucbNil != pfucb )
		{
		DIRClose( pfucb );
		}
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRFixupTable(
	PIB * const ppib,
	const IFMP ifmp,
	TTARRAY * const pttarraySLVAvail,
	FUCB * const pfucbSLVAvail,
	FUCB * const pfucbSLVOwnerMap,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR err 		= JET_errSuccess;
	FUCB * pfucb	= pfucbNil;
	
	const PGNO	pgnoFDP		= prepairtable->pgnoFDP;
	const PGNO	pgnoLV		= prepairtable->pgnoLV;

	TTMAP ttmapLVTree( ppib );
	TTMAP ttmapRecords( ppib );
	
	Call( ttmapLVTree.ErrInit( PinstFromPpib( ppib ) ) );
	Call( ttmapRecords.ErrInit( PinstFromPpib( ppib ) ) );

	if( pgnoNull != pgnoLV )
		{
		Call( ErrREPAIRFixLVs( ppib, ifmp, pgnoLV, &ttmapLVTree, fTrue, prepairtable, popts ) );
		}

	Call( ErrREPAIRFixRecords(
			ppib,
			ifmp,
			pgnoFDP,
			pfucbSLVAvail,
			pfucbSLVOwnerMap,
			&ttmapRecords,
			&ttmapLVTree,
			pttarraySLVAvail,
			prepairtable,
			popts ) );
	Call( ErrREPAIRFixLVRefcounts( ppib, ifmp, pgnoLV, &ttmapRecords, &ttmapLVTree, prepairtable, popts ) );
		
HandleError:		
	return err;
	}


//  ================================================================
LOCAL BOOL FREPAIRIdxsegIsUserDefinedColumn(
	const IDXSEG idxseg,
	const FCB * const pfcbIndex )
//  ================================================================
	{
	const COLUMNID		columnid	= idxseg.Columnid();
	const TDB * const	ptdb		= pfcbIndex->PfcbTable()->Ptdb();
	const FIELD * const	pfield 		= ptdb->Pfield( columnid );

	return FFIELDUserDefinedDefault( pfield->ffield );
	}


//  ================================================================
LOCAL ERR ErrREPAIRCreateEmptyIndexes(
		PIB * const ppib,
		const IFMP ifmp,
		const PGNO pgnoFDPTable,
		FCB * const pfcbTable,
		const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR 		err 		= JET_errSuccess;
	FCB		* 	pfcbIndex	= NULL;
	
	for( pfcbIndex = pfcbTable->PfcbNextIndex();
		pfcbNil != pfcbIndex;
		pfcbIndex = pfcbIndex->PfcbNextIndex() )
		{
		const BOOL fNonUnique 	= pfcbIndex->FNonUnique();
		const OBJID objidIndex 	= pfcbIndex->ObjidFDP();
		const PGNO pgnoFDPOld	= pfcbIndex->PgnoFDP();
		PGNO pgnoFDPNew			= pgnoNull;

		if( pgnoFDPMSO_NameIndex == pgnoFDPOld
			|| pgnoFDPMSO_RootObjectIndex == pgnoFDPOld )
			{
			pgnoFDPNew = pgnoFDPOld;
			}
		
		Call( ErrREPAIRCreateEmptyFDP(
			ppib,
			ifmp,
			pfcbIndex->ObjidFDP(),
			pgnoFDPTable,
			&pgnoFDPNew,
			CPAGE::fPageIndex,
			!fNonUnique,
			popts ) );

		if( pgnoFDPMSO_NameIndex != pgnoFDPOld
			&& pgnoFDPMSO_RootObjectIndex != pgnoFDPOld )
			{
			Call( ErrCATChangePgnoFDP(
					ppib,
					ifmp,
					pfcbTable->ObjidFDP(),
					pfcbIndex->ObjidFDP(),
					sysobjIndex,
					pgnoFDPNew ) );
			}
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrREPAIRBuildAllIndexes(
	PIB * const ppib,
	const IFMP ifmp,
	FUCB ** const ppfucb,
	REPAIRTABLE * const prepairtable,
	const REPAIROPTS * const popts )
//  ================================================================
	{
	ERR		err			= JET_errSuccess;

	FCB		* pfcbTable	= (*ppfucb)->u.pfcb;
	FDPINFO	fdpinfo;

	Call( ErrREPAIRCreateEmptyIndexes( ppib, ifmp, prepairtable->pgnoFDP, pfcbTable, popts ) );
		
	//  close the table, purge FCBs and re-open to get the new PgnoFDPs
	
	DIRClose( *ppfucb );
	*ppfucb = pfucbNil;
	
	pfcbTable->PrepareForPurge();
	pfcbTable->Purge();
	

	fdpinfo.pgnoFDP = prepairtable->pgnoFDP;
	fdpinfo.objidFDP = prepairtable->objidFDP;
	Call( ErrFILEOpenTable( ppib, ifmp, ppfucb, prepairtable->szTableName, NO_GRBIT, &fdpinfo ) );
	FUCBSetIndex( *ppfucb );

	if( pfcbNil != (*ppfucb)->u.pfcb->PfcbNextIndex() )
		{
#ifdef PARALLEL_BATCH_INDEX_BUILD
		ULONG	cIndexes	= 0;
		for ( FCB * pfcbT = (*ppfucb)->u.pfcb->PfcbNextIndex(); pfcbNil != pfcbT; pfcbT = pfcbT->PfcbNextIndex() )
			{
			cIndexes++;
			}
#else
		const ULONG	cIndexes	= cFILEIndexBatchSizeDefault;
#endif

		Call( ErrFILEBuildAllIndexes( ppib, *ppfucb, (*ppfucb)->u.pfcb->PfcbNextIndex(), NULL, cIndexes ) );
		}

HandleError:
	if( pfucbNil != *ppfucb )
		{
		DIRClose( *ppfucb );
		*ppfucb = pfucbNil;
		}
		
	return err;
	}


//  ================================================================
RECCHECK::RECCHECK()
//  ================================================================
	{
	}

	
//  ================================================================
RECCHECK::~RECCHECK()
//  ================================================================
	{
	}


//  ================================================================
RECCHECKTABLE::RECCHECKTABLE(
	const OBJID objid,
	FUCB * const pfucb,
	const FIDLASTINTDB fidLastInTDB,
	TTMAP * const pttmapLVRefcounts,
	TTARRAY * const pttarraySLVAvail,
	TTARRAY	* const pttarraySLVOwnerMapColumnid,	
	TTARRAY	* const pttarraySLVOwnerMapKey,		
	TTARRAY * const pttarraySLVChecksumLengths,
	const REPAIROPTS * const popts ) :
//  ================================================================
	m_objid( objid ),
	m_pfucb( pfucb ),
	m_fidLastInTDB( fidLastInTDB ),
	m_pttmapLVRefcounts( pttmapLVRefcounts ),
	m_pttarraySLVAvail( pttarraySLVAvail ),
	m_pttarraySLVOwnerMapColumnid( pttarraySLVOwnerMapColumnid ),
	m_pttarraySLVOwnerMapKey( pttarraySLVOwnerMapKey ),
	m_pttarraySLVChecksumLengths( pttarraySLVChecksumLengths ),
	m_popts( popts )
	{
	}


//  ================================================================
RECCHECKTABLE::~RECCHECKTABLE()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckRecord_( 
	const KEYDATAFLAGS& kdf, 
	const BOOL fCheckLVSLV )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	//  Check the basic REC structure
	
	Call( ErrCheckREC_( kdf ) );

	//  Check the fixed columns
	
	Call( ErrCheckFixedFields_( kdf ) );

	//  Check variable columns
	
	Call( ErrCheckVariableFields_( kdf ) );

	//	Check tagged columns
	
	Call( ErrCheckTaggedFields_( kdf ) );
	
	if( fCheckLVSLV)
		{
		//	Check LV and SLV columns
		
		Call( ErrCheckLVAndSLVFields_( kdf ) );
		}

	if( m_pfucb )
		{
		//	Check the primary key

		Call( ErrCheckPrimaryKey_( kdf ) );
		}
		
HandleError:
	return err;
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckREC_( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	const REC * const prec = reinterpret_cast<REC *>( kdf.data.Pv() );
	const BYTE * const pbRecMax = reinterpret_cast<BYTE *>( kdf.data.Pv() ) + kdf.data.Cb();
	
	//  sanity check the pointers in the records
	
	if( prec->PbFixedNullBitMap() > pbRecMax )	//  can point to the end of the record
		{
		(*m_popts->pcprintfError)( "Record corrupted: PbFixedNullBitMap is past the end of the record\r\n" );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
	if( prec->PbVarData() > pbRecMax )	//	can point to the end of the record
		{
		(*m_popts->pcprintfError)( "Record corrupted: PbVarData is past the end of the record\r\n" );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if(	prec->PbTaggedData() > pbRecMax )	//	can point to the end of the record
		{
		(*m_popts->pcprintfError)( "Record corrupted: PbTaggedData is past the end of the record\r\n" );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

HandleError:
	return err;
	}

//  ================================================================
ERR RECCHECKTABLE::ErrCheckFixedFields_( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	const REC * const prec = reinterpret_cast<REC *>( kdf.data.Pv() );
	const FID fidFixedLast  = prec->FidFixedLastInRec();

	// Check if the last fixed column info in catalog is correct
	if( fidFixedLast > m_fidLastInTDB.fidFixedLastInTDB )
		{
		(*m_popts->pcprintfError)(
			"Record corrupted: last fixed column ID not in catalog (fid: %d, catalog last: %d)\r\n",
				fidFixedLast, m_fidLastInTDB.fidFixedLastInTDB );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) ); 
		}
HandleError:
	return err;
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckVariableFields_( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	const REC * const prec = reinterpret_cast<REC *>( kdf.data.Pv() );
	const BYTE * const pbRecMax = reinterpret_cast<BYTE *>( kdf.data.Pv() ) + kdf.data.Cb();
	
	const FID fidVariableFirst = fidVarLeast ;
	const FID fidVariableLast  = prec->FidVarLastInRec();
	const INT cColumnsVariable = max( 0, fidVariableLast - fidVariableFirst + 1 );
	const UnalignedLittleEndian<REC::VAROFFSET> * const pibVarOffs		= prec->PibVarOffsets();

	FID fid;

	REC::VAROFFSET ibStartOfColumnPrev = 0;
	REC::VAROFFSET ibEndOfColumnPrev = 0;

	//  check the variable columns
	
	for( fid = fidVariableFirst; fid <= fidVariableLast; ++fid )
		{
		const UINT				ifid			= fid - fidVarLeast;
		const REC::VAROFFSET	ibStartOfColumn	= prec->IbVarOffsetStart( fid );
		const REC::VAROFFSET	ibEndOfColumn	= IbVarOffset( pibVarOffs[ifid] );

		if( ibEndOfColumn < ibStartOfColumn )
			{
			(*m_popts->pcprintfError)(
					"Record corrupted: variable field offsets not increasing (fid: %d, start: %d, end: %d)\r\n",
					fid, ibStartOfColumn, ibEndOfColumn );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}
			
		if( ibStartOfColumn < ibStartOfColumnPrev )
			{
			(*m_popts->pcprintfError)(
					"Record corrupted: variable field offsets not increasing (fid: %d, start: %d, startPrev: %d)\r\n",
					fid, ibStartOfColumn, ibStartOfColumnPrev );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}
		ibStartOfColumnPrev = ibStartOfColumn;

		if( ibEndOfColumn < ibEndOfColumnPrev )
			{
			(*m_popts->pcprintfError)(
					"Record corrupted: variable field offsets not increasing (fid: %d, end: %d, endPrev: %d)\r\n",
					fid, ibEndOfColumn, ibEndOfColumnPrev );
			Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
			}
		ibEndOfColumnPrev = ibEndOfColumn;
			
		if ( FVarNullBit( pibVarOffs[ifid] ) )
			{
			if( ibStartOfColumn != ibEndOfColumn )
				{
				const INT cbColumn				= ibEndOfColumn - ibStartOfColumn;
				(*m_popts->pcprintfError)(
					"Record corrupted: NULL variable field not zero length (fid: %d, length: %d)\r\n", fid, cbColumn );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}
			}
		else
			{
			const BYTE * const pbColumn 	= prec->PbVarData() + ibStartOfColumn;
			const INT cbColumn				= ibEndOfColumn - ibStartOfColumn;
			const BYTE * const pbColumnEnd 	= pbColumn + cbColumn;

			if( pbColumn >= pbRecMax )
				{
				(*m_popts->pcprintfError)(
					"Record corrupted: variable field is not in record (fid: %d, length: %d)\r\n", fid, cbColumn );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}

			if( pbColumnEnd > pbRecMax )	//	can point to the end of the record
				{
				(*m_popts->pcprintfError)(
					"Record corrupted: variable field is too long (fid: %d, length: %d)\r\n", fid, cbColumn );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
				}
			}
		}		

	// Check if the last variable column info in catalog is correct
	if( fidVariableLast > m_fidLastInTDB.fidVarLastInTDB )
		{
		(*m_popts->pcprintfError)(
			"Record corrupted: last variable column ID not in catalog (fid: %d, catalog last: %d)\r\n",
				fidVariableLast, m_fidLastInTDB.fidVarLastInTDB );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) ); 
		}


HandleError:
	return err;
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckSeparatedLV_(
					const KEYDATAFLAGS& kdf,
					const COLUMNID columnid,
					const ULONG itagSequence,
					const LID lid )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( m_pttmapLVRefcounts )
		{
#ifdef SYNC_DEADLOCK_DETECTION
		COwner* const pownerSaved = Pcls()->pownerLockHead;
		Pcls()->pownerLockHead = NULL;
#endif  //  SYNC_DEADLOCK_DETECTION

		Call( m_pttmapLVRefcounts->ErrIncrementValue( lid ) );
		
#ifdef SYNC_DEADLOCK_DETECTION
		Pcls()->pownerLockHead = pownerSaved;
#endif  //  SYNC_DEADLOCK_DETECTION		
		}
	else
		{
		(*m_popts->pcprintfError)( "separated LV was not expected (columnid: %d, itag: %d, lid: %d)\r\n",
			columnid, itagSequence, lid );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
		}

HandleError:
	return err;
	}

//  ================================================================
ERR RECCHECKTABLE::ErrCheckIntrinsicLV_(
		const KEYDATAFLAGS& kdf,
		const COLUMNID columnid,
		const ULONG itagSequence,
		const DATA& dataLV )
//  ================================================================
	{
	return JET_errSuccess;
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckPrimaryKey_( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	Assert( m_pfucb );
	if( pfcbNil == m_pfucb->u.pfcb
		|| pidbNil == m_pfucb->u.pfcb->Pidb() )
		{
		return JET_errSuccess;
		}

	ERR err = JET_errSuccess;

	IDB * const pidb = m_pfucb->u.pfcb->Pidb();

	BYTE rgbKey[JET_cbPrimaryKeyMost];
	KEY key;
	key.Nullify();
	key.suffix.SetPv( rgbKey );
	key.suffix.SetCb( sizeof( rgbKey ) );

	BOOL fUndefinedUnicodeChars = fFalse;
	
	Call( ErrRECIRetrieveKey(
				m_pfucb,
				pidb,
				const_cast< DATA& >( kdf.data ),
				&key,
				1,
				0,
				&fUndefinedUnicodeChars,
				fFalse,
				prceNil ) );
	
	//	check for unexpected warnings
	//
	switch ( err )
		{
		case wrnFLDNullKey:
		case wrnFLDNullFirstSeg:
		case wrnFLDNullSeg:
			break;
		default:
			CallS( err );
		}
	
	//	strip out warnings from ErrRECIRetrieveKey
	//
	err = JET_errSuccess;
	

	if( fUndefinedUnicodeChars )
		{
		rgfmp[m_pfucb->ifmp].IncrementCundefinedUnicodeEntries();
		}

	if( 0 != CmpKey( key, kdf.key ) )
		{
		(*m_popts->pcprintfError)( "constructed primary key differs from real primary key\r\n" );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );		
		}

HandleError:
	return err;
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckSLV_(
	const KEYDATAFLAGS& kdf, 
	const COLUMNID columnid,
	const ULONG itagSequence,
	const DATA& dataSLV,
	const BOOL	fSeparatedSLV
	)
//  ================================================================
	{
#ifdef DISABLE_SLV
	return ErrERRCheck( JET_wrnNyi );
#else
	ERR err = JET_errSuccess;
			
	CSLVInfo 			slvinfo;
	CSLVInfo::HEADER 	header;

	//  we need to be vigilant against deadlocks between the TTARRAYs because
	//  this function is called by multiple concurrent threads.  to this end,
	//  the TTARRAY runs must be accessed according to the hierarchy given
	//  below.  any lower level TTARRAY must have any outstanding runs ended
	//  before accessing a higher level TTARRAY
	//
	//      m_pttarraySLVAvail
	//      m_pttarraySLVOwnerMapColumnid
	//      m_pttarraySLVOwnerMapKey
	//      m_pttarraySLVChecksumLengths

	TTARRAY::RUN availRun;
	TTARRAY::RUN keyRun;

	QWORD	cbSLV;

	if( NULL == m_pttarraySLVAvail )
		{
		(*m_popts->pcprintfError)( "SLV was not expected (columnid: %d, itag: %d)\r\n",
									columnid, itagSequence );
		CallR( ErrERRCheck( JET_errDatabaseCorrupted ) );		
		}
	Assert( NULL != m_pttarraySLVOwnerMapColumnid );
	Assert( NULL != m_pttarraySLVOwnerMapKey );

	if ( fSeparatedSLV )
		{
		//  Separated SLV

///		(*m_popts->pcprintfVerbose)( "separated SLV: %d bytes (columnid: %d, itag: %d)\r\n",
///										dataSLV.Cb(), columnid, itagSequence );

		}
	Call( slvinfo.ErrLoadFromData( m_pfucb, dataSLV, fSeparatedSLV ) );

	Call( slvinfo.ErrGetHeader( &header ) );

	cbSLV = header.cbSize;

	Call( slvinfo.ErrMoveBeforeFirst() );

	m_pttarraySLVAvail->BeginRun( m_pfucb->ppib, &availRun );

	while( ( err = slvinfo.ErrMoveNext() ) == JET_errSuccess )
		{
		CSLVInfo::RUN slvRun;

		Call( slvinfo.ErrGetCurrentRun( &slvRun ) );

		CPG cpg;
		PGNO pgnoSLVFirst;
		PGNO pgnoSLV;

		cpg 			= slvRun.Cpg();
		pgnoSLVFirst 	= slvRun.PgnoFirst();

		for( pgnoSLV = pgnoSLVFirst; pgnoSLV < pgnoSLVFirst + cpg; ++pgnoSLV )
			{
			OBJID objidOwning;
			
			Call( m_pttarraySLVAvail->ErrGetValue( m_pfucb->ppib, pgnoSLV, &objidOwning, &availRun ) );

			if( objidNil != objidOwning )
				{
				(*m_popts->pcprintfError)(
					"SLV space allocation error page %d is already owned by %d (columnid: %d, itag: %d, objid: %d)\r\n",
					pgnoSLV, columnid, itagSequence, objidOwning, m_objid );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );				
				}
				
			err = m_pttarraySLVAvail->ErrSetValue( m_pfucb->ppib, pgnoSLV, m_objid, &availRun );
			if( JET_errRecordNotFound == err )
				{
				(*m_popts->pcprintfError)(
					"SLV space allocation error page %d is not in the streaming file (columnid: %d, itag: %d, objid: %d)\r\n",
					pgnoSLV, columnid, itagSequence, m_objid );
				err = ErrERRCheck( JET_errDatabaseCorrupted );				
				}
			Call( err ); 	

			//  store the COLUMNID
			
			Call( m_pttarraySLVOwnerMapColumnid->ErrSetValue( m_pfucb->ppib, pgnoSLV, columnid ) );

			//	store the checksum length

			ULONG cbChecksumLength;

			cbChecksumLength 	= static_cast<ULONG>( min( g_cbPage, cbSLV ) );

			cbSLV = ( cbSLV > g_cbPage ? cbSLV - g_cbPage : 0 );

			Call( m_pttarraySLVChecksumLengths->ErrSetValue( m_pfucb->ppib, pgnoSLV, cbChecksumLength ) );
			
			//  store the first 'n' bytes of the key (pad with 0)
			
			ULONG rgulKey[culSLVKeyToStore];
			BYTE * const pbKey = (BYTE *)rgulKey;
			memset( rgulKey, 0, sizeof( rgulKey ) );

			*pbKey = BYTE( kdf.key.Cb() );

			if( kdf.key.Cb() > sizeof( rgulKey ) - 1 )
				{
				(*m_popts->pcprintfError)(
					"INTERNAL ERROR: key of SLV-owning record is too large (%d bytes, buffer is %d bytes)\r\n",
					*pbKey, sizeof( rgulKey ) - 1 );
				Call( ErrERRCheck( JET_errInternalError ) );				
				}
				
			kdf.key.CopyIntoBuffer( pbKey+1, sizeof( rgulKey )-1 );

			TTARRAY::RUN keyRun;
			m_pttarraySLVOwnerMapKey->BeginRun( m_pfucb->ppib, &keyRun );
			
			INT iul;
			for( iul = 0; iul < culSLVKeyToStore; ++iul )
				{
				Call( m_pttarraySLVOwnerMapKey->ErrSetValue(
											m_pfucb->ppib,
											pgnoSLV * culSLVKeyToStore + iul,
											rgulKey[iul],
											&keyRun ) );
				}			
			m_pttarraySLVOwnerMapKey->EndRun( m_pfucb->ppib, &keyRun );
			}
		}

	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
	
HandleError:

	m_pttarraySLVAvail->EndRun( m_pfucb->ppib, &availRun );
	m_pttarraySLVOwnerMapKey->EndRun( m_pfucb->ppib, &keyRun );
	
	slvinfo.Unload();

	if( JET_errSLVCorrupted == err )
		{
		(*m_popts->pcprintfError)( "SLV corruption\r\n" );
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		}
	return err;

#endif	//	DISABLE_SLV
	}
	

//  ================================================================
ERR RECCHECKTABLE::ErrCheckTaggedFields_( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR 		err 			= JET_errSuccess;

	if ( !TAGFIELDS::FIsValidTagfields( kdf.data, m_popts->pcprintfError ) )
		{
		return ErrERRCheck( JET_errDatabaseCorrupted );
		}

	
	FID		fidTaggedLast 		= fidTaggedLeast - 1;
	VOID * 	pvWorkBuf;

	BFAlloc( &pvWorkBuf );

	BYTE * 	pb = (BYTE *)pvWorkBuf;
	UtilMemCpy( pb, kdf.data.Pv(), kdf.data.Cb() );
		
	DATA dataRec;
	dataRec.SetPv( pb );		
	dataRec.SetCb( kdf.data.Cb() );
		
	TAGFIELDS_ITERATOR tagfieldsIterator( dataRec );
	tagfieldsIterator.MoveBeforeFirst();
	while( JET_errSuccess == ( err = tagfieldsIterator.ErrMoveNext() ) )
		{
		if( !tagfieldsIterator.FDerived() )
			{
			fidTaggedLast = max( tagfieldsIterator.Fid(), fidTaggedLast );
			}
			
		if( tagfieldsIterator.FNull() )
			{
			continue;
			}
		else
			{
			tagfieldsIterator.TagfldIterator().MoveBeforeFirst();
		
			if( tagfieldsIterator.FSLV() )
				{
				Call( tagfieldsIterator.TagfldIterator().ErrMoveNext() );		
				}
			}
		}
			
	if( JET_errNoCurrentRecord == err )
		{
		err = JET_errSuccess;
		}
	
HandleError:
	BFFree( pvWorkBuf );

	// Check if the last tagged column info in catalog is correct
	if( fidTaggedLast > m_fidLastInTDB.fidTaggedLastInTDB )
		{
		(*m_popts->pcprintfError)(
			"Record corrupted: last tagged column ID not in catalog (fid: %d, catalog last: %d)\r\n",
				fidTaggedLast, m_fidLastInTDB.fidTaggedLastInTDB );
		return ErrERRCheck( JET_errDatabaseCorrupted );
		}

	return err;
	}


//  ================================================================
ERR RECCHECKTABLE::ErrCheckLVAndSLVFields_( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	// Form ErrCheckTaggedFields_(), we have already 
	// known that FIsValidTagfields is TRUE
	Assert( TAGFIELDS::FIsValidTagfields( kdf.data, m_popts->pcprintfError ) ); 

	TAGFIELDS	tagfields( kdf.data );
	return tagfields.ErrCheckLongValuesAndSLVs( kdf, this );
	}


//  ================================================================
ERR RECCHECKTABLE::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	return ErrCheckRecord_( kdf );
	}


#ifdef DISABLE_SLV
#else

//  ================================================================
RECCHECKSLVSPACE::RECCHECKSLVSPACE( const IFMP ifmp, const REPAIROPTS * const popts ) :
//  ================================================================
	m_popts( popts ),
	m_ifmp( ifmp ),
	m_cpagesSeen( 0 )
	{
	}


//  ================================================================
RECCHECKSLVSPACE::~RECCHECKSLVSPACE()
//  ================================================================
	{
	}



//  ================================================================
ERR RECCHECKSLVSPACE::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	const SLVSPACENODE * const pslvspacenode = (SLVSPACENODE *)kdf.data.Pv();

	if( kdf.key.Cb() != sizeof( PGNO ) )
		{
		(*m_popts->pcprintfError)( "SLV space tree key is incorrect size (%d bytes, expected %d)\r\n", kdf.key.Cb(), sizeof( PGNO ) );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
		
	if( kdf.data.Cb() != sizeof( SLVSPACENODE ) )
		{
		(*m_popts->pcprintfError)( "SLV space tree data is incorrect size (%d bytes, expected %d)\r\n", kdf.data.Cb(), sizeof( SLVSPACENODE ) );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	m_cpagesSeen = m_cpagesSeen + SLVSPACENODE::cpageMap;

	ULONG pgnoCurr;
	LongFromKey( &pgnoCurr, kdf.key );
	if( m_cpagesSeen != pgnoCurr )
		{
		(*m_popts->pcprintfError)( "SLV space tree nodes out of order (pgnoCurr %d, expected %d)\r\n", pgnoCurr, m_cpagesSeen );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	Call( pslvspacenode->ErrCheckNode( m_popts->pcprintfError ) );
		
HandleError:
	return err;
	}


//  ================================================================
RECCHECKSLVOWNERMAP::RECCHECKSLVOWNERMAP(
	const REPAIROPTS * const popts ) :
//  ================================================================
	m_popts( popts )
	{
	}


//  ================================================================
RECCHECKSLVOWNERMAP::~RECCHECKSLVOWNERMAP()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKSLVOWNERMAP::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if ( !SLVOWNERMAP::FValidData( kdf.data ) )
		{
		(*m_popts->pcprintfError)( "SLV space map node corrupted.\r\n" );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	else
		{
		SLVOWNERMAP	slvownermap;
		slvownermap.Retrieve( kdf.data );

		const OBJID	objid				= slvownermap.Objid();
		if( objidNil != objid )
			{
			const COLUMNID columnid		= slvownermap.Columnid();
			const USHORT cbKey			= slvownermap.CbKey();
			const BOOL fValidChecksum 	= slvownermap.FValidChecksum();
			if( 0 == columnid )
				{
				(*m_popts->pcprintfError)( "SLV space map node invalid columnid (%d).\r\n", columnid );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			if( cbKey > JET_cbKeyMost )
				{
				(*m_popts->pcprintfError)( "SLV space map node key is too long (%d bytes).\r\n", cbKey );
				Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
				}
			if( fValidChecksum )
				{
				const ULONG ulChecksum 			= slvownermap.UlChecksum();
				const ULONG cbChecksumLength	= slvownermap.CbDataChecksummed();
				if( cbChecksumLength > g_cbPage )
					{
					(*m_popts->pcprintfError)( "SLV space map node checksum length is too large (%d bytes).\r\n", cbChecksumLength );
					Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
					}
				}
			}
		}
		
HandleError:
	return err;
	}

#endif	//	DISABLE_SLV


//  ================================================================
RECCHECKMACRO::RECCHECKMACRO() :
//  ================================================================
	m_creccheck( 0 )
	{
	memset( m_rgpreccheck, 0, sizeof( m_rgpreccheck ) );
	}

	
//  ================================================================
RECCHECKMACRO::~RECCHECKMACRO()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKMACRO::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	INT ipreccheck;
	for( ipreccheck = 0; ipreccheck < m_creccheck; ipreccheck++ )
		{
		Call( (*m_rgpreccheck[ipreccheck])( kdf ) );
		}

HandleError:
	return err;
	}


//  ================================================================
VOID RECCHECKMACRO::Add( RECCHECK * const preccheck )
//  ================================================================
	{
	m_rgpreccheck[m_creccheck++] = preccheck;
	Assert( m_creccheck < ( sizeof( m_rgpreccheck ) / sizeof( preccheck ) ) );
	}


//  ================================================================
RECCHECKSPACE::RECCHECKSPACE( PIB * const ppib, const REPAIROPTS * const popts ) :
//  ================================================================
	m_ppib( ppib ),
	m_popts( popts ),
	m_pgnoLast( pgnoNull ),
	m_cpgLast( 0 ),
	m_cpgSeen( 0 )
	{
	}


//  ================================================================
RECCHECKSPACE::~RECCHECKSPACE()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKSPACE::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	PGNO pgno = pgnoNull;
	PGNO pgnoT = pgnoNull;
	CPG cpg = 0;
	
	if( kdf.key.Cb() != sizeof( PGNO ) )
		{
		(*m_popts->pcprintfError)( "space tree key is incorrect size (%d bytes, expected %d)\r\n", kdf.key.Cb(), sizeof( PGNO ) );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}
	if( kdf.data.Cb() != sizeof( PGNO ) )
		{
		(*m_popts->pcprintfError)( "space tree data is incorrect size (%d bytes, expected %d)\r\n", kdf.key.Cb(), sizeof( PGNO ) );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( FNDVersion( kdf ) )
		{
		(*m_popts->pcprintfError)( "versioned node in space tree\r\n" );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	LongFromKey( &pgno, kdf.key );
	cpg = *(UnalignedLittleEndian< CPG > *)kdf.data.Pv();

	if( pgno <= m_pgnoLast )
		{
		(*m_popts->pcprintfError)( "space tree corruption (previous pgno %d, current pgno %d)\r\n", m_pgnoLast, pgno );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	if( ( pgno - cpg ) < m_pgnoLast )
		{
		(*m_popts->pcprintfError)( "space tree overlap (previous extent was %d:%d, current extent is %d:%d)\r\n",
									m_pgnoLast, m_cpgLast, pgno, cpg );
		Call( ErrERRCheck( JET_errDatabaseCorrupted ) );
		}

	m_pgnoLast = pgno;
	m_cpgLast = cpg;
	m_cpgSeen += cpg;
		
HandleError:
	return err;
	}


//  ================================================================
RECCHECKSPACEOE::RECCHECKSPACEOE(
	PIB * const ppib,
	TTARRAY * const pttarrayOE,
	const OBJID objid,
	const OBJID objidParent,
	const REPAIROPTS * const popts ) :
//  ================================================================
	RECCHECKSPACE( ppib, popts ),
	m_pttarrayOE( pttarrayOE ),
	m_objid( objid ),
	m_objidParent( objidParent )
	{
	}


//  ================================================================
RECCHECKSPACEOE::~RECCHECKSPACEOE()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKSPACEOE::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	CallR( RECCHECKSPACE::operator()( kdf ) );

	Assert( sizeof( PGNO ) == kdf.key.Cb() );
	PGNO pgnoLast;
	LongFromKey( &pgnoLast, kdf.key );
	
	Assert( sizeof( CPG ) == kdf.data.Cb() );
	const CPG cpgRun = *(UnalignedLittleEndian< CPG > *)(kdf.data.Pv());

	CallR( ErrREPAIRInsertOERunIntoTT(
		m_ppib,
		pgnoLast,
		cpgRun,
		m_objid,
		m_objidParent,
		m_pttarrayOE,
		m_popts ) );
		
	return err;
	}


//  ================================================================
RECCHECKSPACEAE::RECCHECKSPACEAE(
	PIB * const ppib,
	TTARRAY * const pttarrayOE,
	TTARRAY * const pttarrayAE,
	const OBJID objid,
	const OBJID objidParent,
	const REPAIROPTS * const popts ) :
//  ================================================================
	RECCHECKSPACE( ppib, popts ),
	m_pttarrayOE( pttarrayOE ),
	m_pttarrayAE( pttarrayAE ),
	m_objid( objid ),
	m_objidParent( objidParent )
	{
	}


//  ================================================================
RECCHECKSPACEAE::~RECCHECKSPACEAE()
//  ================================================================
	{
	}


//  ================================================================
ERR RECCHECKSPACEAE::operator()( const KEYDATAFLAGS& kdf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	CallR( RECCHECKSPACE::operator()( kdf ) );

	Assert( sizeof( PGNO ) == kdf.key.Cb() );
	PGNO pgnoLast;
	LongFromKey( &pgnoLast, kdf.key );
	
	Assert( sizeof( CPG ) == kdf.data.Cb() );
	const CPG cpgRun = *(UnalignedLittleEndian< CPG > *)(kdf.data.Pv());	

	CallR( ErrREPAIRInsertAERunIntoTT(
		m_ppib,
		pgnoLast,
		cpgRun,
		m_objid,
		m_objidParent,
		m_pttarrayOE,
		m_pttarrayAE,
		m_popts ) );

	return err;
	}


//  ================================================================
REPAIROPTS::REPAIROPTS() :
//  ================================================================
	crit( CLockBasicInfo( CSyncBasicInfo( szRepairOpts ), rankRepairOpts, 0 ) )
	{
	}

	
//  ================================================================
REPAIROPTS::~REPAIROPTS()
//  ================================================================
	{
	}


#ifdef DISABLE_SLV
#else

//  ================================================================
CSLVAvailIterator::CSLVAvailIterator() :
//  ================================================================
	m_pfucb( pfucbNil ),
	m_pgnoCurr( 0 ),
	m_ipage( 0 )
	{
	}


//  ================================================================
CSLVAvailIterator::~CSLVAvailIterator()
//  ================================================================
	{
	Assert( pfucbNil == m_pfucb );
	}
	

//  ================================================================
ERR CSLVAvailIterator::ErrInit( PIB * const ppib, const IFMP ifmp )
//  ================================================================
	{
	Assert( pfucbNil == m_pfucb );
	
	ERR		err;
	
	PGNO 	pgnoSLVAvail;
	OBJID	objidSLVAvail;
	Call( ErrCATAccessDbSLVAvail( ppib, ifmp, szSLVAvail, &pgnoSLVAvail, &objidSLVAvail ) );

	Call( ErrDIROpen( ppib, pgnoSLVAvail, ifmp, &m_pfucb ) );	

	FUCBSetPrereadForward( m_pfucb, cpgPrereadSequential );
	
HandleError:
	return err;
	}


//  ================================================================
ERR CSLVAvailIterator::ErrTerm()
//  ================================================================
	{
	if( pfucbNil != m_pfucb )
		{
		DIRClose( m_pfucb );
		m_pfucb = pfucbNil;
		}
	return JET_errSuccess;
	}


//  ================================================================
ERR CSLVAvailIterator::ErrMoveFirst()
//  ================================================================
	{
	Assert( pfucbNil != m_pfucb );
	
	ERR err = JET_errSuccess;
	
	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm 	= NULL;
	dib.dirflag = fDIRNull;
	Call( ErrDIRDown( m_pfucb, &dib ) );

	m_ipage 		= 0;
	m_pgnoCurr 		= 0;
	m_pspacenode	= (SLVSPACENODE *)m_pfucb->kdfCurr.data.Pv();

HandleError:
	return err;
	}
	

//  ================================================================
ERR CSLVAvailIterator::ErrMoveNext()
//  ================================================================
	{
	Assert( pfucbNil != m_pfucb );
	
	ERR err = JET_errSuccess;
	
	++m_ipage;
	if( SLVSPACENODE::cpageMap == m_ipage )
		{
		
		//  move to the next SLVSPACENODE
		
		Call( ErrDIRNext( m_pfucb, fDIRNull ) );
		m_ipage 		= 0;
		m_pgnoCurr		+= SLVSPACENODE::cpageMap;
		m_pspacenode	= (SLVSPACENODE *)m_pfucb->kdfCurr.data.Pv();	
		}

HandleError:
	return err;
	}

	
//  ================================================================
BOOL CSLVAvailIterator::FCommitted() const
//  ================================================================
	{
	Assert( pfucbNil != m_pfucb );
	Assert( NULL != m_pspacenode );
	Assert( m_ipage < SLVSPACENODE::cpageMap );
	
	const SLVSPACENODE::STATE state = m_pspacenode->GetState( m_ipage );
	return ( SLVSPACENODE::sCommitted == state );
	}

	
//  ================================================================
CSLVOwnerMapIterator::CSLVOwnerMapIterator() :
//  ================================================================
	m_pfucb( pfucbNil ),
	m_slvownermapnode()
	{
	}


//  ================================================================
CSLVOwnerMapIterator::~CSLVOwnerMapIterator()
//  ================================================================
	{
	Assert( pfucbNil == m_pfucb );
	}


//  ================================================================
ERR CSLVOwnerMapIterator::ErrInit( PIB * const ppib, const IFMP ifmp )
//  ================================================================
	{
	Assert( pfucbNil == m_pfucb );
	
	ERR		err;
	
	PGNO 	pgnoSLVOwnerMap;
	OBJID	objidSLVOwnerMap;
	Call( ErrCATAccessDbSLVOwnerMap( ppib, ifmp, szSLVOwnerMap, &pgnoSLVOwnerMap, &objidSLVOwnerMap ) );

	Call( ErrDIROpen( ppib, pgnoSLVOwnerMap, ifmp, &m_pfucb ) );

	FUCBSetPrereadForward( m_pfucb, cpgPrereadSequential );
	
HandleError:
	return err;
	}


//  ================================================================
ERR CSLVOwnerMapIterator::ErrTerm()
//  ================================================================
	{
	if( pfucbNil != m_pfucb )
		{
		DIRClose( m_pfucb );
		m_pfucb = pfucbNil;
		}
	return JET_errSuccess;
	}


//  ================================================================
ERR CSLVOwnerMapIterator::ErrMoveFirst()
//  ================================================================
	{
	Assert( pfucbNil != m_pfucb );
	
	ERR err = JET_errSuccess;
	
	DIB dib;
	dib.pos 	= posFirst;
	dib.pbm 	= NULL;
	dib.dirflag = fDIRNull;
	Call( ErrDIRDown( m_pfucb, &dib ) );

	m_slvownermapnode.Retrieve( m_pfucb->kdfCurr.data );
	
HandleError:
	return err;
	}


//  ================================================================
ERR CSLVOwnerMapIterator::ErrMoveNext()
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	Call( ErrDIRNext( m_pfucb, fDIRNull ) );

	m_slvownermapnode.Retrieve( m_pfucb->kdfCurr.data );

HandleError:
	return err;
	}


//  ================================================================
BOOL CSLVOwnerMapIterator::FNull() const
//  ================================================================
	{
	return ( 0 == CbKey() );
	}

	
//  ================================================================
OBJID CSLVOwnerMapIterator::Objid() const
//  ================================================================
	{
	return m_slvownermapnode.Objid();
	}


//  ================================================================
COLUMNID CSLVOwnerMapIterator::Columnid() const
//  ================================================================
	{
	return m_slvownermapnode.Columnid();
	}


//  ================================================================
const VOID * CSLVOwnerMapIterator::PvKey() const
//  ================================================================
	{
	return m_slvownermapnode.PvKey();
	}
	

//  ================================================================
INT CSLVOwnerMapIterator::CbKey() const
//  ================================================================
	{
	return m_slvownermapnode.CbKey();
	}


//  ================================================================
ULONG CSLVOwnerMapIterator::UlChecksum() const
//  ================================================================
	{
	return m_slvownermapnode.UlChecksum();
	}


//  ================================================================
USHORT CSLVOwnerMapIterator::CbDataChecksummed() const
//  ================================================================
	{
	return m_slvownermapnode.CbDataChecksummed();
	}


//  ================================================================
BOOL CSLVOwnerMapIterator::FChecksumIsValid() const
//  ================================================================
	{
	return m_slvownermapnode.FValidChecksum();
	}

#endif	//	DISABLE_SLV

#endif	//	MINIMAL_FUNCTIONALITY
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\stats.cxx ===
#include "std.hxx"

ERR ErrSTATSComputeIndexStats( PIB *ppib, FCB *pfcbIdx, FUCB *pfucbTable )
	{
	ERR				err = JET_errSuccess;
	FUCB			*pfucbIdx;
	SR				sr;
	JET_DATESERIAL	dt;
	OBJID			objidTable;
	CHAR 			szIndexName[JET_cbNameMost+1];

	CallR( ErrDIROpen( ppib, pfcbIdx, &pfucbIdx ) );
	Assert( pfucbIdx != pfucbNil );
	FUCBSetIndex( pfucbIdx );

	/*	initialize stats record
	/**/
	sr.cPages = sr.cItems = sr.cKeys = 0L;
	UtilGetDateTime( &dt );
	UtilMemCpy( &sr.dtWhenRun, &dt, sizeof sr.dtWhenRun );

	if ( pfcbIdx->FPrimaryIndex() )
		{
		objidTable = pfcbIdx->ObjidFDP();
		}
	else
		{
		FCB	*pfcbTable = pfucbTable->u.pfcb;
		objidTable = pfcbTable->ObjidFDP();
		Assert( pfcbTable->Ptdb() != ptdbNil );

		Assert( pfcbIdx->FTypeSecondaryIndex() );
		if ( pfcbIdx->FDerivedIndex() )
			{
			Assert( pfcbIdx->Pidb()->FTemplateIndex() );
			pfcbTable = pfcbTable->Ptdb()->PfcbTemplateTable();
			Assert( pfcbNil != pfcbTable );
			Assert( pfcbTable->FTemplateTable() );
			Assert( pfcbTable->Ptdb() != ptdbNil );
			}
		
		pfcbTable->EnterDML();
		Assert( pfcbIdx->Pidb()->ItagIndexName() != 0 );
		strcpy( szIndexName,
			pfcbTable->Ptdb()->SzIndexName( pfcbIdx->Pidb()->ItagIndexName() ) );
		pfcbTable->LeaveDML();
		FUCBSetSecondary( pfucbIdx );
		}
		
	Call( ErrDIRComputeStats( pfucbIdx, reinterpret_cast<INT *>( &sr.cItems ), reinterpret_cast<INT *>( &sr.cKeys ), 
								reinterpret_cast<INT *>( &sr.cPages ) ) );
	FUCBResetSecondary( pfucbIdx );

	/*	write stats
	/**/
	Call( ErrCATStats(
			ppib,
			pfucbIdx->ifmp,
			objidTable,
			pfcbIdx->FPrimaryIndex() ? NULL : szIndexName,
			&sr,
			fTrue ) );

HandleError:
	//	set secondary for cursor reuse support
	if ( !pfcbIdx->FPrimaryIndex() )
		FUCBSetSecondary( pfucbIdx );
	DIRClose( pfucbIdx );
	return err;
	}


ERR VTAPI ErrIsamComputeStats( JET_SESID sesid, JET_VTID vtid )
	{
 	PIB		*ppib	= reinterpret_cast<PIB *>( sesid );
	FUCB	*pfucb = reinterpret_cast<FUCB *>( vtid );

	ERR		err = JET_errSuccess;
	FCB		*pfcbTable;
	FCB		*pfcbIdx;

	CallR( ErrPIBCheck( ppib ) );
	CheckTable( ppib, pfucb );

	/*	start a transaction, in case anything fails
	/**/
	CallR( ErrDIRBeginTransaction( ppib, NO_GRBIT ) );

	/*	compute stats for each index
	/**/
	pfcbTable = pfucb->u.pfcb;
	Assert( pfcbTable != pfcbNil );

	Assert( !pfcbTable->FDeletePending() );
	Assert( !pfcbTable->FDeleteCommitted() );
	Call( ErrSTATSComputeIndexStats( ppib, pfcbTable, pfucb ) );
	
	pfcbTable->EnterDML();
	for ( pfcbIdx = pfcbTable->PfcbNextIndex(); pfcbIdx != pfcbNil; pfcbIdx = pfcbIdx->PfcbNextIndex() )
		{
		Assert( pfcbIdx->FTypeSecondaryIndex() );
		Assert( pfcbIdx->Pidb() != pidbNil );

		err = ErrFILEIAccessIndex( ppib, pfcbTable, pfcbIdx );

		if ( err < 0 )
			{
			if ( JET_errIndexNotFound != err )
				{
				pfcbTable->LeaveDML();
				goto HandleError;
				}
			}
		else
			{
			pfcbTable->LeaveDML();
			// Because we're in a transaction, this guarantees that the FCB won't
			// be cleaned up while we're trying to retrieve stats.
			Call( ErrSTATSComputeIndexStats( ppib, pfcbIdx, pfucb ) );
			pfcbTable->EnterDML();
			}
		}
	pfcbTable->LeaveDML();

	/*	commit transaction if everything went OK
	/**/
	Call( ErrDIRCommitTransaction( ppib, JET_bitCommitLazyFlush ) );

	return err;

HandleError:
	Assert( err < 0 );
	CallSx( ErrDIRRollback( ppib ), JET_errRollbackError );
	return err;
	}


/*=================================================================
ErrSTATSRetrieveStats

Description: Returns the number of records and pages used for a table

Parameters:		ppib				pointer to PIB for current session or ppibNil
				ifmp				database id or 0
				pfucb				cursor or pfucbNil
				szTableName			the name of the table or NULL
				pcRecord			pointer to count of records
				pcPage				pointer to count of pages

Errors/Warnings:
				JET_errSuccess or error from called routine.

=================================================================*/
ERR ErrSTATSRetrieveTableStats(
	PIB			* ppib,
	const IFMP 	ifmp,
	char		* szTable,
	long		* pcRecord,
	long		* pcKey,
	long		* pcPage )
	{
	ERR			err;
	FUCB		* pfucb;
	SR			sr;

	CallR( ErrFILEOpenTable( ppib, ifmp, &pfucb, szTable, NO_GRBIT ) );

	Call( ErrCATStats(
				pfucb->ppib,
				pfucb->ifmp,
				pfucb->u.pfcb->ObjidFDP(),
				NULL,
				&sr,
				fFalse));

	/*	set output variables
	/**/
	if ( pcRecord )
		*pcRecord = sr.cItems;
	if ( pcPage )
		*pcPage = sr.cPages;
	if ( pcKey )
		*pcKey = sr.cKeys;

	CallS( err );

HandleError:
	CallS( ErrFILECloseTable( ppib, pfucb ) );
	return err;
	}


ERR ErrSTATSRetrieveIndexStats(
	FUCB   	*pfucbTable,
	char   	*szIndex,
	BOOL	fPrimary,
	long   	*pcItem,
	long   	*pcKey,
	long   	*pcPage )
	{
	ERR		err;
	SR		sr;

	// The name is assumed to be valid.

	CallR( ErrCATStats(
				pfucbTable->ppib,
				pfucbTable->ifmp,
				pfucbTable->u.pfcb->ObjidFDP(),
				( fPrimary ? NULL : szIndex),
				&sr,
				fFalse ) );

	/*	set output variables
	/**/
	if ( pcItem )
		*pcItem = sr.cItems;
	if ( pcPage )
		*pcPage = sr.cPages;
	if ( pcKey )
		*pcKey = sr.cKeys;

	CallS( err );

	return JET_errSuccess;
	}


	ERR VTAPI
ErrIsamGetRecordPosition( JET_SESID vsesid, JET_VTID vtid, JET_RECPOS *precpos, unsigned long cbRecpos )
	{
	ERR		err;
	ULONG  	ulLT;
	ULONG	ulTotal;
	PIB *ppib = (PIB *)vsesid;
	FUCB *pfucb = (FUCB *)vtid;

	CallR( ErrPIBCheck( ppib ) );
	CheckTable( ppib, pfucb );
	Assert( FFUCBIndex( pfucb ) );

	if ( cbRecpos < sizeof(JET_RECPOS) )
		return ErrERRCheck( JET_errInvalidParameter );
	precpos->cbStruct = sizeof(JET_RECPOS);

	//	get position of secondary or primary cursor
	if ( pfucb->pfucbCurIndex != pfucbNil )
		{
		Call( ErrDIRGetPosition( pfucb->pfucbCurIndex, &ulLT, &ulTotal ) );
		}
	else
		{
		Call( ErrDIRGetPosition( pfucb, &ulLT, &ulTotal ) );
		}

	precpos->centriesLT = ulLT;
	//	CONSIDER:	remove this bogus field
	precpos->centriesInRange = 1;
	precpos->centriesTotal = ulTotal;

HandleError:
	return err;
	}


ERR ISAMAPI ErrIsamIndexRecordCount( JET_SESID sesid, JET_TABLEID tableid, unsigned long *pulCount, unsigned long ulCountMost )
	{
	ERR	 	err;
	PIB	 	*ppib = (PIB *)sesid;
	FUCB 	*pfucb;
	FUCB 	*pfucbIdx;

	CallR( ErrPIBCheck( ppib ) );

	/*	get pfucb from tableid
	/**/
	CallR( ErrFUCBFromTableid( ppib, tableid, &pfucb ) );

	CheckTable( ppib, pfucb );

	/*	get cursor for current index
	/**/
	if ( pfucb->pfucbCurIndex != pfucbNil )
		pfucbIdx = pfucb->pfucbCurIndex;
	else
		pfucbIdx = pfucb;

	err = ErrDIRIndexRecordCount( pfucbIdx, pulCount, ulCountMost, fTrue );
	AssertDIRNoLatch( ppib );
	return err;
	};
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\taskmgr.cxx ===
#include "std.hxx"


//	ctor

TASKMGR::TASKMGR()
	{
	m_cContext		= 0;
	m_rgdwContext	= NULL;

	m_fInit			= fFalse;
	}


//	dtor

TASKMGR::~TASKMGR()
	{
	}


//	initialize the TASKMGR

ERR TASKMGR::ErrInit( INST *const pinst, const INT cThread )
	{
	ERR err;

	//	verify input

	Assert( NULL != pinst );
	Assert( cThread > 0 );

	//	the task manager should not be initialized yet

	Assert( !m_fInit );

	//	verify the state of the TASKMGR

	Assert( 0 == m_cContext );
	Assert( NULL == m_rgdwContext );

	//	allocate an array of session handles (the per-thread contexts)

	Assert( sizeof( PIB * ) <= sizeof( DWORD_PTR ) );
	m_cContext = 0;
	m_rgdwContext = new DWORD_PTR[cThread];
	if ( NULL == m_rgdwContext )
		{
		Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
		}
	memset( m_rgdwContext, 0, sizeof( DWORD_PTR ) * cThread );

	//	open all sessions

	while ( m_cContext < cThread )
		{
		PIB *ppib;

		//	open a session

		Call( ErrPIBBeginSession( pinst, &ppib, procidNil, fFalse ) );
		ppib->grbitsCommitDefault = JET_bitCommitLazyFlush;
		ppib->SetFSystemCallback();

		//	bind it to the per-thread context array (AFTER the session has been successfully opened)

		m_rgdwContext[m_cContext++] = DWORD_PTR( ppib );
		}

	//	initialize the real task manager

	Call( m_tm.ErrTMInit( m_cContext, m_rgdwContext ) );

	//	turn the task manager "on"

	{
#ifdef DEBUG
	const BOOL fInitBefore =
#endif	//	DEBUG
	AtomicExchange( (long *)&m_fInit, fTrue );
	Assert( !fInitBefore );
	}

	return JET_errSuccess;
	
HandleError:

	//	cleanup

	CallS( ErrTerm() );

	return err;
	}


//	terminate the TASKMGR
//	NOTE: does not need to return an error-code (left-over from previous version)

ERR TASKMGR::ErrTerm()
	{
	ULONG iThread;

	if ( m_fInit )
		{
		//	turn the task manager "off"
		//
#ifdef DEBUG
		const BOOL fInitBefore =
#endif	//	DEBUG
		AtomicExchange( (long *)&m_fInit, fFalse );
		Assert( fInitBefore );

		//	wait for everyone currently in the middle of posting a task to finish
		//
		m_cmsPost.Partition();
		}

	//	term the real task manager
	//
	m_tm.TMTerm();

	//	cleanup the per-thread contexts
	//
	if ( NULL != m_rgdwContext )
		{
		for ( iThread = 0; iThread < m_cContext; iThread++ )
			{
			Assert( NULL != m_rgdwContext );
			Assert( 0 != m_rgdwContext[iThread] );
			PIBEndSession( (PIB *)m_rgdwContext[iThread] );
			}

		delete m_rgdwContext;
		m_rgdwContext = NULL;
		}
	m_cContext = 0;

	return JET_errSuccess;
	}


//	post a task

ERR TASKMGR::ErrPostTask( const TASK pfnTask, const ULONG_PTR ul )
	{
	ERR				err;
	TASKMGRCONTEXT	*ptmc;
	int				iGroup;

	//	allocate a private context

	ptmc = new TASKMGRCONTEXT;
	if ( NULL == ptmc )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	//	populate the context

	ptmc->pfnTask = pfnTask;
	ptmc->ul = ul;

	//	enter the task-post metered section

	iGroup = m_cmsPost.Enter();

	if ( m_fInit )
		{

		//	pass the task down to the real task manager (will be dispatched via TASKMGR::Dispatch)

		err = m_tm.ErrTMPost( CTaskManager::PfnCompletion( Dispatch ), 0, DWORD_PTR( ptmc ) );
		CallSx( err, JET_errOutOfMemory );	//	we should only see JET_errSuccess or JET_errOutOfMemory
		}
	else
		{

		//	the task manager is not initialized so the task must be dropped

		err = ErrERRCheck( JET_errTaskDropped );
		AssertTracking();	//	this shouldn't happen; trap it because caller may not handle this case well
		}

	//	leave the task-post metered section

	m_cmsPost.Leave( iGroup );

	if ( err < JET_errSuccess )
		{

		//	cleanup the task context

		delete ptmc;
		}

	return err;
	}


//	dispatch a task

VOID TASKMGR::Dispatch(	const DWORD_PTR dwThreadContext,
						const DWORD		dwCompletionKey1,
						const DWORD_PTR	dwCompletionKey2 )
	{
	PIB				*ppib;
	TASKMGRCONTEXT	*ptmc;
	TASK			pfnTask;
	ULONG_PTR		ul;

	//	verify input 

	Assert( 0 != dwThreadContext );
	Assert( 0 == dwCompletionKey1 );
	Assert( 0 != dwCompletionKey2 );

	//	extract the session

	ppib = (PIB *)dwThreadContext;

	//	extract the task's context and clean it up

	ptmc = (TASKMGRCONTEXT *)dwCompletionKey2;
	pfnTask = ptmc->pfnTask;
	ul = ptmc->ul;
	delete ptmc;

	//	run the task

	Assert( NULL != pfnTask );
	pfnTask( ppib, ul );
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\sysinit.cxx ===
#include "std.hxx"

#include <ctype.h>
#include <io.h>

extern	INT 	itibGlobal;


#if defined( DEBUG ) || defined( PERFDUMP )
BOOL	fDBGPerfOutput = fFalse;
long	lAPICallLogLevel = 4;
#endif	/* DEBUG || PERFDUMP */

//	system parameter constants
//
long	g_lSessionsMax = cpibDefault;
long	g_lOpenTablesMax = cfcbDefault;
long	g_lOpenTablesPreferredMax = 0;
long	g_lTemporaryTablesMax = cscbDefault;
long	g_lCursorsMax = cfucbDefault;
long	g_lVerPagesMax = cbucketDefault;
long	g_lVerPagesMin = cbucketDefault;
long	g_lVerPagesPreferredMax = long( cbucketDefault * 0.9 );
long	g_lLogBuffers = csecLogBufferDefault;
long	g_lLogFileSize = csecLogFileSizeDefault;
BOOL	g_fSetLogFileSize = fFalse;
long	g_grbitsCommitDefault = NO_GRBIT;
long	g_lPageFragment = lPageFragmentDefault;
CHAR	g_szLogFilePath[cbFilenameMost] = ".\\";
CHAR	g_szLogFileFailoverPath[cbFilenameMost] = "";
BOOL	g_fLogFileCreateAsynch = fTrue;
BOOL	fDoNotOverWriteLogFilePath = fFalse;
CHAR	g_szRecovery[cbFilenameMost] = "on";
CHAR	g_szAlternateDbDirDuringRecovery[IFileSystemAPI::cchPathMax] = "";
BOOL	g_fAlternateDbDirDuringRecovery = fFalse;
#ifdef ESENT
BOOL	g_fDeleteOldLogs = fTrue;
#else
BOOL	g_fDeleteOldLogs = fFalse;
#endif
BOOL   g_fDeleteOutOfRangeLogs =fFalse;
LONG	g_cbEventHeapMax	= cbEventHeapMaxDefault;
LONG	g_cpgBackupChunk = cpgBackupChunkDefault;
LONG	g_cBackupRead = cBackupReadDefault;
BOOL	g_fLGCircularLogging = fFalse;

LONG	g_cpageTempDBMin				= cpageTempDBMinDefault;
BOOL	g_fTempTableVersioning			= fTrue;
BOOL	g_fScrubDB						= fFalse;

LONG	g_fGlobalOLDLevel				= JET_OnlineDefragAll;
LONG	g_lEventLoggingLevel			= JET_EventLoggingLevelMax;

//	we don't want to use up all the system PIB's so don't post as many tasks are there are PIBs
#ifdef DISABLE_SLV
ULONG	g_ulVERTasksPostMax				= cpibSystemFudge / 2;
#else
ULONG	g_ulVERTasksPostMax				= cpibSystemFudge / 4;
#endif

IDXUNICODE	g_idxunicodeDefault			= { lcidDefault, dwLCMapFlagsDefault };
ULONG	g_chIndexTuplesLengthMin		= chIDXTuplesLengthMinDefault;
ULONG	g_chIndexTuplesLengthMax		= chIDXTuplesLengthMaxDefault;
ULONG	g_chIndexTuplesToIndexMax		= chIDXTuplesToIndexMaxDefault;

JET_CALLBACK	g_pfnRuntimeCallback	= NULL;

CHAR	g_szSystemPath[IFileSystemAPI::cchPathMax]	= ".\\";

BOOL	g_fSLVProviderEnabled = fSLVProviderEnabledDefault;
wchar_t	g_wszSLVProviderRootPath[IFileSystemAPI::cchPathMax] = wszSLVProviderRootPathDefault;

LONG g_lSLVDefragFreeThreshold = lSLVDefragFreeThresholdDefault;  // chunks whose free % is >= this will be allocated from
LONG g_lSLVDefragMoveThreshold = lSLVDefragMoveThresholdDefault;  // chunks whose free % is <= this will be relocated

CHAR	g_szTempDatabase[IFileSystemAPI::cchPathMax]	= szDefaultTempDbFileName szDefaultTempDbExt;	// pathed filename
CHAR	szBaseName[16]				= "edb";
CHAR	szJet[16] 					= "edb";
CHAR	szJetLog[16] 				= "edb.log";
CHAR	szJetLogNameTemplate[16] 	= "edb00000";
CHAR	szJetTmp[16] 				= "edbtmp";
CHAR	szJetTmpLog[16]  			= "edbtmp.log";
CHAR	szJetTxt[16] 				= "edb.txt";

LONG g_cpgSESysMin = cpageDbExtensionDefault;

#ifdef DEBUG
BOOL	g_fCbPageSet = fFalse;
#endif // DEBUG

LONG	g_cbPage = g_cbPageDefault;
LONG	g_cbColumnLVChunkMost = g_cbPage - JET_cbColumnLVPageOverhead;
LONG 	g_cbLVBuf = 8 * g_cbColumnLVChunkMost;
INT		g_shfCbPage = 12;
INT		cbRECRecordMost = REC::CbRecordMax();
#ifdef INTRINSIC_LV
ULONG 	cbLVIntrinsicMost = cbRECRecordMost - sizeof(REC::cbRecordHeader) - sizeof(TAGFLD) - sizeof(TAGFLD_HEADER);
#endif // INTRINSIC_LV

BOOL 	g_fCallbacksDisabled = fFalse;

BOOL	g_fCreatePathIfNotExist = fFalse;

BOOL	g_fCleanupMismatchedLogFiles = fFalse;

LONG	g_cbPageHintCache = cbPageHintCacheDefault;

BOOL	g_fOneDatabasePerSession	= fFalse;

char const *szCheckpoint			= "Checkpoint";
char const *szCritCallbacks			= "Callbacks";
#if defined(DEBUG) && defined(MEM_CHECK)
char const *szCALGlobal				= "rgCAL";
#endif	//	DEBUG && MEM_CHECK
char const *szLGBuf					= "LGBuf";
char const *szLGTrace				= "LGTrace";
char const *szLGResFiles			= "LGResFiles";
char const *szLGWaitQ				= "LGWaitQ";
char const *szLGFlush				= "LGFlush";
char const *szRES					= "RES";
char const *szPIBGlobal				= "PIBGlobal";
char const *szOLDTaskq				= "OLDTaskQueue";
char const *szFCBCreate				= "FCBCreate";
char const *szFCBList				= "FCBList";
char const *szFCBRCEList			= "FCBRCEList";
char const *szFMPSLVSpace			= "FMPSLVSpace";
char const *szFCBSXWL				= "FCB::m_sxwl";
char const *szFMPGlobal				= "FMPGlobal";
char const *szFMP					= "FMP";
char const *szFMPDetaching			= "FMPDetaching";
char const *szDBGPrint				= "DBGPrint";
char const *szRCEClean				= "RCEClean";
char const *szBucketGlobal			= "BucketGlobal";
char const *szRCEChain				= "RCEChain";
char const *szPIBTrx				= "PIBTrx";
char const *szPIBLogDeferBeginTrx	= "PIBLogDeferBeginTrx";
char const *szPIBCursors			= "PIBCursors";
char const *szVERPerf				= "VERPerf";
char const *szLVCreate				= "LVCreate";
char const *szTrxOldest				= "TrxOldest";
char const *szFILEUnverCol			= "FILEUnverCol";
char const *szFILEUnverIndex		= "FILEUnverIndex";
char const *szInstance				= "Instance";
char const *szAPI					= "API";
char const *szBegin0Commit0			= "Begin0/Commit0";
char const *szIndexingUpdating		= "Indexing/Updating";
char const *szDDLDML				= "DDL/DML";
char const *szTTMAP					= "TTMAP";
char const *szIntegGlobals			= "IntegGlobals";
char const *szRepairOpts			= "RepairOpts";
char const *szUpgradeOpts			= "UpgradeOpts";
char const *szCritSLVSPACENODECACHE = "SLVSPACENODECACHE";
char const *szBFDUI					= "BFDUI";
char const *szBFHash				= "BFHash";
char const *szBFLRUK				= "BFLRUK";
char const *szBFOB0					= "BFOB0";
char const *szBFFMPContext			= "BFFMPContext Updating/Accessing";
char const *szBFLatch				= "BFLatch Read/RDW/Write";
char const *szBFDepend				= "BFDepend";
char const *szBFParm				= "BFParm";
char const *szRestoreInstance		= "RestoreInstance";

//	these are actually also used in non-RTM for the Error Trap
const _TCHAR szDEBUGRoot[]			= _T( "DEBUG" );
#define DEBUG_ENV_VALUE_LEN			256


#ifdef DEBUG

BOOL fDBGPrintToStdOut;

_TCHAR* GetDebugEnvValue( _TCHAR* szEnvVar )
	{
	_TCHAR	szBufTemp[ DEBUG_ENV_VALUE_LEN ];

	if ( FOSConfigGet( szDEBUGRoot, szEnvVar, szBufTemp, DEBUG_ENV_VALUE_LEN ) )
		{
		if ( szBufTemp[0] )
			{
			// UNDONE  we don't really want to deal with an OutOfMemory
			// error at this point. Anyway it is debug only.
			
			RFSDisable();
			_TCHAR* szBuf = new _TCHAR[ _tcslen( szBufTemp ) + 1 ];
			RFSEnable();

			//	if we really are OutOfMemory we return NULL as the EnvVariable is not set
			//	we do the same also if the one set in Env too long so ...
			//	and probably we exit soon anyway as we are OutOfMemory
			if ( szBuf )
				{
				_tcscpy( szBuf, szBufTemp );
				}
			return szBuf;
			}
		else
			{
//			FOSConfigGet() will create the key if it doesn't exist
			}
		}
	else
		{
		//  UNDONE:  gripe in the event log that the value was too big
		}

	return NULL;
	}

VOID ITDBGSetConstants( INST * pinst )
	{
	CHAR		*sz;					

	if ( ( sz = GetDebugEnvValue( "Error Trap" ) ) != NULL )
		{
		extern ERR g_errTrap;
		g_errTrap = atol( sz );
		OSMemoryHeapFree( sz );
		}
	
	if ( ( sz = GetDebugEnvValue( "Redo Trap" ) ) != NULL )
		{
		ULONG lGeneration;
		ULONG isec;
		ULONG ib;
		sscanf(	sz,
				"%06X,%04X,%04X",
				&lGeneration,
				&isec,
				&ib );
				
		extern LGPOS g_lgposRedoTrap;
		g_lgposRedoTrap.lGeneration	= lGeneration;
		g_lgposRedoTrap.isec		= USHORT( isec );
		g_lgposRedoTrap.ib			= USHORT( ib );
		
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "PrintToStdOut" ) ) ) != NULL )
		{
		fDBGPrintToStdOut = fTrue;
		OSMemoryHeapFree( sz );
		}
	else
		{
		fDBGPrintToStdOut = fFalse;
		}

#ifdef PROFILE_JET_API
	if ( ( sz = GetDebugEnvValue ( _T( "JETProfileName" ) ) ) != NULL )
		{
		extern CHAR profile_szFileName[];
		strncpy( profile_szFileName, sz, IFileSystemAPI::cchPathMax );
		profile_szFileName[IFileSystemAPI::cchPathMax-1] = 0;
		OSMemoryHeapFree( sz );
		}
	if ( ( sz = GetDebugEnvValue ( _T( "JETProfileOptions" ) ) ) != NULL )
		{
		extern INT profile_detailLevel;
		profile_detailLevel = atoi( sz );
		OSMemoryHeapFree( sz );
		}
#endif // PROFILE_JET_API

	//	use system environment variables to overwrite the default.
	//	if the JETUSEENV is set.
	//
	if ( ( sz = GetDebugEnvValue ( _T( "JETUseEnv" ) ) ) == NULL )
		return;
	
	OSMemoryHeapFree( sz );

	// UNDONE: check what param can be set per instance and move the code before

	// only the previous params can be set with a runnign instance
	if ( NULL != pinst )
		{
		return;
		}
		
	if ( ( sz = GetDebugEnvValue ( _T( "JETEnableOnlineDefrag" ) ) ) != NULL )
		{
		g_fGlobalOLDLevel = atol( sz );
		OSMemoryHeapFree( sz );
		}
		 
	if ( ( sz = GetDebugEnvValue ( _T( "JETRecovery" ) ) ) != NULL )
		{
		if ( strlen( sz ) > sizeof(g_szRecovery) )
			{
			OSMemoryHeapFree( sz );
			return;
			}
		strcpy( g_szRecovery, sz );
		OSMemoryHeapFree( sz );
		}
		 
	if ( ( sz = GetDebugEnvValue ( _T( "JETLogFilePath" ) ) ) != NULL )
		{
		if ( strlen( sz ) > sizeof( g_szLogFilePath ) )
			{
			OSMemoryHeapFree( sz );
			return;
			}
		strcpy( g_szLogFilePath, sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETDbExtensionSize" ) ) ) != NULL )
		{
		g_cpgSESysMin = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETPageTempDBMin" ) ) ) != NULL )
		{
		g_cpageTempDBMin = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETMaxSessions" ) ) ) != NULL )
		{
		g_lSessionsMax = atol( sz );
		OSMemoryHeapFree( sz );
		}
	
	if ( ( sz = GetDebugEnvValue ( _T( "JETMaxOpenTables" ) ) ) != NULL )
		{
		g_lOpenTablesMax = atol( sz );
		OSMemoryHeapFree( sz );
		}
	
	if ( ( sz = GetDebugEnvValue ( _T( "JETMaxTemporaryTables" ) ) ) != NULL )
		{
		g_lTemporaryTablesMax = atol( sz );
		OSMemoryHeapFree( sz );
		}
	
	if ( ( sz = GetDebugEnvValue ( _T( "JETMaxCursors" ) ) ) != NULL )
		{
		g_lCursorsMax = atol( sz );
		OSMemoryHeapFree( sz );
		}
	
	if ( ( sz = GetDebugEnvValue ( _T( "JETMaxVerPages" ) ) ) != NULL )
		{
		g_lVerPagesMax = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETGlobalMinVerPages" ) ) ) != NULL )
		{
		g_lVerPagesMin = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETLogBuffers" ) ) ) != NULL )
		{
		g_lLogBuffers = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETLogFileSize" ) ) ) != NULL )
		{
		g_lLogFileSize = atol(sz);
		OSMemoryHeapFree(sz);
		}

	if ( ( sz = GetDebugEnvValue ( _T( "JETLogCircularLogging" ) ) ) != NULL )
		{
		g_fLGCircularLogging = atol( sz );
		OSMemoryHeapFree( sz );
		}

	if ( ( sz = GetDebugEnvValue ( _T( "PERFOUTPUT" ) ) ) != NULL )
		{
		fDBGPerfOutput = fTrue;
		OSMemoryHeapFree( sz );
		}
	else
		{
		fDBGPerfOutput = fFalse;
		}

	if ( ( sz = GetDebugEnvValue ( _T( "APICallLogLevel" ) ) ) != NULL )
		{
		lAPICallLogLevel = atol(sz);
		OSMemoryHeapFree(sz);
		}
	else
		lAPICallLogLevel = 4;
	}
#endif	//	DEBUG


ERR ErrITSetConstants( INST * pinst )
	{
#ifdef RTM
	//	Error/Redo Trap is disabled in RTM builds
#else

#ifdef DEBUG
	ITDBGSetConstants( pinst );
#else
	_TCHAR		szBuf[ DEBUG_ENV_VALUE_LEN ];
	
	if ( FOSConfigGet( szDEBUGRoot, _T( "Error Trap" ), szBuf, DEBUG_ENV_VALUE_LEN )
		&& 0 != szBuf[0] )
		{
		extern ERR g_errTrap;
		g_errTrap = atol( szBuf );
		}
#endif	//	DEBUG

	_TCHAR		szBuf2[ DEBUG_ENV_VALUE_LEN ];
	
	if ( FOSConfigGet( szDEBUGRoot, _T( "Redo Trap" ), szBuf2, DEBUG_ENV_VALUE_LEN )
		&& 0 != szBuf2[0] )
		{
		ULONG lGeneration;
		ULONG isec;
		ULONG ib;
		sscanf(	szBuf2,
				"%06X,%04X,%04X",
				&lGeneration,
				&isec,
				&ib );
				
		extern LGPOS g_lgposRedoTrap;
		g_lgposRedoTrap.lGeneration	= lGeneration;
		g_lgposRedoTrap.isec		= USHORT( isec );
		g_lgposRedoTrap.ib			= USHORT( ib );
		}
		
	//	initialize rgres.  system path in rgtib[itib].g_szSystemPath,
	//	is initialized by JET layer.
	//
//	rgres[iresFCB].cblockAlloc = g_lOpenTablesMax;
//	rgres[iresFUCB].cblockAlloc = g_lCursorsMax;
//	rgres[iresTDB].cblockAlloc = g_lOpenTablesMax + g_lTemporaryTablesMax;
//	rgres[iresIDB].cblockAlloc = g_lOpenTablesMax + g_lTemporaryTablesMax;

	//	each user session can begin another one for BMCleanBeforeSplit
	//
//	rgres[iresPIB].cblockAlloc = g_lSessionsMax + cpibSystem; 
//	rgres[iresSCB].cblockAlloc = g_lTemporaryTablesMax;
//	if ( g_plog->m_fRecovering )
//		{
//		rgres[iresVER].cblockAlloc =
//			(LONG) max( (ULONG) g_lVerPagesMax * 1.1, (ULONG) g_lVerPagesMax + 2 ) + cbucketSystem;
//		}
//	else
//		{
//		rgres[iresVER].cblockAlloc = g_lVerPagesMax + cbucketSystem;
//		}

#endif	//	RTM

	return JET_errSuccess;
	}


//+API
//	ErrINSTInit
//	========================================================
//	ERR ErrINSTInit( )
//
//	Initialize the storage system: page buffers, log buffers, and the
//	database files.
//
//	RETURNS		JET_errSuccess
//-
ERR INST::ErrINSTInit( )
	{
	ERR		err;
	PIB		*ppib			= ppibNil;
	INT		cSessions;
	IFMP	ifmp			= ifmpMax;
	BOOL	fFCBInitialized	= fFalse;

	LOG		*plog			= m_plog;
	VER		*pver			= m_pver;

	//	sleep while initialization is in progress
	//
	while ( m_fSTInit == fSTInitInProgress )
		{
		UtilSleep( 1000 );
		}

	//	serialize system initialization
	//
	if ( m_fSTInit == fSTInitDone )
		{
		return JET_errSuccess;
		}

	//	initialization in progress
	//
	m_fSTInit = fSTInitInProgress;

	//	initialize Global variables
	//
	Assert( m_ppibGlobal == ppibNil );

	// Set to FALSE (may have gotten set to TRUE by recovery).
	m_fTermAbruptly = fFalse;

	//	initialize subcomponents
	//
	Call( ErrIOInit( this ) );

	cSessions = m_lSessionsMax + cpibSystem;
	CallJ( ErrPIBInit( this, cSessions ), TermIO )

	// initialize FCB, TDB, and IDB.
	// each table can potentially use 2 FCBs (one for the table and one for the LV tree)
	// so we double the number of FCBs

	CallJ( FCB::ErrFCBInit( this, cSessions, m_lOpenTablesMax * 2, m_lTemporaryTablesMax, m_lOpenTablesPreferredMax * 2 ), TermPIB );
	fFCBInitialized = fTrue;

	// initialize SCB

	CallJ( ErrSCBInit( this, m_lTemporaryTablesMax ), TermPIB );

	CallJ( ErrFUCBInit( this, m_lCursorsMax ), TermSCB );

	//	begin storage level session to support all future system
	//	initialization activites that require a user for
	//	transaction control
	//
	if ( m_lTemporaryTablesMax > 0 && !FRecovering() )
		{
		//	temp db not needed during recovery
		
		CallJ( ErrPIBBeginSession( this, &ppib, procidNil, fTrue ), TermFUCB );

		//  open and set size of temp database
		//
		Assert( m_mpdbidifmp[ dbidTemp ] >= ifmpMax );


#ifdef TEMP_SLV
//	UNDONE: If a client needs a streaming file on the temp database,
//	add a system param to indicate that we should pass in JET_bitDbCreateStreamingFile
//	or possibly even the streaming file naae and root
		const ULONG		grbit	= JET_bitDbCreateStreamingFile
									| JET_bitDbRecoveryOff
									| ( m_fTempTableVersioning ? 0 : JET_bitDbVersioningOff );
#else
		const ULONG		grbit	= JET_bitDbRecoveryOff
									| ( m_fTempTableVersioning ? 0 : JET_bitDbVersioningOff );
#endif									

		CallJ( ErrDBCreateDatabase(
					ppib,
					NULL,
					m_szTempDatabase,
					NULL,
					NULL,
					0,
					&ifmp,
					dbidTemp,
					m_cpageTempDBMin,
					grbit,
					NULL ), ClosePIB );
			
		Assert( rgfmp[ ifmp ].Dbid() == dbidTemp );

		PIBEndSession( ppib );
		}
		
	//	begin backup session 
	//
	Assert( m_plog->m_ppibBackup == ppibNil );
	if ( !m_plog->m_fRecovering )
		{
		CallJ( ErrPIBBeginSession( this, &m_plog->m_ppibBackup, procidNil, fFalse ), ClosePIB );
		}

	//	Should be success all the time because it is just another
	//	reference to the global task manager.  Nevertheless,
	//	handle errors anyway just in case
	//
	err = m_taskmgr.ErrTMInit();
	CallS( err );
	CallJ( err, CloseBackupPIB );

	//	intialize version store
	
	if ( plog->m_fRecovering )
		{
		CallJ( pver->ErrVERInit( (INT) max( m_lVerPagesMax * 1.1, m_lVerPagesMax + 2 ) + cbucketSystem,
							m_lVerPagesMax + cbucketSystem,
							cSessions ), TermTM );
		}
	else
		{
		CallJ( pver->ErrVERInit( m_lVerPagesMax + cbucketSystem,
							m_lVerPagesPreferredMax,
							cSessions ), TermTM );
		}

	// initialize LV critical section

	CallJ( ErrLVInit( this ), TermVER );

	this->m_fSTInit = fSTInitDone;

/*	All services threads simply wait on a signal
	(there is no init involved), so shouldn't
	need this sleep any longer

	//	give theads a chance to initialize
	UtilSleep( 1000 );
*/
	return JET_errSuccess;

TermVER:
	m_pver->m_fSyncronousTasks = fTrue;	// BUGFIX(X5:124414): avoid an assert in VERTerm()
	m_pver->VERTerm( fFalse );	//	not normal

TermTM:
	m_taskmgr.TMTerm();

CloseBackupPIB:
	//	terminate backup session
	//
	if ( m_plog->m_ppibBackup != ppibNil )
		{
		PIBEndSession( m_plog->m_ppibBackup );
		m_plog->m_ppibBackup = ppibNil;
		}
		
ClosePIB:
	if ( ifmp != ifmpMax )
		{
		//	clean up buffers for this temp database
		
		Assert( rgfmp[ ifmp ].Dbid() == dbidTemp );
		BFPurge( ifmp );
		BFPurge( ifmp | ifmpSLV );
		}

	if ( ppib != ppibNil )
		PIBEndSession( ppib );

TermFUCB:
	FUCBTerm( this );

TermSCB:
	SCBTerm( this );

TermPIB:
	PIBTerm( this );

TermIO:
	(VOID)ErrIOTerm( this, m_pfsapi, fFalse );	//	not normal

	//	must defer FCB temination to the end because IOTerm() will clean up FCB's
	//	allocated for the temp. database
	if ( fFCBInitialized )
		FCB::Term( this );


HandleError:

	this->m_fSTInit = fSTInitNotDone;
	return err;
	}


//+api------------------------------------------------------
//
//	ErrINSTTerm
//	========================================================
//
//	ERR ErrITTerm( VOID )
//
//	Flush the page buffers to disk so that database file be in 
//	consistent state.  If error in RCCleanUp or in BFFlush, then DO NOT
//	terminate log, thereby forcing redo on next initialization.
//
//----------------------------------------------------------

ERR INST::ErrINSTTerm( TERMTYPE termtype )
	{
	ERR			err;
	ERR			errRet = JET_errSuccess;
	ULONG		icall = 0;

	Assert( m_plog->m_fRecovering || m_fTermInProgress || termtypeError == termtype );

	//	sleep while initialization is in progress
	//
	while ( m_fSTInit == fSTInitInProgress )
		{
		UtilSleep( 1000 );
		}

	//	make sure no other transactions in progress
	//
	//	if write error on page, RCCleanup will return -err
	//	if write error on buffer, BFFlush will return -err
	//	-err passed to LGTerm will cause correctness flag to
	//	be omitted from log thereby forcing recovery on next
	//	startup, and causing LGTerm to return +err, which
	//	may be used by JetQuit to show error
	//
	if ( m_fSTInit == fSTInitNotDone )
		return ErrERRCheck( JET_errNotInitialized );

// If no error (termtype != termError), then need to check if we can continue.
	
	if ( ( termtype == termtypeCleanUp || termtype == termtypeNoCleanUp ) && m_plog->m_fBackupInProgress )
		{
		return ErrERRCheck( JET_errBackupInProgress );
		}

	OLDTermInst( this );

	//	force the version store into synchronous-cleanup mode 
	//	(e.g. circumvent TASKMGR because it is about to go away)
	{
	ENTERCRITICALSECTION	enterCritRCEClean( &(m_pver->m_critRCEClean) );
	m_pver->m_fSyncronousTasks = fTrue;	
	}

	//  Cleanup all the tasks. Tasks will not be accepted by the TASKMGR
	//  until it is re-inited
	//  OLD has been stopped and the version store has been cleaned (as best as possible)
	//  so we shouldn't see any more tasks, unless we are about to fail below
	m_taskmgr.TMTerm();

	//	clean up all entries
	//
	err = m_pver->ErrVERRCEClean( );
	
	if ( termtype == termtypeCleanUp )
		{
		if ( err == JET_wrnRemainingVersions )
			{
			UtilSleep( 3000 );
			err = m_pver->ErrVERRCEClean();
			}
		if ( err < 0 )
			{
			termtype = termtypeError;
			if ( errRet >= 0 )
				{
				errRet = err;
				}
			}
		else
			{
			FCBAssertAllClean( this );
			}
		}
	else if ( JET_wrnRemainingVersions != err ) 
		{
		CallS( err );
		}

	//  fail if there are still active transactions and we are not doing a
	//  clean shutdown

	if ( ( termtype == termtypeCleanUp || termtype == termtypeNoCleanUp ) && TrxOldest( this ) != trxMax )
		{
		err = m_taskmgr.ErrTMInit();
		Assert( JET_errSuccess == err );
		return ErrERRCheck( JET_errTooManyActiveUsers );
		}
	
	//	Enter no-returning point. Once we kill one thread, we kill them all !!!!
	//
	m_fSTInit = fSTInitNotDone;

	m_fTermAbruptly = ( termtype == termtypeNoCleanUp || termtype == termtypeError );

	//	terminate backup session
	//
	if ( m_plog->m_ppibBackup != ppibNil )
		{
		PIBEndSession( m_plog->m_ppibBackup );
		}
	m_plog->m_ppibBackup = ppibNil;
	
	//  close LV critical section and remove session
	//  do not try to insert long values after calling this
	//
	LVTerm( this );

	//	flush and purge all buffers
	//
	if ( termtype == termtypeCleanUp || termtype == termtypeNoCleanUp )
		{
		for ( DBID dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
			{
			if ( m_mpdbidifmp[ dbid ] < ifmpMax )
				{
				err = ErrBFFlush( m_mpdbidifmp[ dbid ] );
				if ( err < 0 )
					{
					termtype = termtypeError;
					m_fTermAbruptly = fTrue;
					if ( errRet >= 0 )
						errRet = err;
					}
				err = ErrBFFlush( m_mpdbidifmp[ dbid ] | ifmpSLV );
				if ( err < 0 )
					{
					termtype = termtypeError;
					m_fTermAbruptly = fTrue;
					if ( errRet >= 0 )
						errRet = err;
					}
				}
			}
		}
	for ( DBID dbid = dbidMin; dbid < dbidMax; dbid++ )
		{
		if ( m_mpdbidifmp[ dbid ] < ifmpMax )
			{
			BFPurge( m_mpdbidifmp[ dbid ] );
			BFPurge( m_mpdbidifmp[ dbid ] | ifmpSLV );
			}
		}

	// terminate the version store only after buffer manager as there are links to undo info RCE's
	// that will point to freed memory if we end the version store before the BF
	m_pver->VERTerm( termtype == termtypeCleanUp || termtype == termtypeNoCleanUp );

	//	Before we term BF, we disable the checkpoint if error occurred

	if ( errRet < 0 || termtype == termtypeError )
		m_plog->m_fDisableCheckpoint = fTrue;

	//	reset initialization flag

	FCB::PurgeAllDatabases( this );

	PIBTerm( this );
	FUCBTerm( this );
	SCBTerm( this );

	//	clean up the fmp entries

	err = ErrIOTerm( this, m_pfsapi, termtype == termtypeCleanUp || termtype == termtypeNoCleanUp );
	if ( err < 0 )
		{
		termtype = termtypeError;
		m_fTermAbruptly = fTrue;
		if ( errRet >= 0 )
			errRet = err;
		}

	//  terminate FCB

	FCB::Term( this );

	//	delete temp file. Temp file should be cleaned up in IOTerm.

	//	the temp database is always on the OS file-system

	DBDeleteDbFiles( this, m_pfsapi, m_szTempDatabase );
	
	return errRet;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\tagfld.cxx ===
#include "std.hxx"

INLINE VOID DeleteEntryAndData(
	BYTE		* const pbEntry,
	const ULONG	cbEntry,
	BYTE		* const pbData,
	const ULONG	cbData,
	BYTE		* const pbMax )
	{
	const BYTE		* const pbNextEntry		= pbEntry + cbEntry;

	UtilMemMove(
		pbEntry,
		pbNextEntry,
		pbMax - pbNextEntry );

	if( 0 != cbData )
		{
		//	we have already shifted the data down by cbEntry so pbNextEntry has changes

		const BYTE * const		pbMaxNew		= pbMax - cbEntry;
		BYTE * const 			pbDataNew		= pbData - cbEntry;
		const BYTE		* const pbNextData		= pbData + cbData - cbEntry;

		UtilMemMove( 
			pbDataNew,
			pbNextData,
			pbMaxNew - pbNextData );		
		}
	}


LOCAL VOID MULTIVALUES::AddInstance(
	const DATA			* const pdataToSet,
	const JET_COLTYP	coltyp,
	const BOOL			fSeparatedLV )
	{
	const ULONG			imvAdd				= CMultiValues();
	const ULONG			cMultiValuesCurr	= CMultiValues();
	const ULONG			cbMultiValuesCurr	= CbMultiValues();
	BYTE				* const pbMaxCurr	= PbMax();
	
	Assert( cMultiValuesCurr >= 2 );

	//	shift to make room for new MVOFFSET
	UtilMemMove(
		PbStartOfMVData() + sizeof(MVOFFSET),
		PbStartOfMVData(),
		PbMax() - PbStartOfMVData() );

	//	fix up offsets
	for ( ULONG imv = 0; imv < cMultiValuesCurr; imv++ )
		{
		DeltaIb( imv, sizeof(MVOFFSET) );
		}

	MVOFFSET	* const pmvoffAdd	= Rgmvoffs() + imvAdd;
	*pmvoffAdd = USHORT( sizeof(MVOFFSET) + cbMultiValuesCurr );		//	implicitly clears high bit
	if ( fSeparatedLV )
		{
		Assert( Pheader()->FColumnCanBeSeparated() );
		Assert( !fSeparatedLV || sizeof(LID) == pdataToSet->Cb() );
		SetFSeparatedInstance( imvAdd );
		UtilMemCpy(
			pbMaxCurr + sizeof(MVOFFSET),
			pdataToSet->Pv(),
			sizeof(LID) );
		}
	else
		{
		RECCopyData(
			pbMaxCurr + sizeof(MVOFFSET),
			pdataToSet,
			coltyp );
		}
		
	m_cMultiValues++;
	m_cbMultiValues += sizeof(MVOFFSET) + pdataToSet->Cb();
	}

LOCAL VOID MULTIVALUES::RemoveInstance( const ULONG itagSequence )
	{
	Assert( itagSequence > 0 );
	Assert( itagSequence <= CMultiValues() );
	Assert( CMultiValues() > 2 );

	const ULONG			imvDelete			= itagSequence - 1;
	const ULONG			cbDataDelete		= CbData( imvDelete );

	Assert( imvDelete < CMultiValues() );

	DeleteEntryAndData( 
		(BYTE *)( Rgmvoffs() + imvDelete ),
		sizeof(MVOFFSET),
		cbDataDelete > 0 ? PbData( imvDelete ) : NULL,
		cbDataDelete,
		PbMax() );


	//	update MULTIVALUES members to reflect deleted instance
	m_cbMultiValues -= sizeof(MVOFFSET) + cbDataDelete;
	m_cMultiValues--;

	//	update offsets
	ULONG imv;
	for ( imv = 0;
		imv < imvDelete;
		imv++ )
		{
		const INT	cbMVOffset	= sizeof(MVOFFSET);
		DeltaIb( imv, -cbMVOffset );
		}
	Assert( imvDelete == imv );
	for ( ;
		imv < CMultiValues();
		imv++ )
		{
		const SHORT	cbMVOffsetAndData		= SHORT( sizeof(MVOFFSET) + cbDataDelete );
		DeltaIb( imv, SHORT( -cbMVOffsetAndData ) );
		}
	}


LOCAL VOID MULTIVALUES::UpdateInstance(
	const ULONG			itagSequence,
	const DATA			* const pdataToSet,
	const JET_COLTYP	coltyp,
	const BOOL			fSeparatedLV )
	{
	const ULONG			imvReplace				= itagSequence - 1;
	BYTE 				* const pbDataReplace	= PbData( imvReplace );
	const ULONG			cbDataReplace			= CbData( imvReplace );
	const INT			delta					= pdataToSet->Cb() - cbDataReplace;
	BYTE				* const pbDataNext		= pbDataReplace + cbDataReplace;

	Assert( itagSequence > 0 );
	Assert( itagSequence <= CMultiValues() );

	if ( 0 != delta )
		{
		//	shift data to accommodate updated instance
		UtilMemMove(
			pbDataReplace + pdataToSet->Cb(),
			pbDataNext,
			PbMax() - pbDataNext );

		//	update offsets to reflect shifted data
		for ( ULONG imv = imvReplace + 1; imv < CMultiValues(); imv++ )
			{
			DeltaIb( imv, (SHORT)delta );
			}
		}

	m_cbMultiValues += delta;
	
	if ( fSeparatedLV )
		{
		Assert( Pheader()->FColumnCanBeSeparated() );
		Assert( sizeof(LID) == pdataToSet->Cb() );
		SetFSeparatedInstance( imvReplace );
		UtilMemCpy(
			pbDataReplace,
			pdataToSet->Pv(),
			sizeof(LID) );
		}
	else
		{
		ResetFSeparatedInstance( imvReplace );
		RECCopyData(
			pbDataReplace,
			pdataToSet,
			coltyp );
		}
	}


LOCAL VOID TWOVALUES::UpdateInstance(
	const ULONG			itagSequence,
	const DATA			* const pdataToSet,
	const JET_COLTYP	coltyp )
	{
	Assert( !Pheader()->FColumnCanBeSeparated() );
	Assert( !Pheader()->FSeparated() );
	Assert( 1 == itagSequence || 2 == itagSequence );
	if ( 1 == itagSequence )
		{
		//	shift second value accordingly
		UtilMemMove(
			PbData() + pdataToSet->Cb(),
			PbData() + CbFirstValue(),
			CbSecondValue() );

		//	copy in updated value
		RECCopyData(
			PbData(),
			pdataToSet,
			coltyp );

		//	update length
		*m_pcbFirstValue = TVLENGTH( pdataToSet->Cb() );
		}
	else
		{
		RECCopyData(
			PbData() + CbFirstValue(),
			pdataToSet,
			coltyp );
		m_cbSecondValue = TVLENGTH( pdataToSet->Cb() );
		}
	}


LOCAL VOID TAGFIELDS::ConvertTwoValuesToMultiValues(
	TWOVALUES			* const ptv,
	const DATA			* const pdataToSet,
	const JET_COLTYP	coltyp )
	{
	TAGFLD_HEADER		* const pheader		= ptv->Pheader();
	const ULONG			cbFirstValue		= ptv->CbFirstValue();
	const ULONG			cbSecondValue		= ptv->CbSecondValue();
	BYTE				* const pbData		= ptv->PbData();

	Assert( !FRECLongValue( coltyp ) );
	Assert( !FRECSLV( coltyp ) );
	Assert( !pheader->FSeparated() );
	Assert( !pheader->FColumnCanBeSeparated() );

	//	must be adding a 3rd instance, so make room for the
	//	appropriate offsets array
	UtilMemMove(
		pbData + ( 3 * sizeof(MULTIVALUES::MVOFFSET) ) - sizeof(TWOVALUES::TVLENGTH),
		pbData,
		cbFirstValue + cbSecondValue );

	MULTIVALUES::MVOFFSET	* const rgmvoffs	= (MULTIVALUES::MVOFFSET *)ptv->PcbFirstValue();

	rgmvoffs[0] = 3 * sizeof(MULTIVALUES::MVOFFSET);
	rgmvoffs[1] = USHORT( rgmvoffs[0] + cbFirstValue );
	rgmvoffs[2] = USHORT( rgmvoffs[1] + cbSecondValue );

	//	copy in new instance
	RECCopyData(
		(BYTE *)rgmvoffs + rgmvoffs[2],
		pdataToSet,
		coltyp );

	Assert( pheader->FMultiValues() );
	pheader->ResetFTwoValues();
	}


LOCAL VOID TAGFIELDS::InsertTagfld(
	const ULONG			itagfldInsert,
	TAGFLD				* const ptagfldInsert,
	const DATA			* const pdataToInsert,
	const JET_COLTYP	coltyp,
	const BOOL			fSeparatedLV )
	{
	//	if pdataToInsert is NULL, we must be setting a column explicitly
	//	to NULL to override a default value
	Assert( ( ptagfldInsert->FNull() && NULL == pdataToInsert )
		|| ( !ptagfldInsert->FNull() && NULL != pdataToInsert ) );
	ULONG				cbDataToInsert;
	BYTE				* pbMoveFrom;

	if ( NULL == pdataToInsert )
		{
		cbDataToInsert = 0;
		}
	else
		{
		cbDataToInsert = pdataToInsert->Cb();
		if ( FRECLongValue( coltyp ) || FRECSLV( coltyp ) )
			{
			//	add header byte
			cbDataToInsert++;
			}
		}

	Assert( itagfldInsert <= CTaggedColumns() );

	if ( itagfldInsert < CTaggedColumns() )
		{
		const ULONG		ibMoveDataFrom	= Ptagfld( itagfldInsert )->Ib();

		//	make room for the new tagged data
		pbMoveFrom = PbData( itagfldInsert );
		UtilMemMove(
			pbMoveFrom + sizeof(TAGFLD) + cbDataToInsert,
			pbMoveFrom,
			PbMax() - pbMoveFrom );

		//	make room for the new TAGFLD entry
		pbMoveFrom = (BYTE *)Ptagfld( itagfldInsert );
		UtilMemMove(
			pbMoveFrom + sizeof(TAGFLD),
			pbMoveFrom,
			ibMoveDataFrom - ( itagfldInsert * sizeof(TAGFLD) ) );

		//	insert at the point we vacated, after making room for the new TAGFLD entry
		ptagfldInsert->SetIb( USHORT( ibMoveDataFrom + sizeof(TAGFLD) ) );
		}
	else if ( CTaggedColumns() > 0 )
		{
		//	append to the end, after making room the new TAGFLD entry
		pbMoveFrom = PbStartOfTaggedData();
		UtilMemMove(
			pbMoveFrom + sizeof(TAGFLD),
			pbMoveFrom,
			CbTaggedData() );
		ptagfldInsert->SetIb( USHORT( CbTaggedColumns() + sizeof(TAGFLD) ) );
		}
	else
		{
		ptagfldInsert->SetIb( sizeof(TAGFLD) );
		}

	//	update TAGFIELD members to reflect new TAGFLD
	m_cbTaggedColumns += sizeof(TAGFLD) + cbDataToInsert;
	m_cTaggedColumns++;

	//	insert new data
	if ( cbDataToInsert > 0 )
		{
		BYTE	* pbInsert		= PbTaggedColumns() + ptagfldInsert->Ib();
			
		Assert( NULL != pdataToInsert );
		Assert( !ptagfldInsert->FNull() );
		if ( ptagfldInsert->FExtendedInfo() )
			{
			//	reserve byte for extended info
			new( (TAGFLD_HEADER *)pbInsert ) TAGFLD_HEADER( coltyp, fSeparatedLV );
			pbInsert++;
			}

		RECCopyData(
			pbInsert,
			pdataToInsert,
			coltyp );
		}

	//	insert new TAGFLD
	UtilMemCpy(
		Ptagfld( itagfldInsert ),
		ptagfldInsert,
		sizeof(TAGFLD) );
	Assert( Ptagfld( itagfldInsert )->Ib() >= ( sizeof(TAGFLD) * CTaggedColumns() ) );
	Assert( Ptagfld( itagfldInsert )->Ib() <= CbTaggedColumns() );

	//	just need to update tag array
	ULONG	itagfld;
	for ( itagfld = 0;
		itagfld < itagfldInsert;
		itagfld++ )
		{
		Ptagfld( itagfld )->DeltaIb( sizeof(TAGFLD) );
		}
	for ( itagfld++;		//	skip inserted TAGFLD
		itagfld < CTaggedColumns();
		itagfld++ )
		{
		Ptagfld( itagfld )->DeltaIb( SHORT( sizeof(TAGFLD) + cbDataToInsert ) );
		}
	}

LOCAL VOID TAGFIELDS::ResizeTagfld(
	const ULONG			itagfldResize,
	const INT			delta )
	{
	Assert( itagfldResize < CTaggedColumns() );
	Assert( 0 != delta );

	//	shift data after this column as needed
	//	to collapse space or make room
	if ( itagfldResize < CTaggedColumns() - 1 )
		{
		BYTE	* const pbMoveFrom	= PbData( itagfldResize+1 );
		UtilMemMove(
			pbMoveFrom + delta,
			pbMoveFrom,
			CbTaggedColumns() - Ptagfld( itagfldResize+1 )->Ib() );
		}

	//	update TAGFIELDS members to reflect new data
	m_cbTaggedColumns += delta;

	//	update offsets
	for ( ULONG itagfld = itagfldResize+1;
		itagfld < CTaggedColumns();
		itagfld++ )
		{
		Ptagfld( itagfld )->DeltaIb( (SHORT)delta );
		}
	}

LOCAL VOID TAGFIELDS::ReplaceTagfldData(
	const ULONG			itagfldReplace,
	const DATA			* const pdataNew,
	const JET_COLTYP	coltyp,
	const BOOL			fSeparatedLV )
	{
	TAGFLD				* const ptagfld		= Ptagfld( itagfldReplace );
	TAGFLD_HEADER		* pheader			= Pheader( itagfldReplace );
	BYTE				* pbData			= PbData( itagfldReplace );

	ptagfld->ResetFNull();

	if ( NULL != pheader )
		{
		Assert( CbData( itagfldReplace ) == pdataNew->Cb() + sizeof(TAGFLD_HEADER) );
		pbData += sizeof(TAGFLD_HEADER);
		if ( fSeparatedLV )
			{
			//	force fSeparated flag to be set
			Assert( pheader->FColumnCanBeSeparated() );
			pheader->SetFSeparated();
			}
		else
			{
			//	reset fSeparated flag
			pheader->ResetFSeparated();
			}
		}
	else
		{
		const BOOL	fNeedHeader		= ( FRECLongValue( coltyp ) || FRECSLV( coltyp ) );

		if ( fNeedHeader )
			{
			Assert( CbData( itagfldReplace ) == pdataNew->Cb() + sizeof(TAGFLD_HEADER) );
			pheader = (TAGFLD_HEADER *)pbData;
			new( pheader ) TAGFLD_HEADER( coltyp, fSeparatedLV );

			ptagfld->SetFExtendedInfo();
			pbData += sizeof(TAGFLD_HEADER);
			}
		else
			{
			Assert( CbData( itagfldReplace ) == pdataNew->Cb() );
#ifdef UNLIMITED_MULTIVALUES
			UNDONE: convert from intrinsic to separated
#else
			Assert( !FRECLongValue( coltyp ) );
			Assert( !FRECSLV( coltyp ) );
			Assert( !fSeparatedLV );
#endif
			}
		}

	RECCopyData( pbData, pdataNew, coltyp );
	}

LOCAL VOID TAGFIELDS::DeleteTagfld(
	const ULONG			itagfldDelete )
	{
	const ULONG			cbDataDelete		= CbData( itagfldDelete );

	Assert( itagfldDelete < CTaggedColumns() );

	DeleteEntryAndData(
		(BYTE *)Ptagfld( itagfldDelete ),
		sizeof(TAGFLD),
		cbDataDelete > 0 ? PbData( itagfldDelete ) : NULL,
		cbDataDelete,
		PbMax() );

	//	update TAGFLD members to reflect deleted TAGFLD
	m_cbTaggedColumns -= sizeof(TAGFLD) + cbDataDelete;
	m_cTaggedColumns--;

	//	update offsets
	ULONG itagfld;
	for ( itagfld = 0;
		itagfld < itagfldDelete;
		itagfld++ )
		{
		const INT	cbTagfld	= sizeof(TAGFLD);
		Ptagfld( itagfld )->DeltaIb( -cbTagfld );
		}
	Assert( itagfldDelete == itagfld );
	for ( ;
		itagfld < CTaggedColumns();
		itagfld++ )
		{
		const SHORT	cbTagfldAndData		= SHORT( sizeof(TAGFLD) + cbDataDelete );
		Ptagfld( itagfld )->DeltaIb( SHORT( -cbTagfldAndData ) );
		}
	}


LOCAL VOID TAGFIELDS::ConvertToTwoValues(
	const ULONG			itagfld,
	const DATA			* const pdataToSet,
	const JET_COLTYP	coltyp )
	{
	Assert( itagfld < CTaggedColumns() );
	BYTE			* const pbData		= PbData( itagfld );
	const ULONG		cbDataCurr			= CbData( itagfld );
	Assert( cbDataCurr <= JET_cbColumnMost );		//	otherwise, it would have been LongText/Binary

	Assert( NULL != pdataToSet );
	ResizeTagfld(
		itagfld,
		sizeof(TAGFLD_HEADER) + sizeof(TWOVALUES::TVLENGTH) + pdataToSet->Cb() );

	//	make room for the header and cbFirstValue
	UtilMemMove(
		pbData + sizeof(TAGFLD_HEADER) + sizeof(TWOVALUES::TVLENGTH),
		pbData,
		cbDataCurr );

	TAGFLD_HEADER		* const pheader			= (TAGFLD_HEADER *)pbData;
	TWOVALUES::TVLENGTH	* const pcbFirstValue	= (TWOVALUES::TVLENGTH *)( pbData + sizeof(TAGFLD_HEADER) );

	Assert( !FRECLongValue( coltyp ) );
	Assert( !FRECSLV( coltyp ) );
	new( pheader ) TAGFLD_HEADER( coltyp, fFalse );
	pheader->SetFMultiValues();
	pheader->SetFTwoValues();

	*pcbFirstValue = (TWOVALUES::TVLENGTH)cbDataCurr;

	RECCopyData( 
		(BYTE *)pcbFirstValue + sizeof(TWOVALUES::TVLENGTH) + cbDataCurr,
		pdataToSet,
		coltyp );

	Assert( CbData( itagfld ) ==
				sizeof(TAGFLD_HEADER)
				+ sizeof(TWOVALUES::TVLENGTH)
				+ cbDataCurr
				+ pdataToSet->Cb() );
	}

LOCAL VOID TAGFIELDS::ConvertToMultiValues(
	const ULONG			itagfld,
	const DATA			* const pdataToSet,
	const BOOL			fDataToSetIsSeparated )
	{
	Assert( itagfld < CTaggedColumns() );
	BYTE			* const pbData			= PbData( itagfld );
	const ULONG		cbDataCurr				= CbData( itagfld );
	TAGFLD_HEADER	* const pheader			= (TAGFLD_HEADER *)pbData;

	Assert( cbDataCurr >= sizeof(TAGFLD_HEADER) );
	const ULONG		cbDataCurrWithoutHeader	= cbDataCurr - sizeof(TAGFLD_HEADER);

	//	must already have a header (ie. either LongValue or SLV),
	//	just upgrade it to MultiValues
	Assert( NULL != Pheader( itagfld ) );
	Assert( pheader == Pheader( itagfld ) );
	Assert( pheader->FColumnCanBeSeparated() );
	const BOOL		fDataCurrIsSeparated	= pheader->FSeparated();
	pheader->ResetFSeparated();
	Assert( !pheader->FMultiValues() );
	Assert( !pheader->FTwoValues() );
	pheader->SetFMultiValues();

	Assert( NULL != pdataToSet );
	ResizeTagfld(
		itagfld,
		( 2 * sizeof(MULTIVALUES::MVOFFSET) ) + pdataToSet->Cb() );

	//	make room for the offset info
	UtilMemMove(
		pbData + sizeof(TAGFLD_HEADER) + ( 2 * sizeof(MULTIVALUES::MVOFFSET) ),
		pbData + sizeof(TAGFLD_HEADER),
		cbDataCurrWithoutHeader );

	MULTIVALUES::MVOFFSET	* const rgmvoffs	= (MULTIVALUES::MVOFFSET *)( pbData + sizeof(TAGFLD_HEADER) );

	rgmvoffs[0] = ( 2 * sizeof(MULTIVALUES::MVOFFSET) );
	rgmvoffs[1] = USHORT( ( 2 * sizeof(MULTIVALUES::MVOFFSET) )
						+ cbDataCurrWithoutHeader );

	//	must be long values, so no need to do endian conversion
//	RECCopyData( (BYTE *)rgmvoffs + rgmvoffs[1], pdataToSet, coltyp );
	Assert( !( rgmvoffs[0] & MULTIVALUES::maskFlags ) );
	Assert( !( rgmvoffs[1] & MULTIVALUES::maskFlags ) );
	UtilMemCpy(
		(BYTE *)rgmvoffs + rgmvoffs[1],
		pdataToSet->Pv(),
		pdataToSet->Cb() );

	if ( fDataCurrIsSeparated )
		{
		Assert( sizeof(LID) == cbDataCurrWithoutHeader );
		rgmvoffs[0] = USHORT( rgmvoffs[0] | MULTIVALUES::fSeparatedInstance );
		}
	if ( fDataToSetIsSeparated )
		{
		Assert( sizeof(LID) == pdataToSet->Cb() );
		rgmvoffs[1] = USHORT( rgmvoffs[1] | MULTIVALUES::fSeparatedInstance );
		}

	Assert( CbData( itagfld ) ==
				sizeof(TAGFLD_HEADER)
				+ ( 2 * sizeof(MULTIVALUES::MVOFFSET) )
				+ cbDataCurrWithoutHeader
				+ pdataToSet->Cb() );
	}

LOCAL ULONG TAGFIELDS::CbConvertTwoValuesToSingleValue(
	const ULONG			itagfld,
	const ULONG			itagSequence )
	{
	ULONG				cbShrink		= 0;

#ifdef DEBUG
	Assert( itagfld < CTaggedColumns() );
	const TAGFLD_HEADER	* const pheader	= Pheader( itagfld );
	Assert( NULL != pheader );
	Assert( pheader->FMultiValues() );
	Assert( pheader->FTwoValues() );
	Assert( !pheader->FLongValue() );
	Assert( !pheader->FSLV() );
#ifdef UNLIMITED_MULTIVALUES
#else
	Assert( !pheader->FSeparated() );
#endif	
#endif	

	if ( 1 == itagSequence || 2 == itagSequence )
		{
		BYTE			* const pbDataCurr		= PbData( itagfld );
		const ULONG		cbDataCurr				= CbData( itagfld );
		const BYTE		* pbDataRemaining;
		ULONG			cbDataRemaining;
		TWOVALUES		tv( pbDataCurr, cbDataCurr );

		if ( 1 == itagSequence )
			{
			//	remove first value, retain second value
	 		pbDataRemaining = tv.PbData() + tv.CbFirstValue();
			cbDataRemaining = tv.CbSecondValue();
			}
		else
			{
			//	remove second value, retain first value
			pbDataRemaining = tv.PbData();
			cbDataRemaining = tv.CbFirstValue();
			}

		Assert( cbDataCurr > cbDataRemaining );
		cbShrink	= cbDataCurr - cbDataRemaining;

		//	move remaining data to beginning of this TAGFLD<
		//	since we have no more need for the TAGFLD_HEADER
		UtilMemMove(
			pbDataCurr,
			pbDataRemaining,
			cbDataRemaining );

		//	shift rest of columns, if necessary
		if ( itagfld < CTaggedColumns() - 1 )
			{
			const BYTE	* const pbDataNextColumn	= PbData( itagfld+1 );
			UtilMemMove(
				pbDataCurr + cbDataRemaining,
				pbDataNextColumn,
				PbMax() - pbDataNextColumn );
			}

		//	clear flags
		Assert( Ptagfld( itagfld )->FExtendedInfo() );
		Ptagfld( itagfld )->ResetFExtendedInfo();

		//	update offsets
		for ( ULONG itagfldT = itagfld+1;
			itagfldT < CTaggedColumns();
			itagfldT++ )
			{
			const SHORT		cbT		= (SHORT)cbShrink;
			Ptagfld( itagfldT )->DeltaIb( SHORT( -cbT ) );
			}
		}
	else
		{
		//	appending NULL, which is a NOP
		}

	//	update size
	m_cbTaggedColumns -= cbShrink;

	return cbShrink;
	}

LOCAL ULONG TAGFIELDS::CbDeleteMultiValue(
	const ULONG		itagfld,
	const ULONG		itagSequence )
	{
	ULONG			cbShrink	= 0;
	MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );

#ifdef DEBUG
	Assert( itagfld < CTaggedColumns() );
	const TAGFLD_HEADER	* const pheader	= Pheader( itagfld );
	Assert( NULL != pheader );
	Assert( pheader->FMultiValues() );
	Assert( !pheader->FTwoValues() );
#endif

	if ( itagSequence > 0 && itagSequence <= mv.CMultiValues() )
		{
		TAGFLD_HEADER	* const pheader		= mv.Pheader();
		ULONG			cbDataRemaining;

		Assert( NULL != pheader );
		Assert( sizeof(TAGFLD_HEADER) + mv.CbMultiValues() == CbData( itagfld ) );

		if ( mv.CMultiValues() > 2 )
			{
			mv.RemoveInstance( itagSequence );
			cbDataRemaining = sizeof(TAGFLD_HEADER) + mv.CbMultiValues();
			}
		else
			{
			//	only one value will be left, so convert to non-multivalue
			Assert( 1 == itagSequence || 2 == itagSequence );
			const ULONG		imvRemaining		= itagSequence % 2;		//	calculate remaining itagSequence and convert to imv
			const BOOL		fSeparatedInstance	= mv.FSeparatedInstance( imvRemaining );
			BYTE			* const pbMoveFrom	= mv.PbData( imvRemaining );
			const ULONG		cbMove				= mv.CbData( imvRemaining );
			BYTE			* pbMoveTo;

			cbDataRemaining = cbMove;

#ifdef UNLIMITED_MULTIVALUES
#else
			Assert( !pheader->FSeparated() );
#endif				
			Assert( !pheader->FTwoValues() );
			pheader->ResetFMultiValues();
			if ( pheader->FNeedExtendedInfo() )
				{
#ifdef UNLIMITED_MULTIVALUES
				UNDONE: how to deal with one remaining
				multi-value (may or may not be separated)
				when the MULTIVALUES structure itself has
				been separated?
#else
				//	if remaining instance is separated,
				//	must now flag it as such in the
				//	extended info
				Assert( pheader->FColumnCanBeSeparated() );
				if ( fSeparatedInstance )
					pheader->SetFSeparated();
				else
					{
					Assert( !pheader->FSeparated() );		//	should already be unset, but better safe than sorry
					pheader->ResetFSeparated();
					}
#endif

				pbMoveTo = (BYTE *)pheader + sizeof(TAGFLD_HEADER);
				cbDataRemaining++;
				}
			else
				{
				//	if no other flags set, then can get rid of header byte
				Assert( !fSeparatedInstance );
				Ptagfld( itagfld )->ResetFExtendedInfo();
				pbMoveTo = (BYTE *)pheader;
				}
			Assert( pbMoveFrom > pbMoveTo );

			UtilMemMove(
				pbMoveTo,
				pbMoveFrom,
				cbMove );
			}

		Assert( cbDataRemaining < CbData( itagfld ) );
		cbShrink = CbData( itagfld ) - cbDataRemaining;

		//	shift rest of columns, if necessary
		if ( itagfld < CTaggedColumns() - 1 )
			{
			const BYTE	* const pbDataNextColumn	= PbData( itagfld+1 );
			UtilMemMove(
				(BYTE *)pheader + cbDataRemaining,
				pbDataNextColumn,
				PbMax() - pbDataNextColumn );
			}

		//	update size
		m_cbTaggedColumns -= cbShrink;

		//	update offsets
		for ( ULONG itagfldT = itagfld+1;
			itagfldT < CTaggedColumns();
			itagfldT++ )
			{
			const SHORT		cbT		= (SHORT)cbShrink;
			Ptagfld( itagfldT )->DeltaIb( SHORT( -cbT ) );
			}
		}

	else
		{
		//	appending NULL, which is a NOP
		}

	return cbShrink;
	}


LOCAL ERR TWOVALUES::ErrCheckUnique(
	const FIELD * const			pfield,
	const DATA&					dataToSet,
	const ULONG					itagSequence,
	const IDXUNICODE * const	pidxunicode,
	const BOOL					fNormalizedDataToSetIsTruncated )
	{
	ERR							err;
	const BOOL					fNormalize		= ( pfieldNil != pfield );
	DATA						dataRec;

	Assert( !fNormalizedDataToSetIsTruncated || fNormalize );

#ifdef UNLIMITED_MULTIVALUES
#else
	Assert( !Pheader()->FSeparated() );
#endif	
	Assert( !Pheader()->FLongValue() );
	Assert( !Pheader()->FSLV() );

	if ( 1 != itagSequence )
		{
		dataRec.SetPv( PbData() );
		dataRec.SetCb( CbFirstValue() );
		CallR( ErrRECICheckUnique(
					pfield,
					dataToSet,
					dataRec,
					pidxunicode,
					fNormalizedDataToSetIsTruncated ) );
		}

	if ( 2 != itagSequence )
		{
		dataRec.SetPv( PbData() + CbFirstValue() );
		dataRec.SetCb( CbSecondValue() );
		CallR( ErrRECICheckUnique(
					pfield,
					dataToSet,
					dataRec,
					pidxunicode,
					fNormalizedDataToSetIsTruncated ) );
		}

	return JET_errSuccess;
	}

LOCAL ERR MULTIVALUES::ErrCheckUnique(
	const FIELD * const			pfield,
	const DATA&					dataToSet,
	const ULONG					itagSequence,
	const IDXUNICODE * const	pidxunicode,
	const BOOL					fNormalizedDataToSetIsTruncated )
	{
	ERR							err;
	const BOOL					fNormalize		= ( pfieldNil != pfield );
	DATA						dataRec;
	ULONG						imv;

	Assert( !fNormalizedDataToSetIsTruncated || fNormalize );

#ifdef UNLIMITED_MULTIVALUES
#else
	Assert( !Pheader()->FSeparated() );
#endif	
	Assert( !Pheader()->FLongValue() );
	Assert( !Pheader()->FSLV() );

	for ( imv = 0; imv < CMultiValues(); imv++ )
		{
		Assert( !FSeparatedInstance( imv ) );
		if ( itagSequence != imv+1 )
			{
			dataRec.SetPv( PbData( imv ) );
			dataRec.SetCb( CbData( imv ) );
			CallR( ErrRECICheckUnique(
						pfield,
						dataToSet,
						dataRec,
						pidxunicode,
						fNormalizedDataToSetIsTruncated ) );
			}
		}

	return JET_errSuccess;
	}

LOCAL ERR TAGFIELDS::ErrCheckUniqueMultiValues(
	const FIELD * const			pfield,
	const DATA&					dataToSet,
	const ULONG					itagfld,
	const ULONG					itagSequence,
	const IDXUNICODE * const	pidxunicode,
	const BOOL					fNormalizedDataToSetIsTruncated )
	{
	ERR							err			= JET_errSuccess;

	Assert( !fNormalizedDataToSetIsTruncated || pfieldNil != pfield );
	Assert( !Ptagfld( itagfld )->FNull() );

	const TAGFLD_HEADER * const	pheader		= Pheader( itagfld );
	if ( NULL != pheader
		&& pheader->FMultiValues() )
		{
		Assert( !pheader->FSeparated() );
		Assert( !pheader->FLongValue() );			//	long values are handled in ErrFLDSetOneColumn()
		Assert( !pheader->FSLV() );
		if ( pheader->FTwoValues() )
			{
			TWOVALUES	tv( PbData( itagfld ), CbData( itagfld ) );
			err = tv.ErrCheckUnique(
							pfield,
							dataToSet,
							itagSequence,
							pidxunicode,
							fNormalizedDataToSetIsTruncated );
			}
		else
			{
			MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
			err = mv.ErrCheckUnique(
							pfield,
							dataToSet,
							itagSequence,
							pidxunicode,
							fNormalizedDataToSetIsTruncated );
			}
		}
	else if ( 1 != itagSequence )
		{
		DATA	dataRec;
		dataRec.SetPv( PbData( itagfld ) );
		dataRec.SetCb( CbData( itagfld ) );
		err = ErrRECICheckUnique(
					pfield,
					dataToSet,
					dataRec,
					pidxunicode,
					fNormalizedDataToSetIsTruncated );
		}
	else
		{
		//	overwriting the only instance, so no need to check anything
		}

	return err;
	}


LOCAL ERR TAGFIELDS::ErrCheckUniqueNormalizedMultiValues(
	const FIELD * const			pfield,
	const DATA&					dataToSet,
	const ULONG					itagfld,
	const ULONG					itagSequence,
	const IDXUNICODE * const	pidxunicode )
	{
	ERR							err;
	DATA						dataToSetNorm;
	BYTE						rgbNorm[KEY::cbKeyMax];
	BOOL						fNormalizedDataToSetIsTruncated;

	Assert( pfieldNil != pfield );

	dataToSetNorm.SetPv( rgbNorm );
	CallR( ErrFLDNormalizeTaggedData(
				pfield,
				dataToSet,
				dataToSetNorm,
				pidxunicode,
				&fNormalizedDataToSetIsTruncated ) );

	CallR( ErrCheckUniqueMultiValues(
				pfield,
				dataToSetNorm,
				itagfld,
				itagSequence,
				pidxunicode,
				fNormalizedDataToSetIsTruncated ) );

	return JET_errSuccess;
	}


ERR TAGFIELDS::ErrSetColumn(
	FUCB			* const pfucb,
	const FIELD		* const pfield,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const DATA		* const pdataToSet,
	const JET_GRBIT	grbit )
	{
	const FID		fid							= FidOfColumnid( columnid );
	const ULONG		cbRec						= pfucb->dataWorkBuf.Cb();
	const BOOL		fUseDerivedBit				= ( grbit & grbitSetColumnUseDerivedBit );
	const BOOL		fEnforceUniqueMultiValues	= ( ( grbit & ( JET_bitSetUniqueMultiValues|JET_bitSetUniqueNormalizedMultiValues ) )
													&& NULL != pdataToSet
													&& !FRECLongValue( pfield->coltyp )	//	long value uniqueness is checked in ErrFLDSetOneColumn()
													&& !FRECSLV( pfield->coltyp ) );

	Assert( ptdbNil != pfucb->u.pfcb->Ptdb() );
	AssertValid( pfucb->u.pfcb->Ptdb() );

	TAGFLD			tagfldNew( fid, fUseDerivedBit );
	const ULONG		itagfld			= ItagfldFind( tagfldNew );

	Assert( itagfld <= CTaggedColumns() );
	Assert( itagfld == CTaggedColumns()
		|| !Ptagfld( itagfld )->FNull()
		|| 0 == CbData( itagfld ) );		// If null, length is 0.

	const BOOL		fExists			= ( itagfld < CTaggedColumns()
										&& Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) );
	Assert( fExists
		|| itagfld == CTaggedColumns()
		|| Ptagfld( itagfld )->FIsGreaterThan( fid, fUseDerivedBit ) );

	if ( !fExists )
		{
		//	Specified column not found, so insert or append new
		ULONG 	cbField;

		//	Adding NULL: In most cases, we do nothing.  However, there
		//	is one specialised case where we have to insert a null entry.
		//	This is the case where there are currently no instances of this
		//	column in the record, and where there is also a default value
		//	for this column.
		//
		if ( pdataToSet == NULL )
			{
			if ( FFIELDDefault( pfield->ffield )
				&& 1 == itagSequence
				&& !( grbit & JET_bitSetRevertToDefaultValue ) )
				{
				tagfldNew.SetFNull();
				cbField = 0;
				}
			else
				return JET_errSuccess;
			}
		else
			{
			cbField = pdataToSet->Cb();

			Assert( !tagfldNew.FExtendedInfo() );
			if ( FRECLongValue( pfield->coltyp ) || FRECSLV( pfield->coltyp ) )
				{
				cbField++;
				tagfldNew.SetFExtendedInfo();
				}
			else
				{
				Assert( cbField <= JET_cbColumnMost );
				Assert( !( grbit & grbitSetColumnSeparated ) );
				}
			}

		//	will column fit?
		//
		if ( cbRec + sizeof(TAGFLD) + cbField > cbRECRecordMost )
			return ErrERRCheck( JET_errRecordTooBig );

		InsertTagfld(
			itagfld,
			&tagfldNew,
			pdataToSet,
			pfield->coltyp,
			grbit & grbitSetColumnSeparated );

		pfucb->dataWorkBuf.DeltaCb( sizeof(TAGFLD) + cbField );
		}

	else if ( pdataToSet != NULL )				// Overwrite with a non-null value.
		{
#ifdef DEBUG		
		Assert( itagfld < CTaggedColumns() );
		Assert( Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) );
		if ( Ptagfld( itagfld )->FNull() )
			{
			Assert( FFIELDDefault( pfield->ffield ) );
			Assert( !Ptagfld( itagfld )->FExtendedInfo() );
			Assert( 0 == CbData( itagfld ) );
			}
#endif

		if ( fEnforceUniqueMultiValues && !Ptagfld( itagfld )->FNull() )
			{
			ERR		errT;
			Assert( !FRECLongValue( pfield->coltyp ) );		//	long values are handled in ErrFLDSetOneColumn()
			Assert( !FRECSLV( pfield->coltyp ) );
			if ( grbit & JET_bitSetUniqueNormalizedMultiValues )
				{
				errT = ErrCheckUniqueNormalizedMultiValues(
								pfield,
								*pdataToSet,
								itagfld,
								itagSequence,
								&PinstFromPfucb( pfucb )->m_idxunicodeDefault );
				}
			else
				{
				errT = ErrCheckUniqueMultiValues(
								pfieldNil,			//	not normalising, so don't need pfield
								*pdataToSet,
								itagfld,
								itagSequence,
								&PinstFromPfucb( pfucb )->m_idxunicodeDefault,
								fFalse );
				}
			if ( errT < 0 )
				return errT;
			}

		Assert( FRECLongValue( pfield->coltyp )
			|| FRECSLV( pfield->coltyp )
			|| pdataToSet->Cb() <= JET_cbColumnMost );

		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FMultiValues() )
			{
			INT			delta		= pdataToSet->Cb();
			Assert( !pheader->FSeparated() );
			if ( pheader->FTwoValues() )
				{
				TWOVALUES	tv( PbData( itagfld ), CbData( itagfld ) );

				Assert( !pheader->FColumnCanBeSeparated() );
				Assert( !( grbit & grbitSetColumnSeparated ) );
				if ( 1 == itagSequence || 2 == itagSequence )
					{
					delta -= ( 1 == itagSequence ? tv.CbFirstValue() : tv.CbSecondValue() );
					if ( cbRec + delta > cbRECRecordMost )
						return ErrERRCheck( JET_errRecordTooBig );

					//	if value is growing, must make room for it
					//	if shrinking, cannot resize until after update is done
					if ( delta > 0 )
						ResizeTagfld( itagfld, delta );

					tv.UpdateInstance(
							itagSequence,
							pdataToSet,
							pfield->coltyp );

					if ( delta < 0 )
						ResizeTagfld( itagfld, delta );

					Assert( sizeof(TAGFLD_HEADER)
								+ sizeof(TWOVALUES::TVLENGTH)
								+ tv.CbFirstValue()
								+ tv.CbSecondValue()
							== CbData( itagfld ) );
					}
				else
					{
					//	adding a new instance, so must convert to MULTIVALUES
					delta += ( ( 3 * sizeof(MULTIVALUES::MVOFFSET) ) - sizeof(TWOVALUES::TVLENGTH) );
					Assert( delta > 0 );

					if ( cbRec + delta > cbRECRecordMost )
						return ErrERRCheck( JET_errRecordTooBig );

					//	first make room for the new data
					ResizeTagfld( itagfld, delta );
					
					ConvertTwoValuesToMultiValues(
							&tv,
							pdataToSet,
							pfield->coltyp );
					}
				}
			else
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );

				if ( 0 == itagSequence || itagSequence > mv.CMultiValues() )
					{
					//	adding a new instance, so must convert to MULTIVALUES
					delta += sizeof(MULTIVALUES::MVOFFSET);
					Assert( delta > 0 );

					if ( cbRec + delta > cbRECRecordMost )
						return ErrERRCheck( JET_errRecordTooBig );

					ResizeTagfld( itagfld, delta );
					mv.AddInstance(
							pdataToSet,
							pfield->coltyp,
							grbit & grbitSetColumnSeparated );

					}
				else
					{
					delta -= mv.CbData( itagSequence-1 );
					if ( cbRec + delta > cbRECRecordMost )
						return ErrERRCheck( JET_errRecordTooBig );

					//	if value is growing, must make room for it
					//	if shrinking, cannot resize until after update is done
					if ( delta > 0 )
						ResizeTagfld( itagfld, delta );

					mv.UpdateInstance(
							itagSequence,
							pdataToSet,
							pfield->coltyp,
							grbit & grbitSetColumnSeparated );

					if ( delta < 0 )
						ResizeTagfld( itagfld, delta );
					}

				Assert( sizeof(TAGFLD_HEADER) + mv.CbMultiValues() == CbData( itagfld ) );
				}

			pfucb->dataWorkBuf.DeltaCb( delta );
			}

		else if ( 1 == itagSequence || Ptagfld( itagfld )->FNull() )
			{
			//	overwrite with non-NULL value: have to shift record data
			//	Compute change in column size.
			//
			const ULONG		cbTagField		= CbData( itagfld );
			INT				dbFieldData		= pdataToSet->Cb() - cbTagField;

			//	this column will no longer be NULL
			
			Ptagfld( itagfld )->ResetFNull();

			if ( FRECLongValue( pfield->coltyp ) || FRECSLV( pfield->coltyp ) )
				{
				//	need header byte
				dbFieldData += sizeof(TAGFLD_HEADER);
				}
			else
				{
				Assert( !Ptagfld( itagfld )->FExtendedInfo() );
				Assert( NULL == pheader );
				}

			if ( cbRec + dbFieldData > cbRECRecordMost )
				return ErrERRCheck( JET_errRecordTooBig );

			if ( 0 != dbFieldData )
				ResizeTagfld( itagfld, dbFieldData );

			ReplaceTagfldData(
				itagfld,
				pdataToSet,
				pfield->coltyp,
				grbit & grbitSetColumnSeparated );

			pfucb->dataWorkBuf.DeltaCb( dbFieldData );
			}
		else
			{
			ULONG	cbGrow;

			//	adding a second instance, so must convert to TWOVALUES/MULTIVALUES
			if ( NULL != pheader )
				{
				Assert( FRECLongValue( pfield->coltyp )
					|| FRECSLV( pfield->coltyp ) );
				Assert( pheader->FColumnCanBeSeparated() );
				cbGrow = ( 2 * sizeof(MULTIVALUES::MVOFFSET) )
							+ pdataToSet->Cb();
				if ( cbRec + cbGrow > cbRECRecordMost )
					return ErrERRCheck( JET_errRecordTooBig );

				//	first make room for new data
				ConvertToMultiValues(
						itagfld,
						pdataToSet,
						grbit & grbitSetColumnSeparated );
				}
			else
				{
				Assert( !FRECLongValue( pfield->coltyp ) );
				Assert( !FRECSLV( pfield->coltyp ) );
				cbGrow = sizeof(TAGFLD_HEADER)
							+ sizeof(TWOVALUES::TVLENGTH)
							+ pdataToSet->Cb();
				if ( cbRec + cbGrow > cbRECRecordMost )
					return ErrERRCheck( JET_errRecordTooBig );

				//	first make room for new data
				ConvertToTwoValues(
					itagfld,
					pdataToSet,
					pfield->coltyp );
				Assert( !Ptagfld( itagfld )->FExtendedInfo() );
				Ptagfld( itagfld )->SetFExtendedInfo();
				}

			pfucb->dataWorkBuf.DeltaCb( cbGrow );
			}
		}

	else if ( !Ptagfld( itagfld )->FNull() )	// Overwriting non-null with null
		{
		// Ensure that we've found a field
		//
		Assert( itagfld < CTaggedColumns() );
		Assert( Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) );

		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FMultiValues() )
			{
			const ULONG cbDataOld	= CbData( itagfld );
			const ULONG	cbShrink	= ( pheader->FTwoValues() ?
												CbConvertTwoValuesToSingleValue( itagfld, itagSequence ) :
												CbDeleteMultiValue( itagfld, itagSequence ) );
			Assert( cbShrink < cbDataOld );
			pfucb->dataWorkBuf.DeltaCb( -cbShrink );
			}

		else if ( 1 == itagSequence )
			{
			//	Overwrite with NULL: In most cases, just delete the occurrence from
			//	the record.  However, there is one rare case where we have to
			//	leave behind a null entry.  This is the case where there are no
			//	other instances of this column for this record, and where this
			//	column has a default value.
			const ULONG		cbTagField		= CbData( itagfld );
			if ( FFIELDDefault( pfield->ffield )
				&& !( grbit & JET_bitSetRevertToDefaultValue ) )
				{
				if ( cbTagField > 0 )
					ResizeTagfld( itagfld, -cbTagField );

				Ptagfld( itagfld )->ResetFExtendedInfo();
				Ptagfld( itagfld )->SetFNull();
				
				pfucb->dataWorkBuf.DeltaCb( -cbTagField );
				}
			else
				{
				DeleteTagfld( itagfld );
				pfucb->dataWorkBuf.DeltaCb( -( (ULONG)sizeof(TAGFLD) + cbTagField ) );
				}
			}
		else
			{
			//	appending NULL, which is a NOP
			}
		}

	else									// Overwriting null with null.  Either revert to default or do nothing.
		{
		Assert( itagfld < CTaggedColumns() );
		Assert( Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) );
		Assert( FFIELDDefault( pfield->ffield ) );
		Assert( !Ptagfld( itagfld )->FExtendedInfo() );
		Assert( 0 == CbData( itagfld ) );

		if ( grbit & JET_bitSetRevertToDefaultValue )
			{
			DeleteTagfld( itagfld );
			pfucb->dataWorkBuf.DeltaCb( -( (ULONG)sizeof(TAGFLD) ) );
			}
		}

#ifdef DEBUG
	const REC	* prec						= (REC *)pfucb->dataWorkBuf.Pv();
	const BYTE	* pbRecMax					= (BYTE *)prec + pfucb->dataWorkBuf.Cb();
	const BYTE	* pbStartOfTaggedColumns	= prec->PbTaggedData();

	Assert( pbStartOfTaggedColumns <= pbRecMax );
	Assert( (BYTE *)Rgtagfld() == pbStartOfTaggedColumns );
	Assert( pbStartOfTaggedColumns + CbTaggedColumns() == pbRecMax );
	AssertValid( pfucb->u.pfcb->Ptdb() );
#endif	

	return JET_errSuccess;
	}


ERR TAGFIELDS::ErrRetrieveColumn(
	FCB				* const pfcb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	const DATA&		dataRec,
	DATA			* const pdataRetrieveBuffer,
	const ULONG		grbit )
	{
	const TDB		* const ptdb		= ( pfcbNil == pfcb ? ptdbNil : pfcb->Ptdb() );
	const FID		fid					= FidOfColumnid( columnid );
	const BOOL		fUseDerivedBit		= ( grbit & grbitRetrieveColumnUseDerivedBit );
	ULONG			itagfld;

#ifdef DEBUG
	const BOOL		fUseDMLLatchDBG		= ( fid > ptdb->FidTaggedLastInitial()
											&& ( grbit & grbitRetrieveColumnDDLNotLocked ) );

	if ( pfcbNil == pfcb )
		{
		//	don't need any meta data info if we're not retrieving default values
		Assert( grbit & JET_bitRetrieveIgnoreDefault );
		}
	else
		{
		Assert( ptdbNil != ptdb );

		Assert( fid >= ptdb->FidTaggedFirst() );
		Assert( fid <= ptdb->FidTaggedLast() );

		AssertValid( ptdb );

		// RECIAccessColumn() should have already been called to verify FID.
		if ( fUseDMLLatchDBG )
			pfcb->EnterDML();
		Assert( fid <= ptdb->FidTaggedLast() );
		Assert( JET_coltypNil != ptdb->PfieldTagged( columnid )->coltyp
			|| ( grbit & grbitRetrieveColumnReadSLVInfo ) );
		if ( fUseDMLLatchDBG )
			pfcb->LeaveDML();
		}
#endif

	TAGFLD			tagfldRetrieve( fid, fUseDerivedBit );

	itagfld = ItagfldFind( tagfldRetrieve );

	Assert( itagfld <= CTaggedColumns() );
	Assert( itagfld == CTaggedColumns()
		|| !Ptagfld( itagfld )->FNull()
		|| 0 == CbData( itagfld ) );		// If null, length is 0.

	if ( itagfld < CTaggedColumns()
		&& Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) )
		{
		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( Ptagfld( itagfld )->FNull() )
			{
			Assert( !Ptagfld( itagfld )->FExtendedInfo() );
			Assert( NULL == pheader );
			Assert( 0 == CbData( itagfld ) );
#ifdef DEBUG
			if ( pfcbNil != pfcb )
				{
				if ( fUseDMLLatchDBG )
					pfcb->EnterDML();
				Assert( FFIELDDefault( ptdb->PfieldTagged( columnid )->ffield ) );
				if ( fUseDMLLatchDBG )
					pfcb->LeaveDML();
				}
#endif				
			}

		else if ( NULL != pheader
			&& pheader->FMultiValues() )
			{
			Assert( Ptagfld( itagfld )->FExtendedInfo() );
			Assert( itagSequence > 0 );

			if ( pheader->FTwoValues() )
				{
				if ( itagSequence <= 2 )
					{
					TWOVALUES	tv( PbData( itagfld ), CbData( itagfld ) );
					tv.RetrieveInstance( itagSequence, pdataRetrieveBuffer );
					return JET_errSuccess;
					}
				}
			else
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				if ( itagSequence <= mv.CMultiValues() )
					{
					return mv.ErrRetrieveInstance( itagSequence, pdataRetrieveBuffer );
					}
				}

			// If we reached here, our desired occurrence is not in the
			// record.  Fall through to NullField.
			}

		else if ( 1 == itagSequence )
			{
			pdataRetrieveBuffer->SetPv( PbData( itagfld ) );
			pdataRetrieveBuffer->SetCb( CbData( itagfld ) );

			if ( NULL != pheader )
				{
				Assert( Ptagfld( itagfld )->FExtendedInfo() );
				const INT	iDelta	= sizeof(TAGFLD_HEADER);
				pdataRetrieveBuffer->DeltaPv( iDelta );
				pdataRetrieveBuffer->DeltaCb( -iDelta );
				return pheader->ErrRetrievalResult();
				}
			else
				{
				return JET_errSuccess;
				}
			}

		else
			{
			//	non-existent itagSequence, so return NULL
			}
		}

	else if ( !( grbit & JET_bitRetrieveIgnoreDefault ) && 1 == itagSequence && ptdb->FTableHasNonEscrowDefault() )
		{
		Assert( pfcbNil != pfcb );
		Assert( ptdbNil != ptdb );

		Assert( !( grbit & grbitRetrieveColumnReadSLVInfo ) );

		const BOOL	fUseDMLLatch	= ( FidOfColumnid( columnid ) > ptdb->FidTaggedLastInitial()
										&& ( grbit & grbitRetrieveColumnDDLNotLocked ) );

		if ( fUseDMLLatch )
			pfcb->EnterDML();
		
		//	assert no infinite recursion
		Assert( dataRec.Pv() != ptdb->PdataDefaultRecord() );

		const FIELDFLAG	ffield	= ptdb->PfieldTagged( columnid )->ffield;
		if ( FFIELDUserDefinedDefault( ffield ) )
			{
			Assert( FFIELDDefault( ffield ) );

			//	no occurrrences found, but a user-defined default value
			//	exists and we are retrieving first occcurence.
			
			if ( fUseDMLLatch )
				pfcb->LeaveDML();

			pdataRetrieveBuffer->Nullify();
			return ErrERRCheck( wrnRECUserDefinedDefault );
			}

		else if ( FFIELDDefault( ffield ) )
			{
			//	no occurrrences found, but a default value exists and
			//	we are retrieving first occcurence.
			const ERR	errT	= ErrRECIRetrieveTaggedDefaultValue(
										pfcb,
										columnid,
										pdataRetrieveBuffer );

			if ( fUseDMLLatch )
				pfcb->LeaveDML();
			
			return errT;			
			}
			
		if ( fUseDMLLatch )
			pfcb->LeaveDML();
		}
		
	//	null column common exit point
	//
	pdataRetrieveBuffer->Nullify();
	return ErrERRCheck( JET_wrnColumnNull );
	}


ULONG TAGFIELDS::UlColumnInstances(
	FCB				* const pfcb,
	const COLUMNID	columnid,
	const BOOL		fUseDerivedBit )
	{
	const FID		fid					= FidOfColumnid( columnid );
	ULONG			itagfld;

#ifdef DEBUG
	Assert( pfcbNil != pfcb );
	
	const TDB		* const ptdb		= pfcb->Ptdb();
	Assert( ptdbNil != ptdb );

	const BOOL		fUseDMLLatchDBG		= ( fid > ptdb->FidTaggedLastInitial() );

	if ( fUseDMLLatchDBG )
		pfcb->EnterDML();

	// RECIAccessColumn() should have already been called to verify FID.
	Assert( fid >= ptdb->FidTaggedFirst() );
	Assert( fid <= ptdb->FidTaggedLast() );

	AssertValid( ptdb );

	const FIELD	*pfield = ptdb->PfieldTagged( columnid );
	Assert( pfieldNil != pfield );
	Assert( JET_coltypNil != pfield->coltyp );

	const BOOL	fDefaultValue = FFIELDDefault( pfield->ffield );

	if ( fUseDMLLatchDBG )
		pfcb->LeaveDML();
#endif

	TAGFLD			tagfldRetrieve( fid, fUseDerivedBit );
	ULONG			ulInstances			= 0;

	itagfld = ItagfldFind( tagfldRetrieve );

	Assert( itagfld <= CTaggedColumns() );
	Assert( itagfld == CTaggedColumns()
		|| !Ptagfld( itagfld )->FNull()
		|| 0 == CbData( itagfld ) );		// If null, length is 0.

	if ( itagfld < CTaggedColumns()
		&& Ptagfld( itagfld )->FIsEqual( fid, fUseDerivedBit ) )
		{
		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( Ptagfld( itagfld )->FNull() )
			{
			Assert( !Ptagfld( itagfld )->FExtendedInfo() );
			Assert( NULL == pheader );
			Assert( 0 == CbData( itagfld ) );
			Assert( 0 == ulInstances );
			Assert( fDefaultValue );
			}

		else if ( NULL != pheader
			&& pheader->FMultiValues() )
			{
			Assert( Ptagfld( itagfld )->FExtendedInfo() );
			if ( pheader->FTwoValues() )
				{
				Assert( !pheader->FColumnCanBeSeparated() );
				ulInstances = 2;
				}
			else
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				ulInstances = mv.CMultiValues();
				}
			}

		else
			{
			ulInstances = 1;
			}
		}

	else
		{
		const TDB * const	ptdb			= pfcb->Ptdb();
		const BOOL			fUseDMLLatch	= ( fid > ptdb->FidTaggedLastInitial() );

		if ( fUseDMLLatch )
			pfcb->EnterDML();

		if ( FFIELDDefault( pfcb->Ptdb()->PfieldTagged( columnid )->ffield ) )
			{
			//	no occurrrences found, but a default value exists
			//
			ulInstances = 1;
			Assert( fDefaultValue );
			}
		else
			{
			Assert( 0 == ulInstances );
			Assert( !fDefaultValue );
			}

		if ( fUseDMLLatch )
			pfcb->LeaveDML();
		}
		
	return ulInstances;
	}


ERR TAGFIELDS::ErrScan(
	FUCB			* const pfucb,
	const ULONG		itagSequence,
	const DATA&		dataRec,
	DATA			* const pdataField,
	COLUMNID		* const pcolumnidRetrieved,
	ULONG			* const pitagSequenceRetrieved,	
	const JET_GRBIT	grbit )
	{
	ERR				err;
	FCB				* const pfcb		= pfucb->u.pfcb;
	const BOOL		fRetrieveNulls		= ( grbit & JET_bitRetrieveNull );
	const BOOL		fRefreshNeeded		= ( dataRec.Pv() == pfucb->kdfCurr.data.Pv() );
	
	if ( fRefreshNeeded )
		{
		Assert( dataRec.Cb() == pfucb->kdfCurr.data.Cb() );
		
		// Only need to refresh ptagfld if we're accessing the database,
		// in which case we must have the page latched.  Note that we
		// may have a page latched, but we may not be accessing the
		// the database (ie. no refresh needed).
		Assert( Pcsr( pfucb )->FLatched() );
		}

	// Verify we're in a transaction in order to ensure read-consistency
	// in case we have a page latch and need to release it while scanning/
	Assert( pfucb->ppib->level > 0 );
	Assert( pfcb != pfcbNil );
	Assert( pdataField != NULL );
	Assert( pcolumnidRetrieved != NULL );

	// If itagSequence == 0, then we're counting the number of tagged columns
	// in the record and we will output the result in pitagSequenceRetrieved.
	Assert( itagSequence != 0 || pitagSequenceRetrieved != NULL );

	Assert( pfcb->Ptdb() != ptdbNil );
	const TDB		* const ptdb		= pfcb->Ptdb();
	AssertValid( ptdb );

	const BOOL		fRetrieveDefaults 	= ( !( grbit & JET_bitRetrieveIgnoreDefault )
											&& ptdb->FTableHasNonEscrowDefault() );
	ULONG			itagfld;
	ULONG			ulNumOccurrences	= 0;
	COLUMNID		columnidCurr		= ColumnidRECFirstTaggedForScan( ptdb );

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		Assert( FCOLUMNIDTagged( columnidCurr ) );

#ifdef DEBUG
#else
		if ( fGlobalRepair )
#endif		
			{
			const TAGFLD	* ptagfldT			= Ptagfld( itagfld );
			BOOL			fBadColumn;
			if ( ptagfldT->FDerived() )
				{
				const FCB	* const pfcbTemplate	= ptdb->PfcbTemplateTable();
				fBadColumn = ( pfcbNil == pfcbTemplate
							|| ptagfldT->Fid() > pfcbTemplate->Ptdb()->FidTaggedLast()
							|| ptagfldT->Fid() < pfcbTemplate->Ptdb()->FidTaggedFirst() );
				}
			else
				{
				fBadColumn = ( ptagfldT->Fid() > ptdb->FidTaggedLast()
							|| ptagfldT->Fid() < ptdb->FidTaggedFirst() );
				}
			if ( fBadColumn )
				{
				//	log event
				//
				AssertTracking();
				UtilReportEvent( eventWarning, REPAIR_CATEGORY, REPAIR_BAD_COLUMN_ID, 0, NULL );
				break;
				}
			}


		// Check for any "gaps" caused by default values (if we
		// want default values retrieved).
		if ( fRetrieveDefaults )
			{
			//	make copy of tagfld so we don't have to worry about losing the page
			//	latch on column access check
			const TAGFLD	* ptagfldT			= Ptagfld( itagfld );
			TAGFLD			tagfldT( ptagfldT->Fid(), ptagfldT->FDerived() );

			for( ;
				tagfldT.FIsGreaterThan( columnidCurr, ptdb );
				columnidCurr = ColumnidRECNextTaggedForScan( ptdb, columnidCurr ) )
				{
				FCB		*pfcbT		= pfcb;

				Assert( ulNumOccurrences < itagSequence || 0 == itagSequence );

				if ( FCOLUMNIDTemplateColumn( columnidCurr ) )
					{
					if ( pfcbNil != ptdb->PfcbTemplateTable() )
						{
						ptdb->AssertValidDerivedTable();
						pfcbT = ptdb->PfcbTemplateTable();
						}
					else
						{
						ptdb->AssertValidTemplateTable();
						}
					}
				else
					{
					err = ErrRECIAccessColumn( pfucb, columnidCurr );
					if ( err < 0 )
						{
						if ( JET_errColumnNotFound == err )
							continue;
						return err;
						}
					}

				const TDB *	const	ptdbT			= pfcbT->Ptdb();
				const BOOL			fUseDMLLatch	= ( FidOfColumnid( columnidCurr ) > ptdbT->FidTaggedLastInitial() );

				if ( fUseDMLLatch )
					pfcbT->EnterDML();

				Assert( JET_coltypNil != ptdbT->PfieldTagged( columnidCurr )->coltyp );
				const FIELDFLAG		ffield			= ptdbT->PfieldTagged( columnidCurr )->ffield;

				if ( FFIELDUserDefinedDefault( ffield ) )
					{
					if ( ++ulNumOccurrences == itagSequence )
						{
						Assert( itagSequence != 0 );
						*pcolumnidRetrieved = columnidCurr;
						if ( pitagSequenceRetrieved != NULL )
							*pitagSequenceRetrieved = 1;

						//	assert no infinite recursion
						Assert( dataRec.Pv() != ptdbT->PdataDefaultRecord() );

						if ( fUseDMLLatch )
							pfcbT->LeaveDML();
						return ErrERRCheck( wrnRECUserDefinedDefault );
						}
					}
				else if ( FFIELDDefault( ffield ) )
					{
					if ( ++ulNumOccurrences == itagSequence )
						{
						Assert( itagSequence != 0 );
						*pcolumnidRetrieved = columnidCurr;
						if ( pitagSequenceRetrieved != NULL )
							*pitagSequenceRetrieved = 1;

						//	assert no infinite recursion
						Assert( dataRec.Pv() != ptdbT->PdataDefaultRecord() );
						err = ErrRECIRetrieveTaggedDefaultValue( pfcbT, columnidCurr, pdataField );

						if ( fUseDMLLatch )
							pfcbT->LeaveDML();
						return err;
						}
					}

				if ( fUseDMLLatch )
					pfcbT->LeaveDML();
				}

			Assert( tagfldT.FIsEqual( columnidCurr, ptdb ) );
			}
		else
			{
			columnidCurr = Ptagfld( itagfld )->Columnid( ptdb );
			}

		if ( FCOLUMNIDTemplateColumn( columnidCurr ) )
			{
#ifdef DEBUG
			DATA	dataSav;

			if ( fRefreshNeeded )
				{
				dataSav.SetPv( pfucb->kdfCurr.data.Pv() );
				dataSav.SetCb( pfucb->kdfCurr.data.Cb() );
				}

			CallS( ErrRECIAccessColumn( pfucb, columnidCurr ) );

			if ( fRefreshNeeded )
				{
				//	verify pointers didn't change - we should not lose latch
				//	because we shouldn't have to consult catalog
				Assert(	pfucb->kdfCurr.data == dataSav );
				}
#endif
			//	template column obtained from either TDB or from record,
			//	so it must exist
			err = JET_errSuccess;
			}
		else
			{
			err = ErrRECIAccessColumn( pfucb, columnidCurr );
			if ( err < 0 && JET_errColumnNotFound != err )
				return err;

			if ( fRefreshNeeded )
				{
				// We may have invalidated our pointer if we had to give
				// up the latch. Force refresh.
				Refresh( pfucb->kdfCurr.data );
				}
			}

		Assert( Ptagfld( itagfld )->FIsEqual( columnidCurr, ptdb ) );

		CallSx( err, JET_errColumnNotFound );
		if ( JET_errColumnNotFound == err )
			{
			// Column not visible to this session.  Skip to next one.
			}

		else if ( Ptagfld( itagfld )->FNull() )
			{
			// If there's an explicit null entry, it should be the only
			// occurrence of this fid.  Also the only reason for an explict
			// null entry is to override a default value.
			Assert( !Ptagfld( itagfld )->FExtendedInfo() );
			Assert( 0 == CbData( itagfld ) );
			Assert( ulNumOccurrences < itagSequence || 0 == itagSequence );
#ifdef DEBUG
			pfcb->EnterDML();
			Assert( FFIELDDefault( ptdb->PfieldTagged( columnidCurr )->ffield ) );
			pfcb->LeaveDML();
#endif

			// Only count columns explicitly set to null if the RetrieveNulls
			// flag is passed.  Otherwise, just skip it.
			if ( fRetrieveNulls && ++ulNumOccurrences == itagSequence )
				{
				Assert( itagSequence != 0 );
				*pcolumnidRetrieved = columnidCurr;
				if ( pitagSequenceRetrieved != NULL )
					*pitagSequenceRetrieved = 1;

				pdataField->Nullify();
				return ErrERRCheck( JET_wrnColumnSetNull );
				}
			}

		else
			{
			Assert( ulNumOccurrences < itagSequence || 0 == itagSequence );
			const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
			if ( NULL != pheader
				&& pheader->FMultiValues() )
				{
				const ULONG		itagSequenceToRetrieve	= ( 0 == itagSequence ? 0 : itagSequence - ulNumOccurrences );
				if ( pheader->FTwoValues() )
					{
					ulNumOccurrences += 2;
					if ( 1 == itagSequenceToRetrieve
						|| 2 == itagSequenceToRetrieve )
						{
						*pcolumnidRetrieved = columnidCurr;
						if ( NULL != pitagSequenceRetrieved )
							*pitagSequenceRetrieved = itagSequenceToRetrieve;

						TWOVALUES	tv( PbData( itagfld ), CbData( itagfld ) );
						tv.RetrieveInstance( itagSequenceToRetrieve, pdataField );
						return JET_errSuccess;
						}
					}
				else
					{
					MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
					ulNumOccurrences += mv.CMultiValues();

					if ( itagSequenceToRetrieve > 0
						&& itagSequenceToRetrieve <= mv.CMultiValues() )
						{
						*pcolumnidRetrieved = columnidCurr;
						if ( NULL != pitagSequenceRetrieved )
							*pitagSequenceRetrieved = itagSequenceToRetrieve;

						return mv.ErrRetrieveInstance( itagSequenceToRetrieve, pdataField );
						}
					}
				}
			else if ( ++ulNumOccurrences == itagSequence )
				{
				Assert( 0 != itagSequence );
				
				pdataField->SetCb( CbData( itagfld ) );
				pdataField->SetPv( PbData( itagfld ) );
				if ( NULL != pheader )
					{
					Assert( Ptagfld( itagfld )->FExtendedInfo() );
					const INT	iDelta	= sizeof(TAGFLD_HEADER);
					pdataField->DeltaPv( iDelta );
					pdataField->DeltaCb( -iDelta );
					}

				*pcolumnidRetrieved = columnidCurr;
				if ( pitagSequenceRetrieved != NULL )
					*pitagSequenceRetrieved = 1;

				return ( NULL == pheader ? JET_errSuccess : pheader->ErrRetrievalResult() );
				}
			}

		//	if we got here, we haven't foudn the instance we're looking for
		Assert( ulNumOccurrences < itagSequence || 0 == itagSequence );

		columnidCurr = ColumnidRECNextTaggedForScan( ptdb, columnidCurr );
		}	// while ( itagfld < CTaggedColumns() )


	if ( fRetrieveDefaults )
		{
		// Take snapshot of FidTaggedLast.  Even if someone's adding
		// columns, we don't have access to it anyways.
		FID	fidTaggedLast = ptdb->FidTaggedLast();

		for( ;
			( FCOLUMNIDTemplateColumn( columnidCurr ) && !ptdb->FTemplateTable() ) || FidOfColumnid( columnidCurr ) <= fidTaggedLast;
			columnidCurr = ColumnidRECNextTaggedForScan( ptdb, columnidCurr ) )
			{
			FCB		*pfcbT		= pfcb;

			Assert( ulNumOccurrences < itagSequence || 0 == itagSequence );

			if ( FCOLUMNIDTemplateColumn( columnidCurr ) )
				{
				if ( pfcbNil != ptdb->PfcbTemplateTable() )
					{
					ptdb->AssertValidDerivedTable();
					pfcbT = ptdb->PfcbTemplateTable();
					}
				else
					{
					ptdb->AssertValidTemplateTable();
					}
				}
			else
				{
				err = ErrRECIAccessColumn( pfucb, columnidCurr );
				if ( err < 0 )
					{
					if ( JET_errColumnNotFound == err )
						continue;
					return err;
					}
				}

			const TDB *	const	ptdbT			= pfcbT->Ptdb();
			const BOOL			fUseDMLLatch	= ( FidOfColumnid( columnidCurr ) > ptdbT->FidTaggedLastInitial() );

			if ( fUseDMLLatch )
				pfcbT->EnterDML();

			Assert( JET_coltypNil != ptdbT->PfieldTagged( columnidCurr )->coltyp );
			const FIELDFLAG		ffield			= ptdbT->PfieldTagged( columnidCurr )->ffield;

			if ( FFIELDUserDefinedDefault( ffield ) )
				{
				if ( ++ulNumOccurrences == itagSequence )
					{
					Assert( itagSequence != 0 );
					*pcolumnidRetrieved = columnidCurr;
					if ( pitagSequenceRetrieved != NULL )
						*pitagSequenceRetrieved = 1;

					//	assert no infinite recursion
					Assert( dataRec.Pv() != ptdbT->PdataDefaultRecord() );

					if ( fUseDMLLatch )
						pfcbT->LeaveDML();
					return ErrERRCheck( wrnRECUserDefinedDefault );
					}
				}
			else if ( FFIELDDefault( ffield ) )
				{
				if ( ++ulNumOccurrences == itagSequence )
					{
					Assert( itagSequence != 0 );
					*pcolumnidRetrieved = columnidCurr;
					if ( pitagSequenceRetrieved != NULL )
						*pitagSequenceRetrieved = 1;

					//	assert no infinite recursion
					Assert( dataRec.Pv() != ptdbT->PdataDefaultRecord() );
					err = ErrRECIRetrieveTaggedDefaultValue( pfcbT, columnidCurr, pdataField );

					if ( fUseDMLLatch )
						pfcbT->LeaveDML();
					return err;
					}
				}

			if ( fUseDMLLatch )
				pfcbT->LeaveDML();
			}
		}

	// If we reached here, no more tagged columns.
	*pcolumnidRetrieved = 0;
	if ( pitagSequenceRetrieved != NULL )
		*pitagSequenceRetrieved = ( itagSequence == 0 ? ulNumOccurrences : 0 );

	//	null column common exit point
	//
	pdataField->Nullify();
	return ErrERRCheck( JET_wrnColumnNull );
	}


ERR TAGFIELDS::ErrAffectLongValuesInWorkBuf(
	FUCB			* const pfucb,
	const LVAFFECT	lvaffect )
	{
	ERR				err				= JET_errSuccess;
	TDB				* const ptdb	= pfucb->u.pfcb->Ptdb();
	ULONG			itagfld			= 0;

#ifdef DEBUG
	const ULONG		cTaggedColumns	= CTaggedColumns();		//	snapshot original count for debugging
	const REC		* prec			= (REC *)( pfucb->dataWorkBuf.Pv() );
	Assert( prec->PbTaggedData() == (BYTE *)m_rgtagfld );
#endif	

	Assert( ptdbNil != ptdb );
	AssertValid( ptdb );
	Assert( !Pcsr( pfucb )->FLatched() );

	//	WARNING: This function performs LV updates and also modifies
	//	the copy buffer, so if this function returns an error and
	//	the LV updates are rolled back, it is up to the caller to
	//	ensure that either the copy buffer is discarded or the
	//	original copy buffer is re-instated
	Assert( pfucb->ppib->level > 0 );
	Assert( lvaffectSeparateAll == lvaffect
		|| ( lvaffectReferenceAll == lvaffect && FFUCBInsertCopyPrepared( pfucb ) ) );

	while ( itagfld < CTaggedColumns() )
		{
		const COLUMNID	columnidCurr	= Ptagfld( itagfld )->Columnid( ptdb );
		TAGFLD_HEADER	* const pheader	= Pheader( itagfld );
		const BOOL		fRemoveSLV		= ( lvaffectReferenceAll == lvaffect	//	can't single-instance SLVs, so must remove them
												&& NULL != pheader
												&& pheader->FSLV() );
		BOOL			fRemoveColumn	= fRemoveSLV;
		
		Assert( !Pcsr( pfucb )->FLatched() );

		if ( !fRemoveColumn && !FCOLUMNIDTemplateColumn( columnidCurr ) )
			{
			//	only check column visibility if we don't already
			//	know we're going to remove it
			err = ErrRECIAccessColumn( pfucb, columnidCurr );
			if ( err < 0 )
				{
				if ( JET_errColumnNotFound != err )
					goto HandleError;

				err = JET_errSuccess;
				fRemoveColumn = fTrue;
				}
			else
				{
				Assert( !fRemoveSLV );
				CallS( err );
				}
			}

		if ( fRemoveColumn )
			{
			const ULONG		cbColumnToRemove	= CbData( itagfld );
			Assert( !FCOLUMNIDTemplateColumn( columnidCurr ) );

			//	Two cases where we must remove the column:
			//	1) SLV: don't support single-instancing SLVs, so
			//	must remove from copy buffer
			//	2) Column not visible to this session.  Since the
			//	column exists in this session's record, the column
			//	could not have been version-added by someone else.
			//	Therefore, it must have been deleted by this session,
			//	or deleted and committed before this transaction began.
#ifdef DEBUG
			if ( !fRemoveSLV )
				{
				pfucb->u.pfcb->EnterDML();
				Assert( FFIELDDeleted( ptdb->PfieldTagged( columnidCurr )->ffield ) );
				pfucb->u.pfcb->LeaveDML();
				}
#endif

			//	if SeparateAll, must first deref all LVs before removing
			//	them from the record
			//	if ReferenceAll, don't deref because we're coming
			//	from InsertCopy and the only ref belongs to the
			//	original record
			if ( lvaffectSeparateAll == lvaffect
				&& NULL != pheader
				&& pheader->FColumnCanBeSeparated() )
				{
				Assert( !pheader->FTwoValues() );
				if ( pheader->FMultiValues() )
					{
					MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
					ULONG			imv;

#ifdef UNLIMITED_MULTIVALUES
#else
					Assert( !pheader->FSeparated() );
#endif					

					for ( imv = 0; imv < mv.CMultiValues(); imv++ )
						{
						if ( mv.FSeparatedInstance( imv ) )
							{
							//	set flag so that on prepCancel, delta RCE
							//	will be properly rolled back
							FUCBSetUpdateSeparateLV( pfucb );
				 			Assert( sizeof(LID) == mv.CbData( imv ) );
							LID		lidT	= LidOfSeparatedLV( mv.PbData( imv ) );
							Call( ErrRECAffectSeparateLV( pfucb, &lidT, fLVDereference ) );
							Assert( JET_wrnCopyLongValue != err );
							}
						}
					}
				else if ( pheader->FSeparated() )
					{
					FUCBSetUpdateSeparateLV( pfucb );
		 			Assert( sizeof(LID) == CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
					LID		lidT	= LidOfSeparatedLV( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
					Call( ErrRECAffectSeparateLV( pfucb, &lidT, fLVDereference ) );
					Assert( JET_wrnCopyLongValue != err );
					}
				}

			DeleteTagfld( itagfld );
			pfucb->dataWorkBuf.DeltaCb( -( (ULONG)sizeof(TAGFLD) + cbColumnToRemove ) );

			//	don't increment itagfld, so we will retrieve whatever
			//	tagged column now occupies the space vacated by
			//	the deleted column
			continue;
			}

		if ( NULL != pheader
			&& pheader->FColumnCanBeSeparated() )
			{
			Assert( !Ptagfld( itagfld )->FNull() );
			Assert( !pheader->FTwoValues() );
			Assert( CbData( itagfld ) >= sizeof(TAGFLD_HEADER) );

			switch ( lvaffect )
				{
				case lvaffectSeparateAll:
					//	note that we do not separate those long values that are
					//	so short that they take even less space in a record
					//	than a LID for separated long value would.
					if ( pheader->FMultiValues() )
						{
#ifdef UNLIMITED_MULTIVALUES
#else
						Assert( !pheader->FSeparated() );
#endif						
						MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
						ULONG			imv;
						ULONG			cbColumnShrink		= 0;

						for ( imv = 0; imv < mv.CMultiValues(); imv++ )
							{
							if ( !mv.FSeparatedInstance( imv )
								&& mv.CbData( imv ) > sizeof(LID) )
								{
								DATA			dataField;
								LID				lid;
								const ULONG		cbData		= mv.CbData( imv );
								const ULONG		cbShrink	= cbData - sizeof(LID);
								BYTE			rgbT[sizeof(LID)];

								//	set flag so that on prepCancel, insert RCE for
								//	this new separated LV will be properly rolled back
								FUCBSetUpdateSeparateLV( pfucb );
						
			 					dataField.SetPv( mv.PbData( imv ) );
			  					dataField.SetCb( cbData );
		 						Call( ErrRECSeparateLV( pfucb, &dataField, &lid, NULL ) );
								Assert( JET_wrnCopyLongValue == err );

								LVSetLidInRecord(
									rgbT,
									lid );
								dataField.SetPv( rgbT );
								dataField.SetCb( sizeof(LID) );

								mv.UpdateInstance(
									imv + 1,
									&dataField,
									JET_coltypNil,
									fTrue );

								cbColumnShrink += cbShrink;
								}
							}
						if ( cbColumnShrink > 0 )
							{
							ResizeTagfld( itagfld, -cbColumnShrink );
							//	update record size
							pfucb->dataWorkBuf.DeltaCb( -cbColumnShrink );
							}
						}
					else if ( !pheader->FSeparated()
							&& CbData( itagfld ) > sizeof(TAGFLD_HEADER) + sizeof(LID) )
						{
						DATA			dataField;
						LID				lid;
						const ULONG		cbData		= CbData( itagfld ) - sizeof(TAGFLD_HEADER);
						const ULONG		cbShrink	= cbData - sizeof(LID);

						//	set flag so that on prepCancel, insert RCE for
						//	this new separated LV will be properly rolled back
						FUCBSetUpdateSeparateLV( pfucb );
						
	 					dataField.SetPv( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
	  					dataField.SetCb( cbData );
 						Call( ErrRECSeparateLV( pfucb, &dataField, &lid, NULL ) );
						Assert( JET_wrnCopyLongValue == err );

						ResizeTagfld( itagfld, -cbShrink );

						pheader->SetFSeparated();
						LVSetLidInRecord(
							PbData( itagfld ) + sizeof(TAGFLD_HEADER),
							lid );

						//	update record size
						pfucb->dataWorkBuf.DeltaCb( -cbShrink );
						}
					break;
				
				case lvaffectReferenceAll:
					//	all SLVs are removed above
					Assert( !pheader->FSLV() );
					if ( pheader->FMultiValues() )
						{
						MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
						ULONG			imv;

						for ( imv = 0; imv < mv.CMultiValues(); imv++ )
							{
							if ( mv.FSeparatedInstance( imv ) )
								{
								//	set flag so that on prepCancel, delta RCE's will
								//	be properly rolled back
								FUCBSetUpdateSeparateLV( pfucb );
					 			Assert( sizeof(LID) == mv.CbData( imv ) );
								LID		lidT	= LidOfSeparatedLV( mv.PbData( imv ) );
								Call( ErrRECAffectSeparateLV( pfucb, &lidT, fLVReference ) );
								if ( JET_wrnCopyLongValue == err )
									{
									// long value got burst, update LID
									Assert( lidT > LidOfSeparatedLV( mv.PbData( imv ) ) );
									LVSetLidInRecord( mv.PbData( imv ), lidT );
									}
								}
							}
						}
					else if ( pheader->FSeparated() )
						{
						//	set flag so that on prepCancel, delta RCE's will
						//	be properly rolled back
						FUCBSetUpdateSeparateLV( pfucb );
			 			Assert( sizeof(LID) == CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
						LID		lidT	= LidOfSeparatedLV( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
						Call( ErrRECAffectSeparateLV( pfucb, &lidT, fLVReference ) );
						if ( JET_wrnCopyLongValue == err )
							{
							// long value got burst, update LID
							Assert( lidT > LidOfSeparatedLV( PbData( itagfld ) + sizeof(TAGFLD_HEADER) ) );
							LVSetLidInRecord(
								PbData( itagfld ) + sizeof(TAGFLD_HEADER),
								lidT );
							}
						}
					break;

				default:
					Assert( fFalse );
					break;
				}
			}

		itagfld++;

		}	//	while ( itagfld < CTaggedColumns() )

HandleError:
	//	this function should never increase the size of the record
	//	(in fact, we typically call this function to free up record space)
	Assert( JET_errRecordTooBig != err );

	Assert( !Pcsr( pfucb )->FLatched() );
	return err;
	}


ERR TAGFIELDS::ErrDereferenceLongValuesInRecord(
	FUCB		* const pfucb )
	{
	ERR			err;
	TDB			* const ptdb	= pfucb->u.pfcb->Ptdb();
	ULONG		itagfld;

	Assert( ptdbNil != ptdb );
	AssertValid( ptdb );
	Assert( Pcsr( pfucb )->FLatched() );

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		Assert( Pcsr( pfucb )->FLatched() );
		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FColumnCanBeSeparated() )
			{
			const COLUMNID		columnidCurr		= Ptagfld( itagfld )->Columnid( ptdb );
			const BOOL			fSLV				= pheader->FSLV();

			Assert( !pheader->FTwoValues() );
			if ( pheader->FMultiValues() )
				{
#ifdef UNLIMITED_MULTIVALUES
#else
				Assert( !pheader->FSeparated() );
#endif						

				Assert( Pcsr( pfucb )->FLatched() );
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				ULONG			imv;

				if ( fSLV )
					{
					CallR( ErrDIRRelease( pfucb ) );

					//	eaiser to iterate twice than iterate once
					//	and deal with latching/unlatching
					for ( imv = 0; imv < mv.CMultiValues(); imv++ )
						{
						CallR( ErrSLVDelete( pfucb, columnidCurr, imv+1, fFalse ) );
						}

					CallR( ErrDIRGet( pfucb ) );
					Refresh( pfucb->kdfCurr.data );
					mv.Refresh( PbData( itagfld ), CbData( itagfld ) );
					}

				for ( imv = 0; imv < mv.CMultiValues(); imv++ )
					{
					Assert( Pcsr( pfucb )->FLatched() );
					const BOOL	fSeparatedLV	= mv.FSeparatedInstance( imv );
					if ( fSeparatedLV )
						{
						Assert( sizeof(LID) == mv.CbData( imv ) );
						LID		lidToDeref		= LidOfSeparatedLV( mv.PbData( imv ) );

						CallR( ErrDIRRelease( pfucb ) );

						CallR( ErrRECAffectSeparateLV( pfucb, &lidToDeref, fLVDereference ) );
						Assert( JET_wrnCopyLongValue != err );

						//	re-latch for next iteration
						CallR( ErrDIRGet( pfucb ) );
						Refresh( pfucb->kdfCurr.data );
						mv.Refresh( PbData( itagfld ), CbData( itagfld ) );
						}
					}

				}
			else
				{
				Assert( Pcsr( pfucb )->FLatched() );
				const BOOL	fSeparatedLV	= pheader->FSeparated();

				if ( fSLV || fSeparatedLV )
					{
					Assert( !pheader->FSeparated() || sizeof(LID) == CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
					LID		lidToDeref		= ( fSeparatedLV ?
													LidOfSeparatedLV( PbData( itagfld ) + sizeof(TAGFLD_HEADER) ) :
													0 );
					CallR( ErrDIRRelease( pfucb ) );

					if ( fSLV )
						{
						CallR( ErrSLVDelete( pfucb, columnidCurr, 1, fFalse ) );
						}
					if ( fSeparatedLV )
						{
						CallR( ErrRECAffectSeparateLV( pfucb, &lidToDeref, fLVDereference ) );
						Assert( JET_wrnCopyLongValue != err );
						}

					//	re-latch for next iteration
					CallR( ErrDIRGet( pfucb ) );
					Refresh( pfucb->kdfCurr.data );
					}
				}
			}
		}

	Assert( Pcsr( pfucb )->FLatched() );
	return JET_errSuccess;
	}


VOID TAGFIELDS::CopyTaggedColumns(
	FUCB			* const pfucbSrc,
	FUCB			* const pfucbDest,
	JET_COLUMNID	* const mpcolumnidcolumnidTagged )
	{
	const TDB		* const ptdbSrc								= pfucbSrc->u.pfcb->Ptdb();
	BOOL			fESE97DerivedColumnsExist					= fFalse;
	BOOL			fESE98DerivedColumnsExist					= fFalse;
	ULONG			cColumnsToCopy								= 0;
	ULONG			itagfldToCopy								= 0;
	ULONG			itagfld;

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD	* const ptagfld			= Ptagfld( itagfld );
		const COLUMNID	columnid				= ptagfld->Columnid( ptdbSrc );
		const FIELD		* const pfieldTagged	= ptdbSrc->PfieldTagged( columnid );

		Assert( JET_coltypNil != pfieldTagged->coltyp
			|| !FCOLUMNIDTemplateColumn( columnid ) );
		if ( JET_coltypNil != pfieldTagged->coltyp )
			{
			cColumnsToCopy++;

			if ( FCOLUMNIDTemplateColumn( columnid )
				&& !ptdbSrc->FTemplateTable() )
				{
				if ( ptagfld->FDerived() )
					{
					//	shouldn't have seen yet any derived columns with the derived bit not set
					Assert( !fESE97DerivedColumnsExist );
					fESE98DerivedColumnsExist = fTrue;
					}
				else
					{
					Assert( ptdbSrc->FESE97DerivedTable() );
					fESE97DerivedColumnsExist = fTrue;
					}
				}
			}
		}

	if ( 0 == cColumnsToCopy )
		return;

	USHORT	ibDataDest				= USHORT( cColumnsToCopy * sizeof(TAGFLD) );
	TAGFLD	* const rgtagfldDest	= (TAGFLD *)(
											(BYTE *)pfucbDest->dataWorkBuf.Pv()
											+ pfucbDest->dataWorkBuf.Cb() );

	//	verify currently no tagged data
	Assert( (BYTE *)rgtagfldDest
		== ( (REC *)pfucbDest->dataWorkBuf.Pv() )->PbTaggedData() );


	//	if both ESE97 and ESE98 derived columns exist, must copy ESE97 derived columns first
	const BOOL	fNeedSeparatePassForESE97DerivedColumns		= ( fESE97DerivedColumnsExist
																&& fESE98DerivedColumnsExist );
	if ( fNeedSeparatePassForESE97DerivedColumns )
		{
		Assert( !ptdbSrc->FTemplateTable() );
		Assert( ptdbSrc->FESE97DerivedTable() );
		ptdbSrc->AssertValidDerivedTable();

		for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
			{
			const TAGFLD	* const ptagfld			= Ptagfld( itagfld );
			const COLUMNID	columnid				= ptagfld->Columnid( ptdbSrc );
			const FIELD		* const pfieldTagged	= ptdbSrc->PfieldTagged( columnid );

			Assert( JET_coltypNil != pfieldTagged->coltyp
				|| !FCOLUMNIDTemplateColumn( columnid ) );
			if ( JET_coltypNil != pfieldTagged->coltyp )
				{
				const FID	fidSrc					= ptagfld->Fid();

				Assert( itagfldToCopy < cColumnsToCopy );

				if ( !FCOLUMNIDTemplateColumn( columnid ) )
					{
					Assert( FCOLUMNIDTagged( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] ) );
					Assert( !FCOLUMNIDTemplateColumn( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] ) );
					Assert( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] <= pfucbDest->u.pfcb->Ptdb()->FidTaggedLast() );
					Assert( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] <= fidSrc );
					Assert( !ptagfld->FDerived() );

					//	hit the non-derived columns, so should be no more derived columns left
					break;
					}

				Assert( pfucbSrc->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast()
					== pfucbDest->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast() );
				Assert( fidSrc <= pfucbDest->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast() );

				//	ignore ESE98 derived columns
				if ( !ptagfld->FDerived() )
					{
					// If column belongs to base table, then FID will not have changed,
					// since base table's DDL is fixed.  Thus, we don't have to bother
					// updating the FID in the destination record.
					new( rgtagfldDest + itagfldToCopy ) TAGFLD( fidSrc, fTrue );
					Assert( rgtagfldDest[itagfldToCopy].FDerived() );
					rgtagfldDest[itagfldToCopy].SetIb( ibDataDest ); 

					if ( ptagfld->FNull() )
						{
						rgtagfldDest[itagfldToCopy].SetFNull();
						}
					else
						{
						if ( ptagfld->FExtendedInfo() )
							{
							rgtagfldDest[itagfldToCopy].SetFExtendedInfo();
							}

						const ULONG		cbData		= CbData( itagfld );
						UtilMemCpy(
							(BYTE *)rgtagfldDest + 	ibDataDest,
							PbData( itagfld ),
							cbData );

						ibDataDest = USHORT( ibDataDest + cbData );
						}

					itagfldToCopy++;
					}
				}
			}

		Assert( itagfldToCopy <= cColumnsToCopy );
		}

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD	* const ptagfld			= Ptagfld( itagfld );
		const COLUMNID	columnid				= ptagfld->Columnid( ptdbSrc );
		const FIELD		* const pfieldTagged	= ptdbSrc->PfieldTagged( columnid );

		Assert( JET_coltypNil != pfieldTagged->coltyp
			|| !FCOLUMNIDTemplateColumn( columnid ) );
		if ( JET_coltypNil != pfieldTagged->coltyp )
			{
			const FID	fidSrc					= ptagfld->Fid();
			FID			fidDest;
			BOOL		fDerivedDest			= fFalse;

			Assert( itagfldToCopy <= cColumnsToCopy );

			if ( !FCOLUMNIDTemplateColumn( columnid ) )
				{
				Assert( FCOLUMNIDTagged( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] ) );
				Assert( !FCOLUMNIDTemplateColumn( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] ) );
				Assert( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] <= pfucbDest->u.pfcb->Ptdb()->FidTaggedLast() );
				Assert( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] <= fidSrc );
				fidDest = FidOfColumnid( mpcolumnidcolumnidTagged[fidSrc-fidTaggedLeast] );

				Assert( !ptagfld->FDerived() );
				}
			else
				{
				if ( ptdbSrc->FTemplateTable() )
					{
					ptdbSrc->AssertValidTemplateTable();
					Assert( !ptagfld->FDerived() );
					Assert( !ptdbSrc->FESE97DerivedTable() );
					}
				else
					{
					ptdbSrc->AssertValidDerivedTable();
					Assert( pfucbSrc->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast()
						== pfucbDest->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast() );
					Assert( ptagfld->FDerived() || ptdbSrc->FESE97DerivedTable() );
					Assert( fidSrc <= pfucbDest->u.pfcb->Ptdb()->PfcbTemplateTable()->Ptdb()->FidTaggedLast() );

					if ( !ptagfld->FDerived() )
						{
						Assert( ptdbSrc->FESE97DerivedTable() );
						if ( fNeedSeparatePassForESE97DerivedColumns )
							{
							//	ESE97 derived columns were copied in the previous pass
							continue;
							}
						}

					fDerivedDest = fTrue;
					}

				// If column belongs to base table, then FID will not have changed,
				// since base table's DDL is fixed.  Thus, we don't have to bother
				// updating the FID in the destination record.
				fidDest = fidSrc;
				}

			new( rgtagfldDest + itagfldToCopy ) TAGFLD( fidDest, fDerivedDest );
			rgtagfldDest[itagfldToCopy].SetIb( ibDataDest ); 

			if ( ptagfld->FNull() )
				{
				rgtagfldDest[itagfldToCopy].SetFNull();
				}
			else
				{
				if ( ptagfld->FExtendedInfo() )
					{
					rgtagfldDest[itagfldToCopy].SetFExtendedInfo();
					}

				const ULONG		cbData		= CbData( itagfld );
				UtilMemCpy(
					(BYTE *)rgtagfldDest + ibDataDest,
					PbData( itagfld ),
					cbData );

				ibDataDest = USHORT( ibDataDest + cbData );
				}

			Assert( itagfldToCopy < cColumnsToCopy );
			itagfldToCopy++;
			}
		}

	Assert( itagfldToCopy == cColumnsToCopy );
	pfucbDest->dataWorkBuf.DeltaCb( ibDataDest );

	Assert( pfucbDest->dataWorkBuf.Cb() >= ibRECStartFixedColumns );
	Assert( pfucbDest->dataWorkBuf.Cb() <= pfucbSrc->kdfCurr.data.Cb() );
	}


ERR TAGFIELDS::ErrUpdateSeparatedLongValuesAfterCopy(
	FUCB		* const pfucbSrc,
	FUCB		* const pfucbDest,
	JET_COLUMNID* const mpcolumnidcolumnidTagged,
	STATUSINFO	* const pstatus )
	{
	ERR			err;
	ULONG		itagfld;

	TDB			* const ptdbDest	= pfucbDest->u.pfcb->Ptdb();
	Assert( ptdbNil != ptdbDest );
	AssertValid( ptdbDest );
	
	Assert( !Pcsr( pfucbSrc )->FLatched() );
	Assert( !Pcsr( pfucbDest )->FLatched() );

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		TAGFLD_HEADER		* const pheader			= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FColumnCanBeSeparated() )
			{
			const COLUMNID		columnidCurr		= Ptagfld( itagfld )->Columnid( ptdbDest );
			const BOOL			fSLV				= pheader->FSLV() && rgfmp[ pfucbDest->ifmp ].FDefragSLVCopy();
			Assert(	FRECLongValue( ptdbDest->PfieldTagged( columnidCurr )->coltyp ) || FRECSLV( ptdbDest->PfieldTagged( columnidCurr )->coltyp ) );
			Assert( !pheader->FTwoValues() );
			if ( pheader->FMultiValues() )
				{
#ifdef UNLIMITED_MULTIVALUES
#else
				Assert( !pheader->FSeparated() );
#endif						

				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				ULONG			imv;

				for ( imv = 0; imv < mv.CMultiValues(); imv++ )
					{
					const BOOL	fSeparatedLV	= mv.FSeparatedInstance( imv );
					if ( fSLV )
						{
						if ( fSeparatedLV )
							{
							Assert( sizeof(LID) == mv.CbData( imv ) );
							mv.ResetFSeparatedInstance( imv );
							}
						JET_COLUMNID columnid = columnidCurr - fidTaggedLeast;
						if ( mpcolumnidcolumnidTagged[columnid] != columnidCurr )
							{	
							FID fidTaggedHighest = pfucbSrc->u.pfcb->Ptdb()->FidTaggedLast();
							for ( columnid++; columnid <= fidTaggedHighest + 1 - fidTaggedLeast; columnid++ )
								{
								if ( mpcolumnidcolumnidTagged[columnid] == columnidCurr )
									{
									break;
									}
								}
							Assert( columnid <= fidTaggedHighest + 1 - fidTaggedLeast );
							}
						Assert( columnidCurr >= fidTaggedLeast );
						DATA dataNil;
						dataNil.Nullify();
						CallR( ErrRECSetLongField( pfucbDest, columnidCurr, imv+1, &dataNil, JET_bitSetSLVFromSLVInfo | JET_bitSetOverwriteLV | JET_bitSetSizeLV ) );
						CallR( ErrSLVCopyUsingData( pfucbSrc, columnid + fidTaggedLeast, imv+1, pfucbDest, columnidCurr, imv+1 ) );
						m_cbTaggedColumns = ULONG( (BYTE *)pfucbDest->dataWorkBuf.Pv() + pfucbDest->dataWorkBuf.Cb() - ((REC *)pfucbDest->dataWorkBuf.Pv())->PbTaggedData() );
						}
					else if ( fSeparatedLV )
						{
						Assert( sizeof(LID) == mv.CbData( imv ) );
						BYTE 		* const pbLid	= mv.PbData( imv );
						const LID	lidSrc			= LidOfSeparatedLV( pbLid );
						LID			lidDest;

						CallR( ErrSORTIncrementLVRefcountDest(
									pfucbSrc->ppib,
									lidSrc,
									&lidDest ) );
						LVSetLidInRecord( pbLid, lidDest );
						}
					else if ( NULL != pstatus )
						{
						pstatus->cbRawData += mv.CbData( imv );
						}
					}
				}
			else
				{
				const BOOL	fSeparatedLV	= pheader->FSeparated();
				if ( fSLV )
					{
					if ( fSeparatedLV )
						{
						Assert( sizeof(LID) == CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
						pheader->ResetFSeparated();
						}
					JET_COLUMNID columnid = columnidCurr - fidTaggedLeast;
					if ( mpcolumnidcolumnidTagged[columnid] != columnidCurr )
						{	
						FID fidTaggedHighest = pfucbSrc->u.pfcb->Ptdb()->FidTaggedLast();
						for ( columnid++; columnid <= fidTaggedHighest + 1 - fidTaggedLeast; columnid++ )
							{
							if ( mpcolumnidcolumnidTagged[columnid] == columnidCurr )
								{
								break;
								}
							}
						Assert( columnid <= fidTaggedHighest + 1 - fidTaggedLeast );
						}
					Assert( columnidCurr >= fidTaggedLeast );
					DATA dataNil;
					dataNil.Nullify();
					CallR( ErrRECSetLongField( pfucbDest, columnidCurr, 1, &dataNil, JET_bitSetSLVFromSLVInfo | JET_bitSetOverwriteLV | JET_bitSetSizeLV ) );
					CallR( ErrSLVCopyUsingData( pfucbSrc, columnid + fidTaggedLeast, 1, pfucbDest, columnidCurr, 1 ) );
					m_cbTaggedColumns = ULONG( (BYTE *)pfucbDest->dataWorkBuf.Pv() + pfucbDest->dataWorkBuf.Cb() - ((REC *)pfucbDest->dataWorkBuf.Pv())->PbTaggedData() );
					}
				else if ( fSeparatedLV )
					{
					Assert( sizeof(LID) == CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
					BYTE 		* const pbLid	= PbData( itagfld ) + sizeof(TAGFLD_HEADER);
					const LID	lidSrc			= LidOfSeparatedLV( pbLid );
					LID			lidDest;

					CallR( ErrSORTIncrementLVRefcountDest(
								pfucbSrc->ppib,
								lidSrc,
								&lidDest ) );
					LVSetLidInRecord( pbLid, lidDest );
					}
				else if ( NULL != pstatus )
					{
					pstatus->cbRawData += CbData( itagfld ) - sizeof(TAGFLD_HEADER);
					}
				}
			}
		
		else if ( NULL != pstatus )
			{
			pstatus->cbRawData +=
						CbData( itagfld )
						- ( NULL != pheader ? sizeof(TAGFLD_HEADER) : 0 );
			}
		}

#ifdef DEBUG
	AssertValid( ptdbDest );
#endif
	
	return JET_errSuccess;
	}


#ifdef DISABLE_SLV
#else

ERR TAGFIELDS::ErrCopySLVColumns(
	FUCB		* const pfucb )
	{
	ERR			err;
	ULONG		itagfld;

	Assert( ptdbNil != pfucb->u.pfcb->Ptdb() );
	AssertValid( pfucb->u.pfcb->Ptdb() );
	Assert( Pcsr( pfucb )->FLatched() );

	//	we start with the page latched (though we may not end up that way)

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		//  reacquire our latch if we lost it
		if ( !Pcsr( pfucb )->FLatched() )
			{
			CallR( ErrDIRGet( pfucb ) );
			Refresh( pfucb->kdfCurr.data );
			}

		const TAGFLD_HEADER	* const pheader		= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FSLV() )
			{
			const COLUMNID		columnidCurr		= Ptagfld( itagfld )->Columnid( pfucb->u.pfcb->Ptdb() );
			ULONG				cMultiValues		= 1;

			Assert( !pheader->FTwoValues() );
			if ( pheader->FMultiValues() )
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				cMultiValues = mv.CMultiValues();
				Assert( cMultiValues > 1 );
				}
			else
				{
				cMultiValues = 1;
				}

			Assert( Pcsr( pfucb )->FLatched() );
			CallR( ErrDIRRelease( pfucb ) );

			err = ErrRECIAccessColumn( pfucb, columnidCurr );
			if ( err < 0 )
				{
				// ignore column if not visible, exit if other error
				if ( JET_errColumnNotFound != err )
					return err;
				}
			else
				{
				Assert( !Pcsr( pfucb )->FLatched() );

				for ( ULONG itagSequenceCurr = 1; itagSequenceCurr <= cMultiValues; itagSequenceCurr++ )
					{
					CallR( ErrSLVCopy( pfucb, columnidCurr, itagSequenceCurr ) );
					}

				//  remember that we have operations to rollback on JET_prepCancel
				FUCBSetUpdateSeparateLV( pfucb );
				}
			}
		}

	return JET_errSuccess;
	}

//	UNDONE: Move this function to SLV.CXX
LOCAL ERR ErrSLVUpdateOwnerMapForColumnInstance(
	FUCB			* const pfucb,
	const COLUMNID	columnid,
	const ULONG		itagSequence,
	BOOKMARK		* const pbmInsert,
	DATA			* const pdata,
	const BOOL		fSeparatedSLV )
	{
	ERR				err;
	CSLVInfo 		slvinfo;

	CallR( slvinfo.ErrLoad(
				pfucb,
				columnid,
				itagSequence,
				fTrue,
				pdata,
				fSeparatedSLV 
				) );

	CallS( slvinfo.ErrMoveBeforeFirst() );
	
	err = slvinfo.ErrMoveNext();
	while ( JET_errSuccess <= err )
		{
		CSLVInfo::RUN	run;
		
		Call( slvinfo.ErrGetCurrentRun( &run ) );
		Call( ErrSLVOwnerMapSetUsageRange(
					pfucb->ppib,
					pfucb->ifmp,
					run.PgnoFirst(),
					run.Cpg(),
					pfucb->u.pfcb->ObjidFDP(),
					columnid,
					pbmInsert,
					fSLVOWNERMAPSetRECInsert,
					FFUCBUpdateForInsertCopyDeleteOriginal( pfucb ) // force update the new owner
					) );
			
		err = slvinfo.ErrMoveNext();
		}
	Assert ( JET_errNoCurrentRecord == err );
	err = JET_errSuccess;
		
HandleError:
	slvinfo.Unload();

	return err;
	}

ERR TAGFIELDS::ErrUpdateSLVOwnerMapForRecordInsert(
	FUCB		* const pfucb,
	BOOKMARK&	bmInsert )
	{
	ERR			err					= JET_errSuccess;
	const TDB	* const ptdb		= pfucb->u.pfcb->Ptdb();
	ULONG		itagfld;

	Assert( pfucb->u.pfcb->FPrimaryIndex() );
	Assert( pfucb->u.pfcb->FTypeTable() );
	AssertValid( ptdb );

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD_HEADER	* const pheader	= Pheader( itagfld );

		if ( NULL != pheader
			&& pheader->FSLV() )
			{
			const COLUMNID	columnidCurr	= Ptagfld( itagfld )->Columnid( ptdb );
			DATA			data;

			Assert( !pheader->FTwoValues() );
			if ( pheader->FMultiValues() )
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				ULONG			itagSequenceCurr;

#ifdef UNLIMITED_MULTIVALUES
#else
				Assert( !pheader->FSeparated() );
#endif

				for ( itagSequenceCurr = 1; itagSequenceCurr <= mv.CMultiValues(); itagSequenceCurr++ )
					{
					const ULONG	imv		= itagSequenceCurr - 1;
					data.SetPv( mv.PbData( imv ) );
					data.SetCb( mv.CbData( imv ) );
					Call( ErrSLVUpdateOwnerMapForColumnInstance(
								pfucb,
								columnidCurr,
								itagSequenceCurr,
								&bmInsert,
								&data,
								mv.FSeparatedInstance( imv ) ) );
					}
				}
			else
				{
				data.SetPv( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
				data.SetCb( CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
				Call( ErrSLVUpdateOwnerMapForColumnInstance(
						pfucb,
						columnidCurr,
						1,
						&bmInsert,
						&data,
						pheader->FSeparated() ) );
				}
			}
		}

HandleError:
	return err;
	}

#endif	//	DISABLE_SLV


#ifdef DEAD_CODE	//	DBUTLDumpRec() now has its own code to dump tagged columns

VOID TAGFIELDS::Dump(
	const CPRINTF	* const pcprintf,
	CHAR			* const szBuf,
	const ULONG		cbWidth )
	{
	ULONG	itagfld;
	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD	* const ptagfld		= Ptagfld( itagfld );

		(*pcprintf)( "%d:\tFlags: ", ptagfld->Fid() );
		if ( ptagfld->FDerived() )
			{
			(*pcprintf)( "FDerived " );
			}
		if ( Ptagfld( itagfld )->FNull() )
			{
			(*pcprintf)( "FNull\n" );
			}
		else
			{
			const TAGFLD_HEADER	* const pheader	= Pheader( itagfld );
			const BYTE			* const pbData	= PbData( itagfld );
			const ULONG			cbData			= CbData( itagfld );

			if ( NULL != pheader )
				{
				if ( pheader->FLongValue() )
					(*pcprintf)( "FLongValue " );
				
				if ( pheader->FSLV() )
					(*pcprintf)( "FSLV " );

				if ( pheader->FSeparated() )
					(*pcprintf)( "FSeparated " );

				if ( pheader->FMultiValues() )
					(*pcprintf)( "FMultiValues " );

				if ( pheader->FTwoValues() )
					(*pcprintf)( "FTwoValues " );
				}

			(*pcprintf)( "\n\tData: %d bytes\n\t      ", cbData );

			//	UNDONE_SORTED_TAGGED_COLUMNS: properly output individual tagged data
			DBUTLSprintHex( szBuf, pbData, cbData, cbWidth );
			(*pcprintf)( "%s\n", szBuf );
			}
		}
	}

#endif	//	DEAD_CODE


ERR TAGFIELDS::ErrCheckLongValuesAndSLVs(
	const KEYDATAFLAGS&	kdf,
	RECCHECKTABLE		* const precchecktable )
	{
#ifdef MINIMAL_FUNCTIONALITY
	return ErrERRCheck( JET_wrnNyi );
#else
	Assert( NULL != precchecktable );

	ERR		err;
	ULONG	itagfld;
	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );
		if ( NULL != pheader
			&& pheader->FColumnCanBeSeparated() )
			{
			const TAGFLD	* const ptagfld		= Ptagfld( itagfld );
			const COLUMNID	columnidCurr		= ColumnidOfFid(
														ptagfld->Fid(),
														ptagfld->FDerived() );
			DATA			dataT;

			Assert( !pheader->FTwoValues() );
			if ( pheader->FMultiValues() )
				{
				MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
				ULONG			imv;

				for ( imv = 0; imv < mv.CMultiValues(); imv++ )
					{
					dataT.SetPv( mv.PbData( imv ) );
					dataT.SetCb( mv.CbData( imv ) );
					if ( pheader->FLongValue() )
						{
						CallR( precchecktable->ErrCheckLV(
								kdf,
								columnidCurr,
								imv+1,
								dataT,
								mv.FSeparatedInstance( imv ) ) );
						}
					else if ( pheader->FSLV() )
						{
#ifdef DISABLE_SLV
						CallR( ErrERRCheck( JET_wrnNyi ) );
#else
						CallR( precchecktable->ErrCheckLV(
								kdf,
								columnidCurr,
								imv+1,
								dataT,
								mv.FSeparatedInstance( imv ) ) );
						
						CallR( precchecktable->ErrCheckSLV(
								kdf,
								columnidCurr,
								imv+1,
								dataT,
								mv.FSeparatedInstance( imv ) ) );
#endif	//	DISABLE_SLV
						}
					}
				}
			else if ( pheader->FLongValue() )
				{
				dataT.SetPv( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
				dataT.SetCb( CbData( itagfld ) - sizeof(TAGFLD_HEADER) );
				CallR( precchecktable->ErrCheckLV(
						kdf,
						columnidCurr,
						1,
						dataT,
						pheader->FSeparated() ) );
				}
			else if ( pheader->FSLV() )
				{
#ifdef DISABLE_SLV
				CallR( ErrERRCheck( JET_wrnNyi ) );
#else
				dataT.SetPv( PbData( itagfld ) + sizeof(TAGFLD_HEADER) );
				dataT.SetCb( CbData( itagfld ) - sizeof(TAGFLD_HEADER) );

				CallR( precchecktable->ErrCheckLV(
						kdf,
						columnidCurr,
						1,
						dataT,
						pheader->FSeparated() ) );
				
				CallR( precchecktable->ErrCheckSLV(
						kdf,
						columnidCurr,
						1,
						dataT,
						pheader->FSeparated() ) );
#endif	//	DISABLE_SLV
				}
			else
				{
				//	should be either LV or SLV
				Assert( fFalse );
				}
			}
		else
			{
#ifdef UNLIMITED_MULTIVALUES
#else
			Assert( NULL == pheader
				|| !pheader->FSeparated() );
#endif				
			}
		}

	return JET_errSuccess;

#endif	//	MINIMAL_FUNCTIONALITY

	}

BOOL TAGFIELDS::FIsValidTwoValues(
	const ULONG				itagfld,
	const CPRINTF			* const pcprintf ) const
	{
	const TAGFLD_HEADER		* const pheader			= (TAGFLD_HEADER *)PbData( itagfld );

	Assert( NULL != pheader );
	Assert( pheader->FMultiValues() );
	Assert( pheader->FTwoValues() );
	Assert( !pheader->FSeparated() );
	Assert( !pheader->FColumnCanBeSeparated() );

	if ( CbData( itagfld ) < sizeof(TAGFLD_HEADER) + sizeof(TWOVALUES::TVLENGTH) )
		{
		(*pcprintf)( "Column is too small to contain TwoValues.\r\n" );
		AssertSz( fFalse, "Column is too small to contain TwoValues." );
		return fFalse;
		}

	if ( CbData( itagfld ) > sizeof(TAGFLD_HEADER) + sizeof(TWOVALUES::TVLENGTH) + ( 2 * JET_cbColumnMost ) )
		{
		(*pcprintf)( "Column is larger than maximum possible size for TWOVALUES.\r\n" );
		AssertSz( fFalse, "Column is larger than maximum possible size for TWOVALUES." );
		return fFalse;
		}

	const ULONG						cbTwoValues			= CbData( itagfld ) - sizeof(TAGFLD_HEADER ) - sizeof(TWOVALUES::TVLENGTH );
	const TWOVALUES::TVLENGTH		cbFirstValue		= *(TWOVALUES::TVLENGTH *)( pheader + 1 );
	if ( cbFirstValue > cbTwoValues )
		{
		(*pcprintf)( "First TWOVALUE is too long.\r\n" );
		AssertSz( fFalse, "First TWOVALUE is too long." );
		return fFalse;
		}

	if ( cbTwoValues - cbFirstValue > JET_cbColumnMost )
		{
		(*pcprintf)( "Column is greater than 255 bytes, but is not a LongValue or SLV column.\r\n" );
		AssertSz( fFalse, "Column is greater than 255 bytes, but is not a LongValue or SLV column." );
		return fFalse;
		}

	return fTrue;
	}

BOOL MULTIVALUES::FValidate(
	const CPRINTF	* const pcprintf ) const
	{
	const BOOL		fLongValue				= ( Pheader()->FLongValue() || Pheader()->FSLV() );
	BOOL			fPrevWasSeparated		= fFalse;
	ULONG			ibPrev					= 0;
	ULONG			imv;
	
	for ( imv = 0; imv < CMultiValues(); imv++ )
		{
		const ULONG		ibCurr		= Ib( imv );
		if ( ibCurr < ibPrev
			|| ibCurr > CbMultiValues() )
			{
			(*pcprintf)( "MULTIVALUE either overlaps previous MULTIVALUE or is out of TAGFLD range.\r\n" );
			AssertSz( fFalse, "MULTIVALUE either overlaps previous MULTIVALUE or is out of TAGFLD range." );
			return fFalse;
			}

		if ( !fLongValue )
			{
			if ( imv == CMultiValues() - 1 )
				{
				if ( CbMultiValues() - ibCurr > JET_cbColumnMost )
					{
					(*pcprintf)( "Column is greater than 255 bytes, but is not a LongValue or SLV column.\r\n" );
					AssertSz( fFalse, "Column is greater than 255 bytes, but is not a LongValue or SLV column." );
					return fFalse;
					}
				}
			else if ( imv > 0 )
				{
				if ( ibCurr - ibPrev > JET_cbColumnMost )
					{
					(*pcprintf)( "Column is greater than 255 bytes, but is not a LongValue or SLV column.\r\n" );
					AssertSz( fFalse, "Column is greater than 255 bytes, but is not a LongValue or SLV column." );
					return fFalse;
					}
				}
			}

		if ( fPrevWasSeparated )
			{
			if ( ibCurr - ibPrev != sizeof(LID) )
				{
				(*pcprintf)( "Separated column has invalid LID.\r\n" );
				AssertSz( fFalse, "Separated column has invalid LID." );
				return fFalse;
				}
			fPrevWasSeparated = fFalse;
			}

		ibPrev = ibCurr;				//	save off ib for next iteration

		if ( FSeparatedInstance( imv ) )
			{
			if ( !Pheader()->FColumnCanBeSeparated() )
				{
				(*pcprintf)( "Separated column is not a LongValue or an SLV.\r\n" );
				AssertSz( fFalse, "Separated column is not a LongValue or an SLV." );
				return fFalse;
				}

			if ( imv == CMultiValues() - 1
				&& CbMultiValues() - ibCurr != sizeof(LID) )
				{
				(*pcprintf)( "Separated column has invalid LID.\r\n" );
				AssertSz( fFalse, "Separated column has invalid LID." );
				return fFalse;
				}

			fPrevWasSeparated = fTrue;
			}
		}

	return fTrue;
	}

BOOL TAGFIELDS::FIsValidMultiValues(
	const ULONG			itagfld,
	const CPRINTF		* const pcprintf ) const
	{
	const TAGFLD_HEADER	* const pheader			= (TAGFLD_HEADER *)PbData( itagfld );

	Assert( NULL != pheader );
	Assert( pheader->FMultiValues() );
	Assert( !pheader->FTwoValues() );

#ifdef UNLIMITED_MULTIVALUES
#else
	Assert( !pheader->FSeparated() );
#endif


	if ( CbData( itagfld ) < sizeof(TAGFLD_HEADER) + ( 2 * sizeof(MULTIVALUES::MVOFFSET) ) )
		{
		(*pcprintf)( "Column is too small to contain MultiValues.\r\n" );
		AssertSz( fFalse, "Column is too small to contain MultiValues." );
		return fFalse;
		}

	const MULTIVALUES::MVOFFSET		* const rgmvoffs	= (MULTIVALUES::MVOFFSET *)( pheader + 1 );
	const ULONG						cbMultiValues		= CbData( itagfld ) - sizeof(TAGFLD_HEADER);
	const ULONG						ibFirstMV			= ( rgmvoffs[0] & MULTIVALUES::maskIb );
	if ( ibFirstMV < 2 * sizeof(MULTIVALUES::MVOFFSET)
		|| ibFirstMV > cbMultiValues
		|| ibFirstMV % sizeof(MULTIVALUES::MVOFFSET) != 0 )
		{
		(*pcprintf)( "First MULTIVALUE has invalid Ib.\r\n" );
		AssertSz( fFalse, "First MULTIVALUE has invalid Ib." );
		return fFalse;
		}

	MULTIVALUES		mv( PbData( itagfld ), CbData( itagfld ) );
	return mv.FValidate( pcprintf );
	}

BOOL TAGFIELDS::FValidate(
	const CPRINTF	* const pcprintf ) const
	{
	BOOL			fSawNonDerived		= fFalse;
	FID				fidPrev				= 0;
	USHORT			ibPrev				= 0;
 	BOOL			fPrevWasNull		= fFalse;
 	BOOL			fPrevWasSeparated	= fFalse;
 	BOOL			fPrevWasTwoValues	= fFalse;
 	BOOL			fPrevWasMultiValues	= fFalse;
 	BOOL			fPrevWasLongValue	= fFalse;
	ULONG			itagfld;

	for ( itagfld = 0; itagfld < CTaggedColumns(); itagfld++ )
		{
		const TAGFLD	* const ptagfld		= Ptagfld( itagfld );

		if ( !FTaggedFid( ptagfld->Fid() ) )
			{
			(*pcprintf)( "FID %d is not a tagged column.\r\n", ptagfld->Fid() );
			AssertSz( fFalse, "FID is not a tagged column." );
			return fFalse;
			}

		if ( ptagfld->FDerived() )
			{
			if ( fSawNonDerived )
				{
				//	all derived columns must come first
				(*pcprintf)( "Derived/NonDerived columns out of order.\r\n" );
				AssertSz( fFalse, "Derived/NonDerived columns out of order." );
				return fFalse;
				}
			if ( ptagfld->Fid() <= fidPrev )
				{
				//	FIDs must be monotonically increasing
				(*pcprintf)( "Columns are not in monotonically-increasing FID order (FID %d <= FID %d).\r\n", ptagfld->Fid(), fidPrev );
				AssertSz( fFalse, "Columns are not in monotonically-increasing FID order." );
				return fFalse;
				}
			}
		else if ( fSawNonDerived )
			{
			if ( ptagfld->Fid() <= fidPrev )
				{
				//	FIDs must be monotonically increasing
				(*pcprintf)( "Columns are not in monotonically-increasing FID order (FID %d <= FID %d).\r\n", ptagfld->Fid(), fidPrev );
				AssertSz( fFalse, "Columns are not in monotonically-increasing FID order." );
				return fFalse;
				}
			}
		else
			{
			fSawNonDerived = fTrue;
			}
		fidPrev = ptagfld->Fid();			//	save off FID for next iteration

		if ( ptagfld->Ib() < ibPrev
			|| ptagfld->Ib() > CbTaggedColumns() )
			{
			(*pcprintf)( "TAGFLD %d either overlaps previous TAGFLD or is out of record range.\r\n", ptagfld->Fid() );
			AssertSz( fFalse, "TAGFLD either overlaps previous TAGFLD or is out of record range." );
			return fFalse;
			}

		if ( !fPrevWasLongValue )
			{
#ifdef UNLIMITED_MULTIVALUES
#else
			Assert( !fPrevWasSeparated );
#endif
			if ( !fPrevWasMultiValues
				&& !fPrevWasTwoValues
				&& 0 != ibPrev
				&& ptagfld->Ib() - ibPrev > JET_cbColumnMost )
				{
				(*pcprintf)( "Column %d is greater than 255 bytes, but is not a LongValue or SLV column.\r\n", ptagfld->Fid() );				
				AssertSz( fFalse, "Column is greater than 255 bytes, but is not a LongValue or SLV column." );
				return fFalse;
				}
			}


		//	if we needed to extract the length of the previous TAGFLD, we can
		//	now do it because we've validated the ib of this TAGFLD
		if ( fPrevWasNull )
			{
			Assert( !fPrevWasTwoValues );
			Assert( !fPrevWasMultiValues );
			Assert( !fPrevWasSeparated );
			Assert( !fPrevWasLongValue );
			if ( ptagfld->Ib() != ibPrev )
				{
				(*pcprintf)( "Previous TAGFLD was NULL but not zero-length.\r\n" );
				AssertSz( fFalse, "Previous TAGFLD was NULL but not zero-length." );
				return fFalse;
				}
			}

		else if ( fPrevWasTwoValues )
			{
			Assert( !fPrevWasNull );
			Assert( !fPrevWasMultiValues );
			Assert( !fPrevWasSeparated );
			Assert( !fPrevWasLongValue );
			Assert( itagfld > 0 );
			if ( !FIsValidTwoValues( itagfld-1, pcprintf ) )
				return fFalse;
			fPrevWasTwoValues = fFalse;
			}
		else if ( fPrevWasMultiValues )
			{
			Assert( !fPrevWasNull );
			Assert( !fPrevWasTwoValues );
			Assert( !fPrevWasSeparated );
			Assert( itagfld > 0 );
			if ( !FIsValidMultiValues( itagfld-1, pcprintf ) )
				return fFalse;
			fPrevWasMultiValues = fFalse;
			}
		else if ( fPrevWasSeparated )
			{
#ifdef UNLIMITED_MULTIVALUES
#else
			Assert( !fPrevWasNull );
			Assert( !fPrevWasTwoValues );
			Assert( !fPrevWasMultiValues );
			if ( ptagfld->Ib() - ibPrev != sizeof(TAGFLD_HEADER) + sizeof(LID) )
				{
				(*pcprintf)( "Separated column %d has invalid LID.\r\n", ptagfld->Fid() );
				AssertSz( fFalse, "Separated column has invalid LID." );
				return fFalse;
				}
#endif				
			fPrevWasSeparated = fFalse;		//	reset for next iteration
			}

		ibPrev = USHORT( ptagfld->Ib() );	//	save off ib for next iteration

		if ( ptagfld->FNull() )
			{
			if ( ptagfld->FExtendedInfo() )
				{
				//	these two bits are mutually exclusive
				(*pcprintf)( "TAGFLD %d has both NULL and ExtendedInfo flags set.\r\n", ptagfld->Fid() );
				AssertSz( fFalse, "TAGFLD has both NULL and ExtendedInfo flags set." );
				return fFalse;
				}

			if ( itagfld == CTaggedColumns() - 1
				&& ptagfld->Ib() != CbTaggedColumns() )
				{
				//	if last column is NULL, it must point to the end of the tagged data
				(*pcprintf)( "Last TAGFLD is NULL but does not point to the end of the tagged data.\r\n" );
				AssertSz( fFalse, "Last TAGFLD is NULL but does not point to the end of the tagged data." );
				return fFalse;
				}
			}
		fPrevWasNull = ptagfld->FNull();	//	save off for next iteration

		fPrevWasLongValue = fFalse;			//	reset for next iteration

		if ( ptagfld->FExtendedInfo() )
			{
			const TAGFLD_HEADER		* const pheader		= Pheader( itagfld );

			//	these are already checked, so just assert
			Assert( NULL != pheader );
			Assert( (BYTE *)pheader >= PbStartOfTaggedData() );
			Assert( (BYTE *)pheader <= PbStartOfTaggedData() + CbTaggedData() );

			if ( *(BYTE *)pheader & BYTE( ~TAGFLD_HEADER::maskFlags ) )
				{
				//	these bits should be unused
				(*pcprintf)( "TAGFLD header (%x) has invalid bits set.\r\n", *(BYTE*)pheader );
				AssertSz( fFalse, "TAGFLD header has invalid bits set." );
				return fFalse;
				}

			if ( pheader->FLongValue() || pheader->FSLV() )
				{
				if ( pheader->FLongValue() && pheader->FSLV() )
					{
					(*pcprintf)( "TAGFLD %d is marked as both a LongValue and an SLV.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "TAGFLD is marked as both a LongValue and an SLV." );
					return fFalse;
					}

				fPrevWasLongValue = fTrue;		//	save off for next iteration
				}

			else if ( !pheader->FMultiValues() )
				{
				if( pheader->FTwoValues() )
					{
					(*pcprintf)( "Column %d is not MultiValues but is TwoValues.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "Column is MultiValues and TwoValues." );
					}
					
				//	if MultiValues not set, no other reason for non-LV, non-SLV
				//	column to have a header byte
				(*pcprintf)( "Column %d has inappropriate header byte.\r\n", ptagfld->Fid() );
				AssertSz( fFalse, "Column has inappropriate header byte." );
				return fFalse;
				}

			if ( pheader->FTwoValues() )
				{
				if ( !pheader->FMultiValues() )
					{
					(*pcprintf)( "TAGFLD %d is marked as TwoValues but not MultiValues.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "TAGFLD is marked as TwoValues but not MultiValues." );
					return fFalse;
					}
				if ( pheader->FLongValue()
					|| pheader->FSLV()
					|| pheader->FSeparated() )	//	even with UNLIMITED_MULTIVALUES, we would make this a true MULTIVALUES before separating it
					{
					(*pcprintf)( "TAGFLD %d is marked as TwoValues but cannot be a LongValue, an SLV, or Separated.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "A TAGFLD marked as TwoValues cannot be a LongValue, an SLV, or Separated." );
					return fFalse;
					}

				if ( itagfld == CTaggedColumns() - 1
					&& !FIsValidTwoValues( itagfld, pcprintf ) )
					{
					return fFalse;
					}

				fPrevWasTwoValues = fTrue;
				}

			else if ( pheader->FMultiValues() )
				{
				if ( pheader->FSeparated() )
					{
#ifdef UNLIMITED_MULTIVALUES
					fPrevWasSeparated = fTrue;
#else
					(*pcprintf)( "Separated multi-value list not currently supported.\r\n" );
					AssertSz( fFalse, "Separated multi-value list not currently supported." );
					return fFalse;
#endif					
					}

				if ( itagfld == CTaggedColumns() - 1
					&& !FIsValidMultiValues( itagfld, pcprintf ) )
					{
					return fFalse;
					}

				fPrevWasMultiValues = fTrue;
				}

			else if ( pheader->FSeparated() )
				{
				if ( !pheader->FColumnCanBeSeparated() )
					{
					(*pcprintf)( "Separated column %d is not a LongValue or an SLV.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "Separated column is not a LongValue or an SLV." );
					return fFalse;
					}
				
				if ( itagfld == CTaggedColumns() - 1
					&& ptagfld->Ib() + sizeof(TAGFLD_HEADER) + sizeof(LID) != CbTaggedColumns() )
					{
					//	if last column is NULL, it must point to the end of the tagged data
					(*pcprintf)( "Separated column %d has invalid LID.\r\n", ptagfld->Fid() );
					AssertSz( fFalse, "Separated column has invalid LID." );
					return fFalse;
					}

				//	column length will be checked on next iteration
				fPrevWasSeparated = fTrue;
				}
			}

		else if ( itagfld == CTaggedColumns() - 1
			&& CbTaggedColumns() - ptagfld->Ib() > JET_cbColumnMost )
			{
			(*pcprintf)( "Column %d is greater than 255 bytes, but is not a LongValue or SLV column.\r\n", ptagfld->Fid() );
			AssertSz( fFalse, "Column is greater than 255 bytes, but is not a LongValue or SLV column." );
			return fFalse;
			}
		}

	return fTrue;
	}


BOOL TAGFIELDS::FIsValidTagfields(
	const DATA&		dataRec,
	const CPRINTF	* const pcprintf )
	{
	if ( NULL == dataRec.Pv()
		|| dataRec.Cb() < REC::cbRecordMin
		|| dataRec.Cb() > REC::CbRecordMax() )
		{
		(*pcprintf)( "Record is an invalid size.\r\n" );
		AssertSz( fGlobalRepair, "Record is an invalid size." );
		return fFalse;
		}

	const REC	* prec						= (REC *)dataRec.Pv();
	const BYTE	* pbRecMax					= (BYTE *)prec + dataRec.Cb();

	//	WARNING: PbTaggedData() could GPF if the record is messed up
	const BYTE	* pbStartOfTaggedColumns	= prec->PbTaggedData();

	if ( pbStartOfTaggedColumns < (BYTE *)dataRec.Pv() + REC::cbRecordMin
		|| pbStartOfTaggedColumns > pbRecMax )
		{
		(*pcprintf)( "Start of tagged columns is out of record range.\r\n" );
		AssertSz( fGlobalRepair, "Start of tagged columns is out of record range." );
		return fFalse;
		}

	const SIZE_T	cbTaggedColumns				= pbRecMax - pbStartOfTaggedColumns;
	if ( cbTaggedColumns > 0 )
		{
		//	there's at least some tagged data
		const TAGFLD	* const ptagfldFirst	= (TAGFLD *)pbStartOfTaggedColumns;

		if ( ptagfldFirst->Ib() < sizeof(TAGFLD)		//	must be at least one TAGFLD
			|| ptagfldFirst->Ib() > cbTaggedColumns
			|| ptagfldFirst->Ib() % sizeof(TAGFLD) != 0 )
			{
			(*pcprintf)( "First TAGFLD has an invalid Ib.\r\n" );
			AssertSz( fGlobalRepair, "First TAGFLD has an invalid Ib." );
			return fFalse;
			}
		}

	//	at this point, it should be safe to call the constructor
	TAGFIELDS	tagfields( dataRec );
	return tagfields.FValidate( pcprintf );
	}


//  ****************************************************************
//	TAGFLD_ITERATOR
//  ****************************************************************


TAGFLD_ITERATOR::TAGFLD_ITERATOR()
	{
	}

	
TAGFLD_ITERATOR::~TAGFLD_ITERATOR()
	{
	}


INT TAGFLD_ITERATOR::Ctags() const
	{
	return 0;
	}

	
ERR TAGFLD_ITERATOR::ErrSetItag( const INT itag )
	{
	return ErrERRCheck( JET_errNoCurrentRecord );
	}


VOID TAGFLD_ITERATOR::MoveBeforeFirst()
	{
	}

	
VOID TAGFLD_ITERATOR::MoveAfterLast()
	{
	}


ERR TAGFLD_ITERATOR::ErrMovePrev()
	{
	return ErrERRCheck( JET_errNoCurrentRecord );
	}

	
ERR TAGFLD_ITERATOR::ErrMoveNext()
	{
	return ErrERRCheck( JET_errNoCurrentRecord );
	}


INT TAGFLD_ITERATOR::Itag() const
	{
	Assert( fFalse );
	return 0;
	}
	

BOOL TAGFLD_ITERATOR::FSeparated() const
	{
	Assert( fFalse );
	return fFalse;
	}
	

INT TAGFLD_ITERATOR::CbData() const
	{
	Assert( fFalse );
	return 0;
	}
	

const BYTE * TAGFLD_ITERATOR::PbData() const
	{
	Assert( fFalse );
	return NULL;
	}


//  ****************************************************************
//	TAGFLD_ITERATOR_INVALID
//  ****************************************************************


class TAGFLD_ITERATOR_INVALID : public TAGFLD_ITERATOR
	{
	public:
		TAGFLD_ITERATOR_INVALID() {}
		~TAGFLD_ITERATOR_INVALID() {}
	};


//  ****************************************************************
//	TAGFLD_ITERATOR_NULLVALUE
//  ****************************************************************


class TAGFLD_ITERATOR_NULLVALUE : public TAGFLD_ITERATOR
	{
	public:
		TAGFLD_ITERATOR_NULLVALUE() {}
		~TAGFLD_ITERATOR_NULLVALUE() {}
	};


//  ****************************************************************
//	TAGFLD_ITERATOR_SINGLEVALUE
//  ****************************************************************


class TAGFLD_ITERATOR_SINGLEVALUE : public TAGFLD_ITERATOR
	{
	public:
		TAGFLD_ITERATOR_SINGLEVALUE( const DATA& data, const BOOL fSeparated );
		~TAGFLD_ITERATOR_SINGLEVALUE();

	public:
		VOID MoveBeforeFirst();
		VOID MoveAfterLast();

		ERR ErrMovePrev();
		ERR ErrMoveNext();

		INT Ctags() const;
		ERR ErrSetItag( const INT itag );

	public:
		INT Itag() const;
		BOOL FSeparated() const;
		INT CbData() const;
		const BYTE * PbData() const;

	private:
		const BOOL 			m_fSeparated;
		const INT 			m_cbData;
		const BYTE * const 	m_pbData;

		INT m_itag;	//	our current location
	};

INT 	TAGFLD_ITERATOR_SINGLEVALUE::Ctags() 		const { return 1; }
INT 	TAGFLD_ITERATOR_SINGLEVALUE::Itag() 		const { return ( 1 == m_itag ) ? 1 : 0; }
BOOL 	TAGFLD_ITERATOR_SINGLEVALUE::FSeparated() 	const { return ( 1 == m_itag ) ? m_fSeparated : fFalse; }
INT 	TAGFLD_ITERATOR_SINGLEVALUE::CbData() 		const { return ( 1 == m_itag ) ? m_cbData : 0; }

const BYTE * TAGFLD_ITERATOR_SINGLEVALUE::PbData() const { return m_pbData; }

TAGFLD_ITERATOR_SINGLEVALUE::TAGFLD_ITERATOR_SINGLEVALUE( const DATA& data, const BOOL fSeparated ) :
		m_fSeparated( fSeparated ),
		m_cbData( data.Cb() ),
		m_pbData( reinterpret_cast<BYTE *>( data.Pv() ) ),
		m_itag( 0 )
	{
	}			


TAGFLD_ITERATOR_SINGLEVALUE::~TAGFLD_ITERATOR_SINGLEVALUE()
	{
	}


ERR TAGFLD_ITERATOR_SINGLEVALUE::ErrSetItag( const INT itag )
	{
	if( 1 == itag )
		{
		m_itag = 1;
		return JET_errSuccess;
		}
	MoveBeforeFirst();
	return ErrERRCheck( JET_errNoCurrentRecord );
	}


VOID TAGFLD_ITERATOR_SINGLEVALUE::MoveBeforeFirst()
	{
	m_itag = 0;
	}

	
VOID TAGFLD_ITERATOR_SINGLEVALUE::MoveAfterLast()
	{
	m_itag = 2;
	}


ERR TAGFLD_ITERATOR_SINGLEVALUE::ErrMovePrev()
	{
	ERR	err;
	switch( m_itag )
		{
		case 2:
			m_itag = 1;
			err = JET_errSuccess;
			break;
		case 1:
		case 0:
			MoveBeforeFirst();
			err = ErrERRCheck( JET_errNoCurrentRecord );
			break;
		default:
			Assert( fFalse );
			err = ErrERRCheck( JET_errInternalError );
			break;
		}
	return err;
	}

	
ERR TAGFLD_ITERATOR_SINGLEVALUE::ErrMoveNext()
	{
	ERR	err;
	switch( m_itag )
		{
		case 0:
			m_itag = 1;
			err = JET_errSuccess;
			break;
		case 1:
		case 2:
			MoveAfterLast();
			err = ErrERRCheck( JET_errNoCurrentRecord );
			break;
		default:
			Assert( fFalse );
			err = ErrERRCheck( JET_errInternalError );
			break;
		}
	return err;
	}


//  ****************************************************************
//	TAGFLD_ITERATOR_TWOVALUES
//  ****************************************************************


class TAGFLD_ITERATOR_TWOVALUES : public TAGFLD_ITERATOR
	{
	public:
		TAGFLD_ITERATOR_TWOVALUES( const DATA& data );
		~TAGFLD_ITERATOR_TWOVALUES();

	public:
		VOID MoveBeforeFirst();
		VOID MoveAfterLast();

		ERR ErrMovePrev();
		ERR ErrMoveNext();

		INT Ctags() const;
		ERR ErrSetItag( const INT itag );

	public:
		INT Itag() const;
		BOOL FSeparated() const;
		INT CbData() const;
		const BYTE * PbData() const;

	private:

		const TWOVALUES m_twovalues;
		INT m_itag;	//	our current location
	};


TAGFLD_ITERATOR_TWOVALUES::TAGFLD_ITERATOR_TWOVALUES( const DATA& data ) :
	m_twovalues( reinterpret_cast<BYTE *>( data.Pv() ), data.Cb() ),
	m_itag( 0 )
	{
	}


TAGFLD_ITERATOR_TWOVALUES::~TAGFLD_ITERATOR_TWOVALUES()
	{
	}


INT TAGFLD_ITERATOR_TWOVALUES::Ctags() const
	{
	return 2;
	}

	
ERR TAGFLD_ITERATOR_TWOVALUES::ErrSetItag( const INT itag )
	{
	if( 1 == itag
		|| 2 == itag )
		{
		m_itag = 1;
		return JET_errSuccess;
		}
	MoveBeforeFirst();
	return ErrERRCheck( JET_errNoCurrentRecord );
	}


VOID TAGFLD_ITERATOR_TWOVALUES::MoveBeforeFirst()
	{
	m_itag = 0;
	}

	
VOID TAGFLD_ITERATOR_TWOVALUES::MoveAfterLast()
	{
	m_itag = 3;
	}


ERR TAGFLD_ITERATOR_TWOVALUES::ErrMovePrev()
	{
	ERR err;
	if( --m_itag < 1 )
		{
		MoveBeforeFirst();
		err = ErrERRCheck( JET_errNoCurrentRecord );
		}
	else
		{
		err = JET_errSuccess;
		}
	return err;
	}

	
ERR TAGFLD_ITERATOR_TWOVALUES::ErrMoveNext()
	{
	ERR err;
	if( ++m_itag > 2 )
		{
		MoveAfterLast();
		err = ErrERRCheck( JET_errNoCurrentRecord );
		}
	else
		{
		err = JET_errSuccess;
		}
	return err;
	}


INT TAGFLD_ITERATOR_TWOVALUES::Itag() const
	{
	if( 1 == m_itag || 2 == m_itag )
		{
		return m_itag;
		}
	return 0;
	}

	
BOOL TAGFLD_ITERATOR_TWOVALUES::FSeparated() const
	{
	//	TWOVALUES are never used for LV columns
	return fFalse;
	}

	
INT TAGFLD_ITERATOR_TWOVALUES::CbData() const
	{
	int cbData;
	switch( m_itag )
		{
		case 2:
			cbData = m_twovalues.CbSecondValue();
			break;
		case 1:
			cbData = m_twovalues.CbFirstValue();
			break;
		case 0:
			Assert( fFalse );
			cbData = 0;
			break;
		default:
			Assert( fFalse );
			cbData = 0xffffffff;
			break;
		}
	return cbData;
	}

	
const BYTE * TAGFLD_ITERATOR_TWOVALUES::PbData() const
	{
	const BYTE * pbData;
	switch( m_itag )
		{
		case 2:
			pbData = m_twovalues.PbData() + m_twovalues.CbFirstValue();
			break;
		case 1:
			pbData = m_twovalues.PbData();
			break;
		case 0:
			Assert( fFalse );
			pbData = 0;
			break;
		default:
			Assert( fFalse );
			pbData = (BYTE *)(~0);
			break;
		}
	return pbData;
	}


//  ****************************************************************
//	TAGFLD_ITERATOR_MULTIVALUES
//  ****************************************************************


class TAGFLD_ITERATOR_MULTIVALUES : public TAGFLD_ITERATOR
	{
	public:
		TAGFLD_ITERATOR_MULTIVALUES( const DATA& data );
		~TAGFLD_ITERATOR_MULTIVALUES();

	public:
		VOID MoveBeforeFirst();
		VOID MoveAfterLast();

		ERR ErrMovePrev();
		ERR ErrMoveNext();

		INT Ctags() const;
		ERR ErrSetItag( const INT itag );

	public:
		INT Itag() const;
		BOOL FSeparated() const;
		INT CbData() const;
		const BYTE * PbData() const;

	private:

		const MULTIVALUES m_multivalues;
		INT m_itag;	//	our current location
	};


TAGFLD_ITERATOR_MULTIVALUES::TAGFLD_ITERATOR_MULTIVALUES( const DATA& data ) :
	m_multivalues( reinterpret_cast<BYTE *>( data.Pv() ), data.Cb() ),
	m_itag( 0 )
	{
	}


TAGFLD_ITERATOR_MULTIVALUES::~TAGFLD_ITERATOR_MULTIVALUES()
	{
	}


INT TAGFLD_ITERATOR_MULTIVALUES::Ctags() const
	{
	return m_multivalues.CMultiValues();
	}

	
ERR TAGFLD_ITERATOR_MULTIVALUES::ErrSetItag( const INT itag )
	{
	if( itag > 0
		&& itag <= m_multivalues.CMultiValues() )
		{
		m_itag = itag;
		return JET_errSuccess;
		}
	MoveBeforeFirst();
	return ErrERRCheck( JET_errNoCurrentRecord );
	}


VOID TAGFLD_ITERATOR_MULTIVALUES::MoveBeforeFirst()
	{
	m_itag = 0;
	}

	
VOID TAGFLD_ITERATOR_MULTIVALUES::MoveAfterLast()
	{
	m_itag = m_multivalues.CMultiValues() + 1;
	}


ERR TAGFLD_ITERATOR_MULTIVALUES::ErrMovePrev()
	{
	ERR err;
	if( --m_itag < 1 )
		{
		MoveBeforeFirst();
		err = ErrERRCheck( JET_errNoCurrentRecord );
		}
	else
		{
		err = JET_errSuccess;
		}
	return err;
	}

	
ERR TAGFLD_ITERATOR_MULTIVALUES::ErrMoveNext()
	{
	ERR err;
	if( ++m_itag > m_multivalues.CMultiValues() )
		{
		MoveAfterLast();
		err = ErrERRCheck( JET_errNoCurrentRecord );
		}
	else
		{
		err = JET_errSuccess;
		}
	return err;
	}


INT TAGFLD_ITERATOR_MULTIVALUES::Itag() const
	{
	if( m_itag >= 1 && m_itag <= m_multivalues.CMultiValues() )
		{
		return m_itag;
		}
	return 0;
	}


BOOL TAGFLD_ITERATOR_MULTIVALUES::FSeparated() const
	{
	if( m_itag >= 1 && m_itag <= m_multivalues.CMultiValues() )
		{
		return m_multivalues.FSeparatedInstance( m_itag - 1 );
		}
	return 0;
	}

	
INT TAGFLD_ITERATOR_MULTIVALUES::CbData() const
	{
	if( m_itag >= 1 && m_itag <= m_multivalues.CMultiValues() )
		{
		return m_multivalues.CbData( m_itag - 1 );
		}
	return 0;
	}


const BYTE * TAGFLD_ITERATOR_MULTIVALUES::PbData() const
	{
	if( m_itag >= 1 && m_itag <= m_multivalues.CMultiValues() )
		{
		return m_multivalues.PbData( m_itag - 1 );
		}
	return 0;
	}


//  ****************************************************************
//	TAGFIELDS_ITERATOR
//  ****************************************************************


TAGFIELDS_ITERATOR::TAGFIELDS_ITERATOR( const DATA& dataRec ) :
	m_tagfields( dataRec ),
	m_ptagfldMic( m_tagfields.Rgtagfld() - 1 ),
	m_ptagfldMax( m_tagfields.Rgtagfld() + m_tagfields.CTaggedColumns() ),
	m_ptagfldCurr( m_ptagfldMic ),
	m_ptagflditerator( new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_INVALID )
	{
	}

	
TAGFIELDS_ITERATOR::~TAGFIELDS_ITERATOR()
	{
	}


#ifdef DEBUG

VOID TAGFIELDS_ITERATOR::AssertValid() const
	{
	Assert( m_ptagflditerator == (TAGFLD_ITERATOR *)m_rgbTagfldIteratorBuf );
	Assert( m_ptagfldCurr >= m_ptagfldMic );
	Assert( m_ptagfldCurr <= m_ptagfldMax );
	}

#endif

VOID TAGFIELDS_ITERATOR::MoveBeforeFirst()
	{
	m_ptagfldCurr = m_ptagfldMic;
	new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_INVALID; 
	}

	
VOID TAGFIELDS_ITERATOR::MoveAfterLast()
	{
	m_ptagfldCurr = m_ptagfldMax;
	new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_INVALID; 
	}


ERR TAGFIELDS_ITERATOR::ErrMovePrev()
	{
	if( --m_ptagfldCurr <= m_ptagfldMic )
		{
		MoveBeforeFirst();
		return ErrERRCheck( JET_errNoCurrentRecord );
		}
	CreateTagfldIterator_();
	return JET_errSuccess;
	}

	
ERR TAGFIELDS_ITERATOR::ErrMoveNext()
	{
	if( ++m_ptagfldCurr >= m_ptagfldMax )
		{
		MoveAfterLast();
		return ErrERRCheck( JET_errNoCurrentRecord );
		}
	CreateTagfldIterator_();
	return JET_errSuccess;
	}


FID TAGFIELDS_ITERATOR::Fid() const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return 0;
		}
	return m_ptagfldCurr->Fid();
	}


COLUMNID TAGFIELDS_ITERATOR::Columnid( const TDB * const ptdb ) const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return 0;
		}
	return m_ptagfldCurr->Columnid( ptdb );
	}


BOOL TAGFIELDS_ITERATOR::FTemplateColumn( const TDB * const ptdb ) const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return 0;
		}
	return m_ptagfldCurr->FTemplateColumn( ptdb );
	}


BOOL TAGFIELDS_ITERATOR::FNull() const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return fFalse;
		}
	return m_ptagfldCurr->FNull();
	}


BOOL TAGFIELDS_ITERATOR::FDerived() const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return fFalse;
		}
	return m_ptagfldCurr->FDerived();
	}

	
BOOL TAGFIELDS_ITERATOR::FLV() const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return fFalse;
		}
	if( !m_ptagfldCurr->FExtendedInfo() )
		{
		return fFalse;
		}
	const BYTE * const pbData = m_tagfields.PbTaggedColumns() + m_ptagfldCurr->Ib();
	const BYTE bExtendedInfo  = *pbData;
	return bExtendedInfo & TAGFLD_HEADER::fLongValue;
	}

	
BOOL TAGFIELDS_ITERATOR::FSLV() const
	{
	if( m_ptagfldCurr >= m_ptagfldMax || m_ptagfldCurr <= m_ptagfldMic )
		{
		Assert( fFalse );
		return fFalse;
		}
	if( !m_ptagfldCurr->FExtendedInfo() )
		{
		return fFalse;
		}
	const BYTE * const pbData = m_tagfields.PbTaggedColumns() + m_ptagfldCurr->Ib();
	const BYTE bExtendedInfo  = *pbData;
	return bExtendedInfo & TAGFLD_HEADER::fSLV;
	}


TAGFLD_ITERATOR& TAGFIELDS_ITERATOR::TagfldIterator()
	{
	return *m_ptagflditerator;
	}

	
const TAGFLD_ITERATOR& TAGFIELDS_ITERATOR::TagfldIterator() const
	{
	return *m_ptagflditerator;
	}


VOID TAGFIELDS_ITERATOR::CreateTagfldIterator_()
	{
	Assert( m_ptagfldCurr > m_ptagfldMic );
	Assert( m_ptagfldCurr < m_ptagfldMax );
	if( m_ptagfldCurr->FNull() )
		{
		Assert( sizeof( m_rgbTagfldIteratorBuf ) >= sizeof( TAGFLD_ITERATOR_NULLVALUE ) );
		new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_NULLVALUE; 
		}
	else
		{
		DATA data;
		
		const BYTE * const pbData 	= m_tagfields.PbTaggedColumns() + m_ptagfldCurr->Ib();
		const SIZE_T cbData 		= m_tagfields.CbData( ULONG( m_ptagfldCurr - m_ptagfldMic - 1 ) );

		data.SetPv( const_cast<BYTE *>( pbData ) );
		data.SetCb( cbData );

		if( !m_ptagfldCurr->FExtendedInfo() )
			{
			
			//	ordinary value

			Assert( sizeof( m_rgbTagfldIteratorBuf ) >= sizeof( TAGFLD_ITERATOR_SINGLEVALUE ) );
			new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_SINGLEVALUE( data, fFalse ); 
			
			}
		else
			{
			const BYTE bExtendedInfo  = *pbData;
			if( bExtendedInfo & TAGFLD_HEADER::fTwoValues )
				{
				Assert( sizeof( m_rgbTagfldIteratorBuf ) >= sizeof( TAGFLD_ITERATOR_TWOVALUES ) );
				new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_TWOVALUES( data ); 
				}
			else if( bExtendedInfo & TAGFLD_HEADER::fMultiValues )
				{
				Assert( sizeof( m_rgbTagfldIteratorBuf ) >= sizeof( TAGFLD_ITERATOR_MULTIVALUES ) );
				new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_MULTIVALUES( data ); 
				}
			else
				{

				//	ordinary column with header byte. skip the header byte
				
				const BOOL fSeparated = bExtendedInfo & TAGFLD_HEADER::fSeparated;
				data.DeltaPv( sizeof( TAGFLD_HEADER ) );
				data.DeltaCb( - (ULONG)sizeof( TAGFLD_HEADER ) );

				Assert( sizeof( m_rgbTagfldIteratorBuf ) >= sizeof( TAGFLD_ITERATOR_SINGLEVALUE ) );
				new( m_rgbTagfldIteratorBuf ) TAGFLD_ITERATOR_SINGLEVALUE( data, fSeparated ); 
				
				}				
			}
		}
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\tm.cxx ===
#include "std.hxx"

BOOL g_fSystemInit = fFalse;

//+api
//	ErrIsamBeginSession
//	========================================================
//	ERR ErrIsamBeginSession( PIB **pppib )
//
//	Begins a session with DAE.  Creates and initializes a PIB for the
//	user and returns a pointer to it.  Calls system initialization.
//
//	PARAMETERS	pppib			Address of a PIB pointer.  On return, *pppib
//		   						will point to the new PIB.
//
//	RETURNS		Error code, one of:
//					JET_errSuccess
//					JET_errTooManyActiveUsers
//
//	SEE ALSO		ErrIsamEndSession
//-
ERR ISAMAPI ErrIsamBeginSession( JET_INSTANCE inst, JET_SESID *psesid )
	{
	ERR		err;
	PIB		**pppib;
	INST 	*pinst = (INST *)inst; 

	Assert( psesid != NULL );
	Assert( sizeof(JET_SESID) == sizeof(PIB *) );
	pppib = (PIB **)psesid;

	//	alllocate process information block
	//
	Call( ErrPIBBeginSession( pinst, pppib, procidNil, fFalse ) );
	(*pppib)->grbitsCommitDefault = pinst->m_grbitsCommitDefault;    /* set default commit flags */
	(*pppib)->SetFUserSession();

HandleError:
	return err;
	}


//+api
//	ErrIsamEndSession
//	=========================================================
//	ERR ErrIsamEndSession( PIB *ppib, JET_GRBIT grbit )
//
//	Ends the session associated with a PIB.
//
//	PARAMETERS	ppib		Pointer to PIB for ending session.
//
//	RETURNS		JET_errSuccess
//
//	SIDE EFFECTS 
//		Rolls back all transaction levels active for this PIB.
//		Closes all FUCBs for files and sorts open for this PIB.
//
//	SEE ALSO 	ErrIsamBeginSession
//-
ERR ISAMAPI ErrIsamEndSession( JET_SESID sesid, JET_GRBIT grbit )
	{		
	ERR	 	err;
	PIB	 	*ppib	= (PIB *)sesid;
	
	CallR( ErrPIBCheck( ppib ) );

	//	lock session
	err = ErrPIBSetSessionContext( ppib, dwPIBSessionContextUnusable );
	if ( err < 0 )
		{
		if ( JET_errSessionContextAlreadySet == err )
			err = ErrERRCheck( JET_errSessionInUse );
		return err;
		}

	INST	*pinst	= PinstFromPpib( ppib );

	//	rollback all transactions
	//
	if ( ppib->level > 0 )
		{
		if ( ppib->FUseSessionContextForTrxContext() )
			{
			Assert( !pinst->FRecovering() );

			//	fake out TrxContext to allow us to rollback on behalf of another thread
			ppib->dwTrxContext = ppib->dwSessionContext;
			}
		else
			{
			//	not using SessionContext model, so if the transaction wasn't started by
			//	this thread, the rollback call below will err out with SessionSharingViolation
			}

		Assert( sizeof(JET_SESID) == sizeof(ppib) );
		Call( ErrIsamRollback( (JET_SESID)ppib, JET_bitRollbackAll ) );
		Assert( 0 == ppib->level );
		}

	//	close all databases for this PIB
	//
	Call( ErrDBCloseAllDBs( ppib ) );

	//	close all cursors still open
	//
	while( ppib->pfucbOfSession != pfucbNil )
		{
		FUCB	*pfucb	= ppib->pfucbOfSession;

		//	close materialized or unmaterialized temporary tables
		//
		if ( FFUCBSort( pfucb ) )
			{
			Assert( !( FFUCBIndex( pfucb ) ) );
			Call( ErrIsamSortClose( ppib, pfucb ) );
			}
		else if ( pinst->FRecovering() )
			{
			//  If the fucb is used for redo, then it is
			//  always being opened as a primary FUCB with default index.
			//  Use DIRClose to close such a fucb.
			DIRClose( pfucb );
			}
		else
			{
			//	should only be sort and temporary file cursors
			//	(cursors for user databases should already have
			//	been closed by the call above to ErrDBCloseAllDBs())
			//
			Assert( dbidTemp == rgfmp[pfucb->ifmp].Dbid() );

			while ( FFUCBSecondary( pfucb ) || FFUCBLongValue( pfucb ) )
				{
				pfucb = pfucb->pfucbNextOfSession;
				Assert( dbidTemp == rgfmp[pfucb->ifmp].Dbid() );

				//	a table cursor that owns this index/lv cursor must
				//	be ahead somewhere
				AssertRTL( pfucbNil != pfucb );
				}

			Assert( FFUCBIndex( pfucb ) );

			if ( pfucb->pvtfndef != &vtfndefInvalidTableid )
				{
				CallS( ErrDispCloseTable( (JET_SESID)ppib, (JET_TABLEID) pfucb ) );
				}
			else
				{
				//	Open internally, not exported to user
				//
				CallS( ErrFILECloseTable( ppib, pfucb ) );
				}
			}
		}
	Assert( pfucbNil == ppib->pfucbOfSession );

	PIBEndSession( ppib );

	return JET_errSuccess;

HandleError:
	Assert( err < 0 );

	//	could not properly terminate session, must leave it in usable state
	Assert( dwPIBSessionContextUnusable == ppib->dwSessionContext );
	PIBResetSessionContext( ppib );

	return err;
	}


ERR ISAMAPI ErrIsamIdle( JET_SESID sesid, JET_GRBIT grbit )
	{
	ERR		err = JET_errSuccess;

	if ( grbit & JET_bitIdleFlushBuffers )
		{
		//	flush some dirty buffers
		Call( ErrERRCheck( JET_errInvalidGrbit ) );
		}
	else if ( grbit & JET_bitIdleStatus )
		{
		//	return error code for status
		VER *pver = PverFromPpib( (PIB *) sesid );
		CallR( pver->ErrVERStatus() );
		}
#ifndef RTM
	else if( grbit & JET_bitIdleVersionStoreTest )
		{
		//	return error code for status
		VER *pver = PverFromPpib( (PIB *) sesid );
		CallR( pver->ErrInternalCheck() );
		}
#endif	//	!RTM
	else if ( 0 == grbit || JET_bitIdleCompact & grbit  )
		{
		//	clean all version buckets
		(void) PverFromPpib( (PIB *)sesid )->ErrVERRCEClean();
		err = ErrERRCheck( JET_wrnNoIdleActivity );
		}

HandleError:
	return err;
	}


ERR VTAPI ErrIsamCapability( JET_SESID vsesid,
	JET_DBID vdbid,
	ULONG ulArea,
	ULONG ulFunction,
	ULONG *pgrbitFeature )
	{
	ERR		err = JET_errSuccess;
	PIB		*ppib = (PIB *)vsesid;

	CallR( ErrPIBCheck( ppib ) );

	return ErrERRCheck( JET_errFeatureNotAvailable );
	}


BOOL fGlobalRepair				= fFalse;

BOOL g_fEnableIndexChecking		= fFalse;	
BOOL g_fEnableIndexCleanup		= fTrue;	

DWORD dwGlobalMajorVersion;
DWORD dwGlobalMinorVersion;
DWORD dwGlobalBuildNumber;
LONG lGlobalSPNumber;


//	Accumulated number of each instance

LONG	g_lSessionsMac = 0;
LONG	g_lVerPagesMac = 0;
LONG	g_lVerPagesPreferredMac = 0;

ERR ISAMAPI ErrIsamSystemInit()
	{
	ERR			err;
	CHAR		szT[4][16];
	const CHAR	*rgszT[4];

	//	US English MUST be installed (to ensure
	//	something like NT:125253 doesn't bite
	//	us again in the future if the NT locale
	//	team decides to start silently returning
	//	different LCMapString() results based on
	//	whether the specified lcid is installed
	CallR( ErrNORMCheckLcid( pinstNil, lcidDefault ) );
	AssertNORMConstants();
	
	//	Get OS info, LocalID etc

	dwGlobalMajorVersion = DwUtilSystemVersionMajor();
	dwGlobalMinorVersion = DwUtilSystemVersionMinor();
	dwGlobalBuildNumber = DwUtilSystemBuildNumber();
	lGlobalSPNumber = DwUtilSystemServicePackNumber();

	//	Set JET constant.

	CallR( ErrITSetConstants( ) );

	//	Initialize global counters
	g_lSessionsMac = 0;
	g_lVerPagesMac = 0;
	g_lVerPagesPreferredMac = 0;

	//	Initialize instances
	
	CallR( INST::ErrINSTSystemInit() );

	//	initialize the OSU layer

	CallJ( ErrOSUInit(), TermInstInit );

	CallJ( LOG::ErrLGSystemInit(), TermOSU );

	//	initialize file map

	CallJ( FMP::ErrFMPInit(), TermLOGTASK );

	CallJ( ErrCALLBACKInit(), TermFMP );

	CallJ( CPAGE::ErrInit(), TermCALLBACK );
	
	CallJ( ErrBFInit(), TermCPAGE );

	CallJ( ErrCATInit(), TermBF );

	CallJ( VER::ErrVERSystemInit(), TermCAT );

	/*	write jet start event
	/**/
	sprintf( szT[0], "%d", DwUtilImageVersionMajor() );
	rgszT[0] = szT[0];
	sprintf( szT[1], "%02d", DwUtilImageVersionMinor() );
	rgszT[1] = szT[1];
	sprintf( szT[2], "%04d", DwUtilImageBuildNumberMajor() );
	rgszT[2] = szT[2];
	sprintf( szT[3], "%04d", DwUtilImageBuildNumberMinor() );
	rgszT[3] = szT[3];
	UtilReportEvent(
			eventInformation,
			GENERAL_CATEGORY,
			START_ID,
			CArrayElements( rgszT ),
			rgszT );

	g_fSystemInit = fTrue;
	
	return JET_errSuccess;

//	re-instate the following if anything gets added after the call to ErrVERSystemInit() above TermVER:
//	VER::VERSystemTerm();
//

TermCAT:
	CATTerm();
	
TermBF:
	BFTerm();

TermCPAGE:
	CPAGE::Term();

TermCALLBACK:
	CALLBACKTerm();

TermFMP:
	FMP::Term();

TermLOGTASK:
	LOG::LGSystemTerm();

TermOSU:
	OSUTerm();

TermInstInit:
	INST::INSTSystemTerm();

	return err;
	}
	

VOID ISAMAPI IsamSystemTerm()
	{
	if ( !g_fSystemInit )
		{
		return;
		}
		
	g_fSystemInit = fFalse;
	
	//	write jet stop event

	UtilReportEvent(
			eventInformation,
			GENERAL_CATEGORY,
			STOP_ID,
			0,
			NULL );

	VER::VERSystemTerm();

	CATTerm();

	BFTerm( );

	CPAGE::Term();

	CALLBACKTerm();
	
	FMP::Term();

	LOG::LGSystemTerm();

	OSUTerm();

	INST::INSTSystemTerm();
}


ERR ISAMAPI ErrIsamInit(	JET_INSTANCE	inst, 
							JET_GRBIT		grbit )
	{
	ERR			err;
	INST		*pinst							= (INST *)inst;
	LOG			*plog							= pinst->m_plog;
	BOOL		fLGInitIsDone					= fFalse;
	BOOL		fJetLogGeneratedDuringSoftStart	= fFalse;
	BOOL		fNewCheckpointFile				= fTrue;
	BOOL		fFSInit							= fFalse;
	CHAR		szT[16];
	const CHAR	*rgszT[1];

	Assert( pinst->m_plog );
	Assert( pinst->m_pver );
	Assert( pinst->m_pfsapi );
	Assert( !pinst->FRecovering() );

	//	create paths now if they do not exist

	if ( pinst->m_fCreatePathIfNotExist )
		{

		//	make the temp path does NOT have a trailing '\' and the log/sys paths do

		Assert( !FOSSTRTrailingPathDelimiter( pinst->m_szTempDatabase ) );
		Assert( FOSSTRTrailingPathDelimiter( pinst->m_szSystemPath ) );
		Assert( FOSSTRTrailingPathDelimiter( plog->m_szLogFilePath ) );

		//	create paths

		CallR( ErrUtilCreatePathIfNotExist( pinst->m_pfsapi, pinst->m_szTempDatabase, NULL ) );
		CallR( ErrUtilCreatePathIfNotExist( pinst->m_pfsapi, pinst->m_szSystemPath, NULL ) );
		CallR( ErrUtilCreatePathIfNotExist( pinst->m_pfsapi, plog->m_szLogFilePath, NULL ) );
		}

	//	Get basic global parameters for checking LG parameters
	
	CallR( plog->ErrLGInitSetInstanceWiseParameters( pinst->m_pfsapi ) );

	//	Check all the system parameter before we continue

	VERSanitizeParameters( pinst );

	if ( !plog->FLGCheckParams() ||
		 !FCB::FCheckParams( pinst ) ||
		 ( pinst->m_fCleanupMismatchedLogFiles && !plog->m_fLGCircularLogging ) )
		{
		return ErrERRCheck( JET_errInvalidSettings );
		}

	//	Add the new setting to the accumlating variables
	
	g_lSessionsMac += pinst->m_lSessionsMax;
	g_lVerPagesMac += pinst->m_lVerPagesMax;
	g_lVerPagesPreferredMac += pinst->m_lVerPagesPreferredMax;
	
	//	Set other variables global to this instance
	
	Assert( pinst->m_updateid == updateidMin );

	
	//	set log disable state
	//
	plog->m_fLogDisabled = pinst->FComputeLogDisabled( );
	
	// No need to display instance name and id in single instance mode
	if ( pinst->m_szInstanceName || pinst->m_szDisplayName ) // runInstModeMultiInst
		{
		/*	write jet instance start event */
		sprintf( szT, "%d", IpinstFromPinst( pinst ) );
		rgszT[0] = szT;
		UtilReportEvent( 
			eventInformation, 
			GENERAL_CATEGORY, 
			START_INSTANCE_ID, 
			CArrayElements( rgszT ), 
			rgszT, 
			0, 
			NULL, 
			pinst );
		}

	/*	initialize system according to logging disabled
	/**/
	if ( !plog->m_fLogDisabled )
		{
		DBMS_PARAM	dbms_param;
//		LGBF_PARAM	lgbf_param;

		/*	initialize log manager, and	check the last generation
		/*	of log files to determine if recovery needed.
		/*  (do not create the reserve logs -- this is done later during soft recovery)
		/**/
		CallJ( plog->ErrLGInit( pinst->m_pfsapi, &fNewCheckpointFile ), Uninitialize );
		fLGInitIsDone = fTrue;

		/*	store the system parameters
		 */
		pinst->SaveDBMSParams( &dbms_param );
//		LGSaveBFParams( &lgbf_param );
		
		/*	recover attached databases to consistent state
		/*	if recovery is successful, then we should have
		/*	a proper edbchk.sys file
		/**/
#ifdef IGNORE_BAD_ATTACH
		plog->m_fReplayingIgnoreMissingDB = grbit & JET_bitReplayIgnoreMissingDB;
#endif // IGNORE_BAD_ATTACH

#ifdef LOGPATCH_UNIT_TEST
		extern void TestLogPatch( INST* const pinst );
		TestLogPatch( pinst );

		err = -1;
		goto TermLG;
#endif	//	LOGPATCH_UNIT_TEST

		err = plog->ErrLGSoftStart( pinst->m_pfsapi, fNewCheckpointFile, &fJetLogGeneratedDuringSoftStart );

		//	HACK: 
		//		LGSaveDBMSParams saves 'csecLGFile' which is the WRONG thing to do:
		//		a> csecLGFile is based on cbSec which can change from machine to machine
		//		b> csecLGFile is not a system parameter; rather, it is deteremined by the system parameter 
		//		   'lLogFileSize' which is REAL thing we should be saving here
		//		c> with the new automatic log-file size management, we don't need to save anything
		//
		//	the fix here is to prevent LGRestoreDBMSParams from restoring csecLGFile
		//	NOTE: we cannot remove csecLGFile from the save/restore structures because it would be format change

		/*	initialize constants again.
		/**/
		const INT csecLGFileSave = plog->m_csecLGFile;
		pinst->RestoreDBMSParams( &dbms_param );
		plog->m_csecLGFile = csecLGFileSave;
//		LGRestoreBFParams( &lgbf_param );

		CallJ( err, TermLG );

		/*  add the first log record
		/**/
		CallJ( ErrLGStart( pinst ), TermLG );

		// Ensure that all data is flushed.
		// Calling ErrLGFlushLog() may not flush all data in the log buffers.
		CallJ( plog->ErrLGWaitAllFlushed( pinst->m_pfsapi ), TermLG );

//		/*	initialize constants again.
//		/**/
//		CallJ( ErrITSetConstants( ), TermLG );
		}

	/*  initialize remaining system
	/**/
	CallJ( pinst->ErrINSTInit(), TermLG );

	Assert( !pinst->FRecovering() );
	Assert( !fJetLogGeneratedDuringSoftStart || !plog->m_fLogDisabled );

	/*	set up FMP from checkpoint.
	/**/
	if ( !plog->m_fLogDisabled )
		{
		CHAR	szPathJetChkLog[IFileSystemAPI::cchPathMax];

#ifdef DEBUG
#ifdef UNLIMITED_DB
		for ( DBID dbidT = dbidUserLeast; dbidT < dbidMax; dbidT++ )
			{
			Assert( ifmpMax == pinst->m_mpdbidifmp[ dbidT ] );
			}
#else
		BYTE	bAttachInfo;		//	shouldn't be any attachments, so one byte for sentinel is sufficient
		LGLoadAttachmentsFromFMP( pinst, &bAttachInfo );
		Assert( 0 == bAttachInfo );
#endif	//	UNLIMITED_DB
#endif	//	DEBUG
		
		plog->LGFullNameCheckpoint( pinst->m_pfsapi, szPathJetChkLog );
		err = plog->ErrLGReadCheckpoint( pinst->m_pfsapi, szPathJetChkLog, plog->m_pcheckpoint, fFalse );
		if ( JET_errCheckpointFileNotFound == err
			&& ( fNewCheckpointFile || fJetLogGeneratedDuringSoftStart ) )
			{
			//	could not locate checkpoint file, but we had previously
			//	deemed it necessary to create a new one (either because it
			//	was missing or because we're beginning from gen 1)
			plog->m_fLGFMPLoaded		= fTrue;
			(VOID) plog->ErrLGUpdateCheckpointFile( pinst->m_pfsapi, fTrue );
			if ( fJetLogGeneratedDuringSoftStart )
				{
				Assert( plog->m_plgfilehdr->lgfilehdr.le_lGeneration == 1 );
				plog->m_plgfilehdr->rgbAttach[0] = 0;
				CallJ( plog->ErrLGWriteFileHdr( plog->m_plgfilehdr ), TermIT );
				}
			}
		else
			{
			CallJ( err, TermIT );

			plog->m_logtimeFullBackup 	= plog->m_pcheckpoint->checkpoint.logtimeFullBackup;
			plog->m_lgposFullBackup 	= plog->m_pcheckpoint->checkpoint.le_lgposFullBackup;
			plog->m_logtimeIncBackup 	= plog->m_pcheckpoint->checkpoint.logtimeIncBackup;
			plog->m_lgposIncBackup 		= plog->m_pcheckpoint->checkpoint.le_lgposIncBackup;
			plog->m_fLGFMPLoaded		= fTrue;
			}

		extern PERFInstance<QWORD> cLGCheckpoint;
		cLGCheckpoint.Set( pinst, plog->CbOffsetLgpos( plog->m_pcheckpoint->checkpoint.le_lgposCheckpoint, lgposMin ) );
		err = JET_errSuccess;
		}
	return err;

TermIT:
	CallS( pinst->ErrINSTTerm( termtypeError ) );

TermLG:
	if ( fLGInitIsDone )
		{
		(VOID)plog->ErrLGTerm( pinst->m_pfsapi, fFalse /* do not flush log */ );
		}
	
	if ( fJetLogGeneratedDuringSoftStart )
		{
		CHAR	szLogName[ IFileSystemAPI::cchPathMax ];

		//	Instead of using m_szLogName (part of the shared-state monster),
		//	generate edb.log's filename now.
		plog->LGMakeLogName( szLogName, plog->m_szJet );
		(VOID)pinst->m_pfsapi->ErrFileDelete( szLogName );
		}

Uninitialize:

	g_lSessionsMac -= pinst->m_lSessionsMax;
	g_lVerPagesMac -= pinst->m_lVerPagesMax;
	g_lVerPagesPreferredMac -= pinst->m_lVerPagesPreferredMax;

	return err;
	}


ERR ISAMAPI ErrIsamTerm( JET_INSTANCE instance, JET_GRBIT grbit )
	{
	ERR				err;
	INST 			* const pinst			= (INST *)instance;
	const BOOL		fInstanceUnavailable	= pinst->FInstanceUnavailable();
	const TERMTYPE	termtype				= ( fInstanceUnavailable ?
													termtypeError :
													( grbit & JET_bitTermAbrupt ? termtypeNoCleanUp : termtypeCleanUp ) );

	err = pinst->ErrINSTTerm( termtype );
	if ( pinst->m_fSTInit != fSTInitNotDone )
		{
		/*	before getting an error before reaching no-returning point in ITTerm().
		 */
		Assert( err < 0 );
		return err;
		}

	const ERR	errT	= pinst->m_plog->ErrLGTerm(
											pinst->m_pfsapi,
											( err >= JET_errSuccess && termtypeError != termtype ) );
	if ( err >= JET_errSuccess && errT < JET_errSuccess )
		{
		err = errT;
		}

	//	term the file-system

	delete pinst->m_pfsapi;
	pinst->m_pfsapi = NULL;

	// No need to display instance name and id in single instance mode
	if ( pinst->m_szInstanceName || pinst->m_szDisplayName ) // runInstModeMultiInst
		/*	write jet stop event */
		{
		if ( err >= JET_errSuccess && termtypeError != termtype )
			{
			CHAR		szT[16];
			const CHAR	*rgszT[1];

			Assert( !fInstanceUnavailable );
	
			sprintf( szT, "%d", IpinstFromPinst( pinst ) );
			rgszT[0] = szT;
			UtilReportEvent(
				eventInformation,
				GENERAL_CATEGORY,
				STOP_INSTANCE_ID,
				CArrayElements( rgszT ),
				rgszT,
				0,
				NULL,
				pinst );
			}
		else		
			{
			CHAR		sz1[16];
			CHAR		sz2[16];
			const CHAR	*rgszT[2];
			
			sprintf( sz1, "%d", IpinstFromPinst( pinst ) );
			sprintf( sz2, "%d", fInstanceUnavailable ? pinst->ErrInstanceUnavailableErrorCode() : err );
			rgszT[0] = sz1;
			rgszT[1] = sz2;
			UtilReportEvent(
				eventInformation,
				GENERAL_CATEGORY,
				STOP_INSTANCE_ID_WITH_ERROR,
				CArrayElements( rgszT ),
				rgszT, 
				0,
				NULL,
				pinst );
			}
		}

	//	uninitialize the global variables

	g_lSessionsMac -= pinst->m_lSessionsMax;
	g_lVerPagesMac -= pinst->m_lVerPagesMax;
	g_lVerPagesPreferredMac -= pinst->m_lVerPagesPreferredMax;
	
	return err;
	}


#ifdef DEBUG
ERR ISAMAPI ErrIsamGetTransaction( JET_SESID vsesid, ULONG_PTR *plevel )
	{
	ERR		err = JET_errSuccess;
	PIB		*ppib = (PIB *)vsesid;

	CallR( ErrPIBCheck( ppib ) );

	*plevel = (ULONG_PTR)ppib->level;
	return err;
	}
#endif


//+api
//	ErrIsamBeginTransaction
//	=========================================================
//	ERR ErrIsamBeginTransaction( PIB *ppib )
//
//	Starts a transaction for the current user.  The user's transaction
//	level increases by one.
//
//	PARAMETERS	ppib 			pointer to PIB for user
//
//	RETURNS		JET_errSuccess
//
//	SIDE EFFECTS	
//		The CSR stack for each active FUCB of this user is copied
//		to the new transaction level.
//
// SEE ALSO		ErrIsamCommitTransaction, ErrIsamRollback
//-
ERR ISAMAPI ErrIsamBeginTransaction( JET_SESID vsesid, JET_GRBIT grbit )
	{
	PIB		* ppib	= (PIB *)vsesid;
	ERR		err;

	CallR( ErrPIBCheck( ppib ) );
	Assert( ppibNil != ppib );

	if ( ppib->level < levelUserMost )
		{
		ppib->ptls = Ptls();

#ifdef DTC
		const JET_GRBIT		grbitsSupported		= ( JET_bitTransactionReadOnly|JET_bitDistributedTransaction );
#else
		const JET_GRBIT		grbitsSupported		= JET_bitTransactionReadOnly;
#endif
		err = ErrDIRBeginTransaction(
					ppib,
					( grbit & grbitsSupported ) );	//	filter out unsupported grbits
		}
	else
		{
		Assert( levelUserMost == ppib->level );
		err = ErrERRCheck( JET_errTransTooDeep );
		}

	return err;
	}


#ifdef DTC
ERR ISAMAPI ErrIsamPrepareToCommitTransaction(
	JET_SESID		sesid,
	const VOID		* const pvData,
	const ULONG		cbData )
	{
	ERR				err;
	PIB				* const ppib	= (PIB *)sesid;

	CallR( ErrPIBCheck( ppib ) );

	if ( 0 == ppib->level )
		err = ErrERRCheck( JET_errNotInTransaction );
	else if ( ppib->level > 1 )
		err = ErrERRCheck( JET_errMustCommitDistributedTransactionToLevel0 );
	else if ( !ppib->FDistributedTrx() )
		err = ErrERRCheck( JET_errNotInDistributedTransaction );
	else
		err = ErrDIRPrepareToCommitTransaction( ppib, pvData, cbData );

	return err;
	}
#endif	//	DTC


//+api
//	ErrIsamCommitTransaction
//	========================================================
//	ERR ErrIsamCommitTransaction( JET_SESID vsesid, JET_GRBIT grbit )
//
//	Commits the current transaction for this user.  The transaction level
//	for this user is decreased by the number of levels committed.
//
//	PARAMETERS	
//
//	RETURNS		JET_errSuccess
//
//	SIDE EFFECTS
//		The CSR stack for each active FUCB of this user is copied
//		from the old ( higher ) transaction level to the new ( lower )
//		transaction level.
//
//	SEE ALSO	ErrIsamBeginTransaction, ErrIsamRollback
//-
ERR ISAMAPI ErrIsamCommitTransaction( JET_SESID vsesid, JET_GRBIT grbit )
	{
	ERR		err;
	PIB		*ppib = (PIB *)vsesid;

	CallR( ErrPIBCheck( ppib ) );

	//	may not be in a transaction, but wait for flush of all
	//	currently commited transactions.
	//
	if ( JET_bitWaitAllLevel0Commit & grbit )
		{
		//	no other grbits may be specified in conjunction with JET_bitWaitAllLevel0Commit
		if ( JET_bitWaitAllLevel0Commit != grbit )
			{
			return ErrERRCheck( JET_errInvalidGrbit );
			}

		return ErrLGForceFlushLog( ppib );
		}
	
	//	may not be in a transaction, but wait for flush of previous
	//	lazy committed transactions.
	//
	if ( grbit & JET_bitWaitLastLevel0Commit )
		{
		//	no other grbits may be specified in conjunction with WaitLastLevel0Commit
		if ( JET_bitWaitLastLevel0Commit != grbit )
			{
			return ErrERRCheck( JET_errInvalidGrbit );
			}

		//	wait for last level 0 commit and rely on good user behavior
		//
		if ( CmpLgpos( &ppib->lgposCommit0, &lgposMax ) == 0 )
			{
			return JET_errSuccess;
			}

		LOG *plog = PinstFromPpib( ppib )->m_plog;
		err = plog->ErrLGWaitCommit0Flush( ppib );
		Assert( err >= 0 || plog->m_fLGNoMoreLogWrite );
		
		return err;
		}

	if ( ppib->level == 0 )
		{
		return ErrERRCheck( JET_errNotInTransaction );
		}

	err = ErrDIRCommitTransaction( ppib, grbit );

	return err;
	}


//+api
//	ErrIsamRollback
//	========================================================
//	ERR ErrIsamRollback( PIB *ppib, JET_GRBIT grbit )
//
//	Rolls back transactions for the current user.  The transaction level of
//	the current user is decreased by the number of levels aborted.
//
//	PARAMETERS	ppib		pointer to PIB for user
//				grbit		unused
//
//	RETURNS		
//		JET_errSuccess
//-
ERR ISAMAPI ErrIsamRollback( JET_SESID vsesid, JET_GRBIT grbit )
	{
	ERR   	 	err;
	PIB    		* ppib			= (PIB *)vsesid;
	FUCB   		* pfucb;
	FUCB   		* pfucbNext;

	/*	check session id before using it
	/**/
	CallR( ErrPIBCheck( ppib ) );
	
	if ( ppib->level == 0 )
		{
		return ErrERRCheck( JET_errNotInTransaction );
		}

	const LEVEL	levelRollback	= LEVEL( ppib->level - 1 );

	do
		{
		/*	get first primary index cusor
		/**/
		for ( pfucb = ppib->pfucbOfSession;
			pfucb != pfucbNil && ( FFUCBSecondary( pfucb ) || FFUCBLongValue( pfucb ) );
			pfucb = pfucb->pfucbNextOfSession )
			NULL;

		/*	LOOP 1 -- first go through all open cursors, and close them
		/*	or reset secondary index cursors, if opened in transaction
		/*	rolled back.  Reset copy buffer status and move before first.
		/*	Some cursors will be fully closed, if they have not performed any
		/*	updates.  This will include secondary index cursors
		/*	attached to primary index cursors, so pfucbNext must
		/*	always be a primary index cursor, to ensure that it will
		/*	be valid for the next loop iteration.  Note that no information
		/*	necessary for subsequent rollback processing is lost, since
		/*	the cursors will only be released if they have performed no
		/*	updates including DDL.
		/**/
		for ( ; pfucb != pfucbNil; pfucb = pfucbNext )
			{
			/*	get next primary index cusor
			/**/
			for ( pfucbNext = pfucb->pfucbNextOfSession;
			  	pfucbNext != pfucbNil && ( FFUCBSecondary( pfucbNext ) || FFUCBLongValue( pfucbNext ) );
			  	pfucbNext = pfucbNext->pfucbNextOfSession )
				NULL;

			/*	if defer closed then continue
			/**/
			if ( FFUCBDeferClosed( pfucb ) )
				continue;

			//	reset copy buffer status for each cursor on rollback
			if ( FFUCBUpdatePreparedLevel( pfucb, pfucb->ppib->level ) )
				{
				RECIFreeCopyBuffer( pfucb );
				FUCBResetUpdateFlags( pfucb );
				}
		
			/*	if current cursor is a table, and was opened in rolled back
			/*	transaction, then close cursor.
			/**/
			if ( FFUCBIndex( pfucb ) && pfucb->u.pfcb->FPrimaryIndex() )
				{
				if ( pfucb->levelOpen > levelRollback )
					{
					if ( pfucb->pvtfndef != &vtfndefInvalidTableid )
						{
						CallS( ErrDispCloseTable( (JET_SESID)ppib, (JET_TABLEID) pfucb ) );
						}
					else
						{
						//	Open internally, not exported to user.
						CallS( ErrFILECloseTable( ppib, pfucb ) );
						}
					continue;
					}

				/*	if primary index cursor, and secondary index set
				/*	in rolled back transaction, then change index to primary
				/*	index.  This must be done, since secondary index
				/*	definition may be rolled back, if the index was created
				/*	in the rolled back transaction.
				/**/
				if ( pfucb->pfucbCurIndex != pfucbNil )
					{
					if ( pfucb->pfucbCurIndex->levelOpen > levelRollback )
						{
						CallS( ErrRECSetCurrentIndex( pfucb, NULL, NULL ) );
						}
					}
				}

			//	if LV cursor was opened at this level, close it
			if ( pfucbNil != pfucb->pfucbLV
				&& pfucb->pfucbLV->levelOpen > levelRollback )
				{
				DIRClose( pfucb->pfucbLV );
				pfucb->pfucbLV = pfucbNil;
				}

			/*	if current cursor is a sort, and was opened in rolled back
			/*	transaction, then close cursor.
			/**/
			if ( FFUCBSort( pfucb ) )
				{
				if ( pfucb->levelOpen > levelRollback )
					{
					SORTClose( pfucb );
					continue;
					}
				}

			/*	if not sort and not index, and was opened in rolled back
			/*	transaction, then close DIR cursor directly.
			/**/
			if ( pfucb->levelOpen > levelRollback )
				{
				DIRClose( pfucb );
				continue;
				}
			}

		/*	call lower level abort routine
		/**/
		err = ErrDIRRollback( ppib );
		if ( JET_errRollbackError == err )
			{
#ifdef INDEPENDENT_DB_FAILURE			
			// this can happen only if run using g_fOneDatabasePerSession  
			Assert( g_fOneDatabasePerSession );

			const IFMP	ifmpError	= IfmpFirstDatabaseOpen( ppib );
			FMP::AssertVALIDIFMP( ifmpError );
			Assert( rgfmp[ifmpError].FAllowForceDetach() );
			err = ppib->m_errFatal;
#else
			err = ppib->ErrRollbackFailure();
#endif			
			
			// recover the error from rollback here
			// and return that one
			Assert( err < JET_errSuccess );
			}
		CallR( err );
		}
	while ( ( grbit & JET_bitRollbackAll ) != 0 && ppib->level > 0 );

	return JET_errSuccess;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\server\makefile.inc ===
$(O)\esent_noalias.lib : esent.def $(IMPLIB_DEPEND) makefile.inc
    cl /nologo /c /DNO_DEF_ALIAS /EP /Tc esent.def > $(O)\esent_noalias.def
    lib -out:$@ -nod -nologo -def:$(O)\esent_noalias.def -machine:IX86 $(IMPLIB_DEPEND)
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\ver.cxx ===
#include "std.hxx"
#include "_bt.hxx"
#include "_ver.hxx"


///#define BREAK_ON_PREFERRED_BUCKET_LIMIT

#ifdef DEBUG

//	DEBUG_VER:  check the consistency of the version store hash table
///#define DEBUG_VER

//  DEBUG_VER_EXPENSIVE:  time-consuming version store consistency check
///#define DEBUG_VER_EXPENSIVE

#endif	//  DEBUG


//  ****************************************************************
//  PERFMON STATISTICS
//  ****************************************************************

PERFInstanceG<> cVERcbucketAllocated;
PERFInstanceG<> cVERcbucketDeleteAllocated;
PERFInstanceG<> cVERBucketAllocWaitForRCEClean;
PERFInstanceG<> cVERcbBookmarkTotal;
PERFInstanceG<> cVERcrceHashEntries;
PERFInstanceG<> cVERUnnecessaryCalls;
PERFInstanceG<> cVERAsyncCleanupDispatched;
PERFInstanceG<> cVERSyncCleanupDispatched;
PERFInstanceG<> cVERCleanupDiscarded;
PERFInstanceG<> cVERCleanupFailed;

PM_CEF_PROC LVERcbucketAllocatedCEFLPv;
PM_CEF_PROC	LVERcbucketDeleteAllocatedCEFLPv;
PM_CEF_PROC LVERBucketAllocWaitForRCECleanCEFLPv;
PM_CEF_PROC LVERcbAverageBookmarkCEFLPv;
PM_CEF_PROC LVERUnnecessaryCallsCEFLPv;
PM_CEF_PROC LVERAsyncCleanupDispatchedCEFLPv;
PM_CEF_PROC LVERSyncCleanupDispatchedCEFLPv;
PM_CEF_PROC LVERCleanupDiscardedCEFLPv;
PM_CEF_PROC LVERCleanupFailedCEFLPv;


//  ================================================================
LONG LVERcbucketAllocatedCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	cVERcbucketAllocated.PassTo( iInstance, pvBuf );
	return 0;
	}


//  ================================================================
LONG LVERcbucketDeleteAllocatedCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	if ( pvBuf )
		{
		LONG counter = cVERcbucketDeleteAllocated.Get( iInstance );
		if ( counter < 0 )
			{
			cVERcbucketDeleteAllocated.Clear( iInstance );
			*((LONG *)pvBuf) = 0;
			}
		else
			{
			*((LONG *)pvBuf) = counter;
			}
		}
	return 0;
	}


//  ================================================================
LONG LVERBucketAllocWaitForRCECleanCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	cVERBucketAllocWaitForRCEClean.PassTo( iInstance, pvBuf );
	return 0;
	}


//  ================================================================
LONG LVERcbAverageBookmarkCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	if ( NULL != pvBuf )
		{
		LONG cHash = cVERcrceHashEntries.Get( iInstance );
		LONG cBookmark = cVERcbBookmarkTotal.Get( iInstance );
		if ( 0 < cHash && 0 <= cBookmark )
			{
			*( LONG * )pvBuf = cBookmark/cHash;
			}
		else if ( 0 > cHash || 0 > cBookmark )
			{
			cVERcrceHashEntries.Clear( iInstance );
			cVERcbBookmarkTotal.Clear( iInstance );
			*( LONG * )pvBuf = 0;
			}
		else
			{
			*( LONG * )pvBuf = 0;
			}
		}
	return 0;
	}


//  ================================================================
LONG LVERUnnecessaryCallsCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	cVERUnnecessaryCalls.PassTo( iInstance, pvBuf );
	return 0;
	}

//  ================================================================
LONG LVERAsyncCleanupDispatchedCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	cVERAsyncCleanupDispatched.PassTo( iInstance, pvBuf );
	return 0;
	}

//  ================================================================
LONG LVERSyncCleanupDispatchedCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	cVERSyncCleanupDispatched.PassTo( iInstance, pvBuf );
	return 0;
	}

//  ================================================================
LONG LVERCleanupDiscardedCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	cVERCleanupDiscarded.PassTo( iInstance, pvBuf );
	return 0;
	}

//  ================================================================
LONG LVERCleanupFailedCEFLPv( LONG iInstance, VOID * pvBuf )
//  ================================================================
	{
	cVERCleanupFailed.PassTo( iInstance, pvBuf );
	return 0;
	}



//  ****************************************************************
//  GLOBALS
//  ****************************************************************

const DIRFLAG	fDIRUndo = fDIRNoLog | fDIRNoVersion | fDIRNoDirty;

//  exported
volatile LONG	crefVERCreateIndexLock	= 0;

//	the bucket resource pool, shared among all instance
#ifdef GLOBAL_VERSTORE_MEMPOOL
CRES		*g_pcresVERPool 						= NULL;
#endif

//  the last global RCE id assigned
ULONG	RCE::rceidLast = rceidMin;


//  ****************************************************************
//  VER class
//  ****************************************************************

VER::VER( INST *pinst ) :
		m_pinst( pinst ),
		m_fVERCleanUpWait( 2 ),
		m_ulVERTasksPostMax( g_ulVERTasksPostMax ),
		m_tickLastRCEClean( 0 ),
		m_msigRCECleanPerformedRecently( CSyncBasicInfo( _T( "m_msigRCECleanPerformedRecently" ) ) ),
		m_asigRCECleanDone( CSyncBasicInfo( _T( "m_asigRCECleanDone" ) ) ),
		m_critRCEClean( CLockBasicInfo( CSyncBasicInfo( szRCEClean ), rankRCEClean, 0 ) ),
		m_critBucketGlobal( CLockBasicInfo( CSyncBasicInfo( szBucketGlobal ), rankBucketGlobal, 0 ) ),
		m_cbucketDynamicAlloc( 0 ),
		m_pbucketGlobalHead( pbucketNil ),
		m_pbucketGlobalTail( pbucketNil ),
#ifdef MOVEABLE_RCE
		m_pbucketGlobalLastDelete( pbucketNil ),
		m_cbucketGlobalAllocDelete( 0 ),
#endif
		m_cbucketGlobalAlloc( 0 ),
		m_cbucketGlobalAllocMost( 0 ),
		m_cbucketGlobalAllocPreferred( 0 ),
		m_ppibRCEClean( ppibNil ),
		m_ppibRCECleanCallback( ppibNil ),
		m_fSyncronousTasks( fFalse ),
		m_trxBegin0LastLongRunningTransaction( trxMin ),
		m_ppibTrxOldestLastLongRunningTransaction( ppibNil ),
		m_dwTrxContextLastLongRunningTransaction( 0 ),
#ifdef GLOBAL_VERSTORE_MEMPOOL
		m_pcresVERPool( g_pcresVERPool )
#else
		m_pcresVERPool( NULL )
#endif
#ifdef VERPERF
		,
		m_cbucketCleaned( 0 ),
		m_cbucketSeen( 0 ),
		m_crceSeen( 0 ),
		m_crceCleaned( 0 ),
		m_crceFlagDelete( 0 ),
		m_crceDeleteLV( 0 ),
#ifdef MOVEABLE_RCE
		m_crceMoved( 0 ),
		m_crceMovedDeleted( 0 ),
#endif
		m_critVERPerf( CLockBasicInfo( CSyncBasicInfo( szVERPerf ), rankVERPerf, 0 ) )
#endif	//	VERPERF
		{

		//	initialised to the Set state (it should
		//	be impossible to wait on the signal if
		//	it didn't first get reset, but this
		//	will guarantee it)
		//
		m_msigRCECleanPerformedRecently.Set();

#ifdef VERPERF
		qwStartTicks = QwUtilHRTCount();
#endif
		}

VER::~VER()
		{
		}


//  ****************************************************************
//  BUCKET LAYER
//  ****************************************************************
//
//  A bucket is a contiguous block of memory used to hold versions.
//
//-

//  ================================================================
INLINE INT CbBUFree( const BUCKET * pbucket )
//  ================================================================
//
//	the cast to (INT) below is necessary to catch the negative case
//
//-
	{
	const INT_PTR	cbBUFree =
			reinterpret_cast<INT_PTR>( pbucket + 1 ) - reinterpret_cast<INT_PTR> (pbucket->hdr.prceNextNew );
	Assert( cbBUFree >= 0 );
	Assert( cbBUFree < sizeof(BUCKET) );
	return (INT)cbBUFree;
	}


//  ================================================================
INLINE BUCKET *VER::PbucketVERIAlloc( const CHAR* szFile, ULONG ulLine )
//  ================================================================
	{
#ifdef GLOBAL_VERSTORE_MEMPOOL
	//	GlobalAlloc should always be less than or equal
	//	to GlobalAllocMost, but our check should err
	//	on the side of safety just in case
	Assert( m_cbucketGlobalAlloc <= m_cbucketGlobalAllocMost );
	if ( m_cbucketGlobalAlloc >= m_cbucketGlobalAllocMost )
		{
		// don't exceed per instance max
		return NULL;
		}
	Assert ( g_pcresVERPool );
	BUCKET *pbucket = reinterpret_cast<BUCKET *>( g_pcresVERPool->PbAlloc( szFile, ulLine ) );
#else
	Assert ( m_pcresVERPool );
	BUCKET *pbucket = reinterpret_cast<BUCKET *>( m_pcresVERPool->PbAlloc( szFile, ulLine ) );
#endif
	Assert( m_cbucketGlobalAlloc >= 0 );
	if ( NULL != pbucket )
		{
		++m_cbucketGlobalAlloc;

		if ( !g_pcresVERPool->FContains( (BYTE *)pbucket ) )
			{
			m_cbucketDynamicAlloc++;
			}

#ifdef VERPERF
		cVERcbucketAllocated.Inc( m_pinst );
#endif // VERPERF
#ifdef BREAK_ON_PREFERRED_BUCKET_LIMIT
		AssertRTL( m_cbucketGlobalAlloc <= m_cbucketGlobalAllocPreferred );
#endif
		}
	return pbucket;
	}
#define PbucketMEMAlloc()		PbucketVERIAlloc( __FILE__, __LINE__ )


//  ================================================================
INLINE VOID VER::VERIReleasePbucket( BUCKET * pbucket )
//  ================================================================
	{
	Assert( m_cbucketGlobalAlloc > 0 );
	m_cbucketGlobalAlloc--;

	if ( !g_pcresVERPool->FContains( (BYTE *)pbucket ) )
		{
		m_cbucketDynamicAlloc--;
		}

#ifdef VERPERF
	cVERcbucketAllocated.Dec( m_pinst );
#endif // VERPERF
#ifdef GLOBAL_VERSTORE_MEMPOOL
	Assert ( g_pcresVERPool );
	g_pcresVERPool->Release( reinterpret_cast<BYTE *>( pbucket ) );
#else
	Assert ( m_pcresVERPool );
	m_pcresVERPool->Release( reinterpret_cast<BYTE *>( pbucket ) );
#endif
	}


#ifdef MOVEABLE_RCE

//  ================================================================
INLINE BOOL VER::FVERICleanWithoutIO()
//  ================================================================
//
//  Should we move FlagDelete RCE's instead of cleaning them up
//  (only if not cleaning the last bucket)
//
	{
#ifdef DEBUG
	const double dblThresh = 0.3;
#else
	const double dblThresh = 0.8;
#endif	//	DEBUG

	const BOOL	fRCECleanWithoutIO	= ( m_pbucketGlobalHead != m_pbucketGlobalTail )
										&& ( ( (double)m_cbucketGlobalAlloc > ( (double)m_cbucketGlobalAllocPreferred * dblThresh ) )
											|| ( ( m_cbucketGlobalAllocMost - m_cbucketGlobalAlloc ) < 5 )
											|| m_pinst->Taskmgr().CPostedTasks() > m_ulVERTasksPostMax )
										&& !m_pinst->m_plog->m_fRecovering
										&& !m_pinst->m_fTermInProgress;

	return fRCECleanWithoutIO;
	}

#endif	//	MOVEABLE_RCE


//  ================================================================
INLINE BOOL VER::FVERICleanDiscardDeletes()
//  ================================================================
//
//  If the version store is really full we will simply discard the
//  RCEs (only if not cleaning the last bucket)
//
	{
	BOOL fDiscardDeletes	= fFalse;

	if ( m_cbucketGlobalAlloc > m_cbucketGlobalAllocPreferred )
		{
		//	discard deletes if we've exceeded the preferred threshold
		//	or if the task manager is being overrun
		//
		fDiscardDeletes = fTrue;
		}

	else if ( m_pbucketGlobalHead != m_pbucketGlobalTail
			&& ( m_cbucketGlobalAllocMost - m_cbucketGlobalAlloc ) < 2 )
		{
		//	discard deletes if this is not the only bucket and there are
		//	less than two buckets left
		//
		fDiscardDeletes = fTrue;
		}

#ifdef MOVEABLE_RCE
	else if ( m_pbucketGlobalHead != m_pbucketGlobalTail
			&& (double)m_cbucketGlobalAllocDelete > ( (double)m_cbucketGlobalAllocMost * 0.25 ) )
		{
		//	discard deletes if this is not the only bucket and over
		//	25% of the buckets are being used for moved RCE's
		//
		fDiscardDeletes = fTrue;
		}
#endif

	return fDiscardDeletes;
	}

LOCAL VOID VER::VERIReportDiscardedDeletes( const RCE * const prce )
	{
	Assert( m_critRCEClean.FOwner() );

	//	if OLD is already running on this database,
	//	don't bother reporting discarded deletes

	//	we keep track of the NEWEST trx at the time discards were last
	//	reported, and will only generate future discard reports for
	//	deletes that occurred after the current trxNewest (this is a
	//	means of ensuring we don't report too often)

	FMP * const	pfmp	= rgfmp + prce->Ifmp();

	if ( !pfmp->FRunningOLD()
		&& TrxCmp( pfmp->TrxNewestWhenDiscardsLastReported(), prce->TrxBegin0() ) >= 0 )
		{
		const CHAR	* rgszT[1]	= { pfmp->SzDatabaseName() };
		UtilReportEvent(
				eventWarning,
				PERFORMANCE_CATEGORY,
				MANY_LOST_COMPACTION_ID,
				1,
				rgszT,
				0,
				NULL,
				pfmp->Pinst() );

		//	this ensures no further reports will be generated for
		//	deletes which were present in the version store at
		//	the time this report was generated
		pfmp->SetTrxNewestWhenDiscardsLastReported( m_pinst->m_trxNewest );
		}
	}

LOCAL VOID VER::VERIReportVersionStoreOOM( const BOOL fCleanupWasRun )
	{
	//	only called by ErrVERIBUAllocBucket()
	Assert( m_critBucketGlobal.FOwner() );

	PIB* ppibTrxOldest = ppibNil;
	for ( size_t iProc = 0; iProc < OSSyncGetProcessorCountMax(); iProc++ )
		{
		INST::PLS* const ppls = m_pinst->Ppls( iProc );
		ppls->m_critPIBTrxOldest.Enter();
		}
	for ( iProc = 0; iProc < OSSyncGetProcessorCountMax(); iProc++ )
		{
		INST::PLS* const ppls = m_pinst->Ppls( iProc );
		PIB* const ppibTrxOldestCandidate = ppls->m_ilTrxOldest.PrevMost();
		if ( ppibTrxOldest == ppibNil
			|| ( ppibTrxOldestCandidate
				&& TrxCmp( ppibTrxOldestCandidate->trxBegin0, ppibTrxOldest->trxBegin0 ) < 0 ) )
			{
			ppibTrxOldest = ppibTrxOldestCandidate;
			}
		}

	//	only generate the eventlog entry once per long-running transaction
	if ( ppibNil != ppibTrxOldest )
		{
		const TRX	trxBegin0			= ppibTrxOldest->trxBegin0;

		if ( trxBegin0 != m_trxBegin0LastLongRunningTransaction )
			{
			CHAR	szSession[32];
			CHAR	szSesContext[32];
			CHAR	szSesContextThreadID[32];
			CHAR	szInst[16];
			CHAR	szMaxVerPages[16];
			CHAR	szCleanupWasRun[16];

			sprintf( szSession, "0x%p", ppibTrxOldest );
			if ( ppibTrxOldest->FUseSessionContextForTrxContext() )
				{
				sprintf( szSesContext, "0x%0*I64X", sizeof(DWORD_PTR)*2, QWORD( ppibTrxOldest->dwSessionContext ) );
				sprintf( szSesContextThreadID, "0x%0*I64X", sizeof(DWORD_PTR)*2, QWORD( ppibTrxOldest->dwSessionContextThreadId ) );
				}
			else
				{
				sprintf( szSesContext, "0x%08X", 0 );
				sprintf( szSesContextThreadID, "0x%0*I64X", sizeof(DWORD_PTR)*2, QWORD( ppibTrxOldest->dwTrxContext ) );
				}

			sprintf( szInst, "%d", IpinstFromPinst( m_pinst ) );
			sprintf( szMaxVerPages, "%d", ( m_cbucketGlobalAllocMost * cbBucket ) / ( 1024 * 1024 ) );
			sprintf( szCleanupWasRun, "%d", fCleanupWasRun );

			Assert( m_cbucketGlobalAlloc <= m_cbucketGlobalAllocMost );
			if ( m_cbucketGlobalAlloc < m_cbucketGlobalAllocMost )
				{
				//	OOM because NT returned OOM
				const UINT	csz		= 8;
				const CHAR	* rgsz[csz];
				CHAR		szCurrVerPages[16];
				CHAR		szGlobalMinVerPages[16];

				sprintf( szCurrVerPages, "%d", ( m_cbucketGlobalAlloc * cbBucket ) / ( 1024 * 1024 ) );
				sprintf( szGlobalMinVerPages, "%d", ( g_lVerPagesMin * cbBucket ) / ( 1024 * 1024 ) );

				rgsz[0] = szInst;
				rgsz[1] = szCurrVerPages;
				rgsz[2] = szMaxVerPages;
				rgsz[3] = szGlobalMinVerPages;
				rgsz[4] = szSession;
				rgsz[5] = szSesContext;
				rgsz[6] = szSesContextThreadID;
				rgsz[7] = szCleanupWasRun;

				UtilReportEvent(
						eventError,
						TRANSACTION_MANAGER_CATEGORY,
						VERSION_STORE_OUT_OF_MEMORY_ID,
						csz,
						rgsz,
						0,
						NULL,
						m_pinst );
				}
			else
				{
				//	OOM because the user-specified max. has been reached
				const UINT	csz		= 6;
				const CHAR	* rgsz[csz];


				rgsz[0] = szInst;
				rgsz[1] = szMaxVerPages;
				rgsz[2] = szSession;
				rgsz[3] = szSesContext;
				rgsz[4] = szSesContextThreadID;
				rgsz[5] = szCleanupWasRun;

				UtilReportEvent(
						eventError,
						TRANSACTION_MANAGER_CATEGORY,
						VERSION_STORE_REACHED_MAXIMUM_ID,
						csz,
						rgsz,
						0,
						NULL,
						m_pinst );
				}

			m_trxBegin0LastLongRunningTransaction = trxBegin0;
			m_ppibTrxOldestLastLongRunningTransaction = ppibTrxOldest;
			m_dwTrxContextLastLongRunningTransaction = ( ppibTrxOldest->FUseSessionContextForTrxContext() ?
															ppibTrxOldest->dwSessionContextThreadId :
															ppibTrxOldest->dwTrxContext );
			}
		}

	for ( iProc = 0; iProc < OSSyncGetProcessorCountMax(); iProc++ )
		{
		INST::PLS* const ppls = m_pinst->Ppls( iProc );
		ppls->m_critPIBTrxOldest.Leave();
		}
	}


//  ================================================================
INLINE ERR VER::ErrVERIBUAllocBucket( const INT cbRCE, const UINT uiHash )
//  ================================================================
//
//	Inserts a bucket to the top of the bucket chain, so that new RCEs
//	can be inserted.  Note that the caller must set ibNewestRCE himself.
//
//-
	{
	//	use m_critBucketGlobal to make sure only one allocation
	//	occurs at one time.
	Assert( m_critBucketGlobal.FOwner() );

	//	caller would only call this routine if they determined
	//	that current bucket was not enough to satisfy allocation
	//
	Assert( m_pbucketGlobalHead == pbucketNil
		|| cbRCE > CbBUFree( m_pbucketGlobalHead ) );

	//	signal RCE clean in case it can now do work
	VERSignalCleanup();

	//	Must allocate within m_critBucketGlobal to make sure that
	//  the allocated bucket will be in circularly increasing order.
	BUCKET * 	pbucket		= PbucketMEMAlloc();

	//	We are really out of bucket, return to high level function
	//	call to retry.
	if ( pbucketNil == pbucket )
		{
		m_critBucketGlobal.Leave();

		if ( uiHashInvalid != uiHash )
			{
			CritRCEChain( uiHash ).Leave();
			}

		//	ensure RCE clean was performed recently (if our wait times out, it
		//	means something is horribly wrong and blocking version cleanup)
		//
		const BOOL	fCleanupWasRun	= m_msigRCECleanPerformedRecently.FWait( cmsecAsyncBackgroundCleanup );

#ifdef VERPERF
		cVERBucketAllocWaitForRCEClean.Inc( m_pinst );
#endif // VERPERF

		if ( uiHashInvalid != uiHash )
			{
			CritRCEChain( uiHash ).Enter();
			}

		m_critBucketGlobal.Enter();

		//	see if someone else beat us to it
		//
		if ( m_pbucketGlobalHead == pbucketNil || cbRCE > CbBUFree( m_pbucketGlobalHead ) )
			{
			//	retry allocation
			//
			pbucket = PbucketMEMAlloc();

			//	check again
			//
			if ( pbucketNil == pbucket )
				{
				//	still couldn't allocate bucket, so bail with
				//	appropriate error
				//
				VERIReportVersionStoreOOM( fCleanupWasRun );
				return ErrERRCheck( fCleanupWasRun ?
										JET_errVersionStoreOutOfMemory :
										JET_errVersionStoreOutOfMemoryAndCleanupTimedOut );
				}
			}
		else
			{
			//	someone allocated a bucket for us, so just bail
			//
			return JET_errSuccess;
			}
		}

	//	ensure RCE's will be properly aligned
	Assert( pbucketNil != pbucket );
	Assert( FAlignedForAllPlatforms( pbucket ) );
	Assert( FAlignedForAllPlatforms( pbucket->rgb ) );
	Assert( (BYTE *)PvAlignForAllPlatforms( pbucket->rgb ) == pbucket->rgb );

	//	Initialize the bucket.
	pbucket->hdr.prceOldest		= (RCE *)pbucket->rgb;
	pbucket->hdr.prceNextNew	= (RCE *)pbucket->rgb;
	pbucket->hdr.pbLastDelete	= pbucket->rgb;
	pbucket->hdr.pbucketNext	= pbucketNil;

	//	Link up the bucket to global list.
	pbucket->hdr.pbucketPrev = m_pbucketGlobalHead;
	if ( pbucket->hdr.pbucketPrev )
		{
		//  there is a bucket after us
		pbucket->hdr.pbucketPrev->hdr.pbucketNext = pbucket;
		}
	else
		{
		//  we are last in the chain
		m_pbucketGlobalTail = pbucket;
		}
	m_pbucketGlobalHead = pbucket;

	//	if RCE doesn't fit into an empty bucket, something is horribly wrong
	//
	Assert( cbRCE <= CbBUFree( pbucket ) );
	return ( cbRCE > CbBUFree( pbucket ) ? ErrERRCheck( JET_errVersionStoreEntryTooBig ) : JET_errSuccess );
	}


//  ================================================================
INLINE BUCKET *VER::PbucketVERIGetOldest( )
//  ================================================================
//
//	find the oldest bucket in the bucket chain
//
//-
	{
	Assert( m_critBucketGlobal.FOwner() );

	BUCKET  * const pbucket = m_pbucketGlobalTail;

	Assert( pbucketNil == pbucket || pbucketNil == pbucket->hdr.pbucketPrev );
	return pbucket;
	}


//  ================================================================
BUCKET *VER::PbucketVERIFreeAndGetNextOldestBucket( BUCKET * pbucket )
//  ================================================================
	{
	Assert( m_critBucketGlobal.FOwner() );

	BUCKET * const pbucketNext = (BUCKET *)pbucket->hdr.pbucketNext;
	BUCKET * const pbucketPrev = (BUCKET *)pbucket->hdr.pbucketPrev;

#ifdef MOVEABLE_RCE
	if ( pbucket == m_pbucketGlobalLastDelete )
		{
		m_pbucketGlobalLastDelete = pbucketNil;
		}
#endif

	//	unlink bucket from bucket chain and free.
	if ( pbucketNil != pbucketNext )
		{
		Assert( m_pbucketGlobalHead != pbucket );
		pbucketNext->hdr.pbucketPrev = pbucketPrev;
		}
	else	//  ( pbucketNil == pbucketNext )
		{
		Assert( m_pbucketGlobalHead == pbucket );
		m_pbucketGlobalHead = pbucketPrev;
		}

	if ( pbucketNil != pbucketPrev )
		{
		Assert( m_pbucketGlobalTail != pbucket );
		pbucketPrev->hdr.pbucketNext = pbucketNext;
		}
	else	//  ( pbucketNil == pbucketPrev )
		{
		m_pbucketGlobalTail = pbucketNext;
		}

	Assert( ( m_pbucketGlobalHead && m_pbucketGlobalTail )
			|| ( !m_pbucketGlobalHead && !m_pbucketGlobalTail ) );

	VERIReleasePbucket( pbucket );
	return pbucketNext;
	}


//  ================================================================
INLINE CCriticalSection& VER::CritRCEChain( UINT ui )
//  ================================================================
	{
	Assert( ui < crceheadGlobalHashTable );
	return m_rgrceheadHashTable[ ui ].crit;
	}

INLINE RCE *VER::GetChain( UINT ui ) const
	{
	Assert( ui < crceheadGlobalHashTable );
	return m_rgrceheadHashTable[ ui ].prceChain;
	}

INLINE RCE **VER::PGetChain( UINT ui )
	{
	Assert( ui < crceheadGlobalHashTable );
	return &m_rgrceheadHashTable[ ui ].prceChain;
	}

INLINE VOID VER::SetChain( UINT ui, RCE *prce )
	{
	Assert( ui < crceheadGlobalHashTable );
	m_rgrceheadHashTable[ ui ].prceChain = prce;
	}

//  ================================================================
CCriticalSection& RCE::CritChain()
//  ================================================================
	{
	Assert( ::FOperInHashTable( m_oper ) );
	Assert( uiHashInvalid != m_uiHash );
	return PverFromIfmp( Ifmp() )->CritRCEChain( m_uiHash );
	}


//  ================================================================
LOCAL UINT UiRCHashFunc( IFMP ifmp, PGNO pgnoFDP, const BOOKMARK& bookmark )
//  ================================================================
	{
	ASSERT_VALID( &bookmark );
	Assert( pgnoNull != pgnoFDP );

	//  OPTIMIZATION:  turning on optimizations here always (#pragma)
	//  OPTIMIZATION:  unrolling the loop for speed

///#define FAST_HASH
///#define SEDGEWICK_HASH
#define SEDGEWICK_HASH2

#ifdef FAST_HASH
	UINT uiHash = ifmp + pgnoFDP +
					bookmark.key.prefix.Cb() +
					bookmark.key.suffix.Cb() +
					bookmark.data.cb;

	union {
		ULONG ul;
		BYTE rgb[4];
		} u;

	if ( bookmark.key.prefix.Cb() > 4 )
		uiHash += *(ULONG *)bookmark.key.prefix.pv;
	else
		{
		u.ul = 0;

		BYTE *pb = u.rgb;
		UtilMemCpy( pb, bookmark.key.prefix.pv, bookmark.key.prefix.Cb() );
		pb += bookmark.key.prefix.Cb();

		INT cbCopy = min( u.rgb + 4 - pb, bookmark.key.suffix.Cb() );
		UtilMemCpy( pb, bookmark.key.suffix.pv, cbCopy );
		pb += cbCopy;

		cbCopy = min( u.rgb + 4 - pb, bookmark.data.cb );
		UtilMemCpy( pb, bookmark.data.pv, cbCopy );

		uiHash += u.ul;
		}

	if ( bookmark.data.cb > 4 )
		{
		uiHash += *(ULONG *)( (BYTE*)bookmark.data.pv + bookmark.data.cb - 4 );
		}
	else
		{
		u.ul = 0;

		BYTE *pb = u.rgb + 4 - bookmark.data.cb;
		UtilMemCpy( pb, bookmark.data.pv, bookmark.data.cb );

		INT cbCopy = min( pb - u.rgb, bookmark.key.suffix.Cb() );
		pb -= cbCopy;
		UtilMemCpy( pb, (BYTE*)bookmark.key.suffix.pv + bookmark.key.suffix.Cb() - cbCopy, cbCopy );

		cbCopy = min( pb - u.rgb, bookmark.key.prefix.Cb() );
		pb -= cbCopy;
		UtilMemCpy( pb, (BYTE*)bookmark.key.prefix.pv + bookmark.key.prefix.Cb() - cbCopy, cbCopy );

		uiHash += u.ul;
		}

	uiHash %= crceheadGlobalHashTable;
#endif	//	FAST_HASH

#ifdef SEDGEWICK_HASH
	//  This is taken from "Algorithms in C++" by Sedgewick
	UINT uiHash = ( ifmp + pgnoFDP ) % crceheadGlobalHashTable;	//  bookmark is only unique in one table of a database

	INT ib;
	const BYTE * pb;

	for ( ib = 0, pb = (BYTE *)bookmark.key.prefix.pv; ib < (INT)bookmark.key.prefix.Cb(); ++ib )
		{
		uiHash = ( _rotl( uiHash, 6 ) + pb[ib] ) % crceheadGlobalHashTable;
		}
	for ( ib = 0, pb = (BYTE *)bookmark.key.suffix.pv; ib < (INT)bookmark.key.suffix.Cb(); ++ib )
		{
		uiHash = ( _rotl( uiHash, 6 ) + pb[ib] ) % crceheadGlobalHashTable;
		}
	for ( ib = 0, pb = (BYTE *)bookmark.data.pv; ib < (INT)bookmark.data.cb; ++ib )
		{
		uiHash = ( _rotl( uiHash, 6 ) + pb[ib] ) % crceheadGlobalHashTable;
		}
#endif	//	SEDGEWICK_HASH

#ifdef SEDGEWICK_HASH2
	//  An optimized version of the hash function from "Algorithms in C++" by Sedgewick

	//lint -e616 -e646 -e744

	UINT uiHash = 	(UINT)ifmp
					+ pgnoFDP
					+ bookmark.key.prefix.Cb()
					+ bookmark.key.suffix.Cb()
					+ bookmark.data.Cb();

	INT ib;
	INT cb;
	const BYTE * pb;

	ib = 0;
	cb = (INT)bookmark.key.prefix.Cb();
	pb = (BYTE *)bookmark.key.prefix.Pv();
	switch( cb % 8 )
		{
		case 0:
		while ( ib < cb )
			{
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 7:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 6:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 5:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 4:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 3:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 2:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 1:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
			}
		}

	ib = 0;
	cb = (INT)bookmark.key.suffix.Cb();
	pb = (BYTE *)bookmark.key.suffix.Pv();
	switch( cb % 8 )
		{
		case 0:
		while ( ib < cb )
			{
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 7:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 6:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 5:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 4:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 3:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 2:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 1:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
			}
		}

	ib = 0;
	cb = (INT)bookmark.data.Cb();
	pb = (BYTE *)bookmark.data.Pv();
	switch( cb % 8 )
		{
		case 0:
		while ( ib < cb )
			{
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 7:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 6:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 5:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 4:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 3:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 2:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
		case 1:
			uiHash = ( _rotl( uiHash, 6 ) + pb[ib++] );
			}
		}

	uiHash %= crceheadGlobalHashTable;

	//lint -restore

#endif	//	SEDGEWICK_HASH2

	return uiHash;
	}


#ifdef DEBUGGER_EXTENSION

//  ================================================================
UINT UiVERHash( IFMP ifmp, PGNO pgnoFDP, const BOOKMARK& bookmark )
//  ================================================================
//
//  Used by the debugger extension
//
	{
	return UiRCHashFunc( ifmp, pgnoFDP, bookmark );
	}

#endif	//	DEBUGGER_EXTENSION

#ifdef DEBUG


//  ================================================================
BOOL RCE::FAssertCritHash_() const
//  ================================================================
	{
	Assert( ::FOperInHashTable( m_oper ) );
	return PverFromIfmp( Ifmp() )->CritRCEChain( m_uiHash ).FOwner();
	}


//  ================================================================
BOOL RCE::FAssertCritFCBRCEList_() const
//  ================================================================
	{
	Assert( !::FOperNull( m_oper ) );
	return Pfcb()->CritRCEList().FOwner();
	}


//  ================================================================
BOOL RCE::FAssertCritPIB_() const
//  ================================================================
	{
	Assert( !FFullyCommitted() );
	Assert( !::FOperNull( m_oper ) );
	return ( m_pfucb->ppib->critTrx.FOwner() || Ptls()->fAddColumn );
	}


//  ================================================================
BOOL RCE::FAssertReadable_() const
//  ================================================================
//
//  We can read the const members of a RCE if we are holding an accessing
//  critical section or the RCE is committed but younger than the oldest active
//  transaction or the RCE is older than the oldest transaction and we are
//  RCECleanup
//
//-
	{
	AssertRTL( !::FOperNull( m_oper ) );
	return fTrue;
	}


#endif	//	DEBUG

//  ================================================================
VOID RCE::AssertValid() const
//  ================================================================
	{
	Assert( FAlignedForAllPlatforms( this  ) );
	Assert( FAssertReadable_() );
	AssertRTL( rceidMin != m_rceid );
	AssertRTL( pfcbNil != Pfcb() );
	AssertRTL( trxMax != m_trxBegin0 );
#ifdef NO_BEGIN0_COMMIT0_LOCK
	AssertRTL( TrxCmp( m_trxBegin0, *m_ptrxCommitted ) < 0 );
	if ( trxMax == *m_ptrxCommitted )
		{
		ASSERT_VALID( m_pfucb );
		}
#else
	AssertRTL( TrxCmp( m_trxBegin0, m_trxCommitted ) < 0 );
	if ( trxMax == m_trxCommitted )
		{
		ASSERT_VALID( m_pfucb );
		}
#endif
	if ( ::FOperInHashTable( m_oper ) )
		{
		BOOKMARK	bookmark;
		GetBookmark( &bookmark );
		AssertRTL( UiRCHashFunc( Ifmp(), PgnoFDP(), bookmark ) == m_uiHash );
		}
	else
		{
		//  No-one can see these members if we are not in the hash table
		AssertRTL( prceNil == m_prceNextOfNode );
		AssertRTL( prceNil == m_prcePrevOfNode );
		}
	AssertRTL( !::FOperNull( m_oper ) );

	switch( m_oper )
		{
		default:
			AssertSzRTL( fFalse, "Invalid RCE operand" );
		case operReplace:
		case operInsert:
		case operReadLock:
		case operWriteLock:
		case operPreInsert:
		case operFlagDelete:
		case operDelta:
		case operAllocExt:
		case operSLVSpace:
		case operCreateTable:
		case operDeleteTable:
		case operAddColumn:
		case operDeleteColumn:
		case operCreateLV:
		case operCreateIndex:
		case operDeleteIndex:
		case operSLVPageAppend:
			break;
		};

#ifdef NO_BEGIN0_COMMIT0_LOCK
	if ( trxMax != m_trxCommittedInactive )
		{
		//  we are committed to level 0
		AssertRTL( 0 == m_level );
		AssertRTL( prceNil == m_prceNextOfSession );
		AssertRTL( prceNil == m_prcePrevOfSession );
		}
#else
	if ( trxMax != m_trxCommitted )
		{
		//  we are committed to level 0
		AssertRTL( 0 == m_level );
		AssertRTL( prceNil == m_prceNextOfSession );
		AssertRTL( prceNil == m_prcePrevOfSession );
		}
#endif

#ifdef SYNC_DEADLOCK_DETECTION
	//	UNDONE: must be in critRCEChain to make these checks
	//	(otherwise the RCE might get nullified from underneath us),
	//	but the FOwner() check currently doesn't work properly if
	//	SYNC_DEADLOCK_DETECTION is disabled

	if ( ::FOperInHashTable( m_oper )
		&& PverFromIfmp( Ifmp() )->CritRCEChain( m_uiHash ).FOwner() )
		{
		if ( prceNil != m_prceNextOfNode )
			{
			AssertRTL( m_rceid < m_prceNextOfNode->m_rceid
						|| operWriteLock == m_prceNextOfNode->Oper() );
			AssertRTL( this == m_prceNextOfNode->m_prcePrevOfNode );
			}

		if ( prceNil != m_prcePrevOfNode )
			{
			AssertRTL( m_rceid > m_prcePrevOfNode->m_rceid
						|| operWriteLock == Oper() );
			AssertRTL( this == m_prcePrevOfNode->m_prceNextOfNode );
#ifdef NO_BEGIN0_COMMIT0_LOCK
			if ( trxMax == *m_prcePrevOfNode->m_ptrxCommitted )
				{
				AssertRTL( TrxCmp( *m_ptrxCommitted, TrxVERISession( m_prcePrevOfNode->m_pfucb ) ) > 0 );
				}
#else
			if ( trxMax == m_prcePrevOfNode->m_trxCommitted )
				{
				AssertRTL( TrxCmp( m_trxCommitted, TrxVERISession( m_prcePrevOfNode->m_pfucb ) ) > 0 );
				}
#endif
			const BOOL	fRedoOrUndo	= ( fRecoveringRedo == PinstFromIfmp( Ifmp() )->m_plog->m_fRecoveringMode
										|| fRecoveringUndo == PinstFromIfmp( Ifmp() )->m_plog->m_fRecoveringMode );
			if( !fRedoOrUndo )
				{
				const BOOL	fPrevRCEIsDelete	= ( operFlagDelete == m_prcePrevOfNode->m_oper
													&& !m_prcePrevOfNode->FMoved() );
				if( fPrevRCEIsDelete )
					{
					switch ( m_oper )
						{
						case operInsert:
						case operPreInsert:
						case operWriteLock:
							//	these are the only valid operations after a delete
							break;

						default:
							{
							//  these cases should have caused a write conflict
							//	in VERModify
							//
							//	due to OLDSLV we can do a versioned delete,
							//	unversioned insert and then a versioned replace
							//	in the slv ownership map or slv avail trees
							const BOOL fIsSLVTree = ( m_pfcb->FTypeSLVAvail() || m_pfcb->FTypeSLVOwnerMap() );
							AssertSzRTL( fIsSLVTree, "RCEs should have write-conflicted" );
							}
						}
					}
				}
			}
		}

#endif	//	SYNC_DEADLOCK_DETECTION

	}


#ifndef RTM


//  ================================================================
ERR VER::ErrCheckRCEChain( const RCE * const prce, const UINT uiHash ) const
//  ================================================================
	{
	const RCE	* prceChainOld	= prceNil;
	const RCE	* prceChain		= prce;
	while ( prceNil != prceChain )
		{
		AssertRTL( prceChain->PrceNextOfNode() == prceChainOld );
		prceChain->AssertValid();
		AssertRTL( prceChain->PgnoFDP() == prce->PgnoFDP() );
		AssertRTL( prceChain->Ifmp() == prce->Ifmp() );
		AssertRTL( prceChain->UiHash() == uiHash );
		AssertRTL( !prceChain->FOperDDL() );

		prceChainOld	= prceChain;
		prceChain 		= prceChain->PrcePrevOfNode();
		}
	return JET_errSuccess;
	}


//  ================================================================
ERR VER::ErrCheckRCEHashList( const RCE * const prce, const UINT uiHash ) const
//  ================================================================
	{
	ERR err = JET_errSuccess;
	const RCE * prceT = prce;
	for ( ; prceNil != prceT; prceT = prceT->PrceHashOverflow() )
		{
		CallR( ErrCheckRCEChain( prceT, uiHash ) );
		}
	return err;
	}


//  ================================================================
ERR VER::ErrInternalCheck()
//  ================================================================
//
//  check the consistency of the entire hash table, entering the
//  critical sections if needed
//
//-
	{
	ERR err = JET_errSuccess;
	UINT uiHash = 0;
	for ( ; uiHash < crceheadGlobalHashTable; ++uiHash )
		{
		if( NULL != GetChain( uiHash ) )
			{
			ENTERCRITICALSECTION crit( &(CritRCEChain( uiHash )) );
			const RCE * const prce = GetChain( uiHash );
			CallR( ErrCheckRCEHashList( prce, uiHash ) );
			}
		}
	return err;
	}


#endif  //  !RTM


//  ================================================================
INLINE RCE::RCE(
			FCB		*pfcb,
			FUCB	*pfucb,
			UPDATEID	updateid,
			TRX		trxBegin0,
#ifdef NO_BEGIN0_COMMIT0_LOCK
			TRX *	ptrxCommit0,
#endif
			LEVEL	level,
			USHORT	cbBookmarkKey,
			USHORT	cbBookmarkData,
			USHORT	cbData,
			OPER	oper,
			UINT	uiHash,
			BOOL	fProxy,
			RCEID	rceid
			) :
//  ================================================================
	m_pfcb( pfcb ),
	m_pfucb( pfucb ),
	m_ifmp( pfcb->Ifmp() ),
	m_pgnoFDP( pfcb->PgnoFDP() ),
	m_updateid( updateid ),
	m_trxBegin0( trxBegin0 ),
	m_cbBookmarkKey( cbBookmarkKey ),
	m_cbBookmarkData( cbBookmarkData ),
	m_cbData( cbData ),
	m_level( level ),
	m_prceNextOfNode( prceNil ),
	m_prcePrevOfNode( prceNil ),
#ifdef NO_BEGIN0_COMMIT0_LOCK
	m_trxCommittedInactive( trxMax ),
	m_ptrxCommitted( NULL != ptrxCommit0 ? ptrxCommit0 : &m_trxCommittedInactive ),
#else
	m_trxCommitted( trxMax ),
#endif
	m_oper( (USHORT)oper ),
	m_uiHash( uiHash ),
	m_pgnoUndoInfo( pgnoNull ),
	m_fRolledBack( fFalse ),			//  protected by m_critBucketGlobal
	m_fMoved( fFalse ),
	m_fProxy( fProxy ? fTrue : fFalse ),
	m_rceid( rceid )
		{
#ifdef DEBUG
		Assert( m_rceid > rceidMin );
		AssertSz( m_rceid < LONG_MAX, "rceid overflow" );
		m_prceUndoInfoNext				= prceInvalid;
		m_prceUndoInfoPrev				= prceInvalid;
		m_prceHashOverflow				= prceInvalid;
		m_prceNextOfSession				= prceInvalid;
		m_prcePrevOfSession				= prceInvalid;
		m_prceNextOfFCB					= prceInvalid;
		m_prcePrevOfFCB					= prceInvalid;
#endif	//	DEBUG

		}


//  ================================================================
INLINE BYTE * RCE::PbBookmark()
//  ================================================================
	{
	Assert( FAssertCritHash_() );
	return m_rgbData + m_cbData;
	}


//  ================================================================
INLINE RCE *&RCE::PrceHashOverflow()
//  ================================================================
	{
	Assert( FAssertCritHash_() );
	return m_prceHashOverflow;
	}


//  ================================================================
INLINE VOID RCE::SetPrceHashOverflow( RCE * prce )
//  ================================================================
	{
	Assert( FAssertCritHash_() );
	m_prceHashOverflow = prce;
	}


//  ================================================================
INLINE VOID RCE::SetPrceNextOfNode( RCE * prce )
//  ================================================================
	{
	Assert( FAssertCritHash_() );
	Assert( prceNil == prce
		|| m_rceid < prce->Rceid() );
	m_prceNextOfNode = prce;
	}


//  ================================================================
INLINE VOID RCE::SetPrcePrevOfNode( RCE * prce )
//  ================================================================
	{
	Assert( FAssertCritHash_() );
	Assert( prceNil == prce
		|| m_rceid > prce->Rceid() );
	m_prcePrevOfNode = prce;
	}


//  ================================================================
INLINE VOID RCE::FlagRolledBack()
//  ================================================================
	{
	Assert( FAssertCritPIB_() );
	m_fRolledBack = fTrue;
	}


//  ================================================================
INLINE VOID RCE::FlagMoved()
//  ================================================================
	{
	m_fMoved = fTrue;
	}


//  ================================================================
INLINE VOID RCE::SetPrcePrevOfSession( RCE * prce )
//  ================================================================
	{
	m_prcePrevOfSession = prce;
	}


//  ================================================================
INLINE VOID RCE::SetPrceNextOfSession( RCE * prce )
//  ================================================================
	{
	m_prceNextOfSession = prce;
	}


//  ================================================================
INLINE VOID RCE::SetLevel( LEVEL level )
//  ================================================================
	{
	Assert( FAssertCritPIB_() || PinstFromIfmp( m_ifmp )->m_plog->m_fRecovering );
	Assert( m_level > level );	// levels are always decreasing
	m_level = level;
	}


//  ================================================================
INLINE VOID RCE::SetTrxCommitted( const TRX trx )
//  ================================================================
	{
	Assert( !FOperInHashTable() || FAssertCritHash_() );
	Assert( FAssertCritPIB_() || PinstFromIfmp( m_ifmp )->m_plog->m_fRecovering );
	Assert( FAssertCritFCBRCEList_()  );
	Assert( prceNil == m_prcePrevOfSession );	//  only uncommitted RCEs in session list
	Assert( prceNil == m_prceNextOfSession );
	Assert( prceInvalid == m_prceUndoInfoNext );	//  no before image when committing to 0
	Assert( prceInvalid == m_prceUndoInfoPrev );
	Assert( pgnoNull == m_pgnoUndoInfo );
	Assert( TrxCmp( trx, m_trxBegin0 ) > 0 );

#ifdef NO_BEGIN0_COMMIT0_LOCK
	//	ptrxCommitted should currently be pointing into the
	//	owning sessions's PIB, since the session has now
	//	committed, we want to copy the trxCommit0 into
	//	the RCE and switch ptrxCommitted to now point there
	//
	Assert( trx == *m_ptrxCommitted );
	Assert( trxMax == m_trxCommittedInactive );
	m_trxCommittedInactive = trx;
	m_ptrxCommitted = &m_trxCommittedInactive;
#else
	Assert( trxMax == m_trxCommitted );
	m_trxCommitted = trx;
#endif
	}


//  ================================================================
INLINE VOID RCE::NullifyOper()
//  ================================================================
//
//  This is the end of the RCEs lifetime. It must have been removed
//  from all lists
//
//-
	{
	Assert( prceNil == m_prceNextOfNode );
	Assert( prceNil == m_prcePrevOfNode );
	Assert( prceNil == m_prceNextOfSession || prceInvalid == m_prceNextOfSession );
	Assert( prceNil == m_prcePrevOfSession || prceInvalid == m_prcePrevOfSession );
	Assert( prceNil == m_prceNextOfFCB || prceInvalid == m_prceNextOfFCB );
	Assert( prceNil == m_prcePrevOfFCB || prceInvalid == m_prcePrevOfFCB );
	Assert( prceInvalid == m_prceUndoInfoNext );
	Assert( prceInvalid == m_prceUndoInfoPrev );
	Assert( pgnoNull == m_pgnoUndoInfo );

	m_oper |= operMaskNull;

#ifdef NO_BEGIN0_COMMIT0_LOCK
	//	make sure ptrxCommitted points into the RCE
	//
	m_ptrxCommitted = &m_trxCommittedInactive;
#endif
	}


//  ================================================================
INLINE VOID RCE::NullifyOperForMove()
//  ================================================================
//
//	RCE has been copied elsewhere -- nullify this copy
//
//-
	{
	m_oper |= ( operMaskNull | operMaskNullForMove );
	}


//  ================================================================
LOCAL BOOL FRCECorrect( IFMP ifmp, PGNO pgnoFDP, const BOOKMARK& bookmark, const RCE * prce )
//  ================================================================
//
//	Checks whether a RCE describes the given BOOKMARK
//
//-
	{
	ASSERT_VALID( prce );

	BOOL fRCECorrect = fFalse;

	//  same database and table
	if ( prce->Ifmp() == ifmp && prce->PgnoFDP() == pgnoFDP )
		{
		if ( bookmark.key.Cb() == prce->CbBookmarkKey() && (INT)bookmark.data.Cb() == prce->CbBookmarkData() )
			{
			//  bookmarks are the same length
			BOOKMARK bookmarkRCE;
			prce->GetBookmark( &bookmarkRCE );

			fRCECorrect = ( CmpKeyData( bookmark, bookmarkRCE ) == 0 );
			}
		}
	return fRCECorrect;
	}


//  ================================================================
LOCAL RCE **PprceRCEChainGet( UINT uiHash, IFMP ifmp, PGNO pgnoFDP, const BOOKMARK& bookmark )
//  ================================================================
//
//	Given a BOOKMARK, get the correct RCEHEAD.
//
//-
	{
	Assert( PverFromIfmp( ifmp )->CritRCEChain( uiHash ).FOwner() );

	AssertRTL( UiRCHashFunc( ifmp, pgnoFDP, bookmark ) == uiHash );

	RCE **pprceChain = PverFromIfmp( ifmp )->PGetChain( uiHash );
	while ( prceNil != *pprceChain )
		{
		RCE * const prceT = *pprceChain;

		AssertRTL( prceT->UiHash() == uiHash );

		if ( FRCECorrect( ifmp, pgnoFDP, bookmark, prceT ) )
			{
			Assert( prceNil == prceT->PrcePrevOfNode()
				|| prceT->PrcePrevOfNode()->UiHash() == prceT->UiHash() );
			if ( prceNil == prceT->PrceNextOfNode() )
				{
				Assert( prceNil == prceT->PrceHashOverflow()
					|| prceT->PrceHashOverflow()->UiHash() == prceT->UiHash() );
				}
			else
				{
				//	if there is a NextOfNode, then we are not in the hash overflow
				//	chain, so can't check PrceHashOverflow().
				Assert( prceT->PrceNextOfNode()->UiHash() == prceT->UiHash() );
				}

#ifdef DEBUG
			if ( prceNil != prceT->PrcePrevOfNode()
				&& prceT->PrcePrevOfNode()->UiHash() != prceT->UiHash() )
				{
				Assert( fFalse );
				}
			if ( prceNil != prceT->PrceNextOfNode() )
				{
				if ( prceT->PrceNextOfNode()->UiHash() != prceT->UiHash() )
					{
					Assert( fFalse );
					}
				}
			//	can only check prceHashOverflow if there is no prceNextOfNode
			else if ( prceNil != prceT->PrceHashOverflow()
				&& prceT->PrceHashOverflow()->UiHash() != prceT->UiHash() )
				{
				Assert( fFalse );
				}
#endif

			return pprceChain;
			}

		pprceChain = &prceT->PrceHashOverflow();
		}

	Assert( prceNil == *pprceChain );
	return NULL;
	}


//  ================================================================
LOCAL RCE *PrceRCEGet( UINT uiHash, IFMP ifmp, PGNO pgnoFDP, const BOOKMARK& bookmark )
//  ================================================================
//
//	Given a BOOKMARK, get the correct hash chain of RCEs.
//
//-
	{
	ASSERT_VALID( &bookmark );

	AssertRTL( UiRCHashFunc( ifmp, pgnoFDP, bookmark ) == uiHash );

	RCE *			prceChain	= prceNil;
	RCE **	const	pprceChain	= PprceRCEChainGet( uiHash, ifmp, pgnoFDP, bookmark );
	if ( NULL != pprceChain )
		{
		prceChain = *pprceChain;
		}

	return prceChain;
	}


//  ================================================================
LOCAL RCE * PrceFirstVersion ( const UINT uiHash, const FUCB * pfucb, const BOOKMARK& bookmark )
//  ================================================================
//
//  gets the first version of a node
//
//-
	{
	RCE * const prce = PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );

	Assert( prceNil == prce
			|| prce->Oper() == operReplace
			|| prce->Oper() == operInsert
			|| prce->Oper() == operFlagDelete
			|| prce->Oper() == operDelta
			|| prce->Oper() == operReadLock
			|| prce->Oper() == operWriteLock
			|| prce->Oper() == operPreInsert
			|| prce->Oper() == operSLVSpace
			);

	return prce;
	}


//  ================================================================
LOCAL const RCE *PrceVERIGetPrevReplace( const RCE * const prce )
//  ================================================================
//
//	Find previous replace of changed by the session within
//	current transaction. The found rce may be in different transaction level.
//
//-
	{
	//	Look for previous replace on this node of this transaction ( level > 0 ).
	const RCE * prcePrevReplace = prce->PrcePrevOfNode();
	for ( ; prceNil != prcePrevReplace
			&& !prcePrevReplace->FOperReplace()
			&& prcePrevReplace->TrxCommitted() == trxMax;
		 prcePrevReplace = prcePrevReplace->PrcePrevOfNode() )
		;

	//  did we find a previous replace operation at level greater than 0
	if ( prceNil != prcePrevReplace )
		{
		if ( !prcePrevReplace->FOperReplace()
			|| prcePrevReplace->TrxCommitted() != trxMax )
			{
			prcePrevReplace = prceNil;
			}
		else
			{
			Assert( prcePrevReplace->FOperReplace() );
			Assert( trxMax == prcePrevReplace->TrxCommitted() );
			Assert( prce->Pfucb()->ppib == prcePrevReplace->Pfucb()->ppib );
			}
		}

	return prcePrevReplace;
	}


//  ================================================================
BOOL FVERActive( const IFMP ifmp, const PGNO pgnoFDP, const BOOKMARK& bm, const TRX trxSession )
//  ================================================================
//
//	is there a version on the node that is visible to an uncommitted trx?
//
//-
	{
	ASSERT_VALID( &bm );

	const UINT				uiHash		= UiRCHashFunc( ifmp, pgnoFDP, bm );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( ifmp )->CritRCEChain( uiHash ) ) );

	BOOL					fVERActive	= fFalse;

	//	get RCE
	const RCE * prce = PrceRCEGet( uiHash, ifmp, pgnoFDP, bm );
	for ( ; prceNil != prce; prce = prce->PrcePrevOfNode() )
		{
		if ( TrxCmp( prce->TrxCommitted(), trxSession ) > 0 )
			{
			fVERActive = fTrue;
			break;
			}
		}

	return fVERActive;
	}


//  ================================================================
BOOL FVERActive( const FUCB * pfucb, const BOOKMARK& bm, const TRX trxSession )
//  ================================================================
//
//	is there a version on the node that is visible to an uncommitted trx?
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bm );

	Assert( trxMax != trxSession || PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering );

	const UINT				uiHash		= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bm );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );

	BOOL					fVERActive	= fFalse;

	//	get RCE
	const RCE * prce = PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bm );
	for ( ; prceNil != prce; prce = prce->PrcePrevOfNode() )
		{
		if ( TrxCmp( prce->TrxCommitted(), trxSession ) > 0 )
			{
			fVERActive = fTrue;
			break;
			}
		}

	return fVERActive;
	}


//  ================================================================
INLINE BOOL FVERIAddUndoInfo( const RCE * const prce )
//  ================================================================
//
//  Do we need to create a deferred before image for this RCE?
//
//	UNDONE:	do not create dependency for
//				replace at same level as before
	{
	BOOL	fAddUndoInfo = fFalse;

	Assert( prce->TrxCommitted() == trxMax );

	Assert( !rgfmp[prce->Pfucb()->ifmp].FLogOn() || !PinstFromIfmp( prce->Pfucb()->ifmp )->m_plog->m_fLogDisabled );
	if ( rgfmp[prce->Pfucb()->ifmp].FLogOn() )
		{
		if ( prce->FOperReplace() )
			{
//			ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( prce->Ifmp() )->CritRCEChain( prce->UiHash() ) ) );
//			const RCE				*prcePrevReplace = PrceVERIGetPrevReplace( prce );

			//	if previous replace is at the same level
			//	before image for rollback is in the other RCE
//			Assert( prce->Level() > 0 );
//			if ( prceNil == prcePrevReplace
//				|| prcePrevReplace->Level() != prce->Level() )
//				{
				fAddUndoInfo = fTrue;
//				}
			}
		else if ( operFlagDelete == prce->Oper() )
			{
			fAddUndoInfo = fTrue;
			}
		else
			{
			//	these operations log the logical bookmark or do not log
			Assert( operInsert == prce->Oper()
					|| operDelta == prce->Oper()
					|| operReadLock == prce->Oper()
			 		|| operWriteLock == prce->Oper()
			 		|| operPreInsert == prce->Oper()
			 		|| operAllocExt == prce->Oper()
			 		|| operSLVSpace == prce->Oper()
			 		|| operCreateLV == prce->Oper()
			 		|| operSLVPageAppend == prce->Oper()
			 		|| prce->FOperDDL() );
			}
		}

	return fAddUndoInfo;
	}


//  ================================================================
VOID VERMoveUndoInfo( FUCB *pfucb, CSR *pcsrSrc, CSR *pcsrDest, const KEY& keySep )
//  ================================================================
//
//	moves undo info of nodes >= keySep from pgnoSrc to pgnoDest
//
//-
	{
	//  move all undo info of nodes whose keys are GTE the seperator key from the
	//  source page to the destination page.  if the destination page is not Write
	//  Latched, we can throw away the undo info because, by convention, we are
	//  recovering and that page does not need to be redone

	BFLatch* pbflDest = NULL;
	if (	pcsrDest->Latch() == latchWrite &&
			rgfmp[ pfucb->ifmp ].FLogOn() )
		{
		Assert( !PinstFromIfmp( pfucb->ifmp )->m_plog->m_fLogDisabled );
		pbflDest = pcsrDest->Cpage().PBFLatch();
		}

	BFMoveUndoInfo( pcsrSrc->Cpage().PBFLatch(), pbflDest, keySep );
	}


//  ================================================================
ERR VER::ErrVERIAllocateRCE( INT cbRCE, RCE ** pprce, const UINT uiHash )
//  ================================================================
//
//  Allocates enough space for a new RCE or return out of memory
//  error. New buckets may be allocated
//
//-
	{
	Assert( m_critBucketGlobal.FOwner() );

	ERR err = JET_errSuccess;

	//	if insufficient bucket space, then allocate new bucket.

	if ( m_pbucketGlobalHead == pbucketNil || cbRCE > CbBUFree( m_pbucketGlobalHead ) )
		{
		Call( ErrVERIBUAllocBucket( cbRCE, uiHash ) );
		}
	Assert( cbRCE <= CbBUFree( m_pbucketGlobalHead ) );

	//	pbucket always on double-word boundary
	Assert( FAlignedForAllPlatforms( m_pbucketGlobalHead ) );

	//	set prce to next avail RCE location, and assert aligned

	*pprce = m_pbucketGlobalHead->hdr.prceNextNew;
	m_pbucketGlobalHead->hdr.prceNextNew =
		reinterpret_cast<RCE *>( PvAlignForAllPlatforms( reinterpret_cast<BYTE *>( *pprce ) + cbRCE ) );

	Assert( FAlignedForAllPlatforms( *pprce ) );
	Assert( FAlignedForAllPlatforms( m_pbucketGlobalHead->hdr.prceNextNew ) );

HandleError:
	return err;
	}


#ifdef MOVEABLE_RCE

//  ================================================================
LOCAL SIZE_T CbBUMoveFree( const volatile BUCKET * const pbucket )
//  ================================================================
	{
	const BYTE * const pbOldest = reinterpret_cast<BYTE *>( pbucket->hdr.prceOldest );
	const BYTE * const pbDelete = pbucket->hdr.pbLastDelete;
	Assert( pbOldest >= pbDelete );
	return pbOldest - pbDelete;
	}

//  ================================================================
ERR VER::ErrVERIMoveRCE( RCE * prce )
//  ================================================================
//
//  Allocates enough space for a new RCE or return out of memory
//  error. New buckets may NOT be allocated
//
//-
	{
	Assert( m_critRCEClean.FOwner() );

	m_critBucketGlobal.Enter();

	Assert( pbucketNil != m_pbucketGlobalHead );
	Assert( pbucketNil != m_pbucketGlobalTail );
	Assert( m_pbucketGlobalHead != m_pbucketGlobalTail );

	const LONG_PTR cbRce = IbAlignForAllPlatforms( prce->CbRce() );

	if ( pbucketNil == m_pbucketGlobalLastDelete )
		{
		m_pbucketGlobalLastDelete = m_pbucketGlobalTail;
		}
	Assert( ( reinterpret_cast<ULONG_PTR>( m_pbucketGlobalLastDelete->hdr.pbLastDelete ) % sizeof( TRX ) ) == 0 );

	//  advance until we find a bucket that has enough space or we discover that
	//  we are the first non-cleaned RCE in the bucket
	while (
		pbucketNil != m_pbucketGlobalLastDelete
		&& m_pbucketGlobalLastDelete->hdr.prceOldest == m_pbucketGlobalLastDelete->hdr.prceNextNew	//  completely cleaned
		&& cbRce > CbBUMoveFree( m_pbucketGlobalLastDelete )
		)
		{
		++m_cbucketGlobalAllocDelete;
		m_pbucketGlobalLastDelete = m_pbucketGlobalLastDelete->hdr.pbucketNext;
#ifdef VERPERF
		cVERcbucketDeleteAllocated.Inc( m_pinst );
#endif // VERPERF
		}

	const BOOL	fMoveable	= ( pbucketNil != m_pbucketGlobalLastDelete
								&& cbRce <= CbBUMoveFree( m_pbucketGlobalLastDelete ) );

	m_critBucketGlobal.Leave();

	if ( !fMoveable )
		{
		//  the version store is too full. throw away this RCE
///		return ErrERRCheck( JET_errVersionStoreOutOfMemory );
		return JET_errSuccess;
		}

	//	assert we don't overlap the RCE we're moving
	Assert( (BYTE *)prce < m_pbucketGlobalLastDelete->hdr.pbLastDelete
		|| (BYTE *)prce >= m_pbucketGlobalLastDelete->hdr.pbLastDelete + cbRce );
	Assert( m_pbucketGlobalLastDelete->hdr.pbLastDelete < (BYTE *)prce
		|| m_pbucketGlobalLastDelete->hdr.pbLastDelete >= (BYTE *)prce + cbRce );

	//  we found enough free space in this bucket. copy the RCE here
	RCE * const prceNewLocation = reinterpret_cast<RCE *>( m_pbucketGlobalLastDelete->hdr.pbLastDelete );
	Assert( prceNil != prceNewLocation );

	ENTERCRITICALSECTION	enterCritChain( &( prce->CritChain() ) );
	ENTERCRITICALSECTION	enterCritRCEList( &( prce->Pfcb()->CritRCEList() ) );

	FCB * const pfcb	= prce->Pfcb();

	//	skip the RCE if:
	//		- someone nullified the RCE from underneath us (eg. VERNullifyAllVersionsOnBM())
	//		- the FDP is scheduled for deletion
	if ( prce->FOperNull() || pfcb->FDeletePending() )
		return JET_errSuccess;

	RCE * const prceNextOfNode = prce->PrceNextOfNode();
	RCE * const prcePrevOfNode = prce->PrcePrevOfNode();

	RCE * const prceNextOfFCB = prce->PrceNextOfFCB();
	RCE * const prcePrevOfFCB = prce->PrcePrevOfFCB();

	BOOKMARK	bookmark;
	prce->GetBookmark( &bookmark );

	RCE ** pprceChain	= NULL;
	if ( prceNil == prceNextOfNode )
		{
		pprceChain	= PprceRCEChainGet(
							prce->UiHash(),
							prce->Ifmp(),
							prce->PgnoFDP(),
							bookmark );
		Assert( NULL != pprceChain );

		if ( NULL == pprceChain )
			{
			FireWall();
			return JET_errSuccess;
			}
		}

	//	only increment pbLastDelete when we're sure the move will occur
	m_pbucketGlobalLastDelete->hdr.pbLastDelete += cbRce;

	//	assert that the two memory locations don't overlap
	Assert( (BYTE *)prce < (BYTE *)prceNewLocation
		|| (BYTE *)prce >= (BYTE *)prceNewLocation + prce->CbRce() );
	Assert( (BYTE *)prceNewLocation < (BYTE *)prce
		|| (BYTE *)prceNewLocation >= (BYTE *)prce + prce->CbRce() );

	memcpy( prceNewLocation, prce, prce->CbRce() );
	prceNewLocation->FlagMoved();

	//	nullify old RCE to ensure RCEClean doesn't try to clean it
	prce->NullifyOperForMove();

	//  Do not use the old RCE after this point!

	if ( prceNil == prceNextOfNode )
		{
		Assert( NULL != pprceChain );
		*pprceChain = prceNewLocation;
		}
	else
		{
		Assert( prceNil != prceNextOfNode );
		prceNextOfNode->SetPrcePrevOfNode( prceNewLocation );
		}

	if ( prceNil != prcePrevOfNode )
		{
		prcePrevOfNode->SetPrceNextOfNode( prceNewLocation );
		}

	if ( prceNil == prceNextOfFCB )
		{
		//  we were at the top of the FCB chain
		Assert( pfcb->PrceNewest() == prce );
		pfcb->SetPrceNewest( prceNewLocation );
		}
	else
		{
		Assert( pfcb->PrceNewest() != prce );
		prceNextOfFCB->SetPrcePrevOfFCB( prceNewLocation );
		}

	if ( prceNil == prcePrevOfFCB )
		{
		//  we were at the end of the FCB chain
		Assert( pfcb->PrceOldest() == prce );
		pfcb->SetPrceOldest( prceNewLocation );
		}
	else
		{
		Assert( pfcb->PrceOldest() != prce );
		prcePrevOfFCB->SetPrceNextOfFCB( prceNewLocation );
		}

	Assert( prceNewLocation->PrceNextOfFCB() == NULL
			|| prceNewLocation->PrceNextOfFCB()->Pfcb() == pfcb );
	Assert( prceNewLocation->PrcePrevOfFCB() == NULL
			|| prceNewLocation->PrcePrevOfFCB()->Pfcb() == pfcb );

#ifdef VERPERF
	++m_crceMoved;
#endif	//	VERPERF


	return wrnVERRCEMoved;
	}

#endif	//	MOVEABLE_RCE


//  ================================================================
ERR VER::ErrVERICreateRCE(
	INT			cbNewRCE,
	FCB			*pfcb,
	FUCB		*pfucb,
	UPDATEID	updateid,
	const TRX	trxBegin0,
#ifdef NO_BEGIN0_COMMIT0_LOCK
	TRX *		ptrxCommit0,
#endif
	const LEVEL	level,
	INT			cbBookmarkKey,
	INT			cbBookmarkData,
	OPER		oper,
	UINT		uiHash,
	RCE			**pprce,
	const BOOL	fProxy,
	RCEID		rceid
	)
//  ================================================================
//
//  Allocate a new RCE in a bucket. m_critBucketGlobal is used to
//  protect the RCE until its oper and trxCommitted are set
//
//-
	{
	ERR			err					= JET_errSuccess;
	RCE *		prce				= prceNil;
	UINT		uiHashConcurrentOp;

	// For concurrent operations, we must be in critHash
	// from the moment the rceid is allocated until the RCE
	// is placed in the hash table, otherwise we may get
	// rceid's out of order.
	if ( FOperConcurrent( oper ) )
		{
		Assert( uiHashInvalid != uiHash );
		CritRCEChain( uiHash ).Enter();
		uiHashConcurrentOp = uiHash;
		}
	else
		{
		uiHashConcurrentOp = uiHashInvalid;
		}

	m_critBucketGlobal.Enter();

	Assert( pfucbNil == pfucb ? 0 == level : level > 0 );

	Call( ErrVERIAllocateRCE( cbNewRCE, &prce, uiHashConcurrentOp ) );

#ifdef DEBUG
	if ( !PinstFromIfmp( pfcb->Ifmp() )->m_plog->m_fRecovering )
		{
		Assert( rceidNull == rceid );
		}
	else
		{
		Assert( rceidNull != rceid && uiHashInvalid != uiHash ||
				rceidNull == rceid && uiHashInvalid == uiHash );
		}
#endif

	Assert( trxMax != trxBegin0 );
#ifdef NO_BEGIN0_COMMIT0_LOCK
	Assert( NULL == ptrxCommit0 || trxMax == *ptrxCommit0 );
#endif

	//	UNDONE: break this new function into two parts. One that only need to
	//	UNDONE: be recognizable by Clean up (trxTime?) and release
	//	UNDONE: the m_critBucketGlobal as soon as possible.

	new( prce ) RCE(
			pfcb,
			pfucb,
			updateid,
			trxBegin0,
#ifdef NO_BEGIN0_COMMIT0_LOCK
			ptrxCommit0,
#endif
			level,
			USHORT( cbBookmarkKey ),
			USHORT( cbBookmarkData ),
			USHORT( cbNewRCE - ( sizeof(RCE) + cbBookmarkKey + cbBookmarkData ) ),
			oper,
			uiHash,
			fProxy,
			rceidNull == rceid ? RCE::RceidLastIncrement() : rceid
			);	//lint !e522

HandleError:
	m_critBucketGlobal.Leave();

	if ( err >= 0 )
		{
		Assert( prce != prceNil );

		//	check RCE
		Assert( FAlignedForAllPlatforms( prce ) );
		Assert( (RCE *)PvAlignForAllPlatforms( prce ) == prce );

		*pprce = prce;
		}
	else if ( err < JET_errSuccess && FOperConcurrent( oper ) )
		{
		CritRCEChain( uiHash ).Leave();
		}

	return err;
	}


//  ================================================================
LOCAL ERR ErrVERIInsertRCEIntoHash( RCE * const prce )
//  ================================================================
//
//	Inserts an RCE to hash table. We must already be in the critical
//  section of the hash chain.
//
//-
	{
	BOOKMARK	bookmark;
	prce->GetBookmark( &bookmark );
#ifdef DEBUG_VER
	Assert( UiRCHashFunc( prce->Ifmp(), prce->PgnoFDP(), bookmark ) == prce->UiHash() );
#endif	//	DEBUG_VER

	RCE ** const	pprceChain	= PprceRCEChainGet( prce->UiHash(),
													prce->Ifmp(),
													prce->PgnoFDP(),
													bookmark );

	if ( pprceChain )
		{
		//	hash chain for node already exists
		Assert( *pprceChain != prceNil );

		//	insert in order of rceid
		Assert( prce->Rceid() != rceidNull );

		RCE * prceNext		= prceNil;
		RCE * prcePrev 		= *pprceChain;
		const RCEID rceid 	= prce->Rceid();


		if( PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering )
			{
			for ( ;
				  prcePrev != prceNil && prcePrev->Rceid() >= rceid;
				  prceNext = prcePrev,
				  prcePrev = prcePrev->PrcePrevOfNode() )
				{
				if ( prcePrev->Rceid() == rceid )
					{
					//	we are recovering and have already created this RCE (redo)
					//
					//	that means we've encountered a deferred undo-info
					//	log record for this operation
					//
					//	this RCE may have a pgno undo info. In which case the
					//	page cannot be flushed (we have no facilities to log
					//	deferred undo-info log records during recovery)
					//
					//	*but*
					//
					//	we've now encountered a deferred-undo log record which
					//	means it is in fact safe to flush this page
					//
					//	remove the undo info

					if( pgnoNull != prcePrev->PgnoUndoInfo() )
						{
						BFRemoveUndoInfo( prce );
						}

					//	release last version created
					//	remove RCE from bucket
					//
					Assert( PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering );
					Assert( FAlignedForAllPlatforms( prce ) );

				    VER *pver = PverFromIfmp( prce->Ifmp() );
					Assert( (RCE *)PvAlignForAllPlatforms( (BYTE *)prce + prce->CbRce() ) ==
								pver->m_pbucketGlobalHead->hdr.prceNextNew );
					pver->m_pbucketGlobalHead->hdr.prceNextNew = prce;

					ERR	err = ErrERRCheck( JET_errPreviousVersion );
					return err;
					}
				}
			}
		else
			{

			//	if we are not recovering don't insert in RCEID order -- if we have waited
			//	for a wait-latch we actually want to insert after it

			}

		//  adjust head links
		if ( prceNil == prceNext )
			{
			//	insert before first rce in chain
			//
			Assert( prcePrev == *pprceChain );
			prce->SetPrceHashOverflow( (*pprceChain)->PrceHashOverflow() );
			*pprceChain = prce;

			#ifdef DEBUG
			prcePrev->SetPrceHashOverflow( prceInvalid );
			#endif	//	DEBUG
			Assert( prceNil == prce->PrceNextOfNode() );
			}
		else
			{
			Assert( PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering );
			Assert( prcePrev != prceNext );
			Assert( prcePrev != *pprceChain );

			prceNext->SetPrcePrevOfNode( prce );
			prce->SetPrceNextOfNode( prceNext );

			Assert( prce->UiHash() == prceNext->UiHash() );
			}

		if ( prcePrev != prceNil )
			{
			prcePrev->SetPrceNextOfNode( prce );

			Assert( prce->UiHash() == prcePrev->UiHash() );
			}

		//  adjust RCE links
		prce->SetPrcePrevOfNode( prcePrev );
		}
	else
		{
		Assert( prceNil == prce->PrceNextOfNode() );
		Assert( prceNil == prce->PrcePrevOfNode() );

		//	create new rce chain
		prce->SetPrceHashOverflow( PverFromIfmp( prce->Ifmp() )->GetChain( prce->UiHash() ) );
		PverFromIfmp( prce->Ifmp() )->SetChain( prce->UiHash(), prce );
		}

	Assert( prceNil != prce->PrceNextOfNode()
		|| prceNil == prce->PrceHashOverflow()
		|| prce->PrceHashOverflow()->UiHash() == prce->UiHash() );

#ifdef DEBUG
	if ( prceNil == prce->PrceNextOfNode()
		&& prceNil != prce->PrceHashOverflow()
		&& prce->PrceHashOverflow()->UiHash() != prce->UiHash() )
		{
		Assert( fFalse );
		}
#endif

#ifdef DEBUG_VER
	Assert( UiRCHashFunc( prce->Ifmp(), prce->PgnoFDP(), bookmark ) == prce->UiHash() );
#endif	//	DEBUG_VER

	Assert( prceNil == prce->PrceNextOfNode() ||
			PinstFromIfmp( prce->Ifmp() )->m_plog->m_fRecovering && prce->PrceNextOfNode()->Rceid() > prce->Rceid() );

	Assert( prce->Pfcb() != pfcbNil );

	//  monitor statistics
#ifdef VERPERF
	{
	//	Do not need critical section here. It is OK to miss a little.

	const INT cbBookmark = prce->CbBookmarkKey() + prce->CbBookmarkData();
    VER *pver = PverFromIfmp( prce->Ifmp() );
    INST *pinst = pver->m_pinst;
    cVERcrceHashEntries.Inc( pinst );
	cVERcbBookmarkTotal.Add( pinst, cbBookmark );
	}
#endif	//  VERPERF

	return JET_errSuccess;
	}


//  ================================================================
LOCAL VOID VERIDeleteRCEFromHash( RCE * const prce )
//  ================================================================
//
//	Deletes RCE from hashtable/RCE chain, and may set hash table entry to
//	prceNil. Must already be in the critical section for the hash chain.
//
//-
	{
	BOOKMARK	bookmark;
	prce->GetBookmark( &bookmark );
#ifdef DEBUG_VER
	Assert( UiRCHashFunc( prce->Ifmp(), prce->PgnoFDP(), bookmark ) == prce->UiHash() );
#endif	//	DEBUG_VER

	RCE ** const pprce = PprceRCEChainGet( prce->UiHash(), prce->Ifmp(), prce->PgnoFDP(), bookmark );
	Assert( pprce != NULL );

	if ( prce == *pprce )
		{
		Assert( prceNil == prce->PrceNextOfNode() );

		//  the RCE is at the head of the chain
		if ( prceNil != prce->PrcePrevOfNode() )
			{
			Assert( prce->PrcePrevOfNode()->UiHash() == prce->UiHash() );
			prce->PrcePrevOfNode()->SetPrceHashOverflow( prce->PrceHashOverflow() );
			*pprce = prce->PrcePrevOfNode();
			(*pprce)->SetPrceNextOfNode( prceNil );
			Assert( prceInvalid != (*pprce)->PrceHashOverflow() );
			ASSERT_VALID( *pprce );
			}
		else
			{
			*pprce = prce->PrceHashOverflow();
			Assert( prceInvalid != *pprce );
			}
		}
	else
		{
		Assert( prceNil != prce->PrceNextOfNode() );
		RCE * const prceNext = prce->PrceNextOfNode();

		Assert( prceNext->UiHash() == prce->UiHash() );

		prceNext->SetPrcePrevOfNode( prce->PrcePrevOfNode() );
		if ( prceNil != prceNext->PrcePrevOfNode() )
			{
			Assert( prceNext->PrcePrevOfNode()->UiHash() == prce->UiHash() );
			prceNext->PrcePrevOfNode()->SetPrceNextOfNode( prceNext );
			}
		}

	prce->SetPrceNextOfNode( prceNil );
	prce->SetPrcePrevOfNode( prceNil );

	//  monitor statistics
#ifdef VERPERF
	//	Do not need critical section here. It is OK to miss a little.

	VER *pver = PverFromIfmp( prce->Ifmp() );
	INST *pinst = pver->m_pinst;
	cVERcrceHashEntries.Dec( pinst );
	cVERcbBookmarkTotal.Add( pinst, -(prce->CbBookmarkKey() + prce->CbBookmarkData()) );
#endif	// VERPERF
	}


//  ================================================================
INLINE VOID VERIInsertRCEIntoSessionList( PIB * const ppib, RCE * const prce )
//  ================================================================
//
//  Inserts the RCE into the head of the session list of the pib provided.
//  No critical section needed to do this, because the only session that
//	inserts at the head of the list is the pib itself.
//
//-
	{
	RCE			*prceNext	= prceNil;
	RCE			*prcePrev 	= ppib->prceNewest;
	const RCEID	rceid 		= prce->Rceid();
	const LEVEL	level		= prce->Level();
	INST		*pinst		= PinstFromPpib( ppib );
	LOG			*plog		= pinst->m_plog;

	Assert( level > 0 );
	Assert( level == ppib->level
		|| ( level < ppib->level && plog->m_fRecovering ) );

	if ( !plog->m_fRecovering
		|| prceNil == prcePrev
		|| level > prcePrev->Level()
		|| ( level == prcePrev->Level() && rceid > prcePrev->Rceid() ) )
		{
		prce->SetPrceNextOfSession( prceNil );
		prce->SetPrcePrevOfSession( ppib->prceNewest );
		if ( prceNil != ppib->prceNewest )
			{
			Assert( prceNil == ppib->prceNewest->PrceNextOfSession() );
			Assert( ppib->prceNewest->Level() <= prce->Level() );
			ppib->prceNewest->SetPrceNextOfSession( prce );
			}
		PIBSetPrceNewest( ppib, prce );
		Assert( prce == ppib->prceNewest );
		}

	else
		{
		Assert( plog->m_fRecovering );

		//	insert RCE in rceid order at the same level
		Assert( level < prcePrev->Level() || rceid < prcePrev->Rceid() );
		for ( ;
			prcePrev != prceNil
				&& ( level < prcePrev->Level()
					|| ( level == prcePrev->Level() && rceid < prcePrev->Rceid() ) );
			prceNext = prcePrev,
			prcePrev = prcePrev->PrcePrevOfSession() )
			{
			NULL;
			}

		Assert( prceNil != prceNext );
		Assert( prceNext->Level() > level
			|| prceNext->Level() == level && prceNext->Rceid() > rceid );
		prceNext->SetPrcePrevOfSession( prce );

		if ( prceNil != prcePrev )
			{
			Assert( prcePrev->Level() < level
				|| ( prcePrev->Level() == level && prcePrev->Rceid() < rceid ) );
			prcePrev->SetPrceNextOfSession( prce );
			}

		prce->SetPrcePrevOfSession( prcePrev );
		prce->SetPrceNextOfSession( prceNext );
		}

	Assert( prceNil == ppib->prceNewest->PrceNextOfSession() );
	}


//  ================================================================
INLINE VOID VERIInsertRCEIntoSessionList( PIB * const ppib, RCE * const prce, RCE * const prceParent )
//  ================================================================
//
//  Inserts the RCE after the given parent RCE in the session list.
//  NEVER inserts at the head (since the insert occurs after an existing
//	RCE), so no possibility of conflict with regular session inserts, and
//	therefore no critical section needed.  However, may conflict with
//	other concurrent create indexers, which is why we must obtain critTrx
//	(to ensure only one concurrent create indexer is processing the parent
//	at a time.
//
//-
	{
	Assert( ppib->critTrx.FOwner() );

	Assert( !PinstFromPpib( ppib )->m_plog->m_fRecovering );

	Assert( prceParent->TrxCommitted() == trxMax );
	Assert( prceParent->Level() == prce->Level() );
	Assert( prceNil != ppib->prceNewest );
	Assert( ppib == prce->Pfucb()->ppib );

	RCE	*prcePrev = prceParent->PrcePrevOfSession();

	prce->SetPrceNextOfSession( prceParent );
	prce->SetPrcePrevOfSession( prcePrev );
	prceParent->SetPrcePrevOfSession( prce );

	if ( prceNil != prcePrev )
		{
		prcePrev->SetPrceNextOfSession( prce );
		}

	Assert( prce != ppib->prceNewest );
	}


//  ================================================================
LOCAL VOID VERIDeleteRCEFromSessionList( PIB * const ppib, RCE * const prce )
//  ================================================================
//
//  Deletes the RCE from the session list of the given PIB. Must be
//  in the critical section of the PIB
//
//-
	{
	Assert( !prce->FFullyCommitted() );	//  only uncommitted RCEs in the session list
	Assert( ppib == prce->Pfucb()->ppib );

	if ( prce == ppib->prceNewest )
		{
		//  we are at the head of the list
		PIBSetPrceNewest( ppib, prce->PrcePrevOfSession() );
		if ( prceNil != ppib->prceNewest )
			{
			Assert( prce == ppib->prceNewest->PrceNextOfSession() );
			ppib->prceNewest->SetPrceNextOfSession( prceNil );
			}
		}
	else
		{
		// we are in the middle/end
		Assert( prceNil != prce->PrceNextOfSession() );
		RCE * const prceNext = prce->PrceNextOfSession();
		RCE * const prcePrev = prce->PrcePrevOfSession();
		prceNext->SetPrcePrevOfSession( prcePrev );
		if ( prceNil != prcePrev )
			{
			prcePrev->SetPrceNextOfSession( prceNext );
			}
		}
	prce->SetPrceNextOfSession( prceNil );
	prce->SetPrcePrevOfSession( prceNil );
	}


//  ================================================================
LOCAL VOID VERIInsertRCEIntoFCBList( FCB * const pfcb, RCE * const prce )
//  ================================================================
//
//  Must be in critFCB
//
//-
	{
	Assert( pfcb->CritRCEList().FOwner() );
	pfcb->AttachRCE( prce );
	}


//  ================================================================
LOCAL VOID VERIDeleteRCEFromFCBList( FCB * const pfcb, RCE * const prce )
//  ================================================================
//
//  Must be in critFCB
//
//-
	{
	Assert( pfcb->CritRCEList().FOwner() );
	pfcb->DetachRCE( prce );
	}


//  ================================================================
VOID VERInsertRCEIntoLists(
	FUCB		*pfucbNode,		// cursor of session performing node operation
	CSR			*pcsr,
	RCE			*prce,
	const VERPROXY	*pverproxy )
//  ================================================================
	{
	Assert( prceNil != prce );

	FCB	* const pfcb = prce->Pfcb();
	Assert( pfcbNil != pfcb );

	LOG *plog = PinstFromIfmp( pfcb->Ifmp() )->m_plog;

	if ( prce->TrxCommitted() == trxMax )
		{
		FUCB	*pfucbVer = prce->Pfucb();	// cursor of session for whom the
											// RCE was created,
		Assert( pfucbNil != pfucbVer );
		Assert( pfcb == pfucbVer->u.pfcb );

#ifdef IGNORE_BAD_ATTACH
		if ( plog->m_fRecovering && prce->Rceid() != rceidNull )
			{
			//	Assert( plog->m_rceidLast <= prce->m_rceid );
			 plog->m_rceidLast = prce->Rceid();
			}
#endif // IGNORE_BAD_ATTACH

		// cursor performing the node operation may be different than cursor
		// for which the version was created if versioning by proxy
		Assert( pfucbNode == pfucbVer || pverproxy != NULL );

		if ( FVERIAddUndoInfo( prce ) )
			{
			Assert( pcsrNil != pcsr || plog->m_fRecovering );	// Allow Redo to override UndoInfo.
			if ( pcsrNil != pcsr )
				{
				Assert( !rgfmp[ pfcb->Ifmp() ].FLogOn() || !plog->m_fLogDisabled );
				if ( rgfmp[ pfcb->Ifmp() ].FLogOn() )
					{
					//	set up UndoInfo dependency
					//	to make sure UndoInfo will be logged if the buffer
					//	is flushed before commit/rollback.
					BFAddUndoInfo( pcsr->Cpage().PBFLatch(), prce );
					}
				}
			}

		if ( NULL != pverproxy && proxyCreateIndex == pverproxy->proxy )
			{
			Assert( !plog->m_fRecovering );
			Assert( pfucbNode != pfucbVer );
			Assert( prce->Oper() == operInsert
				|| prce->Oper() == operReplace		// via FlagInsertAndReplaceData
				|| prce->Oper() == operFlagDelete );
			VERIInsertRCEIntoSessionList( pfucbVer->ppib, prce, pverproxy->prcePrimary );
			}
		else
			{
			Assert( pfucbNode == pfucbVer );
			if ( NULL != pverproxy )
				{
				Assert( plog->m_fRecovering );
				Assert( proxyRedo == pverproxy->proxy );
				Assert( prce->Level() == pverproxy->level );
				Assert( prce->Level() > 0 );
				Assert( prce->Level() <= pfucbVer->ppib->level );
				}
			VERIInsertRCEIntoSessionList( pfucbVer->ppib, prce );
			}
		}
	else
		{
		// Don't put committed RCE's into session list.
		// Only way to get a committed RCE is via concurrent create index.
		Assert( !plog->m_fRecovering );
		Assert( NULL != pverproxy );
		Assert( proxyCreateIndex == pverproxy->proxy );
		Assert( prceNil != pverproxy->prcePrimary );
		Assert( pverproxy->prcePrimary->TrxCommitted() == prce->TrxCommitted() );
		Assert( pfcb->FTypeSecondaryIndex() );
		Assert( pfcb->PfcbTable() == pfcbNil );
		Assert( prce->Oper() == operInsert
				|| prce->Oper() == operReplace		// via FlagInsertAndReplaceData
				|| prce->Oper() == operFlagDelete );
		}


	ENTERCRITICALSECTION enterCritFCBRCEList( &(pfcb->CritRCEList()) );
	VERIInsertRCEIntoFCBList( pfcb, prce );
	}


//  ================================================================
LOCAL VOID VERINullifyUncommittedRCE( RCE * const prce )
//  ================================================================
//
//  Remove undo info from a RCE, remove it from the session list
//  FCB list and the hash table. The oper is then nullified
//
//-
	{
	Assert( prceInvalid != prce->PrceNextOfSession() );
	Assert( prceInvalid != prce->PrcePrevOfSession() );
	Assert( prceInvalid != prce->PrceNextOfFCB() );
	Assert( prceInvalid != prce->PrcePrevOfFCB() );
	Assert( trxMax == prce->TrxCommitted() );

	BFRemoveUndoInfo( prce );

	VERIDeleteRCEFromFCBList( prce->Pfcb(), prce );
	VERIDeleteRCEFromSessionList( prce->Pfucb()->ppib, prce );

	const BOOL fInHash = prce->FOperInHashTable();
	if ( fInHash )
		{
		//  VERIDeleteRCEFromHash may call ASSERT_VALID so we do this deletion first,
		//  before we mess up the other linked lists
		VERIDeleteRCEFromHash( prce );
		}

	prce->NullifyOper();
	}


//  ================================================================
LOCAL VOID VERINullifyCommittedRCE( RCE * const prce )
//  ================================================================
//
//  Remove a RCE from the Hash table (if necessary) and the FCB list.
//  The oper is then nullified
//
//-
	{
	Assert( prce->PgnoUndoInfo() == pgnoNull );
	Assert( prce->TrxCommitted() != trxMax );
	VERIDeleteRCEFromFCBList( prce->Pfcb(), prce );

	const BOOL fInHash = prce->FOperInHashTable();
	if ( fInHash )
		{
		//  VERIDeleteRCEFromHash may call ASSERT_VALID so we do this deletion first,
		//  before we mess up the other linked lists
		VERIDeleteRCEFromHash( prce );
		}

	prce->NullifyOper();
	}


//  ================================================================
INLINE VOID VERINullifyRCE( RCE *prce )
//  ================================================================
	{
	if ( prce->TrxCommitted() != trxMax )
		{
		VERINullifyCommittedRCE( prce );
		}
	else
		{
		VERINullifyUncommittedRCE( prce );
		}
	}


//  ================================================================
VOID VERNullifyFailedDMLRCE( RCE *prce )
//  ================================================================
	{
	ASSERT_VALID( prce );
	Assert( prceInvalid == prce->PrceNextOfSession() || prceNil == prce->PrceNextOfSession() );
	Assert( prceInvalid == prce->PrcePrevOfSession() || prceNil == prce->PrcePrevOfSession() );

	BFRemoveUndoInfo( prce );

	Assert( prce->FOperInHashTable() );

		{
		ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( prce->Ifmp() )->CritRCEChain( prce->UiHash() ) ) );
		VERIDeleteRCEFromHash( prce );
		}

	prce->NullifyOper();
	}


//  ================================================================
VOID VERNullifyAllVersionsOnFCB( FCB * const pfcb )
//  ================================================================
//
//  This is used to nullify all RCEs on an FCB
//
//-
	{
	VER *pver = PverFromIfmp( pfcb->Ifmp() );
	LOG *plog = pver->m_pinst->m_plog;

	pfcb->CritRCEList().Enter();

	Assert( pfcb->FTypeTable()
		|| pfcb->FTypeSecondaryIndex()
		|| pfcb->FTypeTemporaryTable()
		|| pfcb->FTypeSentinel()
		|| pfcb->FTypeLV() );

	while ( prceNil != pfcb->PrceOldest() )
		{
		RCE	* const prce = pfcb->PrceOldest();

		Assert( prce->Pfcb() == pfcb );
		Assert( !prce->FOperNull() );

		Assert( prce->Ifmp() == pfcb->Ifmp() );
		Assert( prce->PgnoFDP() == pfcb->PgnoFDP() );

		//	during recovery, should not see any uncommitted RCE's
		Assert( prce->TrxCommitted() != trxMax || !plog->m_fRecovering );

		if ( prce->FOperInHashTable() )
			{
			const UINT	uiHash			= prce->UiHash();
			const BOOL	fNeedCritTrx	= ( prce->TrxCommitted() == trxMax
											&& rgfmp[ prce->Ifmp() ].Dbid() != dbidTemp );
			PIB			*ppib;

			Assert( !pfcb->FTypeTable()
				|| plog->m_fRecovering
				|| ( prce->FMoved() && operFlagDelete == prce->Oper() ) );

			if ( fNeedCritTrx )
				{
				// If uncommitted, must grab critTrx to ensure that
				// RCE does not commit or rollback on us while
				// nullifying.  Note that we don't need critTrx
				// if this is a temp table because we're the
				// only ones who have access to it.
				Assert( prce->Pfucb() != pfucbNil );
				Assert( prce->Pfucb()->ppib != ppibNil );
				ppib = prce->Pfucb()->ppib;
				}

			pfcb->CritRCEList().Leave();

			ENTERCRITICALSECTION	enterCritRCEClean( &pver->m_critRCEClean, plog->m_fRecovering );
			ENTERCRITICALSECTION	enterCritPIBTrx(
										fNeedCritTrx ? &ppib->critTrx : NULL,
										fNeedCritTrx );	//lint !e644
			ENTERCRITICALSECTION	enterCritHash( &( pver->CritRCEChain( uiHash ) ) );

			pfcb->CritRCEList().Enter();

			// Verify no one nullified the RCE while we were
			// switching critical sections.
			if ( pfcb->PrceOldest() == prce )
				{
				Assert( !prce->FOperNull() );
				VERINullifyRCE( prce );
				}
			}
		else
			{
			// If not hashable, nullification will be expensive,
			// because we have to grab m_critRCEClean.  Fortunately
			// this should be rare:
			// 	- the only non-hashable versioned operation
			//	  possible for a temporary table is CreateTable.
			//	- no non-hashable versioned operations possible
			//	  on secondary index.
			if ( !plog->m_fRecovering )
				{
				Assert( rgfmp[ prce->Ifmp() ].Dbid() == dbidTemp );
				Assert( prce->Pfcb()->FTypeTemporaryTable() );
				Assert( operCreateTable == prce->Oper() );
				}

			pfcb->CritRCEList().Leave();
			ENTERCRITICALSECTION	enterCritRCEClean( &pver->m_critRCEClean );
			pfcb->CritRCEList().Enter();

			// critTrx not needed, since we're the only ones
			// who should have access to this FCB.

			// Verify no one nullified the RCE while we were
			// switching critical sections.
			if ( pfcb->PrceOldest() == prce )
				{
				Assert( !prce->FOperNull() );
				VERINullifyRCE( prce );
				}
			}
		}

	pfcb->CritRCEList().Leave();
	}


//  ================================================================
VOID VERNullifyInactiveVersionsOnBM( const FUCB * pfucb, const BOOKMARK& bm )
//  ================================================================
//
//	Nullifies all RCE's for given bm. Used by online cleanup when
//  a page is being cleaned.
//
//  All the RCE's should be inactive and thus don't need to be
//  removed from a session list
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bm );

	const TRX				trxOldest		= TrxOldest( PinstFromPfucb( pfucb ) );
	const UINT				uiHash			= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bm );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );

	RCE	* prcePrev;
	RCE	* prce		= PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bm );
	for ( ; prceNil != prce; prce = prcePrev )
		{
		prcePrev = prce->PrcePrevOfNode();

		if ( TrxCmp( prce->TrxCommitted(), trxOldest ) < 0 )
			{
			FCB * const pfcb = prce->Pfcb();
			ENTERCRITICALSECTION enterCritFCBRCEList( &( pfcb->CritRCEList() ) );

			VERINullifyCommittedRCE( prce );
			}
		}
	}



//  ****************************************************************
//  VERSION LAYER
//  ****************************************************************

VOID VERSanitizeParameters( INST * pinst )
	{
	//	Already been sanitized in JetSetSystemParameter()
	Assert( pinst->m_lVerPagesMax >= 1 );
	Assert( pinst->m_lVerPagesPreferredMax >= 1 );

	if ( pinst->m_lVerPagesPreferredMax > pinst->m_lVerPagesMax )
		{
		//	Note that we convert the bucket parameters to the 16K "pages"
		//	that the user is "familiar" with.
		TCHAR	szVerMax[ 32 ];
		_itoa( pinst->m_lVerPagesMax * ( cbBucket / ( 16 * 1024 ) ), szVerMax, 10 );
		TCHAR	szPreferredMax[ 32 ];
		_itoa( pinst->m_lVerPagesPreferredMax * ( cbBucket / ( 16 * 1024 ) ), szPreferredMax, 10 );

		//	SANITIZE
		pinst->m_lVerPagesPreferredMax = pinst->m_lVerPagesMax;

		const TCHAR	*rgsz[] = { szPreferredMax, szVerMax, szPreferredMax, szVerMax };
		UtilReportEvent(
				eventWarning,
				SYSTEM_PARAMETER_CATEGORY,
				SYS_PARAM_VERPREFERREDPAGETOOBIG_ID,
				CArrayElements( rgsz ),
				rgsz,
				0,
				NULL,
				pinst );
		}

	Assert( pinst->m_lVerPagesMax >= 1 );
	Assert( pinst->m_lVerPagesPreferredMax >= 1 );
	Assert( pinst->m_lVerPagesPreferredMax <= pinst->m_lVerPagesMax );
	}


VOID VER::VERSignalCleanup()
	{
	//	check whether we already requested clean up of that version store
	if ( NULL != m_pbucketGlobalTail
		&& 0 == AtomicCompareExchange( (LONG *)&m_fVERCleanUpWait, 0, 1 ) )
		{
		//	reset signal in case we need to ensure RCE clean gets
		//	performed soon
		//
		m_msigRCECleanPerformedRecently.Reset();

		//	be aware if we are in syncronous mode already, do not try to start a new task
		if ( m_fSyncronousTasks
			|| m_pinst->Taskmgr().ErrTMPost( VERIRCECleanProc, this ) < JET_errSuccess )
			{
			//	couldn't post task, so set signal to ensure no one waits on it
			//
			m_msigRCECleanPerformedRecently.Set();

			LONG fStatus = AtomicCompareExchange( (LONG *)&m_fVERCleanUpWait, 1, 0 );
			if ( 2 == fStatus )
				{
				m_asigRCECleanDone.Set();
				}
			}
		}
	}


//  ================================================================
DWORD VER::VERIRCECleanProc( VOID *pvThis )
//  ================================================================
//
//	Go through all sessions, cleaning buckets as versions are
//	no longer needed.  Only those versions older than oldest
//	transaction are cleaned up.
//
//	Side Effects:
//		frees buckets.
//
//-
	{
	VER *pver = (VER *)pvThis;

	pver->m_critRCEClean.Enter();
	pver->ErrVERIRCEClean();
	LONG fStatus = AtomicCompareExchange( (LONG *)&pver->m_fVERCleanUpWait, 1, 0 );
	pver->m_critRCEClean.Leave();

	//	indicate that RCE clean was performed recently
	//
	pver->m_msigRCECleanPerformedRecently.Set();

	if ( 2 == fStatus )
		{
		pver->m_asigRCECleanDone.Set();
		}

	return 0;
	}


ERR VER::ErrVERSystemInit( )
{
	ERR     err = JET_errSuccess;

	AssertRTL( IbAlignForAllPlatforms( sizeof(BUCKET) ) == sizeof(BUCKET) );
#ifdef PCACHE_OPTIMIZATION
	AssertRTL( sizeof(BUCKET) % 32 == 0 );
#endif

#ifdef GLOBAL_VERSTORE_MEMPOOL
	//	initialize resVerPagesGlobal
	//	reserve memory for version store
	//	number of buckets reserved must be a multiple of cbucketChunk
	Assert( NULL == g_pcresVERPool );
	g_pcresVERPool = new CRES( NULL, residVER, sizeof( BUCKET ), g_lVerPagesMin, &err );
	if ( JET_errSuccess > err )
		{
		delete g_pcresVERPool;
		g_pcresVERPool = NULL;
		}
	else if ( NULL == g_pcresVERPool )
		{
		err = ErrERRCheck( JET_errOutOfMemory );
		}
	CallR ( err );
#endif
	// UNDONE: get rid of the magic number 4
	///	g_pcresVERPool->SetPreferredThreshold( 4 );

	return err;
}

VOID VER::VERSystemTerm( )
{
#ifdef GLOBAL_VERSTORE_MEMPOOL
	Assert ( g_pcresVERPool );
#endif

#ifdef GLOBAL_VERSTORE_MEMPOOL
	delete g_pcresVERPool ;
	g_pcresVERPool = NULL;
#endif
}

#ifndef GLOBAL_VERSTORE_MEMPOOL

ERR VER::ErrVERMempoolInit( const ULONG cbucketMost )
{
	ERR     err = JET_errSuccess;

	AssertRTL( IbAlignForAllPlatforms( sizeof(BUCKET) ) == sizeof(BUCKET) );
#ifdef PCACHE_OPTIMIZATION
	AssertRTL( sizeof(BUCKET) % 32 == 0 );
#endif
	//	initialize resVerPagesGlobal
	//	reserve memory for version store
	//	number of buckets reserved must be a multiple of cbucketChunk
	m_pcresVERPool = new CRES( m_pinst, residVER, sizeof( BUCKET ), cbucketMost, &err );
	if ( JET_errSuccess > err )
		{
		delete m_pcresVERPool;
		m_pcresVERPool = NULL;
		}
	else if ( NULL == m_pcresVERPool )
		{
		err = ErrERRCheck( JET_errOutOfMemory );
		}

	return err;
}

VOID VER::VERMempoolTerm( )
{
	Assert ( m_pcresVERPool );
	delete m_pcresVERPool ;
}

#endif // GLOBAL_VERSTORE_MEMPOOL


//  ================================================================
ERR VER::ErrVERInit( INT cbucketMost, INT cbucketPreferred, INT cSessions )
//  ================================================================
//
//	Creates background version bucket clean up thread.
//
//-
	{
	ERR     err = JET_errSuccess;

#if defined( DEBUG ) && JET_cbPage == 8192
	//  double size of version store for 8K pages (LV replaces need more space)
	cbucketMost *= 2;
#endif

#ifdef VERPERF
	cVERcbucketAllocated.Clear( m_pinst );
	cVERcbucketDeleteAllocated.Clear( m_pinst );
	cVERBucketAllocWaitForRCEClean.Clear( m_pinst );
	cVERcrceHashEntries.Clear( m_pinst );			//	number of RCEs that currently exist in hash table
	cVERcbBookmarkTotal.Clear( m_pinst );			//	amount of space used by bookmarks of existing RCEs
	cVERUnnecessaryCalls.Clear( m_pinst );			//	calls for a bookmark that doesn't exist
	cVERSyncCleanupDispatched.Clear( m_pinst );		//	cleanup operations dispatched synchronously
	cVERAsyncCleanupDispatched.Clear( m_pinst );	//  cleanup operations dispatched asynchronously
	cVERCleanupDiscarded.Clear( m_pinst );			//  cleanup operations dispatched but failed
	cVERCleanupFailed.Clear( m_pinst );				//  cleanup operations discarded
#endif	//  VERPERF

	AssertRTL( TrxCmp( trxMax, trxMax ) == 0 );
	AssertRTL( TrxCmp( trxMax, 0 ) > 0 );
	AssertRTL( TrxCmp( trxMax, 2 ) > 0 );
	AssertRTL( TrxCmp( trxMax, 2000000 ) > 0 );
	AssertRTL( TrxCmp( trxMax, trxMax - 1 ) > 0 );
	AssertRTL( TrxCmp( 0, trxMax ) < 0 );
	AssertRTL( TrxCmp( 2, trxMax ) < 0 );
	AssertRTL( TrxCmp( 2000000, trxMax ) < 0 );
	AssertRTL( TrxCmp( trxMax - 1, trxMax ) < 0 );
	AssertRTL( TrxCmp( 0xF0000000, 0xEFFFFFFE ) > 0 );
	AssertRTL( TrxCmp( 0xEFFFFFFF, 0xF0000000 ) < 0 );
	AssertRTL( TrxCmp( 0xfffffdbc, 0x000052e8 ) < 0 );
	AssertRTL( TrxCmp( 10, trxMax - 259 ) > 0 );
	AssertRTL( TrxCmp( trxMax - 251, 16 ) < 0 );
	AssertRTL( TrxCmp( trxMax - 257, trxMax - 513 ) > 0 );
	AssertRTL( TrxCmp( trxMax - 511, trxMax - 255 ) < 0 );

	AssertRTL( IbAlignForAllPlatforms( sizeof(BUCKET) ) == sizeof(BUCKET) );
#ifdef PCACHE_OPTIMIZATION
	AssertRTL( sizeof(BUCKET) % 32 == 0 );
#endif

#ifdef GLOBAL_VERSTORE_MEMPOOL
	Assert ( g_pcresVERPool ) ;
#else
	CallR ( ErrVERMempoolInit( cbucketMost ) );
#endif

	// pbucketGlobal{Head,Tail} should be NULL. If they aren't we probably
	// didn't terminate properly at some point
	Assert( pbucketNil == m_pbucketGlobalHead );
	Assert( pbucketNil == m_pbucketGlobalTail );
	m_pbucketGlobalHead = pbucketNil;
	m_pbucketGlobalTail = pbucketNil;

#ifdef DEBUG
///	VERCheckVER();	// call it here to make sure it is linked in
#endif	// DEBUG

	Assert( ppibNil == m_ppibRCEClean );
	Assert( ppibNil == m_ppibRCECleanCallback );
	if ( !m_pinst->m_plog->m_fRecovering )
		{
		CallJ( ErrPIBBeginSession( m_pinst, &m_ppibRCEClean, procidNil, fFalse ), DeleteHash );
		CallJ( ErrPIBBeginSession( m_pinst, &m_ppibRCECleanCallback, procidNil, fFalse ), EndCleanSession );
		m_ppibRCEClean->grbitsCommitDefault = JET_bitCommitLazyFlush;
		m_ppibRCECleanCallback->SetFSystemCallback();
		}

	// sync tasks only during termination
	m_fSyncronousTasks = fFalse;

	m_fVERCleanUpWait = 0;

	//	set m_cbucketGlobalAllocMost
	m_critBucketGlobal.Enter();
	m_cbucketGlobalAllocMost = cbucketMost;
	Assert( cbucketPreferred >= 1 );
	m_cbucketGlobalAllocPreferred = cbucketPreferred;

	Assert( m_cbucketGlobalAllocPreferred <= m_cbucketGlobalAllocMost );
	Assert( m_cbucketGlobalAllocPreferred >= 1 );
	Assert( m_cbucketGlobalAllocMost >= 1 );

	m_critBucketGlobal.Leave();

	return err;

EndCleanSession:
	if ( !m_pinst->m_plog->m_fRecovering )
		{
		PIBEndSession( m_ppibRCEClean );
		}

DeleteHash:

#ifndef GLOBAL_VERSTORE_MEMPOOL
	VERMempoolTerm();
#endif
	return err;
	}


//  ================================================================
VOID VER::VERTerm( BOOL fNormal )
//  ================================================================
//
//	Terminates background thread and releases version store
//	resources.
//
//-
	{
	//  The TASKMGR has gone away by the time we are shutting down
	Assert ( m_fSyncronousTasks );

	//	be sure that the state will not change
	m_critRCEClean.Enter();
	LONG fStatus = AtomicExchange( (LONG *)&m_fVERCleanUpWait, 2 );
	m_critRCEClean.Leave();
	if ( 1 == fStatus )
		{
		m_asigRCECleanDone.Wait();
		}

	if ( fNormal )
		{
		Assert( trxMax == TrxOldest( m_pinst ) );
		ERR err = ErrVERRCEClean( );
		Assert( err != JET_wrnRemainingVersions );
		if ( err < JET_errSuccess )
			{
			fNormal = fFalse;
			AssertTracking();
			}
		}

	if ( ppibNil != m_ppibRCEClean )
		{
		PIBEndSession( m_ppibRCEClean );
		m_ppibRCEClean = ppibNil;
		}

	if ( ppibNil != m_ppibRCECleanCallback )
		{
#ifdef DEBUG
		for ( DBID dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
			{
			Assert( !FPIBUserOpenedDatabase( m_ppibRCECleanCallback, dbid ) );
			}
#endif	//	DEBUG
		PIBEndSession( m_ppibRCECleanCallback );
		m_ppibRCECleanCallback = ppibNil;
		}

	// pbucketGlobal{Head,Tail} should be NULL. If they aren't we probably
	// didn't cleanup properly
	Assert( pbucketNil == m_pbucketGlobalHead || !fNormal);
	Assert( pbucketNil == m_pbucketGlobalTail || !fNormal );
#ifdef GLOBAL_VERSTORE_MEMPOOL
	if ( fNormal )
		{
		//	Should be freed normally
		Assert( pbucketNil == m_pbucketGlobalHead );
		Assert( pbucketNil == m_pbucketGlobalTail );
		}
	else
		{
		//	Need to walk and free to the global version store
		//	Note that linked list goes from head -> prev -> prev -> prev -> nil
		BUCKET* pbucket = m_pbucketGlobalHead;
		while ( pbucketNil != pbucket )
			{
			BUCKET* const pbucketPrev = pbucket->hdr.pbucketPrev;
			VERIReleasePbucket( pbucket );
			pbucket = pbucketPrev;
			}
		}
#endif
	Assert( 0 == m_cbucketGlobalAlloc );
	m_pbucketGlobalHead = pbucketNil;
	m_pbucketGlobalTail = pbucketNil;

	//	deallocate resVerPagesGlobal ( buckets space )

#ifdef GLOBAL_VERSTORE_MEMPOOL
	Assert ( g_pcresVERPool );
#else // GLOBAL_VERSTORE_MEMPOOL
	VERMempoolTerm();
#endif

#ifdef VERPERF
	cVERcbucketAllocated.Clear( m_pinst );
	cVERcbucketDeleteAllocated.Clear( m_pinst );
	cVERBucketAllocWaitForRCEClean.Clear( m_pinst );
	cVERcrceHashEntries.Clear( m_pinst );			//	number of RCEs that currently exist in hash table
	cVERcbBookmarkTotal.Clear( m_pinst );			//	amount of space used by bookmarks of existing RCEs
	cVERUnnecessaryCalls.Clear( m_pinst );			//	calls for a bookmark that doesn't exist
	cVERSyncCleanupDispatched.Clear( m_pinst );		//	cleanup operations dispatched synchronously
	cVERAsyncCleanupDispatched.Clear( m_pinst );	//  cleanup operations dispatched asynchronously
	cVERCleanupDiscarded.Clear( m_pinst );			//  cleanup operations dispatched but failed
	cVERCleanupFailed.Clear( m_pinst );				//  cleanup operations discarded
#endif // VERPERF
	}


//  ================================================================
ERR VER::ErrVERStatus( )
//  ================================================================
//
//	Returns JET_wrnIdleFull if version store more than half full.
//
//-
	{
	ENTERCRITICALSECTION	enterCritRCEBucketGlobal( &m_critBucketGlobal );
	ERR						err = ( m_cbucketGlobalAlloc > ( m_cbucketGlobalAllocPreferred * 0.6 ) ?
									ErrERRCheck( JET_wrnIdleFull ) :
									JET_errSuccess );
	return err;
	}


//  ================================================================
VS VsVERCheck( const FUCB * pfucb, CSR * pcsr, const BOOKMARK& bookmark )
//  ================================================================
//
//	Given a BOOKMARK, returns the version status
//
//	RETURN VALUE
//		vsCommitted
//		vsUncommittedByCaller
//		vsUncommittedByOther
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );

	const UINT 			uiHash 	= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );
	const RCE * const 	prce 	= PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );

	VS vs = vsNone;

	//	if no RCE for node then version bit in node header must
	//	have been orphaned due to crash.  Remove node bit.
	if ( prceNil == prce )
		{
#ifdef VERPERF
		cVERUnnecessaryCalls.Inc( PinstFromPfucb( pfucb ) );
#endif
		if ( FFUCBUpdatable( pfucb ) )
			{
			NDDeferResetNodeVersion( pcsr );
			}
		vs = vsCommitted;
		}
	else if ( prce->TrxCommitted() != trxMax )
		{
		//	committed
		vs = vsCommitted;
		}
	else if ( prce->Pfucb()->ppib != pfucb->ppib )
		{
		//	not modified (uncommitted)
		vs = vsUncommittedByOther;
		}
	else
		{
		//	modifier (uncommitted)
		vs = vsUncommittedByCaller;
		}

	Assert( vsNone != vs );
	return vs;
	}


ERR ErrVERFModified( FUCB *pfucb, const BOOKMARK& bookmark, BOOL *pfModified )
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );

	Assert( !FFUCBUnique( pfucb ) || 0 == bookmark.data.Cb() );
	Assert( Pcsr( pfucb )->FLatched() );

	const UINT uiHash = UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	if ( prceNil == PverFromIfmp( pfucb->ifmp )->GetChain( uiHash ) )
		{
#ifdef VERPERF
		cVERUnnecessaryCalls.Inc( PinstFromPfucb( pfucb ) );
#endif

		if ( FFUCBUpdatable( pfucb ) )
			{
			//  the version bit is set but there is no version. reset the bit
			//  we cast away const because we are secretly modifying the page
			//
			NDDeferResetNodeVersion( Pcsr( const_cast<FUCB *>( pfucb ) ) );
			}

		*pfModified = fFalse;
		return JET_errSuccess;
		}

	ERR					err	= JET_errSuccess;
	const TRX			trxSession	= TrxVERISession( pfucb );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );
	const RCE 			*prce 		= PrceFirstVersion( uiHash, pfucb, bookmark );

	if ( prceNil == prce )
		{
#ifdef VERPERF
		cVERUnnecessaryCalls.Inc( PinstFromPfucb( pfucb ) );
#endif
		if ( FFUCBUpdatable( pfucb ) )
			{
			//  the version bit is set but there is no version. reset the bit
			//  we cast away const because we are secretly modifying the page
			//
			NDDeferResetNodeVersion( Pcsr( const_cast<FUCB *>( pfucb ) ) );
			}

		*pfModified = fFalse;
		}
	else if ( prce->TrxCommitted() == trxMax && prce->Pfucb()->ppib == pfucb->ppib )
		{
		//	node versioned in uncommitted transaction by this session
		//
		*pfModified = fTrue;
		}
	else
		{
		*pfModified = fFalse;
		}

	return err;
}


//  ================================================================
ERR ErrVERAccessNode( FUCB * pfucb, const BOOKMARK& bookmark, NS * pns )
//  ================================================================
//
//	Finds the correct version of a node.
//
//	PARAMETERS
//		pfucb			various fields used/returned.
//		pfucb->kdfCurr	the returned prce or NULL to tell caller to
//						use the node in the DB page.
//
//	RETURN VALUE
//		nsVersion
//		nsDatabase
//		nsInvalid
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );

	//	session with dirty cursor isolation model should never
	//	call NsVERAccessNode.
	Assert( !FPIBDirty( pfucb->ppib ) );
	Assert( FPIBVersion( pfucb->ppib ) );
	Assert( !FFUCBUnique( pfucb ) || 0 == bookmark.data.Cb() );
	Assert( Pcsr( pfucb )->FLatched() );


	//  FAST PATH:  if there are no RCEs in this bucket, immediately bail with
	//  nsDatabase.  the assumption is that the version store is almost always
	//  nearly empty

	const UINT uiHash = UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );

	if ( prceNil == PverFromIfmp( pfucb->ifmp )->GetChain( uiHash ) )
		{
#ifdef VERPERF
		cVERUnnecessaryCalls.Inc( PinstFromPfucb( pfucb ) );
#endif

		if ( FFUCBUpdatable( pfucb ) )
			{
			//  the version bit is set but there is no version. reset the bit
			//  we cast away const because we are secretly modifying the page
			NDDeferResetNodeVersion( Pcsr( const_cast<FUCB *>( pfucb ) ) );
			}

		*pns = nsDatabase;
		return JET_errSuccess;
		}


	ERR	err	= JET_errSuccess;

	const TRX			trxSession	= TrxVERISession( pfucb );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );

	const RCE 			*prce 		= PrceFirstVersion( uiHash, pfucb, bookmark );

	NS nsStatus = nsNone;
	if ( prceNil == prce )
		{
#ifdef VERPERF
		cVERUnnecessaryCalls.Inc( PinstFromPfucb( pfucb ) );
#endif
		if ( FFUCBUpdatable( pfucb ) )
			{
			//  the version bit is set but there is no version. reset the bit
			//  we cast away const because we are secretly modifying the page
			NDDeferResetNodeVersion( Pcsr( const_cast<FUCB *>( pfucb ) ) );
			}
		nsStatus = nsDatabase;
		}
	else if ( prce->TrxCommitted() == trxMax &&
			  prce->Pfucb()->ppib == pfucb->ppib )
		{
		//	cannot be trying to access an RCE that we ourselves rolled back
		Assert( !prce->FRolledBack() );

		//	if caller is modifier of uncommitted version then database
		nsStatus = nsDatabase;
		}
	else if ( TrxCmp( prce->TrxCommitted(), trxSession ) < 0 )
		{
		//	if committed version younger than our transaction then database
		nsStatus = nsDatabase;
		}
	else
		{
		//  active version created by another session. look for before image
		Assert( prceNil != prce && TrxCmp( prce->TrxCommitted(), trxSession ) >= 0);
		//	loop will set prce to the non-delta RCE whose before image was committed
		//	before this transaction began. if all active RCE's are delta RCE's we set prce to
		//  the oldest uncommitted delta RCE
		const RCE * prceLastNonDelta = prce;
		const RCE * prceLastReplace  = prceNil;
		for ( ; prceNil != prce && TrxCmp( prce->TrxCommitted(), trxSession ) >= 0; prce = prce->PrcePrevOfNode() )
			{
			if ( !prce->FRolledBack() )
				{
				switch( prce->Oper() )
					{
					case operDelta:
					case operReadLock:
					case operWriteLock:
					case operSLVSpace:
						break;
					case operReplace:
						prceLastReplace = prce;
						//  FALLTHRU to case below
					default:
						prceLastNonDelta = prce;
						break;
					}
				}
			}

		prce = prceLastNonDelta;

		switch( prce->Oper() )
			{
			case operReplace:
				nsStatus = ( prce->FRolledBack() ? nsDatabase : nsVersion );
				break;
			case operInsert:
				nsStatus = nsInvalid;
				break;
			case operFlagDelete:
				nsStatus = nsVerInDB;
				break;
			case operDelta:
			case operReadLock:
			case operWriteLock:
			case operPreInsert:
			case operSLVSpace:
				//  all the active versions are delta, Lock or SLV space versions. no before images
				nsStatus = nsDatabase;
				break;
			default:
				AssertSz( fFalse, "Illegal operation in RCE chain" );
				break;
			}

		if ( prceNil != prceLastReplace && nsInvalid != nsStatus )
			{
			Assert( prceLastReplace->CbData() >= cbReplaceRCEOverhead );
			Assert( !prceLastReplace->FRolledBack() );

			pfucb->kdfCurr.key.prefix.Nullify();
			pfucb->kdfCurr.key.suffix.SetPv( const_cast<BYTE *>( prceLastReplace->PbBookmark() ) );
			pfucb->kdfCurr.key.suffix.SetCb( prceLastReplace->CbBookmarkKey() );
			pfucb->kdfCurr.data.SetPv( const_cast<BYTE *>( prceLastReplace->PbData() ) + cbReplaceRCEOverhead );
			pfucb->kdfCurr.data.SetCb( prceLastReplace->CbData() - cbReplaceRCEOverhead );

			if ( 0 == pfucb->ppib->level )
				{
				//  Because we are at level 0 this RCE may disappear at any time after
				//  we leave the version store. We copy the before image into the FUCB
				//  to make sure we can always access it

				//  we should not be modifying the page at level 0
				Assert( Pcsr( pfucb )->Latch() == latchReadTouch
						|| Pcsr( pfucb )->Latch() == latchReadNoTouch );

				const INT cbRecord = pfucb->kdfCurr.key.Cb() + pfucb->kdfCurr.data.Cb();
				if ( NULL != pfucb->pvRCEBuffer )
					{
					OSMemoryHeapFree( pfucb->pvRCEBuffer );
					}
				pfucb->pvRCEBuffer = PvOSMemoryHeapAlloc( cbRecord );
				if ( NULL == pfucb->pvRCEBuffer )
					{
					Call( ErrERRCheck( JET_errOutOfMemory ) );
					}
				Assert( 0 == pfucb->kdfCurr.key.prefix.Cb() );
				memcpy( pfucb->pvRCEBuffer,
						pfucb->kdfCurr.key.suffix.Pv(),
						pfucb->kdfCurr.key.suffix.Cb() );
				memcpy( (BYTE *)(pfucb->pvRCEBuffer) + pfucb->kdfCurr.key.suffix.Cb(),
						pfucb->kdfCurr.data.Pv(),
						pfucb->kdfCurr.data.Cb() );
				pfucb->kdfCurr.key.suffix.SetPv( pfucb->pvRCEBuffer );
				pfucb->kdfCurr.data.SetPv( (BYTE *)pfucb->pvRCEBuffer + pfucb->kdfCurr.key.suffix.Cb() );
				}
			ASSERT_VALID( &(pfucb->kdfCurr) );
			}
		}
	Assert( nsNone != nsStatus );

HandleError:

	*pns = nsStatus;

	Assert( JET_errSuccess == err || 0 == pfucb->ppib->level );
	return err;
	}


//  ================================================================
LOCAL BOOL FUpdateIsActive( const PIB * const ppib, const UPDATEID& updateid )
//  ================================================================
	{
	BOOL fUpdateIsActive = fFalse;

	const FUCB * pfucb;
	for ( pfucb = ppib->pfucbOfSession; pfucb != pfucbNil; pfucb = pfucb->pfucbNextOfSession )
		{
		if( FFUCBReplacePrepared( pfucb ) && pfucb->updateid == updateid )
			{
			fUpdateIsActive = fTrue;
			break;
			}
		}

	return fUpdateIsActive;
	}


//  ================================================================
LONG LDeltaVERGetDelta( const FUCB * pfucb, const BOOKMARK& bookmark, INT cbOffset )
//  ================================================================
//
//  Returns the correct compensating delta for this transaction on the given offset
//  of the bookmark. Collect the negation of all active delta versions not created
//  by our session.
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );

	LONG lDelta		= 0;

	if( FPIBDirty( pfucb->ppib ) )
		{
		Assert( 0 == lDelta );
		}
	else
		{
	 	const TRX			trxSession	= TrxVERISession( pfucb );
		const UINT			uiHash		= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
		ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );

		const RCE 			*prce		= PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
		for ( ; prceNil != prce; prce = prce->PrcePrevOfNode() )
			{
			if ( operReplace == prce->Oper() )
				{
				if ( prce->FActiveNotByMe( pfucb->ppib, trxSession ) )
					{
					//	there is an outstanding replace (not by us) on this
					//	node -- must reset compensating delta count to begin
					//	from this point
					lDelta = 0;

					//	UNDONE: return a flag indicating potential write
					//	conflict so ErrRECAOSeparateLV() will not bother even
					//	even trying to do a replace on the LVROOT and instead
					//	burst immediately
					//	*pfPotentialWriteConflict = fTrue;
					}
				}
			else if ( operDelta == prce->Oper() )
				{
				Assert( !prce->FRolledBack() );		// Delta RCE's can never be flag-RolledBack

				const VERDELTA* const pverdelta = ( VERDELTA* )prce->PbData();
				if ( pverdelta->cbOffset == cbOffset
					&& 	( prce->FActiveNotByMe( pfucb->ppib, trxSession )
						|| ( 	trxMax == prce->TrxCommitted()
								&& prce->Pfucb()->ppib == pfucb->ppib
								&& FUpdateIsActive( prce->Pfucb()->ppib, prce->Updateid() ) ) ) )
					{
					//  delta version created by a different session.
					//	the version is either uncommitted or created by
					//  a session that started after us
					//	or a delta done by a currently uncommitted replace
					lDelta += -pverdelta->lDelta;
					}
				}
			}
		}

	return lDelta;
	}


//  ================================================================
BOOL FVERDeltaActiveNotByMe( const FUCB * pfucb, const BOOKMARK& bookmark, INT cbOffset )
//  ================================================================
//
//	get prce for node and look for uncommitted increment/decrement
//	versions created by another session.Used to determine if the delta
//  value we see may change because of a rollback.
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );
	Assert( 0 == cbOffset );	//  should only be used from LV

	const TRX			trxSession		= pfucb->ppib->trxBegin0;
	Assert( trxMax != trxSession );

	const UINT			uiHash 			= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );

	BOOL 				fVersionExists	= fFalse;

	const RCE 			*prce 	= PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	for ( ; prceNil != prce; prce = prce->PrcePrevOfNode() )
		{
		const VERDELTA* const pverdelta = ( VERDELTA* )prce->PbData();
		if ( operDelta == prce->Oper()
			&& pverdelta->cbOffset == cbOffset
			&& pverdelta->lDelta != 0 )
			{
			const TRX	trxCommitted	= prce->TrxCommitted();
			if ( ( trxMax == trxCommitted
					&& prce->Pfucb()->ppib != pfucb->ppib )
				|| ( trxMax != trxCommitted
					&& TrxCmp( trxCommitted, trxSession ) > 0 ) )
				{
				//  uncommitted delta version created by another session
				fVersionExists = fTrue;
				break;
				}
			}
		}

	return fVersionExists;
	}


//  ================================================================
INLINE BOOL FVERIGetReplaceInRangeByUs(
	const PIB		*ppib,
	const RCE		*prceLastBeforeEndOfRange,
	const RCEID		rceidFirst,
	const RCEID		rceidLast,
	const TRX		trxBegin0,
	const TRX		trxCommitted,
	const BOOL		fFindLastReplace,
	const RCE		**pprceReplaceInRange )
//  ================================================================
	{
	const RCE		*prce;
	BOOL			fSawCommittedByUs = fFalse;
	BOOL			fSawUncommittedByUs = fFalse;

	Assert( prceNil != prceLastBeforeEndOfRange );
	Assert( trxCommitted == trxMax ? ppibNil != ppib : ppibNil == ppib );

	Assert( PverFromIfmp( prceLastBeforeEndOfRange->Ifmp() )->CritRCEChain( prceLastBeforeEndOfRange->UiHash() ).FOwner() );

	// Initialize return value.
	*pprceReplaceInRange = prceNil;

	Assert( rceidNull != rceidLast );
	Assert( rceidFirst < rceidLast );
	if ( rceidNull == rceidFirst )
		{
		// If rceidFirst == NULL, then no updates were done in the range (this
		// will force retrieval of the after-image (since no updates were done
		// the before-image will be equivalent to the after-image).
		return fFalse;
		}

	//  go backwards through all RCEs in the range
	const RCE	*prceFirstReplaceInRange = prceNil;
	for ( prce = prceLastBeforeEndOfRange;
		prceNil != prce && rceidFirst < prce->Rceid();
		prce = prce->PrcePrevOfNode() )
		{
		Assert( prce->Rceid() < rceidLast );
		if ( prce->FOperReplace() )
			{
			if ( fSawCommittedByUs )
				{
				// If only looking for the last RCE, we would have exited
				// before finding a second one.
				Assert( !fFindLastReplace );

				Assert( trxMax != trxCommitted );
				Assert( ppibNil == ppib );

				// If one node in the range belogs to us, they must all belong
				// to us (otherwise a WriteConflict would have been generated).
				if ( fSawUncommittedByUs )
					{
					Assert( prce->TrxCommitted() == trxMax );
					Assert( prce->TrxBegin0() == trxBegin0 );
					}
				else if ( prce->TrxCommitted() == trxMax )
					{
					Assert( prce->TrxBegin0() == trxBegin0 );
					fSawUncommittedByUs = fTrue;
					}
				else
					{
					Assert( prce->TrxCommitted() == trxCommitted );
					}
				}

			else if ( fSawUncommittedByUs )
				{
				// If only looking for the last RCE, we would have exited
				// before finding a second one.
				Assert( !fFindLastReplace );

				// If one node in the range belogs to us, they must all belong
				// to us (otherwise a WriteConflict would have been generated).
				Assert( prce->TrxCommitted() == trxMax );
				Assert( prce->TrxBegin0() == trxBegin0 );
				if ( trxMax == trxCommitted )
					{
					Assert( ppibNil != ppib );
					Assert( prce->Pfucb()->ppib == ppib );
					}
				}
			else
				{
				if ( prce->TrxCommitted() == trxMax )
					{
					Assert( ppibNil != prce->Pfucb()->ppib );
					if ( prce->TrxBegin0() != trxBegin0 )
						{
						// The node is uncommitted, but it doesn't belong to us.
						Assert( trxCommitted != trxMax
							|| prce->Pfucb()->ppib != ppib );
						return fFalse;
						}

					//	if original RCE also uncommitted, assert same session.
					//	if original RCE already committed, can't assert anything.
					Assert( trxCommitted != trxMax
						|| prce->Pfucb()->ppib == ppib );

					fSawUncommittedByUs = fTrue;
					}
				else if ( prce->TrxCommitted() == trxCommitted )
					{
					// The node was committed by us.
					Assert( trxMax != trxCommitted );
					Assert( ppibNil == ppib );
					fSawCommittedByUs = fTrue;
					}
				else
					{
					// The node is committed, but not by us.
					Assert( prce->TrxBegin0() != trxBegin0 );
					return fFalse;
					}

				if ( fFindLastReplace )
					{
					// Only interested in the existence of a replace in the range,
					// don't really want the image.
					return fTrue;
					}
				}

			prceFirstReplaceInRange = prce;
			}
		}	// for
	Assert( prceNil == prce || rceidFirst >= prce->Rceid() );

	if ( prceNil != prceFirstReplaceInRange )
		{
		*pprceReplaceInRange = prceFirstReplaceInRange;
		return fTrue;
		}

	return fFalse;
	}


//  ================================================================
#ifdef DEBUG
LOCAL VOID VERDBGCheckReplaceByOthers(
	const RCE	* const prce,
	const PIB	* const ppib,
	const TRX	trxCommitted,
	const RCE	* const prceFirstActiveReplaceByOther,
	BOOL		*pfFoundUncommitted,
	BOOL		*pfFoundCommitted )
	{
	if ( prce->TrxCommitted() == trxMax )
		{
		Assert( ppibNil != prce->Pfucb()->ppib );
		Assert( ppib != prce->Pfucb()->ppib );

		if ( *pfFoundUncommitted )
			{
			// All uncommitted RCE's should be owned by the same session.
			Assert( prceNil != prceFirstActiveReplaceByOther );
			Assert( prceFirstActiveReplaceByOther->TrxCommitted() == trxMax );
			Assert( prceFirstActiveReplaceByOther->TrxBegin0() == prce->TrxBegin0() );
			Assert( prceFirstActiveReplaceByOther->Pfucb()->ppib == prce->Pfucb()->ppib );
			}
		else
			{
			if ( *pfFoundCommitted )
				{
				//	must belong to same session in the middle of committing to level 0
				Assert( prceNil != prceFirstActiveReplaceByOther );
				Assert( prceFirstActiveReplaceByOther->TrxBegin0() == prce->TrxBegin0() );
				}
			else
				{
				Assert( prceNil == prceFirstActiveReplaceByOther );
				}

			*pfFoundUncommitted = fTrue;
			}
		}
	else
		{
		Assert( prce->TrxCommitted() != trxCommitted );

		if ( !*pfFoundCommitted )
			{
			if ( *pfFoundUncommitted )
				{
				// If there's also an uncommitted RCEs on this node,
				// it must have started its transaction after this one committed.
				Assert( prceNil != prceFirstActiveReplaceByOther );
				Assert( prceFirstActiveReplaceByOther->TrxCommitted() == trxMax );
				Assert( prce->Rceid() < prceFirstActiveReplaceByOther->Rceid() );
				Assert( TrxCmp( prce->TrxCommitted(), prceFirstActiveReplaceByOther->TrxBegin0() ) < 0 );
				}

			//	Cannot be any uncommitted RCE's of any type
			//	before a committed replace RCE, except if the
			//	same session is in the middle of committing to level 0.
			const RCE	* prceT;
			for ( prceT = prce; prceNil != prceT; prceT = prceT->PrcePrevOfNode() )
				{
				if ( prceT->TrxCommitted() == trxMax )
					{
					Assert( !*pfFoundUncommitted );
					Assert( prceT->TrxBegin0() == prce->TrxBegin0() );
					}
				}

			*pfFoundCommitted = fTrue;
			}
		}
	}
#endif
//  ================================================================


//  ================================================================
BOOL FVERGetReplaceImage(
	const PIB		*ppib,
	const IFMP		ifmp,
	const PGNO		pgnoLVFDP,
	const BOOKMARK& bookmark,
	const RCEID 	rceidFirst,
	const RCEID		rceidLast,
	const TRX		trxBegin0,
	const TRX		trxCommitted,
	const BOOL		fAfterImage,
	const BYTE 		**ppb,
	ULONG 			* const pcbActual
	)
//  ================================================================
//
//  Extract the before image from the oldest replace RCE for the bookmark
//  that falls exclusively within the range (rceidFirst,rceidLast)
//
//-
	{
	const UINT	uiHash				= UiRCHashFunc( ifmp, pgnoLVFDP, bookmark );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( ifmp )->CritRCEChain( uiHash ) ) );

	const RCE	*prce				= PrceRCEGet( uiHash, ifmp, pgnoLVFDP, bookmark );
	const RCE	*prceDesiredImage	= prceNil;
	const RCE	*prceFirstAfterRange= prceNil;

	Assert( rceidMax != rceidFirst );
	Assert( rceidMax != rceidLast );
	Assert( trxMax != trxBegin0 );

	// find the last RCE before the end of the range
	while ( prceNil != prce && prce->Rceid() >= rceidLast )
		{
		prceFirstAfterRange = prce;
		prce = prce->PrcePrevOfNode();
		}

	const RCE	* const prceLastBeforeEndOfRange	= prce;

	if ( prceNil == prceLastBeforeEndOfRange )
		{
		Assert( prceNil == prceDesiredImage );
		}
	else
		{
		Assert( prceNil == prceFirstAfterRange
			|| prceFirstAfterRange->Rceid() >= rceidLast );
		Assert( prceLastBeforeEndOfRange->Rceid() < rceidLast );
		Assert( prceFirstAfterRange == prceLastBeforeEndOfRange->PrceNextOfNode() );

		const BOOL fReplaceInRangeByUs = FVERIGetReplaceInRangeByUs(
													ppib,
													prceLastBeforeEndOfRange,
													rceidFirst,
													rceidLast,
													trxBegin0,
													trxCommitted,
													fAfterImage,
													&prceDesiredImage );
		if ( fReplaceInRangeByUs )
			{
			if ( fAfterImage )
				{
				// If looking for the after-image, it will be found in the
				// node's next replace RCE after the one found in the range.
				Assert( prceNil == prceDesiredImage );
				Assert( prceNil != prceLastBeforeEndOfRange );
				Assert( prceFirstAfterRange == prceLastBeforeEndOfRange->PrceNextOfNode() );
				}
			else
				{
				Assert( prceNil != prceDesiredImage );
				}
			}

		else if ( prceLastBeforeEndOfRange->TrxBegin0() == trxBegin0 )
			{
			//	If last operation before the end of the range belongs
			//	to us, then there will be no other active images on the
			//	node by other sessions (they would have write-conflicted).
			//	We can just fall through below to grab the proper image.

			if ( prceLastBeforeEndOfRange->TrxCommitted() != trxMax )
				{
				//	If the last RCE in the range has already committed,
				//	then the RCE on which the search was based must
				//	also have been committed at the same time.
				Assert( prceLastBeforeEndOfRange->TrxCommitted() == trxCommitted );
				Assert( ppibNil == ppib );
				}
			else if ( trxCommitted == trxMax )
				{
				//	Verify last RCE in the range belongs to same
				//	transaction.
				Assert( ppibNil != ppib );
				Assert( ppib == prceLastBeforeEndOfRange->Pfucb()->ppib );
				Assert( trxBegin0 == ppib->trxBegin0 );
				}
			else
				{
				//	This is the case where the RCE in the range is uncommitted,
				//	but the RCE on which the search was based has already
				//	committed.  This is a valid case (we could be looking at
				//	the RCE while the transaction is in the middle of being
				//	committed).  Can't really do anything to assert this except
				//	to check that the trxBegin0 is the same, which we've already
				//	done above.
				}

			// Force to look after the specified range for our image.
			Assert( prceNil == prceDesiredImage );
			}

		else
			{
			const RCE	*prceFirstActiveReplaceByOther = prceNil;
#ifdef DEBUG
			BOOL		fFoundUncommitted = fFalse;
			BOOL		fFoundCommitted = fFalse;
#endif

			Assert( prceNil == prceDesiredImage );
			Assert( prceFirstAfterRange == prceLastBeforeEndOfRange->PrceNextOfNode() );

			// No replace RCE's by us between the specified range, or
			// any RCE's by us of any type before the end of the range.
			// Check active RCE's by others.
			for ( prce = prceLastBeforeEndOfRange;
				prceNil != prce;
				prce = prce->PrcePrevOfNode() )
				{
				Assert( prce->Rceid() < rceidLast );
				//  For retrieving a LVROOT while getting a before image we may see deltas
				Assert( prce->TrxBegin0() != trxBegin0 || operDelta == prce->Oper() );

				if ( TrxCmp( prce->TrxCommitted(), trxBegin0 ) < 0 )
					break;	// No more active RCE's.

				if ( prce->FOperReplace() )
					{
#ifdef DEBUG
					VERDBGCheckReplaceByOthers(
								prce,
								ppib,
								trxCommitted,
								prceFirstActiveReplaceByOther,
								&fFoundUncommitted,
								&fFoundCommitted );
#endif

					// There may be multiple active RCE's on
					// the same node. We want the very first one.
					prceFirstActiveReplaceByOther = prce;
					}

				}	// for


			Assert( prceNil == prceDesiredImage );
			if ( prceNil != prceFirstActiveReplaceByOther )
				prceDesiredImage = prceFirstActiveReplaceByOther;
			}
		}

	// If no RCE's within range or before range, look after the range.
	if ( prceNil == prceDesiredImage )
		{
		for ( prce = prceFirstAfterRange;
			prceNil != prce;
			prce = prce->PrceNextOfNode() )
			{
			Assert( prce->Rceid() >= rceidLast );
			if ( prce->FOperReplace() )
				{
				prceDesiredImage = prce;
				break;
				}
			}
		}

	if ( prceNil != prceDesiredImage )
		{
		const VERREPLACE* pverreplace = (VERREPLACE*)prceDesiredImage->PbData();
		*pcbActual 	= prceDesiredImage->CbData() - cbReplaceRCEOverhead;
		*ppb 		= (BYTE *)pverreplace->rgbBeforeImage;
		return fTrue;
		}

	return fFalse;
	}


//  ================================================================
ERR VER::ErrVERICreateDMLRCE(
	FUCB			* pfucb,
	UPDATEID		updateid,
	const BOOKMARK&	bookmark,
	const UINT		uiHash,
	const OPER		oper,
	const LEVEL		level,
	const BOOL		fProxy,
	RCE 			**pprce,
	RCEID			rceid
	)
//  ================================================================
//
//	Creates a DML RCE in a bucket
//
//-
	{
	Assert( pfucb->ppib->level > 0 );
	Assert( level > 0 );
	Assert( pfucb->u.pfcb != pfcbNil );
	Assert( FOperInHashTable( oper ) );

	ERR		err		= JET_errSuccess;
	RCE		*prce	= prceNil;

	//	calculate the length of the RCE in the bucket.
	//	if updating node, set cbData in RCE to length of data. (w/o the key).
	//	set cbNewRCE as well.
	const INT cbBookmark = bookmark.key.Cb() + bookmark.data.Cb();

	INT cbNewRCE = sizeof( RCE ) + cbBookmark;
	switch( oper )
		{
		case operReplace:
			Assert( !pfucb->kdfCurr.data.FNull() );
			cbNewRCE += cbReplaceRCEOverhead + pfucb->kdfCurr.data.Cb();
			break;
		case operDelta:
		  	cbNewRCE += sizeof( VERDELTA );
			break;
		case operSLVSpace:
			cbNewRCE += pfucb->kdfCurr.data.Cb();
			break;
		case operInsert:
		case operFlagDelete:
		case operReadLock:
		case operWriteLock:
		case operPreInsert:
			break;
		default:
			Assert( fFalse );
			break;
		}

	//	Set up a skelton RCE. This holds m_critBucketGlobal, so do it
	//	first before filling the rest.
	//
#ifdef NO_BEGIN0_COMMIT0_LOCK
	Assert( trxMax == pfucb->ppib->trxCommit0 );
	Call( ErrVERICreateRCE(
			cbNewRCE,
			pfucb->u.pfcb,
			pfucb,
			updateid,
			pfucb->ppib->trxBegin0,
			&pfucb->ppib->trxCommit0,
			level,
			bookmark.key.Cb(),
			bookmark.data.Cb(),
			oper,
			uiHash,
			&prce,
			fProxy,
			rceid
			) );
#else
	Call( ErrVERICreateRCE(
			cbNewRCE,
			pfucb->u.pfcb,
			pfucb,
			updateid,
			pfucb->ppib->trxBegin0,
			level,
			bookmark.key.Cb(),
			bookmark.data.Cb(),
			oper,
			uiHash,
			&prce,
			fProxy,
			rceid
			) );
#endif

	if ( FOperConcurrent( oper ) )
		{
		Assert( CritRCEChain( uiHash ).FOwner() );
		}
	else
		{
		Assert( CritRCEChain( uiHash ).FNotOwner() );
		}

	//  copy the bookmark
	prce->CopyBookmark( bookmark );

	Assert( pgnoNull == prce->PgnoUndoInfo( ) );
	Assert( prce->Oper() != operAllocExt );
	Assert( !prce->FOperDDL() );

	//	flag FUCB version
	FUCBSetVersioned( pfucb );

	CallS( err );

HandleError:
	if ( pprce )
		{
		*pprce = prce;
		}

	return err;
	}


//  ================================================================
LOCAL VOID VERISetupInsertedDMLRCE( const FUCB * pfucb, RCE * prce )
//  ================================================================
//
//  This copies the appropriate data from the FUCB into the RCE and
//  propagates the maximum node size. This must be called after
//  insertion so the maximum node size can be found (for operReplace)
//
//  This currently does not need to be called from VERModifyByProxy
//
//-
	{
	Assert( prce->TrxCommitted() == trxMax );
	//	If replacing node, rather than inserting or deleting node,
	//	copy the data to RCE for before image for version readers.
	//	Data size may be 0.
	switch( prce->Oper() )
		{
		case operReplace:
			{
			Assert( prce->CbData() >= cbReplaceRCEOverhead );

			VERREPLACE* const pverreplace	= (VERREPLACE*)prce->PbData();

			if ( pfucb->fUpdateSeparateLV )
				{
				//  we updated a separateLV store the begin time of the PrepareUpdate
				pverreplace->rceidBeginReplace = pfucb->rceidBeginUpdate;
				}
			else
				{
				pverreplace->rceidBeginReplace = rceidNull;
				}

			const RCE * const prcePrevReplace = PrceVERIGetPrevReplace( prce );
			if ( prceNil != prcePrevReplace )
				{
				//  a previous version exists. its max size is the max of the before- and
				//  after-images (the after-image is our before-image)
				Assert( !prcePrevReplace->FRolledBack() );
				const VERREPLACE* const pverreplacePrev = (VERREPLACE*)prcePrevReplace->PbData();
				Assert( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering && fRecoveringUndo != PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecoveringMode ||
						pverreplacePrev->cbMaxSize >= (SHORT)pfucb->kdfCurr.data.Cb() );
				pverreplace->cbMaxSize = pverreplacePrev->cbMaxSize;
				}
			else
				{
				//  no previous replace. max size is the size of our before image
				pverreplace->cbMaxSize = (SHORT)pfucb->kdfCurr.data.Cb();
				}

			pverreplace->cbDelta = 0;

			Assert( prce->Oper() == operReplace );

			// move to data byte and copy old data (before image)
			UtilMemCpy( pverreplace->rgbBeforeImage, pfucb->kdfCurr.data.Pv(), pfucb->kdfCurr.data.Cb() );
			}
			break;

		case operDelta:
			{
			Assert( sizeof( VERDELTA ) == pfucb->kdfCurr.data.Cb() );
			*( VERDELTA* )prce->PbData() = *( VERDELTA* )pfucb->kdfCurr.data.Pv();
			}
			break;

		case operSLVSpace:
			{
			Assert( OffsetOf( VERSLVSPACE, wszFileName ) + sizeof( wchar_t ) <= pfucb->kdfCurr.data.Cb() );
			memcpy( prce->PbData(), pfucb->kdfCurr.data.Pv(), pfucb->kdfCurr.data.Cb() );
			}
			break;

		default:
			break;
		}
	}


//  ================================================================
LOCAL BOOL FVERIWriteConflict(
	FUCB*			pfucb,
	const BOOKMARK&	bookmark,
	UINT			uiHash,
	const OPER		oper
	)
//  ================================================================
	{
	BOOL			fWriteConflict	= fFalse;
	const TRX		trxSession		= TrxVERISession( pfucb );
	RCE*			prce 			= PrceRCEGet(
											uiHash,
											pfucb->ifmp,
											pfucb->u.pfcb->PgnoFDP(),
											bookmark );

	Assert( trxSession != trxMax );

	//  check for write conficts
	//  we can't use the pfucb of a committed transaction as the FUCB has been closed
	//  if a version is committed after we started however, it must have been
	//	created by another session
	if ( prce != prceNil )
		{
		if ( prce->FActiveNotByMe( pfucb->ppib, trxSession ) )
			{
			if ( operReadLock == oper
				|| operDelta == oper
				|| operSLVSpace == oper )
				{
				//	these operations commute. i.e. two sessions can perform
				//	these operations without conflicting
 				//  we can only do this modification if all the active RCE's
				//	in the chain are of this type
				//  look at all active versions for an operation not of this type
				//  OPTIMIZATION:	if the session changes again (i.e. we get a
				//					_third_ session) we can stop looking as we
				//					know that the second session commuted with
				//					the first, therefore the third will commute
				//					with the second and first (transitivity)
				const RCE	* prceT			= prce;
				for ( ;
					prceNil != prceT && TrxCmp( prceT->TrxCommitted(), trxSession ) > 0;
					prceT = prceT->PrcePrevOfNode() )
					{
					//  if all active RCEs have the same oper we are OK,
					//	else WriteConflict.
					if ( prceT->Oper() != oper )
						{
						fWriteConflict = fTrue;
						break;
						}
					}
				}
			else
				{
				fWriteConflict = fTrue;
				}
			}

		else
			{
#ifdef DEBUG
			if ( prce->TrxCommitted() == trxMax )
				{
				// Must be my uncommitted version, otherwise it would have been
				// caught by FActiveNotByMe().
				Assert( prce->Pfucb()->ppib == pfucb->ppib );
				Assert( prce->Level() <= pfucb->ppib->level
						|| PinstFromIfmp( pfucb->ifmp )->FRecovering() );		//	could be an RCE created by redo of UndoInfo
				}
			else
				{
				//	RCE exists, but it committed before our session began, so no
				//	danger of write conflict.
				//	Normally, this session's Begin0 cannot be equal to anyone else's Commit0,
				//	but because we only log an approximate trxCommit0, during recovery, we
				//	may find that this session's Begin0 is equal to someone else's Commit0
				Assert( TrxCmp( prce->TrxCommitted(), trxSession ) < 0
					|| ( prce->TrxCommitted() == trxSession && PinstFromIfmp( pfucb->ifmp )->FRecovering() ) );
				}
#endif

			if ( prce->Oper() != oper && (  operReadLock == prce->Oper()
											|| operDelta == prce->Oper()
											|| operSLVSpace == prce->Oper() ) )
				{
				//  these previous operation commuted. i.e. two sessions can perform
				//	these operations without conflicting
				//
				//	we are creating a different type of operation that does
				//	not commute
				//
				//  therefore we must check all active versions to make sure
				//	that we are the only session that has created them
				//
				//  we can only do this modification if all the active RCE's
				//	in the chain created by us
				//
				//  look at all versions for a active versions for a different session
				//
				const RCE	* prceT			= prce;
				for ( ;
					 prceNil != prceT;
					 prceT = prceT->PrcePrevOfNode() )
					{
					if ( prceT->FActiveNotByMe( pfucb->ppib, trxSession ) )
						{
						fWriteConflict = fTrue;
						break;
						}
					}

				Assert( fWriteConflict || prceNil == prceT );
				}
			}


#ifdef DEBUG
		if ( !fWriteConflict )
			{
			if ( prce->TrxCommitted() == trxMax )
				{
				Assert( prce->Pfucb()->ppib != pfucb->ppib
					|| prce->Level() <= pfucb->ppib->level
					|| PinstFromIfmp( prce->Pfucb()->ifmp )->FRecovering() );		//	could be an RCE created by redo of UndoInfo
				}

			if ( prce->Oper() == operFlagDelete )
				{
				//	normally, the only RCE that can follow a FlagDelete is an Insert.
				//	unless the RCE was moved, or if we're recovering, in which case we might
				//	create RCE's for UndoInfo
				Assert( operInsert == oper
					|| operPreInsert == oper
					|| operWriteLock == oper
					|| prce->FMoved()
					|| PinstFromIfmp( prce->Ifmp() )->FRecovering() );
				}
			}
#endif
		}

	return fWriteConflict;
	}

BOOL FVERWriteConflict(
	FUCB			* pfucb,
	const BOOKMARK&	bookmark,
	const OPER		oper )
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );

	const UINT		uiHash	= UiRCHashFunc(
									pfucb->ifmp,
									pfucb->u.pfcb->PgnoFDP(),
									bookmark );
	ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );

	return FVERIWriteConflict( pfucb, bookmark, uiHash, oper );
	}


//  ================================================================
INLINE ERR VER::ErrVERModifyCommitted(
	FCB				*pfcb,
	const BOOKMARK&	bookmark,
	const OPER		oper,
	const TRX		trxBegin0,
	const TRX		trxCommitted,
	RCE				**pprce
	)
//  ================================================================
//
//  Used by concurrent create index to create a RCE as though it was done
//  by another session. The trxCommitted of the RCE is set and no checks for
//  write conflicts are done
//
//-
	{
	ASSERT_VALID( &bookmark );
	Assert( pfcb->FTypeSecondaryIndex() );		// only called from concurrent create index.
	Assert( pfcb->PfcbTable() == pfcbNil );
	Assert( trxCommitted != trxMax );

	const UINT 			uiHash	= UiRCHashFunc( pfcb->Ifmp(), pfcb->PgnoFDP(), bookmark );

	ERR     			err 	= JET_errSuccess;
	RCE					*prce	= prceNil;

	//  assert default return value
	Assert( NULL != pprce );
	Assert( prceNil == *pprce );

		{
		//	calculate the length of the RCE in the bucket.
		const INT cbBookmark = bookmark.key.Cb() + bookmark.data.Cb();
		const INT cbNewRCE = sizeof( RCE ) + cbBookmark;

		//	Set up a skeleton RCE. This holds m_critBucketGlobal, so do it
		//	first before filling the rest.
		//
#ifdef NO_BEGIN0_COMMIT0_LOCK
		Call( ErrVERICreateRCE(
				cbNewRCE,
				pfcb,
				pfucbNil,
				updateidNil,
				trxBegin0,
				NULL,
				0,
				bookmark.key.Cb(),
				bookmark.data.Cb(),
				oper,
				uiHash,
				&prce,
				fTrue
				) );
#else
		Call( ErrVERICreateRCE(
				cbNewRCE,
				pfcb,
				pfucbNil,
				updateidNil,
				trxBegin0,
				0,
				bookmark.key.Cb(),
				bookmark.data.Cb(),
				oper,
				uiHash,
				&prce,
				fTrue
				) );
#endif

		//  copy the bookmark
		prce->CopyBookmark( bookmark );

		if( !prce->FOperConcurrent() )
			{
			ENTERCRITICALSECTION enterCritHash( &( CritRCEChain( uiHash ) ) );
			Call( ErrVERIInsertRCEIntoHash( prce ) );
			prce->SetCommittedByProxy( trxCommitted );
			}
		else
			{
			Call( ErrVERIInsertRCEIntoHash( prce ) );
			prce->SetCommittedByProxy( trxCommitted );
			CritRCEChain( uiHash ).Leave();
			}

		Assert( pgnoNull == prce->PgnoUndoInfo( ) );
		Assert( prce->Oper() == operWriteLock || prce->Oper() == operFlagDelete || prce->Oper() == operPreInsert );

		*pprce = prce;

		ASSERT_VALID( *pprce );
		}


HandleError:
	Assert( err < JET_errSuccess || prceNil != *pprce );
	return err;
	}


//  ================================================================
ERR VER::ErrVERModify(
	FUCB			* pfucb,
	const BOOKMARK&	bookmark,
	const OPER		oper,
	RCE				**pprce,
	const VERPROXY	* const pverproxy
	)
//  ================================================================
//
//	Create an RCE for a DML operation.
//
//  OPTIMIZATION:	combine delta/readLock/replace versions
//					remove redundant replace versions
//
//	RETURN VALUE
//		Jet_errWriteConflict for two cases:
//			-for any committed node, caller's transaction begin time
//			is less than node's level 0 commit time.
//			-for any uncommitted node except operDelta/operReadLock at all by another session
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );
	Assert( FOperInHashTable( oper ) );	//  not supposed to be in hash table? use VERFlag
	Assert( !bookmark.key.FNull() );
	Assert( !rgfmp[pfucb->ifmp].FVersioningOff() );
	Assert( !pfucb->ppib->FReadOnlyTrx() );

	//  set default return value
	Assert( NULL != pprce );
	*pprce = prceNil;

	ERR			err 			= JET_errSuccess;
	BOOL		fRCECreated		= fFalse;

	UPDATEID	updateid		= updateidNil;
	RCEID		rceid			= rceidNull;
	LEVEL		level;
	RCE			*prcePrimary	= prceNil;
	FUCB		*pfucbProxy		= pfucbNil;
	const BOOL	fProxy			= ( NULL != pverproxy );

	//  we never create an insert version at runtime. instead we create a writeLock version
	//  and use ChangeOper to change it into an insert
	//
	Assert( m_pinst->m_plog->m_fRecovering || operInsert != oper );

	Assert( !m_pinst->m_plog->m_fRecovering || ( fProxy && proxyRedo == pverproxy->proxy ) );
	if ( fProxy )
		{
		if ( proxyCreateIndex == pverproxy->proxy )
			{
			Assert( !m_pinst->m_plog->m_fRecovering );
			Assert( oper == operWriteLock
				|| oper == operPreInsert
				|| oper == operReplace		// via FlagInsertAndReplaceData
				|| oper == operFlagDelete );
			Assert( prceNil != pverproxy->prcePrimary );
			prcePrimary = pverproxy->prcePrimary;

			if ( pverproxy->prcePrimary->TrxCommitted() != trxMax )
				{
				err = ErrVERModifyCommitted(
							pfucb->u.pfcb,
							bookmark,
							oper,
							prcePrimary->TrxBegin0(),
							prcePrimary->TrxCommitted(),
							pprce );
				return err;
				}
			else
				{
				Assert( prcePrimary->Pfucb()->ppib->critTrx.FOwner() );

				level = prcePrimary->Level();

				// Need to allocate an FUCB for the proxy, in case it rolls back.
				CallR( ErrDIROpenByProxy(
							prcePrimary->Pfucb()->ppib,
							pfucb->u.pfcb,
							&pfucbProxy,
							level ) );
				Assert( pfucbNil != pfucbProxy );

				//	force pfucbProxy to be defer-closed, so that it will
				//	not be released until the owning session commits
				//	or rolls back
				Assert( pfucbProxy->ppib->level > 0 );
				FUCBSetVersioned( pfucbProxy );

				// Use proxy FUCB for versioning.
				pfucb = pfucbProxy;
				}
			}
		else
			{
			Assert( proxyRedo == pverproxy->proxy );
			Assert( m_pinst->m_plog->m_fRecovering );
			Assert( rceidNull != pverproxy->rceid );
			rceid = pverproxy->rceid;
			level = LEVEL( pverproxy->level );
			}
		}
	else
		{
		if ( FUndoableLoggedOper( oper )
			|| operPreInsert == oper )		//	HACK: this oper will get promoted to operInsert on success (or manually nullified on failure)
			{
			updateid = UpdateidOfPpib( pfucb->ppib );
			}
		else
			{
			//	If in the middle of an update, only a few
			//	non-DML RCE's are possible.  These RCE's
			//	will remain outstanding (by design) even
			//	if the update rolls back.
			//
			Assert( updateidNil == UpdateidOfPpib( pfucb->ppib )
				|| operWriteLock == oper
				|| operReadLock == oper );
			}
		level = pfucb->ppib->level;
		}

	const UINT 			uiHash		= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );

	Call( ErrVERICreateDMLRCE(
			pfucb,
			updateid,
			bookmark,
			uiHash,
			oper,
			level,
			fProxy,
			pprce,
			rceid ) );
	Assert( prceNil != *pprce );
	fRCECreated = fTrue;

	if ( FOperConcurrent( oper ) )
		{
		// For concurrent operations, we had to obtain critHash before
		// allocating an rceid, to ensure that rceid's are chained in order.
		Assert( CritRCEChain( uiHash ).FOwner() );
		}
	else
		{
		// For non-concurrent operations, rceid's won't be chained out
		// of order because all but one RCE will fail with write-conflict
		// below.
		CritRCEChain( uiHash ).Enter();
		}

	// UNDONE: CIM support for updates is currently broken -- only
	// used for dirty reads by concurrent create index.
	Assert( FPIBVersion( pfucb->ppib )
		|| ( prceNil != prcePrimary && prcePrimary->Pfucb()->ppib == pfucb->ppib ) );

	if ( !m_pinst->m_plog->m_fRecovering && FVERIWriteConflict( pfucb, bookmark, uiHash, oper ) )
		{
		Call( ErrERRCheck( JET_errWriteConflict ) );
		}

	Call( ErrVERIInsertRCEIntoHash( *pprce ) );

	VERISetupInsertedDMLRCE( pfucb, *pprce );

#ifdef DEBUG_VER
	{
	BOOKMARK bookmarkT;
	(*pprce)->GetBookmark( &bookmarkT );
	CallR( ErrCheckRCEChain(
		*PprceRCEChainGet( (*pprce)->UiHash(), (*pprce)->Ifmp(), (*pprce)->PgnoFDP(), bookmarkT ),
		(*pprce)->UiHash() ) );
	}
#endif	//	DEBUG_VER

	CritRCEChain( uiHash ).Leave();

	ASSERT_VALID( *pprce );

	CallS( err );

HandleError:
	if ( err < 0 && fRCECreated )
		{
		(*pprce)->NullifyOper();
		Assert( CritRCEChain( uiHash ).FOwner() );
		CritRCEChain( uiHash ).Leave();
		*pprce = prceNil;
		}

	if ( pfucbNil != pfucbProxy )
		{
		Assert( pfucbProxy->ppib->level > 0 );	// Ensure defer-closed, even on error
		Assert( FFUCBVersioned( pfucbProxy ) );
		DIRClose( pfucbProxy );
		}

	return err;
	}


//  ================================================================
ERR VER::ErrVERFlag( FUCB * pfucb, OPER oper, const VOID * pv, INT cb )
//  ================================================================
//
//  Creates a RCE for a DDL or space operation. The RCE is not put
//  in the hash table
//
//-
	{
#ifdef DEBUG
	ASSERT_VALID( pfucb );
	Assert( pfucb->ppib->level > 0 );
	Assert( cb >= 0 );
	Assert( !FOperInHashTable( oper ) );	//  supposed to be in hash table? use VERModify
	Ptls()->fAddColumn = operAddColumn == oper;
#endif	//	DEBUG

	ERR		err		= JET_errSuccess;
	RCE		*prce	= prceNil;
	FCB 	*pfcb	= pfcbNil;

	if ( rgfmp[pfucb->ifmp].FVersioningOff() )
		{
		Assert( !rgfmp[pfucb->ifmp].FLogOn() );
		return JET_errSuccess;
		}

	pfcb = pfucb->u.pfcb;
	Assert( pfcb != NULL );

	//	Set up a skeleton RCE. This holds m_critBucketGlobal, so do it
	//	first before filling the rest.
	//
#ifdef NO_BEGIN0_COMMIT0_LOCK
	Assert( trxMax == pfucb->ppib->trxCommit0 );
	Call( ErrVERICreateRCE(
			sizeof(RCE) + cb,
			pfucb->u.pfcb,
			pfucb,
			updateidNil,
			pfucb->ppib->trxBegin0,
			&pfucb->ppib->trxCommit0,
			pfucb->ppib->level,
			0,
			0,
			oper,
			uiHashInvalid,
			&prce
			) );
#else
	Call( ErrVERICreateRCE(
			sizeof(RCE) + cb,
			pfucb->u.pfcb,
			pfucb,
			updateidNil,
			pfucb->ppib->trxBegin0,
			pfucb->ppib->level,
			0,
			0,
			oper,
			uiHashInvalid,
			&prce
			) );
#endif

	UtilMemCpy( prce->PbData(), pv, cb );

	Assert( prce->TrxCommitted() == trxMax );
	VERInsertRCEIntoLists( pfucb, pcsrNil, prce, NULL );

	ASSERT_VALID( prce );

	FUCBSetVersioned( pfucb );

HandleError:
#ifdef DEBUG
	Ptls()->fAddColumn = fFalse;
#endif	//	DEBUG

	return err;
	}


//  ================================================================
VOID VERSetCbAdjust(
			CSR 		*pcsr,
	const 	RCE		 	*prce,
			INT 		cbDataNew,
			INT 		cbDataOld,
			UPDATEPAGE 	updatepage )
//  ================================================================
//
//  Sets the max size and delta fields in a replace RCE
//
//	WARNING:  The following comments explain how a Replace RCE's delta field
//	(ie. the second SHORT stored in rgbData) is used.  The semantics can get
//	pretty confusing, so PLEASE DO NOT REMOVE THESE COMMENTS.  -- JL
//
//	*psDelta records how much the operation contributes to deferred node
//	space reservation. A positive cbDelta here means the node is growing,
//	so we will use up space which may have been reserved (ie. *psDelta will
//	decrease).  A negative cbDelta here means the node is shrinking,
//	so we must add abs(cbDelta) to the *psDelta to reflect how much more node
//	space must be reserved.
//
//	This is how to interpret the value of *psDelta:
//		- if *psDelta is positive, then *psDelta == reserved node space.  *psDelta can only
//		  be positive after a node shrinkage.
//		- if *psDelta is negative, then abs(*psDelta) is the reserved node space that
//		  was consumed during a node growth.  *psDelta can only become negative
//		  after a node shrinkage (which sets aside some reserved node space)
//		  followed by a node growth (which consumes some/all of that
//		  reserved node space).
//
//-
	{
	ASSERT_VALID( pcsr );
	Assert( pcsr->Latch() == latchWrite || fDoNotUpdatePage == updatepage );
	Assert( fDoNotUpdatePage == updatepage || pcsr->Cpage().FLeafPage() );
	Assert( prce->FOperReplace() );

	INT	cbDelta = cbDataNew - cbDataOld;

	Assert( cbDelta != 0 );

	VERREPLACE* const pverreplace = (VERREPLACE*)prce->PbData();
	INT	cbMax = pverreplace->cbMaxSize;
	Assert( pverreplace->cbMaxSize >= cbDataOld );

	//	set new node maximum size.
	if ( cbDataNew > cbMax )
		{
		//  this is the largest the node has ever been. set the max size and
		//  free all previously reserved space
		Assert( cbDelta > 0 );
		pverreplace->cbMaxSize	= SHORT( cbDataNew );
		cbDelta 				= cbMax - cbDataOld;
		}

	pverreplace->cbDelta = SHORT( pverreplace->cbDelta - cbDelta );

	if ( fDoUpdatePage == updatepage )
		{
		if ( cbDelta > 0 )
			{
			// If, during this transaction, we've shrunk the node.  There will be
			// some uncommitted freed space.  Reclaim as much of this as needed to
			// satisfy the new node growth.  Note that we can update cbUncommittedFreed
			// in this fashion because the subsequent call to ErrPMReplace() is
			// guaranteed to succeed (ie. the node is guaranteed to grow).
			pcsr->Cpage().ReclaimUncommittedFreed( cbDelta );
			}
		else if ( cbDelta < 0 )
			{
			// Node has decreased in size.  The page header's cbFree has already
			// been increased to reflect this.  But we must also increase
			// cbUncommittedFreed to indicate that the increase in cbFree is
			// contingent on commit of this operation.
			pcsr->Cpage().AddUncommittedFreed( -cbDelta );
			}
		}
#ifdef DEBUG
	else
		{
		Assert( fDoNotUpdatePage == updatepage );
		}
#endif	//	DEBUG
	}


//  ================================================================
LOCAL INT CbVERIGetNodeMax( const FUCB * pfucb, const BOOKMARK& bookmark, UINT uiHash )
//  ================================================================
//
//  This assumes nodeMax is propagated through the replace RCEs. Assumes
//  it is in the critical section to CbVERGetNodeReserverved can use it for
//  debugging
//
//-
	{
	INT			nodeMax = 0;

	// Look for any replace RCE's.
	const RCE *prce = PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	for ( ; prceNil != prce && trxMax == prce->TrxCommitted(); prce = prce->PrcePrevOfNode() )
		{
		if ( prce->FOperReplace() && !prce->FRolledBack() )
			{
			nodeMax = ((const VERREPLACE*)prce->PbData())->cbMaxSize;
			break;
			}
		}

	Assert( nodeMax >= 0 );
	return nodeMax;
	}


//  ================================================================
INT CbVERGetNodeMax( const FUCB * pfucb, const BOOKMARK& bookmark )
//  ================================================================
//
//  This enters the critical section and calls CbVERIGetNodeMax
//
//-
	{
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );

	const UINT				uiHash	= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );
	const INT				nodeMax = CbVERIGetNodeMax( pfucb, bookmark, uiHash );

	Assert( nodeMax >= 0 );
	return nodeMax;
	}


//  ================================================================
INT CbVERGetNodeReserve( const PIB * ppib, const FUCB * pfucb, const BOOKMARK& bookmark, INT cbCurrentData )
//  ================================================================
	{
	Assert( ppibNil == ppib || (ASSERT_VALID( ppib ), fTrue) );
	ASSERT_VALID( pfucb );
	ASSERT_VALID( &bookmark );
	Assert( cbCurrentData >= 0 );

	const UINT				uiHash			= UiRCHashFunc( pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( pfucb->ifmp )->CritRCEChain( uiHash ) ) );
	const BOOL				fIgnorePIB		= ( ppibNil == ppib );

	INT						cbNodeReserve	= 0;

	// Find all uncommitted RCE's for this node.
	const RCE * prce = PrceRCEGet( uiHash, pfucb->ifmp, pfucb->u.pfcb->PgnoFDP(), bookmark );
	for ( ; prceNil != prce && trxMax == prce->TrxCommitted(); prce = prce->PrcePrevOfNode() )
		{
		ASSERT_VALID( prce );
		if ( ( fIgnorePIB || prce->Pfucb()->ppib == ppib )
			&& prce->FOperReplace()
			&& !prce->FRolledBack() )
			{
			const VERREPLACE* const pverreplace = (VERREPLACE*)prce->PbData();
			cbNodeReserve += pverreplace->cbDelta;
			}
		}

	// The deltas should always net out to a non-negtative value.
	Assert( cbNodeReserve >= 0 );
	Assert(	cbNodeReserve == 0  ||
			cbNodeReserve == CbVERIGetNodeMax( pfucb, bookmark, uiHash ) - (INT)cbCurrentData );

	return cbNodeReserve;
	}


//  ================================================================
BOOL FVERCheckUncommittedFreedSpace(
	const FUCB	* pfucb,
	CSR			* const pcsr,
	const INT	cbReq,
	const BOOL	fPermitUpdateUncFree )
//  ================================================================
//
// This function is called after it has been determined that cbFree will satisfy
// cbReq. We now check that cbReq doesn't use up any uncommitted freed space.
//
//-
	{
	BOOL	fEnoughPageSpace	= fTrue;

	ASSERT_VALID( pfucb );
	ASSERT_VALID( pcsr );
	Assert( cbReq <= pcsr->Cpage().CbFree() );

	//	during recovery we would normally set cbUncommitted free to 0
	//	but if we didn't redo anything on the page and are now rolling
	//	back this may not be the case
	if( PinstFromPfucb( pfucb )->FRecovering() && fPermitUpdateUncFree )
		{
		if( pcsr->Cpage().CbUncommittedFree() != 0 )
			{
			LATCH	latchOld;

			//	do this until we succeed. recovery is single threaded so we
			//	should only conflict with the buffer manager
			while ( pcsr->ErrUpgradeToWARLatch( &latchOld ) != JET_errSuccess )
				{
				UtilSleep( cmsecWaitGeneric );
				}

			pcsr->Cpage().SetCbUncommittedFree( 0 );
			BFDirty( pcsr->Cpage().PBFLatch(), bfdfUntidy );
			pcsr->DowngradeFromWARLatch( latchOld );
			}

		return fTrue;
		}


	// We should already have performed the check against cbFree only (in other
	// words, this function is only called from within FNDFreePageSpace(),
	// or something that simulates its function).  This tells us that if all
	// currently-uncommitted transactions eventually commit, we should have
	// enough space to satisfy this request.
	Assert( cbReq <= pcsr->Cpage().CbFree() );

	// The amount of space freed but possibly uncommitted should be a subset of
	// the total amount of free space for this page.
	Assert( pcsr->Cpage().CbUncommittedFree() >= 0 );
	Assert( pcsr->Cpage().CbFree() >= pcsr->Cpage().CbUncommittedFree() );

	// In the worst case, all transactions that freed space on this page will
	// rollback, causing the space freed to be reclaimed.  If the space
	// required can be satisfied even in the worst case, then we're okay;
	// otherwise, we have to do more checking.
	if ( cbReq > pcsr->Cpage().CbFree() - pcsr->Cpage().CbUncommittedFree() )
		{
		Assert( !FFUCBSpace( pfucb ) );
		Assert( !pcsr->Cpage().FSpaceTree() );

		//	UNDONE:	use the CbNDUncommittedFree call
		//			to get rglineinfo for later split
		//			this will reduce CPU usage for RCE hashing
		//
		const INT	cbUncommittedFree = CbNDUncommittedFree( pfucb, pcsr );
		Assert( cbUncommittedFree <= pcsr->Cpage().CbUncommittedFree() );
		Assert( cbUncommittedFree >= 0 );

		if ( cbUncommittedFree == pcsr->Cpage().CbUncommittedFree() )
			{
			//	cbUncommittedFreed in page is correct
			//	return
			//
			fEnoughPageSpace = fFalse;
			}
		else
			{
			if ( fPermitUpdateUncFree )
				{
				// Try updating cbUncommittedFreed, in case some freed space was committed.
				LATCH latchOld;
				if ( pcsr->ErrUpgradeToWARLatch( &latchOld ) == JET_errSuccess )
					{
					pcsr->Cpage().SetCbUncommittedFree( cbUncommittedFree );
					BFDirty( pcsr->Cpage().PBFLatch(), bfdfUntidy );
					pcsr->DowngradeFromWARLatch( latchOld );
					}
				}

			// The amount of space freed but possibly uncommitted should be a subset of
			// the total amount of free space for this page.
			Assert( pcsr->Cpage().CbUncommittedFree() >= 0 );
			Assert( pcsr->Cpage().CbFree() >= pcsr->Cpage().CbUncommittedFree() );

			fEnoughPageSpace = ( cbReq <= ( pcsr->Cpage().CbFree() - cbUncommittedFree ) );
			}
		}

	return fEnoughPageSpace;
	}



//  ****************************************************************
//  RCE CLEANUP
//  ****************************************************************


//  ================================================================
LOCAL ERR VER::ErrVERIPossiblyDeleteLV( const RCE * const prce )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Assert( dbidTemp != rgfmp[ prce->Ifmp() ].Dbid() );
	Assert( operDelta == prce->Oper() );

	const VERDELTA* const pverdelta = reinterpret_cast<const VERDELTA*>( prce->PbData() );
	Assert( pverdelta->fDeferredDelete );
	Assert( !prce->Pfcb()->FDeleteCommitted() );
	Assert( !m_pinst->m_plog->m_fRecovering );
	Assert( !prce->Pfcb()->Ptdb() );	//  LV trees don't have TDB's

	BOOKMARK	bookmark;
	prce->GetBookmark( &bookmark );
	DELETELVTASK * const ptask = new DELETELVTASK( prce->PgnoFDP(), prce->Pfcb(), prce->Ifmp(), bookmark );
	if ( NULL == ptask )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
	if ( m_fSyncronousTasks
		|| rgfmp[prce->Ifmp()].FDetachingDB()
		|| m_pinst->Taskmgr().CPostedTasks() > m_ulVERTasksPostMax )	//	if task manager is overloaded, perform task synchronously
		{
		IncrementCSyncCleanupDispatched();
		TASK::Dispatch( m_ppibRCEClean, (ULONG_PTR)ptask );
		}
	else
		{
		IncrementCAsyncCleanupDispatched();
		err = m_pinst->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask );
		if( err < JET_errSuccess )
			{
			//  The task was not enqueued sucessfully.
			IncrementCCleanupFailed();
			delete ptask;
			}
		}

	return err;
	}


//  ================================================================
LOCAL ERR VER::ErrVERIPossiblyFinalize( const RCE * const prce )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Assert( dbidTemp != rgfmp[ prce->Ifmp() ].Dbid() );
	Assert( operDelta == prce->Oper() );
	Assert( trxMax != prce->TrxCommitted() );
	Assert( TrxCmp( prce->TrxCommitted(), TrxOldest( m_pinst ) ) < 0 );

	const VERDELTA* const pverdelta = reinterpret_cast<const VERDELTA*>( prce->PbData() );

	Assert( pverdelta->fCallbackOnZero || pverdelta->fDeleteOnZero );
	Assert( !prce->Pfcb()->FDeleteCommitted() );
	Assert( !m_pinst->m_plog->m_fRecovering );
	Assert( prce->Pfcb()->Ptdb() );

	BOOKMARK	bookmark;
	prce->GetBookmark( &bookmark );
	FINALIZETASK * const ptask = new FINALIZETASK(
										prce->PgnoFDP(),
										prce->Pfcb(),
										prce->Ifmp(),
										bookmark,
										pverdelta->cbOffset,
										pverdelta->fCallbackOnZero,
										pverdelta->fDeleteOnZero );
	if ( NULL == ptask )
		{
		err = ErrERRCheck( JET_errOutOfMemory );
		}
	else if ( m_fSyncronousTasks
		|| rgfmp[prce->Ifmp()].FDetachingDB()
		|| m_pinst->Taskmgr().CPostedTasks() > m_ulVERTasksPostMax )	//	if task manager is overloaded, perform task synchronously
		{
		IncrementCSyncCleanupDispatched();
		TASK::Dispatch( m_ppibRCECleanCallback, (ULONG_PTR)ptask );
		}
	else
		{
		IncrementCAsyncCleanupDispatched();
		err = m_pinst->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask );
		if( err < JET_errSuccess )
			{
			//  The task was not enqueued sucessfully.
			IncrementCCleanupFailed();
			delete ptask;
			}
		}

	return err;
	}


//  ================================================================
LOCAL ERR RCE::ErrGetTaskForDelete( VOID ** ppvtask ) const
//  ================================================================
	{
	ERR					err		= JET_errSuccess;
	DELETERECTASK *		ptask	= NULL;

	Assert( PverFromIfmp( Ifmp() )->m_critRCEClean.FOwner() );

	//	since we have m_critRCEClean, we're guaranteed that the RCE will not go away,
	//	though it may get nullified (eg. VERNullifyInactiveVersionsOnBM().  For this reason,
	//	we cannot access members using methods, or else FAssertReadable() asserts
	//	may go off.
	//
	const UINT				uiHash		= m_uiHash;
	ENTERCRITICALSECTION	enterCritRCEChain( &( PverFromIfmp( Ifmp() )->CritRCEChain( uiHash ) ) );

	if ( !FOperNull() )
		{
		Assert( operFlagDelete == Oper() );

		//	no cleanup for temporary tables or tables/indexes scheduled for deletion
		Assert( dbidTemp != rgfmp[ Ifmp() ].Dbid() );
		if ( !Pfcb()->FDeletePending() )
			{
			BOOKMARK	bm;
			GetBookmark( &bm );

			ptask = new DELETERECTASK( PgnoFDP(), Pfcb(), Ifmp(), bm );
			if( NULL == ptask )
				{
				err = ErrERRCheck( JET_errOutOfMemory );
				}
			}
		}

	*ppvtask = ptask;

	return err;
	}


//  ================================================================
LOCAL ERR VER::ErrVERIDelete( PIB * ppib, const RCE * const prce )
//  ================================================================
	{
	ERR				err;
	DELETERECTASK *	ptask;

	Assert( ppibNil != ppib );
	Assert( 0 == ppib->level );

	Assert( dbidTemp != rgfmp[prce->Ifmp()].Dbid() );
	Assert( !m_pinst->FRecovering() );

	CallR( prce->ErrGetTaskForDelete( (VOID **)&ptask ) );

	if( NULL == ptask )
		{
		//	we determined that a task wasn't needed for whatever reason
		//
		CallS( err );
		}

	else if ( m_fSyncronousTasks
		|| rgfmp[prce->Ifmp()].FDetachingDB()
		|| m_pinst->Taskmgr().CPostedTasks() > m_ulVERTasksPostMax )	//	if task manager is overloaded, perform task synchronously
		{
		IncrementCSyncCleanupDispatched();
		TASK::Dispatch( m_ppibRCECleanCallback, (ULONG_PTR)ptask );
		CallS( err );
		}

	else
		{
		IncrementCAsyncCleanupDispatched();
		err = m_pinst->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask );
		if( err < JET_errSuccess )
			{
			//  The task was not enqueued sucessfully.
			//
			IncrementCCleanupFailed();
			delete ptask;
			}
		}

	return err;
	}


//  ================================================================
LOCAL VOID VERIFreeExt( PIB * const ppib, FCB *pfcb, PGNO pgnoFirst, CPG cpg )
//  ================================================================
//	Throw away any errors encountered -- at worst, we just lose space.
	{
	Assert( pfcb );

	ERR     err;
	FUCB    *pfucb = pfucbNil;

	Assert( !PinstFromPpib( ppib )->m_plog->m_fRecovering );
	Assert( ppib->level > 0 );

	err = ErrDBOpenDatabaseByIfmp( ppib, pfcb->Ifmp() );
	if ( err < 0 )
		return;

	// Can't call DIROpen() because this function gets called
	// only during rollback AFTER the logic in DIRRollback()
	// which properly resets the navigation level of this
	// session's cursors.  Thus, when we close the cursor
	// below, the navigation level will not get properly reset.
	// Call( ErrDIROpen( ppib, pfcb, &pfucb ) );
	Call( ErrBTOpen( ppib, pfcb, &pfucb ) );

	Assert( !FFUCBSpace( pfucb ) );
	(VOID)ErrSPFreeExt( pfucb, pgnoFirst, cpg );

HandleError:
	if ( pfucbNil != pfucb )
		{
		Assert( !FFUCBDeferClosed( pfucb ) );
		FUCBAssertNoSearchKey( pfucb );
		Assert( !FFUCBCurrentSecondary( pfucb ) );
		BTClose( pfucb );
		}

	(VOID)ErrDBCloseDatabase( ppib, pfcb->Ifmp(), NO_GRBIT );
	}


#ifdef DEBUG
//  ================================================================
BOOL FIsRCECleanup()
//  ================================================================
//
//  DEBUG:  is the current thread a RCECleanup thread?
	{
	return Ptls()->fIsRCECleanup;
	}

//  ================================================================
BOOL FInCritBucket( VER *pver )
//  ================================================================
//
//  DEBUG:  is the current thread in m_critBucketGlobal
	{
	return pver->m_critBucketGlobal.FOwner();
	}
#endif	//	DEBUG


//  ================================================================
BOOL FPIBSessionRCEClean( PIB *ppib )
//  ================================================================
//
// Is the given PIB the one used by RCE clean?
//
//-
	{
	Assert( ppibNil != ppib );
	return ( (PverFromPpib( ppib ))->m_ppibRCEClean == ppib );
	}


//  ================================================================
INLINE VOID VERIUnlinkDefunctSecondaryIndex(
	PIB	* const ppib,
	FCB	* const pfcb )
//  ================================================================
	{
	Assert( pfcb->FTypeSecondaryIndex() );

	// Must unlink defunct FCB from all deferred-closed cursors.
	// The cursors themselves will be closed when the
	// owning session commits or rolls back.
	pfcb->Lock();

	while ( pfcb->Pfucb() != pfucbNil )
		{
		FUCB * const	pfucbT = pfcb->Pfucb();
		PIB	* const		ppibT = pfucbT->ppib;

		pfcb->Unlock();

		Assert( ppibNil != ppibT );
		if ( ppib == ppibT )
			{
			FCBUnlink( pfucbT );

			BTReleaseBM( pfucbT );
			// If cursor belongs to us, we can close
			// it right now.
			FUCBClose( pfucbT );
			}
		else
			{
			ppibT->critTrx.Enter();

			//	if undoing CreateIndex, we know other session must be
			//	in a transaction if it has a link to this FCB because
			//	the index is not visible yet.

			// FCB may have gotten unlinked if other session
			// committed or rolled back while we were switching
			// critical sections.
			if ( pfcb->Pfucb() == pfucbT )
				FCBUnlink( pfucbT );

			ppibT->critTrx.Leave();
			}

		pfcb->Lock();
		}

	pfcb->Unlock();
	}


//  ================================================================
INLINE VOID VERIUnlinkDefunctLV(
	PIB	* const ppib,
	FCB	* const pfcb )
//  ================================================================
//
//  If we are rolling back, only the owning session could have seen
//  the LV tree, because
//		-- the table was opened exclusively and the session that opened
//		the table created the LV tree
//		-- ppibLV created the LV tree and is rolling back before returning
//
//-
	{
	Assert( pfcb->FTypeLV() );

	pfcb->Lock();

	while ( pfcb->Pfucb() != pfucbNil )
		{
		FUCB * const	pfucbT = pfcb->Pfucb();
		PIB	* const		ppibT = pfucbT->ppib;

		pfcb->Unlock();

		Assert( ppib == ppibT );
		FCBUnlink( pfucbT );
		FUCBClose( pfucbT );

		pfcb->Lock();
		}

	pfcb->Unlock();
	}


//  ================================================================
ERR VER::ErrVERICleanDeltaRCE( const RCE * const prce )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	const VERDELTA* const pverdelta = reinterpret_cast<const VERDELTA*>( prce->PbData() );
	if ( pverdelta->fDeferredDelete && !prce->Pfcb()->FDeletePending() )
		{
		err = ErrVERIPossiblyDeleteLV( prce );
		}
	else if ( ( pverdelta->fCallbackOnZero || pverdelta->fDeleteOnZero )
			&& !prce->Pfcb()->FDeletePending() )
		{
		err = ErrVERIPossiblyFinalize( prce );
		}
	return err;
	}


#ifdef DISABLE_SLV
#define ErrVERICleanSLVSpaceRCE(a)	ErrERRCheck( JET_wrnNyi )
#else
//  ================================================================
ERR VER::ErrVERICleanSLVSpaceRCE( const RCE * const prce )
//  ================================================================
	{
	ERR		err		= JET_errSuccess;
	TASK*	ptask	= NULL;

	//  this space was deleted and is now older than the oldest transaction

	const VERSLVSPACE* const pverslvspace = (VERSLVSPACE*)( prce->PbData() );
	if ( slvspaceoperCommittedToDeleted == pverslvspace->oper )
		{
		//  the SLV Provider is enabled

		if ( PinstFromIfmp( prce->Ifmp() )->FSLVProviderEnabled() )
			{
			//  register the space as deleted with the SLV Provider.  it will
			//  move the space to free when no SLV File handles are open on the
			//  space any longer

			BOOKMARK	bookmark;
			prce->GetBookmark( &bookmark );
			Assert( sizeof( PGNO ) == bookmark.key.Cb() );
			Assert( 0 == bookmark.data.Cb() );

			PGNO pgnoLastInExtent;
			LongFromKey( &pgnoLastInExtent, bookmark.key );
			Assert( pgnoLastInExtent != 0 );
			Assert( pgnoLastInExtent % cpgSLVExtent == 0 );
			PGNO pgnoFirst = ( pgnoLastInExtent - cpgSLVExtent + 1 ) + pverslvspace->ipage;
			QWORD ibLogical = OffsetOfPgno( pgnoFirst );
			QWORD cbSize = QWORD( pverslvspace->cpages ) * SLVPAGE_SIZE;

			ptask = new OSSLVDELETETASK(
							prce->Ifmp(),
							ibLogical,
							cbSize,
							CSLVInfo::FILEID( pverslvspace->fileid ),
							pverslvspace->cbAlloc,
							(const wchar_t*)pverslvspace->wszFileName );
			}

		//  the SLV Provider is not enabled

		else
			{
			//  move the space to free

			BOOKMARK	bookmark;
			prce->GetBookmark( &bookmark );
			Assert( sizeof( PGNO ) == bookmark.key.Cb() );
			Assert( 0 == bookmark.data.Cb() );
			ptask = new SLVSPACETASK(
							prce->PgnoFDP(),
							prce->Pfcb(),
							prce->Ifmp(),
							bookmark,
							slvspaceoperDeletedToFree,
							pverslvspace->ipage,
							pverslvspace->cpages );
			}

		//  execute the task

		if ( NULL == ptask )
			{
			return ErrERRCheck( JET_errOutOfMemory );
			}
		if ( m_fSyncronousTasks
			|| rgfmp[prce->Ifmp()].FDetachingDB()
			|| m_pinst->Taskmgr().CPostedTasks() > m_ulVERTasksPostMax )	//	if task manager is overloaded, perform task synchronously
			{
			IncrementCSyncCleanupDispatched();
			TASK::Dispatch( m_ppibRCEClean, (ULONG_PTR)ptask );
			}
		else
			{
			IncrementCAsyncCleanupDispatched();
			err = m_pinst->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask );
			if ( err < JET_errSuccess )
				{
				//  The task was not enqueued sucessfully.
				IncrementCCleanupFailed();
				delete ptask;
				}
			}
		}

	return err;
	}

#endif	//	DISABLE_SLV


//  ================================================================
LOCAL VOID VERIRemoveCallback( const RCE * const prce )
//  ================================================================
//
//  Remove the callback from the list
//
//-
	{
	Assert( prce->CbData() == sizeof(VERCALLBACK) );
	const VERCALLBACK* const pvercallback = (VERCALLBACK*)prce->PbData();
	CBDESC * const pcbdescRemove = pvercallback->pcbdesc;
	prce->Pfcb()->EnterDDL();
	prce->Pfcb()->Ptdb()->UnregisterPcbdesc( pcbdescRemove );
	prce->Pfcb()->LeaveDDL();
	delete pcbdescRemove;
	}


INLINE VOID VER::IncrementCAsyncCleanupDispatched()
	{
#ifdef VERPERF
	cVERAsyncCleanupDispatched.Inc( m_pinst );
#endif
	}

INLINE VOID VER::IncrementCSyncCleanupDispatched()
	{
#ifdef VERPERF
	cVERSyncCleanupDispatched.Inc( m_pinst );
#endif
	}

VOID VER::IncrementCCleanupFailed()
	{
#ifdef VERPERF
	cVERCleanupFailed.Inc( m_pinst );
#endif
	}

LOCAL VOID VER::IncrementCCleanupDiscarded( const RCE * const prce )
	{
#ifdef VERPERF
	cVERCleanupDiscarded.Inc( m_pinst );
#endif	//	VERPERF

	VERIReportDiscardedDeletes( prce );
	}


//  ================================================================
ERR VER::ErrVERICleanOneRCE( RCE * const prce )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	Assert( m_critRCEClean.FOwner() );
	Assert( dbidTemp != rgfmp[ prce->Ifmp() ].Dbid() );
	Assert( prce->TrxCommitted() != trxMax );
	Assert( !prce->FRolledBack() );

	switch( prce->Oper() )
		{
		case operCreateTable:
			// RCE list ensures FCB is still pinned
			Assert( pfcbNil != prce->Pfcb() );
			Assert( prce->Pfcb()->PrceOldest() != prceNil );
			if ( prce->Pfcb()->FTypeTable() )
				{
				if ( FCATHashActive( m_pinst ) )
					{

					//	catalog hash is active so we need to insert this table

					CHAR szTable[JET_cbNameMost+1];

					//	read the table-name from the TDB

					prce->Pfcb()->EnterDML();
					strcpy( szTable, prce->Pfcb()->Ptdb()->SzTableName() );
					prce->Pfcb()->LeaveDML();

					//	insert the table into the catalog hash

					CATHashIInsert( prce->Pfcb(), szTable );
					}
				}
			else
				{
				Assert( prce->Pfcb()->FTypeTemporaryTable() );
				}
			break;

		case operAddColumn:
			{
			// RCE list ensures FCB is still pinned
			Assert( prce->Pfcb()->PrceOldest() != prceNil );

			Assert( prce->CbData() == sizeof(VERADDCOLUMN) );
			const JET_COLUMNID		columnid			= ( (VERADDCOLUMN*)prce->PbData() )->columnid;
			BYTE					* pbOldDefaultRec	= ( (VERADDCOLUMN*)prce->PbData() )->pbOldDefaultRec;
			FCB						* pfcbTable			= prce->Pfcb();

			pfcbTable->EnterDDL();

			TDB						* const ptdb		= pfcbTable->Ptdb();
			FIELD					* const pfield		= ptdb->Pfield( columnid );

			FIELDResetVersionedAdd( pfield->ffield );

			// Only reset the Versioned bit if a Delete
			// is not pending.
			if ( FFIELDVersioned( pfield->ffield ) && !FFIELDDeleted( pfield->ffield ) )
				{
				FIELDResetVersioned( pfield->ffield );
				}

			//	should be impossible for current default record to be same as old default record,
			//	but check anyways to be safe
			Assert( NULL == pbOldDefaultRec
				|| (BYTE *)ptdb->PdataDefaultRecord() != pbOldDefaultRec );
			if ( NULL != pbOldDefaultRec
				&& (BYTE *)ptdb->PdataDefaultRecord() != pbOldDefaultRec )
				{
				for ( RECDANGLING * precdangling = pfcbTable->Precdangling();
					;
					precdangling = precdangling->precdanglingNext )
					{
					if ( NULL == precdangling )
						{
						//	not in list, go ahead and free it
						OSMemoryHeapFree( pbOldDefaultRec );
						break;
						}
					else if ( (BYTE *)precdangling == pbOldDefaultRec )
						{
						//	pointer is already in the list, just get out
						AssertTracking();
						break;
						}
					}
				}

			pfcbTable->LeaveDDL();

			break;
			}

		case operDeleteColumn:
			{
			// RCE list ensures FCB is still pinned
			Assert( prce->Pfcb()->PrceOldest() != prceNil );

			prce->Pfcb()->EnterDDL();

			Assert( prce->CbData() == sizeof(COLUMNID) );
			const COLUMNID	columnid		= *( (COLUMNID*)prce->PbData() );
			TDB				* const ptdb	= prce->Pfcb()->Ptdb();
			FIELD			* const pfield	= ptdb->Pfield( columnid );

			// If field was version-added, it would have been cleaned
			// up by now.
			Assert( pfield->coltyp != JET_coltypNil );

			// UNDONE: Don't reset coltyp to Nil, so that we can support
			// column access at level 0.
///					pfield->coltyp = JET_coltypNil;

			//	remove the column name from the TDB name space
			ptdb->MemPool().DeleteEntry( pfield->itagFieldName );

			// Reset version and autoinc fields.
			Assert( !( FFIELDVersion( pfield->ffield )
					 && FFIELDAutoincrement( pfield->ffield ) ) );
			if ( FFIELDVersion( pfield->ffield ) )
				{
				Assert( ptdb->FidVersion() == FidOfColumnid( columnid ) );
				ptdb->ResetFidVersion();
				}
			else if ( FFIELDAutoincrement( pfield->ffield ) )
				{
				Assert( ptdb->FidAutoincrement() == FidOfColumnid( columnid ) );
				ptdb->ResetFidAutoincrement();
				}

			Assert( !FFIELDVersionedAdd( pfield->ffield ) );
			Assert( FFIELDDeleted( pfield->ffield ) );
			FIELDResetVersioned( pfield->ffield );

			prce->Pfcb()->LeaveDDL();

			break;
			}

		case operCreateIndex:
			{
			//	pfcb of secondary index FCB or pfcbNil for primary
			//	index creation
			FCB						* const pfcbT = *(FCB **)prce->PbData();
			FCB						* const pfcbTable = prce->Pfcb();
			FCB						* const pfcbIndex = ( pfcbT == pfcbNil ? pfcbTable : pfcbT );
			IDB						* const pidb = pfcbIndex->Pidb();

			pfcbTable->EnterDDL();

			Assert( pidbNil != pidb );

			Assert( pfcbTable->FTypeTable() );

			if ( pfcbTable == pfcbIndex )
				{
				// VersionedCreate flag is reset at commit time for primary index.
				Assert( !pidb->FVersionedCreate() );
				Assert( !pidb->FDeleted() );
				pidb->ResetFVersioned();
				}
			else if ( pidb->FVersionedCreate() )
				{
				pidb->ResetFVersionedCreate();

				// If deleted, Versioned bit will be properly reset when
				// Delete commits or rolls back.
				if ( !pidb->FDeleted() )
					{
					pidb->ResetFVersioned();
					}
				}

			pfcbTable->LeaveDDL();

			break;
			}

		case operDeleteIndex:
			{
			FCB	* const pfcbIndex	= (*(FCB **)prce->PbData());
			FCB	* const pfcbTable	= prce->Pfcb();

			Assert( pfcbIndex->FDeletePending() );
			Assert( pfcbIndex->FDeleteCommitted() );

			//	wait for all tasks on the FCB to complete
			//	no new tasks should be created because there is a delete on the FCB
			//
			//	UNDONE: we're assuming that the tasks we're waiting
			//	for will be serviced by other threads while this task
			//	thread waits, but is it possible that NT only assigns
			//	one thread to us, meaning the other tasks will never
			//	get serviced and we will therefore end up waiting
			//	forever?
			//
			CallS( pfcbIndex->ErrWaitForTasksToComplete() );

			pfcbTable->SetIndexing();
			pfcbTable->EnterDDL();

			Assert( pfcbTable->FTypeTable() );
			Assert( pfcbIndex->FTypeSecondaryIndex() );
			Assert( pfcbIndex != pfcbTable );
			Assert( pfcbIndex->PfcbTable() == pfcbTable );

			// Use dummy ppib because we lost the original one when the
			// transaction committed.
			Assert( pfcbIndex->FDomainDenyRead( m_ppibRCEClean ) );

			Assert( pfcbIndex->Pidb() != pidbNil );
			Assert( pfcbIndex->Pidb()->CrefCurrentIndex() == 0 );
			Assert( pfcbIndex->Pidb()->FDeleted() );
			Assert( !pfcbIndex->Pidb()->FVersioned() );

			pfcbTable->UnlinkSecondaryIndex( pfcbIndex );

			pfcbTable->LeaveDDL();
			pfcbTable->ResetIndexing();

			if ( pfcbIndex >= PfcbFCBPreferredThreshold( PinstFromIfmp( prce->Ifmp() ) ) )
				{

				//	the index FCB is above the threshold; thus, removing it may
				//	cause the table FCB to move below the threshold; if the
				//	table FCB is in the avail-above list, it must be moved
				//	to the avail-below list

				pfcbTable->UpdateAvailListPosition();
				}

			//	verify not called during recovery, which would be
			//	bad because VERNullifyAllVersionsOnFCB() enters
			//	m_critRCEClean, which we already have
			Assert( !m_pinst->FRecovering() );
			VERNullifyAllVersionsOnFCB( pfcbIndex );
			VERIUnlinkDefunctSecondaryIndex( ppibNil, pfcbIndex );

			//	prepare the FCB to be purged
			//	this removes the FCB from the hash-table among other things
			//		so that the following case cannot happen:
			//			we free the space for this FCB
			//			someone else allocates it
			//			someone else BTOpen's the space
			//			we try to purge the table and find that the refcnt
			//				is not zero and the state of the FCB says it is
			//				currently in use!
			//			result --> CONCURRENCY HOLE

			pfcbIndex->PrepareForPurge( fFalse );

			//	if the parent (ie. the table) is pending deletion, we
			//	don't need to bother freeing the index space because
			//	it will be freed when the parent is freed
			//	Note that the DeleteCommitted flag is only ever set
			//	when the delete is guaranteed to be committed.  The
			//	flag NEVER gets reset, so there's no need to grab
			//	the FCB critical section to check it.

			if ( !pfcbTable->FDeleteCommitted() )
				{
				//	ignore errors if we can't free the space
				//	(it will be leaked)
				//
				(VOID)ErrSPFreeFDP(
							m_ppibRCEClean,
							pfcbIndex,
							pfcbTable->PgnoFDP() );
				}

			//	purge the FCB

			pfcbIndex->Purge();

			break;
			}

		case operDeleteTable:
			{
			INT			fState;
			const IFMP	ifmp				= prce->Ifmp();
			const PGNO	pgnoFDPTable		= *(PGNO*)prce->PbData();
			FCB			* const	pfcbTable	= FCB::PfcbFCBGet(
													ifmp,
													pgnoFDPTable,
													&fState );
			Assert( pfcbNil != pfcbTable );
			Assert( pfcbTable->FTypeTable() || pfcbTable->FTypeSentinel() );
			Assert( fFCBStateInitialized == fState
					|| fFCBStateSentinel == fState );

			//	verify VERNullifyAllVersionsOnFCB() not called during recovery,
			//	which would be bad because VERNullifyAllVersionsOnFCB() enters
			//	m_critRCEClean, which we already have
			Assert( !m_pinst->FRecovering() );

			// Remove all associated FCB's from hash table, so they will
			// be available for another file using the FDP that is about
			// about to be freed.
			for ( FCB *pfcbT = pfcbTable; pfcbT != pfcbNil; pfcbT = pfcbT->PfcbNextIndex() )
				{
				Assert( pfcbT->FDeletePending() );
				Assert( pfcbT->FDeleteCommitted() );

				//	wait for all tasks on the FCB to complete
				//	no new tasks should be created because there is a delete on the FCB
				//
				//	UNDONE: we're assuming that the tasks we're waiting
				//	for will be serviced by other threads while this task
				//	thread waits, but is it possible that NT only assigns
				//	one thread to us, meaning the other tasks will never
				//	get serviced and we will therefore end up waiting
				//	forever?
				//
				CallS( pfcbT->ErrWaitForTasksToComplete() );

				// bugfix (#45382): May have outstanding moved RCE's
				Assert( pfcbT->PrceOldest() == prceNil
					|| ( pfcbT->PrceOldest()->Oper() == operFlagDelete
						&& pfcbT->PrceOldest()->FMoved() ) );
				VERNullifyAllVersionsOnFCB( pfcbT );

				pfcbT->PrepareForPurge( fFalse );
				}

			if ( pfcbTable->Ptdb() != ptdbNil )
				{
				Assert( fFCBStateInitialized == fState );
				FCB	* const pfcbLV = pfcbTable->Ptdb()->PfcbLV();
				if ( pfcbNil != pfcbLV )
					{
					Assert( pfcbLV->FDeletePending() );
					Assert( pfcbLV->FDeleteCommitted() );

					//	wait for all tasks on the FCB to complete
					//	no new tasks should be created because there is a delete on the FCB
					//
					//	UNDONE: we're assuming that the tasks we're waiting
					//	for will be serviced by other threads while this task
					//	thread waits, but is it possible that NT only assigns
					//	one thread to us, meaning the other tasks will never
					//	get serviced and we will therefore end up waiting
					//	forever?
					//
					CallS( pfcbLV->ErrWaitForTasksToComplete() );

					// bugfix (#36315): processing of delta RCEs may have created flagDelete
					// RCEs after this RCE.
					Assert( pfcbLV->PrceOldest() == prceNil || pfcbLV->PrceOldest()->Oper() == operFlagDelete );
					VERNullifyAllVersionsOnFCB( pfcbLV );

					pfcbLV->PrepareForPurge( fFalse );
					}
				}
			else
				{
				Assert( fFCBStateSentinel == fState );
				}

			//	free table FDP (which implicitly frees child FDP's)
			//
			//	ignore errors if we can't free the space
			//	(it will be leaked)
			//
			(VOID)ErrSPFreeFDP(
						m_ppibRCEClean,
						pfcbTable,
						pgnoSystemRoot );

			if ( fFCBStateInitialized == fState )
				{
				pfcbTable->Release();

				Assert( pfcbTable->PgnoFDP() == pgnoFDPTable );
				Assert( pfcbTable->FDeletePending() );
				Assert( pfcbTable->FDeleteCommitted() );

				// All transactions which were able to access this table
				// must have committed and been cleaned up by now.
				Assert( pfcbTable->PrceOldest() == prceNil );
				Assert( pfcbTable->PrceNewest() == prceNil );
				}
			else
				{
				Assert( fFCBStateSentinel == fState );
				}
			pfcbTable->Purge();

			break;
			}

		case operRegisterCallback:
			{
			//  the callback is now visible to all transactions
			//  CONSIDER: unset the fVersioned flag if the callback has not been unregistered
			}
			break;

		case operUnregisterCallback:
			{
			//  the callback cannot be seen by any transaction. remove the callback from the list
			VERIRemoveCallback( prce );
			}
			break;

		case operFlagDelete:
			{
#ifdef MOVEABLE_RCE
			const BOOL	fMoveRCE	= ( FVERICleanWithoutIO()
										&& !rgfmp[prce->Ifmp()].FDetachingDB()	// if TRUE, it means we called RCE clean to detach one DB, so we MUST nullify all RCE's
										&& !prce->FMoved() );					//  if we have already moved it, try to delete
#else
			const BOOL	fMoveRCE	= fFalse;
#endif

			if ( fMoveRCE )
				{
#ifdef MOVEABLE_RCE
				//  UNDONE: try to perform the delete without waiting for latches or IO

				err = ErrVERIMoveRCE( prce );
				CallSx( err, wrnVERRCEMoved );
#else
				Assert( fFalse );
#endif
				}

			else if ( FVERICleanDiscardDeletes() )
				{
				IncrementCCleanupDiscarded( prce );
				err = JET_errSuccess;
				}

			else if ( !m_pinst->FRecovering() )
				{
#ifdef VERPERF
#ifdef MOVEABLE_RCE
				if ( prce->FMoved() )
					{
					++m_crceMovedDeleted;
					}
#endif
#endif

				//	don't bother cleaning if there are future versions
				if ( !prce->FFutureVersionsOfNode() )
					{
					err = ErrVERIDelete( m_ppibRCEClean, prce );
					}
				}

			break;
			}

		case operDelta:
			//  we may have to defer delete a LV
			if ( FVERICleanDiscardDeletes() )
				{
				IncrementCCleanupDiscarded( prce );
				err = JET_errSuccess;
				}
			else if ( !m_pinst->FRecovering() )
				{
				err = ErrVERICleanDeltaRCE( prce );
				}
			break;

		case operSLVSpace:
			if ( !m_pinst->FRecovering() )
				{
				err = ErrVERICleanSLVSpaceRCE( prce );
				}
			break;

		default:
			break;
		}

	return err;
	}


//  ================================================================
ERR RCE::ErrPrepareToDeallocate( TRX trxOldest )
//  ================================================================
//
// Called by RCEClean to clean/nullify RCE before deallocation.
//
	{
	ERR			err		= JET_errSuccess;
	const OPER	oper 	= m_oper;
	const UINT	uiHash 	= m_uiHash;

	Assert( PinstFromIfmp( m_ifmp )->m_pver->m_critRCEClean.FOwner() );

#ifdef DEBUG
	const TRX	trxDBGOldest = TrxOldest( PinstFromIfmp( m_ifmp ) );
	Assert( TrxCommitted() != trxMax );
	Assert( TrxCmp( trxDBGOldest, trxOldest ) >= 0 || trxMax == trxOldest );
	Assert( dbidTemp != rgfmp[ m_ifmp ].Dbid() );
#endif

	VER *pver = PverFromIfmp( m_ifmp );
	CallR( pver->ErrVERICleanOneRCE( this ) );

	const BOOL	fInHash = ::FOperInHashTable( oper );
	ENTERCRITICALSECTION enterCritHash(
							fInHash ? &( PverFromIfmp( Ifmp() )->CritRCEChain( uiHash ) ) : NULL,
							fInHash );

	if ( FOperNull() || wrnVERRCEMoved == err )
		{
		//  RCE may have been nullified by VERNullifyAllVersionsOnFCB()
		//  (which is called while closing the temp table).
		//  or RCE may have been moved instead of being cleaned up
		}
	else
		{
		Assert( !FRolledBack() );
		Assert( !FOperNull() );

		//	Clean up the RCE. Also clean up any RCE of the same list to reduce
		//	Enter/leave critical section calls.

		RCE *prce = this;

		FCB * const pfcb = prce->Pfcb();
		ENTERCRITICALSECTION enterCritFCBRCEList( &( pfcb->CritRCEList() ) );

		do
			{
			RCE *prceNext;
			if ( prce->FOperInHashTable() )
				prceNext = prce->PrceNextOfNode();
			else
				prceNext = prceNil;

			ASSERT_VALID( prce );
			Assert( !prce->FOperNull() );
			VERINullifyCommittedRCE( prce );

			prce = prceNext;
			} while (
				prce != prceNil &&
				TrxCmp( prce->TrxCommitted(), trxOldest ) < 0 &&
				operFlagDelete != prce->Oper() &&		// Let RCE clean do the nullify for delete.
				operSLVSpace != prce->Oper() &&			// May need to move space from deleted to free
				operDelta != prce->Oper() );			// Delta is used to indicate if
														// it needs to do LV delete.
		}

	return JET_errSuccess;
	}


//  ================================================================
ERR VER::ErrVERRCEClean( const IFMP ifmp )
//  ================================================================
//	critical section wrapper
{
	//	clean PIB in critical section held across IO operations

	ENTERCRITICALSECTION	enterCritRCEClean( &m_critRCEClean );

	return ErrVERIRCEClean( ifmp );
}
//  ================================================================
ERR VER::ErrVERIRCEClean( const IFMP ifmp )
//  ================================================================
//
//	Cleans RCEs in bucket chain.
//	We only clean up the RCEs that has a commit timestamp older
//	that the oldest XactBegin of any user.
//
//-
	{
	Assert( m_critRCEClean.FOwner() );

	const BOOL	fCleanOneDb = ( ifmp != ifmpMax );

	// Only time we do per-DB RCE Clean is just before we want to
	// detach the database.
	Assert( !fCleanOneDb || rgfmp[ifmp].FDetachingDB() );

	// keep the original value
	const BOOL fSyncronousTasks = m_fSyncronousTasks;

	//  override the default if we are cleaning one database
	m_fSyncronousTasks = fCleanOneDb ? fTrue : m_fSyncronousTasks;

#ifdef DEBUG
	Ptls()->fIsRCECleanup = fTrue;
#endif	//	DEBUG

#ifdef VERPERF
	//	UNDONE:	why do these get reset every RCEClean??
	m_cbucketCleaned	= 0;
	m_cbucketSeen		= 0;
	m_crceSeen		= 0;
	m_crceCleaned		= 0;
	qwStartTicks	= QwUtilHRTCount();
	m_crceFlagDelete	= 0;
	m_crceDeleteLV	= 0;
#ifdef MOVEABLE_RCE
	m_crceMoved		= 0;
	m_crceMovedDeleted= 0;
#endif
#endif	//	VERPERF

	ERR     	err		= JET_errSuccess;
	BUCKET *	pbucket;

	//	get oldest bucket and clean RCEs from oldest to youngest

	m_critBucketGlobal.Enter();

#ifdef MOVEABLE_RCE
	if ( FVERICleanWithoutIO()
		&& !FVERICleanDiscardDeletes()
		&& pbucketNil != m_pbucketGlobalLastDelete
		&& !fCleanOneDb )
		{
		pbucket = m_pbucketGlobalLastDelete;
		}
	else
		{
		pbucket = PbucketVERIGetOldest();
		}
#else
	pbucket = PbucketVERIGetOldest();
#endif

	m_critBucketGlobal.Leave();

	TRX trxOldest = TrxOldest( m_pinst );
	//	loop through buckets, oldest to newest. stop when we run out of buckets
	//	or find an uncleanable RCE

	while ( pbucketNil != pbucket )
		{
#ifdef VERPERF
		INT	crceInBucketSeen 	= 0;
		INT	crceInBucketCleaned = 0;
#endif	//	VERPERF

		//	Check if need to get RCE within m_critBucketGlobal

		BOOL	fNeedBeInCritBucketGlobal	= ( pbucket->hdr.pbucketNext == pbucketNil );

		//	Only clean can change prceOldest and only one clean thread is active.

		Assert( m_critRCEClean.FOwner() );
		RCE *	prce						= pbucket->hdr.prceOldest;
		BOOL	fSkippedRCEInBucket			= fFalse;
;
#ifdef MOVEABLE_RCE
		if ( pbucket->rgb != pbucket->hdr.pbLastDelete )
			{
			if ( !FVERICleanWithoutIO() || FVERICleanDiscardDeletes() || fCleanOneDb )
				{
				//  there are moved FlagDelete RCEs in this bucket. start with them
				//  we will either try to clean them or will discard them
				prce = reinterpret_cast<RCE *>( pbucket->rgb );
				}
			else
				{
				fSkippedRCEInBucket = fTrue;
				Assert( pbucket->hdr.prceOldest == prce );
				}
			}
		else
			{
			Assert( !fSkippedRCEInBucket );
			Assert( pbucket->hdr.prceOldest == prce );
			}
#endif

		forever
			{
#ifdef VERPERF
			++m_crceSeen;
			++crceInBucketSeen;
#endif

			//	verify RCE is within the bucket
			Assert( (BYTE *)prce >= pbucket->rgb );
			Assert( (BYTE *)prce < (BYTE *)( pbucket + 1 )
				|| ( (BYTE *)prce == (BYTE *)( pbucket + 1 )
					&& prce == pbucket->hdr.prceNextNew ) );

			if ( fNeedBeInCritBucketGlobal )
				m_critBucketGlobal.Enter();

#ifdef MOVEABLE_RCE
			const BOOL	fRecalcOldestRCE	= ( !fSkippedRCEInBucket
												&& reinterpret_cast<BYTE *>( prce ) >= pbucket->hdr.pbLastDelete );
#else
			const BOOL	fRecalcOldestRCE	= ( !fSkippedRCEInBucket );
#endif

			if ( fRecalcOldestRCE )
				{
				pbucket->hdr.prceOldest = prce;
				}

			Assert( pbucket->rgb <= pbucket->hdr.pbLastDelete );
			Assert( pbucket->hdr.pbLastDelete <= reinterpret_cast<BYTE *>( pbucket->hdr.prceOldest ) );
			Assert( pbucket->hdr.prceOldest <= pbucket->hdr.prceNextNew );

			//	break to release the bucket
			if ( pbucket->hdr.prceNextNew == prce )
				break;

			if ( fNeedBeInCritBucketGlobal )
				m_critBucketGlobal.Leave();

			//	Save the size for use later
			const INT	cbRce = prce->CbRce();

			//	verify RCE is within the bucket
			Assert( cbRce > 0 );
			Assert( (BYTE *)prce >= pbucket->rgb );
			Assert( (BYTE *)prce < (BYTE *)( pbucket + 1 ) );
			Assert( (BYTE *)prce + cbRce <= (BYTE *)( pbucket + 1 ) );

			if ( !prce->FOperNull() )
				{
#ifdef DEBUG
				const TRX	trxDBGOldest = TrxOldest( m_pinst );
#endif
				const TRX	trxRCECommitted = prce->TrxCommitted();
				if ( trxMax == trxOldest && !m_pinst->m_plog->m_fRecovering )  // trxOldest is always trxMax when recovering
					{
					//  trxOldest may no longer be trxMax. if so we may not be able to
					//  clean up this RCE after all. we retrieve the trxCommitted of the rce
					//  first to avoid a race condition
					trxOldest = TrxOldest( m_pinst );
					}

				if ( trxRCECommitted == trxMax
					|| TrxCmp( trxRCECommitted, trxOldest ) >= 0 )
					{
					if ( fCleanOneDb )
						{
						Assert( dbidTemp != rgfmp[ ifmp ].Dbid() );
						if ( prce->Ifmp() == ifmp )
							{
							if ( prce->TrxCommitted() == trxMax )
								{
								//	this can be caused by a task that is active
								//	stop here as we have cleaned up what we can
								err = ErrERRCheck( JET_wrnRemainingVersions );
								goto HandleError;
								}
							else
								{
								// Fall through and clean the RCE.
								}
							}
						else
							{
							// Skip uncleanable RCE's that don't belong to
							// the db we're trying to clean.
							fSkippedRCEInBucket = fTrue;
							goto NextRCE;
							}
						}
					else
						{
						Assert( pbucketNil != pbucket );
						Assert( !prce->FMoved() );
						err = ErrERRCheck( JET_wrnRemainingVersions );
						goto HandleError;
						}
					}

				Assert( prce->TrxCommitted() != trxMax );
				Assert( TrxCmp( prce->TrxCommitted(), trxDBGOldest ) < 0
						|| fCleanOneDb
						|| TrxCmp( prce->TrxCommitted(), trxOldest ) < 0 );

#ifdef VERPERF
				if ( operFlagDelete == prce->Oper() )
					{
					++m_crceFlagDelete;
					}
				else if ( operDelta == prce->Oper() )
					{
					const VERDELTA* const pverdelta = reinterpret_cast<VERDELTA*>( prce->PbData() );
					if ( pverdelta->fDeferredDelete )
						{
						++m_crceDeleteLV;
						}
					}
#endif	//	DEBUG

				Call( prce->ErrPrepareToDeallocate( trxOldest ) );

#ifdef VERPERF
				++crceInBucketCleaned;
				++m_crceCleaned;
#endif	//	VERPERF
				}

			//	Set the oldest to next prce entry in the bucket
			//	Rce clean thread ( run within m_critRCEClean ) is
			//	the only one touch prceOldest
NextRCE:
			Assert( m_critRCEClean.FOwner() );


			const BYTE	*pbRce		= reinterpret_cast<BYTE *>( prce );
			const BYTE	*pbNextRce	= reinterpret_cast<BYTE *>( PvAlignForAllPlatforms( pbRce + cbRce ) );

#ifdef MOVEABLE_RCE
			//	verify we don't straddle pbLastDelete
			const BYTE	*pbLastDelete = reinterpret_cast<BYTE *>( pbucket->hdr.pbLastDelete );
			if ( pbRce < pbLastDelete )
				{
				Assert( pbNextRce <= pbLastDelete );
				}
			else
				{
				Assert( pbNextRce > pbLastDelete );
				}

			if ( pbNextRce != pbucket->hdr.pbLastDelete )
				{
				prce = (RCE *)pbNextRce;
				}
			else
				{
				//  we just looked at the last moved RCE. go to the first old RCE
				Assert( prce->FMoved() );
				prce = pbucket->hdr.prceOldest;
				--m_cbucketGlobalAllocDelete;
				if ( m_cbucketGlobalAllocDelete < 0 )
					{
					m_cbucketGlobalAllocDelete = 0;
					}
#ifdef VERPERF
				cVERcbucketDeleteAllocated.Dec( m_pinst );
#endif // VERPERF
				pbucket->hdr.pbLastDelete = pbucket->rgb;
				}
#else
			prce = (RCE *)pbNextRce;
#endif	//	MOVEABLE_RCE

			//	verify RCE is within the bucket
			Assert( (BYTE *)prce >= pbucket->rgb );
			Assert( (BYTE *)prce < (BYTE *)( pbucket + 1 )
				|| ( (BYTE *)prce == (BYTE *)( pbucket + 1 )
					&& prce == pbucket->hdr.prceNextNew ) );

			Assert( pbucket->hdr.prceOldest <= pbucket->hdr.prceNextNew );
			}

		//	all RCEs in bucket cleaned.  Now get next bucket and free
		//	cleaned bucket.

		if ( fNeedBeInCritBucketGlobal )
			Assert( m_critBucketGlobal.FOwner() );
		else
			m_critBucketGlobal.Enter();

#ifdef VERPERF
		++m_cbucketSeen;
#endif

#ifdef MOVEABLE_RCE
		const BOOL	fRemainingRCEs	= ( fSkippedRCEInBucket || pbucket->rgb != pbucket->hdr.pbLastDelete );
#else
		const BOOL	fRemainingRCEs	= ( fSkippedRCEInBucket );
#endif

		if ( fRemainingRCEs )
			{
			pbucket = pbucket->hdr.pbucketNext;
			}
		else
			{
			Assert( pbucket->rgb == pbucket->hdr.pbLastDelete );
			pbucket = PbucketVERIFreeAndGetNextOldestBucket( pbucket );

#ifdef VERPERF
			++m_cbucketCleaned;
#endif
			}

		m_critBucketGlobal.Leave();
		}

	//	stop as soon as find RCE commit time younger than oldest
	//	transaction.  If bucket left then set ibOldestRCE and
	//	unlink back offset of last remaining RCE.
	//	If no error then set warning code if some buckets could
	//	not be cleaned.

	Assert( pbucketNil == pbucket );
	err = JET_errSuccess;

	// If only cleaning one db, we don't clean buckets because
	// there may be outstanding versions on other databases.
	if ( !fCleanOneDb )
		{
		m_critBucketGlobal.Enter();
		if ( pbucketNil != m_pbucketGlobalHead )
			{
			//	return warning if remaining versions
			Assert( pbucketNil != m_pbucketGlobalTail );
			err = ErrERRCheck( JET_wrnRemainingVersions );
			}
		else
			{
			Assert( pbucketNil == m_pbucketGlobalTail );
			}
		m_critBucketGlobal.Leave();
		}


HandleError:

#ifdef DEBUG_VER_EXPENSIVE
	if ( !m_pinst->m_plog->m_fRecovering )
		{
		double	dblElapsedTime 	= DblUtilHRTElapsedTime( qwStartTicks );
		CHAR	szBuf[512];

		sprintf( szBuf, "RCEClean: "
						"elapsed time %10.10f seconds, "
						"saw %6.6d RCEs, "
						"( %6.6d flagDelete, "
						"%6.6d deleteLV ), "
						"cleaned %6.6d RCEs, "
#ifdef MOVEABLE_RCE
						"moved %6.6d RCEs, "
						"cleaned %6.6d previously moved RCEs, "
#endif
						"cleaned %4.4d buckets, "
						"%4.4d buckets left",
						dblElapsedTime,
						m_crceSeen,
						m_crceFlagDelete,
						m_crceDeleteLV,
						m_crceCleaned,
#ifdef MOVEABLE_RCE
						m_crceMoved,
						m_crceMovedDeleted,
#endif
						m_cbucketCleaned,
						m_cbucketGlobalAlloc
					);
		(VOID)m_pinst->m_plog->ErrLGTrace( ppibNil, szBuf );
		}
#endif	//	DEBUG_VER_EXPENSIVE

#ifdef DEBUG
	Ptls()->fIsRCECleanup = fFalse;
#endif	//	DEBUG

	// restore the original value
	m_fSyncronousTasks = fSyncronousTasks;

	if ( !fCleanOneDb )
		{
		//	record when we performed this pass of version cleanup
		//
		m_tickLastRCEClean = TickOSTimeCurrent();
		}

	return err;
	}


//  ================================================================
VOID VERICommitRegisterCallback( const RCE * const prce, const TRX trxCommit0 )
//  ================================================================
//
//  Set the trxRegisterCommit0 in the CBDESC
//
//-
	{
#ifdef VERSIONED_CALLBACKS
	Assert( prce->CbData() == sizeof(VERCALLBACK) );
	const VERCALLBACK* const pvercallback = (VERCALLBACK*)prce->PbData();
	CBDESC * const pcbdesc = pvercallback->pcbdesc;
	prce->Pfcb()->EnterDDL();
	Assert( trxMax != pcbdesc->trxRegisterBegin0 );
	Assert( trxMax == pcbdesc->trxRegisterCommit0 );
	Assert( trxMax == pcbdesc->trxUnregisterBegin0 );
	Assert( trxMax == pcbdesc->trxUnregisterCommit0 );
	pvercallback->pcbdesc->trxRegisterCommit0 = trxCommit0;
	prce->Pfcb()->LeaveDDL();
#endif	//	VERSIONED_CALLBACKS
	}


//  ================================================================
VOID VERICommitUnregisterCallback( const RCE * const prce, const TRX trxCommit0 )
//  ================================================================
//
//  Set the trxUnregisterCommit0 in the CBDESC
//
//-
	{
#ifdef VERSIONED_CALLBACKS
	Assert( prce->CbData() == sizeof(VERCALLBACK) );
	const VERCALLBACK* const pvercallback = (VERCALLBACK*)prce->PbData();
	CBDESC * const pcbdesc = pvercallback->pcbdesc;
	prce->Pfcb()->EnterDDL();
	Assert( trxMax != pcbdesc->trxRegisterBegin0 );
	Assert( trxMax != pcbdesc->trxRegisterCommit0 );
	Assert( trxMax != pcbdesc->trxUnregisterBegin0 );
	Assert( trxMax == pcbdesc->trxUnregisterCommit0 );
	pvercallback->pcbdesc->trxUnregisterCommit0 = trxCommit0;
	prce->Pfcb()->LeaveDDL();
#endif	//	VERSIONED_CALLBACKS
	}


//  ================================================================
LOCAL VOID VERICommitTransactionToLevelGreaterThan0( const PIB * const ppib )
//  ================================================================
	{
	const LEVEL	level 	= ppib->level;
	Assert( level > 1 );

	//  we do not need to lock the RCEs as other transactions do not care about the level,
	//  only that they are uncommitted

	RCE 		*prce	= ppib->prceNewest;
	for ( ; prceNil != prce && prce->Level() == level; prce = prce->PrcePrevOfSession() )
		{
		Assert( prce->TrxCommitted() == trxMax );

		prce->SetLevel( LEVEL( level - 1 ) );
		}
	}


RCE * PIB::PrceOldest()
	{
	RCE		* prcePrev	= prceNil;

	for ( RCE * prceCurr = prceNewest;
		prceNil != prceCurr;
		prceCurr = prceCurr->PrcePrevOfSession() )
		{
		prcePrev = prceCurr;
		}

	return prcePrev;
	}

INLINE VOID VERICommitOneRCEToLevel0( PIB * const ppib, RCE * const prce )
	{
	Assert( !prce->FFullyCommitted() );

	prce->SetLevel( 0 );

	VERIDeleteRCEFromSessionList( ppib, prce );

	prce->Pfcb()->CritRCEList().Enter();
	prce->SetTrxCommitted( ppib->trxCommit0 );
	prce->Pfcb()->CritRCEList().Leave();
	}

//  ================================================================
LOCAL VOID VERICommitTransactionToLevel0( PIB * const ppib )
//  ================================================================
	{
	RCE 		* prce				= ppib->prceNewest;

	Assert( 1 == ppib->level );

	//  because of some optimizations at the DIR/LOG level we cannot always assert this
///	Assert( TrxCmp( ppib->trxCommit0, ppib->trxBegin0 ) > 0 );

	RCE * prceNextToCommit;
#ifdef DEBUG
	prceNextToCommit = prceInvalid;
#endif	//	DEBUG
	for ( ; prceNil != prce; prce = prceNextToCommit )
		{
		prceNextToCommit 	= prce->PrcePrevOfSession();

		ASSERT_VALID( prce );
		Assert( !prce->FOperNull() );
		Assert( !prce->FFullyCommitted() );
		Assert( 1 == prce->Level() || PinstFromPpib( ppib )->m_plog->m_fRecovering );
		Assert( prce->Pfucb()->ppib == ppib );

		Assert( prceInvalid != prce );

		if ( rgfmp[ prce->Ifmp() ].Dbid() == dbidTemp )
			{
			//  RCEs for temp tables are used only for rollback.
			//   - The table is not shared so there is no risk of write conflicts
			//   - The table is not recovered so no undo-info is needed
			//   - No committed RCEs exist on temp tables so RCEClean will never access the FCB
			//   - No concurrent-create-index on temp tables
			//  Thus, we simply nullify the RCE so that the FCB can be freed
			//
			//  UNDONE: RCEs for temp tables should not be inserted into the FCB list or hash table

			// DDL currently not supported on temp tables.
			Assert( !prce->FOperNull() );
			Assert( !prce->FOperDDL() || operCreateTable == prce->Oper() );
			Assert( !PinstFromPpib( ppib )->m_plog->m_fRecovering );
			Assert( !prce->FFullyCommitted() );
			Assert( prce->PgnoUndoInfo() == pgnoNull );	//	no logging on temp tables

			const BOOL fInHash = prce->FOperInHashTable();

			ENTERCRITICALSECTION enterCritHash( fInHash ? &( PverFromPpib( ppib )->CritRCEChain( prce->UiHash() ) ) : 0, fInHash );
			ENTERCRITICALSECTION enterCritFCBRCEList( &prce->Pfcb()->CritRCEList() );

			VERIDeleteRCEFromSessionList( ppib, prce );

			prce->SetLevel( 0 );
			prce->SetTrxCommitted( ppib->trxCommit0 );

			VERIDeleteRCEFromFCBList( prce->Pfcb(), prce );
			if ( fInHash )
				{
				VERIDeleteRCEFromHash( prce );
				}
			prce->NullifyOper();

			continue;
			}

		//	Remove UndoInfo dependency if committing to level 0

		if ( prce->PgnoUndoInfo() != pgnoNull )
			BFRemoveUndoInfo( prce );

		ENTERCRITICALSECTION enterCritHash(
			prce->FOperInHashTable() ? &( PverFromPpib( ppib )->CritRCEChain( prce->UiHash() ) ) : 0,
			prce->FOperInHashTable()
			);

		//	if version for DDL operation then reset deny DDL
		//	and perform special handling
		if ( prce->FOperDDL() )
			{
			switch( prce->Oper() )
				{
				case operAddColumn:
					// RCE list ensures FCB is still pinned
					Assert( prce->Pfcb()->PrceOldest() != prceNil );
					Assert( prce->CbData() == sizeof(VERADDCOLUMN) );
					break;

				case operDeleteColumn:
					// RCE list ensures FCB is still pinned
					Assert( prce->Pfcb()->PrceOldest() != prceNil );
					Assert( prce->CbData() == sizeof(COLUMNID) );
					break;

				case operCreateIndex:
					{
					const FCB	* const pfcbT		= *(FCB **)prce->PbData();
					if ( pfcbNil == pfcbT )
						{
						FCB		* const pfcbTable	= prce->Pfcb();

						pfcbTable->EnterDDL();
						Assert( pfcbTable->FPrimaryIndex() );
						Assert( pfcbTable->FTypeTable() );
						Assert( pfcbTable->Pidb() != pidbNil );
						Assert( pfcbTable->Pidb()->FPrimary() );

						// For primary index, must reset VersionedCreate()
						// flag at commit time so updates can occur
						// immediately once the primary index has been
						// committed (see ErrSetUpdatingAndEnterDML()).
						pfcbTable->Pidb()->ResetFVersionedCreate();

						pfcbTable->LeaveDDL();
						}
					else
						{
						Assert( pfcbT->FTypeSecondaryIndex() );
						Assert( pfcbT->Pidb() != pidbNil );
						Assert( !pfcbT->Pidb()->FPrimary() );
						Assert( pfcbT->PfcbTable() != pfcbNil );
						}
					break;
					}

				case operCreateLV:
					{
					//  no further action is required
					break;
					}

				case operDeleteIndex:
					{
					FCB * const pfcbIndex = (*(FCB **)prce->PbData());
					FCB * const pfcbTable = prce->Pfcb();

					Assert( pfcbTable->FTypeTable() );
					Assert( pfcbIndex->FDeletePending() );
					Assert( !pfcbIndex->FDeleteCommitted() );
					Assert( pfcbIndex->FTypeSecondaryIndex() );
					Assert( pfcbIndex != pfcbTable );
					Assert( pfcbIndex->PfcbTable() == pfcbTable );
					Assert( pfcbIndex->FDomainDenyReadByUs( prce->Pfucb()->ppib ) );

					pfcbTable->EnterDDL();

					//  free in-memory structure

					Assert( pfcbIndex->Pidb() != pidbNil );
					Assert( pfcbIndex->Pidb()->CrefCurrentIndex() == 0 );
					Assert( pfcbIndex->Pidb()->FDeleted() );
					pfcbIndex->Pidb()->ResetFVersioned();
					pfcbIndex->SetDeleteCommitted();

					//	update all index mask
					FILESetAllIndexMask( pfcbTable );

					pfcbTable->LeaveDDL();
					break;
					}

				case operDeleteTable:
					{
					FCB		*pfcbTable;
					INT		fState;

					//	pfcb should be found, even if it's a sentinel
					pfcbTable = FCB::PfcbFCBGet( prce->Ifmp(), *(PGNO*)prce->PbData(), &fState );
					Assert( pfcbTable != pfcbNil );
					Assert( pfcbTable->FTypeTable() || pfcbTable->FTypeSentinel() );
					Assert( fFCBStateInitialized == fState
						|| fFCBStateSentinel == fState );

					if ( pfcbTable->Ptdb() != ptdbNil )
						{
						Assert( fFCBStateInitialized == fState );

						pfcbTable->EnterDDL();

						// Nothing left to prevent access to this FCB except
						// for DeletePending.
						Assert( !pfcbTable->FDeleteCommitted() );
						for ( FCB *pfcbT = pfcbTable; pfcbT != pfcbNil; pfcbT = pfcbT->PfcbNextIndex() )
							{
							Assert( pfcbT->FDeletePending() );
							pfcbT->SetDeleteCommitted();
							}

						FCB	* const pfcbLV = pfcbTable->Ptdb()->PfcbLV();
						if ( pfcbNil != pfcbLV )
							{
							Assert( pfcbLV->FDeletePending() );
							pfcbLV->SetDeleteCommitted();
							}

						pfcbTable->LeaveDDL();

						// If regular FCB, decrement refcnt
						pfcbTable->Release();
						}
					else
						{
						Assert( fFCBStateSentinel == fState );
						Assert( pfcbTable->PfcbNextIndex() == pfcbNil );
						pfcbTable->SetDeleteCommitted();
						}

					break;
					}

				case operCreateTable:
					break;

				case operRegisterCallback:
					VERICommitRegisterCallback( prce, ppib->trxCommit0 );
					break;

				case operUnregisterCallback:
					VERICommitUnregisterCallback( prce, ppib->trxCommit0 );
					break;

				default:
					Assert( fFalse );
					break;
				}
			}
#ifdef DEBUG
		else
			{
			//	the deferred before image chain of the prce should have been
			//	cleaned up in the beginning of this while block
			const PGNO	pgnoUndoInfo = prce->PgnoUndoInfo();
			Assert( pgnoNull == pgnoUndoInfo );
			}
#endif	//	DEBUG

		//  set level and trxCommitted
		VERICommitOneRCEToLevel0( ppib, prce );
		}	//	WHILE

	Assert( prceNil == ppib->prceNewest );

	//	UNDONE: prceNewest should already be prceNil
	Assert( prceNil == ppib->prceNewest );
	PIBSetPrceNewest( ppib, prceNil );
	}


//  ================================================================
VOID VERCommitTransaction( PIB * const ppib )
//  ================================================================
//
//  OPTIMIZATION:	combine delta/readLock/replace versions
//					remove redundant replace versions
//
//-
	{
	ASSERT_VALID( ppib );

	const LEVEL				level = ppib->level;

	//	must be in a transaction in order to commit
	Assert( level > 0 );
	Assert( PinstFromPpib( ppib )->m_plog->m_fRecovering || trxMax != TrxOldest( PinstFromPpib( ppib ) ) );
	Assert( PinstFromPpib( ppib )->m_plog->m_fRecovering || TrxCmp( ppib->trxBegin0, TrxOldest( PinstFromPpib( ppib ) ) ) >= 0 );

	//	handle commit to intermediate transaction level and
	//	commit to transaction level 0 differently.
	if ( level > 1 )
		{
		VERICommitTransactionToLevelGreaterThan0( ppib );
		}
	else
		{
		VERICommitTransactionToLevel0( ppib );
		}

	Assert( ppib->level > 0 );
	--ppib->level;
	}


//  ================================================================
LOCAL ERR ErrVERILogUndoInfo( RCE *prce, CSR* pcsr )
//  ================================================================
//
//	log undo information [if not in redo phase]
//	remove rce from before-image chain
//
	{
	ERR		err				= JET_errSuccess;
	LGPOS	lgpos			= lgposMin;
	LOG		* const plog	= PinstFromIfmp( prce->Ifmp() )->m_plog;

	Assert( pcsr->FLatched() );

	if ( plog->m_fRecoveringMode != fRecoveringRedo )
		{
		CallR( ErrLGUndoInfo( prce, &lgpos ) );
		}

	//	remove RCE from deferred BI chain
	//
	BFRemoveUndoInfo( prce, lgpos );

	return err;
	}



//  ================================================================
ERR ErrVERIUndoReplacePhysical( RCE * const prce, CSR *pcsr, const BOOKMARK& bm )
//  ================================================================
	{
	ERR		err;
	DATA  	data;
	FUCB	* const pfucb	= prce->Pfucb();
	LOG		*plog = PinstFromIfmp( prce->Ifmp() )->m_plog;

	if ( prce->PgnoUndoInfo() != pgnoNull )
		{
		CallR( ErrVERILogUndoInfo( prce, pcsr ) );
		}

	//	dirty page and log operation
	//
	CallR( ErrLGUndo( prce, pcsr, fMustDirtyCSR ) );

 	//	replace should not fail since splits are avoided at undo
	//	time via deferred page space release.  This refers to space
	//	within a page and not pages freed when indexes and tables
	//	are deleted
	//
	Assert( prce->FOperReplace() );

	data.SetPv( prce->PbData() + cbReplaceRCEOverhead );
	data.SetCb( prce->CbData() - cbReplaceRCEOverhead );

	const VERREPLACE* const pverreplace = (VERREPLACE*)prce->PbData();
	const INT cbDelta = pverreplace->cbDelta;

	//  if we are recovering we don't need to track the cbUncommitted free
	//  (the version store will be empty at the end of recovery)
	if ( cbDelta > 0 )
		{
		//	Rolling back replace that shrunk the node.  To satisfy the rollback,
		//	we will consume the reserved node space, but first we must remove this
		//	reserved node space from the uncommitted freed count so that BTReplace()
		//	can see it.
		//	(This complements the call to AddUncommittedFreed() in SetCbAdjust()).
		pcsr->Cpage().ReclaimUncommittedFreed( cbDelta );
		}

	//  ND expects that fucb will contain bookmark of replaced node
	//
	if ( PinstFromIfmp( pfucb->ifmp )->m_plog->m_fRecovering )
		{
		pfucb->bmCurr = bm;
		}
	else
		{
		Assert( CmpKeyData( pfucb->bmCurr, bm ) == 0 );
		}

	CallS( ErrNDReplace	( pfucb, pcsr, &data, fDIRUndo, rceidNull, prceNil ) );

	if ( cbDelta < 0 )
		{
		// Rolling back a replace that grew the node.  Add to uncommitted freed
		// count the amount of reserved node space, if any, that we must restore.
		// (This complements the call to ReclaimUncommittedFreed in SetCbAdjust()).
		pcsr->Cpage().AddUncommittedFreed( -cbDelta );
		}

	return err;
	}


//  ================================================================
ERR ErrVERIUndoInsertPhysical( RCE * const prce, CSR *pcsr )
//  ================================================================
//
//	set delete bit in node header and let RCE clean up
//	remove the node later
//
//-
	{
	ERR		err;
	FUCB	* const pfucb = prce->Pfucb();

	Assert( pgnoNull == prce->PgnoUndoInfo() );

	//	dirty page and log operation
	//
	CallR( ErrLGUndo( prce, pcsr, fMustDirtyCSR ) );

	CallS( ErrNDFlagDelete( pfucb, pcsr, fDIRUndo, rceidNull, NULL ) );

	return err;
	}


//  ================================================================
LOCAL ERR ErrVERIUndoFlagDeletePhysical( RCE * prce, CSR *pcsr )
//  ================================================================
//
//	reset delete bit
//
//-
	{
	ERR		err;
#ifdef DEBUG
	FUCB	* const pfucb 	= prce->Pfucb();
#endif	//	DEBUG

	if ( prce->PgnoUndoInfo() != pgnoNull )
		{
		CallR( ErrVERILogUndoInfo( prce, pcsr ) );
		}

	//	dirty page and log operation
	//
	CallR( ErrLGUndo( prce, pcsr, fMustDirtyCSR ) );

	NDResetFlagDelete( pcsr );
	return err;
	}


//  ================================================================
ERR ErrVERIUndoDeltaPhysical( RCE * const prce, CSR	*pcsr )
//  ================================================================
//
//	undo delta change. modifies the RCE by setting the lDelta to 0
//
//-
	{
	ERR		err;
	FUCB	* const pfucb = prce->Pfucb();
	LOG		*plog = PinstFromIfmp( prce->Ifmp() )->m_plog;

	VERDELTA* const 	pverdelta 	= (VERDELTA*)prce->PbData();
	const LONG 			lDelta 		= -pverdelta->lDelta;

	Assert( pgnoNull == prce->PgnoUndoInfo() );

	//  NDDelta is dependant on the data that it is operating on. for this reason we use
	//  NDGet to get the real data from the database (DIRGet will get the versioned copy)
///	AssertNDGet( pfucb, pcsr );
///	NDGet( pfucb, pcsr );

	if ( pverdelta->lDelta < 0 && !plog->m_fRecovering )
		{
		ENTERCRITICALSECTION enterCritHash( &( PverFromIfmp( prce->Ifmp() )->CritRCEChain( prce->UiHash() ) ) );

		//  we are rolling back a decrement. we need to remove all the deferredDelete flags
		RCE * prceT = prce;
		for ( ; prceNil != prceT->PrceNextOfNode(); prceT = prceT->PrceNextOfNode() )
			;
		for ( ; prceNil != prceT; prceT = prceT->PrcePrevOfNode() )
			{
			VERDELTA* const pverdeltaT = ( VERDELTA* )prceT->PbData();
			if ( operDelta == prceT->Oper() && pverdelta->cbOffset == pverdeltaT->cbOffset )
				{
				pverdeltaT->fDeferredDelete = fFalse;
				pverdeltaT->fCallbackOnZero = fFalse;
				pverdeltaT->fDeleteOnZero = fFalse;
				}
			}
		}

	//	dirty page and log operation
	//
	CallR( ErrLGUndo( prce, pcsr, fMustDirtyCSR ) );


	LONG lPrev;
	CallS( ErrNDDelta(
			pfucb,
			pcsr,
			pverdelta->cbOffset,
			&lDelta,
			sizeof( lDelta ),
			&lPrev,
			sizeof( lPrev ),
			NULL,
			fDIRUndo,
			rceidNull ) );
	if ( 0 == ( lPrev + lDelta ) )
		{
		//  by undoing an increment delta we have reduced the refcount of a LV to zero
		//  UNDONE:  morph the RCE into a committed level 0 decrement with deferred delete
		//         	 the RCE must be removed from the list of RCE's on the pib
///		AssertSz( fFalse, "LV cleanup breakpoint. Call LaurionB" );
		}

	//  in order that the compensating delta is calculated properly, set the delta value to 0
	pverdelta->lDelta = 0;

	return err;
	}


#ifdef DISABLE_SLV
#define ErrVERIUndoSLVSpacePhysical(a,b)	ErrERRCheck( JET_wrnNyi )
#else
//  ================================================================
ERR ErrVERIUndoSLVSpacePhysical( RCE * const prce, CSR *pcsr )
//  ================================================================
//
//-
	{
	ERR		err;
	FUCB	* const pfucb = prce->Pfucb();

	Assert( pgnoNull == prce->PgnoUndoInfo() );

	//	dirty page and log operation
	CallR( ErrLGUndo( prce, pcsr, fMustDirtyCSR ) );

	const VERSLVSPACE* const pverslvspace = (VERSLVSPACE*)( prce->PbData() );

	SLVSPACEOPER operUndo = slvspaceoperInvalid;
	switch( pverslvspace->oper )
		{
		case slvspaceoperFreeToReserved:
			operUndo = slvspaceoperFreeReserved;
			break;
		case slvspaceoperReservedToCommitted:
			operUndo = slvspaceoperCommittedToDeleted;
			break;
		case slvspaceoperFreeToCommitted:
			operUndo = slvspaceoperFree;
			break;
		case slvspaceoperCommittedToDeleted:
			operUndo = slvspaceoperDeletedToCommitted;
			break;

		//  These operations are only generated by a rollback and thus should not
		//  have an RCE
		case slvspaceoperFree:
		case slvspaceoperDeletedToCommitted:
		case slvspaceoperDeletedToFree:
			Assert( fFalse );
			break;
		}

	err = ErrNDMungeSLVSpace(
				pfucb,
				pcsr,
				operUndo,
				pverslvspace->ipage,
				pverslvspace->cpages,
				fDIRUndo,
				rceidNull );
	CallS( err );

	//  this space was moved from committed to deleted and the SLV Provider is
	//  enabled then issue a task to register this space for freeing with the
	//  SLV Provider.  we must do this because we cannot reuse the space until
	//  we know that all SLV File handles pointing to this space have been closed
	//
	//  NOTE:  if we get an error we must ignore it and leak the space because
	//  rollback cannot fail.  we will recover the space the next time we attach
	//  to the database

	Assert( !PinstFromIfmp( prce->Ifmp() )->m_pver->m_fSyncronousTasks );

	if (	slvspaceoperCommittedToDeleted == operUndo &&
			PinstFromIfmp( prce->Ifmp() )->FSLVProviderEnabled() )
		{
		BOOKMARK	bookmark;
		prce->GetBookmark( &bookmark );
		Assert( sizeof( PGNO ) == bookmark.key.Cb() );
		Assert( 0 == bookmark.data.Cb() );

		PGNO pgnoLastInExtent;
		LongFromKey( &pgnoLastInExtent, bookmark.key );
		Assert( pgnoLastInExtent != 0 );
		Assert( pgnoLastInExtent % cpgSLVExtent == 0 );
		PGNO pgnoFirst = ( pgnoLastInExtent - cpgSLVExtent + 1 ) + pverslvspace->ipage;
		QWORD ibLogical = OffsetOfPgno( pgnoFirst );
		QWORD cbSize = QWORD( pverslvspace->cpages ) * SLVPAGE_SIZE;

		OSSLVDELETETASK * const ptask = new OSSLVDELETETASK(
												prce->Ifmp(),
												ibLogical,
												cbSize,
												CSLVInfo::FILEID( pverslvspace->fileid ),
												pverslvspace->cbAlloc,
												(const wchar_t*)pverslvspace->wszFileName );

		if ( ptask )
			{
			if ( PinstFromIfmp( prce->Ifmp() )->Taskmgr().ErrTMPost( TASK::DispatchGP, ptask ) < JET_errSuccess )
				{
				//  The task was not enqueued sucessfully.
				delete ptask;
				}
			}
		}

	return err;
	}

#endif	//	DISABLE_SLV

//  ================================================================
VOID VERRedoPhysicalUndo( INST *pinst, const LRUNDO *plrundo, FUCB *pfucb, CSR *pcsr, BOOL fRedoNeeded )
//  ================================================================
//	retrieve RCE to be undone
//	call corresponding physical undo
//
	{
	//	get RCE for operation
	//
	BOOKMARK	bm;
	bm.key.prefix.Nullify();
	bm.key.suffix.SetPv( (VOID *) plrundo->rgbBookmark );
	bm.key.suffix.SetCb( plrundo->CbBookmarkKey() );
	bm.data.SetPv( (BYTE *) plrundo->rgbBookmark + plrundo->CbBookmarkKey() );
	bm.data.SetCb( plrundo->CbBookmarkData() );

	const DBID				dbid 	= plrundo->dbid;
	IFMP 					ifmp = pinst->m_mpdbidifmp[ dbid ];

	const UINT				uiHash	= UiRCHashFunc( ifmp, plrundo->le_pgnoFDP, bm );
	ENTERCRITICALSECTION	enterCritHash( &( PverFromIfmp( ifmp )->CritRCEChain( uiHash ) ) );

	RCE * prce = PrceRCEGet( uiHash, ifmp, plrundo->le_pgnoFDP, bm );
	Assert( !fRedoNeeded || prce != prceNil );

	for ( ; prce != prceNil ; prce = prce->PrcePrevOfNode() )
		{
		if ( prce->Rceid() == plrundo->le_rceid )
			{
			//	UNDONE:	use rceid instead of level and procid
			//			to identify RCE
			//
			Assert( prce->Pfucb() == pfucb );
			Assert( prce->Pfucb()->ppib == pfucb->ppib );
			Assert(	prce->Oper() == plrundo->le_oper );
			Assert( prce->TrxCommitted() == trxMax );

			//	UNDONE: the following assert will fire if
			//	the original node operation was created by proxy
			//	(ie. concurrent create index).
			Assert( prce->Level() == plrundo->level );

			if ( fRedoNeeded )
				{
				Assert( prce->FUndoableLoggedOper( ) );
				OPER oper = plrundo->le_oper;

				switch ( oper )
					{
					case operReplace:
						CallS( ErrVERIUndoReplacePhysical( prce,
														   pcsr,
														   bm ) );
						break;

					case operInsert:
						CallS( ErrVERIUndoInsertPhysical( prce, pcsr ) );
						break;

					case operFlagDelete:
						CallS( ErrVERIUndoFlagDeletePhysical( prce, pcsr ) );
						break;

					case operDelta:
						CallS( ErrVERIUndoDeltaPhysical( prce, pcsr ) );
						break;

					case operSLVSpace:
						CallS( ErrVERIUndoSLVSpacePhysical( prce, pcsr ) );
						break;

					default:
						Assert( fFalse );
					}
				}

			ENTERCRITICALSECTION critRCEList( &(prce->Pfcb()->CritRCEList()) );
			VERINullifyUncommittedRCE( prce );
			break;
			}
		}

	Assert( !fRedoNeeded || prce != prceNil );

	return;
	}


LOCAL VOID VERINullifyRolledBackRCE(
	PIB				*ppib,
	RCE				* const prceToNullify,
	RCE				**pprceNextToNullify = NULL )
	{
	const BOOL			fOperInHashTable		= prceToNullify->FOperInHashTable();
	CCriticalSection	*pcritHash				= fOperInHashTable ?
													&( PverFromIfmp( prceToNullify->Ifmp() )->CritRCEChain( prceToNullify->UiHash() ) ) :
													NULL;
	CCriticalSection&	critRCEList				= prceToNullify->Pfcb()->CritRCEList();
	const BOOL			fPossibleSecondaryIndex	= prceToNullify->Pfcb()->FTypeTable()
													&& !prceToNullify->Pfcb()->FFixedDDL()
													&& prceToNullify->FOperAffectsSecondaryIndex();


	Assert( ppib->critTrx.FOwner() );

	if ( fOperInHashTable )
		{
		pcritHash->Enter();
		}
	critRCEList.Enter();

	prceToNullify->FlagRolledBack();

	// Take snapshot of count of people concurrently creating a secondary
	// index entry.  Since this count is only ever incremented within this
	// table's critRCEList (which we currently have) it doesn't matter
	// if value changes after the snapshot is taken, because it will
	// have been incremented for some other table.
	// Also, if this FCB is not a table or if it is but has fixed DDL, it
	// doesn't matter if others are doing concurrent create index -- we know
	// they're not doing it on this FCB.
	// Finally, the only RCE types we have to lock are Insert, FlagDelete, and Replace,
	// because they are the only RCE's that concurrent CreateIndex acts upon.
	const LONG	crefCreateIndexLock		= ( fPossibleSecondaryIndex ? crefVERCreateIndexLock : 0 );
	Assert( crefVERCreateIndexLock >= 0 );
	if ( 0 == crefCreateIndexLock )
		{
		//	set return value before the RCE is nullified
		if ( NULL != pprceNextToNullify )
			*pprceNextToNullify = prceToNullify->PrcePrevOfSession();

		Assert( !prceToNullify->FOperNull() );
		Assert( prceToNullify->Level() <= ppib->level );
		Assert( prceToNullify->TrxCommitted() == trxMax );

		VERINullifyUncommittedRCE( prceToNullify );

		critRCEList.Leave();
		if ( fOperInHashTable )
			{
			pcritHash->Leave();
			}
		}
	else
		{
		Assert( crefCreateIndexLock > 0 );

		critRCEList.Leave();
		if ( fOperInHashTable )
			{
			pcritHash->Leave();
			}


		if ( NULL != pprceNextToNullify )
			{
			ppib->critTrx.Leave();
			UtilSleep( cmsecWaitGeneric );
			ppib->critTrx.Enter();

			//	restart RCE scan
			*pprceNextToNullify = ppib->prceNewest;
			}
		}
	}



#ifdef DISABLE_SLV
#define ErrVERITryUndoSLVPageAppend(a,b)	ErrERRCheck( JET_wrnNyi )
#else
//  ================================================================
LOCAL ERR ErrVERITryUndoSLVPageAppend( PIB *ppib, RCE * const prce )
//  ================================================================
	{
	ERR		err 			= JET_errSuccess;
	FUCB	* const pfucb	= prce->Pfucb();
	INT		crepeat 		= 0;
	LATCH	latch 			= latchReadNoTouch;
	BOOL	fGotoBookmark	= fFalse;

	Assert( pfucbNil != pfucb );
	ASSERT_VALID( pfucb );
	Assert( ppib == pfucb->ppib );
	Assert( prce->TrxCommitted() == trxMax );
	Assert ( operSLVPageAppend == prce->Oper() );

	//  we have sucessfully undone the operation
	//	must now set RolledBack flag before releasing page latch
	Assert( !prce->FOperNull() );
	Assert( !prce->FRolledBack() );

	RCE::SLVPAGEAPPEND *pData;

	Assert ( prce->CbData() == sizeof(RCE::SLVPAGEAPPEND) );
	pData = (RCE::SLVPAGEAPPEND *) prce->PbData();
	Assert ( NULL != pData );

	IFMP	ifmpDb	= prce->Ifmp( );
	IFMP	ifmp	= ifmpDb | ifmpSLV;
	PGNO	pgno	= PgnoOfOffset( pData->ibLogical );
	DWORD	ib		= DWORD( pData->ibLogical % SLVPAGE_SIZE );
	DWORD cb 		= pData->cbData;

	// only in this care we will undo SLV page operations
	Assert ( !PinstFromIfmp( ifmpDb )->FSLVProviderEnabled() );

	Assert ( SLVPAGE_SIZE - ib >= cb );

	BFLatch bflPage;

	Call( ErrBFWriteLatchPage( &bflPage, ifmp, pgno , ib ? bflfDefault : bflfNew ) );

	// warnings like wrnBFCacheMiss are OK
	if ( err > JET_errSuccess )
		{
		err = JET_errSuccess;
		}

	BFDirty( &bflPage );
	memset ( (BYTE*)bflPage.pv + ib, 0, cb );

	BFWriteUnlatch( &bflPage );

	//	must nullify RCE while page is still latched, to avoid inconsistency
	//	between what's on the page and what's in the version store
	VERINullifyRolledBackRCE( ppib, prce );

	CallS( err );
	return JET_errSuccess;

HandleError:

	Assert( err < JET_errSuccess );
	return err;
	}
#endif	//	DISABLE_SLV

//  ================================================================
LOCAL ERR ErrVERITryUndoLoggedOper( PIB *ppib, RCE * const prce )
//  ================================================================
//	seek to bookmark, upgrade latch
//	call corresponding physical undo
//
	{
	ERR				err;
	FUCB * const	pfucb		= prce->Pfucb();
	LATCH			latch 		= latchReadNoTouch;
	BOOKMARK		bm;
	BOOKMARK		bmSave;

	Assert( pfucbNil != pfucb );
	ASSERT_VALID( pfucb );
	Assert( ppib == pfucb->ppib );
	Assert( prce->FUndoableLoggedOper() );
	Assert( prce->TrxCommitted() == trxMax );

	// if Force Detach, just nullify the RCE
	if ( rgfmp[ prce->Ifmp() ].FForceDetaching() )
		{
		VERINullifyRolledBackRCE( ppib, prce );
		return JET_errSuccess;
		}

	prce->GetBookmark( &bm );

	//  reset index range on this cursor
	//  we may be using a deferred-closed cursor or a cursor that
	//  had an index-range on it before the rollback
	DIRResetIndexRange( pfucb );

	//	save off cursor's current bookmark, set
	//	to bookmark of operation to be rolled back,
	//	then release any latches to force re-seek
	//	to bookmark
	bmSave = pfucb->bmCurr;
	pfucb->bmCurr = bm;

	BTUp( pfucb );

Refresh:
	err = ErrBTIRefresh( pfucb, latch );
	Assert( JET_errRecordDeleted != err );
	Call( err );

	//	upgrade latch on page
	//
	err = Pcsr( pfucb )->ErrUpgrade();
	if ( errBFLatchConflict == err )
		{
		Assert( !Pcsr( pfucb )->FLatched() );
		latch = latchRIW;
		goto Refresh;
		}
	Call( err );

	switch( prce->Oper() )
		{
		//	logged operations
		//
		case operReplace:
			Call( ErrVERIUndoReplacePhysical( prce, Pcsr( pfucb ), bm ) );
			break;

		case operInsert:
			Call( ErrVERIUndoInsertPhysical( prce, Pcsr( pfucb ) ) );
			break;

		case operFlagDelete:
			Call( ErrVERIUndoFlagDeletePhysical( prce, Pcsr( pfucb ) ) );
			break;

		case operDelta:
			Call( ErrVERIUndoDeltaPhysical( prce, Pcsr( pfucb ) ) );
			break;

		case operSLVSpace:
			Call( ErrVERIUndoSLVSpacePhysical( prce, Pcsr( pfucb ) ) );
			break;

		default:
			Assert( fFalse );
		}

	//  we have sucessfully undone the operation
	//	must now set RolledBack flag before releasing page latch
	Assert( !prce->FOperNull() );
	Assert( !prce->FRolledBack() );

	//	must nullify RCE while page is still latched, to avoid inconsistency
	//	between what's on the page and what's in the version store
	VERINullifyRolledBackRCE( ppib, prce );

	//	re-instate original bookmark
	pfucb->bmCurr = bmSave;
	BTUp( pfucb );

	CallS( err );
	return JET_errSuccess;

HandleError:
	//	re-instate original bookmark
	pfucb->bmCurr = bmSave;
	BTUp( pfucb );

	Assert( err < JET_errSuccess );
	Assert( JET_errDiskFull != err );

	return err;
	}


//  ================================================================
LOCAL ERR ErrVERIUndoLoggedOper( PIB *ppib, RCE * const prce )
//  ================================================================
//	seek to bookmark, upgrade latch
//	call corresponding physical undo
//
	{
	ERR				err 	= JET_errSuccess;
	FUCB	* const pfucb	= prce->Pfucb();

	Assert( pfucbNil != pfucb );

	if ( operSLVPageAppend == prce->Oper() )
		{
		Assert ( !prce->FOperInHashTable() );

		Assert ( prce->CbData () == sizeof( RCE::SLVPAGEAPPEND ) );
		Call ( ErrVERITryUndoSLVPageAppend( ppib, prce ) );
		}
	else
		{
		Assert( ppib == prce->Pfucb()->ppib );
		Call ( ErrVERITryUndoLoggedOper( ppib, prce ) );
		}

	return JET_errSuccess;

HandleError:
	Assert( err < JET_errSuccess );

	//  if rollback fails due to an error, then we will disable
	//  logging in order to force the system down (we are in
	//  an uncontinuable state).  Recover to restart the database.

	//	We should never fail to rollback due to disk full

	Assert( JET_errDiskFull != err );

	switch ( err )
		{
		// failure with impact on the database only
		case JET_errDiskIO:
		case JET_errReadVerifyFailure:
		case JET_errOutOfBuffers:
		case JET_errOutOfMemory	:		// BF may hit OOM when trying to latch the page
		case JET_errCheckpointDepthTooDeep:


#ifdef INDEPENDENT_DB_FAILURE
			// if we know that transactions are limited at
			// one database, we can put the database
			// in the "unusable" state and error out
			// (on error on TMP database fail the instance)
			if ( g_fOneDatabasePerSession && FUserIfmp( prce->Ifmp() ) )
				{

				// UNDONE: do we want to retry, at least on some errors
				// like OutOfMemory ?

				Assert ( prce );
				FMP::AssertVALIDIFMP( prce->Ifmp() );

				FMP * pfmp = &rgfmp[ prce->Ifmp() ];
				Assert ( pfmp );

				Assert ( !pfmp->FAllowForceDetach() );
				pfmp->SetAllowForceDetach( ppib, err );
				Assert ( pfmp->FAllowForceDetach() );

				// UNDONE: EventLog error during rollback

				// convert error to JET_errRollbackError
				err = ErrERRCheck ( JET_errRollbackError );
				break;
				}
#endif

		// failure with impact on the instance
		case JET_errLogWriteFail:
		case JET_errLogDiskFull:
				{

				//	rollback failed -- log an event message

				INST * const	pinst		= PinstFromPpib( ppib );
				LOG * const		plog		= pinst->m_plog;
				CHAR			szRCEID[16];
				CHAR			szERROR[16];

				if ( JET_errLogWriteFail == err
					&& plog->m_fLGNoMoreLogWrite
					&& JET_errLogDiskFull == plog->m_errNoMoreLogWrite )
					{
					//	special-case: trap JET_errLogDiskFull
					//
					err = ErrERRCheck( JET_errLogDiskFull );
					}

				LOSSTRFormatA( szRCEID, "%d", prce->Rceid() );
				LOSSTRFormatA( szERROR, "%d", err );

				const CHAR *rgpsz[3] = { szRCEID, rgfmp[pfucb->ifmp & ifmpMask].SzDatabaseName(), szERROR };
				UtilReportEvent(
						eventError,
						LOGGING_RECOVERY_CATEGORY,
						UNDO_FAILED_ID,
						3,
						rgpsz,
						0,
						NULL,
						pinst );

				//	REVIEW: (LaurionB) how can we get a logging failure if FLogOn() is
				//	not set?
				if ( rgfmp[pfucb->ifmp & ifmpMask].FLogOn() )
					{
					Assert( plog );
					Assert( !plog->m_fLogDisabled );

					//  flush and halt the log

					plog->LGSignalFlush();
					UtilSleep( cmsecWaitLogFlush );
					plog->SetFNoMoreLogWrite( err );
					}


				//	There may be an older version of this page which needs
				//	the undo-info from this RCE. Take down the instance and
				//	error out
				//
				//	(Yes, this could be optimized to only do this for RCEs
				//	with before-images, but why complicate the code to optimize
				//	a failure case)

				Assert( !prce->FOperNull() );
				Assert( !prce->FRolledBack() );

				pinst->SetInstanceUnavailable( err );
				ppib->SetErrRollbackFailure( err );
				err = ErrERRCheck ( JET_errRollbackError );
				break;
				}

		default:
			//	error is non-fatal, so caller will attempt to redo rollback
			break;
		}

	return err;
	}


//  ================================================================
INLINE VOID VERINullifyForUndoCreateTable( PIB * const ppib, FCB * const pfcb )
//  ================================================================
//
//  This is used to nullify all RCEs on table FCB because CreateTable
//	was rolled back.
//
//-
	{
	Assert( pfcb->FTypeTable()
		|| pfcb->FTypeTemporaryTable() );

	// Because rollback is done in two phases, the RCE's on
	// this FCB are still outstanding -- they've already
	// been processed for rollback, they just need to be
	// nullified.
	while ( prceNil != pfcb->PrceNewest() )
		{
		RCE	* const prce = pfcb->PrceNewest();

		Assert( prce->Pfcb() == pfcb );
		Assert( !prce->FOperNull() );

		// Since we're rolling back CreateTable, all operations
		// on the table must also be uncommitted and rolled back
		Assert( prce->TrxCommitted() == trxMax );

		if ( !prce->FRolledBack() )
			{
			// The CreateTable RCE itself should be the only
			// RCE on this FCB that is not yet marked as
			// rolled back, because that's what we're in the
			// midst of doing
			Assert( prce->Oper() == operCreateTable );
			Assert( pfcb->FTypeTable() || pfcb->FTypeTemporaryTable() );
			Assert( pfcb->PrceNewest() == prce );
			Assert( pfcb->PrceOldest() == prce );
			}

		Assert( prce->Pfucb() != pfucbNil );
		Assert( ppib == prce->Pfucb()->ppib );	// only one session should have access to the table

		ENTERCRITICALSECTION	enterCritHash(
									prce->FOperInHashTable() ? &( PverFromIfmp( prce->Ifmp() )->CritRCEChain( prce->UiHash() ) ) : NULL,
									prce->FOperInHashTable() );
		ENTERCRITICALSECTION	enterCritFCBRCEList( &( pfcb->CritRCEList() ) );
		VERINullifyUncommittedRCE( prce );
		}

	// should be no more versions on the FCB
	Assert( pfcb->PrceOldest() == prceNil );
	Assert( pfcb->PrceNewest() == prceNil );
	}

//  ================================================================
INLINE VOID VERICleanupForUndoCreateTable( RCE * const prceCreateTable )
//  ================================================================
//
//  This is used to cleanup RCEs and deferred-closed cursors
//	on table FCB because CreateTable was rolled back.
//
//-
	{
	FCB	* const pfcbTable = prceCreateTable->Pfcb();

	Assert( pfcbTable->FInitialized() );
	Assert( pfcbTable->FTypeTable() || pfcbTable->FTypeTemporaryTable() );
	Assert( pfcbTable->FPrimaryIndex() );

	// Last RCE left should be the CreateTable RCE (ie. this RCE).
	Assert( operCreateTable == prceCreateTable->Oper() );
	Assert( pfcbTable->PrceOldest() == prceCreateTable );

	Assert( prceCreateTable->Pfucb() != pfucbNil );
	PIB	* const ppib	= prceCreateTable->Pfucb()->ppib;
	Assert( ppibNil != ppib );

	Assert( pfcbTable->Ptdb() != ptdbNil );
	FCB	* pfcbT			= pfcbTable->Ptdb()->PfcbLV();

	// force-close any deferred closed cursors
	if ( pfcbNil != pfcbT )
		{
		Assert( pfcbT->FTypeLV() );

		//	all RCE's on the table's LV should have already been rolled back AND nullified
		//	(since we don't suppress nullification of LV RCE's during concurrent create-index
		//	like we do for Table RCE's)
		Assert( prceNil == pfcbT->PrceNewest() );
//		VERINullifyForUndoCreateTable( ppib, pfcbT );

		FUCBCloseAllCursorsOnFCB( ppib, pfcbT );
		}

	for ( pfcbT = pfcbTable->PfcbNextIndex(); pfcbNil != pfcbT; pfcbT = pfcbT->PfcbNextIndex() )
		{
		Assert( pfcbT->FTypeSecondaryIndex() );

		//	all RCE's on the table's indexes should have already been rolled back AND nullified
		//	(since we don't suppress nullification of Index RCE's during concurrent create-index
		//	like we do for Table RCE's)
		Assert( prceNil == pfcbT->PrceNewest() );
//		VERINullifyForUndoCreateTable( ppib, pfcbT );

		FUCBCloseAllCursorsOnFCB( ppib, pfcbT );
		}

	//	there may be rolled-back versions on the table that couldn't be nullified
	//	because concurrent create-index was in progress on another table (we
	//	suppress nullification of certain table RCE's if a CCI is in progress
	//	anywhere)
	VERINullifyForUndoCreateTable( ppib, pfcbTable );
	FUCBCloseAllCursorsOnFCB( ppib, pfcbTable );
	}

//  ================================================================
LOCAL VOID VERIUndoCreateTable( PIB * const ppib, RCE * const prce )
//  ================================================================
//
//  Takes a non-const RCE as VersionDecrement sets the pfcb to NULL
//
//-
	{
	FUCB	* pfucb			= prce->Pfucb();
	FCB 	* const pfcb	= prce->Pfcb();

	ASSERT_VALID( pfucb );
	ASSERT_VALID( ppib );
	Assert( prce->Oper() == operCreateTable );
	Assert( prce->TrxCommitted() == trxMax );

	Assert( pfcb->FInitialized() );
	Assert( pfcb->FTypeTable() || pfcb->FTypeTemporaryTable() );
	Assert( pfcb->FPrimaryIndex() );

	//	set FDeleteCommitted flag to silence asserts in ErrSPFreeFDP()
	//
	pfcb->SetDeleteCommitted();

	//	ignore errors if we can't free the space (it will be leaked)
	//
	(VOID)ErrSPFreeFDP( ppib, pfcb, pgnoSystemRoot, fTrue );

	//	close all cursors on this table
	while ( pfucbNil != pfucb )
		{
		ASSERT_VALID( pfucb );
		FUCB * const pfucbNext = pfucb->pfucbNextOfFile;

		// Since CreateTable is uncommitted, we should be the
		// only one who could have opened cursors on it.
		Assert( pfucb->ppib == ppib );

		//	if defer closed then continue
		if ( !FFUCBDeferClosed( pfucb ) )
			{
			if ( pfucb->pvtfndef != &vtfndefInvalidTableid )
				{
				CallS( ErrDispCloseTable( ( JET_SESID )ppib, ( JET_TABLEID )pfucb ) );
				}
			else
				{
				CallS( ErrFILECloseTable( ppib, pfucb ) );
				}
			}

		pfucb = pfucbNext;
		}

	VERICleanupForUndoCreateTable( prce );

	pfcb->PrepareForPurge();
	pfcb->Purge();
	}


//  ================================================================
LOCAL VOID VERIUndoAddColumn( const RCE * const prce )
//  ================================================================
	{
	Assert( prce->Oper() == operAddColumn );
	Assert( prce->TrxCommitted() == trxMax );

	Assert( prce->CbData() == sizeof(VERADDCOLUMN) );
	Assert( prce->Pfcb() == prce->Pfucb()->u.pfcb );

	FCB			* const pfcbTable	= prce->Pfcb();

	pfcbTable->EnterDDL();

	// RCE list ensures FCB is still pinned
	Assert( pfcbTable->PrceOldest() != prceNil );

	TDB 			* const	ptdb 		= pfcbTable->Ptdb();
	const COLUMNID	columnid			= ( (VERADDCOLUMN*)prce->PbData() )->columnid;
	BYTE			* pbOldDefaultRec	= ( (VERADDCOLUMN*)prce->PbData() )->pbOldDefaultRec;
	FIELD			* const	pfield		= ptdb->Pfield( columnid );

	Assert( ( FCOLUMNIDTagged( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidTaggedLast() )
			|| ( FCOLUMNIDVar( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidVarLast() )
			|| ( FCOLUMNIDFixed( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidFixedLast() ) );

	//	rollback the added column by marking it as deleted
	Assert( !FFIELDDeleted( pfield->ffield ) );
	FIELDSetDeleted( pfield->ffield );

	FIELDResetVersioned( pfield->ffield );
	FIELDResetVersionedAdd( pfield->ffield );

	//	rollback version and autoinc fields, if set.
	Assert( !( FFIELDVersion( pfield->ffield ) && FFIELDAutoincrement( pfield->ffield ) ) );
	if ( FFIELDVersion( pfield->ffield ) )
		{
		Assert( ptdb->FidVersion() == FidOfColumnid( columnid ) );
		ptdb->ResetFidVersion();
		}
	else if ( FFIELDAutoincrement( pfield->ffield ) )
		{
		Assert( ptdb->FidAutoincrement() == FidOfColumnid( columnid ) );
		ptdb->ResetFidAutoincrement();
		}

	//	itag 0 in the TDB is reserved for the FIELD structures.  We
	//	cannibalise it for itags of field names to indicate that a name
	//	has not been added to the buffer.
	if ( 0 != pfield->itagFieldName )
		{
		//	remove the column name from the TDB name space
		ptdb->MemPool().DeleteEntry( pfield->itagFieldName );
		}

	//  UNDONE: remove the CBDESC for this from the TDB
	//  the columnid will not be re-used so the callback
	//  will not be called. The CBDESC will be freed when
	//  the table is closed so no memory will be lost
	//  Its just not pretty though...

	//	if we modified the default record, the changes will be invisible,
	//	since the field is now flagged as deleted (unversioned).  The
	//	space will be reclaimed by a subsequent call to AddColumn.

	//	if there was an old default record and we were successful in
	//	creating a new default record for the TDB, must get rid of
	//	the old default record.  However, we can't just free the
	//	memory because other threads could have stale pointers.
	//	So build a list hanging off the table FCB and free the
	//	memory when the table FCB is freed.
	if ( NULL != pbOldDefaultRec
		&& (BYTE *)ptdb->PdataDefaultRecord() != pbOldDefaultRec )
		{
		//	user-defined defaults are not stored in the default
		//	record (ie. this AddColumn would not have caused
		//	use to rebuild the default record)
		Assert( !FFIELDUserDefinedDefault( pfield->ffield ) );

		for ( RECDANGLING * precdangling = pfcbTable->Precdangling();
			;
			precdangling = precdangling->precdanglingNext )
			{
			if ( NULL == precdangling )
				{
				//	not in list, so add it;
				//	assumes that the memory pointed to by pmemdangling is always at
				//	least sizeof(ULONG_PTR) bytes
				Assert( NULL == ( (RECDANGLING *)pbOldDefaultRec )->precdanglingNext );
				( (RECDANGLING *)pbOldDefaultRec )->precdanglingNext = pfcbTable->Precdangling();
				pfcbTable->SetPrecdangling( (RECDANGLING *)pbOldDefaultRec );
				break;
				}
			else if ( (BYTE *)precdangling == pbOldDefaultRec )
				{
				//	pointer is already in the list, just get out
				AssertTracking();
				break;
				}
			}
		}

	pfcbTable->LeaveDDL();
	}


//  ================================================================
LOCAL VOID VERIUndoDeleteColumn( const RCE * const prce )
//  ================================================================
	{
	Assert( prce->Oper() == operDeleteColumn );
	Assert( prce->TrxCommitted() == trxMax );

	Assert( prce->CbData() == sizeof(COLUMNID) );
	Assert( prce->Pfcb() == prce->Pfucb()->u.pfcb );

	FCB			* const pfcbTable	= prce->Pfcb();

	pfcbTable->EnterDDL();

	// RCE list ensures FCB is still pinned
	Assert( pfcbTable->PrceOldest() != prceNil );

	TDB 			* const	ptdb 		= pfcbTable->Ptdb();
	const COLUMNID	columnid			= *( (COLUMNID*)prce->PbData() );
	FIELD			* const	pfield		= ptdb->Pfield( columnid );

	Assert( ( FCOLUMNIDTagged( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidTaggedLast() )
			|| ( FCOLUMNIDVar( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidVarLast() )
			|| ( FCOLUMNIDFixed( columnid ) && FidOfColumnid( columnid ) <= ptdb->FidFixedLast() ) );

	Assert( pfield->coltyp != JET_coltypNil );
	Assert( FFIELDDeleted( pfield->ffield ) );
	FIELDResetDeleted( pfield->ffield );

	if ( FFIELDVersioned( pfield->ffield ) )
		{
		// UNDONE: Instead of the VersionedAdd flag, scan the version store
		// for other outstanding versions on this column (should either be
		// none or a single AddColumn version).
		if ( !FFIELDVersionedAdd( pfield->ffield ) )
			{
			FIELDResetVersioned( pfield->ffield );
			}
		}
	else
		{
		Assert( !FFIELDVersionedAdd( pfield->ffield ) );
		}

	pfcbTable->LeaveDDL();
	}


//  ================================================================
LOCAL VOID VERIUndoDeleteTable( const RCE * const prce )
//  ================================================================
	{
	if ( rgfmp[ prce->Ifmp() ].Dbid() == dbidTemp )
		{
		// DeleteTable (ie. CloseTable) doesn't get rolled back for temp. tables.
		return;
		}

	Assert( prce->Oper() == operDeleteTable );
	Assert( prce->TrxCommitted() == trxMax );

	//	may be pfcbNil if sentinel
	INT	fState;
	FCB * const pfcbTable = FCB::PfcbFCBGet( prce->Ifmp(), *(PGNO*)prce->PbData(), &fState );
	Assert( pfcbTable != pfcbNil );
	Assert( pfcbTable->FTypeTable() || pfcbTable->FTypeSentinel() );
	Assert( fFCBStateInitialized == fState || fFCBStateSentinel == fState );

	// If regular FCB, decrement refcnt, else free sentinel.
	if ( pfcbTable->Ptdb() != ptdbNil )
		{
		Assert( fFCBStateInitialized == fState );

		pfcbTable->EnterDDL();

		for ( FCB *pfcbT = pfcbTable; pfcbT != pfcbNil; pfcbT = pfcbT->PfcbNextIndex() )
			{
			Assert( pfcbT->FDeletePending() );
			Assert( !pfcbT->FDeleteCommitted() );
			pfcbT->ResetDeletePending();
			}

		FCB	* const pfcbLV = pfcbTable->Ptdb()->PfcbLV();
		if ( pfcbNil != pfcbLV )
			{
			Assert( pfcbLV->FDeletePending() );
			Assert( !pfcbLV->FDeleteCommitted() );
			pfcbLV->ResetDeletePending();
			}

		pfcbTable->LeaveDDL();

		pfcbTable->Release();
		}
	else
		{
		Assert( fFCBStateSentinel == fState );
		Assert( pfcbTable->FDeletePending() );
		Assert( !pfcbTable->FDeleteCommitted() );
		pfcbTable->ResetDeletePending();

		pfcbTable->PrepareForPurge();
		pfcbTable->Purge();
		}
	}


//  ================================================================
LOCAL VOID VERIUndoCreateLV( PIB *ppib, const RCE * const prce )
//  ================================================================
	{
	if ( pfcbNil != prce->Pfcb()->Ptdb()->PfcbLV() )
		{
		//  if we rollback the creation of the LV tree, unlink the LV FCB
		//  the FCB will be lost (memory leak)
		FCB * const pfcbLV = prce->Pfcb()->Ptdb()->PfcbLV();

		VERIUnlinkDefunctLV( prce->Pfucb()->ppib, pfcbLV );
		pfcbLV->PrepareForPurge( fFalse );
		pfcbLV->Purge();
		prce->Pfcb()->Ptdb()->SetPfcbLV( pfcbNil );

		if ( pfcbLV >= PfcbFCBPreferredThreshold( PinstFromIfmp( prce->Ifmp() ) ) )
			{

			//	the LV FCB is above the threshold; thus, removing it may
			//	cause the table FCB to move below the threshold; if the
			//	table FCB is in the avail-above list, it must be moved
			//	to the avail-below list

			prce->Pfcb()->UpdateAvailListPosition();
			}
		}
	}


//  ================================================================
LOCAL VOID VERIUndoCreateIndex( PIB *ppib, const RCE * const prce )
//  ================================================================
	{
	Assert( prce->Oper() == operCreateIndex );
	Assert( prce->TrxCommitted() == trxMax );
	Assert( prce->CbData() == sizeof(TDB *) );

	//	pfcb of secondary index FCB or pfcbNil for primary
	//	index creation
	FCB	* const pfcb = *(FCB **)prce->PbData();
	FCB	* const pfcbTable = prce->Pfucb()->u.pfcb;

	Assert( pfcbNil != pfcbTable );
	Assert( pfcbTable->FTypeTable() );
	Assert( pfcbTable->PrceOldest() != prceNil );	// This prevents the index FCB from being deallocated.

	//	if secondary index then close all cursors on index
	//	and purge index FCB, else free IDB for primary index.

	if ( pfcb != pfcbNil )
		{
		// This can't be the primary index, because we would not have allocated
		// an FCB for it.
		Assert( prce->Pfucb()->u.pfcb != pfcb );

		Assert( pfcb->FTypeSecondaryIndex() );
		Assert( pfcb->Pidb() != pidbNil );

		//	Normally, we grab the updating/indexing latch before we grab
		//	the ppib's critTrx, but in this case, we are already in the
		//	ppib's critTrx (because we are rolling back) and we need the
		//	updating/indexing latch.  We can guarantee that this will not
		//	cause a deadlock because the only person that grabs critTrx
		//	after grabbing the updating/indexing latch is concurrent create
		//	index, which quiesces all rollbacks before it begins.
		CLockDeadlockDetectionInfo::NextOwnershipIsNotADeadlock();

		pfcbTable->SetIndexing();
		pfcbTable->EnterDDL();

		Assert( !pfcb->Pidb()->FDeleted() );
		Assert( !pfcb->FDeletePending() );
		Assert( !pfcb->FDeleteCommitted() );

		// Mark as committed delete so no one else will attempt version check.
		pfcb->Pidb()->SetFDeleted();
		pfcb->Pidb()->ResetFVersioned();

		if ( pfcb->PfcbTable() == pfcbNil )
			{
			// Index FCB not yet linked into table's FCB list, but we did use
			// table's TDB memory pool to store some information for the IDB.
			pfcb->UnlinkIDB( pfcbTable );
			}
		else
			{
			Assert( pfcb->PfcbTable() == pfcbTable );

			// Unlink the FCB from the table's index list.
			// Note that the only way the FCB could have been
			// linked in is if the IDB was successfully created.
			pfcbTable->UnlinkSecondaryIndex( pfcb );

			//	update all index mask
			FILESetAllIndexMask( pfcbTable );
			}

		pfcbTable->LeaveDDL();
		pfcbTable->ResetIndexing();

		if ( pfcb >= PfcbFCBPreferredThreshold( PinstFromIfmp( prce->Ifmp() ) ) )
			{

			//	the index FCB is above the threshold; thus, removing it may
			//	cause the table FCB to move below the threshold; if the
			//	table FCB is in the avail-above list, it must be moved
			//	to the avail-below list

			pfcbTable->UpdateAvailListPosition();
			}

		//	must leave critTrx because we may enter the critTrx
		//	of other sessions when we try to remove their RCEs
		//	or cursors on this index
		ppib->critTrx.Leave();

		// Index FCB has been unlinked from table, so we're
		// guaranteed no further versions will occur on this
		// FCB.  Clean up remaining versions.
		VERNullifyAllVersionsOnFCB( pfcb );

		//	set FDeleteCommitted flag to silence asserts in ErrSPFreeFDP()
		//
		pfcb->SetDeleteCommitted();

		//	ignore errors if we can't free the space (it will be leaked)
		//
		//	WARNING: if this is recovery and the index was not logged,
		//	it's very possible that not all pages in the OwnExt tree
		//	were flushed and the btree is therefore inconsistent, so
		//	this may fail with weird errors (though I don't believe
		//	it will crash or corrupt space in the parent) and
		//	therefore leak the space
		//
		(VOID)ErrSPFreeFDP( ppib, pfcb, pfcbTable->PgnoFDP(), fTrue );

		VERIUnlinkDefunctSecondaryIndex( prce->Pfucb()->ppib, pfcb );

		ppib->critTrx.Enter();

		// The table's version count will prevent the
		// table FCB (and thus this secondary index FCB)
		// from being deallocated before we can delete
		// this index FCB.
		pfcb->PrepareForPurge( fFalse );
		pfcb->Purge();
		}

	else if ( pfcbTable->Pidb() != pidbNil )
		{
		pfcbTable->EnterDDL();

		IDB		*pidb	= pfcbTable->Pidb();

		Assert( !pidb->FDeleted() );
		Assert( !pfcbTable->FDeletePending() );
		Assert( !pfcbTable->FDeleteCommitted() );
		Assert( !pfcbTable->FSequentialIndex() );

		// Mark as committed delete so no one else will attempt version check.
		pidb->SetFDeleted();
		pidb->ResetFVersioned();
		pidb->ResetFVersionedCreate();

		pfcbTable->UnlinkIDB( pfcbTable );
		pfcbTable->SetPidb( pidbNil );
		pfcbTable->SetSequentialIndex();

		// UNDONE: Reset density to original value.
		pfcbTable->SetCbDensityFree( (SHORT)( ( ( 100 - ulFILEDefaultDensity ) * g_cbPage ) / 100 ) );

		//	update all index mask
		FILESetAllIndexMask( pfcbTable );

		pfcbTable->LeaveDDL();
		}
	}


//  ================================================================
LOCAL VOID VERIUndoDeleteIndex( const RCE * const prce )
//  ================================================================
	{
	FCB	* const pfcbIndex = *(FCB **)prce->PbData();
	FCB	* const pfcbTable = prce->Pfcb();

	Assert( prce->Oper() == operDeleteIndex );
	Assert( prce->TrxCommitted() == trxMax );

	Assert( pfcbTable->FTypeTable() );
	Assert( pfcbIndex->PfcbTable() == pfcbTable );
	Assert( prce->CbData() == sizeof(FCB *) );
	Assert( pfcbIndex->FTypeSecondaryIndex() );

	pfcbIndex->ResetDeleteIndex();
	}


//  ================================================================
INLINE VOID VERIUndoAllocExt( const RCE * const prce )
//  ================================================================
	{
	Assert( prce->CbData() == sizeof(VEREXT) );
	Assert( prce->PgnoFDP() == ((VEREXT*)prce->PbData())->pgnoFDP );

	const VEREXT* const pverext = (const VEREXT*)(prce->PbData());

	VERIFreeExt( prce->Pfucb()->ppib, prce->Pfcb(),
		pverext->pgnoFirst,
		pverext->cpgSize );
	}


//  ================================================================
INLINE VOID VERIUndoRegisterCallback( const RCE * const prce )
//  ================================================================
//
//  Remove the callback from the list
//
//-
	{
	VERIRemoveCallback( prce );
	}


//  ================================================================
VOID VERIUndoUnregisterCallback( const RCE * const prce )
//  ================================================================
//
//  Set the trxUnregisterBegin0 in the CBDESC to trxMax
//
//-
	{
#ifdef VERSIONED_CALLBACKS
	Assert( prce->CbData() == sizeof(VERCALLBACK) );
	const VERCALLBACK* const pvercallback = (VERCALLBACK*)prce->PbData();
	CBDESC * const pcbdesc = pvercallback->pcbdesc;
	prce->Pfcb()->EnterDDL();
	Assert( trxMax != pcbdesc->trxRegisterBegin0 );
	Assert( trxMax != pcbdesc->trxRegisterCommit0 );
	Assert( trxMax != pcbdesc->trxUnregisterBegin0 );
	Assert( trxMax == pcbdesc->trxUnregisterCommit0 );
	pvercallback->pcbdesc->trxUnregisterBegin0 = trxMax;
#endif	//	VERSIONED_CALLBACKS
	prce->Pfcb()->LeaveDDL();
	}


//  ================================================================
INLINE VOID VERIUndoNonLoggedOper( PIB *ppib, RCE * const prce, RCE **pprceNextToUndo )
//  ================================================================
	{
	Assert( *pprceNextToUndo == prce->PrcePrevOfSession() );

	switch( prce->Oper() )
		{
		//	non-logged operations
		//
		case operAllocExt:
			VERIUndoAllocExt( prce );
			break;
		case operDeleteTable:
			VERIUndoDeleteTable( prce );
			break;
		case operAddColumn:
			VERIUndoAddColumn( prce );
			break;
		case operDeleteColumn:
			VERIUndoDeleteColumn( prce );
			break;
		case operCreateLV:
			VERIUndoCreateLV( ppib, prce );
			break;
		case operCreateIndex:
			VERIUndoCreateIndex( ppib, prce );
			//	refresh prceNextToUndo in case RCE list was
			//	updated when we lost critTrx
			*pprceNextToUndo = prce->PrcePrevOfSession();
			break;
		case operDeleteIndex:
			VERIUndoDeleteIndex( prce );
			break;
		case operRegisterCallback:
			VERIUndoRegisterCallback( prce );
			break;
		case operUnregisterCallback:
			VERIUndoUnregisterCallback( prce );
			break;
		case operReadLock:
		case operWriteLock:
			break;
		case operPreInsert:
			//	should never need to rollback an operPreInsert, because
			//	they are either promoted to an operInsert or manually nullified
			//	in the same transaction
			//
		default:
			Assert( fFalse );
			break;

		case operCreateTable:
			VERIUndoCreateTable( ppib, prce );
			// For CreateTable only, the RCE is nullified, so no need
			// to set RolledBack flag -- get out immediately.
			Assert( prce->FOperNull() );
			return;
		}

	Assert( !prce->FOperNull() );
	Assert( !prce->FRolledBack() );

	//  we have sucessfully undone the operation
	VERINullifyRolledBackRCE( ppib, prce );
	}


//  ================================================================
ERR ErrVERRollback( PIB *ppib )
//  ================================================================
//
//	Rollback is done in 2 phase. 1st phase is to undo the versioned
//	operation and may involve IO. 2nd phase is nullify the undone
//	RCE. 2 phases are needed so that the version will be held till all
//	IO is done, then wipe them all. If it is mixed, then once we undo
//	a RCE, the record become writable to other session. This may mess
//	up recovery where we may get write conflict since we have not guarrantee
//	that the log for operations on undone record won't be logged before
//	Rollback record is logged.
//	UNDONE: rollback should behave the same as commit, and need two phase log.
//
//-
	{
	ASSERT_VALID( ppib );
	Assert( ppib->level > 0 );

	const 	LEVEL	level				= ppib->level;
#ifdef DEBUG
			INT 	cRepeat = 0;
#endif

	ERR err = JET_errSuccess;

	ppib->critTrx.Enter();

	RCE	*prceToUndo;
	RCE	*prceNextToUndo;
	RCE	*prceToNullify;
	for( prceToUndo = ppib->prceNewest;
		prceNil != prceToUndo && prceToUndo->Level() == level;
		prceToUndo = prceNextToUndo )
		{
		Assert( !prceToUndo->FOperNull() );
		Assert( prceToUndo->Level() <= level );
		Assert( prceToUndo->TrxCommitted() == trxMax );
		Assert( prceToUndo->Pfcb() != pfcbNil );
		Assert( !prceToUndo->FRolledBack() );

#ifdef INDEPENDENT_DB_FAILURE
		// if we force detach
		// we have to check that we don't rollback operations
		// on other DBs.: it is not allowed to have operation on multiple databases
		if ( ifmpMax != ppib->m_ifmpForceDetach )
			{
			EnforceSz ( ppib->m_ifmpForceDetach == prceToUndo->Ifmp(),
				"Force detach not allowed if sessions are used cross-databases");
			}
#endif

		//  Save next RCE to process, because RCE will attempt to be
		//	nullified if undo is successful.
		prceNextToUndo = prceToUndo->PrcePrevOfSession();

		if ( prceToUndo->FUndoableLoggedOper() )
			{
			Assert( pfucbNil != prceToUndo->Pfucb() );
			Assert( ppib == prceToUndo->Pfucb()->ppib );
			Assert( JET_errSuccess == ppib->ErrRollbackFailure() );

			//	logged operations
			//

			err = ErrVERIUndoLoggedOper( ppib, prceToUndo );
			if ( err < JET_errSuccess )
				{
				// if due to an error we stopped a database usage
				// error out from the rollback
				// (the user will be able to ForceDetach the database
				if ( JET_errRollbackError == err )
					{
#ifdef INDEPENDENT_DB_FAILURE
					Assert ( rgfmp[ prceToUndo->Ifmp() ].FAllowForceDetach() );
#else
					Assert( ppib->ErrRollbackFailure() < JET_errSuccess );
#endif
					Call ( err );
					}
				else
					{
					Assert( ++cRepeat < 100 );
					prceNextToUndo = prceToUndo;
					continue;		//	sidestep resetting of cRepeat
					}
				Assert ( fFalse );
				}
			}

		else
			{
			// non-logged operations can never fail to rollback
			VERIUndoNonLoggedOper( ppib, prceToUndo, &prceNextToUndo );
			}

#ifdef DEBUG
		cRepeat = 0;
#endif
		}

	//	must loop through again and catch any RCE's that couldn't be nullified
	//	during the first pass because of concurrent create index
	prceToNullify = ppib->prceNewest;
	while ( prceNil != prceToNullify && prceToNullify->Level() == level )
		{
		// Only time nullification should have failed during the first pass is
		// if the RCE was locked for concurrent create index.  This can only
		// occur on a non-catalog, non-fixed DDL table for an Insert,
		// FlagDelete, or Replace operation.
		Assert( pfucbNil != prceToNullify->Pfucb() );
		Assert( ppib == prceToNullify->Pfucb()->ppib );
		Assert( !prceToNullify->FOperNull() );
		Assert( prceToNullify->FOperAffectsSecondaryIndex() );
		Assert( prceToNullify->FRolledBack() );
		Assert( prceToNullify->Pfcb()->FTypeTable() );
		Assert( !prceToNullify->Pfcb()->FFixedDDL() );
		Assert( !FCATSystemTable( prceToNullify->PgnoFDP() ) );

		RCE	*prceNextToNullify;
		VERINullifyRolledBackRCE( ppib, prceToNullify, &prceNextToNullify );

		prceToNullify = prceNextToNullify;
		}

	// If this PIB has any RCE's remaining, they must be at a lower level.
	Assert( prceNil == ppib->prceNewest
		|| ppib->prceNewest->Level() <= level );

	ppib->critTrx.Leave();

	//	decrement session transaction level
	Assert( level == ppib->level );
	if ( 1 == ppib->level )
		{
		//  we should have processed all RCEs
		Assert( prceNil == ppib->prceNewest );
		PIBSetPrceNewest( ppib, prceNil );			//	safety measure, in case it's not NULL for whatever reason
		}
	Assert( ppib->level > 0 );
	--ppib->level;

	//	rollback always succeeds
	return JET_errSuccess;

HandleError:
	ppib->critTrx.Leave();

	return err;
	}


//  ================================================================
ERR ErrVERRollback( PIB *ppib, UPDATEID updateid )
//  ================================================================
//
//  This is used to rollback the operations from one particular update
//  all levels are rolled back.
//
//	Rollback is done in 2 phase. 1st phase is to undo the versioned
//	operation and may involve IO. 2nd phase is nullify the undone
//	RCE. 2 phases are needed so that the version will be held till all
//	IO is done, then wipe them all. If it is mixed, then once we undo
//	a RCE, the record become writable to other session. This may mess
//	up recovery where we may get write conflict since we have not guarrantee
//	that the log for operations on undone record won't be logged before
//	Rollback record is logged.
//	UNDONE: rollback should behave the same as commit, and need two phase log.
//
//-
	{
	ASSERT_VALID( ppib );
	Assert( ppib->level > 0 );

	Assert( updateidNil != updateid );

#ifdef DEBUG
	const 	LEVEL	level	= ppib->level;
			INT 	cRepeat = 0;
#endif

	ERR err = JET_errSuccess;

	ppib->critTrx.Enter();

	RCE	*prceToUndo;
	RCE	*prceNextToUndo;

	for( prceToUndo = ppib->prceNewest;
		prceNil != prceToUndo && prceToUndo->TrxCommitted() == trxMax;
		prceToUndo = prceNextToUndo )
		{
		//  Save next RCE to process, because RCE will attemp to be nullified
		//	if undo is successful.
		prceNextToUndo = prceToUndo->PrcePrevOfSession();

		if ( prceToUndo->Updateid() == updateid )
			{
			Assert( pfucbNil != prceToUndo->Pfucb() );
			Assert( ppib == prceToUndo->Pfucb()->ppib );
			Assert( !prceToUndo->FOperNull() );
			Assert( prceToUndo->Pfcb() != pfcbNil );
			Assert( !prceToUndo->FRolledBack() );

			//	the only RCEs with an updateid should be DML RCE's.
			Assert( prceToUndo->FUndoableLoggedOper() );
			Assert( JET_errSuccess == ppib->ErrRollbackFailure() );

			err = ErrVERIUndoLoggedOper( ppib, prceToUndo );
			if ( err < JET_errSuccess )
				{
				// if due to an error we stopped a database usage
				// error out from the rollback
				// (the user will be able to ForceDetach the database
				if ( JET_errRollbackError == err )
					{
#ifdef INDEPENDENT_DB_FAILURE
					Assert ( rgfmp[ prceToUndo->Ifmp() ].FAllowForceDetach() );
#else
					Assert( ppib->ErrRollbackFailure() < JET_errSuccess );
#endif
					Call( err );
					}
				else
					{
					Assert( ++cRepeat < 100 );
					prceNextToUndo = prceToUndo;
					continue;		//	sidestep resetting of cRepeat
					}
				Assert ( fFalse );
				}
			}

#ifdef DEBUG
		cRepeat = 0;
#endif
		}


	//	must loop through again and catch any RCE's that couldn't be nullified
	//	during the first pass because of concurrent create index
	RCE	*prceToNullify;
	prceToNullify = ppib->prceNewest;
	while ( prceNil != prceToNullify && prceToNullify->TrxCommitted() == trxMax )
		{
		if ( prceToNullify->Updateid() == updateid )
			{
			// Only time nullification should have failed during the first pass is
			// if the RCE was locked for concurrent create index.  This can only
			// occur on a non-catalog, non-fixed DDL table for an Insert,
			// FlagDelete, or Replace operation.
			Assert( pfucbNil != prceToNullify->Pfucb() );
			Assert( ppib == prceToNullify->Pfucb()->ppib );
			Assert( !prceToNullify->FOperNull() );
			Assert( prceToNullify->FOperAffectsSecondaryIndex() );
			Assert( prceToNullify->FRolledBack() );
			Assert( prceToNullify->Pfcb()->FTypeTable() );
			Assert( !prceToNullify->Pfcb()->FFixedDDL() );
			Assert( !FCATSystemTable( prceToNullify->PgnoFDP() ) );

			RCE	*prceNextToNullify;
			VERINullifyRolledBackRCE( ppib, prceToNullify, &prceNextToNullify );

			prceToNullify = prceNextToNullify;
			}
		else
			{
			prceToNullify = prceToNullify->PrcePrevOfSession();
			}
		}

	ppib->critTrx.Leave();

	return JET_errSuccess;
HandleError:
	ppib->critTrx.Leave();

	return err;
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\upgrade.cxx ===
/*******************************************************************

Converting a database from ESE97 to ESE98 format

*******************************************************************/

#include "std.hxx"


#ifdef MINIMAL_FUNCTIONALITY
#else


//  ****************************************************************
//  STRUCTURES
//  ****************************************************************


//  ================================================================
struct UPGRADEPAGESTATS
//  ================================================================
	{
	LONG 		err;				//  error condition from the first thread to encounter an error
	LONG		cpgSeen;			//  total pages seen
	};


/*	long value column in old record format
/**/

PERSISTED
struct LV
	{
	union
		{
		BYTE	bFlags;
		struct
			{
			BYTE	fSeparated:1;
			BYTE	fSLVField:1;
			BYTE	fReserved:6;
			};
		}; 		// ATTENTION: the size of the union must remain 1 byte
	
	UnalignedLittleEndian< LID >		m_lid;
	};


//  ****************************************************************
//  CLASSES
//  ****************************************************************


//  ================================================================
class CONVERTPAGETASK : public DBTASK
//  ================================================================
	{
	public:
		CONVERTPAGETASK( const IFMP ifmp, const PGNO pgnoFirst, const CPG cpg, UPGRADEPAGESTATS * const pstats );
		virtual ~CONVERTPAGETASK();
		
		virtual ERR ErrExecute( PIB * const ppib );
		VOID HandleError( const ERR err );

	protected:
		const PGNO 		m_pgnoFirst;
		const CPG		m_cpg;

		UPGRADEPAGESTATS * const m_pstats;
		
	private:
		CONVERTPAGETASK( const CONVERTPAGETASK& );
		CONVERTPAGETASK& operator=( const CONVERTPAGETASK& );
	};
	

//  ================================================================
class CONVERTPAGETASKPOOL
//  ================================================================
	{
	public:
		CONVERTPAGETASKPOOL( const IFMP ifmp );
		~CONVERTPAGETASKPOOL();

		ERR ErrInit( PIB * const ppib, const INT cThreads );
		ERR ErrTerm();

		ERR ErrConvertPages( const PGNO pgnoFirst, const CPG cpg );

		const volatile UPGRADEPAGESTATS& Stats() const;
		
	private:
		CONVERTPAGETASKPOOL( const CONVERTPAGETASKPOOL& );
		CONVERTPAGETASKPOOL& operator=( const CONVERTPAGETASKPOOL& );
		
	private:
		const IFMP 	m_ifmp;
		TASKMGR		m_taskmgr;

		UPGRADEPAGESTATS	m_stats;
	};


//  ****************************************************************
//  PROTOTYPES
//  ****************************************************************


//	record conversion

VOID UPGRADECheckConvertNode(
	const VOID * const pvRecOld,
	const INT cbRecOld,
	const VOID * const pvRecNew,
	const INT cbRecNew );
ERR ErrUPGRADEConvertNode(
	CPAGE * const pcpage,
	const INT iline,
	VOID * const pvBuf );
ERR ErrUPGRADEConvertPage(
	CPAGE * const pcpage,
	VOID * const pvBuf );


//	perf counters

PM_CEF_PROC LUPGRADEPagesConvertedCEFLPv;
PM_CEF_PROC LUPGRADENodesConvertedCEFLPv;

//  ****************************************************************
//  GLOBAL VARIABLES
//  ****************************************************************

PERFInstanceGlobal<> cUPGRADEPagesConverted;
PERFInstanceGlobal<> cUPGRADENodesConverted;

//  ****************************************************************
//  FUNCTIONS
//  ****************************************************************


//  ================================================================
long LUPGRADEPagesConvertedCEFLPv( long iInstance, void* pvBuf )
//  ================================================================
	{
	cUPGRADEPagesConverted.PassTo( iInstance, pvBuf );
	return 0;
	}

//  ================================================================
long LUPGRADENodesConvertedCEFLPv( long iInstance, void* pvBuf )
//  ================================================================
	{
	cUPGRADENodesConverted.PassTo( iInstance, pvBuf );
	return 0;
	}


//  ================================================================
CONVERTPAGETASKPOOL::CONVERTPAGETASKPOOL( const IFMP ifmp ) :
	m_ifmp( ifmp )
//  ================================================================
	{
	m_stats.err					= JET_errSuccess;
	m_stats.cpgSeen 			= 0;	
	}


//  ================================================================
CONVERTPAGETASKPOOL::~CONVERTPAGETASKPOOL()
//  ================================================================
	{
	}


//  ================================================================
ERR CONVERTPAGETASKPOOL::ErrInit( PIB * const ppib, const INT cThreads )
//  ================================================================
	{
	ERR err;
	
	INST * const pinst = PinstFromIfmp( m_ifmp );

	Call( m_taskmgr.ErrInit( pinst, cThreads ) );
	
	return err;
	
HandleError:
	CallS( ErrTerm() );
	return err;	
	}


//  ================================================================
ERR CONVERTPAGETASKPOOL::ErrTerm()
//  ================================================================
	{
	ERR err;
	
	Call( m_taskmgr.ErrTerm() );

	err = m_stats.err;
	
HandleError:
	return err;
	}


//  ================================================================
ERR CONVERTPAGETASKPOOL::ErrConvertPages( const PGNO pgnoFirst, const CPG cpg )
//  ================================================================
	{
	CONVERTPAGETASK * ptask = new CONVERTPAGETASK( m_ifmp, pgnoFirst, cpg, &m_stats );
	if( NULL == ptask )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
	const ERR err = m_taskmgr.ErrPostTask( TASK::Dispatch, (ULONG_PTR)ptask );
	if( err < JET_errSuccess )
		{
		//  The task was not enqued sucessfully.
		delete ptask;
		}
	return err;	
	}


//  ================================================================
const volatile UPGRADEPAGESTATS& CONVERTPAGETASKPOOL::Stats() const
//  ================================================================
	{
	return m_stats;
	}
		

//  ================================================================
CONVERTPAGETASK::CONVERTPAGETASK(
	const IFMP ifmp,
	const PGNO pgnoFirst,
	const CPG cpg,
	UPGRADEPAGESTATS * const pstats ) :
//  ================================================================
	DBTASK( ifmp ),
	m_pgnoFirst( pgnoFirst ),
	m_cpg( cpg ),
	m_pstats( pstats )
	{
	//	don't fire off async tasks on the temp. database because the
	//	temp. database is simply not equipped to deal with concurrent access
	AssertRTL( dbidTemp != rgfmp[ifmp].Dbid() );
	}


//  ================================================================
CONVERTPAGETASK::~CONVERTPAGETASK()
//  ================================================================
	{
	}
		

//  ================================================================
ERR CONVERTPAGETASK::ErrExecute( PIB * const ppib )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	CSR csr;
	
	PGNO pgno;
	for( pgno = m_pgnoFirst; pgno < m_pgnoFirst + m_cpg; ++pgno )
		{
		AtomicExchangeAdd( &m_pstats->cpgSeen, 1 );
		
		err = csr.ErrGetRIWPage( ppib, m_ifmp, pgno );
		if( JET_errPageNotInitialized == err )
			{
			//	error is expected
			err = JET_errSuccess;
			continue;
			}
		Call( err );
		
///		Call( csr.ErrUpgrade() );
///		UNDONE:	reorganize the data on the page
///		Call( csr.Cpage().ReorganizeAndZero( 'C' ) );

		csr.ReleasePage( fTrue );
		csr.Reset();
		}

HandleError:
	return err;
	}


//  ================================================================
VOID CONVERTPAGETASK::HandleError( const ERR err )
//  ================================================================
	{
	AssertSz( fFalse, "Unable to run a CONVERTPAGETASK" );
	}


//  ================================================================
ERR ErrDBUTLConvertRecords( JET_SESID sesid, const JET_DBUTIL * const pdbutil )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	PIB * const ppib = reinterpret_cast<PIB *>( sesid );
	CONVERTPAGETASKPOOL * pconverttasks = NULL;
	
	Call( ErrIsamAttachDatabase( sesid, pdbutil->szDatabase, pdbutil->szSLV, pdbutil->szSLV, 0, NO_GRBIT ) );

	//	WARNING: must set ifmp to 0 to ensure high-dword is
	//	initialised on 64-bit, because we'll be casting this
	//	to a JET_DBID, which is a dword
	IFMP ifmp;
	ifmp = 0;
	Call( ErrIsamOpenDatabase(
		sesid,
		pdbutil->szDatabase,
		NULL,
		reinterpret_cast<JET_DBID *>( &ifmp ),
		NO_GRBIT
		) );

	PGNO pgnoLast;
	pgnoLast = rgfmp[ifmp].PgnoLast();

	pconverttasks = new CONVERTPAGETASKPOOL( ifmp );
	if( NULL == pconverttasks )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	Call( pconverttasks->ErrInit( ppib, CUtilProcessProcessor() ) );
	
	JET_SNPROG snprog;
	memset( &snprog, 0, sizeof( snprog ) );	
	snprog.cunitTotal 	= pgnoLast;
	snprog.cunitDone 	= 0;

	JET_PFNSTATUS pfnStatus;
	pfnStatus = reinterpret_cast<JET_PFNSTATUS>( pdbutil->pfnCallback );
	
	if ( NULL != pfnStatus )
		{
		(VOID)pfnStatus( sesid, JET_snpUpgradeRecordFormat, JET_sntBegin, NULL );	
		}

	PGNO pgno;
	pgno = 1;

	while( pgnoLast	>= pgno && JET_errSuccess == pconverttasks->Stats().err )
		{
		const CPG cpgChunk = 256;
		const CPG cpgPreread = min( cpgChunk, pgnoLast - pgno + 1 );
		BFPrereadPageRange( ifmp, pgno, cpgPreread );

		Call( pconverttasks->ErrConvertPages( pgno, cpgPreread ) );
		pgno += cpgPreread;

		snprog.cunitDone = pconverttasks->Stats().cpgSeen;
		if ( NULL != pfnStatus )
			{
			(VOID)pfnStatus( sesid, JET_snpUpgradeRecordFormat, JET_sntProgress, &snprog );
			}

		while( 	( pconverttasks->Stats().cpgSeen + ( cpgChunk * 4 ) ) < pgno
				&& JET_errSuccess == pconverttasks->Stats().err )
			{
			snprog.cunitDone = pconverttasks->Stats().cpgSeen;
			if ( NULL != pfnStatus )
				{
				(VOID)pfnStatus( sesid, JET_snpUpgradeRecordFormat, JET_sntProgress, &snprog );
				}
			UtilSleep( cmsecWaitGeneric );
			}
		}

	snprog.cunitDone = pconverttasks->Stats().cpgSeen;
	if ( NULL != pfnStatus )
		{
		(VOID)pfnStatus( sesid, JET_snpUpgradeRecordFormat, JET_sntProgress, &snprog );
		}

	Call( pconverttasks->ErrTerm() );
	
	if ( NULL != pfnStatus )
		{
		(VOID)pfnStatus( sesid, JET_snpUpgradeRecordFormat, JET_sntComplete, NULL );
		}
	
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d pages seen\n", pconverttasks->Stats().cpgSeen );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d pages converted\n", cUPGRADEPagesConverted.Get( PinstFromPpib( ppib ) ) );
	(*CPRINTFSTDOUT::PcprintfInstance())( "%d records converted\n", cUPGRADENodesConverted.Get( PinstFromPpib( ppib ) ) );

	err = pconverttasks->Stats().err;
	
	delete pconverttasks;
	pconverttasks = NULL;

	Call( err );
	
HandleError:
	if( NULL != pconverttasks )
		{
		const ERR errT = pconverttasks->ErrTerm();
		if( err >= 0 && errT < 0 )
			{
			err = errT;
			}
		delete pconverttasks;
		}
	return err;
	}


//  ================================================================
ERR ErrUPGRADEPossiblyConvertPage(
		CPAGE * const pcpage,
		VOID * const pvWorkBuf )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( pcpage->FNewRecordFormat() )
		{
		return JET_errSuccess;
		}
	
	if( pcpage->FPrimaryPage()
		&& !pcpage->FRepairedPage()
		&& pcpage->FLeafPage()
		&& !pcpage->FSpaceTree()
		&& !pcpage->FLongValuePage()
		&& !pcpage->FSLVAvailPage()
		&& !pcpage->FSLVOwnerMapPage() )
		{
		Call( ErrUPGRADEConvertPage( pcpage, pvWorkBuf ) );
		//	CONSIDER: return a warning saying the page was converted
		}
	else
		{
		//	this page doesn't need converting but we will flag it to avoid
		//	trying to convert in the future
		pcpage->SetFNewRecordFormat();
		BFDirty( pcpage->PBFLatch(), CPAGE::bfdfRecordUpgradeFlags );	
		}

HandleError:
	return err;
	}


//  ================================================================
LOCAL ERR ErrUPGRADECheckConvertNode(
	const VOID* const	pvRecOld,
	const SIZE_T		cbRecOld,
	const VOID* const	pvRecNew,
	const SIZE_T		cbRecNew )
//  ================================================================
	{
	const REC* const	precOld		= reinterpret_cast<const REC *>( pvRecOld );
	const REC* const	precNew		= reinterpret_cast<const REC *>( pvRecNew );

	if( cbRecOld < cbRecNew )
		{
		AssertSz( fFalse, "Record format conversion: the new record is larger" );
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
	
	if( precOld->FidFixedLastInRec() != precNew->FidFixedLastInRec() )
		{
		AssertSz( fFalse, "Record format conversion: fid fixed last changed" );
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	if( precOld->FidVarLastInRec() != precNew->FidVarLastInRec() )
		{
		AssertSz( fFalse, "Record format conversion: fid var last changed" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	if( precOld->IbEndOfFixedData() != precNew->IbEndOfFixedData() )
		{
		AssertSz( fFalse, "Record format conversion: end of fixed data changed" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	if( precOld->CbFixedNullBitMap() != precNew->CbFixedNullBitMap() )
		{
		AssertSz( fFalse, "Record format conversion: size of fixed null bitmap changed" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	if( precOld->IbEndOfVarData() != precNew->IbEndOfVarData() )
		{
		AssertSz( fFalse, "Record format conversion: end if var data changed" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	if( !!precOld->FTaggedData( cbRecOld ) != !!precNew->FTaggedData( cbRecNew ) )
		{
		AssertSz( fFalse, "Record format conversion: tagged data missing/added!" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}
		
	const TAGFLD_OLD* const		ptagfldoldMin		= reinterpret_cast<TAGFLD_OLD *>( precOld->PbTaggedData() );
	const TAGFLD_OLD* const		ptagfldoldMax		= reinterpret_cast<TAGFLD_OLD *>( (BYTE*)pvRecOld + cbRecOld );	
	const TAGFLD_OLD*			ptagfldold 			= ptagfldoldMin;

	DATA dataRecNew;
	dataRecNew.SetPv( const_cast<VOID *>( pvRecNew ) );
	dataRecNew.SetCb( cbRecNew );

	if( !TAGFIELDS::FIsValidTagfields( dataRecNew, CPRINTFNULL::PcprintfInstance() ) )
		{
		AssertSz( fFalse, "Record format conversion: new record is not valid" );		
		return ErrERRCheck( JET_errRecordFormatConversionFailed );
		}


	//	sadly, the code below can't deal with derived/non-derived columns
	/*
	

	TAGFIELDS tagfields( dataRecNew );
	
	COLUMNID columnidPrev = 0;
	INT itagSequence = 1;
	for( ; ptagfldold < ptagfldoldMax; ptagfldold = ptagfldold->PtagfldNext() )
		{
		const FID 			fid			= ptagfldold->Fid();
		const COLUMNID		columnid	= ColumnidOfFid( fid, !ptagfldold->FDerived() );
		const VOID * const 	pvDataOld 	= ptagfldold->Rgb() + !!ptagfldold->FLongValue();
		const INT 			cbDataOld 	= ptagfldold->CbData() - !!ptagfldold->FLongValue();
		
		DATA 				dataOld;
		DATA				dataNew;
		
		dataOld.SetPv( const_cast<VOID *>( pvDataOld ) );
		dataOld.SetCb( cbDataOld );

		if( columnid == columnidPrev )
			{
			++itagSequence;
			}
		else
			{
			columnidPrev 	= columnid;
			itagSequence	= 1;
			}

		DATA dataNewRec;

		const JET_ERR err = tagfields.ErrRetrieveColumn(
				pfcbNil,
				columnid,
				itagSequence,
				dataRecNew,
				&dataNew,
				JET_bitRetrieveIgnoreDefault );
						
		AssertRtl( dataNew.Cb() == dataOld.Cb() );
		AssertRtl( 0 == memcmp( dataNew.Pv(), dataOld.Pv(), dataNew.Cb() ) );
		}

	*/
	
	return JET_errSuccess;
	}


//  ================================================================
ERR ErrUPGRADEConvertNode(
	CPAGE * const pcpage,
	const INT iline,
	VOID * const pvBuf )
//  ================================================================
//
//	+------+-----+------+-----+-----+------+-----+-------+-------+-----+-------+
//	| fid1 | ib1 | fid2 | ib2 | ... | fidn | ibn | data1 | data2 | ... | datan |
//	+------+-----+------+-----+-----+------+-----+-------+-------+-----+-------+
//     2B    2B     2B    2B           2B    2B
//
//	The high bits of the ib's are used to store derived, extended info byte and NULL
//	bits
//
//-
	{
	ERR	err = JET_errSuccess;

	Assert( !pcpage->FNewRecordFormat() );

	//	get the record from the page
	
	KEYDATAFLAGS kdf;
	NDIGetKeydataflags( *pcpage, iline, &kdf );

	if( FNDDeleted( kdf ) )
		{
		return JET_errSuccess;
		}
		
	const INT cbRec				= kdf.data.Cb();
	const BYTE * const pbRec	= reinterpret_cast<const BYTE *>( kdf.data.Pv() );
	const BYTE * const pbRecMax = reinterpret_cast<const BYTE *>( kdf.data.Pv() ) + cbRec;

	const REC * const prec = reinterpret_cast<const REC *>( pbRec );

	//	how much tagged and non-tagged data is there
	
	const SIZE_T cbNonTaggedData 	= prec->PbTaggedData() - pbRec;
	const SIZE_T cbTaggedData 		= cbRec - cbNonTaggedData;

	//	go through the old-format TAGFLDs. How many are there? Are there multivalues?

	const TAGFLD_OLD * const ptagfldoldMin		= reinterpret_cast<const TAGFLD_OLD *>( prec->PbTaggedData() );
	const TAGFLD_OLD * const ptagfldoldMax		= reinterpret_cast<const TAGFLD_OLD *>( pbRecMax );	
	const TAGFLD_OLD * 	ptagfldold 				= NULL;
	
	BOOL				fRecordHasMultivalues	= fFalse;
	INT 				cTAGFLD 				= 0;		//	number of unique multi-values
	
	FID					fidPrev 				= 0;
	for( ptagfldold = ptagfldoldMin; ptagfldold < ptagfldoldMax; ptagfldold = ptagfldold->PtagfldNext() )
		{
		if( ptagfldold->Fid() == fidPrev )
			{
			fRecordHasMultivalues = fTrue;
			}
		else
			{
			fidPrev = ptagfldold->Fid();
			++cTAGFLD;
			}
		}

	//	copy in the non-tagged data
	
	BYTE * const pb	= reinterpret_cast<BYTE *>( pvBuf );
	memcpy( pb, pbRec, cbNonTaggedData );

	//	create the TAGFLD array and copy in the data

	BYTE * const pbTagfldsStart = pb + cbNonTaggedData;
	BYTE * pbTagflds 			= pbTagfldsStart;
	BYTE * const pbDataStart 	= pbTagfldsStart + ( cTAGFLD * sizeof(TAGFLD) );
	BYTE * pbData				= pbDataStart;

	INT ibCurr = cTAGFLD * sizeof(TAGFLD);

	ptagfldold = ptagfldoldMin;
	while( ptagfldold < ptagfldoldMax )
		{		
		static const USHORT fDerived		= 0x8000;		//	if TRUE, then current column is derived from a template
		static const USHORT fExtendedInfo	= 0x4000;		//	if TRUE, must go to TAGFLD_HEADER byte to check more flags
		static const USHORT fNull			= 0x2000;		//	if TRUE, column set to NULL to override default value
		
		const FID fid 							= ptagfldold->Fid();
		const TAGFLD_OLD * const ptagfldoldNext = ptagfldold->PtagfldNext();
		
		if( ptagfldoldNext < ptagfldoldMax
			&& ptagfldoldNext->Fid() == fid
			&& ptagfldoldNext->FDerived() == ptagfldold->FDerived() )
			{			
			const TAGFLD_OLD * 	ptagfldoldNextFid 	= ptagfldold;
			INT 				cMULTIVALUES 		= 0;
			INT 				cbDataTotal			= 0;
			
			while( 	ptagfldoldNextFid < ptagfldoldMax
					&& ptagfldoldNextFid->Fid() == fid )
				{
				++cMULTIVALUES;
				cbDataTotal += ptagfldoldNextFid->CbData();
				ptagfldoldNextFid = ptagfldoldNextFid->PtagfldNext();
				}

			if( 2 == cMULTIVALUES
				&& !ptagfldold->FLongValue() )
				{
				//	convert to the TWOVALUES format
				//
				//	+---------------+---------------+-----------+-----------+
				//	| extended info | cbData1       | data1 ... | data2 ... |
				//	+---------------+---------------+-----------+-----------+
				//            1B             1B       
				//
				//	note that the length is one byte (non-lv columns
				//	are limited to 255 bytes)
				
				Assert( !ptagfldold->FLongValue() );
				Assert( !ptagfldoldNext->FLongValue() );
				Assert( !ptagfldold->FNull() );
				Assert( !ptagfldoldNext->FNull() );

				//	create the entry in the TAGFLD array

				USHORT ibFlags		= fExtendedInfo;
				if( ptagfldold->FDerived() )
					{
					ibFlags |= fDerived;
					}
			
				const USHORT ib	= static_cast<USHORT>( ibCurr | ibFlags );

				*((UnalignedLittleEndian<USHORT> *)pbTagflds) = fid;
				pbTagflds += sizeof( USHORT );
				*((UnalignedLittleEndian<USHORT> *)pbTagflds) = ib;
				pbTagflds += sizeof( USHORT );
			
				ibCurr += sizeof( TAGFLD_HEADER ) + sizeof( BYTE ) + cbDataTotal;

				//	set extended info to say we are in the twovalues format

				*pbData = 0;
				TAGFLD_HEADER * const ptagfldheader = reinterpret_cast<TAGFLD_HEADER *>( pbData );
				ptagfldheader->SetFTwoValues();				
				ptagfldheader->SetFMultiValues();				
				pbData += sizeof( TAGFLD_HEADER );
				
				//	set cbData1

				*pbData = (BYTE)ptagfldold->CbData();
				pbData += sizeof( BYTE );
				
				//	copy in the data
			
				memcpy( pbData, ptagfldold->Rgb(), ptagfldold->CbData() );
				pbData 		+= ptagfldold->CbData();

				memcpy( pbData, ptagfldoldNext->Rgb(), ptagfldoldNext->CbData() );
				pbData 		+= ptagfldoldNext->CbData();
				}
			else
				{
				//	convert to the MULTIVALUES format
				//
				//	+-------------------+------------+-----+-----------+-----------+-----------+-----+-----------+
				//	| info |     ib1    |     ib2    | ... |    ibn    | data1 ... | data2 ... | ... | datan ... |
				//	+-------------------+------------+-----+-----------+-----------+-----------+-----+-----------+
				//	   1B        2B           2B                2B
				//
				//
				//	if the data is a separated LV the high bit of its ib will be set
					
				//	create the entry in the TAGFLD array
				
				USHORT ibFlags		= fExtendedInfo;
				if( ptagfldold->FDerived() )
					{
					ibFlags |= fDerived;
					}
			
				const USHORT ib	= static_cast<USHORT>( ibCurr | ibFlags );

				*((UnalignedLittleEndian<USHORT> *)pbTagflds) = fid;
				pbTagflds += sizeof( USHORT );
				*((UnalignedLittleEndian<USHORT> *)pbTagflds) = ib;
				pbTagflds += sizeof( USHORT );

				if( !ptagfldold->FLongValue() )
					{
					ibCurr += sizeof( TAGFLD_HEADER ) + ( sizeof( USHORT ) * cMULTIVALUES ) + cbDataTotal;
					}
				else
					{
					//	we will be losing the header byte from the LV structure
					
					ibCurr += sizeof( TAGFLD_HEADER ) + ( sizeof( USHORT ) * cMULTIVALUES ) + cbDataTotal - ( sizeof( BYTE ) * cMULTIVALUES );
					}

				//	set extended info to say we are in the multivalues format

				*pbData = 0;
				TAGFLD_HEADER * const ptagfldheader = reinterpret_cast<TAGFLD_HEADER *>( pbData );
				ptagfldheader->SetFMultiValues();				
				if( ptagfldold->FLongValue() )
					{
					const LV * const plv = reinterpret_cast<const LV *>( ptagfldold->Rgb() );					
					if( plv->fSLVField )
						{
						ptagfldheader->SetFSLV();
						}
					else
						{
						ptagfldheader->SetFLongValue();
						}
					}
				pbData += sizeof( TAGFLD_HEADER );				

				//	first make space for the ibOffsets

				INT		ibOffsetCurr = sizeof( USHORT ) * cMULTIVALUES;
				BYTE * pbIbOffsets = pbData;
				pbData += sizeof( USHORT ) * cMULTIVALUES;

				const TAGFLD_OLD * 	ptagfldoldT 	= NULL;
				for( 	ptagfldoldT = ptagfldold;
						ptagfldoldT < ptagfldoldNextFid;
						ptagfldoldT = ptagfldoldT->PtagfldNext() )
					{
					Assert( ptagfldoldT->Fid() == fid );
					if( ptagfldoldT->FLongValue() )
						{												

						//	do we need to set the separated bit?
						
						const LV * const plv = reinterpret_cast<const LV *>( ptagfldoldT->Rgb() );
						if( plv->fSeparated )
							{
							static const USHORT fSeparatedInstance = 0x8000;					
							*((UnalignedLittleEndian<USHORT> *)pbIbOffsets) = static_cast<USHORT>( ibOffsetCurr | fSeparatedInstance );
							}
						else
							{
							*((UnalignedLittleEndian<USHORT> *)pbIbOffsets) = static_cast<USHORT>( ibOffsetCurr );
							}
						pbIbOffsets += sizeof( USHORT );					

						memcpy( pbData, ptagfldoldT->Rgb() + sizeof( BYTE ), ptagfldoldT->CbData() - sizeof( BYTE ) );
						ibOffsetCurr += ptagfldoldT->CbData() - sizeof( BYTE );
						pbData += ptagfldoldT->CbData() - sizeof( BYTE );
						}
					else
						{
						*((UnalignedLittleEndian<USHORT> *)pbIbOffsets) = static_cast<USHORT>( ibOffsetCurr );
						pbIbOffsets += sizeof( USHORT );					
						memcpy( pbData, ptagfldoldT->Rgb(), ptagfldoldT->CbData() );
						ibOffsetCurr += ptagfldoldT->CbData();
						pbData += ptagfldoldT->CbData();
						}
					}

				}
				
			ptagfldold 	= ptagfldoldNextFid;
			}
		else
			{
			USHORT ibFlags		= 0;
			if( ptagfldold->FNull() )
				{
				ibFlags |= fNull;
				}
			else
				{
				if( ptagfldold->FLongValue() )
					{
					ibFlags |= fExtendedInfo;	//	LVs have a header byte so the length isn't changed
					}
				}
				
			if( ptagfldold->FDerived() )
				{
				ibFlags |= fDerived;
				}
			
			const USHORT ib	= static_cast<USHORT>( ibCurr | ibFlags );

			*((UnalignedLittleEndian<USHORT> *)pbTagflds) = fid;
			pbTagflds += sizeof( USHORT );
			*((UnalignedLittleEndian<USHORT> *)pbTagflds) = ib;
			pbTagflds += sizeof( USHORT );
			
			ibCurr += ptagfldold->CbData();

			//	copy in the data
			
			memcpy( pbData, ptagfldold->Rgb(), ptagfldold->CbData() );
			
			//	set the extended info byte
			//	in this pass we only do it for LVs

			if( ibFlags & fExtendedInfo )
				{
				Assert( ptagfldold->FLongValue() );

				const LV * const plv = reinterpret_cast<const LV *>( ptagfldold->Rgb() );
				*pbData = 0;
				TAGFLD_HEADER * const ptagfldheader = reinterpret_cast<TAGFLD_HEADER *>( pbData );

				if( plv->fSLVField )
					{
					ptagfldheader->SetFSLV();
					}
				else
					{
					ptagfldheader->SetFLongValue();
					}
					
				if( plv->fSeparated )
					{
					ptagfldheader->SetFSeparated();
					}
				}

			pbData 		+= ptagfldold->CbData();
			ptagfldold 	= ptagfldoldNext;
			}
		}
		
	const BYTE* const	pbMax		= pbData;
	const SIZE_T		cbRecNew	= pbMax - pb;

	CallR( ErrUPGRADECheckConvertNode( kdf.data.Pv(), kdf.data.Cb(), pvBuf, cbRecNew ) );

	DATA data;
	data.SetPv( pvBuf );
	data.SetCb( cbRecNew );
	
	NDReplaceForUpgrade( pcpage, iline, &data, kdf );

	CallS( err );
	return err;
	}


//  ================================================================
ERR ErrUPGRADEConvertPage(
	CPAGE * const pcpage,
	VOID * const pvBuf )
//  ================================================================
	{
	ERR				err		= JET_errSuccess;
	INST * const	pinst	= PinstFromIfmp( pcpage->Ifmp() );

	Assert( NULL != pinst );
	Assert( !pcpage->FNewRecordFormat() );

#ifdef DEBUG
	memset( pvBuf, 0xff, g_cbPage );
#endif	//	DEBUG

	INT iline;
	for( iline = 0; iline < pcpage->Clines(); ++iline )
		{
		Call( ErrUPGRADEConvertNode( pcpage, iline, pvBuf ) );
		}

	pcpage->SetFNewRecordFormat();
	BFDirty( pcpage->PBFLatch(), CPAGE::bfdfRecordUpgradeFlags );

	cUPGRADENodesConverted.Add( pinst, pcpage->Clines() );
	cUPGRADEPagesConverted.Inc( pinst );

HandleError:
	if( err < 0 )
		{
		if( JET_errRecordFormatConversionFailed == err )
			{

			//	eventlog this failure

			const CHAR * rgsz[3];
			INT isz = 0;
			
			const OBJID objid = pcpage->ObjidFDP();			
			CHAR szObjid[16];
			sprintf( szObjid, "%d", objid );
			
			const PGNO pgno = pcpage->Pgno();
			CHAR szPgno[16];
			sprintf( szPgno, "%d", pgno );

			CHAR szIline[16];
			sprintf( szIline, "%d", iline );
			
			rgsz[isz++] = szObjid;
			rgsz[isz++] = szPgno;
			rgsz[isz++] = szIline;

			Assert( isz == sizeof( rgsz ) / sizeof( rgsz[0] ) );

			UtilReportEvent(
				eventError,
				CONVERSION_CATEGORY,
				RECORD_FORMAT_CONVERSION_FAILED_ID,
				isz,
				rgsz,
				0,
				NULL,
				pinst );				
			
			}
		else
			{

			//	we only expect the above error

			Assert( fFalse );

			}
		}
	else
		{
		CallS( err );
		Assert( pcpage->FNewRecordFormat() );
		}
	return err;
	}

#endif	//	MINIMAL_FUNCTIONALITY
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\util.cxx ===
#include "std.hxx"

#ifdef DEBUG
#include <stdarg.h>
#endif

VOID UtilLoadDbinfomiscFromPdbfilehdr(
	JET_DBINFOMISC *pdbinfomisc,
	DBFILEHDR_FIX *pdbfilehdr )
	{
	pdbinfomisc->ulVersion			= pdbfilehdr->le_ulVersion;
	pdbinfomisc->dbstate			= pdbfilehdr->le_dbstate;				
	pdbinfomisc->signDb				= *(JET_SIGNATURE *) &(pdbfilehdr->signDb);	
	pdbinfomisc->lgposConsistent	= *(JET_LGPOS *) &(pdbfilehdr->le_lgposConsistent);
	pdbinfomisc->logtimeConsistent	= *(JET_LOGTIME *) &(pdbfilehdr->logtimeConsistent);	
	pdbinfomisc->logtimeAttach		= *(JET_LOGTIME *) &(pdbfilehdr->logtimeAttach);
	pdbinfomisc->lgposAttach		= *(JET_LGPOS *) &(pdbfilehdr->le_lgposAttach);
	pdbinfomisc->logtimeDetach		= *(JET_LOGTIME *) &(pdbfilehdr->logtimeDetach);
	pdbinfomisc->lgposDetach		= *(JET_LGPOS *) &(pdbfilehdr->le_lgposDetach);
	pdbinfomisc->signLog			= *(JET_SIGNATURE *) &(pdbfilehdr->signLog);
	pdbinfomisc->bkinfoFullPrev		= *(JET_BKINFO *) &(pdbfilehdr->bkinfoFullPrev);
	pdbinfomisc->bkinfoIncPrev		= *(JET_BKINFO *) &(pdbfilehdr->bkinfoIncPrev);
	pdbinfomisc->bkinfoFullCur		= *(JET_BKINFO *) &(pdbfilehdr->bkinfoFullCur);
	pdbinfomisc->fShadowingDisabled	= pdbfilehdr->FShadowingDisabled();
	pdbinfomisc->dwMajorVersion		= pdbfilehdr->le_dwMajorVersion;
	pdbinfomisc->dwMinorVersion		= pdbfilehdr->le_dwMinorVersion;
	pdbinfomisc->dwBuildNumber		= pdbfilehdr->le_dwBuildNumber;
	pdbinfomisc->lSPNumber			= pdbfilehdr->le_lSPNumber;
	pdbinfomisc->ulUpdate			= pdbfilehdr->le_ulUpdate;
	pdbinfomisc->cbPageSize			= pdbfilehdr->le_cbPageSize == 0 ? (ULONG) g_cbPageDefault : (ULONG) pdbfilehdr->le_cbPageSize;
	}
	

CODECONST(unsigned char) mpcoltypcb[] =
	{
	0,					/* JET_coltypNil (coltypNil is used for vltUninit parms) */
	sizeof(char),		/* JET_coltypBit */
	sizeof(char),		/* JET_coltypUnsignedByte */
	sizeof(short),		/* JET_coltypShort */
	sizeof(long),		/* JET_coltypLong */
	sizeof(long)*2,		/* JET_coltypCurrency */
	sizeof(float),		/* JET_coltypIEEESingle */
	sizeof(double),		/* JET_coltypIEEEDouble */
	sizeof(double),		/* JET_coltypDateTime */
	0,					/* JET_coltypBinary */
	0,					/* JET_coltypText */
	sizeof(long),		/* JET_coltypLongBinary */
	sizeof(long),		/* JET_coltypLongText */
	0,					/* JET_coltypDatabase */
	sizeof(JET_TABLEID)	/* JET_coltypTableid */
	};


LOCAL CODECONST(unsigned char) rgbValidName[16] = {
	0xff,			       /* 00-07 No control characters */
	0xff,			       /* 08-0F No control characters */
	0xff,			       /* 10-17 No control characters */
	0xff,			       /* 18-1F No control characters */
	0x02,			       /* 20-27 No ! */
	0x40,			       /* 28-2F No . */
	0x00,			       /* 30-37 */
	0x00,			       /* 38-3F */
	0x00,			       /* 40-47 */
	0x00,			       /* 48-4F */
	0x00,			       /* 50-57 */
	0x28,			       /* 58-5F No [ or ] */
	0x00,			       /* 60-67 */
	0x00,			       /* 68-6F */
	0x00,			       /* 70-77 */
	0x00,			       /* 78-7F */
	};

//	WARNING: Assumes an output buffer of JET_cbNameMost+1
//
ERR ErrUTILICheckName(
	CHAR * const		szNewName,
	const CHAR * const	szName, 
	const BOOL			fTruncate )
	{
	CHAR *				pchLast		= szNewName;
	SIZE_T				cch;
	BYTE				ch;

	//	a name may not begin with a space
	if ( ' ' == *szName )
		return ErrERRCheck( JET_errInvalidName );

	for ( cch = 0;
		cch < JET_cbNameMost && ( ( ch = (BYTE)szName[cch] ) != '\0' );
		cch++ )
		{
		//	extended characters always valid
		if ( ch < 0x80 )
			{
			if ( ( rgbValidName[ch >> 3] >> (ch & 0x7) ) & 1 )
				return ErrERRCheck( JET_errInvalidName );
			}

		szNewName[cch] = (CHAR)ch;

		//	last significant character
		if ( ' ' != ch )
			pchLast = szNewName + cch + 1;
		}

	//	check name too long
	//	UNDONE: insignificant trailing spaces that cause
	//	the length of the name to exceed cbNameMost will
	//	currently trigger an error
	if ( JET_cbNameMost == cch )
		{
		if ( !fTruncate && '\0' != szName[JET_cbNameMost] )
			return ErrERRCheck( JET_errInvalidName );
		}

	//	length of significant portion
	Assert( pchLast >= szNewName );
	Assert( pchLast <= szNewName + JET_cbNameMost );
	cch = pchLast - szNewName;

	if ( 0 == cch )
		return ErrERRCheck( JET_errInvalidName );

	//	we assume an output buffer of JET_cbNameMost+1
	Assert( cch <= JET_cbNameMost );
	szNewName[cch] = '\0';

	return JET_errSuccess;
	}


//	WARNING: Assumes an output buffer of IFileSystemAPI::cchPathMax
//
ERR ErrUTILICheckPathName(
	CHAR * const		szNewName,
	const CHAR * const	szName, 
	const BOOL			fTruncate )
	{
	SIZE_T				ichT;

	//	path may not begin with a space
	//
	if ( ' ' == *szName )
		{
		return ErrERRCheck( JET_errInvalidPath );
		}

	for ( ichT = 0;
		( ( ichT < IFileSystemAPI::cchPathMax ) && ( szName[ichT] != '\0' ) );
		ichT++ )
		{
			szNewName[ichT] = szName[ichT];
		}

	//	check for empty path
	//
	if ( 0 == ichT )
		{
		return ErrERRCheck( JET_errInvalidPath );
		}	

	//	check name too long
	//
	//	FUTURE: insignificant trailing spaces
	//	that cause the length of the name to exceed IFileSystemAPI::cchPathMax
	//	will currently trigger an error.
	//
	if ( IFileSystemAPI::cchPathMax == ichT )
		{
		if ( !fTruncate )
			{
			return ErrERRCheck( JET_errInvalidPath );
			}
		else
			{
			ichT = IFileSystemAPI::cchPathMax - 1;
			}
		}

	//	we assume an output buffer of IFileSystemAPI::cchPathMax
	//
	Assert( ichT < IFileSystemAPI::cchPathMax );
	szNewName[ichT] = '\0';

	return JET_errSuccess;
	}


#ifdef DEBUG

typedef void ( *PFNvprintf)(const char  *, va_list);

struct {
	PFNvprintf pfnvprintf;
	}  pfn = { NULL };


void VARARG DebugPrintf(const char  *szFmt, ...)
	{
	va_list arg_ptr;

	if (pfn.pfnvprintf == NULL)	       /* No op if no callback registered */
		return;

	va_start(arg_ptr, szFmt);
	(*pfn.pfnvprintf)(szFmt, arg_ptr);
	va_end(arg_ptr);
	}


	/*	The following pragma affects the code generated by the C
	/*	compiler for all FAR functions.  Do NOT place any non-API
	/*	functions beyond this point in this file.
	/**/

void JET_API JetDBGSetPrintFn(JET_SESID sesid, PFNvprintf pfnParm)
	{
	Unused( sesid );
	
	pfn.pfnvprintf = pfnParm;
	}

/*
 *	level 0 - all log s.
 *	level 1 - log read and update operations.
 *	level 2 - log update operations only.
 *	level 99- never log
 */

LOCAL CODECONST(unsigned char) mpopLogLevel[opMax] = {
/*							0	*/		0,
/*  opIdle					1	*/		2,
/*	opGetTableIndexInfo		2	*/		1,
/*	opGetIndexInfo			3	*/		1,
/*	opGetObjectInfo			4	*/		1,
/*	opGetTableInfo			5	*/		1,
/*	opCreateObject			6	*/		2,
/*	opDeleteObject			7	*/		2,
/*	opRenameObject			8	*/		2,
/*	opBeginTransaction		9	*/		2,
/*	opCommitTransaction		10	*/		2,
/*	opRollback				11	*/		2,
/*	opOpenTable				12	*/		1,
/*	opDupCursor				13	*/		1,
/*	opCloseTable			14	*/		1,
/*	opGetTableColumnInfo	15	*/		1,
/*	opGetColumnInfo			16	*/		1,
/*	opRetrieveColumn		17	*/		1,
/*	opRetrieveColumns		18	*/		1,
/*	opSetColumn				19	*/		2,
/*	opSetColumns			20	*/		2,
/*	opPrepareUpdate			21	*/		2,
/*	opUpdate				22	*/		2,
/*	opDelete				23	*/		2,
/*	opGetCursorInfo			24	*/		1,
/*	opGetCurrentIndex		25	*/		1,
/*	opSetCurrentIndex		26	*/		1,
/*	opMove					27	*/		1,
/*	opMakeKey				28	*/		1,
/*	opSeek					29	*/		1,
/*	opGetBookmark			30	*/		1,
/*	opGotoBookmark			31	*/		1,
/*	opGetRecordPosition		32	*/		1,
/*	opGotoPosition			33	*/		1,
/*	opRetrieveKey			34	*/		1,
/*	opCreateDatabase		35	*/		2,
/*	opOpenDatabase			36	*/		1,
/*	opGetDatabaseInfo		37	*/		1,
/*	opCloseDatabase			38	*/		1,
/*	opCapability			39	*/		1,
/*	opCreateTable			40	*/		2,
/*	opRenameTable			41	*/		2,
/*	opDeleteTable			42	*/		2,
/*	opAddColumn				43	*/		2,
/*	opRenameColumn			44	*/		2,
/*	opDeleteColumn			45	*/		2,
/*	opCreateIndex			46	*/		2,
/*	opRenameIndex			47	*/		2,
/*	opDeleteIndex			48	*/		2,
/*	opComputeStats			49	*/		2,
/*	opAttachDatabase		50	*/		2,
/*	opDetachDatabase		51	*/		2,
/*	opOpenTempTable			52	*/		2,
/*	opSetIndexRange			53	*/		1,
/*	opIndexRecordCount		54	*/		1,
/*	opGetChecksum			55	*/		1,
/*	opGetObjidFromName		56	*/		1,
/*	opEscrowUpdate			57	*/		1,
/*	opGetLock				58	*/		1,
/*	opRetrieveTaggedColumnList	59	*/	1,
/*	opCreateTableColumnIndex	60	*/	2,
/*	opSetColumnDefaultValue	61	*/		2,
/*	opPrepareToCommitTransaction 62 */	2,
/*	opSetTableSequential	63	*/		99,
/*	opResetTableSequential	64	*/		99,
/*	opRegisterCallback		65	*/		99,
/*	opUnregisterCallback	66	*/		99,
/*	opSetLS					67	*/		99,
/*	opGetLS					68	*/		99,
/*	opGetVersion			69	*/		99,
/*	opBeginSession			70	*/		99,
/*	opDupSession			71	*/		99,
/*	opEndSession			72	*/		99,
/*	opBackupInstance		73	*/		99,
/*	opBeginExternalBackupInstance 74 */	99,
/*	opGetAttachInfoInstance	75	*/		99,
/*	opOpenFileInstance		76	*/		99,
/*	opReadFileInstance		77	*/		99,
/*	opCloseFileInstance		78	*/		99,
/*	opGetLogInfoInstance	79	*/		99,
/*	opGetTruncateLogInfoInstance 80 */	99,
/*	opTruncateLogInstance	81	*/		99,
/*	opEndExternalBackupInstance	82	*/	99,
/*	opSnapshotStart			83	*/		99,
/*	opSnapshotStop			84	*/		99,
/*	opResetCounter			85	*/		99,
/*	opGetCounter			86	*/		99,
/*	opCompact				87	*/		99,
/*	opConvertDDL			88	*/		99,
/*	opUpgradeDatabase		89	*/		99,
/*	opDefragment			90	*/		99,
/*	opSetDatabaseSize		91	*/		99,
/*	opGrowDatabase			92	*/		99,
/*	opSetSessionContext		93	*/		99,
/*	opResetSessionContext	94	*/		99,
/*	opSetSystemParameter	95	*/		99,
/*	opGetSystemParameter	96	*/		99,
/*	opTerm					97	*/		99,
/*	opInit					98	*/		99,
/*	opIntersectIndexes		99	*/		99,
/*	opDBUtilities			100	*/		99,
/*	opEnumerateColumns		101	*/		1,
};

/* function in logapi to store jetapi calls */
extern void LGJetOp( JET_SESID sesid, int op );

void DebugLogJetOp( JET_SESID sesid, int op )
	{
	Unused( sesid );
	
	// UNDONE: should be controlled by a system parameter to decide
	// UNDONE: which log level it should be.

	/* log level 2 operations */
	if ( op < opMax && mpopLogLevel[ op ] >= lAPICallLogLevel )
		{
//		LGJetOp( sesid, op );
		}
	}

#endif	/* DEBUG */
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\ttmap.cxx ===
#include "std.hxx"

//  ================================================================
TTMAP::TTMAP( PIB * const ppib ) :
//  ================================================================
	m_tableid( JET_tableidNil ),
	m_sesid( reinterpret_cast<JET_SESID>( ppib ) ),
	m_columnidKey( 0 ),
	m_columnidValue( 0 ),
	m_crecords( 0 )
	{
	}


//  ================================================================
TTMAP::~TTMAP()
//  ================================================================
	{
	if( JET_tableidNil != m_tableid )
		{
		CallS( ErrDispCloseTable( m_sesid, m_tableid ) );
		m_tableid = JET_tableidNil;
		}
	}


//  ================================================================
ERR TTMAP::ErrInit( INST * const )
//  ================================================================
	{
	ERR err = JET_errSuccess;
	
	JET_COLUMNDEF	rgcolumndef[2] = {
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypLong, 0, 0, 0, 0, JET_cbKeyMost, JET_bitColumnTTKey },  //  Key
		{ sizeof( JET_COLUMNDEF ), 0, JET_coltypLong, 0, 0, 0, 0, 0, 0 },								//  Value
		};	
	JET_COLUMNID	rgcolumnid[sizeof(rgcolumndef)/sizeof(JET_COLUMNDEF)];
	
	CallR( ErrIsamOpenTempTable(
		m_sesid,
		rgcolumndef,
		sizeof(rgcolumndef)/sizeof(JET_COLUMNDEF),
		0,
		JET_bitTTIndexed | JET_bitTTUnique | JET_bitTTScrollable | JET_bitTTUpdatable,
		&m_tableid,
		rgcolumnid ) );

	m_columnidKey 		= rgcolumnid[0];
	m_columnidValue 	= rgcolumnid[1];

	return err;
	}


//  ================================================================
ERR TTMAP::ErrInsertKeyValue_( const ULONG ulKey, const ULONG ulValue )
//  ================================================================
	{
	ERR	err = JET_errSuccess;

	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}
	
	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );
	Call( ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepInsert ) );

	Call( ErrDispSetColumn(
				m_sesid, 
				m_tableid, 
				m_columnidKey,
				(BYTE *)&ulKey, 
				sizeof( ulKey ),
				0, 
				NULL ) );

	Call( ErrDispSetColumn(
				m_sesid, 
				m_tableid, 
				m_columnidValue,
				(BYTE *)&ulValue, 
				sizeof( ulValue ),
				0, 
				NULL ) );

	Call( ErrDispUpdate( m_sesid, m_tableid, NULL, 0, NULL, NO_GRBIT ) );
	Call( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );

	++m_crecords;
	
	return err;

HandleError:
	(VOID)ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepCancel );
	CallS( ErrIsamRollback( m_sesid, NO_GRBIT ) );
	return err;
	}

	
//  ================================================================
ERR TTMAP::ErrRetrieveValue_( const ULONG ulKey, ULONG * const pulValue ) const
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );
	CallS( ErrDispMakeKey( m_sesid, m_tableid, (BYTE *)&ulKey, sizeof(ulKey), JET_bitNewKey ) );

	err = ErrDispSeek( m_sesid, m_tableid, JET_bitSeekEQ );

	if( err >= 0 && pulValue )
		{
		ULONG cbActual = 0;
		err = ErrDispRetrieveColumn(
				m_sesid,
				m_tableid,
				m_columnidValue,
				pulValue,
				sizeof( ULONG ),
				&cbActual,
				NO_GRBIT,
				NULL );
		}
		
	CallS( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );
	return err;
	}


//  ================================================================
ERR TTMAP::ErrIncrementValue( const ULONG ulKey )
//  ================================================================
	{
	ERR		err 		= JET_errSuccess;
	BOOL	fUpdatePrepared	= fFalse;

	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );

	Call( ErrDispMakeKey( m_sesid, m_tableid, (BYTE *)&ulKey, sizeof(ulKey), JET_bitNewKey ) );
	err = ErrDispSeek( m_sesid, m_tableid, JET_bitSeekEQ );
	if( JET_errRecordNotFound == err )
		{
		Call( ErrInsertKeyValue_( ulKey, 1 ) );
		}
	else
		{
		ULONG	cbActual;
		ULONG	ulValue = 0;
		
		Call( err );
		Call( ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepReplace ) );
		fUpdatePrepared = fTrue;

		Call( ErrDispRetrieveColumn(
				m_sesid,
				m_tableid,
				m_columnidValue,
				&ulValue,
				sizeof( ulValue ),
				&cbActual,
				JET_bitRetrieveCopy,
				NULL ) );
		Assert( sizeof( ulValue ) == cbActual );

		++ulValue;
		Call( ErrDispSetColumn(
					m_sesid, 
					m_tableid, 
					m_columnidValue,
					(BYTE *)&ulValue, 
					sizeof( ulValue ),
					NO_GRBIT, 
					NULL ) );
		Call( ErrDispUpdate( m_sesid, m_tableid, NULL, 0, NULL, NO_GRBIT ) );
		}

	Call( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );

	return err;

HandleError:
	if( fUpdatePrepared )
		{
		(VOID)ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepCancel );
		}
	CallS( ErrIsamRollback( m_sesid, NO_GRBIT ) );
	return err;
	}


//  ================================================================
ERR TTMAP::ErrSetValue( const ULONG ulKey, const ULONG ulValue )
//  ================================================================
	{
	ERR		err 		= JET_errSuccess;
	BOOL	fUpdatePrepared	= fFalse;

	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );

	Call( ErrDispMakeKey( m_sesid, m_tableid, (BYTE *)&ulKey, sizeof(ulKey), JET_bitNewKey ) );
	err = ErrDispSeek( m_sesid, m_tableid, JET_bitSeekEQ );
	if( JET_errRecordNotFound == err )
		{
		Call( ErrInsertKeyValue_( ulKey, ulValue ) );
		}
	else
		{
		Call( err );
		Call( ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepReplace ) );
		fUpdatePrepared = fTrue;
		Call( ErrDispSetColumn(
					m_sesid, 
					m_tableid, 
					m_columnidValue,
					(BYTE *)&ulValue, 
					sizeof( ulValue ),
					NO_GRBIT, 
					NULL ) );
		Call( ErrDispUpdate( m_sesid, m_tableid, NULL, 0, NULL, NO_GRBIT ) );
		}

	Call( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );

	return err;

HandleError:
	if( fUpdatePrepared )
		{
		(VOID)ErrDispPrepareUpdate( m_sesid, m_tableid, JET_prepCancel );
		}
	CallS( ErrIsamRollback( m_sesid, NO_GRBIT ) );
	return err;
	}


//  ================================================================
ERR TTMAP::ErrGetValue( const ULONG ulKey, ULONG * const pulValue ) const
//  ================================================================
	{
	return ErrRetrieveValue_( ulKey, pulValue );
	}


//  ================================================================
ERR TTMAP::ErrGetCurrentKeyValue( ULONG * const pulKey, ULONG * const pulValue ) const
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );

	ULONG cbActual;
	Call( ErrDispRetrieveColumn(
			m_sesid,
			m_tableid,
			m_columnidKey,
			pulKey,
			sizeof( ULONG ),
			&cbActual,
			NO_GRBIT,
			NULL ) );

	Call( ErrDispRetrieveColumn(
			m_sesid,
			m_tableid,
			m_columnidValue,
			pulValue,
			sizeof( ULONG ),
			&cbActual,
			NO_GRBIT,
			NULL ) );

HandleError:
	CallS( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );
	return err;
	}


//  ================================================================
ERR TTMAP::ErrDeleteKey( const ULONG ulKey )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	CallR( ErrIsamBeginTransaction( m_sesid, NO_GRBIT ) );
	CallS( ErrDispMakeKey( m_sesid, m_tableid, (BYTE *)&ulKey, sizeof(ulKey), JET_bitNewKey ) );
	Call( ErrDispSeek( m_sesid, m_tableid, JET_bitSeekEQ ) );

	Call( ErrDispDelete( m_sesid, m_tableid ) );
	Call( ErrIsamCommitTransaction( m_sesid, NO_GRBIT ) );

	--m_crecords;
	
	return err;
	
HandleError:
	CallS( ErrIsamRollback( m_sesid, NO_GRBIT ) );
	return err;
	}


//  ================================================================
ERR TTMAP::ErrMoveFirst()
//  ================================================================
	{
	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	FUCBSetSequential( (FUCB *)m_tableid );
	FUCBSetPrereadForward( (FUCB *)m_tableid, cpgPrereadSequential );
	return ErrDispMove( m_sesid, m_tableid, JET_MoveFirst, NO_GRBIT );
	}


//  ================================================================
ERR TTMAP::ErrMoveNext()
//  ================================================================
	{
	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	return ErrDispMove( m_sesid, m_tableid, JET_MoveNext, NO_GRBIT );
	}


//  ================================================================
ERR TTMAP::ErrFEmpty( BOOL * const pfEmpty ) const
//  ================================================================
	{
	if( JET_tableidNil == m_tableid )
		{
		return ErrERRCheck( JET_errInternalError );
		}

	ERR err = ErrDispMove( m_sesid, m_tableid, JET_MoveFirst, NO_GRBIT );

	*pfEmpty = fFalse;
	if( JET_errNoCurrentRecord == err )
		{
		*pfEmpty = fTrue;
		err = JET_errSuccess;
		}
	Call( err );

#ifdef DEBUG
	if( !*pfEmpty )
		{
		ULONG ulKey;
		ULONG cbActual;
		const ERR errT = ErrDispRetrieveColumn(
			m_sesid,
			m_tableid,
			m_columnidKey,
			&ulKey,
			sizeof( ulKey ),
			&cbActual,
			NO_GRBIT,
			NULL );
		}
#endif	//	DEBUG

HandleError:
	return err;
	}


//  ================================================================
TTARRAY::TTARRAY( const ULONG culEntries, const ULONG ulDefault ) :
//  ================================================================
	m_culEntries( culEntries ),
	m_ulDefault( ulDefault ),
	m_ppib( ppibNil ),
	m_pfucb( pfucbNil ),
	m_pgnoFirst( pgnoNull ),
	m_rgbitInit( NULL )
	{
	}


//  ================================================================
TTARRAY::~TTARRAY()
//  ================================================================
	{
	//  delete our bit-map of init space if it exists

	if ( m_rgbitInit )
		{
		OSMemoryPageFree( m_rgbitInit );
		m_rgbitInit = NULL;
		}
		
	//  delete the dummy table if it exists
	
	if ( m_pfucb != pfucbNil )
		{
		CallS( ErrFILECloseTable( m_ppib, m_pfucb ) );
		m_pfucb = pfucbNil;
		}

	//  close our session

	if ( m_ppib != ppibNil )
		{
		PIBEndSession( m_ppib );
		m_ppib = ppibNil;
		}
	}


DWORD TTARRAY::s_cTTArray = 0;

//  ================================================================
ERR TTARRAY::ErrInit( INST * const pinst )
//  ================================================================
	{
	ERR					err						= JET_errSuccess;
	CHAR   				szName[JET_cbNameMost];
	JET_TABLECREATE2	tablecreate;

	//  get a session

	Call( ErrPIBBeginSession( pinst, &m_ppib, procidNil, fFalse ) );

	//	create a dummy table large enough to contain all the elements of the array
	//  as well as the FDP

	DWORD cpg;
	cpg = sizeof( DWORD ) * m_culEntries / g_cbPage + 1;

	sprintf( szName, "TTArray%d", AtomicIncrement( (long*)&s_cTTArray ) );

	tablecreate.cbStruct			= sizeof( JET_TABLECREATE2 );
	tablecreate.szTableName			= szName;
	tablecreate.szTemplateTableName	= NULL;
	tablecreate.ulPages				= cpg + 16;
	tablecreate.ulDensity			= 100;
	tablecreate.rgcolumncreate		= NULL;
	tablecreate.cColumns			= 0;
	tablecreate.rgindexcreate		= NULL;
	tablecreate.cIndexes			= 0;
	tablecreate.szCallback			= NULL;
	tablecreate.cbtyp				= JET_cbtypNull;
	tablecreate.grbit				= NO_GRBIT;
	tablecreate.tableid				= JET_TABLEID( pfucbNil );
	tablecreate.cCreated			= 0;

	Call( ErrFILECreateTable( m_ppib, pinst->m_mpdbidifmp[ dbidTemp ], &tablecreate ) );
	m_pfucb = (FUCB*)( tablecreate.tableid );
	Assert( m_pfucb != pfucbNil );
	Assert( tablecreate.cCreated == 1 );

	m_pgnoFirst	= m_pfucb->u.pfcb->PgnoFDP() + 16;

	//  issue a preread for up to the first 512 pages of the map file

	BFPrereadPageRange( m_pfucb->ifmp, m_pgnoFirst, min( 512, cpg ) );

	//  allocate a bit-map to represent init space in the array.  the array will
	//  be pre-initialized to zero by the OS
	//
	//  NOTE:  the largest possible amount of memory allocated will be 512KB

	if ( !( m_rgbitInit = (ULONG*)PvOSMemoryPageAlloc( cpg / 8 + 1, NULL ) ) )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

HandleError:
	return err;
	}


//  ================================================================
VOID TTARRAY::BeginRun( PIB* const ppib, RUN* const prun )
//  ================================================================
	{
	//  nop
	}

//  ================================================================
VOID TTARRAY::EndRun( PIB* const ppib, RUN* const prun )
//  ================================================================
	{
	//  unlatch existing page if any

	if ( prun->pgno != pgnoNull )
		{
		if ( prun->fWriteLatch )
			{
			BFWriteUnlatch( &prun->bfl );
			}
		else
			{
			BFRDWUnlatch( &prun->bfl );
			}
		prun->pgno = pgnoNull;
		}
	}

//  ================================================================
ERR TTARRAY::ErrSetValue( PIB * const ppib, const ULONG ulEntry, const ULONG ulValue, RUN* const prun )
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( ulEntry >= m_culEntries )
		{
		return ErrERRCheck( JET_errRecordNotFound );
		}
		
	//  compute the pgno that this entry lives on

	const PGNO pgno = m_pgnoFirst + ulEntry / ( g_cbPage / sizeof( DWORD ) );

	//  we do not already have this page latched

	RUN runT;
	runT.pgno			= pgnoNull;
	runT.bfl.pv			= NULL;
	runT.bfl.dwContext	= NULL;
	runT.fWriteLatch 	= fFalse;
	RUN * const  prunT = prun ? prun : &runT;

	if ( prunT->pgno != pgno )
		{
		//  unlatch existing page if any

		if ( prunT->pgno != pgnoNull )
			{
			if ( prunT->fWriteLatch )
				{
				BFWriteUnlatch( &prunT->bfl );
				}
			else
				{
				BFRDWUnlatch( &prunT->bfl );
				}
			prunT->pgno 		= pgnoNull;
			prunT->fWriteLatch 	= fFalse;
			}

		//  latch the new page

		Call( ErrBFRDWLatchPage( &prunT->bfl, m_pfucb->ifmp, pgno ) );
		prunT->pgno 		= pgno;
		prunT->fWriteLatch 	= fFalse;
		}

	if( fFalse == prunT->fWriteLatch )
		{
		CallS( ErrBFUpgradeRDWLatchToWriteLatch( &prunT->bfl ) );
		prunT->fWriteLatch 	= fTrue;
		}

	//  the page is not yet init

	if ( !FPageInit( pgno ) )
		{
		//  init the page

		DWORD iEntryMax;
		iEntryMax = g_cbPage / sizeof( DWORD );
		
		DWORD iEntry;
		for ( iEntry = 0; iEntry < iEntryMax; iEntry++ )
			{
			*( (ULONG*) prunT->bfl.pv + iEntry ) = m_ulDefault;
			}

		//  mark the page as init

		SetPageInit( pgno );
		}

	//  set the value on the page

	DWORD iEntry;
	iEntry = ulEntry % ( g_cbPage / sizeof( DWORD ) );
	
	*( (ULONG*) prunT->bfl.pv + iEntry ) = ulValue;

	BFDirty( &prunT->bfl );

HandleError:
	if ( runT.pgno != pgnoNull )
		{
		BFWriteUnlatch( &runT.bfl );
		}
	return err;
	}


//  ================================================================
ERR TTARRAY::ErrGetValue( PIB * const ppib, const ULONG ulEntry, ULONG * const pulValue, RUN* const prun ) const
//  ================================================================
	{
	ERR err = JET_errSuccess;

	if( ulEntry >= m_culEntries )
		{
		*pulValue = m_ulDefault;
		return JET_errSuccess;
		}

	//  compute the pgno that this entry lives on

	const PGNO pgno = m_pgnoFirst + ulEntry / ( g_cbPage / sizeof( DWORD ) );

	//  we do not already have this page latched

	RUN runT;
	runT.pgno			= pgnoNull;
	runT.bfl.pv			= NULL;
	runT.bfl.dwContext	= NULL;
	runT.fWriteLatch 	= fFalse;
	RUN * const  prunT = prun ? prun : &runT;

	if ( prunT->pgno != pgno )
		{
		//  unlatch existing page if any

		if ( prunT->pgno != pgnoNull )
			{
			if ( prunT->fWriteLatch )
				{
				BFWriteUnlatch( &prunT->bfl );
				}
			else
				{
				BFRDWUnlatch( &prunT->bfl );
				}
			prunT->pgno 		= pgnoNull;
			prunT->fWriteLatch 	= fFalse;
			}

		//  latch the new page
		//  if we are not part of a run, just use a read-latch for speed and concurrency
		//  use we are part of a run use a RDW latch for consistency, but still allow reads

		if( prun )
			{
			Call( ErrBFRDWLatchPage( &prunT->bfl, m_pfucb->ifmp, pgno ) );
			prunT->fWriteLatch 	= fFalse;
			}
		else
			{
			Assert( prunT == &runT );
			Call( ErrBFReadLatchPage( &prunT->bfl, m_pfucb->ifmp, pgno ) );
			prunT->fWriteLatch 	= fFalse;
			}
			
		prunT->pgno = pgno;
		}

	//  this page is initialized

	if ( FPageInit( pgno ) )
		{
		//  retrieve the value from the page

		DWORD iEntry;
		iEntry = ulEntry % ( g_cbPage / sizeof( DWORD ) );
		
		*pulValue = *( (ULONG*) prunT->bfl.pv + iEntry );
		}

	//  this page is not initialized

	else
		{
		//  the answer is the default value

		*pulValue = m_ulDefault;
		}

HandleError:
	if ( runT.pgno != pgnoNull )
		{
		BFReadUnlatch( &runT.bfl );
		}
	return err;
	}


//  ================================================================
BOOL TTARRAY::FPageInit( const PGNO pgno ) const
//  ================================================================
	{
	const ULONG iPage 	= pgno - m_pgnoFirst;
	const ULONG iul		= iPage / 32;
	const ULONG ibit	= iPage % 32;
	
	return ( m_rgbitInit[ iul ] & ( 1 << ibit ) ) != 0;
	}

	
//  ================================================================
VOID TTARRAY::SetPageInit( const PGNO pgno )
//  ================================================================
	{
	Assert( !FPageInit( pgno ) );
	
	const ULONG iPage 	= pgno - m_pgnoFirst;
	const ULONG iul		= iPage / 32;
	const ULONG ibit	= iPage % 32;

	ULONG ulBIExpected;
	ULONG ulBI;
	ULONG ulAI;
	do	{
		ulBIExpected = m_rgbitInit[ iul ];
		ulAI = ulBIExpected | ( 1 << ibit );

		ulBI = AtomicCompareExchange( (long*)&m_rgbitInit[ iul ], ulBIExpected, ulAI );
		}
	while ( ulBI != ulBIExpected );

	Assert( FPageInit( pgno ) );
	}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\_cat.cxx ===
//	version number embedded in name
//	if the table is changed in an incompatible way, increase the version string
//	and delete the old ones
const CHAR  szMSU[]                 = "MSysUnicodeFixupVer1";	//	also referenced in eseutil.cxx	
const CHAR	szMSO[]					= "MSysObjects";
const CHAR	szMSOShadow[]			= "MSysObjectsShadow";
const CHAR	szMSOIdIndex[]			= "Id";
const CHAR	szMSONameIndex[]		= "Name";
const CHAR	szMSORootObjectsIndex[]	= "RootObjects";

const CHAR	szLVRoot[]				= "LV";
const ULONG cbLVRoot				= sizeof(szLVRoot) - 1;

const CHAR	szTableCallback[]		= "CB";
const ULONG cbTableCallback			= sizeof(szTableCallback) - 1;


struct CDESC							// Column Description
	{
	CHAR	  			*szColName;		// column name
	COLUMNID			columnid;		// hard-coded fid
	JET_COLTYP			coltyp; 		// column type
	JET_GRBIT			grbit;			// flag bits
	};

struct IDESC							// Index Description
	{
	CHAR				*szIdxName;		// index name
	CHAR				*szIdxKeys;		// key string
	JET_GRBIT			grbit;			// flag bits
	};



LOCAL const CDESC	rgcdescMSO[]	=
	{
	//	cluster system objects by table - all related table objects have
	//	the same clustering id, which will be the object id of the table
	"ObjidTable", 		fidMSO_ObjidTable,		JET_coltypLong, 	   	JET_bitColumnNotNULL, 

	//	see SYSOBJ for valid types
	"Type",				fidMSO_Type,			JET_coltypShort,		JET_bitColumnNotNULL,

	//	object id for tables/indexes/LV, columnid for columns
	"Id",				fidMSO_Id,				JET_coltypLong,			JET_bitColumnNotNULL,

	//	table/column/index/LV name
	"Name",					fidMSO_Name,				JET_coltypText,			JET_bitColumnNotNULL,

	//	column type for columns, pgnoFDP for tables/indexes/LV
	"ColtypOrPgnoFDP",		fidMSO_Coltyp,				JET_coltypLong,			JET_bitColumnNotNULL,
	
	//	density for tables/indexes/LV, column length for columns
	"SpaceUsage",			fidMSO_SpaceUsage,			JET_coltypLong,			JET_bitColumnNotNULL,
	
	"Flags",				fidMSO_Flags,				JET_coltypLong,			JET_bitColumnNotNULL,

	//	initial pages for table/LV, code page for columns, localeid for indexes
	"PagesOrLocale",		fidMSO_Pages,				JET_coltypLong,			JET_bitColumnNotNULL,

	//	unused by columns
	"Stats",				fidMSO_Stats,				JET_coltypBinary,		NO_GRBIT,


	//	set TableFlag to TRUE for tables, NULL otherwise
	"RootFlag",				fidMSO_RootFlag,			JET_coltypBit,			NO_GRBIT,

	//  name of the template table for table, callback name for callbacks and columns
	"TemplateTable",		fidMSO_TemplateTable,		JET_coltypText,			NO_GRBIT,
//	"Callback",				fidMSO_Callback,			JET_coltypText,			NO_GRBIT,

	//	COLUMN-specific items
	"RecordOffset",			fidMSO_RecordOffset,		JET_coltypShort,		NO_GRBIT,
	"DefaultValue",			fidMSO_DefaultValue,		JET_coltypBinary,		NO_GRBIT,

	//	INDEX-specific items
	//	max. # of fields in a key is (size of coltypBinary)/(size of JET_COLUMNID) = 255/8 = 31
	"KeyFldIDs",			fidMSO_KeyFldIDs,			JET_coltypBinary,		NO_GRBIT,
	"VarSegMac",		  	fidMSO_VarSegMac,			JET_coltypBinary,		NO_GRBIT,
	"ConditionalColumns",	fidMSO_ConditionalColumns,	JET_coltypBinary,		NO_GRBIT,
	"LCMapFlags",			fidMSO_LCMapFlags,			JET_coltypLong,			NO_GRBIT,
	"TupleLimits",			fidMSO_TupleLimits,			JET_coltypBinary,		NO_GRBIT,
	"Version",				fidMSO_Version,				JET_coltypBinary,		NO_GRBIT,	

	//  CALLBACK-related items
	//  These are used by columns with callbacks (user-defined default values)
	//  Added in database version 0x620,3
	"CallbackData",			fidMSO_CallbackData,		JET_coltypLongBinary,	JET_bitColumnTagged,
	"CallbackDependencies",	fidMSO_CallbackDependencies,JET_coltypLongBinary,	JET_bitColumnTagged,
	};

LOCAL const ULONG	cColumnsMSO		= sizeof(rgcdescMSO)/sizeof(CDESC);


LOCAL const IDESC	rgidescMSO[]	=
	{
	(CHAR *)szMSOIdIndex, 			"+ObjidTable\0+Type\0+Id\0", 	JET_bitIndexPrimary|JET_bitIndexDisallowNull,
	(CHAR *)szMSONameIndex, 		"+ObjidTable\0+Type\0+Name\0",	JET_bitIndexUnique|JET_bitIndexDisallowNull,

	//	This index is only used to filter out duplicate table names (all root objects
	//	object -- currently only tables -- have the RootFlag field set, all non-root
	//	objects don't set it)
	(CHAR *)szMSORootObjectsIndex,	"+RootFlag\0+Name\0",			JET_bitIndexUnique|JET_bitIndexIgnoreFirstNull
	};

LOCAL const ULONG	cIndexesMSO		= sizeof(rgidescMSO)/sizeof(IDESC);


LOCAL const ULONG	cbMaxDeletedColumnStubName	= 32;
LOCAL const CHAR	szDeletedColumnStubPrefix[]	= "JetStub_";
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\_log\log.cxx ===
#include "std.hxx"
#include <ctype.h>

//	Source Insight users:
//	------------------
//	If you get parse errors with this file, open up C.tom in your Source
//	Insight directory, comment out the "TRY try {" line with a semi-colon,
//	and then add a line with "PERSISTED" only.
//
//	If you don't use Source Insight, you are either a fool or else there is
//	a better editor by the time you read this.

//	D O C U M E N T A T I O N ++++++++++++++++++++++++++++
//
//	FASTFLUSH Physical Logging Overview
//	==========================
//	For information on how the FASTFLUSH Physical Logging works, please see
//	ese\doc\ESE Physical Logging.doc.
//
//	Asynchronous Log File Creation Overview
//	============================
//	Background
//	----------
//	Before asynchronous log file creation, ESE would stall in ErrLGNewLogFile()
//	when it would create and format a new 5MB log file (formatting means
//	applying a special signature pattern to the file which is used to determine
//	if corruption has occurred; log file size is settable via a Jet system
//	parameter). 5MB of I/O isn't that fast, especially considering that NTFS
//	makes extending I/Os synchronous (thus a maximum of 1 outstanding I/O),
//	plus each of the 1MB I/Os causes a write to the MFT.
//
//	Solution
//	-------
//	Once ErrLGNewLogFile() is about to return (since it has completed setting
//	up the next log file), we should create edbtmp.log immediately (or rename
//	an archived edb%05X.log file in the case of circular logging). Then we set
//	a "TRIGGER" for the first asynchronous 1MB formatting I/O to start once
//	we have logged 512K to the log buffer.
//
//	What is this TRIGGER about?
//	-------------------------
//	One way we could have done this is just to immediately start a 1MB
//	asynchronous formatting I/O, then once it finishes, issue another one
//	immediately. Unfortunately back-to-back 1MB I/Os basically consume
//	the disk and keep it busy -- thus no logging would be able to be done
//	to edb.log since we'd be busy using the disk to format edbtmp.log!
//	(I actually determined this experimentally with a prototype).
//
//	The trigger allows us to throttle I/O to the edbtmp.log that we're
//	formatting, so that we log 1MB of data to the in-memory log buffer, then
//	write 1MB to edbtmp.log, then... etc.
//
//	The trigger is handled such that once we pass the trigger, we will
//	issue the asynch formatting I/O, then once it completes AND we pass
//	the next trigger, we will issue the next, etc. If we reach ErrLGNewLogFile()
//	before edbtmp.log is completely formatted (or there is currently an
//	outstanding I/O), we will wait in ErrLGNewLogFile() for any asynch I/O
//	to complete, then we format the rest of the file if necessary.
//
//	Why is the policy based on how much we've logged to the log buffer
//	and not how much we've flushed to edb.log?
//	--------------------------------------------------------------
//	In the case of many lazy commit transactions, waiting for a log flush
//	may be a long time with a large log buffer (i.e. there are some
//	performance recommendations that Exchange Servers should
//	have log buffers set to `9000' which is 9000 * 512 bytes = ~4.4MB;
//	BTW, do not set the log buffers equal to the log file size or greater or
//	you will get some bizarre problems). So in this case, we should try to
//	format edbtmp.log while those lazy commit transactions are coming in,
//	especially since they're not causing edb.log to be used at all.
//
//	Why set the trigger to 512K instead of issuing the first I/O immediately?
//	----------------------------------------------------------------
//	In ErrLGNewLogFile() we're doing a lot with files, deleting unnecessary
//	log files in some circumstances, creating files, renaming files, etc. In other
//	words, this is going to take a while since we have to wait for NTFS to
//	update metadata by writing to the NTFS transaction log, at a minimum.
//	While we're waiting for NTFS to do this stuff, it is pretty likely that some
//	clients are doing transactions and they are now waiting for their commit
//	records to hit the disk -- in other words, they are waiting for their stuff
//	to be flushed to edb.log.
//
//	If we write 1MB to edbtmp.log now, those clients will have to wait until
//	the 1MB hits the disk before their records hit the disk.
//
//	Thus, this is why we wait for 512K to be logged to the log buffer -- as
//	a heuristic wait.
//
//	What about error handling with this asynch business?
//	-----------------------------------------------
//	If we get an error creating the new edbtmp.log, we'll handle the error
//	later when ErrLGNewLogFile() is next called. The same with errors from
//	any asynch I/O.
//
//	We handle all the weird errors with disk-full and using reserve log files, etc.



#ifdef LOGPATCH_UNIT_TEST

#error "LOGPATCH_UNIT_TEST is broken because of spenlow's asynch log file creation (Dec 2000)."

//	Should investigate using a RAM disk to reduce LOGPATCH_UNIT_TEST code.

#endif

//	constants

const LGPOS		lgposMax = { 0xffff, 0xffff, 0x7fffffff };
const LGPOS		lgposMin = { 0x0,  0x0,  0x0 };

#ifdef DEBUG
CCriticalSection g_critDBGPrint( CLockBasicInfo( CSyncBasicInfo( szDBGPrint ), rankDBGPrint, 0 ) );
#endif  //  DEBUG


//  monitoring statistics
//
PM_CEF_PROC LLGWriteCEFLPv;
PERFInstanceG<> cLGWrite;
long LLGWriteCEFLPv( long iInstance, void *pvBuf )
	{
	cLGWrite.PassTo( iInstance, pvBuf );
	return 0;
	}

PM_CEF_PROC LLGBytesWrittenCEFLPv;
PERFInstanceGlobal<> cLGBytesWritten;
long LLGBytesWrittenCEFLPv( long iInstance, void *pvBuf )
	{
	cLGBytesWritten.PassTo( iInstance, pvBuf );
	return 0;
	}

PM_CEF_PROC LLGUsersWaitingCEFLPv;
PERFInstanceG<> cLGUsersWaiting;
long LLGUsersWaitingCEFLPv( long iInstance, void *pvBuf )
	{
	cLGUsersWaiting.PassTo( iInstance, pvBuf );
	return 0;
	}

PM_CEF_PROC LLGRecordCEFLPv;
PERFInstanceG<> cLGRecord;
long LLGRecordCEFLPv(long iInstance,void *pvBuf)
{
	cLGRecord.PassTo( iInstance, pvBuf );
	return 0;
}

PM_CEF_PROC LLGCapacityFlushCEFLPv;
PERFInstanceG<> cLGCapacityFlush;
long LLGCapacityFlushCEFLPv(long iInstance,void *pvBuf)
{
	cLGCapacityFlush.PassTo( iInstance, pvBuf );
	return 0;
}

PM_CEF_PROC LLGCommitFlushCEFLPv;
PERFInstanceG<> cLGCommitFlush;
long LLGCommitFlushCEFLPv(long iInstance,void *pvBuf)
{
	cLGCommitFlush.PassTo( iInstance, pvBuf );
	return 0;
}

PM_CEF_PROC LLGFlushCEFLPv;
long LLGFlushCEFLPv(long iInstance, void *pvBuf)
{
	if ( NULL != pvBuf )
		{
		*(LONG*)pvBuf = cLGCapacityFlush.Get( iInstance ) + cLGCommitFlush.Get( iInstance );
		}
	return 0;
}

PM_CEF_PROC LLGStallCEFLPv;
PERFInstanceG<> cLGStall;
long LLGStallCEFLPv(long iInstance,void *pvBuf)
{
	cLGStall.PassTo( iInstance, pvBuf );
	return 0;
}

PM_CEF_PROC LLGCheckpointDepthCEFLPv;
PERFInstance<QWORD> cLGCheckpoint;
PERFInstance<QWORD> cLGRecordOffset;
LONG LLGCheckpointDepthCEFLPv( LONG iInstance, VOID *pvBuf )
	{
	if ( pvBuf )
		{
		const QWORD		ibLogRec		= cLGRecordOffset.Get( iInstance );
		const QWORD		ibCheckpoint	= cLGCheckpoint.Get( iInstance );

		*(QWORD *)pvBuf = ( ibLogRec > ibCheckpoint ? ibLogRec - ibCheckpoint : 0 );
		}
	return 0;
	}
	
LOG::LOG( INST *pinst ) :
	m_pinst( pinst ),
	m_fLogInitialized( fFalse ),
	m_fLogDisabled( fFalse ),
	m_fLogDisabledDueToRecoveryFailure( fFalse ),
	m_fNewLogRecordAdded( fFalse ),
	m_fLGNoMoreLogWrite( fFalse ),
	m_errNoMoreLogWrite( JET_errSuccess ),
	m_ls( lsNormal ),
	m_fLogSequenceEnd( fFalse ),
	m_errCheckpointUpdate( JET_errSuccess ),
	m_lgposCheckpointUpdateError( lgposMin ),
	m_fRecovering( fFalse ),
	m_fRecoveringMode( fRecoveringNone ),
	m_fHardRestore( fFalse ),
	m_fRestoreMode( fRecoveringNone ),
	m_lgposSnapshotStart( lgposMin ),
	m_fSnapshotMode( fSnapshotNone ),
	m_fLGCircularLogging( g_fLGCircularLogging ),
	m_fLGFMPLoaded( fFalse ),
#ifdef UNLIMITED_DB
	m_pbLGDbListBuffer( NULL ),
	m_cbLGDbListBuffer( 0 ),
	m_cbLGDbListInUse( 0 ),
	m_cLGAttachments( 0 ),
	m_fLGNeedToLogDbList( fFalse ),
#endif
	m_fAlternateDbDirDuringRecovery( g_fAlternateDbDirDuringRecovery ),
	m_fSignLogSet( fFalse ),
	m_fLGIgnoreVersion( g_fLGIgnoreVersion ),
	m_pfapiLog( NULL ),
	//	Asynchronous log file creation
	m_fCreateAsynchLogFile( g_fLogFileCreateAsynch ),
	m_pfapiJetTmpLog( NULL ),
	m_fCreateAsynchResUsed( fFalse ),
	m_errCreateAsynch( JET_errSuccess ),
	m_asigCreateAsynchIOCompleted( CSyncBasicInfo( _T( "LOG::m_asigCreateAsynchIOCompleted" ) ) ),
	m_ibJetTmpLog( 0 ),
	m_lgposCreateAsynchTrigger( lgposMax ),
	m_szLogCurrent( NULL ),
	m_plgfilehdr( NULL ),
	m_plgfilehdrT( NULL ),
	m_pbLGBufMin( NULL ),
	m_pbLGBufMax( NULL ),
	m_cbLGBuf( 0 ),
	m_pbEntry( NULL ),
	m_pbWrite( NULL ),
	m_isecWrite( 0 ),
	m_lgposLogRec( lgposMin ),
	m_lgposToFlush( lgposMin ),
	m_lgposMaxFlushPoint( lgposMin ),
	m_pbLGFileEnd( pbNil ),
	m_isecLGFileEnd( 0 ),
	m_lgposFullBackup( lgposMin ),
	m_lgposLastChecksum( lgposMin ),
	m_pbLastChecksum( pbNil ),
	// XXX
	// quick hack to always start writing in safe-safe mode
	// in regards to killing a shadow and killing the data sector.
	m_fHaveShadow( fTrue ),
	m_lgposIncBackup( lgposMin ),
	m_cbSec( 0 ),
	m_cbSecVolume( 0 ),
	m_csecHeader( 0 ),
	m_fLGFlushWait( 2 ),
	m_fLGFailedToPostFlushTask( fFalse ),
	m_asigLogFlushDone( CSyncBasicInfo( _T( "LOG::m_asigLogFlushDone" ) ) ),
	m_critLGFlush( CLockBasicInfo( CSyncBasicInfo( szLGFlush ), rankLGFlush, CLockDeadlockDetectionInfo::subrankNoDeadlock ) ),
	m_critLGBuf( CLockBasicInfo( CSyncBasicInfo( szLGBuf ), rankLGBuf, 0 ) ),
	m_critLGTrace( CLockBasicInfo( CSyncBasicInfo( szLGTrace ), rankLGTrace, 0 ) ),
	m_critLGWaitQ( CLockBasicInfo( CSyncBasicInfo( szLGWaitQ ), rankLGWaitQ, 0 ) ),
	m_critLGResFiles( CLockBasicInfo( CSyncBasicInfo( szLGResFiles ), rankLGResFiles, 0 ) ),
	m_ppibLGFlushQHead( ppibNil ),
	m_ppibLGFlushQTail( ppibNil ),
	m_cLGWrapAround( 0 ),
	m_pcheckpoint( NULL ),
	m_critCheckpoint( CLockBasicInfo( CSyncBasicInfo( szCheckpoint ), rankCheckpoint, CLockDeadlockDetectionInfo::subrankNoDeadlock ) ),
	m_fDisableCheckpoint( fTrue ),
	m_fReplayingReplicatedLogFiles( fFalse ),
#ifdef IGNORE_BAD_ATTACH
	m_fReplayingIgnoreMissingDB( fFalse ),
	m_rceidLast( rceidMin ),
#endif // IGNORE_BAD_ATTACH
	m_fAbruptEnd( fFalse ),
	m_plread( pNil ),
	m_lgposPbNextPreread( lgposMin ),
	m_lgposLastChecksumPreread( lgposMin ),
	m_cPagesPrereadInitial( 0 ),
	m_cPagesPrereadThreshold( 0 ),
	m_cPagesPrereadAmount( 0 ),
	m_cPageRefsConsumed( 0 ),
	m_fPreread( fFalse ),
	m_fAfterEndAllSessions( fFalse ),
	m_fLastLRIsShutdown( fFalse ),
	m_fNeedInitialDbList( fFalse ),
	m_rgcppib( NULL ),
	m_pcppibAvail( NULL ),
	m_ccppib( 0 ),
	m_ptablehfhash( NULL ),
	m_critBackupInProgress( CLockBasicInfo( CSyncBasicInfo( "BackupInProcess" ), rankBackupInProcess, 0 ) ),
	m_fBackupInProgress( fFalse ),
	m_fStopBackup( fFalse ),
	m_fBackupStatus( backupStateNotStarted ),
	m_ppibBackup( ppibNil ),
	m_rgppatchlst( NULL ),
	m_rgrstmap( NULL ),
	m_irstmapMac( 0 ),
	m_fScrubDB( fFalse ),
	m_pscrubdb( NULL ),
	m_ulSecsStartScrub( 0 ),
	m_dbtimeLastScrubNew( 0 ),
	m_crhfMac( 0 ),
	m_cNOP( 0 ),
	m_fExternalRestore( fFalse ),
	m_lGenLowRestore( 0 ),
	m_lGenHighRestore( 0 ),
	m_lGenHighTargetInstance( 0 ),
	m_fDumppingLogs( fFalse ),
	m_fDeleteOldLogs( g_fDeleteOldLogs ),
	m_fDeleteOutOfRangeLogs ( g_fDeleteOutOfRangeLogs )
#ifdef DEBUG
	,
	m_lgposLastLogRec( lgposMin ),
	m_fDBGFreezeCheckpoint( fFalse ),
	m_fDBGTraceLog( fFalse ),
	m_fDBGTraceLogWrite( fFalse ),
	m_fDBGTraceRedo( fFalse ),
	m_fDBGTraceBR( fFalse ),
	m_fDBGNoLog( fFalse ),
	m_cbDBGCopied( 0 )
#endif
	,
	m_pcheckpointDeleted( NULL )
	{
	//	log perf counters
	cLGWrite.Clear( m_pinst );
	cLGUsersWaiting.Clear( m_pinst );
	cLGCapacityFlush.Clear( m_pinst );
	cLGCommitFlush.Clear( m_pinst );
	cLGStall.Clear( m_pinst );
	cLGRecord.Clear( m_pinst );
	cLGBytesWritten.Clear( m_pinst );
	cLGCheckpoint.Clear( m_pinst );
	cLGRecordOffset.Clear( m_pinst );
	
	INT	irhf;
	for ( irhf = 0; irhf < crhfMax; irhf++ )
		{
		m_rgrhf[irhf].fInUse = fFalse;
		m_rgrhf[irhf].pLogVerifier = pNil;
		m_rgrhf[irhf].pSLVVerifier = pNil;
		m_rgrhf[irhf].pfapi = NULL;
		}

	strcpy( m_szAlternateDbDirDuringRecovery, g_szAlternateDbDirDuringRecovery );

	strcpy( m_szBaseName, szBaseName );
	strcpy( m_szJet, szJet );
	strcpy( m_szJetLog, szJetLog );
	strcpy( m_szJetLogNameTemplate, szJetLogNameTemplate );
	strcpy( m_szJetTmp, szJetTmp );
	strcpy( m_szJetTmpLog, szJetTmpLog );

	//	store absolute path if possible
	Assert( pinstNil != m_pinst );
	if ( NULL == m_pinst
		|| NULL == m_pinst->m_pfsapi
		|| m_pinst->m_pfsapi->ErrPathComplete( g_szLogFilePath, m_szLogFilePath ) < 0 )
		{
		strcpy( m_szLogFilePath, g_szLogFilePath );
		}

	strcpy( m_szRecovery, g_szRecovery );
	m_szRestorePath[0] = '\0';
	m_szNewDestination[0] = '\0';
	m_szTargetInstanceLogPath[0] = '\0';

	//	perform unit test for checksum code, on LE only.
	
	AssertRTL( TestChecksumBytes() );
	}

LOG::~LOG()
	{
	//	log perf counters
	cLGWrite.Clear( m_pinst );
	cLGUsersWaiting.Clear( m_pinst );
	cLGCapacityFlush.Clear( m_pinst );
	cLGCommitFlush.Clear( m_pinst );
	cLGStall.Clear( m_pinst );
	cLGRecord.Clear( m_pinst );
	cLGBytesWritten.Clear( m_pinst );
	cLGCheckpoint.Clear( m_pinst );
	cLGRecordOffset.Clear( m_pinst );
	}


VOID LOG::LGMakeLogName( CHAR *szLogName, const CHAR *szFName )
	{
	CallS( m_pinst->m_pfsapi->ErrPathBuild( m_szLogCurrent, szFName, szLogExt, szLogName ) );
	}

VOID LOG::LGReportError( const MessageId msgid, const ERR err, const _TCHAR* const pszLogFile )
	{
	_TCHAR			szError[ 64 ];
	const _TCHAR*	rgpsz[] = { pszLogFile, szError };

	_stprintf( szError, _T( "%i (0x%08x)" ), err, err );

	UtilReportEvent(	eventError,
						LOGGING_RECOVERY_CATEGORY,
						msgid,
						sizeof( rgpsz ) / sizeof( rgpsz[ 0 ] ),
						rgpsz,
						0,
						NULL,
						m_pinst );
	}

VOID LOG::LGReportError( const MessageId msgid, const ERR err )
	{
	_TCHAR	szAbsPath[ IFileSystemAPI::cchPathMax ];
	
	if ( m_pinst->m_pfsapi->ErrPathComplete( m_szLogName, szAbsPath ) < JET_errSuccess )
		{
		_tcscpy( szAbsPath, m_szLogName );
		}
	LGReportError( msgid, err, szAbsPath );
	}

VOID LOG::LGReportError( const MessageId msgid, const ERR err, IFileAPI* const pfapi )
	{
	_TCHAR		szAbsPath[ IFileSystemAPI::cchPathMax ];
	const ERR	errPath = pfapi->ErrPath( szAbsPath );
	if ( errPath < JET_errSuccess )
		{
		//	If we get an error here, this is the best we can do.

		_stprintf(	szAbsPath,
					_T( "%s [%i (0x%08x)]" ),
					m_szLogName,	// closest thing we have to pfapi's path
					errPath,
					errPath );
		}
	LGReportError( msgid, err, szAbsPath );
	}


VOID SIGGetSignature( SIGNATURE *psign )
	{
	INT cbComputerName;

	// init the rand seed with a per thread value, this will prevent
	// the generation of the same sequence in 2 different threads
	// (the seed is per thread if compiled with multithreaded libraries as we are)
	// Also put time in the seek to don't get the same random 3 numbers on all 
	// signature generation.
	srand( DwUtilThreadId() + TickOSTimeCurrent() );

	LGIGetDateTime( &psign->logtimeCreate );
	psign->le_ulRandom = rand() + rand() + rand() + TickOSTimeCurrent();
//	(VOID) GetComputerName( psign->szComputerName, &cbComputerName );
	cbComputerName = 0;
	memset( psign->szComputerName + cbComputerName,
		0,
		sizeof( psign->szComputerName ) - cbComputerName );
	}

BOOL FSIGSignSet( const SIGNATURE *psign )
	{
	SIGNATURE	signNull;

	memset( &signNull, 0, sizeof(SIGNATURE) );
	return ( 0 != memcmp( psign, &signNull, sizeof(SIGNATURE) ) );
	}


VOID LOG::GetLgpos( BYTE *pb, LGPOS *plgpos )
	{
	BYTE	*pbAligned;
	INT		csec;
	INT		isecCurrentFileEnd;

#ifdef DEBUG
	if ( !m_fRecovering )
		{
		Assert( m_critLGBuf.FOwner() );
		}
#endif

	//	m_pbWrite is always aligned
	//
	Assert( m_pbWrite != NULL );
	Assert( m_pbWrite == PbSecAligned( m_pbWrite ) );
	Assert( m_isecWrite >= m_csecHeader );

	// pb is a pointer into the log buffer, so it should be valid.
	Assert( pb >= m_pbLGBufMin );
	Assert( pb < m_pbLGBufMax );
	// m_pbWrite should also be valid since we're using it for comparisons here
	Assert( m_pbWrite >= m_pbLGBufMin );
	Assert( m_pbWrite < m_pbLGBufMax );

	pbAligned = PbSecAligned( pb );

	Assert( pbAligned >= m_pbLGBufMin );
	Assert( pbAligned < m_pbLGBufMax );

	plgpos->ib = USHORT( pb - pbAligned );
	if ( pbAligned < m_pbWrite )
		{
		csec = m_csecLGBuf - ULONG( m_pbWrite - pbAligned ) / m_cbSec;
		}
	else
		{
		csec = ULONG( pbAligned - m_pbWrite ) / m_cbSec;
		}

	plgpos->isec = USHORT( m_isecWrite + csec );

	isecCurrentFileEnd = m_isecLGFileEnd ? m_isecLGFileEnd : m_csecLGFile - 1;
	if ( plgpos->isec >= isecCurrentFileEnd )
		{
		plgpos->isec = (WORD) ( plgpos->isec - isecCurrentFileEnd + m_csecHeader );
		plgpos->lGeneration = m_plgfilehdr->lgfilehdr.le_lGeneration + 1;
		}
	else
		{
		plgpos->lGeneration = m_plgfilehdr->lgfilehdr.le_lGeneration;
		}

	return;
	}



//********************* INIT/TERM **************************
//**********************************************************

ERR LOG::ErrLGInitSetInstanceWiseParameters( IFileSystemAPI *const pfsapi )
	{
	ERR		err;
	CHAR  	rgchFullName[IFileSystemAPI::cchPathMax];

	//	verify the log path

	if ( pfsapi->ErrPathComplete( m_szLogFilePath, rgchFullName ) == JET_errInvalidPath )
		{
		const CHAR *szPathT[1] = { m_szLogFilePath };

		UtilReportEvent(
				eventError,
				LOGGING_RECOVERY_CATEGORY,
				FILE_NOT_FOUND_ERROR_ID,
				1,
				szPathT,
				0,
				NULL,
				m_pinst );

		return ErrERRCheck( JET_errFileNotFound );
		}

	//	get the atomic write size

	CallR( pfsapi->ErrFileAtomicWriteSize( rgchFullName, (DWORD*)&m_cbSecVolume ) );
	m_cbSec = m_cbSecVolume;
	Assert( m_cbSec >= 512 );
	Assert( sizeof( LGFILEHDR ) % m_cbSec == 0 );
	m_csecHeader = sizeof( LGFILEHDR ) / m_cbSec;

	//	log file size must be at least a
	//	header and at least 2 pages of log data.
	//	Prevent overflow by limiting parmater values.
	//
	m_csecLGFile = CsecLGIFromSize( m_pinst->m_lLogFileSize );
		
	return JET_errSuccess;
	}


BOOL LOG::FLGCheckParams()
	{
	const LONG	csecReserved	= m_csecHeader + 1 + 1;	// +1 for shadow sector, +1 for ultra-safety

	//	ErrSetSystemParameter() ensures a minimum log buffer size and logfile size
	Assert( m_pinst->m_lLogBuffers >= lLogBufferMin );
	Assert( m_csecLGFile * m_cbSec >= lLogFileSizeMin * 1024 );
	Assert( lLogBufferMin * m_cbSec < lLogFileSizeMin * 1024 );

	//	if user set log buffer size greater than logfile size, consider it an error
	if ( m_pinst->m_lLogBuffers > m_csecLGFile )
		{
		char szSize[32];
		_itoa( ( ( m_csecLGFile - csecReserved ) * m_cbSec ) / 1024, szSize, 10 );

		char szBuf[32];
		_itoa( m_pinst->m_lLogBuffers, szBuf, 10 );

		const char *rgsz[] = { szBuf, szSize };
		UtilReportEvent( eventError,
				SYSTEM_PARAMETER_CATEGORY,
				SYS_PARAM_LOGBUFFER_FILE_ERROR_ID, 
				2, 
				rgsz,
				0,
				NULL,
				m_pinst );

		return fFalse;
		}

	//	user specified a log buffer size less than or equal to the log file
	//	size, but we need to sanitize the value to ensure it meets the
	//	following requirements:
	//		- less than or equal to logfilesize-csecReserved (this is
	//		  because we must guarantee that a given log flush will never
	//		  span more than one log file, given that the most amount of
	//		  data we will write to a log file is logfilesize-csecReserved)
	//		- a multiple of the system allocation granularity
	//	NOTE: This fix supercedes the (incomplete) fix AFOXMAN_FIX_148537
	m_pinst->m_lLogBuffers = min( m_pinst->m_lLogBuffers, m_csecLGFile - csecReserved );
	Assert( m_pinst->m_lLogBuffers >= lLogBufferMin );

	UINT	csecAligned	= CsecUtilAllocGranularityAlign( m_pinst->m_lLogBuffers, m_cbSec );
	if ( csecAligned > m_pinst->m_lLogBuffers )
		{
		csecAligned -= ( OSMemoryPageReserveGranularity() / m_cbSec );
		Assert( CsecUtilAllocGranularityAlign( csecAligned, m_cbSec ) == csecAligned );
		Assert( csecAligned < m_pinst->m_lLogBuffers );
		Assert( csecAligned >= lLogBufferMin );
		m_pinst->m_lLogBuffers = csecAligned;
		}
	else
		{
		Assert( csecAligned == m_pinst->m_lLogBuffers );
		}

	return fTrue;
	}


// Rounds up the number of buffers (sectors in the log buffer) to make the log
// buffer be a multiple of the OS memory allocation granularity (currently 64K).
// *ASSUMES* that the OS memory allocation granularity is a multiple of the
// sector size.

ULONG LOG::CsecUtilAllocGranularityAlign( const LONG lBuffers, const LONG cbSec )
	{
	const DWORD		dwPageReserveGranT	= ::OSMemoryPageReserveGranularity();

	Assert( lBuffers > 0 );
	Assert( cbSec > 0 );

	Assert( dwPageReserveGranT % cbSec == 0 );

	return ( ( lBuffers * cbSec - 1 ) / dwPageReserveGranT + 1 ) * dwPageReserveGranT / cbSec;
	}

// Deallocates any storage for the log buffer, if necessary.

void LOG::LGTermLogBuffers()
	{
	if ( !COSMemoryMap::FCanMultiMap() )
		{
		if ( m_pbLGBufMin )
			{
			OSMemoryPageFree( m_pbLGBufMin );
			m_pbLGBufMax = m_pbLGBufMin = NULL;
			}
		}
	else
		{

		//	free the log buffer

		if ( m_pbLGBufMin )
			{
			Assert( m_pbLGBufMax );
			m_osmmLGBuf.OSMMPatternFree();
			m_pbLGBufMin = pbNil;
			m_pbLGBufMax = pbNil;
			}

		//	term the memory map

		m_osmmLGBuf.OSMMTerm();
		}		
	}


ERR LOG::ErrLGInitLogBuffers( LONG lIntendLogBuffers )
	{
	ERR err = JET_errSuccess;
	
	m_critLGBuf.Enter();

	m_csecLGBuf = lIntendLogBuffers;

	Assert( m_csecLGBuf > 4 );
	//	UNDONE: enforce log buffer > a data page

	//	reset log buffer
	//
	// Kill any existing log buffer.
	LGTermLogBuffers();

	// Round up number of sectors in log buffer to make log buffer
	// a multiple of system memory allocation granularity
	// (and thus a multiple of sector size).
	m_csecLGBuf = CsecUtilAllocGranularityAlign( m_csecLGBuf, m_cbSec );
	Assert( m_csecLGBuf > 0 );
	Assert( m_cbSec > 0 );
	m_cbLGBuf = m_csecLGBuf * m_cbSec;
	Assert( m_cbLGBuf > 0 );

	// Log buffer must be multiple of sector size
	Assert( 0 == m_cbLGBuf % m_cbSec );
	// Log buffer must be multiple of memory allocation granularity
	Assert( 0 == m_cbLGBuf % OSMemoryPageReserveGranularity() );

	if ( !COSMemoryMap::FCanMultiMap() )
		{
		if ( m_pbLGBufMin )
			{
			OSMemoryPageFree( m_pbLGBufMin );
			m_pbLGBufMin = NULL;
			}

		m_pbLGBufMin = (BYTE *) PvOSMemoryPageAlloc( m_cbLGBuf * 2, NULL );
		if ( m_pbLGBufMin == NULL )
			{
			Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
			}
		m_pbLGBufMax = m_pbLGBufMin + m_cbLGBuf;
		}
	else
		{
		Assert( pbNil == m_pbLGBufMin );
		Assert( pbNil == m_pbLGBufMax );

		//	init the memory map

		COSMemoryMap::ERR errOSMM;
		errOSMM = m_osmmLGBuf.ErrOSMMInit();
		if ( COSMemoryMap::errSuccess != errOSMM )
			{
			Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
			}

		//	allocate the buffer

		errOSMM = m_osmmLGBuf.ErrOSMMPatternAlloc( m_cbLGBuf, 2 * m_cbLGBuf, (void**)&m_pbLGBufMin );
		if ( COSMemoryMap::errSuccess != errOSMM )
			{
			AssertSz(	COSMemoryMap::errOutOfBackingStore == errOSMM ||
						COSMemoryMap::errOutOfAddressSpace == errOSMM ||
						COSMemoryMap::errOutOfMemory == errOSMM, 
						"unexpected error while allocating memory pattern" );
			Error( ErrERRCheck( JET_errOutOfMemory ), HandleError );
			}
		Assert( m_pbLGBufMin );
		m_pbLGBufMax = m_pbLGBufMin + m_cbLGBuf;
		}

HandleError:
	if ( err < 0 )
		{
		LGTermLogBuffers();
		}

	m_critLGBuf.Leave();
	return err;
	}


#ifdef UNLIMITED_DB
VOID LOG::LGIInitDbListBuffer()
	{
	if ( !m_fLogInitialized )
		return;

	//	place sentinel in ATTACHINFO buffer;

	LRDBLIST*	const plrdblist		= (LRDBLIST *)m_pbLGDbListBuffer;
	plrdblist->lrtyp = lrtypDbList;
	plrdblist->ResetFlags();
	plrdblist->SetCAttachments( 0 );
	plrdblist->SetCbAttachInfo( 1 );
	plrdblist->rgb[0] = 0;

	m_cbLGDbListInUse = sizeof(LRDBLIST) + 1;
	m_cLGAttachments = 0;
	m_fLGNeedToLogDbList = fFalse;
	}
#endif	

CTaskManager *g_plogtaskmanager = NULL;
VOID LOG::LGSignalFlush( void )
	{
	if ( 0 == AtomicCompareExchange( (LONG *)&m_fLGFlushWait, 0, 1 ) )
		{
		Assert( NULL != g_plogtaskmanager );
		if ( JET_errSuccess > g_plogtaskmanager->ErrTMPost( LGFlushLog, 0, (DWORD_PTR)this ) )
			{
			if ( !AtomicExchange( (LONG *)&m_fLGFailedToPostFlushTask, fTrue ) )
				{
				AtomicIncrement( (LONG *)&m_cLGFailedToPost );
				}
				
			if ( 2 == AtomicCompareExchange( (LONG *)&m_fLGFlushWait, 1, 0 ) )
				{
				m_asigLogFlushDone.Set();
				}
			}
		}
	}
	
ERR LOG::ErrLGSystemInit( void )
	{
	ERR err = JET_errSuccess;
	g_plogtaskmanager = new CTaskManager;

	if ( NULL == g_plogtaskmanager )
		{
		err = ErrERRCheck( JET_errOutOfMemory );
		}
	else
		{
		err = g_plogtaskmanager->ErrTMInit( min( 8 * CUtilProcessProcessor(), 100 ), NULL, cmsecAsyncBackgroundCleanup, LGFlushAllLogs );
		if ( JET_errSuccess > err )
			{
			delete g_plogtaskmanager;
			g_plogtaskmanager = NULL;
			}
		}
	return err;
	}

VOID LOG::LGSystemTerm( void )
	{
	if ( NULL != g_plogtaskmanager )
		{
		g_plogtaskmanager->TMTerm();
		delete g_plogtaskmanager;
		}
	g_plogtaskmanager = NULL;
	}

VOID LOG::LGTasksTerm( void )
	{
Retry:
	LGSignalFlush();
	if ( 1 == AtomicExchange( (LONG *)&m_fLGFlushWait, 2 ) )
		{
		m_asigLogFlushDone.Wait();
		}
	m_msLGTaskExec.Partition();

	//	oops, not everything is flushed
	if ( m_fLGFailedToPostFlushTask )
		{
		LONG fStatus = AtomicExchange( (LONG *)&m_fLGFlushWait, 0 );
		Assert( 2 == fStatus );
		UtilSleep( cmsecWaitGeneric );
		goto Retry;
		}
	}

//
//  Initialize global variablas and threads for log manager.
//
ERR LOG::ErrLGInit( IFileSystemAPI *const pfsapi, BOOL *pfNewCheckpointFile )
	{
	ERR		err;
	LONG	lGenMax;

	if ( m_fLogInitialized )
		return JET_errSuccess;

	Assert( m_fLogDisabled == fFalse );

	cLGUsersWaiting.Clear( m_pinst );

#ifdef DEBUG
	 m_lgposLastLogRec = lgposMin;
#endif // DEBUG

#ifdef DEBUG
	AssertLRSizesConsistent();
#endif
	
#ifdef DEBUG
	{
	CHAR	*sz;

	if ( ( sz = GetDebugEnvValue ( _T( "TRACELOG" ) ) ) != NULL )
		{
		m_fDBGTraceLog = fTrue;
		OSMemoryHeapFree(sz);
		}
	else
		m_fDBGTraceLog = fFalse;

	if ( ( sz = GetDebugEnvValue ( _T( "TRACELOGWRITE" ) ) ) != NULL )
		{
		m_fDBGTraceLogWrite = fTrue;
		OSMemoryHeapFree(sz);
		}
	else
		m_fDBGTraceLogWrite = fFalse;

	if ( ( sz = GetDebugEnvValue ( _T( "FREEZECHECKPOINT" ) ) ) != NULL )
		{
		m_fDBGFreezeCheckpoint = fTrue;
		OSMemoryHeapFree(sz);
		}
	else
		m_fDBGFreezeCheckpoint = fFalse;

	if ( ( sz = GetDebugEnvValue ( _T( "TRACEREDO" ) ) ) != NULL )
		{
		m_fDBGTraceRedo = fTrue;
		OSMemoryHeapFree(sz);
		}
	else
		m_fDBGTraceRedo = fFalse;

	if ( ( sz = GetDebugEnvValue ( _T( "TRACEBR" ) ) ) != NULL )
		{
		m_fDBGTraceBR = atoi( sz );
		OSMemoryHeapFree(sz);
		}
	else
		m_fDBGTraceBR = 0;

	if ( ( sz = GetDebugEnvValue ( _T( "IGNOREVERSION" ) ) ) != NULL )
		{
		m_fLGIgnoreVersion = fTrue;
		OSMemoryHeapFree(sz);
		}
	else
		g_fLGIgnoreVersion = fFalse;
	}
#endif

	//	assuming everything will work out
	//
	ResetFNoMoreLogWrite();

	//	log file header must be aligned on correct boundary for device;
	//	which is 16-byte for MIPS and 512-bytes for at least one NT
	//	platform.
	//
	if ( !( m_plgfilehdr = (LGFILEHDR *)PvOSMemoryPageAlloc( sizeof(LGFILEHDR) * 2, NULL ) ) )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
	m_plgfilehdrT = m_plgfilehdr + 1;


#ifdef UNLIMITED_DB
	const ULONG		cbDbListBufferDefault	= OSMemoryPageCommitGranularity();
	if ( !( m_pbLGDbListBuffer = (BYTE *)PvOSMemoryPageAlloc( cbDbListBufferDefault, NULL ) ) )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
	m_cbLGDbListBuffer = cbDbListBufferDefault;

	LGIInitDbListBuffer();
#endif	

	//	the log buffer MUST be SMALLER than any log file so that we only ever 
	//	have 2 log files open in the buffer at any given time;  the current
	//	log file manager only has one m_pbLGFileEnd to designate the end of 
	//	the current log in the buffer -- it cannot represent the ends of 2 log files

	Assert( m_pinst->m_lLogBuffers < m_csecLGFile );

	// always start with new buffer here!

	CallJ( ErrLGInitLogBuffers( m_pinst->m_lLogBuffers ), FreeLGFileHdr );

	//	Initialize trace mechanism

	m_pttFirst = NULL;
	m_pttLast = NULL;

#ifndef LOGPATCH_UNIT_TEST

	m_fLGFlushWait	= 0;

	CallJ( ErrLGICheckpointInit( pfsapi, pfNewCheckpointFile ), StopFlushThread );

#endif	//	!LOGPATCH_UNIT_TEST

	memset( &m_signLog, 0, sizeof( m_signLog ) );
	m_fSignLogSet = fFalse;

	//	determine if we are in "log sequence end" mode

	(void)ErrLGIGetGenerationRange( pfsapi, m_szLogFilePath, NULL, &lGenMax );
	lGenMax += 1;	//	assume edb.log exists (if not, who cares -- if they hit this, they have to shutdown and wipe the log anyway)
	if ( lGenMax >= lGenerationMax )
		{
		m_critLGResFiles.Enter();
		Assert( !m_fLogSequenceEnd );
		m_fLogSequenceEnd = fTrue;
		m_critLGResFiles.Leave();
		}

	m_fLogInitialized = fTrue;

	return err;

#ifndef LOGPATCH_UNIT_TEST
StopFlushThread:

	LGTasksTerm();
#endif	//	!LOGPATCH_UNIT_TEST

FreeLGFileHdr:
	if ( m_plgfilehdr )
		{
		OSMemoryPageFree( m_plgfilehdr );
		}

	LGTermLogBuffers();
	
	return err;
	}


//	Terminates update logging.	Adds quit record to in-memory log,
//	flushes log buffer to disk, updates checkpoint and closes active
//	log generation file.  Frees buffer memory.
//
//	RETURNS	   JET_errSuccess, or error code from failing routine
//
ERR LOG::ErrLGTerm( IFileSystemAPI *const pfsapi, const BOOL fLogQuitRec )
	{
	ERR			err				= JET_errSuccess;
	LE_LGPOS	le_lgposStart;

	//	if logging has been initialized, terminate it!
	//
	if ( !m_fLogInitialized )
		return JET_errSuccess;

	if ( !fLogQuitRec
		|| m_fLGNoMoreLogWrite
		|| !m_pfapiLog )
		goto FreeResources;

	Assert( !m_fRecovering );
		
	//	last written sector should have been written during final flush
	//
	le_lgposStart = m_lgposStart;
	Call( ErrLGQuit( this, &le_lgposStart ) );


	// Keep doing synchronous flushes until all log data
	// is definitely flushed to disk. With FASTFLUSH, a single call to
	// ErrLGFlushLog() may not flush everything in the log buffers.
	Call( ErrLGWaitAllFlushed( pfsapi ) );

#ifdef DEBUG
		{
		// verify that everything in the log buffers have been
		// flushed to disk.
		m_critLGFlush.Enter();
		m_critLGBuf.Enter();

		//	BUG X5:83888 
		//
		//		we create a torn-write after a clean shutdown because we don't flush the last LRCK record
		//		(we weren't seeing it because PbGetEndOfLogData() was pointing AT the LRCK instead of PAST it)
		//
		//		make sure we have flushed up to m_pbEntry (instead of PbGetEndOfLogData())

		LGPOS lgposEndOfData;
		GetLgpos( m_pbEntry, &lgposEndOfData );
		// Everything in the log buffer better be flushed out to disk,
		// otherwise we're hosed.
		Assert( CmpLgpos( &lgposEndOfData, &m_lgposToFlush ) <= 0 );

		m_critLGBuf.Leave();
		m_critLGFlush.Leave();
		}
#endif

	//	flush must have checkpoint log so no need to do checkpoint again
	//
//	Call( ErrLGWriteFileHdr( m_plgfilehdr ) );


	//	check for log-sequence-end
	//	(since we logged a term/rcvquit record, we know this is a clean shutdown)

	if ( err >= JET_errSuccess )
		{
		m_critLGResFiles.Enter();
		if ( m_fLogSequenceEnd )
			{
			err = ErrERRCheck( JET_errLogSequenceEndDatabasesConsistent );
			}
		m_critLGResFiles.Leave();
		Call( err );
		}

FreeResources:
HandleError:

#ifndef LOGPATCH_UNIT_TEST

	//	terminate log checkpoint
	//
	LGICheckpointTerm();

	LGTasksTerm();

#endif	//	!LOGPATCH_UNIT_TEST

	//	close the log file
	//
	delete m_pfapiLog;
	m_pfapiLog = NULL;

		{
		CHAR	szLogName[ IFileSystemAPI::cchPathMax ];

		//	If edbtmp.log is being written to, wait for it to complete and close file.

		LGCreateAsynchWaitForCompletion();
		
		//	Delete any existing edbtmp.log since we would need to recreate
		//	it at next startup anyway (we never trust any prepared log files,
		//	not edbtmp.log, not res1.log, nor res2.log).
		
		LGMakeLogName( szLogName, m_szJetTmp );
		(void) m_pinst->m_pfsapi->ErrFileDelete( szLogName );
		}
	
	//	clean up allocated resources
	//
	LGTermLogBuffers();

#ifdef UNLIMITED_DB
	OSMemoryPageFree( m_pbLGDbListBuffer );
	m_pbLGDbListBuffer = NULL;
	m_cbLGDbListBuffer = 0;
#endif	
	
	OSMemoryPageFree( m_plgfilehdr );
	m_plgfilehdr = NULL;

	OSMemoryPageFree ((void *) m_pcheckpointDeleted);
	m_pcheckpointDeleted = NULL;

	//	check for log-sequence-end

	if ( err >= JET_errSuccess )
		{
		m_critLGResFiles.Enter();
		if ( m_fLogSequenceEnd )
			{
			err = ErrERRCheck( JET_errLogSequenceEnd );
			}
		m_critLGResFiles.Leave();
		}

	m_fLogInitialized = fFalse;
	
	return err;
	}


//********************* LOGGING ****************************
//**********************************************************


// Log buffer utilities to make it easy to verify that we're
// using data in the log buffer that is "used" (allocated)
// (between m_pbWrite and m_pbEntry) and "free" (unallocated).
// Takes into consideration VM wrap-around and the circularity
// of the log buffer.


// We should not reference data > PbMaxEntry() when dealing
// with valid data in the log buffer.

const BYTE*	LOG::PbMaxEntry()
	{
	// need for access to m_pbWrite and m_pbEntry
#ifdef DEBUG
	if ( !m_fRecovering )
		{
		Assert( m_critLGBuf.FOwner() );
		}
#endif
	Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );
	Assert( m_pbWrite >= m_pbLGBufMin && m_pbWrite < m_pbLGBufMax );
	if ( m_pbEntry > m_pbWrite )
		return m_pbEntry;
	else
		return m_pbEntry + m_cbLGBuf;
	}


// When adding log records to the log buffer, we should not copy data
// into the region past PbMaxWrite().

const BYTE*	LOG::PbMaxWrite()
	{
#ifdef DEBUG
	if ( !m_fRecovering )
		{
		Assert( m_critLGBuf.FOwner() );
		}
#endif
	Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );
	Assert( m_pbWrite >= m_pbLGBufMin && m_pbWrite < m_pbLGBufMax );
	if ( m_pbWrite >= m_pbEntry )
		return m_pbWrite;
	else
		return m_pbWrite + m_cbLGBuf;
	}

// Normalizes a pointer into the log buffer for use with
// comparisons to test whether the data the pointer points to
// is used.

const BYTE*	LOG::PbMaxPtrForUsed(const BYTE* const pb)
	{
#ifdef DEBUG
	if ( !m_fRecovering )
		{
		Assert( m_critLGBuf.FOwner() );
		}
#endif
	Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );
	Assert( m_pbWrite >= m_pbLGBufMin && m_pbWrite < m_pbLGBufMax );
	// below may need to be pb <= m_pbLGBufMax
	Assert( pb >= m_pbLGBufMin && pb < m_pbLGBufMax );
	if ( pb < m_pbEntry )
		return pb + m_cbLGBuf;
	else
		return pb;
	}

// Normalizes a pointer into the log buffer for use with
// comparisons to test whether the data the pointer points to
// is free.

const BYTE*	LOG::PbMaxPtrForFree(const BYTE* const pb)
	{
#ifdef DEBUG
	if ( !m_fRecovering )
		{
		Assert( m_critLGBuf.FOwner() );
		}
#endif
	Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );
	Assert( m_pbWrite >= m_pbLGBufMin && m_pbWrite < m_pbLGBufMax );
	// below may need to be pb <= m_pbLGBufMax
	Assert( pb >= m_pbLGBufMin && pb < m_pbLGBufMax );
	if ( pb < m_pbWrite )
		return pb + m_cbLGBuf;
	else
		return pb;
	}

// In use data in log buffer is between m_pbWrite and PbMaxEntry().

ULONG	LOG::CbLGUsed()
	{
	return ULONG( PbMaxEntry() - m_pbWrite );
	}

// Available space in log buffer is between the entry point
// and the start of the real data

ULONG	LOG::CbLGFree()
	{
	return ULONG( PbMaxWrite() - m_pbEntry );
	}

// Internal implementation function to determine whether
// cb bytes at pb are in the free portion of the log buffer.

BOOL	LOG::FLGIIsFreeSpace(const BYTE* const pb, ULONG cb)
	{
	Assert( cb < m_cbLGBuf );
	// There is never a time when the entire log buffer
	// is free. This state never occurs.
	Assert( cb != m_cbLGBuf );

	if ( cb == 0 )
		return fTrue;

	return ( CbLGFree() >= cb ) &&
		( PbMaxPtrForUsed( pb ) >= m_pbEntry && PbMaxPtrForUsed( pb ) < PbMaxWrite() ) &&
		( PbMaxPtrForUsed( pb ) + cb > m_pbEntry && PbMaxPtrForUsed( pb ) + cb <= PbMaxWrite() );
	}

// Returns whether cb bytes at pb are in the free portion of the log buffer.

BOOL	LOG::FIsFreeSpace(const BYTE* const pb, ULONG cb)
	{
	const BOOL fResult = FLGIIsFreeSpace( pb, cb );
	// verify with cousin
	Assert( fResult == ! FLGIIsUsedSpace( pb, cb ) );
	return fResult;
	}

// Internal implementation function to determine whether cb
// bytes at pb are in the used portion of the log buffer.

BOOL	LOG::FLGIIsUsedSpace(const BYTE* const pb, ULONG cb)
	{
	Assert( cb <= m_cbLGBuf );

	if ( cb == 0 )
		return fFalse;

	if ( cb < m_cbLGBuf )
		{
		return ( CbLGUsed() >= cb ) &&
			( PbMaxPtrForFree( pb ) >= m_pbWrite && PbMaxPtrForFree( pb ) < PbMaxEntry() ) &&
			( PbMaxPtrForFree( pb ) + cb > m_pbWrite && PbMaxPtrForFree( pb ) + cb <= PbMaxEntry() );
		}
	else
		{
		// special case
		return pb == m_pbWrite && pb == m_pbEntry;
		}
	}

// Returns whether cb bytes at pb are in the used portion of the log buffer.

BOOL	LOG::FIsUsedSpace(const BYTE* const pb, ULONG cb)
	{
	const BOOL fResult = FLGIIsUsedSpace( pb, cb );
	// verify with cousin
#ifdef DEBUG
	// If the user is asking if all of the log buffer is used,
	// we don't want to verify with FLGIIsFreeSpace() because that
	// is an impossibility for us.
	if ( cb != m_cbLGBuf )
		{
		Assert( fResult == ! FLGIIsFreeSpace( pb, cb ) );
		}
#endif
	return fResult;
	}


//	adds log record defined by (pb,cb) 
//	at *ppbET, wrapping around if necessary

VOID LOG::LGIAddLogRec( const BYTE* const pb, INT cb, BYTE **ppbET )
	{
	BYTE *pbET = *ppbET;

	Assert( cb < m_pbLGBufMax - m_pbLGBufMin );

	// With wraparound, we don't need to do 2 separate copies.
	// We can just blast into the address space and it'll go
	// into the second mapping.

	// We must be pointing into the main mapping of the log buffer.
	Assert( pbET >= m_pbLGBufMin );
	Assert( pbET < m_pbLGBufMax );

	// Ensure that we don't memcpy() past our 2 mappings.
	Assert( pbET + cb <= m_pbLGBufMin + 2 * ( m_cbLGBuf ) );

#ifdef DEBUG
		{
		// m_pbWrite should be pointing inside the log buffer.
		Assert( m_pbWrite >= m_pbLGBufMin && m_pbWrite < m_pbLGBufMax );
		
		BYTE *pbWrapWrite = ( m_pbWrite < pbET ) ? ( m_pbWrite + m_cbLGBuf ) : m_pbWrite;
		// Ensure that we don't write into space where valid
		// log records already exist.
		Assert( pbET + cb <= pbWrapWrite );
		}
#endif

	// Do not kill valid log records!
	Assert( FIsFreeSpace( pbET, cb ) );

	UtilMemCpy( pbET, pb, cb );
	if ( !COSMemoryMap::FCanMultiMap() )
		{
		// simulate wrap around by copying the logs on LGBuf and mapped LGBuf
		Assert( pbET < m_pbLGBufMax );
		ULONG cbToLGBufEnd = ULONG( m_pbLGBufMax - pbET );
	
		memcpy( pbET + m_cbLGBuf, pb, min( cb, cbToLGBufEnd ) );
		if ( cb > cbToLGBufEnd )
			{
			memcpy( m_pbLGBufMin, pb + cbToLGBufEnd, cb - cbToLGBufEnd );
			}
		}

	//	return next available entry
	//
	pbET += cb;
	// If we're now pointing into the 2nd mapping, fix us up
	// so we point into the main mapping of the log buffer.
	if ( pbET >= m_pbLGBufMax )
		{
		// We should never be pointing past the 2nd mapping.
		Assert( pbET < m_pbLGBufMax + m_cbLGBuf );
		pbET -= m_cbLGBuf;
		}
	// Point inside first mapping.
	Assert( pbET >= m_pbLGBufMin && pbET < m_pbLGBufMax );
	*ppbET = pbET;
	return;
	}


//
//	Add log record to circular log buffer. Signal flush thread to flush log
//	buffer if at least (g_lLogBuffers / 2) disk sectors are ready for flushing.
//	Return error if we run out of log buffer space.
//
//	RETURNS		JET_errSuccess, or error code from failing routine
//				or errLGNotSynchronous if there's not enough space,
//				or JET_errLogWriteFail if the log is down.
//

#ifdef DEBUG
BYTE rgbDumpLogRec[ 8192 ];
#endif

ERR LOG::ErrLGILogRec( const DATA *rgdata, const INT cdata, const BOOL fLGFlags, LGPOS *plgposLogRec )
	{
	ERR		err			= JET_errSuccess;
	INT		cbReq;
	INT		idata;
	BOOL	fNewGen		= ( fLGFlags & fLGCreateNewGen );
	BYTE*	pbSectorBoundary;
	BOOL	fFormatJetTmpLog	= fFalse;

#ifdef DEBUG
	BYTE*	pbOldEntry = pbNil;
#endif

	// No one should be adding log records if we're in redo mode.
	Assert( fFalse == m_fRecovering || ( fTrue == m_fRecovering && fRecoveringRedo != m_fRecoveringMode ) );

	Assert( !m_fLogDisabledDueToRecoveryFailure );
	if ( m_fLogDisabledDueToRecoveryFailure )
		return ErrERRCheck( JET_errLogDisabledDueToRecoveryFailure );
		
	Assert( m_fLogDisabled == fFalse );
	Assert( rgdata[0].Pv() != NULL );
	Assert( !m_fDBGNoLog );

	//	cbReq is total net space required for record
	//
	for ( cbReq = 0, idata = 0; idata < cdata; idata++ )
		{
		cbReq += rgdata[idata].Cb();
		}

	//	get m_pbEntry in order to add log record
	//
	forever
		{
		INT		ibEntry;
		INT		csecReady;
		INT		cbAvail;
		INT		csecToExclude;
		INT		cbFraction;
		INT		csecToAdd;
		INT		isecLGFileEndT = 0;

		m_critLGBuf.Enter();
		if ( m_fLGNoMoreLogWrite )
			{
			m_critLGBuf.Leave();
			return ErrERRCheck( JET_errLogWriteFail );
			}

		else if ( !m_fRecovering )
			{
			const LONG	lgenCurr			= m_plgfilehdr->lgfilehdr.le_lGeneration;
			const LONG	lgenCheckpoint		= m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration;
			const LONG	lgenOutstanding		= lgenCurr - lgenCheckpoint;

			//	bail out if we're getting too close to the max
			//	precision for the BFOB0 approximate index
			//
			if ( lgenOutstanding > ( lgenCheckpointDepthMax - 0x10 )
				&& lgenCheckpoint > 0 )			//	may be 0 if previous shutdown was clean and haven't updated checkpoint yet
				{
				const LRTYP		lrtyp		= *( (LRTYP *)rgdata[0].Pv() );
				switch ( lrtyp )
					{
					case lrtypRollback:
					case lrtypUndo:
					case lrtypUndoInfo:
					case lrtypShutDownMark:
					case lrtypTerm:
					case lrtypEnd:
					case lrtypChecksum:
					case lrtypTrace:
					case lrtypNOP:
						//	attempt to permit clean shutdown, by letting
						//	rollback and term log records go a little further
						//
						if ( lgenOutstanding <= ( lgenCheckpointDepthMax - 0x8 ) )
							{
							break;
							}

						//	FALL THROUGH
						
					default:
						m_critLGBuf.Leave();
						return ErrERRCheck( JET_errCheckpointDepthTooDeep );
					}
				}
			}

		// XXX
		// Should probably Assert() that we have a valid m_pbLastChecksum

		//	if just initialized or no input since last flush
		//

		// We should always be dealing with valid pointers.
		Assert( m_pbWrite >= m_pbLGBufMin );
		Assert( m_pbWrite < m_pbLGBufMax );
		Assert( m_pbEntry >= m_pbLGBufMin );
		Assert( m_pbEntry < m_pbLGBufMax );

		//	calculate available space
		//
		// This case is handled properly by the below code, but we're
		// just curious how often this happens.
		AssertSz( m_pbWrite != m_pbEntry,
			"We just wanted to know how often m_pbWrite == m_pbEntry. Press OK to continue.");
		// m_pbWrite == m_pbEntry means log buffer is FULL (which is ok)!!!!
		if ( m_pbWrite >= m_pbEntry )
			cbAvail = ULONG( m_pbWrite - m_pbEntry );
		else
			cbAvail = ULONG( ( m_pbLGBufMax - m_pbEntry ) + ( m_pbWrite - m_pbLGBufMin ) );

		if ( 0 == cbAvail )
			{
			// XXX
			// I now believe that this can happen. Consider the case where
			// we're flushing and we just added the new LRCHECKSUM at the
			// end of the log buffer, using up all available space. Then
			// we release m_critLGBuf and start doing our I/O. During that
			// I/O time, someone tries to add new log records -- then
			// m_pbEntry == m_pbWrite which means no space. They just need
			// to wait until the I/O completes and the flush thread
			// moves forward m_pbWrite which will free up space.
			//
			// What will happen now is that the goto Restart will signal
			// the flush thread to flush (it's currently flushing),
			// we'll return errLGSynchronousWait_Whatever, the flush will
			// finish and the client will try to add the log record again.
			AssertSz( fFalse, "No space in log buffer to append (or flush). Flush must be in progress. Press OK to continue.");
			// See bug VisualStudio7:202353 for info on VC7 beta not
			// generating the code for this goto statement.
			goto Restart;
			}

		//	calculate sectors of buffer ready to flush. Excluding
		//	the half filled sector.
		 
		csecToExclude = ( cbAvail - 1 ) / m_cbSec + 1;
		csecReady = m_csecLGBuf - csecToExclude;

		//	check if add this record, it will reach the point of
		//	the sector before last sector of current log file and the point
		//	is enough to hold a lrtypMS and lrtypEnd. Note we always
		//	reserve the last sector as a shadow sector.
		//	if it reach the point, check if there is enough space to put
		//	NOP to the end of log file and cbLGMSOverhead of NOP's for the
		//	first sector of next generation log file. And then set m_pbEntry
		//	there and continue adding log record.
		 
		
		if ( m_pbLGFileEnd != pbNil )
			{
			Assert( m_isecLGFileEnd != 0 );
			// already creating new log, do not bother to check fNewGen. 
			goto AfterCheckEndOfFile;
			}

		ibEntry = ULONG( m_pbEntry - PbSecAligned( m_pbEntry ) );
		
		//	check if after adding this record, the sector will reach
		//	the sector before last sector. If it does, let's patch NOP.
		 
		if ( fNewGen )
			{
#ifdef UNLIMITED_DB
			//	if AttachInfo needs to be logged, then we must have just
			//	started a new gen
			Assert( !m_fLGNeedToLogDbList );
#endif

			// last sector is the one that will be patched with NOP

			// Notice that with FASTFLUSH, we don't need any terminating
			// LRCHECKSUM at the end of the log file. The last LRCHECKSUM
			// (before the last bunch of data) will have a cbNext == 0
			// which signifies that its range that it covers is the last
			// range of data in the log file. Note that it may be preferable
			// to change the format to always have a terminating LRCHECKSUM
			// simplify the startup code (so we don't need to check to see
			// if we need to create a new log file in a certain circumstance).
			cbFraction = ibEntry;
			if ( 0 == cbFraction )
				{
				csecToAdd = 0;
				}
			else
				{
				csecToAdd = ( cbFraction - 1 ) / m_cbSec + 1;
				}
			Assert( csecToAdd >= 0 && csecToAdd <= 1 );
			isecLGFileEndT = m_isecWrite + csecReady + csecToAdd;
			}
		else
			{
			// check if new gen is necessary

			// for the end of the log file, we do not need to stick
			// in an LRCHECKSUM at the end.
#ifdef UNLIMITED_DB
			cbFraction = ibEntry + cbReq + ( m_fLGNeedToLogDbList ? m_cbLGDbListInUse : 0 );
#else
			cbFraction = ibEntry + cbReq;
#endif			
			if ( 0 == cbFraction )
				{
				csecToAdd = 0;
				}
			else
				{
				csecToAdd = ( cbFraction - 1 ) / m_cbSec + 1;
				}
			Assert( csecToAdd >= 0 );

			// The m_csecLGFile - 1 is because the last sector of the log file
			// is reserved for the shadow sector.

			if ( csecReady + m_isecWrite + csecToAdd >= ( m_csecLGFile - 1 ) )
				{
#ifdef UNLIMITED_DB
				//	if this enforce fires, it means we're at the start of
				//	a log record, but the logfile is not big enough to
				//	fit the attachment list plus the current log record
				Enforce( !m_fLGNeedToLogDbList );
#endif

				// We can't fit the new log records and an LRCHECKSUM,
				// so let's make a new log generation.
				// - 1 is to reserve last sector as shadow sector.
				isecLGFileEndT = m_csecLGFile - 1;
				fNewGen = fTrue;
				}
			}

		if ( fNewGen )
			{
			INT cbToFill;

			//	Adding the new record, we will reach the point. So let's
			//	check if there is enough space to patch NOP all the way to
			//	the end of file. If not enough, then wait the log flush to
			//	flush.
			 
			INT csecToFill = isecLGFileEndT - ( m_isecWrite + csecReady );
			Assert( csecToFill > 0 || csecToFill == 0 && ibEntry == 0 );

			// We fill up to the end of the file, plus the first
			// cbLGMSOverhead bytes of the next sector in the log file.
			// This is to reserve space for the beginning LRMS in the
			// next log file.

			// This implicitly takes advantage of the knowledge that
			// we'll have free space up to a sector boundary -- in other
			// words, m_pbWrite is always sector aligned.

			// See bug VisualStudio7:202353 for info on VC7 beta not
			// generating the code for this goto statement.

			if ( ( cbAvail / m_cbSec ) <= csecToFill + 1 )
				//	available space is not enough to fill to end of file plus
				//	first sector of next generation. Wait flush to generate
				//	more space.
				 
				goto Restart;

			//	now we have enough space to patch.
			//	Let's set m_pbLGFileEnd for log flush to generate new log file.

			//	Zero out sizeof( LRCHECKSUM ) bytes in the log buffer that will
			//	later be written to the next log file.
			cbToFill = csecToFill * m_cbSec - ibEntry + sizeof( LRCHECKSUM );
			
			//	If this Assert() goes off, please verify that the compiler has
			//	generated code for the "goto Restart" above. In January 2001,
			//	spenlow found a codegen problem with version 13.0.9037
			//	of the compiler (VC7 beta). See bug VisualStudio7:202353.
			AssertRTL( cbToFill <= cbAvail );

#ifdef DEBUG
			pbOldEntry = m_pbEntry;
#endif
			// Takes advantage of VM wrap-around to memset() over wraparound boundary.
			Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );
			Assert( FIsFreeSpace( m_pbEntry, cbToFill ) );
			memset( m_pbEntry, lrtypNOP, cbToFill );

			if ( !COSMemoryMap::FCanMultiMap() )
				{
				memset( m_pbEntry + m_cbLGBuf, lrtypNOP, min( cbToFill, m_pbLGBufMax - m_pbEntry ) );
				if ( m_pbEntry + cbToFill > m_pbLGBufMax )
					memset( m_pbLGBufMin, lrtypNOP, m_pbEntry + cbToFill - m_pbLGBufMax );
				}

			m_pbEntry += cbToFill;
			if ( m_pbEntry >= m_pbLGBufMax )
				{
				m_pbEntry -= m_cbLGBuf;
				}
			Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );
			Assert( sizeof( LRCHECKSUM ) == m_pbEntry - PbSecAligned( m_pbEntry ) );
			Assert( FIsUsedSpace( pbOldEntry, cbToFill ) );

			// m_pbLGFileEnd points to the LRMS for use in the next log file.

			// Setup m_pbLGFileEnd to point to the sector boundary of the last
			// stuff that should be written to the log file. This is noticed
			// by ErrLGIWriteFullSectors() to switch to the next log file.
			m_pbLGFileEnd = m_pbEntry - sizeof( LRCHECKSUM );
			// m_pbLGFileEnd should be sector aligned
			Assert( PbSecAligned( m_pbLGFileEnd ) == m_pbLGFileEnd );
			m_isecLGFileEnd = isecLGFileEndT;

#ifdef UNLIMITED_DB
			Assert( !m_fLGNeedToLogDbList );
			m_fLGNeedToLogDbList = fTrue;
#endif			

			// No need to setup m_lgposMaxFlushPoint here since
			// flushing thread will do that. Notice that another
			// thread can get in to m_critLGBuf before flushing thread
			// and they can increase m_lgposMaxFlushPoint.

			// send signal to generate new log file 
			LGSignalFlush();
			
			// start all over again with new m_pbEntry, cbAvail etc. 
			m_critLGBuf.Leave();
			continue;
			}


AfterCheckEndOfFile:

		// Because m_csecLGBuf differs from m_pinst->m_lLogBuffers
		// because the log buffers have to be a multiple of the system
		// memory allocation granularity, the flush threshold should be
		// half of the actual size of the log buffer (not the logical
		// user requested size).
		if ( csecReady > m_csecLGBuf / 2 )
			{
			//	reach the threshold, flush before adding new record
			//
			// XXX
			// The above comment seems misleading because although
			// this signals the flush thread, the actual flushing
			// doesn't happen until we release m_critLGBuf which
			// doesn't happen until we add the new log records and exit.
			LGSignalFlush();
			cLGCapacityFlush.Inc( m_pinst );
			}

#ifdef UNLIMITED_DB
		//	we're now committed to adding the log record
		//	to the log buffer, so we may update cbReq
		//	if we need to log AttachInfo
		if ( m_fLGNeedToLogDbList )
			{
			cbReq += m_cbLGDbListInUse;
			}
#endif			

		// make sure cbAvail is enough to hold one LRMS and end type.

		// XXX
		// I think this can be ">=" since we don't need space
		// for an lrtypEnd or anything like that. Why did the
		// !FASTFLUSH code use > instead of >=??? Since
		// cbLGMSOverhead takes into account the lrtypEnd,
		// why does it need to be >????

		// If we have enough space in the log buffer for
		// the log records and for an LRCHECKSUM (we always need
		// space for it for the next flush to insert).

		// Note that this implicitly keeps in mind the NOP-space
		// that we will have to fill the last sector before
		// putting an LRMS on the next sector (so it's contiguous
		// on one sector).
		if ( cbAvail > cbReq + sizeof( LRCHECKSUM ) )
			{
			//	no need to flush 
			break;
			}
		else
			{
			//	restart.  Leave critical section for other users
			//
Restart:
			LGSignalFlush();
			cLGStall.Inc( m_pinst );
			err = ErrERRCheck( errLGNotSynchronous );
			Assert( m_critLGBuf.FOwner() );
			goto FormatJetTmpLog;
			}
		}


#ifdef UNLIMITED_DB
	if ( fLGFlags & fLGInterpretLR )
		{
		const LRTYP		lrtyp	= *( (LRTYP *)rgdata[0].Pv() );
		switch ( lrtyp )
			{
			case lrtypCreateDB:
				err = ErrLGPreallocateDbList_( ( (LRCREATEDB *)rgdata[0].Pv() )->dbid );
				break;
			case lrtypAttachDB:
				err = ErrLGPreallocateDbList_( ( (LRATTACHDB *)rgdata[0].Pv() )->dbid );
				break;

			default:
				AssertSzRTL( fFalse, "Unexpected LRTYP" );
			case lrtypDetachDB:
			case lrtypForceDetachDB:
				err = JET_errSuccess;
				break;
			}

		if ( err < 0 )
			{
			CallSx( err, JET_errOutOfMemory );
			Assert( m_fLGNoMoreLogWrite );
			m_critLGBuf.Leave();
			return err;
			}
		}
#endif


	//	now we are holding m_pbEntry, let's add the log record.
	//
	GetLgposOfPbEntry( &m_lgposLogRec );
	if ( m_fRecovering )
		{
		cLGRecordOffset.Clear( m_pinst );
		}
	else
		{
		cLGRecordOffset.Set( m_pinst, CbOffsetLgpos( m_lgposLogRec, lgposMin ) );
		}
#ifdef DEBUG
	Assert( CmpLgpos( &m_lgposLastLogRec, &m_lgposLogRec ) < 0 );
	m_lgposLastLogRec = m_lgposLogRec;
#endif

	if ( plgposLogRec )
		*plgposLogRec = m_lgposLogRec;

	Assert( m_pbEntry >= m_pbLGBufMin );
	Assert( m_pbEntry < m_pbLGBufMax );


	//	setup m_lgposMaxFlushPoint so it points to the first byte of log data that will NOT be making it to disk
	//
	//	in the case of a multi-sector flush, this will point to the log record which "hangs over" the sector
	//		boundary; it will mark that log record as not being flushed and will thus prevent the buffer manager
	//		from flushing the database page
	//
	//	in the case of a single-sector flush, this will not be used at all

	//	calculate the sector boundary where the partial data begins 
	//		(full-sector data is before this point -- there may not be any)

	pbSectorBoundary = PbSecAligned( m_pbEntry + cbReq );

	// note that we use PbGetEndOfLogData() so we don't point after
	// a LRCK that was just added. bwahahahaha!!
	if ( PbSecAligned( PbGetEndOfLogData() ) == pbSectorBoundary )
		{

		//	the new log record did not put us past a sector boundary
		//
		//	it was put into a partially full sector and does not hang over, so do not bother settting
		//		m_lgposMaxFlushPoint
		
		}
	else
		{

		//	the new log record is part of a multi-sector flush
		//
		//	if it hangs over the edge, then we cannot include it when calculating m_lgposMaxFlushPoint
		
		BYTE *		pbEnd					= m_pbEntry;

#ifdef UNLIMITED_DB
		BOOL		fCheckDataFitsInSector	= fTrue;
		if ( m_fLGNeedToLogDbList )
			{
			BYTE * const pbLREnd = pbEnd + m_cbLGDbListInUse;

			if ( pbLREnd > pbSectorBoundary )
				{
				//	DbList already forces us to go beyond the
				//	sector boundary, so no need to check rest
				//	of data
				fCheckDataFitsInSector = fFalse;
				Assert( pbEnd == m_pbEntry );
				}
			else
				{
				pbEnd = pbLREnd;
				}
			}
#else
		const BOOL	fCheckDataFitsInSector	= fTrue;
#endif

		if ( fCheckDataFitsInSector )
			{
			for ( idata = 0; idata < cdata; idata++ )
				{
				BYTE * const pbLREnd = pbEnd + rgdata[ idata ].Cb();
				if ( pbLREnd > pbSectorBoundary )
					{

					//	the log record hangs over the edge and cannot be included in m_lgposMaxFlushPoint
					//
					//	reset pbEnd so it points to the start of the log record 
					//		(we use pbEnd to set m_lgposMaxFlushPoint)

					pbEnd = m_pbEntry;
					break;
					}
				else
					{

					//	this segment of the log record will not hang over
					//
					//	include it and continue checking

					pbEnd = pbLREnd;
					}
				}
			}


#ifdef DEBUG
		LGPOS lgposOldMaxFlushPoint = m_lgposMaxFlushPoint;
#endif	//	DEBUG

		//	notice that pbEnd may have wrapped into the second mapping of the log buffer

		Assert( pbEnd >= m_pbLGBufMin && pbEnd < ( m_pbLGBufMax + m_cbLGBuf ) );
		GetLgpos( ( pbEnd >= m_pbLGBufMax ) ? ( pbEnd - m_cbLGBuf ) : ( pbEnd ),
			&m_lgposMaxFlushPoint );
		Assert( CmpLgpos( &m_lgposMaxFlushPoint, &m_lgposLogRec ) >= 0 );

		// max flush point must always be increasing.

		Assert( CmpLgpos( &lgposOldMaxFlushPoint, &m_lgposMaxFlushPoint ) <= 0 );

#ifdef DEBUG
		//	m_lgposMaxFlushPoint should be pointing to the beginning OR the end of the last log record

		LGPOS lgposLogRecEndT = m_lgposLogRec;
		AddLgpos( &lgposLogRecEndT, cbReq );
		Assert( CmpLgpos( &m_lgposMaxFlushPoint, &m_lgposLogRec ) == 0 ||
				CmpLgpos( &m_lgposMaxFlushPoint, &lgposLogRecEndT ) == 0 );
#endif	//	DEBUG
		}

#ifdef DEBUG
	pbOldEntry = m_pbEntry;
#endif
	Assert( FIsFreeSpace( m_pbEntry, cbReq ) );

#ifdef UNLIMITED_DB
	if ( m_fLGNeedToLogDbList )
		{
		LGIAddLogRec( m_pbLGDbListBuffer, m_cbLGDbListInUse, &m_pbEntry );
		m_fLGNeedToLogDbList = fFalse;
		}
#endif		

	//  add all the data streams to the log buffer.  if we catch an exception
	//  during this process, we will fail the addition of the log record with an
	//  error but we will still store a fully formed log record in the buffer by
	//  filling it out with a fill pattern

	//  CONSIDER:  handle this zero buffer differently

		{
		static const BYTE rgbFill[ g_cbPageMax ] = { 0 };

		TRY
			{
			for ( idata = 0; idata < cdata; idata++ )
				{
				Assert( rgdata[idata].Cb() <= sizeof( rgbFill ) );
				
				LGIAddLogRec( (BYTE *)rgdata[idata].Pv(), rgdata[idata].Cb(), &m_pbEntry );
				}
			}
		EXCEPT( efaExecuteHandler )
			{
			for ( ; idata < cdata; idata++ )
				{
				Assert( rgdata[idata].Cb() <= sizeof( rgbFill ) );
				
				LGIAddLogRec( rgbFill, rgdata[idata].Cb(), &m_pbEntry );
				}

			err = ErrERRCheck( errLGRecordDataInaccessible );
			}
		ENDEXCEPT
		}

	Assert( FIsUsedSpace( pbOldEntry, cbReq ) );

	Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );

	//	add a dummy fill record to indicate end-of-data
	//
	// XXX
	// Since we just did a bunch of AddLogRec()'s, how can m_pbEntry be
	// equal to m_pbLGBufMax? This seems like it should be an Assert().
	// Maybe in the case where the number of log records added is zero?
	// That itself should be an Assert()

	Assert( m_pbEntry < m_pbLGBufMax && m_pbEntry >= m_pbLGBufMin );

#ifdef DEBUG
	if ( m_fDBGTraceLog )
		{
		DWORD dwCurrentThreadId = DwUtilThreadId();
		BYTE *pb;
		
		g_critDBGPrint.Enter();

		//	must access rgbDumpLogRec in g_critDBGPrint.
		 
		pb = rgbDumpLogRec;
		for ( idata = 0; idata < cdata; idata++ )
			{
			UtilMemCpy( pb, rgdata[idata].Pv(), rgdata[idata].Cb() );
			pb += rgdata[idata].Cb();
			}

		LR	*plr	= (LR *)rgbDumpLogRec;

		//	we nevee explicitly log NOPs
		Assert( lrtypNOP != plr->lrtyp );
		Assert( 0 == GetNOP() );
		
		if ( dwCurrentThreadId == m_dwDBGLogThreadId )
			DBGprintf( "$");
		else if ( FLGDebugLogRec( plr ) )
			DBGprintf( "#");
		else
			DBGprintf( "<");
				
		DBGprintf( " {%u} %u,%u,%u",
					dwCurrentThreadId,
					m_lgposLogRec.lGeneration,
					m_lgposLogRec.isec,
					m_lgposLogRec.ib );
		ShowLR( plr, this );
		
		g_critDBGPrint.Leave();
		}
#endif

//	GetLgposOfPbEntry( &lgposEntryT );

	//  monitor statistics  
	cLGRecord.Inc( m_pinst );


#ifdef UNLIMITED_DB
	if ( fLGFlags & fLGInterpretLR )
		{
		const LRTYP		lrtyp	= *( (LRTYP *)rgdata[0].Pv() );
		switch ( lrtyp )
			{
			case lrtypCreateDB:
				LGAddToDbList_( ( (LRCREATEDB *)rgdata[0].Pv() )->dbid );
				break;
			case lrtypAttachDB:
				LGAddToDbList_( ( (LRATTACHDB *)rgdata[0].Pv() )->dbid );
				break;
			case lrtypDetachDB:
			case lrtypForceDetachDB:
				LGRemoveFromDbList_( ( (LRDETACHCOMMON *)rgdata[0].Pv() )->dbid );
				break;
			default:
				AssertSzRTL( fFalse, "Unexpected LRTYP" );
			}
		}
#endif
	
FormatJetTmpLog:

	Assert( m_critLGBuf.FOwner() );

	if ( CmpLgpos( m_lgposLogRec, m_lgposCreateAsynchTrigger ) >= 0 &&
		FLGICreateAsynchIOCompletedTryWait() )
		{
		Assert( m_fCreateAsynchLogFile );

		//	Everyone else's trigger-checks should fail since this thread will handle this
		
		m_lgposCreateAsynchTrigger = lgposMax;

		fFormatJetTmpLog = fTrue;
		}
		
	//	now we are done with the insertion to buffer.
	//
	m_critLGBuf.Leave();

	if ( fFormatJetTmpLog )
		{
		LGICreateAsynchIOIssue();
		}

	return err;
	}


//	This function was created as a workaround to a VC7 beta compiler
//	codegen bug that caused "goto Restart" statements in ErrLGILogRec()
//	to have no code (and no magic fall-through). See bug VisualStudio7:202353.
//	The workaround is to reduce the complexity of ErrLGILogRec() by
//	putting this code in FLGICreateAsynchIOCompleted() and forcing the
//	function to never be inlined by the compiler.

#pragma auto_inline( off )

BOOL LOG::FLGICreateAsynchIOCompletedTryWait()
	{
	return m_asigCreateAsynchIOCompleted.FTryWait();
	}

#pragma auto_inline( on )


ERR LOG::ErrLGTryLogRec(
	const DATA	* const rgdata,
	const ULONG	cdata,
	const BOOL	fLGFlags,
	LGPOS		* const plgposLogRec )
	{
	if ( m_pttFirst )
		{
		ERR		err		= JET_errSuccess;

		//	There a list of trace in temp memory structure. Log them first.
		do {
			m_critLGTrace.Enter();
			if ( m_pttFirst )
				{
				TEMPTRACE *ptt = m_pttFirst;
				err = ErrLGITrace( ptt->ppib, ptt->szData, fTrue /* fInternal */ );
				m_pttFirst = ptt->pttNext;
				if ( m_pttFirst == NULL )
					{
					Assert( m_pttLast == ptt );
					m_pttLast = NULL;
					}
				OSMemoryHeapFree( ptt );
				}
			m_critLGTrace.Leave();
			} while ( m_pttFirst != NULL && err == JET_errSuccess );

		if ( err != JET_errSuccess )
			return err;
		}

	//	No trace to log or trace list is taken care of, log the normal log record

	return ErrLGILogRec( rgdata, cdata, fLGFlags, plgposLogRec );
	}

//	Group commits, by waiting to give others a chance to flush this
//	and other commits.
//
ERR LOG::ErrLGWaitCommit0Flush( PIB *ppib )
	{
	ERR		err			= JET_errSuccess;
	BOOL	fFlushLog	= fFalse;

	//  if the log is disabled or we are recovering, skip the wait

	if ( m_fLogDisabled || m_fRecovering )
		{
		goto Done;
		}

	//  this session had better have a log record it is waiting on
	//  NOTE:  this is usually a Commit0, but can be a CreateDB/AttachDB/DetachDB

	Assert( CmpLgpos( &ppib->lgposCommit0, &lgposMax ) != 0 );

	//  if our Commit0 record has already been written to the log, no need to wait

	m_critLGBuf.Enter();
	if ( CmpLgpos( &ppib->lgposCommit0, &m_lgposToFlush ) < 0 )
		{
		m_critLGBuf.Leave();
		goto Done;
		}

	//  if the log is down, you're hosed

	if ( m_fLGNoMoreLogWrite )
		{
		err = ErrERRCheck( JET_errLogWriteFail );
		m_critLGBuf.Leave();
		goto Done;
		}

	//  add this session to the log flush wait queue
	
	m_critLGWaitQ.Enter();
	cLGUsersWaiting.Inc( m_pinst );
	
	Assert( !ppib->FLGWaiting() );
	ppib->SetFLGWaiting();
	ppib->ppibNextWaitFlush = ppibNil;

	if ( m_ppibLGFlushQHead == ppibNil )
		{
		m_ppibLGFlushQTail = m_ppibLGFlushQHead = ppib;
		ppib->ppibPrevWaitFlush = ppibNil;
		}
	else
		{
		Assert( m_ppibLGFlushQTail != ppibNil );
		ppib->ppibPrevWaitFlush = m_ppibLGFlushQTail;
		m_ppibLGFlushQTail->ppibNextWaitFlush = ppib;
		m_ppibLGFlushQTail = ppib;
		}

	m_critLGWaitQ.Leave();
	m_critLGBuf.Leave();

	//  signal a log flush

	LGSignalFlush();
	cLGCommitFlush.Inc( m_pinst );

	//  wait forever for our Commit0 to be flushed to the log

	ppib->asigWaitLogFlush.Wait();

	//  the log write failed

	if ( m_fLGNoMoreLogWrite )
		{
		err = ErrERRCheck( JET_errLogWriteFail );
		}

	//  the log write succeeded

	else
		{
#ifdef DEBUG

		//  verify that out Commit0 record is now on disk
		
		LGPOS lgposToFlushT;

		m_critLGBuf.Enter();
		lgposToFlushT = m_lgposToFlush;
		// XXX
		// Curiously, this non-FASTFLUSH code uses a >= comparison
		// to see if something has gone to disk, but it seems like
		// > should be used (for both new and old flushing).

		Assert( CmpLgpos( &m_lgposToFlush, &ppib->lgposCommit0 ) > 0 );
		m_critLGBuf.Leave();

#endif  //  DEBUG
		}

Done:
	Assert( !ppib->FLGWaiting() );
	Assert(	err == JET_errLogWriteFail || err == JET_errSuccess );
	
	return err;
	}

// Kicks off synchronous log flushes until all log data is flushed

ERR LOG::ErrLGWaitAllFlushed( IFileSystemAPI *const pfsapi )
	{
	forever
		{
		LGPOS	lgposEndOfData;
		INT		cmp;
		
		m_critLGBuf.Enter();

		//	BUG X5:83888 
		//
		//		we create a torn-write after a clean shutdown because we don't flush the last LRCK record
		//		(we weren't seeing it because PbGetEndOfLogData() was pointing AT the LRCK instead of PAST it)
		//
		//		make sure we wait until we flush up to m_pbEntry (rather than PbGetEndOfLogData())

		BYTE* const	pbEndOfData = m_pbEntry;

		GetLgpos( pbEndOfData, &lgposEndOfData );
		cmp = CmpLgpos( &lgposEndOfData, &m_lgposToFlush );

		m_critLGBuf.Leave();

		if ( cmp > 0 )
			{
			// synchronously ask for flush
			ERR err = ErrLGFlushLog( pfsapi, fTrue );
			if ( err < 0 )
				{
				return err;
				}
			}
		else
			{
			// All log data is flushed, so we're done.
			break;
			}
		}
		
	return JET_errSuccess;
	}

ERR LOG::ErrLGITrace( PIB *ppib, CHAR *sz, BOOL fInternal )
	{
	ERR				err;
	const ULONG		cdata			= 3;
	DATA 			rgdata[cdata];
	LRTRACE			lrtrace;
	DATETIME		datetime;
	const ULONG		cbDateTimeBuf	= 31;
	CHAR			szDateTimeBuf[cbDateTimeBuf+1];

	//	No trace in recovery mode

	if ( m_fRecovering )
		return JET_errSuccess;

	if ( m_fLogDisabled )
		return JET_errSuccess;

	lrtrace.lrtyp = lrtypTrace;
	if ( ppib != ppibNil )
		{
		Assert( ppib->procid < 64000 );
		lrtrace.le_procid = ppib->procid;
		}
	else
		{
		lrtrace.le_procid = procidNil;
		}

	UtilGetCurrentDateTime( &datetime );

	//	UNDONE: a better idea would be to add a LOGTIME
	//	field to LRTRACE, but that would require a log
	//	format change, or at least a new lrtyp, so just
	//	add the date/timestamp to the trace string
	//
	szDateTimeBuf[cbDateTimeBuf] = 0;
	_snprintf(
		szDateTimeBuf,
		cbDateTimeBuf,
		"[%u/%u/%u %u:%02u:%02u] ",
		datetime.month,
		datetime.day,
		datetime.year,
		datetime.hour,
		datetime.minute,
		datetime.second );

	rgdata[0].SetPv( (BYTE *) &lrtrace );
	rgdata[0].SetCb( sizeof( lrtrace ) );

	rgdata[1].SetPv( reinterpret_cast<BYTE *>( szDateTimeBuf ) );
	rgdata[1].SetCb( strlen( szDateTimeBuf ) );

	rgdata[2].SetPv( reinterpret_cast<BYTE *>( sz ) );
	rgdata[2].SetCb( strlen( sz ) + 1 );

	lrtrace.le_cb = USHORT( rgdata[1].Cb() + rgdata[2].Cb() );

	if ( fInternal )
		{
		//	To prevent recursion, do not call ErrLGLogRec which then callback
		//	ErrLGTrace

		return ErrLGILogRec( rgdata, cdata, fLGNoNewGen, pNil );
		}

	err = ErrLGTryLogRec( rgdata, cdata, fLGNoNewGen, pNil );
	if ( err == errLGNotSynchronous )
		{
		//	Trace should not block anyone, put the record in a temp
		//	space and log it next time. It is OK to loose trace if the system
		//	crashes.

		TEMPTRACE *ptt;

		ptt = (TEMPTRACE *) PvOSMemoryHeapAlloc( sizeof( TEMPTRACE ) + lrtrace.le_cb );
		if ( !ptt )
			{
			return( ErrERRCheck( JET_errOutOfMemory ) );
			}
		ptt->ppib = ppib;
		ptt->pttNext = NULL;
		
		m_critLGTrace.Enter();
		if ( m_pttLast != NULL )
			m_pttLast->pttNext = ptt;
		else
			m_pttFirst = ptt;
		m_pttLast = ptt;
		m_critLGTrace.Leave();
		UtilMemCpy( ptt->szData, sz, lrtrace.le_cb );

		err = JET_errSuccess;
		}

	return err;
	}

ERR LOG::ErrLGTrace( PIB *ppib, CHAR *sz )
	{
	return ErrLGITrace( ppib, sz, fFalse );
	}


/*	write log file header.  No need to make a shadow since
 *	it will not be overwritten.
/**/
ERR LOG::ErrLGWriteFileHdr(
	LGFILEHDR* const	plgfilehdr,	// log file header to set checksum of and write
	IFileAPI* const		pfapi		// file to write to (default: m_pfapiLog)
	)
	{
	ERR		err;

	Assert( plgfilehdr->lgfilehdr.dbms_param.le_lLogBuffers );

	plgfilehdr->lgfilehdr.le_ulChecksum = UlUtilChecksum( (BYTE*) plgfilehdr, sizeof( LGFILEHDR ) );

	//	the log file header should be aligned on any volume regardless of sector size

	Assert( m_cbSecVolume <= sizeof( LGFILEHDR ) );
	Assert( sizeof( LGFILEHDR ) % m_cbSecVolume == 0 );

	//	issue the write

 	err = pfapi->ErrIOWrite( 0, m_csecHeader * m_cbSec, (BYTE *)plgfilehdr );
 	if ( err < 0 )
		{
		LGReportError( LOG_FILE_SYS_ERROR_ID, err, pfapi );
		LGReportError( LOG_HEADER_WRITE_ERROR_ID, JET_errLogWriteFail, pfapi );
		SetFNoMoreLogWrite( err );

		err = ErrERRCheck( JET_errLogWriteFail );
		}

	//	update performance counters

	cLGWrite.Inc( m_pinst );
	cLGBytesWritten.Add( m_pinst, m_csecHeader * m_cbSec );

	return err;
	}

ERR LOG::ErrLGWriteFileHdr( LGFILEHDR* const plgfilehdr )
	{
	return ErrLGWriteFileHdr( plgfilehdr, m_pfapiLog );
	}

/*
 *	Read log file header, detect any incomplete or
 *	catastrophic write failures.  These failures will be used to
 *	determine if the log file is valid or not.
 *
 *	On error, contents of plgfilehdr are unknown.
 *
 *	RETURNS		JET_errSuccess, or error code from failing routine
 */
ERR LOG::ErrLGReadFileHdr(
	IFileAPI * const	pfapiLog,
	LGFILEHDR *			plgfilehdr,
	const BOOL			fNeedToCheckLogID,
	const BOOL			fBypassDbPageSizeCheck )
	{
	ERR		err;
	QWORD	qwFileSize;
	QWORD	qwCalculatedFileSize;
	QWORD	qwSystemFileSize;

	/*	read log file header.  Header is written only during
	/*	log file creation and cannot become corrupt unless system
	/*	crash in the middle of file creation.
	/**/
	Call( pfapiLog->ErrIORead( 0, sizeof( LGFILEHDR ), (BYTE* const)plgfilehdr ) );

	/*	check if the data is bogus.
	 */
	if ( plgfilehdr->lgfilehdr.le_ulChecksum != UlUtilChecksum( (BYTE*)plgfilehdr, sizeof(LGFILEHDR) ) )
		{
		err = ErrERRCheck( JET_errLogFileCorrupt );
		}

	/*	check for old JET version
	/**/
	if ( *(long *)(((char *)plgfilehdr) + 24) == 4
		 && ( *(long *)(((char *)plgfilehdr) + 28) == 909		//	NT version
			  || *(long *)(((char *)plgfilehdr) + 28) == 995 )	//	Exchange 4.0
		 && *(long *)(((char *)plgfilehdr) + 32) == 0
		)
		{
		/*	version 500
		/**/
		err = ErrERRCheck( JET_errDatabase500Format );
		}
	else if ( *(long *)(((char *)plgfilehdr) + 20) == 443
		 && *(long *)(((char *)plgfilehdr) + 24) == 0
		 && *(long *)(((char *)plgfilehdr) + 28) == 0 )
		{
		/*	version 400
		/**/
		err = ErrERRCheck( JET_errDatabase400Format );
		}
	else if ( *(long *)(((char *)plgfilehdr) + 44) == 0
		 && *(long *)(((char *)plgfilehdr) + 48) == 0x0ca0001 )
		{
		/*	version 200
		/**/
		err = ErrERRCheck( JET_errDatabase200Format );
		}
	Call( err );

	if ( ulLGVersionMajor != plgfilehdr->lgfilehdr.le_ulMajor
		|| ulLGVersionMinor != plgfilehdr->lgfilehdr.le_ulMinor
		|| ulLGVersionUpdate < plgfilehdr->lgfilehdr.le_ulUpdate )
		{
		Call( ErrERRCheck( JET_errBadLogVersion ) );
		}

	// check filetype
	if( filetypeUnknown != plgfilehdr->lgfilehdr.le_filetype // old format
		&& filetypeLG != plgfilehdr->lgfilehdr.le_filetype )
		{
		// not a log file
		Call( ErrERRCheck( JET_errFileInvalidType ) );
		}

	if ( m_fSignLogSet )
		{
		if ( fNeedToCheckLogID )
			{
			if ( memcmp( &m_signLog, &plgfilehdr->lgfilehdr.signLog, sizeof( m_signLog ) ) != 0 )
				Error( ErrERRCheck( JET_errBadLogSignature ), HandleError );
			}
		}
	else
		{
		m_signLog = plgfilehdr->lgfilehdr.signLog;

		if ( !fBypassDbPageSizeCheck )
			{
			const UINT	cbPageT	= ( plgfilehdr->lgfilehdr.le_cbPageSize == 0 ?
										g_cbPageDefault :
										plgfilehdr->lgfilehdr.le_cbPageSize );
			if ( cbPageT != g_cbPage )
				Call( ErrERRCheck( JET_errPageSizeMismatch ) );
			}

		m_fSignLogSet = fTrue;
		}

//
//	SEARCH-STRING: SecSizeMismatchFixMe
//
/*****
	//	make sure the log sector size is correct

	if ( m_fRecovering || m_fHardRestore || m_cbSecVolume == ~(ULONG)0 )
		{

		//	we are recovering (or we are about to -- thus, the sector size is not set) which means we 
		//		must bypass the enforcement of the log sector size and allow the user to recover 
		//		(we will error out after recovery is finished)

		//	nop
		}
	else
		{

		//	we are not recovering

		if ( plgfilehdr->lgfilehdr.le_cbSec != m_cbSecVolume )
			{

			//	the sector size of this log does not match the current volume's sector size

			Call( ErrERRCheck( JET_errLogSectorSizeMismatch ) );
			}
		}
****/
//	---** TEMPORARY FIX **---
if ( plgfilehdr->lgfilehdr.le_cbSec != m_cbSecVolume )
	{
	Call( ErrERRCheck( JET_errLogSectorSizeMismatch ) );
	}


	//	make sure all the log file sizes match properly (real size, size from header, and system-param size)

	Call( pfapiLog->ErrSize( &qwFileSize ) );
	qwCalculatedFileSize = QWORD( plgfilehdr->lgfilehdr.le_csecLGFile ) * 
						   QWORD( plgfilehdr->lgfilehdr.le_cbSec );
	if ( qwFileSize != qwCalculatedFileSize )
		{
		Call( ErrERRCheck( JET_errLogFileSizeMismatch ) );
		}
	if ( m_pinst->m_fUseRecoveryLogFileSize )
		{

		//	we are recovering which means we must bypass the enforcement of the log file size
		//		and allow the user to recover (so long as all the recovery logs are the same size)

		if ( m_pinst->m_lLogFileSizeDuringRecovery == 0 )
			{

			//	this is the first log the user is reading during recovery

			//	initialize the recovery log file size

			Assert( qwCalculatedFileSize % 1024 == 0 );
			m_pinst->m_lLogFileSizeDuringRecovery = LONG( qwCalculatedFileSize / 1024 );
			}

		//	enforce the recovery log file size
		
		qwSystemFileSize = QWORD( m_pinst->m_lLogFileSizeDuringRecovery ) * 1024;
		}
	else if ( m_pinst->m_fSetLogFileSize )
		{

		//	we are not recovering, so we must enforce the size set by the user

		qwSystemFileSize = QWORD( m_pinst->m_lLogFileSize ) * 1024;
		}
	else
		{

		//	we are not recovering, but the user never set a size for us to enforce

		//	set the size using the current log file on the user's behalf

		qwSystemFileSize = qwCalculatedFileSize;
		m_pinst->m_fSetLogFileSize = fTrue;
		m_pinst->m_lLogFileSize = LONG( qwCalculatedFileSize / 1024 );
		}
	if ( qwFileSize != qwSystemFileSize )
		{
		Call( ErrERRCheck( JET_errLogFileSizeMismatch ) );
		}

HandleError:
	if ( err == JET_errSuccess )
		{
		
		//	we have successfully opened the log file, and the volume sector size has been
		//		set meaning we are allowed to adjust the sector sizes
		//		(when the volume sector size is not set, we are recovering but we have not
		//		 yet started the redo phase -- we are in preparation)

		m_cbSec = plgfilehdr->lgfilehdr.le_cbSec;
		Assert( m_cbSec >= 512 );
		m_csecHeader = sizeof( LGFILEHDR ) / m_cbSec;
		m_csecLGFile = plgfilehdr->lgfilehdr.le_csecLGFile;
		}
	else
		{
		CHAR		szLogfile[IFileSystemAPI::cchPathMax];
		CHAR		szErr[8];
		const UINT 	csz = 2;
		const CHAR	* rgszT[csz] = { szLogfile, szErr };

		if ( pfapiLog->ErrPath( szLogfile ) < 0 )
			{
			szLogfile[0] = 0;
			}
		sprintf( szErr, "%d", err );

		if ( JET_errBadLogVersion == err )
			{
			UtilReportEvent(
				( m_fDeleteOldLogs ? eventWarning : eventError ),
				LOGGING_RECOVERY_CATEGORY,
				LOG_BAD_VERSION_ERROR_ID,
				csz - 1,
				rgszT,
				0, 
				NULL,
				m_pinst );
			}
		else
			{
			UtilReportEvent(
				eventError,
				LOGGING_RECOVERY_CATEGORY,
				LOG_HEADER_READ_ERROR_ID,
				csz,
				rgszT,
				0,
				NULL,
				m_pinst );
			}

		SetFNoMoreLogWrite( err );
		}

	return err;
	}


/*	Create the log file name (no extension) corresponding to the lGeneration
/*	in szFName. NOTE: szFName need minimum 9 bytes.
/*
/*	PARAMETERS	rgbLogFileName	holds returned log file name
/*				lGeneration 	log generation number to produce	name for
/*	RETURNS		JET_errSuccess
/**/

const char rgchLGFileDigits[] =
	{
	'0', '1', '2', '3', '4', '5', '6', '7',
	'8', '9', 'A', 'B', 'C', 'D', 'E', 'F',
	};
const LONG lLGFileBase = sizeof(rgchLGFileDigits)/sizeof(char); 


VOID LOG::LGSzFromLogId( CHAR *szFName, LONG lGeneration )
	{
	LONG	ich;

	strcpy( szFName, m_szJetLogNameTemplate );
	for ( ich = 7; ich > 2; ich-- )
		{
		szFName[ich] = rgchLGFileDigits[lGeneration % lLGFileBase];
		lGeneration = lGeneration / lLGFileBase;
		}

	return;
	}


/*	copy tm to a smaller structure logtm to save space on disk.
 */
VOID LGIGetDateTime( LOGTIME *plogtm )
	{
	DATETIME tm;

	UtilGetCurrentDateTime( &tm );

	plogtm->bSeconds = BYTE( tm.second );
	plogtm->bMinutes = BYTE( tm.minute );
	plogtm->bHours = BYTE( tm.hour );
	plogtm->bDay = BYTE( tm.day );
	plogtm->bMonth = BYTE( tm.month );
	plogtm->bYear = BYTE( tm.year - 1900 );
	}


//	open edb.log and recognize the case where we crashed while creating a new log file

ERR LOG::ErrLGOpenJetLog( IFileSystemAPI *const pfsapi )
	{
	ERR		err;
	CHAR	szPathJetTmpLog[IFileSystemAPI::cchPathMax];
	CHAR	szFNameT[IFileSystemAPI::cchPathMax];
#ifdef DEBUG
	BOOL	fRetryOpen = fFalse;
#endif	//	DEBUG

	//	create the file name/path for edb.log and edbtmp.log

	LGMakeLogName( m_szLogName, m_szJet );
	LGMakeLogName( szPathJetTmpLog, m_szJetTmp );

OpenLog:

	//	try to open edb.log

	err = pfsapi->ErrFileOpen( m_szLogName, &m_pfapiLog, fTrue );

	if ( err < 0 )
		{
		if ( JET_errFileNotFound != err )
			{
			//	we failed to open edb.log, but the failure was not related to the file not being found
			//	we must assume there was a critical failure during file-open such as "out of memory"
			LGReportError( LOG_OPEN_FILE_ERROR_ID, err );
			return err; 
			}

		err = ErrUtilPathExists( pfsapi, szPathJetTmpLog, NULL );
		if ( err < 0 )
			{
			//	edb.log does not exist, and there's some problem accessing
			//	edbtmp.log (most likely it doesn't exist either), so
			//	return file-not-found on edb.log
			return ErrERRCheck( JET_errFileNotFound );
			}

		CallS( err );	//	warnings not expected

		//	we don't have edb.log, but we have edbtmp.log
		//
		//	this can only happen one of two possible ways:
		//		a) we were creating the first generation in edbtmp.log -- edb.log never even existed
		//		   we crashed during this process
		//		b) we had just finished creating the next generation in edbtmp.log
		//		   we renamed edb.log to edbXXXXX.log
		//		   we were ABOUT to rename edbtmp.log to edb.log, but crashed
		//
		//
		//	in case 'a', we never finished creating edb.log (using edbtmp.log), so we can safely say 
		//	that it never really existed
		//
		//	we delete edbtmp.log and return JET_errFileNotFound to the user to reflect this case
		//
		//
		//	in case 'b', we expect edbtmp.log to be a valid log file, although we can't be sure because the
		//	logfile size might be wrong or the pattern might be compromized...
		//
		//	instead of trying to use edbtmp.log, we will rename the previous generational log to edb.log and 
		//	then delete edbtmp.log.  since the new edb.log will be full, the user will immediately create the
		//	next log generation using edbtmp.log (which is what they were doing before they crashed)
		//
		//	NOTE: if we crash during the rename and the delete, we will have edb.log and edbtmp.log which is
		//		  a perfectly valid state -- seeing this will cause us to delete edbtmp.log right away

		LONG lGenMaxT;

		CallR ( ErrLGIGetGenerationRange( pfsapi, m_szLogCurrent, NULL, &lGenMaxT ) );
		if ( 0 != lGenMaxT )
			{
			CHAR szPathJetGenerationalLogT[IFileSystemAPI::cchPathMax];

			//	we are in case 'b' -- a generational log exists

			//	rename the generation log to edb.log
			//	NOTE: m_szLogName was setup at the start of this function

			LGSzFromLogId( szFNameT, lGenMaxT );
			LGMakeLogName( szPathJetGenerationalLogT, szFNameT );
			CallR( pfsapi->ErrFileMove( szPathJetGenerationalLogT, m_szLogName ) );

			//	try to open edb.log again

#ifdef DEBUG
			Assert( !fRetryOpen );
			fRetryOpen = fTrue;
#endif	//	DEBUG

			goto OpenLog;
			}

		//	we are in case 'a' -- no previous generational log exists
		//	fall through so that we delete edbtmp.log and return JET_errFileNotFound
		err = ErrERRCheck( JET_errFileNotFound );
		}
	else
		{
		CallS( err );	//	no warnings expected
		}


	//	if we got here, then edb.log exists, so clean up edbtmp.log
	(VOID)pfsapi->ErrFileDelete( szPathJetTmpLog );

	return err;
	}


ERR LOG::ErrLGIUpdateGenRequired(
	IFileSystemAPI * const	pfsapi,
	const LONG 				lGenMinRequired,
	const LONG 				lGenMaxRequired, 
	const LOGTIME 			logtimeGenMaxCreate,
	BOOL * const			pfSkippedAttachDetach )
	{
	ERR						errRet 			= JET_errSuccess;
	IFileSystemAPI *		pfsapiT;

	if ( pfSkippedAttachDetach )
		{
		*pfSkippedAttachDetach = fFalse;
		}

	Assert( m_critCheckpoint.FOwner() );

	FMP::EnterCritFMPPool();

	for ( DBID dbidT = dbidUserLeast; dbidT < dbidMax; dbidT++ )
		{
		ERR err;
		IFMP ifmp;

		ifmp = m_pinst->m_mpdbidifmp[ dbidT ];
		if ( ifmp >= ifmpMax )
			continue;

		FMP 		*pfmpT			= &rgfmp[ ifmp ];
		
		if ( pfmpT->FReadOnlyAttach() )
			continue;

		pfmpT->RwlDetaching().EnterAsReader();

		//	need to check ifmp again. Because the db may be cleaned up
		//	and FDetachingDB is reset, and so is its ifmp in m_mpdbidifmp.

		if (   m_pinst->m_mpdbidifmp[ dbidT ] >= ifmpMax 
			|| pfmpT->FSkippedAttach() || pfmpT->FDeferredAttach()
			|| !pfmpT->Pdbfilehdr() )
			{
			pfmpT->RwlDetaching().LeaveAsReader();
			continue;
			}

			
		if ( !pfmpT->FAllowHeaderUpdate() ) 
			{
			if ( pfSkippedAttachDetach )
				{
				*pfSkippedAttachDetach = fTrue;
				}
			pfmpT->RwlDetaching().LeaveAsReader();
			continue;
			}

		// during Snapshot we don't update the database pages so
		// we don't want to update the db header
		// (we can have genRequired not up-to-date as no pages are flushed)
		if ( pfmpT->FDuringSnapshot() )
			{
			pfmpT->RwlDetaching().LeaveAsReader();
			continue;
			}
			
		DBFILEHDR_FIX *pdbfilehdr	= pfmpT->Pdbfilehdr();
		Assert( pdbfilehdr );

		if( lGenMaxRequired )
			{
			LONG lGenMaxRequiredOld;

#ifdef DEBUG
			// we should have the same time for the same generation
			LOGTIME tmEmpty;
			memset( &tmEmpty, '\0', sizeof( LOGTIME ) );
			Assert ( pdbfilehdr->le_lGenMaxRequired != lGenMaxRequired || 0 == lGenMaxRequired || 0 == pdbfilehdr->le_lGenMaxRequired ||
					0 == memcmp( &pdbfilehdr->logtimeGenMaxCreate, &tmEmpty, sizeof( LOGTIME ) ) || 
					0 == memcmp( &pdbfilehdr->logtimeGenMaxCreate, &logtimeGenMaxCreate, sizeof( LOGTIME ) ) );
#endif					

			lGenMaxRequiredOld 	= pdbfilehdr->le_lGenMaxRequired;
			
			/*
			We have the following scenario:
			- start new log -> update m_plgfilehdrT -> call this function with genMax = 5 (from m_plgfilehdrT)
			- other thread is updating the checkpoint file setting genMin but is passing genMax from m_plgfilehdr
			which is still 4 because the log thread is moving m_plgfilehdrT into m_plgfilehdr later on

			The header ends up with genMax 4 and data is logged in 5 -> Wrong !!! We need max (old, new)
			*/
			pdbfilehdr->le_lGenMaxRequired = max( lGenMaxRequired, lGenMaxRequiredOld );
			Assert ( lGenMaxRequiredOld <= pdbfilehdr->le_lGenMaxRequired );			

			// if we updated the genMax with the value passed in, update the time with the new value as well
			if ( pdbfilehdr->le_lGenMaxRequired == lGenMaxRequired )
				{
				memcpy( &pdbfilehdr->logtimeGenMaxCreate, &logtimeGenMaxCreate, sizeof( LOGTIME ) );
				}

			}


		if ( lGenMinRequired )
			{
			LONG lGenMinRequiredOld;

			lGenMinRequiredOld 	= pdbfilehdr->le_lGenMinRequired;
			pdbfilehdr->le_lGenMinRequired = max( lGenMinRequired, pdbfilehdr->le_lgposAttach.le_lGeneration );
			Assert ( lGenMinRequiredOld <= pdbfilehdr->le_lGenMinRequired );			
			}

		pfsapiT = pfsapi;

		err = ErrUtilWriteShadowedHeader(
				pfsapiT,
				pfmpT->SzDatabaseName(),
				fTrue,
				(BYTE *)pdbfilehdr,
				g_cbPage,
				pfmpT->Pfapi() );
			
		pfmpT->RwlDetaching().LeaveAsReader();

		if ( errRet == JET_errSuccess )
			errRet = err;
		}

	FMP::LeaveCritFMPPool();

	return errRet;
	}


//	used by ErrLGCheckState

ERR LOG::ErrLGICheckState( IFileSystemAPI *const pfsapi )
	{
	ERR		err		= JET_errSuccess;

	m_critLGResFiles.Enter();

	//	re-check status in case disk space has been freed

	if ( m_ls != lsNormal )
		{
		CHAR	* szT	= m_szLogCurrent;

		m_szLogCurrent = m_szLogFilePath;
		(VOID)ErrLGICreateReserveLogFiles( pfsapi );
		m_szLogCurrent = szT;

		if ( m_ls != lsNormal )
			{
			err = ErrERRCheck( JET_errLogDiskFull );
			}
		}
	if ( err >= JET_errSuccess && m_fLogSequenceEnd )
		{
		err = ErrERRCheck( JET_errLogSequenceEnd );
		}

	m_critLGResFiles.Leave();

	return err;
	}



//	make sure the reserve logs exist (res1.log and res2.log)

ERR LOG::ErrLGICreateReserveLogFiles( IFileSystemAPI *const pfsapi, const BOOL fCleanupOld )
	{
	const LS			lsBefore				= m_ls;
	BOOL				fQuiesce				= fFalse;
	CHAR  				szPathJetLog[IFileSystemAPI::cchPathMax];
	const CHAR* const	rgszResLogBaseNames[]	= { szLogRes2, szLogRes1 };
	const size_t		cResLogBaseNames		= sizeof( rgszResLogBaseNames ) / sizeof( rgszResLogBaseNames[ 0 ] );

	Assert( m_critLGResFiles.FOwner() );

	if ( lsBefore == lsOutOfDiskSpace )
		return ErrERRCheck( JET_errLogDiskFull );

	if ( fCleanupOld )
		{
		IFileFindAPI*	pffapi			= NULL;
		QWORD			cbSize;
		const QWORD		cbSizeExpected	= QWORD( m_csecLGFile ) * m_cbSec;
			
		//	we need to cleanup the old reserve logs if they are the wrong size
		//	NOTE: if the file-delete call(s) fail, it is ok because this will be
		//		handled later when ErrLGNewLogFile expands/shrinks the reserve
		//		reserve log to its proper size
		//		(this solution is purely proactive)

		for ( size_t iResLogBaseNames = 0; iResLogBaseNames < cResLogBaseNames; ++iResLogBaseNames )
			{
			LGMakeLogName( szPathJetLog, rgszResLogBaseNames[ iResLogBaseNames ] );
			if (	pfsapi->ErrFileFind( szPathJetLog, &pffapi ) == JET_errSuccess &&
					pffapi->ErrNext() == JET_errSuccess &&
					pffapi->ErrSize( &cbSize ) == JET_errSuccess &&
					cbSizeExpected != cbSize )
				{
				(void)pfsapi->ErrFileDelete( szPathJetLog );
				}
			delete pffapi;
			pffapi = NULL;
			}
		}

	for ( size_t iResLogBaseNames = 0; iResLogBaseNames < cResLogBaseNames; ++iResLogBaseNames )
		{
		LGMakeLogName( szPathJetLog, rgszResLogBaseNames[ iResLogBaseNames ] );
		ERR err = ErrUtilPathExists( pfsapi, szPathJetLog );
		if ( err == JET_errFileNotFound )
			{
			IFileAPI*	pfapiT = NULL;
			err = ErrUtilCreateLogFile(	pfsapi, 
										szPathJetLog, 
										&pfapiT, 
										QWORD( m_csecLGFile ) * m_cbSec );
			if ( err >= 0 )
				{
				delete pfapiT;
				}
			}
		if ( err < 0 )
			{
			if ( lsBefore == lsNormal )
				{
				UtilReportEvent(
						eventError,
						LOGGING_RECOVERY_CATEGORY,
						LOW_LOG_DISK_SPACE,
						0,
						NULL,
						0,
						NULL,
						m_pinst );
				}
			fQuiesce = fTrue;
			break;
			}
		}

	//	We tried to ensure that our reserve log files exist and may or may not have succeeded

	m_ls = ( fQuiesce ? lsQuiesce : lsNormal );
	return JET_errSuccess;
	}

//	Create Asynchronous Log File Assert

#define CAAssert( expr ) AssertRTL( expr )

//	Parameters
//
//		pfsapi
//			[in] file system interface for log directory
//		ppfapi
//			[out] file interface to opened edbtmp.log
//		pszPathJetTmpLog
//			[in] full path to edbtmp.log that we will be opening.
//		pfResUsed
//			[out] Whether the edbtmp.log file that was opened was formerly
//			a reserve log file.
//
//	Return Values
//
//		Errors cause *ppfapi to be NULL.

ERR LOG::ErrLGIOpenTempLogFile(
	IFileSystemAPI* const	pfsapi,
	IFileAPI** const		ppfapi,
	const _TCHAR* const		pszPathJetTmpLog,
	BOOL* const				pfResUsed
	)
	{
	ERR		err	= JET_errSuccess;

	CAAssert( pfsapi );
	CAAssert( ppfapi );
	CAAssert( pszPathJetTmpLog );
	CAAssert( pfResUsed );

	*ppfapi = NULL;
	*pfResUsed = fFalse;

	//	Delete any existing edbtmp.log in case it exists.

	err = pfsapi->ErrFileDelete( pszPathJetTmpLog );
	Call( JET_errFileNotFound == err ? JET_errSuccess : err );

	err = ErrLGIOpenArchivedLogFile( pfsapi, ppfapi, pszPathJetTmpLog );

	//  We don't have an edbtmp.log open because we couldn't reuse an old log file,
	//	so try to create a new one.

	if ( NULL == *ppfapi && err >= JET_errSuccess )
		{
		err = pfsapi->ErrFileCreate( pszPathJetTmpLog, ppfapi, fFalse, fFalse, fFalse );
		CAAssert( ( err >= JET_errSuccess && NULL != *ppfapi )
				|| ( err < JET_errSuccess && NULL == *ppfapi ) );

		//	Error in trying to create a new edbtmp.log so try to use a reserve log
		//	instead.

		if ( JET_errDiskFull == err )
			{
			Call( ErrLGIOpenReserveLogFile( pfsapi, ppfapi, pszPathJetTmpLog ) );
			*pfResUsed = fTrue;
			CAAssert( *ppfapi );
			}
		}

HandleError:
	CAAssert( ( err >= JET_errSuccess && NULL != *ppfapi )
			|| ( err < JET_errSuccess && NULL == *ppfapi ) );

	if ( err < JET_errSuccess )
		{
		if ( *ppfapi )
			{
			delete *ppfapi;
			*ppfapi = NULL;
			
			const ERR errFileDel = pfsapi->ErrFileDelete( pszPathJetTmpLog );
			}
		*pfResUsed = fFalse;
		}
	return err;
	}

ERR LOG::ErrLGIOpenArchivedLogFile(
	IFileSystemAPI* const	pfsapi,
	IFileAPI** const		ppfapi,
	const _TCHAR* const		pszPathJetTmpLog
	)
	{
	ERR		err = JET_errSuccess;
	_TCHAR	szLogName[ IFileSystemAPI::cchPathMax ];

	CAAssert( pfsapi );
	CAAssert( ppfapi );
	CAAssert( pszPathJetTmpLog );
	
	*ppfapi = NULL;
	
	//	If circular log file flag set and backup not in progress
	//	then find oldest log file and if no longer needed for
	//	soft-failure recovery, then rename to szJetTempLog.
	//	Also delete any unnecessary log files.

	//	SYNC: No locking necessary for m_fLGCircularLogging since it is immutable
	//	while the instance is initialized.

	//	SYNC: We don't need to worry about a backup starting or stopping while
	//	we're executing since if circular logging is enabled, a backup will not mess
	//	with the archived log files.

	if ( m_fLGCircularLogging )
		{
		//	when find first numbered log file, set lgenLGGlobalOldest to log file number
		//
		LONG lgenLGGlobalOldest = 0;

		(void)ErrLGIGetGenerationRange( pfsapi, m_szLogFilePath, &lgenLGGlobalOldest, NULL );

		//	if found oldest generation and older than checkpoint,
		//	then move log file to szJetTempLog.  Note that the checkpoint
		//	must be flushed to ensure that the flushed checkpoint is
		//	after then oldest generation.
		//
		//	if a backup is in progress, then also need to verify
		//	that the oldest generation is less than the minimum
		//	logfile to be copied (it should always be less
		//	than the checkpoint, but we perform both checks
		//	just to be safe)
		//
		if ( lgenLGGlobalOldest != 0
			&& ( !m_fBackupInProgress || lgenLGGlobalOldest < m_lgenCopyMic ) )
			{
			//	UNDONE:	related issue of checkpoint write
			//			synchronization with dependent operations
			//	UNDONE:	error handling for checkpoint write
			//
			(VOID) ErrLGUpdateCheckpointFile( pfsapi, fFalse );
			if ( lgenLGGlobalOldest < m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration )
				{
				CHAR	szFNameT[ IFileSystemAPI::cchPathMax ];
				
				LGSzFromLogId( szFNameT, lgenLGGlobalOldest );
				LGMakeLogName( szLogName, szFNameT );
				err = pfsapi->ErrFileMove( szLogName, pszPathJetTmpLog );
				CAAssert( err < 0 || err == JET_errSuccess );
				if ( err == JET_errSuccess )
					{
					err = pfsapi->ErrFileOpen( pszPathJetTmpLog, ppfapi );
					if ( err < JET_errSuccess )
						{
						//	We renamed the archived log file to edbtmp.log but were
						//	unable to open it, so let us try to rename it back.
						//
						if ( pfsapi->ErrFileMove( pszPathJetTmpLog, szLogName ) < JET_errSuccess )
							{
							//	Unable to rename log file back (a file which we can't open), so
							//	try to delete it as last resort to stablize filesystem state.
							//
							(VOID) pfsapi->ErrFileDelete( pszPathJetTmpLog );
							}
						}

					else
						{
						//	delete unnecessary log files.  At all times, retain
						//	a continguous and valid sequence.
						//
						do
							{
							lgenLGGlobalOldest++;
							CAAssert( lgenLGGlobalOldest <= m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration );

							//	if backup is in progress, then to be safe,
							//	don't remove any of the extra logfiles
							//
							if ( lgenLGGlobalOldest >= m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration
								|| m_fBackupInProgress )
								break;

							LGSzFromLogId( szFNameT, lgenLGGlobalOldest );
							LGMakeLogName( szLogName, szFNameT );
							}
						while ( JET_errSuccess == pfsapi->ErrFileDelete( szLogName ) );
						}
					}
				}
			}
		}

	return err;
	}

ERR LOG::ErrLGIOpenReserveLogFile(
	IFileSystemAPI* const	pfsapi,
	IFileAPI** const		ppfapi,
	const _TCHAR* const		pszPathJetTmpLog
	)
	{
	ERR		err = JET_errSuccess;
	_TCHAR	szLogName[ IFileSystemAPI::cchPathMax ];

	CAAssert( pfsapi );
	CAAssert( ppfapi );
	CAAssert( pszPathJetTmpLog );

	*ppfapi = NULL;

	CAAssert( m_critLGResFiles.FNotOwner() );
	m_critLGResFiles.Enter();
	
	/*	use reserved log file and change to log state
	/**/
	LGMakeLogName( szLogName, szLogRes2 );
	err = pfsapi->ErrFileMove( szLogName, pszPathJetTmpLog );
	CAAssert( err < 0 || err == JET_errSuccess );
	if ( err == JET_errSuccess )
		{
		err = pfsapi->ErrFileOpen( pszPathJetTmpLog, ppfapi );
		if ( ! *ppfapi )
			{
			//	We renamed res2.log to edbtmp.log but were unable to open it,
			//	so try to rename it back. If this fails, we should delete edbtmp.log
			//	because we can't rename it and we can't open it and we still need
			//	to try to use res1.log.
			
			if ( pfsapi->ErrFileMove( pszPathJetTmpLog, szLogName ) < JET_errSuccess )
				{
				//	If this fails, we're in real bad shape where we can't even try
				//	to use res1.log.
				
				(void) pfsapi->ErrFileDelete( pszPathJetTmpLog );
				}
			}
		}

	if ( m_ls == lsNormal )
		{
		UtilReportEvent(
				eventError,
				LOGGING_RECOVERY_CATEGORY,
				LOW_LOG_DISK_SPACE,
				0,
				NULL,
				0,
				NULL,
				m_pinst );
		m_ls = lsQuiesce;
		}

	if ( ! *ppfapi )
		{
		//	couldn't use res2, try res1
		LGMakeLogName( szLogName, szLogRes1 );
		err = pfsapi->ErrFileMove( szLogName, pszPathJetTmpLog );
		CAAssert( err < 0 || err == JET_errSuccess );
		if ( err == JET_errSuccess )
			{
			err = pfsapi->ErrFileOpen( pszPathJetTmpLog, ppfapi );
			if ( ! *ppfapi )
				{
				//	Unable to open edbtmp.log (formerly known as res1.log), so
				//	at least try to rename it back (yeesh, we are in a bad place).

				(void) pfsapi->ErrFileMove( pszPathJetTmpLog, szLogName );
				}
			}
		CAAssert( m_ls == lsQuiesce || m_ls == lsOutOfDiskSpace );
		}

	if ( ! *ppfapi )
		{
		CAAssert( err < JET_errSuccess );
		CAAssert( m_ls == lsQuiesce || m_ls == lsOutOfDiskSpace );

		//	this function is only called if we're out of log
		//	disk space and trying to resort to the reserved
		//	logs, so if we can't open the reserved logs for
		//	ANY reason (most common reason is that we're
		//	getting FileNotFound because the reserved logs
		//	have already been consumed), then just bail with
		//	LogDiskFull
		//
		err = ErrERRCheck( JET_errLogDiskFull );

		if ( m_ls == lsQuiesce )
			{
			const CHAR*	rgpszString[] = {
				m_szJetLog	// short name without any directory components i.e. edb.log
				};

			// If we could not open either of the reserved log files,
			// because the disk is full, then we're out of luck and
			// really out of disk space.
			UtilReportEvent(
					eventError,
					LOGGING_RECOVERY_CATEGORY,
					LOG_DISK_FULL,
					sizeof( rgpszString ) / sizeof( rgpszString[ 0 ] ),
					rgpszString,
					0,
					NULL,
					m_pinst );
			m_ls = lsOutOfDiskSpace;
			}
		}
	else
		{
		CAAssert( err >= JET_errSuccess );
		}

	m_critLGResFiles.Leave();

	return err;
	}

DWORD LOG::CbLGICreateAsynchIOSize()
	{
	CAAssert( !m_asigCreateAsynchIOCompleted.FTryWait() );
	CAAssert( m_ibJetTmpLog < QWORD( m_csecLGFile ) * m_cbSec );
	const QWORD	cbIO = min(
		cbLogExtendPattern - m_ibJetTmpLog % cbLogExtendPattern,
		( QWORD( m_csecLGFile ) * m_cbSec ) - m_ibJetTmpLog );
	CAAssert( DWORD( cbIO ) == cbIO );
	return DWORD( cbIO );
	}

VOID LOG::LGICreateAsynchIOIssue()
	{
	//	We acquired the signal so we're the only thread messing with edbtmp.log

	CAAssert( ! m_asigCreateAsynchIOCompleted.FTryWait() );
	
	const QWORD	ib	= m_ibJetTmpLog;
	const DWORD	cb	= CbLGICreateAsynchIOSize();

	CAAssert( ib < QWORD( m_csecLGFile ) * m_cbSec );
	CAAssert( cb > 0 );
	
	m_ibJetTmpLog += cb;

	//	Should not hold log buffer over the issuing of this asynch I/O.
	//	Issuing of asynch I/O isn't guaranteed to be fast (i.e. might be
	//	waiting for a free IO request-related block to be available).
	
	CAAssert( m_critLGBuf.FNotOwner() );
	
	//	Error will be completed.
	CAAssert( m_pfapiJetTmpLog );
	(void) m_pfapiJetTmpLog->ErrIOWrite(
		ib,
		cb,
		rgbLogExtendPattern,
		LGICreateAsynchIOComplete_,
		reinterpret_cast< DWORD_PTR >( this ) );
	}

void LOG::LGICreateAsynchIOComplete_(
	const ERR			err,
	IFileAPI* const		pfapi,
	const QWORD			ibOffset,
	const DWORD			cbData,
	const BYTE* const	pbData,
	const DWORD_PTR		keyIOComplete
	)
	{
	LOG* const	plog = reinterpret_cast< LOG* >( keyIOComplete );
	CAAssert( plog );
	plog->LGICreateAsynchIOComplete( err, pfapi, ibOffset, cbData, pbData );
	}

void LOG::LGICreateAsynchIOComplete(
	const ERR			err,
	IFileAPI* const		pfapi,
	const QWORD			ibOffset,
	const DWORD			cbData,
	const BYTE* const	pbData
	)
	{
	CAAssert( m_fCreateAsynchLogFile );
	CAAssert( m_pfapiJetTmpLog );
	CAAssert( JET_errSuccess == m_errCreateAsynch );
	m_errCreateAsynch = err;

	//	If success, and more I/O to do, set trigger for when next I/O
	//	should occur, otherwise leave trigger as lgposMax to prevent any
	//	further checking or writing.
	
	if ( err >= JET_errSuccess )
		{
		CAAssert( m_ibJetTmpLog <= QWORD( m_csecLGFile ) * m_cbSec );
		if ( m_ibJetTmpLog < QWORD( m_csecLGFile ) * m_cbSec )
			{
			m_critLGBuf.Enter();
			LGPOS	lgpos = { 0, 0, m_plgfilehdr->lgfilehdr.le_lGeneration };
			CAAssert( static_cast< DWORD >( m_ibJetTmpLog ) == m_ibJetTmpLog );
			AddLgpos( &lgpos, static_cast< DWORD >( m_ibJetTmpLog ) );
			
			m_lgposCreateAsynchTrigger = lgpos;
			m_critLGBuf.Leave();
			}
		}
	CAAssert( !m_asigCreateAsynchIOCompleted.FTryWait() );
	m_asigCreateAsynchIOCompleted.Set();
	}

//	Wait for any IOs to the edbtmp.log to complete, then close the file.

VOID LOG::LGCreateAsynchWaitForCompletion()
	{
	//	SYNC:
	//	Must be inside of m_critLGFlush or in LOG termination because we
	//	can't have the flush task concurrently modify m_pfapiJetTmpLog.
	
	if ( m_pfapiJetTmpLog ||					// edbtmp.log opened
		m_errCreateAsynch < JET_errSuccess )	// error trying to open edbtmp.log
		{
		CAAssert( m_fCreateAsynchLogFile );
		
		if ( m_pfapiJetTmpLog )
			{
			CallS( m_pfapiJetTmpLog->ErrIOIssue() );
			}
		m_asigCreateAsynchIOCompleted.Wait();
		delete m_pfapiJetTmpLog;
		m_pfapiJetTmpLog = NULL;
		m_fCreateAsynchResUsed = fFalse;
		m_errCreateAsynch = JET_errSuccess;
		m_ibJetTmpLog = 0;
		m_lgposCreateAsynchTrigger = lgposMax;
		}
	}	


//  ================================================================
VOID LOG::CheckForGenerationOverflow()
//  ================================================================
	{
	//	generate error on log generation number roll over
	
	Assert( lGenerationMax < lGenerationMaxDuringRecovery );
	const LONG	lGenMax = ( m_fRecovering ? lGenerationMaxDuringRecovery : lGenerationMax );
	if ( m_plgfilehdr->lgfilehdr.le_lGeneration >= lGenMax )
		{
		//	we should never hit this during recovery because we allocated extra log generations
		//	if we do hit this, it means we need to reserve MORE generations!

		Assert( !m_fRecovering );

		//	enter "log sequence end" mode where we prevent all operations except things like rollback/term
		//	NOTE: we allow the new generation to be created so we can use it to make a clean shutdown

		m_critLGResFiles.Enter();
		m_fLogSequenceEnd = fTrue;
		m_critLGResFiles.Leave();
		}
	}


//  ================================================================
VOID LOG::CheckLgposCreateAsyncTrigger()
//  ================================================================
//
//	It is possible that m_lgposCreateAsynchTrigger != lgposMax if
//	edbtmp.log hasn't been completely formatted (the last I/O that
//	completed properly set the trigger). This will prevent
//	anyone from trying to wait for the signal since the trigger will
//	be lgpos infinity. Note that in this situation it is ok to read
//	m_lgposCreateAsynchTrigger outside of m_critLGBuf since no
//	one else can write to it (we're in m_critLGFlush which
//	prevents this function from writing to it, and any outstanding
//	I/O has completed so it won't write).
//
//-
	{
	if ( CmpLgpos( lgposMax, m_lgposCreateAsynchTrigger ) )
		{
		m_critLGBuf.Enter();
		m_lgposCreateAsynchTrigger = lgposMax;
		m_critLGBuf.Leave();
		}
	}


//  ================================================================
VOID LOG::InitLgfilehdrT()
//  ================================================================
	{
	m_plgfilehdrT->lgfilehdr.le_ulMajor		= ulLGVersionMajor;
	m_plgfilehdrT->lgfilehdr.le_ulMinor		= ulLGVersionMinor;
	m_plgfilehdrT->lgfilehdr.le_ulUpdate	= ulLGVersionUpdate;

	m_plgfilehdrT->lgfilehdr.le_cbSec		= USHORT( m_cbSec );
	m_plgfilehdrT->lgfilehdr.le_csecHeader	= USHORT( m_csecHeader );
	m_plgfilehdrT->lgfilehdr.le_csecLGFile	= USHORT( m_csecLGFile );

	m_plgfilehdrT->lgfilehdr.le_cbPageSize = USHORT( g_cbPage );	
	}


//  ================================================================
VOID LOG::SetFLGFlagsInLgfilehdrT( const BOOL fResUsed )
//  ================================================================
	{
	m_plgfilehdrT->lgfilehdr.fLGFlags = 0;
	m_plgfilehdrT->lgfilehdr.fLGFlags |= fResUsed ? fLGReservedLog : 0;
	m_plgfilehdrT->lgfilehdr.fLGFlags |= m_fLGCircularLogging ? fLGCircularLoggingCurrent : 0;
	//	Note that we check the current log file header for circular logging currently or in the past
	m_plgfilehdrT->lgfilehdr.fLGFlags |= ( m_plgfilehdr->lgfilehdr.fLGFlags & ( fLGCircularLoggingCurrent | fLGCircularLoggingHistory ) ) ? fLGCircularLoggingHistory : 0;
	}


//  ================================================================
VOID LOG::AdvanceLgposToFlushToNewGen( const LONG lGeneration )
//  ================================================================
	{
	m_lgposToFlush.lGeneration = lGeneration;
	m_lgposToFlush.isec = (WORD) m_csecHeader;
	m_lgposToFlush.ib = 0;
	}


//  ================================================================
VOID LOG::GetSignatureInLgfilehdrT( IFileSystemAPI * const pfsapi )
//  ================================================================
//
//	This is the first logfile in the series (generation 1). Generate
//	a new signature
//
//-
	{
	Assert( 1 == m_plgfilehdrT->lgfilehdr.le_lGeneration );
	
	SIGGetSignature( &m_plgfilehdrT->lgfilehdr.signLog );

	//	first generation, set checkpoint
	
	m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration = m_plgfilehdrT->lgfilehdr.le_lGeneration;
	m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_isec = (WORD) m_csecHeader;
	m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_ib = 0;
	cLGCheckpoint.Set( m_pinst, CbOffsetLgpos( m_pcheckpoint->checkpoint.le_lgposCheckpoint, lgposMin ) );
	m_pcheckpoint->checkpoint.signLog = m_plgfilehdrT->lgfilehdr.signLog;
	
	//	update checkpoint file before write log file header to make the
	//	attachment information in check point will be consistent with
	//	the newly generated log file.
	
	(VOID) ErrLGUpdateCheckpointFile( pfsapi, fFalse );
	}


//  ================================================================
VOID LOG::SetSignatureInLgfilehdrT( IFileSystemAPI * const pfsapi )
//  ================================================================
//
//	Set the signature in the new logfile header
//	If this is the first logfile (generation 1) this will generate a new signature
//
//-
	{
	if ( 1 == m_plgfilehdrT->lgfilehdr.le_lGeneration )
		{
		GetSignatureInLgfilehdrT( pfsapi );
		}
	else
		{
		Assert( m_fSignLogSet );
		m_plgfilehdrT->lgfilehdr.signLog = m_signLog;
		}
	}


//  ================================================================
ERR LOG::ErrOpenTempLogFile(
	IFileSystemAPI * const 	pfsapi,
	IFileAPI ** const 		ppfapi,
	const CHAR * const 		szPathJetTmpLog,
	BOOL * const 			pfResUsed,
	QWORD * const 			pibPattern )
//  ================================================================
//
//	Open a temporary logfile. This involves either:
//		- waiting for the background thread to create the logfile
//		- opening/creating the file conventionally
//
//-
	{	
	ERR err = JET_errSuccess;

	if ( ( m_errCreateAsynch < JET_errSuccess ) || m_pfapiJetTmpLog )
		{
		CAAssert( m_fCreateAsynchLogFile );
		
		//	Wait for any outstanding IO to complete

		if ( m_pfapiJetTmpLog )
			{
			CallS( m_pfapiJetTmpLog->ErrIOIssue() );
			}
		m_asigCreateAsynchIOCompleted.Wait();
		
		*ppfapi 	= m_pfapiJetTmpLog;
		*pfResUsed 	= m_fCreateAsynchResUsed;

		//	If there was an error, leave *pibPattern to default so we don't
		//	accidentally use the wrong value for the wrong file. Scenario:
		//	some new developer modifies some code below that opens up
		//	a different file, yet leaves *pibPattern set to the previous
		//	high-water mark from the previous file we're err'ing out on.
		//
		if ( m_errCreateAsynch >= JET_errSuccess )
			{
			*pibPattern	= m_ibJetTmpLog;
			}
		
		err 		= m_errCreateAsynch;
		
		m_pfapiJetTmpLog 		= NULL;
		m_fCreateAsynchResUsed 	= fFalse;
		m_errCreateAsynch 		= JET_errSuccess;
		m_ibJetTmpLog 			= 0;

		CheckLgposCreateAsyncTrigger();

		if ( JET_errDiskFull == err && ! *pfResUsed )
			{
			CAAssert( JET_errDiskFull == err );
			CAAssert( ! *pfResUsed );
						
			delete *ppfapi;
			*ppfapi = NULL;

			//	Upon disk full from asynchronous log file creation, there must
			//	be an edbtmp.log at this point because any creation errors
			//	during asynch creation would have caused the reserves to
			//	be tried. And if the reserves have been tried, we can't be
			//	in this code path.
			
			Call( pfsapi->ErrFileDelete( szPathJetTmpLog ) );

			Call( ErrLGIOpenReserveLogFile( pfsapi, ppfapi, szPathJetTmpLog ) );
			*pfResUsed = fTrue;
			}
		else
			{
			Call( err );
			}
		}
	else
		{
		Call( ErrLGIOpenTempLogFile( pfsapi, ppfapi, szPathJetTmpLog, pfResUsed ) );
		}
		
	CAAssert( *ppfapi );
	CAAssert( !m_asigCreateAsynchIOCompleted.FTryWait() );

HandleError:
	return err;
	}

//  ================================================================
ERR LOG::ErrFormatLogFile(
	IFileSystemAPI * const 	pfsapi,
	IFileAPI ** const 		ppfapi,
	const CHAR * const 		szPathJetTmpLog,
	BOOL * const 			pfResUsed,
	const QWORD& 			cbLGFile,
	const QWORD& 			ibPattern )
//  ================================================================
//
//	Synchronously fill the rest of the file.
//
//	If we run out of space on the disk when making a new log file or
//	reusing an archived log file (which might be a different size than
//	our current log files, I believe), then we should delete the file we're
//	trying to write to (which may free up some more space in the case
//	of extending an archived log file).
//
//	Complete formatting synchronously if it hasn't completed and truncate
//	log file to correct size if necessary
//
//-
	{
	ERR err = JET_errSuccess;
	
	err = ErrUtilFormatLogFile( *ppfapi, cbLGFile, ibPattern );
	if ( JET_errDiskFull == err && ! *pfResUsed )
		{
		delete *ppfapi;
		*ppfapi = NULL;

		//	If we can't delete the current edbtmp.log, we are in a bad place.
		
		Call( pfsapi->ErrFileDelete( szPathJetTmpLog ) );

		Call( ErrLGIOpenReserveLogFile( pfsapi, ppfapi, szPathJetTmpLog ) );
		*pfResUsed = fTrue;

		//	If we are unable to fill/extend the reserved log file, we've
		//	made all attempts to make a new log file.
		Call( ErrUtilFormatLogFile( *ppfapi, cbLGFile ) );
		}
	else
		{
		Call( err );
		}

HandleError:
	return err;
	}


//  ================================================================
VOID LOG::InitPlrckChecksumExistingLogfile(
	const LOGTIME& 			tmOldLog,
	LRCHECKSUM ** const 	pplrck )
//  ================================================================
	{
	//	set position of first record
	
	Assert( m_lgposToFlush.lGeneration && m_lgposToFlush.isec );

	//	overhead contains one LRMS and one will-be-overwritten lrtypNOP
	
	Assert( m_pbEntry >= m_pbLGBufMin && m_pbEntry < m_pbLGBufMax );
	Assert( m_pbWrite >= m_pbLGBufMin && m_pbWrite < m_pbLGBufMax );

	// I think this is the case where we're being called from
	// ErrLGIWriteFullSectors() and the stuff before m_pbWrite has already
	// been written to the old log file. ErrLGILogRec() has reserved
	// space for this LRCHECKSUM that we're setting up here, and
	// other threads may be appending log records to the log buffer
	// right after this reserved space.
	//
	// This could also be the case when we're called during ErrLGRedoFill()
	// and that just setup m_pbWrite = m_pbEntry = m_pbLGBufMin.

	// sector aligned
	
	Assert( m_pbWrite < m_pbLGBufMax );
	Assert( PbSecAligned( m_pbWrite ) == m_pbWrite );
	*pplrck = reinterpret_cast< LRCHECKSUM* >( m_pbWrite );

	// In the first case (from the comment above), this will already be setup
	
	m_pbLastChecksum = m_pbWrite;

	m_plgfilehdrT->lgfilehdr.tmPrevGen = tmOldLog;
	}	


//  ================================================================
VOID LOG::InitPlrckChecksumNewLogfile( LRCHECKSUM ** const pplrck )
//  ================================================================
	{		
	//	no currently valid logfile initialize checkpoint to start of file
	// This is the case when we're called from ErrLGSoftStart().
	// Append to log buffer after an initial LRCHECKSUM record.
	
	m_pbEntry = m_pbLGBufMin + sizeof( LRCHECKSUM );
	m_pbWrite = m_pbLGBufMin;

	*pplrck = reinterpret_cast< LRCHECKSUM* >( m_pbLGBufMin );

	m_pbLastChecksum = m_pbLGBufMin;
	}
	

//  ================================================================
VOID LOG::InitPlrckChecksum(
	const LOGTIME& tmOldLog,
	const BOOL fLGFlags,
	LRCHECKSUM ** const pplrck )
//  ================================================================
//
//	set lgfilehdr and m_pbLastMSFlush and m_lgposLastMSFlush.
//
//-
	{
	if ( fLGFlags == fLGOldLogExists || fLGFlags == fLGOldLogInBackup )
		{
		InitPlrckChecksumExistingLogfile( tmOldLog, pplrck );		
		}
	else
		{
		InitPlrckChecksumNewLogfile( pplrck );		
		}

	memset( reinterpret_cast< BYTE* >( *pplrck ), 0, sizeof( LRCHECKSUM ) );
	(*pplrck)->lrtyp = lrtypChecksum;
	
	if ( !COSMemoryMap::FCanMultiMap() )
		{
		*(LRCHECKSUM*)(m_pbWrite + m_cbLGBuf) = **pplrck;
		}		
	}


//  ================================================================
ERR LOG::ErrWriteLrck(
	IFileAPI * const 		pfapi,
	LRCHECKSUM * const 		plrck,
	const BOOL				fLogAttachments )
//  ================================================================
//
//  Write down the single "empty" LRCK to disk to signify at correct log file.
//
//-
	{
	ERR err = JET_errSuccess;
	
	Assert( pNil != plrck );
	Assert( lrtypChecksum == plrck->lrtyp );
	
	plrck->bUseShortChecksum = bShortChecksumOff;
	plrck->le_ulShortChecksum = 0;
	plrck->le_ulChecksum = UlComputeChecksum( plrck, m_plgfilehdrT->lgfilehdr.le_lGeneration );

	if ( !COSMemoryMap::FCanMultiMap() )
		{
		*(LRCHECKSUM*)(m_pbWrite + m_cbLGBuf) = *plrck;
		}

#ifdef UNLIMITED_DB
	if ( fLogAttachments )
		{
		Assert( !m_fLGNeedToLogDbList );

		Call( ErrLGLoadDbListFromFMP_() );

		//	force logging of attachments next time a new log record is buffered
		//	don't need critLGBuf here because we are in the non-concurrent logging case
		m_fLGNeedToLogDbList = fTrue;
		}
	else
		{

		//	this is the case where we are concurrently logging to the buffer while creating a new log file
		//	ErrLGILogRec should have setup the database attachments log record already (if there were any)
		//	UNDONE: may go off on RecoveryUndo if it is the first record in the log file
		Assert( m_pbEntry == m_pbWrite + sizeof(LRCHECKSUM)
			|| lrtypDbList == ( (LR*)( m_pbWrite + sizeof( LRCHECKSUM ) ) )->lrtyp );
		}
#endif	//	UNLIMITED_DB

	//	the log sector sizes must match (mismatch cases should prevent us from
	//		getting to this codepath)

	Assert( m_cbSecVolume == m_cbSec );

	//	write the first log record and its shadow

	err = pfapi->ErrIOWrite( m_csecHeader * m_cbSec, m_cbSec, m_pbWrite );
	if ( err < 0 )
		{
		//	report errors and disable the log
		LGReportError( LOG_FILE_SYS_ERROR_ID, err, pfapi );
		LGReportError( LOG_WRITE_ERROR_ID, JET_errLogWriteFail, pfapi );
		SetFNoMoreLogWrite( err );

		err = ErrERRCheck( JET_errLogWriteFail );
		goto HandleError;
		}

	//	update performance counters

	cLGWrite.Inc( m_pinst );
	cLGBytesWritten.Add( m_pinst, m_cbSec );

	//	ulShortChecksum does not change for the shadow sector
	
	plrck->le_ulChecksum = UlComputeShadowChecksum( plrck->le_ulChecksum );
	
	//	write the shadow sector

	err = pfapi->ErrIOWrite( ( m_csecHeader + 1 ) * m_cbSec, m_cbSec, m_pbWrite );
 	if ( err < 0 )
		{
		//	report errors and disable the log
		LGReportError( LOG_FILE_SYS_ERROR_ID, err, pfapi );
		LGReportError( LOG_WRITE_ERROR_ID, JET_errLogWriteFail, pfapi );
		SetFNoMoreLogWrite( err );

		err = ErrERRCheck( JET_errLogWriteFail );
		goto HandleError;
		}

	//	update performance counters

	cLGWrite.Inc( m_pinst );
	cLGBytesWritten.Add( m_pinst, m_cbSec );

	Assert( m_critLGFlush.FOwner() );
	m_fHaveShadow = fTrue;

HandleError:
	return err;
	}


//  ================================================================
ERR LOG::ErrStartAsyncLogFileCreation(
	IFileSystemAPI * const 	pfsapi,
	const CHAR * const 		szPathJetTmpLog )
//  ================================================================
//
//	Start working on preparing edbtmp.log by opening up the file and
//	setting the trigger for the first I/O. Note that we will only do
//	asynch creation if we can extend the log file with arbitrarily big
//	enough I/Os.
//
//-
	{
	if ( m_fCreateAsynchLogFile && cbLogExtendPattern >= 1024 * 1024 )
		{
		CAAssert( !m_pfapiJetTmpLog );
		CAAssert( !m_fCreateAsynchResUsed );
		CAAssert( !m_ibJetTmpLog );
		CAAssert( JET_errSuccess == m_errCreateAsynch );
		CAAssert( !m_asigCreateAsynchIOCompleted.FTryWait() );
		CAAssert( 0 == m_ibJetTmpLog );

		//	Treat an error in opening like an error in an asynch I/O
		//	(except for the fact that m_pfapiJetTmpLog == NULL which
		//	isn't true in the case of an asynch I/O error).
		
		m_errCreateAsynch = ErrLGIOpenTempLogFile(
								pfsapi,
								&m_pfapiJetTmpLog,
								szPathJetTmpLog,
								&m_fCreateAsynchResUsed );
				
		if ( m_errCreateAsynch >= JET_errSuccess )
			{
			//	Configure trigger to cause first formatting I/O when a half
			//	chunk of log buffer has been filled. This is because we don't
			//	want to consume the log disk with our edbtmp.log I/O since
			//	it's pretty likely that someone is waiting for a log flush to
			//	edb.log right now (especially since we've been spending our
			//	time finishing formatting, renaming files, etc.).
			
			m_critLGBuf.Enter();
			LGPOS	lgpos = { 0, 0, m_plgfilehdr->lgfilehdr.le_lGeneration };
			AddLgpos( &lgpos, CbLGICreateAsynchIOSize() / 2 );
			m_lgposCreateAsynchTrigger = lgpos;
			m_critLGBuf.Leave();
			}

		m_asigCreateAsynchIOCompleted.Set();
		}

	return JET_errSuccess;		
	}


//  ================================================================
ERR LOG::ErrUpdateGenRequired( IFileSystemAPI * const pfsapi )
//  ================================================================
//
//	Update all the database headers with proper lGenMinRequired and lGenMaxRequired.
//
//-
	{
	LONG	lGenMinRequired;
	
	m_critCheckpoint.Enter();
	
	// Now disallow header update by other threads (log writer or checkpoint advancement)
	// 1. For the log writer it is OK to generate a new log w/o updating the header as no log operations
	// for this db will be logged in new logs <- THIS CASE
	// 2. For the checkpoint: don't advance the checkpoint if db's header weren't update 

	if ( m_fRecovering && m_fRecoveringMode == fRecoveringRedo )
		{
		// during recovery we don't update  the checkpoint  (or don't have it during hard recovery)
		// so we may have the current genMin in the header 
		// bigger than m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration
		// We want to keep the existing one so reset the new genMin
		lGenMinRequired = 0;
		}
	else
		{
		lGenMinRequired = m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration;
		}
		
	const ERR err = ErrLGIUpdateGenRequired(
			pfsapi,
			lGenMinRequired,
			m_plgfilehdrT->lgfilehdr.le_lGeneration,
			m_plgfilehdrT->lgfilehdr.tmCreate,
			NULL );
	m_critCheckpoint.Leave();

	return err;
	}


//  ================================================================
ERR LOG::ErrRenameTmpLog(
	IFileSystemAPI * const 	pfsapi,
	const CHAR * const 		szPathJetTmpLog,
	const CHAR * const		szPathJetLog )
//  ================================================================
//
//	Rename szJetTmpLog
//	Start the async creation of a new temporary logfile
//	Update lGenMin/MaxRequired in database headers
//
//-
	{
	ERR err = JET_errSuccess;
	
	//	rename szJetTmpLog to szJetLog, and open it as szJetLog
	//	i.e. edbtmp.log => edb.log
	
	Call( pfsapi->ErrFileMove( szPathJetTmpLog, szPathJetLog ) );

	//  FILESYSTEM STATE: edb.log [archive files including edb00001.log]

	Call( ErrStartAsyncLogFileCreation( pfsapi, szPathJetTmpLog ) );

	//	update all the database headers with proper lGenMinRequired and lGenMaxRequired.
	
	Call( ErrUpdateGenRequired( pfsapi ) );

HandleError:
	return err;
	}


//  ================================================================
ERR LOG::ErrLGStartNewLogFile(
	IFileSystemAPI* const	pfsapi,
	const LONG				lgenToClose,
	BOOL					fLGFlags
	)
//  ================================================================
//
//	Begin the creation of a new logfile. This leaves us with 
//	EDBTMP.LOG and [archived logs]
//
//-
	{
	ERR				err			= JET_errSuccess;
	CHAR  			szPathJetLog[ IFileSystemAPI::cchPathMax ];
	CHAR  			szPathJetTmpLog[ IFileSystemAPI::cchPathMax ];
	CHAR			szArchivePath[ IFileSystemAPI::cchPathMax ];	// generational name of current log file if any
	CHAR			szFNameT[ IFileSystemAPI::cchPathMax ];
	LOGTIME			tmOldLog;
	IFileAPI*		pfapi		= NULL;
	LRCHECKSUM*		plrck		= pNil;
	BOOL			fResUsed	= fFalse;
	const QWORD		cbLGFile	= QWORD( m_csecLGFile ) * m_cbSec;
	QWORD			ibPattern	= 0;
	BOOL			fLogAttachments		= ( fLGFlags & fLGLogAttachments );

	//	all other flags are mutually exclusive, so filter out this flag
	fLGFlags &= ~fLGLogAttachments;

	Assert( 0 == fLGFlags
		|| fLGOldLogExists == fLGFlags
		|| fLGOldLogNotExists == fLGFlags
		|| fLGOldLogInBackup == fLGFlags );

	Assert( m_critLGFlush.FOwner() );

	CheckForGenerationOverflow();
	
	LGMakeLogName( szPathJetLog, m_szJet );
	LGMakeLogName( szPathJetTmpLog, m_szJetTmp );

	Call( ErrOpenTempLogFile(
			pfsapi,
			&pfapi,
			szPathJetTmpLog,
			&fResUsed,
			&ibPattern ) );
			
	Call( ErrFormatLogFile(
			pfsapi,
			&pfapi,
			szPathJetTmpLog,
			&fResUsed,
			cbLGFile,
			ibPattern ) );
		
	if ( fLGFlags != fLGOldLogExists && fLGFlags != fLGOldLogInBackup )
		{
		//	reset file header

		memset( m_plgfilehdr, 0, sizeof( LGFILEHDR ) );
		}
	else
		{
		// there was a previous szJetLog file, close it and
		// create an archive name for it, do not rename it yet.
		
		tmOldLog = m_plgfilehdr->lgfilehdr.tmCreate;

		// Compute a generational-name for the current edb.log
		
		LGSzFromLogId( szFNameT, m_plgfilehdr->lgfilehdr.le_lGeneration );
		
		// Remember the current log file name as the generational-name
		
		LGMakeLogName( szArchivePath, szFNameT );
		}

	//	edbtmp.log is still open

	m_critLGBuf.Enter();

	//	initialize the checksum logrecord
	
	InitPlrckChecksum( tmOldLog, fLGFlags, &plrck );
		
	AdvanceLgposToFlushToNewGen( lgenToClose + 1 );
	
	CAAssert( 0 == CmpLgpos( lgposMax, m_lgposCreateAsynchTrigger ) );

	m_critLGBuf.Leave();

	//	initialize the new szJetTempLog file header
	//	write new header into edbtmp.log
	
	InitLgfilehdrT();
	m_plgfilehdrT->lgfilehdr.le_lGeneration = lgenToClose + 1;
	LGIGetDateTime( &m_plgfilehdrT->lgfilehdr.tmCreate );
	m_pinst->SaveDBMSParams( &m_plgfilehdrT->lgfilehdr.dbms_param );

#ifdef UNLIMITED_DB
#else
	LGLoadAttachmentsFromFMP( m_pinst, m_plgfilehdrT->rgbAttach );
#endif	

	SetSignatureInLgfilehdrT( pfsapi );
	SetFLGFlagsInLgfilehdrT( fResUsed );	
	m_plgfilehdrT->lgfilehdr.le_filetype = filetypeLG;

	Call( ErrLGWriteFileHdr( m_plgfilehdrT, pfapi ) );

	//	write the checksum record to the new logfile
	
	Call( ErrWriteLrck( pfapi, plrck, fLogAttachments ) );

	//	Close edb.log if it is open so it may be renamed to edbXXXXX.log
	
	delete m_pfapiLog;
	m_pfapiLog = NULL;

	// Notice that we do not do the rename if we're
	// restoring from backup since edb.log may not exist.
	// (the case where fLGFlags == fLGOldLogInBackup).
	
	if ( fLGFlags == fLGOldLogExists )
		{
		//  FILESYSTEM STATE: edb.log edbtmp.log [archive files]
		
		//	there was a previous szJetLog, rename it to its archive name
		//	i.e. edb.log => edb00001.log
		
		Call( pfsapi->ErrFileMove( szPathJetLog, szArchivePath ) );

		//  FILESYSTEM STATE: edbtmp.log [archive files including edb00001.log]
		}
	else
		{
		//  FILESYSTEM STATE: edbtmp.log [archive files]
		}

	//  FILESYSTEM STATE: edbtmp.log [archive files]

	//	Close edbtmp.log so it may be renamed to edb.log
	
HandleError:

	delete m_pfapiLog;
	m_pfapiLog = NULL;
	
	delete pfapi;
	pfapi = NULL;

	return err;
	}


//  ================================================================
ERR LOG::ErrLGFinishNewLogFile( IFileSystemAPI * const pfsapi )
//  ================================================================
//
//	When this is called we should have EDBTMP.LOG and [archived logs]
//	EDBTMP.LOG will be renamed to EDB.LOG
//
//-
	{
	ERR				err			= JET_errSuccess;
	CHAR  			szPathJetLog[ IFileSystemAPI::cchPathMax ];
	CHAR  			szPathJetTmpLog[ IFileSystemAPI::cchPathMax ];

	Assert( m_critLGFlush.FOwner() );
	
	LGMakeLogName( szPathJetLog, m_szJet );
	LGMakeLogName( szPathJetTmpLog, m_szJetTmp );

	Call( ErrRenameTmpLog( pfsapi, szPathJetTmpLog, szPathJetLog ) );

HandleError:
	return err;
	}


/*
 *	Closes current log generation file, creates and initializes new
 *	log generation file in a safe manner.
 *	create new log file	under temporary name
 *	close active log file (if fLGFlags is fTrue)
 *	rename active log file to archive name numbered log (if fOld is fTrue)
 *	close new log file (still under temporary name)
 *	rename new log file to active log file name
 *	open new active log file with ++lGenerationToClose
 *
 *	PARAMETERS	plgfilehdr		pointer to log file header
 *				lGeneration 	current generation being closed
 *				fOld			TRUE if a current log file needs closing
 *
 *	RETURNS		JET_errSuccess, or error code from failing routine
 *
 *	COMMENTS	Active log file must be completed before new log file is
 *				called. Log file will not be opened upon completion.
 */
ERR LOG::ErrLGNewLogFile(
	IFileSystemAPI* const	pfsapi,
	const LONG				lgenToClose,
	BOOL					fLGFlags
	)
	{
	ERR						err;
	
	Call( ErrLGStartNewLogFile( pfsapi, lgenToClose, fLGFlags ) );
	Call( ErrLGFinishNewLogFile( pfsapi ) );

HandleError:
	if ( JET_errDiskFull == err )
		{
		err = ErrERRCheck( JET_errLogDiskFull );
		}
	
	if ( err < 0 )
		{
		UtilReportEventOfError( LOGGING_RECOVERY_CATEGORY, NEW_LOG_ERROR_ID, err, m_pinst );
		SetFNoMoreLogWrite( err );
		}
	else
		{
		ResetFNoMoreLogWrite();
		}
	
	return err;
	}


// There are certain possible situations where we just finished reading a
// finished log file (never to be written to again) and it is either
// named edb.log or edbXXXXX.log (generational or archive name). We want to
// make ensure that it gets renamed to an archive name if necessary, and then
// a new log file is created as edb.log that we switch to.

 ERR LOG::ErrLGISwitchToNewLogFile( IFileSystemAPI *const pfsapi, BOOL fLGFlags )
	{
	// special case where the current log file has ended, so we need
	// to create a new log file and write to that instead.
	// Only need to do the switch when we notice that the current log generation
	// is different from the flush point generation
	Assert( pNil != m_plgfilehdr );
	
	if ( m_plgfilehdr->lgfilehdr.le_lGeneration != m_lgposToFlush.lGeneration )
		{
		// If this fires, we probably picked up some other horrible special case or
		// impossible condition. 
		Assert( m_lgposToFlush.lGeneration == m_plgfilehdr->lgfilehdr.le_lGeneration + 1 );

		// There must be a log file open from just calling ErrLGCheckReadLastLogRecord()
		Assert( m_pfapiLog );
		
		ERR			err			= JET_errSuccess;
		CHAR  		szPathJetLog[IFileSystemAPI::cchPathMax];

		// edb.log
		LGMakeLogName( szPathJetLog, m_szJet );
		
		m_critLGFlush.Enter();

		err = ErrUtilPathExists( pfsapi, szPathJetLog );
		if ( err < 0 )
			{
			if ( JET_errFileNotFound == err )
				{
				// edb.log does not exist, so don't try to rename a nonexistant
				// file from edb.log to generational name
				// Some kind of log file is open, so it must be a generational name
				fLGFlags |= fLGOldLogInBackup;
				// close log file because ErrLGNewLogFile( fLGOldLogInBackup ) does not
				// expect an open log file.
				delete m_pfapiLog;
				m_pfapiLog = NULL;
				}
			else
				{
				// other unexpected error
				Call( err );
				}
			}
		else
			{
			// edb.log does exist, so we want it renamed to generational name
			fLGFlags |= fLGOldLogExists;
			// generational archive named log file is still open, but ErrLGNewLogFile
			// will close it and rename it
			}

		Call( ErrLGNewLogFile( pfsapi, m_plgfilehdr->lgfilehdr.le_lGeneration, fLGFlags ) );
		LGMakeLogName( m_szLogName, m_szJet );
		Call( pfsapi->ErrFileOpen( m_szLogName, &m_pfapiLog ) );

		m_critLGBuf.Enter();	// <----------------------------

		Assert( m_plgfilehdrT->lgfilehdr.le_lGeneration == m_plgfilehdr->lgfilehdr.le_lGeneration + 1 );
		UtilMemCpy( m_plgfilehdr, m_plgfilehdrT, sizeof( LGFILEHDR ) );
		m_isecWrite = m_csecHeader;
		m_pbLGFileEnd = pbNil;
		m_isecLGFileEnd = 0;

		m_critLGBuf.Leave();	// <----------------------------

HandleError:
		m_critLGFlush.Leave();

		return err;
		}
	return JET_errSuccess;
	}

/*
 *	Log flush thread is signalled to flush log asynchronously when at least
 *	cThreshold disk sectors have been filled since last flush.
 */

VOID LOG::LGFlushLog( const DWORD_PTR dwThreadContext, const DWORD fCalledFromFlushAll, const DWORD_PTR dwThis )
	{
	LOG *plog = (LOG *)dwThis;
#ifdef DEBUG
	plog->m_dwDBGLogThreadId = DwUtilThreadId();
#endif

	int iGroup = plog->m_msLGTaskExec.Enter();

	//	we succeeded to post the flush task.
	//	If we were failed before, decrement the number of failed tasks
	if ( AtomicExchange( (LONG *)&plog->m_fLGFailedToPostFlushTask, fFalse ) )
		{
		const LONG	lCount	= AtomicDecrement( (LONG *)&m_cLGFailedToPost );
		Assert( lCount >= 0 );
		}

	plog->m_critLGFlush.Enter();

	LONG lStatus = AtomicCompareExchange( (LONG *)&plog->m_fLGFlushWait, 1, 0 );

	//	if we have shutdown request
	//	notify the client that we're almost done

	if ( 2 == lStatus )
		{
		plog->m_asigLogFlushDone.Set();
		}
	else
		{
		Assert( 1 == lStatus );
		}
		
	//  try to flush the log

	const BOOL fLogDownBeforeFlush = plog->m_fLGNoMoreLogWrite;
	(void)plog->ErrLGIFlushLog( plog->m_pinst->m_pfsapi );
	const BOOL fLogDownAfterFlush = plog->m_fLGNoMoreLogWrite;

	plog->m_critLGFlush.Leave();

	//  if the log just went down for whatever reason, report it

	if ( !fLogDownBeforeFlush && fLogDownAfterFlush )
		{
		const _TCHAR*	rgpsz[ 1 ];
		DWORD			irgpsz		= 0;
		_TCHAR			szAbsPath[ IFileSystemAPI::cchPathMax ];

		if ( plog->m_pinst->m_pfsapi->ErrPathComplete( plog->m_szLogFilePath, szAbsPath ) < JET_errSuccess )
			{
			_tcscpy( szAbsPath, plog->m_szLogFilePath );
			}

		rgpsz[ irgpsz++ ]	= szAbsPath;

		UtilReportEvent(
				eventError,
				LOGGING_RECOVERY_CATEGORY,
				LOG_DOWN_ID,
				irgpsz,
				rgpsz,
				0,
				NULL,
				plog->m_pinst );
		}

	plog->m_msLGTaskExec.Leave( iGroup );

	//	if LGFlush is not called from LGFlushAllLogs 
	//	then do the flush all check
	if ( !fCalledFromFlushAll )
		{
		//	if there are any logs that are waiting to be flushed
		//	use current thread to flush them
		LGFlushAllLogs( 0, fTrue, 0 );
		}
	}

LONG volatile LOG::m_cLGFailedToPost	= 0;
LONG volatile LOG::m_fLGInFlushAllLogs	= fFalse;

VOID LOG::LGFlushAllLogs(
	const DWORD_PTR,					// ignored
	const DWORD fAfterFlushCheck,		// true if it is clean up check after regular log flush, false if it is thread timeout
	const DWORD_PTR )					// ignored
	{
	//	nothing to do?
	if ( 0 == m_cLGFailedToPost )
		{
		return;
		}

	//	try to enter flush all logs crit section
	if ( !AtomicCompareExchange( (LONG *)&m_fLGInFlushAllLogs, fFalse, fTrue ) )
		{
		//	loop check through all the instances
		for ( int ipinst = 0; ipinst < ipinstMax && 0 < m_cLGFailedToPost; ipinst++ )
			{
			//	lock the instance
			extern CRITPOOL< INST* > critpoolPinstAPI;
			CCriticalSection *pcritInst = &critpoolPinstAPI.Crit(&g_rgpinst[ ipinst ]);
			pcritInst->Enter();
			INST *pinst;
			pinst = g_rgpinst[ ipinst ];
			if ( NULL != pinst && pinst->m_fJetInitialized )
				{
				//	if it is waiting to be flushed or is time to flush all the logs flush it
				//	try to enter flush mode
				if ( ( pinst->m_plog->m_fLGFailedToPostFlushTask )
					&& 0 == AtomicCompareExchange( (LONG *)&(pinst->m_plog->m_fLGFlushWait), 0, 1 ) )
					{
					LGFlushLog( 0, fTrue, (DWORD_PTR)(pinst->m_plog) );
					}
				}
			pcritInst->Leave();
			}
			
		//	leave flush all logs critical section
		AtomicExchange( (LONG *)&m_fLGInFlushAllLogs, fFalse );
		}
	}

// Given a pointer into the log buffer and the size of a block
// we want to fit on a single sector, tell us where we should
// put the block.
BYTE* LOG::PbGetSectorContiguous( BYTE* pb, size_t cb )
	{
	Assert( pb >= m_pbLGBufMin && pb < m_pbLGBufMax );
	// If you're going to use this to get space as big as a sector,
	// please review the code.
	Assert( cb > 0 && cb < m_cbSec );
	
	INT		cbToFill = ( m_cbSec * 2 - ULONG( pb - PbSecAligned( pb ) ) ) % m_cbSec;
	
	if ( 0 == cbToFill )
		{
		// We're looking at a sector boundary right now.
		Assert( PbSecAligned( pb ) == pb );
		}
	else if ( cbToFill < cb )
		{
		// We need to fill cbToFill bytes with NOPs and
		// use the space after it.
		Assert( FIsFreeSpace( pb, cbToFill ) );
		pb += cbToFill;
		Assert( pb >= ( m_pbLGBufMin + m_cbSec ) && pb <= m_pbLGBufMax );
		Assert( PbSecAligned( pb ) == pb );
		if ( pb == m_pbLGBufMax )
			{
			pb = m_pbLGBufMin;
			}
		Assert( pb >= m_pbLGBufMin && pb < m_pbLGBufMax );
		// This requires the use of another sector.
		}
	else if ( cbToFill == cb )
		{
		// We've got just the perfect amount of space.
		}
	else
		{
		// We've got plenty of space already
		}
	Assert( FIsFreeSpace( pb, ULONG( cb ) ) );
	return pb;
	}

//	XXX
//	If you modify this function, be sure to modify the class VERIFYCHECKSUM, especially
//	ErrVerifyLRCKStart.

ULONG32 LOG::UlComputeChecksum( const LRCHECKSUM* const plrck, const ULONG32 lGeneration )
	{
	ULONG32 ulChecksum = ulLRCKChecksumSeed;
	const BYTE*	pb = pbNil;
	Assert( NULL != plrck );
	const BYTE*	pbEnd = pbNil;

	// Takes advantage of VM wrap-around

	Assert( reinterpret_cast< const BYTE* >( plrck ) >= m_pbLGBufMin &&
		reinterpret_cast< const BYTE* >( plrck ) < m_pbLGBufMax );

	pb = reinterpret_cast< const BYTE* >( plrck ) - plrck->le_cbBackwards;
	pbEnd = reinterpret_cast< const BYTE* >( plrck );
	
	Assert( pb >= m_pbLGBufMin && pb < m_pbLGBufMax );
	Assert( pbEnd >= m_pbLGBufMin && pbEnd < m_pbLGBufMax );

	Assert( ( pbEnd - pb == 0 ) ? fTrue : FIsUsedSpace( pb, ULONG( pbEnd - pb ) ) );
	Assert( pbEnd >= pb );

	//	checksum the "backward" range
	ulChecksum = UlChecksumBytes( pb, pbEnd, ulChecksum );

 	pb = reinterpret_cast< const BYTE* >( plrck ) + sizeof( *plrck );
	pbEnd = reinterpret_cast< const BYTE* >( plrck )
		+ sizeof( *plrck ) + plrck->le_cbForwards;

	// Stay in region of the 2 mappings
	Assert( pb >= ( m_pbLGBufMin + sizeof( *plrck ) ) );
	Assert( pb <= m_pbLGBufMax );
	Assert( pbEnd > m_pbLGBufMin && pbEnd <= ( m_pbLGBufMax + m_cbLGBuf ) );

	Assert( ( pbEnd - pb == 0 ) ? fTrue 
								: FIsUsedSpace( ( pb == m_pbLGBufMax )	? m_pbLGBufMin 
																		: pb, 
												ULONG( pbEnd - pb ) ) );
	Assert( pbEnd >= pb );

	//	checksum the "forward" range
	ulChecksum = UlChecksumBytes( pb, pbEnd, ulChecksum );

	//	checksum the backward/forward/next pointers
	ulChecksum ^= plrck->le_cbBackwards;
	ulChecksum ^= plrck->le_cbForwards;
	ulChecksum ^= plrck->le_cbNext;

	//	checksum the short checksum on/off byte
	Assert( plrck->bUseShortChecksum == bShortChecksumOn || 
			plrck->bUseShortChecksum == bShortChecksumOff );
	ulChecksum ^= (ULONG32)plrck->bUseShortChecksum;
	
	// The checksum includes the log generation number, so we can't
	// eat old log data from a previous generation and think that it is current data.
	ulChecksum ^= lGeneration;

	return ulChecksum;
	}


ULONG32 LOG::UlComputeShadowChecksum( const ULONG32 ulOriginalChecksum )
	{
	return ulOriginalChecksum ^ ulShadowSectorChecksum;
	}


ULONG32 LOG::UlComputeShortChecksum( const LRCHECKSUM* const plrck, const ULONG32 lGeneration )
	{
	ULONG32 ulChecksum = ulLRCKShortChecksumSeed;
	const BYTE*	pb = pbNil;
	Assert( NULL != plrck );
	const BYTE*	pbEnd = pbNil;

	Assert( reinterpret_cast< const BYTE* >( plrck ) >= m_pbLGBufMin &&
		reinterpret_cast< const BYTE* >( plrck ) < m_pbLGBufMax );

	pb = reinterpret_cast< const BYTE* >( plrck ) - plrck->le_cbBackwards;
	// the region should start on a sector boundary, or plrck->cbBackwards can be zero
	Assert( PbSecAligned( const_cast< BYTE* >( reinterpret_cast< const BYTE* >( plrck ) ) ) == pb || 0 == plrck->le_cbBackwards );
	pbEnd = reinterpret_cast< const BYTE* >( plrck );
	
	Assert( pb >= m_pbLGBufMin && pb < m_pbLGBufMax );
	Assert( pbEnd >= m_pbLGBufMin && pbEnd < m_pbLGBufMax );

	// Can't call without m_critLGBuf
//	Assert( pbEnd - pb == 0 ? fTrue : FIsUsedSpace( pb, pbEnd - pb ) );
	Assert( pbEnd >= pb );

	//	checksum the "backward" region
	ulChecksum = UlChecksumBytes( pb, pbEnd, ulChecksum );

 	pb = reinterpret_cast< const BYTE* >( plrck ) + sizeof( *plrck );
 	// end of short checksum region is the end of the sector.
	pbEnd = PbSecAligned( const_cast< BYTE* >( reinterpret_cast< const BYTE* >( plrck ) ) ) + m_cbSec;

	// Stay in region of the 2 mappings
	Assert( pb >= ( m_pbLGBufMin + sizeof( *plrck ) ) );
	Assert( pb <= m_pbLGBufMax );
	Assert( pbEnd > m_pbLGBufMin && pbEnd <= m_pbLGBufMax );

	// Can't call without m_critLGBuf
//	Assert( pbEnd - pb == 0 ? fTrue : FIsUsedSpace( 
//		( pb == m_pbLGBufMax ) ? m_pbLGBufMin : pb,
//		pbEnd - pb ) );
	Assert( pbEnd >= pb );
	
	//	checksum to the end of the sector (forward region and any garbage in there)
	ulChecksum = UlChecksumBytes( pb, pbEnd, ulChecksum );

	//	checksum the backward/forward/next pointers
	ulChecksum ^= plrck->le_cbBackwards;
	ulChecksum ^= plrck->le_cbForwards;
	ulChecksum ^= plrck->le_cbNext;

	//	checksum the short checksum on/off byte
	Assert( plrck->bUseShortChecksum == bShortChecksumOn || 
			plrck->bUseShortChecksum == bShortChecksumOff );
	ulChecksum ^= (ULONG32)plrck->bUseShortChecksum;
	
	// The checksum includes the log generation number, so we can't
	// eat old log data from a previous generation and think that it is current data.
	ulChecksum ^= lGeneration;

	return ulChecksum;
	}


// Setup the last LRCHECKSUM record in the log file.
// Extend its range to the end of the log file and calculate the checksum.

VOID LOG::SetupLastFileChecksum(
	BYTE* const pbLastChecksum,
	const BYTE* const pbEndOfData
	)
	{
	Assert( pbNil != pbLastChecksum );
	Assert( pbNil != pbEndOfData );
	// we should be sector aligned.
	Assert( PbSecAligned( const_cast< BYTE* >( pbEndOfData ) ) == pbEndOfData );
	LRCHECKSUM* const plrck = reinterpret_cast< LRCHECKSUM* const >( pbLastChecksum );
	// The sector with the last checksum should be the next sector to write out
	Assert( PbSecAligned( m_pbWrite ) == PbSecAligned( pbLastChecksum ) );

	Assert( FIsUsedSpace( pbLastChecksum, sizeof( LRCHECKSUM ) ) );

	// should already have type setup
	Assert( lrtypChecksum == plrck->lrtyp );
#ifdef DEBUG
	ULONG	cbOldForwards = plrck->le_cbForwards;
#endif
	plrck->le_cbForwards = ( pbEndOfData > pbLastChecksum ) ?
		( pbEndOfData - pbLastChecksum - sizeof( LRCHECKSUM ) ) :
		( pbEndOfData + m_cbLGBuf - pbLastChecksum - sizeof( LRCHECKSUM ) );
	Assert( plrck->le_cbForwards < m_cbLGBuf );
	// cbOldForwards could be the same if we were setup near the end of a log file.
	Assert( plrck->le_cbForwards >= cbOldForwards );
	// Last LRCHECKSUM should have cbNext == 0 to signify that it's range
	// is the last data in the log file.
	Assert( 0 == plrck->le_cbNext );
	// this range of data is for the current log generation.

	//	this may be a single-sector range (old checksum is 1 sector behind new checksum)
	//		or a multi-sector range (old checksum is atleast 2 sectors behind old checksum)
	//	only use the short checksum if it is a multi-sector flush
	const ULONG32 cbBackwards = ULONG32( (BYTE*)plrck - PbSecAligned( (BYTE*)plrck ) );
	Assert( plrck->le_cbBackwards == cbBackwards || plrck->le_cbBackwards == 0 );
	if ( cbBackwards + sizeof( LRCHECKSUM ) + plrck->le_cbForwards > m_cbSec )
		{
		Assert( ( cbBackwards + sizeof( LRCHECKSUM ) + plrck->le_cbForwards ) >= ( m_cbSec * 2 ) );
		plrck->bUseShortChecksum = bShortChecksumOn;
		plrck->le_ulShortChecksum = UlComputeShortChecksum( plrck, m_plgfilehdr->lgfilehdr.le_lGeneration );
		}
	else
		{
		plrck->bUseShortChecksum = bShortChecksumOff;
		plrck->le_ulShortChecksum = 0;
		}		
	plrck->le_ulChecksum = UlComputeChecksum( plrck, m_plgfilehdr->lgfilehdr.le_lGeneration );
	if ( !COSMemoryMap::FCanMultiMap() )
		*(LRCHECKSUM*)(m_pbWrite + m_cbLGBuf) = *plrck;
	}

// Setup the last LRCHECKSUM record in the log buffer that will be written out soon.
VOID LOG::SetupLastChecksum(
	BYTE*	pbLastChecksum,
	BYTE*	pbEndOfData
	)
	{
	Assert( pbNil != pbLastChecksum );
	Assert( pbNil != pbEndOfData );
	LRCHECKSUM* const	plrck = reinterpret_cast< LRCHECKSUM* const >( pbLastChecksum );

	Assert( FIsUsedSpace( pbLastChecksum, sizeof( LRCHECKSUM ) ) );

	// plrck->lrtyp should have already been setup
	Assert( lrtypChecksum == plrck->lrtyp );
	// plrck->cbBackwards should already be setup

	//	assume a single-sector range (e.g. no short checksum)
	plrck->bUseShortChecksum = bShortChecksumOff;
	plrck->le_ulShortChecksum = 0;

	// If the LRCHECKSUM is on the same sector as the end of data,
	// we just extend the LRCHECKSUM's range to cover more data.
	if ( PbSecAligned( pbLastChecksum ) == PbSecAligned( pbEndOfData ) )
		{
		// If the last LRCHECKSUM is on the same sector as where we'd
		// hypothetically put the new LRCHECKSUM, just extend the region
		// of the last checksum
		Assert( pbEndOfData > pbLastChecksum );
#ifdef DEBUG
		const ULONG	cbOldForwards = plrck->le_cbForwards;
#endif
		Assert( pbLastChecksum + sizeof( LRCHECKSUM ) <= pbEndOfData );
		plrck->le_cbForwards = ULONG32( pbEndOfData - pbLastChecksum - sizeof( LRCHECKSUM ) );
		// We should only be enlarging (or keeping the same)
		// the range covered by the LRCHECKSUM
		Assert( plrck->le_cbForwards >= cbOldForwards );
		Assert( plrck->le_cbForwards < m_cbSec );

		// Since this LRCHECKSUM is the last in the buffer, and there won't
		// be one added, this should be zero.
		Assert( 0 == plrck->le_cbNext );
		}
	else
		{
		// If LRCHECKSUM records are on different sectors
		// then the range extends up to the sector boundary of the sector
		// containing the next LRCHECKSUM
		BYTE* pbNewAligned = PbSecAligned( pbEndOfData );
		ULONG cb;
		if ( pbNewAligned > pbLastChecksum )
			cb = ULONG( pbNewAligned - pbLastChecksum );
		else
			cb = ULONG( pbNewAligned + m_cbLGBuf - pbLastChecksum );
		plrck->le_cbForwards = cb - sizeof( LRCHECKSUM );		
		Assert( plrck->le_cbForwards < m_cbLGBuf );

		plrck->le_cbNext = ( ( pbEndOfData > pbLastChecksum ) ?
			pbEndOfData - pbLastChecksum :
			pbEndOfData + m_cbLGBuf - pbLastChecksum )
			- sizeof( LRCHECKSUM );

		//	this may be a single-sector range (old checksum is 1 sector behind new checksum)
		//		or a multi-sector range (old checksum is atleast 2 sectors behind old checksum)
		//	only use the short checksum if it is a multi-sector flush
		const ULONG32 cbBackwards = ULONG32( (BYTE*)plrck - PbSecAligned( (BYTE*)plrck ) );
		Assert( plrck->le_cbBackwards == cbBackwards || plrck->le_cbBackwards == 0 );
		if ( cbBackwards + sizeof( LRCHECKSUM ) + plrck->le_cbForwards > m_cbSec )
			{
			Assert( ( cbBackwards + sizeof( LRCHECKSUM ) + plrck->le_cbForwards ) >= ( m_cbSec * 2 ) );
			plrck->bUseShortChecksum = bShortChecksumOn;
			plrck->le_ulShortChecksum = UlComputeShortChecksum( plrck, m_plgfilehdr->lgfilehdr.le_lGeneration );
			}
		}

	Assert( plrck->le_cbNext < m_cbLGBuf );
	// This range of data is for this current log generation.
	plrck->le_ulChecksum = UlComputeChecksum( plrck, m_plgfilehdr->lgfilehdr.le_lGeneration );
	}

// After we know that there's space for a new LRCHECKSUM in the log buffer,
// add it.
BYTE* LOG::PbSetupNewChecksum(
	// where to add the LRCK
	BYTE*	pb,
	// location in log buffer of the last LRCHECKSUM before this one
	BYTE*	pbLastChecksum
	)
	{
	Assert( pbNil != pb );
	Assert( pbNil != pbLastChecksum );
	LRCHECKSUM	lrck;
	BYTE*		pbNew = pb;

	Assert( FIsUsedSpace( pbLastChecksum, sizeof( LRCHECKSUM ) ) );

	lrck.lrtyp = lrtypChecksum;
	// We shouldn't be setting up a new LRCHECKSUM record on
	// the same sector as the last LRCHECKSUM record.
	Assert( PbSecAligned( pb ) != PbSecAligned( pbLastChecksum ) );
	// Backwards range goes up to the beginning of the sector we're on.
	lrck.le_cbBackwards = ULONG32( pb - PbSecAligned( pb ) );
	lrck.le_cbForwards = 0;
	lrck.le_cbNext = 0;
	// Checksums will be calculated later.
	lrck.bUseShortChecksum = bShortChecksumOff;
	lrck.le_ulShortChecksum = 0;
	lrck.le_ulChecksum = 0;

	// Add this new LRCHECKSUM to the log buffer.
	Assert( FIsFreeSpace( pb, sizeof( LRCHECKSUM ) ) );
	LGIAddLogRec( reinterpret_cast< BYTE* >( &lrck ), sizeof( lrck ), &pbNew );
	return pbNew;
	}

// Returns fTrue if there are people waiting for log records to
// be flushed after *plgposToFlush

BOOL LOG::FWakeWaitingQueue( LGPOS * plgposToFlush )
	{
	/*  go through the waiting list and wake those whose log records
	/*  were flushed in this batch.
	/*
	/*  also wake all threads if the log has gone down
	/**/

	/*	wake it up!
	/**/
	m_critLGWaitQ.Enter();

	PIB *	ppibT				= m_ppibLGFlushQHead;
	BOOL	fWaitersExist		= fFalse;

	while ( ppibNil != ppibT )
		{
		//	WARNING: need to save off ppibNextWaitFlush
		//	pointer because once we release any waiter,
		//	the PIB may get released
		PIB	* const	ppibNext	= ppibT->ppibNextWaitFlush;

		// XXX
		// This should be < because the LGPOS we give clients to wait on
		// is the LGPOS of the start of the log record (the first byte of
		// the log record). When we flush, the flush LGPOS points to the
		// first byte of the log record that did *NOT* make it to disk.
		if ( CmpLgpos( &ppibT->lgposCommit0, plgposToFlush ) < 0 || m_fLGNoMoreLogWrite )
			{
			Assert( ppibT->FLGWaiting() );
			ppibT->ResetFLGWaiting();

			if ( ppibT->ppibPrevWaitFlush )
				{
				ppibT->ppibPrevWaitFlush->ppibNextWaitFlush = ppibT->ppibNextWaitFlush;
				}
			else
				{
				m_ppibLGFlushQHead = ppibT->ppibNextWaitFlush;
				}

			if ( ppibT->ppibNextWaitFlush )
				{
				ppibT->ppibNextWaitFlush->ppibPrevWaitFlush = ppibT->ppibPrevWaitFlush;
				}
			else
				{
				m_ppibLGFlushQTail = ppibT->ppibPrevWaitFlush;
				}

			//	WARNING: cannot reference ppibT after this point
			//	because once we free the waiter, the PIB may
			//	get released
			ppibT->asigWaitLogFlush.Set();
			cLGUsersWaiting.Dec( m_pinst );
			}
		else
			{
			fWaitersExist = fTrue;
			}

		ppibT = ppibNext;
		}

	m_critLGWaitQ.Leave();

	return fWaitersExist;
	}


#ifdef LOGPATCH_UNIT_TEST

enum EIOFAILURE
	{
	iofNone			= 0,
	iofClean		= 1,//	clean		-- crash after I/O completes
	//iofTornSmall	= 2,//	torn		-- last sector is torn (anything other than last sector being torn implies
	//iofTornLarge	= 3,//										that later sectors were never written and degrades
						//										to an incomplete-I/O case)
	iofIncomplete1	= 2,//	incomplete1	-- 1 sector was not flushed
	iofIncomplete2	= 3,//	incomplete2	-- 2 sectors were not flushed
	iofMax			= 4
	};

extern BOOL			g_fEnableFlushCheck;	//	enable/disable log-flush checking
extern BOOL			g_fFlushIsPartial;		//	true --> flush should be partial, false --> flush should be full
extern ULONG		g_csecFlushSize;		//	when g_fFlushIsPartial == false, this is the number of full sectors we expect to flush

extern BOOL			g_fEnableFlushFailure;	//	enable/disable log-flush failures
extern ULONG 		g_iIO;					//	I/O that should fail
extern EIOFAILURE	g_iof;					//	method of failure



__declspec( thread ) LONG g_lStandardRandSeed = 12830917;

LONG LTestLogPatchIStandardRand()
	{
    const long a = 16807;
    const long m = 2147483647;
    const long q = 127773;
    const long r = 2836;
    long lo, hi, test;

    hi = g_lStandardRandSeed/q;
    lo = g_lStandardRandSeed%q;
    test = a*lo - r*hi;
    if ( test > 0 )
        g_lStandardRandSeed = test;
    else
        g_lStandardRandSeed = test + m;

    return g_lStandardRandSeed-1;
	}

//	do a single flush and fail when requested

ERR ErrTestLogPatchIFlush(	IFileAPI* const		pfapiLog,
							const QWORD			ibOffset,
							const DWORD			cbData,
							const BYTE* const	pbData )
	{
	ERR err;

	AssertRTL( cbData >= 512 );

	if ( g_fEnableFlushFailure )
		{
		if ( 0 == g_iIO )
			{

			//	this I/O will fail

			switch ( g_iof )
				{
				case iofClean:
					{

					//	do the I/O normally (should succeed)

					err = pfapiLog->ErrIOWrite( ibOffset, cbData, pbData );
					AssertRTL( JET_errSuccess == err );

					//	fail with a common error

					return ErrERRCheck( JET_errDiskIO );
					}

	/****************************
				case iofTornSmall:
				case iofTornLarge:
					{
					BYTE	pbDataTorn[512];
					ULONG	cb;

					//	setup a torn version of the last sector

					memcpy( pbDataTorn, pbData, 512 );

					if ( iofTornSmall == g_iof )
						{
						cb = 1 + ( LTestLogPatchIStandardRand() % 10 );
						}
					else
						{
						cb = 502 + ( LTestLogPatchIStandardRand() % 10 );
						}
					memcpy( pbDataTorn, rgbLogExtendPattern, cb );

					//	do the I/O normally (should succeed)

					err = pfapiLog->ErrIOWrite( ibOffset, cbData - 512, pbData );
					AssertRTL( JET_errSuccess == err );
					err = pfapiLog->ErrIOWrite( ibOffset + cbData - 512, 512, pbDataTorn );
					AssertRTL( JET_errSuccess == err );

					//	fail with a common error

					return ErrERRCheck( JET_errDiskIO );
					}
	*****************************/

				case iofIncomplete1:
					{
					ULONG csec;

					csec = cbData / 512;
					if ( 0 != cbData % 512 )
						{
						AssertSzRTL( fFalse, "cbData is not sector-granular!" );
						return ErrERRCheck( JET_errDiskIO );
						}
					if ( csec < 2 )
						{
						AssertSzRTL( fFalse, "csec is too small!" );
						return ErrERRCheck( JET_errDiskIO );
						}

					//	flush (should be successfull)

					err = pfapiLog->ErrIOWrite( ibOffset, ( csec - 1 ) * 512, pbData );
					AssertRTL( JET_errSuccess == err );

					//	fail with a common error

					return ErrERRCheck( JET_errDiskIO );
					}

				case iofIncomplete2:
					{
					ULONG csec;

					csec = cbData / 512;
					if ( 0 != cbData % 512 )
						{
						AssertSzRTL( fFalse, "cbData is not sector-granular!" );
						return ErrERRCheck( JET_errDiskIO );
						}
					if ( csec < 3 )
						{
						AssertSzRTL( fFalse, "csec is too small!" );
						return ErrERRCheck( JET_errDiskIO );
						}

					//	flush (should be successfull)

					err = pfapiLog->ErrIOWrite(	ibOffset, ( csec - 2 ) * 512, pbData );
					AssertRTL( JET_errSuccess == err );

					//	fail with a common error

					return ErrERRCheck( JET_errDiskIO );
					}

				default:
					AssertSzRTL( fFalse, "Invalid EIOFAILURE value!" );
					return ErrERRCheck( JET_errDiskIO );
				}
			}

		//	decrease the I/O counter

		g_iIO--;
		}

	//	do the I/O normally (should succeed)

	err = pfapiLog->ErrIOWrite( ibOffset, cbData, pbData );
	AssertRTL( JET_errSuccess == err );
	return err;
	}

#endif	//	LOGPATCH_UNIT_TEST


// Writes all the full sectors we have in the log buffer to disk.

ERR LOG::ErrLGIWriteFullSectors(
	IFileSystemAPI *const pfsapi,
	const UINT			csecFull,
	// Sector to start the write at
	const UINT			isecWrite,
	// What to write
	const BYTE* const	pbWrite,
	// Whether there are clients waiting for log records to be flushed
	// after we finish our write
	BOOL* const			pfWaitersExist,
	// LRCHECKSUM in the log buffer that we're going to write to disk
	BYTE* const			pbFirstChecksum,
	// The LGPOS of the last record not completely written out in this write.
	const LGPOS* const	plgposMaxFlushPoint
	)
	{
	ERR				err = JET_errSuccess;
	LGPOS			lgposToFlushT = lgposMin;
	UINT			csecToWrite = csecFull;
	UINT			isecToWrite = isecWrite;
	const BYTE		*pbToWrite = pbWrite;
	BYTE			*pbFileEndT = pbNil;

	// m_critLGFlush guards m_fHaveShadow
	Assert( m_critLGFlush.FOwner() );

	Assert( m_cbSecVolume != ~(ULONG)0 );
	Assert( m_cbSecVolume % 512 == 0 );
	Assert( m_cbSec == m_cbSecVolume );

#ifdef LOGPATCH_UNIT_TEST

	if ( g_fEnableFlushCheck )
		{
		AssertSzRTL( !g_fFlushIsPartial, "expected a partial flush -- got a full flush!" );
		AssertSzRTL( g_csecFlushSize == csecFull, "did not get the number of expected sectors in this full-sector flush!" );

		ULONG _ibT;
		ULONG _cbT;

		_ibT = m_cbSec;	//	do not include the first sector -- it has the LRCK record
		_cbT = csecFull * m_cbSec;
		while ( _ibT < _cbT )
			{
			AssertRTL( 0 != UlChecksumBytes( pbWrite + _ibT, pbWrite + _ibT + m_cbSec, 0 ) );
			_ibT += m_cbSec;
			}
		}

#endif	//	LOGPATCH_UNIT_TEST

	if ( m_fHaveShadow )
		{
		// shadow was written, which means, we need to write
		// out the first sector of our bunch of full sectors,
		// then the rest.

		// start of write has to be in data portion of log file.
		// last sector of log file is reserved for shadow sector
		Assert( isecToWrite >= m_csecHeader && isecToWrite < ( m_csecLGFile - 1 ) );
		// make sure we don't write past end
		Assert( isecToWrite + 1 <= ( m_csecLGFile - 1 ) );

#ifdef LOGPATCH_UNIT_TEST

		CallJ( ErrTestLogPatchIFlush(	m_pfapiLog,
										QWORD( isecToWrite ) * m_cbSec,
										m_cbSec * 1, 
										pbToWrite ), LHandleWrite0Error );

#else	//	!LOGPATCH_UNIT_TEST

		CallJ( m_pfapiLog->ErrIOWrite(	QWORD( isecToWrite ) * m_cbSec,
										m_cbSec * 1, 
										pbToWrite ), LHandleWrite0Error );

#endif	//	LOGPATCH_UNIT_TEST

		cLGWrite.Inc( m_pinst );
		cLGBytesWritten.Add( m_pinst, m_cbSec );
		isecToWrite++;
		pbToWrite += m_cbSec;
		csecToWrite--;
		if ( pbToWrite == m_pbLGBufMax )
			{
			pbToWrite = m_pbLGBufMin;
			}

		// write out rest of the full sectors, if any
		if ( 0 < csecToWrite )
			{
			// start of write has to be in data portion of log file.
			Assert( isecToWrite >= m_csecHeader && isecToWrite < ( m_csecLGFile - 1 ) );
			// make sure we don't write past end
			Assert( isecToWrite + csecToWrite <= ( m_csecLGFile - 1 ) );

			// If the end of the block to write goes past the end of the
			// first mapping of the log buffer, this is considered using
			// the VM wraparound trick.
			if ( pbToWrite + ( m_cbSec * csecToWrite ) > m_pbLGBufMax )
				{
				m_cLGWrapAround++;
				}

#ifdef LOGPATCH_UNIT_TEST

			CallJ( ErrTestLogPatchIFlush(	m_pfapiLog,
											QWORD( isecToWrite ) * m_cbSec,
											m_cbSec * csecToWrite, 
											pbToWrite ), LHandleWrite1Error );

#else	//	!LOGPATCH_UNIT_TEST

			CallJ( m_pfapiLog->ErrIOWrite(	QWORD( isecToWrite ) * m_cbSec,
											m_cbSec * csecToWrite, 
											pbToWrite ), LHandleWrite1Error );

#endif	//	LOGPATCH_UNIT_TEST

			cLGWrite.Inc( m_pinst );
			cLGBytesWritten.Add( m_pinst, m_cbSec * csecToWrite );
			isecToWrite += csecToWrite;
			pbToWrite += m_cbSec * csecToWrite;
			if ( pbToWrite >= m_pbLGBufMax )
				{
				pbToWrite -= m_cbLGBuf;
				}
			}
		}
	else
		{
		// no shadow, which means the last write ended "perfectly"
		// on a sector boundary, so we can just blast out in 1 I/O

		// start of write has to be in data portion of log file.
		Assert( isecToWrite >= m_csecHeader && isecToWrite < ( m_csecLGFile - 1 ) );
		// make sure we don't write past end
		Assert( isecToWrite + csecToWrite <= ( m_csecLGFile - 1 ) );

		// If the end of the block to write goes past the end of the
		// first mapping of the log buffer, this is considered using
		// the VM wraparound trick.
		if ( pbToWrite + ( m_cbSec * csecToWrite ) > m_pbLGBufMax )
			{
			m_cLGWrapAround++;
			}

#ifdef LOGPATCH_UNIT_TEST

		CallJ( ErrTestLogPatchIFlush(	m_pfapiLog,
										QWORD( isecToWrite ) * m_cbSec,
										m_cbSec * csecToWrite, 
										pbToWrite ), LHandleWrite2Error );

#else	//	!LOGPATCH_UNIT_TEST

		CallJ( m_pfapiLog->ErrIOWrite(	QWORD( isecToWrite ) * m_cbSec,
										m_cbSec * csecToWrite, 
										pbToWrite ), LHandleWrite2Error );

#endif	//	LOGPATCH_UNIT_TEST

		cLGWrite.Inc( m_pinst );
		cLGBytesWritten.Add( m_pinst, m_cbSec * csecToWrite );
		isecToWrite += csecToWrite;
		pbToWrite += m_cbSec * csecToWrite;
		if ( pbToWrite >= m_pbLGBufMax )
			{
			pbToWrite -= m_cbLGBuf;
			}
		}

	// There is no shadow sector on the disk for the last chunk of data on disk.
	m_fHaveShadow = fFalse;

	// Free up space in the log buffer since we don't need it 
	// anymore.
	// Once these new log records hit the disk, we should
	// release the clients waiting.

	m_critLGBuf.Enter();	// <====================

	// what we wrote was used space
	Assert( FIsUsedSpace( pbWrite, csecFull * m_cbSec ) );

	// The new on disk LGPOS should be increasing or staying
	// the same (case of a partial sector write, then a really full
	// buffer with a big log record, then a full sector write that
	// doesn't write all of the big log record).
	Assert( CmpLgpos( &m_lgposToFlush, plgposMaxFlushPoint ) <= 0 );
	// remember the flush point we setup earlier at log adding time
	m_lgposToFlush = lgposToFlushT = *plgposMaxFlushPoint;

#ifdef DEBUG
	LGPOS lgposEndOfWrite;
	// Position right after what we wrote up to.
	GetLgpos( const_cast< BYTE* >( pbToWrite ), &lgposEndOfWrite );
	// The flush point should be less than or equal to the LGPOS of
	// the end of what we physically put to disk.
	Assert( CmpLgpos( &lgposToFlushT, &lgposEndOfWrite ) <= 0 );
#endif
	m_isecWrite = isecToWrite;
	// free space in log buffer
	m_pbWrite = const_cast< BYTE* >( pbToWrite );

	// now freed
	Assert( FIsFreeSpace( pbWrite, csecFull * m_cbSec ) );

	pbFileEndT = m_pbLGFileEnd;

	m_critLGBuf.Leave();	// <====================

	// We want to wake clients before we do other time consuming
	// operations like updating checkpoint file, creating new generation, etc.
	
	(VOID) FWakeWaitingQueue( &lgposToFlushT );

	Assert( pbNil != pbToWrite );
	// If we wrote up to the end of the log file
	if ( pbFileEndT == pbToWrite )
		{
		(VOID) ErrLGUpdateCheckpointFile( pfsapi, fFalse );

		Call( ErrLGNewLogFile( pfsapi, m_plgfilehdr->lgfilehdr.le_lGeneration, fLGOldLogExists ) );
		LGMakeLogName( m_szLogName, m_szJet );
		CallJ( pfsapi->ErrFileOpen( m_szLogName, &m_pfapiLog ), LHandleOpenError );

		m_critLGBuf.Enter();	// <----------------------------

		Assert( m_plgfilehdrT->lgfilehdr.le_lGeneration == m_plgfilehdr->lgfilehdr.le_lGeneration + 1 );
		UtilMemCpy( m_plgfilehdr, m_plgfilehdrT, sizeof( LGFILEHDR ) );
		m_isecWrite = m_csecHeader;
		m_pbLGFileEnd = pbNil;
		m_isecLGFileEnd = 0;

		m_critLGBuf.Leave();	// <----------------------------
		}

	goto HandleError;

LHandleOpenError:
	LGReportError( LOG_FLUSH_OPEN_NEW_FILE_ERROR_ID, err );
	SetFNoMoreLogWrite( err );
	goto HandleError;

LHandleWrite0Error:
	LGReportError( LOG_FLUSH_WRITE_0_ERROR_ID, err );
	SetFNoMoreLogWrite( err );
	goto HandleError;
	
LHandleWrite1Error:
	LGReportError( LOG_FLUSH_WRITE_1_ERROR_ID, err );
	SetFNoMoreLogWrite( err );
	goto HandleError;

LHandleWrite2Error:
	LGReportError( LOG_FLUSH_WRITE_2_ERROR_ID, err );
	SetFNoMoreLogWrite( err );
	goto HandleError;

HandleError:
		{
		// If there was an error, log is down and this will wake
		// everyone, else wake the people we put to disk.
		const BOOL fWaitersExist = FWakeWaitingQueue( &lgposToFlushT );
		if ( pfWaitersExist )
			{
			*pfWaitersExist = fWaitersExist;
			}
		}

	return err;
	}

// Write the last sector in the log buffer which happens to
// be not completely full.

ERR LOG::ErrLGIWritePartialSector(
	// Pointer to the end of real data in this last sector
	// of the log buffer
	BYTE*	pbFlushEnd,
	UINT	isecWrite,
	BYTE*	pbWrite
	)
	{
	ERR		err = JET_errSuccess;
	// We don't grab m_lgposToFlush since that's protected
	// by a critical section we don't want to grab.
	LGPOS	lgposToFlushT = lgposMin;

	// data writes must be past the header of the log file.
	Assert( isecWrite >= m_csecHeader );
	// The real data sector can be at most the 2nd to last sector
	// in the log file.
	Assert( isecWrite < m_csecLGFile - 1 );

	Assert( m_cbSecVolume != ~(ULONG)0 );
	Assert( m_cbSecVolume % 512 == 0 );
	Assert( m_cbSec == m_cbSecVolume );


#ifdef LOGPATCH_UNIT_TEST

	if ( g_fEnableFlushCheck )
		{
		AssertSzRTL( g_fFlushIsPartial, "expected a full flush -- got a partial flush!" );
		}

	CallJ( ErrTestLogPatchIFlush(	m_pfapiLog,
									QWORD( isecWrite ) * m_cbSec,
									m_cbSec * 1, 
									pbWrite ), LHandleWrite3Error );

#else	//	!LOGPATCH_UNIT_TEST

	// write real data
	CallJ( m_pfapiLog->ErrIOWrite(	QWORD( isecWrite ) * m_cbSec,
									m_cbSec * 1, 
									pbWrite ), LHandleWrite3Error );

#endif	//	LOGPATCH_UNIT_TEST

	cLGWrite.Inc( m_pinst );
	cLGBytesWritten.Add( m_pinst, m_cbSec );

		{
		// The shadow should have a different checksum from regular
		// log data sectors in the log file, so we can differentiate
		// the shadow, so we don't accidentally interpret the shadow
		// as regular log data.

		// We do not acquire a critical section, because we're
		// already in m_critLGFlush which is the only place from
		// which m_pbLastChecksum is modified. In addition, the log
		// buffer is guaranteed not to be reallocated during m_critLGFlush.

		Assert( m_critLGFlush.FOwner() );

		// pbWrite better be sector aligned
		Assert( PbSecAligned( pbWrite ) == pbWrite );
		// the last LRCHECKSUM record should be on the shadow sector
		Assert( PbSecAligned( m_pbLastChecksum ) == pbWrite );
		
		LRCHECKSUM* const plrck = reinterpret_cast< LRCHECKSUM* >( m_pbLastChecksum );

		// We modify the checksum by adding a special shadow sector checksum.
		plrck->le_ulChecksum = UlComputeShadowChecksum( plrck->le_ulChecksum );
		}

	// The shadow can be at most the last sector in the log file.
	Assert( isecWrite + 1 < m_csecLGFile );

#ifdef LOGPATCH_UNIT_TEST

	CallJ( ErrTestLogPatchIFlush(	m_pfapiLog,
									QWORD( isecWrite + 1 ) * m_cbSec,
									m_cbSec * 1, 
									pbWrite ), LHandleWrite4Error );

#else	//	!LOGPATCH_UNIT_TEST

	// write shadow sector
	CallJ( m_pfapiLog->ErrIOWrite(	QWORD( isecWrite + 1 ) * m_cbSec,
									m_cbSec * 1, 
									pbWrite ), LHandleWrite4Error );

#endif	//	LOGPATCH_UNIT_TEST

	cLGWrite.Inc( m_pinst );
	cLGBytesWritten.Add( m_pinst, m_cbSec );

	Assert( m_critLGFlush.FOwner() );
	// Flag for ErrLGIWriteFullSectors() to split up a big I/O
	// into 2 I/Os to prevent from overwriting shadow and then killing
	// the real last data sector (because order of sector updating is
	// not guaranteed when writing multiple sectors to disk).
	m_fHaveShadow = fTrue;

	// Free up buffer space
	m_critLGBuf.Enter();	// <====================

	Assert( pbWrite < pbFlushEnd );
	// make sure we wrote stuff that was valid in the log buffer.
	// We are not going to write garbage!!
	Assert( FIsUsedSpace( pbWrite, ULONG( pbFlushEnd - pbWrite ) ) );

	// flush end is end of real data in log buffer.
	GetLgpos( pbFlushEnd, &lgposToFlushT );
	m_lgposToFlush = lgposToFlushT;
	// m_pbWrite and m_isecWrite are already setup fine.
	// m_pbWrite is still pointing to the beginning of this
	// partial sector which will need to be written again
	// once it fills up.
	// m_isecWrite is still pointing to this sector on disk
	// and that makes sense.

	m_critLGBuf.Leave();	// <====================

	goto HandleError;

LHandleWrite3Error:
	LGReportError( LOG_FLUSH_WRITE_3_ERROR_ID, err );
	SetFNoMoreLogWrite( err );
	goto HandleError;

LHandleWrite4Error:
	LGReportError( LOG_FLUSH_WRITE_4_ERROR_ID, err );
	SetFNoMoreLogWrite( err );
	goto HandleError;

HandleError:

	// We don't care if anyone is waiting, since they can
	// ask for a flush, themselves.
	// Wake anyone we flushed to disk.
	(VOID) FWakeWaitingQueue( &lgposToFlushT );

	return err;
	}

// Called to write out more left over log buffer data after doing a
// full sector write. The hope was that while the full sector write
// was proceeding, more log records were added and we now have another
// full sector write ready to go. If no more stuff was added, just
// finish writing whatever's left in the log buffer.

ERR LOG::ErrLGIDeferredFlush( IFileSystemAPI *const pfsapi, const BOOL fFlushAll )
	{
	ERR		err = JET_errSuccess;
	BYTE*	pbEndOfData = pbNil;
	UINT	isecWrite;
	BYTE*	pbWrite = pbNil;
	BYTE*	pbFirstChecksum = pbNil;
	LGPOS	lgposMaxFlushPoint = lgposMax;
		
	m_critLGBuf.Enter();

	// m_pbLastChecksum should be on the next sector we're going to write out
	// (since it could not be written out last time, unless on a partial).
	Assert( PbSecAligned( m_pbWrite ) == PbSecAligned( m_pbLastChecksum ) );

	// Remember where first LRCHECKSUM record in the log buffer is, so we
	// can pass it to ErrLGIWriteFullSectors() in case of short checksum fixup.
	pbFirstChecksum = m_pbLastChecksum;

	if ( pbNil == m_pbLGFileEnd )
		{
		// append a new checksum if necessary, get a pointer to
		// the end of real log data.
		pbEndOfData = PbAppendNewChecksum( fFlushAll );
		// need max flush point for possible full sector write coming up
		lgposMaxFlushPoint = m_lgposMaxFlushPoint;

		if ( fFlushAll && pbEndOfData != m_pbEntry )
			{
			Assert( pbEndOfData == m_pbLastChecksum );
			Assert( FIsUsedSpace( pbEndOfData, sizeof( LRCHECKSUM ) ) );
			Assert( pbEndOfData + sizeof( LRCHECKSUM ) == m_pbEntry );

			//	BUG X5:83888 
			//
			//		we create a torn-write after a clean shutdown because we don't flush the last LRCK record
			//		(we weren't seeing it because pbEndOfData was pointing AT the LRCK instead of PAST it)
			//
			//		move the end of data past the last LRCHECKSUM record to REAL end of data

			pbEndOfData += sizeof( LRCHECKSUM );
			}
		}
	else
		{
		pbEndOfData = m_pbLGFileEnd;
		Assert( PbSecAligned( pbEndOfData ) == pbEndOfData );
		// max flush point for end of file
		GetLgpos( pbEndOfData, &lgposMaxFlushPoint );
		SetupLastFileChecksum( m_pbLastChecksum, pbEndOfData );
		// m_pbLastChecksum is also setup in ErrLGNewLogFile().
		m_pbLastChecksum = pbEndOfData;
		}
		
	// Get current values.
	isecWrite = m_isecWrite;
	pbWrite = m_pbWrite;
	
	UINT csecFull;
	BOOL fPartialSector;
	
	if ( pbEndOfData < pbWrite )
		{
		csecFull = ULONG( ( pbEndOfData + m_cbLGBuf ) - pbWrite ) / m_cbSec;
		}
	else
		{
		csecFull = ULONG( pbEndOfData - pbWrite ) / m_cbSec;
		}

	fPartialSector = ( 0 !=
		( ( pbEndOfData - PbSecAligned( pbEndOfData ) ) % m_cbSec ) );

#ifdef DEBUG
	// Follow same algorithm as below, but just make sure space is used properly
	if ( 0 == csecFull )
		{
		Assert( fPartialSector );
		Assert( pbEndOfData > pbWrite );
		Assert( FIsUsedSpace( pbWrite, ULONG( pbEndOfData - pbWrite ) ) );
		}
	else
		{
		Assert( FIsUsedSpace( pbWrite, csecFull * m_cbSec ) );
		}
#endif

	m_critLGBuf.Leave();

	if ( 0 == csecFull )
		{
		Assert( fPartialSector );
		Call( ErrLGIWritePartialSector( pbEndOfData, isecWrite, pbWrite ) );
		}
	else
		{
		// we don't care if anyone is waiting, since they'll ask for
		// a flush themselves, since this means they got into the buffer
		// during our I/O.
		Call( ErrLGIWriteFullSectors( pfsapi, csecFull, isecWrite, pbWrite, NULL,
			pbFirstChecksum, &lgposMaxFlushPoint ) );
		}

HandleError:

	return err;
	}

// Returns a pointer to the end of real data in the log buffer.
// This may *NOT* append a new LRCHECKSUM record if it's not needed
// (i.e. the end of the log buffer is on the same sector as the last
// LRCHECKSUM in the log buffer).

BYTE* LOG::PbAppendNewChecksum( const BOOL fFlushAll )
	{
	Assert( m_critLGBuf.FOwner() );

	BYTE* pbEndOfData = pbNil;
	const BOOL fEntryOnSameSector = PbSecAligned( PbGetEndOfLogData() ) == PbSecAligned( m_pbLastChecksum );

	// If m_pbEntry is on the same sector as the last LRCHECKSUM record
	// in the log buffer, we do not need to add a new LRCHECKSUM because
	// we're not done with this sector anyway (clients could add more log
	// records on this sector).
	if ( !fEntryOnSameSector )
		{
		// add a new LRCHECKSUM record
		// start of where we can place lrtypNOP
		BYTE* const pbStartNOP = m_pbEntry;

		//	BUG X5:83888 
		//
		//		prevent an LRCHECKSUM record from using up the rest of the sector exactly
		//		the reasoning for this is tricky to follow:
		//			when fFluahAll=TRUE, we force the user to flush ALL log records
		//			this includes the LRCHECKSUM at the end of the log that is normally invisible thanks
		//				to the messed up special-case code in PbGetEndOfLogData()
		//			so, look at the code below for PbGetSectorContiguous()
		//			in the case where we see that we have exactly enough bytes for an LRCHECKSUM record,
		//				we will not do a NOP-fill -- we will use those bytes and effectively move the
		//				end of data to the sector boundary
		//			when we return to ErrLGFlushLog(), we will see this new end of data (the sector boundary)
		//				and decide that we have N full sectors to flush
		//			we will call ErrLGIWriteFullSectors()
		//			assume no more log records come in while we are flushing (e.g. m_pbEntry still points at
		//				at the beginning of the sector right after the last LRCHECKSUM record)
		//			at the end of ErrLGIWriteFullSectors, we advance m_pbWrite by the number of full sectors
		//				flushed (we can't go back and change them -- they aren't shadowed)
		//
		//			WE HAVE JUST CAUSED OURSELVES LOTS OF PROBLEMS! we wrote the LRCHECKSUM record without
		//				updating its ptrs and we cannot update it!
		//
		//			THE FIX -- during an fFlushAll case, make sure we don't end a sector with an LRCK record!

		pbEndOfData = PbGetSectorContiguous( pbStartNOP, sizeof( LRCHECKSUM ) + ( fFlushAll ? 1 : 0 ) );
		int cbNOP = (int)(( pbEndOfData < pbStartNOP ) ?
			pbEndOfData + m_cbLGBuf - pbStartNOP :
			pbEndOfData - pbStartNOP);
		// NOP space must be free
		Assert( FIsFreeSpace( pbStartNOP, cbNOP ) );
		memset( pbStartNOP, lrtypNOP, cbNOP );
		if ( !COSMemoryMap::FCanMultiMap() )
			{
			memset( pbStartNOP + m_cbLGBuf, lrtypNOP, min( cbNOP, m_pbLGBufMax - pbStartNOP ) );
			if ( pbStartNOP + cbNOP > m_pbLGBufMax )
				memset( m_pbLGBufMin, lrtypNOP, pbStartNOP + cbNOP - m_pbLGBufMax );
			}
		
		if ( PbSecAligned( pbEndOfData ) == pbEndOfData )
			{
			// We either already were pointing to the beginning of a
			// sector, or we just NOPed out some space.
			// If we were pointing to the beginning of a sector
			// then the max flush point is already this value.
			// Otherwise, we'll be increasing the max flush point
			// to the right thing.
#ifdef DEBUG
			LGPOS lgposOldMaxFlushPoint = m_lgposMaxFlushPoint;
#endif
			GetLgpos( pbEndOfData, &m_lgposMaxFlushPoint );
			Assert( CmpLgpos( &lgposOldMaxFlushPoint, &m_lgposMaxFlushPoint ) <= 0 );
			}

		// record NOP-space as used data.
		m_pbEntry = pbEndOfData;

		//	the NOPed space should now be used
		Assert( cbNOP == 0 || FIsUsedSpace( pbStartNOP, cbNOP ) );
		//	we should also have enough room for a new LRCHECKSUM record
		Assert( FIsFreeSpace( pbEndOfData, sizeof( LRCHECKSUM ) ) );

		SetupLastChecksum( m_pbLastChecksum, pbEndOfData );

		// append new LRCHECKSUM in log buffer
		m_pbEntry = PbSetupNewChecksum( pbEndOfData, m_pbLastChecksum );
		// m_pbEntry is now pointing to after the new LRCHECKSUM added.

		//	the checksum record space is now used
		Assert( FIsUsedSpace( pbEndOfData, sizeof( LRCHECKSUM ) ) );

		// remember the new one as the last checksum
		m_pbLastChecksum = pbEndOfData;

		}
	else
		{
		// may be pointing to after the last bit of data,
		// or may be pointing to after the last LRCHECKSUM on the sector
		// (and in the log buffer).
		pbEndOfData = m_pbEntry;
		SetupLastChecksum( m_pbLastChecksum, pbEndOfData );
		// return the end of real log data, so we don't
		// return a pointer to the end of the last LRCHECKSUM.
		pbEndOfData = PbGetEndOfLogData();
		}

	// either pointing to the LRCHECKSUM that was just added, or
	// whatever's at the end of m_pbEntry.
	return pbEndOfData;
	}

// Return the end of actual log data in the log buffer, excluding
// an end LRCHECKSUM record, since that's not real log data that the
// user wants to disk.

BYTE* LOG::PbGetEndOfLogData()
	{
	Assert( FIsUsedSpace( m_pbLastChecksum, sizeof( LRCHECKSUM ) ) );

	BYTE* pbEndOfLastLR = m_pbLastChecksum + sizeof( LRCHECKSUM );
	
	Assert( pbEndOfLastLR > m_pbLGBufMin && pbEndOfLastLR <= m_pbLGBufMax );

	if ( pbEndOfLastLR == m_pbLGBufMax )
		pbEndOfLastLR = m_pbLGBufMin;

	if ( pbEndOfLastLR == m_pbEntry )
		{
		// nothing has been added past the last LRCHECKSUM, so the
		// end of real log data is at the start of the LRCHECKSUM.
		return m_pbLastChecksum;
		}
	else
		{
		// there is data past the last LRCHECKSUM (perhaps on the same
		// sector, or maybe on more sectors).

		return m_pbEntry;
		}
	}

// Flush has been requested. Flush some of the data that we have in 
// the log buffer. This is *NOT* guaranteed to flush everything in the
// log buffer. We'll only flush everything if it goes right up to
// a sector boundary, or if the entire log buffer is being waited on
// (ppib->lgposCommit0).

ERR LOG::ErrLGFlushLog( IFileSystemAPI *const pfsapi, const BOOL fFlushAll )
	{
	ENTERCRITICALSECTION enter( &m_critLGFlush );

	return ErrLGIFlushLog( pfsapi, fFlushAll );
	}

ERR LOG::ErrLGIFlushLog( IFileSystemAPI *const pfsapi, const BOOL fFlushAll )
	{
	ERR		err;
	BOOL	fPartialSector;
	UINT	isecWrite;
	BYTE*	pbWrite;
	UINT	csecFull;
	BYTE*	pbEndOfData;
	BYTE*	pbFirstChecksum;
	LGPOS	lgposMaxFlushPoint;
	BOOL	fNewGeneration;

Repeat:
	fNewGeneration	= fFalse;
	err				= JET_errSuccess;
	fPartialSector	= fFalse;
	pbWrite			= pbNil;
	csecFull		= 0;
	pbEndOfData		= pbNil;
	pbFirstChecksum	= pbNil;
	// Set this to max so we'll Assert() or hang elsewhere if this
	// is used without really being initialized.
	lgposMaxFlushPoint = lgposMax;
	
	m_critLGBuf.Enter();	// <===================================


	if ( m_fLGNoMoreLogWrite )
		{
		m_critLGBuf.Leave();
		Call( ErrERRCheck( JET_errLogWriteFail ) );
		}

	if ( !m_pfapiLog )
		{
		m_critLGBuf.Leave();
		err = JET_errSuccess;
		goto HandleError;
		}

	// XXX
	// gross temp hack to prevent trying to flush during ErrLGSoftStart
	// and recovery redo time.
	if ( ! m_fNewLogRecordAdded )
		{
		m_critLGBuf.Leave();
		err = JET_errSuccess;
		goto HandleError;
		}

	// m_pbLastChecksum should be on the next sector we're going to write out
	// (since it could not be written out last time, unless on a partial).
	Assert( PbSecAligned( m_pbWrite ) == PbSecAligned( m_pbLastChecksum ) );

	// Remember where this checksum record is, before we add a new checksum
	// record, so we can use it to tell ErrLGIWriteFullSectors() which
	// LRCHECKSUM to "fix up" in the special 2 I/O case.
	
	pbFirstChecksum = m_pbLastChecksum;

	if ( pbNil != m_pbLGFileEnd )
		{
		fNewGeneration = fTrue;
		pbEndOfData = m_pbLGFileEnd;
		Assert( PbSecAligned( pbEndOfData ) == pbEndOfData );

		// set flush point for full sector write coming up
		GetLgpos( pbEndOfData, &lgposMaxFlushPoint );

		SetupLastFileChecksum( m_pbLastChecksum, pbEndOfData );
		// m_pbLastChecksum is also set in ErrLGNewLogFile().
		m_pbLastChecksum = pbEndOfData;
		}
	else
		{
		LGPOS	lgposEndOfData = lgposMin;
		
		// Figure out the end of real log data.
		pbEndOfData = PbGetEndOfLogData();

		if ( fFlushAll && pbEndOfData != m_pbEntry )
			{
			Assert( pbEndOfData == m_pbLastChecksum );
			Assert( FIsUsedSpace( pbEndOfData, sizeof( LRCHECKSUM ) ) );
			Assert( pbEndOfData + sizeof( LRCHECKSUM ) == m_pbEntry );

			//	BUG X5:83888 
			//
			//		we create a torn-write after a clean shutdown because we don't flush the last LRCK record
			//		(we weren't seeing it because pbEndOfData was pointing AT the LRCK instead of PAST it)
			//
			//		move the end of data past the last LRCHECKSUM record to REAL end of data

			pbEndOfData += sizeof( LRCHECKSUM );
			}

		GetLgpos( pbEndOfData, &lgposEndOfData );

		// If all real data in the log buffer has been flushed,
		// we're done.
		if ( CmpLgpos( &lgposEndOfData, &m_lgposToFlush ) <= 0 )
			{
			m_critLGBuf.Leave();
			err = JET_errSuccess;
			goto HandleError;		
			}

		// Add an LRCHECKSUM if necessary (if we're on a new
		// sector).
		pbEndOfData = PbAppendNewChecksum( fFlushAll );

		lgposMaxFlushPoint = m_lgposMaxFlushPoint;	

		if ( fFlushAll && pbEndOfData != m_pbEntry )
			{
			Assert( pbEndOfData == m_pbLastChecksum );
			Assert( FIsUsedSpace( pbEndOfData, sizeof( LRCHECKSUM ) ) );
			Assert( pbEndOfData + sizeof( LRCHECKSUM ) == m_pbEntry );

			//	BUG X5:83888 
			//
			//		we create a torn-write after a clean shutdown because we don't flush the last LRCK record
			//		(we weren't seeing it because pbEndOfData was pointing AT the LRCK instead of PAST it)
			//
			//		move the end of data past the last LRCHECKSUM record to REAL end of data

			pbEndOfData += sizeof( LRCHECKSUM );
			}
		}

	// Get current values.
	isecWrite = m_isecWrite;
	pbWrite = m_pbWrite;
	
	if ( pbEndOfData < pbWrite )
		{
		csecFull = ULONG( ( pbEndOfData + m_cbLGBuf ) - pbWrite ) / m_cbSec;
		}
	else
		{
		csecFull = ULONG( pbEndOfData - pbWrite ) / m_cbSec;
		}

	fPartialSector = ( 0 !=
		( ( pbEndOfData - PbSecAligned( pbEndOfData ) ) % m_cbSec ) );

	Assert( csecFull + ( fPartialSector ? 1 : 0 ) <= m_csecLGBuf );

#ifdef DEBUG
		{
		if ( pbNil != m_pbLGFileEnd )
			{
			Assert( csecFull > 0 );
			Assert( ! fPartialSector );
			}
		}
#endif

	// The next time someone grabs the buffer, they'll see our new
	// LRCHECKSUM record and any NOPs. We've only "extended" the used
	// area of the log buffer in these changes.
	// Summary of atomic changes:
	// m_pbEntry <-- points after new LRCHECKSUM
	// m_pbLastChecksum <-- points to the new one we added

#ifdef DEBUG
	// Follow same algorithm as below to do assertions inside of crit. section

	if ( 0 == csecFull )
		{
		Assert( fPartialSector );
		Assert( pbEndOfData > pbWrite );
		Assert( FIsUsedSpace( pbWrite, ULONG( pbEndOfData - pbWrite ) ) );
		}
	else
		{
		Assert( FIsUsedSpace( pbWrite, csecFull * m_cbSec ) );
		}
#endif

	m_critLGBuf.Leave();

	if ( 0 == csecFull )
		{
		// No full sectors, so just write the partial and be done
		// with it.
		Assert( fPartialSector );
		Call( ErrLGIWritePartialSector( pbEndOfData, isecWrite, pbWrite ) );
		}
	else
		{
		// Some full sectors to write once, so blast them out
		// now and hope that someone adds log records while we're writing.
		BOOL fWaitersExist = fFalse;
		Call( ErrLGIWriteFullSectors( pfsapi, csecFull, isecWrite, pbWrite,
			&fWaitersExist, pbFirstChecksum, &lgposMaxFlushPoint ) );

		//	HACK (IVANTRIN): we assume that we can remove following fPartial sector verification
		//	but Adam is not sure. So when we create new generation and there are waiters
		//	left repeat the flush check.
		if ( fWaitersExist && fNewGeneration )
			{
			goto Repeat;
			}

		// Only write some more if we have some more, and if
		// there are people waiting for it to be flushed.
		if ( fPartialSector && fWaitersExist )
			{
			// If there was previously a partial sector in the log buffer
			// the user probably wanted the log recs to disk, so do the
			// smart deferred flush.
			Call( ErrLGIDeferredFlush( pfsapi, fFlushAll ) );
			}
		}


HandleError:

	// should not need to wake anyone up -- should be
	// handled by the other functions.

	return err;
	}

/********************* CHECKPOINT **************************
/***********************************************************
/**/

#ifdef UNLIMITED_DB
LOCAL ERR LOG::ErrLGLoadDbListFromFMP_()
	{
	ATTACHINFO*		pattachinfo		= (ATTACHINFO*)( m_pbLGDbListBuffer + sizeof(LRDBLIST) );

	//	during Redo phase of recovery, must compute attachment list
	//	using atchchk
	//	all other times, the DbList buffer should be dynamically updated
	//	as attach/detach operations occur
	Assert( m_fRecovering );
	Assert( fRecoveringRedo == m_fRecoveringMode );

	FMP::EnterCritFMPPool();

	m_cbLGDbListInUse = sizeof(LRDBLIST);
	m_cLGAttachments = 0;

	for ( DBID dbidT = dbidUserLeast; dbidT < dbidMax; dbidT++ )
		{
		Assert( dbidTemp != dbidT );
		const IFMP	ifmp			= m_pinst->m_mpdbidifmp[ dbidT ];

		if ( ifmp >= ifmpMax )
			continue;

		FMP*		pfmpT			= &rgfmp[ ifmp ];
		ATCHCHK*	patchchk		= pfmpT->Patchchk();
		Assert( NULL != patchchk );
		Assert( pfmpT->FLogOn() );

		CHAR* 		szDatabaseName		= pfmpT->SzDatabaseName();
		CHAR* 		szSLVName			= pfmpT->SzSLVName();
		CHAR*		szSLVRoot			= pfmpT->SzSLVRoot();
		INT			irstmap;

		if ( m_fAlternateDbDirDuringRecovery )
			{
			//	HACK: original database name hangs off the end
			//	of the relocated database name
			szDatabaseName += strlen( szDatabaseName ) + 1;
			}
		else
			{
			irstmap = IrstmapSearchNewName( szDatabaseName );
			if ( irstmap >= 0 )
				{
				szDatabaseName = m_rgrstmap[irstmap].szDatabaseName;
				Assert( NULL != szDatabaseName );
				}
			}

		if ( NULL != szSLVName )
			{
			if ( m_fAlternateDbDirDuringRecovery )
				{
				//	HACK: original SLV name hangs off the end
				//	of the relocated SLV name
				szSLVName += strlen( szSLVName ) + 1;
				}
			else
				{
				irstmap = IrstmapSearchNewName( szSLVName );
				if ( irstmap >= 0 )
					{
					szSLVName = m_rgrstmap[irstmap].szDatabaseName;
					Assert( NULL != szSLVName );
					}
				}
			}


		ULONG			cbNames;
		const ULONG		cbDbName		= (ULONG)strlen( szDatabaseName ) + 1;
		const ULONG		cbSLVName		= ( NULL != szSLVName ? (ULONG)strlen( szSLVName ) + 1 : 0 );
		const ULONG		cbSLVRoot		= ( NULL != szSLVRoot ? (ULONG)strlen( szSLVRoot ) + 1 : 0 );
		const ULONG		cbRequired		= sizeof(ATTACHINFO)
												+ cbDbName
												+ cbSLVName
												+ cbSLVRoot;

		if ( m_cbLGDbListInUse + cbRequired + 1 > m_cbLGDbListBuffer )	//	+1 for sentinel
			{
			pfmpT->RwlDetaching().LeaveAsReader();
			FMP::LeaveCritFMPPool();

			ERR		err;
			CallR( ErrLGResizeDbListBuffer_( fFalse ) );

			//	on success, retry from the beginning
			FMP::EnterCritFMPPool();
			pattachinfo = (ATTACHINFO*)m_pbLGDbListBuffer;
			dbidT = 0;
			continue;
			}
			
		memset( pattachinfo, 0, sizeof(ATTACHINFO) );
			
		Assert( !pfmpT->FVersioningOff() );
		Assert( !pfmpT->FReadOnlyAttach() );
		Assert( !pattachinfo->FSLVExists() );
		Assert( !pattachinfo->FSLVProviderNotEnabled() );

		pattachinfo->SetDbid( dbidT );

		Assert( pfmpT->FLogOn() );

		if( !m_pinst->FSLVProviderEnabled() )
			{
			pattachinfo->SetFSLVProviderNotEnabled();
			}

		Assert( !pfmpT->FReadOnlyAttach() );

		if ( NULL != pfmpT->SzSLVName() )
			{
			//	SLV must always have a root
			Assert( NULL != pfmpT->SzSLVRoot() );
			pattachinfo->SetFSLVExists();
			}

		pattachinfo->SetDbtime( pfmpT->Patchchk()->Dbtime() );
		pattachinfo->SetObjidLast( pfmpT->Patchchk()->ObjidLast() );
		pattachinfo->SetCpgDatabaseSizeMax( pfmpT->Patchchk()->CpgDatabaseSizeMax() );
		pattachinfo->le_lgposAttach = pfmpT->Patchchk()->lgposAttach;
		pattachinfo->le_lgposConsistent = pfmpT->Patchchk()->lgposConsistent;
		UtilMemCpy( &pattachinfo->signDb, &pfmpT->Patchchk()->signDb, sizeof( SIGNATURE ) );
			
			
		strcpy( pattachinfo->szNames, szDatabaseName );
		cbNames = cbDbName;

		if ( pattachinfo->FSLVExists() )
			{
			Assert( NULL != szSLVName );
			strcpy( pattachinfo->szNames + cbNames, szSLVName );
			cbNames += cbSLVName;

			Assert( NULL != pfmpT->SzSLVRoot() );
			strcpy( pattachinfo->szNames + cbNames, pfmpT->SzSLVRoot() );
			cbNames += cbSLVRoot;
			}

		Assert( cbRequired == sizeof(ATTACHINFO) + cbNames );
		pattachinfo->SetCbNames( (USHORT)cbNames );

		//	advance to next attachinfo
		m_cLGAttachments++;
		m_cbLGDbListInUse += cbRequired;
		pattachinfo = (ATTACHINFO*)( (BYTE *)pattachinfo + cbRequired );
		Assert( (BYTE *)pattachinfo - m_pbLGDbListBuffer == m_cbLGDbListInUse );

		//	verify we don't overrun buffer (ensure we have enough room for sentinel)
		Assert( m_cbLGDbListInUse < m_cbLGDbListBuffer );
		}

	//	put a sentinal
	*(BYTE *)pattachinfo = 0;
	m_cbLGDbListInUse++;

	//	must always have at least an LRDBLIST and a sentinel
	Assert( m_cbLGDbListInUse > sizeof(LRDBLIST) );
	Assert( m_cbLGDbListInUse <= m_cbLGDbListBuffer );

	LRDBLIST*	const plrdblist		= (LRDBLIST *)m_pbLGDbListBuffer;
	plrdblist->lrtyp = lrtypDbList;
	plrdblist->ResetFlags();
	plrdblist->SetCAttachments( m_cLGAttachments );
	plrdblist->SetCbAttachInfo( m_cbLGDbListInUse - sizeof(LRDBLIST) );

	FMP::LeaveCritFMPPool();

	return JET_errSuccess;
	}


//	ensure there is enough space to add specified dbid to the db list
LOCAL ERR LOG::ErrLGPreallocateDbList_( const DBID dbid )
	{
	const IFMP		ifmp			= m_pinst->m_mpdbidifmp[ dbid ];
	FMP*			pfmpT			= &rgfmp[ ifmp ];

	Enforce( ifmp < ifmpMax );

	Assert( m_critLGBuf.FOwner() );
	Assert( !m_fRecovering );
	Assert( pfmpT->FLogOn() );
	Assert( NULL != pfmpT->Pdbfilehdr() );
	Assert( 0 == CmpLgpos( lgposMin, pfmpT->LgposAttach() ) );	//	pre-allocating list before attach is actually logged
	Assert( 0 == CmpLgpos( lgposMin, pfmpT->LgposDetach() ) );

	CHAR* 			szDatabaseName	= pfmpT->SzDatabaseName();
	CHAR* 			szSLVName		= pfmpT->SzSLVName();
	CHAR*			szSLVRoot		= pfmpT->SzSLVRoot();

	Assert( NULL != szDatabaseName );
	Assert( NULL == szSLVName || NULL != szSLVRoot );

	const ULONG		cbDbName		= (ULONG)strlen( szDatabaseName ) + 1;
	const ULONG		cbSLVName		= ( NULL != szSLVName ? (ULONG)strlen( szSLVName ) + 1 : 0 );
	const ULONG		cbSLVRoot		= ( NULL != szSLVRoot ? (ULONG)strlen( szSLVRoot ) + 1 : 0 );
	const ULONG		cbRequired		= sizeof(ATTACHINFO)
										+ cbDbName
										+ cbSLVName
										+ cbSLVRoot;

	if ( m_cbLGDbListInUse + cbRequired > m_cbLGDbListBuffer )
		{
		ERR		err;
		CallR( ErrLGResizeDbListBuffer_( fTrue ) );
		}

	return JET_errSuccess;
	}

LOCAL VOID LOG::LGAddToDbList_( const DBID dbid )
	{
	const IFMP		ifmp			= m_pinst->m_mpdbidifmp[ dbid ];
	FMP*			pfmpT			= &rgfmp[ ifmp ];

	Enforce( ifmp < ifmpMax );

	Assert( m_critLGBuf.FOwner() );
	Assert( !m_fRecovering );
	Assert( pfmpT->FLogOn() );
	Assert( NULL != pfmpT->Pdbfilehdr() );
	Assert( 0 != CmpLgpos( lgposMin, pfmpT->LgposAttach() ) );
	Assert( 0 == CmpLgpos( lgposMin, pfmpT->LgposDetach() ) );

	CHAR* 			szDatabaseName	= pfmpT->SzDatabaseName();
	CHAR* 			szSLVName		= pfmpT->SzSLVName();
	CHAR*			szSLVRoot		= pfmpT->SzSLVRoot();

	Assert( NULL != szDatabaseName );
	Assert( NULL == szSLVName || NULL != szSLVRoot );

	ULONG			cbNames;
	const ULONG		cbDbName		= (ULONG)strlen( szDatabaseName ) + 1;
	const ULONG		cbSLVName		= ( NULL != szSLVName ? (ULONG)strlen( szSLVName ) + 1 : 0 );
	const ULONG		cbSLVRoot		= ( NULL != szSLVRoot ? (ULONG)strlen( szSLVRoot ) + 1 : 0 );
	const ULONG		cbRequired		= sizeof(ATTACHINFO)
										+ cbDbName
										+ cbSLVName
										+ cbSLVRoot;

	Enforce( m_cbLGDbListInUse + cbRequired <= m_cbLGDbListBuffer );

	BYTE*			pbSentinel			= m_pbLGDbListBuffer + m_cbLGDbListInUse - 1;
	ATTACHINFO*		pattachinfo;

#ifdef DEBUG
	pattachinfo = (ATTACHINFO *)( m_pbLGDbListBuffer + sizeof(LRDBLIST) );
	Assert( (BYTE *)pattachinfo > m_pbLGDbListBuffer );
	Assert( (BYTE *)pattachinfo <= pbSentinel );
	Assert( 0 == *pbSentinel );
	Assert( pbSentinel + cbRequired < m_pbLGDbListBuffer + m_cbLGDbListBuffer );

	while ( 0 != *( (BYTE *)pattachinfo ) )
		{
		Assert( (BYTE *)pattachinfo > m_pbLGDbListBuffer );
		Assert( (BYTE *)pattachinfo < pbSentinel );

		//	assert not already in list
		Assert( dbid != pattachinfo->Dbid() );
		pattachinfo = (ATTACHINFO *)( (BYTE *)pattachinfo + sizeof(ATTACHINFO) + pattachinfo->CbNames() );
		}
#endif

	m_cbLGDbListInUse += cbRequired;
	m_cLGAttachments++;

	LRDBLIST* const		plrdblist	= (LRDBLIST *)m_pbLGDbListBuffer;
	plrdblist->SetCAttachments( plrdblist->CAttachments() + 1 );
	plrdblist->SetCbAttachInfo( plrdblist->CbAttachInfo() + cbRequired );

	Assert( plrdblist->CAttachments() == m_cLGAttachments );
	Assert( plrdblist->CbAttachInfo() + sizeof(LRDBLIST) == m_cbLGDbListInUse );


	//	append new attachment to the end of the list
	pattachinfo = (ATTACHINFO *)pbSentinel;
	Assert( (BYTE *)pattachinfo >= m_pbLGDbListBuffer + sizeof(LRDBLIST) );

	//	move sentinel
	*( pbSentinel + cbRequired ) = 0;

	//	initialise new attachinfo

	memset( pattachinfo, 0, sizeof(ATTACHINFO) );
	
	Assert( !pfmpT->FVersioningOff() );
	Assert( !pfmpT->FReadOnlyAttach() );
	Assert( !pattachinfo->FSLVExists() );
	Assert( !pattachinfo->FSLVProviderNotEnabled() );

	pattachinfo->SetDbid( dbid );

	Assert( pfmpT->FLogOn() );

	if( !m_pinst->FSLVProviderEnabled() )
		{
		pattachinfo->SetFSLVProviderNotEnabled();
		}

	Assert( !pfmpT->FReadOnlyAttach() );

	if ( NULL != pfmpT->SzSLVName() )
		{
		//	SLV must always have a root
		Assert( NULL != pfmpT->SzSLVRoot() );
		pattachinfo->SetFSLVExists();
		}

	pattachinfo->SetDbtime( pfmpT->DbtimeLast() );
	pattachinfo->SetObjidLast( pfmpT->ObjidLast() );
	pattachinfo->SetCpgDatabaseSizeMax( pfmpT->CpgDatabaseSizeMax() );
	pattachinfo->le_lgposAttach = pfmpT->LgposAttach();

	//	 relays DBISetHeaderAfterAttach behavior for resetting lgposConsistent
	if ( 0 == memcmp( &pfmpT->Pdbfilehdr()->signLog, &m_signLog, sizeof(SIGNATURE) ) )
		{
		pattachinfo->le_lgposConsistent = pfmpT->Pdbfilehdr()->le_lgposConsistent;
		}
	else
		{
		pattachinfo->le_lgposConsistent = lgposMin;
		}
	UtilMemCpy( &pattachinfo->signDb, &pfmpT->Pdbfilehdr()->signDb, sizeof( SIGNATURE ) );

	Assert( NULL != szDatabaseName );
	strcpy( pattachinfo->szNames, szDatabaseName );
	cbNames = (ULONG)strlen( szDatabaseName ) + 1;

	if ( pattachinfo->FSLVExists() )
		{
		Assert( NULL != szSLVName );
		strcpy( pattachinfo->szNames + cbNames, szSLVName );
		cbNames += (ULONG) strlen( szSLVName ) + 1;

		Assert( NULL != pfmpT->SzSLVRoot() );
		strcpy( pattachinfo->szNames + cbNames, pfmpT->SzSLVRoot() );
		cbNames += (ULONG) strlen( pfmpT->SzSLVRoot() ) + 1;
		}

	pattachinfo->SetCbNames( (USHORT)cbNames );
	}

LOCAL VOID LOG::LGRemoveFromDbList_( const DBID dbid )
	{
	const IFMP		ifmp			= m_pinst->m_mpdbidifmp[ dbid ];
	FMP*			pfmpT			= &rgfmp[ ifmp ];

	Enforce( ifmp < ifmpMax );

	Assert( m_critLGBuf.FOwner() );
	Assert( !m_fRecovering );
	Assert( pfmpT->FLogOn() );
	Assert( NULL != pfmpT->Pdbfilehdr() );
	Assert( 0 != CmpLgpos( lgposMin, pfmpT->LgposAttach() ) );
	Assert( 0 != CmpLgpos( lgposMin, pfmpT->LgposDetach() ) );
	Assert( m_cLGAttachments > 0 );

	BYTE*			pbSentinel		= m_pbLGDbListBuffer + m_cbLGDbListInUse - 1;
	ATTACHINFO*		pattachinfo		= (ATTACHINFO*)( m_pbLGDbListBuffer + sizeof(LRDBLIST) );

	Assert( (BYTE *)pattachinfo > m_pbLGDbListBuffer );
	Assert( (BYTE *)pattachinfo <= pbSentinel );
	Assert( 0 == *pbSentinel );

	while ( 0 != *( (BYTE *)pattachinfo ) )
		{
		Assert( (BYTE *)pattachinfo > m_pbLGDbListBuffer );
		Assert( (BYTE *)pattachinfo < pbSentinel );

		//	assert not already in list
		if ( dbid == pattachinfo->Dbid() )
			{
			LRDBLIST* const		plrdblist				= (LRDBLIST *)m_pbLGDbListBuffer;
			const ULONG			cbAttachInfoCurr		= sizeof(ATTACHINFO) + pattachinfo->CbNames();
			BYTE*				pbEndOfAttachInfoCurr	= (BYTE *)pattachinfo + cbAttachInfoCurr;
			UtilMemMove(
				pattachinfo,
				pbEndOfAttachInfoCurr,
				pbSentinel - pbEndOfAttachInfoCurr + 1 );	//	+1 for the sentinel

			Assert( m_cbLGDbListInUse > cbAttachInfoCurr );
			m_cbLGDbListInUse -= cbAttachInfoCurr;
			m_cLGAttachments--;

			Assert( plrdblist->CAttachments() > 0 );
			Assert( plrdblist->CbAttachInfo() > cbAttachInfoCurr );
			plrdblist->SetCAttachments( plrdblist->CAttachments() - 1 );
			plrdblist->SetCbAttachInfo( plrdblist->CbAttachInfo() - cbAttachInfoCurr );

			Assert( plrdblist->CAttachments() == m_cLGAttachments );
			Assert( plrdblist->CbAttachInfo() + sizeof(LRDBLIST) == m_cbLGDbListInUse );
			break;
			}

		pattachinfo = (ATTACHINFO *)( (BYTE *)pattachinfo + sizeof(ATTACHINFO) + pattachinfo->CbNames() );

		//	assert dbid must be in the list
		Assert( 0 != *( (BYTE *)pattachinfo ) );
		}

#ifdef DEBUG
	pbSentinel = m_pbLGDbListBuffer + m_cbLGDbListInUse - 1;
	pattachinfo = (ATTACHINFO*)( m_pbLGDbListBuffer + sizeof(LRDBLIST) );
	while ( 0 != *( (BYTE *)pattachinfo ) )
		{
		Assert( (BYTE *)pattachinfo > m_pbLGDbListBuffer );
		Assert( (BYTE *)pattachinfo < pbSentinel );

		//	assert not in list
		Assert( dbid != pattachinfo->Dbid() );
		pattachinfo = (ATTACHINFO *)( (BYTE *)pattachinfo + sizeof(ATTACHINFO) + pattachinfo->CbNames() );
		}
	Assert( (BYTE *)pattachinfo == pbSentinel );
#endif		
	}

LOCAL ERR LOG::ErrLGResizeDbListBuffer_( const BOOL fCopyContents )
	{
	BYTE*		pbOldBuffer	= m_pbLGDbListBuffer;
	ULONG		cbOldBuffer;

	Assert( m_critLGBuf.FOwner() );

	Assert( NULL != pbOldBuffer );
	if ( !fCopyContents )
		{
		//	if not copying contents, release old buffer
		//	up front to permit reuse
		OSMemoryPageFree( pbOldBuffer );
		cbOldBuffer	= m_cbLGDbListInUse;
		}

	m_cbLGDbListInUse = 0;
	m_cLGAttachments = 0;

	//	double the buffer size, up to 64k
	const ULONG		cbDbListBufferGrowMax	= 0x10000;
	if ( m_cbLGDbListBuffer < cbDbListBufferGrowMax )
		{
		m_cbLGDbListBuffer *= 2;
		}
	else
		{
		m_cbLGDbListBuffer += cbDbListBufferGrowMax;
		}
	m_pbLGDbListBuffer = (BYTE *)PvOSMemoryPageAlloc( m_cbLGDbListBuffer, NULL );

	const ERR	errT	= ( NULL != m_pbLGDbListBuffer ? JET_errSuccess : ErrERRCheck( JET_errOutOfMemory ) );

	if ( fCopyContents )
		{
		if ( JET_errSuccess == errT )
			{
			UtilMemCpy( m_pbLGDbListBuffer, pbOldBuffer, cbOldBuffer );
			}

		OSMemoryPageFree( pbOldBuffer );
		}

	if ( errT < 0 )
		{
		//	UNDONE: report log disabled due to out-of-memory updating attachments list
		SetFNoMoreLogWrite( errT );
		}

	return errT;
	}

#else	//	!UNLIMITED_DB

VOID LGLoadAttachmentsFromFMP( INST *pinst, BYTE *pbBuf )
	{
	ATTACHINFO	*pattachinfo	= (ATTACHINFO*)pbBuf;
	
	FMP::EnterCritFMPPool();

	for ( DBID dbidT = dbidUserLeast; dbidT < dbidMax; dbidT++ )
		{
		IFMP ifmp = pinst->m_mpdbidifmp[ dbidT ];

		if ( ifmp >= ifmpMax )
			continue;

		FMP 		*pfmpT			= &rgfmp[ ifmp ];
		ULONG		cbNames;
		const BOOL	fUsePatchchk	= ( pfmpT->Patchchk()
										&& pinst->m_plog->m_fRecovering
										&& pinst->m_plog->m_fRecoveringMode == fRecoveringRedo );

		pfmpT->RwlDetaching().EnterAsReader();
		Assert( pinst->m_plog->m_plgfilehdrT->lgfilehdr.le_lGeneration >= pfmpT->LgposDetach().lGeneration );
		if ( pfmpT->FLogOn() 
			&& ( ( NULL != pfmpT->Pdbfilehdr() 
					&& 0 != CmpLgpos( lgposMin, pfmpT->LgposAttach() )
					&& pinst->m_plog->m_plgfilehdrT->lgfilehdr.le_lGeneration > pfmpT->LgposAttach().lGeneration ) 
					&& ( 0 == CmpLgpos( lgposMin, pfmpT->LgposDetach() )
						|| pinst->m_plog->m_plgfilehdrT->lgfilehdr.le_lGeneration == pfmpT->LgposDetach().lGeneration )
				|| fUsePatchchk ) )
			{

			memset( pattachinfo, 0, sizeof(ATTACHINFO) );
			
			Assert( !pfmpT->FVersioningOff() );
			Assert( !pfmpT->FReadOnlyAttach() );
			Assert( !pattachinfo->FSLVExists() );
			Assert( !pattachinfo->FSLVProviderNotEnabled() );

			pattachinfo->SetDbid( dbidT );

			Assert( pfmpT->FLogOn() );

			if( !pinst->FSLVProviderEnabled() )
				{
				pattachinfo->SetFSLVProviderNotEnabled();
				}

			Assert( !pfmpT->FReadOnlyAttach() );

			if ( NULL != pfmpT->SzSLVName() )
				{
				//	SLV must always have a root
				Assert( NULL != pfmpT->SzSLVRoot() );
				pattachinfo->SetFSLVExists();
				}

			if ( fUsePatchchk )
				{
				pattachinfo->SetDbtime( pfmpT->Patchchk()->Dbtime() );
				pattachinfo->SetObjidLast( pfmpT->Patchchk()->ObjidLast() );
				pattachinfo->SetCpgDatabaseSizeMax( pfmpT->Patchchk()->CpgDatabaseSizeMax() );
				pattachinfo->le_lgposAttach = pfmpT->Patchchk()->lgposAttach;
				pattachinfo->le_lgposConsistent = pfmpT->Patchchk()->lgposConsistent;
				UtilMemCpy( &pattachinfo->signDb, &pfmpT->Patchchk()->signDb, sizeof( SIGNATURE ) );
				}
			else
				{
				pattachinfo->SetDbtime( pfmpT->DbtimeLast() );
				pattachinfo->SetObjidLast( pfmpT->ObjidLast() );
				pattachinfo->SetCpgDatabaseSizeMax( pfmpT->CpgDatabaseSizeMax() );
				pattachinfo->le_lgposAttach = pfmpT->LgposAttach();
				//	 relays DBISetHeaderAfterAttach behavior for resetting lgposConsistent
				if ( 0 == memcmp( &pfmpT->Pdbfilehdr()->signLog, &pinst->m_plog->m_signLog, sizeof(SIGNATURE) ) )
					{
					pattachinfo->le_lgposConsistent = pfmpT->Pdbfilehdr()->le_lgposConsistent;
					}
				else
					{
					pattachinfo->le_lgposConsistent = lgposMin;
					}
				UtilMemCpy( &pattachinfo->signDb, &pfmpT->Pdbfilehdr()->signDb, sizeof( SIGNATURE ) );
				}
			CHAR * szDatabaseName = pfmpT->SzDatabaseName();
			Assert ( szDatabaseName );
			
			if ( pinst->m_plog->m_fRecovering )
				{
				if ( pinst->m_plog->m_fAlternateDbDirDuringRecovery )
					{
					//	HACK: original database name hangs off the end
					//	of the relocated database name
					szDatabaseName += strlen( szDatabaseName ) + 1;
					}
				else
					{
					INT irstmap = pinst->m_plog->IrstmapSearchNewName( szDatabaseName );
					if ( irstmap >= 0 )
						{
						szDatabaseName = pinst->m_plog->m_rgrstmap[irstmap].szDatabaseName;
						}
					}
				}
			Assert( szDatabaseName );

			CHAR * szSLVName = pfmpT->SzSLVName();

			if ( pinst->m_plog->m_fRecovering && szSLVName )
				{
				if ( pinst->m_plog->m_fAlternateDbDirDuringRecovery )
					{
					//	HACK: original SLV name hangs off the end
					//	of the relocated SLV name
					szSLVName += strlen( szSLVName ) + 1;
					}
				else
					{
					INT irstmap = pinst->m_plog->IrstmapSearchNewName( szSLVName );
					if ( irstmap >= 0 )
						{
						szSLVName = pinst->m_plog->m_rgrstmap[irstmap].szDatabaseName;
						}
					Assert( szSLVName );
					}
				}


			strcpy( pattachinfo->szNames, szDatabaseName );
			cbNames = (ULONG) strlen( szDatabaseName ) + 1;

			if ( pattachinfo->FSLVExists() )
				{
				Assert( NULL != szSLVName );
				strcpy( pattachinfo->szNames + cbNames, szSLVName );
				cbNames += (ULONG) strlen( szSLVName ) + 1;

				Assert( NULL != pfmpT->SzSLVRoot() );
				strcpy( pattachinfo->szNames + cbNames, pfmpT->SzSLVRoot() );
				cbNames += (ULONG) strlen( pfmpT->SzSLVRoot() ) + 1;
				}

			pattachinfo->SetCbNames( (USHORT)cbNames );

			//	advance to next attachinfo
			pattachinfo = (ATTACHINFO*)( (BYTE *)pattachinfo + sizeof(ATTACHINFO) + cbNames );

			//	verify we don't overrun buffer (ensure we have enough room for sentinel)
			EnforceSz( pbBuf + cbAttach > (BYTE *)pattachinfo, "Too many databases attached (ATTACHINFO overrun)." );
			}
		pfmpT->RwlDetaching().LeaveAsReader();
		}

	FMP::LeaveCritFMPPool();

	/*	put a sentinal
	/**/
	*(BYTE *)pattachinfo = 0;

	//	UNDONE: next version we will allow it go beyond 4kByte limit
	Assert( pbBuf + cbAttach > (BYTE *)pattachinfo );
	}

#endif	//	UNLIMITED_DB


//	Load attachment information - how and what the db is attached.

ERR ErrLGLoadFMPFromAttachments( INST *pinst, IFileSystemAPI *const pfsapi, BYTE *pbAttach )
	{
	ERR 							err 			= JET_errSuccess;
	const ATTACHINFO				*pattachinfo 	= NULL;
	const BYTE						*pbT;

	for ( pbT = pbAttach; 0 != *pbT; pbT += sizeof(ATTACHINFO) + pattachinfo->CbNames() )
		{
		Assert( pbT - pbAttach < cbAttach );
		pattachinfo = (ATTACHINFO*)pbT;

		CallR ( pinst->m_plog->ErrLGRISetupFMPFromAttach( pfsapi, ppibSurrogate, &pinst->m_plog->m_signLog, pattachinfo) );
		}

	return JET_errSuccess;
	}

VOID LOG::LGFullNameCheckpoint( IFileSystemAPI *const pfsapi, CHAR *szFullName )
	{
	CallS( pfsapi->ErrPathBuild( m_pinst->m_szSystemPath, m_szJet, szChkExt, szFullName ) );
	}

ERR LOG::ErrLGICheckpointInit( IFileSystemAPI *const pfsapi, BOOL *pfGlobalNewCheckpointFile )
	{
	ERR 			err;
	IFileAPI	*pfapiCheckpoint = NULL;
	CHAR			szPathJetChkLog[IFileSystemAPI::cchPathMax];

	*pfGlobalNewCheckpointFile = fFalse;

	Assert( m_pcheckpoint == NULL );
	m_pcheckpoint = (CHECKPOINT *)PvOSMemoryPageAlloc( sizeof(CHECKPOINT), NULL );
	if ( m_pcheckpoint == NULL )
		{
		err = ErrERRCheck( JET_errOutOfMemory );
		goto HandleError;
		}

	//	Initialize. Used by perfmon counters
	m_pcheckpoint->checkpoint.le_lgposCheckpoint = lgposMin;
	LGFullNameCheckpoint( pfsapi, szPathJetChkLog );
 	err = pfsapi->ErrFileOpen( szPathJetChkLog, &pfapiCheckpoint );

	if ( err == JET_errFileNotFound )
		{
		m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration = 1; /* first generation */
		m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_isec = USHORT( sizeof( LGFILEHDR ) / m_cbSec );
		m_pcheckpoint->checkpoint.le_lgposCheckpoint.le_ib = 0;
		cLGCheckpoint.Set( m_pinst, CbOffsetLgpos( m_pcheckpoint->checkpoint.le_lgposCheckpoint, lgposMin ) );

		*pfGlobalNewCheckpointFile = fTrue;
		}
	else
		{
		if ( err >= 0 )
			{
			delete pfapiCheckpoint;
			pfapiCheckpoint = NULL;
			}
		}

	m_fDisableCheckpoint = fFalse;
	err = JET_errSuccess;
	
HandleError:
	if ( err < 0 )
		{
		if ( m_pcheckpoint != NULL )
			{
			OSMemoryPageFree( m_pcheckpoint );
			m_pcheckpoint = NULL;
			}
		}

	Assert( !pfapiCheckpoint );
	return err;
	}


VOID LOG::LGICheckpointTerm( VOID )
	{
	if ( m_pcheckpoint != NULL )
		{
		m_fDisableCheckpoint = fTrue;
		OSMemoryPageFree( m_pcheckpoint );
		m_pcheckpoint = NULL;
		}

	return;
	}


/*	read checkpoint from file.
/**/
ERR LOG::ErrLGReadCheckpoint(
	IFileSystemAPI * const	pfsapi,
	CHAR *					szCheckpointFile,
	CHECKPOINT *			pcheckpoint,
	const BOOL				fReadOnly )
	{
	ERR						err;

	m_critCheckpoint.Enter();

	err = ( fReadOnly ? ErrUtilReadShadowedHeader : ErrUtilReadAndFixShadowedHeader )(
								pfsapi,
								szCheckpointFile,
								(BYTE*)pcheckpoint,
								sizeof(CHECKPOINT),
								0,
								NULL );
	if ( err < 0 )
		{
		/*	it should never happen that both checkpoints in the checkpoint
		/*	file are corrupt.  The only time this can happen is with a
		/*	hardware error.
		/**/
		if ( JET_errFileNotFound == err )
			{
			err = ErrERRCheck( JET_errCheckpointFileNotFound );
			}
		else
			{
			err = ErrERRCheck( JET_errCheckpointCorrupt );
			}
		}
	else if ( m_fSignLogSet )
		{
		if ( memcmp( &m_signLog, &pcheckpoint->checkpoint.signLog, sizeof( m_signLog ) ) != 0 )
			err = ErrERRCheck( JET_errBadCheckpointSignature );
		}

	Call( err );

	// Check if the file indeed is checkpoint file
	if( filetypeUnknown != pcheckpoint->checkpoint.le_filetype // old format
		&& filetypeCHK != pcheckpoint->checkpoint.le_filetype )
		{
		// not a checkpoint file
		Call( ErrERRCheck( JET_errFileInvalidType ) );
		}

HandleError:
	m_critCheckpoint.Leave();
	return err;
	}


/*	write checkpoint to file.
/**/
ERR LOG::ErrLGIWriteCheckpoint( IFileSystemAPI *const pfsapi, CHAR *szCheckpointFile, CHECKPOINT *pcheckpoint )
	{
	ERR		err;
	
	Assert( m_critCheckpoint.FOwner() );
	Assert( pcheckpoint->checkpoint.le_lgposCheckpoint.le_isec >= m_csecHeader );
	Assert( pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration >= 1 );
				
	err = ErrUtilWriteShadowedHeader(	pfsapi, 
										szCheckpointFile, 
										fFalse,
										(BYTE*)pcheckpoint, 
										sizeof( CHECKPOINT ) );

	//	Ignore errors. Bet on that it may be failed temporily (e.g. being locked)	
//	if ( err < 0 )
//		m_fDisableCheckpoint = fTrue;

	return err;
	}

/*	update in memory checkpoint.
/*
/*	computes log checkpoint, which is the lGeneration, isec and ib
/*	of the oldest transaction which either modified a currently-dirty buffer
/*	an uncommitted version (RCE).  Recovery begins redoing from the
/*	checkpoint.
/*
/*	The checkpoint is stored in the checkpoint file, which is rewritten
/*	whenever a isecChekpointPeriod disk sectors are written.
/**/
VOID LOG::LGIUpdateCheckpoint( CHECKPOINT *pcheckpoint )
	{
	PIB		*ppibT;
	LGPOS	lgposCheckpoint;

	Assert( !m_fLogDisabled );

#ifdef DEBUG
	if ( m_fDBGFreezeCheckpoint )
		return;
#endif

	//  we start with the most recent log record on disk.

	// XXX
	// Note that with FASTFLUSH, m_lgposToFlush points to the
	// start of a log record that didn't make it to disk
	// (or to data that hasn't yet been added to the log buffer).

	m_critLGBuf.Enter();
	lgposCheckpoint = m_lgposToFlush;
	m_critLGBuf.Leave();

	/*	find the oldest transaction with an uncommitted update
	 *	must be in critJet to make sure no new transaction are created.
	 */
	m_pinst->m_critPIB.Enter();
	for ( ppibT = m_pinst->m_ppibGlobal; ppibT != NULL; ppibT = ppibT->ppibNext )
		{
		if ( ppibT->level != levelNil &&			/* pib active */
			 ppibT->FBegin0Logged() &&				/* open transaction */
			 CmpLgpos( &ppibT->lgposStart, &lgposCheckpoint ) < 0 )
			{
			lgposCheckpoint = ppibT->lgposStart;
			}
		}
	m_pinst->m_critPIB.Leave();

	/*	find the oldest transaction which dirtied a current buffer
	/*  NOTE:  no concurrency is required as all transactions that could
	/*  NOTE:  dirty any BF are already accounted for by the above code
	/**/

	FMP::EnterCritFMPPool();
	for ( DBID dbid = dbidMin; dbid < dbidMax; dbid++ )
		{
		const IFMP	ifmp	= m_pinst->m_mpdbidifmp[ dbid ];

		if ( ifmp < ifmpMax )
			{
			LGPOS lgposOldestBegin0;
			BFGetLgposOldestBegin0( ifmp, &lgposOldestBegin0 );
			if ( CmpLgpos( &lgposCheckpoint, &lgposOldestBegin0 ) > 0 )
				{
				lgposCheckpoint = lgposOldestBegin0;
				}
			BFGetLgposOldestBegin0( ifmp | ifmpSLV, &lgposOldestBegin0 );
			if ( CmpLgpos( &lgposCheckpoint, &lgposOldestBegin0 ) > 0 )
				{
				lgposCheckpoint = lgposOldestBegin0;
				}


			//	we shouldn't be checkpointing during recovery
			//	except when we're terminating
			//	UNDONE: is there a better way to tell if we're
			//	terminating besides this m_ppibGlobal check?
			Assert( !m_fRecovering
				|| ppibNil == m_pinst->m_ppibGlobal );

			//	update dbtime/trxOldest
			if ( !m_fRecovering
				&& ppibNil != m_pinst->m_ppibGlobal )
				{
				rgfmp[ifmp].UpdateDbtimeOldest();
				}
			}
		}
	FMP::LeaveCritFMPPool();

	/*	set the new checkpoint if it is valid and advancing
	 */
	if (	CmpLgpos( &lgposCheckpoint, &pcheckpoint->checkpoint.le_lgposCheckpoint ) > 0 &&
			lgposCheckpoint.isec != 0 )
		{
		Assert( lgposCheckpoint.lGeneration != 0 );
		pcheckpoint->checkpoint.le_lgposCheckpoint = lgposCheckpoint;
		}
	Assert( pcheckpoint->checkpoint.le_lgposCheckpoint.le_isec >= m_csecHeader );

	/*	set DBMS parameters
	/**/
	m_pinst->SaveDBMSParams( &pcheckpoint->checkpoint.dbms_param );
	Assert( pcheckpoint->checkpoint.dbms_param.le_lLogBuffers );

	// if the checkpoint is on a log file we haven't generated (disk full, etc.)
	// move the checkoint back ONE LOG file
	// anyway the current checkpoint should not be more that one generation ahead of the current log generation
	{
	LONG lCurrentLogGeneration;

	m_critLGBuf.Enter();
	lCurrentLogGeneration = m_plgfilehdr->lgfilehdr.le_lGeneration;

	Assert ( pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration <= lCurrentLogGeneration + 1 );
	pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration = min ( pcheckpoint->checkpoint.le_lgposCheckpoint.le_lGeneration, lCurrentLogGeneration );
	
	m_critLGBuf.Leave();
	}

//	/*	set database attachments
//	/**/
//	LGLoadAttachmentsFromFMP( m_pinst, pcheckpoint->rgbAttach );

	if ( m_lgposFullBackup.lGeneration )
		{
		/*	full backup in progress
		/**/
		pcheckpoint->checkpoint.le_lgposFullBackup = m_lgposFullBackup;
		pcheckpoint->checkpoint.logtimeFullBackup = m_logtimeFullBackup;
		}

	if ( m_lgposIncBackup.lGeneration )
		{
		/*	incremental backup in progress
		/**/
		pcheckpoint->checkpoint.le_lgposIncBackup = m_lgposIncBackup;
		pcheckpoint->checkpoint.logtimeIncBackup = m_logtimeIncBackup;
		}

	return;
	}


/*	update checkpoint file.
/**/
ERR LOG::ErrLGUpdateCheckpointFile( IFileSystemAPI *const pfsapi, BOOL fUpdatedAttachment )
	{
	ERR				err				= JET_errSuccess;
	LGPOS			lgposCheckpointT;
	CHECKPOINT *	pcheckpointT;
	CHAR			szPathJetChkLog[IFileSystemAPI::cchPathMax];

	if ( m_fDisableCheckpoint
		|| m_fLogDisabled
		|| !m_fLGFMPLoaded
		|| m_fLGNoMoreLogWrite
		|| m_pinst->FInstanceUnavailable() )
		return JET_errSuccess;

	pcheckpointT = (CHECKPOINT *)PvOSMemoryPageAlloc( sizeof(CHECKPOINT), NULL );
	if ( pcheckpointT == NULL )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}
	
	m_critCheckpoint.Enter();

	/*	save checkpoint
	/**/
	lgposCheckpointT = m_pcheckpoint->checkpoint.le_lgposCheckpoint;
	*pcheckpointT = *m_pcheckpoint;

	/*	update checkpoint
	/**/
	LGIUpdateCheckpoint( pcheckpointT );
	const BOOL	fCheckpointUpdated	= ( CmpLgpos( &lgposCheckpointT, &pcheckpointT->checkpoint.le_lgposCheckpoint ) < 0 );

	/*	if checkpoint unchanged then return JET_errSuccess
	/**/
	if ( fUpdatedAttachment || fCheckpointUpdated )
		{
		//	no in-memory checkpoint change if failed to write out to any of
		//	the database headers.

		BOOL fSkippedAttachDetach;

		// Now disallow header update by other threads (log writer or checkpoint advancement)
		// 1. For the log writer it is OK to generate a new log w/o updating the header as no log operations
		// for this db will be logged in new logs
		// 2. For the checkpoint: don't advance the checkpoint if db's header weren't update  <- THIS CASE

		Call( ErrLGIUpdateGenRequired(
			pfsapi,
			pcheckpointT->checkpoint.le_lgposCheckpoint.le_lGeneration,
			m_plgfilehdr->lgfilehdr.le_lGeneration,
			m_plgfilehdr->lgfilehdr.tmCreate,
			&fSkippedAttachDetach ) );

		if ( fSkippedAttachDetach )
			{
			Call ( ErrERRCheck( errSkippedDbHeaderUpdate ) );
			}

		Assert( m_fSignLogSet );
		pcheckpointT->checkpoint.signLog = m_signLog;
		
		// Write filetypeCHK to header
		pcheckpointT->checkpoint.le_filetype = filetypeCHK;

		LGFullNameCheckpoint( pfsapi, szPathJetChkLog );
		err = ErrLGIWriteCheckpoint( pfsapi, szPathJetChkLog, pcheckpointT );
		if ( err < 0 )
			{
			//	for debuggability, record checkpoint update failure
			//	(if repeated failures occur trying to move from a
			//	particular checkpoint, just record the first error
			//	encountered)
			//
			if ( JET_errSuccess == m_errCheckpointUpdate
				|| 0 != CmpLgpos( lgposCheckpointT, m_lgposCheckpointUpdateError ) )
				{
				m_errCheckpointUpdate = err;
				m_lgposCheckpointUpdateError = lgposCheckpointT;
				}

			goto HandleError;
			}
		else
			{
			*m_pcheckpoint = *pcheckpointT;
			cLGCheckpoint.Set( m_pinst, CbOffsetLgpos( m_pcheckpoint->checkpoint.le_lgposCheckpoint, lgposMin ) );
			}
		}
		
HandleError:
	m_critCheckpoint.Leave();
	OSMemoryPageFree( pcheckpointT );
	return err;
	}

VOID
LOG::CHECKSUMINCREMENTAL::ChecksumBytes( const BYTE* const pbMin, const BYTE* const pbMax )
	{
	const UINT		cbitsPerBytes = 8;
	const ULONG32	ulChecksum = UlChecksumBytes( pbMin, pbMax, 0 );
	const ULONG32	ulChecksumAdjusted = _rotl( ulChecksum, m_cLeftRotate );
	
	m_cLeftRotate = ( ULONG( ( pbMax - pbMin ) % sizeof( ULONG32 ) ) * cbitsPerBytes + m_cLeftRotate ) % ( sizeof( ULONG32 ) * cbitsPerBytes );

	m_ulChecksum ^= ulChecksumAdjusted;
	}

//  ================================================================
ULONG32 LOG::UlChecksumBytes( const BYTE * const pbMin, const BYTE * const pbMax, const ULONG32 ulSeed )
//  ================================================================
//
//  The entire aligned NATIVE_WORD containing pbMin and pbMax must be accessible
//
//  For speed we do the checksum NATIVE_WORD aligned. UlChecksum would end up containing the
//  checksum for all the NATIVE_WORDs containing the bytes we want to checksum, possibly
//  including some unwanted bytes at the start and end. To strip out the unwanted bytes
//  we compute a mask to XOR in. The BYTES we want to keep are replaced in the mask by 0,
//  otherwise we use the original bytes.
//
//  At the end we rotate the checksum to take into account the alignment of the start byte.
//
	{
//	Assert( FHostIsLittleEndian() );
	Assert( pbMin <= pbMax );

	//	special case
	if ( pbMin == pbMax )
		return ulSeed;

	const NATIVE_WORD cBitsInByte = 8;
	const INT cLoop = 16; /* 8 */

	//  round down to the start of the NATIVE_WORD containing the start pointer
	const NATIVE_WORD * pwMin = (NATIVE_WORD *) ( ( (NATIVE_WORD)pbMin / sizeof( NATIVE_WORD ) ) * sizeof( NATIVE_WORD ) );

	//  round up to the start of the next NATIVE_WORD that does not contain any of the end pointer
	const NATIVE_WORD * const pwMax = (NATIVE_WORD *) ( ( ( (NATIVE_WORD)pbMax + sizeof( NATIVE_WORD ) - 1 ) / sizeof( NATIVE_WORD ) ) * sizeof( NATIVE_WORD ) );

	Assert( pwMin < pwMax );

	//  calculate the number of bits in the last NATIVE_WORD that we do _not_ want
	const NATIVE_WORD cbitsUnusedLastWord = ( sizeof( NATIVE_WORD ) - ( (NATIVE_WORD)pbMax % sizeof( NATIVE_WORD ) ) ) * cBitsInByte * ( (NATIVE_WORD)pwMax != (NATIVE_WORD)pbMax );
	Assert( cbitsUnusedLastWord < sizeof( NATIVE_WORD ) * cBitsInByte );
	const NATIVE_WORD wByteMaskLast	= ~( (NATIVE_WORD)(~0) >> cbitsUnusedLastWord );

	//  calculate the number of bits in the first NATIVE_WORD that we _do_ want
	const NATIVE_WORD cbitsUsedFirstWord = ( sizeof( NATIVE_WORD ) - ( (NATIVE_WORD)pbMin % sizeof( NATIVE_WORD ) ) ) * cBitsInByte;
	Assert( cbitsUsedFirstWord > 0 );

	//  strip out the unused bytes in the first NATIVE_WORD. do this first to avoid cache misses
	//  take ~0 to get 0xffff...
	//  right shift to get zeroes in the bytes we want to remove
	//  OR with the original NATIVE_WORD
	// right shifting by 32 is undefined, so do it in two steps
	const NATIVE_WORD wByteMaskFirst	= (NATIVE_WORD)(~0) >> ( cbitsUsedFirstWord - 1 ) >> 1;
	const NATIVE_WORD wFirst			= *(LittleEndian<NATIVE_WORD>*)pwMin;
	const NATIVE_WORD wMaskFirst		= wByteMaskFirst & wFirst;

	NATIVE_WORD wChecksum = 0;
	NATIVE_WORD wChecksumT = 0;

	const ULONG cw = ULONG( pwMax - pwMin );
	pwMin -= ( ( cLoop - ( cw % cLoop ) ) & ( cLoop - 1 ) );

	//	'^' can be calculated with either endian format. Convert
	//	the checksum result for BE at the end instead of convert for
	//	each word.
	
	if( 8 == cLoop )
		{
		switch ( cw % cLoop )
			{	
			while ( 1 )
				{
				case 0:	//  we can put this at the top because pdwMax != pdwMin
					wChecksum  ^= pwMin[0];
				case 7:
					wChecksumT ^= pwMin[1];
				case 6:
					wChecksum  ^= pwMin[2];
				case 5:
					wChecksumT ^= pwMin[3];
				case 4:
					wChecksum  ^= pwMin[4];
				case 3:
					wChecksumT ^= pwMin[5];
				case 2:
					wChecksum  ^= pwMin[6];
				case 1:
					wChecksumT ^= pwMin[7];
					pwMin += cLoop;
					if( pwMin >= pwMax )
						{
						goto EndLoop;
						}
				}
			}
		}
	else if ( 16 == cLoop )
		{
		switch ( cw % cLoop )
			{	
			while ( 1 )
				{
				case 0:	//  we can put this at the top because pdwMax != pdwMin
					wChecksum  ^= pwMin[0];
				case 15:
					wChecksumT ^= pwMin[1];
				case 14:
					wChecksum  ^= pwMin[2];
				case 13:
					wChecksumT ^= pwMin[3];
				case 12:
					wChecksum  ^= pwMin[4];
				case 11:
					wChecksumT ^= pwMin[5];
				case 10:
					wChecksum  ^= pwMin[6];
				case 9:
					wChecksumT ^= pwMin[7];
				case 8:
					wChecksum  ^= pwMin[8];
				case 7:
					wChecksumT ^= pwMin[9];
				case 6:
					wChecksum  ^= pwMin[10];
				case 5:
					wChecksumT ^= pwMin[11];
				case 4:
					wChecksum  ^= pwMin[12];
				case 3:
					wChecksumT ^= pwMin[13];
				case 2:
					wChecksum  ^= pwMin[14];
				case 1:
					wChecksumT ^= pwMin[15];
					pwMin += cLoop;
					if( pwMin >= pwMax )
						{
						goto EndLoop;
						}
				}
			}
		}

EndLoop:
	wChecksum ^= wChecksumT;

	//	It is calculated in little endian form, convert to machine
	//	endian for other arithmatic operations.

	wChecksum = ReverseBytesOnBE( wChecksum );

	//	Take the first unaligned portion into account
	
	wChecksum ^= wMaskFirst;

	//  strip out the unused bytes in the last NATIVE_WORD. do this last to avoid cache misses
	//  take ~0 to get 0xffff..
	//  right shift to get zeroes in the bytes we want to keep
	//  invert (make the zeroes 0xff)
	//  OR with the original NATIVE_WORD
	const NATIVE_WORD wLast			= *((LittleEndian<NATIVE_WORD>*)pwMax-1);
	const NATIVE_WORD wMaskLast		= wByteMaskLast & wLast;
	wChecksum ^= wMaskLast;

	ULONG32 ulChecksum;

	if( sizeof( ulChecksum ) != sizeof( wChecksum ) )
		{
		Assert( sizeof( NATIVE_WORD ) == sizeof( ULONG64 ) );
		const NATIVE_WORD wUpper = ( wChecksum >> ( sizeof( NATIVE_WORD ) * cBitsInByte / 2 ) );
		const NATIVE_WORD wLower = wChecksum & 0x00000000FFFFFFFF;
		Assert( wUpper == ( wUpper & 0x00000000FFFFFFFF ) );
		Assert( wLower == ( wLower & 0x00000000FFFFFFFF ) );

		wChecksum = wUpper ^ wLower;
		Assert( wChecksum == ( wChecksum & 0x00000000FFFFFFFF ) );
		}
	else
		{
		Assert( sizeof( NATIVE_WORD ) == sizeof( ULONG32 ) );
		}
	ulChecksum = ULONG32( wChecksum );

	//  we want the checksum we would have gotten if we had done this unaligned
	//  simply rotate the checksum by the appropriate number of bytes
	ulChecksum = _rotl( ulChecksum, ULONG( cbitsUsedFirstWord ) );

	//  now we have rotated, we can XOR in the seed
	ulChecksum ^= ulSeed;

	return ulChecksum;
	}


#ifndef RTM

//  ================================================================
ULONG32 LOG::UlChecksumBytesNaive( const BYTE * pbMin, const BYTE * pbMax, const ULONG32 ulSeed )
//  ================================================================
	{
	const INT cBitsInByte = 8;

	ULONG32 ulChecksum = ulSeed;

	INT cul = INT( pbMax - pbMin ) / 4;
	const Unaligned< ULONG32 >* pul				= (Unaligned< ULONG32 >*)pbMin;
	const Unaligned< ULONG32 >* const pulMax	= pul + cul;
	while( pul < pulMax )
		{
		ulChecksum ^= *pul++;
		}

	const BYTE * pb = (BYTE *)(pulMax);
	if ( FHostIsLittleEndian() )
		{
		INT ib = 0;
		while( pb < pbMax )
			{
			const BYTE b	= *pb;
			const ULONG32 w	= b;
			ulChecksum ^= ( w << ( ib * cBitsInByte ) );
			++ib;
			Assert( ib < 4 );
			++pb;
			}
		}
	else
		{
		INT ib = 3;
		while( pb < pbMax )
			{
			const BYTE b	= *pb;
			const ULONG32 w	= b;
			ulChecksum ^= ( w << ( ib * cBitsInByte ) );
			--ib;
			Assert( ib >= 0 );
			++pb;
			}

		ulChecksum = ReverseBytes( ulChecksum );
		}

	return ulChecksum;
	}


//  ================================================================
ULONG32 LOG::UlChecksumBytesSlow( const BYTE * pbMin, const BYTE * pbMax, const ULONG32 ulSeed )
//  ================================================================
	{
	const INT cBitsInByte = 8;

	ULONG32 ulChecksum = ulSeed;

	INT ib = 0;
	const BYTE * pb = pbMin;
	while( pb < pbMax )
		{
		const BYTE b	= *pb;
		const ULONG32 w	= b;
		ulChecksum ^= ( w << ( ib * cBitsInByte ) );
		ib++;
		ib %= 4;
		++pb;
		}

	return ulChecksum;
	}


//  ================================================================
BOOL LOG::TestChecksumBytesIteration( const BYTE * pbMin, const BYTE * pbMax, const ULONG32 ulSeed )
//  ================================================================
	{
	const ULONG32 ulChecksum		= UlChecksumBytes( pbMin, pbMax, ulSeed );
	const ULONG32 ulChecksumSlow	= UlChecksumBytesSlow( pbMin, pbMax, ulSeed );
	const ULONG32 ulChecksumNaive	= UlChecksumBytesNaive( pbMin, pbMax, ulSeed );
//	const ULONG32 ulChecksumSpencer	= UlChecksumBytesSpencer( pbMin, pbMax, ulSeed );

	CHECKSUMINCREMENTAL	ck;
	ck.BeginChecksum( ulSeed );
	ck.ChecksumBytes( pbMin, pbMax );
	const ULONG32 ulChecksumIncremental = ck.EndChecksum();

	const BYTE* const pbMid = pbMin + ( ( pbMax - pbMin ) + 2 - 1 ) / 2;	// round up to at least 1 byte if cb == 1
	ck.BeginChecksum( ulSeed );
	ck.ChecksumBytes( pbMin, pbMid );
	ck.ChecksumBytes( pbMid, pbMax );
	const ULONG32 ulChecksumIncremental2 = ck.EndChecksum();

	const BYTE* const pbMax1 = pbMin + ( ( pbMax - pbMin ) + 3 - 1 ) / 3;
	const BYTE* const pbMax2 = pbMin + ( 2 * ( pbMax - pbMin ) + 3 - 1 ) / 3;
	ck.BeginChecksum( ulSeed );
	ck.ChecksumBytes( pbMin, pbMax1 );
	ck.ChecksumBytes( pbMax1, pbMax2 );
	ck.ChecksumBytes( pbMax2, pbMax );
	const ULONG32 ulChecksumIncremental3 = ck.EndChecksum();

	return ( ulChecksum == ulChecksumSlow && 
//			 ulChecksum == ulChecksumSpencer &&
			 ulChecksum == ulChecksumNaive &&
			 ulChecksum == ulChecksumIncremental &&
			 ulChecksum == ulChecksumIncremental2 &&
			 ulChecksum == ulChecksumIncremental3 );
	}


LOCAL const BYTE const rgbTestChecksum[] = {
	0x80, 0x23, 0x48, 0x04, 0x1B, 0x13, 0xA0, 0x03, 0x78, 0x55, 0x4C, 0x54, 0x88, 0x18, 0x4B, 0x63,
	0x98, 0x05, 0x4B, 0xC7, 0x36, 0xE5, 0x12, 0x00, 0xB1, 0x90, 0x1F, 0x02, 0x85, 0x05, 0xE5, 0x04,
	0x80, 0x55, 0x4C, 0x04, 0x00, 0x31, 0x60, 0x89, 0x01, 0x5E, 0x39, 0x66, 0x80, 0x55, 0x12, 0x98,
	0x18, 0x00, 0x00, 0x00, 0x48, 0xE5, 0x12, 0x00, 0x9C, 0x65, 0x1B, 0x04, 0x80, 0x55, 0x4C, 0x04,

	0x60, 0xE5, 0x12, 0x00, 0xDF, 0x7F, 0x1F, 0x04, 0x80, 0x55, 0x4C, 0x04, 0x00, 0x72, 0x91, 0x45,
	0x80, 0x23, 0x48, 0x04, 0x1B, 0x13, 0xA0, 0x03, 0x78, 0x55, 0x4C, 0x54, 0x88, 0x18, 0x4B, 0x63,
	0x98, 0x05, 0x4B, 0xC7, 0x36, 0xE5, 0x12, 0x00, 0xB1, 0x90, 0x1F, 0x02, 0x85, 0x05, 0xE5, 0x04,
	0x80, 0x55, 0x4C, 0x04, 0x00, 0x31, 0x60, 0x89, 0x01, 0x5E, 0x39, 0x66, 0x80, 0x55, 0x12, 0x98

//	128 bytes total
	};


//  ================================================================
BOOL LOG::TestChecksumBytes()
//  ================================================================
	{

	//	perform all possible iterations of checksumming
	//
	//	assume N is the number of bytes in the array
	//	the resulting number of iterations is approximately: ((n)(n+1)) / 2 
	//
	//	for 128 bytes, there are about 16,500 iterations, each of decreasing length
	int i;
	int j;
	for( i = 0; i < sizeof( rgbTestChecksum ) - 1; ++i )
		{
		for( j = i + 1; j < sizeof( rgbTestChecksum ); ++j )
			{
			if ( !TestChecksumBytesIteration( rgbTestChecksum + i, rgbTestChecksum + j, j ) )
				return fFalse;
			}
		}
	return fTrue;
	}


#endif	//	!RTM
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\_log\logread.cxx ===
#include "std.hxx"


#if defined( DEBUG ) || !defined( RTM )

//	all cases except !DEBUG && RTM

#ifndef LOGPATCH_UNIT_TEST

//	enabling this generates logpatch.txt
//	it contains trace information for ErrLGCheckReadLastLogRecordFF

#define ENABLE_LOGPATCH_TRACE	

#endif	//	!LOGPATCH_UNIT_TEST
#endif	//	!DEBUG || RTM



VOID LOG::GetLgposOfPbNext(LGPOS *plgpos)
	{
	Assert( m_pbNext != pbNil );
	BYTE	* pb	= PbSecAligned(m_pbNext);
	INT		ib		= ULONG( m_pbNext - pb );
	INT		isec;

	if ( pb > m_pbRead )
		isec = ULONG( m_pbRead + SIZE_T( m_csecLGBuf * m_cbSec ) - pb ) / m_cbSec;
	else
		isec = ULONG( m_pbRead - pb ) / m_cbSec;

	isec = m_isecRead - isec;

	plgpos->isec		= (USHORT)isec;
	plgpos->ib			= (USHORT)ib;
	plgpos->lGeneration	= m_plgfilehdr->lgfilehdr.le_lGeneration;
	}

// Convert pb in log buffer to LGPOS *only* when we're
// actively reading the log file. Do *NOT* use when we're writing
// to the log file.

VOID LOG::GetLgposDuringReading(
	const BYTE * const pb,
	LGPOS * const plgpos
	) const
	{
	Assert( pbNil != pb );
	Assert( pNil != plgpos );
	Assert( pb >= m_pbLGBufMin && pb < m_pbLGBufMax );
	
	const BYTE	* const pbAligned	= PbSecAligned( pb );
	const INT	ib					= ULONG( pb - pbAligned );
	INT			isec;

	// Compute how many sectors from the end of the log buffer we're at.
	// m_pbRead is the end of data in the log buffer.
	// This first case is for wrapping around in the log buffer.
	if ( pbAligned > m_pbRead )
		isec = ULONG( m_pbRead + SIZE_T( m_csecLGBuf * m_cbSec ) - pbAligned ) / m_cbSec;
	else
		isec = ULONG( m_pbRead - pbAligned ) / m_cbSec;

	// m_isecRead is the next sector to pull in from disk.
	isec = m_isecRead - isec;

	plgpos->isec		= (USHORT)isec;
	plgpos->ib			= (USHORT)ib;
	plgpos->lGeneration = m_plgfilehdr->lgfilehdr.le_lGeneration;
	}

#ifdef DEBUG

/* calculate the lgpos of the LR */
void LOG::PrintLgposReadLR ( VOID )
	{
	LGPOS lgpos;

	GetLgposOfPbNext(&lgpos);
	DBGprintf(">%06X,%04X,%04X",
			LONG(m_plgfilehdr->lgfilehdr.le_lGeneration),
			lgpos.isec, lgpos.ib);
	}

#endif

/*  open a generation file on CURRENT directory
/**/
ERR LOG::ErrLGOpenLogGenerationFile(	IFileSystemAPI *const	pfsapi, 
										LONG 						lGeneration, 
										IFileAPI **const 		ppfapi )
	{
	CHAR	szFNameT[IFileSystemAPI::cchPathMax];

	LGSzFromLogId ( szFNameT, lGeneration );
	LGMakeLogName( m_szLogName, szFNameT );

	return pfsapi->ErrFileOpen( m_szLogName, ppfapi, fTrue );
	}


/*  open the redo point log file which must be in current directory.
/**/
ERR LOG::ErrLGRIOpenRedoLogFile( IFileSystemAPI *const pfsapi, LGPOS *plgposRedoFrom, INT *pfStatus )
	{
	ERR		err;
	BOOL	fJetLog = fFalse;

	/*	try to open the redo from file as a normal generation log file
	/**/
	err = ErrLGOpenLogGenerationFile( pfsapi, plgposRedoFrom->lGeneration, &m_pfapiLog );
	if( err < 0 )
		{
		if ( JET_errFileNotFound == err )
			{
			//	gen doesn't exist, so see if redo point is in szJetLog.
			err = ErrLGOpenJetLog( pfsapi );
			if ( err < 0 )
				{
				//	szJetLog is not available either
				if ( JET_errFileNotFound == err )
					{
					//	should be nearly impossible, because we've previously validated
					//	the existence of szJetLog (one possibility may be someone
					//	manually deleted it after we validated it)
					Assert( fFalse );

					//	all other errors will be reported in ErrLGOpenJetLog()
					LGReportError( LOG_OPEN_FILE_ERROR_ID, JET_errFileNotFound );
					}

				*pfStatus = fNoProperLogFile;
				return JET_errSuccess;
				}

			fJetLog = fTrue;
			}
		else
			{
			LGReportError( LOG_OPEN_FILE_ERROR_ID, err );
			return err;
			}
		}

	/*	read the log file header to verify generation number
	/**/
	CallR( ErrLGReadFileHdr( m_pfapiLog, m_plgfilehdr, fCheckLogID ) );

	m_lgposLastRec.isec = 0;
	if ( fJetLog )
		{
		//	scan the log for the end record

		BOOL fCloseNormally;
		err = ErrLGCheckReadLastLogRecordFF( pfsapi, &fCloseNormally );
		if ( err == JET_errSuccess || FErrIsLogCorruption( err ) )
			{
			//	eat errors about corruption
			
			err = JET_errSuccess;
			}
		else
			{
			Assert( err < 0 );
			CallR( err );
			}

		// doesn't matter if we setup log buffer for writing,
		// because it'll get blown away in a moment.
		}

	// XXX
	// At this point, if m_lgposLastRec.isec == 0, that means
	// we want to read all of the log records in the file that
	// we just opened up. If m_lgposLastRec.isec != 0, that means
	// that this is the last log file and we only want to read
	// up to a certain amount.
		
	/*	the following checks are necessary if the szJetLog is opened
	/**/
	if( m_plgfilehdr->lgfilehdr.le_lGeneration == plgposRedoFrom->lGeneration)
		{
		*pfStatus = fRedoLogFile;
		}
	else if ( m_plgfilehdr->lgfilehdr.le_lGeneration == plgposRedoFrom->lGeneration + 1 )
		{
		/*  this file starts next generation, set start position for redo
		/**/
		plgposRedoFrom->lGeneration++;
		plgposRedoFrom->isec = (WORD) m_csecHeader;
		plgposRedoFrom->ib	 = 0;

		*pfStatus = fRedoLogFile;
		}
	else
		{
		/*	log generation gap is found.  Current szJetLog can not be
		/*  continuation of backed up logfile.  Close current logfile
		/*	and return error flag.
		/**/
		delete m_pfapiLog;
		m_pfapiLog = NULL;

		*pfStatus = fNoProperLogFile;
		}

	Assert( err >= 0 );
	return err;
	}

LogReader::LogReader()
	{
	m_plog					= pNil;

	m_fReadSectorBySector	= fFalse;

	m_isecStart				= 0;
	m_csec					= 0;
	m_isec					= 0;
	
	m_csecSavedLGBuf		= 0;
	m_isecCorrupted			= 0;

	m_szLogName[0]			= 0;

#ifdef DEBUG
	m_fLrstateSet			= fFalse;
#endif	//	DEBUG
	}
	
LogReader::~LogReader()
	{
	Assert( pNil == m_plog );
	}

BOOL LogReader::FInit()
	{
	return BOOL( pNil != m_plog );
	}

// Resize the log buffers to requested size, hook up with LOG object

ERR LogReader::ErrLReaderInit( LOG * const plog, UINT csecBufSize )
	{
	ERR err;

	Assert( pNil == m_plog );
	Assert( pNil != plog );
	Assert( csecBufSize > 0 );

	//	setup members

	m_fReadSectorBySector	= fFalse;

	m_isecStart				= 0;
	m_csec					= 0;
	m_isec					= 0;

	m_csecSavedLGBuf		= plog->m_csecLGBuf;
	m_isecCorrupted			= 0;

	m_szLogName[0]			= 0;

	//	allocate log buffers

	CallR( plog->ErrLGInitLogBuffers( csecBufSize ) );

	//	make "none" of log buffer used. Note that it is
	//	an impossible state for every single byte to be
	//	unused, so this work around is needed.

	plog->m_pbWrite			= plog->m_pbLGBufMin;
	plog->m_pbEntry			= plog->m_pbWrite + 1;

	//	the log reader is now initialized

	m_plog					= plog;

	return JET_errSuccess;
	}

// Resize log buffers to original size

ERR LogReader::ErrLReaderTerm()
	{
	ERR err = JET_errSuccess;

	if ( m_plog )
		{

		//	term the log buffers

		err = m_plog->ErrLGInitLogBuffers( m_csecSavedLGBuf );

		//	reset the buffer pointers

		m_plog->m_pbWrite = m_plog->m_pbLGBufMin;
		m_plog->m_pbEntry = m_plog->m_pbWrite + 1;
		}

	//	the log reader is now uninitialized

	m_plog			= pNil;

	m_szLogName[0]	= 0;

	return err;
	}


//	this is like calling Term followed by Init without deallocating and reallocating the log buffers
//	it just resets the internal pointers to invalidate the log buffer data and force a fresh read from disk

VOID LogReader::Reset()
	{
	Assert( FInit() );

	//	invalidate buffers for log-flush code

	m_plog->m_pbWrite = m_plog->m_pbLGBufMin;
	m_plog->m_pbEntry = m_plog->m_pbWrite + 1;

	//	reset state

	m_fReadSectorBySector	= fFalse;

	m_isecStart				= 0;
	m_csec					= 0;
	m_isec					= 0;

	m_isecCorrupted			= 0;
	}


// Make sure that our buffer reflects the current log file.

ERR LogReader::ErrEnsureLogFile()
	{
	Assert( FInit() );

	// ensure that we're going to be giving the user the right log file.

	if ( 0 == UtilCmpFileName( m_plog->m_szLogName, this->m_szLogName ) )
		{
		// yup, got the right file already
		}
	else
		{
		// wrong file
		strcpy( this->m_szLogName, m_plog->m_szLogName );
		// setup so next ErrEnsureSector() will pull in from the right file.
		m_fReadSectorBySector = fFalse;
		m_isecStart = 0;
		m_csec = 0;
		m_isec = 0;
		m_isecCorrupted = 0;
		m_plog->m_pbWrite = m_plog->m_pbLGBufMin;
		m_plog->m_pbEntry = m_plog->m_pbWrite + 1;
#ifdef DEBUG
		m_fLrstateSet = fFalse;
#endif
		}
	return JET_errSuccess;
	}

// Ensure that the data from the log file starting at sector isec for csec
// sectors is in the log buffer (at a minimum). The addr of the data is
// returned in ppb.

ERR LogReader::ErrEnsureSector(
	UINT	isec,
	UINT	csec,
	BYTE**	ppb
	)
	{
	ERR err = JET_errSuccess;

	Assert( FInit() );
	Assert( isec >= m_plog->m_csecHeader );
	Assert( csec > 0 );
	
	// Set this now to pbNil, in case we return an error.
	// We want to know if someone's using *ppb without checking for
	// an error first.
	*ppb = pbNil;

	// We only have it if we have the same log file that the user has open now
	if ( isec >= m_isecStart && isec + csec <= m_isecStart + m_csec )
		{
		// already have it in the buffer.
		*ppb = m_plog->m_pbLGBufMin +
			( m_isec + ( isec - m_isecStart ) ) * m_plog->m_cbSec;
		m_lrstate.m_isec = isec;
		m_lrstate.m_csec = csec;
#ifdef DEBUG
		m_fLrstateSet = fTrue;
#endif
		return JET_errSuccess;
		}

	// If log buffer isn't big enough to fit the sectors user
	// wants, we must enlarge
	if ( m_plog->m_csecLGBuf < csec )
		{
		Call( m_plog->ErrLGInitLogBuffers( csec ) );
		m_plog->m_pbWrite = m_plog->m_pbLGBufMin;
		m_plog->m_pbEntry = m_plog->m_pbWrite + 1;
		}

LReadSectorBySector:

	// read in the data
	if ( m_fReadSectorBySector )
		{
		// we've got 0 sectors stored in the log buffer
		m_csec = 0;
		// the first sector in the buffer is the user requested one.
		m_isecStart = isec;
		// sectors start at beginning of buffer
		m_isec = 0;
		for ( UINT ib = isec * m_plog->m_cbSec;
			m_csec < csec;
			ib += m_plog->m_cbSec, m_csec++ )
			{
			err = m_plog->m_pfapiLog->ErrIORead(	ib, 
													m_plog->m_cbSec, 
													m_plog->m_pbLGBufMin + ( m_csec * m_plog->m_cbSec ) );
			if ( err < 0 )
				{
				if ( 0 != m_isecCorrupted )
					{
					// We found a corrupted sector in the log file. This
					// is unexpected and an error.
					Call( ErrERRCheck( err ) );
					}
				m_isecCorrupted = m_isecStart + m_csec;
				// treat bad sector read as zero filled sector.
				// Hopefully the next sector will be a shadow sector.
				memset( m_plog->m_pbLGBufMin + m_csec * m_plog->m_cbSec, 0,
					m_plog->m_cbSec );
				}
			}
		}
	else
		{
		QWORD	qwSize;
		Call( m_plog->m_pfapiLog->ErrSize( &qwSize ) );
		UINT	csecLeftInFile	= UINT( ( qwSize - ( QWORD( isec ) * m_plog->m_cbSec ) ) / m_plog->m_cbSec );
		// user should never request more than what's in the log file
		Assert( csec <= csecLeftInFile );
		// We don't want to read more than the space we have in the log buffer
		// and not more than what's left in the log file.
		UINT	csecRead		= min( csecLeftInFile, m_plog->m_csecLGBuf );
		// Of the user requested, we'd like to read more, if we have space
		// in the log buffer and if the file has that space.
		csecRead = max( csecRead, csec );

		// if we have an error doing a huge read, try going sector by
		// sector in the case of the corrupted sector that is causing
		// this to give an error.

		//	read the log in 64k chunks to avoid NT quota problems 
		//	(quota issues are most likely due to the number of pages locked down for the read)

		const DWORD	cbReadMax	= 64 * 1024;
		QWORD		ib			= QWORD( isec ) * m_plog->m_cbSec;
		DWORD		cb			= csecRead * m_plog->m_cbSec;
		BYTE		*pb			= m_plog->m_pbLGBufMin;

		while ( cb )
			{

			//	decide how much to read

			const DWORD cbRead = min( cb, cbReadMax );

			//	issue the read (fall back to sector-by-sector reads at the first sign of trouble)

			err = m_plog->m_pfapiLog->ErrIORead( ib, cbRead, pb );
			if ( err < 0 )
				{
				m_fReadSectorBySector = fTrue;
				goto LReadSectorBySector;
				}

			//	update the read counters

			ib += cbRead;
			cb -= cbRead;
			pb += cbRead;

			//	verify that we don't exceed the log buffer

			Assert( ( cb > 0 && pb < m_plog->m_pbLGBufMax ) || ( 0 == cb && pb <= m_plog->m_pbLGBufMax ) );
			}

		// data starts at beginning of log buffer
		m_isec = 0;
		// the first sector is what the user requested
		m_isecStart = isec;
		// this is how many sectors we have in the buffer
		m_csec = csecRead;
		}
		
	m_plog->m_pbWrite = m_plog->m_pbLGBufMin;
	m_plog->m_pbEntry = m_plog->m_pbWrite + m_csec * m_plog->m_cbSec;
	if ( m_plog->m_pbEntry == m_plog->m_pbLGBufMax )
		{
		m_plog->m_pbEntry = m_plog->m_pbLGBufMin;
		}

	// if we get here, we just read in the stuff the user wanted
	// into the beginning of the log file.
	*ppb = m_plog->m_pbLGBufMin;
	m_lrstate.m_isec = isec;
	m_lrstate.m_csec = csec;
#ifdef DEBUG
	m_fLrstateSet = fTrue;
#endif

HandleError:

	return err;
	}

// If a client modified the log file on disk, we want our buffer to reflect that
// change. Copy a modified sector from a user buffer (perhaps the log buffer itself),
// into the log buffer.

VOID LogReader::SectorModified(
	const UINT isec,
	const BYTE* const pb
	)
	{
	Assert( FInit() );
	Assert( isec >= m_plog->m_csecHeader );
	Assert( pbNil != pb );
	
	// If we have the sector in memory, update our in memory copy
	// so that we'll be consistent with what's on the disk.
	if ( isec >= m_isecStart && isec + 1 <= m_isecStart + m_csec )
		{
		BYTE* const	pbDest = m_plog->m_pbLGBufMin + ( m_isec + ( isec - m_isecStart ) ) * m_plog->m_cbSec;
		if ( pbDest != pb )
			{
			UtilMemCpy( pbDest, pb, m_plog->m_cbSec );
			}
		}
	}

BYTE* LogReader::PbGetEndOfData()
	{
	Assert( FInit() );
	BYTE*	pb = m_plog->m_pbLGBufMin + ( m_isec + m_csec ) * m_plog->m_cbSec;
	Assert( pb >= m_plog->m_pbLGBufMin && pb <= m_plog->m_pbLGBufMax );
	return pb;
	}
	
UINT LogReader::IsecGetNextReadSector()
	{
	Assert( FInit() );
	return m_isecStart + m_csec;
	}

ERR
LogReader::ErrSaveState(
	LRSTATE* const plrstate
	) const
	{
	// An internal state must have been reached before
	Assert( m_fLrstateSet );
	Assert( pNil != plrstate );
	*plrstate = m_lrstate;
	return JET_errSuccess;
	}
	
ERR
LogReader::ErrRestoreState(
	const LRSTATE* const plrstate,
	BYTE** const ppb
	)
	{
	// m_lrstate would have had to be set before for us to
	// call restore now
	Assert( m_fLrstateSet );
	Assert( pNil != plrstate );
	return ErrEnsureSector( plrstate->m_isec, plrstate->m_csec, ppb );
	}

//	verifies the supposedly valid LRCHECKSUM record 
//	if there is a short checksum (bUseShortChecksum == bShortChecksumOn),
//		then we verify the whole sector with the short checksum value -- which in turn verifies the 
//		bUseShortChecksum byte
//	if there was not a short checksum, the short checksum value is ignored
//	from this point, other heuristics are employed to see if the pointers in the LRCHECKSUM record look good
//		enough to use
//
//	THIS FUNCTION DOES NOT GUARANTEE ANYTHING ABOUT THE RANGE OF THE LRCHECKSUM RECORD!
//	while it MAY verify the first sector of the range using the short checksum, it does not guarantee the rest
//		of the data
//
//	NOTE: this assumes the entire sector containing the LRCHECKSUM record is in memory

#ifdef DEBUG
LOCAL BOOL g_fLRCKValidationTrap = fFalse;
#endif

BOOL LOG::FValidLRCKRecord(
	// LRCHECKSUM to validate
	const LRCHECKSUM * const plrck,
	// LGPOS of this record
	const LGPOS	* const plgpos
	)
	{
	LGPOS	lgpos;
	QWORD	qwSize = 0;
	ULONG	cbRange = 0;
	ERR		err = JET_errSuccess;
	UINT	ib = 0;
	BOOL	fMultisector = fFalse;
	
	Assert( pNil != plrck );

	//	verify the type as lrtypChecksum
	
	if ( lrtypChecksum != plrck->lrtyp )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	//	is the short checksum in use?

	if ( plrck->bUseShortChecksum == bShortChecksumOn )
		{
		
		//	verify the short checksum
	
		if ( !FValidLRCKShortChecksum( plrck, plgpos->lGeneration ) )
			{
			Assert( !g_fLRCKValidationTrap );
			return fFalse;
			}

		//	since there was a short checksum, this must be a multi-sector flush

		fMultisector = fTrue;
		}
	else if ( plrck->bUseShortChecksum == bShortChecksumOff )
		{

		//	this verifies that the bUseShortChecksum byte was set to a known value and stayed that way

		if ( plrck->le_ulShortChecksum != 0 )
			{
			Assert( !g_fLRCKValidationTrap );
			return fFalse;
			}

		//	since there was no short checksum, this must be a single-sector flush
		
		fMultisector = fFalse;
		}
	else
		{

		//	bUseShortChecksum is corrupt

		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	//	the backward range cannot exceed the length of a full sector (minus the size of the LRCHECKSUM record)

	if ( plrck->le_cbBackwards > ( m_cbSec - sizeof( LRCHECKSUM ) ) )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}
		
	//	the backward pointer must land exactly on the start of the sector OR be 0

	const BYTE* const pbRegionStart = reinterpret_cast< const BYTE* >( plrck ) - plrck->le_cbBackwards;
	if ( PbSecAligned( const_cast< BYTE* >( pbRegionStart ) ) != pbRegionStart &&
		0 != plrck->le_cbBackwards )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	//	the forward range cannot exceed the length of the file

	lgpos = *plgpos;
	AddLgpos( &lgpos, sizeof( LRCHECKSUM ) + plrck->le_cbForwards );
	Call( m_pfapiLog->ErrSize( &qwSize ) );
	Assert( ( qwSize % (QWORD)m_cbSec ) == 0 );
	if ( ( ( (QWORD)lgpos.isec * (QWORD)m_cbSec ) + (QWORD)lgpos.ib ) > qwSize )
		{
		//	if this assert happens, it means that the short checksum value was tested and came out OK
		//	thus, the sector with data (in which plrck resides) must be OK
		//	however, the forward pointer in plrck stretched beyond the end of the file???
		//	this can happen 1 of 2 ways:
		//		we set it up wrong (most likely)
		//		it was mangled in such a way that it passed the checksum (practically impossible)
		AssertSz( !fMultisector, "We have a verified short checksum value and the forward pointer is still wrong!" );

		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	//	calculate the offset of the LRCHECKSUM record from the nearest sector boundary in memory

	ib = ULONG( reinterpret_cast< const BYTE* >( plrck ) - PbSecAligned( reinterpret_cast< const BYTE* >( plrck ) ) );
	Assert( ib == plgpos->ib );

	//	the backwards pointer in the LRCHECKSUM record must be either equal to ib or 0

	if ( plrck->le_cbBackwards != 0 && plrck->le_cbBackwards != ib )
		{

		//	the backward pointer is wrong

		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	//	calculate the range using the backward pointer instead of 'ib'
	//	NOTE: le_cbBackwards can be 0 here making this range calculation fall short of the real thing; 
	//		  having the range too short doesn't matter here because we only do tests if we see that
	//		  the range is a multi-sector range; nothing happens if we decide that this is a single-sector 
	//		  range (e.g. being too short makes it appear < 1 sector)

	cbRange = plrck->le_cbBackwards + sizeof( LRCHECKSUM ) + plrck->le_cbForwards;
	if ( cbRange > m_cbSec )
		{
		
		//	the range covers multiple sectors

		//	we should have a short checksum if this happens (causing fMultisector == fTrue)

		if ( !fMultisector )
			{
			Assert( !g_fLRCKValidationTrap );
			return fFalse;
			}

		//	also, multi-sector flushes must have a range that is sector-granular so that the forward
		//		pointer goes right up to the end of the last sector in the range
		
		if ( ( cbRange % m_cbSec ) != 0 )
			{

			//	the only way we could get a range that is not sector granular is when the 
			//		backward pointer is 0

			if ( plrck->le_cbBackwards != 0 )
				{

				//	woops -- the backward pointer is not what we expect
				
				Assert( !g_fLRCKValidationTrap );
				return fFalse;
				}
			}
		}


	//	re-test the range using the offset of the plrck pointer's alignment rather than the 
	//		backward pointer because the backward pointer could be 0

	cbRange = ib + sizeof( LRCHECKSUM ) + plrck->le_cbForwards;
	if ( cbRange > m_cbSec )
		{

		//	the range covers multiple sectors

		//	we should have a short checksum if this happens (causing fMultisector == fTrue)

		if ( !fMultisector )
			{
			Assert( !g_fLRCKValidationTrap );
			return fFalse;
			}

		//	also, multi-sector flushes must have a range that is sector-granular so that the forward
		//		pointer goes right up to the end of the last sector in the range
		
		if ( ( cbRange % m_cbSec ) != 0 )
			{

			//	woops, the range is not granular

			Assert( !g_fLRCKValidationTrap );
			return fFalse;
			}
		}
	else
		{

		//	the range is 1 sector or less 

		//	we should have no short checksum if this happens (causing fMultisector == fFalse)

		if ( fMultisector )
			{
			
			//	oops, we have a short checksum on a single-sector range

			Assert( !g_fLRCKValidationTrap );
			return fFalse;
			}
		}


	//	test the next pointer

	if ( plrck->le_cbNext != 0 )
		{

		//	there are other LRCHECKSUM records after this one

		//	the range must be atleast 1 sector long and must land on a sector-granular boundary

		if ( cbRange < m_cbSec || ( cbRange % m_cbSec ) != 0 )
			{
			Assert( !g_fLRCKValidationTrap );
			return fFalse;
			}

		//	calculate the position of the next LRCHECKSUM record
		
		lgpos = *plgpos;
		AddLgpos( &lgpos, sizeof( LRCHECKSUM ) + plrck->le_cbNext );

		//	see if the next pointer lands in the sector after the range of this LRCHECKSUM record
		//
		//	NOTE: this should imply that it does not reach the end of the file since earlier we
		//		  made sure the range was less than the end of the file:
		//			next < range, range < EOF; therefore, next < EOF
		//
		//	NOTE: cbRange was previously set

		if ( lgpos.isec != plgpos->isec + ( cbRange / m_cbSec ) )
			{

			//	it lands somewhere else (before or after)

			Assert( !g_fLRCKValidationTrap );
			return fFalse;
			}


		/*******
		//	see if it reaches beyond the end of the file
		
		if ( QWORD( lgpos.isec ) * m_cbSec + lgpos.ib > ( qwSize - sizeof( LRCHECKSUM ) ) )
			{
			return fFalse;
			// the LRCHECKSUM is bad because it's next pointer points to
			// past the end of the log file.
			// Note that plrckNext->cbNext can be zero if it's the last LRCHECKSUM
			// in the log file.
			}
		*******/
		}

	return fTrue;

HandleError:

	//	this is the weird case where IFileAPI::ErrSize() returns an error??? should never happen...

	return fFalse;
	}

// Checks if an LRCHECKSUM on a particular sector is a valid shadow sector.

BOOL LOG::FValidLRCKShadow(
	const LRCHECKSUM * const plrck,
	const LGPOS * const plgpos,
	const LONG lGeneration
	)
	{

	//	validate the LRCHECKSUM record
	
	if ( !FValidLRCKRecord( plrck, plgpos ) )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	//	we should not have a short checksum value

	if ( plrck->bUseShortChecksum != bShortChecksumOff )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	//	we should not have another LRCHECKSUM record after this one

	if ( plrck->le_cbNext != 0 )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	//	verify the range checksum

	if ( UlComputeShadowChecksum( UlComputeChecksum( plrck, (ULONG32)lGeneration ) ) != plrck->le_ulChecksum )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	return fTrue;
	}


// Checks if an LRCHECKSUM on a particular sector is a valid shadow sector without looking at cbNext

BOOL LOG::FValidLRCKShadowWithoutCheckingCBNext(
	const LRCHECKSUM * const plrck,
	const LGPOS * const plgpos,
	const LONG lGeneration
	)
	{

	//	validate the LRCHECKSUM record
	
	if ( !FValidLRCKRecord( plrck, plgpos ) )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	//	we should not have a short checksum value

	if ( plrck->bUseShortChecksum != bShortChecksumOff )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	//	verify the range checksum

	if ( UlComputeShadowChecksum( UlComputeChecksum( plrck, (ULONG32)lGeneration ) ) != plrck->le_ulChecksum )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}

	return fTrue;
	}


// Determines if an LRCHECKSUM on a sector has a valid short checksum.

BOOL LOG::FValidLRCKShortChecksum(
	const LRCHECKSUM * const plrck,
	const LONG lGeneration
	)
	{

	//	assume the caller has checked to see if we even have a short checksum

	Assert( plrck->bUseShortChecksum == bShortChecksumOn );

	//	compute the checksum
	
	if ( UlComputeShortChecksum( plrck, (ULONG32)lGeneration ) != plrck->le_ulShortChecksum )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		}
		
	return fTrue;
	}


// Once we know the structure of an LRCHECKSUM record is valid, we need
// to verify the validity of the region of data covered by the LRCHECKSUM.

BOOL LOG::FValidLRCKRange(
	const LRCHECKSUM * const plrck,
	const LONG lGeneration
	)
	{

	// If the sector is a shadow sector, the checksum won't match
	// because a shadow sector has ulShadowSectorChecksum added to its
	// checksum.
	if ( UlComputeChecksum( plrck, (ULONG32)lGeneration ) != plrck->le_ulChecksum )
		{
		Assert( !g_fLRCKValidationTrap );
		return fFalse;
		// the LRCHECKSUM is bad because it's checksum doesn't match
		}

	return fTrue;

	}


//	verify the LRCK record

void LOG::AssertValidLRCKRecord(
	const LRCHECKSUM * const plrck,
	const LGPOS	* const plgpos )
	{
#ifdef DEBUG
	const BOOL fValid = FValidLRCKRecord( plrck, plgpos );
	Assert( fValid );
#endif	//	DEBUG
	}


//	verify the current LRCK range

void LOG::AssertValidLRCKRange(
		const LRCHECKSUM * const plrck,
		const LONG lGeneration )
	{
#ifdef DEBUG

	//	we can directly assert this condition because it will never fail unless the checksum is actually bad

	Assert( FValidLRCKRange( plrck, lGeneration ) );
#endif	//	DEBUG
	}


//	verify the current LRCK record in DBG or RTL

void LOG::AssertRTLValidLRCKRecord(
		const LRCHECKSUM * const plrck,
		const LGPOS	* const plgpos )
	{
	const BOOL fValid = FValidLRCKRecord( plrck, plgpos );
	AssertRTL( fValid );
	}


//	verify the current LRCK range in DBG or RTL

void LOG::AssertRTLValidLRCKRange(
		const LRCHECKSUM * const plrck,
		const LONG lGeneration )
	{

	//	we can directly assert this condition because it will never fail unless the checksum is actually bad

	AssertRTL( FValidLRCKRange( plrck, lGeneration ) );
	}


//	verify the current LRCK shadow in DBG or RTL

void LOG::AssertRTLValidLRCKShadow(
		const LRCHECKSUM * const plrck,
		const LGPOS * const plgpos,
		const LONG lGeneration )
	{
	const BOOL fValid = FValidLRCKShadow( plrck, plgpos, lGeneration );
	AssertRTL( fValid );
	}


//	verify that the current LRCK range is INVALID

void LOG::AssertInvalidLRCKRange(
		const LRCHECKSUM * const plrck,
		const LONG lGeneration )
	{

	//	we can directly assert this condition because it will never fail unless the checksum is actually bad

	AssertRTL( !FValidLRCKRange( plrck, lGeneration ) );
	}






#ifdef ENABLE_LOGPATCH_TRACE

//	helper for writing to the log-patch text file in ErrLGCheckReadLastLogRecordFF

INLINE BOOL FLGILogPatchDate( const char* pszPath, CPRINTFFILE **const ppcprintf )
	{
	CPRINTFFILE	*pcprintf = *ppcprintf;
	DATETIME	datetime;

	if ( !pcprintf )
		{

		//	allocate a new trace file object

		pcprintf = new CPRINTFFILE( pszPath );
		if ( !pcprintf )
			{
			return fFalse;
			}
		*ppcprintf = pcprintf;

		//	start the trace file with a standard header

		(*pcprintf)( "==============================================================================\r\n" );
		(*pcprintf)( "ErrLGCheckReadLastLogRecordFF trace-log generated by ESE.DLL\r\n" );
		(*pcprintf)( "Do NOT delete this unless you know what you are doing...\r\n" );
		(*pcprintf)( "\r\n" );
		}

	UtilGetCurrentDateTime( &datetime );
	(*pcprintf)( "%02d:%02d:%02d %02d/%02d/%02d: ", 
				datetime.hour,
				datetime.minute,
				datetime.second,
				datetime.month,
				datetime.day,
				datetime.year );

	return fTrue;
	}

#endif	//	ENABLE_LOGPATCH_TRACE


#ifdef LOGPATCH_UNIT_TEST

//	run the following flush sequence: partial --> full --> partial

enum EIOFAILURE
	{
	iofNone			= 0,
	iofClean		= 1,//	clean		-- crash after I/O completes
//	iofTornSmall	= 2,//	torn		-- last sector is torn (anything other than last sector being torn implies
//	iofTornLarge	= 3,//										that later sectors were never written and degrades
						//										to an incomplete-I/O case)
	iofIncomplete1	= 2,//	incomplete1	-- 1 sector was not flushed
	iofIncomplete2	= 3,//	incomplete2	-- 2 sectors were not flushed
	iofMax			= 4
	};


//	mapping array to determine the maximum number of I/Os the current flush will have
//
//		prevflush:	0 = partial, 1 = full
//		curflush:	0 = partial, 1 = full
//		flushsize:	0..3 (only used when curflush == full)

const ULONG g_cIO[2][2][4]		= 		//	[prevflush][curflush][flushsize][io]
	{
		//	prevflush = partial
		{
			//	curflush = partial
			{
				1 + 1,	//	flushsize = <ignored>, partial sector + shadow
				1 + 1,	//	flushsize = <ignored>, partial sector + shadow
				1 + 1,	//	flushsize = <ignored>, partial sector + shadow
				1 + 1	//	flushsize = <ignored>, partial sector + shadow
			},
			//	curflush = full
			{
				1,		//	flushsize = 0(+1) -- implicit +1, flush full sector
				1 + 1,	//	flushsize = 1(+1) -- implicit +1, flush first full sector without touching shadow, flush the rest
				1 + 1,	//	flushsize = 2(+1) -- implicit +1, flush first full sector without touching shadow, flush the rest
				1 + 1	//	flushsize = 3(+1) -- implicit +1, flush first full sector without touching shadow, flush the rest
			}
		},
		//	prevflush = full
		{
			//	curflush = partial
			{
				1 + 1,	//	flushsize = <ignored>, partial sector + shadow
				1 + 1,	//	flushsize = <ignored>, partial sector + shadow
				1 + 1,	//	flushsize = <ignored>, partial sector + shadow
				1 + 1	//	flushsize = <ignored>, partial sector + shadow
			},
			//	curflush = full
			{
				1,		//	flushsize = 0(+1) -- implicit +1, flush all sectors
				1,		//	flushsize = 1(+1) -- implicit +1, flush all sectors
				1,		//	flushsize = 2(+1) -- implicit +1, flush all sectors
				1		//	flushsize = 3(+1) -- implicit +1, flush all sectors
			}
		}
	};


//	mapping array to determine the maximum level of failure given the following params:
//
//		prevflush:	0 = partial, 1 = full
//		curflush:	0 = partial, 1 = full
//		flushsize:	0..3 (only used when curflush == full)
//		io:			0..1 (1 is ignored when prevflush == curflush == full)

const ULONG	g_ciof[2][2][4][2]	= 		//	[prevflush][curflush][flushsize][io]
	{
		//	prevflush = partial
		{	
			//	curflush = partial
			{	
				//	flushsize = <ignored>
				{	
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1,
					iofClean + 1,//iofTornLarge + 1 
				},
				//	flushsize = <ignored>
				{	
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1,
					iofClean + 1,//iofTornLarge + 1 
				},	
				//	flushsize = <ignored>
				{	
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1,
					iofClean + 1,//iofTornLarge + 1 
				},	
				//	flushsize = <ignored>
				{	
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1,
					iofClean + 1,//iofTornLarge + 1 
				}
			},
			//	curflush = full
			{
				//	flushsize = 0(+1) -- implicit +1
				{
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1, 
					iofClean + 1,//iofTornLarge + 1 
				},
				//	flushsize = 1(+1) -- implicit +1
				{
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1, 
					iofClean + 1,//iofTornLarge + 1 
				},
				//	flushsize = 2(+1) -- implicit +1
				{
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1, 
					iofIncomplete1 + 1
				},
				//	flushsize = 3(+1) -- implicit +1
				{
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1, 
					iofIncomplete2 + 1
				}
			}
		},
		//	prevflush = full
		{
			//	curflush = partial
			{
				//	flushsize = <ignored>
				{	
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1,
					iofClean + 1,//iofTornLarge + 1 
				},
				//	flushsize = <ignored>
				{	
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1,
					iofClean + 1,//iofTornLarge + 1 
				},	
				//	flushsize = <ignored>
				{	
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1,
					iofClean + 1,//iofTornLarge + 1 
				},	
				//	flushsize = <ignored>
				{	
					//	I/O = 0, 1
					iofClean + 1,//iofTornLarge + 1,
					iofClean + 1,//iofTornLarge + 1 
				}
			},
			//	curflush = full
			{
				//	flushsize = 0(+1) -- implicit +1
				{ 
					//	I/O = 0, 1<ignored>
					iofClean + 1,//iofTornLarge + 1, 
					iofNone
				},
				//	flushsize = 1(+1) -- implicit +1
				{ 
					//	I/O = 0, 1<ignored>
					iofIncomplete1 + 1, 
					iofNone
				},
				//	flushsize = 2(+1) -- implicit +1
				{ 
					//	I/O = 0, 1<ignored>
					iofIncomplete2 + 1, 
					iofNone
				},
				//	flushsize = 3(+1) -- implicit +1
				{ 
					//	I/O = 0, 1<ignored>
					iofIncomplete2 + 1, 
					iofNone
				},
			}
		}
	};


//	array of different cases (determines coverage of this test)
//
//		prevflush:	0 = partial, 1 = full
//		curflush:	0 = partial, 1 = full
//		flushsize:	0..3 (only used when curflush == full)
//		io:			0..1 (1 is ignored when prevflush == curflush == full)
//		failure:	0..3 (EIOFAILURE - 1)

ULONG g_cCoverage[2][2][4][2][5];



//	convert the current flush iteration to array indices

void TestLogPatchIGetArrayIndices(	const ULONG		iPartial0,
									const ULONG 	iFull0,
									const ULONG		iPartial1,
									const ULONG		iFull1,
									const ULONG		iPartial2,
									ULONG* const	piFlushPrev,
									ULONG* const	piFlushCur,
									ULONG* const	pcsecFlush )
	{
	const ULONG	iFlushPartial	= 0;
	const ULONG	iFlushFull		= 1;

	if ( 0 == iFull0 )
		{
		AssertRTL( iPartial0 > 0 );

		AssertRTL( 0 == iPartial1 );
		AssertRTL( 0 == iFull1 );
		AssertRTL( 0 == iPartial2 );

		*piFlushPrev	= iFlushPartial;
		*piFlushCur		= iFlushPartial;
		*pcsecFlush		= 1;

		return;
		}

	if ( 0 == iFull1 )
		{
		AssertRTL( 0 == iPartial2 );

		if ( iPartial1 > 1 )		//	2 partial flushes
			{
			*piFlushPrev	= iFlushPartial;	
			*piFlushCur		= iFlushPartial;
			*pcsecFlush		= 1;
			}
		else if ( 1 == iPartial1 )	//	1 full followed by 1 partial
			{
			*piFlushPrev	= iFlushFull;
			*piFlushCur		= iFlushPartial;
			*pcsecFlush		= 1;
			}
		else						//	1 partial followed by 1 full (partial was from log-file creation)
			{
			*piFlushPrev	= iFlushPartial;
			*piFlushCur		= iFlushFull;
			*pcsecFlush		= iFull0;
			}

		return;
		}
		
	if ( iPartial2 > 1 )		//	2 partial flushes
		{
		*piFlushPrev	= iFlushPartial;	
		*piFlushCur		= iFlushPartial;
		*pcsecFlush		= 1;
		}
	else if ( 1 == iPartial2 )	//	1 full followed by 1 partial
		{
		*piFlushPrev	= iFlushFull;
		*piFlushCur		= iFlushPartial;
		*pcsecFlush		= 1;
		}
	else						//	1 ??? followed by 1 full (full comes from iFull0 != 0)
		{
		*piFlushPrev	= ( iPartial1 > 0 ) ? iFlushPartial : iFlushFull;
		*piFlushCur		= iFlushFull;
		*pcsecFlush		= iFull1;
		}
	}




BOOL		g_fEnableFlushCheck = fFalse;	//	enable/disable log-flush checking
BOOL		g_fFlushIsPartial;				//	true --> flush should be partial, false --> flush should be full
ULONG		g_csecFlushSize;				//	when g_fFlushIsPartial == false, this is the number of full sectors we expect to flush

BOOL		g_fEnableFlushFailure = fFalse;	//	enable/disable log-flush failures
ULONG 		g_iIO;							//	I/O that should fail
EIOFAILURE	g_iof;							//	method of failure



extern LONG LTestLogPatchIStandardRand();



//	setup a region of buffer to be flushed

void TestLogPatchISetupFlushBuffer( BYTE* const pb, const ULONG cb )
	{
	BYTE	*pbNextRecord;
	ULONG	cbRemaining;

	pbNextRecord = pb;
	for ( cbRemaining = 0; cbRemaining < cb; cbRemaining++ )
		{
		pbNextRecord[cbRemaining] = BYTE( LTestLogPatchIStandardRand() & 0xFF );
		}

	while ( fTrue )
		{
		if ( cbRemaining < sizeof( LRBEGIN ) )
			{
			memset( pbNextRecord, lrtypNOP, cbRemaining );
			return;
			}

		if ( cbRemaining >= sizeof( LRREPLACE ) )
			{
			LRREPLACE* const	plrreplace	=	(LRREPLACE*)pbNextRecord;
			const ULONG			cbReplace	= 	cbRemaining > sizeof( LRREPLACE ) ?
												LTestLogPatchIStandardRand() % ( cbRemaining - sizeof( LRREPLACE ) ) : 0;
			plrreplace->lrtyp = lrtypReplace;
			plrreplace->SetCb( USHORT( cbReplace ) );

			cbRemaining		-= sizeof( LRREPLACE ) + cbReplace;
			pbNextRecord	+= sizeof( LRREPLACE ) + cbReplace;
			}
		else if ( cbRemaining >= sizeof( LRCOMMIT0 ) )
			{
			LRCOMMIT0* const plrcommit0 = (LRCOMMIT0*)pbNextRecord;

			plrcommit0->lrtyp = lrtypCommit0;

			cbRemaining		-= sizeof( LRCOMMIT0 );
			pbNextRecord	+= sizeof( LRCOMMIT0 );
			}
		else
			{
			LRBEGIN* const plrbegin = (LRBEGIN*)pbNextRecord;

			plrbegin->lrtyp = lrtypBegin;

			cbRemaining		-= sizeof( LRBEGIN );
			pbNextRecord	+= sizeof( LRBEGIN );
			}
		}
	}
									



//	do a partial flush

void TestLogPatchIFlushPartial( INST* const pinst, LOG* const plog )
	{
	ERR		err;
	ULONG	cbRemaining;
	ULONG	cb;
	BYTE	rgb[512];
	DATA	rgdata[1];
	LGPOS	lgpos;
	BYTE	*pbWriteBefore;
	ULONG	isecWriteBefore;

	pbWriteBefore = plog->m_pbWrite;
	isecWriteBefore = plog->m_isecWrite;

	//	determine how many bytes we have left in the current sector

	AssertRTL( plog->m_pbWrite == plog->PbSecAligned( plog->m_pbEntry ) );
	cbRemaining = 512 - ( ULONG_PTR( plog->m_pbEntry ) % 512 );

	//	use between 1% and 50% of those bytes

	cb = 1 + LTestLogPatchIStandardRand() % ( cbRemaining / 2 );

	//	setup the log record buffer

	TestLogPatchISetupFlushBuffer( rgb, cb );

	//	setup the flush-check params

	AssertRTL( g_fEnableFlushCheck );
	g_fFlushIsPartial = fTrue;

	//	log the "log record"

	rgdata[0].SetPv( rgb );
	rgdata[0].SetCb( cb );
	err = plog->ErrLGLogRec( rgdata, 1, fLGNoNewGen, &lgpos );
	AssertRTL( JET_errSuccess == err );

	//	flush the log

	err = plog->ErrLGFlushLog( pinst->m_pfsapi, fFalse );
	if ( g_fEnableFlushFailure )
		{
		AssertRTL( err < JET_errSuccess );
		}
	else
		{
		AssertRTL( JET_errSuccess == err );
		AssertRTL( plog->m_pbWrite == pbWriteBefore );
		AssertRTL( plog->m_isecWrite == isecWriteBefore );
		}
	}


//	do a full flush

void TestLogPatchIFlushFull( INST* const pinst, LOG* const plog, const ULONG 
csecFlushSize )
	{
	ERR		err;
	ULONG	cbRemaining;
	ULONG	cb;
	BYTE	rgb[4 * 512];
	DATA	rgdata[1];
	LGPOS	lgpos;
	BYTE	*pbWriteBefore;
	ULONG	isecWriteBefore;

	Enforce( csecFlushSize <= 4 );

	pbWriteBefore = plog->m_pbWrite;
	isecWriteBefore = plog->m_isecWrite;

	//	determine how many bytes we have left in the current sector

	AssertRTL( plog->m_pbWrite == plog->PbSecAligned( plog->m_pbEntry ) );
	cbRemaining = 512 - ( ULONG_PTR( plog->m_pbEntry ) % 512 );

	//	use enough space to fill up the desired number of sectors

	cb = cbRemaining + ( ( csecFlushSize - 1 ) * 512 );

	//	setup the log record buffer with NOPs

	TestLogPatchISetupFlushBuffer( rgb, cb );

	//	setup the flush-check params

	AssertRTL( g_fEnableFlushCheck );
	g_fFlushIsPartial	= fFalse;
	g_csecFlushSize		= csecFlushSize;

	//	log the "log record"

	rgdata[0].SetPv( rgb );
	rgdata[0].SetCb( cb );
	err = plog->ErrLGLogRec( rgdata, 1, fLGNoNewGen, &lgpos );
	AssertRTL( JET_errSuccess == err );

	//	flush the log

	err = plog->ErrLGFlushLog( pinst->m_pfsapi, fFalse );
	if ( g_fEnableFlushFailure )
		{
		AssertRTL( err < JET_errSuccess );
		}
	else
		{
		AssertRTL( JET_errSuccess == err );
		AssertRTL( plog->m_pbWrite != pbWriteBefore );	//	shouldn't be equal since we flush at most 4 sectors
		AssertRTL( plog->m_isecWrite > isecWriteBefore );
		}
	}


//	verify the log after patching it

void TestLogPatchIVerify(	LOG* const			plog,
							const ULONG			iFull0,
							const ULONG			iPartial1,
							const ULONG			iFull1,
							const ULONG			iPartial2,
							const ULONG			iIO,
							const EIOFAILURE	iof )
	{
	ERR			err;
	ULONG		csecData;
	ULONG		cLRCK;
	LogReader	lread;
	BYTE		*pbEnsure;
	ULONG		isec;
	ULONG		iLRCK;
	LGPOS		lgpos;
	LRCHECKSUM	*plrck;
	ULONG		csecPattern;

	//	calculate the expected condition of the log

	if ( 0 == iFull0 )			//	last flush is partial, prev-last flush is partial (from iPartial0 > 0 or from new log file)
		{
		csecData	= 1 + 1;	//	includes shadow sector
		cLRCK		= 1;		//	we have 1 LRCK because we'll recover the torn sector using the previous shadow sector
		}
	else if ( 0 == iFull1 )
		{
		if ( iPartial1 > 0 )	//	last flush is partial, prev-last flush is partial OR
			{					//	last flush is partial, prev-last flush is full
			csecData	= iFull0 + 1 + 1;	//	includes shadow sector
			cLRCK		= 2;
			}
		else					//	last flush is full, prev-last flush is partial   (either from iPartial0 or from new log file)
			{
			if ( 0 == iIO )
				{
				//if ( iofClean == iof )
				AssertRTL( iofClean == iof );
					{
					if ( 1 == iFull0 )	//	full-sector flush will NOT overwrite the shadow from the prev flush
						{
						csecData	= iFull0 + 1 + 1;	//	full sector followed by new partial flush (created by patch code)
						cLRCK		= 2;
						}
					else
						{
						csecData	= 1 + 1;	//	first sector of full flush followed by old shadow (we know prev flush was partial)
						cLRCK		= 1;
						}
					}
				//else
				//	{
				//	AssertRTL( iofTornSmall == iof || iofTornLarge == iof );
				//
				//	csecData	= 1 + 1;	//	includes shadow sector used to recover previous partial flush
				//	cLRCK		= 1;
				//	}
				}
			else
				{
				AssertRTL( 1 == iIO );

				if ( iofClean == iof )
					{
					csecData	= iFull0 + 1 + 1;	//	includes partial flush (w/shadow) that gets created by the patch code
					cLRCK		= 2;
					}
				else
					{
					AssertRTL(	//iofTornSmall == iof		|| iofTornLarge == iof || 
								iofIncomplete1 == iof	|| iofIncomplete2 == iof );

					csecData	= 1 + 1;	//	includes shadow sector that gets created by the patch code after shrinking the LRCK
					cLRCK		= 1;
					}
				}
			}
		}
	else	//	last flush may be full; prev-last flush may be full
		{
		if ( iPartial2 > 0 )		//	last flush is partial, prev-last flush is partial OR
			{						//	last flush is partial, prev-last flush is full
			csecData	= iFull0 + iFull1 + 1 + 1;	//	includes shadow sector
			cLRCK		= 3;
			}
		else if ( iPartial1 > 0 )	//	last flush is full, prev-last flush is partial
			{
			if ( 0 == iIO )
				{
				//if ( iofClean == iof )
				AssertRTL( iofClean == iof );
					{
					if ( 1 == iFull1 )
						{
						csecData	= iFull0 + iFull1 + 1 + 1;	//	full followed by new partial (created by log patch code)
						cLRCK		= 3;
						}
					else
						{
						csecData	= iFull0 + 1 + 1;	//	first sector of full flush followed by old shadow (we know prev flush was partial)
						cLRCK		= 2;
						}
					}
				//else
				//	{
				//	AssertRTL( iofTornSmall == iof || iofTornLarge == iof );
				//
				//	csecData	= iFull0 + 1 + 1;	//	includes shadow sector used to recover previous partial flush
				//	cLRCK		= 2;
				//	}
				}
			else
				{
				AssertRTL( 1 == iIO );

				if ( iofClean == iof )
					{
					csecData	= iFull0 + iFull1 + 1 + 1;	//	includes partial flush (w/shadow) that gets created by the patch code
					cLRCK		= 3;
					}
				else
					{
					AssertRTL(	//iofTornSmall == iof		|| iofTornLarge == iof || 
								iofIncomplete1 == iof	|| iofIncomplete2 == iof );

					csecData	= iFull0 + 1 + 1;	//	includes shadow sector that gets created by the patch code after shrinking the LRCK
					cLRCK		= 2;
					}
				}
			}
		else						//	last flush is full, prev-last flush is full
			{
			AssertRTL( 0 == iIO );

			if ( iofClean == iof )
				{
				csecData	= iFull0 + iFull1 + 1 + 1;	//	includes partial flush (w/shadow) that gets created by the patch code
				cLRCK		= 3;
				}
			else
				{
				AssertRTL(	//iofTornSmall == iof		|| iofTornLarge == iof || 
							iofIncomplete1 == iof	|| iofIncomplete2 == iof );


				csecData	= iFull0 + 1 + 1;	//	includes partial flush (w/shadow) that gets created by the patch code
				cLRCK		= 2;
				}
			}
		}

	//	allocate a buffer for reading the log

	AssertRTL( !plog->m_plread );
	plog->m_plread = &lread;

	err = lread.ErrLReaderInit( plog, ( 5 * 1024 * 1024 ) / 512 );
	AssertRTL( JET_errSuccess == err );
	err = lread.ErrEnsureLogFile();
	AssertRTL( JET_errSuccess == err );

	//	read in the data portion of the log (should be successful)

	err = lread.ErrEnsureSector( 8, csecData, &pbEnsure );
	AssertRTL( JET_errSuccess == err );

	//	verify that the pattern is NOT in the data portion

	for ( isec = 0; isec < csecData; isec++ )
		{
		if ( 0 == memcmp( pbEnsure + isec * 512, rgbLogExtendPattern, 512 ) )
			{
			AssertSzRTL( fFalse, "The log-extend pattern was found in the data portion of log file!" );
			}
		}

	//	verify the checksums

	const BOOL fOldRecovering = plog->m_fRecovering;
	plog->m_fRecovering = fTrue;

	iLRCK = 0;
	lgpos.ib = 0;
	lgpos.isec = 8;
	lgpos.lGeneration = 1;
	while ( fTrue )
		{

		//	verify the checksum

		plrck = (LRCHECKSUM*)( pbEnsure + ( ( lgpos.isec - 8 ) * 512 ) + lgpos.ib );

		plog->AssertRTLValidLRCKRecord( plrck, &lgpos );
		plog->AssertRTLValidLRCKRange( plrck, lgpos.lGeneration );

		//	move next

		iLRCK++;
		if ( iLRCK >= cLRCK )
			{
			AssertRTL( 0 == plrck->le_cbNext );
			break;
			}
		AssertRTL( plrck->le_cbNext > 0 );
		plog->AddLgpos( &lgpos, sizeof( LRCHECKSUM ) + plrck->le_cbNext );
		}

	//	verify the shadow of the last partial sector

	plrck = (LRCHECKSUM*)( (BYTE*)plrck + 512 );
	plog->AssertRTLValidLRCKShadow( plrck, &lgpos, lgpos.lGeneration ) );

	//	verify that we are in the last sector of the data portion of the log

	AssertRTL( ( ULONG_PTR( plrck ) & ~511 ) + 512 == ULONG_PTR( pbEnsure ) + ( csecData * 512 ) );

	//	verify the data

	iLRCK = 0;
	lgpos.ib = 0;
	lgpos.isec = 8;
	lgpos.lGeneration = 1;
	while ( fTrue )
		{
		LR *plr;

		plr = (LR*)( pbEnsure + ( ( lgpos.isec - 8 ) * 512 ) + lgpos.ib );

		AssertRTL(	lrtypNOP == plr->lrtyp ||
					lrtypBegin == plr->lrtyp ||
					lrtypCommit0 == plr->lrtyp ||
					lrtypReplace == plr->lrtyp ||
					lrtypChecksum == plr->lrtyp );

		if ( lrtypChecksum == plr->lrtyp )
			{
			iLRCK++;
			if ( iLRCK >= cLRCK )
				{
				break;
				}
			}

		plog->AddLgpos( &lgpos, CbLGSizeOfRec( plr ) );
		}


	plog->m_fRecovering = fFalse;

	//	read in the pattern portion of the log (should be successful)

	csecPattern = ( ( 5 * 1024 * 1024 ) / 512 ) - ( 8 + csecData );
	err = lread.ErrEnsureSector( 8 + csecData, csecPattern, &pbEnsure );
	AssertRTL( JET_errSuccess == err );

	//	verify that we have the pattern after the last sector of data

	for ( isec = 0; isec < csecPattern; isec++ )
		{
		AssertRTL( 0 == memcmp( pbEnsure + ( isec * 512 ), rgbLogExtendPattern, 512 ) );
		}

	//	cleanup memory

	err = lread.ErrLReaderTerm();
	AssertRTL( JET_errSuccess == err );

	plog->m_plread = NULL;
	}


//	test one log-flush failure case

void TestLogPatchITest(	INST* const			pinst,
						LOG* const			plog,
						const ULONG			iPartial0,
						const ULONG			iFull0,
						const ULONG			iPartial1,
						const ULONG			iFull1,
						const ULONG			iPartial2,
						const ULONG			iIO,
						const EIOFAILURE	iof )
	{
	ERR		err;
	ULONG	iLoop;

	//	create a new log file (in memory)

	delete plog->m_pfapiLog;
	plog->m_pfapiLog = NULL;

	plog->m_lgposLastLogRec = lgposMin;
	plog->m_lgposMaxFlushPoint = lgposMin;
	plog->LGMakeLogName( plog->m_szLogName, plog->m_szJet );

	plog->m_critLGFlush.Enter();
	err = plog->ErrLGNewLogFile( pinst->m_pfsapi, 0, fLGOldLogNotExists );
	AssertRTL( JET_errSuccess == err );
	plog->m_critLGFlush.Leave();

	memcpy( plog->m_plgfilehdr, plog->m_plgfilehdrT, sizeof( LGFILEHDR ) );
	plog->m_isecWrite = 8;
	plog->m_pbLGFileEnd = pbNil;
	plog->m_isecLGFileEnd = 0;

	//	enable the flush-check mechanism

	g_fEnableFlushCheck = fTrue;

	//	first partial flush sequence

	if ( iPartial0 > 0 )
		{

		//	determine if this is the last flush sequence
		
		const BOOL fLast = ( 0 == iFull0 ) ? fTrue : fFalse;

		for ( iLoop = 0; iLoop < iPartial0 - 1; iLoop++ )
			{
			TestLogPatchIFlushPartial( pinst, plog );	//	do a partial flush
			}

		if ( fLast )
			{
			g_fEnableFlushFailure	= fTrue;	//	enable the flush-failure mechanism
			g_iIO					= iIO;
			g_iof					= iof;
			}

		TestLogPatchIFlushPartial( pinst, plog );		//	do the last flush

		if ( fLast )
			{
			goto DoneFlushing;
			}
		}

	//	first full flush sequence

	if ( iFull0 > 0 )
		{

		//	determine if this is the last flush sequence

		const BOOL fLast = ( 0 == iPartial1 && 0 == iFull1 ) ? fTrue : fFalse;

		if ( fLast )
			{
			g_fEnableFlushFailure	= fTrue;	//	enable the flush-failure mechanism
			g_iIO					= iIO;
			g_iof					= iof;
			}

		TestLogPatchIFlushFull( pinst, plog, iFull0 );	//	do the flush

		if ( fLast )
			{
			goto DoneFlushing;
			}
		}

	//	second partial flush sequence

	if ( iPartial1 > 0 )
		{

		//	determine if this is the last flush sequence
		
		const BOOL fLast = ( 0 == iFull1 ) ? fTrue : fFalse;

		for ( iLoop = 0; iLoop < iPartial1 - 1; iLoop++ )	
			{
			TestLogPatchIFlushPartial( pinst, plog );	//	do a partial flush
			}

		if ( fLast )
			{
			g_fEnableFlushFailure	= fTrue;	//	enable the flush-failure mechanism
			g_iIO					= iIO;
			g_iof					= iof;
			}

		TestLogPatchIFlushPartial( pinst, plog );		//	do the last flush

		if ( fLast )
			{
			goto DoneFlushing;
			}
		}

	//	second full flush sequence

	if ( iFull1 > 0 )
		{

		//	determine if this is the last flush sequence

		const BOOL fLast = ( 0 == iPartial2 ) ? fTrue : fFalse;

		if ( fLast )
			{
			g_fEnableFlushFailure	= fTrue;	//	enable the flush-failure mechanism
			g_iIO					= iIO;
			g_iof					= iof;
			}

		TestLogPatchIFlushFull( pinst, plog, iFull1 );	//	do the flush

		if ( fLast )
			{
			goto DoneFlushing;
			}
		}

	//	third partial flush sequence

	for ( iLoop = 0; iLoop < iPartial2 - 1; iLoop++ )
		{
		TestLogPatchIFlushPartial( pinst, plog );		//	do the flush
		}

	g_fEnableFlushFailure	= fTrue;			//	enable the flush-failure mechanism
	g_iIO					= iIO;
	g_iof					= iof;

	TestLogPatchIFlushPartial( pinst, plog );			//	do the last flush

DoneFlushing:

	//	disable the flush-failure mechanism

	g_fEnableFlushFailure = fFalse;

	//	disable the flush-check mechanism

	g_fEnableFlushCheck = fFalse;

	//	patch the log

	BOOL fCloseNormally;
	Call( plog->ErrLGCheckReadLastLogRecordFF( pinst->m_pfsapi, &fCloseNormally, fFalse, NULL ) );
	AssertRTL( !fCloseNormally );

	//	verify the log

	TestLogPatchIVerify(	plog,
							iFull0,
							iPartial1,
							iFull1,
							iPartial2,
							iIO,
							iof );

	return;

HandleError:
	AssertRTL( JET_errSuccess == err );
	}



//#define TEST_LOG_LEVEL 0		//	partial
//#define TEST_LOG_LEVEL 1		//	partial -> full -> partial
#define TEST_LOG_LEVEL 2		//	partial -> full -> partial -> full -> partial




//
//#error some of these cases may be interpreted as corruption -- 
//		should we ignore some cases that we know will be corruption?
//		(e.g. I think almost all torn-sector cases will have this problem)
//		(also, I think this will happen with some of the full->full flush cases)
//




//	test the log-patch code

void TestLogPatch( INST* const pinst )
	{
	LOG* const	plog = pinst->m_plog;

	ULONG 		iPartial0;
	ULONG		iFull0;
	ULONG		iPartial1;
	ULONG		iFull1;
	ULONG		iPartial2;

#if TEST_LOG_LEVEL == 0
	const ULONG	iPartial0Min	= 1;
	const ULONG	iPartial0Max	= 10;
	const ULONG iFull0Min		= 0;
	const ULONG iFull0Max		= 0;
	const ULONG	iPartial1Min	= 0;
	const ULONG	iPartial1Max	= 0;
	const ULONG iFull1Min		= 0;
	const ULONG iFull1Max		= 0;
	const ULONG	iPartial2Min	= 0;
	const ULONG	iPartial2Max	= 0;
	const ULONG citerTotal		= iPartial0Max - iPartial0Min;
#elif TEST_LOG_LEVEL == 1
	const ULONG	iPartial0Min	= 0;
	const ULONG	iPartial0Max	= 10;
	const ULONG iFull0Min		= 1;
	const ULONG iFull0Max		= 4;
	const ULONG	iPartial1Min	= 0;
	const ULONG	iPartial1Max	= 10;
	const ULONG iFull1Min		= 0;
	const ULONG iFull1Max		= 0;
	const ULONG	iPartial2Min	= 0;
	const ULONG	iPartial2Max	= 0;
	const ULONG citerTotal		=	( iPartial0Max - iPartial0Min ) *
									( iFull0Max - iFull0Min ) *
									( iPartial1Max - iPartial1Min );
#elif TEST_LOG_LEVEL == 2
	const ULONG	iPartial0Min	= 0;
	const ULONG	iPartial0Max	= 10;
	const ULONG iFull0Min		= 1;
	const ULONG iFull0Max		= 4;
	const ULONG	iPartial1Min	= 0;
	const ULONG	iPartial1Max	= 10;
	const ULONG iFull1Min		= 1;
	const ULONG iFull1Max		= 4;
	const ULONG	iPartial2Min	= 0;
	const ULONG	iPartial2Max	= 10;
	const ULONG citerTotal		=	( iPartial0Max - iPartial0Min ) *
									( iFull0Max - iFull0Min ) *
									( iPartial1Max - iPartial1Min ) *
									( iFull1Max - iFull1Min ) *
									( iPartial2Max - iPartial2Min );
#else
#error Invalid value for TEST_LOG_LEVEL
#endif	//	TEST_LOG_LEVEL

	ULONG		iFlushPrev;
	ULONG		iFlushCur;
	ULONG		csecFlush;

	ULONG		iIO;

	EIOFAILURE	iof;

	//	reset the coverage map

	memset( (void*)&g_cCoverage, 0, sizeof( g_cCoverage ) );

	//	fixup the current log path for ErrLGNewLogFile()

	plog->m_szLogCurrent = plog->m_szLogFilePath;
	strcpy( plog->m_szLogFilePath, ".\\" );
	plog->m_isecWrite = 8;
	plog->m_fNewLogRecordAdded = fTrue;

	//	iterate over all possible combinations of the flush sequence

	iPartial0	= iPartial0Min;
	iFull0		= iFull0Min;
	iPartial1	= iPartial1Min;
	iFull1		= iFull1Min;
	iPartial2	= iPartial2Min;

	printf( "\n" );
	printf( "testing the log-patch code\n" );
	printf( "     iPartial0 = %d..%d\n", iPartial0Min, iPartial0Max );
	printf( "     iFull0    = %d..%d\n", iFull0Min, iFull0Max );
	printf( "     iPartial1 = %d..%d\n", iPartial1Min, iPartial1Max );
	printf( "     iFull1    = %d..%d\n", iFull1Min, iFull1Max );
	printf( "     iPartial2 = %d..%d\n", iPartial2Min, iPartial2Max );
	printf( "  ---------------------\n" );
	printf( "     SUB-TOTAL = %d state sequences\n", citerTotal );
	printf( "  ---------------------\n" );
	printf( "      max I/Os = 2\n" );
	printf( "  max failures = 3\n" );
	printf( "  ---------------------\n" );
	printf( "  APPROX TOTAL = %d failure cases (will actually be slightly less than this)\n", citerTotal * 2 * 3 );
	printf( "\n" );

	while ( fTrue )
		{
		AssertRTL( 0 != iPartial0 + iFull0 + iPartial1 + iFull1 + iPartial2 );

		if ( iPartial0Min == iPartial0 )
			{
			printf( "iPartial0=%d..%d", iPartial0Min, iPartial0Max );
			if ( 0 != iFull0 )
				{
				printf( ",iFull0=%d,iPartial1=%d", iFull0, iPartial1 );
				if ( 0 != iFull1 )
					{
					printf( ",iFull1=%d,iPartial2=%d", iFull1, iPartial2 );
					}
				}
			printf( "\n" );
			}

		//	convert this flush sequence to array indices

		TestLogPatchIGetArrayIndices(	iPartial0, 
										iFull0, 
										iPartial1, 
										iFull1, 
										iPartial2, 
										&iFlushPrev, 
										&iFlushCur, 
										&csecFlush );
		AssertRTL( csecFlush >= 1 );
		AssertRTL( csecFlush <= 4 );

		//	extract the maximum number of I/Os for this iteration

		const ULONG cIOMax = g_cIO[iFlushPrev][iFlushCur][csecFlush-1];
		AssertRTL( cIOMax <= 2 );

		for ( iIO = 0; iIO < cIOMax; iIO++ )
			{

			//	extract the maximum level of I/O failure

			const ULONG	ciofMax = g_ciof[iFlushPrev][iFlushCur][csecFlush-1][iIO];
			AssertRTL( ciofMax > iofNone );
			AssertRTL( ciofMax <= iofMax );

			for ( iof = iofClean; iof < ciofMax; iof = EIOFAILURE( ULONG_PTR( iof ) + 1 ) )
				{

				//	run the current test

				TestLogPatchITest(	pinst,
									plog,
									iPartial0,
									iFull0,
									iPartial1,
									iFull1,
									iPartial2,
									iIO,
									iof );

				//	remember that we got coverage of this case

				g_cCoverage[iFlushPrev][iFlushCur][csecFlush-1][iIO][iof-1]++;
				}
			}

		//	advance the loop counters

		if ( iPartial0 >= iPartial0Max )
			{
			iPartial0 = iPartial0Min;
			if ( iFull0 >= iFull0Max )
				{
				iFull0 = iFull0Min;
				if ( iPartial1 >= iPartial1Max )
					{
					iPartial1 = iPartial1Min;
					if ( iFull1 >= iFull1Max )
						{
						iFull1 = iFull1Min;
						if ( iPartial2 >= iPartial2Max )
							{
							break;
							}
						else
							{
							iPartial2++;
							}
						}
					else
						{
						iFull1++;
						}
					}
				else
					{
					iPartial1++;
					}
				}
			else
				{
				iFull0++;
				}
			}
		else
			{
			iPartial0++;
			}
		}

	printf( "\n" );

	//	display test coverage

	printf( "\nTest coverage map:\n\n" );
	for ( iFlushPrev = 0; iFlushPrev < 2; iFlushPrev++ )
		{
		for ( iFlushCur = 0; iFlushCur < 2; iFlushCur++ )
			{
			char szSequence[30];

			sprintf( szSequence, "%s,%s", ( 0 == iFlushPrev ) ? "partial" : "full", ( 0 == iFlushCur  ) ? "partial" : "full" );

			//printf( "sequence         sectors  I/O | clean  tornsmall  tornlarge  incomp1  incomp2\n" );
			//printf( "---------------  -------  --- | =====  =========  =========  =======  =======\n" );

			printf( "sequence         sectors  I/O | clean  incomp1  incomp2\n" );
			printf( "---------------  -------  --- | =====  =======  =======\n" );

			//printf( "%-15s        1    0 | %5d  %9d  %9d  %7d  %7d\n",
			printf( "%-15s        1    0 | %5d  %7d  %7d\n",
					szSequence,
					g_cCoverage[iFlushPrev][iFlushCur][0][0][0],
					g_cCoverage[iFlushPrev][iFlushCur][0][0][1],
					g_cCoverage[iFlushPrev][iFlushCur][0][0][2] );
					//g_cCoverage[iFlushPrev][iFlushCur][0][0][3],
					//g_cCoverage[iFlushPrev][iFlushCur][0][0][4] );
			//printf( "%-15s        1    1 | %5d  %9d  %9d  %7d  %7d\n",
			printf( "%-15s        1    1 | %5d  %7d  %7d\n",
					szSequence,
					g_cCoverage[iFlushPrev][iFlushCur][0][1][0],
					g_cCoverage[iFlushPrev][iFlushCur][0][1][1],
					g_cCoverage[iFlushPrev][iFlushCur][0][1][2] );
					//g_cCoverage[iFlushPrev][iFlushCur][0][1][3],
					//g_cCoverage[iFlushPrev][iFlushCur][0][1][4] );

			//printf( "%-15s        2    0 | %5d  %9d  %9d  %7d  %7d\n",
			printf( "%-15s        2    0 | %5d  %7d  %7d\n",
					szSequence,
					g_cCoverage[iFlushPrev][iFlushCur][1][0][0],
					g_cCoverage[iFlushPrev][iFlushCur][1][0][1],
					g_cCoverage[iFlushPrev][iFlushCur][1][0][2] );
					//g_cCoverage[iFlushPrev][iFlushCur][1][0][3],
					//g_cCoverage[iFlushPrev][iFlushCur][1][0][4] );
			//printf( "%-15s        2    1 | %5d  %9d  %9d  %7d  %7d\n",
			printf( "%-15s        2    1 | %5d  %7d  %7d\n",
					szSequence,
					g_cCoverage[iFlushPrev][iFlushCur][1][1][0],
					g_cCoverage[iFlushPrev][iFlushCur][1][1][1],
					g_cCoverage[iFlushPrev][iFlushCur][1][1][2] );
					//g_cCoverage[iFlushPrev][iFlushCur][1][1][3],
					//g_cCoverage[iFlushPrev][iFlushCur][1][1][4] );

			//printf( "%-15s        3    0 | %5d  %9d  %9d  %7d  %7d\n",
			printf( "%-15s        3    0 | %5d  %7d  %7d\n",
					szSequence,
					g_cCoverage[iFlushPrev][iFlushCur][2][0][0],
					g_cCoverage[iFlushPrev][iFlushCur][2][0][1],
					g_cCoverage[iFlushPrev][iFlushCur][2][0][2] );
					//g_cCoverage[iFlushPrev][iFlushCur][2][0][3],
					//g_cCoverage[iFlushPrev][iFlushCur][2][0][4] );
			//printf( "%-15s        3    1 | %5d  %9d  %9d  %7d  %7d\n",
			printf( "%-15s        3    1 | %5d  %7d  %7d\n",
					szSequence,
					g_cCoverage[iFlushPrev][iFlushCur][2][1][0],
					g_cCoverage[iFlushPrev][iFlushCur][2][1][1],
					g_cCoverage[iFlushPrev][iFlushCur][2][1][2] );
					//g_cCoverage[iFlushPrev][iFlushCur][2][1][3],
					//g_cCoverage[iFlushPrev][iFlushCur][2][1][4] );

			//printf( "%-15s        4    0 | %5d  %9d  %9d  %7d  %7d\n",
			printf( "%-15s        4    0 | %5d  %7d  %7d\n",
					szSequence,
					g_cCoverage[iFlushPrev][iFlushCur][3][0][0],
					g_cCoverage[iFlushPrev][iFlushCur][3][0][1],
					g_cCoverage[iFlushPrev][iFlushCur][3][0][2] );
					//g_cCoverage[iFlushPrev][iFlushCur][3][0][3],
					//g_cCoverage[iFlushPrev][iFlushCur][3][0][4] );
			//printf( "%-15s        4    1 | %5d  %9d  %9d  %7d  %7d\n",
			printf( "%-15s        4    1 | %5d  %7d  %7d\n",
					szSequence,
					g_cCoverage[iFlushPrev][iFlushCur][3][1][0],
					g_cCoverage[iFlushPrev][iFlushCur][3][1][1],
					g_cCoverage[iFlushPrev][iFlushCur][3][1][2] );
					//g_cCoverage[iFlushPrev][iFlushCur][3][1][3],
					//g_cCoverage[iFlushPrev][iFlushCur][3][1][4] );
			}
		}

	AssertSzRTL( fFalse, "TestLogPatch() is complete -- you may want to break in with the debugger to verify internal state" );
	}


#endif	//	LOGPATCH_UNIT_TEST


//	check the log, searching for the last log record and seeing whether or not it is a term record
//	this will patch any damange to the log as well (when it can)

// Implicit output parameters:
//		m_pbLastChecksum
//		m_fAbruptEnd	(UNDONE)
//		m_pbWrite
//		m_isecWrite
//		m_pbEntry

// At the end of this function, we should have done any necessary fix-ups
// to the log file. i.e. writing a shadow sector that didn't get written,
// writing over a corrupted regular data sector.

ERR LOG::ErrLGCheckReadLastLogRecordFF(	IFileSystemAPI *const	pfsapi, 
										BOOL 					*pfCloseNormally, 
										const BOOL 				fReadOnly,
										BOOL					*pfIsPatchable )
	{
	ERR 			err;
	LRCHECKSUM		*plrck;
	LGPOS			lgposCurrent;
	LGPOS			lgposLast;
	LGPOS			lgposEnd;
	LGPOS			lgposNext;
	BYTE			*pbEnsure;
	BYTE			*pbLastSector;
	BYTE			*pbLastChecksum;
	UINT			isecLastSector;
	ULONG			isecPatternFill;
	ULONG			csecPatternFill;
	LogReader		*plread;
	BOOL			fGotQuit;
	BOOL			fCreatedLogReader;
	BOOL			fLogToNextLogFile;
	BOOL			fDoScan;
	BOOL			fOldRecovering;
	RECOVERING_MODE fOldRecoveringMode;
	BOOL			fRecordOkRangeBad;
	LGPOS			lgposScan;
	BOOL			fJetLog;
	BOOL			fTookJump;
	BOOL			fSingleSectorTornWrite;
	BOOL			fSkipScanAndApplyPatch;
	ULONG			cbChecksum;
#ifdef ENABLE_LOGPATCH_TRACE
	CPRINTFFILE		*pcprintfLogPatch = NULL;
	CHAR			szLogPatchPath[ IFileSystemAPI::cchPathMax ];
#endif	//	ENABLE_LOGPATCH_TRACE
//
//	SEARCH-STRING: SecSizeMismatchFixMe
//
//	BOOL			fWriteOnSectorSizeMismatch;

	//	we must have an initialized volume sector size

	Assert( m_cbSecVolume != ~(ULONG)0 );
	Assert( m_cbSecVolume % 512 == 0 );

	//	we should not be probing for a torn-write unless we are dumping logs

	Assert( !pfIsPatchable || m_fDumppingLogs );

	//	initialize variables

	err							= JET_errSuccess;	//	assume success
	plrck 						= pNil;				//	LRCHECKSUM class pointer
	lgposCurrent.ib				= 0;				//	what we are looking at right now
	lgposCurrent.isec			= USHORT( m_csecHeader );
	lgposCurrent.lGeneration	= m_plgfilehdr->lgfilehdr.le_lGeneration;
	lgposLast.ib				= 0;				//	the last LRCHECKSUM we saw 
	lgposLast.isec				= 0;
	lgposLast.lGeneration		= m_plgfilehdr->lgfilehdr.le_lGeneration;
	lgposEnd.ib					= 0;				//	position after the end of logical data
	lgposEnd.isec				= 0;
	lgposEnd.lGeneration		= m_plgfilehdr->lgfilehdr.le_lGeneration;
	pbEnsure 					= pbNil;			//	start of data we're looking at in the log buffer
	pbLastSector				= pbNil;			//	separate page with copy of last sector of log data
	pbLastChecksum				= pbNil;			//	offset of the LRCHECKSUM within pbLastSector
	isecLastSector				= m_csecHeader;		//	sector offset within log file of data in pbLastSector
	isecPatternFill				= 0;				//	sector to start rewriting the logfile extend pattern
	csecPatternFill				= 0;				//	number of sectors of logfile extent pattern to rewrite
	plread						= pNil;				//	LogReader class pointer
	fDoScan						= fFalse;			//	perform a scan for corruption/torn-write
	fGotQuit					= fFalse;			//	found lrtypQuit or lrtypTerm? ("clean-log" flag)
	fCreatedLogReader			= fFalse;			//	did we allocate a LogReader or are we sharing one?
	fLogToNextLogFile			= fFalse;			//	should we start a new gen. or continue the old one?
	fRecordOkRangeBad			= fFalse;			//	true when the LRCK recors is OK in itself, but its 
													//		range has a bad checksum value
	lgposScan.ib				= 0;				//	position from which to scan for corruption/torn-writes
	lgposScan.isec				= 0;
	lgposScan.lGeneration		= m_plgfilehdr->lgfilehdr.le_lGeneration;
	fTookJump					= fFalse;
	fSingleSectorTornWrite		= fFalse;			//	set when we have a single-sector torn-write
	fSkipScanAndApplyPatch		= fFalse;			//	see where this is set for details about it
	cbChecksum					= 0;				//	number of bytes involved in the last bad checksum
//
//	SEARCH-STRING: SecSizeMismatchFixMe
//
//	fWriteOnSectorSizeMismatch	= fFalse;			//	tried to write to the log using a different sector size

#ifdef ENABLE_LOGPATCH_TRACE

	//	compute the path for the trace log

	{
	CHAR	szFolder[ IFileSystemAPI::cchPathMax ];
	CHAR	szT[ IFileSystemAPI::cchPathMax ];

	if (	pfsapi->ErrPathParse( m_szLogName, szFolder, szT, szT ) < JET_errSuccess ||
			pfsapi->ErrPathBuild( szFolder, "LOGPATCH", "TXT", szLogPatchPath ) < JET_errSuccess )
		{
		OSSTRCopyA( szLogPatchPath, "LOGPATCH.TXT" );
		}
	}
#endif	//	ENABLE_LOGPATCH_TRACE

	if ( pfIsPatchable )
		{
		*pfIsPatchable = fFalse;
		}

	//	mark the end of data for redo time

	m_lgposLastRec = lgposCurrent;

	//	lock the log-flush thread

	m_critLGFlush.Enter();

	//	are we looking at edb.log?

	{
		CHAR szT[IFileSystemAPI::cchPathMax], szFNameT[IFileSystemAPI::cchPathMax];

		CallS( pfsapi->ErrPathParse( m_szLogName, szT, szFNameT, szT ) );
		fJetLog = ( UtilCmpFileName( szFNameT, m_szJet ) == 0 );
	}

	//	save the old state

	fOldRecovering = m_fRecovering;
	fOldRecoveringMode = m_fRecoveringMode;

	// doing what we're doing here is similar to recovery in redo mode.
	// Quell UlComputeChecksum()'s checking to see if we're in m_critLGBuf

	m_fRecovering = fTrue;
	m_fRecoveringMode = fRecoveringRedo;

	//	allocate a LogReader class (coming from dump code) or share the existing one (coming from redo code)

	if ( pNil != m_plread )
		{

		//	someone has a log reader setup already 
		//		(must be the recovery code before redoing operations -- not the dump code)
		//		(see ErrLGRRedo(); the first call to this function)

		//	this should only be done by recovery

		Assert( fOldRecovering );

		//	we should not be in read/write mode

		Assert( !fReadOnly );

		plread = m_plread;
		fCreatedLogReader = fFalse;

		//	reset it so we get fresh data from disk

		plread->Reset();

#ifdef DEBUG

		//	make sure the log buffer is large enough for the whole file

		Assert( m_pfapiLog );
		QWORD	cbSize;
		Call( m_pfapiLog->ErrSize( &cbSize ) );

		Assert( m_cbSec > 0 );
		Assert( ( cbSize % m_cbSec ) == 0 );
		UINT	csecSize;
		csecSize = UINT( cbSize / m_cbSec );
		Assert( csecSize > m_csecHeader );

#ifdef AFOXMAN_FIX_148537
		Assert( m_csecLGBuf >= csecSize );
#else	//	!AFOXMAN_FIX_148537
		Assert( m_csecLGBuf <= csecSize );
#endif	//	AFOXMAN_FIX_148537
#endif	//	DEBUG
		}
	else
		{

		//	no one has allocated a log reader yet 
		//		(must be either the recovery code after redoing operation, or the dump code)
		//		(after redo ==> see ErrLGRRedo(); the second call to this function)
		//		(dump code ==> see dump.cxx)

		//	create a log reader

		plread = new LogReader();
		if ( pNil == plread )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		fCreatedLogReader = fTrue;

		//	get the size of the log in sectors

		Assert( m_pfapiLog );
		QWORD	cbSize;
		Call( m_pfapiLog->ErrSize( &cbSize ) );

		Assert( m_cbSec > 0 );
		Assert( ( cbSize % m_cbSec ) == 0 );
		UINT	csecSize;
		csecSize = UINT( cbSize / m_cbSec );
		Assert( csecSize > m_csecHeader );

		//	initialize the log reader

		Call( plread->ErrLReaderInit( this, csecSize ) );
		}

	//	assume the result will not be a cleanly shutdown log generation
	//	   (e.g. we expect not to find an lrtypTerm or lrtypQuit record)

	Assert( pfCloseNormally );
	*pfCloseNormally = fFalse;

#ifndef LOGPATCH_UNIT_TEST

	//	re-open the log in read/write mode if requested by the caller

	Assert( m_pfapiLog );
	if ( !fReadOnly )
		{
		delete m_pfapiLog;
		m_pfapiLog = NULL;
		Assert( NULL != m_szLogName );
		Assert( '\0' != m_szLogName[ 0 ] );
		Call( pfsapi->ErrFileOpen( m_szLogName, &m_pfapiLog ) );
		}

#endif	//	!LOGPATCH_UNIT_TEST

	//	make sure we have the right log file

	Call( plread->ErrEnsureLogFile() );

	//	allocate memory for pbLastSector
	
	pbLastSector = reinterpret_cast< BYTE* >( PvOSMemoryHeapAlloc( m_cbSec ) );
	if ( pbNil == pbLastSector )
		{
		Call( ErrERRCheck( JET_errOutOfMemory ) );
		}

	// XXX
	// gross temp hack so that entire log buffer is considered in use,
	// so Assert()s don't fire off like crazy, especially in UlComputeChecksum().
	//
	//	??? what is Spencer's comment (above) referring to ???

	//	loop forever while scanning this log generation

	forever
		{

		//	read the first sector and point to the LRCHECKSUM record

		Call( plread->ErrEnsureSector( lgposCurrent.isec, 1, &pbEnsure ) );
		plrck = reinterpret_cast< LRCHECKSUM* >( pbEnsure + lgposCurrent.ib );

		//	we are about to check data in the sector containing the LRCHECKSUM record
		//	we may checksum the whole sector, or we may just verify the LRCHECKSUM record itself through heuristics

		cbChecksum = m_cbSec;

		//	check the validity of the LRCHECKSUM (does not verify the checksum value!)

		if ( !FValidLRCKRecord( plrck, &lgposCurrent ) )
			{

#ifdef ENABLE_LOGPATCH_TRACE
			if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
				{
				ULONG _ibdumpT_;

				(*pcprintfLogPatch)( "invalid LRCK record in logfile %s\r\n", m_szLogName );

				(*pcprintfLogPatch)( "\r\n\tdumping state of ErrLGCheckReadLastLogRecordFF:\r\n" );
				(*pcprintfLogPatch)( "\t\terr                    = %d \r\n", err );
				(*pcprintfLogPatch)( "\t\tlgposCurrent           = {0x%x,0x%x,0x%x}\r\n", lgposCurrent.lGeneration, lgposCurrent.isec, lgposCurrent.ib );
				(*pcprintfLogPatch)( "\t\tlgposLast              = {0x%x,0x%x,0x%x}\r\n", lgposLast.lGeneration, lgposLast.isec, lgposLast.ib  );
				(*pcprintfLogPatch)( "\t\tlgposEnd               = {0x%x,0x%x,0x%x}\r\n", lgposEnd.lGeneration, lgposEnd.isec, lgposEnd.ib );
				(*pcprintfLogPatch)( "\t\tisecLastSector         = %d\r\n", isecLastSector );
				(*pcprintfLogPatch)( "\t\tisecPatternFill        = 0x%x\r\n", isecPatternFill );
				(*pcprintfLogPatch)( "\t\tcsecPatternFill        = 0x%x\r\n", csecPatternFill );
				(*pcprintfLogPatch)( "\t\tfGotQuit               = %s\r\n", ( fGotQuit ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfCreatedLogReader      = %s\r\n", ( fCreatedLogReader ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfLogToNextLogFile      = %s\r\n", ( fLogToNextLogFile ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfDoScan                = %s\r\n", ( fDoScan ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfOldRecovering         = %s\r\n", ( fOldRecovering ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfOldRecoveringMode     = %d\r\n", fOldRecoveringMode );
				(*pcprintfLogPatch)( "\t\tfRecordOkRangeBad      = %s\r\n", ( fRecordOkRangeBad ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tlgposScan              = {0x%x,0x%x,0x%x}\r\n", lgposScan.lGeneration, lgposScan.isec, lgposScan.ib );
				(*pcprintfLogPatch)( "\t\tfJetLog                = %s\r\n", ( fJetLog ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfTookJump              = %s\r\n", ( fTookJump ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfSingleSectorTornWrite = %s\r\n", ( fSingleSectorTornWrite ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfSkipScanAndApplyPatch = %s\r\n", ( fSkipScanAndApplyPatch ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tcbChecksum             = 0x%x\r\n", cbChecksum );

				(*pcprintfLogPatch)( "\r\n\tdumping partial state of LOG:\r\n" );
				(*pcprintfLogPatch)( "\t\tLOG::m_fRecovering     = %s\r\n", ( m_fRecovering ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tLOG::m_fRecoveringMode = %d\r\n", m_fRecoveringMode );
				(*pcprintfLogPatch)( "\t\tLOG::m_fHardRestore    = %s\r\n", ( m_fHardRestore ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tLOG::m_fRestoreMode    = %d\r\n", m_fRestoreMode );
				(*pcprintfLogPatch)( "\t\tLOG::m_csecLGFile      = 0x%08x\r\n", m_csecLGFile );
				(*pcprintfLogPatch)( "\t\tLOG::m_csecLGBuf       = 0x%08x\r\n", m_csecLGBuf );
				(*pcprintfLogPatch)( "\t\tLOG::m_csecHeader      = %d\r\n", m_csecHeader );
				(*pcprintfLogPatch)( "\t\tLOG::m_cbSec           = %d\r\n", m_cbSec );
				(*pcprintfLogPatch)( "\t\tLOG::m_cbSecVolume     = %d\r\n", m_cbSecVolume );
				(*pcprintfLogPatch)( "\t\tLOG::m_pbLGBufMin      = 0x%0*I64x\r\n", sizeof( char* ) * 2, QWORD( m_pbLGBufMin ) );
				(*pcprintfLogPatch)( "\t\tLOG::m_pbLGBufMax      = 0x%0*I64x\r\n", sizeof( char* ) * 2, QWORD( m_pbLGBufMax ) );
				(*pcprintfLogPatch)( "\t\tLOG::m_pbWrite         = 0x%0*I64x\r\n", sizeof( char* ) * 2, QWORD( m_pbWrite ) );
				(*pcprintfLogPatch)( "\t\tLOG::m_pbEntry         = 0x%0*I64x: 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\r\n", sizeof( char* ) * 2, QWORD( m_pbEntry ), m_pbEntry[0], m_pbEntry[1], m_pbEntry[2], m_pbEntry[3], m_pbEntry[4], m_pbEntry[5], m_pbEntry[6], m_pbEntry[7] );
				(*pcprintfLogPatch)( "\t\tLOG::m_isecWrite       = 0x%08x\r\n", m_isecWrite );

				(*pcprintfLogPatch)( "\r\n\tdumping data:\r\n" );

				(*pcprintfLogPatch)( "\t\tplrck (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( plrck ) );
				(*pcprintfLogPatch)( "\t\t  cbBackwards       = 0x%08x\r\n", plrck->le_cbBackwards );
				(*pcprintfLogPatch)( "\t\t  cbForwards        = 0x%08x\r\n", plrck->le_cbForwards );
				(*pcprintfLogPatch)( "\t\t  cbNext            = 0x%08x\r\n", plrck->le_cbNext );
				(*pcprintfLogPatch)( "\t\t  ulChecksum        = 0x%08x\r\n", plrck->le_ulChecksum );
				(*pcprintfLogPatch)( "\t\t  ulShortChecksum   = 0x%08x\r\n", plrck->le_ulShortChecksum );
				(*pcprintfLogPatch)( "\t\t  bUseShortChecksum = %s (0x%02x)\r\n", 
									( bShortChecksumOn == plrck->bUseShortChecksum ?
									  "Yes" : 
									  ( bShortChecksumOff == plrck->bUseShortChecksum ?
									    "No" : "???" ) ),
									BYTE( plrck->bUseShortChecksum ) );

				(*pcprintfLogPatch)( "\t\tpbEnsure (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( pbEnsure ) );

				_ibdumpT_ = 0;
				while ( _ibdumpT_ < m_cbSec )
					{
					(*pcprintfLogPatch)( "\t\t0x%04x: %02x %02x %02x %02x %02x %02x %02x %02x-%02x %02x %02x %02x %02x %02x %02x %02x\r\n",
										_ibdumpT_,
										pbEnsure[_ibdumpT_+0],  pbEnsure[_ibdumpT_+1],  pbEnsure[_ibdumpT_+2],  pbEnsure[_ibdumpT_+3],
										pbEnsure[_ibdumpT_+4],  pbEnsure[_ibdumpT_+5],  pbEnsure[_ibdumpT_+6],  pbEnsure[_ibdumpT_+7],
										pbEnsure[_ibdumpT_+8],  pbEnsure[_ibdumpT_+9],  pbEnsure[_ibdumpT_+10], pbEnsure[_ibdumpT_+11],
										pbEnsure[_ibdumpT_+12], pbEnsure[_ibdumpT_+13], pbEnsure[_ibdumpT_+14], pbEnsure[_ibdumpT_+15] );
					_ibdumpT_ += 16;
					Assert( _ibdumpT_ <= m_cbSec );
					}

				(*pcprintfLogPatch)( "\t\tpbLastSector (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( pbLastSector ) );

				_ibdumpT_ = 0;
				while ( _ibdumpT_ < m_cbSec )
					{
					(*pcprintfLogPatch)( "\t\t0x%04x: %02x %02x %02x %02x %02x %02x %02x %02x-%02x %02x %02x %02x %02x %02x %02x %02x\r\n",
										_ibdumpT_,
										pbLastSector[_ibdumpT_+0],  pbLastSector[_ibdumpT_+1],  pbLastSector[_ibdumpT_+2],  pbLastSector[_ibdumpT_+3],
										pbLastSector[_ibdumpT_+4],  pbLastSector[_ibdumpT_+5],  pbLastSector[_ibdumpT_+6],  pbLastSector[_ibdumpT_+7],
										pbLastSector[_ibdumpT_+8],  pbLastSector[_ibdumpT_+9],  pbLastSector[_ibdumpT_+10], pbLastSector[_ibdumpT_+11],
										pbLastSector[_ibdumpT_+12], pbLastSector[_ibdumpT_+13], pbLastSector[_ibdumpT_+14], pbLastSector[_ibdumpT_+15] );
					_ibdumpT_ += 16;
					Assert( _ibdumpT_ <= m_cbSec );
					}

				if ( pbLastChecksum )
					{
					LRCHECKSUM *_plrckT_ = (LRCHECKSUM *)pbLastChecksum;

					(*pcprintfLogPatch)( "\t\tpbLastChecksum (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( _plrckT_ ) );
					(*pcprintfLogPatch)( "\t\t  cbBackwards       = 0x%08x\r\n", _plrckT_->le_cbBackwards );
					(*pcprintfLogPatch)( "\t\t  cbForwards        = 0x%08x\r\n", _plrckT_->le_cbForwards );
					(*pcprintfLogPatch)( "\t\t  cbNext            = 0x%08x\r\n", _plrckT_->le_cbNext );
					(*pcprintfLogPatch)( "\t\t  ulChecksum        = 0x%08x\r\n", _plrckT_->le_ulChecksum );
					(*pcprintfLogPatch)( "\t\t  ulShortChecksum   = 0x%08x\r\n", _plrckT_->le_ulShortChecksum );
					(*pcprintfLogPatch)( "\t\t  bUseShortChecksum = %s (0x%02x)\r\n", 
										( bShortChecksumOn == _plrckT_->bUseShortChecksum ?
										  "Yes" : 
										  ( bShortChecksumOff == _plrckT_->bUseShortChecksum ?
										    "No" : "???" ) ),
										BYTE( _plrckT_->bUseShortChecksum ) );
					}
				else
					{
					(*pcprintfLogPatch)( "\t\tpbLastChecksum (null)\r\n" );
					}

				(*pcprintfLogPatch)( "\r\n" );
				}
#endif	//	ENABLE_LOGPATCH_TRACE

			//	remember that we got here by not having a valid record
			
			fTookJump = fFalse;

RecoverWithShadow:

			//	the LRCHECKSUM is invalid; revert to the shadow sector (if there is one)

			//	read the shadow sector and point to the LRCHECKSUM record
			
			Call( plread->ErrEnsureSector( lgposCurrent.isec + 1, 1, &pbEnsure ) );
			plrck = reinterpret_cast< LRCHECKSUM* >( pbEnsure + lgposCurrent.ib );

			//	check the validity of the shadowed LRCHECKSUM including the checksum value
			//		(we can verify the checksum value here because we know the data doesn't go
			//		 past the end of the shadow sector -- thus, we don't need to bring in more data)

			if ( !FValidLRCKShadow( plrck, &lgposCurrent, m_plgfilehdr->lgfilehdr.le_lGeneration ) )
				{
#ifdef ENABLE_LOGPATCH_TRACE
				if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
					{
					(*pcprintfLogPatch)( "invalid shadow in sector 0x%x (%d)\r\n", 
										lgposCurrent.isec + 1,
										lgposCurrent.isec + 1 );

					(*pcprintfLogPatch)( "\t\tpbEnsure (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( pbEnsure ) );

					ULONG _ibdumpT_ = 0;
					while ( _ibdumpT_ < m_cbSec )
						{
						(*pcprintfLogPatch)( "\t\t0x%04x: %02x %02x %02x %02x %02x %02x %02x %02x-%02x %02x %02x %02x %02x %02x %02x %02x\r\n",
											_ibdumpT_,
											pbEnsure[_ibdumpT_+0],  pbEnsure[_ibdumpT_+1],  pbEnsure[_ibdumpT_+2],  pbEnsure[_ibdumpT_+3],
											pbEnsure[_ibdumpT_+4],  pbEnsure[_ibdumpT_+5],  pbEnsure[_ibdumpT_+6],  pbEnsure[_ibdumpT_+7],
											pbEnsure[_ibdumpT_+8],  pbEnsure[_ibdumpT_+9],  pbEnsure[_ibdumpT_+10], pbEnsure[_ibdumpT_+11],
											pbEnsure[_ibdumpT_+12], pbEnsure[_ibdumpT_+13], pbEnsure[_ibdumpT_+14], pbEnsure[_ibdumpT_+15] );
						_ibdumpT_ += 16;
						Assert( _ibdumpT_ <= m_cbSec );
						}
					}
#endif	//	ENABLE_LOGPATCH_TRACE

				//	the shadowed LRCHECKSUM is also invalid

				//	one special case exists where we can still be ok in this situation:
				//		if the last LRCHECKSUM range (which was validated previously) covers 1 sector,
				//		and it was the last flush because we terminated abruptly, the disk would
				//		contain that last LRCHECKSUM record with a shadowed copy of it, followed by 
				//		the log-extend pattern; this is a TORN-WRITE case, but the shadow is getting
				//		in the way and making it hard for us to see things clearly

				//	we must have an invalid LRCHECKSUM record and the last sector must be 1 away 
				//		from the current sector

				if ( !fTookJump && lgposCurrent.isec == lgposLast.isec + 1 )
					{

					//	make sure we have a valid lgposLast -- we should

					Assert( lgposLast.isec >= m_csecHeader && lgposLast.isec < ( m_csecLGFile - 1 ) );
					
					//	load data from the previous sector and the current sector

					Call( plread->ErrEnsureSector( lgposLast.isec, 2, &pbEnsure ) );

					//	set the LRCHECKSUM pointer

					plrck = (LRCHECKSUM *)( pbEnsure + lgposLast.ib );

					//	make sure things are as they should be

					AssertValidLRCKRecord( plrck, &lgposLast );
					AssertValidLRCKRange( plrck, lgposLast.lGeneration );
					Assert( plrck->bUseShortChecksum == bShortChecksumOff );
					Assert( plrck->le_cbNext != 0 );
					Assert( lgposLast.ib + sizeof( LRCHECKSUM ) + plrck->le_cbForwards == m_cbSec );

					//	move to the supposedly shadowed LRCHECKSUM record

					plrck = (LRCHECKSUM *)( pbEnsure + m_cbSec + lgposLast.ib );

					//	see if this is in fact a valid shadow sector (use the special shadow validation)

					if ( FValidLRCKShadowWithoutCheckingCBNext( plrck, &lgposLast, lgposLast.lGeneration ) )
						{
#ifdef ENABLE_LOGPATCH_TRACE
						if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
							{
							(*pcprintfLogPatch)( "special case!\r\n" );
							(*pcprintfLogPatch)( "\tlgposCurrent is NOT pointing to the next LRCHECKSUM record because we crashed before we could write it\r\n" );
							(*pcprintfLogPatch)( "\tinstead, it is pointing to a shadow of the >>LAST<< LRCHECKSUM record (lgposLast)\r\n" );
							}
#endif	//	ENABLE_LOGPATCH_TRACE

						//	indeed we are looking at a shadow sector! this means we have torn-write
						//	set things up for a special scan to make sure we do have the pattern everywhere

						//	this should only happen in edb.log -- all other logs should be clean or corrupt
						//		(they should never have a torn-write)

						Assert( fJetLog );

						//	setup for a scan

						fDoScan = fTrue;

						//	mark the end of data as the start of the shadow sector

						lgposEnd.isec = lgposCurrent.isec;
						lgposEnd.ib = 0;

						//	prepare a special scan which starts after the shadow sector

						lgposScan = lgposEnd;
						AddLgpos( &lgposScan, m_cbSec );

						//	mark the end of data for redo time

						m_lgposLastRec = lgposEnd;

						//	prepare pbLastSector with data for the next flush which will occur after we
						//		exit the "forever" loop

						//	copy the last LRCHECKSUM record 
				
						UtilMemCpy( pbLastSector, pbEnsure, m_cbSec );

						//	set the pointers to the copy of the sector with the LRCHECKSUM record
				
						pbLastChecksum = pbLastSector + lgposLast.ib;
						isecLastSector = lgposLast.isec;

						//	remember that this has the potential to be a single-sector torn-write

						fSingleSectorTornWrite = fTrue;

						//	terminate the "forever" loop

						break;
						}
					}


				//	we need to do a scan to determine what kind of damage occurred

				fDoScan = fTrue;

				//	NOTE: since we haven't found a good LRCHECKSUM record, 
				//		lgposLast remains pointing to the last good LRCHECKSUM record we found 
				//		(unless we haven't seen even one, in which case lgposLast.isec == 0)

				//	set lgposEnd to indicate that the garbage to scan starts at the current sector, offset 0

				lgposEnd.isec = lgposCurrent.isec;
				lgposEnd.ib = 0;

				//	start the scan at lgposEnd

				lgposScan = lgposEnd;
				
				//	mark the end of data for redo time

				m_lgposLastRec = lgposEnd;

				//	prepare pbLastSector with data for the next flush which will occur after we
				//		exit the "forever" loop

				pbLastChecksum = pbLastSector + lgposCurrent.ib;

//
//	LEAVE THE OLD DATA THERE! DO NOT FILL THE SPACE WITH THE PATTERN!
//
//				// fill with known value, so if we ever use the filled data we'll know it (hopefully)
//
//				Assert( cbLogExtendPattern > lgposCurrent.ib );
//				Assert( rgbLogExtendPattern );
//				memcpy( pbLastSector, rgbLogExtendPattern, lgposCurrent.ib );

				//	prepare the new LRCHECKSUM record
				
				plrck = reinterpret_cast< LRCHECKSUM* >( pbLastChecksum );
				memset( pbLastChecksum, 0, sizeof( LRCHECKSUM ) );
				plrck->lrtyp = lrtypChecksum;
				plrck->bUseShortChecksum = bShortChecksumOff;

				//	NOTE: we leave the backward pointer set to 0 to cover the case where the head of
				//			  a partial log record was written in the last sector, but the tail of that
				//			  record was never written safely to this sector (we just filled the space
				//			  where the tail was with log-extend pattern)
				//		  the partial log record will be ignored when we scan record by record later on
				//			  in this function
				
				isecLastSector = lgposCurrent.isec;

				//	terminate the "forever" loop

				break;
				}

#ifdef ENABLE_LOGPATCH_TRACE
			if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
				{
				(*pcprintfLogPatch)( "shadow is OK -- we will patch with it (unless we are in read-only mode)\r\n" );

				(*pcprintfLogPatch)( "\r\n\t\tpbEnsure (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( pbEnsure ) );

				ULONG _ibdumpT_ = 0;
				while ( _ibdumpT_ < m_cbSec )
					{
					(*pcprintfLogPatch)( "\t\t0x%04x: %02x %02x %02x %02x %02x %02x %02x %02x-%02x %02x %02x %02x %02x %02x %02x %02x\r\n",
										_ibdumpT_,
										pbEnsure[_ibdumpT_+0],  pbEnsure[_ibdumpT_+1],  pbEnsure[_ibdumpT_+2],  pbEnsure[_ibdumpT_+3],
										pbEnsure[_ibdumpT_+4],  pbEnsure[_ibdumpT_+5],  pbEnsure[_ibdumpT_+6],  pbEnsure[_ibdumpT_+7],
										pbEnsure[_ibdumpT_+8],  pbEnsure[_ibdumpT_+9],  pbEnsure[_ibdumpT_+10], pbEnsure[_ibdumpT_+11],
										pbEnsure[_ibdumpT_+12], pbEnsure[_ibdumpT_+13], pbEnsure[_ibdumpT_+14], pbEnsure[_ibdumpT_+15] );
					_ibdumpT_ += 16;
					Assert( _ibdumpT_ <= m_cbSec );
					}
				}
#endif	//	ENABLE_LOGPATCH_TRACE
				
			//	the shadowed LRCHECKSUM is valid and has a good checksum value

			//	this means we are definitely at the end of the log, and we can repair the corruption
			//		using the shadow sector (make repairs quietly except for an event-log message)
			//
			//	NOTE: it doesn't matter which recovery mode we are in or which log file we are dealing
			//		  with -- shadow sectors can ALWAYS be patched!

			Assert( plrck->le_cbNext == 0 );
			Assert( ( plrck->le_cbBackwards + sizeof( LRCHECKSUM ) + plrck->le_cbForwards ) <= m_cbSec );

			//	if we are in read-only mode, we cannot patch anything (dump code uses this mode)

			if ( fReadOnly )
				{

				//	try to make sure this really is the dump code

				Assert( fOldRecovering && fOldRecoveringMode == fRecoveringNone );
				Assert( !m_fHardRestore );
				Assert( fCreatedLogReader );

				//	mark the end of data for redo time

				m_lgposLastRec = lgposCurrent;
				m_lgposLastRec.ib = 0;
				
				if ( pfIsPatchable )
					{

					//	the caller wants to know if the log is patchable
					//	by returning TRUE, we imply that the log is damaged so we don't need to return 
					//		an error as well

					*pfIsPatchable = fTrue;
					err = JET_errSuccess;
					goto HandleError;
					}
				else
					{
					//  report log file corruption to the event log

					const QWORD		ibOffset	= QWORD( lgposCurrent.isec ) * QWORD( m_cbSec );
					const DWORD		cbLength	= cbChecksum;
					
					const _TCHAR*	rgpsz[ 4 ];
					DWORD			irgpsz		= 0;
					_TCHAR			szAbsPath[ IFileSystemAPI::cchPathMax ];
					_TCHAR			szOffset[ 64 ];
					_TCHAR			szLength[ 64 ];
					_TCHAR			szError[ 64 ];

					if ( m_pfapiLog->ErrPath( szAbsPath ) < JET_errSuccess )
						{
						OSSTRCopy( szAbsPath, _T( "<< cannot get filepath >>" ) );
						}
					_stprintf( szOffset, _T( "%I64i (0x%016I64x)" ), ibOffset, ibOffset );
					_stprintf( szLength, _T( "%u (0x%08x)" ), cbLength, cbLength );
					_stprintf( szError, _T( "%i (0x%08x)" ), JET_errLogFileCorrupt, JET_errLogFileCorrupt );

					rgpsz[ irgpsz++ ]	= szAbsPath;
					rgpsz[ irgpsz++ ]	= szOffset;
					rgpsz[ irgpsz++ ]	= szLength;
					rgpsz[ irgpsz++ ]	= szError;


					UtilReportEvent(	eventError,
										LOGGING_RECOVERY_CATEGORY,
										LOG_RANGE_CHECKSUM_MISMATCH_ID,
										irgpsz,
										rgpsz,
										0,
										NULL,
										m_pinst );
										
					//	return corruption

					Call( ErrERRCheck( JET_errLogFileCorrupt ) );
					}
				}

			//	do not scan since we are going to repair the corruption

			fDoScan = fFalse;

			//	send the event-log message

			const ULONG	csz = 1;
			const CHAR 	*rgpsz[csz] = { m_szLogName };
				
			UtilReportEvent(	eventWarning,
								LOGGING_RECOVERY_CATEGORY, 
								LOG_USING_SHADOW_SECTOR_ID, 
								csz, 
								rgpsz,
								0,
								NULL,
								m_pinst );

			//	before patching the original LRCHECKSUM record with the shadowed LRCHECKSUM record,
			//		recalculate the checksum value since shadowed checksum values are different

			plrck->le_ulChecksum = UlComputeChecksum( plrck, m_plgfilehdr->lgfilehdr.le_lGeneration );

			//	patch the original LRCHECKSUM record

//
//	SEARCH-STRING: SecSizeMismatchFixMe
//
Assert( m_cbSec == m_cbSecVolume );
//			if ( m_cbSec == m_cbSecVolume )
//				{
				CallJ( m_pfapiLog->ErrIOWrite( m_cbSec * QWORD( lgposCurrent.isec ), m_cbSec, pbEnsure ), LHandleErrorWrite );
				plread->SectorModified( lgposCurrent.isec, pbEnsure );
//				}
//			else
//				{
//				fWriteOnSectorSizeMismatch = fTrue;
//				}

			//	check for data after the LRCHECKSUM (at most, it will run to the end of the sector)

			if ( plrck->le_cbForwards == 0 )
				{

				//	no forward data was found
				//
				//	in this case, we want to search for the lrtypTerm and/or lrtypRecoveryQuit 
				//		log records up to and including the current LRCHECKSUM record
				//		(e.g. the forward range of the last LRCHECKSUM and the backward 
				//			  range of this LRCHECKSUM)
				
				//	to make the search work, we must setup lgposLast and lgposEnd: 
				//		we leave lgposLast (the last valid LRCHECKSUM we found) pointing at the 
				//			previous LRCHECKSUM even though we just patched and made valid
				//			the current LRCHECKSUM at lgposCurrent
				//		we set lgposEnd (the point to stop searching) to the position 
				//			immediately after this LRCHECKSUM since this LRCHECKSUM had no
				//			forward data

				lgposEnd = lgposCurrent;
				AddLgpos( &lgposEnd, sizeof( LRCHECKSUM ) );
				}
			else
				{

				//	forward data was found
				//
				//	in this case, we want to search for the lrtypTerm and/or lrtypRecoveryQuit 
				//		log records within the backward data of the patched LRCHECKSUM

				//	set lgposLast (the last valid LRCHECKSUM record) to the current LRCHECKSUM
					
				lgposLast = lgposCurrent;

				//	set lgposEnd (the point after the last good data) to the first byte after
				//		the forward range of this LRCHECKSUM

				lgposEnd = lgposCurrent;
				Assert( plrck->le_cbBackwards == lgposCurrent.ib || 0 == plrck->le_cbBackwards );
				AddLgpos( &lgposEnd, sizeof( LRCHECKSUM ) + plrck->le_cbForwards );
				}

			//	mark the end of data for redo time

			m_lgposLastRec = lgposEnd;

			//	prepare pbLastSector with data for the next flush which will occur after we
			//		exit the "forever" loop

			//	copy the patched shadow sector (contains the real LRCHECKSUM at this point)
				
			UtilMemCpy( pbLastSector, pbEnsure, m_cbSec );

			//	set the pointers to the copy of the sector with the patched LRCHECKSUM
				
			pbLastChecksum = pbLastSector + lgposCurrent.ib;
			isecLastSector = lgposCurrent.isec;	

			//	terminate the "forever" loop

			break;
			}


		//	at this point, the LRCHECKSUM record is definitely valid
		//	however, we still need to verify the checksum value over its range

		//	load the rest of the data in the range of this LRCHECKSUM

		Call( plread->ErrEnsureSector( lgposCurrent.isec, ( ( lgposCurrent.ib +
			sizeof( LRCHECKSUM ) + plrck->le_cbForwards - 1 ) / m_cbSec ) + 1,
			&pbEnsure ) );

		//	adjust the pointer to the current LRCHECKSUM record 
		//		(it may have moved when reading in new sectors in ErrEnsureSector )
		
		plrck = reinterpret_cast< LRCHECKSUM* >( pbEnsure + lgposCurrent.ib );

		//	we are about to check data in the range of the LRCHECKSUM record

		cbChecksum = lgposCurrent.ib + sizeof( LRCHECKSUM ) + plrck->le_cbForwards;

		//	verify the checksum value

		if ( !FValidLRCKRange( plrck, m_plgfilehdr->lgfilehdr.le_lGeneration ) )
			{

#ifdef ENABLE_LOGPATCH_TRACE
			if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
				{
				ULONG _ibdumpT_;

				(*pcprintfLogPatch)( "invalid LRCK range in logfile %s\r\n", m_szLogName );

				(*pcprintfLogPatch)( "\r\n\tdumping state of ErrLGCheckReadLastLogRecordFF:\r\n" );
				(*pcprintfLogPatch)( "\t\terr                    = %d \r\n", err );
				(*pcprintfLogPatch)( "\t\tlgposCurrent           = {0x%x,0x%x,0x%x}\r\n", lgposCurrent.lGeneration, lgposCurrent.isec, lgposCurrent.ib );
				(*pcprintfLogPatch)( "\t\tlgposLast              = {0x%x,0x%x,0x%x}\r\n", lgposLast.lGeneration, lgposLast.isec, lgposLast.ib  );
				(*pcprintfLogPatch)( "\t\tlgposEnd               = {0x%x,0x%x,0x%x}\r\n", lgposEnd.lGeneration, lgposEnd.isec, lgposEnd.ib );
				(*pcprintfLogPatch)( "\t\tisecLastSector         = %d\r\n", isecLastSector );
				(*pcprintfLogPatch)( "\t\tisecPatternFill        = 0x%x\r\n", isecPatternFill );
				(*pcprintfLogPatch)( "\t\tcsecPatternFill        = 0x%x\r\n", csecPatternFill );
				(*pcprintfLogPatch)( "\t\tfGotQuit               = %s\r\n", ( fGotQuit ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfCreatedLogReader      = %s\r\n", ( fCreatedLogReader ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfLogToNextLogFile      = %s\r\n", ( fLogToNextLogFile ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfDoScan                = %s\r\n", ( fDoScan ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfOldRecovering         = %s\r\n", ( fOldRecovering ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfOldRecoveringMode     = %d\r\n", fOldRecoveringMode );
				(*pcprintfLogPatch)( "\t\tfRecordOkRangeBad      = %s\r\n", ( fRecordOkRangeBad ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tlgposScan              = {0x%x,0x%x,0x%x}\r\n", lgposScan.lGeneration, lgposScan.isec, lgposScan.ib );
				(*pcprintfLogPatch)( "\t\tfJetLog                = %s\r\n", ( fJetLog ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfTookJump              = %s\r\n", ( fTookJump ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfSingleSectorTornWrite = %s\r\n", ( fSingleSectorTornWrite ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tfSkipScanAndApplyPatch = %s\r\n", ( fSkipScanAndApplyPatch ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tcbChecksum             = 0x%x\r\n", cbChecksum );

				(*pcprintfLogPatch)( "\r\n\tdumping partial state of LOG:\r\n" );
				(*pcprintfLogPatch)( "\t\tLOG::m_fRecovering     = %s\r\n", ( m_fRecovering ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tLOG::m_fRecoveringMode = %d\r\n", m_fRecoveringMode );
				(*pcprintfLogPatch)( "\t\tLOG::m_fHardRestore    = %s\r\n", ( m_fHardRestore ? "TRUE" : "FALSE" ) );
				(*pcprintfLogPatch)( "\t\tLOG::m_fRestoreMode    = %d\r\n", m_fRestoreMode );
				(*pcprintfLogPatch)( "\t\tLOG::m_csecLGFile      = 0x%08x\r\n", m_csecLGFile );
				(*pcprintfLogPatch)( "\t\tLOG::m_csecLGBuf       = 0x%08x\r\n", m_csecLGBuf );
				(*pcprintfLogPatch)( "\t\tLOG::m_csecHeader      = %d\r\n", m_csecHeader );
				(*pcprintfLogPatch)( "\t\tLOG::m_cbSec           = %d\r\n", m_cbSec );
				(*pcprintfLogPatch)( "\t\tLOG::m_cbSecVolume     = %d\r\n", m_cbSecVolume );
				(*pcprintfLogPatch)( "\t\tLOG::m_pbLGBufMin      = 0x%0*I64x\r\n", sizeof( char* ) * 2, QWORD( m_pbLGBufMin ) );
				(*pcprintfLogPatch)( "\t\tLOG::m_pbLGBufMax      = 0x%0*I64x\r\n", sizeof( char* ) * 2, QWORD( m_pbLGBufMax ) );
				(*pcprintfLogPatch)( "\t\tLOG::m_pbWrite         = 0x%0*I64x\r\n", sizeof( char* ) * 2, QWORD( m_pbWrite ) );
				(*pcprintfLogPatch)( "\t\tLOG::m_pbEntry         = 0x%0*I64x: 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\r\n", sizeof( char* ) * 2, QWORD( m_pbEntry ), m_pbEntry[0], m_pbEntry[1], m_pbEntry[2], m_pbEntry[3], m_pbEntry[4], m_pbEntry[5], m_pbEntry[6], m_pbEntry[7] );
				(*pcprintfLogPatch)( "\t\tLOG::m_isecWrite       = 0x%08x\r\n", m_isecWrite );

				(*pcprintfLogPatch)( "\r\n\tdumping data:\r\n" );

				(*pcprintfLogPatch)( "\t\tplrck (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( plrck ) );
				(*pcprintfLogPatch)( "\t\t  cbBackwards       = 0x%08x\r\n", plrck->le_cbBackwards );
				(*pcprintfLogPatch)( "\t\t  cbForwards        = 0x%08x\r\n", plrck->le_cbForwards );
				(*pcprintfLogPatch)( "\t\t  cbNext            = 0x%08x\r\n", plrck->le_cbNext );
				(*pcprintfLogPatch)( "\t\t  ulChecksum        = 0x%08x\r\n", plrck->le_ulChecksum );
				(*pcprintfLogPatch)( "\t\t  ulShortChecksum   = 0x%08x\r\n", plrck->le_ulShortChecksum );
				(*pcprintfLogPatch)( "\t\t  bUseShortChecksum = %s (0x%02x)\r\n", 
									( bShortChecksumOn == plrck->bUseShortChecksum ?
									  "Yes" : 
									  ( bShortChecksumOff == plrck->bUseShortChecksum ?
									    "No" : "???" ) ),
									BYTE( plrck->bUseShortChecksum ) );

				(*pcprintfLogPatch)( "\t\tpbEnsure (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( pbEnsure ) );

				_ibdumpT_ = 0;
				while ( _ibdumpT_ < m_cbSec )
					{
					(*pcprintfLogPatch)( "\t\t0x%04x: %02x %02x %02x %02x %02x %02x %02x %02x-%02x %02x %02x %02x %02x %02x %02x %02x\r\n",
										_ibdumpT_,
										pbEnsure[_ibdumpT_+0],  pbEnsure[_ibdumpT_+1],  pbEnsure[_ibdumpT_+2],  pbEnsure[_ibdumpT_+3],
										pbEnsure[_ibdumpT_+4],  pbEnsure[_ibdumpT_+5],  pbEnsure[_ibdumpT_+6],  pbEnsure[_ibdumpT_+7],
										pbEnsure[_ibdumpT_+8],  pbEnsure[_ibdumpT_+9],  pbEnsure[_ibdumpT_+10], pbEnsure[_ibdumpT_+11],
										pbEnsure[_ibdumpT_+12], pbEnsure[_ibdumpT_+13], pbEnsure[_ibdumpT_+14], pbEnsure[_ibdumpT_+15] );
					_ibdumpT_ += 16;
					Assert( _ibdumpT_ <= m_cbSec );
					}

				(*pcprintfLogPatch)( "\t\tpbLastSector (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( pbLastSector ) );

				_ibdumpT_ = 0;
				while ( _ibdumpT_ < m_cbSec )
					{
					(*pcprintfLogPatch)( "\t\t0x%04x: %02x %02x %02x %02x %02x %02x %02x %02x-%02x %02x %02x %02x %02x %02x %02x %02x\r\n",
										_ibdumpT_,
										pbLastSector[_ibdumpT_+0],  pbLastSector[_ibdumpT_+1],  pbLastSector[_ibdumpT_+2],  pbLastSector[_ibdumpT_+3],
										pbLastSector[_ibdumpT_+4],  pbLastSector[_ibdumpT_+5],  pbLastSector[_ibdumpT_+6],  pbLastSector[_ibdumpT_+7],
										pbLastSector[_ibdumpT_+8],  pbLastSector[_ibdumpT_+9],  pbLastSector[_ibdumpT_+10], pbLastSector[_ibdumpT_+11],
										pbLastSector[_ibdumpT_+12], pbLastSector[_ibdumpT_+13], pbLastSector[_ibdumpT_+14], pbLastSector[_ibdumpT_+15] );
					_ibdumpT_ += 16;
					Assert( _ibdumpT_ <= m_cbSec );
					}

				if ( pbLastChecksum )
					{
					LRCHECKSUM *_plrckT_ = (LRCHECKSUM *)pbLastChecksum;

					(*pcprintfLogPatch)( "\t\tpbLastChecksum (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( _plrckT_ ) );
					(*pcprintfLogPatch)( "\t\t  cbBackwards       = 0x%08x\r\n", _plrckT_->le_cbBackwards );
					(*pcprintfLogPatch)( "\t\t  cbForwards        = 0x%08x\r\n", _plrckT_->le_cbForwards );
					(*pcprintfLogPatch)( "\t\t  cbNext            = 0x%08x\r\n", _plrckT_->le_cbNext );
					(*pcprintfLogPatch)( "\t\t  ulChecksum        = 0x%08x\r\n", _plrckT_->le_ulChecksum );
					(*pcprintfLogPatch)( "\t\t  ulShortChecksum   = 0x%08x\r\n", _plrckT_->le_ulShortChecksum );
					(*pcprintfLogPatch)( "\t\t  bUseShortChecksum = %s (0x%02x)\r\n", 
										( bShortChecksumOn == _plrckT_->bUseShortChecksum ?
										  "Yes" : 
										  ( bShortChecksumOff == _plrckT_->bUseShortChecksum ?
										    "No" : "???" ) ),
										BYTE( _plrckT_->bUseShortChecksum ) );
					}
				else
					{
					(*pcprintfLogPatch)( "\t\tpbLastChecksum (null)\r\n" );
					}

				(*pcprintfLogPatch)( "\r\n" );
				}
#endif	//	ENABLE_LOGPATCH_TRACE
			
			//	the checksum over the entire range of the LRCHECKSUM record is wrong

			//	do we have a short checksum?

			if ( plrck->bUseShortChecksum == bShortChecksumOn )
				{

				//	we have a short checksum, so we know that the first sector is valid and that the range
				//		of this LRCHECKSUM record is a multi-sector range

				//	we are either in the middle of the log file (most likely), or we are at the end of a
				//		generation whose last flush was a multisector flush (unlikely -- special case)

				//	in either case, this may really be the end of the log data!
				//		middle of log case: just say the power went out as we were flushing and the first few
				//		  (cbNext != 0)		sectors went to disk (including the LRCHECKSUM), but the rest did not
				//							make it; that would make it look like we are in the middle of the log
				//							when we are really at the end of it
				//		end of log case:    we already know we were at the end of the log!
				//		  (cbNext == 0)

				//	to see if this is the real end of the log, we must scan through data after the "forward" 
				//		range of the current LRCHECKSUM record (lgposCurrent) all the way through to the end 
				//		of the log file
				//	
				//	if we find nothing but the log pattern, we can recover up this point and say that this is 
				//		as far as we can safely go -- the last little bit of data is an incomplete fragment of 
				//		our last flush during whatever caused us to crash, and as far as we are concerned,
				//		it is a casualty of the crash!
				//	if we do find something besides the log pattern, then we know there is some other log data
				//		out there, and we cannot safely say that we have restored as much information as possible

				//	set the flag to perform the corruption/torn-write scan

				fDoScan = fTrue;

				//	set lgposLast to the current LRCHECKSUM record

				lgposLast = lgposCurrent;

				//	set lgposEnd to indicate that the end of good data is the end of the current sector
			
				lgposEnd.isec = lgposCurrent.isec;
				lgposEnd.ib = 0;
				AddLgpos( &lgposEnd, m_cbSec );

				//	mark the end of data for redo time

				m_lgposLastRec = lgposEnd;

				//	start the scan at the end of the data in the range of this LRCHECKSUM record

				lgposScan = lgposCurrent;
				AddLgpos( &lgposScan, sizeof( LRCHECKSUM ) + plrck->le_cbForwards );
				
				//	this should be sector-aligned since it was a multi-sector flush
			
				Assert( lgposScan.ib == 0 );

				//	flag to defer adjusting the forward pointer until later
				//		(also indicates that the flush was a multi-sector range with a bad sector
				//		 somewhere past the first sector)

				fRecordOkRangeBad = fTrue;

				//	NOTE: leave pbLastSector alone for now -- see the comment below to know when it gets updated
			
				//	NOTE: if the corruption is fixable, 
				//			  le_cbForwards will be properly adjusted after the fixup to include data from only
				//				  the current sector
				//			  the fixed up sector will be copied to pbLastSector
				//
				//			  << other thing happen and we eventually get to the cleanup code >>
				//
				//			  the cleanup code will take the last sector and put it onto the log buffers 
				//				  as a single sector marked for the start of the next flush

				//	terminate the "forever" loop

				break;
				}

			//	we do not have a short checksum

			//	make sure the bUseShortChecksum flag is definitely set to OFF
			
			Assert( plrck->bUseShortChecksum == bShortChecksumOff );

			//	SPECIAL CASE: the log may look like this
			//
			//	 /---\  /---\ /---\  /---\
			//	|----|--|----|----|--|----|------------
			//	|???(LRCK)???|???(LRCK)???|  pattern 
			//	|-------|----|-------|----|------------
			//          \--------/   \->NULL
			//
			//	 last sector    shadow(old)
			//
			//
			//	NOTE: the cbNext ptr in "last sector" happens to point to the LRCK in the shadow sector
			//
			//	how do we get to this case?
			//		"last sector" was flushed and shadowed, and we crashed before the next flush could occur
			//	where are our ptrs?
			//		lgposLast is pointing to the checksum in "last sector"
			//		lgposCurrent is pointing to the checksum in "shadow" which has just failed the range checksum

			//	make sure the range of the current sector (SPECIAL CASE or not) is at most 1 sector long

			Assert( lgposCurrent.ib + sizeof( LRCHECKSUM ) + plrck->le_cbForwards <= m_cbSec );

			//	see if we are experiencing the SPECIAL CASE (lgposLast.ib will match lgposCurrent.ib)
			//	NOTE: lgposLast may not be set if we are on the first LRCK record

			if (	lgposLast.isec >= m_csecHeader &&
					lgposLast.isec == lgposCurrent.isec - 1 &&
					lgposLast.ib == lgposCurrent.ib )
				{

				//	read both "last sector" and "shadow" (right now, we only have "shadow")

				Call( plread->ErrEnsureSector( lgposLast.isec, 2, &pbEnsure ) );
				plrck = reinterpret_cast< LRCHECKSUM* >( pbEnsure + lgposCurrent.ib );

				//	see if the "shadow" sector is a real shadow

				if ( FValidLRCKShadow(	reinterpret_cast< LRCHECKSUM* >( pbEnsure + m_cbSec + lgposCurrent.ib ), 
										&lgposCurrent, m_plgfilehdr->lgfilehdr.le_lGeneration ) )
					{
#ifdef ENABLE_LOGPATCH_TRACE
					if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
						{
						(*pcprintfLogPatch)( "special case!\r\n" );
						(*pcprintfLogPatch)( "\twe crashed during a multi-sector flush which was replacing a single-sector shadowed flush\r\n" );
						(*pcprintfLogPatch)( "\tthe crash occurred AFTER writing the first sector but BEFORE writing sectors 2-N\r\n" );

						LRCHECKSUM *_plrckT_ = (LRCHECKSUM *)( pbEnsure + m_cbSec + lgposCurrent.ib );

						(*pcprintfLogPatch)( "\r\n\tplrck (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( _plrckT_ ) );
						(*pcprintfLogPatch)( "\t  cbBackwards       = 0x%08x\r\n", _plrckT_->le_cbBackwards );
						(*pcprintfLogPatch)( "\t  cbForwards        = 0x%08x\r\n", _plrckT_->le_cbForwards );
						(*pcprintfLogPatch)( "\t  cbNext            = 0x%08x\r\n", _plrckT_->le_cbNext );
						(*pcprintfLogPatch)( "\t  ulChecksum        = 0x%08x\r\n", _plrckT_->le_ulChecksum );
						(*pcprintfLogPatch)( "\t  ulShortChecksum   = 0x%08x\r\n", _plrckT_->le_ulShortChecksum );
						(*pcprintfLogPatch)( "\t  bUseShortChecksum = %s (0x%02x)\r\n", 
											( bShortChecksumOn == _plrckT_->bUseShortChecksum ?
											  "Yes" : 
											  ( bShortChecksumOff == _plrckT_->bUseShortChecksum ?
											    "No" : "???" ) ),
											BYTE( _plrckT_->bUseShortChecksum ) );
						(*pcprintfLogPatch)( "\tpbEnsure (0x%0*I64x) -- 2 sectors\r\n", sizeof( char* ) * 2, QWORD( pbEnsure ) );

						ULONG _ibdumpT_ = 0;
						while ( _ibdumpT_ < 2 * m_cbSec )
							{
							(*pcprintfLogPatch)( "\t0x%04x: %02x %02x %02x %02x %02x %02x %02x %02x-%02x %02x %02x %02x %02x %02x %02x %02x\r\n",
												_ibdumpT_,
												pbEnsure[_ibdumpT_+0],  pbEnsure[_ibdumpT_+1],  pbEnsure[_ibdumpT_+2],  pbEnsure[_ibdumpT_+3],
												pbEnsure[_ibdumpT_+4],  pbEnsure[_ibdumpT_+5],  pbEnsure[_ibdumpT_+6],  pbEnsure[_ibdumpT_+7],
												pbEnsure[_ibdumpT_+8],  pbEnsure[_ibdumpT_+9],  pbEnsure[_ibdumpT_+10], pbEnsure[_ibdumpT_+11],
												pbEnsure[_ibdumpT_+12], pbEnsure[_ibdumpT_+13], pbEnsure[_ibdumpT_+14], pbEnsure[_ibdumpT_+15] );
							_ibdumpT_ += 16;
							Assert( _ibdumpT_ <= 2 * m_cbSec );
							}
						}
#endif	//	ENABLE_LOGPATCH_TRACE

					//	enter the scan code, but skip the scan itself 

					fDoScan = fTrue;
					fSkipScanAndApplyPatch = fTrue;

					//	setup lgposScan's generation (the rest of it is not used)
					
					lgposScan.lGeneration = lgposLast.lGeneration;

					//	the end ptr is the start of the shadow sector

					lgposEnd.ib				= 0;
					lgposEnd.isec			= lgposCurrent.isec;
					lgposEnd.lGeneration	= lgposCurrent.lGeneration;

					//	mark the end of data for redo time

					m_lgposLastRec = lgposEnd;

					//	prepare the patch buffer ptrs

					pbLastChecksum = pbLastSector + lgposCurrent.ib;
					isecLastSector = lgposCurrent.isec;

					//	fill with known value, so if we ever use the filled data we'll know it (hopefully)

					Assert( cbLogExtendPattern > m_cbSec );
					Assert( rgbLogExtendPattern );
					memcpy( pbLastSector, rgbLogExtendPattern, m_cbSec );

					//	prepare the new LRCHECKSUM record

					plrck = reinterpret_cast< LRCHECKSUM* >( pbLastChecksum );
					memset( pbLastChecksum, 0, sizeof( LRCHECKSUM ) );
					plrck->lrtyp = lrtypChecksum;
					plrck->bUseShortChecksum = bShortChecksumOff;

					//	NOTE: we leave the backward pointer set to 0 to cover the case where the head of
					//			  a partial log record was written in the last sector, but the tail of that
					//			  record was never written safely to this sector (we just filled the space
					//			  where the tail was with log-extend pattern)
					//		  the partial log record will be ignored when we scan record by record later on
					//			  in this function
				
					//	terminate the "forever" loop

					break;					
					}

				//	"shadow" was not a shadow sector

				//	this case must be a standard case of corruption/torn-write and should run through the
				//		regular channels (continue on)
				
				}

			//	since the short checksum is off, the range is limited to the first sector only, and the
			//		data in that range is invalid
			//	it also means we should have a shadow sector making this the end of the log

			//	jump to the code that attempts to recover using the shadow sector

			fTookJump = fTrue;
			goto RecoverWithShadow;				
			}

		//	at this point, the LRCHECKSUM record and its range are completely valid

#ifdef LOGPATCH_UNIT_TEST
			{
			ULONG _ibT;
			ULONG _cbT;

			if ( lgposCurrent.ib + sizeof( LRCHECKSUM ) + plrck->le_cbForwards > m_cbSec )
				{
				_ibT = m_cbSec;
				_cbT = lgposCurrent.ib + sizeof( LRCHECKSUM ) + plrck->le_cbForwards;
				AssertRTL( 0 == _cbT % m_cbSec );
				while ( _ibT < _cbT )
					{
					AssertRTL( 0 != memcmp( pbEnsure + _ibT, rgbLogExtendPattern, m_cbSec ) );
					_ibT += m_cbSec;
					}
				}
			}
#endif	//	LOGPATCH_UNIT_TEST

		//	is this the last LRCHECKSUM record?

		if ( plrck->le_cbNext == 0 )
			{
			BOOL fIsMultiSector;

			//	this is the last LRCHECKSUM record in the log file

			//	since the LRCHECKSUM record is valid, we won't need a scan for any corruption

			fDoScan = fFalse;

			//	see if this LRCHECKSUM record is a multi-sector flush (it has a short checksum)
			//		(remember that logs can end in either single-sector or multi-sector flushes)

			fIsMultiSector = ( plrck->bUseShortChecksum == bShortChecksumOn );

			//	if this was a single-sector flush, we need to create a shadow sector now (even if one
			//		already exists); this ensures that if the flush at the end of this function goes
			//		bad, we can get back the data we lost
			//	NOTE: only do this for edb.log!

			if ( !fIsMultiSector && fJetLog )
				{

				//	this was a single-sector flush; create the shadow sector now
				
				Assert( PbSecAligned( reinterpret_cast< const BYTE* >( plrck ) ) == pbEnsure );
				const ULONG ulNormalChecksum = plrck->le_ulChecksum;

				//	the checksum value in the LRCHECKSUM record should match the re-calculated checksum

				Assert( ulNormalChecksum == UlComputeChecksum( plrck, (ULONG32)m_plgfilehdr->lgfilehdr.le_lGeneration ) );

				//	get the shadow checksum value
			
				plrck->le_ulChecksum = UlComputeShadowChecksum( ulNormalChecksum );

				//	write the shadow sector
			
				if ( !fReadOnly )
					{
//
//	SEARCH-STRING: SecSizeMismatchFixMe
//
Assert( m_cbSec == m_cbSecVolume );
//					if ( m_cbSec == m_cbSecVolume )
//						{
						CallJ( m_pfapiLog->ErrIOWrite( m_cbSec * QWORD( lgposCurrent.isec + 1 ), m_cbSec, pbEnsure ), LHandleErrorWrite );
//						}
//					else
//						{
//						fWriteOnSectorSizeMismatch = fTrue;
//						}
					}

				//	mark the sector as modified
				
				plread->SectorModified( lgposCurrent.isec + 1, pbEnsure );

				//	fix the checksum value in the original LRCHECKSUM record
			
				plrck->le_ulChecksum = ulNormalChecksum;
				}

			//	does this LRCHECKSUM record have a "forward" range? (may be single- or multi- sector)

			if ( plrck->le_cbForwards == 0 )
				{

				//	no forward data was found

				//	this flush should have been a single-sector flush

				Assert( !fIsMultiSector );

				//	in this case, we want to search for the lrtypTerm and/or lrtypRecoveryQuit 
				//		log records up to and including the current LRCHECKSUM record
				//		(e.g. the forward range of the last LRCHECKSUM and the backward 
				//			  range of this LRCHECKSUM)
				
				//	to make the search work, we must setup lgposLast and lgposEnd: 
				//		we leave lgposLast (the last valid LRCHECKSUM we found) pointing at the 
				//			previous LRCHECKSUM even though we know the current LRCHECKSUM is ok
				//			because we want to start scanning from the last LRCHECKSUM record
				//		we set lgposEnd (the point to stop searching) to the position 
				//			immediately after this LRCHECKSUM since this LRCHECKSUM had no
				//			forward data

				lgposEnd = lgposCurrent;
				AddLgpos( &lgposEnd, sizeof( LRCHECKSUM ) );

				//	prepare pbLastSector with data for the next flush which will occur after we
				//		exit the "forever" loop

				//	copy the sector with the LRCHECKSUM record
				
				UtilMemCpy( pbLastSector, pbEnsure, m_cbSec );

				//	set the pointers to the copy of the LRCHECKSUM record
				
				pbLastChecksum = pbLastSector + lgposCurrent.ib;
				isecLastSector = lgposCurrent.isec;
				}
			else
				{

				//	forward data was found
				//
				//	in this case, we want to search for the lrtypTerm and/or lrtypRecoveryQuit 
				//		log records within the forward data of the LRCHECKSUM record

				//	set lgposLast (the last valid LRCHECKSUM record) to the current LRCHECKSUM record
					
				lgposLast = lgposCurrent;

				//	set lgposEnd (the point after the last good data) to the first byte after
				//		the forward range of this LRCHECKSUM

				lgposEnd = lgposCurrent;
				Assert( plrck->le_cbBackwards == lgposCurrent.ib || 0 == plrck->le_cbBackwards );
				AddLgpos( &lgposEnd, sizeof( LRCHECKSUM ) + plrck->le_cbForwards );

				//	was this flush a multi-sector flush?
			
				if ( fIsMultiSector )
					{

					//	this happens when the last flush is was multi-sector flush, which in turn happens when:
					//		backup/restore request a new generation and we happen to be doing a multi-sector flush
					//			because we are under high load
					//		we are at the end of the log file, we need to flush N sectors where N is the exact
					//			amount of sectors left in the log file, and there is no room for another 
					//			LRCHECKSUM record at the end of the (N-1)th sector
					//			
					//	naturally, we want to continue using the log file (just like any other case) after the
					//		last good data; however, we cannot because we would need to safely re-write the 
					//		LRCHECKSUM with an updated cbNext pointer
					//	that is not SAFELY possible without a shadow sector to back us up in case we fail
					//
					//	but do we even really want to re-claim the empty space in the current generation? NO!
					//		in the case of backup/restore, that generation is OVER and needs to stay that way
					//			so that the database remains in the state we expect it to be in when we are
					//			trying to restore it
					//		in the end of log case, there is no space to reclaim anyway!
					//
					//	conclusion --> these cases, however rare they may be, are fine and SHOULD behave the way
					//				   that they do

					//	since we can't use this generation, set the flag to start a new one

					//	NOTE: we can only start a new generation after edb.log!

					if ( fJetLog )
						{
						fLogToNextLogFile = fTrue;
						}
					}
				else
					{

					//	the last flush was a single-sector flush

					//	prepare pbLastSector with data for the next flush which will occur after we
					//		exit the "forever" loop

					//	copy the sector with the LRCHECKSUM record

					UtilMemCpy( pbLastSector, pbEnsure, m_cbSec );

					//	set the pointers to the copy of the LRCHECKSUM record
				
					pbLastChecksum = pbLastSector + lgposCurrent.ib;
					isecLastSector = lgposCurrent.isec;	
					}
				}

			//	mark the end of data for redo time

			m_lgposLastRec = lgposEnd;

			//	terminate the "forever" loop

			break;
			}


		//	move to the next LRCHECKSUM record

		//	set lgposLast (the last good LRCHECKSUM record) to the current LRCHECKSUM record

		lgposLast = lgposCurrent;

		//	set lgposCurrent to point to the supposed next LRCHECKSUM record (could be garbage)

		Assert( lgposCurrent.ib == plrck->le_cbBackwards || 0 == plrck->le_cbBackwards );
		AddLgpos( &lgposCurrent, sizeof( LRCHECKSUM ) + plrck->le_cbNext );

		//	mark the end of data for redo time

		m_lgposLastRec = lgposCurrent;
		m_lgposLastRec.ib = 0;
		}	//	bottom of "forever" loop

	//	we have now finished scanning the chain of LRCHECKSUM records

	//	did we find ANY records at all (including LRCHECKSUM records)?
	
	if ( lgposEnd.isec == m_csecHeader && 0 == lgposEnd.ib )
		{
		
		//	the beginning of the log file was also the end; this indicates corruption because 
		//		every log file must start with an LRCHECKSUM record

		//	this should not be a single-sector torn-write

		Assert( !fSingleSectorTornWrite );

		//	we should already be prepared to scan for the cause of the corruption

		Assert( fDoScan );

		//	the end position should be set to the current position

		Assert( CmpLgpos( &lgposCurrent, &lgposEnd ) == 0 );

		//	the last position should not have been touched since it was initialized

		Assert( lgposLast.isec == 0 && lgposLast.ib == 0 );

		//	fix lgposLast to be lgposCurrent (we created an empty LRCHECKSUM record here)

		lgposLast = lgposCurrent;

		//	pbLastChecksum should be set to the start of the sector

		Assert( pbLastChecksum == pbLastSector );

		//	isecLastSector should be set to the current sector

		Assert( isecLastSector == lgposCurrent.isec );
		}
	else if ( lgposLast.isec == 0 )
		{

		//	the end of the log was NOT the beginning; HOWEVER, lgposLast was never set
		//
		//	this indicates that only 1 good LRCHECKSUM record exists and it is at the beginning of
		//		the log file; if we had found a second good LRCHECKSUM record, we would have set
		//		lgposLast to the location of the first good LRCHECKSUM record
		//
		//	we may or may not be setup to scan for corruption, so we are limited in the checking we can do
		//		(we might even be corruption free if there is only 1 LRCHECKSUM in the log!)

		//	adjust lgposLast for the record-to-record search for the lrtypTerm and/or lrtypRecoveryQuit records
				
		lgposLast.isec = USHORT( m_csecHeader );
		Assert( lgposLast.ib == 0 );
		
		//	double check that lgposEnd is atleast 1 full LRCHECKSUM record ahead of lgposLast

		Assert( lgposEnd.isec > lgposLast.isec || 
				( lgposEnd.isec == lgposLast.isec && lgposEnd.ib >= ( lgposLast.ib + sizeof( LRCHECKSUM ) ) ) );
		}

	//	lgposCurrent, lgposLast, and lgposEnd should now be setup properly

	Assert( CmpLgpos( &lgposLast, &lgposCurrent ) <= 0 );	//	lgposLast <= lgposCurrent
	//	bad assert -- in the first failure case (invalid RECORD and invalid SHADOW),
	//		lgposCurrent will be greater than lgposEnd (or equal to it) because lgposCurrent will
	//		point directly to the location of the bad LRCHECKSUM record
	//Assert( CmpLgpos( &lgposCurrent, &lgposEnd ) <= 0 );	//	lgposCurrent <= lgposEnd

	//	scan for corruption

	if ( fDoScan )
		{
#ifdef ENABLE_LOGPATCH_TRACE
		if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
			{
			ULONG _ibdumpT_;

			(*pcprintfLogPatch)( "scanning logfile %s for a torn-write or for corruption\r\n", m_szLogName );

			(*pcprintfLogPatch)( "\r\n\tdumping state of ErrLGCheckReadLastLogRecordFF:\r\n" );
			(*pcprintfLogPatch)( "\t\terr                    = %d \r\n", err );
			(*pcprintfLogPatch)( "\t\tlgposCurrent           = {0x%x,0x%x,0x%x}\r\n", lgposCurrent.lGeneration, lgposCurrent.isec, lgposCurrent.ib );
			(*pcprintfLogPatch)( "\t\tlgposLast              = {0x%x,0x%x,0x%x}\r\n", lgposLast.lGeneration, lgposLast.isec, lgposLast.ib  );
			(*pcprintfLogPatch)( "\t\tlgposEnd               = {0x%x,0x%x,0x%x}\r\n", lgposEnd.lGeneration, lgposEnd.isec, lgposEnd.ib );
			(*pcprintfLogPatch)( "\t\tisecLastSector         = %d\r\n", isecLastSector );
			(*pcprintfLogPatch)( "\t\tisecPatternFill        = 0x%x\r\n", isecPatternFill );
			(*pcprintfLogPatch)( "\t\tcsecPatternFill        = 0x%x\r\n", csecPatternFill );
			(*pcprintfLogPatch)( "\t\tfGotQuit               = %s\r\n", ( fGotQuit ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tfCreatedLogReader      = %s\r\n", ( fCreatedLogReader ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tfLogToNextLogFile      = %s\r\n", ( fLogToNextLogFile ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tfDoScan                = %s\r\n", ( fDoScan ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tfOldRecovering         = %s\r\n", ( fOldRecovering ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tfOldRecoveringMode     = %d\r\n", fOldRecoveringMode );
			(*pcprintfLogPatch)( "\t\tfRecordOkRangeBad      = %s\r\n", ( fRecordOkRangeBad ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tlgposScan              = {0x%x,0x%x,0x%x}\r\n", lgposScan.lGeneration, lgposScan.isec, lgposScan.ib );
			(*pcprintfLogPatch)( "\t\tfJetLog                = %s\r\n", ( fJetLog ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tfTookJump              = %s\r\n", ( fTookJump ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tfSingleSectorTornWrite = %s\r\n", ( fSingleSectorTornWrite ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tfSkipScanAndApplyPatch = %s\r\n", ( fSkipScanAndApplyPatch ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tcbChecksum             = 0x%x\r\n", cbChecksum );

			(*pcprintfLogPatch)( "\r\n\tdumping partial state of LOG:\r\n" );
			(*pcprintfLogPatch)( "\t\tLOG::m_fRecovering     = %s\r\n", ( m_fRecovering ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tLOG::m_fRecoveringMode = %d\r\n", m_fRecoveringMode );
			(*pcprintfLogPatch)( "\t\tLOG::m_fHardRestore    = %s\r\n", ( m_fHardRestore ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\t\tLOG::m_fRestoreMode    = %d\r\n", m_fRestoreMode );
			(*pcprintfLogPatch)( "\t\tLOG::m_csecLGFile      = 0x%08x\r\n", m_csecLGFile );
			(*pcprintfLogPatch)( "\t\tLOG::m_csecLGBuf       = 0x%08x\r\n", m_csecLGBuf );
			(*pcprintfLogPatch)( "\t\tLOG::m_csecHeader      = %d\r\n", m_csecHeader );
			(*pcprintfLogPatch)( "\t\tLOG::m_cbSec           = %d\r\n", m_cbSec );
			(*pcprintfLogPatch)( "\t\tLOG::m_cbSecVolume     = %d\r\n", m_cbSecVolume );
			(*pcprintfLogPatch)( "\t\tLOG::m_pbLGBufMin      = 0x%0*I64x\r\n", sizeof( char* ) * 2, QWORD( m_pbLGBufMin ) );
			(*pcprintfLogPatch)( "\t\tLOG::m_pbLGBufMax      = 0x%0*I64x\r\n", sizeof( char* ) * 2, QWORD( m_pbLGBufMax ) );
			(*pcprintfLogPatch)( "\t\tLOG::m_pbWrite         = 0x%0*I64x\r\n", sizeof( char* ) * 2, QWORD( m_pbWrite ) );
			(*pcprintfLogPatch)( "\t\tLOG::m_pbEntry         = 0x%0*I64x: 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\r\n", sizeof( char* ) * 2, QWORD( m_pbEntry ), m_pbEntry[0], m_pbEntry[1], m_pbEntry[2], m_pbEntry[3], m_pbEntry[4], m_pbEntry[5], m_pbEntry[6], m_pbEntry[7] );
			(*pcprintfLogPatch)( "\t\tLOG::m_isecWrite       = 0x%08x\r\n", m_isecWrite );

			(*pcprintfLogPatch)( "\r\n\tdumping data:\r\n" );

			(*pcprintfLogPatch)( "\t\tplrck (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( plrck ) );
			(*pcprintfLogPatch)( "\t\t  cbBackwards       = 0x%08x\r\n", plrck->le_cbBackwards );
			(*pcprintfLogPatch)( "\t\t  cbForwards        = 0x%08x\r\n", plrck->le_cbForwards );
			(*pcprintfLogPatch)( "\t\t  cbNext            = 0x%08x\r\n", plrck->le_cbNext );
			(*pcprintfLogPatch)( "\t\t  ulChecksum        = 0x%08x\r\n", plrck->le_ulChecksum );
			(*pcprintfLogPatch)( "\t\t  ulShortChecksum   = 0x%08x\r\n", plrck->le_ulShortChecksum );
			(*pcprintfLogPatch)( "\t\t  bUseShortChecksum = %s (0x%02x)\r\n", 
								( bShortChecksumOn == plrck->bUseShortChecksum ?
								  "Yes" : 
								  ( bShortChecksumOff == plrck->bUseShortChecksum ?
								    "No" : "???" ) ),
								BYTE( plrck->bUseShortChecksum ) );
			(*pcprintfLogPatch)( "\t\tpbEnsure (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( pbEnsure ) );

			_ibdumpT_ = 0;
			while ( _ibdumpT_ < m_cbSec )
				{
				(*pcprintfLogPatch)( "\t\t0x%04x: %02x %02x %02x %02x %02x %02x %02x %02x-%02x %02x %02x %02x %02x %02x %02x %02x\r\n",
									_ibdumpT_,
									pbEnsure[_ibdumpT_+0],  pbEnsure[_ibdumpT_+1],  pbEnsure[_ibdumpT_+2],  pbEnsure[_ibdumpT_+3],
									pbEnsure[_ibdumpT_+4],  pbEnsure[_ibdumpT_+5],  pbEnsure[_ibdumpT_+6],  pbEnsure[_ibdumpT_+7],
									pbEnsure[_ibdumpT_+8],  pbEnsure[_ibdumpT_+9],  pbEnsure[_ibdumpT_+10], pbEnsure[_ibdumpT_+11],
									pbEnsure[_ibdumpT_+12], pbEnsure[_ibdumpT_+13], pbEnsure[_ibdumpT_+14], pbEnsure[_ibdumpT_+15] );
				_ibdumpT_ += 16;
				Assert( _ibdumpT_ <= m_cbSec );
				}

			(*pcprintfLogPatch)( "\t\tpbLastSector (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( pbLastSector ) );

			_ibdumpT_ = 0;
			while ( _ibdumpT_ < m_cbSec )
				{
				(*pcprintfLogPatch)( "\t\t0x%04x: %02x %02x %02x %02x %02x %02x %02x %02x-%02x %02x %02x %02x %02x %02x %02x %02x\r\n",
									_ibdumpT_,
									pbLastSector[_ibdumpT_+0],  pbLastSector[_ibdumpT_+1],  pbLastSector[_ibdumpT_+2],  pbLastSector[_ibdumpT_+3],
									pbLastSector[_ibdumpT_+4],  pbLastSector[_ibdumpT_+5],  pbLastSector[_ibdumpT_+6],  pbLastSector[_ibdumpT_+7],
									pbLastSector[_ibdumpT_+8],  pbLastSector[_ibdumpT_+9],  pbLastSector[_ibdumpT_+10], pbLastSector[_ibdumpT_+11],
									pbLastSector[_ibdumpT_+12], pbLastSector[_ibdumpT_+13], pbLastSector[_ibdumpT_+14], pbLastSector[_ibdumpT_+15] );
				_ibdumpT_ += 16;
				Assert( _ibdumpT_ <= m_cbSec );
				}

			if ( pbLastChecksum )
				{
				LRCHECKSUM *_plrckT_ = (LRCHECKSUM *)pbLastChecksum;

				(*pcprintfLogPatch)( "\t\tpbLastChecksum (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( _plrckT_ ) );
				(*pcprintfLogPatch)( "\t\t  cbBackwards       = 0x%08x\r\n", _plrckT_->le_cbBackwards );
				(*pcprintfLogPatch)( "\t\t  cbForwards        = 0x%08x\r\n", _plrckT_->le_cbForwards );
				(*pcprintfLogPatch)( "\t\t  cbNext            = 0x%08x\r\n", _plrckT_->le_cbNext );
				(*pcprintfLogPatch)( "\t\t  ulChecksum        = 0x%08x\r\n", _plrckT_->le_ulChecksum );
				(*pcprintfLogPatch)( "\t\t  ulShortChecksum   = 0x%08x\r\n", _plrckT_->le_ulShortChecksum );
				(*pcprintfLogPatch)( "\t\t  bUseShortChecksum = %s (0x%02x)\r\n", 
									( bShortChecksumOn == _plrckT_->bUseShortChecksum ?
									  "Yes" : 
									  ( bShortChecksumOff == _plrckT_->bUseShortChecksum ?
									    "No" : "???" ) ),
									BYTE( _plrckT_->bUseShortChecksum ) );
				}
			else
				{
				(*pcprintfLogPatch)( "\t\tpbLastChecksum (null)\r\n" );
				}

			(*pcprintfLogPatch)( "\r\n" );
			}
#endif	//	ENABLE_LOGPATCH_TRACE

		//	an inconsistency has occurred somewhere in the physical logging structures and/or log data

		//	NOTE: we can only patch torn writes in edb.log; all other patching must be done offline!

		//	we are either in the middle of the log file (most likely), or we are at the end of a 
		//		generation whose last flush was a multisector flush (unlikely -- special case)

		//	in either case, this may really be the end of the log data!
		//		middle of log case: just say the power went out as we were flushing and the first few
		//		  (cbNext != 0)		sectors went to disk (including the LRCHECKSUM), but the rest did not
		//							make it; that would make it look like we are in the middle of the log
		//							when we are really at the end of it
		//		end of log case:    we already know we were at the end of the log!
		//		  (cbNext == 0)

		//	to see if this is the real end of the log, we must scan through data after the "forward" range 
		//		of the current LRCHECKSUM record (lgposCurrent) all the way through to the end of the log
		//
		//	if we find nothing but the log pattern, we know this was the last data in the log 
		//		(we can recover up this point and say that this is as far as we can safely go)
		//	if we do find something besides the log pattern, then we know there is some other log data
		//		out there, and we cannot safely say that this is the end of the log

		//	after examinig the different cases of corruption, it turns out that there are 2 cases where
		//		we actaully have a torn-write for sure; they are listed below:
		//
		//
		//	in the first case, we have an LRCHECKSUM record covering a range of 1 or more full sectors
		//		(the #'s represent a gap of sector ranging from none to infinity); the range is clean
		//		meaning that all checksums are correct and all data is valid; the next pointer of the 
		//		LRCHECKSUM record points into the pattern which runs to the end of the log file
		//	this is the typical situation when we abruptly get a TerminateProcess() or suffer a power
		//		outage -- there is absolutely no chance to flush anything, and we had no idea it was
		//		coming in the first place
		//	NOTE: when the LRCHECKSUM record covers only 1 sector, that sector will have been shadowed
		//		  immediately after it -- there is a special case to handle this situation
		//
		//     /---\  /--- # ---\
		//	--|----|--|--- # ----|---------------------------
		//    |...(LRCK).. # ....| pattern 
		//	--|-----|----- # ----|---------------------------
		//          \----- # --------/
		//
		//
		//	in the second case, we have an LRCHECKSUM record covering a range of 2 or more full sectors
		//		(the #'s represent a gap of sectors ranging from 1 to infinity); the short checksum
		//		range is clean (it checks out OK and the data in the first sector is OK), but the long
		//		checksum range is dirty (some bits are wrong in one or more of the sectors after the
		//		first sector); notice that atleast one of the sectors exclusive to the long checksum 
		//		range contains the pattern; the next pointer is aimed properly, but points at the 
		//		the pattern instead of the next LRCHECKSUM record
		//	this is a more rare case; since we find the pattern within the flush, we assume that the 
		//		disk never finished the IO request we made when we were flushing the data; we base
		//		this on the fact that there was no data after this, so this must be the end -- finding
		//		the pattern within the flush helps to confirm that this is an incomplete IO and not a
		//		case of real corruption
		//
		//     /---\  /--- # --------------\
		//	--|----|--|--- # --|-------|----|---------------------------
		//    |...(LRCK).. # ..|pattern|....| pattern 
		//	--|-----|----- # --|-------|----|---------------------------
		//          \----- # -------------------/
		//
		//	NOTE: case #2 could occur when we crash in between I/Os while trying to overwrite the
		//		  shadow sector; the disk will contain the NEW first sector, the OLD shadow sector,
		//		  and the pattern; IF THE RANGE ONLY COVERS 2 SECTORS, WE WILL NOT SEE THE PATTERN
		//		  IN THE RANGE -- ONLY THE SHADOW SECTOR WILL BE IN THE RANGE! so, we need to realize 
		//		  this case and check for the shadow-sector instead of the pattern within the range
		//	we should see 2 things in this case: the shadow sector should be a valid shadow sector, and 
		//		  the log records in the newly flushed first sector and the old shadow sector should
		//		  match exactly (the shadow sector will have less than or equal to the number of
		//		  log records in the new sector); the LRCHECKSUM records from each sector should not match
		//

		BYTE 		*pbEnsureScan	= NULL;
		BYTE		*pbScan			= NULL;
		ULONG		csecScan		= 0;
		BOOL		fFoundPattern	= fTrue;	//	assume we will find a torn write
		BOOL		fIsTornWrite	= fTrue;	//	assume we will find a torn write
		CHAR 		szSector[30];
		CHAR		szCorruption[30];
		const CHAR*	rgpsz[3]		= { m_szLogName, szSector, szCorruption };

		const QWORD	ibOffset		= QWORD( lgposCurrent.isec ) * QWORD( m_cbSec );
		const DWORD	cbLength		= cbChecksum;

		//	prepare the event-log messages

		sprintf( szSector, "%d:%d", lgposCurrent.isec, lgposCurrent.ib );
		szCorruption[0] = 0;

		//	skip the scan if we are coming from a special-case above (see code for details)

		if ( fSkipScanAndApplyPatch )
			goto ApplyPatch;

		//	make sure lgposScan was set

		Assert( lgposScan.isec > 0 );

		//	we should be sector-aligned

		Assert( lgposScan.ib == 0 );

		//	we should be at or past the end of the good data

		Assert( CmpLgpos( &lgposEnd, &lgposScan ) <= 0 );

		//	determine if there is any data to scan through
		
		if ( lgposScan.isec >= ( m_csecLGFile - 1 ) )
			{
			
			//	we are at the file size limits of the log file meaning there is nothing left to scan

			//	since there isn't any data left to look through, we must be at the end of the log; that 
			//		means that we definitely have a torn-write

			//	we should NEVER EVER point past the end of the log file

			Assert( lgposScan.isec <= ( m_csecLGFile - 1 ) );

			//	leave fFoundPattern set to fTrue

			//	skip the scanning loop

			goto SkipScan;
			}

		//	pull in the data we need to scan 
		//	(including the extra sector we reserve for the final shadow sector)

		csecScan = m_csecLGFile - lgposScan.isec;
		Call( plread->ErrEnsureSector( lgposScan.isec, csecScan, &pbEnsureScan ) );

		//	scan sector by sector looking for the log-extend pattern

		Assert( cbLogExtendPattern >= m_cbSec );
		pbScan = pbEnsureScan;
		while ( fFoundPattern && csecScan )
			{

			//	does the entire sector match the log-extend pattern?
				
			if ( memcmp( pbScan, rgbLogExtendPattern, m_cbSec ) != 0 )
				{
				//	no; this is not a torn write
					
				fFoundPattern = fFalse;
				fIsTornWrite = fFalse;

				//	stop scanning

				break;
				}
				
			//	advance the scan counters

			pbScan += m_cbSec;
			csecScan--;
			}

		//	assume that finding the pattern means we have a torn-write

		fIsTornWrite = fFoundPattern;

#ifdef ENABLE_LOGPATCH_TRACE
		if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
			{
			(*pcprintfLogPatch)( "pattern scan complete -- fFoundPattern = %s\r\n", ( fFoundPattern ? "TRUE" : "FALSE" ) );
			}
#endif	//	ENABLE_LOGPATCH_TRACE

		//	what about case #2 (see comments above) where we must also search for the pattern
		//		within the range of the LRCHECKSUM record itself? this is just to be sure that
		//		the reason the long checksum failed was in fact due to finding the pattern...
		//	NOTE: if the range is only 2 sectors and we crashed in the middle of trying to
		//		  overwrite the previous shadow sector, the range will contain only the shadow
		//		  sector (when the range is 3 or more sectors, it should contain atleast 1 sector
		//		  with the pattern if it is a torn-write case)

		if ( fFoundPattern && fRecordOkRangeBad )
			{
			BYTE		*pbScanT		= NULL;
			BYTE		*pbEnsureScanT	= NULL;
			ULONG		csecScanT		= 0;
			LRCHECKSUM	*plrckT			= NULL;

#ifdef ENABLE_LOGPATCH_TRACE
			if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
				{
				(*pcprintfLogPatch)( "special case!\r\n" );
				(*pcprintfLogPatch)( "\trecall that we had previously detected a bad LRCK range\r\n" );
				(*pcprintfLogPatch)( "\twe now suspect that this BAD-ness is the result of an incomplete I/O because fFoundPattern=TRUE\r\n" );
				}
#endif	//	ENABLE_LOGPATCH_TRACE

			Assert( !fSingleSectorTornWrite );

			//	read the sector with the LRCHECKSUM record
				
			Call( plread->ErrEnsureSector( lgposCurrent.isec, 1, &pbEnsureScanT ) );
			plrck = reinterpret_cast< LRCHECKSUM* >( pbEnsureScanT + lgposCurrent.ib );
			AssertValidLRCKRecord( plrck, &lgposCurrent );

			//	calculate the number of sectors in its range
			
			csecScanT = ( ( lgposCurrent.ib + sizeof( LRCHECKSUM ) + plrck->le_cbForwards - 1 ) / m_cbSec ) + 1;
			Assert( csecScanT > 1 );

			//	read the entire range

			Call( plread->ErrEnsureSector( lgposCurrent.isec, csecScanT, &pbEnsureScanT ) );
			plrck = reinterpret_cast< LRCHECKSUM* >( pbEnsureScanT + lgposCurrent.ib );
			AssertValidLRCKRecord( plrck, &lgposCurrent );
			AssertInvalidLRCKRange( plrck, lgposCurrent.lGeneration );

			if ( csecScanT == 2 )
				{

				//	this is the special case where we only have 2 sectors; the second sector could be
				//		an old shadow sector, it could be the pattern, or it could be corruption;
				//
				//	if it is an old shadow sector, we must have failed while trying to safely overwrite it
				//		using 2 I/Os (never made it to the second I/O) making this a torn-write case
				//	if we find the pattern, we must have been in the middle of a single I/O and crashed
				//		leaving "holes" of the pattern throughout the range of this LRCHECKSUM record
				//		making this a torn-write case
				//	if we find corruption, we cannot have a torn-write

				plrckT = (LRCHECKSUM *)( (BYTE *)plrck + m_cbSec );
				if ( FValidLRCKShadowWithoutCheckingCBNext( plrckT, &lgposCurrent, lgposCurrent.lGeneration ) )
					{

					//	this should only happen in edb.log -- all other logs should be clean or corrupt
					//		(they should never have a torn-write)

					Assert( fJetLog );

					//	the log records in the shadow sector must match the log records in the new sector

					if ( memcmp( pbEnsureScanT, 
								 pbEnsureScanT + m_cbSec, 
								 (BYTE *)plrck - pbEnsureScanT ) == 0 &&
						 memcmp( (BYTE *)plrck + sizeof( LRCHECKSUM ),
						 		 (BYTE *)plrckT + sizeof( LRCHECKSUM ),
						 		 plrckT->le_cbForwards ) == 0 )
						{

						//	make sure the LRCHECKSUM pointers are correct

						Assert( plrckT->le_cbForwards <= plrck->le_cbForwards );
						
						// type cast explicitly for CISCO compiler
						Assert( (ULONG32)plrckT->le_cbBackwards == (ULONG32)plrck->le_cbBackwards );

						//	we are now sure that we have a torn-write

						Assert( fIsTornWrite );
						}
					else
						{

						//	this is not a torn-write because the log records do not match? this should
						//		never happen -- especially if we have determined that the shadow sector
						//		is valid! it seems to suggest that the shadow-sector validation is bad

						AssertTracking();

						//	this is not a torn-write

						fIsTornWrite = fFalse;
						}
					}
				else if ( memcmp( pbEnsureScanT + m_cbSec, rgbLogExtendPattern, m_cbSec ) == 0 )
					{

					//	the sector we are looking at is not a shadow-sector, but it does contain the 
					//		pattern meaning we issued 1 I/O but that I/O never completed; this is 
					//		a torn-write case

					Assert( fIsTornWrite );
					}
				else
					{

					//	the sector we are looking at is neither an old shadow or the pattern; it must
					//		be corruption

					fIsTornWrite = fFalse;
					}
				}
			else
				{

				//	we have more than 2 sectors to scan
				//	we can either find the pattern in 1 of the sectors making this case a torn-write, OR
				//		we won't find anything making this case corruption

				//	scan the sectors within the range of the LRCHECKSUM record
				//	if we find the log-extend pattern in just one sector, we know that the corruption
				//		was caused by that sector and not real corruption

				csecScanT--;
				pbScanT = pbEnsureScanT + m_cbSec;
				while ( csecScanT )
					{

					//	see if we have atleast 1 sector with the log-extend pattern

					if ( memcmp( pbScanT, rgbLogExtendPattern, m_cbSec ) == 0 )
						{
							
						//	we have one! this has to be a torn-write

						Assert( fIsTornWrite );		//	this should be previously set

						break;
						}

					csecScanT--;
					pbScanT += m_cbSec;
					}

				//	did we find the pattern?

				if ( !csecScanT )
					{

					//	no; this is not a torn-write

					fIsTornWrite = fFalse;
					}
				}
			}

SkipScan:

		//	if a torn-write was detected, we may have a single-sector torn-write

		fSingleSectorTornWrite = fSingleSectorTornWrite && fIsTornWrite;

#ifdef ENABLE_LOGPATCH_TRACE
		if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
			{
			(*pcprintfLogPatch)( "torn-write detection complete\r\n" );
			(*pcprintfLogPatch)( "\tfIsTornWrite           = %s\r\n", ( fIsTornWrite ? "TRUE" : "FALSE" ) );
			(*pcprintfLogPatch)( "\tfSingleSectorTornWrite = %s\r\n", ( fSingleSectorTornWrite ? "TRUE" : "FALSE" ) );
			}
#endif	//	ENABLE_LOGPATCH_TRACE

		if ( fIsTornWrite )
			{

			//	we have a torn-write

			Assert( csecScan == 0 );

			//	we should have found the log-extend pattern

			Assert( fFoundPattern );

ApplyPatch:
			//	what recovery mode are we in?

			if ( m_fHardRestore )
				{

				//	we are in hard-recovery mode

				if ( lgposScan.lGeneration <= m_lGenHighRestore )
					{
#ifdef ENABLE_LOGPATCH_TRACE
					if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
						{
						(*pcprintfLogPatch)( "failed to patch torn-write because we are dealing with a logfile in the backup set\r\n" );
						}
#endif	//	ENABLE_LOGPATCH_TRACE

					//  report log file corruption to the event log

					const _TCHAR*	rgpszT[ 4 ];
					DWORD			irgpsz		= 0;
					_TCHAR			szAbsPath[ IFileSystemAPI::cchPathMax ];
					_TCHAR			szOffset[ 64 ];
					_TCHAR			szLength[ 64 ];
					_TCHAR			szError[ 64 ];

					if ( m_pfapiLog->ErrPath( szAbsPath ) < JET_errSuccess )
						{
						OSSTRCopy( szAbsPath, _T( "<< cannot get filepath >>" ) );
						}
					_stprintf( szOffset, _T( "%I64i (0x%016I64x)" ), ibOffset, ibOffset );
					_stprintf( szLength, _T( "%u (0x%08x)" ), cbLength, cbLength );
					_stprintf( szError, _T( "%i (0x%08x)" ), JET_errLogTornWriteDuringHardRestore, JET_errLogTornWriteDuringHardRestore );

					rgpszT[ irgpsz++ ]	= szAbsPath;
					rgpszT[ irgpsz++ ]	= szOffset;
					rgpszT[ irgpsz++ ]	= szLength;
					rgpszT[ irgpsz++ ]	= szError;

					UtilReportEvent(	eventError,
										LOGGING_RECOVERY_CATEGORY,
										LOG_RANGE_CHECKSUM_MISMATCH_ID,
										irgpsz,
										rgpszT,
										0,
										NULL,
										m_pinst );
										
					//	this generation is part of a backup set -- we must fail

					Assert( lgposScan.lGeneration >= m_lGenLowRestore );
					UtilReportEvent(	eventError, 
										LOGGING_RECOVERY_CATEGORY, 
										LOG_TORN_WRITE_DURING_HARD_RESTORE_ID, 
										2, 
										rgpsz,
										0,
										NULL,
										m_pinst );

					Call( ErrERRCheck( JET_errLogTornWriteDuringHardRestore ) );
					}

				//	the current log generation is not part of the backup-set
				//	we can only patch edb.log -- all other generations require offline patching
				//		because generations after the bad one need to be deleted

				//	only report an error message when we cannot patch the log file

				if ( !fJetLog || fReadOnly )
					{
#ifdef ENABLE_LOGPATCH_TRACE
					if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
						{
						(*pcprintfLogPatch)( "failed to patch torn-write because %s\r\n",
											( !fJetLog ? "fJetLog==FALSE" : "we are in read-only mode" ) );
						}
#endif	//	ENABLE_LOGPATCH_TRACE

					//  report log file corruption to the event log

					const _TCHAR*	rgpszT[ 4 ];
					DWORD			irgpsz		= 0;
					_TCHAR			szAbsPath[ IFileSystemAPI::cchPathMax ];
					_TCHAR			szOffset[ 64 ];
					_TCHAR			szLength[ 64 ];
					_TCHAR			szError[ 64 ];

					if ( m_pfapiLog->ErrPath( szAbsPath ) < JET_errSuccess )
						{
						OSSTRCopy( szAbsPath, _T( "<< cannot get filepath >>" ) );
						}
					_stprintf( szOffset, _T( "%I64i (0x%016I64x)" ), ibOffset, ibOffset );
					_stprintf( szLength, _T( "%u (0x%08x)" ), cbLength, cbLength );
					_stprintf( szError, _T( "%i (0x%08x)" ), JET_errLogTornWriteDuringHardRecovery, JET_errLogTornWriteDuringHardRecovery );

					rgpszT[ irgpsz++ ]	= szAbsPath;
					rgpszT[ irgpsz++ ]	= szOffset;
					rgpszT[ irgpsz++ ]	= szLength;
					rgpszT[ irgpsz++ ]	= szError;

					UtilReportEvent(	eventError,
										LOGGING_RECOVERY_CATEGORY,
										LOG_RANGE_CHECKSUM_MISMATCH_ID,
										irgpsz,
										rgpszT,
										0,
										NULL,
										m_pinst );

					UtilReportEvent(	eventError,
										LOGGING_RECOVERY_CATEGORY, 
										LOG_TORN_WRITE_DURING_HARD_RECOVERY_ID, 
										2, 
										rgpsz,
										0,
										NULL,
										m_pinst );

					Call( ErrERRCheck( JET_errLogTornWriteDuringHardRecovery ) );
					}

				CallS( err );
				}
			else if ( fJetLog && !fReadOnly )
				{

				//	we are in soft-recovery mode

				//UtilReportEvent(	eventWarning,
				//					LOGGING_RECOVERY_CATEGORY, 
				//					LOG_TORN_WRITE_DURING_SOFT_RECOVERY_ID, 
				//					2, 
				//					rgpsz,
				//					0,
				//					NULL,
				//					m_pinst );

				CallS( err );
				}
			else
				{

				//	we are not in edb.log OR we are in read-only mode

				if ( fJetLog )
					{

					//	we are in edb.log, so we can theoretically patch the torn-write

					if ( pfIsPatchable )
						{

						//	the caller wants to know if the log is patchable
						//	by returning TRUE, we imply that the log is damaged so we don't need to return 
						//		an error as well

						*pfIsPatchable = fTrue;
						err = JET_errSuccess;
						goto HandleError;
						}
					}

#ifdef ENABLE_LOGPATCH_TRACE
				if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
					{
					(*pcprintfLogPatch)( "failed to patch torn-write because %s\r\n",
										 fJetLog ? "we are in read-only mode" : "we are not in the last log file (e.g. edb.log)" );
					}
#endif	//	ENABLE_LOGPATCH_TRACE

				//  report log file corruption to the event log

				const _TCHAR*	rgpszT[ 4 ];
				DWORD			irgpsz		= 0;
				_TCHAR			szAbsPath[ IFileSystemAPI::cchPathMax ];
				_TCHAR			szOffset[ 64 ];
				_TCHAR			szLength[ 64 ];
				_TCHAR			szError[ 64 ];

				if ( m_pfapiLog->ErrPath( szAbsPath ) < JET_errSuccess )
					{
					OSSTRCopy( szAbsPath, _T( "<< cannot get filepath >>" ) );
					}
				_stprintf( szOffset, _T( "%I64i (0x%016I64x)" ), ibOffset, ibOffset );
				_stprintf( szLength, _T( "%u (0x%08x)" ), cbLength, cbLength );
				_stprintf( szError, _T( "%i (0x%08x)" ), JET_errLogFileCorrupt, JET_errLogFileCorrupt );

				rgpszT[ irgpsz++ ]	= szAbsPath;
				rgpszT[ irgpsz++ ]	= szOffset;
				rgpszT[ irgpsz++ ]	= szLength;
				rgpszT[ irgpsz++ ]	= szError;

				UtilReportEvent(	eventError,
									LOGGING_RECOVERY_CATEGORY,
									LOG_RANGE_CHECKSUM_MISMATCH_ID,
									irgpsz,
									rgpszT,
									0,
									NULL,
									m_pinst );

				//	return corruption

				Call( ErrERRCheck( JET_errLogFileCorrupt ) );
				}
			}
		else
			{

			//	we have corruption

			if ( csecScan == 0 )
				{

				//	we found the pattern outside the forward range, but not inside it
				//	thus, the corruption was inside the forward range

				Assert( fRecordOkRangeBad );
				sprintf( szCorruption, "%d", lgposCurrent.isec + 1 );
				}
			else
				{

				//	we did not find the pattern
				//	the corruption must be after the forward range

				Assert( pbScan == PbSecAligned( pbScan ) );
				Assert( pbEnsureScan == PbSecAligned( pbEnsureScan ) );
				sprintf( szCorruption, "%d", lgposScan.isec + ( ( pbScan - pbEnsureScan ) / m_cbSec ) );
				}
				
			//	what recovery mode are we in?

			if ( m_fHardRestore )
				{
				//	we are in hard-recovery mode

				if ( lgposScan.lGeneration <= m_lGenHighRestore )
					{
#ifdef ENABLE_LOGPATCH_TRACE
					if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
						{
						(*pcprintfLogPatch)( "corruption detected during hard restore (this log is in the backup set)\r\n" );
						}
#endif	//	ENABLE_LOGPATCH_TRACE

					//  report log file corruption to the event log

					const _TCHAR*	rgpszT[ 4 ];
					DWORD			irgpsz		= 0;
					_TCHAR			szAbsPath[ IFileSystemAPI::cchPathMax ];
					_TCHAR			szOffset[ 64 ];
					_TCHAR			szLength[ 64 ];
					_TCHAR			szError[ 64 ];

					if ( m_pfapiLog->ErrPath( szAbsPath ) < JET_errSuccess )
						{
						OSSTRCopy( szAbsPath, _T( "<< cannot get filepath >>" ) );
						}
					_stprintf( szOffset, _T( "%I64i (0x%016I64x)" ), ibOffset, ibOffset );
					_stprintf( szLength, _T( "%u (0x%08x)" ), cbLength, cbLength );
					_stprintf( szError, _T( "%i (0x%08x)" ), JET_errLogCorruptDuringHardRestore, JET_errLogCorruptDuringHardRestore );

					rgpszT[ irgpsz++ ]	= szAbsPath;
					rgpszT[ irgpsz++ ]	= szOffset;
					rgpszT[ irgpsz++ ]	= szLength;
					rgpszT[ irgpsz++ ]	= szError;

					UtilReportEvent(	eventError,
										LOGGING_RECOVERY_CATEGORY,
										LOG_RANGE_CHECKSUM_MISMATCH_ID,
										irgpsz,
										rgpszT,
										0,
										NULL,
										m_pinst );

					//	this generation is part of a backup set -- we must fail

					Assert( lgposScan.lGeneration >= m_lGenLowRestore );
					UtilReportEvent(	eventError, 
										LOGGING_RECOVERY_CATEGORY, 
										LOG_CORRUPTION_DURING_HARD_RESTORE_ID, 
										3, 
										rgpsz,
										0,
										NULL,
										m_pinst );

					Call( ErrERRCheck( JET_errLogCorruptDuringHardRestore ) );
					}

#ifdef ENABLE_LOGPATCH_TRACE
					if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
						{
						(*pcprintfLogPatch)( "corruption detected during hard recovery\r\n" );
						}
#endif	//	ENABLE_LOGPATCH_TRACE

				//  report log file corruption to the event log

				const _TCHAR*	rgpszT[ 4 ];
				DWORD			irgpsz		= 0;
				_TCHAR			szAbsPath[ IFileSystemAPI::cchPathMax ];
				_TCHAR			szOffset[ 64 ];
				_TCHAR			szLength[ 64 ];
				_TCHAR			szError[ 64 ];

				if ( m_pfapiLog->ErrPath( szAbsPath ) < JET_errSuccess )
					{
					OSSTRCopy( szAbsPath, _T( "<< cannot get filepath >>" ) );
					}
				_stprintf( szOffset, _T( "%I64i (0x%016I64x)" ), ibOffset, ibOffset );
				_stprintf( szLength, _T( "%u (0x%08x)" ), cbLength, cbLength );
				_stprintf( szError, _T( "%i (0x%08x)" ), JET_errLogCorruptDuringHardRecovery, JET_errLogCorruptDuringHardRecovery );

				rgpszT[ irgpsz++ ]	= szAbsPath;
				rgpszT[ irgpsz++ ]	= szOffset;
				rgpszT[ irgpsz++ ]	= szLength;
				rgpszT[ irgpsz++ ]	= szError;

				UtilReportEvent(	eventError,
									LOGGING_RECOVERY_CATEGORY,
									LOG_RANGE_CHECKSUM_MISMATCH_ID,
									irgpsz,
									rgpszT,
									0,
									NULL,
									m_pinst );

				//	the current log generation is not part of the backup-set

				UtilReportEvent(	eventError,
									LOGGING_RECOVERY_CATEGORY, 
									LOG_CORRUPTION_DURING_HARD_RECOVERY_ID, 
									3, 
									rgpsz,
									0,
									NULL,
									m_pinst );

				Call( ErrERRCheck( JET_errLogCorruptDuringHardRecovery ) );
				}
			else if ( !fReadOnly )
				{

#ifdef ENABLE_LOGPATCH_TRACE
				if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
					{
					(*pcprintfLogPatch)( "corruption detected during soft recovery\r\n" );
					}
#endif	//	ENABLE_LOGPATCH_TRACE

				//  report log file corruption to the event log

				const _TCHAR*	rgpszT[ 4 ];
				DWORD			irgpsz		= 0;
				_TCHAR			szAbsPath[ IFileSystemAPI::cchPathMax ];
				_TCHAR			szOffset[ 64 ];
				_TCHAR			szLength[ 64 ];
				_TCHAR			szError[ 64 ];

				if ( m_pfapiLog->ErrPath( szAbsPath ) < JET_errSuccess )
					{
					OSSTRCopy( szAbsPath, _T( "<< cannot get filepath >>" ) );
					}
				_stprintf( szOffset, _T( "%I64i (0x%016I64x)" ), ibOffset, ibOffset );
				_stprintf( szLength, _T( "%u (0x%08x)" ), cbLength, cbLength );
				_stprintf( szError, _T( "%i (0x%08x)" ), JET_errLogFileCorrupt, JET_errLogFileCorrupt );

				rgpszT[ irgpsz++ ]	= szAbsPath;
				rgpszT[ irgpsz++ ]	= szOffset;
				rgpszT[ irgpsz++ ]	= szLength;
				rgpszT[ irgpsz++ ]	= szError;

				UtilReportEvent(	eventError,
									LOGGING_RECOVERY_CATEGORY,
									LOG_RANGE_CHECKSUM_MISMATCH_ID,
									irgpsz,
									rgpszT,
									0,
									NULL,
									m_pinst );

				//	we are in soft-recovery mode

				UtilReportEvent(	eventError,
									LOGGING_RECOVERY_CATEGORY, 
									LOG_CORRUPTION_DURING_SOFT_RECOVERY_ID, 
									3, 
									rgpsz,
									0,
									NULL,
									m_pinst );

				Call( ErrERRCheck( JET_errLogFileCorrupt ) );
				}
			else
				{
#ifdef ENABLE_LOGPATCH_TRACE
				if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
					{
					(*pcprintfLogPatch)( "corruption detected\r\n" );
					}
#endif	//	ENABLE_LOGPATCH_TRACE

				//	we are in the dump code

				//	try to make sure this really is the dump code

				Assert( fOldRecovering && fOldRecoveringMode == fRecoveringNone );
				Assert( !m_fHardRestore );
				Assert( fCreatedLogReader );

				//  report log file corruption to the event log

				const _TCHAR*	rgpszT[ 4 ];
				DWORD			irgpsz		= 0;
				_TCHAR			szAbsPath[ IFileSystemAPI::cchPathMax ];
				_TCHAR			szOffset[ 64 ];
				_TCHAR			szLength[ 64 ];
				_TCHAR			szError[ 64 ];

				if ( m_pfapiLog->ErrPath( szAbsPath ) < JET_errSuccess )
					{
					OSSTRCopy( szAbsPath, _T( "<< cannot get filepath >>" ) );
					}
				_stprintf( szOffset, _T( "%I64i (0x%016I64x)" ), ibOffset, ibOffset );
				_stprintf( szLength, _T( "%u (0x%08x)" ), cbLength, cbLength );
				_stprintf( szError, _T( "%i (0x%08x)" ), JET_errLogFileCorrupt, JET_errLogFileCorrupt );

				rgpszT[ irgpsz++ ]	= szAbsPath;
				rgpszT[ irgpsz++ ]	= szOffset;
				rgpszT[ irgpsz++ ]	= szLength;
				rgpszT[ irgpsz++ ]	= szError;

				UtilReportEvent(	eventError,
									LOGGING_RECOVERY_CATEGORY,
									LOG_RANGE_CHECKSUM_MISMATCH_ID,
									irgpsz,
									rgpszT,
									0,
									NULL,
									m_pinst );

				//	return corruption

				Call( ErrERRCheck( JET_errLogFileCorrupt ) );
				}
			}
		}
	else
		{

		//	we did not need to scan for a torn-write or corruption
		//	however, we should scan the rest of the log to make sure we only see the pattern
		//	(if we find anything, we must assume that the log is corrupt [probably due to an I/O not making it out to disk])

		ULONG		isecScan;
		ULONG		csecScan		= 0;
		BYTE 		*pbEnsureScan	= NULL;
		BYTE		*pbScan			= NULL;
		BOOL		fFoundPattern	= fTrue;	//	assume we will find a torn write
		CHAR 		szSector[30];
		CHAR		szCorruption[30];
		const CHAR*	rgpsz[3]		= { m_szLogName, szSector, szCorruption };

		//	prepare the event-log messages

		OSSTRCopyA( szSector, "END" );
		szCorruption[0] = 0;

		//	we should NEVER EVER point past the end of the log file

		Assert( lgposEnd.isec <= ( m_csecLGFile - 1 ) );

		//	get the starting sector for the scan

		isecScan = lgposEnd.isec;
		if ( lgposEnd.ib )
			{
			isecScan++;					//	round up to the next sector
			}
		isecScan++;						//	skip the shadow sector
	
		//	determine if there is any data to scan through
		
		if ( isecScan < m_csecLGFile )
			{

			//	read in the data we need to scan 
			//	(including the extra sector we reserve for the final shadow sector)

			Call( plread->ErrEnsureSector( isecScan, m_csecLGFile - isecScan, &pbEnsureScan ) );

			//	scan sector by sector looking for the log-extend pattern

			Assert( cbLogExtendPattern >= m_cbSec );
			pbScan = pbEnsureScan;
			csecScan = 0;
			while ( isecScan + csecScan < m_csecLGFile )
				{

				//	does the entire sector match the log-extend pattern?
					
				if ( memcmp( pbScan, rgbLogExtendPattern, m_cbSec ) != 0 )
					{

					//	no; this is not a torn write

					fFoundPattern = fFalse;
					break;
					}
					
				//	advance the scan counters

				pbScan += m_cbSec;
				csecScan++;
				}

			if ( !fFoundPattern )
				{

				//	we didn't find the pattern -- the log file must be corrupt!

				sprintf( szCorruption, "%d (0x%08X)", isecScan + csecScan, isecScan + csecScan );

				//	this should only happen to edb.log

				Assert( fJetLog );

				if ( m_fHardRestore )
					{

					//	we are in hard-recovery mode

					if ( lgposScan.lGeneration <= m_lGenHighRestore )
						{
#ifdef ENABLE_LOGPATCH_TRACE
						if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
							{
							(*pcprintfLogPatch)( "corruption detected during hard restore (this log is in the backup set) -- data was found after the end of the log!\r\n" );
							}
#endif	//	ENABLE_LOGPATCH_TRACE

						//  report log file corruption to the event log

						Assert( lgposScan.lGeneration >= m_lGenLowRestore );
						UtilReportEvent(	eventError, 
											LOGGING_RECOVERY_CATEGORY, 
											LOG_CORRUPTION_DURING_HARD_RESTORE_ID, 
											3, 
											rgpsz,
											0,
											NULL,
											m_pinst );

						Call( ErrERRCheck( JET_errLogCorruptDuringHardRestore ) );
						}

#ifdef ENABLE_LOGPATCH_TRACE
					if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
						{
						(*pcprintfLogPatch)( "corruption detected during hard recovery -- data was found after the end of the log!\r\n" );
						}
#endif	//	ENABLE_LOGPATCH_TRACE

					//  report log file corruption to the event log

					UtilReportEvent(	eventError,
										LOGGING_RECOVERY_CATEGORY, 
										LOG_CORRUPTION_DURING_HARD_RECOVERY_ID, 
										3, 
										rgpsz,
										0,
										NULL,
										m_pinst );

					Call( ErrERRCheck( JET_errLogCorruptDuringHardRecovery ) );
					}
				else if ( !fReadOnly )
					{
#ifdef ENABLE_LOGPATCH_TRACE
					if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
						{
						(*pcprintfLogPatch)( "corruption detected during soft recovery -- data was found after the end of the log!\r\n" );
						}
#endif	//	ENABLE_LOGPATCH_TRACE

					//  report log file corruption to the event log

					UtilReportEvent(	eventError,
										LOGGING_RECOVERY_CATEGORY, 
										LOG_CORRUPTION_DURING_SOFT_RECOVERY_ID, 
										3, 
										rgpsz,
										0,
										NULL,
										m_pinst );

					Call( ErrERRCheck( JET_errLogFileCorrupt ) );
					}
				else
					{
#ifdef ENABLE_LOGPATCH_TRACE
					if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
						{
						(*pcprintfLogPatch)( "corruption detected -- data was found after the end of the log!\r\n" );
						}
#endif	//	ENABLE_LOGPATCH_TRACE

					//	we are in the dump code

					//	try to make sure this really is the dump code

					Assert( fOldRecovering && fOldRecoveringMode == fRecoveringNone );
					Assert( !m_fHardRestore );
					Assert( fCreatedLogReader );

					Call( ErrERRCheck( JET_errLogFileCorrupt ) );
					}
				}
			}
		else
			{
			
			//	there is nothing to scan because we are at the end of the log file

			//	nop
			}
		}

	//	the log should be completely valid or atleast fixable by now
	//		lgposLast is the last LRCHECKSUM record we verified and now trust
	//		lgposEnd is the absolute end of good data (this marks the area we are going to
	//			chop off and treat as garbage -- it include any corrupt data we don't trust)
	//
	//	start searching the last range of log records for markings of a graceful exit:
	//		lrtypTerm, lrtypRecoveryQuit log records are what we want
	//		lrtypNOP, lrtypChecksum records have no effect and are ignored
	//		all other log records indicate a non-graceful exit
	//
	//	NOTE: since we trust the data we are scanning (because we verified it with the checksum),
	//		  we assume that every log record will be recognizable

	//	start at the first record after the last good LRCHECKSUM record

	lgposCurrent = lgposLast;
	AddLgpos( &lgposCurrent, sizeof( LRCHECKSUM ) );

	//	read in the LRCHECKSUM record

	Call( plread->ErrEnsureSector( lgposLast.isec, 1, &pbEnsure ) );
	plrck = (LRCHECKSUM *)( pbEnsure + lgposLast.ib );
	AssertValidLRCKRecord( plrck, &lgposLast );

	//	get the sector position of the next LRCHECKSUM record (if there is one)
	//
	//	we will use this in the record-to-record scan to determine when we have crossed over
	//		into the backward range of the next LRCHECKSUM record; more specifically, it will
	//		be important when the backward range of the next LRCHECKSUM record is zero even
	//		though there is space there (e.g. not covered by a checksum record)
	//	SPECIAL CASE: when fSkipScanAndApplyPatch is set, we need to bypass using lgposNext
	//					even though it SHOULD work out (not point in testing unfriendly waters)

	lgposNext = lgposMin;
	if ( plrck->le_cbNext != 0 && !fSkipScanAndApplyPatch )
		{

		//	point to the next LRCHECKSUM record 
		
		lgposNext = lgposLast;
		AddLgpos( &lgposNext, sizeof( LRCHECKSUM ) + plrck->le_cbNext );
		Assert( lgposNext.isec > lgposLast.isec );

		//	bring in the next LRCHECKSUM record

		Call( plread->ErrEnsureSector( lgposNext.isec, 1, &pbEnsure ) );
		plrck = (LRCHECKSUM *)( pbEnsure + lgposNext.ib );

		//	if the LRCHECKSUM is invalid or the backward range covers all data, 
		//		then we won't need to use lgposNext

		if ( !FValidLRCKRecord( plrck, &lgposNext ) || plrck->le_cbBackwards == lgposNext.ib )
			{
			lgposNext.isec = 0;
			}
		else
			{

			//	there is no need to validate the range to verify the data in the LRCHECKSUM record's
			//		backward range; look at the cases where we could have a valid record and an invalid range:
			//			- multi-sector LRCHECKSUM with a short checksum
			//			- single-sector LRCHECKSUM with no short checksum
			//
			//		in the case of the short checksum, the validation of the LRCHECKSUM record's backward
			//			range is the short checksum itself
			//		in the case of not having a short checksum, we must be looking at:
			//			- a valid range
			//			- an invalid range that was fixed by patching with the shadow sector
			//			***> there is no chance of a torn-write because that would mean the next LRCHECKSUM 
			//				 pointer would be aimed at the pattern, and FValidLRCKRecord would have failed
			//			***> there is no chance of corruption because that would have already been detected
			//				 and handled by dispatching an error
			
			Assert( plrck->le_cbBackwards == 0 );

//
//	NOT ANY MORE -- WE LEAVE THE OLD DATA THERE FOR DEBUGGING PURPOSES
//
//			//	the area before the LRCHECKSUM record should be the pattern
//			
//			Assert( memcmp( pbEnsure, rgbLogExtendPattern, lgposNext.ib ) == 0 );
			}
		}

	//	if the end of data is before this LRCHECKSUM, then we won't need to use lgposNext
		
	if ( CmpLgpos( &lgposNext, &lgposEnd ) >= 0 )
		{		
		lgposNext.isec = 0;
		}

#ifdef ENABLE_LOGPATCH_TRACE
	if ( pcprintfLogPatch && FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
		{
		(*pcprintfLogPatch)( "scanning the log records in the last LRCK range\r\n" );
		(*pcprintfLogPatch)( "\tlgposCurrent = {0x%x,0x%x,0x%x}\r\n", lgposCurrent.lGeneration, lgposCurrent.isec, lgposCurrent.ib );
		(*pcprintfLogPatch)( "\tlgposEnd     = {0x%x,0x%x,0x%x}\r\n", lgposEnd.lGeneration, lgposEnd.isec, lgposEnd.ib );
		(*pcprintfLogPatch)( "\tlgposNext    = {0x%x,0x%x,0x%x}\r\n", lgposNext.lGeneration, lgposNext.isec, lgposNext.ib  );
		}
#endif	//	ENABLE_LOGPATCH_TRACE

	//	loop forever

	forever
		{
		BYTE	*pb;
		UINT	cIteration, cb;
		LGPOS	lgposEndOfRec;

		//	bring in an entire log record in three passes
		//		first, bring in the byte for the LRTYP
		//		second, bring in the FIXED portion of the record
		//		third, bring in the entire record

RestartIterations:

		//	have we passed the end of the good data area?
		
		if ( CmpLgpos( &lgposCurrent, &lgposEnd ) >= 0 )
			{
			
			//	yes; stop looping

			goto DoneRecordScan;
			}


		for ( cIteration = 0; cIteration < 3; cIteration++ )
			{
			if ( cIteration == 0 )
				{
				cb = sizeof( LRTYP );
				Assert( sizeof( LRTYP ) == 1 );
				}
			else if ( cIteration == 1 )
				{
				cb = CbLGFixedSizeOfRec( reinterpret_cast< const LR * >( pb ) );
				}
			else
				{
				Assert( cIteration == 2 );
				cb = CbLGSizeOfRec( reinterpret_cast< const LR * >( pb ) );
				}

			//	point to the end of the data we want

			lgposEndOfRec = lgposCurrent;
			AddLgpos( &lgposEndOfRec, cb );

			//	have we passed the end of the good data area?
			
			if ( CmpLgpos( &lgposEndOfRec, &lgposEnd ) > 0 )
				{
				
				//	yes; stop looping

				goto DoneRecordScan;
				}

			//	see if we have crossed over into the area before the next LRCHECKSUM record which is
			//		not covered by the backward range

			if ( lgposNext.isec )
				{

				//	we should always land exactly on the sector with the next LRCHECKSUM record
				//		(we can't shoot past it -- otherwise, we would have overwritten it)
				
				Assert( lgposEndOfRec.isec <= lgposNext.isec );

				//	see if we are in the same sector
				
				if ( lgposEndOfRec.isec == lgposNext.isec )
					{

					//	we are pointing into the area before the next LRCHECKSUM record
					//	this area is not be covered by the backward range, so we need to do a fixup
					//		on lgposCurrent by moving it to the LRCHECKSUM record itself
					
					lgposCurrent = lgposNext;

					//	do not let this run again

					lgposNext.isec = 0;

					//	restart the iterations
					
					goto RestartIterations;
					}
				}

			//	read the data we need for this iteration

			Assert( cb > 0 );
			Call( plread->ErrEnsureSector( lgposCurrent.isec, ( lgposCurrent.ib + cb - 1 ) / m_cbSec + 1, &pbEnsure ) );

			//	set a pointer to it's record type information (1 byte)

			pb = pbEnsure + lgposCurrent.ib;

			//	make sure we recognize the log record

			Assert( *pb >= 0 && *pb < lrtypMax );
			}


		//	we now have a complete log record

		Assert( *pb >= 0 && *pb < lrtypMax );
		Assert( cb == CbLGSizeOfRec( (LR *)pb ) );
		
		//	check the type

		if ( *pb == lrtypTerm || *pb == lrtypTerm2 || *pb == lrtypRecoveryQuit || *pb == lrtypRecoveryQuit2 )
			{
			//	these types indicate a clean exit
			
			fGotQuit = fTrue;
			}
		else if ( *pb == lrtypNOP )
			{
			//	NOPs are ignored and have no influence on the fGotQuit flag
			}
		else if ( *pb == lrtypChecksum )
			{
			
			//	LRCKECUSMs are ignored and have no influence on the fGotQuit flag

			//	remember an LRCHECKSUM record as good if we see one

			lgposLast = lgposCurrent;

			//	this should only happen in the case where we have no corruption and no forward range
			//		in the current LRCHECKSUM

			//	we shouldn't have scanned for corruption

			Assert( !fDoScan );

			//	we should find this record right before we hit the end of data

#ifdef DEBUG
			LGPOS lgposT = lgposLast;
			AddLgpos( &lgposT, sizeof( LRCHECKSUM ) );
			Assert( CmpLgpos( &lgposT, &lgposEnd ) == 0 );
#endif
			}
		else
			{

			//	we cannot assume a clean exit since the last record we saw was not a Term or a Recovery-Quit

			fGotQuit = fFalse;
			}

		//	set the current log position to the next log record

		lgposCurrent = lgposEndOfRec;
		}

DoneRecordScan:

	if ( fRecordOkRangeBad )
		{

		//	we need to do a fixup in the case where we successfully verified the sector with the LRCHECKSUM 
		//		record (using the short checksum value) but could not verify the other sectors in the range 
		//		of the LRCHECKSUM record (using the long checksum)
		//
		//	the problem is that the "forward" pointer reaches past the sector boundary of the good sector
		//		(which is holding the LRCHECKSUM record), and, in doing so, it covers bad data
		//
		//	the fix is to shorten the "forward" pointer to include only the good data (up to lgposCurrent)
		//		and update the corrupt area in the log file with this new LRCHECKSUM record

		Assert( !fSingleSectorTornWrite );
		Assert( fDoScan );		//	we should have seen corruption or a torn-write

		//	load the sector with the last good LRCHECKSUM record

		Call( plread->ErrEnsureSector( lgposLast.isec, 1, &pbEnsure ) );
		plrck = reinterpret_cast< LRCHECKSUM* >( pbEnsure + lgposLast.ib );

		AssertValidLRCKRecord( plrck, &lgposLast );		//	it should be valid
		Assert( plrck->bUseShortChecksum == bShortChecksumOn );	//	it should be a multi-sector range
		Assert( plrck->le_cbBackwards == ( (BYTE *)plrck - PbSecAligned( (BYTE *)plrck ) )
			|| 0 == plrck->le_cbBackwards );
		const UINT cbBackwards		= ULONG( (BYTE *)plrck - PbSecAligned( (BYTE *)plrck ) );

		//	calculate the byte offset (this may have wrapped around to 0 when we were scanning record-to-record)

		const UINT ib = ( lgposCurrent.ib > 0 ) ? lgposCurrent.ib : m_cbSec;

		//	the current position should be at or past the point in the current sector immediately
		//	following the backward range and the LRCHECKSUM record itself
		//		(lgposCurrent should point at the first bad log record)

		Assert( ib >= cbBackwards + sizeof( LRCHECKSUM ) );

		//	remember that we must overwrite the part of the LRCHECKSUM range that we are "chopping off"

		Assert( 0 == ( lgposLast.ib + sizeof( LRCHECKSUM ) + plrck->le_cbForwards ) % m_cbSec );
		const ULONG csecRange = ( lgposLast.ib + sizeof( LRCHECKSUM ) + plrck->le_cbForwards ) / m_cbSec;
		if ( csecRange > 2 )
			{
			isecPatternFill = lgposLast.isec + 2;	//	skip the sector we are patching and its shadow
			csecPatternFill = csecRange - 2;
			}
		else
			{

			//	the range should only encompass 2 sectors
			
			//	we don't need to rewrite the logfile pattern because those 2 sectors will be overwritten
			//	when we write the patched sector and its shadow

			Assert( 2 == csecRange );
			}

		//	adjust the "forward" pointer

		plrck->le_cbForwards = ib - ( cbBackwards + sizeof( LRCHECKSUM ) );

		//	there is no data after this LRCHECKSUM record

		plrck->le_cbNext = 0;

		//	copy the last sector into pbLastSector for the next flush

		UtilMemCpy( pbLastSector, pbEnsure, m_cbSec );

		//	prepare pbLastChecksum and isecLastWrite for the next flush

		pbLastChecksum = pbLastSector + cbBackwards;
		isecLastSector = lgposLast.isec;
		}

	//	we have now completed scanning the last range of log records covered by the
	//		last good LRCHECKSUM record and we know if a graceful exit was previously made (fGotQuit)

	//	set the graceful exit flag for the user

	Assert( pfCloseNormally );
	*pfCloseNormally = fGotQuit;

	//	set m_lgposLastRec to the lesser of lgposCurrent or lgposEnd for REDO time 
	//		(the REDO engine will not exceed this log position)
	//	in most cases, lgposCurrent will be less than lgposEnd and will point to the first BAD log record
	//	in the rare case where we don't have a single good LRCHECKSUM record, lgposCurrent will point past
	//		lgposEnd due to the way we initialized it before entering the search-loop

	if ( CmpLgpos( &lgposCurrent, &lgposEnd ) <= 0 )
		{
		m_lgposLastRec = lgposCurrent;
		}
	else
		{
		m_lgposLastRec = lgposEnd;
		}

#ifdef ENABLE_LOGPATCH_TRACE
	if ( pcprintfLogPatch && FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
		{
		(*pcprintfLogPatch)( "finished scanning the last LRCK range\r\n" );
		(*pcprintfLogPatch)( "\tfGotQuit            = %s\r\n", ( fGotQuit ? "TRUE" : "FALSE" ) );
		(*pcprintfLogPatch)( "\tLOG::m_lgposLastRec = {0x%x,0x%x,0x%x}\r\n", m_lgposLastRec.lGeneration, m_lgposLastRec.isec, m_lgposLastRec.ib );
		}
#endif	//	ENABLE_LOGPATCH_TRACE

	//	if this is a single-sector torn-write case, we need to patch the forward/next pointers

	if ( fSingleSectorTornWrite )
		{

		//	the first bad log record should be in the same sector as the checksum record
		//		OR should be aligned perfectly with the start of the next sector

		Assert( ( m_lgposLastRec.isec == lgposLast.isec && CmpLgpos( &lgposLast, &m_lgposLastRec ) < 0 ) ||
				( m_lgposLastRec.isec == lgposLast.isec + 1 && m_lgposLastRec.ib == 0 ) );

		//	point to the LRCHECKSUM record within the copy of the last sector

		Assert( pbLastChecksum != pbNil );
		plrck = (LRCHECKSUM *)pbLastChecksum;

		//	reset the pointers

		const UINT ibT = ( m_lgposLastRec.ib == 0 ) ? m_cbSec : m_lgposLastRec.ib;
		Assert( ibT >= lgposLast.ib + sizeof( LRCHECKSUM ) );
		plrck->le_cbForwards = ibT - ( lgposLast.ib + sizeof( LRCHECKSUM ) );
		plrck->le_cbNext = 0;
		}

	//	skip the write-error handler

	goto HandleError;


LHandleErrorWrite:

/********
//	when we fail to fixup the log file, is it a read or a write?
//	LGReportError( LOG_READ_ERROR_ID, err );
//	goto HandleError;
*********/

	//	report log-file write failures to the event log
	LGReportError( LOG_WRITE_ERROR_ID, err );


HandleError:

	//	handle errors and exit the function

	//	cleanup the LogReader class 

	if ( fCreatedLogReader )
		{
		ERR errT = JET_errSuccess;

		if ( plread->FInit() )
			{

			//	terminate the log reader

			errT = plread->ErrLReaderTerm();
			}
		
		//	delete the log reader

		delete plread;
		plread = NULL;

		//	fire off the error trap if necessary

		if ( errT != JET_errSuccess )
			{
			errT = ErrERRCheck( errT );
			}

		//	return the error code if no other error code yet exists

		if ( err == JET_errSuccess )
			{
			err = errT;
			}
		}
	else if ( NULL != plread )
		{

		//	we never created the log reader, so we can't destroy it because other users may have pointers to it
		//	instead, we just invalidate its data pointers forcing it go to disk the next time someone does a read

		plread->Reset();
		}

	//	if things went well, we can patch the current log before returning
	//		(we can only do this if we are in read/write mode and the log is edb.log)

	if ( err >= 0 && !fReadOnly && fJetLog )
		{

		//	we are not expecting any warnings here

		CallS( err );

		//	do we need to make a new log file?

		if ( fLogToNextLogFile )
			{

			//	since the log creation is deferred, we must setup the buffers here and let the log
			//		flush thread pick up on them later; however, if we didn't create the log reader,
			//		we will lose the contents of our buffers later when the log reader is terminated
			//	thus, we shouldn't bother doing anything until we own the log reader ourselves
			
			if ( fCreatedLogReader ) 
				{

				//	the last sector should not be setup

				Assert( pbLastChecksum == pbNil );

				//	the last LRCHECKSUM record should be completely clean

				Assert( !fRecordOkRangeBad );

				//	we should not have scanned for corruption

				Assert( !fDoScan );

				//	lock the log pointers

				m_critLGBuf.Enter();

				//	the first sector to write is the first sector in the next log file

				m_isecWrite					= m_csecHeader;

				//	the write pointer and the last-checksum pointer both start at the beginning of 
				//		the log buffer

				m_pbWrite					= m_pbLGBufMin;
				m_pbLastChecksum 			= m_pbLGBufMin;

				//	the entry pointer is the area in the log buffer immediately following the  
				//		newly-created LRCHECKSUM record

				m_pbEntry					= m_pbLastChecksum + sizeof( LRCHECKSUM );

				//	setup the beginning of the new log generation with a single LRCHECKSUM record; 
				//		put it at the start of the log buffers

				Assert( pbNil != m_pbLGBufMin );
				memset( m_pbLGBufMin, 0, sizeof( LRCHECKSUM ) );
				plrck = reinterpret_cast< LRCHECKSUM* >( m_pbLastChecksum );
				plrck->lrtyp = lrtypChecksum;

				//	the next flush should go all the way up to (but not including) the newly-created 
				//		LRCHECKSUM record

				m_lgposToFlush.isec 		= USHORT( m_isecWrite );
				m_lgposToFlush.lGeneration 	= m_plgfilehdr->lgfilehdr.le_lGeneration + 1;
				m_lgposToFlush.ib 			= 0;

				//	the maximum flush position is the next flush position

				m_lgposMaxFlushPoint 		= m_lgposToFlush;

				//	setup the rest of the LRCHECKSUM record

				plrck->bUseShortChecksum = bShortChecksumOff;
				plrck->le_ulShortChecksum = 0;
				plrck->le_ulChecksum = UlComputeChecksum( plrck, m_lgposToFlush.lGeneration );

				//	make sure the LRCHECKSUM record is valid

				AssertValidLRCKRecord( plrck, &m_lgposToFlush );
				AssertValidLRCKRange( plrck, m_lgposToFlush.lGeneration );

#ifdef ENABLE_LOGPATCH_TRACE
				if ( FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
					{
					(*pcprintfLogPatch)( "forcing a new log generation because fLogToNextLogFile=TRUE\r\n" );
					}
#endif	//	ENABLE_LOGPATCH_TRACE

				//	unlock the log pointers

				m_critLGBuf.Leave();
				}
			}
		else if ( pbLastSector != pbNil )
			{
			BYTE	*pbEndOfData, *pbWrite;
			INT		isecWrite;

			//	we need to patch the last sector of the current log

			//	lock the log pointers

			m_critLGBuf.Enter();

			//	transfer the copy of the last sector into the log buffers

			Assert( pbNil != m_pbLGBufMin );
			UtilMemCpy( m_pbLGBufMin, pbLastSector, m_cbSec );

			//	calculate the distance of the backward pointer 

			const UINT cbBackwards 		= ULONG( pbLastChecksum - pbLastSector );

			//	set the last LRCHECKSUM record pointer to the LRECHECKSUM record within the log buffer

			m_pbLastChecksum 			= m_pbLGBufMin + cbBackwards;

			//	get a pointer to the copy of the last LRCHECKSUM record
			
			plrck = reinterpret_cast< LRCHECKSUM* >( m_pbLastChecksum );

			//	the first sector to write was previously determined

			Assert( isecLastSector >= m_csecHeader && isecLastSector <= ( m_csecLGFile - 1 ) );

			m_isecWrite 				= isecLastSector;

			//	the write pointer is the beginning of the area we just copied into the log buffers
			//		from pbLastSector (it will all need to be flushed again when the LRCHECKSUM record
			//		is updated with a new cbNext, checksum value, etc...)

			m_pbWrite 					= m_pbLGBufMin;

			//	set the entry pointer to the area after the forward range of the LRCHECKSUM record
			
			m_pbEntry 					= m_pbLastChecksum + sizeof( LRCHECKSUM ) + plrck->le_cbForwards;

			//	the next flush position is the area after the existing validated range of 
			//		the LRCHECKSUM record

			m_lgposToFlush.isec 		= USHORT( m_isecWrite );
			m_lgposToFlush.lGeneration 	= m_plgfilehdr->lgfilehdr.le_lGeneration;
			m_lgposToFlush.ib 			= 0;
			AddLgpos( &m_lgposToFlush, cbBackwards + sizeof( LRCHECKSUM ) + plrck->le_cbForwards );

			//	the maximum flush position is the next flush position

			m_lgposMaxFlushPoint 		= m_lgposToFlush;

			//	setup the rest of the LRCHECKSUM record

			plrck->bUseShortChecksum = bShortChecksumOff;
			plrck->le_ulShortChecksum = 0;
			plrck->le_ulChecksum = UlComputeChecksum( plrck, m_plgfilehdr->lgfilehdr.le_lGeneration );

			//	make sure the fix to the log is good

#ifdef DEBUG
			LGPOS lgposChecksum;
			lgposChecksum.ib			= (USHORT)cbBackwards;
			lgposChecksum.isec			= (USHORT)m_isecWrite;
			lgposChecksum.lGeneration	= m_plgfilehdr->lgfilehdr.le_lGeneration;
			AssertValidLRCKRecord( (LRCHECKSUM *)m_pbLastChecksum, &lgposChecksum );
			AssertValidLRCKRange( (LRCHECKSUM *)m_pbLastChecksum, lgposChecksum.lGeneration );
#endif									  

			//	capture the data pointers

			pbEndOfData 				= m_pbEntry;
			pbWrite 					= m_pbWrite;
			isecWrite 					= m_isecWrite;

#ifdef ENABLE_LOGPATCH_TRACE
			if ( pcprintfLogPatch && FLGILogPatchDate( szLogPatchPath, &pcprintfLogPatch ) )
				{
				(*pcprintfLogPatch)( "!!DANGER WILL ROBINSON!! we are about to PATCH the last LRCHECKSUM range\r\n" );

				(*pcprintfLogPatch)( "\tisecPatternFill           = 0x%x\r\n", isecPatternFill );
				(*pcprintfLogPatch)( "\tcsecPatternFill           = 0x%x\r\n", csecPatternFill );
				(*pcprintfLogPatch)( "\tLOG::m_isecWrite          = 0x%08x\r\n", m_isecWrite );
				(*pcprintfLogPatch)( "\t\tLOG::m_pbWrite         = 0x%0*I64x\r\n", sizeof( char* ) * 2, QWORD( m_pbWrite ) );
				(*pcprintfLogPatch)( "\t\tLOG::m_pbEntry         = 0x%0*I64x\r\n", sizeof( char* ) * 2, QWORD( m_pbEntry ) );
				(*pcprintfLogPatch)( "\tLOG::m_lgposToFlush       = {0x%x,0x%x,0x%x}\r\n", m_lgposToFlush.lGeneration, m_lgposToFlush.isec, m_lgposToFlush.ib );
				(*pcprintfLogPatch)( "\tLOG::m_lgposMaxFlushPoint = {0x%x,0x%x,0x%x}\r\n", m_lgposMaxFlushPoint.lGeneration, m_lgposMaxFlushPoint.isec, m_lgposMaxFlushPoint.ib );
				(*pcprintfLogPatch)( "\t\tLOG::m_pbLGBufMin (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( m_pbLGBufMin ) );

				ULONG _ibdumpT_ = 0;
				while ( _ibdumpT_ < m_cbSec )
					{
					(*pcprintfLogPatch)( "\t0x%04x: %02x %02x %02x %02x %02x %02x %02x %02x-%02x %02x %02x %02x %02x %02x %02x %02x\r\n",
										_ibdumpT_,
										m_pbLGBufMin[_ibdumpT_+0],  m_pbLGBufMin[_ibdumpT_+1],  m_pbLGBufMin[_ibdumpT_+2],  m_pbLGBufMin[_ibdumpT_+3],
										m_pbLGBufMin[_ibdumpT_+4],  m_pbLGBufMin[_ibdumpT_+5],  m_pbLGBufMin[_ibdumpT_+6],  m_pbLGBufMin[_ibdumpT_+7],
										m_pbLGBufMin[_ibdumpT_+8],  m_pbLGBufMin[_ibdumpT_+9],  m_pbLGBufMin[_ibdumpT_+10], m_pbLGBufMin[_ibdumpT_+11],
										m_pbLGBufMin[_ibdumpT_+12], m_pbLGBufMin[_ibdumpT_+13], m_pbLGBufMin[_ibdumpT_+14], m_pbLGBufMin[_ibdumpT_+15] );
					_ibdumpT_ += 16;
					Assert( _ibdumpT_ <= m_cbSec );
					}

				LRCHECKSUM *_plrckT_ = (LRCHECKSUM *)m_pbLastChecksum;

				(*pcprintfLogPatch)( "\tLOG::m_pbLastChecksum (0x%0*I64x)\r\n", sizeof( char* ) * 2, QWORD( _plrckT_ ) );
				(*pcprintfLogPatch)( "\t  cbBackwards       = 0x%08x\r\n", _plrckT_->le_cbBackwards );
				(*pcprintfLogPatch)( "\t  cbForwards        = 0x%08x\r\n", _plrckT_->le_cbForwards );
				(*pcprintfLogPatch)( "\t  cbNext            = 0x%08x\r\n", _plrckT_->le_cbNext );
				(*pcprintfLogPatch)( "\t  ulChecksum        = 0x%08x\r\n", _plrckT_->le_ulChecksum );
				(*pcprintfLogPatch)( "\t  ulShortChecksum   = 0x%08x\r\n", _plrckT_->le_ulShortChecksum );
				(*pcprintfLogPatch)( "\t  bUseShortChecksum = %s (0x%02x)\r\n", 
									( bShortChecksumOn == _plrckT_->bUseShortChecksum ?
									  "Yes" : 
									  ( bShortChecksumOff == _plrckT_->bUseShortChecksum ?
									    "No" : "???" ) ),
									BYTE( _plrckT_->bUseShortChecksum ) );
				}
#endif	//	ENABLE_LOGPATCH_TRACE

			//	unlock the log pointers

			m_critLGBuf.Leave();

			//	flush the fixup to the log right now

//
//	SEARCH-STRING: SecSizeMismatchFixMe
//
Assert( m_cbSec == m_cbSecVolume );
//			if ( m_cbSec == m_cbSecVolume )
//				{
				if ( 0 != csecPatternFill )
					{
					ULONG csecPatternFillT;
					ULONG csecT;

					//	we are chopping off some partial data to patch a torn-write

					Assert( isecPatternFill >= m_csecHeader );
					Assert( isecPatternFill < m_csecLGFile - 1 );
					Assert( isecPatternFill + csecPatternFill <= m_csecLGFile - 1 );
					Assert( fRecordOkRangeBad );

					//	rewrite the logfile pattern to "erase" the partial data

					csecPatternFillT = 0;
					while ( csecPatternFillT < csecPatternFill )
						{

						//	compute the size of the next write

						csecT = min( csecPatternFill - csecPatternFillT, cbLogExtendPattern / m_cbSec );

						//	do the write

						Assert( isecPatternFill + csecPatternFillT + csecT <= m_csecLGFile - 1 );
						err = m_pfapiLog->ErrIOWrite(	( isecPatternFill + csecPatternFillT ) * m_cbSec,
														csecT * m_cbSec,
														rgbLogExtendPattern );
						if ( err < JET_errSuccess )
							{
							LGReportError( LOG_FLUSH_WRITE_5_ERROR_ID, err );
							SetFNoMoreLogWrite( err );
							break;
							}

						//	advance the counters

						csecPatternFillT += csecT;
						}
					}

				if ( err >= JET_errSuccess )
					{

					//	flush the sector and shadow it

					err = ErrLGIWritePartialSector( pbEndOfData, isecWrite, pbWrite );
					}
//				}
//			else
//				{
//				fWriteOnSectorSizeMismatch = fTrue;
//				}
			}
		}

//
//	SEARCH-STRING: SecSizeMismatchFixMe
//
//	if ( err >= 0 && fWriteOnSectorSizeMismatch )
//		{
//
//		//	we tried to write to this log file, but our sector size was wrong
//
//		//	return a general sector-size mismatch error to indicate that the databases are
//		//		not necessarily consistent
//
//		err = ErrERRCheck( JET_errLogSectorSizeMismatch );
//		}

#ifndef LOGPATCH_UNIT_TEST

	//	re-open the log file in read-only mode
	
	if ( !fReadOnly )
		{
		ERR errT;

		//	is the file open?  close it
		
		delete m_pfapiLog;
		m_pfapiLog = NULL;

		//	check the name of the file

		Assert( NULL != m_szLogName );
		Assert( '\0' != m_szLogName[ 0 ] );

		//	open the file again

		errT = pfsapi->ErrFileOpen( m_szLogName, &m_pfapiLog, fTrue );

		//	fire off the error trap if necessary

		if ( errT != JET_errSuccess )
			{
			errT = ErrERRCheck( errT );
			}

		//	return the error code if no other error code yet exists

		if ( err == JET_errSuccess )
			{
			err = errT;
			}
		}

#endif	//	!LOGPATCH_UNIT_TEST

	//	free the memory holding the copy of the last sector
			
	OSMemoryHeapFree( pbLastSector );

	//	restore the old state

	m_fRecovering = fOldRecovering;
	m_fRecoveringMode = fOldRecoveringMode;

	//	leave the log-flush critical section

	m_critLGFlush.Leave();

#ifdef ENABLE_LOGPATCH_TRACE

	//	close the output file

	if ( pcprintfLogPatch )
		{
		delete pcprintfLogPatch;
		}
#endif	//	ENABLE_LOGPATCH_TRACE

	//	return any error code we have
	
	return err;
	}


/*
 *  Read first record pointed by plgposFirst.
 *  Initialize m_isecRead, m_pbRead, and m_pbNext.
 *	The first redo record must be within the good portion
 *	of the log file.
 */

//  VC21:  optimizations disabled due to code-gen bug with /Ox
#pragma optimize( "agw", off )
//	Above comment is referring to ErrLGLocateFirstRedoLogRec(), not the
//	FASTFLUSH version, which has never been tried with opt. on.

const UINT cPagesPrereadInitial = 512;
const UINT cPagesPrereadThreshold = 256;
const UINT cPagesPrereadAmount = 256;

ERR LOG::ErrLGLocateFirstRedoLogRecFF
	(
	LE_LGPOS	*ple_lgposRedo,	
	BYTE		**ppbLR			
	)
	{
	ERR			err				= JET_errSuccess;
	LGPOS		lgposCurrent	= { 0, USHORT( m_csecHeader ), m_plgfilehdr->lgfilehdr.le_lGeneration };
	LRCHECKSUM	* plrck			= pNil;
	BYTE		* pbEnsure		= pbNil;	// start of data we're looking at in buffer
	BYTE		* pb			= pbNil;
	LogReader	* plread		= m_plread;
	UINT		cb				= 0;
	BOOL		fValid			= fFalse;

	m_critLGFlush.Enter();

	//	m_lgposLastRec should be set

	Assert( m_lgposLastRec.isec >= m_csecHeader && 
			( m_lgposLastRec.isec < ( m_csecLGFile - 1 ) || 
			  ( m_lgposLastRec.isec == ( m_csecLGFile - 1 ) && m_lgposLastRec.ib == 0 ) ) &&
			m_lgposLastRec.lGeneration == m_plgfilehdr->lgfilehdr.le_lGeneration );

	//	see if we got corruption at the start of the log file
	//		if we have atleast sizeof( LRCHECKSUM ) bytes, we can read the first record which is an
	//		LRCHECKSUM record
	LGPOS lgposHack;
	lgposHack.ib			= sizeof( LRCHECKSUM );
	lgposHack.isec			= USHORT( m_csecHeader );
	lgposHack.lGeneration	= m_plgfilehdr->lgfilehdr.le_lGeneration;
	if ( CmpLgpos( &m_lgposLastRec, &lgposHack ) < 0 )
		{
		//	lgposHack was past the stopping point
		Call( errLGNoMoreRecords );
		}

	*ppbLR = pbNil;
	m_pbNext = pbNil;
	m_pbRead = pbNil;
	m_isecRead = 0;
	m_pbLastChecksum = pbNil;
	m_lgposLastChecksum = lgposMin;

	Assert( pNil != plread );
	
	// This is lazy version of ErrLGLocateFirstRedoLogRec() that will only
	// start from the beginning of the log file.
	Assert( ple_lgposRedo->le_isec == m_csecHeader );
	Assert( ple_lgposRedo->le_ib == 0 );

	m_lgposLastChecksum.lGeneration	= m_plgfilehdr->lgfilehdr.le_lGeneration;
	m_lgposLastChecksum.isec		= USHORT( m_csecHeader );
	m_lgposLastChecksum.ib			= 0;

	// make sure we've got the right log file loaded up
	Call( plread->ErrEnsureLogFile() );

	// read in start of log file
	Call( plread->ErrEnsureSector( lgposCurrent.isec, 1, &pbEnsure ) );
	m_pbLastChecksum = pbEnsure + lgposCurrent.ib;
	plrck = reinterpret_cast< LRCHECKSUM* >( m_pbLastChecksum );

	fValid = FValidLRCKRecord( plrck, &lgposCurrent );

	//	the LRCHECKSUM should always be valid because ErrLGCheckReadLastLogRecordFF will have patched
	//		any and all corruption
	//	just in case...

	if ( !fValid )
		{
		AssertSz( fFalse, "Unexpected invalid LRCK record!" );
		//	start of log file was corrupted
		Call( ErrERRCheck( JET_errLogFileCorrupt ) );
		}

	// Pull in sectors that are part of the range
	Call( plread->ErrEnsureSector( lgposCurrent.isec, ( ( lgposCurrent.ib +
		sizeof( LRCHECKSUM ) + plrck->le_cbForwards - 1 ) / m_cbSec ) + 1,
		&pbEnsure ) );
	m_pbLastChecksum = pbEnsure + lgposCurrent.ib;
	plrck = reinterpret_cast< LRCHECKSUM* >( m_pbLastChecksum );

	fValid = FValidLRCKRange( plrck, m_plgfilehdr->lgfilehdr.le_lGeneration );

	//	the LRCHECKSUM should always be valid because ErrLGCheckReadLastLogRecordFF will have patched
	//		any and all corruption 
	//	just in case...

	if ( !fValid )
		{
		AssertSz( fFalse, "Unexpected invalid LRCK range!" );
		
		//	in a multi-sector flush, it is possible for us to be pointing to an LRCHECKSUM record with 
		//		a corrupt range yet still be able to use the data in the first sector because we
		//		verified it with the short checksum

		if ( plrck->bUseShortChecksum != bShortChecksumOn )
			{

			//	no short checksum -- return corruption

			Call( ErrERRCheck( JET_errLogFileCorrupt ) );
			}

		//	verify that the end of data marker is either already at the end of the current sector
		//		or it is a little before the end (e.g. it is at the start of the last log record in
		//		the sector and that log record stretches over into the next sector)

		Assert( ( m_lgposLastRec.isec == m_csecHeader && m_lgposLastRec.ib >= sizeof( LRCHECKSUM ) ) ||
				( m_lgposLastRec.isec == m_csecHeader + 1 && m_lgposLastRec.ib == 0 ) );
		}

	// we've got a range in the buffer and it's valid, so let's return
	// the LRCK to the clients.

	// LRCK is start of the file, so it must be there
	Assert( sizeof( LRCHECKSUM ) == CbLGFixedSizeOfRec( reinterpret_cast< const LR* >( pbEnsure ) ) );
	Assert( sizeof( LRCHECKSUM ) == CbLGSizeOfRec( reinterpret_cast< const LR* >( pbEnsure ) ) );

	//	setup return variables

	pb = pbEnsure;
	m_pbRead = plread->PbGetEndOfData();
	m_isecRead = plread->IsecGetNextReadSector();
	*ppbLR = m_pbNext = pb;
	
	//	we should have success at this point

	CallS( err );

	// convert m_pbNext & m_pbLastChecksum to LGPOS for preread.
	// Database preread for this log file starts at the same point
	// as the normal log record processing.
	Call( plread->ErrSaveState( &m_lrstatePreread ) );
	GetLgposDuringReading( m_pbNext, &m_lgposPbNextPreread );
	GetLgposDuringReading( m_pbLastChecksum, &m_lgposLastChecksumPreread );

	// Preread is enabled for this log file.
	m_fPreread = fTrue;
	m_cPageRefsConsumed = 0;

#ifdef DEBUG

	if ( getenv( "PREREAD_INITIAL" ) )
		{
		m_cPagesPrereadInitial = atoi( getenv( "PREREAD_INITIAL" ) );
		}
	else
		{
		m_cPagesPrereadInitial = cPagesPrereadInitial;
		}

	if ( getenv( "PREREAD_THRESH" ) )
		{
		m_cPagesPrereadThreshold = atoi( getenv( "PREREAD_THRESH" ) );
		}
	else
		{
		m_cPagesPrereadThreshold = cPagesPrereadThreshold;
		}

	if ( getenv( "PREREAD_AMOUNT" ) )
		{
		m_cPagesPrereadAmount = atoi( getenv( "PREREAD_AMOUNT" ) );
		}
	else
		{
		m_cPagesPrereadAmount = cPagesPrereadAmount;
		}

#else	//	!DEBUG

	m_cPagesPrereadInitial = cPagesPrereadInitial;
	m_cPagesPrereadThreshold = cPagesPrereadThreshold;
	m_cPagesPrereadAmount = cPagesPrereadAmount;

#endif	//	DEBUG

HandleError:

	//	did we get corruption?

	if ( err == JET_errLogFileCorrupt )
		{

		//	what recovery mode are we in?

		if ( m_fHardRestore )
			{

			//	we are in hard-recovery mode

			if ( m_plgfilehdr->lgfilehdr.le_lGeneration <= m_lGenHighRestore )
				{

				//	this generation is part of a backup set
				//	we must fail

				Assert( m_plgfilehdr->lgfilehdr.le_lGeneration >= m_lGenLowRestore );
				err = ErrERRCheck( JET_errLogCorruptDuringHardRestore );
				}
			else
				{

				//	the current log generation is not part of the backup-set
				//	we can patch the log and continue safely

				err = ErrERRCheck( JET_errLogCorruptDuringHardRecovery );
				}
			}
		else
			{

			//	we are in soft-recovery mode -- keep the original error JET_errLogFileCorrupt

			}
		}

	m_critLGFlush.Leave();

	// Do not preread if we had any kind of error
	// or if we're dumping the log
	if ( JET_errSuccess == err && ! m_fDumppingLogs )
		{
		// preread initial pages
		if ( m_cPagesPrereadInitial < m_cPagesPrereadThreshold )
			{
			// next preread will occur once initial page refs have been passed
			m_cPageRefsConsumed = m_cPagesPrereadThreshold - m_cPagesPrereadInitial;
			}
		else
			{
			m_cPageRefsConsumed = 0;
			}
		err = ErrLGIPrereadPages( m_cPagesPrereadInitial );
		}

	return err;
	}


#pragma optimize( "", on )

//	reads/verifies the next LRCHECKSUM record so we can get a complete log record that stretches across
//		two LRCHECKSUM ranges
//	reading the entire LRCHECKSUM range into memory ensures that we have the full log record
//		(the full log record can only use the backward range anyway)
//	
//	lgposNextRec is the start of the log record which stretches across the LRCHECKSUM boundary

//	Implicit input parameters:
//		m_plread
//			To read in the next CK
//		m_pbLastChecksum
//			The current CK record on input, so we can find the next one
//		m_lgposLastChecksum
//			LGPOS of the current CK record on input, so we can compute location of next
//		m_pbNext
//			Points to earliest record to keep in buffer. Nothing before will
//			be in buffer after we return.

ERR LOG::ErrLGIGetNextChecksumFF
	( 
	// LGPOS of next record we want to return to caller.
	// LGPOS of m_pbNext.
	LGPOS 		*plgposNextRec, 
	// Points to start of next record we want to return to caller (UNUSED!!!).
	BYTE 		**ppbLR
	)
	{
	ERR				err = JET_errSuccess;
	// plrck shadows m_pbLastChecksum so we don't have to cast all over.
	LRCHECKSUM*&	plrck = reinterpret_cast< LRCHECKSUM*& >( m_pbLastChecksum );
	LGPOS			lgposNextChecksum;
	UINT			csec;
	BYTE			*pbEnsure;
	LogReader 		*plread = m_plread; 


	//	m_lgposLastRec should be set

	Assert( m_lgposLastRec.isec >= m_csecHeader );
	Assert( m_lgposLastRec.isec < ( m_csecLGFile - 1 ) || 
			( m_lgposLastRec.isec == ( m_csecLGFile - 1 ) && m_lgposLastRec.ib == 0 ) );
	Assert( m_lgposLastRec.lGeneration == m_plgfilehdr->lgfilehdr.le_lGeneration );

	//	validate input params

	Assert( plread != pNil );
	Assert( plgposNextRec->isec >= m_csecHeader );
	Assert( plgposNextRec->isec < ( m_csecLGFile - 1 ) );
	Assert( ppbLR != pNil );
#ifdef DEBUG
	{
	//	check m_pbNext
	LGPOS lgposPbNext;
	GetLgposOfPbNext( &lgposPbNext );
	Assert( CmpLgpos( &lgposPbNext, plgposNextRec ) == 0 );
	}
#endif

	Assert( m_pbLastChecksum != pbNil );
	
	//	is there another LRCHECKSUM record to read?

	if ( plrck->le_cbNext == 0 )
		{

		//	this is the last LRCHECKSUM record meaning we have a partial record whose starting half is 
		//		hanging over the end of the current LRCHECKSUM and whose ending half is ???
		//	the only conclusion is that this must be corruption 

		//	set the end of data marker to this log record (we should never move forwards)

		Assert( CmpLgpos( &m_lgposLastRec, plgposNextRec ) >= 0 );
		m_lgposLastRec = *plgposNextRec;

		//	return the error for corruption

		CallR( ErrERRCheck( JET_errLogFileCorrupt ) );
		}

	//	calculate the log position of the next LRCHECKSUM record

	lgposNextChecksum = m_lgposLastChecksum;
	AddLgpos( &lgposNextChecksum, sizeof( LRCHECKSUM ) + plrck->le_cbNext );

		//	is the next LRCHECKSUM record beyond the end of data pointer?

		//	we must be able to read up to and including the LRCHECKSUM record

		{
		LGPOS lgposEndOfNextChecksumRec = lgposNextChecksum;
		AddLgpos( &lgposEndOfNextChecksumRec, sizeof( LRCHECKSUM ) );

		//	will we pass the end of good data?
			
		if ( CmpLgpos( &lgposEndOfNextChecksumRec, &m_lgposLastRec ) > 0 )
			{

			//	we cannot safely read this LRCHECKSUM record

			CallR( ErrERRCheck( errLGNoMoreRecords ) );
			}
		}

	//	calculate the number of sectors from the log record we want to the next LRCHECKSUM record

	csec = lgposNextChecksum.isec + 1 - plgposNextRec->isec;
	Assert( csec > 0 );

	//	read from the first sector of the next log record to the first sector of the next LRCHECKSUM record

	CallR( plread->ErrEnsureSector( plgposNextRec->isec, csec, &pbEnsure ) );

	//	now that we have the next LRCHECKSUM record, setup our markers

	m_pbLastChecksum = pbEnsure + ( ( csec - 1 ) * m_cbSec ) + lgposNextChecksum.ib;
	m_lgposLastChecksum = lgposNextChecksum;

	//	reset the pointer to the next log record (it may have moved due to ErrEnsureSector)

	m_pbNext = pbEnsure + plgposNextRec->ib;

	//	validate the next LRCHECKSUM record
	//	NOTE: this should never fail because we pre-scan and patch each log file before this is called!
	
	const BOOL fValidRecord = FValidLRCKRecord( plrck, &m_lgposLastChecksum );

	//	just in case...

	if ( !fValidRecord )
		{
		AssertSz( fFalse, "Unexpected invalid LRCK record!" );

		//	the LRCHECKSUM record is invalid -- we must fail because we cannot read the entire log record

		//	set the end of data marker to the log position of the record we are trying to read in
		//	that way, we shouldn't even try to read it next time -- we certainly shouldn't trust it at all

		m_lgposLastRec = *plgposNextRec;

		//	return the standard corruption error (ErrLGGetNextRecFF will parse this into the right error code)

		CallR( ErrERRCheck( JET_errLogFileCorrupt ) );
		}

	//	now that we have the new LRCHECKSUM record and it looks good, read the entire range in and validate it

	//	move lgposNextChecksum to the end of the forward range of the LRCHECKSUM record
	//		(lgposNextChecksumEnd should either be farther along in the same sector, or on the boundary
	//		 of a later sector)

	AddLgpos( &lgposNextChecksum, sizeof( LRCHECKSUM ) + plrck->le_cbForwards );
	Assert( ( lgposNextChecksum.isec == m_lgposLastChecksum.isec && 
			  lgposNextChecksum.ib   >  m_lgposLastChecksum.ib ) ||
			( lgposNextChecksum.isec >  m_lgposLastChecksum.isec && 
			  lgposNextChecksum.ib   == 0 ) );

	//	calculate the number of sectors we need for the log record and the new LRCHECKSUM range

	csec = lgposNextChecksum.isec - m_lgposLastChecksum.isec;

	if ( csec > 0 )
		{

		//	read from the first sector of the next log record to the end of the LRCHECKSUM range

		CallR( plread->ErrEnsureSector( plgposNextRec->isec, csec, &pbEnsure ) );

		//	now that we have the next LRCHECKSUM record, setup our markers

		m_pbLastChecksum = pbEnsure + ( ( m_lgposLastChecksum.isec - plgposNextRec->isec ) * m_cbSec ) + m_lgposLastChecksum.ib;

		//	reset the pointer to the next log record (it may have moved due to ErrEnsureSector)

		m_pbNext = pbEnsure + plgposNextRec->ib;
		}

	//	validate the LRCHECKSUM range

	const BOOL fValidRange = FValidLRCKRange( plrck, m_lgposLastChecksum.lGeneration );
	if ( !fValidRange )
		{

		//	the LRCHECKSUM range is invalid, but all is not lost!

		//	set the end of data marker to start at the sector with the LRCHECKSUM record

		m_lgposLastRec.ib = 0;
		m_lgposLastRec.isec = m_lgposLastChecksum.isec;
		m_lgposLastRec.lGeneration = m_plgfilehdr->lgfilehdr.le_lGeneration;

		//	if we have a short checksum, that means that the sector with the LRCHECKSUM record is salvagable
		//		(but the rest are not)

		if ( plrck->bUseShortChecksum == bShortChecksumOn )
			{

			//	advance the end of data pointer past the salvagable sector
			
			m_lgposLastRec.isec++;
			Assert( m_lgposLastRec.isec < ( m_csecLGFile - 1 ) );

			//	since the first sector of the range is valid, we trust the pointers in the LRCHECKSUM record
			//	the backward pointer could be 0 if we had patched this sector in a previous crash
			//	the forward/next pointers may or may not point ahead to data (in this example they do)
			//
			//	so, we need to realize that even though we trust the data in the backward range which should
			//		be the rest of X, there may be no backward range to trust making Y the next record
			//	also, we need to realize that Y could stretch into the corruption we just found; however,
			//		that case will be handled after we recurse back into ErrLGGetNextRecFF and realize that
			//		log record Y does not end before m_lgposLastRec and is therefore useless as well
			//
			//              0  /---------------\
			//	----|-------|--|------|--------|------
			//	  (X|DADADA(LRCK)(YYYY| doodoo | ???
			//	----|--------|--------|--------|------
			//				 \--------------------/

			//	see if we do in fact need to handle the special "no backward range" case
			
			if ( plrck->le_cbBackwards != m_lgposLastChecksum.ib )
				goto NoBackwardRange;
			}

		//	return the standard corruption error (ErrLGGetNextRecFF will parse this into the right error code)

		CallR( ErrERRCheck( JET_errLogFileCorrupt ) );
		}

	//	what about when the backward range of the LRCHECKSUM doesn't cover the log record we want to read in?
	//
	//	this indicates that in a previous crash, we sensed that the LRCHECKSUM record or range was invalid, and
	//		we patched it by chopping it off (including the entire sector it was in)
	//
	//	a new LRCHECKSUM record was created with a backward range of 0 to indicate that the data in the 
	//		area before the LRCHECKSUM record is not to be trusted; in fact, we fill it with a known pattern
	//
	//	thus, the log record that is hanging between the two LRCHECKSUM ranges is not usable
	//
	//	the next record we are able to replay is the LRCHECKSUM record we just read in
	//
	//	since we already recovered from the last crash (e.g. we made the special LRCHECKSUM record with no
	//		backward range), there SHOULD be a series of undo operations following the LRCHECKSUM record;
	//		(there is no concrete way to tell -- we could have crashed exactly after a commit to level 0)
	//		these will be replayed first, then any other operations will be replayed as well

	if ( plrck->le_cbBackwards != m_lgposLastChecksum.ib )
		{

		//	if this is the last LRCHECKSUM record, we set m_lgposLastRec to the end of the forward range

		if ( plrck->le_cbNext == 0 )
			{
			m_lgposLastRec = lgposNextChecksum;
			}

NoBackwardRange:

		//	the backwards pointer should be 0

		Assert( plrck->le_cbBackwards == 0 );

//
//	THE PATTERN IS NO LONGER PUT HERE SO THAT WE HAVE THE OLD DATA IF WE NEED IT (DEBUGGING PURPOSES)
//
//		//	look for the known pattern 
//
//		Assert( memcmp( PbSecAligned( m_pbLastChecksum ), rgbLogExtendPattern, m_lgposLastChecksum.ib ) == 0 );

		//	point to the LRCHECKSUM record as the next log record

		m_pbNext = m_pbLastChecksum;
		}

	//	we now have the next LRCHECKSUM range in memory and validated

	//	return success

	CallS( err );
	return err;
	}

//  ================================================================
LOCAL VOID InsertPageIntoRgpgno( PGNO rgpgno[], INT * const pipgno, const INT cpgnoMax, const PGNO pgno )
//  ================================================================
	{
	Assert( NULL != rgpgno );
	Assert( NULL != pipgno );
	Assert( 0 <= *pipgno );
	Assert( cpgnoMax >= 0 );

	// Removes consecutive duplicates and invalid page numbers
	if( pgno != rgpgno[(*pipgno)-1]
		&& *pipgno < cpgnoMax
		&& pgnoNull != pgno )
		{
		rgpgno[(*pipgno)++] = pgno;
		}
	}

//	Implicit output parameters
//		m_pbNext
//	via ErrLGIReadMS
//		m_pbLastMSFlush		// emulated with lastChecksum
//		m_lgposLastMSFlush	// emulated with lastChecksum
//		m_pbRead
//		m_pbNext
//		m_isecRead
//	via LGISearchLastSector
//		m_pbEntry		// not emulated
//		m_lgposLastRec	// emulated

ERR LOG::ErrLGGetNextRecFF( BYTE **ppbLR )
	{
	ERR			err;
	UINT		cbAvail, cbNeed, cIteration;
	LRCHECKSUM	*plrck;
	LGPOS		lgposPbNext, lgposPbNextNew, lgposChecksumEnd;
	BOOL		fDidRead;

	//	initialize variables

	err 		= JET_errSuccess;
	fDidRead	= fFalse;

	//	lock the flush information

	Assert( m_critLGFlush.FNotOwner() );
	m_critLGFlush.Enter();

	//	m_lgposLastRec should be set

	Assert( m_lgposLastRec.isec >= m_csecHeader && 
			( m_lgposLastRec.isec < ( m_csecLGFile - 1 ) || 
			  ( m_lgposLastRec.isec == ( m_csecLGFile - 1 ) && m_lgposLastRec.ib == 0 ) ) &&
			m_lgposLastRec.lGeneration == m_plgfilehdr->lgfilehdr.le_lGeneration );

	//	we should have a LogReader created for us

	Assert( m_plread != pNil );

	//	m_pbNext points to the log record we just replayed
	//	get that log position

	Assert( m_pbNext != pbNil );
	GetLgposOfPbNext( &lgposPbNextNew );

	//	save this log position for future reference

	lgposPbNext = lgposPbNextNew;

	//	make sure we don't exceed the end of the replay data

	//	since we already replayed the data at m_pbNext, we should be able to assert that m_pbNext is not
	//		past the end of usable data (if it is, we replayed unusable data -- e.g. GARBAGE)

	AssertSz( ( CmpLgpos( &lgposPbNextNew, &m_lgposLastRec ) < 0 ), 
			  "We somehow managed to replay garbage, and now, when we are moving past the garbage we just played, we are detecting it for the first time!?! Woops..." );

	//	just in case

	if ( CmpLgpos( &lgposPbNextNew, &m_lgposLastRec ) >= 0 )
		{

		//	this is a serious problem -- we replayed data that was not trusted
		//	the database may be really messed up at this point (I'm surprised the REDO code actually worked)
		//
		//	there is no way to get out of this; we'll have to trust UNDO to try and rectify things
		//		(unless the garbage we replayed happened to be a commit to level 0)
		//
		//	the only thing we can do is stop the REDO code by returning that there are no more log records
			
		Call( ErrERRCheck( errLGNoMoreRecords ) );
		}

	//	get the size of both the fixed and variable length portions of the current log record

	cbNeed = CbLGSizeOfRec( reinterpret_cast< const LR* >( m_pbNext ) );
	Assert( cbNeed > 0 );

	//	advance m_pbNext past the current log record and on to the next log record

	m_pbNext += cbNeed;

	//	m_pbNext should never suffer from wrap-around (it will always be in the first mapping of the buffers)
	
	Assert( m_pbNext > m_pbLGBufMin && m_pbNext < m_pbLGBufMax );
	
	//	get the log position of the next log record (we will be replaying this next if its ok to replay it)

	GetLgposOfPbNext( &lgposPbNextNew );

	//	the new record should be at a greater log position than the old record

	Assert( CmpLgpos( &lgposPbNextNew, &lgposPbNext ) > 0 );

	//	in fact, the difference in the log positions should be exactly equal to the the size of the old record

	Assert( CbOffsetLgpos( lgposPbNextNew, lgposPbNext ) == QWORD( cbNeed ) );

	//	see if this was the last log record

	if ( CmpLgpos( &lgposPbNextNew, &m_lgposLastRec ) >= 0 )
		{
		Call( ErrERRCheck( errLGNoMoreRecords ) );
		}

	//	set the pointer to the current LRCHECKSUM record

	Assert( m_pbLastChecksum != pbNil );
	plrck = (LRCHECKSUM *)m_pbLastChecksum;

	//	calculate the end of the current LRCHECKSUM record

	lgposChecksumEnd = m_lgposLastChecksum;
	AddLgpos( &lgposChecksumEnd, sizeof( LRCHECKSUM ) + plrck->le_cbForwards );

	//	calculate the available space in the current LRCHECKSUM range so we can see how much data we have 
	//		in memory vs. how much data we need to bring in (from the next LRCHECKSUM record) 

	Assert( CmpLgpos( &lgposChecksumEnd, &lgposPbNextNew ) >= 0 );
	cbAvail = (UINT)CbOffsetLgpos( lgposChecksumEnd, lgposPbNextNew );

	//	move lgposPbNext up to lgposPbNextNew

	lgposPbNext = lgposPbNextNew;

	//	we now have m_pbNext pointing to the next log record (some/all of it might be missing)
	//
	//	we need to bring in the entire log record (fixed and variable length portions) with regard to possible
	//		corruption, and return to the replay code so the record can be redone
	//
	//	do this in 3 iterations:
	//		first, read in the LRTYP
	//		second, using the LRTYP, read in the fixed portion
	//		third, using the fixed portion, read in the variable portion

	for ( cIteration = 0; cIteration < 3; cIteration++ )
		{

		//	decide how much to read for each iteration
		
		if ( cIteration == 0 )
			{
		
			//	stage 1 -- bring in the LRTYP (should be 1 byte)

			Assert( sizeof( LRTYP ) == 1 );
			cbNeed = sizeof( LRTYP );
			}
		else if ( cIteration == 1 )
			{

			//	stage 2 -- bring in the fixed portion using the LRTYP

			cbNeed = CbLGFixedSizeOfRec( reinterpret_cast< const LR * >( m_pbNext ) );
			}
		else
			{

			//	stage 3 -- bring in the dynamic portion using the fixed portion
			
			Assert( cIteration == 2 );
			cbNeed = CbLGSizeOfRec( reinterpret_cast< const LR * >( m_pbNext ) );
			}

		//	advance lgposPbNextNew to include the data we need

		lgposPbNextNew = lgposPbNext;
		AddLgpos( &lgposPbNextNew, cbNeed );

		//	make sure we don't exceed the end of the replay data

		if ( CmpLgpos( &lgposPbNextNew, &m_lgposLastRec ) > 0 )
			{
			Call( ErrERRCheck( errLGNoMoreRecords ) );
			}

		//	do we have the data we need?

		if ( cbAvail < cbNeed )
			{

			//	no; we need the next LRCHECKSUM range

			//	we should only be doing 1 read per iteration of this function

			Assert( !fDidRead );

			//	do the read

			err = ErrLGIGetNextChecksumFF( &lgposPbNext, ppbLR );
			fDidRead = fTrue;

			//	m_pbNext may have moved because the log record we wanted was partially covered by the next
			//		LRCHECKSUM's backward range and that LRCHECKSUM had no backward range
			//	we need to refresh our currency on m_pbNext to determine exactly what happened

			//	m_pbNext should never suffer from wrap-around (it will always be in the first mapping of the buffers)
	
			Assert( m_pbNext > m_pbLGBufMin && m_pbNext < m_pbLGBufMax );

			//	get the new log position m_pbNext

			GetLgposOfPbNext( &lgposPbNextNew );

			//	m_pbNext should not have moved backwards

			Assert( CmpLgpos( &lgposPbNextNew, &lgposPbNext ) >= 0 );

			//	did m_pbNext move?

			if ( CmpLgpos( &lgposPbNextNew, &lgposPbNext ) > 0 )
				{

				//	yes, meaning that the LRCHECKSUM range we read in had no backward range, so we skipped
				//		to the next good record

				//	this should be a successful case

				CallS( err );

				//	return the log record

				goto HandleError;
				}

			//	did we get corruption?

			if ( err == JET_errLogFileCorrupt )
				{

				//	there was corruption in the next LRCHECKSUM range
				//	this should NEVER happen because ErrLGCheckReadLastLogRecordFF should have patched this up!

				//	m_lgposLastRec should point to the end of the last LRCHECKSUM's range or the end of the first
				//		sector in the new LRCHECKSUM we just read

				Assert( CmpLgpos( &m_lgposLastRec, &lgposChecksumEnd ) == 0 ||
						( m_lgposLastRec.ib == lgposChecksumEnd.ib &&
						  m_lgposLastRec.isec == lgposChecksumEnd.isec + 1 &&
						  m_lgposLastRec.lGeneration == lgposChecksumEnd.lGeneration ) );

				//	did we get any data anyway? we may have if we had a valid short checksum; m_lgposLastRec will be
				//		one full sector past lgposChecksumEnd if we did get more data
			
				AddLgpos( &lgposChecksumEnd, m_cbSec );
				if ( CmpLgpos( &m_lgposLastRec, &lgposChecksumEnd ) == 0 )
					{

					//	we got more data
				
					//	it should be enough for the whole log record because we should have atleast gotten up to
					//		and including the LRCHECKSUM (meaning the entire backward range) and the missing 
					//		portion of the log record must be in the backward range

					err = JET_errSuccess;
					goto GotMoreData;
					}

				//	we did not get any more data; handle the corruption error

				Call( err );
				}
			else if ( err != JET_errSuccess )
				{

				//	another error occurred -- it could be errLGNoMoreRecords (we do not expect warnings)

				Assert( err < 0 );
				Call( err );
				}

GotMoreData:
			//	we have enough data to continue

			//	since we read a new LRCHECKSUM range, we need to refresh some of our internal variables
		
			//	set the pointer to the next LRCHECKSUM record

			Assert( m_pbLastChecksum != pbNil );
			plrck = (LRCHECKSUM *)m_pbLastChecksum;

			//	if lgposChecksumEnd was not setup for the new LRCHECKSUM record, set it up now
			//		(it may have been set to the end of the first sector in the case of corruption)

			if ( CmpLgpos( &lgposChecksumEnd, &m_lgposLastChecksum ) <= 0 )
				{
				lgposChecksumEnd = m_lgposLastChecksum;
				AddLgpos( &lgposChecksumEnd, sizeof( LRCHECKSUM ) + plrck->le_cbForwards );
				}

			//	recalculate the amount of available space as the number of bytes from the current
			//		log record to the start of the newly read LRCHECKSUM record
			
			cbAvail = (UINT)CbOffsetLgpos( m_lgposLastChecksum, lgposPbNextNew );

			//	if cbAvail == 0, then the next log record must be the new LRCHECKSUM record because
			//		we calculated cbAvail as the number of bytes BEFORE the LRCHECKSUM record
			//		(e.g. there is nothing before the LRCHECLSUM record to get)

			if ( cbAvail == 0 )
				{

				//	we must be looking at an LRCHECKSUM record

				Assert( *m_pbNext == lrtypChecksum );

				//	we should be on the first iteration because we shouldn't have had a partial
				//		LRCHECKSUM record hanging over the edge of a sector; it would have been
				//		sector aligned using lrtypNOP records

				Assert( cIteration == 0 );

				//	m_pbNext should be sector-aligned 

				Assert( lgposPbNext.ib == 0 );
				Assert( m_pbNext == PbSecAligned( m_pbNext ) );

				//	set cbAvail to be the size of the LRCHECKSUM record

				cbAvail = sizeof( LRCHECKSUM );
				}
				
			//	we should now have the data we need

			Assert( cbAvail >= cbNeed );
			}

		//	iteration complete -- loop back for next pass
		}

	//	we now have the log record in memory and validated

	Assert( cIteration == 3 );

	//	we should also have enough data to cover the entire log record

	Assert( cbAvail >= CbLGSizeOfRec( reinterpret_cast< const LR * >( m_pbNext ) ) );

	//	we should have no errors or warnings at this point

	CallS( err );

HandleError:

	//	setup the pointers in all cases (error or success)

	*ppbLR = m_pbNext;
	m_pbRead = m_plread->PbGetEndOfData();
	m_isecRead = m_plread->IsecGetNextReadSector();

	//	did we get corruption? we NEVER should because ErrLGCheckReadLastLogRecordFF patches it first!

	if ( err == JET_errLogFileCorrupt )
		{

		//	what recovery mode are we in?

		if ( m_fHardRestore )
			{

			//	we are in hard-recovery mode

			if ( m_plgfilehdr->lgfilehdr.le_lGeneration <= m_lGenHighRestore )
				{

				//	this generation is part of a backup set

				Assert( m_plgfilehdr->lgfilehdr.le_lGeneration >= m_lGenLowRestore );
				err = ErrERRCheck( JET_errLogCorruptDuringHardRestore );
				}
			else
				{

				//	the current log generation is not part of the backup-set

				err = ErrERRCheck( JET_errLogCorruptDuringHardRecovery );
				}
			}
		else
			{

			//	we are in soft-recovery mode or in the dump code
			//	keep the original error JET_errLogFileCorrupt

			}
		}

	m_critLGFlush.Leave();
	
	return err;
	}

//	If it's time to preread more database pages, they will be
//	preread, otherwise do nothing.

ERR
LOG::ErrLGIPrereadCheck()
	{
	if ( m_fPreread )
		{
		if ( m_cPageRefsConsumed >= m_cPagesPrereadThreshold )
			{
			m_cPageRefsConsumed = 0;
			return ErrLGIPrereadPages( m_cPagesPrereadAmount );
			}
		}
	return JET_errSuccess;
	}

//	Look forward in log records and ask buffer manager to preread pages


ERR
LOG::ErrLGIPrereadExecute(
	const UINT cPagesToPreread
	)
	{
	const LR *	plr = pNil;
	ERR			err = JET_errSuccess;
	const INT	ipgnoMax = cPagesToPreread + 1;
	UINT		cPagesReferenced = 0;

	// page array structure:
	// [pgnoNull] [cPagesToPreread number of pages] [pgnoNull]
	
	PGNO * 	rgrgpgno[cdbidMax];
	INT 	rgipgno[cdbidMax];

	rgrgpgno[ 0 ] = pNil;
	
	rgrgpgno[0] = (PGNO*) PvOSMemoryHeapAlloc( cdbidMax * (ipgnoMax + 1) * sizeof(PGNO) );
	if ( !rgrgpgno[0] )
		{
		// Upper layer can hide this however they want
		return JET_errOutOfMemory;
		}

	for ( INT idbid = 0; idbid < cdbidMax; ++idbid )
		{
		rgrgpgno[idbid] = rgrgpgno[0] + idbid * (ipgnoMax + 1);
		rgrgpgno[idbid][0] = pgnoNull;
		rgipgno[idbid] = 1;
		}

	// Notice that we remove consecutive duplicates (and pgnoNull) from our preread
	// list, but that we count duplicates and null's to keep our bookkeeping simple.

	while ( ( JET_errSuccess == ( err = ErrLGGetNextRecFF( (BYTE **) &plr ) ) ) &&
		cPagesReferenced < cPagesToPreread )
		{
		switch( plr->lrtyp )
			{
			case lrtypInsert:
			case lrtypFlagInsert:
			case lrtypFlagInsertAndReplaceData:
			case lrtypReplace:
			case lrtypReplaceD:
			case lrtypFlagDelete:
			case lrtypDelete:
			case lrtypDelta:
			case lrtypSLVSpace:
			case lrtypSetExternalHeader:
				{
				const LRPAGE_ * const plrpage = (LRPAGE_ *)plr;
				const PGNO pgno = plrpage->le_pgno;
				const INT idbid = plrpage->dbid - 1;
				if( idbid >= cdbidMax )
					break;

				const INT cPages = rgipgno[idbid];
					
				InsertPageIntoRgpgno( rgrgpgno[idbid], rgipgno+idbid, ipgnoMax, pgno );

				cPagesReferenced += rgipgno[idbid] - cPages;
				}
				break;

			case lrtypSplit:
				{
				const LRSPLIT * const plrsplit = (LRSPLIT *)plr;
				const INT idbid = plrsplit->dbid - 1;
				if( idbid >= cdbidMax )
					break;

				const INT cPages = rgipgno[idbid];

				InsertPageIntoRgpgno( rgrgpgno[idbid], rgipgno+idbid, ipgnoMax, plrsplit->le_pgno );
				InsertPageIntoRgpgno( rgrgpgno[idbid], rgipgno+idbid, ipgnoMax, plrsplit->le_pgnoNew );
				InsertPageIntoRgpgno( rgrgpgno[idbid], rgipgno+idbid, ipgnoMax, plrsplit->le_pgnoParent );
				InsertPageIntoRgpgno( rgrgpgno[idbid], rgipgno+idbid, ipgnoMax, plrsplit->le_pgnoRight );

				cPagesReferenced += rgipgno[idbid] - cPages;
				}
				break;
				
			case lrtypMerge:
				{
				const LRMERGE * const plrmerge = (LRMERGE *)plr;	
				const INT idbid = plrmerge->dbid - 1;
				if( idbid >= cdbidMax )
					break;

				const INT cPages = rgipgno[idbid];

				InsertPageIntoRgpgno( rgrgpgno[idbid], rgipgno+idbid, ipgnoMax, plrmerge->le_pgno );
				InsertPageIntoRgpgno( rgrgpgno[idbid], rgipgno+idbid, ipgnoMax, plrmerge->le_pgnoRight );
				InsertPageIntoRgpgno( rgrgpgno[idbid], rgipgno+idbid, ipgnoMax, plrmerge->le_pgnoLeft );
				InsertPageIntoRgpgno( rgrgpgno[idbid], rgipgno+idbid, ipgnoMax, plrmerge->le_pgnoParent );

				cPagesReferenced += rgipgno[idbid] - cPages;
				}
				break;

			default:
				break;
			}

		}

	if ( JET_errSuccess == err || errLGNoMoreRecords == err )
		{		
		for ( INT idbid = 0; idbid < cdbidMax; ++idbid )
			{
			IFMP ifmp = m_pinst->m_mpdbidifmp[idbid + 1];
			if ( ifmp < ifmpMax && FIODatabaseOpen( ifmp ) )
				{
				Assert( rgipgno[idbid] <= ipgnoMax );
				rgrgpgno[idbid][(rgipgno[idbid])++] = pgnoNull;
				BFPrereadPageList( ifmp, rgrgpgno[idbid] + 1 );
				}
			}
		}
		
	if ( rgrgpgno[ 0 ] )
		{
		::OSMemoryHeapFree( rgrgpgno[ 0 ] );
		}

	return err;
	}

//	Setup state to look forward in log for page references

ERR
LOG::ErrLGIPrereadPages(
	// Number of pages to preread ahead
	UINT	cPagesToPreread
	)
	{
	ERR		err = JET_errSuccess;
	LRSTATE	lrstateNormal;
	BYTE*	pbEnsure = pbNil;

	// ================ Save normal log reading state ===================== //
	// These two will be used to retore m_pbNext and m_pbLastChecksum later
	LGPOS	lgposPbNextSaved;
	GetLgposOfPbNext( &lgposPbNextSaved );
	LGPOS	lgposLastChecksumSaved = m_lgposLastChecksum;
	// m_pbRead and m_isecRead do not need to be saved. They'll simply
	// be copied from m_plread at the end.

	err = m_plread->ErrSaveState( &lrstateNormal );
	if ( JET_errSuccess != err )
		{
		// Assume state is still fine.
		// Don't try to preread again
		m_fPreread = fFalse;
		return JET_errSuccess;
		}

	// ================ Restore to prereading state ======================= //
	// Return the LogReader to the state of the last preread. This will
	// ensure that the last CK that we were prereading in is available,
	// and that ErrLGIGetNextChecksumFF will be able to deal with it
	// sensibly.
	// state includes isec and csec from last call to ErrEnsureSector
	err = m_plread->ErrRestoreState( &m_lrstatePreread, &pbEnsure );
	if ( JET_errSuccess != err )
		{
		// non-fatal
		err = JET_errSuccess;
		m_fPreread = fFalse;
		goto LRestoreNormal;
		}	
	// Following necessary so GetLgpos{DuringReading,OfPbNext}() works (!)
	m_pbRead = m_plread->PbGetEndOfData();
	m_isecRead = m_plread->IsecGetNextReadSector();
	
	Assert( pbNil != pbEnsure );

	Assert( m_lgposPbNextPreread.isec >= m_lrstatePreread.m_isec );
	m_pbNext = pbEnsure + m_cbSec * ( m_lgposPbNextPreread.isec - m_lrstatePreread.m_isec ) +
		m_lgposPbNextPreread.ib;
	Assert( m_pbNext >= m_pbLGBufMin && m_pbNext < m_pbLGBufMax );
	
	Assert( m_lgposLastChecksumPreread.isec >= m_lrstatePreread.m_isec );
	m_pbLastChecksum = pbEnsure +
		m_cbSec * ( m_lgposLastChecksumPreread.isec - m_lrstatePreread.m_isec ) +
		m_lgposLastChecksumPreread.ib;
	Assert( m_pbLastChecksum >= m_pbLGBufMin && m_pbLastChecksum < m_pbLGBufMax );

	m_lgposLastChecksum = m_lgposLastChecksumPreread;

	// =========== Do actual prereading ================================== //

		{
		err = ErrLGIPrereadExecute( cPagesToPreread );
		if ( err )
			{
			err = JET_errSuccess;
			// If any error occurs (including no more log records)
			// don't ever again try to preread this file again.
			// This is re-enabled in FirstRedoLogRec which will
			// open up the next log file.
			m_fPreread = fFalse;
			}
		}

	// =========== Save prereading state ================================= //

	err = m_plread->ErrSaveState( &m_lrstatePreread );
	if ( JET_errSuccess != err )
		{
		// non-fatal
		err = JET_errSuccess;
		m_fPreread = fFalse;
		goto LRestoreNormal;
		}	
	GetLgposDuringReading( m_pbNext, &m_lgposPbNextPreread );
	GetLgposDuringReading( m_pbLastChecksum, &m_lgposLastChecksumPreread );
#ifdef DEBUG
		{
		LGPOS lgposT;
		GetLgposOfPbNext( &lgposT );
		Assert( CmpLgpos( &lgposT, &m_lgposPbNextPreread ) == 0 );
		}
#endif

	// =========== Restore to normal log reading state =================== //
LRestoreNormal:
	// Need to restore LogReader to state on entry, so
	// that normal GetNextRec code will find the records it
	// expects. This is fatal if we can't restore the state to what
	// normal reading expects.
	err = m_plread->ErrRestoreState( &lrstateNormal, &pbEnsure );
	if ( JET_errSuccess != err )
		{
		m_fPreread = fFalse;
		return err;
		}

	// set m_pbNext and m_pbLastChecksum based on saved
	// m_pb{Next,LastChecksum} and pbEnsure from ErrEnsureSector().
	Assert( lgposPbNextSaved.isec >= lrstateNormal.m_isec );
	m_pbNext = pbEnsure + m_cbSec * ( lgposPbNextSaved.isec - lrstateNormal.m_isec ) +
		lgposPbNextSaved.ib;
	Assert( m_pbNext >= m_pbLGBufMin && m_pbNext < m_pbLGBufMax );
	
	Assert( lgposLastChecksumSaved.isec >= lrstateNormal.m_isec );
	m_pbLastChecksum = pbEnsure +
		m_cbSec * ( lgposLastChecksumSaved.isec - lrstateNormal.m_isec ) +
		lgposLastChecksumSaved.ib;
	Assert( m_pbLastChecksum >= m_pbLGBufMin && m_pbLastChecksum < m_pbLGBufMax );

	m_lgposLastChecksum = lgposLastChecksumSaved;
	
	m_pbRead = m_plread->PbGetEndOfData();
	m_isecRead = m_plread->IsecGetNextReadSector();

	return err;
	}
	

//+------------------------------------------------------------------------
//
//	CbLGSizeOfRec
//	=======================================================================
//
//	ERR CbLGSizeOfRec( plgrec )
//
//	Returns the length of a log record.
//
//	PARAMETER	plgrec	pointer to log record
//
//	RETURNS		size of log record in bytes
//
//-------------------------------------------------------------------------
typedef struct {
	int cb;
	BOOL fDebugOnly;
	} LRD;		/* log record descriptor */


const LRD mplrtyplrd[ ] = {
	{	/* 	0 	NOP      */			sizeof( LRTYP ),				0	},
	{	/* 	1 	Init	 */			sizeof( LRINIT ),				0	},
	{	/* 	2 	Term     */			sizeof( LRTERMREC ),			0	},
	{	/* 	3 	MS       */			sizeof( LRMS ),					0	},
	{	/* 	4 	Fill     */			sizeof( LRTYP ),				0	},

	{	/* 	5 	Begin    */			sizeof( LRBEGIN ),				0	},
	{	/*	6 	Commit   */			sizeof( LRCOMMIT ),				0	},
	{	/*	7 	Rollback */			sizeof( LRROLLBACK ),			0	},
	{	/* 	8 	Begin0   */			sizeof( LRBEGIN0 ),				0	},
	{	/*	9  	Commit0  */			sizeof( LRCOMMIT0 ),			0	},
	{	/*	10	Refresh	 */			sizeof( LRREFRESH ),			0	},
	{	/* 	11 	McrBegin */			sizeof( LRMACROBEGIN ),			0	},
	{	/* 	12 	McrCmmt  */			sizeof( LRMACROEND ),			0	},
	{	/* 	13 	McrAbort */			sizeof( LRMACROEND ),			0	},
		
	{	/*	14 	CreateDB */			0,								0	},
	{	/* 	15 	AttachDB */			0,								0	},
	{	/*	16	DetachDB */			0,								0	},

	{	/*  17  RcvrUndo */			sizeof( LRRECOVERYUNDO ),		0	},
	{	/*  18  RcvrQuit */			sizeof( LRTERMREC ),			0	},
	{	/*  19  FullBkUp */			0,								0	},
	{	/*  20  IncBkUp  */			0,								0	},

	{	/*  21  JetOp    */			sizeof( LRJETOP ),				1	},
	{	/*	22 	Trace    */			0,								1	},
		
	{	/* 	23 	ShutDown */			sizeof( LRSHUTDOWNMARK ),		0	},

	{	/*	24	Create ME FDP  */	sizeof( LRCREATEMEFDP ),		0	},
	{	/*	25	Create SE FDP  */	sizeof( LRCREATESEFDP ),		0	},
	{	/*	26	Convert FDP    */ 	sizeof( LRCONVERTFDP ),			0	},

	{	/*	27	Split    */			0,								0	},
	{	/*	28	Merge	 */			0,								0	},

	{	/* 	29	Insert	 */			0,								0	},
	{	/*	30	FlagInsert */		0,								0	},
	{	/*	31	FlagInsertAndReplaceData */		
									0,								0	},
	
	{	/* 	32	FDelete  */			sizeof( LRFLAGDELETE ),			0	},
	{	/* 	33	Replace  */			0,								0	},
	{	/* 	34	ReplaceD */			0,								0	},

	{	/*	35	Delete  */			sizeof( LRDELETE ),				0	},
	{	/*	36	UndoInfo */			0,								0	},
	
	{	/*	37	Delta	 */			0,								0	},

	{	/*	38	SetExtHeader */		0,								0	},
	{	/*	39	Undo     */			0,								0	},

	{	/*	40	SLVPageAppend */	0,								0	},
	{	/*	41	SLVSpace */			0,								0	},
	{	/*	42	Checksum */			sizeof( LRCHECKSUM ),			0	},
	{	/*	43	SLVPageMove */		sizeof( LRSLVPAGEMOVE ),		0	},
	{	/*	44	ForceDetachDB */	0,								0	},
	{	/*	45	ExtRestore */		0,								0	},
	{	/*  46	Backup */			0,								0	},
	{   /*	47	UpgradeDB */		0,								0 	},
	{	/*	48	EmptyTree */		0,								0	},
	{	/*	49	Init2 */			sizeof( LRINIT2 ),				0	},
	{	/*	50	Term2 */			sizeof( LRTERMREC2 ),			0	},
	{	/*	51	RecoveryUndo */		sizeof( LRRECOVERYUNDO2 ),		0	},
	{	/*	52	RecoveryQuit */		sizeof( LRTERMREC2 ),			0	},
	{	/*	53	BeginDT */			sizeof( LRBEGINDT ),			0	},
	{	/*	54	PrepCommit */		0,								0	},
	{	/*	55	PrepRollback */		0,								0	},
	{	/*	56	DbList */			0,								0	},
	{	/*	57	ForceFlushLog */		sizeof( LRFORCEFLUSHLOG),			0	},

	};


#ifdef DEBUG
BOOL FLGDebugLogRec( LR *plr )
	{
	return mplrtyplrd[plr->lrtyp].fDebugOnly;
	}
#endif


//	For CbLGSizeOfRec() to work properly with FASTFLUSH, CbLGSizeOfRec()
//	must only use the fixed fields of log records to calculate their size.

INT CbLGSizeOfRec( const LR *plr )
	{
	INT		cb;

	Assert( plr->lrtyp < lrtypMax );

	if ( ( cb = mplrtyplrd[plr->lrtyp].cb ) != 0 )
		return cb;

	switch ( plr->lrtyp )
		{
	case lrtypFullBackup:
	case lrtypIncBackup:
		{
		LRLOGRESTORE *plrlogrestore = (LRLOGRESTORE *) plr;
		return sizeof(LRLOGRESTORE) + plrlogrestore->le_cbPath;
		}
	case lrtypBackup:
		{
		LRLOGBACKUP *plrlogbackup = (LRLOGBACKUP *) plr;
		return sizeof(LRLOGBACKUP) + plrlogbackup->le_cbPath;
		}

	case lrtypCreateDB:
		{
		LRCREATEDB *plrcreatedb = (LRCREATEDB *)plr;
		Assert( plrcreatedb->CbPath() != 0 );
		return sizeof(LRCREATEDB) + plrcreatedb->CbPath();
		}
	case lrtypAttachDB:
		{
		LRATTACHDB *plrattachdb = (LRATTACHDB *)plr;
		Assert( plrattachdb->CbPath() != 0 );
		return sizeof(LRATTACHDB) + plrattachdb->CbPath();
		}
	case lrtypDetachDB:
		{
		LRDETACHDB *plrdetachdb = (LRDETACHDB *)plr;
		Assert( plrdetachdb->CbPath() != 0 );
		return sizeof( LRDETACHDB ) + plrdetachdb->CbPath();
		}
	case lrtypDbList:
		{
		LRDBLIST* const	plrdblist	= (LRDBLIST *)plr;
		return sizeof(LRDBLIST) + plrdblist->CbAttachInfo();
		}

	case lrtypSplit:
		{
		LRSPLIT *plrsplit = (LRSPLIT *) plr;
		return sizeof( LRSPLIT ) + 
				plrsplit->le_cbKeyParent +
				plrsplit->le_cbPrefixSplitOld + 
				plrsplit->le_cbPrefixSplitNew;
		}
	case lrtypMerge:
		{
		LRMERGE *plrmerge = (LRMERGE *) plr;
		return sizeof( LRMERGE ) + plrmerge->le_cbKeyParentSep;
		}

	case lrtypEmptyTree:
		{
		LREMPTYTREE *plremptytree = (LREMPTYTREE *)plr;
		return sizeof(LREMPTYTREE) + plremptytree->CbEmptyPageList();
		}

	case lrtypDelta:
		{
		LRDELTA	*plrdelta = (LRDELTA *) plr;
		return sizeof( LRDELTA ) + 
			   plrdelta->CbBookmarkKey() + plrdelta->CbBookmarkData();
		}
		
	case lrtypInsert:
		{
		LRINSERT *plrinsert = (LRINSERT *) plr;
		return	sizeof(LRINSERT) +
				plrinsert->CbSuffix() + plrinsert->CbPrefix() + plrinsert->CbData();
		}
		
	case lrtypFlagInsert:
		{
		LRFLAGINSERT	*plrflaginsert = (LRFLAGINSERT *) plr;
		return 	sizeof(LRFLAGINSERT) +
				plrflaginsert->CbKey() + plrflaginsert->CbData();
		}
		
	case lrtypFlagInsertAndReplaceData:
		{
		LRFLAGINSERTANDREPLACEDATA *plrfiard = (LRFLAGINSERTANDREPLACEDATA *) plr;
		return sizeof(LRFLAGINSERTANDREPLACEDATA) + plrfiard->CbKey() + plrfiard->CbData();
		}
		
	case lrtypReplace:
	case lrtypReplaceD:
		{
		LRREPLACE *plrreplace = (LRREPLACE *) plr;
		return sizeof(LRREPLACE) + plrreplace->Cb();
		}
		
	case lrtypUndoInfo:
		{
		LRUNDOINFO *plrdbi = (LRUNDOINFO *) plr;
		return sizeof( LRUNDOINFO ) + plrdbi->le_cbData +
			   plrdbi->CbBookmarkKey() +
			   plrdbi->CbBookmarkData();
		}

	case lrtypUndo:
		{
		LRUNDO	*plrundo = (LRUNDO *) plr;

		return sizeof( LRUNDO ) +
			   plrundo->CbBookmarkKey() +
			   plrundo->CbBookmarkData();
		}
		
	case lrtypTrace:
		{
		LRTRACE *plrtrace = (LRTRACE *) plr;
		return sizeof(LRTRACE) + plrtrace->le_cb;
		}
		
	case lrtypSetExternalHeader:
		{
		LRSETEXTERNALHEADER *plrsetextheader = (LRSETEXTERNALHEADER *) plr;
		return sizeof(LRSETEXTERNALHEADER) + plrsetextheader->CbData();
		}
		
	case lrtypSLVPageAppend:
		{
		LRSLVPAGEAPPEND *plrSLVPageAppend = (LRSLVPAGEAPPEND *) plr;
		return sizeof(LRSLVPAGEAPPEND) + ( plrSLVPageAppend->FDataLogged() ? (DWORD) plrSLVPageAppend->le_cbData : 0 );
		}

	case lrtypSLVSpace:
		{
		const LRSLVSPACE * const plrslvspace = (LRSLVSPACE *) plr;
		return sizeof( LRSLVSPACE ) + 
			   plrslvspace->le_cbBookmarkKey + plrslvspace->le_cbBookmarkData;
		}

	case lrtypExtRestore:
		{
		LREXTRESTORE *plrextrestore = (LREXTRESTORE *) plr;
		return	sizeof(LREXTRESTORE) + 	plrextrestore->CbInfo();
		}

	case lrtypForceDetachDB:
		{
		LRFORCEDETACHDB *plrfdetachdb = (LRFORCEDETACHDB *)plr;
		Assert( plrfdetachdb->CbPath() != 0 );
		return sizeof( LRFORCEDETACHDB ) + plrfdetachdb->CbPath();
		}

	case lrtypPrepCommit:
		{
		const LRPREPCOMMIT	* const plrprepcommit	= (LRPREPCOMMIT *)plr;
		return sizeof(LRPREPCOMMIT) + plrprepcommit->le_cbData;
		}

	default:
		Assert( fFalse );
		return 0;
		}
	}

// mplrtypn
//
// Maps an lrtyp to the size of the fixed structure.
// 

// Notice that we do not specify a size for this array. This is so we can
// catch developers that add a new lrtyp, but forget to modify this array
// by Assert( ( sizeof( mplrtypn ) / sizeof( mplrtypn[ 0 ] ) ) == lrtypMax )

const INT mplrtypn[ ] =
	{
	sizeof( LRNOP ),
	sizeof( LRINIT ),
	sizeof( LRTERMREC ),
	sizeof( LRMS ),
	sizeof( LRTYP ),	// lrtypEnd
	sizeof( LRBEGIN ),
	sizeof( LRCOMMIT ),
	sizeof( LRROLLBACK ),
	sizeof( LRBEGIN0 ),
	sizeof( LRCOMMIT0 ),
	sizeof( LRREFRESH ),
	sizeof( LRMACROBEGIN ),
	sizeof( LRMACROEND ),
	sizeof( LRMACROEND ),	// lrtypMacroAbort
	sizeof( LRCREATEDB ),
	sizeof( LRATTACHDB ),
	sizeof( LRDETACHDB ),
	sizeof( LRRECOVERYUNDO ),
	sizeof( LRTERMREC ),	// lrtypRecoveryQuit
	sizeof( LRLOGRESTORE ),	// lrtypFullBackup
	sizeof( LRLOGRESTORE ),	// lrtypIncBackup
	sizeof( LRJETOP ),
	sizeof( LRTRACE ),
	sizeof( LRSHUTDOWNMARK ),
	sizeof( LRCREATEMEFDP ),
	sizeof( LRCREATESEFDP ),
	sizeof( LRCONVERTFDP ),
	sizeof( LRSPLIT ),
	sizeof( LRMERGE ),
	sizeof( LRINSERT ),
	sizeof( LRFLAGINSERT ),
	sizeof( LRFLAGINSERTANDREPLACEDATA ),
	sizeof( LRFLAGDELETE ),
	sizeof( LRREPLACE ),
	sizeof( LRREPLACE ),	// lrtypReplaceD
	sizeof( LRDELETE ),
	sizeof( LRUNDOINFO ),
	sizeof( LRDELTA ),
	sizeof( LRSETEXTERNALHEADER ),
	sizeof( LRUNDO ),
	sizeof( LRSLVPAGEAPPEND ),
	sizeof( LRSLVSPACE ),
	sizeof( LRCHECKSUM ),
	sizeof( LRSLVPAGEMOVE ),
	sizeof( LRFORCEDETACHDB ),
	sizeof( LREXTRESTORE ),
	sizeof( LRLOGBACKUP ),	// lrtypBackup
	0,
	sizeof( LREMPTYTREE ),
	sizeof( LRINIT2 ),
	sizeof( LRTERMREC2 ),
	sizeof( LRRECOVERYUNDO2 ),
	sizeof( LRTERMREC2 ),
	sizeof( LRBEGINDT ),
	sizeof( LRPREPCOMMIT ),
	sizeof( LRPREPROLLBACK ),
	sizeof( LRDBLIST ),
	sizeof( LRFORCEFLUSHLOG ),
	};

INT CbLGFixedSizeOfRec( const LR * const plr )
	{
	// Should only be passed valid lrtyp's.
	// If this fires, be sure to check the mlrtypn array above and lrtypMax
	Assert( plr->lrtyp < lrtypMax );

	return mplrtypn[ plr->lrtyp ];
	}

#ifdef DEBUG

VOID AssertLRSizesConsistent()
	{
	// If this fires, someone added a new lrtyp (hence, increased lrtypMax),
	// but they didn't modify mplrtypn[] above, or they didn't set lrtypMax properly.
	Assert( ( sizeof( mplrtypn ) / sizeof( mplrtypn[ 0 ] ) ) == lrtypMax );
	// If this fires, someone added a new lrtyp (hence, increased lrtypMax),
	// but they didn't modify mplrtyplrd[] far above, or they didn't set lrtypMax properly.
	Assert( ( sizeof( mplrtyplrd ) / sizeof( mplrtyplrd[ 0 ] ) ) == lrtypMax );

	// Verify that fixed sizes in mplrtyplrd[] are the same as in mplrtypn[].
	for ( LRTYP iLrtyp = 0; iLrtyp < lrtypMax; iLrtyp++ )
		{
		if ( 0 != mplrtyplrd[ iLrtyp ].cb )
			{
			Assert( mplrtyplrd[ iLrtyp ].cb == mplrtypn[ iLrtyp ] );
			}
		}

	}

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\ds\ese98\src\ese\_log\logredo.cxx ===
//	LOGREDO - logical part of soft/hard recovery
//	============================================
//
//	ENTRY POINT(S):
//		ErrLGSoftStart
//
//	PURPOSE:
//		Logical part of soft/hard recovery. Replays logfiles, starting from 
//		the begining of logfile pointed by checkpoint. If there is no checkpoint
//		starts from the begining of the lowest available log generation file.
//
//		Main loop is through ErrLRIRedoOperations
//
//	BASE PROTOTYPES:
//		class LOG in log.hxx
//
//	OFTEN USED PROTOTYPES:
//		classes LRxxxx in logapi.hxx
//
/////////////////////////////////////////////

#include "std.hxx"
#include "_ver.hxx"
#include "_space.hxx"
#include "_bt.hxx"

extern LONG g_cbEventHeapMax;
 

const DIRFLAG	fDIRRedo = fDIRNoLog | fDIRNoVersion;

//	checks if page needs a redo of operation
//
INLINE BOOL FLGINeedRedo( const CSR& csr, const DBTIME dbtime )
	{
 	Assert( csr.FLatched() );
	Assert( csr.Dbtime() == csr.Cpage().Dbtime() );
	
	return dbtime > csr.Dbtime();
	}

//	checks if page needs a redo of operation
//
INLINE BOOL FLGNeedRedoCheckDbtimeBefore(
	const CSR&		csr,
	const DBTIME	dbtime,
	const DBTIME	dbtimeBefore,
	ERR*			const perr )
	{
	const BOOL		fRedoNeeded		= FLGINeedRedo( csr, dbtime );

	*perr = JET_errSuccess;

	Assert( dbtimeNil != dbtimeBefore );
	Assert( dbtimeInvalid != dbtimeBefore );
	if ( fRedoNeeded )
		{
		// dbtimeBefore an page should be the same one as in the record
		Assert( csr.Dbtime() == dbtimeBefore );
		if ( csr.Dbtime() != dbtimeBefore )
			{
			*perr = ErrERRCheck( csr.Dbtime() < dbtimeBefore ? JET_errDbTimeTooOld : JET_errDbTimeTooNew );
			}			
		}
	else
		{
		Assert( dbtime > dbtimeBefore );
		Assert( csr.Dbtime() >= dbtime );
		}

	Assert( fRedoNeeded || JET_errSuccess == *perr );
	return fRedoNeeded;
	}

//	checks if page needs a redo of operation
//
INLINE BOOL FLGNeedRedoPage( const CSR& csr, const DBTIME dbtime )
	{
	return FLGINeedRedo( csr, dbtime );
	}

#ifdef DEBUG
//	checks if page needs a redo of operation
//
INLINE BOOL FAssertLGNeedRedo( const CSR& csr, const DBTIME dbtime, const DBTIME dbtimeBefore )
	{
	ERR 		err;
	const BOOL	fRedoNeeded	= FLGNeedRedoCheckDbtimeBefore( csr, dbtime, dbtimeBefore, &err );

	return ( fRedoNeeded && JET_errSuccess == err );
	}
#endif // DEBUG

LOCAL PATCH *PpatchLGSearch( PATCHLST **rgppatchlst, DBTIME dbtimeRedo, PGNO pgno, DBID dbid )
	{
	PATCHLST	*ppatchlst = rgppatchlst[ IppatchlstHash( pgno, dbid ) ];
	PATCH		*ppatch = NULL;

	while ( ppatchlst != NULL && 
			( ppatchlst->pgno != pgno ||
			  ppatchlst->dbid != dbid ) )
		{
		ppatchlst = ppatchlst->ppatchlst;
		}
	
	if ( ppatchlst != NULL )
		{
		ppatch = ppatchlst->ppatch;
		while( ppatch != NULL && ppatch->dbtime < dbtimeRedo )
			{
			PATCH *ppatchT = ppatch;
			ppatch = ppatch->ppatch;
			delete ppatchT;
			}
		ppatchlst->ppatch = ppatch;
		}
	return ppatch;
	}


//	access page RIW latched
//	remove dependence
//
INLINE ERR ErrLGIAccessPage(
	PIB				*ppib,
	CSR				*pcsr,
	const IFMP		ifmp,
	const PGNO		pgno )
	{
	Assert( pgnoNull != pgno );
	Assert( NULL != ppib );
	Assert( !pcsr->FLatched() );

	const ERR		err		= pcsr->ErrGetRIWPage( ppib, ifmp, pgno );
	switch( err )
		{
		case JET_errOutOfMemory:
		case JET_errOutOfBuffers:
		case wrnBFPageFault:
		case wrnBFPageFlushPending:
		case wrnBFBadLatchHint:
		case JET_errReadVerifyFailure:
		case JET_errDiskIO:
		case JET_errDiskFull:
		case JET_errPageNotInitialized:
		case JET_errFileIOBeyondEOF:
 			break;
		default:
			CallS( err );
		}
			
	return err;
	}


//	retrieves new page from database or patch file
//		
ERR LOG::ErrLGRIAccessNewPage(
	PIB *				ppib,
	CSR *				pcsrNew,
	const IFMP			ifmp,
	const PGNO			pgnoNew,
	const DBTIME		dbtime,
	const SPLIT * const	psplit,
	const BOOL			fRedoSplitPage,
	BOOL *				pfRedoNewPage )
	{
	//	access new page
	//	if hard-restore
	//		if page exists
	//			if page's dbtime < dbtime of oper
	//				release page
	//				get new page
	//			else
	//				check patch file
	//		if must check patch file
	//			get patch for page
	//			if patch page exists
	//				Assert patch page's dbtime >= dbtime of oper
	//				replace page with patch
	//				acquire RIW latch
	//			else
	//				get new page
	//		else
	//			if page's dbtime < dbtime of oper
	//				release page
	//				get new page
	//	else if page exists
	//		if page's dbtime < dbtime of oper
	//			release page
	//			get new page
	//	else
	//		get new page
	//
	ERR					err;
	const BOOL			fDependencyRequired		= ( NULL == psplit
													|| FBTISplitDependencyRequired( psplit ) );

	//	assume new page needs to be redone
	*pfRedoNewPage = fTrue;

	err = ErrLGIAccessPage( ppib, pcsrNew, ifmp, pgnoNew );

	if ( m_fHardRestore )
		{
		//	We can only consult the patch file if the split page does not
		//	need to be redone, otherwise we might end up grabbing a copy of
		//	the new page that's way too advanced.  If the split page needs
		//	to be redone, then don't bother going to the patch file and just
		//	force the new page to be redone as well.
		//	Note that if a dependency is not required, NEVER check the patch
		//	file -- the page will simply get recreated if necessary.
		BOOL	fCheckPatch		= !fRedoSplitPage && fDependencyRequired;

		if ( err >= 0 )
			{
			//	see if page is already up-to-date
			Assert( latchRIW == pcsrNew->Latch() );

			*pfRedoNewPage = FLGNeedRedoPage( *pcsrNew, dbtime );
			if ( *pfRedoNewPage )
				{
				//	fall through to check the patch and see if we can find the
				//	up-to-date page there
				pcsrNew->ReleasePage();
				}
			else
				{
				//	page is up-to-date for this operation, don't bother trying
				//	to patch it.
				fCheckPatch = fFalse;
				}
			}


		//	At this point, if fCheckPatch is TRUE, then either the new page
		//	was successfully read, but it's not up-to-date, or the new page
		//	could not successfully be read.
		if ( fCheckPatch )
			{
			PATCH	*ppatch;

			Assert( !fRedoSplitPage );
			Assert( fDependencyRequired );

			Assert( fRestorePatch == m_fRestoreMode || fRestoreRedo == m_fRestoreMode );

			AssertSz ( fSnapshotNone == m_fSnapshotMode, "No patch file for snapshot restore" ); 
			ppatch = PpatchLGSearch( m_rgppatchlst, dbtime, pgnoNew, rgfmp[ifmp].Dbid() );

			if ( ppatch != NULL )
				{
#ifdef ELIMINATE_PAGE_PATCHING
				//	should be impossible
				EnforceSz( fFalse, "Patching no longer supported." );
				return ErrERRCheck( JET_errBadPatchPage );
#else
				//	patch exists and is later than operation
				//
				Assert( ppatch->dbtime >= dbtime );
				CallR( ErrLGIPatchPage( ppib, pgnoNew, ifmp, ppatch ) );

				CallS( ErrLGIAccessPage( ppib, pcsrNew, ifmp, pgnoNew ) ); 
				Assert( latchRIW == pcsrNew->Latch() );

				*pfRedoNewPage = FLGNeedRedoPage( *pcsrNew, dbtime );
				if ( *pfRedoNewPage )
					{
					Assert( fFalse );	//	shouldn't need to redo the page, since page's dbtime is >= operation dbtime
					pcsrNew->ReleasePage();
					}
#endif	//	ELIMINATE_PAGE_PATCHING
				}
			}
		}

	else if ( err >= 0 )
		{
		//	get new page if page is older than operation
		//
		Assert( latchRIW == pcsrNew->Latch() );

		*pfRedoNewPage = FLGNeedRedoPage( *pcsrNew, dbtime );
		if ( *pfRedoNewPage )
			{
			Assert( fRedoSplitPage || !fDependencyRequired );
			pcsrNew->ReleasePage();
			}
		}

	Assert( !*pfRedoNewPage || !pcsrNew->FLatched() );
	
	//	if new page needs to be redone, then so should split page
	//	(because split page is dependent on new page)
	if ( *pfRedoNewPage && !fRedoSplitPage && fDependencyRequired )
		{
		Assert( fFalse );	//	should be impossible
		return ErrERRCheck( JET_errDatabaseBufferDependenciesCorrupted );
		}
	
	return JET_errSuccess;
	}


ERR ErrDBStoreDBPath( INST *pinst, IFileSystemAPI *const pfsapi, CHAR *szDBName, CHAR **pszDBPath )
	{
	CHAR	szFullName[IFileSystemAPI::cchPathMax];
	SIZE_T	cb;
	CHAR	*sz;

	if ( pfsapi->ErrPathComplete( szDBName, szFullName ) < 0 )
		{
		// UNDONE: should be illegal name or name too long etc.
		return ErrERRCheck( JET_errDatabaseNotFound );
		}

	cb = strlen(szFullName) + 1;
	if ( !( sz = static_cast<CHAR *>( PvOSMemoryHeapAlloc( cb ) ) ) )
		{
		*pszDBPath = NULL;
		return ErrERRCheck( JET_errOutOfMemory );
		}
	UtilMemCpy( sz, szFullName, cb );
	Assert( sz[cb - 1] == '\0' );
	if ( *pszDBPath != NULL )
		OSMemoryHeapFree( *pszDBPath );
	*pszDBPath = sz;

	return JET_errSuccess;
	}

//	Returns ppib for a given procid from log record.
//
//			PARAMETERS      procid          process id of session being redone
//                          pppib           out ppib
//
//      RETURNS         JET_errSuccess or error from called routine
//

CPPIB *LOG::PcppibLGRIOfProcid( PROCID procid )
	{
	CPPIB   *pcppib = m_rgcppib;
	CPPIB   *pcppibMax = pcppib + m_ccppib;

	//	find pcppib corresponding to procid if it exists
	//
	for ( ; pcppib < pcppibMax; pcppib++ )
		{
		if ( procid == pcppib->procid )
			{
			Assert( procid == pcppib->ppib->procid );
			return pcppib;
			}
		}
	return NULL;
	}


//+------------------------------------------------------------------------
//
//      ErrLGRIPpibFromProcid
//      =======================================================================
//
//      ERR ErrLGRIPpibFromProcid( procid, pppib )
//
//      Initializes a redo information block for a session to be redone.
//      A BeginSession is performed and the corresponding ppib is stored
//      in the block.  Start transaction level and transaction level
//      validity are initialized.  Future references to this sessions
//      information block will be identified by the given procid.
//
//      PARAMETERS      procid  process id of session being redone
//                              pppib
//
//      RETURNS         JET_errSuccess, or error code from failing routine
//
//-------------------------------------------------------------------------
ERR LOG::ErrLGRIPpibFromProcid( PROCID procid, PIB **pppib )
	{
	ERR		err = JET_errSuccess;

	//	if no record for procid then start new session
	//
	if ( ( *pppib = PpibLGRIOfProcid( procid ) ) == ppibNil )
		{
		//	check if have run out of ppib table lookup
		//	positions. This could happen if between the
		//	failure and redo, the number of system PIBs
		//	set in CONFIG.DAE has been changed.
		//
		if ( m_pcppibAvail >= m_rgcppib + m_ccppib )
			{
			Assert( 0 );    /* should never happen */
			return ErrERRCheck( JET_errTooManyActiveUsers );
			}
		m_pcppibAvail->procid = procid;

		//	use procid as unique user name
		//
		CallR( ErrPIBBeginSession( m_pinst, &m_pcppibAvail->ppib, procid, fFalse ) );
		Assert( procid == m_pcppibAvail->ppib->procid );
		*pppib = m_pcppibAvail->ppib;

		m_pcppibAvail++;
		}

	return JET_errSuccess;
	}


//	gets fucb from hash table that meets given criteria
//
FUCB * TABLEHFHASH::PfucbGet( IFMP ifmp, PGNO pgnoFDP, PROCID procid, BOOL fSpace ) const
	{
	const UINT uiHash = UiHash( pgnoFDP, ifmp, procid );

	TABLEHF	*ptablehf = rgtablehf[ uiHash ];

	for ( ; ptablehf != NULL; ptablehf = ptablehf->ptablehfNext )
		{
		FUCB *pfucb = ptablehf->pfucb;
		if ( pfucb->ifmp == ifmp &&
			 PgnoFDP( pfucb ) == pgnoFDP &&
			 pfucb->ppib->procid == procid &&
			 ( fSpace && FFUCBSpace( pfucb ) || 
			   !fSpace && !FFUCBSpace( pfucb ) ) )
			{
			return pfucb;
			}
		}

	Assert( ptablehf == NULL );
	return pfucbNil;
	}


//	creates new fucb with given criteria
//	links fucb to hash table
//
ERR	TABLEHFHASH::ErrCreate(
	PIB			*ppib,
	const IFMP	ifmp,
	const PGNO	pgnoFDP,
	const OBJID	objidFDP,
	const BOOL	fUnique,
	const BOOL	fSpace,
	FUCB		**ppfucb )
	{
	Assert( pfucbNil == PfucbGet( ifmp, pgnoFDP, ppib->procid, fSpace ) );

	ERR			err		= JET_errSuccess;
	
	TABLEHF	*ptablehf = (TABLEHF *) PvOSMemoryHeapAlloc( sizeof( TABLEHF ) );
	if ( NULL == ptablehf )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	FUCB		*pfucb	= pfucbNil;
	FCB			*pfcb;
	BOOL		fState;
	const UINT	uiHash	=  UiHash( pgnoFDP, ifmp, ppib->procid );

	//	create fucb
	//
	Call( ErrFUCBOpen( ppib, ifmp, &pfucb ) );
	pfucb->pvtfndef = &vtfndefIsam;

	//	set ifmp
	//
	pfucb->ifmp	= ifmp;

	Assert( pfcbNil == pfucb->u.pfcb );

	//	get fcb for table, if one exists
	//
	pfcb = FCB::PfcbFCBGet( ifmp, pgnoFDP, &fState );
	Assert( pfcbNil == pfcb || fFCBStateInitialized == fState );
	if ( pfcbNil == pfcb )
		{
		//	there exists no fcb for FDP
		//		allocate new fcb as a regular table FCB 
		//		and set up for FUCB
		//
		err = FCB::ErrCreate( ppib, ifmp, pgnoFDP, &pfcb );
		Assert( err != errFCBExists );	//	should not get this here since we are single-threaded
		Call( err );
		Assert( pfcb != pfcbNil );
		Assert( pfcb->IsLocked() );
		Assert( pfcb->WRefCount() == 0 );
		Assert( !pfcb->UlFlags() );

		Assert( objidNil == pfcb->ObjidFDP() );
		pfcb->SetObjidFDP( objidFDP );

		if ( pgnoSystemRoot == pgnoFDP )
			{
			pfcb->SetTypeDatabase();
			}
		else
			{
			pfcb->SetTypeTable();
			}

		//	FCB always initialised as unique, though
		//	this flag is ignored during recovery
		//	(uniqueness is strictly determined by
		//	FUCB's flags)
		Assert( pfcb->FUnique() );		// FCB always initialised as unique
		if ( !fUnique )
			pfcb->SetNonUnique();
		pfcb->Unlock();

		//	insert the FCB into the global list

		pfcb->InsertList();

		//	link in the FUCB to new FCB
		//
		pfcb->Link( pfucb );
		Assert( pfcb->WRefCount() >= 1 );

		//	complete the creation of this FCB

		pfcb->Lock();
		pfcb->CreateComplete();
		pfcb->Unlock();
		}
	else
		{
		//	link in the FUCB to new FCB
		//
		pfcb->Link( pfucb ); 

		//	release FCB for the PfcbFCBGet() call
		//
		Assert( pfcb->WRefCount() > 1 );
		pfcb->Release();
		}

	//	set unique-ness in FUCB if requested
	//
	Assert( !FFUCBSpace( pfucb ) );
	Assert( ( fUnique && FFUCBUnique( pfucb ) )
		|| ( !fUnique && !FFUCBUnique( pfucb ) ) );

	if ( fSpace )
		{
		FUCBSetOwnExt( pfucb );
		}
	
	pfucb->dataSearchKey.Nullify();
	pfucb->cColumnsInSearchKey = 0;
	KSReset( pfucb );

	*ppfucb = pfucb;
	
	//	link to hash
	//
	Assert( pfucb != pfucbNil );
	ptablehf->ptablehfNext 	= rgtablehf[ uiHash ];
	ptablehf->pfucb			= pfucb;
	rgtablehf[uiHash]		= ptablehf;

HandleError:
	if ( err < 0 )
		{
		OSMemoryHeapFree( ptablehf );
		if ( pfucb != pfucbNil )
			{
			FUCBClose( pfucb );
			}
		}

	return err;
	}


//	deletes fucb from hash
//
VOID TABLEHFHASH::Delete( FUCB *pfucb )
	{
	const PGNO	pgnoFDP = PgnoFDP( pfucb );
	const IFMP	ifmp	= pfucb->ifmp;
	Assert( pfucb == PfucbGet( ifmp, pgnoFDP, pfucb->ppib->procid, FFUCBSpace( pfucb ) ) );

	const UINT	uiHash = UiHash( pgnoFDP, ifmp, pfucb->ppib->procid );

	Assert( NULL != rgtablehf[uiHash] );
	
	TABLEHF		**pptablehfPrev = &(rgtablehf[uiHash]);
	
	for ( ; (*pptablehfPrev) != NULL ; pptablehfPrev = &((*pptablehfPrev)->ptablehfNext) )
		{
		if ( (*pptablehfPrev)->pfucb == pfucb )
			{
			//	unlink tablehf
			//
			TABLEHF	*ptablehfDelete = *pptablehfPrev;
			*pptablehfPrev = ptablehfDelete->ptablehfNext;

			//	release tablehf
			//
			OSMemoryHeapFree( ptablehfDelete );
			return;
			}
		}
		
	Assert( fFalse );
	return;
	}

	
//	closes all cursors created on database during recovery by given session
//	releases corresponding table handles
//
VOID TABLEHFHASH::Purge( PIB *ppib, IFMP ifmp )
	{
	Assert( PinstFromPpib( ppib )->m_plog->m_ptablehfhash != NULL );
	
	FUCB	*pfucb = ppib->pfucbOfSession;
	FUCB	*pfucbNext;

	for ( ; pfucb != pfucbNil; pfucb = pfucbNext )
		{
		pfucbNext = pfucb->pfucbNextOfSession;

		if ( ifmp == pfucb->ifmp &&
			 pfucb->ppib->procid == ppib->procid )
			{
			Assert( pfucb->ppib == ppib );
			Assert( pfucb == this->PfucbGet( ifmp, 
										   PgnoFDP( pfucb ), 
										   ppib->procid, 
										   FFUCBSpace( pfucb ) ) );
			this->Delete( pfucb );

			//	unlink and close fucb
			//
			FCBUnlink( pfucb );
			FUCBClose( pfucb );
			}
		}

	return;
	}


//	releases all references to this table
//
VOID TABLEHFHASH::Purge( IFMP ifmp, PGNO pgnoFDP )
	{
	BOOL	fState;
	FCB		*pfcb = FCB::PfcbFCBGet( ifmp, pgnoFDP, &fState );
	
	if ( pfcbNil == pfcb )
		{
		return;
		}

	FUCB	*pfucb;
	FUCB	*pfucbNext;
	
	//	close every cursor opened on FCB
	//
	for ( pfucb = pfcb->Pfucb(); pfucb != pfucbNil; pfucb = pfucbNext )
		{
		pfucbNext = pfucb->pfucbNextOfFile;
		Assert( pfucb == this->PfucbGet( ifmp, 
										   PgnoFDP( pfucb ), 
										   pfucb->ppib->procid, 
										   FFUCBSpace( pfucb ) ) );
		this->Delete( pfucb );
		
		//	unlink and close fucb
		//
		FCBUnlink( pfucb );
		FUCBClose( pfucb );
		}

	Assert( 1 == pfcb->WRefCount() );
	pfcb->Release();

	VERNullifyAllVersionsOnFCB( pfcb );

	pfcb->PrepareForPurge();
	pfcb->Purge();
	return;
	}


//	releases and closes all unversioned tables in hash
//
VOID TABLEHFHASH::PurgeUnversionedTables( )
	{
	INT		i;
	
	for ( i = 0; i < ctablehf; i++ )
		{
		TABLEHF		*ptablehf;
		TABLEHF		*ptablehfNext; 
		
		for ( ptablehf = rgtablehf[i]; ptablehf != NULL; ptablehf = ptablehfNext )
			{
			ptablehfNext = ptablehf->ptablehfNext;
			
			FUCB	*pfucb = ptablehf->pfucb;
			Assert( pfucb != pfucbNil );
			Assert( pfucb->ppib != ppibNil );

			if ( !FFUCBVersioned( pfucb ) )
				{
				this->Delete( pfucb );

				//	unlink and close fucb
				//
				FCBUnlink( pfucb );
				FUCBClose( pfucb );
				}
			}
		}
		
	return;
	}

	
//	Returns pfucb for given pib and FDP.
//
//  PARAMETERS		ppib	pib of session being redone
//					fdp		FDP page for logged page
//					ppfucb	out FUCB for open table for logged page
//
//  RETURNS			JET_errSuccess or error from called routine
//

LOCAL ERR ErrLGRIGetFucb(
	TABLEHFHASH *ptablehfhash,
	PIB			*ppib,
	const IFMP	ifmp,
	const PGNO	pgnoFDP,
	const OBJID	objidFDP,
	const BOOL	fUnique,
	const BOOL	fSpace,
	FUCB		**ppfucb )
	{
	FUCB    	*pfucb		= ptablehfhash->PfucbGet( ifmp, pgnoFDP, ppib->procid, fSpace );

	Assert( ptablehfhash != NULL );
	
	//	allocate an all-purpose fucb for this table, if not already allocated
	//
	if ( NULL == pfucb )
		{
		//	fucb not created
		//
		ERR	err	= ptablehfhash->ErrCreate(
									ppib, 
									ifmp, 
									pgnoFDP,
									objidFDP,
									fUnique, 
									fSpace, 
									&pfucb );
		Assert( errFCBAboveThreshold != err );
		Assert( errFCBTooManyOpen != err );
		
		if ( JET_errTooManyOpenTables == err
			|| JET_errOutOfCursors == err )
			{
			//	release tables without uncommitted versions and retry
			//
			ptablehfhash->PurgeUnversionedTables( );
			err = ptablehfhash->ErrCreate(
									ppib, 
									ifmp, 
									pgnoFDP,
									objidFDP,
									fUnique, 
									fSpace, 
									&pfucb );
			}

		CallR( err );
		}
		
	pfucb->bmCurr.Nullify();

	//	reset copy buffer and flags
	//
	Assert( !FFUCBDeferredChecksum( pfucb ) );
	Assert( !FFUCBUpdateSeparateLV( pfucb ) );
	FUCBResetUpdateFlags( pfucb );

	Assert( pfucb->ppib == ppib );

#ifdef DEBUG
	if ( fSpace )
		{
		Assert( FFUCBUnique( pfucb ) );
		Assert( FFUCBSpace( pfucb ) );
		}
	else
		{
		Assert( !FFUCBSpace( pfucb ) );
		if ( fUnique )
			{
			Assert( FFUCBUnique( pfucb ) );
			}
		else
			{
			Assert( !FFUCBUnique( pfucb ) );
			}
		}
#endif		

	*ppfucb = pfucb;
	return JET_errSuccess;
	}


VOID LOG::LGEndPpibAndTableHashGlobal(  )
	{	
	delete [] m_rgcppib;
	m_pcppibAvail =
	m_rgcppib = NULL;
	m_ccppib = 0;

	if ( m_ptablehfhash != NULL )
		{
		m_ptablehfhash->Purge();
		OSMemoryHeapFree( m_ptablehfhash );
		m_ptablehfhash = NULL;
		}		
	}


ERR LOG::ErrLGRIInitSession(
	IFileSystemAPI *const	pfsapi,
	DBMS_PARAM				*pdbms_param,
	BYTE					*pbAttach,
	LGSTATUSINFO			*plgstat,
	const REDOATTACHMODE	redoattachmode )
	{
	ERR						err				= JET_errSuccess;
	CPPIB					*pcppib;
	CPPIB					*pcppibMax;
	DBID					dbid;

	/*	set log stored db environment
	/**/
	if ( pdbms_param )
		m_pinst->RestoreDBMSParams( pdbms_param );

	CallR( ErrITSetConstants( m_pinst ) );

	CallR( m_pinst->ErrINSTInit() );

#ifdef UNLIMITED_DB
	for ( dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
		{
		//	should not have any FMPs allocated at this point
		Assert( ifmpMax == m_pinst->m_mpdbidifmp[ dbid ] );
		}
#else
	//	restore the attached dbs
	Assert( pbAttach );
	if ( redoattachmodeInitBeforeRedo == redoattachmode )
		{
		err = ErrLGLoadFMPFromAttachments( m_pinst, pfsapi, pbAttach );
		CallS( err );
		Call( err );
		}
	else
		{
		Assert( 0 == *pbAttach );
		}

	/*	Make sure all the attached database are consistent!
	 */
	for ( dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
		{
		IFMP		ifmp		= m_pinst->m_mpdbidifmp[ dbid ];
		if ( ifmp >= ifmpMax )
			continue;

		FMP			*pfmp		= &rgfmp[ifmp];
		CHAR		*szDbName;
		REDOATTACH	redoattach;

		if ( !pfmp->FInUse() || !pfmp->Patchchk() )
			continue;

		szDbName = pfmp->SzDatabaseName();
		Assert ( szDbName );

		Assert( redoattachmodeInitBeforeRedo == redoattachmode );

		if ( m_fHardRestore || m_irstmapMac > 0 )
			{
			if ( 0 > IrstmapSearchNewName( szDbName ) )
				{
				/*	not in the restore map, set to skip it.
				 */
				Assert( pfmp->Pdbfilehdr() == NULL );
				pfmp->SetSkippedAttach();
				err = JET_errSuccess;
				continue;
				}
			}

		Assert( !pfmp->FReadOnlyAttach() );
		Call( ErrLGRICheckAttachedDb(
					pfsapi,
					ifmp,
					&m_signLog,
					&redoattach,
					redoattachmode ) );
		Assert( NULL != pfmp->Pdbfilehdr() );

		switch ( redoattach )
			{
			case redoattachNow:
				Assert( !pfmp->FReadOnlyAttach() );
				Call( ErrLGRIRedoAttachDb(
							pfsapi,
							ifmp,
							pfmp->Patchchk()->CpgDatabaseSizeMax(),
							redoattachmode ) );
				break;

			case redoattachCreate:
			default:
				Assert( fFalse );	//	should be impossible, but as a firewall, set to defer the attachment
			case redoattachDefer:
				Assert( !pfmp->FReadOnlyAttach() );
				LGRISetDeferredAttachment( ifmp );
				break;
			}

		/* keep attachment info and update it. */
		Assert( pfmp->Patchchk() != NULL );
		}
#endif		

	/*	initialize CPPIB structure
	/**/
	Assert( g_lSessionsMax > 0 );
	m_ccppib = m_pinst->m_lSessionsMax + cpibSystem;
	Assert( m_rgcppib == NULL );
	m_rgcppib = new CPPIB[m_ccppib];
	if ( m_rgcppib == NULL )
		{
		m_ccppib = 0;
		Call ( ErrERRCheck( JET_errOutOfMemory ) );
		}

	pcppibMax = m_rgcppib + m_ccppib;
	for ( pcppib = m_rgcppib; pcppib < pcppibMax; pcppib++ )
		{
		pcppib->procid = procidNil;
		pcppib->ppib = NULL;
		}
	m_pcppibAvail = m_rgcppib;

	//	allocate and initialize global hash for table handles
	//
	if ( NULL == m_ptablehfhash )
		{
		m_ptablehfhash = (TABLEHFHASH *) PvOSMemoryHeapAlloc( sizeof( TABLEHFHASH ) );
		if ( NULL == m_ptablehfhash )
			{
			LGEndPpibAndTableHashGlobal(  );
			Call ( ErrERRCheck( JET_errOutOfMemory ) );
			}

		new ( m_ptablehfhash ) TABLEHFHASH;
		}
		
	Assert ( JET_errSuccess <= err );
	return err;
HandleError:
	Assert ( JET_errSuccess > err );

	// clean up instance resources allocated in ErrINSTInit
	m_fLogDisabledDueToRecoveryFailure = fTrue;
	m_pinst->ErrINSTTerm( termtypeError );
	m_fLogDisabledDueToRecoveryFailure = fFalse;

	return err;
	}


ERR ErrLGICheckDatabaseFileSize( IFileSystemAPI *const pfsapi, PIB *ppib, IFMP ifmp )
	{
	ERR		err;
	FMP		*pfmp	= rgfmp + ifmp;

	err = ErrDBSetLastPageAndOpenSLV( pfsapi, ppib, ifmp, fFalse );
	if ( JET_errFileNotFound == err )
		{
		//	UNDONE: The file should be there. Put this code to get around
		//	UNDONE: such that DS database file that was not detached can
		//	UNDONE: continue recovering.
		const CHAR	*rgszT[1];
		rgszT[0] = pfmp->SzDatabaseName();
		UtilReportEvent(
				eventError,
				LOGGING_RECOVERY_CATEGORY,
				FILE_NOT_FOUND_ERROR_ID,
				1,
				rgszT,
				0,
				NULL,
				PinstFromPpib( ppib ) );
		}
	else if( err >= JET_errSuccess )
		{
		CallS( err );
		
		/*	set file size to what the FMP (and OwnExt) says it should be.
		 */
		const PGNO	pgnoLast	= pfmp->PgnoLast();
		err = ErrIONewSize( ifmp, pgnoLast );
		}

	return err;
	}

LOCAL VOID LGICleanupTransactionToLevel0( PIB * const ppib )
	{
	//	there should be no more RCEs
	Assert( 0 == ppib->level );
	Assert( prceNil == ppib->prceNewest );
	for ( FUCB *pfucb = ppib->pfucbOfSession; pfucb != pfucbNil; pfucb = pfucb->pfucbNextOfSession )
		{
		FUCBResetVersioned( pfucb );
		}

	ppib->trxBegin0		= trxMax;
	ppib->lgposStart	= lgposMax;
	ppib->ResetDistributedTrx();
	VERSignalCleanup( ppib );

	//	empty the list of expected RCEs
	ppib->RemoveAllDeferredRceid();
	}

ERR LOG::ErrLGEndAllSessionsMacro( BOOL fLogEndMacro )
	{
	ERR		err 		= JET_errSuccess;
	CPPIB   *pcppib 	= m_rgcppib;
	CPPIB   *pcppibMax 	= m_rgcppib + m_ccppib;

	Assert( pcppib != NULL || m_ccppib == 0 );
	for ( ; pcppib < pcppibMax; pcppib++ )
		{
		PIB *ppib = pcppib->ppib;

		if ( ppib == NULL )
			break;

		Assert( sizeof(JET_SESID) == sizeof(ppib) );
		CallR( ppib->ErrAbortAllMacros( fLogEndMacro ) );
		}
		
	return JET_errSuccess;
	}

ERR LOG::ErrLGRIEndAllSessionsWithError( )
	{
	ERR		err;

	CallR( ErrLGEndAllSessionsMacro( fFalse /* fLogEndMacro */ ) );

	(VOID) m_pinst->m_pver->ErrVERRCEClean();

	Assert( m_rgcppib != NULL );
	Assert( m_ptablehfhash != NULL );		
	LGEndPpibAndTableHashGlobal();

	/*	term with checkpoint updates
	/**/
	CallS( m_pinst->ErrINSTTerm( termtypeError ) );

	return err;
	}

LOCAL VOID LGReportAttachedDbMismatch(
	const INST * const	pinst,
	const CHAR * const	szDbName,
	const BOOL			fEndOfRecovery )
	{
	CHAR				szErrT[8];
	const CHAR *		rgszT[2]		= { szErrT, szDbName };

	Assert( NULL != szDbName );
	Assert( strlen( szDbName ) > 0 );

	sprintf( szErrT, "%d", JET_errAttachedDatabaseMismatch );

	UtilReportEvent(
		eventError,
		LOGGING_RECOVERY_CATEGORY,
		( fEndOfRecovery ?
				ATTACHED_DB_MISMATCH_END_OF_RECOVERY_ID :
				ATTACHED_DB_MISMATCH_DURING_RECOVERY_ID ),
		2,
		rgszT,
		0,
		NULL,
		pinst );
	}

/*
 *      Ends redo session.
 *  If fEndOfLog, then write log records to indicate the operations
 *  for recovery. If fPass1 is true, then it is for phase1 operations,
 *  otherwise for phase 2.
 */

ERR LOG::ErrLGRIEndAllSessions( IFileSystemAPI *const pfsapi, BOOL fEndOfLog, const LE_LGPOS *ple_lgposRedoFrom, BYTE *pbAttach )
	{
	ERR		err			= JET_errSuccess;
	CPPIB   *pcppib;
	CPPIB   *pcppibMax;
	PIB		*ppib		= ppibNil;

	BOOL fNeedCallNSTTerm = fTrue;

	BOOL fNeedSoftRecovery = fFalse;


	//	UNDONE: is this call needed?
	//
	//(VOID)ErrVERRCEClean( );

	//	Set current time to attached db's dbfilehdr

	for ( DBID dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
		{
		IFMP ifmp = m_pinst->m_mpdbidifmp[ dbid ];
		if ( ifmp >= ifmpMax )
			continue;
			
		FMP *pfmp = &rgfmp[ ifmp ];

		//	If there is no redo operations on an attached db, then
		//	pfmp->dbtimeCurrent may == 0, i.e. no operation, then do
		//	not change pdbfilehdr->dbtime

		if ( pfmp->Pdbfilehdr() != NULL &&
			 pfmp->DbtimeCurrentDuringRecovery() > pfmp->DbtimeLast() )
			{
			pfmp->SetDbtimeLast( pfmp->DbtimeCurrentDuringRecovery() );
			}
		}

#ifndef RTM

	//	make sure there are no deferred RCEs for any session	
	pcppib = m_rgcppib;
	pcppibMax = pcppib + m_ccppib;
	for ( ; pcppib < pcppibMax; pcppib++ )
		{
		ppib = pcppib->ppib;

		if ( ppib == NULL )
			break;

		ppib->AssertNoDeferredRceid();
		}
#endif	//	!RTM		
	
	CallR ( ErrLGEndAllSessionsMacro( fTrue /* fLogEndMacro */ ) );

	pcppib = m_rgcppib;
	pcppibMax = pcppib + m_ccppib;
	for ( ; pcppib < pcppibMax; pcppib++ )
		{
		ppib = pcppib->ppib;

		if ( ppib == NULL )
			break;

#ifdef DTC
		if ( ppib->FPreparedToCommitTrx() )
			{
			JET_CALLBACK	pfn		= m_pinst->m_pfnRuntimeCallback;

			Assert( ppib->FDistributedTrx() );
			Assert( 1 == ppib->level );

			if ( NULL == pfn )
				{
				return ErrERRCheck( JET_errDTCMissingCallbackOnRecovery );
				}

			const ERR	errCallback		= (*pfn)(
												JET_sesidNil,
												JET_dbidNil,
												JET_tableidNil,
												JET_cbtypDTCQueryPreparedTransaction,
												ppib->PvDistributedTrxData(),
												(VOID *)ppib->CbDistributedTrxData(),
												NULL,
												NULL );
			switch( errCallback )
				{
				case JET_wrnDTCCommitTransaction:
					{
					if ( TrxCmp( m_pinst->m_trxNewest, ppib->trxBegin0 ) > 0 )
						{
						ppib->trxCommit0 = m_pinst->m_trxNewest;
						}
					else
						{
						ppib->trxCommit0 = ppib->trxBegin0 + 2;
						m_pinst->m_trxNewest = ppib->trxCommit0;
						}
						
					LGPOS	lgposCommit;
					CallR( ErrLGCommitTransaction( ppib, 0, ppib->trxCommit0, &lgposCommit ) );

					VERCommitTransaction( ppib );
					LGICleanupTransactionToLevel0( ppib );
					break;
					}

				case JET_wrnDTCRollbackTransaction:
					//	EndSession() below will force rollback, but must the fact we're going
					//	to rollback in case we crash before fully rolling-back (to ensure that
					//	on next recovery, DTC won't direct us to commit the transaction instead)
					CallR( ErrLGPrepareToRollback( ppib ) );
					break;
				
				default:
					//	UNDONE: add eventlog
					err = ErrERRCheck( JET_errDTCCallbackUnexpectedError );
					return err;
				}
			}
#endif	//	DTC

		Assert( sizeof(JET_SESID) == sizeof(ppib) );
		CallR( ErrIsamEndSession( (JET_SESID)ppib, 0 ) );
		pcppib->procid = procidNil;
		pcppib->ppib = NULL;
		}

	// (VOID) m_pinst->m_pver->ErrVERRCEClean( );
	(VOID) m_pinst->m_pver->ErrVERRCEClean( );

	for ( dbid = dbidUserLeast; dbid < dbidMax; dbid ++ )
		{
		if ( m_pinst->m_mpdbidifmp[ dbid ] < ifmpMax )
			{
			CallR( ErrBFFlush( m_pinst->m_mpdbidifmp[ dbid ] ) );
			CallR( ErrBFFlush( m_pinst->m_mpdbidifmp[ dbid ] | ifmpSLV ) );
			}
		}

	/*	Detach all the faked attachment. Detach all the databases that were restored
	 *	to new location. Attach those database with new location.
	 */
		{
		Assert( ppibNil == ppib );
		CallR( ErrPIBBeginSession( m_pinst, &ppib, procidNil, fFalse ) );

		Assert( !ppib->FRecoveringEndAllSessions() );
		ppib->SetFRecoveringEndAllSessions();

		for ( dbid = dbidUserLeast; fEndOfLog && dbid < dbidMax; dbid++ )
			{
			const IFMP	ifmp	= m_pinst->m_mpdbidifmp[ dbid ];
			if ( ifmp >= ifmpMax )
				continue;

			FMP *pfmp = &rgfmp[ ifmp ];

			if ( m_fHardRestore && pfmp->FSkippedAttach() )
				{
				Assert (!pfmp->Pdbfilehdr());
				Assert (!pfmp->FAttached() );

				// if the skipped database is attached in other instance
				// that instance will take care of the database state
				if ( ifmpMax == FMP::ErrSearchAttachedByNameSz( pfmp->SzDatabaseName() ) )
					{
					// the database isn't attached (in this process at least !)
					// and we still have the attach pending (skipped) so the database should be
					// - inconsistent after a crash and we need to allow soft recovery
					// - we haven't played forward but in this case the current logs are temporary logs
					// - we played forward but some logs were deleted at the end, we don't know if
					// the db is consistent or not but we should allow play forward just to check that
					fNeedSoftRecovery = fTrue;
					}
				}
			}
			

		for ( dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
			{
			const IFMP	ifmp	= m_pinst->m_mpdbidifmp[ dbid ];
			if ( ifmp >= ifmpMax )
				continue;

			FMP *pfmp = &rgfmp[ ifmp ];

			if ( pfmp->Pdbfilehdr()
				&& CmpLgpos( &lgposMax, &pfmp->Pdbfilehdr()->le_lgposAttach ) != 0 )
				{
				//	make sure the attached database's size is consistent with the file size.

				Call( ErrLGICheckDatabaseFileSize( pfsapi, ppib, ifmp ) );
				}

			if ( !fEndOfLog )
				continue;


			/*	if a db is attached. Check if it is restored to a new location.
			 */
			if ( ( m_fHardRestore || m_irstmapMac > 0 ) && pfmp->Pdbfilehdr() )
				{
				// the recovered databases should shutdown: as we will not have a term record
				// if we will softrecover un-restored databases, we need the 
				// detach for the restored databases
				if ( fNeedSoftRecovery )
					{
					Call( ErrIsamDetachDatabase( (JET_SESID) ppib, NULL, pfmp->SzDatabaseName(), 0 ) );
					}
				}
			else
				{
				/*	for each faked attached database, log database detachment
				/*	This happen only when someone restore a database that was compacted,
				/*	attached, used, then crashed. When restore, we ignore the compact and
				/*	attach after compact since the db does not match. At the end of restore
				/*	we fake a detach since the database is not recovered.
				/**/
				if ( !pfmp->Pdbfilehdr() && pfmp->Patchchk() )
					{
					Assert( pfmp->Patchchk() );
					Assert( pfmp->FInUse() );

					if ( pfmp->FDeferredAttach() )
						{
						//	we deferred an attachment because the db was missing,
						//	or because it was brought to a consistent state beyond
						//	the attachment point.  However, we've now hit the
						//	end of the log and the attachment is still outstanding.
						//	UNDONE: Need eventlog entry
						if ( !m_fLastLRIsShutdown )
							{
#ifdef IGNORE_BAD_ATTACH							
							const BOOL	fIgnoreMissingDB	= m_fReplayingIgnoreMissingDB;
#else
							const BOOL	fIgnoreMissingDB	= fFalse;
#endif
							if ( m_fReplayingIgnoreMissingDB )
								{
								LGPOS lgposRecT;
								Call( ErrLGForceDetachDB( ppib, ifmp, fLRForceDetachCloseSessions, &lgposRecT ) );
								}
							else
								{
								LGReportAttachedDbMismatch( m_pinst, pfmp->SzDatabaseName(), fTrue );
								AssertTracking();
								Call( ErrERRCheck( JET_errAttachedDatabaseMismatch ) );
								}
							}
						}
					else
						{
						//	only reason to have outstanding ATCHCHK
						//	is for deferred attachment or because hard restore skipped
						//	some attachments
						Assert( pfmp->FSkippedAttach() );
						}

					/*	clean up the fmp entry
					/**/
					pfmp->ResetFlags();
					pfmp->Pinst()->m_mpdbidifmp[ pfmp->Dbid() ] = ifmpMax;

				//	SLV name/root, if any, is allocated in same space as db name
					OSMemoryHeapFree( pfmp->SzDatabaseName());
					pfmp->SetSzDatabaseName( NULL );
					pfmp->SetSzSLVName( NULL );
					pfmp->SetSzSLVRoot( NULL );

					OSMemoryHeapFree( pfmp->Patchchk() );
					pfmp->SetPatchchk( NULL );
					}

				if ( pfmp->PatchchkRestored() )
					{
					OSMemoryHeapFree( pfmp->PatchchkRestored() );
					pfmp->SetPatchchkRestored( NULL );
					}
				}
			}

		PIBEndSession( ppib );
		ppib = ppibNil;
		}

	Assert( m_rgcppib != NULL );
	Assert( m_ptablehfhash != NULL );		
	LGEndPpibAndTableHashGlobal(  );
	
	if ( fEndOfLog && !fNeedSoftRecovery )
		{
		/*	enable checkpoint updates
		/**/
		m_fLGFMPLoaded = fTrue;
		}

	*pbAttach = 0;

	/*	term with checkpoint updates
	/**/
	CallS( m_pinst->ErrINSTTerm( fNeedSoftRecovery?termtypeError:termtypeNoCleanUp ) );
	
	m_pinst->m_pbAttach = NULL;
	fNeedCallNSTTerm = fFalse;
	
	/*	stop checkpoint updates
	/**/	
	m_fLGFMPLoaded = fFalse;

	if ( fEndOfLog )
		{

		if ( !fNeedSoftRecovery )
			{
			LE_LGPOS le_lgposRecoveryUndo;
			le_lgposRecoveryUndo = m_lgposRecoveryUndo;
			Call( ErrLGRecoveryQuit(
				this,
				&le_lgposRecoveryUndo,
				ple_lgposRedoFrom,
				m_fHardRestore ) );
			}
		else if ( NULL != m_pcheckpointDeleted )
			{
			// write back the checkpoint file
			CHAR	szPathJetChkLog[IFileSystemAPI::cchPathMax];
			
			Assert( m_fSignLogSet );
			Assert ( 0 == memcmp( &(m_pcheckpointDeleted->checkpoint.signLog), &m_signLog, sizeof(SIGNATURE) ) );
		
			m_critCheckpoint.Enter();
			LGFullNameCheckpoint( pfsapi, szPathJetChkLog );
			err = ErrLGIWriteCheckpoint( pfsapi, szPathJetChkLog, m_pcheckpointDeleted );
			m_critCheckpoint.Leave();
			Call ( err );
			}
			
		if ( NULL != m_pcheckpointDeleted )
			{
			OSMemoryPageFree ((void *) m_pcheckpointDeleted);
			m_pcheckpointDeleted = NULL;
			}
		}
		
	/*	Note: flush is needed in case a new generation is generated and
	/*	the global variable szLogName is set while it is changed to new names.
	/*	critical section not requested, not needed
	/**/

	// The above comment is misleading, because we really want to flush the log
	// so that all our data will hit the disk if we've logged a lrtypRecoveryQuit.
	// If it doesn't hit the disk, we'll be missing a quit record on disk and we'll
	// be in trouble. Note that calling ErrLGFlushLog() will not necessarily flush
	// all of the log buffers.
	if ( m_fRecovering && fRecoveringRedo == m_fRecoveringMode )
		{
		// If we're in redo mode, we didn't log anything, so there's no need
		// to try and flush. We don't want to call ErrLGWaitAllFlushed() when
		// we're in redo mode.
		}
	else
		{
		err = ErrLGWaitAllFlushed( pfsapi );
		}

HandleError:

	if ( ppib != ppibNil )
		PIBEndSession( ppib );

	LGEndPpibAndTableHashGlobal(  );

	if (fNeedCallNSTTerm)
		{
		Assert ( JET_errSuccess > err);
		m_fLogDisabledDueToRecoveryFailure = fTrue;
		m_pinst->ErrINSTTerm( termtypeError );
		m_fLogDisabledDueToRecoveryFailure = fFalse;
		}
		
	return err;
	}


#define cbSPExt 30

#ifdef DEBUG
void LOG::LGRITraceRedo(const LR *plr)
	{
	/* easier to debug */
	if ( m_fDBGTraceRedo )
		{
		g_critDBGPrint.Enter();

		if ( GetNOP() > 0 )
			{
			CheckEndOfNOPList( plr, this );
			}

		if ( 0 == GetNOP() || plr->lrtyp != lrtypNOP )
			{
			PrintLgposReadLR();
			ShowLR(plr, this);
			}

		g_critDBGPrint.Leave();
		}
	}
#endif


#ifdef DEBUG
#ifndef RFS2

#undef CallJ
#undef CallR

#define CallJ(f, hndlerr)                                                                       \
		{                                                                                                       \
		if ((err = (f)) < 0)                                                            \
			{                                                                                               \
			AssertSz(0,"Debug Only: ignore this assert");   \
			goto hndlerr;                                                                   \
			}                                                                                               \
		}

#define CallR(f)                                                                                        \
		{                                                                                                       \
		if ((err = (f)) < 0)                                                            \
			{                                                                                               \
			AssertSz(0,"Debug Only: ignore this assert");   \
			return err;                                                                             \
			}                                                                                               \
		}

#endif
#endif


VOID LGIReportEventOfReadError( IFMP ifmp, PGNO pgno, ERR err )
	{
	CHAR szT1[16];
	CHAR szT2[16];
	const CHAR *rgszT[3];

	rgszT[0] = rgfmp[ifmp].SzDatabaseName();
	sprintf( szT1, "%d", pgno );
	rgszT[1] = szT1;
	sprintf( szT2, "%d", err );
	rgszT[2] = szT2;
 
	UtilReportEvent(
			eventError,
			LOGGING_RECOVERY_CATEGORY,
			RESTORE_DATABASE_READ_PAGE_ERROR_ID,
			3,
			rgszT,
			0,
			NULL,
			PinstFromIfmp( ifmp ) );
	}


ERR ErrLGIStoreLogRec( PIB *ppib, DBTIME dbtime, const LR *plr )
	{
	INT cb = CbLGSizeOfRec( plr );

	return ppib->ErrInsertLogrec( dbtime, plr, cb );
	}


//	Check redo condition to decide if we need to skip redoing
//	this log record.

ERR LOG::ErrLGRICheckRedoCondition(
	const DBID		dbid,					//	dbid from the log record.
	DBTIME			dbtime,					//	dbtime from the log record.
	OBJID			objidFDP,				//	objid so far,
	PIB				*ppib,					//	returned ppib
	const BOOL		fUpdateCountersOnly,	//	if TRUE, operation will NOT be redone, but still need to update dbtimeLast and objidLast counters
	BOOL			*pfSkip )				//	returned skip flag
	{
	ERR				err;
	INST * const	pinst	= PinstFromPpib( ppib );
	const IFMP		ifmp	= pinst->m_mpdbidifmp[ dbid ];

	//	By default we want to skip it.

	*pfSkip = fTrue;

	//	check if we have to redo the database.

	if ( ifmp >= ifmpMax )
		return JET_errSuccess;

	FMP * const		pfmp	= rgfmp + ifmp;

	if ( NULL == pfmp->Pdbfilehdr() )
		return JET_errSuccess;

	//	We haven't reach the point the database is attached.

	if ( CmpLgpos( &m_lgposRedo, &pfmp->Pdbfilehdr()->le_lgposAttach ) <= 0 )
		return JET_errSuccess;

	//	check if database needs opening.

	if ( !fUpdateCountersOnly
		&& !FPIBUserOpenedDatabase( ppib, dbid ) )
		{
		IFMP	ifmpT	= ifmp;
		CallR( ErrDBOpenDatabase( ppib, 
								  pfmp->SzDatabaseName(), 
								  &ifmpT, 
								  NO_GRBIT ) );
		Assert( ifmpT == ifmp );
		}
	
	//	Keep track of lasgest dbtime so far, since the number could be
	//	logged out of order, we need to check if dbtime > than dbtimeCurrent.

	Assert( m_fRecoveringMode == fRecoveringRedo );
	if ( dbtime > pfmp->DbtimeCurrentDuringRecovery() )
		pfmp->SetDbtimeCurrentDuringRecovery( dbtime );
	if ( objidFDP > pfmp->ObjidLast() )
		pfmp->SetObjidLast( objidFDP );

	if ( fUpdateCountersOnly )
		{
		Assert( *pfSkip );
		}
	else
		{
		//	We do need to redo this log record.
		*pfSkip = fFalse;
		}

	return JET_errSuccess;
	}


ERR LOG::ErrLGRICheckRedoCondition2(
	const PROCID	procid,
	const DBID		dbid,					//	dbid from the log record.
	DBTIME			dbtime,					//	dbtime from the log record.
	OBJID			objidFDP,
	LR				*plr,
	PIB				**pppib,				//	returned ppib
	BOOL			*pfSkip )				//	returned skip flag
	{
	ERR				err;
	PIB				*ppib;
	BOOL			fUpdateCountersOnly		= fFalse;	//	if TRUE, redo not needed, but must still update dbtimeLast and objidLast counters

	*pfSkip = fTrue;

	CallR( ErrLGRIPpibFromProcid( procid, &ppib ) );
	*pppib = ppib;

	if ( ppib->FAfterFirstBT() )
		{
		Assert( NULL != plr || !ppib->FMacroGoing( dbtime ) );
		if ( ppib->FMacroGoing( dbtime )
			&& lrtypUndoInfo != plr->lrtyp )
			{
			return ErrLGIStoreLogRec( ppib, dbtime, plr );
			}
		}
	else
		{
		//	BUGFIX (X5:178265 and NT:214397): it's possible
		//	that there are no Begin0's between the first and
		//	last log records, in which case nothing needs to
		//	get redone.  HOWEVER, the dbtimeLast and objidLast
		//	counters in the db header would not have gotten
		//	flushed (they only get flushed on a DetachDb or a
		//	clean shutdown), so we must still track these
		//	counters during recovery so that we can properly
		//	update the database header on RecoveryQuit (since
		//	we pass TRUE for the fUpdateCountersOnly param to
		//	ErrLGRICheckRedoCondition() below, that function
		//	will do nothing but update the counters for us).
		fUpdateCountersOnly = fTrue;
		}

	return ErrLGRICheckRedoCondition(
			dbid,
			dbtime,
			objidFDP,
			ppib,
			fUpdateCountersOnly,
			pfSkip );
	}


//	sets dbtime on write-latched pages
//
INLINE VOID LGRIRedoDirtyAndSetDbtime( CSR *pcsr, DBTIME dbtime )
	{
	if ( latchWrite == pcsr->Latch() )
		{
		pcsr->Dirty();
		pcsr->SetDbtime( dbtime );
		}
	}

LOCAL ERR ErrLGRIRedoFreeEmptyPages(
	FUCB				* const pfucb,
	LREMPTYTREE			* const plremptytree )
	{
	ERR					err						= JET_errSuccess;
	const INST			* pinst					= PinstFromPpib( pfucb->ppib );
	const IFMP			ifmp					= pfucb->ifmp;
	CSR					* const pcsr			= &pfucb->csr;
	const EMPTYPAGE		* const rgemptypage		= (EMPTYPAGE *)plremptytree->rgb;
	const CPG			cpgToFree				= plremptytree->CbEmptyPageList() / sizeof(EMPTYPAGE);

	Assert( ifmp == pinst->m_mpdbidifmp[ plremptytree->dbid ] );
	Assert( plremptytree->CbEmptyPageList() % sizeof(EMPTYPAGE) == 0 );
	Assert( cpgToFree > 0 );

	for ( INT i = 0; i < cpgToFree; i++ )
		{
		BOOL fRedoNeeded;

		CallR( ErrLGIAccessPage( pfucb->ppib, pcsr, ifmp, rgemptypage[i].pgno ) );

		Assert( latchRIW == pcsr->Latch() );
		Assert( rgemptypage[i].pgno == pcsr->Pgno() );

		fRedoNeeded	= FLGNeedRedoCheckDbtimeBefore( *pcsr,
													plremptytree->le_dbtime,
													rgemptypage[i].dbtimeBefore,
													&err );

		// for the FLGNeedRedoCheckDbtimeBefore error code
		if ( err < 0 )
			{
			pcsr->ReleasePage();
			return err;
			}

		//	upgrade latch if needed
		//
		if ( fRedoNeeded )
			{
			pcsr->SetILine( 0 );
			pcsr->UpgradeFromRIWLatch();
			pcsr->CoordinatedDirty( plremptytree->le_dbtime );
			pcsr->Cpage().SetEmpty();
			}

		pcsr->ReleasePage();
		}

	return JET_errSuccess;
	}

	
ERR LOG::ErrLGRIRedoNodeOperation( const LRNODE_ *plrnode, ERR *perr )
	{
	ERR				err;
	PIB				*ppib;
	const PGNO		pgno		= plrnode->le_pgno;
	const PGNO		pgnoFDP		= plrnode->le_pgnoFDP;
	const OBJID		objidFDP	= plrnode->le_objidFDP;	// Debug only info.
	const PROCID	procid 		= plrnode->le_procid;
	const DBID		dbid		= plrnode->dbid;
	const DBTIME	dbtime		= plrnode->le_dbtime;
	const BOOL		fUnique		= plrnode->FUnique();
	const BOOL		fSpace		= plrnode->FSpace();
	const DIRFLAG	dirflag 	= plrnode->FVersioned() ? fDIRNull : fDIRNoVersion;
	VERPROXY		verproxy;

	verproxy.rceid = plrnode->le_rceid;
	verproxy.level = plrnode->level;
	verproxy.proxy = proxyRedo;

	Assert( !plrnode->FVersioned() || !plrnode->FSpace() );
	Assert( !plrnode->FVersioned() || rceidNull != verproxy.rceid );
	Assert( !plrnode->FVersioned() || verproxy.level > 0 );
	
	BOOL fSkip;
	CallR( ErrLGRICheckRedoCondition2(
				procid,
				dbid,
				dbtime,
				objidFDP,
				(LR *) plrnode,	//	can be in macro.
				&ppib,
				&fSkip ) );
	if ( fSkip )
		return JET_errSuccess;

	if ( ppib->level > plrnode->level )
		{
		//	if operation was performed by concurrent CreateIndex, the
		//	updater could be at a higher trx level than when the
		//	indexer logged the operation
		//	UNDONE: explain why Undo and UndoInfo can have ppib at higher trx
		Assert( plrnode->FConcCI()
			|| lrtypUndoInfo == plrnode->lrtyp
			|| lrtypUndo == plrnode->lrtyp );
		}
	else
		{
		//	UNDONE: for lrtypUndoInfo, is it really possible for ppib to
		//	be at lower level than logged operation?
		Assert( ppib->level == plrnode->level
			|| lrtypUndoInfo == plrnode->lrtyp );
		}

	//	reset CSR
	//
	CSR		csr;
	INST	*pinst = PinstFromPpib( ppib );
	IFMP	ifmp = pinst->m_mpdbidifmp[ dbid ];
	BOOL	fRedoNeeded;

	CallR( ErrLGIAccessPage( ppib, &csr, ifmp, pgno ) );
	Assert( latchRIW == csr.Latch() );

	fRedoNeeded = lrtypUndoInfo == plrnode->lrtyp ?
								fFalse : 
								FLGNeedRedoCheckDbtimeBefore( csr, dbtime, plrnode->le_dbtimeBefore, &err );

	// for the FLGNeedRedoCheckDbtimeBefore error code
	Call( err );
	
	//	set CSR
	//	upgrade latch if needed
	//
	csr.SetILine( plrnode->ILine() );
	if ( fRedoNeeded )
		{
		csr.UpgradeFromRIWLatch();
		}

	LGRITraceRedo( plrnode );

	FUCB	*pfucb;

	Assert( !fRedoNeeded || objidFDP == csr.Cpage().ObjidFDP() );

	Call( ErrLGRIGetFucb( m_ptablehfhash, ppib, ifmp, pgnoFDP, objidFDP, fUnique, fSpace, &pfucb ) );

	//	BUG?? pfucb->ppib maybe the indexer ppib while the logged ppib
	//	BUG?? maybe the updater, the proxy ppib
	//	BUG??
	//	BUG?? -cheen

	Assert( pfucb->ppib == ppib );
	
	switch ( plrnode->lrtyp )
		{
		default:
			Assert( fFalse );
			break;

		case lrtypSetExternalHeader:
			{			
			if ( !fRedoNeeded )
				{
				CallS( err );
				goto HandleError;
				}

			DATA				data;
			LRSETEXTERNALHEADER	*plrsetextheader = (LRSETEXTERNALHEADER *) plrnode;

			data.SetPv( plrsetextheader->rgbData );
			data.SetCb( plrsetextheader->CbData() );

			NDResetVersionInfo( &csr.Cpage() );
			err = ErrNDSetExternalHeader( pfucb, &csr, &data, dirflag | fDIRRedo );
			CallS( err );
			Call( err );
			}
			break;
			
		case lrtypUndoInfo:
			{
			//	restore undo information in version store
			//		for a page flushed with uncommitted change
			//		if RCE already exists for this operation at the same level,
			//			do nothing
			//
			LRUNDOINFO	*plrundoinfo = (LRUNDOINFO *) plrnode;
			RCE			*prce;
			const OPER	oper = plrundoinfo->le_oper;

			Assert( !fRedoNeeded );
			
			//	mask PIB fields to logged values for creating version
			//
			const TRX	trxOld 		= ppib->trxBegin0;
			const LEVEL	levelOld	= ppib->level;

			Assert( pfucb->ppib == ppib );

			if ( 0 == ppib->level )
				{
				//	RCE is not useful, since there is no transaction to roll back
				//
				goto HandleError;
				}

			//	remove this RCE from the list of uncreated RCEs
			Call( ppib->ErrDeregisterDeferredRceid( plrundoinfo->le_rceid ) );
				
			Assert( trxOld == plrundoinfo->le_trxBegin0 );
			ppib->level		= min( plrundoinfo->level, ppib->level );
			ppib->trxBegin0 = plrundoinfo->le_trxBegin0;
			
			//	force RCE to be recreated at same current level as ppib
			Assert( plrundoinfo->level == verproxy.level );
			verproxy.level = ppib->level;
			
			Assert( operReplace == oper ||
					operFlagDelete == oper );
			Assert( FUndoableLoggedOper( oper ) );
			
			BOOKMARK	bm;
			bm.key.prefix.Nullify();
			bm.key.suffix.SetPv( plrundoinfo->rgbData );
			bm.key.suffix.SetCb( plrundoinfo->CbBookmarkKey() );

			bm.data.SetPv( plrundoinfo->rgbData + plrundoinfo->CbBookmarkKey() );
			bm.data.SetCb( plrundoinfo->CbBookmarkData() );

			//	set up fucb as in do-time
			//
			if ( operReplace == oper )
				{
				pfucb->kdfCurr.Nullify();
				pfucb->kdfCurr.key = bm.key;
				pfucb->kdfCurr.data.SetPv( plrundoinfo->rgbData + 
											 plrundoinfo->CbBookmarkKey() +
											 plrundoinfo->CbBookmarkData() );
				pfucb->kdfCurr.data.SetCb( plrundoinfo->le_cbData );
				}
			
			//	create RCE
			//
			Assert( plrundoinfo->le_pgnoFDP == PgnoFDP( pfucb ) );
			CallJ( PverFromPpib( ppib )->ErrVERModify( pfucb, bm, oper, &prce, &verproxy ), RestorePIB );
			Assert( prceNil != prce );

			//	if oper is replace, set verreplace in RCE
			if ( prce->FOperReplace() )
				{
				VERREPLACE* pverreplace = (VERREPLACE*)prce->PbData();

				pverreplace->cbMaxSize	= plrundoinfo->le_cbMaxSize;
				pverreplace->cbDelta	= 0;
				}

			// Pass pcsrNil to prevent creation of UndoInfo
			VERInsertRCEIntoLists( pfucb, pcsrNil, prce );

		RestorePIB:
			ppib->level 	= levelOld;
			ppib->trxBegin0 = trxOld;

			if ( JET_errPreviousVersion == err )
				{
				err = JET_errSuccess;
				}
			Call( err );

			//	skip to release page
			//
			goto HandleError;
			}
			break;
			
		case lrtypUndo:
			{
			LRUNDO  *plrundo = (LRUNDO *)plrnode;

			CallR( ErrLGRIPpibFromProcid( plrundo->le_procid, &ppib ) );

			Assert( !ppib->FMacroGoing( dbtime ) );

			//	check transaction level
			//
			if ( ppib->level <= 0 )
				{
				Assert( fFalse );
				break;
				}

			LGRITraceRedo( plrnode );

			Assert( plrundo->le_pgnoFDP == PgnoFDP( pfucb ) );
			VERRedoPhysicalUndo( m_pinst, plrundo, pfucb, &csr, fRedoNeeded );

			if ( !fRedoNeeded )
				{
				CallS( err );
				goto HandleError;
				}
			}
			break;
		
		case lrtypInsert:
			{			
			LRINSERT	    *plrinsert = (LRINSERT *)plrnode;

			KEYDATAFLAGS	kdf;
			
			kdf.key.prefix.SetPv( (BYTE *) plrinsert->szKey );
			kdf.key.prefix.SetCb( plrinsert->CbPrefix() );

			kdf.key.suffix.SetPv( (BYTE *)( plrinsert->szKey ) + plrinsert->CbPrefix() );
			kdf.key.suffix.SetCb( plrinsert->CbSuffix() );

			kdf.data.SetPv( (BYTE *)( plrinsert->szKey ) + kdf.key.Cb() );
			kdf.data.SetCb( plrinsert->CbData() );

			kdf.fFlags	= 0;

			if ( kdf.key.prefix.Cb() > 0 )
				{
				Assert( plrinsert->CbPrefix() > cbPrefixOverhead );
				kdf.fFlags	|= fNDCompressed; 
				}

			//	even if operation is not redone, create version
			//	for rollback support.
			//
			if ( plrinsert->FVersioned() )
				{
				//  we don't need to set the version bit as the version store will be empty at the end of recovery
				RCE			*prce = prceNil;
				BOOKMARK	bm;
					
				NDGetBookmarkFromKDF( pfucb, kdf, &bm );
				Call( PverFromPpib( ppib )->ErrVERModify( pfucb, bm, operInsert, &prce, &verproxy ) );
				Assert( prceNil != prce );
				//  we have the page latched and we are not logging so the NDInsert won't fail
				VERInsertRCEIntoLists( pfucb, &csr, prce );
				}

			if( fRedoNeeded )
				{
				Assert( plrinsert->ILine() == csr.ILine() );
				//  no logging of versioning so this can't fail
				NDResetVersionInfo( &csr.Cpage() );				
				CallS( ErrNDInsert( pfucb, &csr, &kdf, dirflag | fDIRRedo, rceidNull, NULL ) );
				}
			else
				{
				err = JET_errSuccess;
				goto HandleError;
				}
			}
			break;

		case lrtypFlagInsert:
			{
			LRFLAGINSERT    *plrflaginsert = (LRFLAGINSERT *)plrnode;
			
			if ( plrflaginsert->FVersioned() )
				{
				KEYDATAFLAGS	kdf;

				kdf.data.SetPv( plrflaginsert->rgbData + plrflaginsert->CbKey() );
				kdf.data.SetCb( plrflaginsert->CbData() );

				kdf.key.prefix.Nullify();
				kdf.key.suffix.SetCb( plrflaginsert->CbKey() );
				kdf.key.suffix.SetPv( plrflaginsert->rgbData );

				BOOKMARK	bm;
				RCE			*prce = prceNil;
					
				NDGetBookmarkFromKDF( pfucb, kdf, &bm );
				Call( PverFromPpib( ppib )->ErrVERModify( pfucb, bm, operInsert, &prce, &verproxy ) );
				Assert( prceNil != prce );
				VERInsertRCEIntoLists( pfucb, &csr, prce );
				}

			if( fRedoNeeded )
				{
				Assert( plrflaginsert->ILine() == csr.ILine() );
				NDGet( pfucb, &csr );
				Assert( FNDDeleted( pfucb->kdfCurr ) );
			

				NDResetVersionInfo( &csr.Cpage() );
				err = ErrNDFlagInsert( pfucb, &csr, dirflag | fDIRRedo, rceidNull, NULL );
				CallS( err );
				}
			else
				{
				err = JET_errSuccess;
				goto HandleError;
				}
			Call( err );
			}
			break;
			
		case lrtypFlagInsertAndReplaceData:
			{
			LRFLAGINSERTANDREPLACEDATA	*plrfiard = 
										(LRFLAGINSERTANDREPLACEDATA *)plrnode;
			KEYDATAFLAGS	kdf;

			kdf.data.SetPv( plrfiard->rgbData + plrfiard->CbKey() );
			kdf.data.SetCb( plrfiard->CbData() );

			VERPROXY		verproxyReplace;
			verproxyReplace.rceid = plrfiard->le_rceidReplace;
			verproxyReplace.level = plrfiard->level;
			verproxyReplace.proxy = proxyRedo;

			if ( plrfiard->FVersioned() )
				{
				kdf.key.prefix.Nullify();
				kdf.key.suffix.SetCb( plrfiard->CbKey() );
				kdf.key.suffix.SetPv( plrfiard->rgbData );

				BOOKMARK	bm;
				RCE			*prce = prceNil;
					
				NDGetBookmarkFromKDF( pfucb, kdf, &bm );
				Call( PverFromPpib( ppib )->ErrVERModify( pfucb, bm, operInsert, &prce, &verproxy ) );
				Assert( prceNil != prce );
				VERInsertRCEIntoLists( pfucb, &csr, prce );
				}

			if( fRedoNeeded )
				{
				Assert( verproxyReplace.level == verproxy.level );		
				Assert( plrfiard->ILine() == csr.ILine() );
				NDGet( pfucb, &csr );
				Assert( FNDDeleted( pfucb->kdfCurr ) );

				//	page may be reorganized 
				//	copy key from node
				//

				BYTE *rgb;
				BFAlloc( (VOID **)&rgb );

//				BYTE	rgb[g_cbPageMax];
				
				pfucb->kdfCurr.key.CopyIntoBuffer( rgb, g_cbPage );
			
				kdf.key.prefix.SetCb( pfucb->kdfCurr.key.prefix.Cb() );
				kdf.key.prefix.SetPv( rgb );
				kdf.key.suffix.SetCb( pfucb->kdfCurr.key.suffix.Cb() );
				kdf.key.suffix.SetPv( rgb + kdf.key.prefix.Cb() );
				Assert( FKeysEqual( kdf.key, pfucb->kdfCurr.key ) );

				if( plrfiard->FVersioned() )
					{
					verproxy.rceid = plrfiard->le_rceidReplace;
					
					//  we need to create the replace version for undo
					//  if we don't need to redo the operation, the undo-info will create
					//  the version if necessary
					BOOKMARK	bm;
					RCE			*prce	= prceNil;

					NDGetBookmarkFromKDF( pfucb, pfucb->kdfCurr, &bm );
//					Call( PverFromPpib( ppib )->ErrVERModify( pfucb, bm, operReplace, &prce, &verproxy ) );
					err = PverFromPpib( ppib )->ErrVERModify( pfucb, bm, operReplace, &prce, &verproxy );
					if ( err < 0 )
						{
						BFFree( rgb );
						goto HandleError;
						}					
					Assert( prceNil != prce );
					VERInsertRCEIntoLists( pfucb, &csr, prce );				
					}
								
				NDResetVersionInfo( &csr.Cpage() );
				err = ErrNDFlagInsertAndReplaceData( pfucb, 
													 &csr, 
													 &kdf, 
													 dirflag | fDIRRedo,
													 rceidNull, 
													 rceidNull,
													 prceNil,
													 NULL );

				BFFree( rgb );
				
				CallS( err );
				Call( err );
				}
			else
				{
				err = JET_errSuccess;
				goto HandleError;
				}
			}
			break;
			
		case lrtypReplace:
		case lrtypReplaceD:
			{
			LRREPLACE	* const plrreplace	= (LRREPLACE *)plrnode;

			if ( !fRedoNeeded )
				{

				//	add this RCE to the list of uncreated RCEs
				if ( plrreplace->FVersioned() )
					{
					Call( ppib->ErrRegisterDeferredRceid( plrnode->le_rceid ) );
					}
				
				CallS( err );
				goto HandleError;
				}
				
			RCE				*prce = prceNil;
			const UINT		cbOldData = plrreplace->CbOldData();
			const UINT		cbNewData = plrreplace->CbNewData();
			DATA			data;
			BYTE			*rgbRecNew = NULL;
//			BYTE			rgbRecNew[g_cbPageMax];

			//	cache node
			//
			Assert( plrreplace->ILine() == csr.ILine() );
			NDGet( pfucb, &csr );

			if ( plrnode->lrtyp == lrtypReplaceD )
				{
				SIZE_T 	cb;
				BYTE 	*pbDiff	= (BYTE *)( plrreplace->szData );
				ULONG	cbDiff 	= plrreplace->Cb();

				BFAlloc( (VOID **)&rgbRecNew );

				LGGetAfterImage( pbDiff,
								 cbDiff, 
								 (BYTE *)pfucb->kdfCurr.data.Pv(),
								 pfucb->kdfCurr.data.Cb(),
								 rgbRecNew,
								 &cb );
				Assert( cb < g_cbPage );
//				Assert( cb < sizeof( rgbRecNew ) );

				data.SetPv( rgbRecNew );
				data.SetCb( cb );
				}
			else
				{
				data.SetPv( plrreplace->szData );
				data.SetCb( plrreplace->Cb() );
				}
			Assert( data.Cb() == cbNewData );

			//	copy bm to pfucb->bmCurr
			//
			BYTE *rgb;
			BFAlloc( (VOID **)&rgb );
//			BYTE	rgb[g_cbPageMax];
			pfucb->kdfCurr.key.CopyIntoBuffer( rgb, g_cbPage );
			
			Assert( FFUCBUnique( pfucb ) );
			pfucb->bmCurr.data.Nullify();
			
			pfucb->bmCurr.key.prefix.SetCb( pfucb->kdfCurr.key.prefix.Cb() );
			pfucb->bmCurr.key.prefix.SetPv( rgb );
			pfucb->bmCurr.key.suffix.SetCb( pfucb->kdfCurr.key.suffix.Cb() );
			pfucb->bmCurr.key.suffix.SetPv( rgb + pfucb->kdfCurr.key.prefix.Cb() );
			Assert( CmpKey( pfucb->bmCurr.key, pfucb->kdfCurr.key ) == 0 );

			//  if we have to redo the operation we have to create the version
			//  the page has the proper before-image on it
			if( plrreplace->FVersioned() )
				{
//				Call( PverFromPpib( ppib )->ErrVERModify( pfucb, pfucb->bmCurr, operReplace, &prce, &verproxy ) );
				err = PverFromPpib( ppib )->ErrVERModify( pfucb, pfucb->bmCurr, operReplace, &prce, &verproxy );
				if ( err < 0 )
					{
					BFFree( rgb );
					if ( NULL != rgbRecNew )
						{
						BFFree( rgbRecNew );
						}
					goto HandleError;
					}
				Assert( prceNil != prce );
				VERInsertRCEIntoLists( pfucb, &csr, prce );
				}

			NDResetVersionInfo( &csr.Cpage() );
			err = ErrNDReplace( pfucb, &csr, &data, dirflag | fDIRRedo, rceidNull, prceNil );

			BFFree( rgb );
			if ( NULL != rgbRecNew )
				{
				BFFree( rgbRecNew );
				}
			CallS( err );
			Call( err );
			}
			break;

		case lrtypFlagDelete:
			{
			const LRFLAGDELETE 	* const plrflagdelete	= (LRFLAGDELETE *)plrnode;

			if ( !fRedoNeeded )
				{
				
				//  we'll create the version using the undo-info if necessary

				//	add this RCE to the list of uncreated RCEs
				if ( plrflagdelete->FVersioned() )
					{
					Call( ppib->ErrRegisterDeferredRceid( plrnode->le_rceid ) );
					}
				
				CallS( err );
				goto HandleError;
				}
				
			//	cache node
			//
			Assert( plrflagdelete->ILine() == csr.ILine() );
			NDGet( pfucb, &csr );

			//  if we have to redo the operation we have to create the version
			if( plrflagdelete->FVersioned() )
				{
				BOOKMARK	bm;
				RCE			*prce = prceNil;
					
				NDGetBookmarkFromKDF( pfucb, pfucb->kdfCurr, &bm );
				Call( PverFromPpib( ppib )->ErrVERModify( pfucb, bm, operFlagDelete, &prce, &verproxy ) );
				Assert( prceNil != prce );
				VERInsertRCEIntoLists( pfucb, &csr, prce );
				}

			//	redo operation
			//
			NDResetVersionInfo( &csr.Cpage() );
			err = ErrNDFlagDelete( pfucb, &csr, dirflag | fDIRRedo, rceidNull, NULL );
			CallS( err );
			Call( err );
			}
			break;

		case lrtypDelete:
			{
			LRDELETE        *plrdelete = (LRDELETE *) plrnode;

			if ( !fRedoNeeded )
				{
				err = JET_errSuccess;
				goto HandleError;
				}

			//	redo node delete
			//
			Assert( plrdelete->ILine() == csr.ILine() );
			NDResetVersionInfo( &csr.Cpage() );
			err = ErrNDDelete( pfucb, &csr, fDIRNull );
			CallS( err );
			Call( err );
			}
			break;

			
		case lrtypDelta:
			{
			LRDELTA *plrdelta	= (LRDELTA *) plrnode;
			LONG    lDelta 		= plrdelta->LDelta();
			USHORT	cbOffset	= plrdelta->CbOffset();

			Assert( plrdelta->ILine() == csr.ILine() );

			if ( dirflag & fDIRNoVersion )
				{
				Assert( fFalse );
				err = JET_errLogFileCorrupt;
				goto HandleError;
				}

			VERDELTA	verdelta;
			RCE			*prce = prceNil; 
				
			verdelta.lDelta				= lDelta;
			verdelta.cbOffset			= cbOffset;
			verdelta.fDeferredDelete	= fFalse;
			verdelta.fCallbackOnZero	= fFalse;
			verdelta.fDeleteOnZero		= fFalse;

			BOOKMARK	bm;
			bm.key.prefix.Nullify();
			bm.key.suffix.SetPv( plrdelta->rgbData );
			bm.key.suffix.SetCb( plrdelta->CbBookmarkKey() );
			bm.data.SetPv( (BYTE *)plrdelta->rgbData + plrdelta->CbBookmarkKey() );
			bm.data.SetCb( plrdelta->CbBookmarkData() );

			pfucb->kdfCurr.data.SetPv( &verdelta );
			pfucb->kdfCurr.data.SetCb( sizeof( VERDELTA ) );

			Call( PverFromPpib( ppib )->ErrVERModify( pfucb, bm, operDelta, &prce, &verproxy ) );
			Assert( prce != prceNil );
			VERInsertRCEIntoLists( pfucb, &csr, prce );
			
			if( fRedoNeeded )
				{
				NDGet( pfucb, &csr );
				NDResetVersionInfo( &csr.Cpage() );
				err = ErrNDDelta( pfucb, 
							  &csr, 
							  cbOffset,
							  &lDelta, 
							  sizeof( lDelta ),
							  NULL, 
							  0,
							  NULL,
							  dirflag | fDIRRedo,
							  rceidNull );
				CallS( err );
				}
			else
				{
				err = JET_errSuccess;
				goto HandleError;
				}
			Call( err );
			}
			break;

		case lrtypSLVSpace:
			{
#ifdef DISABLE_SLV
			Call( ErrERRCheck( JET_wrnNyi ) );
#else
			const LRSLVSPACE * const plrslvspace	= (LRSLVSPACE *) plrnode;

			Assert( plrslvspace->ILine() == csr.ILine() );

			if ( !( dirflag & fDIRNoVersion ) )
				{
				VERSLVSPACE	verslvspace;
				RCE			*prce = prceNil; 

				verslvspace.oper	= SLVSPACEOPER( BYTE( plrslvspace->oper ) );
				verslvspace.ipage	= plrslvspace->le_ipage;
				verslvspace.cpages	= plrslvspace->le_cpages;
				verslvspace.fileid	= CSLVInfo::fileidNil;
				verslvspace.cbAlloc	= 0;
				verslvspace.wszFileName[0] = 0;
				
				BOOKMARK	bm;
				bm.key.prefix.Nullify();
				bm.key.suffix.SetPv( const_cast<BYTE *>( plrslvspace->rgbData ) );
				bm.key.suffix.SetCb( plrslvspace->le_cbBookmarkKey );
				bm.data.SetPv( (BYTE *)plrslvspace->rgbData + plrslvspace->le_cbBookmarkKey );
				bm.data.SetCb( plrslvspace->le_cbBookmarkData );

				pfucb->kdfCurr.data.SetPv( &verslvspace );
				pfucb->kdfCurr.data.SetCb( OffsetOf( VERSLVSPACE, wszFileName ) + sizeof( wchar_t ) );

				Call( PverFromPpib( ppib )->ErrVERModify( pfucb, bm, operSLVSpace, &prce, &verproxy ) );
				Assert( prce != prceNil );
				VERInsertRCEIntoLists( pfucb, &csr, prce );
				}
			
			if( fRedoNeeded )
				{
				NDGet( pfucb, &csr );
				NDResetVersionInfo( &csr.Cpage() );
				err = ErrNDMungeSLVSpace( pfucb, 
										  &csr, 
										  SLVSPACEOPER( BYTE( plrslvspace->oper ) ),
										  plrslvspace->le_ipage, 
										  plrslvspace->le_cpages,
										  dirflag | fDIRRedo,
										  rceidNull );
				CallS( err );
				Call( err );
				}
			else
				{
				err = JET_errSuccess;
				goto HandleError;
				}

			break;
#endif	//	DISABLE_SLV
			}

		case lrtypEmptyTree:
			{
			Call( ErrLGRIRedoFreeEmptyPages( pfucb, (LREMPTYTREE *)plrnode ) );
			if ( fRedoNeeded )
				{
				csr.Dirty();
				NDSetEmptyTree( &csr );
				}
			else
				{
				err = JET_errSuccess;
				goto HandleError;
				}
				
			break;
			}
		}

	Assert( fRedoNeeded );
	Assert( csr.FDirty() );
	
	//	the timestamp set in ND operation is not correct so reset it
	//
	csr.SetDbtime( dbtime );

	err = JET_errSuccess;

HandleError:
	Assert( pgno == csr.Pgno() );
	Assert( csr.FLatched() );

	csr.ReleasePage();
	return err;
	}


#define	fNSGotoDone		1
#define	fNSGotoCheck	2

#define	FSameTime( ptm1, ptm2 )		( memcmp( (ptm1), (ptm2), sizeof(LOGTIME) ) == 0 )


ERR LOG::ErrLGRedoFill( IFileSystemAPI *const pfsapi, LR **pplr, BOOL fLastLRIsQuit, INT *pfNSNextStep )
	{
	ERR     err, errT = JET_errSuccess;
	LONG    lgen;
	BOOL    fCloseNormally;
	LOGTIME tmOldLog = m_plgfilehdr->lgfilehdr.tmCreate;
	CHAR    szT[IFileSystemAPI::cchPathMax];
	CHAR    szFNameT[IFileSystemAPI::cchPathMax];
	LE_LGPOS   le_lgposFirstT;
	BOOL    fJetLog = fFalse;
	CHAR	szOldLogName[ IFileSystemAPI::cchPathMax ];
	LGPOS	lgposOldLastRec = m_lgposLastRec;

	/*	split log name into name components
	/**/
	CallS( pfsapi->ErrPathParse( m_szLogName, szT, szFNameT, szT ) );

	/*	end of redoable logfile, read next generation
	/**/
	if ( UtilCmpFileName( szFNameT, m_szJet ) == 0 )
		{
		Assert( m_szLogCurrent != m_szRestorePath );

		/*	we have done all the log records
		/**/
		*pfNSNextStep = fNSGotoDone;
		err = JET_errSuccess;
		goto CheckGenMaxReq;
		}

	/* close current logfile, open next generation */
	delete m_pfapiLog;
	/* set m_pfapiLog as NULL to indicate it is closed. */
	m_pfapiLog = NULL;

	strcpy( szOldLogName, m_szLogName );

OpenNextLog:

	lgen = m_plgfilehdr->lgfilehdr.le_lGeneration + 1;
	LGSzFromLogId( szFNameT, lgen );
	LGMakeLogName( m_szLogName, szFNameT );

	// if we replayed all logs from target directory
	if ( m_szLogCurrent == m_szTargetInstanceLogPath && m_lGenHighTargetInstance == m_plgfilehdr->lgfilehdr.le_lGeneration )
		{
		Assert ( '\0' != m_szTargetInstanceLogPath[0] );	
		// we don't have to try nothing more
		err = JET_errFileNotFound;
		goto NoMoreLogs;
		}

	err = pfsapi->ErrFileOpen( m_szLogName, &m_pfapiLog );

	if ( err == JET_errFileNotFound )
		{
		if ( m_szLogCurrent == m_szRestorePath || m_szLogCurrent == m_szTargetInstanceLogPath )
			{
NoMoreLogs:
			// if we have a target instance or we didn't replayed those
			// try that directory
			if ( m_szTargetInstanceLogPath[0] && m_szLogCurrent != m_szTargetInstanceLogPath)
				m_szLogCurrent = m_szTargetInstanceLogPath;
			// try instance directory
			else
				m_szLogCurrent = m_szLogFilePath;
	
			LGSzFromLogId( szFNameT, lgen );
			LGMakeLogName( m_szLogName, szFNameT );
			err = pfsapi->ErrFileOpen( m_szLogName, &m_pfapiLog );
			}
			
		}
		
	if ( err == JET_errFileNotFound )
		{

		//	open failed, try szJet

		Assert( !m_pfapiLog );
		strcpy( szFNameT, m_szJet );
		LGMakeLogName( m_szLogName, szFNameT );
		err = pfsapi->ErrFileOpen( m_szLogName, &m_pfapiLog );
		if ( JET_errSuccess == err )
			{
			fJetLog = fTrue;
			}
		}
	if ( err < 0 )
		{
		//	If open next generation fail and we haven't finished
		//	all the backup logs, then return as abrupt end.
		
		if ( !m_fReplayingReplicatedLogFiles &&
			 m_fHardRestore && lgen <= m_lGenHighRestore )
			{
			m_fAbruptEnd = fTrue;
			goto AbruptEnd;
			}

		if ( m_fReplayingReplicatedLogFiles &&
				(	err == JET_errFileNotFound ||
					err == JET_errFileAccessDenied ) )
			{
			//	We are in the mode of replaying a log. The expected
			//	log file was not found, sleep for 300 ms and retry to
			//	to open the next generation log file till it succeeds.

			UtilSleep( 300 );
			(VOID)ErrLGUpdateCheckpointFile( pfsapi, fFalse );
			goto OpenNextLog;
			}

		if ( err == JET_errFileNotFound )
			{
StartNewLog:
			/* we run out of log files, create a new edb.log in current
			 * directory for later use.
			 */
			m_critLGFlush.Enter();
			/*	Reset LG buf pointers since we are done with all the
			 *      old log records in buffer.
			 */
			m_critLGBuf.Enter();
			m_pbEntry = m_pbLGBufMin;
			m_pbWrite = m_pbLGBufMin;
			m_critLGBuf.Leave();

#ifdef UNLIMITED_DB
			const BOOL	fLGFlags	= fLGOldLogInBackup|fLGLogAttachments;
#else
			const BOOL	fLGFlags	= fLGOldLogInBackup;
#endif
			if ( ( err = ErrLGNewLogFile( pfsapi, lgen - 1, fLGFlags ) ) < 0 )
				{
				m_critLGFlush.Leave();
				return err;
				}

			m_critLGBuf.Enter();
			UtilMemCpy( m_plgfilehdr, m_plgfilehdrT, sizeof( LGFILEHDR ) );
			m_isecWrite = m_csecHeader;
			m_critLGBuf.Leave();

			m_critLGFlush.Leave();

			Assert( m_plgfilehdr->lgfilehdr.le_lGeneration == lgen );

			strcpy( szFNameT, m_szJet );
			LGMakeLogName( m_szLogName, szFNameT );
			err = pfsapi->ErrFileOpen( m_szLogName, &m_pfapiLog );

			*pfNSNextStep = fNSGotoDone;
			return err;
			}

		/* Open Fails */
		Assert( fFalse );
		Assert( !m_pfapiLog );
		if ( fLastLRIsQuit )
			{
			/* we are lucky, we have a normal end */
			*pfNSNextStep = fNSGotoDone;
			err = JET_errSuccess;
			goto CheckGenMaxReq;
			}
		return err;
		}

	//	We got the next log to play, but if last one is abruptly ended
	//	we want to stop here.

	if ( m_fAbruptEnd )
		{
		CHAR szT1[16];
		CHAR szT2[16];
		const CHAR *rgszT[3];
AbruptEnd:
		rgszT[0] = szOldLogName;
		sprintf( szT1, "%d", lgposOldLastRec.isec );
		rgszT[1] = szT1;
		sprintf( szT2, "%d", lgposOldLastRec.ib );
		rgszT[2] = szT2;
		UtilReportEvent(	eventError,
							LOGGING_RECOVERY_CATEGORY,
							REDO_END_ABRUPTLY_ERROR_ID,
							sizeof( rgszT ) / sizeof( rgszT[ 0 ] ),
							rgszT,
							0, 
							NULL,
							m_pinst );

		return ErrERRCheck( JET_errRedoAbruptEnded );
		}

	// save the previous log header
	UtilMemCpy( m_plgfilehdrT, m_plgfilehdr, sizeof(LGFILEHDR) );

	/* reset the log buffers */
	CallR( ErrLGReadFileHdr( m_pfapiLog, m_plgfilehdr, fCheckLogID ) );

	if ( !FSameTime( &tmOldLog, &m_plgfilehdr->lgfilehdr.tmPrevGen ) )
		{
		//	we found a edb.log older, which must be deleted if requested by the user
		//
		if ( m_fHardRestore && m_fDeleteOutOfRangeLogs && fJetLog && m_plgfilehdr->lgfilehdr.le_lGeneration < lgen )
			{
			//	close current logfile, open next generation
			//
			delete m_pfapiLog;
			
			//	set m_pfapiLog as NULL to indicate it is closed
			//
			m_pfapiLog = NULL;
			fJetLog = fFalse;

			CHAR szGenLast[32];
			CHAR szGenDelete[32];
			const CHAR *rgszT[] = { szGenLast, szGenDelete, m_szLogName };
			sprintf( szGenLast, "%d", lgen - 1);
			sprintf( szGenDelete, "%d", LONG( m_plgfilehdr->lgfilehdr.le_lGeneration ) );
			UtilReportEvent(
					eventWarning,
					LOGGING_RECOVERY_CATEGORY,
					DELETE_LAST_LOG_FILE_TOO_OLD_ID,
					sizeof(rgszT) / sizeof(rgszT[0]),
					rgszT,
					0,
					NULL,
					m_pinst );

			CallR ( pfsapi->ErrFileDelete( m_szLogName ) );

			// we need to copy back the previous log log header
			// so that the new edb.log will have the right prevInfo
			//
			UtilMemCpy( m_plgfilehdr, m_plgfilehdrT, sizeof(LGFILEHDR) );

			goto StartNewLog;
			}

		return ErrERRCheck( JET_errInvalidLogSequence );
		}

	le_lgposFirstT.le_lGeneration = m_plgfilehdr->lgfilehdr.le_lGeneration;
	le_lgposFirstT.le_isec = (WORD)m_csecHeader;
	le_lgposFirstT.le_ib = 0;

	m_lgposLastRec.isec = 0;

	//	scan the log to find traces of corruption before going record-to-record
	//	if any corruption is found, an error will be returned
	err = ErrLGCheckReadLastLogRecordFF( pfsapi, &fCloseNormally );
	if ( err == JET_errSuccess || FErrIsLogCorruption( err ) )
		{
		errT = err;
		}
	else
		{
		Assert( err < 0 );
		CallR( err );
		}

	//	Check if abrupt end if the file size is not the same as recorded.
	
	QWORD cbSize;
	CallR( m_pfapiLog->ErrSize( &cbSize ) );
	if ( cbSize != m_plgfilehdr->lgfilehdr.le_cbSec * QWORD( m_plgfilehdr->lgfilehdr.le_csecLGFile ) )
		{
		m_fAbruptEnd = fTrue;
		}
	
	//	now scan the first record
	//	there should be no errors about corruption since they'll be handled by ErrLGCheckReadLastLogRecordFF
	CallR( ErrLGLocateFirstRedoLogRecFF( &le_lgposFirstT, (BYTE **)pplr ) );
	//	we expect no warnings -- only success at this point
	CallS( err );
	*pfNSNextStep = fNSGotoCheck;

	//	If log is not end properly and we haven't finished
	//	all the backup logs, then return as abrupt end.
	
	//	UNDONE: for soft recovery, we need some other way to know
	//	UNDONE: if the record is played up to the crashing point.
		
	if ( m_fHardRestore && lgen <= m_lGenHighRestore
		 && m_fAbruptEnd )
		{
		goto AbruptEnd;
		}

CheckGenMaxReq:
	// check the genMaxReq+genMaxCreateTime of this log with all dbs
	if ( m_fRecovering )
		{
		LOGTIME tmEmpty;

		memset( &tmEmpty, '\0', sizeof( LOGTIME ) );
		
		for ( DBID dbidT = dbidUserLeast; dbidT < dbidMax; dbidT++ )
			{
			const IFMP	ifmp	= m_pinst->m_mpdbidifmp[ dbidT ];

			if ( ifmp >= ifmpMax )
				continue;

			FMP *		pfmpT	= &rgfmp[ ifmp ];

			Assert( !pfmpT->FReadOnlyAttach() );
			if ( pfmpT->FSkippedAttach() || pfmpT->FDeferredAttach() )
				{
				//	skipped attachments is a restore-only concept
				Assert( !pfmpT->FSkippedAttach() || m_fHardRestore );
				continue;
				}

			DBFILEHDR_FIX *pdbfilehdr	= pfmpT->Pdbfilehdr();
			Assert( pdbfilehdr );

			LONG lGenMaxRequired = pdbfilehdr->le_lGenMaxRequired;
			LONG lGenCurrent = m_plgfilehdr->lgfilehdr.le_lGeneration;
			
			// if the time of the log file does not match 
			if ( lGenMaxRequired == lGenCurrent && 
				 memcmp( &pdbfilehdr->logtimeGenMaxCreate, &tmEmpty, sizeof( LOGTIME ) ) &&
				 memcmp( &pdbfilehdr->logtimeGenMaxCreate, &m_plgfilehdr->lgfilehdr.tmCreate, sizeof( LOGTIME ) ) )
				{	
				LONG lGenMinRequired = pfmpT->Pdbfilehdr()->le_lGenMinRequired;
		
				CHAR szT2[32];
				CHAR szT3[32];
				const CHAR *rgszT[4];
				LOGTIME tm;

				rgszT[0] = pfmpT->SzDatabaseName();
				rgszT[1] = m_szLogName;

				tm = pdbfilehdr->logtimeGenMaxCreate;
				sprintf( szT2, " %02d/%02d/%04d %02d:%02d:%02d ",
							(short) tm.bMonth, (short) tm.bDay,	(short) tm.bYear + 1900,
							(short) tm.bHours, (short) tm.bMinutes, (short) tm.bSeconds);
				rgszT[2] = szT2;

				tm = m_plgfilehdr->lgfilehdr.tmCreate;
				sprintf( szT3, " %02d/%02d/%04d %02d:%02d:%02d ",
							(short) tm.bMonth, (short) tm.bDay,	(short) tm.bYear + 1900,
							(short) tm.bHours, (short) tm.bMinutes, (short) tm.bSeconds);
				rgszT[3] = szT3;
		
				UtilReportEvent(
						eventError,
						LOGGING_RECOVERY_CATEGORY,
						REDO_HIGH_LOG_MISMATCH_ERROR_ID,
						sizeof( rgszT ) / sizeof( rgszT[ 0 ] ),
						rgszT,
						0,
						NULL,
						m_pinst );
				
				return ErrERRCheck( JET_errRequiredLogFilesMissing );
				}
			}		
		}

	//	we should have success at this point
	CallS( err );

	//	return the error from ErrLGCheckReadLastLogRecordFF
	return errT;
	}

ERR LOG::ErrReplaceRstMapEntry( const CHAR *szName, SIGNATURE * pDbSignature, const BOOL fSLVFile )
	{
	INT  irstmap;
	
	Assert ( pDbSignature );

	for ( irstmap = 0; irstmap < m_irstmapMac; irstmap++ )
		{
		if ( 0 == memcmp( pDbSignature, &m_rgrstmap[irstmap].signDatabase, sizeof(SIGNATURE) ) &&
			fSLVFile == m_rgrstmap[irstmap].fSLVFile )
			{
			char * szSrcDatabaseName;
			
			// found the entry we want to replace
			Assert ( m_rgrstmap[irstmap].szDatabaseName );

			if ( 0 == UtilCmpFileName(m_rgrstmap[irstmap].szDatabaseName, szName) )
				return JET_errSuccess;
			
			if ( ( szSrcDatabaseName = static_cast<CHAR *>( PvOSMemoryHeapAlloc( strlen( szName ) + 1 ) ) ) == NULL )
				return ErrERRCheck( JET_errOutOfMemory );
				
			strcpy( szSrcDatabaseName, szName );

			{
			const UINT	csz	= 2;
			const CHAR	* rgszT[csz];

			rgszT[0] = m_rgrstmap[irstmap].szDatabaseName;
			rgszT[1] = szSrcDatabaseName;
			UtilReportEvent(
					eventInformation,
					LOGGING_RECOVERY_CATEGORY, 
					DB_LOCATION_CHANGE_DETECTED,
					csz,
					rgszT,
					0,
					NULL,
					m_pinst );
			}
			
			OSMemoryHeapFree( m_rgrstmap[irstmap].szDatabaseName );
			m_rgrstmap[irstmap].szDatabaseName = szSrcDatabaseName;
			
			return JET_errSuccess;
			}
		}
	
	return ErrERRCheck( JET_errFileNotFound );
	}

INT LOG::IrstmapLGGetRstMapEntry( const CHAR *szName, const CHAR *szDbNameIfSLV )
	{
	INT  irstmap;
	BOOL fFound = fFalse;
	BOOL fSLVFile = (NULL != szDbNameIfSLV);

	Assert( szName );
	//	NOTE: rstmap ALWAYS uses the OS file-system

	for ( irstmap = 0; irstmap < m_irstmapMac; irstmap++ )
		{
		CHAR			szPathT[IFileSystemAPI::cchPathMax];
		CHAR			szFNameT[IFileSystemAPI::cchPathMax];
		const CHAR	* 	szT;
		const CHAR *	szRst;

		if ( m_fExternalRestore || m_fReplayingReplicatedLogFiles )
			{
			/*	Use the database path to search.
			 */
			szT = szName;
			szRst = m_rgrstmap[irstmap].szDatabaseName;
			}
		else
			{
			/*	use the generic name to search
			 */
			 // if we search a SLV file, the generic name is the database one
			CallS( m_pinst->m_pfsapi->ErrPathParse( fSLVFile?szDbNameIfSLV:szName, szPathT, szFNameT, szPathT ) );
			szT = szFNameT;
			szRst = m_rgrstmap[irstmap].szGenericName;
			}

		if ( _stricmp( szRst, szT ) == 0 && (fSLVFile == m_rgrstmap[irstmap].fSLVFile ) )
			{
			fFound = fTrue;
			break;
			}
		}
	if ( !fFound )
		return -1;
	else
		return irstmap;
	}


INT LOG::IrstmapSearchNewName( const CHAR *szName )
	{
	INT  irstmap;
	BOOL fFound = fFalse;

	Assert( szName );

	for ( irstmap = 0; irstmap < m_irstmapMac; irstmap++ )
		{
		if ( m_rgrstmap[irstmap].szNewDatabaseName && 0 == UtilCmpFileName( m_rgrstmap[irstmap].szNewDatabaseName, szName ) )
			{
			Assert ( m_rgrstmap[irstmap].szDatabaseName );
			return irstmap;
			}			
		}
		
	return -1;
	}


ERR LOG::ErrLGRISetupFMPFromAttach(
				IFileSystemAPI *const	pfsapi,
				PIB						*ppib,
				const SIGNATURE			*pSignLog,
				const ATTACHINFO * 		pAttachInfo,
				LGSTATUSINFO *			plgstat )
	{
	ERR								err 				= JET_errSuccess;
	const CHAR						*szDbName;
	const CHAR						*szSLVName;
	const CHAR						*szSLVRoot;
	BOOL 							fSkippedAttach = fFalse;

	Assert ( pAttachInfo );
	Assert ( pAttachInfo->CbNames() > 0 );

	szDbName = pAttachInfo->szNames;
	if ( pAttachInfo->FSLVExists() )
		{
		szSLVName = szDbName + strlen( szDbName ) + 1;
		Assert( 0 != *szSLVName );

		szSLVRoot = szSLVName + strlen( szSLVName ) + 1;
		Assert( 0 != *szSLVRoot );
		}
	else
		{
		szSLVName = NULL;
		szSLVRoot = NULL;
		}

	if ( m_fExternalRestore || m_irstmapMac > 0  )
		{
		INT			irstmap;
		//	attach the database specified in restore map.
		//
		err = ErrLGGetDestDatabaseName( pfsapi, szDbName, &irstmap, plgstat );
		if ( JET_errFileNotFound == err )
			{
			/*	not in the restore map, set to skip it.
			 */
			fSkippedAttach = fTrue;
			err = JET_errSuccess;
			}
		else
			{
			Call( err );
			szDbName = m_rgrstmap[irstmap].szNewDatabaseName;
			}			
		}
					
	if ( NULL != szSLVName )
		{
		if ( m_fExternalRestore || m_irstmapMac > 0 )
			{
			INT		irstmap;
			// pAttachInfo->szNames is still original szDbName for this SLV
			err = ErrLGGetDestDatabaseName( pfsapi, szSLVName, &irstmap, plgstat, pAttachInfo->szNames );
			if ( JET_errFileNotFound == err )
				{
				/*	not in the restore map, set to skip it.
				 */
				// the database should be set skipped already
				Assert ( fSkippedAttach  );
				fSkippedAttach = fTrue;
				err = JET_errSuccess;
				}
			else
				{
				Call( err );
				szSLVName = m_rgrstmap[irstmap].szNewDatabaseName;
				}
			}
		}
	//	Get one free fmp entry

	IFMP	ifmp;
	CallR( FMP::ErrNewAndWriteLatch(
					&ifmp,
					szDbName,
					szSLVName,
					szSLVRoot,
					ppib,
					m_pinst,
					pfsapi,
					pAttachInfo->Dbid() ) );

	FMP		*pfmpT;
	pfmpT = &rgfmp[ ifmp ];
		
	Assert( !pfmpT->Pfapi() );
	Assert( NULL == pfmpT->Pdbfilehdr() );
	Assert( pfmpT->FInUse() );
	Assert( pfmpT->Dbid() == pAttachInfo->Dbid() );

	//	get logging/versioning flags (versioning can only be disabled if logging is disabled)
	pfmpT->ResetReadOnlyAttach();
	pfmpT->ResetVersioningOff();
	pfmpT->SetLogOn();

	if( pAttachInfo->FSLVProviderNotEnabled() )
		{
		pfmpT->SetSLVProviderNotEnabled();
		}

	if ( fSkippedAttach )
		{
		pfmpT->SetSkippedAttach();
		pfmpT->SetLogOn();
		pfmpT->ResetVersioningOff();
		}

	LGPOS	lgposConsistent;
	LGPOS	lgposAttach;

	lgposConsistent = pAttachInfo->le_lgposConsistent;
	lgposAttach = pAttachInfo->le_lgposAttach;

	//	get lgposAttch
	err = ErrLGRISetupAtchchk(
				ifmp,
				&pAttachInfo->signDb,
				pSignLog,
				&lgposAttach,
				&lgposConsistent,
				pAttachInfo->Dbtime(),
				pAttachInfo->ObjidLast(),
				pAttachInfo->CpgDatabaseSizeMax() );
				
	pfmpT->ReleaseWriteLatch( ppib );
	
HandleError:
	return err;
	}

ERR ErrLGRISetupAtchchk(
	const IFMP					ifmp,
	const SIGNATURE				*psignDb,
	const SIGNATURE				*psignLog,
	const LGPOS					*plgposAttach,
	const LGPOS					*plgposConsistent,
	const DBTIME				dbtime,
	const OBJID					objidLast,
	const CPG					cpgDatabaseSizeMax )
	{
	FMP				*pfmp		= &rgfmp[ifmp];
	ATCHCHK			*patchchk;

	Assert( NULL != pfmp );
	
	if ( pfmp->Patchchk() == NULL )
		{
		patchchk = static_cast<ATCHCHK *>( PvOSMemoryHeapAlloc( sizeof( ATCHCHK ) ) );
		if ( NULL == patchchk )
			return ErrERRCheck( JET_errOutOfMemory );
		pfmp->SetPatchchk( patchchk );
		}
	else
		{
		patchchk = pfmp->Patchchk();
		}

	patchchk->signDb = *psignDb;
	patchchk->signLog = *psignLog;
	patchchk->lgposAttach = *plgposAttach;
	patchchk->lgposConsistent = *plgposConsistent;
	patchchk->SetDbtime( dbtime );
	patchchk->SetObjidLast( objidLast );
	patchchk->SetCpgDatabaseSizeMax( cpgDatabaseSizeMax );
	pfmp->SetLgposAttach( *plgposAttach );

	return JET_errSuccess;
	}

LOCAL VOID LGRIReportUnableToReadDbHeader(
	const INST * const	pinst,
	const ERR			err,
	const CHAR *		szDbName )
	{
	//	even though this is a warning, suppress it
	//	if information events are suppressed
	//
	if ( !pinst->m_fNoInformationEvent )
		{
		CHAR		szT1[16];
		const CHAR	* rgszT[2];

		rgszT[0] = szDbName;
		sprintf( szT1, "%d", err );
		rgszT[1] = szT1;
 
		UtilReportEvent(
				eventWarning,
				LOGGING_RECOVERY_CATEGORY,
				RESTORE_DATABASE_READ_HEADER_WARNING_ID,
				2,
				rgszT,
				0,
				NULL,
				pinst );
		}
	}

ERR LOG::ErrLGRICheckRedoCreateDb(
	IFileSystemAPI *const	pfsapi,
	const IFMP					ifmp,
	DBFILEHDR					*pdbfilehdr,
	REDOATTACH					*predoattach )
	{
	ERR				err;
	FMP	* 			pfmp		= &rgfmp[ifmp];
	ATCHCHK	* 		patchchk	= pfmp->Patchchk();
	const CHAR * 	szDbName 	= pfmp->SzDatabaseName();

	Assert( NULL != pfmp );
	Assert( NULL == pfmp->Pdbfilehdr() );
	Assert( NULL != patchchk );
	Assert( NULL != szDbName );
	Assert( NULL != pdbfilehdr );
	Assert( NULL != predoattach );
	
	Assert( !pfmp->FReadOnlyAttach() );
	err = ErrUtilReadAndFixShadowedHeader( pfsapi, szDbName, (BYTE*)pdbfilehdr, g_cbPage, OffsetOf( DBFILEHDR, le_cbPageSize ) );
	if ( err >= JET_errSuccess && pdbfilehdr->Dbstate() == JET_dbstateJustCreated )
		{
		// we have to delete the edb and the stm but we have to make sure
		// that the stm is matching if it exists
		if ( pdbfilehdr->FSLVExists() )
			{
			SLVFILEHDR *pslvfilehdr = NULL;
			err = ErrSLVAllocAndReadHeader(	pfsapi, 
											rgfmp[ifmp].FReadOnlyAttach(),
											rgfmp[ifmp].SzSLVName(),
											pdbfilehdr,
											&pslvfilehdr );
			// if the SLV file is missing, delete the EDB as well because it is just created
			if ( JET_errSLVStreamingFileMissing == err )
				{
				Assert( NULL == pslvfilehdr );
				err = JET_errSuccess;
				}
			else
				{
				if ( NULL != pslvfilehdr )
					{
					if ( JET_errDatabaseStreamingFileMismatch == err )
						{
						// we get mismatch but the signature is matching as
						//	we do get the header we know that it is the
						//	same db so we can delete the stm as well
						err = JET_errSuccess;
						}

					OSMemoryPageFree( (VOID *)pslvfilehdr );
					pslvfilehdr = NULL;
					}
				CallR( err );
				
				// the SLV and EDB are matching, we first delete the SLV
// ***			deletion now performed in ErrDBCreateDatabase() via JET_bitDbOverwriteExisting
// ***			CallR( pfsapi->ErrFileDelete( rgfmp[ifmp].SzSLVName() ) );
				}
			}

// ***	deletion now performed in ErrDBCreateDatabase() via JET_bitDbOverwriteExisting
// ***	CallR( pfsapi->ErrFileDelete( szDbName ) );

		*predoattach = redoattachCreate;
		return JET_errSuccess;
		}

	else if ( JET_errDiskIO == err
		|| ( err >= 0
			&& pdbfilehdr->Dbstate() != JET_dbstateConsistent
			&& pdbfilehdr->Dbstate() != JET_dbstateForceDetach
			&& pdbfilehdr->Dbstate() != JET_dbstateInconsistent ) )
		{
		// JET_dbstateJustCreated dealt with above
		Assert( JET_errDiskIO == err || pdbfilehdr->Dbstate() != JET_dbstateJustCreated );

		//	header checksums incorrectly or invalid state, so go ahead and recreate the db
		*predoattach = redoattachCreate;
		return JET_errSuccess;
		}

	else if ( err < 0 )
		{
		switch ( err )
			{
			case JET_errFileNotFound:
			case JET_errInvalidPath:
			case JET_errFileAccessDenied:
				//	assume database got deleted in the future
				*predoattach = redoattachDefer;
				err = JET_errSuccess;
				break;

			default:
				LGRIReportUnableToReadDbHeader( m_pinst, err, szDbName );
			}

		return err;
		}
	
	if ( memcmp( &pdbfilehdr->signLog, &m_signLog, sizeof( SIGNATURE ) ) != 0 )
		{
		CallR( ErrERRCheck( JET_errDatabaseLogSetMismatch ) );
		}

	const INT	i	= CmpLgpos( &pdbfilehdr->le_lgposAttach, &patchchk->lgposAttach );
	if ( 0 == i )
		{
		if ( 0 == memcmp( &pdbfilehdr->signDb, &patchchk->signDb, sizeof(SIGNATURE) ) )
			{
			if ( CmpLgpos( &patchchk->lgposAttach, &pdbfilehdr->le_lgposConsistent ) <= 0 )
				{
				//	db was brought to a consistent state in the future, so no
				//	need to redo operations until then, so attach null
				*predoattach = redoattachDefer;
				}
			else
				{
				CallR( ErrLGRICheckAttachNow( pdbfilehdr, szDbName ) );
			
				//	db never brought to consistent state after it was created
				Assert( 0 == CmpLgpos( &lgposMin, &pdbfilehdr->le_lgposConsistent ) );
				Assert( JET_dbstateInconsistent == pdbfilehdr->Dbstate() );
				*predoattach = redoattachNow;
				}
			}
		else
			{
			//	database has same log signature and lgposAttach as
			//	what was logged, but db signature is different - must
			//	have manipulated the db with logging disabled, which
			//	causes us to generate a new signDb.
			//	Defer this attachment.  We will attach later on when
			//	we hit the AttachDb log record matching this signDb.
			AssertTracking();
			*predoattach = redoattachDefer;
			}
		}
	else if ( i > 0 )
		{
		//	database was attached in the future (if db signatures match)
		//	or deleted then recreated in the future (if db signatures don't match),
		//	but in either case, we simply defer attachment to the future
		*predoattach = redoattachDefer;
		}
	else
		{
		//	this must be a different database (but with the same name)
		//	that was deleted in the past, so just overwrite it
		Assert( 0 != memcmp( &pdbfilehdr->signDb, &patchchk->signDb, sizeof(SIGNATURE) ) );
		*predoattach = redoattachCreate;
		}

	return err;
	}


ERR LOG::ErrLGRICheckRedoAttachDb(
	IFileSystemAPI *const	pfsapi,
	const IFMP					ifmp,
	DBFILEHDR					*pdbfilehdr,
	const SIGNATURE				*psignLogged,
	REDOATTACH					*predoattach,
	const REDOATTACHMODE		redoattachmode )
	{
	ERR				err;
	FMP * 			pfmp		= &rgfmp[ifmp];
	ATCHCHK * 		patchchk	= pfmp->Patchchk();
	const CHAR * 	szDbName 	= pfmp->SzDatabaseName();

	Assert( NULL != pfmp );
	Assert( NULL == pfmp->Pdbfilehdr() );
	Assert( NULL != patchchk );
	Assert( NULL != szDbName );
	Assert( NULL != pdbfilehdr );
	Assert( NULL != psignLogged );
	Assert( NULL != predoattach );
	
	Assert( !pfmp->FReadOnlyAttach() );
	err = ErrUtilReadAndFixShadowedHeader( pfsapi, szDbName, (BYTE*)pdbfilehdr, g_cbPage, OffsetOf( DBFILEHDR, le_cbPageSize ) );
	if ( JET_errDiskIO == err
		|| ( err >= 0
			&& pdbfilehdr->Dbstate() != JET_dbstateConsistent
			&& pdbfilehdr->Dbstate() != JET_dbstateForceDetach
			&& pdbfilehdr->Dbstate() != JET_dbstateInconsistent ) )
		{
		if ( JET_dbstateForceDetach == pdbfilehdr->Dbstate() )
			{
			CHAR		szT1[128];
			const CHAR	*rgszT[3];
			LOGTIME		tm;

			/*	log event that the database is not recovered completely
			/**/
			rgszT[0] = szDbName;
			tm = pdbfilehdr->signDb.logtimeCreate;
			sprintf( szT1, "%d/%d/%d %d:%d:%d",
				(short) tm.bMonth, (short) tm.bDay,	(short) tm.bYear + 1900,
				(short) tm.bHours, (short) tm.bMinutes, (short) tm.bSeconds);
			rgszT[1] = szT1;

			UtilReportEvent(
					eventError,
					LOGGING_RECOVERY_CATEGORY,
					RESTORE_DATABASE_MISSED_ERROR_ID,
					2,
					rgszT,
					0,
					NULL,
					m_pinst );
			}
		
		err = ErrERRCheck( JET_errDatabaseCorrupted );
		return err;
		}
	else if ( err < 0 )
		{
		switch ( err )
			{
			case JET_errFileNotFound:
			case JET_errInvalidPath:
			case JET_errFileAccessDenied:
				//	assume database got deleted in the future
				*predoattach = redoattachDefer;
				err = JET_errSuccess;
				break;

			default:
				LGRIReportUnableToReadDbHeader( m_pinst, err, szDbName );
			}

		return err;
		}
	
	const BOOL	fMatchingSignDb			= ( 0 == memcmp( &pdbfilehdr->signDb, &patchchk->signDb, sizeof(SIGNATURE) ) );
	const BOOL	fMatchingSignLog		= ( 0 == memcmp( &pdbfilehdr->signLog, &m_signLog, sizeof(SIGNATURE) ) );
	const BOOL	fMatchingLoggedSignLog	= ( 0 == memcmp( &pdbfilehdr->signLog, psignLogged, sizeof(SIGNATURE) ) );

	if ( fMatchingSignLog )
		{
		//	db is in sync with current log set, so use normal attach logic below

		//	only way logged log signature doesn't match db log signature
		//	is if we're redoing an attachment
		Assert( fMatchingLoggedSignLog || redoattachmodeAttachDbLR == redoattachmode );
		}
	else if ( fMatchingLoggedSignLog )
		{
		//	if db matches prev log signature, then it should also match lgposConsistent
		//	(since dbfilehdr never got updated, it must have both prev log signature
		//	and prev lgposConsistent)
		if ( 0 == CmpLgpos( &patchchk->lgposConsistent, &pdbfilehdr->le_lgposConsistent ) )
			{
			if ( fMatchingSignDb )
				{
				Assert( !pfmp->FReadOnlyAttach() );
				CallR( ErrLGRICheckAttachNow( pdbfilehdr, szDbName ) );
			
				//	the attach operation was logged, but header was not changed.
				//	set up the header such that it looks like it is set up after
				//	attach (if this is currently a ReadOnly attach, the header
				//	update will be deferred to the next non-ReadOnly attach)
				Assert( redoattachmodeAttachDbLR == redoattachmode );
				Assert( 0 == CmpLgpos( &patchchk->lgposAttach, &m_lgposRedo ) );

				//	UNDONE: in theory, lgposAttach should already have been set
				//	when the ATCHCHK was setup, but ivantrin says he's not 100%
				//	sure, so to be safe, we definitely set the lgposAttach here
				Assert( 0 == CmpLgpos( patchchk->lgposAttach, pfmp->LgposAttach() ) );
				pfmp->SetLgposAttach( patchchk->lgposAttach );
				if ( pdbfilehdr->FSLVExists() )
					{
					SLVFILEHDR *pslvfilehdr = NULL;
					err = ErrSLVAllocAndReadHeader(	pfsapi, 
													rgfmp[ifmp].FReadOnlyAttach(),
													rgfmp[ifmp].SzSLVName(),
													pdbfilehdr,
													&pslvfilehdr );
					if ( pslvfilehdr == NULL )
						{
						CallR( err );
						}
					DBISetHeaderAfterAttach( pdbfilehdr, patchchk->lgposAttach, ifmp, fFalse /* no keep bkinfo */);
					Assert( pdbfilehdr->le_objidLast > 0 );
					Assert( pslvfilehdr != NULL );
					//	Previous attach succeeded to update only SLV header, but not a DB header
					if ( err < 0 )
						{
						//	if we successfully detached STM file, but database.
						//	reset consistent position
						pslvfilehdr->le_lgposConsistent = pdbfilehdr->le_lgposConsistent;
						err = ErrSLVCheckDBSLVMatch( pdbfilehdr, pslvfilehdr );
						}
					if ( err < 0 )
						{
						OSMemoryPageFree( (VOID *)pslvfilehdr );
						pslvfilehdr = NULL;
						CallR( err );
						}
					Assert( pslvfilehdr != NULL );
					err = ErrSLVSyncHeader(	pfsapi, 
											rgfmp[ifmp].FReadOnlyAttach(),
											rgfmp[ifmp].SzSLVName(), 
											pdbfilehdr, 
											pslvfilehdr );
					OSMemoryPageFree( (VOID *)pslvfilehdr );
					pslvfilehdr = NULL;
					CallR( err );	
					}
				else
					{
					DBISetHeaderAfterAttach( pdbfilehdr, patchchk->lgposAttach, ifmp, fFalse /* no keep bkinfo */);
					Assert( pdbfilehdr->le_objidLast > 0 );
					}
				CallR( ErrUtilWriteShadowedHeader(	pfsapi, 
													szDbName,
													fTrue,
													(BYTE *)pdbfilehdr, 
													g_cbPage ) );
				*predoattach = redoattachNow;
				}
			else
				{
				//	database has same log signature and lgposConsistent as
				//	what was logged, but db signature is different - must
				//	have manipulated the db with logging disabled, which
				//	causes us to generate a new signDb.
				//	Defer this attachment.  We will attach later on when
				//	we hit the AttachDb log record matching this signDb.
				AssertTracking();
				*predoattach = redoattachDefer;
				}
			}
		else
			{
			if ( fMatchingSignDb )
				{
				//	this should be impossible, because it means that we missed
				//	replaying a shutdown (to bring the db to a consistent state)
				Assert( fFalse );
				err = ErrERRCheck( JET_errConsistentTimeMismatch );
				}
			else
				{
				*predoattach = redoattachDefer;
				}
			}

		return err;
		}
	else
		{
		//	the database's log signature is not the same as current log set or
		//	as the log set before it was attached, so just ignore it
		*predoattach = redoattachDefer;
		return JET_errSuccess;
		}

	const INT	i	= CmpLgpos( &pdbfilehdr->le_lgposConsistent, &patchchk->lgposConsistent );

	//	if log signature in db header doesn't match logged log signature,
	//	then comparing lgposConsistent is irrelevant and must instead
	//	rely on lgposAttach comparison
	if ( !fMatchingLoggedSignLog
		|| 0 == i
		|| 0 == CmpLgpos( &pdbfilehdr->le_lgposConsistent, &lgposMin ) )
		{
		if ( fMatchingSignDb )
			{
			const INT	j	= CmpLgpos( &pdbfilehdr->le_lgposAttach, &patchchk->lgposAttach );
			if ( 0 == j )
				{
				CallR( ErrLGRICheckAttachNow( pdbfilehdr, szDbName ) );
			
				//	either lgposAttach also matches, or we're redoing a new attach
				*predoattach = redoattachNow;
				}
			else if ( j < 0 )
				{
				if ( redoattachmodeAttachDbLR == redoattachmode && fMatchingLoggedSignLog )
					{
					CallR( ErrLGRICheckAttachNow( pdbfilehdr, szDbName ) );
					*predoattach = redoattachNow;
					}
				else
					{
					//	lgposConsistent match, but the lgposAttach in the database is before
					//	the logged lgposAttach, which means the attachment was somehow skipped
					//	in this database (typically happens when a file copy of the database
					//	is copied back in)
					LGReportAttachedDbMismatch( m_pinst, szDbName, fFalse );
					AssertTracking();
					CallR( ErrERRCheck( JET_errAttachedDatabaseMismatch ) );
					}
				}
			else
				{
				// Removed the following assert. The assert where probably here when
				// the 3rd OR condition (0 == CmpLgpos( &pdbfilehdr->le_lgposConsistent, &lgposMin ) )
				// was missing. See bug X5:114743 for the scenario in with the assert is false 
				// and we have to defer the attachements
				//	only way we could have same lgposConsistent, but lgposAttach
				//	in db is in the future is if this is a ReadOnly attach
				//Assert( redoattachmodeAttachDbLR == redoattachmode );
				//Assert( fReadOnly || !fMatchingLoggedSignLog );
				
				*predoattach = redoattachDefer;
				}
			}
		else
			{
#ifdef DEBUG			
			//	database has same log signature and lgposConsistent as
			//	what was logged, but db signature is different - must
			//	have recreated db or manipulated it with logging disabled,
			//	which causes us to generate a new signDb
			if ( 0 == CmpLgpos( &pdbfilehdr->le_lgposConsistent, &lgposMin ) )
				{
				//	there's a chance we can still hook up with the correct
				//	signDb in the future
				}
			else
				{
				//	can no longer replay operations against this database
				//	because we've hit a point where logging was disabled
				//	UNDONE: add eventlog entry
				AssertTracking();
				}
#endif

			*predoattach = redoattachDefer;
			}
		}
	else if ( i > 0
		&& ( redoattachmodeInitBeforeRedo == redoattachmode 
			|| CmpLgpos( &pdbfilehdr->le_lgposConsistent, &m_lgposRedo ) > 0 ) )
		{
		//	database was brought to a consistent state in the future
		//	(if db signatures match) or deleted then recreated and 
		//	reconsisted in the future (if db signatures don't match),
		//	but in either case, we simply defer attachment to the future
		Assert( redoattachmodeInitBeforeRedo == redoattachmode
			|| redoattachmodeAttachDbLR == redoattachmode );
		Assert( 0 != CmpLgpos( &pdbfilehdr->le_lgposConsistent, &lgposMin ) );
		Assert( CmpLgpos( &pdbfilehdr->le_lgposAttach, &patchchk->lgposAttach ) >= 0 );
		*predoattach = redoattachDefer;
		}
	else
		{
		if ( fMatchingSignDb )
			{
			//	One way to get here is to do the following:
			//		- shutdown cleanly
			//		- make a file copy of the db
			//		- start up and re-attach
			//		- make a backup
			//		- shutdown cleanly
			//		- copy back original database
			//		- start up and re-attach
			//		- shutdown cleanly
			//		- restore from backup
			//	When hard recovery hits the attachment to the
			//	original database, the dbfilehdr's lgposConsistent
			//	will be greater than the logged one, but less
			//	than the current lgposRedo.
			//
			//	Another way to hit this is if the lgposConsistent
			//	in the dbfilehdr is less than the logged lgposConsistent.
			//	This is usually caused when an old copy of the database
			//	is being played against a more current copy of the log files.
			//
			AssertTracking();
			CallR( ErrERRCheck( JET_errConsistentTimeMismatch ) );
			}
		else
			{
			Assert( 0 != CmpLgpos( &pdbfilehdr->le_lgposConsistent, &lgposMin ) );

			//	database has been manipulated with logging disabled in
			//	the past.  Therefore, we cannot replay operations
			//	since we are missing the non-logged operations.
			//	UNDONE: add eventlog entry
			AssertTracking();

			*predoattach = redoattachDefer;
			}
		}

	return err;
	}

			
ERR LOG::ErrLGRICheckAttachedDb(
	IFileSystemAPI *const	pfsapi,
	const IFMP					ifmp,
	const SIGNATURE				*psignLogged,			//	pass NULL for CreateDb
	REDOATTACH					*predoattach,
	const REDOATTACHMODE		redoattachmode )
	{
	ERR				err;
	FMP				*pfmp		= &rgfmp[ifmp];
	DBFILEHDR		*pdbfilehdr;

	Assert( NULL != pfmp );
	Assert( NULL == pfmp->Pdbfilehdr() );
	Assert( NULL != pfmp->Patchchk() );
	Assert( NULL != predoattach );

	pdbfilehdr = (DBFILEHDR *)PvOSMemoryPageAlloc( g_cbPage, NULL );
	if ( NULL == pdbfilehdr )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	//	WARNING: must zero out memory, because we may end
	//	up defer-attaching the database, but still compare
	//	against stuff in this memory
	//
	memset( pdbfilehdr, 0, g_cbPage );

	Assert( !pfmp->FReadOnlyAttach() );
	if ( redoattachmodeCreateDbLR == redoattachmode )
		{
		Assert( NULL == psignLogged );
		err = ErrLGRICheckRedoCreateDb( pfsapi, ifmp, pdbfilehdr, predoattach );
		}
	else
		{
		Assert( NULL != psignLogged );
		err = ErrLGRICheckRedoAttachDb(
					pfsapi,
					ifmp,
					pdbfilehdr,
					psignLogged,
					predoattach,
					redoattachmode );
		Assert( redoattachCreate != *predoattach );
		}

	//	if redoattachCreate, dbfilehdr will be allocated
	//	when we re-create the db
	if ( err >= 0 && redoattachCreate != *predoattach )
		{
		const ATCHCHK	*patchchk	= pfmp->Patchchk();
		Assert( NULL != patchchk );
		
		const DBTIME	dbtime	= max( (DBTIME) patchchk->Dbtime(), (DBTIME) pdbfilehdr->le_dbtimeDirtied );
		const OBJID		objid	= max( (OBJID) patchchk->ObjidLast(), (OBJID) pdbfilehdr->le_objidLast );

		pfmp->SetDbtimeLast( dbtime );
		pfmp->SetObjidLast( objid );
		err = pfmp->ErrSetPdbfilehdr( pdbfilehdr );
		if( err < JET_errSuccess )
			{
			OSMemoryPageFree( pdbfilehdr );
			}		
		}
	else
		{
		OSMemoryPageFree( pdbfilehdr );
		}

	return err;
	}

//	we've determined this is the correct attachment point,
//	but first check database header for possible logfile mismatches
LOCAL ERR LOG::ErrLGRICheckAttachNow(
	DBFILEHDR	* pdbfilehdr,
	const CHAR	* szDbName )
	{
	ERR			err					= JET_errSuccess;
	const LONG	lGenMinRequired		= pdbfilehdr->le_lGenMinRequired;
	const LONG	lGenCurrent			= m_plgfilehdr->lgfilehdr.le_lGeneration;
		
	if ( lGenMinRequired			//	0 means db is consistent or this is an ESE97 db
		&& lGenMinRequired < lGenCurrent )
		{
		const LONG	lGenMaxRequired	= pdbfilehdr->le_lGenMaxRequired;
		CHAR		szT1[16];
		CHAR		szT2[16];
		CHAR		szT3[16];
		const UINT	csz = 4;
		const CHAR *rgszT[csz];

		rgszT[0] = szDbName;
		sprintf( szT1, "%d", lGenMinRequired );
		rgszT[1] = szT1;
		sprintf( szT2, "%d", lGenMaxRequired );
		rgszT[2] = szT2;
		sprintf( szT3, "%d", lGenCurrent );
		rgszT[3] = szT3;
		
		UtilReportEvent( 
				eventError, 
				LOGGING_RECOVERY_CATEGORY,
				REDO_MISSING_LOW_LOG_ERROR_ID, 
				sizeof( rgszT ) / sizeof( rgszT[ 0 ] ), 	
				rgszT, 
				0, 
				NULL, 
				m_pinst );

		err = ErrERRCheck( JET_errRequiredLogFilesMissing );
		}

	else if ( pdbfilehdr->bkinfoFullCur.le_genLow && !m_fHardRestore )
		{
		//	soft recovery on backup set database
		const CHAR	*rgszT[1];

		rgszT[0] = szDbName;
	
		//	attempting to use a database which did not successfully
		//	complete conversion
		UtilReportEvent(
				eventError,
				LOGGING_RECOVERY_CATEGORY,
				ATTACH_TO_BACKUP_SET_DATABASE_ERROR_ID,
				1,
				rgszT,
				0,
				NULL,
				m_pinst );
							
		//	soft recovery on backup set database
		err = ErrERRCheck( JET_errSoftRecoveryOnBackupDatabase );
		}


	return err;
	}

ERR LOG::ErrLGRIRedoCreateDb(
	PIB				*ppib,
	const IFMP		ifmp,
	const DBID		dbid,
	const JET_GRBIT	grbit,
	SIGNATURE		*psignDb )
	{
	ERR				err;
	FMP				*pfmp	= &rgfmp[ifmp];
	IFMP			ifmpT	= ifmp;
	const CHAR		*szDbName = pfmp->SzDatabaseName() ;
	const CHAR		*szSLVName = pfmp->SzSLVName() ;
	const CHAR		*szSLVRoot = pfmp->SzSLVRoot();

	Assert( dbid < dbidMax );
	Assert( dbidTemp != dbid );
	CallR( ErrDBCreateDatabase(
				ppib,
				NULL,
				szDbName,
				szSLVName,
				szSLVRoot,
				0,
				&ifmpT,
				dbid,
				cpgDatabaseMin,
				grbit,
				psignDb ) );
	Assert( ifmp == ifmpT );

	/*	close it as it will get reopened on first use
	/**/
	CallR( ErrDBCloseDatabase( ppib, ifmp, 0 ) );
	CallSx( err, JET_wrnDatabaseAttached );

	/*	restore information stored in database file
	/**/
	BKINFO * pbkInfoToCopy;
					
	if ( FSnapshotRestore() )
		{
		pbkInfoToCopy = &(pfmp->Pdbfilehdr()->bkinfoSnapshotCur);
		}
	else
		{
		pbkInfoToCopy = &(pfmp->Pdbfilehdr()->bkinfoFullCur);
		}

	pbkInfoToCopy->le_genLow = m_lGenLowRestore;
	pbkInfoToCopy->le_genHigh = m_lGenHighRestore;

	pfmp->SetDbtimeCurrentDuringRecovery( pfmp->DbtimeLast() );

	Assert( pfmp->Pdbfilehdr()->le_objidLast == pfmp->ObjidLast() );

	return err;
	}

ERR LOG::ErrLGRIRedoAttachDb(
	IFileSystemAPI *const	pfsapi,
	const IFMP					ifmp,
	const CPG					cpgDatabaseSizeMax,
	const REDOATTACHMODE		redoattachmode )
	{
	ERR				err			= JET_errSuccess;
	FMP				*pfmp		= &rgfmp[ifmp];
	const DBID		dbid		= pfmp->Dbid();
	IFileAPI	*pfapi;
	const CHAR					*szDbName = pfmp->SzDatabaseName();

	Assert( NULL != pfmp );
	Assert( NULL != szDbName );

#ifdef ELIMINATE_PATCH_FILE
#else
	if ( m_fHardRestore )
		{
		//	load patch file only for inconsistent backup databases
		if ( JET_dbstateInconsistent == pfmp->Pdbfilehdr()->Dbstate() )
			{
			CallR( ErrLGPatchDatabase( pfsapi, ifmp, dbid ) );
			}
		}
#endif

	const BOOL fSLVProviderNotEnabled = pfmp->FSLVProviderNotEnabled();
	
	/*	Do not re-create the database. Simply attach it. Assuming the
	 *	given database is a good one since signature matches.
	 */
	pfmp->ResetFlags();
	pfmp->SetAttached();

	pfmp->SetDatabaseSizeMax( cpgDatabaseSizeMax );
	Assert( pfmp->CpgDatabaseSizeMax() != 0xFFFFFFFF );

	// Versioning flag is not persisted (since versioning off
	// implies logging off).
	Assert( !pfmp->FVersioningOff() );
	pfmp->ResetVersioningOff();

	// If there's a log record for CreateDatabase(), then logging
	// must be on.
	Assert( !pfmp->FLogOn() );
	pfmp->SetLogOn();
	
	/*	Update database file header as necessary
	 */
	Assert( !pfmp->FReadOnlyAttach() );
	BOOL	fUpdateHeader	= fFalse;
	SLVFILEHDR *pslvfilehdr = NULL;
	ERR	errSLV = JET_errSuccess;

	if ( pfmp->Pdbfilehdr()->FSLVExists() )
		{
		errSLV = ErrSLVAllocAndReadHeader(	pfsapi, 
										rgfmp[ifmp].FReadOnlyAttach(),
										rgfmp[ifmp].SzSLVName(),
										rgfmp[ifmp].Pdbfilehdr(),
										&pslvfilehdr );
		if ( pslvfilehdr == NULL )
			{
			CallR( errSLV );
			}
		}
	if ( redoattachmodeAttachDbLR == redoattachmode )
		{
		BOOL	fKeepBackupInfo = m_fHardRestore;

		//	on attach after a hard restore, the attach must be created
		//	by previous recovery undo mode. Do not erase backup info then.
		if ( fKeepBackupInfo )
			{
			const INT irstmap = IrstmapSearchNewName( szDbName ) ;
			Assert ( 0 <= irstmap );
			fKeepBackupInfo = (0 == UtilCmpFileName(m_rgrstmap[irstmap].szDatabaseName, m_rgrstmap[irstmap].szNewDatabaseName ) );
			}
			
		//	UNDONE: in theory, lgposAttach should already have been set
		//	when the ATCHCHK was setup, but ivantrin says he's not 100%
		//	sure, so to be safe, we definitely set the lgposAttach here
		Assert( 0 == CmpLgpos( m_lgposRedo, pfmp->LgposAttach() ) );
		pfmp->SetLgposAttach( m_lgposRedo );
		DBISetHeaderAfterAttach( pfmp->Pdbfilehdr(), m_lgposRedo, ifmp, fKeepBackupInfo );
		fUpdateHeader = fTrue;
		}
	else if ( JET_dbstateInconsistent != pfmp->Pdbfilehdr()->Dbstate() )
		{
		//	must force inconsistent during recovery (may currently be marked as
		//	consistent because we replayed a RcvQuit and are now replaying an Init)
		FireWall();		//	should no longer be possible with forced detach on shutdown
		pfmp->Pdbfilehdr()->SetDbstate( JET_dbstateInconsistent, m_plgfilehdr->lgfilehdr.le_lGeneration, &m_plgfilehdr->lgfilehdr.tmCreate );
		fUpdateHeader = fTrue;
		}

	if ( pfmp->Pdbfilehdr()->FSLVExists() )
		{
		if ( errSLV < JET_errSuccess )
			{
			//	if we successfully detached STM file, but database.
			//	reset consistent position
			Assert( NULL != pslvfilehdr );
			pslvfilehdr->le_lgposConsistent = pfmp->Pdbfilehdr()->le_lgposConsistent;
			err = ErrSLVCheckDBSLVMatch( pfmp->Pdbfilehdr(), pslvfilehdr );
			fUpdateHeader = fTrue;
			}
		OSMemoryPageFree( (VOID*) pslvfilehdr );
		CallR( err );
		}

	//	SoftRecoveryOnBackupDatabase check should already have been performed in
	//	ErrLGRICheckAttachedDb()
	Assert( 0 == pfmp->Pdbfilehdr()->bkinfoFullCur.le_genLow
		|| m_fHardRestore );
		
	if ( fUpdateHeader )
		{
		Assert( pfmp->Pdbfilehdr()->le_objidLast > 0 );
		if ( pfmp->Pdbfilehdr()->FSLVExists() )
			{
			CallR( ErrSLVSyncHeader(	pfsapi, 
										rgfmp[ifmp].FReadOnlyAttach(),
										rgfmp[ifmp].SzSLVName(),
										rgfmp[ifmp].Pdbfilehdr() ) );
			}
		CallR( ErrUtilWriteShadowedHeader(	pfsapi, 
											szDbName,
											fTrue,
											(BYTE *)pfmp->Pdbfilehdr(), 
											g_cbPage ) );
		}

	Assert( JET_dbstateInconsistent == pfmp->Pdbfilehdr()->Dbstate() );

	/*	restore information stored in database file
	/**/
	BKINFO * pbkInfoToCopy;
					
	if ( FSnapshotRestore() )
		{
		pbkInfoToCopy = &(pfmp->Pdbfilehdr()->bkinfoSnapshotCur);
		}
	else
		{
		pbkInfoToCopy = &(pfmp->Pdbfilehdr()->bkinfoFullCur);
		}
		
	pbkInfoToCopy->le_genLow = m_lGenLowRestore;
	pbkInfoToCopy->le_genHigh = m_lGenHighRestore;

	CallR( pfsapi->ErrFileOpen( szDbName, &pfapi ) );
	pfmp->SetPfapi( pfapi );
	pfmp->SetDbtimeCurrentDuringRecovery( 0 );
	Assert( pfmp->ObjidLast() == pfmp->Pdbfilehdr()->le_objidLast
		|| ( pfmp->ObjidLast() > pfmp->Pdbfilehdr()->le_objidLast && redoattachmodeInitBeforeRedo == redoattachmode ) );

	Assert( !pfmp->FReadOnlyAttach() );

	// we allow header update before starting o log any operation
	pfmp->SetAllowHeaderUpdate();

	if ( pfmp->Pdbfilehdr()->FSLVExists() )
		{
		//	must call after setting ReadOnly flag because this functions reads the flag

		CallR( ErrFILEOpenSLV( pfsapi, ppibNil, ifmp ) );
		}

	/*	Keep extra copy of patchchk for error message.
	 */
	CallR( pfmp->ErrCopyAtchchk() );

	return err;
	}

VOID LOG::LGRISetDeferredAttachment( const IFMP ifmp )
	{
	FMP		*pfmp	= &rgfmp[ifmp];

	Assert( NULL != pfmp );
	
	Assert( !pfmp->Pfapi() );
	Assert( pfmp->FInUse() );
	Assert( NULL != pfmp->Pdbfilehdr() );
	pfmp->FreePdbfilehdr();

	// Versioning flag is not persisted (since versioning off
	// implies logging off).
	Assert( !pfmp->FVersioningOff() );
	pfmp->ResetVersioningOff();			//	

	Assert( !pfmp->FReadOnlyAttach() );
	//	still have to set fFlags for keep track of the db status.
///	Assert( !pfmp->FLogOn() );
	pfmp->SetLogOn( );


	pfmp->SetDeferredAttach();
	}

#ifdef UNLIMITED_DB
LOCAL ERR LOG::ErrLGRIRedoInitialAttachments_( IFileSystemAPI *const pfsapi )
	{
	ERR		err		= JET_errSuccess;

	/*	Make sure all the attached database are consistent!
	 */
	for ( DBID dbid = dbidUserLeast; dbid < dbidMax; dbid++ )
		{
		IFMP		ifmp		= m_pinst->m_mpdbidifmp[ dbid ];
		if ( ifmp >= ifmpMax )
			continue;

		FMP			*pfmp		= &rgfmp[ifmp];
		CHAR		*szDbName;
		REDOATTACH	redoattach;

		if ( !pfmp->FInUse() || !pfmp->Patchchk() )
			continue;

		szDbName = pfmp->SzDatabaseName();
		Assert ( szDbName );

		if ( m_fHardRestore || m_irstmapMac > 0 )
			{
			if ( 0 > IrstmapSearchNewName( szDbName ) )
				{
				/*	not in the restore map, set to skip it.
				 */
				Assert( pfmp->Pdbfilehdr() == NULL );
				pfmp->SetSkippedAttach();
				err = JET_errSuccess;
				continue;
				}
			}

		Assert( !pfmp->FReadOnlyAttach() );
		Call( ErrLGRICheckAttachedDb(
					pfsapi,
					ifmp,
					&m_signLog,
					&redoattach,
					redoattachmodeInitBeforeRedo ) );
		Assert( NULL != pfmp->Pdbfilehdr() );

		switch ( redoattach )
			{
			case redoattachNow:
				Assert( !pfmp->FReadOnlyAttach() );
				Call( ErrLGRIRedoAttachDb(
							pfsapi,
							ifmp,
							pfmp->Patchchk()->CpgDatabaseSizeMax(),
							redoattachmodeInitBeforeRedo ) );
				break;

			case redoattachCreate:
			default:
				Assert( fFalse );	//	should be impossible, but as a firewall, set to defer the attachment
			case redoattachDefer:
				Assert( !pfmp->FReadOnlyAttach() );
				LGRISetDeferredAttachment( ifmp );
				break;
			}

		/* keep attachment info and update it. */
		Assert( pfmp->Patchchk() != NULL );
		}

HandleError:
	return err;
	}
#endif	//	UNLIMITED_DB


ERR LOG::ErrLGRIRedoSpaceRootPage( 	PIB 				*ppib, 
									const LRCREATEMEFDP *plrcreatemefdp, 
									BOOL 				fAvail )
	{
	ERR			err;
	const DBID	dbid 		= plrcreatemefdp->dbid;
	const PGNO	pgnoFDP		= plrcreatemefdp->le_pgno;
	const OBJID	objidFDP	= plrcreatemefdp->le_objidFDP;
	const CPG	cpgPrimary	= plrcreatemefdp->le_cpgPrimary;
	const PGNO	pgnoRoot	= fAvail ? 
									plrcreatemefdp->le_pgnoAE : 
									plrcreatemefdp->le_pgnoOE ;
	const CPG	cpgExtent	= fAvail ?
								cpgPrimary - 1 - 1 - 1 :
								cpgPrimary;

	const PGNO	pgnoLast 	= pgnoFDP + cpgPrimary - 1;
	const ULONG	fPageFlags	= plrcreatemefdp->le_fPageFlags;
								
	FUCB		*pfucb		= pfucbNil;

	CSR		csr;
	INST	*pinst = PinstFromPpib( ppib );
	IFMP	ifmp = pinst->m_mpdbidifmp[ dbid ];

	//	could be a page beyond db-size for a hard-restore
	//
	err = ErrLGIAccessPage( ppib, &csr, ifmp, pgnoRoot );

	//	check if the FDP page need be redone
	//
	if ( err >= 0 && !FLGNeedRedoPage( csr, plrcreatemefdp->le_dbtime ) )
		{
		csr.ReleasePage();
		goto HandleError;
		}

	if ( err >= 0 )
		{
		csr.ReleasePage();
		}
	Assert( !csr.FLatched() );

	LGRITraceRedo( plrcreatemefdp );

	Call( ErrLGRIGetFucb( m_ptablehfhash, ppib, ifmp, pgnoFDP, objidFDP, plrcreatemefdp->FUnique(), fTrue, &pfucb ) );
	pfucb->u.pfcb->SetPgnoOE( plrcreatemefdp->le_pgnoOE );
	pfucb->u.pfcb->SetPgnoAE( plrcreatemefdp->le_pgnoAE );
	Assert( plrcreatemefdp->le_pgnoOE + 1 == plrcreatemefdp->le_pgnoAE );

	Assert( FFUCBOwnExt( pfucb ) );
	FUCBResetOwnExt( pfucb );

	Call( csr.ErrGetNewPageForRedo(
					ppib, 
					ifmp,
					fAvail ? PgnoAE( pfucb ) : PgnoOE( pfucb ), 
					objidFDP,
					plrcreatemefdp->le_dbtime,
					fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf | CPAGE::fPageSpaceTree ) );

	SPICreateExtentTree( pfucb, &csr, pgnoLast, cpgExtent, fAvail );
	
	//	set dbtime to logged dbtime
	//	release page 
	//
	Assert( latchWrite == csr.Latch() );
	csr.SetDbtime( plrcreatemefdp->le_dbtime );
	csr.ReleasePage();

HandleError:
	if ( pfucb != pfucbNil )
		{
		FUCBSetOwnExt( pfucb );
		}
		
	Assert( !csr.FLatched() );
	return err;
	}
	

//	redo single extent creation
//
LOCAL ERR ErrLGIRedoFDPPage( TABLEHFHASH *ptablehfhash, PIB *ppib, const LRCREATESEFDP *plrcreatesefdp )
	{
	ERR				err;
	const DBID		dbid 		= plrcreatesefdp->dbid;
	const PGNO		pgnoFDP		= plrcreatesefdp->le_pgno;
	const OBJID		objidFDP	= plrcreatesefdp->le_objidFDP;
	const DBTIME	dbtime		= plrcreatesefdp->le_dbtime;
	const PGNO		pgnoParent	= plrcreatesefdp->le_pgnoFDPParent;
	const CPG		cpgPrimary	= plrcreatesefdp->le_cpgPrimary;
	const BOOL		fUnique		= plrcreatesefdp->FUnique();
	const ULONG		fPageFlags	= plrcreatesefdp->le_fPageFlags;


	FUCB			*pfucb;
	CSR				csr;
	INST			*pinst = PinstFromPpib( ppib );
	IFMP			ifmp = pinst->m_mpdbidifmp[ dbid ];

	//	could be a page beyond db-size for a hard-restore
	//
	err = ErrLGIAccessPage( ppib, &csr, ifmp, pgnoFDP );

	//	check if the FDP page need be redone
	//
	if ( err >= 0 && !FLGNeedRedoPage( csr, dbtime ) )
		{
		csr.ReleasePage();
		goto HandleError;
		}

	if ( err >= 0 )
		{
		csr.ReleasePage();
		}
	Assert( !csr.FLatched() );

	Call( ErrLGRIGetFucb( ptablehfhash, ppib, ifmp, pgnoFDP, objidFDP, fUnique, fFalse, &pfucb ) );
	pfucb->u.pfcb->SetPgnoOE( pgnoNull );
	pfucb->u.pfcb->SetPgnoAE( pgnoNull );

	//	UNDONE: I can't tell if it's possible to get a cached FCB that used to belong to
	//	a different btree, so to be safe, reset the values for objidFDP and fUnique
	pfucb->u.pfcb->SetObjidFDP( objidFDP );
	if ( fUnique )
		pfucb->u.pfcb->SetUnique();
	else
		pfucb->u.pfcb->SetNonUnique();

	//	It is okay to call ErrGetNewPage() instead of ErrGetNewPageForRedo()
	//	here because if the new page succeeds, we are guaranteed to make it
	//	to the code below that updates the dbtime
	Call( ErrSPICreateSingle(
				pfucb,
				&csr,
				pgnoParent,
				pgnoFDP,
				objidFDP,
				cpgPrimary,
				fUnique,
				fPageFlags ) );

	//	set dbtime to logged dbtime
	//	release page 
	//
	Assert( latchWrite == csr.Latch() );
	csr.SetDbtime( dbtime );
	csr.ReleasePage();

HandleError:
	Assert( !csr.FLatched() );
	return err;
	}
	

//	redo multiple extent creation
//
LOCAL ERR ErrLGIRedoFDPPage( TABLEHFHASH *ptablehfhash, PIB *ppib, const LRCREATEMEFDP *plrcreatemefdp )
	{
	ERR				err;
	const DBID		dbid 		= plrcreatemefdp->dbid;
	const PGNO		pgnoFDP		= plrcreatemefdp->le_pgno;
	const OBJID		objidFDP	= plrcreatemefdp->le_objidFDP;
	const DBTIME	dbtime		= plrcreatemefdp->le_dbtime;
	const PGNO		pgnoOE		= plrcreatemefdp->le_pgnoOE;
	const PGNO		pgnoAE		= plrcreatemefdp->le_pgnoAE;
	const PGNO		pgnoParent	= plrcreatemefdp->le_pgnoFDPParent;
	const CPG		cpgPrimary	= plrcreatemefdp->le_cpgPrimary;
	const BOOL		fUnique		= plrcreatemefdp->FUnique();
	const ULONG		fPageFlags	= plrcreatemefdp->le_fPageFlags;

	FUCB			*pfucb;
	SPACE_HEADER	sph;

	CSR		csr;
	INST			*pinst = PinstFromPpib( ppib );
	IFMP			ifmp = pinst->m_mpdbidifmp[ dbid ];

	//	could be a page beyond db-size for a hard-restore
	//
	err = ErrLGIAccessPage( ppib, &csr, ifmp, pgnoFDP );

	//	check if the FDP page need be redone
	//
	if ( err >= 0 && !FLGNeedRedoPage( csr, dbtime ) )
		{
		csr.ReleasePage();
		goto HandleError;
		}

	if ( err >= 0 )
		{
		csr.ReleasePage();
		}
	Assert( !csr.FLatched() );

	Call( ErrLGRIGetFucb( ptablehfhash, ppib, ifmp, pgnoFDP, objidFDP, fUnique, fFalse, &pfucb ) );
	pfucb->u.pfcb->SetPgnoOE( pgnoOE );
	pfucb->u.pfcb->SetPgnoAE( pgnoAE );
	Assert( pgnoOE + 1 == pgnoAE );

	//	UNDONE: I can't tell if it's possible to get a cached FCB that used to belong to
	//	a different btree, so to be safe, reset the values for objidFDP and fUnique
	pfucb->u.pfcb->SetObjidFDP( objidFDP );
	if ( fUnique )
		pfucb->u.pfcb->SetUnique();
	else
		pfucb->u.pfcb->SetNonUnique();

	//	get pgnoFDP to initialize in current CSR pgno
	//
	Call( csr.ErrGetNewPageForRedo(
					pfucb->ppib,
					pfucb->ifmp,
					pgnoFDP,
					objidFDP,
					dbtime,
					fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf ) );

	sph.SetPgnoParent( pgnoParent );
	sph.SetCpgPrimary( cpgPrimary );
	
	Assert( sph.FSingleExtent() );	// initialised with these defaults
	Assert( sph.FUnique() );

	sph.SetMultipleExtent();
	
	if ( !fUnique )
		sph.SetNonUnique();
	
	sph.SetPgnoOE( pgnoOE );
	Assert( sph.PgnoOE() == pgnoFDP + 1 );

	SPIInitPgnoFDP( pfucb, &csr, sph );

	//	set dbtime to logged dbtime
	//	release page 
	//
	Assert( latchWrite == csr.Latch() );
	csr.SetDbtime( dbtime );
	csr.ReleasePage();

HandleError:
	Assert( !csr.FLatched() );
	return err;
	}


ERR	LOG::ErrLGRIRedoConvertFDP( PIB *ppib, const LRCONVERTFDP *plrconvertfdp )
	{
	ERR				err;
	const PGNO		pgnoFDP				= plrconvertfdp->le_pgno;
	const OBJID		objidFDP			= plrconvertfdp->le_objidFDP;
	const DBID		dbid				= plrconvertfdp->dbid;
	const DBTIME	dbtime				= plrconvertfdp->le_dbtime;
	const PGNO		pgnoAE				= plrconvertfdp->le_pgnoAE;
	const PGNO		pgnoOE				= plrconvertfdp->le_pgnoOE;
	const PGNO		pgnoSecondaryFirst	= plrconvertfdp->le_pgnoSecondaryFirst;
	const CPG		cpgSecondary		= plrconvertfdp->le_cpgSecondary;
	
	BOOL			fRedoAE;
	BOOL			fRedoOE;
	FUCB			*pfucb = pfucbNil;
	SPACE_HEADER	sph;
	EXTENTINFO		rgext[(cpgSmallSpaceAvailMost+1)/2 + 1];
	INT				iextMac = 0;

	CSR		csrRoot;
	CSR		csrAE;
	CSR		csrOE;
	INST			*pinst = PinstFromPpib( ppib );
	IFMP			ifmp = pinst->m_mpdbidifmp[ dbid ];

	LGRITraceRedo( plrconvertfdp );

	//	get cursor for operation
	//
	Assert( rgfmp[ifmp].Dbid() == dbid );
	Call( ErrLGRIGetFucb( m_ptablehfhash, ppib, ifmp, pgnoFDP, objidFDP, plrconvertfdp->FUnique(), fTrue, &pfucb ) );
	pfucb->u.pfcb->SetPgnoOE( pgnoOE );
	pfucb->u.pfcb->SetPgnoAE( pgnoAE );
	Assert( pgnoOE + 1 == pgnoAE );

	//	could be a page beyond db-size for a hard-restore
	//
	Call( ErrLGIAccessPage( ppib, &csrRoot, ifmp, pgnoFDP ) );

	//	check if the FDP page need be redone
	//
	if ( !FLGNeedRedoCheckDbtimeBefore( csrRoot, dbtime, plrconvertfdp->le_dbtimeBefore, &err ) )
		{
		//	read in AvailExt and OwnExt root pages (get from patch file if necessary)
		//	and verify they don't have to be redone either.

		Call( ErrLGRIAccessNewPage( ppib, &csrAE, ifmp, pgnoAE, dbtime, NULL, fFalse, &fRedoAE ) );
		Call( ErrLGRIAccessNewPage( ppib, &csrOE, ifmp, pgnoOE, dbtime, NULL, fFalse, &fRedoOE ) );

		Assert( !fRedoAE );
		Assert( !fRedoOE );

		err = JET_errSuccess;
		goto HandleError;
		}

	// for the FLGNeedRedoCheckDbtimeBefore error code
	Call ( err );

	ULONG fPageFlags;
	fPageFlags = csrRoot.Cpage().FFlags()
					& ~CPAGE::fPageRepair
					& ~CPAGE::fPageRoot
					& ~CPAGE::fPageLeaf
					& ~CPAGE::fPageParentOfLeaf;

	//	get AvailExt and OwnExt root pages from db or patch file
	Call( ErrLGRIAccessNewPage( ppib, &csrAE, ifmp, pgnoAE, dbtime, NULL, fTrue, &fRedoAE ) );
	Call( ErrLGRIAccessNewPage( ppib, &csrOE, ifmp, pgnoOE, dbtime, NULL, fTrue, &fRedoOE ) );

	SPIConvertGetExtentinfo( pfucb, &csrRoot, &sph, rgext, &iextMac );
	Assert( sph.FSingleExtent() );
	sph.SetMultipleExtent();
	sph.SetPgnoOE( pgnoSecondaryFirst );

	//	create new pages for OwnExt and AvailExt root
	//		if redo is needed
	//
	if ( fRedoOE )
		{
		Call( csrOE.ErrGetNewPageForRedo(
						ppib,
						ifmp,
						pgnoOE,
						objidFDP,
						dbtime,
						fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf | CPAGE::fPageSpaceTree ) );
		Assert( latchWrite == csrOE.Latch() );
		Call( ErrBFDepend( csrOE.Cpage().PBFLatch(), csrRoot.Cpage().PBFLatch() ) );
		}

	if ( fRedoAE )
		{
		Call( csrAE.ErrGetNewPageForRedo(
						ppib,
						ifmp,
						pgnoAE,
						objidFDP,
						dbtime,
						fPageFlags | CPAGE::fPageRoot | CPAGE::fPageLeaf | CPAGE::fPageSpaceTree ) );
		Assert( latchWrite == csrAE.Latch() );
		Call( ErrBFDepend( csrAE.Cpage().PBFLatch(), csrRoot.Cpage().PBFLatch() ) );
		}
	
	Assert( FAssertLGNeedRedo( csrRoot, dbtime, plrconvertfdp->le_dbtimeBefore ) );
	
	csrRoot.UpgradeFromRIWLatch();

	Assert( FFUCBOwnExt( pfucb ) );
	FUCBResetOwnExt( pfucb );

	//	dirty pages and set dbtime to logged dbtime
	//
	Assert( latchWrite == csrRoot.Latch() );
	LGRIRedoDirtyAndSetDbtime( &csrRoot, dbtime );
	LGRIRedoDirtyAndSetDbtime( &csrAE, dbtime );
	LGRIRedoDirtyAndSetDbtime( &csrOE, dbtime );

	SPIPerformConvert( pfucb, &csrRoot, &csrAE, &csrOE, &sph, pgnoSecondaryFirst, cpgSecondary, rgext, iextMac );

HandleError:
	csrRoot.ReleasePage();
	csrOE.ReleasePage();
	csrAE.ReleasePage();
	
	if ( pfucb != pfucbNil )
		{
		FUCBSetOwnExt( pfucb );
		}
		
	return err;
	}

	
ERR LOG::ErrLGRIRedoOperation( LR *plr )
	{
	ERR		err = JET_errSuccess;
	LEVEL   levelCommitTo;

	Assert( !m_fNeedInitialDbList );

	switch ( plr->lrtyp )
		{

	default:
		{
#ifndef RFS2
		AssertSz( fFalse, "Debug Only, Ignore this Assert" );
#endif
		return ErrERRCheck( JET_errLogCorrupted );
		}

	//	****************************************************
	//	single-page operations
	//	****************************************************

	case lrtypInsert:
	case lrtypFlagInsert:
	case lrtypFlagInsertAndReplaceData:
	case lrtypReplace:
	case lrtypReplaceD:
	case lrtypFlagDelete:
	case lrtypDelete:
	case lrtypDelta:
	case lrtypUndo:
	case lrtypUndoInfo:
	case lrtypSetExternalHeader:
	case lrtypSLVSpace:
	case lrtypEmptyTree:
		err = ErrLGRIRedoNodeOperation( (LRNODE_ * ) plr, &m_errGlobalRedoError );
		
		Assert( JET_errWriteConflict != err );
		CallS( err );
		CallR( err );
		break;


	/****************************************************
	 *     Transaction Operations                       *
	 ****************************************************/

	case lrtypBegin:
	case lrtypBegin0:
#ifdef DTC	
	case lrtypBeginDT:
#endif	
		{
		LRBEGINDT	*plrbeginDT = (LRBEGINDT *)plr;
		PIB			*ppib;

		LGRITraceRedo( plr );

		Assert( plrbeginDT->clevelsToBegin >= 0 );
		Assert( plrbeginDT->clevelsToBegin <= levelMax );
		CallR( ErrLGRIPpibFromProcid( plrbeginDT->le_procid, &ppib ) );

		Assert( !ppib->FMacroGoing() );

		/*	do BT only after first BT based on level 0 is executed
		/**/
		if ( ppib->FAfterFirstBT() || 0 == plrbeginDT->levelBeginFrom )
			{
			Assert( ppib->level <= plrbeginDT->levelBeginFrom );

			if ( 0 == ppib->level )
				{
				Assert( 0 == plrbeginDT->levelBeginFrom );
				Assert( lrtypBegin != plr->lrtyp );
				ppib->trxBegin0 = plrbeginDT->le_trxBegin0;
				ppib->lgposStart = m_lgposRedo;
				m_pinst->m_trxNewest = ( TrxCmp( m_pinst->m_trxNewest, ppib->trxBegin0 ) > 0 ? m_pinst->m_trxNewest : ppib->trxBegin0 );
				//  at redo time RCEClean can throw away any committed RCE as they are only
				//  needed for rollback
				}
			else
				{
				Assert( lrtypBegin == plr->lrtyp );
				}

			/*	issue begin transactions
			/**/
			while ( ppib->level < plrbeginDT->levelBeginFrom + plrbeginDT->clevelsToBegin )
				{
				VERBeginTransaction( ppib );
				}

			/*	assert at correct transaction level
			/**/
			Assert( ppib->level == plrbeginDT->levelBeginFrom + plrbeginDT->clevelsToBegin );

			ppib->SetFAfterFirstBT();

#ifdef DTC
			if ( lrtypBeginDT == plr->lrtyp )
				ppib->SetFDistributedTrx();
#endif				
			}
		break;
		}

	case lrtypRefresh:
		{
		LRREFRESH	*plrrefresh = (LRREFRESH *)plr;
		PIB			*ppib;

		LGRITraceRedo( plr );

		CallR( ErrLGRIPpibFromProcid( plrrefresh->le_procid, &ppib ) );

		Assert( !ppib->FMacroGoing() );
		if ( !ppib->FAfterFirstBT() )
			break;

		/*	imitate a begin transaction.
		 */
		Assert( ppib->level <= 1 );
		ppib->level = 1;
		ppib->trxBegin0 = plrrefresh->le_trxBegin0;
			
		break;
		}

	case lrtypCommit:
	case lrtypCommit0:
		{
		LRCOMMIT0	*plrcommit0 = (LRCOMMIT0 *)plr;
		PIB			*ppib;

		CallR( ErrLGRIPpibFromProcid( plrcommit0->le_procid, &ppib ) );

		if ( !ppib->FAfterFirstBT() )
			break;

		/*	check transaction level
		/**/
		Assert( !ppib->FMacroGoing() );
		Assert( ppib->level >= 1 );

		LGRITraceRedo( plr );

		levelCommitTo = plrcommit0->levelCommitTo;
		Assert( levelCommitTo <= ppib->level );

		while ( ppib->level > levelCommitTo )
			{
			Assert( ppib->level > 0 );
			if ( 1 == ppib->level )
				{
				Assert( lrtypCommit0 == plr->lrtyp );
				ppib->trxCommit0 = plrcommit0->le_trxCommit0;
				m_pinst->m_trxNewest = ( TrxCmp( m_pinst->m_trxNewest, ppib->trxCommit0 ) > 0 ?
											m_pinst->m_trxNewest :
											ppib->trxCommit0 );
				VERCommitTransaction( ppib );
				LGICleanupTransactionToLevel0( ppib );
				}
			else
				{
				Assert( lrtypCommit == plr->lrtyp );
				VERCommitTransaction( ppib );
				}
			}

		break;
		}

	case lrtypRollback:
		{
		LRROLLBACK	*plrrollback = (LRROLLBACK *)plr;
		LEVEL   	level = plrrollback->levelRollback;
		PIB			*ppib;

		CallR( ErrLGRIPpibFromProcid( plrrollback->le_procid, &ppib ) );

		Assert( !ppib->FMacroGoing() );
		if ( !ppib->FAfterFirstBT() )
			break;

		/*	check transaction level
		/**/
		Assert( ppib->level >= level );

		LGRITraceRedo( plr );

		while ( level-- && ppib->level > 0 )
			{
			err = ErrVERRollback( ppib );
			CallSx( err, JET_errRollbackError );
			CallR ( err );
			}

		if ( 0 == ppib->level )
			{
			LGICleanupTransactionToLevel0( ppib );
			}
			
		break;
		}

#ifdef DTC
	case lrtypPrepCommit:
		{
		const LRPREPCOMMIT	* const plrprepcommit	= (LRPREPCOMMIT *)plr;
		PIB					* ppib;

		CallR( ErrLGRIPpibFromProcid( plrprepcommit->le_procid, &ppib ) );

		if ( !ppib->FAfterFirstBT() )
			break;

		Assert( !ppib->FMacroGoing() );
		Assert( 1 == ppib->level );
		Assert( ppib->FDistributedTrx() );

		LGRITraceRedo( plr );

		CallR( ppib->ErrAllocDistributedTrxData( plrprepcommit->rgbData, plrprepcommit->le_cbData ) );
		ppib->SetFPreparedToCommitTrx();
		break;
		}

	case lrtypPrepRollback:
		{
		const LRPREPROLLBACK	* const plrpreprollback	= (LRPREPROLLBACK *)plr;
		PIB						* ppib;

		CallR( ErrLGRIPpibFromProcid( plrpreprollback->le_procid, &ppib ) );

		if ( !ppib->FAfterFirstBT() )
			break;

		Assert( !ppib->FMacroGoing() );
		Assert( 1 == ppib->level );
		Assert( ppib->FDistributedTrx() );
		Assert( ppib->FPreparedToCommitTrx() );

		LGRITraceRedo( plr );

		//	resetting the PreparedToCommit flag will force rollback at the
		//	end of recovery
		ppib->ResetFPreparedToCommitTrx();
		break;
		}
#endif	//	DTC

		           
	/****************************************************
	 *     Split Operations                             *
	 ****************************************************/

	case lrtypSplit:
	case lrtypMerge:
		{
		LRPAGE_ *		plrpage		= (LRPAGE_ *)plr;
		PIB *			ppib;
		const DBTIME	dbtime		= plrpage->le_dbtime;

		CallR( ErrLGRIPpibFromProcid( plrpage->le_procid, &ppib ) );	

		if ( !ppib->FAfterFirstBT() )
			{
			//	BUGFIX (X5:178265 and NT:214397): it's possible
			//	that there are no Begin0's between the first and
			//	last log records, in which case nothing needs to
			//	get redone.  HOWEVER, the dbtimeLast and objidLast
			//	counters in the db header would not have gotten
			//	flushed (they only get flushed on a DetachDb or a
			//	clean shutdown), so we must still track these
			//	counters during recovery so that we can properly
			//	update the database header on RecoveryQuit (since
			//	we pass TRUE for the fUpdateCountersOnly param to
			//	ErrLGRICheckRedoCondition() below, that function
			//	will do nothing but update the counters for us).

			BOOL		fSkip;
			const OBJID	objidFDP	= ( lrtypSplit == plr->lrtyp ?
											( (LRSPLIT *)plr )->le_objidFDP :
											( (LRMERGE *)plr )->le_objidFDP );

			CallS( ErrLGRICheckRedoCondition(
						plrpage->dbid,
						dbtime,
						objidFDP,
						ppib,
						fTrue,
						&fSkip ) );
			Assert( fSkip );
			break;
			}

		Assert( ppib->FMacroGoing( dbtime ) );

		CallR( ErrLGIStoreLogRec( ppib, dbtime, plrpage ) );
		break;
		}


	//***************************************************
	//	Misc Operations
	//***************************************************

	case lrtypCreateMultipleExtentFDP:
		{
		LRCREATEMEFDP	*plrcreatemefdp = (LRCREATEMEFDP *)plr;
		const DBID		dbid = plrcreatemefdp->dbid;
		PIB				*ppib;

		INST			*pinst = m_pinst;
		IFMP			ifmp = pinst->m_mpdbidifmp[ dbid ];
		
		FMP::AssertVALIDIFMP( ifmp );

		//	remove cursors created on pgnoFDP earlier
		//
		if ( NULL != m_ptablehfhash )
			{
			m_ptablehfhash->Purge( ifmp, plrcreatemefdp->le_pgno );
			}
			
		BOOL fSkip;
		CallR( ErrLGRICheckRedoCondition2(
				plrcreatemefdp->le_procid,
				dbid,
				plrcreatemefdp->le_dbtime,
				plrcreatemefdp->le_objidFDP,
				NULL,	//	can not be in macro.
				&ppib,
				&fSkip ) );
		if ( fSkip )
			break;

		LGRITraceRedo( plrcreatemefdp );
		CallR( ErrLGIRedoFDPPage( m_ptablehfhash, ppib, plrcreatemefdp ) );

		CallR( ErrLGRIRedoSpaceRootPage( ppib, plrcreatemefdp, fTrue ) );
		
		CallR( ErrLGRIRedoSpaceRootPage( ppib, plrcreatemefdp, fFalse ) );
		break;
		}

	case lrtypCreateSingleExtentFDP:
		{
		LRCREATESEFDP	*plrcreatesefdp = (LRCREATESEFDP *)plr;
		const PGNO		pgnoFDP = plrcreatesefdp->le_pgno;
		const DBID		dbid = plrcreatesefdp->dbid;
		PIB				*ppib;

		INST			*pinst = m_pinst;
		IFMP			ifmp = pinst->m_mpdbidifmp[ dbid ];
		FMP::AssertVALIDIFMP( ifmp );
		
		//	remove cursors created on pgnoFDP earlier
		//
		if ( NULL != m_ptablehfhash )
			{
			m_ptablehfhash->Purge( ifmp, pgnoFDP );
			}
			
		BOOL fSkip;
		CallR( ErrLGRICheckRedoCondition2(
					plrcreatesefdp->le_procid,
					dbid,
					plrcreatesefdp->le_dbtime,
					plrcreatesefdp->le_objidFDP,
					NULL,
					&ppib,
					&fSkip ) );
		if ( fSkip )
			break;

		//	redo FDP page if needed
		//
		LGRITraceRedo( plrcreatesefdp );
		CallR( ErrLGIRedoFDPPage( m_ptablehfhash, ppib, plrcreatesefdp ) );
		break;
		}

	case lrtypConvertFDP:
		{
		LRCONVERTFDP	*plrconvertfdp = (LRCONVERTFDP *)plr;
		PIB				*ppib;

		BOOL fSkip;
		CallR( ErrLGRICheckRedoCondition2(
				plrconvertfdp->le_procid,
				plrconvertfdp->dbid,
				plrconvertfdp->le_dbtime,
				plrconvertfdp->le_objidFDP,
				plr,
				&ppib,
				&fSkip ) );
		if ( fSkip )
			break;

		CallR( ErrLGRIRedoConvertFDP( ppib, plrconvertfdp ) );
		break;
		}

	case lrtypSLVPageAppend:
		{
#ifdef DISABLE_SLV
		return ErrERRCheck( JET_wrnNyi );
#else
		LRSLVPAGEAPPEND	*	plrSLVPageAppend	= (LRSLVPAGEAPPEND *)plr;
		PIB				*	ppib;
		BOOL 				fSkip;

		if( !plrSLVPageAppend->FDataLogged() )
			{
			//	the information wasn't logged, we can't redo anything
			
			// on hard recovery we have to error out if we don't have the data
			// Even during hard recovery we may don't have the data before the backup start 
			// as if cicrular logging is on we log the data only during backup
			// If we hit a log after the backup (play forward) the data is not in the backup SLV
			// and we have to error out: "don't play forward if circular logging !"
			Assert ( !m_fHardRestore || m_lGenHighRestore );
			if ( m_fHardRestore && m_plgfilehdr->lgfilehdr.le_lGeneration > m_lGenHighRestore )
				{
				CallR( ErrERRCheck ( JET_errStreamingDataNotLogged ) );
				}
			
			Assert ( !m_fHardRestore || m_plgfilehdr->lgfilehdr.le_lGeneration <= m_lGenHighRestore );
			break;
			}
			
		//	for SLV operations, we don't care about transactional visibility
		//	(can't roll them back) so skip transaction check in
		//	ErrLGRICheckRedoCondition2() and ALWAYS redo SLV operations even
		//	if we're in the middle of a transaction.
		CallR( ErrLGRIPpibFromProcid(
					plrSLVPageAppend->le_procid,
					&ppib ) );
		CallR( ErrLGRICheckRedoCondition(
					DBID( plrSLVPageAppend->dbid & dbidMask ),
					dbtimeInvalid,
					objidNil,
					ppib,
					fFalse,
					&fSkip ) );
		if ( fSkip )
			break;

		CallR( ErrLGRIRedoSLVPageAppend( ppib, plrSLVPageAppend ) );
		break;
#endif	//	DISABLE_SLV
		}

	} /*** end of switch statement ***/

	return JET_errSuccess;
	}


//	reconstructs rglineinfo during recovery
//		calcualtes kdf and cbPrefix of lineinfo
//		cbSize info is not calculated correctly since it is not needed in redo
//
LOCAL ERR ErrLGIRedoSplitLineinfo( FUCB					*pfucb,
								   SPLITPATH 			*psplitPath, 
								   DBTIME				dbtime,
								   const KEYDATAFLAGS&	kdf )
	{
	SPLIT	*psplit = psplitPath->psplit;
	ERR err = JET_errSuccess;

	Assert( psplit != NULL );
	Assert( psplit->psplitPath == psplitPath );
	Assert( NULL == psplit->rglineinfo );
	Assert( FAssertLGNeedRedo( psplitPath->csr, dbtime, psplitPath->dbtimeBefore )
		|| !FBTISplitDependencyRequired( psplit ) );

	psplit->rglineinfo = new LINEINFO[psplit->clines];
							 
	if ( NULL == psplit->rglineinfo )
		{
		return ErrERRCheck( JET_errOutOfMemory );
		}

	memset( psplit->rglineinfo, 0, sizeof( LINEINFO ) * psplit->clines );

	if ( !FLGNeedRedoCheckDbtimeBefore( psplitPath->csr, dbtime, psplitPath->dbtimeBefore, &err ) )
		{
		CallS( err );

		//	split page does not need redo but new page needs redo
		//	set rglineinfo and cbPrefix for appended node
		//
		Assert( !FBTISplitDependencyRequired( psplit ) );
		Assert( psplit->clines - 1 == psplit->ilineOper );
		Assert( psplit->ilineSplit == psplit->ilineOper );
		Assert( FLGNeedRedoPage( psplit->csrNew, dbtime ) );
		
		psplit->rglineinfo[psplit->ilineOper].kdf = kdf;

		if ( ilineInvalid != psplit->prefixinfoNew.ilinePrefix )
			{
			Assert( 0 == psplit->prefixinfoNew.ilinePrefix );
			Assert( kdf.key.Cb() > cbPrefixOverhead );
			psplit->rglineinfo[psplit->ilineOper].cbPrefix = kdf.key.Cb();
			}

		return JET_errSuccess;
		}
	else
		{
		Call( err );
		}
	
	INT		ilineFrom;
	INT		ilineTo;

	for ( ilineFrom = 0, ilineTo = 0; ilineTo < psplit->clines; ilineTo++ )
		{
		if ( psplit->ilineOper == ilineTo && 
			 splitoperInsert == psplit->splitoper )
			{
			//	place to be inserted node here
			//
			psplit->rglineinfo[ilineTo].kdf = kdf;
			
			//	do not increment ilineFrom
			//
			continue;
			}

		//	get node from page
		//	
		psplitPath->csr.SetILine( ilineFrom );

		NDGet( pfucb, &psplitPath->csr );

		if ( ilineTo == psplit->ilineOper &&
			 splitoperNone != psplit->splitoper )
			{
			//	get key from node
			//	and data from parameter
			//
			Assert( splitoperInsert != psplit->splitoper );
			Assert( splitoperReplace == psplit->splitoper ||
					splitoperFlagInsertAndReplaceData == psplit->splitoper );

			psplit->rglineinfo[ilineTo].kdf.key		= pfucb->kdfCurr.key;
			psplit->rglineinfo[ilineTo].kdf.data	= kdf.data;
			psplit->rglineinfo[ilineTo].kdf.fFlags	= pfucb->kdfCurr.fFlags;
			}
		else
			{
			psplit->rglineinfo[ilineTo].kdf			= pfucb->kdfCurr;
			}

		Assert( ilineFrom <= ilineTo &&
				ilineFrom + 1 >= ilineTo ); 
		ilineFrom++;
		}

	//	set cbPrefixes for nodes in split page
	//
	if ( psplit->prefixinfoSplit.ilinePrefix != ilineInvalid )
		{
		Assert( psplit->prefixSplitNew.Cb() > 0 );
		
		KEY		keyPrefix;
		keyPrefix.Nullify();
		keyPrefix.suffix = psplit->prefixSplitNew;
		Assert( FKeysEqual( keyPrefix, 
						psplit->rglineinfo[psplit->prefixinfoSplit.ilinePrefix].kdf.key ) );

		for ( INT iline = 0; iline < psplit->ilineSplit ; iline++ )
			{
			LINEINFO	*plineinfo = &psplit->rglineinfo[iline];
			const INT	cbCommon = CbCommonKey( keyPrefix, plineinfo->kdf.key );

			Assert( 0 == plineinfo->cbPrefix );
			if ( cbCommon > cbPrefixOverhead )
				{
				plineinfo->cbPrefix = cbCommon;
				}
			}
		}

	//	set cbPrefixes for nodes in new page
	//
	if ( FLGNeedRedoPage( psplit->csrNew, dbtime )
		&& ilineInvalid != psplit->prefixinfoNew.ilinePrefix )
		{
		const INT	ilinePrefix = psplit->ilineSplit + 
								  psplit->prefixinfoNew.ilinePrefix;
		Assert( psplit->clines > ilinePrefix );
		
		KEY		keyPrefix = psplit->rglineinfo[ilinePrefix].kdf.key;

		for ( INT iline = psplit->ilineSplit; iline < psplit->clines ; iline++ )
			{
			LINEINFO	*plineinfo = &psplit->rglineinfo[iline];
			const INT	cbCommon = CbCommonKey( keyPrefix, plineinfo->kdf.key );

			Assert( 0 == plineinfo->cbPrefix );
			if ( cbCommon > cbPrefixOverhead )
				{
				plineinfo->cbPrefix = cbCommon;
				}
			}
		}
	return JET_errSuccess;

HandleError:		
	delete [] psplit->rglineinfo;
	
	return err;
	}
	

//	reconstructs split structre during recovery
//		access new page and upgrade to write-latch, if necessary
//		access right page and upgrade to write-latch, if necessary
//		update split members from log record	
//
ERR LOG::ErrLGRIRedoInitializeSplit( PIB *ppib, const LRSPLIT *plrsplit, SPLITPATH *psplitPath )
	{
	ERR				err;
	BOOL			fRedoNewPage	= fFalse;

	const DBID		dbid			= plrsplit->dbid;
	const PGNO		pgnoSplit		= plrsplit->le_pgno;
	const PGNO		pgnoNew			= plrsplit->le_pgnoNew;
	const PGNO		pgnoRight		= plrsplit->le_pgnoRight;
	const OBJID		objidFDP		= plrsplit->le_objidFDP;
	const DBTIME	dbtime			= plrsplit->le_dbtime;
	const SPLITTYPE	splittype		= SPLITTYPE( BYTE( plrsplit->splittype ) );

	const ULONG		fNewPageFlags	= plrsplit->le_fNewPageFlags;
	const ULONG		fSplitPageFlags	= plrsplit->le_fSplitPageFlags;

	INST			*pinst			= m_pinst;
	IFMP			ifmp			= pinst->m_mpdbidifmp[ dbid ];
	
	Assert( pgnoNew != pgnoNull );
	Assert( latchRIW == psplitPath->csr.Latch() );

	//	allocate split structure
	//
	SPLIT	*psplit = static_cast<SPLIT *>( PvOSMemoryHeapAlloc( sizeof(SPLIT) ) );
	if ( psplit == NULL )
		{
		CallR( ErrERRCheck( JET_errOutOfMemory ) );
		}
	memset( (BYTE *)psplit, 0, sizeof(SPLIT) );
	new( &psplit->csrRight ) CSR;
	new( &psplit->csrNew ) CSR;

	psplit->pgnoSplit 	= pgnoSplit;

	psplit->splittype	= splittype;
	psplit->splitoper	= SPLITOPER( BYTE( plrsplit->splitoper ) );
	
	psplit->ilineOper	= plrsplit->le_ilineOper;
	psplit->clines		= plrsplit->le_clines;
	Assert( psplit->clines < g_cbPage );

	psplit->ilineSplit	= plrsplit->le_ilineSplit;

	psplit->fNewPageFlags	= fNewPageFlags;
	psplit->fSplitPageFlags	= fSplitPageFlags;

	psplit->cbUncFreeSrc	= plrsplit->le_cbUncFreeSrc;
	psplit->cbUncFreeDest	= plrsplit->le_cbUncFreeDest;
			
	psplit->prefixinfoSplit.ilinePrefix	= plrsplit->le_ilinePrefixSplit;
	psplit->prefixinfoNew.ilinePrefix	= plrsplit->le_ilinePrefixNew;

	//	latch the new-page 

	psplit->pgnoNew		= plrsplit->le_pgnoNew;
	Assert( rgfmp[ifmp].Dbid() == dbid );
	Call( ErrLGRIAccessNewPage(
				ppib,
				&psplit->csrNew,
				ifmp,
				pgnoNew,
				dbtime,
				psplit,
				FLGNeedRedoPage( psplitPath->csr, dbtime ),
				&fRedoNewPage ) );

	if ( fRedoNewPage )
		{
		//	create new page
		//
		Assert( !psplit->csrNew.FLatched() );
		Call( psplit->csrNew.ErrGetNewPageForRedo(
									ppib,
									ifmp,
									pgnoNew,
									objidFDP,
									dbtime,
									fNewPageFlags ) );

		if ( FBTISplitDependencyRequired( psplit ) )
			{
			Call( ErrBFDepend(	psplit->csrNew.Cpage().PBFLatch(),
								psplitPath->csr.Cpage().PBFLatch() ) );
			}
		}
	else
		{
		Assert( latchRIW == psplit->csrNew.Latch() );
 		Assert( !FLGNeedRedoPage( psplit->csrNew, dbtime ) );
		}

	if ( pgnoRight != pgnoNull )
		{
		Call( ErrLGIAccessPage( ppib, &psplit->csrRight, ifmp, pgnoRight ) );
		Assert( latchRIW == psplit->csrRight.Latch() );
		Assert( dbtimeNil != plrsplit->le_dbtimeRightBefore );
		Assert( dbtimeInvalid != plrsplit->le_dbtimeRightBefore );
		psplit->dbtimeRightBefore = plrsplit->le_dbtimeRightBefore;
		}

	if ( plrsplit->le_cbKeyParent > 0 )
		{
		const INT	cbKeyParent = plrsplit->le_cbKeyParent;

		psplit->kdfParent.key.suffix.SetPv( PvOSMemoryHeapAlloc( cbKeyParent ) );
		if ( psplit->kdfParent.key.suffix.Pv() == NULL )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}

		psplit->fAllocParent		= fTrue;
		psplit->kdfParent.key.suffix.SetCb( cbKeyParent );
		UtilMemCpy( psplit->kdfParent.key.suffix.Pv(),
				((BYTE *) plrsplit ) + sizeof( LRSPLIT ),
				cbKeyParent );

		psplit->kdfParent.data.SetCb( sizeof( PGNO ) );
		psplit->kdfParent.data.SetPv( &psplit->pgnoSplit );
		}

	if ( plrsplit->le_cbPrefixSplitOld > 0 )
		{
		const INT	cbPrefix = plrsplit->le_cbPrefixSplitOld;
		psplit->prefixSplitOld.SetPv( PvOSMemoryHeapAlloc( cbPrefix ) );
		if ( psplit->prefixSplitOld.Pv() == NULL )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		psplit->prefixSplitOld.SetCb( cbPrefix );

		UtilMemCpy( psplit->prefixSplitOld.Pv(), 
				((BYTE*) plrsplit) + sizeof( LRSPLIT ) + plrsplit->le_cbKeyParent,
				cbPrefix );
		}
		
	if ( plrsplit->le_cbPrefixSplitNew > 0 )
		{
		const INT	cbPrefix = plrsplit->le_cbPrefixSplitNew;
		psplit->prefixSplitNew.SetPv( PvOSMemoryHeapAlloc( cbPrefix ) );
		if ( psplit->prefixSplitNew.Pv() == NULL )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}
		psplit->prefixSplitNew.SetCb( cbPrefix );

		UtilMemCpy( psplit->prefixSplitNew.Pv(), 
				((BYTE*) plrsplit) + 
					sizeof( LRSPLIT ) + 
					plrsplit->le_cbKeyParent + 
					plrsplit->le_cbPrefixSplitOld,
				cbPrefix );
		}

	Assert( psplit->csrNew.Pgno() != pgnoNull );
	Assert( plrsplit->le_pgnoNew == psplit->csrNew.Pgno() );
	Assert( plrsplit->le_pgnoRight == psplit->csrRight.Pgno() );

	//	link psplit ot psplitPath
	//
	psplitPath->psplit = psplit;
	psplit->psplitPath = psplitPath;
	
	//	if (non-append) split page is flushed
	//		new page should be as well
	Assert( FAssertLGNeedRedo( psplitPath->csr, dbtime, plrsplit->le_dbtimeBefore )
		|| !fRedoNewPage
		|| !FBTISplitDependencyRequired( psplit ) );
	if ( fRedoNewPage )
		{
		Assert( latchWrite == psplit->csrNew.Latch() );
		}
	else
		{
		Assert( latchRIW == psplit->csrNew.Latch() );
 		}

HandleError:
	if ( err < 0 )
		{
		BTIReleaseSplit( pinst, psplit );
		}
	return err;
	}


//	reconstructs splitPath and split
//
ERR LOG::ErrLGRIRedoSplitPath( PIB *ppib, const LRSPLIT *plrsplit, SPLITPATH **ppsplitPath )
	{
	Assert( lrtypSplit == plrsplit->lrtyp );
	ERR				err;
	const DBID		dbid		= plrsplit->dbid;
	const PGNO		pgnoSplit	= plrsplit->le_pgno; 
	const PGNO		pgnoNew		= plrsplit->le_pgnoNew;
	const DBTIME	dbtime		= plrsplit->le_dbtime;

	INST			*pinst = m_pinst;
	IFMP			ifmp = pinst->m_mpdbidifmp[ dbid ];

	//	allocate new splitPath
	//
	CallR( ErrBTINewSplitPath( ppsplitPath ) );
	
	SPLITPATH *psplitPath = *ppsplitPath;

	CallR( ErrLGIAccessPage( ppib, &psplitPath->csr, ifmp, pgnoSplit ) );
	Assert( latchRIW == psplitPath->csr.Latch() );

	//	allocate new split if needed
	//
	if ( pgnoNew != pgnoNull )
		{
		Call( ErrLGRIRedoInitializeSplit( ppib, plrsplit, psplitPath ) );
		Assert( NULL != psplitPath->psplit );
		}

	psplitPath->dbtimeBefore = plrsplit->le_dbtimeBefore;
	if ( psplitPath->psplitPathParent != NULL )
		{
		Assert( psplitPath == psplitPath->psplitPathParent->psplitPathChild );
		psplitPath->psplitPathParent->dbtimeBefore = plrsplit->le_dbtimeParentBefore;
		}
		
HandleError:
	return err;
	}


//	allocate and initialize mergePath structure
//	access merged page 
//	if redo is needed,
//		upgrade latch
//
LOCAL ERR ErrLGIRedoMergePath( PIB 				*ppib, 
							   const LRMERGE	*plrmerge,
							   MERGEPATH		**ppmergePath )
	{
	ERR				err;
	const DBID		dbid	= plrmerge->dbid;
	const PGNO		pgno	= plrmerge->le_pgno;
	const DBTIME	dbtime	= plrmerge->le_dbtime;
	INST			*pinst	= PinstFromPpib( ppib );
	IFMP			ifmp	= pinst->m_mpdbidifmp[ dbid ];

	//	initialize merge path
	//
	CallR( ErrBTINewMergePath( ppmergePath ) );

	MERGEPATH *pmergePath = *ppmergePath;

	CallR( ErrLGIAccessPage( ppib, &pmergePath->csr, ifmp, pgno ) );
	Assert( latchRIW == pmergePath->csr.Latch() );

	pmergePath->iLine		= plrmerge->ILine();	
	pmergePath->fKeyChange 	= ( plrmerge->FKeyChange() ? fTrue : fFalse );
	pmergePath->fDeleteNode	= ( plrmerge->FDeleteNode() ? fTrue : fFalse );
	pmergePath->fEmptyPage	= ( plrmerge->FEmptyPage() ? fTrue : fFalse );

	pmergePath->dbtimeBefore = plrmerge->le_dbtimeBefore;
	if ( pmergePath->pmergePathParent != NULL )
		{
		Assert( pmergePath == pmergePath->pmergePathParent->pmergePathChild );
		pmergePath->pmergePathParent->dbtimeBefore = plrmerge->le_dbtimeParentBefore;
		}

	return err;
	}


//	allocates and initializes leaf-level merge structure
//	access sibling pages 
//	if redo is needed, 
//		upgrade to write-latch
//
ERR LOG::ErrLGRIRedoInitializeMerge( PIB 			*ppib, 
									 FUCB			*pfucb,
									 const LRMERGE 	*plrmerge, 
									 MERGEPATH		*pmergePath )
	{
	ERR				err;

	Assert( NULL == pmergePath->pmergePathChild );
	CallR( ErrBTINewMerge( pmergePath ) );

	MERGE			*pmerge				= pmergePath->pmerge;
	const PGNO		pgnoRight			= plrmerge->le_pgnoRight;
	const PGNO		pgnoLeft			= plrmerge->le_pgnoLeft;
	const DBID		dbid				= plrmerge->dbid;
	const DBTIME	dbtime				= plrmerge->le_dbtime;
	const MERGETYPE	mergetype			= MERGETYPE( BYTE( plrmerge->mergetype ) );

	INST			*pinst				= PinstFromPpib( ppib );
	IFMP			ifmp				= pinst->m_mpdbidifmp[ dbid ];

	BOOL			fRedoRightPage		= fFalse;
	const BOOL		fRedoMergedPage		= FLGNeedRedoCheckDbtimeBefore( pmergePath->csr, dbtime, plrmerge->le_dbtimeBefore, &err );
	Call( err );
	

	//	access left page
	//
	if ( pgnoLeft != pgnoNull )
		{
		Call( ErrLGIAccessPage( ppib, &pmerge->csrLeft, ifmp, pgnoLeft ) );
		Assert( latchRIW == pmerge->csrLeft.Latch() );
		Assert( dbtimeNil != plrmerge->le_dbtimeLeftBefore );
		Assert( dbtimeInvalid != plrmerge->le_dbtimeLeftBefore );
		pmerge->dbtimeLeftBefore = plrmerge->le_dbtimeLeftBefore;
		}

	if ( pgnoRight != pgnoNull )
		{
		//	access right page
		//	if hard-restore
		//		get patch for page
		//		if patch page exists
		//			Assert patch page's dbtime >= dbtime of oper
		//			replace page with patch
		//			acquire RIW latch
		//		else if page does not exist
		//			return
		//		else
		//			if page's dbtime < dbtime of oper
		//				upgrade latch
		//	else
		//		if page does not exist
		//			return
		//		if page's dbtime < dbtime of oper
		//			upgrade latch
		//

		//	unlike split, the right page should already exist, so we
		//	shouldn't get errors back from AccessPage()
		Call( ErrLGIAccessPage( ppib, &pmerge->csrRight, ifmp, pgnoRight ) );
		Assert( latchRIW == pmerge->csrRight.Latch() );
		Assert( dbtimeNil != plrmerge->le_dbtimeRightBefore );
		Assert( dbtimeInvalid != plrmerge->le_dbtimeRightBefore );
		pmerge->dbtimeRightBefore = plrmerge->le_dbtimeRightBefore;

		Assert( latchRIW == pmerge->csrRight.Latch() );
 		fRedoRightPage = FLGNeedRedoCheckDbtimeBefore( pmerge->csrRight, dbtime, pmerge->dbtimeRightBefore, &err );
		Call( err );

		//	If restoring, then need to consult patch file only if right
		//	page is not new enough, but merged page is (in which case
		//	up-to-date right page MUST be in patch file).
		//	In addition, there's no need to obtain page from patch file
		//	for an EmptyPage merge, even if it's out-of-date, because
		//	all that will be updated is the page pointer.
		if ( m_fHardRestore
			&& fRedoRightPage
			&& !fRedoMergedPage
			&& mergetypeEmptyPage != mergetype )
			{
			//	merged page is up-to-date, but right page isn't, so the
			//	correct right page must be available in the patch file
			PATCH	*ppatch;
			
			Assert( fRestorePatch == m_fRestoreMode || fRestoreRedo == m_fRestoreMode );
			Assert( rgfmp[ifmp].Dbid() == dbid );

			// it should not happend during snapshot restore
			AssertSz ( fSnapshotNone == m_fSnapshotMode, "No patch file for snapshot restore" ); 
			ppatch = PpatchLGSearch( m_rgppatchlst, dbtime, pgnoRight, dbid );

			if ( ppatch != NULL )
				{
#ifdef ELIMINATE_PAGE_PATCHING
				//	should be impossible
				EnforceSz( fFalse, "Patching no longer supported." );
				return ErrERRCheck( JET_errBadPatchPage );
#else
				//	patch exists and is later than operation
				//
				Assert( ppatch->dbtime >= dbtime );

				//	release currently-latched page so we can load the
				//	page from the patch file
				Assert( latchRIW == pmerge->csrRight.Latch() );
				pmerge->csrRight.ReleasePage();
				
				CallR( ErrLGIPatchPage( ppib, pgnoRight, ifmp, ppatch ) );
				
				CallS( ErrLGIAccessPage( ppib, &pmerge->csrRight, ifmp, pgnoRight ) );
				Assert( latchRIW == pmerge->csrRight.Latch() );

				fRedoRightPage = FLGNeedRedoCheckDbtimeBefore( pmerge->csrRight, dbtime, pmerge->dbtimeRightBefore, &err );
				Assert( !fRedoRightPage );
				CallS( err );
#endif	//	ELIMINATE_PAGE_PATCHING
				}
			}

		if ( fRedoRightPage )
			{
			Assert( FAssertLGNeedRedo( pmerge->csrRight, dbtime, pmerge->dbtimeRightBefore ) );
			
			if ( mergetypeEmptyPage != mergetype )
				{
				//	if right page needs to be redone, then so should merged page
				//	(because merged page is dependent on right page)
				if ( !fRedoMergedPage )
					{
					Assert( fFalse );	//	should be impossible
					Call( ErrERRCheck( JET_errDatabaseBufferDependenciesCorrupted ) );
					}
				
				Assert( pmerge->csrRight.Latch() == latchRIW );

				Assert( mergetypeFullRight == mergetype ||
						mergetypePartialRight == mergetype );

				//  depend the right page on the merge page so that the data
				//  moved from the merge page to the right page will always
				//  be available no matter when we crash

				Call( ErrBFDepend(	pmerge->csrRight.Cpage().PBFLatch(),
									pmergePath->csr.Cpage().PBFLatch() ) );
				}
			}
		}

	if ( plrmerge->le_cbKeyParentSep > 0 )
		{
		const INT	cbKeyParentSep = plrmerge->le_cbKeyParentSep;

		pmerge->kdfParentSep.key.suffix.SetPv( PvOSMemoryHeapAlloc( cbKeyParentSep ) );
		if ( pmerge->kdfParentSep.key.suffix.Pv() == NULL )
			{
			Call( ErrERRCheck( JET_errOutOfMemory ) );
			}

		pmerge->fAllocParentSep		= fTrue;
		pmerge->kdfParentSep.key.suffix.SetCb( cbKeyParentSep );
		UtilMemCpy( pmerge->kdfParentSep.key.suffix.Pv(),
				((BYTE *) plrmerge ) + sizeof( LRMERGE ),
				cbKeyParentSep );
		}

	Assert( 0 == plrmerge->ILineMerge() || mergetypePartialRight == mergetype );
	pmerge->ilineMerge		= plrmerge->ILineMerge();	
	pmerge->mergetype		= mergetype;
			
	pmerge->cbSizeTotal		= plrmerge->le_cbSizeTotal;
	pmerge->cbSizeMaxTotal	= plrmerge->le_cbSizeMaxTotal;
	pmerge->cbUncFreeDest	= plrmerge->le_cbUncFreeDest;

	//	if merged page needs redo
	//		allocate and initialize rglineinfo
	//
	if ( fRedoMergedPage )
		{
		CSR		*pcsr = &pmergePath->csr;
		CSR		*pcsrRight = &pmerge->csrRight;
		Assert( latchRIW == pcsr->Latch() );
		
		const INT	clines	= pmergePath->csr.Cpage().Clines();
		pmerge->clines		= clines;

		Assert( pmerge->rglineinfo == NULL );
		pmerge->rglineinfo 	= new LINEINFO[clines];
		if ( NULL == pmerge->rglineinfo )
			{
			return ErrERRCheck( JET_errOutOfMemory );
			}

		KEY		keyPrefix;
		keyPrefix.Nullify();
		if ( fRedoRightPage )
			{
			Assert( FAssertLGNeedRedo( *pcsrRight, dbtime, pmerge->dbtimeRightBefore ) );
			
			NDGetPrefix( pfucb, pcsrRight );
			keyPrefix = pfucb->kdfCurr.key;
			Assert( pfucb->kdfCurr.data.FNull() );
			}
			
		memset( pmerge->rglineinfo, 0, sizeof( LINEINFO ) * clines );

		for ( INT iline = 0; iline < clines; iline++ )
			{
			LINEINFO	*plineinfo = pmerge->rglineinfo + iline;
			pcsr->SetILine( iline );
			NDGet( pfucb, pcsr );
			plineinfo->kdf = pfucb->kdfCurr;

			if ( fRedoRightPage )
				{
				Assert( FAssertLGNeedRedo( *pcsrRight, dbtime, pmerge->dbtimeRightBefore ) );

				//	calculate cbPrefix for node 
				//	with respect to prefix on right page
				//
				INT		cbCommon = CbCommonKey( pfucb->kdfCurr.key,	keyPrefix );

				Assert( 0 == plineinfo->cbPrefix );
				if ( cbCommon > cbPrefixOverhead )
					{
					plineinfo->cbPrefix = cbCommon;
					}
				}
			}
		}

HandleError:
	return err;
	}

	
//	recons