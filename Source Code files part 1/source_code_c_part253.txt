ken();
    BYTE* startAddress;
    if (IsMethodPitched(token))
    {
        startAddress =  RejitMethod((JittedMethodInfo*) token,0);
       
    }
    else 
    {
        startAddress = JitToken2StartAddress(token);
    }
    return ::CallJitEHFilter(pCf,startAddress,EHClausePtr,nestingLevel,thrownObj);

}

void   EconoJitManager::CallJitEHFinally(CrawlFrame* pCf, EE_ILEXCEPTION_CLAUSE *EHClausePtr, DWORD nestingLevel)
{
#ifdef _X86_
    METHODTOKEN token = pCf->GetMethodToken();
    BYTE* startAddress;
    if (IsMethodPitched(token))
    {
        PREGDISPLAY pRD = pCf->GetRegisterSet();
        m_pThunkCritSec->Enter();
        PitchedCodeThunk* thunk = GetNewThunk();
        thunk->Busy = true;
        thunk->LinkedInFreeList = false;
        thunk->retTypeProtect = MethodDesc::RETNONOBJ;
        thunk->CallInstruction[0] = CALL_OPCODE;
        void** callSite = (void**) &(thunk->CallInstruction[1]);
        *callSite = (void*) ((size_t)RejitThunk - (size_t)(callSite+1));
        thunk->relReturnAddress = (unsigned) EHClausePtr->HandlerStartPC;
        thunk->u.pMethodInfo = (JittedMethodInfo*) token;

        MachState ms((void**) pRD->pEdi, (void**) pRD->pEsi, (void**) pRD->pEbx, (void**) pRD->pEbp, 
                     (void*)(size_t)pRD->Esp, (void**) &thunk);
        m_pThunkCritSec->Leave();
        
        HelperMethodFrame HelperFrame(&ms, 0);
        startAddress =  RejitMethod((JittedMethodInfo*) token,0);
        HelperFrame.Pop();
    }
    else 
    {
        startAddress = JitToken2StartAddress(token);
    }
    ::CallJitEHFinally(pCf,startAddress,EHClausePtr,nestingLevel);
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - EconoJitManager::CallJitEHFinally (EjitMgr.cpp)");
#endif // _X86_
}




HRESULT   EconoJitManager::alloc(
                                 size_t code_len, 
                                 unsigned char** ppCode,        /* IN/OUT */  // this is sort of a hack, the ejit is using this parameter to pass 
                                                                              // the code buffer so it can copy the jitted code into the allocated block
                                                                              // this is needed in order to prevent a race
                                 size_t EHinfo_len, 
                                 unsigned char** ppEHinfo,      /* OUT */
                                 size_t GCinfo_len, 
                                 unsigned char** ppGCinfo  ,    /* OUT */  
                                 MethodDesc* pMethodDescriptor
                                 )
{
    unsigned codeHeaderSize = sizeof(CodeHeader*);

    JittedMethodInfo* existingJMI = NULL;  


    _ASSERTE(EHinfo_len + GCinfo_len);      // this may not be true if we optimize GCinfo 
    codeHeaderSize += sizeof(void*);

    // make sure we always begin at proper machine word boundary
    unsigned adjusted_code_len = (unsigned)(((code_len+sizeof(void*)-1)/sizeof(void*)) * sizeof(void*));

    m_pHeapCritSec->Enter();

    if ( OutOfCodeMemory(adjusted_code_len+codeHeaderSize) 
#ifdef SUPPORT_CODE_PITCH_TRIGGER
            || (int) m_cMethodsJitted == g_pConfig->GetCodePitchTrigger()
#endif
       )
    {
        size_t totalMemNeeded = adjusted_code_len + codeHeaderSize + usedMemoryInCodeHeap(); // TODO LBS - truncation

        // if we haven't hit the initial code heap size target OR we have incurred a high pitch overhead,
        // then we will try to either increase our committed space and if necessary also increase our reserved space
        // The rationale is that until we reach our target code heap size, we do not want to consider code pitching at all.
        if (m_CodeHeapReservedSize <= InitialCodeHeapSize() || 
            PitchOverhead() >= (unsigned) g_pConfig->GetMaxPitchOverhead())
        {
            while (totalMemNeeded <= m_CodeHeapReservedSize && totalMemNeeded > m_CodeHeapCommittedSize)
            {
                m_pHeapCritSec->Leave();
                if (!SetCodeHeapCommittedSize((unsigned int) min(totalMemNeeded+PAGE_SIZE,m_CodeHeapReservedSize))  )
                {
                    return (E_FAIL);
                }
                m_pHeapCritSec->Enter();
                totalMemNeeded = (unsigned)(adjusted_code_len + codeHeaderSize + usedMemoryInCodeHeap()); // @TODO LBS - truncation
            }
            // at this point we either have enough committed memory or we have hit the reserved size
            if ((totalMemNeeded > m_CodeHeapReservedSize) && 
                (m_CodeHeapReservedSize < (unsigned) g_pConfig->GetMaxCodeCacheSize()) &&  // we haven't hid the hard upper limit
#ifdef SUPPORT_CODE_PITCH_TRIGGER
                (m_cMethodsJitted != (unsigned) g_pConfig->GetCodePitchTrigger()) &&  // force pitch if trigger value reached, 
#endif
                (PitchOverhead() >= (unsigned) g_pConfig->GetMaxPitchOverhead()))
            {   //so try growing the heap instead of pitching
                unsigned delta = (unsigned)((totalMemNeeded - m_CodeHeapReservedSize) + m_CodeHeapReserveIncrement -1);
                delta = (delta/m_CodeHeapReserveIncrement)*m_CodeHeapReserveIncrement;
                GrowCodeHeapReservedSpace((unsigned)(m_CodeHeapReservedSize + delta),(unsigned)(adjusted_code_len+codeHeaderSize));
                if (m_CodeHeapReserveIncrement < CODE_HEAP_RESERVED_INCREMENT_LIMIT)
                    m_CodeHeapReserveIncrement *= 2;
            }      
        }
        // At this point we have attempted to meet our memory needs without pitching code, if we did not succeed, there is 
        // no option but to pitch code
        while (OutOfCodeMemory(adjusted_code_len+codeHeaderSize) 
#ifdef SUPPORT_CODE_PITCH_TRIGGER
            || ((int)m_cMethodsJitted == g_pConfig->GetCodePitchTrigger())
#endif
            )
        {
            m_pHeapCritSec->Leave();
            if (!PitchAllJittedMethods((unsigned)(adjusted_code_len+codeHeaderSize),(unsigned)(adjusted_code_len+codeHeaderSize),true,true))       // During pitching we also adjust the cache code size if too 
                return E_FAIL;                                        // few or too many methods have been jitted since the last pitch   
            m_pHeapCritSec->Enter();
        }
    }
    m_cMethodsJitted++;

#ifdef _DEBUG
    m_AllocLock_Holder = GetCurrentThreadId();  
#endif
    if (m_PitchOccurred)
    {   // this could be a rejit of a method, need to find out if we already have an entry in the jmi table
         existingJMI = MethodDesc2MethodInfo(pMethodDescriptor);
        _ASSERTE(!existingJMI || existingJMI->flags.JittedMethodPitched);
    }

    unsigned char* pCodeBlock = (unsigned char*) allocCodeBlock(adjusted_code_len+codeHeaderSize);
    if (!pCodeBlock) {
        m_pHeapCritSec->Leave();
        return (E_FAIL);
    }


    // make sure we always begin at proper machine word boundary
    size_t adjusted_EhGc_len = ((EHinfo_len + GCinfo_len+1)/2) * 2;
    unsigned char* pEhGcBlock;
    if (!existingJMI || existingJMI->flags.EHandGCInfoPitched)
    {
        pEhGcBlock = (unsigned char*) allocEHGCBlock(adjusted_EhGc_len);
        if (!pEhGcBlock) {
            freeCodeBlock(adjusted_code_len+codeHeaderSize);
            m_pHeapCritSec->Leave();
            return (E_FAIL);
        }
    }
    else // no need to allocate EHGCBlock
    {
        _ASSERTE(existingJMI->flags.JittedMethodPitched); // since it doesnt make sense to pitch EhGC and retain code
        pEhGcBlock = (BYTE*) ((size_t)existingJMI->u2.pEhGcInfo & ~1);
    }

    JIT_PERF_UPDATE_X86_CODE_SIZE((unsigned)(adjusted_code_len + codeHeaderSize + EHinfo_len + GCinfo_len));
    
    //if (EHinfo_len + GCinfo_len) is zero, the following can be optimized away
    * (void**)pCodeBlock = pEhGcBlock;
    pCodeBlock += sizeof(void*);

    *((MethodDesc**)pCodeBlock) = pMethodDescriptor;
    pCodeBlock += sizeof(void*);

    // it is important to do this copy here before we change the call to a jmp in the thunk
    // otherwise we'd have a race, where a thread might pick up the new address before this 
    // thread completes the copy.
    memcpy(pCodeBlock, *ppCode, code_len);

    *ppCode = pCodeBlock;
    *ppEHinfo = pEhGcBlock;


    if (existingJMI)
    {
        // we have to be careful while updating the following since other
        // threads might be reading this concurrently. It is guaranteed that
        // no one is writing into it since we hold the alloc lock and the
        // thread store lock is not held by anyone. 

        CodeHeader* codeHdr = (CodeHeader*) (pCodeBlock - sizeof(void*));
        _ASSERTE(((size_t)codeHdr & 3) == 0);
        existingJMI->u1.pCodeHeader = codeHdr; // verify that this is atomic
        BYTE* codeEnd = adjusted_code_len + (BYTE*) (pCodeBlock);
        _ASSERTE(((size_t)codeEnd & 3) == 0);
        existingJMI->u2.pCodeEnd = codeEnd;     // verify that this is atomic
        AddPC2MDMap(pMethodDescriptor,codeEnd);
        existingJMI->flags.JittedMethodPitched = false;     // this should not be read unless code pitching
        existingJMI->flags.EHandGCInfoPitched = false;      // this is either already false, in which case the operation is safe
                                                            // or it was true => method not in any callstack => no one can be reading it
        BYTE* JmpStub = &(existingJMI->JmpInstruction[0]);
#ifdef _X86_
        DWORD oldhi32 = *(DWORD*)(JmpStub+4);
        DWORD oldlow32 = *(DWORD*)(JmpStub);
        __int64 m64 = *(__int64*) JmpStub;
        __int64 newInstruction = m64;
        *((BYTE*)(&newInstruction)) = JMP_OPCODE;
        *((unsigned*)((BYTE*)(&newInstruction)+1)) = (unsigned)((size_t)pCodeBlock - (size_t)(JmpStub+1) - sizeof(void*));
        DWORD newhi32 = *(((DWORD*)(&newInstruction)) + 1);
        DWORD newlow32 = *(((DWORD*)(&newInstruction)));
        __asm {
            mov  eax, oldlow32
            mov  edx, oldhi32  
            mov  ebx, newlow32
            mov  ecx, newhi32
            mov  edi, JmpStub
            lock cmpxchg8b qword ptr[edi]
        }
#else
    _ASSERTE(!"@TODO Alpha - EconoJitManager::alloc (EjitMgr.cpp) Manufacture a jmp instruction");
#endif
#ifdef _DEBUG
    m_AllocLock_Holder = 0;  
#endif
    m_pHeapCritSec->Leave();
        return (HRESULT)(size_t)JmpStub; // @TODO WIN64 - Pointer Truncation
    }

    // if got here, then this is a newly jitted method
    _ASSERTE(m_JMIT_size);
    JittedMethodInfo* newJmiEntry = GetNextJmiEntry();
    if (newJmiEntry == NULL)
        return E_FAIL;

    _ASSERTE(newJmiEntry->flags.JittedMethodPitched == 0); 
    if (EHinfo_len)
    {
        newJmiEntry->flags.EHInfoExists = 1;
    }
    if (GCinfo_len)
    {
        newJmiEntry->flags.GCInfoExists = 1;
    }
    _ASSERTE(newJmiEntry->flags.EHandGCInfoPitched == 0); 

    newJmiEntry->SetEhGcInfo_len((UINT)adjusted_EhGc_len,&m_LargeEhGcInfo);

    BYTE* JmpStub = &(newJmiEntry->JmpInstruction[0]);
#ifdef _X86_
    *JmpStub = JMP_OPCODE;
    *((unsigned*) (JmpStub+1)) =
          (unsigned) ((size_t)pCodeBlock - (size_t)(JmpStub+1) - sizeof(void*))    ;
#else 
    _ASSERTE(!"@TODO Alpha - EconoJitManager::alloc (Ejitmgr.cpp) Manufacture a jmp instruction");
#endif

    newJmiEntry->u1.pCodeHeader = (CodeHeader*) (pCodeBlock - sizeof(void*));
    newJmiEntry->u2.pCodeEnd = adjusted_code_len + (BYTE*) (pCodeBlock);
    _ASSERTE(((size_t)newJmiEntry->u2.pCodeEnd & 1) == 0);
    AddPC2MDMap(pMethodDescriptor,newJmiEntry->u2.pCodeEnd);

#ifdef _DEBUG
    m_AllocLock_Holder = 0;  
#endif
    m_pHeapCritSec->Leave();
    return (HRESULT)(size_t)JmpStub; // @TODO WIN64 - Pointer Truncation
}

EconoJitManager::JittedMethodInfo* EconoJitManager::GetNextJmiEntry()
{
#ifdef _DEBUG
    m_pHeapCritSec->OwnedByCurrentThread();
#endif
    JittedMethodInfo* newEntry;

    if (m_JMIT_freelist)
    {
        newEntry = (JittedMethodInfo*) m_JMIT_freelist;
        m_JMIT_freelist = m_JMIT_freelist->next;
    }
    else //free list is empty
    {
        if ((size_t)(m_JMIT_free+1) >= ((size_t)m_JittedMethodInfoHdr)+JMIT_BLOCK_SIZE)
        {
            if (!growJittedMethodInfoTable())
                return NULL;
        }
        newEntry = m_JMIT_free;
        m_JMIT_free++;
    }
    return newEntry;
}

// It is assumed that code pitching happens synchronously at GC safe
// points. Therefore, it is not necessary to protect the pitching mechanims
// for concurrent access to the code cache. If this assumption changes, the
// code has to be appropriately protected.
BOOL EconoJitManager::PitchAllJittedMethods(unsigned minSpaceRequired,unsigned minCommittedSpaceRequired, BOOL PitchEHInfo, BOOL PitchGCInfo)
{
    if (!g_pConfig->IsCodePitchEnabled())
    {
        return FALSE;
    }
    TICKS startPitchTime = GET_TIMESTAMP();       // this is needed to record the pitching overhead
    // For now piggyback on the GC's suspend EE mechanism
    GCHeap::SuspendEE(GCHeap::SUSPEND_FOR_CODE_PITCHING);

    // ASSERT: All threads are now suspended at GC safe points
    // NOTE: In general, it is not safe to pitch the code if the debugger
    // has the thread suspended while it is executing the code. But such 
    // a point is not GC safe as far as the Ejit is concerned. In other 
    // words whenever we are pitching, the method is guaranteed to be on the
    // call stack and not at a leaf. 
    
    MarkThunksForRelease();               // tentatively mark all thunks as free

    MarkHeapsForPitching();               // tentatively, mark every method to be pitched 
    StackWalkForCodePitching();           // at the end of this all references to the code heap have
                                          // been replaced by references to thunks, also some candidates methods for preserving have
                                          // been identified
    UnmarkPreservedCandidates(minSpaceRequired);   // Guarantees that the space recovered is greater than minSpaceRequired
                                                   // OR that all methods have been marked for pitching
    MoveAllPreservedEhGcInfo();
    unsigned cMethodsMarked = PitchMarkedCode(); 
    MovePreservedMethods();

    //_ASSERTE((unsigned) m_cMethodsJitted == cMethodsMarked);         
    m_cMethodsJitted = 0;
    m_cCalleeRejits = 0;
    m_cCallerRejits = 0;

#if defined(ENABLE_PERF_COUNTERS)
    int iCodeSize = GetCodeHeapSize() + GetEHGCHeapSize();
#endif // ENABLE_PERF_COUNTERS

    m_PitchOccurred = true;
    HRESULT ret = TRUE;
    if (minSpaceRequired  > (unsigned) m_CodeHeapReservedSize)
    {
        if (minSpaceRequired > (unsigned)g_pConfig->GetMaxCodeCacheSize())
            ret = FALSE;
        else
        {
            ReplaceCodeHeap(minSpaceRequired,minCommittedSpaceRequired);
            ret = m_CodeHeap ? TRUE : FALSE;
        }
    }
    else if (minCommittedSpaceRequired > m_CodeHeapCommittedSize)
    {
        if (!SetCodeHeapCommittedSize(minCommittedSpaceRequired))          
                ret = FALSE;
    }


#if defined(ENABLE_PERF_COUNTERS)
    JIT_PERF_UPDATE_X86_CODE_SIZE(GetCodeHeapSize() + GetEHGCHeapSize() - iCodeSize); // Would be a -ive number for successful pitch

    // Perf counters temporarily don't support pitched bytes since Ejit is not in the product.
//    COUNTER_ONLY(GetPrivatePerfCounters().m_Jit.cbPitched+= -(GetCodeHeapSize() + GetEHGCHeapSize() - iCodeSize));
//    COUNTER_ONLY(GetGlobalPerfCounters().m_Jit.cbPitched+= -(GetCodeHeapSize() + GetEHGCHeapSize() - iCodeSize));
#endif

    GarbageCollectUnusedThunks();
    
    if (!PitchEHInfo || !PitchGCInfo) 
    {
        _ASSERTE(!"NYI");
        /*
        for (unsigned i = 0; i < m_JMIT_len; i++) {
            if (!PitchEHInfo) PreserveEHInfo(i);
            if (!PitchGCInfo) PreserveGCInfo(i);
        }
        */
    }

#ifdef _DEBUG
    SetBreakpointsInUnusedHeap(); // this enables us to immediately catch an attempt to execute code that has been pitched
#endif 
    TICKS endPitchTime = GET_TIMESTAMP();
    AddPitchOverhead(endPitchTime-startPitchTime);    // no need to protect this for multi-threads since
                                                        // we still haven't restarted EE.
    GCHeap::RestartEE(FALSE, TRUE);
    return ret;
}


//*********************************************************************
//                          Private functions                         *
//*********************************************************************

_inline EconoJitManager::JittedMethodInfo* EconoJitManager::Token2JittedMethodInfo(METHODTOKEN token)
{
#ifdef _DEBUG
    // check that this is a valid token
    JittedMethodInfo* pJMITstart = (JittedMethodInfo*) (m_JittedMethodInfoHdr+1);
    if (pJMITstart <= (JittedMethodInfo*) token && (JittedMethodInfo*)token < m_JMIT_free)
    {
        _ASSERTE( ((((size_t)token - (size_t)pJMITstart)/sizeof(JittedMethodInfo)) * sizeof(JittedMethodInfo))
                  + (size_t) pJMITstart == (size_t) token );
    }
    else
    {
        JittedMethodInfoHdr* pJMIT = (m_JittedMethodInfoHdr->next);
        while (pJMIT)
        {
            pJMITstart = (JittedMethodInfo*) (pJMIT+1);
            if ((size_t)pJMITstart  <=  (size_t)token  && 
                (size_t)token       <   ((size_t) pJMIT) + JMIT_BLOCK_SIZE)
            {
                _ASSERTE( ((((size_t) token - (size_t)pJMITstart)/sizeof(JittedMethodInfo)) * sizeof(JittedMethodInfo))
                  + (size_t) pJMITstart == (size_t) token );
                break;
            }
            pJMIT= pJMIT->next;
        }
    }
#endif
    return (JittedMethodInfo*) token;
    
}


inline MethodDesc* EconoJitManager::JitMethodInfo2MethodDesc(JittedMethodInfo* jmi)
{
    BYTE* u1 = (BYTE*) (jmi->u1.pMethodDescriptor);
    if ((size_t)u1 & 1) // method has been pitched
        return (MethodDesc*) ((size_t)u1 & ~1); // it is possible that someone by now has rejitted the method,
                                        // but we've already obtained the methoddesc!
    else // method has not been pitched
    {
        //_ASSERTE(jmi->flags.JittedMethodPitched == 0);  // unfortunately can't have this because there is a small
                                                          // window between the time jmi->u1 gets written and jit->flags gets
                                                          // updated in which this assert is not true.
        return ((CodeHeader*) u1)->pMethodDescriptor;
    }
}

inline BYTE* EconoJitManager::JitMethodInfo2EhGcInfo(JittedMethodInfo* jmi)
{
    BYTE* u2 = (BYTE*) (jmi->u2.pEhGcInfo);
    if ((size_t)u2 & 1) // method has been pitched
        return (BYTE*) ((size_t)u2 & ~1); // it is possible that someone by now has rejitted the method,
                                        // but we've already obtained the EhGc info!
    else // method has not been pitched
    {
        _ASSERTE(jmi->flags.JittedMethodPitched == 0);
        return *(BYTE**) (jmi->u1.pCodeHeader - 1);
    }
}


// This method is called while holding the alloc lock to determine if there already exists
// entry for a method. Is such an entry exists, then it must contain the methoddesc because
// the code has been pitched away. Also, we are guaranteed that no one is updating the JMI table.
EconoJitManager::JittedMethodInfo*   EconoJitManager::MethodDesc2MethodInfo(MethodDesc* pMethodDesc)
{
    pMethodDesc = (MethodDesc*) ((size_t)pMethodDesc | 1);   // if the entry exists it will have to be pitched, so the last bit will be set
    JittedMethodInfoHdr* pJMIT = m_JittedMethodInfoHdr;
    size_t limit = (size_t)m_JMIT_free;
    while (pJMIT) {
        JittedMethodInfo* pJMI = (JittedMethodInfo*) (pJMIT+1);
        while ((size_t)(pJMI+1) <= limit)
        {
            if ( (pJMI->u1.pMethodDescriptor) == pMethodDesc)
            {
                _ASSERTE(pJMI->flags.JittedMethodPitched);
                return pJMI;
            }
            pJMI++;
        }
        pJMIT = pJMIT->next;
        limit = ((size_t)pJMIT) + JMIT_BLOCK_SIZE;        // all other JMIT blocks are completely full
    }
    return NULL;
}

BOOL EconoJitManager::growJittedMethodInfoTable()
{
    //DEBUG_LOG("ALLOC(growJMIT)",PAGE_SIZE);
    JittedMethodInfoHdr* newJMITHdr = (JittedMethodInfoHdr*) VirtualAlloc(NULL,PAGE_SIZE,MEM_RESERVE|MEM_COMMIT,PAGE_READWRITE);
    if (!newJMITHdr)
        return FALSE;
    WS_PERF_SET_HEAP(ECONO_JIT_HEAP);
    WS_PERF_UPDATE("growJittedMethodInfoTable", PAGE_SIZE, newJMITHdr);

    // register this address range since the jump stubs are here
    if (!ExecutionManager::AddRange((LPVOID)newJMITHdr, (LPVOID)((size_t)newJMITHdr + PAGE_SIZE),this, NULL))
    {
        VirtualFree(newJMITHdr,PAGE_SIZE,MEM_DECOMMIT);
        VirtualFree(newJMITHdr,0,MEM_RELEASE);
        return FALSE;
    }

    newJMITHdr->next = m_JittedMethodInfoHdr;
    m_JittedMethodInfoHdr = (JittedMethodInfoHdr*) newJMITHdr;
    m_JMIT_free = (JittedMethodInfo*) (newJMITHdr+1);
    return true;
}

BOOL EconoJitManager::growPC2MDMap()
{
    unsigned newSize = m_PcToMdMap_size + INITIAL_PC2MD_MAP_SIZE;
    PCToMDMap* temp = new PCToMDMap[newSize];
    if (!temp)
    {
      return false;
    }
    _ASSERTE(m_PcToMdMap_len);
    memcpy((BYTE*)temp,(BYTE*)m_PcToMdMap,m_PcToMdMap_size);
    
    // cannot delete m_PcToMdMap since a thread might be using it, so just
    // collect it in a recycle list which will be cleared during the next pitch
    ((PC2MDBlock*)m_PcToMdMap)->next = m_RecycledPC2MDMaps;
    m_RecycledPC2MDMaps = (PC2MDBlock*)m_PcToMdMap;

    m_PcToMdMap = temp;
    m_PcToMdMap_size = newSize;
    return true;
}

BOOL  EconoJitManager::AddPC2MDMap(MethodDesc* pMD, BYTE* pCodeEnd)
{
    if ((m_PcToMdMap_len+sizeof(PCToMDMap) > m_PcToMdMap_size) && !growPC2MDMap())
        return false;
    _ASSERTE(m_PcToMdMap_len+sizeof(PCToMDMap) <= m_PcToMdMap_size);
    PCToMDMap* newMap = (PCToMDMap*) ((size_t)m_PcToMdMap+m_PcToMdMap_len);
    newMap->pMD = pMD;
    newMap->pCodeEnd = pCodeEnd;
    m_PcToMdMap_len += sizeof(PCToMDMap);
    return true;
}

#ifdef _DEBUG
void EconoJitManager::LogAction(MethodDesc* pMD, LPCUTF8 action, void* codeStart, void* codeEnd)
{
    LPCUTF8 cls  = pMD->GetClass() ? pMD->GetClass()->m_szDebugClassName
                                   : "GlobalFunction";
    LPCUTF8 name = pMD->GetName();

    LOG((LF_JIT,LL_INFO1000,"%s", action));
    LOG((LF_JIT, LL_INFO1000,
         " method %s.%s [%x,%x]\n", cls, name,codeStart,codeEnd));
}
#endif


#define MAX_ENREGISTERED 2      /* max number of arguments passed in registers by ejit */

BYTE* __cdecl RejitCalleeMethod(struct MachState ms, void* arg2, void* arg1, BYTE* thunkReturnAddress)
{
#ifdef _X86_
    BYTE* retAddress;
    EconoJitManager::JittedMethodInfo* jmi =NULL;
    jmi = (EconoJitManager::JittedMethodInfo*) (thunkReturnAddress - 
                                                 (BYTE*) &(jmi->JmpInstruction[5]) + 
                                                 (BYTE*) jmi);
    MethodDesc* pMD = EconoJitManager::JitMethodInfo2MethodDesc(jmi);
    // This creates a transition frame so stackwalk happens properly
	_ASSERTE(ms.isValid());		/* This is not a lazy machine state (_pRetAddr != 0) */
    HelperMethodFrame HelperFrame(&ms,pMD, (ArgumentRegisters*)&arg2);
    retAddress =  EconoJitManager::RejitMethod(jmi,0);
    HelperFrame.Pop();
    return retAddress;
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - RejitCalleMethod (EjitMgr.cpp)");
    return NULL;
#endif // _X86_
}

static void** gIgnoredPtr;  /* This is used to thwart the compiler from optimizing assignments to retLow */

BYTE* __cdecl RejitCallerMethod(struct MachState ms, void* retLow, void* retHigh, BYTE* thunkReturnAddress)
{
    gIgnoredPtr =  (&retLow);   /* dummy store, needed to prevent the compiler from optimizing stores to retLow*/
#ifdef _X86_
    BYTE* retAddress;
	_ASSERTE(ms.isValid());		/* This is not a lazy machine state (_pRetAddr != 0) */
    HelperMethodFrame HelperFrame(&ms, 0);    // This creates a transition frame so stackwalk happens properly
    EconoJitManager::PitchedCodeThunk* pThunk = NULL;
    pThunk = (EconoJitManager::PitchedCodeThunk*) (thunkReturnAddress - 
                                                    (BYTE*) &(pThunk->CallInstruction[5]) +
                                                    (BYTE*) pThunk);
    EconoJitManager::JittedMethodInfo* jmi = pThunk->u.pMethodInfo;
    unsigned returnOffset = pThunk->relReturnAddress;

        // cant protect retLow directly because GCPROTECT_END wacks it
    void* objToProtect = retLow;
    if (pThunk->retTypeProtect == MethodDesc::RETNONOBJ) 
    {
        retAddress = EconoJitManager::RejitMethod(jmi,returnOffset);
    }
    else if (pThunk->retTypeProtect == MethodDesc::RETOBJ)
    {
        GCPROTECT_BEGIN(objToProtect)
        retAddress = EconoJitManager::RejitMethod(jmi,returnOffset);
        retLow = objToProtect;
        GCPROTECT_END();
    }
    else
    {
        _ASSERTE(pThunk->retTypeProtect == MethodDesc::RETBYREF);
        GCPROTECT_BEGININTERIOR(objToProtect)
        retAddress = EconoJitManager::RejitMethod(jmi,returnOffset);
        retLow = objToProtect;
        GCPROTECT_END();
    }
    HelperFrame.Pop();
    return retAddress;
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - RejitCallerMethod (EjitMgr.cpp)");
    return NULL;
#endif // _X86_
}


BYTE* EconoJitManager::RejitMethod(JittedMethodInfo* pJMI, unsigned returnOffset)
{
#ifdef _X86_
    TICKS startRejitTime = GET_TIMESTAMP();
    EconoJitManager* jitMgr = (EconoJitManager*) ExecutionManager::GetJitForType(miManaged_IL_EJIT);
    Thread* thread = GetThread();

    BYTE* startAddress;

#ifdef STRESS_HEAP
        // for GCStress > 2, the EnablePremtiveGC does it for us
    if (g_pConfig->GetGCStressLevel() & EEConfig::GCSTRESS_TRANSITION)  
        g_pGCHeap->StressHeap();
#endif

        thread->EnablePreemptiveGC();
        m_pRejitCritSec->Enter();
#ifdef _DEBUG
        m_RejitLock_Holder = GetCurrentThreadId();  
#endif
        thread->DisablePreemptiveGC();

    if (pJMI->flags.JittedMethodPitched)
    {
        // we found the flag to be true, and we are holding the rejit lock, so it will remain true until
        // we rejit
        MethodDesc* ftn =  jitMgr->JitMethodInfo2MethodDesc(pJMI);
        COR_ILMETHOD_DECODER ILHeader(ftn->GetILHeader(), ftn->GetMDImport());
        BOOL ignored;
#ifdef _DEBUG

        LOG((LF_JIT, LL_INFO1000,returnOffset ? "REJIT caller: " : "REJIT callee" ));
#endif
        // This is commented out right now because I have to give a private
        // drop to test, since this breaks their tests.  I will uncomment it
        // when they can handle re-jit events.
#ifdef PROFILING_SUPPORTED
        if (CORProfilerTrackJITInfo())
            g_profControlBlock.pProfInterface->JITCompilationStarted(
                reinterpret_cast<ThreadID>(thread),
                reinterpret_cast<FunctionID>(ftn), TRUE);
#endif // PROFILING_SUPPORTED

        Stub *pStub = ::JITFunction(ftn, &ILHeader, &ignored);

#ifdef PROFILING_SUPPORTED
        if (CORProfilerTrackJITInfo())
            g_profControlBlock.pProfInterface->JITCompilationFinished(
                reinterpret_cast<ThreadID>(thread),
                reinterpret_cast<FunctionID>(ftn),
                (pStub != NULL ? S_OK : E_FAIL),
                FALSE);
#endif // PROFILING_SUPPORTED
    }
    // else,  someone beat us, the method has already been rejitted!
    // we are guaranteed that the method will not be pitched until we get out of here
    _ASSERTE( (((size_t)(pJMI->u1.pCodeHeader)) & 1) == 0);
    startAddress = (BYTE*) (pJMI->u1.pCodeHeader + 1);
#ifdef _DEBUG
    m_RejitLock_Holder = 0;  
#endif
    
    if (returnOffset)
        m_cCallerRejits++;      // this operation is protected by the Rejit crst
    else
        m_cCalleeRejits++;

    TICKS endRejitTime = GET_TIMESTAMP();
    AddRejitOverhead(endRejitTime-startRejitTime);    // this is protected by the RejitCritSec
                                                        
    m_pRejitCritSec->Leave();
   
    return (startAddress + returnOffset);
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - RejitMethod (EjitMgr.cpp)");
    return NULL;
#endif // _X86_
}

#ifndef _ALPHA_ // Alpha doesn't understand naked
__declspec (naked)
#endif // _ALPHA_
void CallThunk()
{
#ifdef _X86_
    __asm {
        lea     eax, [esp+4]// get address of return address into the caller of the THUNK. 
                            // This well be used as the return address in the machine state

        // The thunk return address is already pushed and acts as a param

        push    ecx         // pass arg1    // Need to be in this order for ArgIterator

        push    edx         // pass arg2    (save the value)

            // From here we are making a MachState structure.  
        push    eax         // address of return address of whoever called the thunk
        push    0xCCCCCCCC  // The ESP after we return.  We dont know this
                            // since this is a shared thunk and we dont know
                            // how many arguments to pop.  Put an illegal value
                            // here to insure we don't use it 
        push    ebp 
        push    esp         // pEbp
        push    ebx 
        push    esp         // pEbx
        push    esi 
        push    esp         // pEsi
        push    edi 
        push    esp         // pEdi
    
        call    RejitCalleeMethod 

        mov     edi, [esp+4]// restore regs
        mov     esi, [esp+12]
        mov     ebx, [esp+20]
        mov     ebp, [esp+28]
        add     esp, 40     // Pop off sizeof(MachineState)

        pop     edx         // restore arg2
        pop     ecx         // restore arg1
        lea     esp, [esp+4]// pop thunk return address
        jmp     eax
    }
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - CallThunk (EjitMgr.cpp)");
#endif // _X86_
}

#ifndef _ALPHA_ // Alpha doesn't understand naked
__declspec (naked)
#endif // _ALPHA_
void RejitThunk()
{
#ifdef _X86_
    __asm {
        mov     ecx, esp    // save pointer to return address for MachState. From a stack
                            // crawling point of view we have not really 'returned' from
                            // the method whose return cause this code to be invoked 

        // The thunk return address is already pushed and acts as a param

        push    edx         // pass upper bytes of return value (if it is a long)

        push    eax         // pass return value (so we can protect it

            // From here we are making a MachState structure.  
        push    ecx         // return address from a stack crawling point of view
        add     ecx, 4
        push    ecx         // The ESP after we return.  (we poped the return value)
        push    ebp 
        push    esp         // pEbp
        push    ebx 
        push    esp         // pEbx
        push    esi 
        push    esp         // pEsi
        push    edi 
        push    esp         // pEdi

        call    RejitCallerMethod 
        mov     ecx, eax    // save location to return to

        mov     edi, [esp+4]// restore regs
        mov     esi, [esp+12]
        mov     ebx, [esp+20]
        mov     ebp, [esp+28]
        add     esp, 40     // Pop off sizeof(MachineState)

        pop     eax         // restore return value
        pop     edx         // restore upper bytes of return value (if it is a long)
        lea     esp, [esp+4]// pop thunk return address (now we have actually returned!)
        jmp     ecx
    }
#else // !_X86_
    _ASSERTE (!"@TODO Alpha - RejitThunk (EjitMgr.cpp)");
#endif // _X86_
}

const BYTE *GetCallThunkAddress()
{
    return (const BYTE *)CallThunk;
}

const BYTE *GetRejitThunkAddress()
{
    return (const BYTE *)RejitThunk;
}

typedef struct 
{
    EconoJitManager*    jitMgr;
    BYTE                retTypeProtect;
    unsigned            threadIndex;
    unsigned            preserveCandidateIndex;
    unsigned            cThreads;
} StackWalkData;

inline CORINFO_MODULE_HANDLE GetScopeHandle(MethodDesc* method) {
    return(CORINFO_MODULE_HANDLE(method));
}


void CreateThunk_callback(IJitManager* ejitMgr,LPVOID* pHijackLocation, ICodeInfo *pCodeInfo)
{
    if (ExecutionManager::FindJitMan((SLOT)*pHijackLocation) == ejitMgr) 
    {
        METHODTOKEN methodToken = ((EECodeInfo*)pCodeInfo)->m_methodToken;
        if (((EconoJitManager*)ejitMgr)->IsThunk((BYTE*) *pHijackLocation))
            ((EconoJitManager*)ejitMgr)->SetThunkBusy((BYTE*)*pHijackLocation, methodToken);
        else
            ((EconoJitManager*)ejitMgr)->CreateThunk(pHijackLocation,false,methodToken);
    }
}


StackWalkAction StackWalkCallback_CodePitch(CrawlFrame* pCF, void* data)
{
#ifdef _X86_

    StackWalkData* swd = (StackWalkData*)data;
    EconoJitManager* ejitMgr = swd->jitMgr;
    BYTE prevRetType = swd->retTypeProtect;
    // update return type
    _ASSERTE(pCF->ReturnsObject() < 256);
    swd->retTypeProtect = (BYTE) pCF->ReturnsObject();

    PREGDISPLAY pRD = pCF->GetRegisterSet();
    SLOT* pPC = pRD->pPC;
   
    
    if (ExecutionManager::FindJitMan(*pPC) != ejitMgr ||
        !pCF->IsFrameless() ) // the EE inserts frames for context transition between managed calls; these should be ignored.
    {
        return SWA_CONTINUE; 
    }
    ICodeManager* codeman = ejitMgr->GetCodeManager();
    _ASSERTE(codeman);
    METHODTOKEN methodToken = pCF->GetMethodToken();

#ifdef _DEBUG
    MethodDesc* pMD = ejitMgr->JitTokenToMethodDesc(methodToken);
    LPCUTF8 cls  = pMD->GetClass() ? pMD->GetClass()->m_szDebugClassName
                                   : "GlobalFunction";
    LPCUTF8 name = pMD->GetName();

    LOG((LF_JIT,LL_INFO10000,"SW callback for %s:%s\n", cls,name));
#endif

    // if this is already a thunk, simply mark thunk as busy
    if (ejitMgr->IsThunk((BYTE*) *pPC))
    {
        ejitMgr->SetThunkBusy((BYTE*) *pPC,methodToken);
    }
    else
    {
        ejitMgr->AddPreserveCandidate(swd->threadIndex,
                                      swd->cThreads,
                                      swd->preserveCandidateIndex,
                                      methodToken);
        swd->preserveCandidateIndex++;
        ejitMgr->CreateThunk((LPVOID*)pPC,prevRetType,methodToken);
    }
    // also create thunks for each return from finally's and filters
    // NOTE: if we ever make the regular jit pitchable, make the following a virtual method on ICodeManager
    EECodeInfo codeInfo(methodToken, ejitMgr);
    Fjit_EETwain::HijackHandlerReturns(pRD,ejitMgr->GetGCInfo(methodToken),&codeInfo, ejitMgr, CreateThunk_callback);
    return SWA_CONTINUE;
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - StackWalkCallback_CodePitch (EjitMgr.cpp)");
    return SWA_ABORT;
#endif // _X86_

}


void EconoJitManager::StackWalkForCodePitching() {
#ifdef _X86_
    // for each thread call StackWalkThreadForCodePitching();
    Thread  *thread = NULL;
    StackWalkData stackWalkData;
    stackWalkData.jitMgr = this;
    stackWalkData.retTypeProtect = MethodDesc::RETNONOBJ;
    stackWalkData.threadIndex = 0;
    stackWalkData.preserveCandidateIndex = 0;
    unsigned cThreads = GetThreadCount();
    stackWalkData.cThreads = cThreads;

    if (cThreads*m_MaxUnpitchedPerThread > m_PreserveCandidates_size)
        growPreserveCandidateArray(cThreads*m_MaxUnpitchedPerThread);

    while ((thread = ThreadStore::GetThreadList(thread)) != NULL)
    {
        thread->StackWalkFrames(StackWalkCallback_CodePitch, (void*) &stackWalkData);
        stackWalkData.threadIndex++;
    }
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - EconoJitManager::StackWalkForCodePitching (EjitMgr.cpp)");
#endif // _X86_

}

unsigned EconoJitManager::GetThreadCount() 
{
    unsigned n=0;
    Thread  *thread = NULL;
    while ((thread = ThreadStore::GetThreadList(thread)) != NULL)
        n++;
    return n;
}

void EconoJitManager::AddPreserveCandidate(unsigned threadIndex,
                                           unsigned cThreads,
                                           unsigned candidateIndex,
                                           METHODTOKEN methodToken)
{
    _ASSERTE(!((JittedMethodInfo*) methodToken)->flags.JittedMethodPitched);
    if (candidateIndex < m_MaxUnpitchedPerThread)
        m_PreserveCandidates[threadIndex +cThreads*candidateIndex] = (JittedMethodInfo*) methodToken;

}

void EconoJitManager::MarkHeapsForPitching()
{
#ifdef _X86_
    _ASSERTE(m_JittedMethodInfoHdr);
    JittedMethodInfoHdr* pJMIT_Hdr = m_JittedMethodInfoHdr;
    size_t limit = (size_t)m_JMIT_free;

    do 
    {
        JittedMethodInfo* jmit = (JittedMethodInfo*) (pJMIT_Hdr+1);
        while ((size_t)(jmit+1) <= limit)
        {
            if (!(jmit->flags.JittedMethodPitched) && !(jmit->flags.MarkedForPitching))
            {
                jmit->flags.MarkedForPitching = true;
                jmit->flags.EHandGCInfoPitched = true;
                jmit->JmpInstruction[0] = CALL_OPCODE;
                unsigned *pCallOffset = (unsigned*) &(jmit->JmpInstruction[1]);
                *pCallOffset = ((unsigned)(size_t) CallThunk - (size_t)(pCallOffset+1));
            }
            jmit++;

        }
        pJMIT_Hdr = pJMIT_Hdr->next;
        limit = ((size_t)pJMIT_Hdr) + JMIT_BLOCK_SIZE;        // all other JMIT blocks are completely full
    } while (pJMIT_Hdr);

#else // !_X86_
    _ASSERTE(!"@TODO Alpha - EconoJitManager::MarkHeapsForPitching (EjitMgr.cpp)");
#endif // _X86_
}

void EconoJitManager::UnmarkPreservedCandidates(unsigned minSpaceRequired)
{
    JittedMethodInfo** candidate = m_PreserveCandidates;
    unsigned cThreads = GetThreadCount();
    if (minSpaceRequired >= m_CodeHeapCommittedSize)
    {
        while (candidate < &(m_PreserveCandidates[cThreads*m_MaxUnpitchedPerThread]))
            *candidate++ = NULL;
        return;
    }
    unsigned totalSpacePreserved = 0;
    while (candidate < &(m_PreserveCandidates[cThreads*m_MaxUnpitchedPerThread]))
    {
        if (*candidate) 
            
        {
            JittedMethodInfo* jmi = *candidate;
            unsigned spaceRequiredByMethod = (unsigned)((size_t)jmi->u2.pCodeEnd - (size_t)jmi->u1.pCodeHeader);
            if (availableMemoryInCodeHeap() + (usedMemoryInCodeHeap() - totalSpacePreserved - spaceRequiredByMethod) 
                >= minSpaceRequired)
            {
                _ASSERTE(!(jmi->flags.JittedMethodPitched));
                jmi->flags.MarkedForPitching = false;
                // We'll fix the {CALL instruction to the thunk}, by changing it to
                // a {JMP instruction to the actual code}, in MovePreservedMethod
                totalSpacePreserved += spaceRequiredByMethod;
            }
            else // delete it from the list of preserved candidates
            {
                *candidate = NULL;
            }
        }
        candidate++;
    } // while
    _ASSERTE(availableMemoryInCodeHeap() + (usedMemoryInCodeHeap() - totalSpacePreserved) >= minSpaceRequired);
}


unsigned EconoJitManager::PitchMarkedCode()
{
    _ASSERTE(m_JittedMethodInfoHdr);
    JittedMethodInfoHdr* pJMIT_Hdr = m_JittedMethodInfoHdr;
    size_t limit = (size_t)m_JMIT_free;    // the first JMIT block may be partially full
    unsigned cMethodsPitched = 0;

    
    do 
    {
        JittedMethodInfo* jmit = (JittedMethodInfo*) (pJMIT_Hdr+1);
        while ((size_t)(jmit+1) <= limit)
        {
            if (jmit->flags.MarkedForPitching)
            {
#ifdef DEBUGGING_SUPPORTED
                MethodDesc *pMD = *(MethodDesc**)(jmit->u1.pCodeHeader);
                DWORD dwDebugBits = pMD->GetModule()->GetDebuggerInfoBits();

                //quick, tell the debugger before we pitch it!
                if (CORDebuggerTrackJITInfo(dwDebugBits))
                {
                    g_pDebugInterface->PitchCode(pMD, 
                            (const BYTE*)jmit->u1.pCodeHeader+sizeof(void*));
                }
#endif // DEBUGGING_SUPPORTED

#ifdef PROFILING_SUPPORTED
                // Notify the profiler that this function is being pitched
                if (CORProfilerTrackJITInfo())
                {
                    MethodDesc *pMD = *(MethodDesc**)(jmit->u1.pCodeHeader);

                    g_profControlBlock.pProfInterface->
                        JITFunctionPitched(reinterpret_cast<ThreadID>(GetThread()),
                                           reinterpret_cast<FunctionID>(pMD));
                }
#endif // PROFILING_SUPPORTED

                jmit->flags.JittedMethodPitched = true;
                jmit->flags.MarkedForPitching = false;
#ifdef _DEBUG
                LogAction((MethodDesc*)((size_t) (* (MethodDesc**) (jmit->u1.pCodeHeader))), 
                    "Pitched",
                    jmit->u1.pCodeHeader,
                    jmit->u2.pCodeEnd);
#endif
                // set last bit to indicate that code has been pitched
                jmit->u2.pEhGcInfo =  (BYTE*) ((size_t)(*(BYTE **) (jmit->u1.pCodeHeader - 1)) | 1);
                jmit->u1.pMethodDescriptor = (MethodDesc*) ((size_t)(* (MethodDesc**) (jmit->u1.pCodeHeader)) | 1);
                cMethodsPitched++;
            }

            jmit++;
        }
        pJMIT_Hdr = pJMIT_Hdr->next;
        limit = ((size_t)pJMIT_Hdr) + JMIT_BLOCK_SIZE;        // all other JMIT blocks are completely full
    } while (pJMIT_Hdr);
    ResetPc2MdMap();
    return cMethodsPitched;
}
int __cdecl EconoJitManager::compareJMIstart( const void *arg1, const void *arg2 )
{
    JittedMethodInfo* jmi1 = *(JittedMethodInfo**) arg1;
    JittedMethodInfo* jmi2 = *(JittedMethodInfo**) arg2;
    _ASSERTE(!(jmi1->flags.JittedMethodPitched) && !(jmi2->flags.JittedMethodPitched));
    return (jmi1->u1.pCodeHeader < jmi2->u1.pCodeHeader ? -1 : 
             (jmi1->u1.pCodeHeader == jmi2->u1.pCodeHeader ? 0 : 1));
}

void EconoJitManager::MovePreservedMethods()
{
    unsigned cThreads = GetThreadCount();
    unsigned maxCandidates = cThreads * m_MaxUnpitchedPerThread;
    unsigned numCandidates = CompressPreserveCandidateArray(maxCandidates);
    if (numCandidates > 1) 
    {
        qsort(m_PreserveCandidates,numCandidates,sizeof(void*),EconoJitManager::compareJMIstart);
#ifdef _DEBUG
        // verify that it is sorted correctly
        for (unsigned j=0;j<numCandidates-1;j++)
        {
            _ASSERTE(m_PreserveCandidates[j]->u1.pCodeHeader <= m_PreserveCandidates[j+1]->u1.pCodeHeader);
        }
#endif

    }

#ifdef _DEBUG
    if (m_MaxUnpitchedPerThread == 0)
        memset(m_CodeHeap,0xcc,m_CodeHeapFree-m_CodeHeap);
#endif

    _ASSERTE(m_CodeHeap);
    m_CodeHeapFree = m_CodeHeap;
    for (unsigned i=0;i<numCandidates; i++)
    {
        if (i == 0 || m_PreserveCandidates[i] != m_PreserveCandidates[i-1])
        {
            JittedMethodInfo* jmi = m_PreserveCandidates[i];
            if (!(jmi->flags.JittedMethodPitched))
            {
                MovePreservedMethod(jmi);
#ifdef _DEBUG
                LogAction((* (MethodDesc**) (jmi->u1.pCodeHeader)), 
                    "Preserved",
                    jmi->u1.pCodeHeader,
                    jmi->u2.pCodeEnd);
#endif
            }
        }
    }
    memset(m_PreserveCandidates,0,cThreads*m_MaxUnpitchedPerThread*sizeof(void*));

}

unsigned EconoJitManager::CompressPreserveCandidateArray(unsigned size)
{
    JittedMethodInfo** pFront, **pRear = m_PreserveCandidates;
    //_ASSERTE(size);
    unsigned cCompressedSize=0;

    while (cCompressedSize < size && (*pRear))
    {
        cCompressedSize++;
        pRear++;
    }
    if (cCompressedSize == size)
        return cCompressedSize;
    pFront=pRear;
    while (pFront < &(m_PreserveCandidates[size]))
    {
        if (*pFront)
        {
            *pRear++ = *pFront;
            cCompressedSize++;
        }
        pFront++;
    }
    return cCompressedSize;
}


void EconoJitManager::MovePreservedMethod(JittedMethodInfo* jmi)
{
    _ASSERTE(m_CodeHeapFree);
    BYTE* startPreserved = ((BYTE*)jmi->u1.pCodeHeader) - sizeof (void*); // to account for EhGc info
    size_t size =  (size_t)jmi->u2.pCodeEnd - (size_t)startPreserved;

    _ASSERTE(m_CodeHeapFree <= startPreserved);
    memcpy(m_CodeHeapFree,startPreserved, size);
    jmi->u1.pCodeHeader = (CodeHeader*) ((size_t) m_CodeHeapFree + sizeof(void*));
    jmi->u2.pCodeEnd = m_CodeHeapFree + size;

    // Change the call back to a jmp
    jmi->JmpInstruction[0] = JMP_OPCODE;
    unsigned *pJmpOffset = (unsigned*) &(jmi->JmpInstruction[1]);
    size_t pCodeBlock = (size_t)(jmi->u1.pCodeHeader) + sizeof(void*);
    *pJmpOffset = (unsigned)((size_t)pCodeBlock - (size_t)pJmpOffset - sizeof(void*));

    MethodDesc *pMD = JitMethodInfo2MethodDesc(jmi);
    AddPC2MDMap(pMD,jmi->u2.pCodeEnd);
    m_CodeHeapFree += size;
    _ASSERTE((size_t)m_CodeHeapFree - (size_t)m_CodeHeap <= m_CodeHeapCommittedSize);

#ifdef DEBUGGING_SUPPORTED
    // Tell the debugger that something's moved.
    DWORD dwDebugBits = pMD->GetModule()->GetDebuggerInfoBits();
    if (CORDebuggerTrackJITInfo(dwDebugBits))
        g_pDebugInterface->MovedCode(pMD, 
                                     (const BYTE*)(startPreserved +2*sizeof(void*)),
                                     (const BYTE*)pCodeBlock);
#endif // DEBUGGING_SUPPORTED
}


void EconoJitManager::MoveAllPreservedEhGcInfo()
{
    _ASSERTE(m_JittedMethodInfoHdr);
    JittedMethodInfoHdr* pJMIT_Hdr = m_JittedMethodInfoHdr;
    size_t limit = (size_t)m_JMIT_free;    // the first JMIT block may be partially full
    m_cPreserveEhGcInfoList = 0;
    do 
    {
        JittedMethodInfo* jmit = (JittedMethodInfo*) (pJMIT_Hdr+1);
        while ((size_t)(jmit+1) <= limit)
        {
            if (!jmit->flags.EHandGCInfoPitched)
            {
                AddToPreservedEhGcInfoList(jmit);
            }
            jmit++;
        }
        pJMIT_Hdr = pJMIT_Hdr->next;
        limit = ((size_t)pJMIT_Hdr) + JMIT_BLOCK_SIZE;        // all other JMIT blocks are completely full
    } while (pJMIT_Hdr);
/*   THIS IS TEMPORARILY COMMENTED OUT, We will need this when we
     change EhGcHeap management scheme:
    if (m_cPreserveEhGcInfoList > 1)
        qsort(m_PreserveEhGcInfoList,m_cPreserveEhGcInfoList,sizeof(void*),EconoJitManager::compareEhGcPtr);
#ifdef _DEBUG
        for (unsigned j=0;j<m_cPreserveEhGcInfoList-1;j++)
        {
            unsigned add1 = m_PreserveEhGcInfoList[j]->flags.JittedMethodPitched ?
                (unsigned) m_PreserveEhGcInfoList[j]->u2.pEhGcInfo :
                *(unsigned *) (m_PreserveEhGcInfoList[j]->u1.pCodeHeader-1);
            unsigned add2 =  m_PreserveEhGcInfoList[j+1]->flags.JittedMethodPitched ?
                (unsigned) (m_PreserveEhGcInfoList[j+1]->u2.pEhGcInfo) :
                *(unsigned *)(m_PreserveEhGcInfoList[j+1]->u1.pCodeHeader-1);

            _ASSERTE(add1 < add2);
        }
#endif
*/    
    EHGCBlockHdr* oldHeap = m_EHGCHeap;
    ResetEHGCHeap();
    for (unsigned i=0; i<m_cPreserveEhGcInfoList; i++)
        MoveSinglePreservedEhGcInfo(m_PreserveEhGcInfoList[i]);
    CleanupLargeEhGcInfoList();
    // free all the unused heap space
    while (oldHeap)
    {
        EHGCBlockHdr* pHeap = oldHeap->next;
        unsigned size = oldHeap->blockSize;
        VirtualFree(oldHeap,size,MEM_DECOMMIT);      
        VirtualFree(oldHeap,0,MEM_RELEASE);
        //DEBUG_LOG("FREE(EhGcHeaps)",size);
        oldHeap = pHeap;
    }
}


void EconoJitManager::MoveSinglePreservedEhGcInfo(JittedMethodInfo* jmi)
{
    unsigned char* pDestEhGcBlock = (unsigned char*) allocEHGCBlock(jmi->GetEhGcInfo_len(m_LargeEhGcInfo));
    BYTE* u2 = (BYTE*) (jmi->u2.pEhGcInfo);
    if ((size_t)u2 & 1) // method has been pitched
    {
        _ASSERTE(jmi->flags.JittedMethodPitched);
        memcpy(pDestEhGcBlock,
               (BYTE*) ((size_t)u2 & ~1),
               jmi->GetEhGcInfo_len(m_LargeEhGcInfo));
        jmi->u2.pEhGcInfo = (BYTE*) ((size_t) pDestEhGcBlock | 1);
    }

    else // method has not been pitched
    {
        _ASSERTE(jmi->flags.JittedMethodPitched == 0);
        memcpy(pDestEhGcBlock,
               *(BYTE**)(jmi->u1.pCodeHeader - 1),
               jmi->GetEhGcInfo_len(m_LargeEhGcInfo));
        *(BYTE**) (jmi->u1.pCodeHeader - 1) = pDestEhGcBlock;
    }

}

void EconoJitManager::CleanupLargeEhGcInfoList()
{
    LargeEhGcInfoList* p = (LargeEhGcInfoList*) &m_LargeEhGcInfo, *q = m_LargeEhGcInfo;
    _ASSERTE(p->next == q); 
    while (q)
    {
        if (q->jmi->flags.EHandGCInfoPitched)
        {
            LargeEhGcInfoList* temp = q;
            q = q->next; 
            delete temp;
        }
        else
        {
            p = q;
            q = q->next;
        }
    }
}

int __cdecl EconoJitManager::compareEhGcPtr( const void *arg1, const void *arg2 )
{
    JittedMethodInfo* jmi1 = *(JittedMethodInfo**) arg1;
    JittedMethodInfo* jmi2 = *(JittedMethodInfo**) arg2;
    size_t EhGc1 = (size_t)JitMethodInfo2EhGcInfo(jmi1);
    size_t EhGc2 = (size_t)JitMethodInfo2EhGcInfo(jmi2);
    return (EhGc1 < EhGc2 ? -1 :
                EhGc1 == EhGc2 ? 0: 1);
}
void EconoJitManager::AddToPreservedEhGcInfoList(JittedMethodInfo* jmi)
{
    _ASSERTE(m_cPreserveEhGcInfoList <= m_PreserveEhGcInfoList_size);
    if (m_cPreserveEhGcInfoList == m_PreserveEhGcInfoList_size)
    {
        growPreservedEhGcInfoList();
    }
    m_PreserveEhGcInfoList[m_cPreserveEhGcInfoList++] = jmi;
}

void EconoJitManager::growPreservedEhGcInfoList()
{
    _ASSERTE(m_cPreserveEhGcInfoList == m_PreserveEhGcInfoList_size);
    JittedMethodInfo** temp = m_PreserveEhGcInfoList;
    m_PreserveEhGcInfoList_size *= 2;
    m_PreserveEhGcInfoList = new (throws) pJittedMethodInfo[m_PreserveEhGcInfoList_size];
    _ASSERTE(m_PreserveEhGcInfoList != NULL);
    memcpy(m_PreserveEhGcInfoList,temp,m_cPreserveEhGcInfoList*sizeof(void*));
    delete[] temp;
}

#if defined(ENABLE_PERF_COUNTERS) 
int EconoJitManager::GetCodeHeapSize()
{
#if defined(ENABLE_JIT_PERF)
    if (g_fJitPerfOn)
    {
        return (int) usedMemoryInCodeHeap();
    }
#endif // ENABLE_JIT_PERF
    return 0;
}
int EconoJitManager::GetEHGCHeapSize()
{
    int icurSum = 0;
#if defined(ENABLE_JIT_PERF)
    if(g_fJitPerfOn)
    {
        EHGCBlockHdr *pHp = (EHGCBlockHdr*) m_EHGCHeap;
        // FOr the first node, count the actual # of bytes in use:
        if (m_EHGCHeap) {
            icurSum += (int)(m_EHGC_alloc_end - (unsigned char *)m_EHGCHeap);
            pHp = pHp->next;
        }
        // For the remaining nodes just calculate the total allocated size.
        while (pHp)
        {   
            icurSum += pHp->blockSize; 
            //@TODO: Change this to correct calculation if more than one page are allocated for EHGC.
            pHp = pHp->next;
        }    
    }
#endif // ENABLE_JIT_PERF
    return icurSum;
}
#endif // ENABLE_PERF_COUNTERS

#ifdef _DEBUG
// Puts a breakpoint at every byte of unused heap.
// this enables us to immediately catch an attempt to execute code that has been pitched
void  EconoJitManager::SetBreakpointsInUnusedHeap()
{ 
    _ASSERTE(m_CodeHeap && m_CodeHeapFree);
    memset(m_CodeHeapFree,
           BREAK_OPCODE,
           availableMemoryInCodeHeap());
}

void  EconoJitManager::VerifyAllCodePitched()
{
    // Walk the jittedmethodinfo table and verify that all code is pitched
    JittedMethodInfoHdr* pJMIT_Hdr = m_JittedMethodInfoHdr;
    size_t limit = (size_t)m_JMIT_free;
    do 
    {
        JittedMethodInfo* jmit = (JittedMethodInfo*) (pJMIT_Hdr+1);
        while ((size_t)(jmit+1) <= limit)
        {
            _ASSERTE(jmit->flags.JittedMethodPitched);
            jmit++;

        }
        pJMIT_Hdr = pJMIT_Hdr->next;
        limit = ((size_t)pJMIT_Hdr) + JMIT_BLOCK_SIZE;        // all other JMIT blocks are completely full
    } while (pJMIT_Hdr);
}
#endif 

/***************************************************************************************************
                    Thunk Management
****************************************************************************************************/
void EconoJitManager::MarkThunksForRelease() 
{
    ThunkBlock* thunkBlock = m_ThunkBlocks;
    while (thunkBlock) 
    {
        PitchedCodeThunk* thunks = (PitchedCodeThunk*) ( thunkBlock+1 );
        for (unsigned i=0;i<THUNKS_PER_BLOCK;i++)
        {
            thunks[i].Busy = false;
        }
        thunkBlock = thunkBlock->next;          
    }
}


BOOL EconoJitManager::IsThunk(BYTE* address)
{
    ThunkBlock*  pThunkBlock = m_ThunkBlocks;
    while (pThunkBlock) 
    {
        if ((BYTE*) pThunkBlock < address && address < ((BYTE*)pThunkBlock+THUNK_BLOCK_SIZE))
        {
            address = (BYTE*) ((size_t) address & THUNK_BEGIN_MASK);
            _ASSERTE(((PitchedCodeThunk*)address)->relReturnAddress != 0xffffffff);
            return true;
        }
        pThunkBlock=pThunkBlock->next;
    }  
    return false;
}

void  EconoJitManager::SetThunkBusy(BYTE* address, METHODTOKEN methodToken)
{
    PitchedCodeThunk* pThunk = (PitchedCodeThunk*)(((size_t) address) & THUNK_BEGIN_MASK);
    _ASSERTE(IsThunk((BYTE*) pThunk));
    pThunk->Busy = true;
    JittedMethodInfo* jmi = (JittedMethodInfo*) methodToken; 
    jmi->flags.EHandGCInfoPitched = false;     // we always preserve EhGcInfo for methods on the stack

}

EconoJitManager::PitchedCodeThunk* EconoJitManager::GetNewThunk()
{
    PitchedCodeThunk* newThunk;
    if (m_FreeThunkList) //  && 0 is TEMPORARY for debugging only! Remove this before checkin
    {
        newThunk = m_FreeThunkList;
        m_FreeThunkList = m_FreeThunkList->u.next;
    }
    else if (m_cThunksInCurrentBlock && (m_cThunksInCurrentBlock < THUNKS_PER_BLOCK))
    {
        newThunk = ((PitchedCodeThunk*)(m_ThunkBlocks+1)) + m_cThunksInCurrentBlock;
        m_cThunksInCurrentBlock++;
    }
    else { // this is the first thunk to be created 
        ThunkBlock* newThunkBlock = (ThunkBlock*) VirtualAlloc(NULL,
                                                 THUNK_BLOCK_SIZE,
                                                 MEM_RESERVE|MEM_COMMIT,
                                                 PAGE_EXECUTE_READWRITE);
        //DEBUG_LOG("ALLOC(GetNewThunk)",THUNK_BLOCK_SIZE);
        if (!newThunkBlock)
        {
            // @bug: throw an exception or return a failure 
            _ASSERTE(!"Out of memory!");
        }
        WS_PERF_SET_HEAP(ECONO_JIT_HEAP);
        WS_PERF_UPDATE("CreateThunk", THUNK_BLOCK_SIZE, newThunkBlock);
        
        if (!ExecutionManager::AddRange((LPVOID)newThunkBlock, 
                                        (LPVOID)((size_t)newThunkBlock + THUNK_BLOCK_SIZE),
                                        this, 
                                        NULL))
        {
            VirtualFree(newThunkBlock,THUNK_BLOCK_SIZE,MEM_DECOMMIT);
            VirtualFree(newThunkBlock,0,MEM_RELEASE);
            // @bug: throw an exception or return a failure
            _ASSERTE(!"Error: AddRange");
        }

        newThunkBlock->next = m_ThunkBlocks;
        m_ThunkBlocks = newThunkBlock; 
        newThunk = (PitchedCodeThunk*)(newThunkBlock+1);
        m_cThunksInCurrentBlock = 1;
    }
    return newThunk;
}
    
// This is called while code pitching, so we are free to read and update jmi entries
void EconoJitManager::CreateThunk(LPVOID* pHijackLocation,BYTE retTypeProtect, METHODTOKEN methodToken)
{
    BYTE* returnAddress = (BYTE*) *pHijackLocation;
    JittedMethodInfo* jmi = (JittedMethodInfo*) methodToken; //JitCode2MethodInfo(returnAddress);
    _ASSERTE(jmi);

    //@perf: We always create a new thunk, later we might want to avoid creating
    //       duplicate thunks

    PitchedCodeThunk* newThunk = GetNewThunk();

    newThunk->u.pMethodInfo = jmi;
    _ASSERTE(jmi->flags.JittedMethodPitched == 0);
    jmi->flags.EHandGCInfoPitched = false;     // we always preserve EhGcInfo for methods on the stack
    newThunk->relReturnAddress = (unsigned)(size_t)returnAddress - ((size_t)jmi->u1.pCodeHeader + 1);
    newThunk->Busy = true;
    newThunk->LinkedInFreeList = false;
    newThunk->retTypeProtect = retTypeProtect;
    newThunk->CallInstruction[0] = CALL_OPCODE;
    void** callSite = (void**) &(newThunk->CallInstruction[1]);
    *callSite = (void*) ((size_t) RejitThunk - (size_t) (callSite+1));
    *pHijackLocation = (LPVOID) &(newThunk->CallInstruction[0]);

}


// Check to see if this return address is into a thunk, and if 
// so, mark the thunk so that it is not garbage collected
/*
void EconoJitManager::CheckIfThunkAndMark(BYTE* returnAddress)
{
    unsigned address = (unsigned) returnAddress;
    ThunkBlock* thunkBlock = m_ThunkBlocks;
    while (thunkBlock)
    {
        PitchedCodeThunk* thunks = (PitchedCodeThunk*)(thunkBlock+1);
        if (((unsigned) thunks < address) && (address < ((unsigned)thunkBlock+THUNK_BLOCK_SIZE)))
        { // yes, it is a thunk
            PitchedCodeThunk dummy;
            unsigned callInstrnOffset = (unsigned) (&(dummy.CallInstruction[0]) - (BYTE*) &dummy);
            ((PitchedCodeThunk*)(returnAddress - callInstrnOffset))->Busy = true;                
            return;
        }
        thunkBlock = thunkBlock->next;
    }
}

*/
void EconoJitManager::GarbageCollectUnusedThunks()
{
    ThunkBlock* thunkBlock = m_ThunkBlocks;
    size_t limit = (size_t) (thunkBlock+1) + m_cThunksInCurrentBlock*sizeof(PitchedCodeThunk);
    while (thunkBlock)
    {
        PitchedCodeThunk* thunk = (PitchedCodeThunk*)(thunkBlock+1);
        while ((size_t) (thunk+1) <= limit)
        {
            if (!thunk->Busy && !thunk->LinkedInFreeList) 
            {
                thunk->u.next = m_FreeThunkList;
                thunk->LinkedInFreeList = true;
#ifdef _DEBUG
                thunk->relReturnAddress = 0xffffffff;
#endif
                m_FreeThunkList = thunk;
            }
            thunk++;
        }
        thunkBlock = thunkBlock->next;
        limit = (size_t)thunkBlock + THUNK_BLOCK_SIZE;
    }
}

void EconoJitManager::growPreserveCandidateArray(unsigned numberOfCandidates)
{
    if (m_PreserveCandidates)
        delete [] m_PreserveCandidates;
    m_PreserveCandidates = new pJittedMethodInfo[numberOfCandidates];
    if (m_PreserveCandidates)
        memset(m_PreserveCandidates,0,numberOfCandidates*sizeof(void*));
    m_PreserveCandidates_size = m_PreserveCandidates ? numberOfCandidates : 0;
    return;
}

/***************************************************************************************************
                    Code Memory Management
****************************************************************************************************/
// frees the old heap which is guaranteed to be empty and replaces it with a new one of specified size
void EconoJitManager::ReplaceCodeHeap(unsigned newReservedSize,unsigned newCommittedSize)
{
    _ASSERTE(m_CodeHeapReservedSize < newReservedSize);
    if (m_CodeHeap)
    {
        VirtualFree(m_CodeHeap,m_CodeHeapCommittedSize,MEM_DECOMMIT);
        VirtualFree(m_CodeHeap,0,MEM_RELEASE);
        //DEBUG_LOG("FREE(CodeHeap)",m_CodeHeapCommittedSize);
        ExecutionManager::DeleteRange(m_CodeHeap);
    }
    newReservedSize = RoundToPageSize(newReservedSize);
    newCommittedSize = RoundToPageSize(newCommittedSize);
    m_CodeHeap = (BYTE*)VirtualAlloc(NULL,newReservedSize,MEM_RESERVE,PAGE_EXECUTE_READWRITE);
    if (m_CodeHeap)
    {
        //DEBUG_LOG("ALLOC(ReplaceCodeHeap0)",newSize);
        ExecutionManager::AddRange((LPVOID)m_CodeHeap, (LPVOID)((size_t)m_CodeHeap + newReservedSize),this, NULL);
        m_CodeHeapFree = m_CodeHeap;
        m_CodeHeapReservedSize = newReservedSize;
        
        BYTE* additionalMemory = (BYTE*)VirtualAlloc(m_CodeHeap,
                                                     newCommittedSize,
                                                     MEM_COMMIT,
                                                     PAGE_EXECUTE_READWRITE);
        if (additionalMemory == NULL) 
        {
            m_CodeHeapCommittedSize = 0;
            return;
        }
        _ASSERTE(additionalMemory == m_CodeHeap);
        m_CodeHeapCommittedSize = newCommittedSize;
    }
    else
    {
        m_CodeHeapFree = NULL;
        m_CodeHeapCommittedSize = 0;
        m_CodeHeapReservedSize = 0;
    }
                                             
}

// newReservedSize is self-explanatory
// minCommittedSize is used if we fail to expand the reservedSize and have to pitch code
BOOL EconoJitManager::GrowCodeHeapReservedSpace(unsigned newReservedSize,unsigned minCommittedSize)
{
    _ASSERTE(newReservedSize > m_CodeHeapReservedSize);
    if (newReservedSize > (unsigned)g_pConfig->GetMaxCodeCacheSize())
        return false;

    // if got here, we either did not get any memory or got memory that was not contiguous
    // so we need to pitch code, and try again
    unsigned minCommittedSizeReqd = max(minCommittedSize,m_CodeHeapCommittedSize);
    do
    {
        m_pHeapCritSec->Leave();
        BOOL result = PitchAllJittedMethods(newReservedSize,minCommittedSizeReqd,true,true);
        m_pHeapCritSec->Enter();
        if (result)
        {
            _ASSERTE(m_CodeHeap && m_CodeHeapReservedSize >= newReservedSize);
            return TRUE;
        }
        // else failed to grow to the new size, try something smaller
        newReservedSize -= MINIMUM_VIRTUAL_ALLOC_SIZE;
        if (minCommittedSizeReqd > newReservedSize)
            return FALSE;
    } while (newReservedSize > 0);
    
    return FALSE;
}

unsigned EconoJitManager::RoundToPageSize(unsigned size)
{
    unsigned adjustedSize = (size + PAGE_SIZE-1)/PAGE_SIZE * PAGE_SIZE;
    if (adjustedSize < size)       // check for overflow
    {
        adjustedSize = (size/PAGE_SIZE)*PAGE_SIZE;
    }
    return adjustedSize;
}

// For now there is no pre-set limit on how big a request can be made so the function always returns true
BOOL EconoJitManager::SetCodeHeapCommittedSize(unsigned size)    // sets code cache to the given size rounded to next page size
{
    unsigned adjustedSize = RoundToPageSize(size);
    m_pHeapCritSec->Enter();
    if (adjustedSize <= m_CodeHeapCommittedSize)    // check again to make sure that no other
                                                    // thread beat us to this. 
    {
        m_pHeapCritSec->Leave();
        return true;
    }
    _ASSERTE(adjustedSize <= m_CodeHeapReservedSize && adjustedSize > m_CodeHeapCommittedSize);
    
    BYTE* additionalMemory = (BYTE*)VirtualAlloc(m_CodeHeap+m_CodeHeapCommittedSize,
                                                 adjustedSize - m_CodeHeapCommittedSize,
                                                 MEM_COMMIT,
                                                 PAGE_EXECUTE_READWRITE);
    if (additionalMemory == NULL) 
    {
        m_pHeapCritSec->Leave();
        return false;
    }
    _ASSERTE(additionalMemory == m_CodeHeap+m_CodeHeapCommittedSize);
    m_CodeHeapCommittedSize = adjustedSize;
    m_pHeapCritSec->Leave();
    return true;
   
}


unsigned char* EconoJitManager::allocCodeBlock(size_t blockSize)
{
    // Ensure minimal size
   /* if (blockSize < BBT)
        blockSize = BBT;
   */
    _ASSERTE(m_CodeHeap);
#ifdef DEBUGGING_SUPPORTED
    _ASSERTE(CORDebuggerAttached() || availableMemoryInCodeHeap() >= blockSize);
#else // !DEBUGGING_SUPPORTED
    _ASSERTE(availableMemoryInCodeHeap() >= blockSize);
#endif // !DEBUGGING_SUPPORTED

    if (availableMemoryInCodeHeap() < blockSize)
    {
        if (blockSize > m_CodeHeapReservedSize)
        {
            unsigned adjustedSize = RoundToPageSize((unsigned)blockSize); 
            if (!GrowCodeHeapReservedSpace(adjustedSize,adjustedSize))
                return NULL;
        }

        unsigned additionalMemorySize = (unsigned)(((blockSize + PAGE_SIZE -1)/PAGE_SIZE) * PAGE_SIZE);
        BYTE* additionalMemory = (BYTE*)VirtualAlloc(m_CodeHeap+m_CodeHeapCommittedSize,
                                                 additionalMemorySize,
                                                 MEM_COMMIT,
                                                 PAGE_EXECUTE_READWRITE);
        if (additionalMemory == NULL) 
            return NULL;
        _ASSERTE(additionalMemory == m_CodeHeap+m_CodeHeapCommittedSize);
        m_CodeHeapCommittedSize += additionalMemorySize;
    }
    unsigned char* pAllocatedBlock = m_CodeHeapFree;
    m_CodeHeapFree += blockSize;
    return pAllocatedBlock;
}

// This should be called only to free the last allocated block within the 
// same sync block that made the allocation
void EconoJitManager::freeCodeBlock(size_t blockSize)
{
    _ASSERTE((size_t) (m_CodeHeapFree - m_CodeHeap) >= blockSize);
    m_CodeHeapFree -= blockSize;
}
/****************************************************************************************************/
//                      EHandGC Memory Management
/****************************************************************************************************/
BOOL EconoJitManager::NewEHGCBlock(unsigned minsize)
{
    unsigned allocsize = EHGC_BLOCK_SIZE;
    if (minsize + sizeof(EHGCBlockHdr) > allocsize) {
        allocsize = ((minsize + sizeof(EHGCBlockHdr) + EHGC_BLOCK_SIZE - 1)/EHGC_BLOCK_SIZE) * EHGC_BLOCK_SIZE;
    }

    EHGCBlockHdr* newBlock = (EHGCBlockHdr*)  VirtualAlloc(NULL,
                                                             allocsize,
                                                             MEM_RESERVE | MEM_COMMIT,
                                                             PAGE_READWRITE);
    if (!newBlock)
        return FALSE;
    //DEBUG_LOG("ALLOC(NewEHGCBlock",allocsize);
    WS_PERF_SET_HEAP(ECONO_JIT_HEAP);
    WS_PERF_UPDATE("NewEHGCBlock", allocsize, newBlock);
    
    newBlock->next = m_EHGCHeap;
    newBlock->blockSize = allocsize;
    m_EHGCHeap = newBlock;
    m_EHGC_block_end = ((unsigned char*) newBlock)+allocsize;
    newBlock++;
    m_EHGC_alloc_end = (unsigned char*) newBlock;
    return TRUE;

}

unsigned char* EconoJitManager::allocEHGCBlock(size_t blockSize)
{
    if ((!m_EHGCHeap || (availableEHGCMemory() < blockSize)) && !(NewEHGCBlock((unsigned)blockSize)))
        return NULL;
    
    _ASSERTE((unsigned) (m_EHGC_block_end - m_EHGC_alloc_end) >= blockSize);

    unsigned char* pAllocatedBlock = m_EHGC_alloc_end;
    m_EHGC_alloc_end += blockSize;

    return pAllocatedBlock;

}

void EconoJitManager::ResetEHGCHeap()
{
    unsigned newEhGcBlockSize=0;
    while (m_EHGCHeap)
    {
        newEhGcBlockSize += (m_EHGCHeap->blockSize - sizeof(CodeBlockHdr));
        m_EHGCHeap = m_EHGCHeap->next;
    }
    if (!NewEHGCBlock(newEhGcBlockSize))
    {
        m_EHGCHeap = NULL;
        m_EHGC_alloc_end = 0;
        m_EHGC_block_end = 0;
    }
}


/*****************************************************************************************************
                        EjitStub Manager
******************************************************************************************************/
EjitStubManager::EjitStubManager()
{ }

EjitStubManager::~EjitStubManager()
{ }
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\eeprofinterfaces.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
//*****************************************************************************
//*****************************************************************************

#ifndef _EEPROFINTERFACES_H_
#define _EEPROFINTERFACES_H_

#include <stddef.h>
#include "corprof.h"
#include "profilepriv.h"

#define PROF_USER_MASK 0xFFFFFFFF

class EEToProfInterface;
class ProfToEEInterface;
class Thread;
class Frame;
class MethodDesc;
class Object;
class Module;

/*
 * GetEEProfInterface is used to get the interface with the profiler code.
 */
typedef void __cdecl GETEETOPROFINTERFACE(EEToProfInterface **ppEEProf);

/*
 * SetProfEEInterface is used to provide the profiler code with an interface
 * to the profiler.
 */
typedef void __cdecl SETPROFTOEEINTERFACE(ProfToEEInterface *pProfEE);

/*
 * This virtual class provides the EE with access to the Profiling code.
 */
class EEToProfInterface
{
public:
    virtual ~EEToProfInterface()
    {
    }

    virtual HRESULT Init() = 0;

    virtual void Terminate(BOOL fProcessDetach) = 0;

    virtual HRESULT CreateProfiler(WCHAR *wszCLSID) = 0;

    virtual HRESULT ThreadCreated(ThreadID threadID) = 0;

    virtual HRESULT ThreadDestroyed(ThreadID threadID) = 0;

    virtual HRESULT ThreadAssignedToOSThread(ThreadID managedThreadId, DWORD osThreadId) = 0;

    virtual HRESULT Shutdown(ThreadID threadID) = 0;

    virtual HRESULT FunctionUnloadStarted(ThreadID threadID, FunctionID functionId) = 0;

    virtual HRESULT JITCompilationFinished(ThreadID threadID, FunctionID functionId,
                                           HRESULT hrStatus, BOOL fIsSafeToBlock) = 0;

    virtual HRESULT JITCompilationStarted(ThreadID threadId, FunctionID functionId,
                                          BOOL fIsSafeToBlock) = 0;

	virtual HRESULT JITCachedFunctionSearchStarted(
		/* [in] */	ThreadID threadId,
        /* [in] */  FunctionID functionId,
		/* [out] */ BOOL *pbUseCachedFunction) = 0;

	virtual HRESULT JITCachedFunctionSearchFinished(
		/* [in] */	ThreadID threadId,
		/* [in] */  FunctionID functionId,
		/* [in] */  COR_PRF_JIT_CACHE result) = 0;

    virtual HRESULT JITInlining(
        /* [in] */  ThreadID      threadId,
        /* [in] */  FunctionID    callerId,
        /* [in] */  FunctionID    calleeId,
        /* [out] */ BOOL         *pfShouldInline) = 0;

    virtual HRESULT JITFunctionPitched(ThreadID threadId,
                                             FunctionID functionId) = 0;

	virtual HRESULT ModuleLoadStarted(ThreadID threadID, ModuleID moduleId) = 0;

	virtual HRESULT ModuleLoadFinished(
        ThreadID    threadID, 
		ModuleID	moduleId, 
		HRESULT		hrStatus) = 0;

	virtual HRESULT ModuleUnloadStarted(
        ThreadID    threadID, 
        ModuleID    moduleId) = 0;

	virtual HRESULT ModuleUnloadFinished(
        ThreadID    threadID, 
        ModuleID	moduleId, 
		HRESULT		hrStatus) = 0;

    virtual HRESULT ModuleAttachedToAssembly( 
        ThreadID    threadID, 
        ModuleID    moduleId,
        AssemblyID  AssemblyId) = 0;

	virtual HRESULT ClassLoadStarted(
        ThreadID    threadID, 
		ClassID		classId) = 0;

	virtual HRESULT ClassLoadFinished(
        ThreadID    threadID, 
		ClassID		classId,
		HRESULT		hrStatus) = 0;

	virtual HRESULT ClassUnloadStarted(
        ThreadID    threadID, 
		ClassID		classId) = 0;

	virtual HRESULT ClassUnloadFinished(
        ThreadID    threadID, 
		ClassID		classId,
		HRESULT		hrStatus) = 0;
    
    virtual HRESULT AppDomainCreationStarted( 
        ThreadID    threadId, 
        AppDomainID appDomainId) = 0;

    virtual HRESULT AppDomainCreationFinished( 
        ThreadID    threadId, 
        AppDomainID appDomainId,
        HRESULT     hrStatus) = 0;

    virtual HRESULT AppDomainShutdownStarted( 
        ThreadID    threadId, 
        AppDomainID appDomainId) = 0;

    virtual HRESULT AppDomainShutdownFinished( 
        ThreadID    threadId, 
        AppDomainID appDomainId,
        HRESULT     hrStatus) = 0;

    virtual HRESULT AssemblyLoadStarted( 
        ThreadID    threadId, 
        AssemblyID  appDomainId) = 0;

    virtual HRESULT AssemblyLoadFinished( 
        ThreadID    threadId, 
        AssemblyID  appDomainId,
        HRESULT     hrStatus) = 0;

    virtual HRESULT AssemblyUnloadStarted( 
        ThreadID    threadId, 
        AssemblyID  appDomainId) = 0;

    virtual HRESULT AssemblyUnloadFinished( 
        ThreadID    threadId, 
        AssemblyID  appDomainId,
        HRESULT     hrStatus) = 0;

    virtual HRESULT UnmanagedToManagedTransition(
        FunctionID functionId,
        COR_PRF_TRANSITION_REASON reason) = 0;

    virtual HRESULT ManagedToUnmanagedTransition(
        FunctionID functionId,
        COR_PRF_TRANSITION_REASON reason) = 0;
        
    virtual HRESULT ExceptionThrown(
        ThreadID threadId,
        ObjectID thrownObjectId) = 0;

    virtual HRESULT ExceptionSearchFunctionEnter(
        ThreadID threadId,
        FunctionID functionId) = 0;

    virtual HRESULT ExceptionSearchFunctionLeave(
        ThreadID threadId) = 0;

    virtual HRESULT ExceptionSearchFilterEnter(
        ThreadID threadId,
        FunctionID funcId) = 0;

    virtual HRESULT ExceptionSearchFilterLeave(
        ThreadID threadId) = 0;

    virtual HRESULT ExceptionSearchCatcherFound (
        ThreadID threadId,
        FunctionID functionId) = 0;

    virtual HRESULT ExceptionOSHandlerEnter(
        ThreadID threadId,
        FunctionID funcId) = 0;

    virtual HRESULT ExceptionOSHandlerLeave(
        ThreadID threadId,
        FunctionID funcId) = 0;

    virtual HRESULT ExceptionUnwindFunctionEnter(
        ThreadID threadId,
        FunctionID functionId) = 0;

    virtual HRESULT ExceptionUnwindFunctionLeave(
        ThreadID threadId) = 0;
    
    virtual HRESULT ExceptionUnwindFinallyEnter(
        ThreadID threadId,
        FunctionID functionId) = 0;

    virtual HRESULT ExceptionUnwindFinallyLeave(
        ThreadID threadId) = 0;
    
    virtual HRESULT ExceptionCatcherEnter(
        ThreadID threadId,
        FunctionID functionId,
        ObjectID objectId) = 0;

    virtual HRESULT ExceptionCatcherLeave(
        ThreadID threadId) = 0;
    
    virtual HRESULT COMClassicVTableCreated( 
        /* [in] */ ClassID wrappedClassId,
        /* [in] */ REFGUID implementedIID,
        /* [in] */ void *pVTable,
        /* [in] */ ULONG cSlots,
        /* [in] */ ThreadID threadId) = 0;

    virtual HRESULT COMClassicVTableDestroyed( 
        /* [in] */ ClassID wrappedClassId,
        /* [in] */ REFGUID implementedIID,
        /* [in] */ void *pVTable,
        /* [in] */ ThreadID threadId) = 0;

    virtual HRESULT RuntimeSuspendStarted(
        COR_PRF_SUSPEND_REASON suspendReason,
        ThreadID    threadId) = 0;
    
    virtual HRESULT RuntimeSuspendFinished(
        ThreadID    threadId) = 0;
    
    virtual HRESULT RuntimeSuspendAborted(
        ThreadID    threadId) = 0;
    
    virtual HRESULT RuntimeResumeStarted(
        ThreadID    threadId) = 0;
    
    virtual HRESULT RuntimeResumeFinished(
        ThreadID    threadId) = 0;
    
    virtual HRESULT RuntimeThreadSuspended(
        ThreadID    suspendedThreadId,
        ThreadID    threadId) = 0;

    virtual HRESULT RuntimeThreadResumed(
        ThreadID    resumedThreadId,
        ThreadID    threadId) = 0;

    virtual HRESULT ObjectAllocated( 
        /* [in] */ ObjectID objectId,
        /* [in] */ ClassID classId) = 0;

    virtual HRESULT MovedReference(BYTE *pbMemBlockStart,
                                   BYTE *pbMemBlockEnd,
                                   ptrdiff_t cbRelocDistance,
                                   void *pHeapId) = 0;

    virtual HRESULT EndMovedReferences(void *pHeapId) = 0;

    virtual HRESULT RootReference(ObjectID objId,
                                  void *pHeapId) = 0;

    virtual HRESULT EndRootReferences(void *pHeapId) = 0;

    virtual HRESULT ObjectReference(ObjectID objId,
                                    ClassID clsId,
                                    ULONG cNumRefs,
                                    ObjectID *arrObjRef) = 0;

    virtual HRESULT AllocByClass(ObjectID objId, ClassID clsId, void *pHeapId) = 0;

    virtual HRESULT EndAllocByClass(void *pHeapId) = 0;

    virtual HRESULT RemotingClientInvocationStarted(ThreadID threadId) = 0;
    
    virtual HRESULT RemotingClientSendingMessage(ThreadID threadId, GUID *pCookie,
                                                 BOOL fIsAsync) = 0;

    virtual HRESULT RemotingClientReceivingReply(ThreadID threadId, GUID *pCookie,
                                                 BOOL fIsAsync) = 0;
    
    virtual HRESULT RemotingClientInvocationFinished(ThreadID threadId) = 0;

    virtual HRESULT RemotingServerReceivingMessage(ThreadID threadId, GUID *pCookie,
                                                   BOOL fIsAsync) = 0;
    
    virtual HRESULT RemotingServerInvocationStarted(ThreadID threadId) = 0;

    virtual HRESULT RemotingServerInvocationReturned(ThreadID threadId) = 0;
    
    virtual HRESULT RemotingServerSendingReply(ThreadID threadId, GUID *pCookie,
                                               BOOL fIsAsync) = 0;

    virtual HRESULT InitGUID() = 0;

    virtual void GetGUID(GUID *pGUID) = 0;

    virtual HRESULT ExceptionCLRCatcherFound() = 0;

    virtual HRESULT ExceptionCLRCatcherExecute() = 0;
};

enum PTR_TYPE
{
    PT_MODULE,
    PT_ASSEMBLY,
};

/*
 * This virtual class provides the Profiling code with access to the EE.
 */
class ProfToEEInterface
{
public:
    virtual ~ProfToEEInterface()
    {
    }

    virtual HRESULT Init() = 0;

    virtual void Terminate() = 0;

    virtual bool SetEventMask(DWORD dwEventMask) = 0;

    virtual void DisablePreemptiveGC(ThreadID threadId) = 0;
    virtual void EnablePreemptiveGC(ThreadID threadId) = 0;
    virtual BOOL PreemptiveGCDisabled(ThreadID threadId) = 0;

    virtual HRESULT GetHandleFromThread(ThreadID threadId,
                                        HANDLE *phThread) = 0;

    virtual HRESULT GetObjectSize(ObjectID objectId,
                                  ULONG *pcSize) = 0;

    virtual HRESULT IsArrayClass(
        /* [in] */  ClassID classId,
        /* [out] */ CorElementType *pBaseElemType,
        /* [out] */ ClassID *pBaseClassId,
        /* [out] */ ULONG   *pcRank) = 0;

    virtual HRESULT GetThreadInfo(ThreadID threadId,
                                  DWORD *pdwWin32ThreadId) = 0;

	virtual HRESULT GetCurrentThreadID(ThreadID *pThreadId) = 0;

    virtual HRESULT GetFunctionFromIP(LPCBYTE ip, FunctionID *pFunctionId) = 0;

    virtual HRESULT GetTokenFromFunction(FunctionID functionId,
                                         REFIID riid, IUnknown **ppOut,
                                         mdToken *pToken) = 0;

    virtual HRESULT GetCodeInfo(FunctionID functionId, LPCBYTE *pStart, 
                                ULONG *pcSize) = 0;

	virtual HRESULT GetModuleInfo(
		ModuleID	moduleId,
		LPCBYTE		*ppBaseLoadAddress,
		ULONG		cchName, 
		ULONG		*pcchName,
		WCHAR		szName[],
        AssemblyID  *pAssemblyId) = 0;

	virtual HRESULT GetModuleMetaData(
		ModuleID	moduleId,
		DWORD		dwOpenFlags,
		REFIID		riid,
		IUnknown	**ppOut) = 0;

	virtual HRESULT GetILFunctionBody(
		ModuleID	moduleId,
		mdMethodDef	methodid,
		LPCBYTE		*ppMethodHeader,
		ULONG		*pcbMethodSize) = 0;

	virtual HRESULT GetILFunctionBodyAllocator(
		ModuleID	moduleId,
		IMethodMalloc **ppMalloc) = 0;

	virtual HRESULT SetILFunctionBody(
		ModuleID	moduleId,
		mdMethodDef	methodid,
		LPCBYTE		pbNewILMethodHeader) = 0;

	virtual HRESULT SetFunctionReJIT(
		FunctionID	functionId) = 0;

	virtual HRESULT GetClassIDInfo( 
		ClassID classId,
		ModuleID *pModuleId,
		mdTypeDef *pTypeDefToken) = 0;

	virtual HRESULT GetFunctionInfo( 
		FunctionID functionId,
		ClassID *pClassId,
		ModuleID *pModuleId,
		mdToken *pToken) = 0;

	virtual HRESULT GetClassFromObject(
        ObjectID objectId,
        ClassID *pClassId) = 0;

	virtual HRESULT GetClassFromToken( 
		ModuleID moduleId,
		mdTypeDef typeDef,
		ClassID *pClassId) = 0;

	virtual HRESULT GetFunctionFromToken( 
		ModuleID moduleId,
		mdToken typeDef,
		FunctionID *pFunctionId) = 0;

    virtual HRESULT GetAppDomainInfo(
        AppDomainID appDomainId,
		ULONG  		cchName, 
		ULONG  		*pcchName,
        WCHAR		szName[],
        ProcessID   *pProcessId) = 0;

    virtual HRESULT GetAssemblyInfo(
        AssemblyID  assemblyId,
		ULONG		cchName, 
		ULONG		*pcchName,
		WCHAR		szName[],
        AppDomainID *pAppDomainId,
        ModuleID    *pModuleId) = 0;
        
    virtual HRESULT SetILInstrumentedCodeMap(
        FunctionID functionId,
        BOOL fStartJit,
        ULONG cILMapEntries,
        COR_IL_MAP rgILMapEntries[]) = 0;

    virtual HRESULT ForceGC() = 0;

	virtual HRESULT SetEnterLeaveFunctionHooks(
		FunctionEnter *pFuncEnter,
		FunctionLeave *pFuncLeave,
        FunctionTailcall *pFuncTailcall) = 0;

	virtual HRESULT SetEnterLeaveFunctionHooksForJit(
		FunctionEnter *pFuncEnter,
		FunctionLeave *pFuncLeave,
        FunctionTailcall *pFuncTailcall) = 0;

	virtual HRESULT SetFunctionIDMapper(
		FunctionIDMapper *pFunc) = 0;

    virtual HRESULT GetInprocInspectionInterfaceFromEE( 
        IUnknown **iu, bool fThisThread ) = 0;

    virtual HRESULT GetThreadContext(
        ThreadID threadId,
        ContextID *pContextId) = 0;

    virtual HRESULT BeginInprocDebugging(
        /* [in] */  BOOL   fThisThreadOnly,
        /* [out] */ DWORD *pdwProfilerContext) = 0;
    
    virtual HRESULT EndInprocDebugging(
        /* [in] */  DWORD  dwProfilerContext) = 0;

    virtual HRESULT GetILToNativeMapping(
                /* [in] */  FunctionID functionId,
                /* [in] */  ULONG32 cMap,
                /* [out] */ ULONG32 *pcMap,
                /* [out, size_is(cMap), length_is(*pcMap)] */
                    COR_DEBUG_IL_TO_NATIVE_MAP map[]) = 0;

    // This way we won't have to have 50 zillion of these (one per type)
    // Remember to set the ptr to NULL once the load/etc callback finishes.
    virtual HRESULT SetCurrentPointerForDebugger(
        void *ptr,
        PTR_TYPE ptrType) = 0;
};

void __stdcall ProfilerManagedToUnmanagedTransition(Frame *pFrame,
                                                          COR_PRF_TRANSITION_REASON reason);

void __stdcall ProfilerUnmanagedToManagedTransition(Frame *pFrame,
                                                          COR_PRF_TRANSITION_REASON reason);

void __stdcall ProfilerManagedToUnmanagedTransitionMD(MethodDesc *pMD,
                                                          COR_PRF_TRANSITION_REASON reason);

void __stdcall ProfilerUnmanagedToManagedTransitionMD(MethodDesc *pMD,
                                                          COR_PRF_TRANSITION_REASON reason);

#endif //_EEPROFINTERFACES_H_
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\ejitmgr.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*++

Module Name: EjitMgr.h

Abstract: Defines the EconojitManager Interface 

  Date        Author      Comments
----        ------      --------
2/15/99     sanjaybh     Created     


--*/


#ifndef H_EJITMGR
#define H_EJITMGR

#include <member-offset-info.h>

class EjitStubManager;

class EconoJitManager  :public IJitManager
{
        friend EjitStubManager;
        friend struct MEMBER_OFFSET_INFO(EconoJitManager);
public:
    EconoJitManager();
    ~EconoJitManager();

    virtual void            JitCode2MethodTokenAndOffset(SLOT currentPC, METHODTOKEN* pMethodToken, DWORD* pPCOffset, ScanFlag scanFlag=ScanReaderLock);
    virtual MethodDesc*     JitCode2MethodDesc(SLOT currentPC, ScanFlag scanFlag=ScanReaderLock);
    static  void            JitCode2MethodTokenAndOffsetStatic(SLOT currentPC, METHODTOKEN* pMethodToken, DWORD* pPCOffset);
    static  void            JitCode2Offset(SLOT currentPC, DWORD* pPCOffset);
    static  BYTE*           JitToken2StartAddressStatic(METHODTOKEN MethodToken);
    virtual BYTE*           JitToken2StartAddress(METHODTOKEN MethodToken, ScanFlag scanFlag=ScanReaderLock);
    MethodDesc*             JitTokenToMethodDesc(METHODTOKEN MethodToken, ScanFlag scanFlag=ScanReaderLock);
    virtual unsigned        InitializeEHEnumeration(METHODTOKEN MethodToken, EH_CLAUSE_ENUMERATOR* pEnumState);
    virtual EE_ILEXCEPTION_CLAUSE*          GetNextEHClause(METHODTOKEN MethodToken,
                                            EH_CLAUSE_ENUMERATOR* pEnumState, 
                                            EE_ILEXCEPTION_CLAUSE* pEHclause); 
    virtual void            ResolveEHClause(METHODTOKEN MethodToken,
                                            EH_CLAUSE_ENUMERATOR* pEnumState, 
                                            EE_ILEXCEPTION_CLAUSE* pEHClauseOut);
    void*                   GetGCInfo(METHODTOKEN methodToken);
    virtual void            RemoveJitData(METHODTOKEN methodToken);
    virtual void            Unload(MethodDesc *pFD);
    virtual void            Unload(AppDomain *pDomain) {}
    virtual BOOL            LoadJIT(LPCWSTR wzJITdll);
    virtual void            ResumeAtJitEH   (CrawlFrame* pCf, EE_ILEXCEPTION_CLAUSE *EHClausePtr, DWORD nestingLevel, Thread *pThread, BOOL unwindStack);
    virtual int             CallJitEHFilter (CrawlFrame* pCf, EE_ILEXCEPTION_CLAUSE *EHClausePtr, DWORD nestingLevel, OBJECTREF thrownObj);
    virtual void            CallJitEHFinally(CrawlFrame* pCf, EE_ILEXCEPTION_CLAUSE *EHClausePtr, DWORD nestingLevel);

    HRESULT                 alloc(size_t code_len, unsigned char** pCode,
                                            size_t EHinfo_len, unsigned char** pEHinfo,
                                            size_t GCinfo_len, unsigned char** pGCinfo,
                                            MethodDesc* pMethodDescriptor);
    
    BOOL SupportsPitching(void)
    {
        return TRUE;
    }
    
    BOOL IsMethodInfoValid(METHODTOKEN methodToken)
    {
        return (BOOL) !(((JittedMethodInfo*)methodToken)->flags.EHandGCInfoPitched);
    }


    _inline virtual BOOL    IsStub(const BYTE* address)
    {
        // It's important to keep in mind that IsStub is used by the EJitMgr
        // and thus isn't concerned with thunks.  As opposed to CheckIsStub
        // on the EjitStubManager, which (being used by the debugger) is 
        // concerned with thunks.
        return IsInStub(address, FALSE);
    }

    virtual const BYTE*     FollowStub(const BYTE* address);

    // The following three should eventually be removed
    _inline CodeHeader*         allocCode(MethodDesc* pFD, size_t numBytes)
    {
        _ASSERTE(!"NYI - should not get here!");
        return NULL;
    }

    _inline BYTE*               allocGCInfo(CodeHeader* pCodeHeader, DWORD numBytes)
    {
        _ASSERTE(!"NYI - should not get here!");
        return NULL;
    }

    _inline EE_ILEXCEPTION*     allocEHInfo(CodeHeader* pCodeHeader, unsigned numClauses)
    {
        _ASSERTE(!"NYI - should not get here!");
        return NULL;
    }
   

    BOOL	            PitchAllJittedMethods(unsigned minSpaceRequired,unsigned minCommittedSpaceRequired,BOOL PitchEHInfo, BOOL PitchGCInfo);
    // The following are called by the stackwalker callback
    void                AddPreserveCandidate(unsigned threadIndex,unsigned cThreads,unsigned candidateIndex,METHODTOKEN methodToken);
    void                CreateThunk(LPVOID* pHijackLocation,BYTE retTypeProtect, METHODTOKEN methodToken);
    static BOOL         IsThunk(BYTE* address);
    static void         SetThunkBusy(BYTE* address, METHODTOKEN methodToken);
    friend BYTE*        __cdecl RejitCalleeMethod(struct MachState ms, void* arg2, void* arg1, BYTE* thunkReturnAddr);
    friend BYTE*        __cdecl RejitCallerMethod(struct MachState ms, void* retLow, void* retHigh, BYTE* thunkReturnAddr);

    static BOOL         IsInStub(const BYTE* address, BOOL fSearchThunks);
    static BOOL         IsCodePitched(const BYTE* address);
    inline virtual BYTE* GetNativeEntry(BYTE* startAddress)
    {
        return (startAddress + 5 + *((DWORD*) (startAddress+1)));
    }


protected:
    typedef BYTE            GC_INFO;
    typedef EE_ILEXCEPTION  EH_INFO;
    typedef struct {
        EH_INFO*         phdrJitEHInfo;
        GC_INFO*         phdrJitGCInfo;
    } EH_AND_GC_INFO;
    
    typedef union {
        EH_AND_GC_INFO*   EHAndGC;
        EH_INFO*   EH;
        GC_INFO*   GC;
    } EH_OR_GC_INFO;
    
    typedef struct {
        MethodDesc *    pMethodDescriptor;
    } CodeHeader;

	typedef struct _link {			// a generic structure to implement singly linked list
		struct _link* next;
	} Link;

#ifdef _X86_
#define JMP_OPCODE   0xE9
#define CALL_OPCODE  0xE8
#define BREAK_OPCODE 0xCC
    struct JittedMethodInfo;
    // structure to create linked list of (jmi,EhGcInfo_len) pairs for very large methods.
    struct LargeEhGcInfoList {
        LargeEhGcInfoList *         next;
        JittedMethodInfo*           jmi;
        unsigned                    length;

        LargeEhGcInfoList(JittedMethodInfo* j, unsigned int l)
        {
            jmi = j;
            length = l;
        }
    };

    struct JittedMethodInfo {
        BYTE      JmpInstruction[5]  ;          // this is the start address that is exposed to the EE so it can
                                                // patch all the vtables, etc. It contains a jump to the real start.
        struct {
            __int8 JittedMethodPitched: 1 ;   // if 1, the jitted method has been pitched
            __int8 MarkedForPitching  : 1 ;   // if 1, the jitted method is scheduled to be pitched, but has not been pitched yet
            __int8 EHInfoExists       : 1 ;   // if 0, no exception info in this method 
            __int8 GCInfoExists       : 1 ;   // if 0, no gc info in this method
            __int8 EHandGCInfoPitched : 1 ;   // (if at least one of EHInfo or GCInfo exists) if 1, the info has been pitched
            __int8 Unused             : 3 ;
        } flags;
        unsigned short EhGcInfo_len;
        union {
            MethodDesc* pMethodDescriptor;      // If code pitched
            CodeHeader* pCodeHeader;            // If not pitched : points to code header which points to the methoddesc. Code begins after the code header
        } u1;
        union {
            BYTE*       pEhGcInfo;        // If code pitched: points to beginning of EH/GC info
            BYTE*       pCodeEnd;               // If not pitched : points to end of jitted code for this method. 
        } u2;
        //COR_ILMETHOD*  ILHeader;                 // ptr to IL code so method can be rejitted when we go to 2-byte methoddescs
        void SetEhGcInfo_len(unsigned int len, LargeEhGcInfoList** ppEhGcInfoList) 
        {
            //_ASSERTE(adjusted_EhGc_len < 0xffff);
            if (len < 0xffff)
            {
                EhGcInfo_len = (unsigned short)len;
            }
            else
            {
                EhGcInfo_len = 0xffff;
                // @TODO: Check for out of memory
                LargeEhGcInfoList* elem = new LargeEhGcInfoList(this,len);
                _ASSERTE(elem != NULL);
                elem->next = *ppEhGcInfoList;
                *ppEhGcInfoList = elem;
            }
        }

        unsigned GetEhGcInfo_len(LargeEhGcInfoList* pEhGcInfoList)
        {
            if (EhGcInfo_len < 0xffff)
                return EhGcInfo_len;
            LargeEhGcInfoList* ptr = pEhGcInfoList;
            while (ptr) 
            {
                if (ptr->jmi == this)
                    return ptr->length;
                ptr = ptr->next;
            }
            _ASSERTE(!"Error: EhGc length info corrupted");
            return 0;
        }

    };
#elif _ALPHA_
// @TODO: ALPHA determine the following opcodes and the JmpInstruction contents
#define JMP_OPCODE   0xE9
#define CALL_OPCODE  0xE8
#define BREAK_OPCODE 0xCC
    struct JittedMethodInfo;
    // structure to create linked list of (jmi,EhGcInfo_len) pairs for very large methods.
    struct LargeEhGcInfoList {
        LargeEhGcInfoList *         next;
        JittedMethodInfo*           jmi;
        unsigned                    length;

        LargeEhGcInfoList(JittedMethodInfo* j, unsigned int l)
        {
            jmi = j;
            length = l;
        }
    };
    struct JittedMethodInfo {
        BYTE      JmpInstruction[14]  ;         // this is the start address that is exposed to the EE so it can
                                                // patch all the vtables, etc. It contains a jump to the real start.
                                                // We don't need this much space on alpha, but we want this structure
                                                // to be a multiple of 16 bytes to properly allocate it on the stack.
        struct {
            __int8 JittedMethodPitched: 1 ;   // if 1, the jitted method has been pitched
            __int8 MarkedForPitching  : 1 ;   // if 1, the jitted method is scheduled to be pitched, but has not been pitched yet
            __int8 EHInfoExists       : 1 ;   // if 0, no exception info in this method 
            __int8 GCInfoExists       : 1 ;   // if 0, no gc info in this method
            __int8 EHandGCInfoPitched : 1 ;   // (if at least one of EHInfo or GCInfo exists) if 1, the info has been pitched
            __int8 Unused             : 3 ;
        } flags;
        unsigned short EhGcInfo_len;
        union {
            MethodDesc* pMethodDescriptor;      // If code pitched
            CodeHeader* pCodeHeader;            // If not pitched : points to code header which points to the methoddesc. Code begins after the code header
        } u1;
        union {
            BYTE*       pEhGcInfo;        // If code pitched: points to beginning of EH/GC info
            BYTE*       pCodeEnd;               // If not pitched : points to end of jitted code for this method. 
        } u2;
        //COR_ILMETHOD*  ILHeader;                 // ptr to IL code so method can be rejitted when we go to 2-byte methoddescs
        void SetEhGcInfo_len(unsigned int len, LargeEhGcInfoList** ppEhGcInfoList) 
        {
            //_ASSERTE(adjusted_EhGc_len < 0xffff);
            if (len < 0xffff)
            {
                EhGcInfo_len = (unsigned short)len;
            }
            else
            {
                EhGcInfo_len = 0xffff;
                // @TODO: Check for out of memory
                LargeEhGcInfoList* elem = new LargeEhGcInfoList(this,len);
                _ASSERTE(elem != NULL);
                elem->next = *ppEhGcInfoList;
                *ppEhGcInfoList = elem;
            }
        }

        unsigned GetEhGcInfo_len(LargeEhGcInfoList* pEhGcInfoList)
        {
            if (EhGcInfo_len < 0xffff)
                return EhGcInfo_len;
            LargeEhGcInfoList* ptr = pEhGcInfoList;
            while (ptr) 
            {
                if (ptr->jmi == this)
                    return ptr->length;
                ptr = ptr->next;
            }
            _ASSERTE(!"Error: EhGc length info corrupted");
            return 0;
        }

    };

#elif _IA64_
// @TODO: IA64 determine the following opcodes and the JmpInstruction contents
#define JMP_OPCODE   0xE9
#define CALL_OPCODE  0xE8
#define BREAK_OPCODE 0xCC
    struct JittedMethodInfo;
    // structure to create linked list of (jmi,EhGcInfo_len) pairs for very large methods.
    struct LargeEhGcInfoList {
        LargeEhGcInfoList *         next;
        JittedMethodInfo*           jmi;
        unsigned                    length;

        LargeEhGcInfoList(JittedMethodInfo* j, unsigned int l)
        {
            jmi = j;
            length = l;
        }
    };
    struct JittedMethodInfo {
        BYTE      JmpInstruction[14]  ;         // this is the start address that is exposed to the EE so it can
                                                // patch all the vtables, etc. It contains a jump to the real start.
                                                // We don't need this much space on alpha, but we want this structure
                                                // to be a multiple of 16 bytes to properly allocate it on the stack.
        struct {
            __int8 JittedMethodPitched: 1 ;   // if 1, the jitted method has been pitched
            __int8 MarkedForPitching  : 1 ;   // if 1, the jitted method is scheduled to be pitched, but has not been pitched yet
            __int8 EHInfoExists       : 1 ;   // if 0, no exception info in this method 
            __int8 GCInfoExists       : 1 ;   // if 0, no gc info in this method
            __int8 EHandGCInfoPitched : 1 ;   // (if at least one of EHInfo or GCInfo exists) if 1, the info has been pitched
            __int8 Unused             : 3 ;
        } flags;
        unsigned short EhGcInfo_len;
        union {
            MethodDesc* pMethodDescriptor;      // If code pitched
            CodeHeader* pCodeHeader;            // If not pitched : points to code header which points to the methoddesc. Code begins after the code header
        } u1;
        union {
            BYTE*       pEhGcInfo;        // If code pitched: points to beginning of EH/GC info
            BYTE*       pCodeEnd;               // If not pitched : points to end of jitted code for this method. 
        } u2;
        //COR_ILMETHOD*  ILHeader;                 // ptr to IL code so method can be rejitted when we go to 2-byte methoddescs
        void SetEhGcInfo_len(unsigned int len, LargeEhGcInfoList** ppEhGcInfoList) 
        {
            //_ASSERTE(adjusted_EhGc_len < 0xffff);
            if (len < 0xffff)
            {
                EhGcInfo_len = (unsigned short)len;
            }
            else
            {
                EhGcInfo_len = 0xffff;
                LargeEhGcInfoList* elem = new LargeEhGcInfoList(this,len);
                elem->next = *ppEhGcInfoList;
                *ppEhGcInfoList = elem;
            }
        }

        unsigned GetEhGcInfo_len(LargeEhGcInfoList* pEhGcInfoList)
        {
            if (EhGcInfo_len < 0xffff)
                return EhGcInfo_len;
            LargeEhGcInfoList* ptr = pEhGcInfoList;
            while (ptr) 
            {
                if (ptr->jmi == this)
                    return ptr->length;
                ptr = ptr->next;
            }
            _ASSERTE(!"Error: EhGc length info corrupted");
            return 0;
        }

    };

#else //!_ALPHA_ && !_X86_ && !_IA64_

    // the structure must be redefined for each architecture since the jump 
    // instruction size is architecture specific
#endif _X86_

    typedef struct {
        MethodDesc*     pMD;
        BYTE*           pCodeEnd;
    } PCToMDMap;

    typedef struct PC2MDBlock_tag {
        struct PC2MDBlock_tag *next;
    } PC2MDBlock;

#define JMIT_BLOCK_SIZE PAGE_SIZE           // size of individual blocks of JMITs that are chained together                     
    typedef struct JittedMethodInfoHdr_tag {
        struct JittedMethodInfoHdr_tag* next;          // ptr to next block  
    } JittedMethodInfoHdr; 

    typedef struct CodeBlock_tag {
        unsigned char*      startFree;       // beginning of free space in code region
        unsigned char*      end;             // end of code region
    } CodeBlockHdr; 

    typedef struct EHGCBlock_tag {
        struct EHGCBlock_tag* next;          // ptr to next EHGC block  
        unsigned blockSize;
    } EHGCBlockHdr;
     
    typedef EHGCBlockHdr HeapList;

    typedef struct Thunk_tag {
        BYTE                CallInstruction[5]; // ***IMPORTANT: This should be first field. space to insert a call instruction here
        bool                Busy;               // used to mark thunks is use, is cleared before each stackwalk
        bool                LinkedInFreeList;   // set when thunks are threaded into a free list, to avoid being linked
                                                // twice during thunk garbage collection
        BYTE                retTypeProtect;     // if true, then return value must be protected
        union {
            JittedMethodInfo*   pMethodInfo;    
            struct Thunk_tag*   next;           // used to create linked list of free thunks
        } u;
        unsigned            relReturnAddress;
    }  PitchedCodeThunk;

        private:

#define THUNK_BEGIN_MASK 0xfffffff0

#define PAGE_SIZE   0x1000
#define CODE_BLOCK_SIZE PAGE_SIZE
#define EHGC_BLOCK_SIZE PAGE_SIZE
#define INITIAL_PC2MD_MAP_SIZE ((EHGC_BLOCK_SIZE/sizeof(JittedMethodInfo)) * sizeof(PCToMDMap))
#define THUNK_BLOCK_SIZE PAGE_SIZE
#define THUNKS_PER_BLOCK  ((PAGE_SIZE - sizeof(unsigned) - sizeof(void*))/ sizeof(PitchedCodeThunk))
#define DEFAULT_CODE_HEAP_RESERVED_SIZE 0x10000         
#define MINIMUM_VIRTUAL_ALLOC_SIZE 0x10000 // 64K
#define CODE_HEAP_RESERVED_INCREMENT_LIMIT  0x10000      // we double the reserved code heap size until we hit this limit
                                                         // after which we increase in this amount each time
#define DEFAULT_MAX_PRESERVES_PER_THREAD 10
#define DEFAULT_PRESERVED_EHGCINFO_SIZE 10

#define TARGET_MIN_JITS_BETWEEN_PITCHES 500 
#define MINIMUM_PITCH_OVERHEAD  10         // in milliseconds

    typedef struct ThunkBlockTag{
        struct ThunkBlockTag*   next;
        size_t               Fillers[16 / sizeof size_t - 1];   // This is to ensure that all thunks start at 16 byte boundaries
    } ThunkBlock;

private:
    // JittedMethodInfo is kept as a linked list of 1 page blocks
    // Within each block there is a table of jittedMethodInfo structs. 
    typedef JittedMethodInfo* pJittedMethodInfo;
    static JittedMethodInfoHdr* m_JittedMethodInfoHdr;
    static JittedMethodInfo*    m_JMIT_free;         // points at start of next available entry
	static Link*				m_JMIT_freelist;	 // pointer to head of list of freed entries
    static PCToMDMap*           m_PcToMdMap;         
    static unsigned             m_PcToMdMap_len;
    static unsigned             m_PcToMdMap_size;
    static PC2MDBlock*          m_RecycledPC2MDMaps;  // a linked list of PC2MDMaps that are freed at pitch cycles

#define  m_JMIT_size  (PAGE_SIZE-sizeof(JittedMethodInfoHdr))/sizeof(JittedMethodInfo)
    static MethodDesc*          JitCode2MethodDescStatic(SLOT currentPC);
    
    static JittedMethodInfo*    JitCode2MethodInfo(SLOT currentPC);
    static MethodDesc*          JitMethodInfo2MethodDesc(JittedMethodInfo* jittedMethodInfo);
    JittedMethodInfo*           Token2JittedMethodInfo(METHODTOKEN methodToken);
    static BYTE*                JitMethodInfo2EhGcInfo(JittedMethodInfo* jmi);
    JittedMethodInfo*           MethodDesc2MethodInfo(MethodDesc* pMethodDesc);
	static JittedMethodInfo*    JitCode2MethodTokenInEnCMode(SLOT currentPC);

    static HINSTANCE           m_JITCompiler;
    static BYTE*               m_CodeHeap;
    static BYTE*               m_CodeHeapFree;           // beginning of free space in code heap
    static unsigned            m_CodeHeapTargetSize;     // this size is increased as needed until the global max is reached
    static unsigned            m_CodeHeapCommittedSize;  // this is a number between 0 and m_CodeHeapReservedSize
    static unsigned            m_CodeHeapReservedSize;
    static unsigned            m_CodeHeapReserveIncrement;
 
    static EHGCBlockHdr*       m_EHGCHeap;
    static unsigned char*      m_EHGC_alloc_end;      // ptr to next free byte in current block
    static unsigned char*      m_EHGC_block_end;      // ptr to end of current block

    static Crst*               m_pHeapCritSec;
    static BYTE                m_HeapCritSecInstance[sizeof(Crst)];
    static Crst*               m_pRejitCritSec;
    static BYTE                m_RejitCritSecInstance[sizeof(Crst)];
    static Crst*               m_pThunkCritSec;          // used to synchronize concurrent creation of thunks
    static BYTE                m_ThunkCritSecInstance[sizeof(Crst)];
    static ThunkBlock*         m_ThunkBlocks;
    static PitchedCodeThunk*   m_FreeThunkList;
    static unsigned            m_cThunksInCurrentBlock;         // total number of thunks in current block
    static EjitStubManager*    m_stubManager;
    static unsigned            m_cMethodsJitted;                // number of methods jitted since last pitch
    static unsigned            m_cCalleeRejits;                 // number of callees rejitted since last pitch
    static unsigned            m_cCallerRejits;                 // number of callers rejitted since last pitch
    static JittedMethodInfo**  m_PreserveCandidates;            // methods that are possible candidates for pitching
    static unsigned            m_PreserveCandidates_size;       // current size of m_PreserveCandidates array
    static JittedMethodInfo**  m_PreserveEhGcInfoList;          // list of EhGc info that needs to be preserved during pitching
    static unsigned            m_cPreserveEhGcInfoList;         // count of members in m_PreserveEhGcInfoList
    static unsigned            m_PreserveEhGcInfoList_size;     // current size of m_PreserveEhGcInfoList
    static unsigned            m_MaxUnpitchedPerThread;         // maximum number of methods in each thread that will be pitched
    static LargeEhGcInfoList*  m_LargeEhGcInfo;                 // linked list of structures that encode EhGcInfo length >= 64K
    
    // Clock ticks measurement for code pitch heuristics
#ifdef _X86_

    typedef __int64 TICKS;
    static TICKS GET_TIMESTAMP() 
    {
        LARGE_INTEGER lpPerfCtr;
        BOOL exists = QueryPerformanceCounter(&lpPerfCtr);
        _ASSERTE(exists);
        return lpPerfCtr.QuadPart;
    }
    TICKS TICK_FREQUENCY()
    {
        LARGE_INTEGER lpPerfCtr;
        BOOL exists = QueryPerformanceFrequency(&lpPerfCtr);
        _ASSERTE(exists);
        return lpPerfCtr.QuadPart;
    }
#else
    typedef unsigned TICKS;
#define GET_TIMESTAMP() GetTickCount()
#define TICK_FREQUENCY() 1000.0

#endif

    static TICKS               m_CumulativePitchOverhead;       // measures the total overhead due to pitching and rejitting
    static TICKS               m_AveragePitchOverhead;
    static unsigned            m_cPitchCycles;
    static TICKS               m_EjitStartTime;

#ifdef _DEBUG
    static DWORD               m_RejitLock_Holder;  
    static DWORD               m_AllocLock_Holder; 
#endif
    _inline BOOL IsMethodPitched(METHODTOKEN token)
    {
        return ((JittedMethodInfo*)token)->flags.JittedMethodPitched;
    }
    // Memory management support 
    static BOOL                 m_PitchOccurred;                // initially false, set to true after the first pitch
    unsigned char*              allocCodeBlock(size_t blockSize);
    void                        freeCodeBlock(size_t blockSize);


    unsigned                    minimum(unsigned x, unsigned y);
    unsigned                    InitialCodeHeapSize();
    void                        EconoJitManager::ReplaceCodeHeap(unsigned newReservedSize,unsigned newCommittedSize);
    unsigned                    EconoJitManager::RoundToPageSize(unsigned size);
    BOOL                        EconoJitManager::GrowCodeHeapReservedSpace(unsigned newReservedSize,unsigned minCommittedSize);
    BOOL                        EconoJitManager::SetCodeHeapCommittedSize(unsigned size);

    
    inline size_t     EconoJitManager::usedMemoryInCodeHeap()
    {   
        return (m_CodeHeapFree - m_CodeHeap); 
    }
    inline size_t     EconoJitManager::availableMemoryInCodeHeap()
    {
        return (m_CodeHeapCommittedSize - usedMemoryInCodeHeap());
    }

    inline BOOL EconoJitManager::OutOfCodeMemory(size_t newRequest)
    {
        return (newRequest > availableMemoryInCodeHeap());
    }

    BOOL                NewEHGCBlock(unsigned minsize);
    unsigned char*      allocEHGCBlock(size_t blockSize);
    
    inline size_t EconoJitManager::availableEHGCMemory()
    {
        return (m_EHGC_block_end - m_EHGC_alloc_end);
    }

    void                ResetEHGCHeap();
    void                InitializeCodeHeap();
    static BYTE*        RejitMethod(JittedMethodInfo* pJMI, unsigned returnOffset);
    void                StackWalkForCodePitching();
    unsigned            GetThreadCount();
    void                MarkHeapsForPitching();
    void                UnmarkPreservedCandidates(unsigned minSpaceRequired);
    unsigned            PitchMarkedCode();          // returns number of methods pitched
    void                MovePreservedMethods();
    unsigned            CompressPreserveCandidateArray(unsigned size);
    void                MovePreservedMethod(JittedMethodInfo* jmi);
    int static __cdecl  compareJMIstart( const void *arg1, const void *arg2 );

    void                MoveAllPreservedEhGcInfo();
    void                MoveSinglePreservedEhGcInfo(JittedMethodInfo* jmi);
    void                AddToPreservedEhGcInfoList(JittedMethodInfo* jmi);
    int static __cdecl  compareEhGcPtr( const void *arg1, const void *arg2 );
    void                growPreservedEhGcInfoList();
    void static         CleanupLargeEhGcInfoList();

    __inline void AddPitchOverhead(TICKS time)
    {
        // this is always called within a single user critical section  
#ifndef _X86_
        _ASSERTE(!"NYI");
        if (time == 0) 
            time = 1;       // make sure we allocate at least a millisecond for each pitch overhead
#endif
        m_CumulativePitchOverhead += time;
        m_cPitchCycles++;
        m_AveragePitchOverhead = (m_AveragePitchOverhead + m_CumulativePitchOverhead)/(m_cPitchCycles+1);
        if (m_AveragePitchOverhead == 0)
            m_AveragePitchOverhead = 1;
    }
    __inline static void AddRejitOverhead(TICKS time)
    {
#ifndef _X86_
        _ASSERTE(!"NYI");
        if (time == 0) 
            time = 1;       // make sure we allocate at least a millisecond for each pitch overhead
#endif
        // this is always called within a single user critical section        
        m_CumulativePitchOverhead += time;
    }

    __inline TICKS EconoJitManager::PitchOverhead()
    {
        // this is always called within a single user critical section  
        TICKS totalExecTime = GET_TIMESTAMP()-m_EjitStartTime;
        if (totalExecTime > m_CumulativePitchOverhead)      // this can only be possible if we don't have a high resolution ctr
            totalExecTime -= m_CumulativePitchOverhead;
        return ((m_CumulativePitchOverhead+m_AveragePitchOverhead)*100)/(totalExecTime);
    }

	JittedMethodInfo*	GetNextJmiEntry();
    BOOL                growJittedMethodInfoTable();
    BOOL                growPC2MDMap();
    BOOL                AddPC2MDMap(MethodDesc* pMD, BYTE* pCodeEnd);
    void __inline ResetPc2MdMap()
    {
        m_PcToMdMap_len = 0;
        // delete each element in m_RecycledPC2MDMaps
        while (m_RecycledPC2MDMaps)
        {
            PCToMDMap* temp = (PCToMDMap*) m_RecycledPC2MDMaps;
            m_RecycledPC2MDMaps = m_RecycledPC2MDMaps->next;
            delete[] temp;
        }
        
    }
    PitchedCodeThunk*   GetNewThunk();
    void                MarkThunksForRelease(); 
    void                GarbageCollectUnusedThunks();
    void                growPreserveCandidateArray(unsigned numberOfCandidates);

#ifdef _DEBUG
    void                SetBreakpointsInUnusedHeap();
    void                VerifyAllCodePitched();
    void                LogAction(MethodDesc* pMD, LPCUTF8 action,void* codeStart ,void* codeEnd);

#endif

#if defined(ENABLE_PERF_COUNTERS)
    int                 GetCodeHeapSize();
    int                 GetEHGCHeapSize();
#endif // ENABLE_PERF_COUNTERS
};

const BYTE *GetCallThunkAddress();
const BYTE *GetRejitThunkAddress();

class EjitStubManager :public StubManager
{
public:
    EjitStubManager();
    ~EjitStubManager();
protected:

        // It's important to keep in mind that CheckIsStub
        // which (being used by the debugger) is 
        // concerned with thunks.  As opposed to IsStub, which 
        // is used by the EconoJitManager and thus isn't concerned with thunks.  
    __inline BOOL CheckIsStub(const BYTE *stubStartAddress)
    {
        return EconoJitManager::IsInStub(stubStartAddress, TRUE);
    }

        __inline virtual BOOL DoTraceStub(const BYTE *stubStartAddress, 
                                                         TraceDestination *trace)
    {
        trace->type = TRACE_MANAGED;
        trace->address = stubStartAddress + 5 +
            *((DWORD*) (stubStartAddress+1));

                 if (trace->address == GetCallThunkAddress() )
        {
            MethodDesc* pMD = EconoJitManager::
                JitMethodInfo2MethodDesc(
                   (EconoJitManager::JittedMethodInfo *)
                                stubStartAddress);
            trace->type = TRACE_UNJITTED_METHOD;
            trace->address = (const BYTE*)pMD;
        }

        if ( trace->address == GetRejitThunkAddress())
        {
                        _ASSERTE( offsetof( EconoJitManager::PitchedCodeThunk,CallInstruction)==0);
        
            EconoJitManager::PitchedCodeThunk *pct =
                                (EconoJitManager::PitchedCodeThunk *)stubStartAddress;
                                
                        EconoJitManager::JittedMethodInfo *jmi = 
                                (EconoJitManager::JittedMethodInfo *)pct->u.pMethodInfo;

                        _ASSERTE( jmi != NULL );

                        if (jmi->flags.JittedMethodPitched)
                        {
                    trace->type = TRACE_UNJITTED_METHOD;
                trace->address = (const BYTE*)EconoJitManager
                        ::JitMethodInfo2MethodDesc(jmi);
            }
                        else
                        {
                                trace->type = TRACE_MANAGED;
                                trace->address = ((const BYTE*)jmi->u1.pCodeHeader) +
                                        sizeof( MethodDesc *);
                        }
                }
        return true;
    }
    MethodDesc *Entry2MethodDesc(const BYTE *StubStartAddress, MethodTable *pMT)  {return NULL;}
};


#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\eventstore.hpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#ifndef __EventStore_hpp
#define __EventStore_hpp

// Used inside Thread class to chain all events that a thread is waiting for by Object::Wait
struct WaitEventLink {
    SyncBlock      *m_WaitSB;
    HANDLE          m_EventWait;
    Thread         *m_Thread;       // Owner of this WaitEventLink.
    WaitEventLink  *m_Next;         // Chain to the next waited SyncBlock.
    SLink           m_LinkSB;       // Chain to the next thread waiting on the same SyncBlock.
    DWORD           m_RefCount;     // How many times Object::Wait is called on the same SyncBlock.
};

HANDLE GetEventFromEventStore();
void StoreEventToEventStore(HANDLE hEvent);
void InitEventStore();
void TerminateEventStore();

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\eventstore.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#include "common.h"
#include "EventStore.hpp"

// A class to maintain a pool of available events.

const int EventStoreLength = 8;
class EventStore
{
public: 
    EventStore ()
        : m_Store (NULL), m_pEventStoreCrst (NULL)
    {
    }

    ~EventStore ()
    {
    }

    void Init()
    {
        m_pEventStoreCrst = new (m_EventStoreCrstMemory) Crst ("EventStore", CrstEventStore);
    }

    void Destroy()
    {
        _ASSERTE (g_fEEShutDown);
        
        if (m_pEventStoreCrst) {
            m_pEventStoreCrst->Destroy ();
        }
        
        if (m_Store == NULL) {
            return;
        }

        EventStoreElem *walk;
        EventStoreElem *next;

        walk = m_Store;
        m_Store = NULL;
        while (walk) {
            next = walk->next;
            delete (walk);
            walk = next;
        }
    }

    void StoreHandleForEvent (HANDLE handle)
    {
        _ASSERTE (handle);
        CLR_CRST (m_pEventStoreCrst);
        if (m_Store == NULL) {
            m_Store = new EventStoreElem ();
        }
        EventStoreElem *walk;
#ifdef _DEBUG
        // See if we have some leakage.
        LONG count = 0;
        walk = m_Store; 
        while (walk) {
            count += walk->AvailableEventCount();
            walk = walk->next;
        }
        // The number of events stored in the pool should be small.
        _ASSERTE (count <= g_pThreadStore->ThreadCountInEE() * 2 + 10);
#endif
        walk = m_Store;        
        while (walk) {
            if (walk->StoreHandleForEvent (handle) )
                return;
            if (walk->next == NULL) {
                break;
            }
            walk = walk->next;
        }
        walk->next = new EventStoreElem ();
        walk->next->hArray[0] = handle;
    }

    HANDLE GetHandleForEvent ()
    {
        HANDLE handle;
        CLR_CRST (m_pEventStoreCrst);
        EventStoreElem *walk = m_Store;
        while (walk) {
            handle = walk->GetHandleForEvent();
            if (handle != INVALID_HANDLE_VALUE) {
                return handle;
            }
            walk = walk->next;
        }
        handle = ::WszCreateEvent (NULL, TRUE/*ManualReset*/,
                                          TRUE/*Signalled*/, NULL);
        if (handle == NULL) 
            handle = INVALID_HANDLE_VALUE;

        _ASSERTE (handle != INVALID_HANDLE_VALUE);
        return handle;
    }

private:
    struct EventStoreElem
    {
        HANDLE hArray[EventStoreLength];
        EventStoreElem *next;
        
        EventStoreElem ()
        {
            next = NULL;
            for (int i = 0; i < EventStoreLength; i ++) {
                hArray[i] = INVALID_HANDLE_VALUE;
            }
        }

        ~EventStoreElem ()
        {
            for (int i = 0; i < EventStoreLength; i++) {
                if (hArray[i]) {
                    CloseHandle (hArray[i]);
                    hArray[i] = INVALID_HANDLE_VALUE;
                }
            }
        }

        // Store a handle in the current EventStoreElem.  Return TRUE if succeessful.
        // Return FALSE if failed due to no free slot.
        BOOL StoreHandleForEvent (HANDLE handle)
        {
            int i;
            for (i = 0; i < EventStoreLength; i++) {
                if (hArray[i] == INVALID_HANDLE_VALUE) {
                    hArray[i] = handle;
                    return TRUE;
                }
            }
            return FALSE;
        }

        // Get a handle from the current EventStoreElem.
        HANDLE GetHandleForEvent ()
        {
            int i;
            for (i = 0; i < EventStoreLength; i++) {
                if (hArray[i] != INVALID_HANDLE_VALUE) {
                    HANDLE handle = hArray[i];
                    hArray[i] = INVALID_HANDLE_VALUE;
                    return handle;
                }
            }

            return INVALID_HANDLE_VALUE;
        }

#ifdef _DEBUG
        LONG AvailableEventCount ()
        {
            LONG count = 0;
            for (int i = 0; i < EventStoreLength; i++) {
                if (hArray[i] != INVALID_HANDLE_VALUE) {
                    count ++;
                }
            }
            return count;
        }
#endif
    };

    EventStoreElem *m_Store;
    
    // Critical section for adding and removing event used for Object::Wait
    Crst        *m_pEventStoreCrst;
    BYTE         m_EventStoreCrstMemory[sizeof(Crst)];
};

static EventStore s_EventStore;
 
HANDLE GetEventFromEventStore()
{
    return s_EventStore.GetHandleForEvent();
}

void StoreEventToEventStore(HANDLE hEvent)
{
    s_EventStore.StoreHandleForEvent(hEvent);
}

void InitEventStore()
{
    s_EventStore.Init();
}

void TerminateEventStore()
{
    s_EventStore.Destroy();
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\excep.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
// EXCEP.H - Copyrioht (C) 1998 Microsoft Corporation
//

#ifndef __excep_h__
#define __excep_h__

#include "exceptmacros.h"
#include "CorError.h"  // HResults for the COM+ Runtime
#include "frames.h"
class Thread;

#include "..\\dlls\\mscorrc\\resource.h"

#include <ExcepCpu.h>

// All COM+ exceptions are expressed as a RaiseException with this exception
// code.

#define EXCEPTION_MSVC    0xe06d7363    // 0xe0000000 | 'msc'
#define EXCEPTION_COMPLUS 0xe0434f4d    // 0xe0000000 | 'COM'

// Check if the Win32 Error code is an IO error.
#define IsWin32IOError(scode)           \
    (                                   \
     (scode) == ERROR_FILE_NOT_FOUND ||   \
     (scode) == ERROR_PATH_NOT_FOUND ||   \
     (scode) == ERROR_TOO_MANY_OPEN_FILES || \
     (scode) == ERROR_ACCESS_DENIED ||    \
     (scode) == ERROR_INVALID_HANDLE ||   \
     (scode) == ERROR_INVALID_DRIVE ||    \
     (scode) == ERROR_WRITE_PROTECT ||    \
     (scode) == ERROR_NOT_READY ||        \
     (scode) == ERROR_WRITE_FAULT ||      \
     (scode) == ERROR_SHARING_VIOLATION ||    \
     (scode) == ERROR_LOCK_VIOLATION ||   \
     (scode) == ERROR_SHARING_BUFFER_EXCEEDED ||  \
     (scode) == ERROR_HANDLE_DISK_FULL || \
     (scode) == ERROR_BAD_NETPATH ||      \
     (scode) == ERROR_DEV_NOT_EXIST ||    \
     (scode) == ERROR_FILE_EXISTS ||      \
     (scode) == ERROR_CANNOT_MAKE ||      \
     (scode) == ERROR_NET_WRITE_FAULT ||  \
     (scode) == ERROR_DRIVE_LOCKED ||     \
     (scode) == ERROR_OPEN_FAILED ||      \
     (scode) == ERROR_BUFFER_OVERFLOW ||  \
     (scode) == ERROR_DISK_FULL ||        \
     (scode) == ERROR_INVALID_NAME ||     \
     (scode) == ERROR_FILENAME_EXCED_RANGE || \
     (scode) == ERROR_IO_DEVICE ||        \
     (scode) == ERROR_DISK_OPERATION_FAILED \
    )

// Enums
// return values of LookForHandler
enum LFH {
    LFH_NOT_FOUND = 0,
    LFH_FOUND = 1,
    LFH_CONTINUE_EXECUTION = 2,
};


//==========================================================================
// Identifies commonly-used exception classes for COMPlusThrowable().
//==========================================================================
enum RuntimeExceptionKind {
#define EXCEPTION_BEGIN_DEFINE(ns, reKind, hr) k##reKind,
#define EXCEPTION_ADD_HR(hr)
#define EXCEPTION_END_DEFINE()
#include "rexcep.h"
kLastException
#undef EXCEPTION_BEGIN_DEFINE
#undef EXCEPTION_ADD_HR
#undef EXCEPTION_END_DEFINE
};


// Structures
struct ThrowCallbackType {
    MethodDesc * pFunc;     // the function containing a filter that returned catch indication
    int     dHandler;       // the index of the handler whose filter returned catch indication
    BOOL    bIsUnwind;      // are we currently unwinding an exception
    BOOL    bUnwindStack;   // unwind the stack before calling the handler? (Stack overflow only)
    BOOL    bAllowAllocMem; // are we allowed to allocate memory?
    BOOL    bDontCatch;     // can we catch this exception?
    BOOL    bLastChance;    // should we perform last chance handling?
    BYTE    *pStack;
    Frame * pTopFrame;
    Frame * pBottomFrame;
    MethodDesc * pProfilerNotify;   // Context for profiler callbacks -- see COMPlusFrameHandler().
    
#ifdef _DEBUG
    void * pCurrentExceptionRecord;
    void * pPrevExceptionRecord;
#endif
    ThrowCallbackType() : 
        pFunc(NULL), 
        dHandler(0), 
        bIsUnwind(FALSE), 
        bUnwindStack(FALSE), 
        bAllowAllocMem(TRUE), 
        bDontCatch(FALSE), 
        bLastChance(TRUE), 
        pStack(NULL), 
        pTopFrame((Frame *)-1), 
        pBottomFrame((Frame *)-1),
        pProfilerNotify(NULL)        
    {
#ifdef _DEBUG
        pCurrentExceptionRecord = 0;
        pPrevExceptionRecord = 0;
#endif
    }
};



struct ExInfo;
struct EE_ILEXCEPTION_CLAUSE;

BOOL InitializeExceptionHandling();
void TerminateExceptionHandling();

// Prototypes
VOID ResetCurrentContext();
// global pointer to the RtlUnwind function
// fixed up when needed
typedef VOID (__stdcall * TRtlUnwind)
        ( IN PVOID TargetFrame OPTIONAL,
    IN PVOID TargetIp OPTIONAL,
    IN PEXCEPTION_RECORD ExceptionRecord OPTIONAL,
    IN PVOID ReturnValue
    );
void CheckStackBarrier(EXCEPTION_REGISTRATION_RECORD *exRecord);
void UnwindFrames(Thread *pThread, ThrowCallbackType *tct);
void UnwindExInfo(ExInfo *pExInfo, void* limit);
void UnwindFrameChain(Thread *pThread, Frame* limit);
ExInfo *FindNestedExInfo(EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame);
EXCEPTION_REGISTRATION_RECORD *FindNestedEstablisherFrame(EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame);
DWORD MapWin32FaultToCOMPlusException(DWORD Code);
LFH LookForHandler(const EXCEPTION_POINTERS *pExceptionPointers, Thread *pThread, ThrowCallbackType *tct);
void SaveStackTraceInfo(ThrowCallbackType *pData, ExInfo *pExInfo, OBJECTHANDLE *hThrowable, BOOL bReplaceStack, BOOL bSkipLastElement);
void RtlUnwindCallback();
TRtlUnwind GetRtlUnwind();
StackWalkAction COMPlusThrowCallback (CrawlFrame *pCf, ThrowCallbackType *pData);
DWORD COMPlusComputeNestingLevel( IJitManager *pIJM, METHODTOKEN mdTok, SIZE_T offsNat, bool fGte);
BOOL IsException(EEClass *pClass);
BOOL IsExceptionOfType(RuntimeExceptionKind reKind, OBJECTREF *pThrowable);
BOOL IsAsyncThreadException(OBJECTREF *pThrowable);
BOOL IsUncatchable(OBJECTREF *pThrowable);
VOID FixupOnRethrow(Thread *pCurThread, EXCEPTION_POINTERS *pExceptionPointers);
#ifdef _DEBUG
BOOL IsValidClause(EE_ILEXCEPTION_CLAUSE *EHClause);
BOOL IsCOMPlusExceptionHandlerInstalled();
#endif

void InstallUnhandledExceptionFilter();
void UninstallUnhandledExceptionFilter();

LONG COMUnhandledExceptionFilter(struct _EXCEPTION_POINTERS  *pExceptionInfo);

//////////////
// A list of places where we might have unhandled exceptions or other serious faults. These can be used as a mask in
// DbgJITDebuggerLaunchSetting to help control when we decide to ask the user about whether or not to launch a debugger.
//
enum UnhandledExceptionLocation {
    ProcessWideHandler    = 0x000001,
    ManagedThread         = 0x000002, // Does not terminate the application. CLR swallows the unhandled exception.
    ThreadPoolThread      = 0x000004, // ditto.
    FinalizerThread       = 0x000008, // ditto.
    FatalStackOverflow    = 0x000010,
    FatalOutOfMemory      = 0x000020,
    FatalExecutionEngineException = 0x000040,
    ClassInitUnhandledException   = 0x000080, // Does not terminate the application. CLR transforms this into TypeInitializationException

    MaximumLocationValue  = 0x800000, // This is the maximum location value you're allowed to use. (Max 24 bits allowed.)

    // This is a mask of all the locations that the debugger will attach to by default.
    DefaultDebuggerAttach = ProcessWideHandler | 
                            FatalStackOverflow | 
                            FatalOutOfMemory   | 
                            FatalExecutionEngineException
};

LONG ThreadBaseExceptionFilter(struct _EXCEPTION_POINTERS  *pExceptionInfo, Thread* pThread, UnhandledExceptionLocation);



DWORD GetEIP();
void LogFatalError(DWORD id);

#if 0

// Hardcoded ID Based
#define FATAL_EE_ERROR(DWORD id)						\
{													\
	LogFatalError(id);									\
	FatalInternalError();								\
}

#else

// IP Based
#define FATAL_EE_ERROR()							\
{													\
	DWORD address = GetEIP();						\
	LogFatalError(address);							\
	FatalInternalError();								\
}

#endif

void
FailFast(Thread* pThread, 
        UnhandledExceptionLocation reason, 
        EXCEPTION_RECORD *pExceptionRecord = NULL, 
        CONTEXT *pContext = NULL);

void FatalOutOfMemoryError();
void FatalInternalError();

void STDMETHODCALLTYPE DefaultCatchHandler(OBJECTREF *Throwable = NULL, BOOL isTerminating = FALSE);

BOOL COMPlusIsMonitorException(struct _EXCEPTION_POINTERS *pExceptionInfo);
BOOL COMPlusIsMonitorException(EXCEPTION_RECORD *pExceptionRecord, 
                               CONTEXT *pContext);

void 
ReplaceExceptionContextRecord(CONTEXT *pTarget, CONTEXT *pSource);

// externs

// This variable gets set the first time a COM+ exception is thrown.
EXTERN LPVOID gpRaiseExceptionIP;


//==========================================================================
// Takes appropriate default action on an unhandled exception.
//
// Used this way
//
// ON_UNHANDLED_EXCEPTION {
// } CALL_DEFAULT_CATCH_HANDLER(TRUE)
// 
// Argument indicates whether the thread is about to terminate.  At most
// call sites, this will be false.
//==========================================================================
#define ON_EXCEPTION __try

#define CALL_DEFAULT_CATCH_HANDLER(isTerminating) \
  __except( (DefaultCatchHandler(NULL, isTerminating), EXCEPTION_CONTINUE_SEARCH)) {\
  }


//==========================================================================
// Various routines to throw COM+ objects.
//==========================================================================

//==========================================================================
// Throw an object.
//==========================================================================

VOID RealCOMPlusThrow(OBJECTREF pThrowable);

//==========================================================================
// Throw an undecorated runtime exception.
//==========================================================================

VOID RealCOMPlusThrow(RuntimeExceptionKind reKind);

//==========================================================================
// Throw an undecorated runtime exception with a specific string parameter
// that won't be localized.  If possible, try using 
// COMPlusThrow(reKind, LPCWSTR wszResourceName) instead.
//==========================================================================

VOID RealCOMPlusThrowNonLocalized(RuntimeExceptionKind reKind, LPCWSTR wszTag);

//==========================================================================
// Throw an undecorated runtime exception with a localized message.  Given 
// a resource name, the ResourceManager will find the correct paired string
// in our .resources file.
//==========================================================================

VOID RealCOMPlusThrow(RuntimeExceptionKind reKind, LPCWSTR wszResourceName);

// Localization helper function
void ResMgrGetString(LPCWSTR wszResourceName, STRINGREF * ppMessage);


//==========================================================================
// Throw a decorated runtime exception.
//==========================================================================

VOID __cdecl RealCOMPlusThrow(RuntimeExceptionKind  reKind,
                          UINT                  resID);

//==========================================================================
// Throw a decorated runtime exception.
//==========================================================================

VOID __cdecl RealCOMPlusThrow(RuntimeExceptionKind  reKind,
                          UINT                  resID,
                          LPCWSTR               szArg1);

//==========================================================================
// Throw a decorated runtime exception.
//==========================================================================

VOID __cdecl RealCOMPlusThrow(RuntimeExceptionKind  reKind,
                          UINT                  resID,
                          LPCWSTR               szArg1,
                          LPCWSTR               szArg2);

//==========================================================================
// Throw a decorated runtime exception.
//==========================================================================

VOID __cdecl RealCOMPlusThrow(RuntimeExceptionKind  reKind,
                          UINT                  resID,
                          LPCWSTR               szArg1,
                          LPCWSTR               szArg2,
                          LPCWSTR               szArg3);

//==========================================================================
// Throw a runtime exception based on an HResult
//==========================================================================

VOID RealCOMPlusThrowHR(HRESULT hr, IErrorInfo* pErrInfo);
VOID RealCOMPlusThrowHR(HRESULT hr);
VOID RealCOMPlusThrowHR(HRESULT hr, LPCWSTR wszArg1);
VOID RealCOMPlusThrowHR(HRESULT hr, UINT resourceID, LPCWSTR wszArg1, LPCWSTR wszArg2);


//==========================================================================
// Throw a runtime exception based on an HResult, check for error info
//==========================================================================

VOID RealCOMPlusThrowHR(HRESULT hr, IUnknown *iface, REFIID riid);


//==========================================================================
// Throw a runtime exception based on an EXCEPINFO. This function will free
// the strings in the EXCEPINFO that is passed in.
//==========================================================================

VOID RealCOMPlusThrowHR(EXCEPINFO *pExcepInfo);


//==========================================================================
// Throw a runtime exception based on the last Win32 error (GetLastError())
//==========================================================================

VOID RealCOMPlusThrowWin32();

//==========================================================================
// Throw a runtime exception based on the last Win32 error (GetLastError())
// with some error information.  If FormatMessage has a %1 in it, call this.
// Note that this behavior is really specific to a particular HResult, and
// we only support one argument at this point.
//==========================================================================

VOID RealCOMPlusThrowWin32(DWORD hr, WCHAR* arg);

//==========================================================================
// Create an exception object
//==========================================================================
BOOL CreateExceptionObject(RuntimeExceptionKind reKind, OBJECTREF *pThrowable);
void CreateExceptionObject(RuntimeExceptionKind reKind, LPCWSTR message, OBJECTREF *pThrowable);
void CreateExceptionObject(RuntimeExceptionKind reKind, UINT iResourceID, LPCWSTR wszArg1, LPCWSTR wszArg2, LPCWSTR wszArg3, OBJECTREF *pThrowable);
void CreateExceptionObjectWithResource(RuntimeExceptionKind reKind, LPCWSTR resourceName, OBJECTREF *pThrowable);

void CreateMethodExceptionObject(RuntimeExceptionKind reKind, MethodDesc *pMethod, OBJECTREF *pThrowable);
void CreateFieldExceptionObject(RuntimeExceptionKind reKind, FieldDesc *pField, OBJECTREF *pThrowable);
void CreateTypeInitializationExceptionObject(LPCWSTR pTypeThatFailed, OBJECTREF *pException, OBJECTREF *pThrowable);

//==========================================================================
// Examine an exception object
//==========================================================================

ULONG GetExceptionMessage(OBJECTREF pThrowable, LPWSTR buffer, ULONG bufferLength);
void GetExceptionMessage(OBJECTREF pThrowable, CQuickWSTRNoDtor *pBuffer);

//==========================================================================
// Re-Throw the last error. Do not use this - it is only for rethrows
// from IL and rarely in the EE. You should use EE_FINALLY instead of 
// rethrowing an exception
//==========================================================================

VOID RealCOMPlusRareRethrow();

//==========================================================================
// Throw an ArithmeticException
//==========================================================================

VOID RealCOMPlusThrowArithmetic();

//==========================================================================
// Throw an ArgumentNullException
//==========================================================================

VOID RealCOMPlusThrowArgumentNull(LPCWSTR argName, LPCWSTR wszResourceName);

VOID RealCOMPlusThrowArgumentNull(LPCWSTR argName);

//==========================================================================
// Throw an ArgumentOutOfRangeException
//==========================================================================

VOID RealCOMPlusThrowArgumentOutOfRange(LPCWSTR argName, LPCWSTR wszResourceName);

//==========================================================================
// Throw a MissingMethodException
//==========================================================================
VOID RealCOMPlusThrowMissingMethod(mdScope sc, mdToken mdtoken);

//==========================================================================
// Throw an exception pertaining to a member. E.g. MissingMethod, MissingField,
// MemberAccess.
//==========================================================================
VOID RealCOMPlusThrowMember(RuntimeExceptionKind excep, IMDInternalImport *pInternalImport, mdToken mdtoken);

//==========================================================================
// Throw an exception pertaining to a member. E.g. MissingMethod, MissingField,
// MemberAccess.
//==========================================================================
VOID RealCOMPlusThrowMember(RuntimeExceptionKind excep, IMDInternalImport *pInternalImport, MethodTable *pClassMT, LPCWSTR memberName, PCCOR_SIGNATURE memberSig);

//==========================================================================
// Throw an ArgumentException
//==========================================================================
VOID RealCOMPlusThrowArgumentException(LPCWSTR argName, LPCWSTR wszResourceName);

//==========================================================================
// EE-specific types for storing/querying exception info in memory. Use these
// rather than cor.h names directly to allow for decoupling in future if necessary
// This structure should be exactly the same as IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT, but with 
// the offset/lenghts resolved to native instructions and the class token replace by pEEClass. 
// This is do that the code manager can resolve to the class and cache it
// NOTE !!! NOTE This structure should line up with IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT,
// otherwise you'll have to adjust code in Excep.cpp, re: EHRangeTree NOTE !!! NOTE
//==========================================================================
struct EE_ILEXCEPTION_CLAUSE  {
    CorExceptionFlag    Flags;  
    DWORD               TryStartPC;    
    DWORD               TryEndPC;
    DWORD               HandlerStartPC;  
    DWORD               HandlerEndPC;  
    union { 
        EEClass         *pEEClass;
        DWORD           FilterOffset;
    };  
};

struct EE_ILEXCEPTION : public COR_ILMETHOD_SECT_EH_FAT 
{
    void Init(unsigned ehCount) {
        Kind = CorILMethod_Sect_FatFormat;
        DataSize = (unsigned)sizeof(IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT) * ehCount;
    }

    unsigned EHCount() const {
        return DataSize / sizeof(IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT);
    }

    EE_ILEXCEPTION_CLAUSE *EHClause(unsigned i) {
        _ASSERTE(sizeof(EE_ILEXCEPTION_CLAUSE) == sizeof(IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT));
        return (EE_ILEXCEPTION_CLAUSE *)(&Clauses[i]);
    }
};

#define COR_ILEXCEPTION_CLAUSE_CACHED_CLASS 0x10000000

inline BOOL HasCachedEEClass(EE_ILEXCEPTION_CLAUSE *EHClause)
{
    _ASSERTE(sizeof(EHClause->Flags) == sizeof(DWORD));
    return (EHClause->Flags & COR_ILEXCEPTION_CLAUSE_CACHED_CLASS);
}

inline void SetHasCachedEEClass(EE_ILEXCEPTION_CLAUSE *EHClause)
{
    _ASSERTE(! HasCachedEEClass(EHClause));
    EHClause->Flags = (CorExceptionFlag)(EHClause->Flags | COR_ILEXCEPTION_CLAUSE_CACHED_CLASS);
}

inline BOOL IsFinally(EE_ILEXCEPTION_CLAUSE *EHClause)
{
    return (EHClause->Flags & COR_ILEXCEPTION_CLAUSE_FINALLY);
}

inline BOOL IsFault(EE_ILEXCEPTION_CLAUSE *EHClause)
{
    return (EHClause->Flags & COR_ILEXCEPTION_CLAUSE_FAULT);
}

inline BOOL IsFaultOrFinally(EE_ILEXCEPTION_CLAUSE *EHClause)
{
    return IsFault(EHClause) || IsFinally(EHClause);
}

inline BOOL IsFilterHandler(EE_ILEXCEPTION_CLAUSE *EHClause)
{
    return EHClause->Flags & COR_ILEXCEPTION_CLAUSE_FILTER;
}

inline BOOL IsTypedHandler(EE_ILEXCEPTION_CLAUSE *EHClause)
{
    return ! (IsFilterHandler(EHClause) || IsFaultOrFinally(EHClause));
}


struct ExInfo {

    // Note: the debugger assumes that m_pThrowable is a strong
    // reference so it can check it for NULL with preemptive GC
    // enabled.
    OBJECTHANDLE m_pThrowable;   // thrown exception
    Frame  *m_pSearchBoundary;   // topmost frame for current managed frame group
    union {
        EXCEPTION_REGISTRATION_RECORD *m_pBottomMostHandler; // most recent EH record registered
        EXCEPTION_REGISTRATION_RECORD *m_pCatchHandler;      // reg frame for catching handler
    };

    // for building stack trace info
    void *m_pStackTrace;            // pointer to stack trace storage (of type SystemNative::StackTraceElement)
    unsigned m_cStackTrace;         // size of stack trace storage
    unsigned m_dFrameCount;         // current frame in stack trace

    ExInfo *m_pPrevNestedInfo; // pointer to nested info if are handling nested exception

    size_t * m_pShadowSP;           // Zero this after endcatch

    // Original exception info for rethrow.
    DWORD m_ExceptionCode;      // After a catch of a COM+ exception, pointers/context are trashed.
    EXCEPTION_RECORD *m_pExceptionRecord;
    CONTEXT *m_pContext;
    EXCEPTION_POINTERS *m_pExceptionPointers;


#ifdef _X86_
    DWORD   m_dEsp;         // Esp when  fault occured, OR esp to restore on endcatch
#endif

    // We have a rare case where (re-entry to the EE from an unmanaged filter) where we 
    // need to create a new ExInfo ... but don't have a nested handler for it.  The handlers
    // use stack addresses to figure out their correct lifetimes.  This stack location is
    // used for that.  For most records, it will be the stack address of the ExInfo ... but
    // for some records, it will be a pseudo stack location -- the place where we think
    // the record should have been (except for the re-entry case).  
    //
    // If you're thinking, "Urgh!  This is not pretty, you're right."  Ideally, we'll get
    // rid of the nested exception records completely.  A V2 task.
    // 
    void *m_StackAddress; // A pseudo or real stack location for this record. 
    BOOL IsHeapAllocated() {
        return m_StackAddress != (void *) this;
    }

private:
    INT m_flags;
    enum {
       Ex_IsRethrown       = 0x00000001,     
       Ex_UnwindHasStarted = 0x00000004,
       Ex_IsInUnmanagedHandler = 0x0000008  // set=1 each time we leave a managed handler, reset=0 each time we enter a managed handler
                                                // if we leave a managed handler, enter an unmanaged handler, which then swallows the exception
                                                // we will find the bit set the next time the thread traps
    };

           
public:
    BOOL IsRethrown() { return m_flags & Ex_IsRethrown; }
    void SetIsRethrown()   { m_flags |= Ex_IsRethrown; }
    void ResetIsRethrown() { m_flags &= ~Ex_IsRethrown; }

    BOOL UnwindHasStarted()      { return m_flags & Ex_UnwindHasStarted; }
    void SetUnwindHasStarted()   { m_flags |= Ex_UnwindHasStarted; }
    void ResetUnwindHasStarted() { m_flags &= Ex_UnwindHasStarted; }

    BOOL IsInUnmanagedHandler() { return m_flags & Ex_IsInUnmanagedHandler; }
    void SetIsInUnmanagedHandler()   { m_flags |= Ex_IsInUnmanagedHandler; }
    void ResetIsInUnmanagedHandler() { m_flags &= ~Ex_IsInUnmanagedHandler; }

    void Init();
    ExInfo();
    ExInfo& operator=(const ExInfo &from);

    void ClearStackTrace();
    void FreeStackTrace();
};
    

struct NestedHandlerExRecord : public FrameHandlerExRecord {
    ExInfo m_handlerInfo;
    NestedHandlerExRecord() : m_handlerInfo() {}  
    void Init(EXCEPTION_REGISTRATION_RECORD *pNext, LPVOID pvFrameHandler, Frame *pEntryFrame)
        { m_pNext=pNext; m_pvFrameHandler=pvFrameHandler; m_pEntryFrame=pEntryFrame; 
    	  m_handlerInfo.Init(); }
};

//-------------------------------------------------------------------------------
// This simply tests to see if the exception object is a subclass of
// the descriminating class specified in the exception clause.
//-------------------------------------------------------------------------------
__forceinline BOOL ExceptionIsOfRightType(EEClass *pClauseClass, EEClass *pThrownClass)
{
    // if not resolved to, then it wasn't loaded and couldn't have been thrown
    if (! pClauseClass)
        return FALSE;

    if (pClauseClass == pThrownClass)
        return TRUE;

    // now look for parent match
    EEClass *pSuper = pThrownClass;
    while (pSuper) {
        if (pSuper == pClauseClass) {
            break;
        }
        pSuper = pSuper->GetParentClass();
    }

    return pSuper != NULL;
}

//==========================================================================
// The stuff below is what works "behind the scenes" of the public macros.
//==========================================================================

DWORD COMPlusEndCatch( Thread *pCurThread, CONTEXT *pCtx, void *pSEH = NULL );
LPVOID __fastcall COMPlusCheckForAbort(LPVOID retAddress, DWORD esp, DWORD ebp);
LONG COMPlusFilter(const EXCEPTION_POINTERS *pExceptionPointers, DWORD fCatchFlag, void * limit);
EXCEPTION_DISPOSITION __cdecl COMPlusFrameHandler(EXCEPTION_RECORD *pExceptionRecord, 
                         EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame,
                         CONTEXT *pContext,
                         void *DispatcherContext);
EXCEPTION_DISPOSITION __cdecl COMPlusNestedExceptionHandler(EXCEPTION_RECORD *pExceptionRecord, 
                         EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame,
                         CONTEXT *pContext,
                         void *DispatcherContext);
EXCEPTION_DISPOSITION __cdecl ContextTransitionFrameHandler(EXCEPTION_RECORD *pExceptionRecord, 
                         EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame,
                         CONTEXT *pContext,
                         void *DispatcherContext);

// Pop off any SEH handlers we have registered below pTargetSP
VOID __cdecl PopSEHRecords(LPVOID pTargetSP);
VOID PopSEHRecords(LPVOID pTargetSP, CONTEXT *pCtx, void *pSEH);

// Set the SEH record pointed to by pSEH as the topmost handler.  Make sure to
// link this handler with the previous handler so that the chain isn't broken.
VOID SetCurrentSEHRecord(LPVOID pSEH);

// InsertCOMPlusFramHandler and RemoveCOMPlusFrameHandler macros have been moved to
// ExcepX86.h and ExcepAlpha.h because of processor specific issues.


BOOL CallRtlUnwind(EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame, void *callback, EXCEPTION_RECORD *pExceptionRecord, void *retVal);

#define STACK_OVERWRITE_BARRIER_SIZE 20
#define STACK_OVERWRITE_BARRIER_VALUE 0xabcdefab
#ifdef _DEBUG
struct FrameHandlerExRecordWithBarrier {
    DWORD m_StackOverwriteBarrier[STACK_OVERWRITE_BARRIER_SIZE];
    EXCEPTION_REGISTRATION_RECORD *m_pNext;
    LPVOID m_pvFrameHandler;
    Frame *m_pEntryFrame;
    Frame *GetCurrFrame() {
        return m_pEntryFrame;
    }
};
#endif // _DEBUG

//==========================================================================
// This is a hack designed to allow the use of the StubLinker object at bootup
// time where the EE isn't sufficient awake to create COM+ exception objects.
// Instead, COMPlusThrow(rexcep) does a simple RaiseException using this code.
// Or use COMPlusThrowBoot() to explicitly do so.
//==========================================================================
#define BOOTUP_EXCEPTION_COMPLUS  0xC0020001

void COMPlusThrowBoot(HRESULT hr);


//==========================================================================
// Used by the classloader to record a managed exception object to explain
// why a classload got botched.
//
// - Can be called with gc enabled or disabled.
// - pThrowable must point to a buffer protected by a GCFrame.
// - If pThrowable is NULL, this function does nothing.
// - If (*pThrowable) is non-NULL, this function does nothing.
//   This allows a catch-all error path to post a generic catchall error
//   message w/out bonking more specific error messages posted by inner functions.
// - If pThrowable != NULL, this function is guaranteed to leave
//   a valid managed exception in it on exit.
//==========================================================================
VOID PostTypeLoadException(LPCUTF8 pNameSpace, LPCUTF8 pTypeName,
                           LPCWSTR pAssemblyName, LPCUTF8 pMessageArg,
                           UINT resIDWhy, OBJECTREF *pThrowable);
VOID PostFileLoadException(LPCSTR pFileName, BOOL fRemovePath,
                           LPCWSTR pFusionLog,HRESULT hr, OBJECTREF *pThrowable);


HRESULT PostFieldLayoutError(mdTypeDef cl,                // cl of the NStruct being loaded
                             Module* pModule,             // Module that defines the scope, loader and heap (for allocate FieldMarshalers)
                             DWORD   dwOffset,            // Field offset
                             DWORD   dwID,                // Message id
                             OBJECTREF *pThrowable);

VOID PostOutOfMemoryException(OBJECTREF *pThrowable);



LPVOID __stdcall FormatTypeLoadExceptionMessage(struct _FormatTypeLoadExceptionMessageArgs *args);

LPVOID __stdcall FormatFileLoadExceptionMessage(struct _FormatFileLoadExceptionMessageArgs *args);

LPVOID __stdcall MissingMethodException_FormatSignature(struct MissingMethodException_FormatSignature_Args *args);

#define EXCEPTION_NONCONTINUABLE 0x1    // Noncontinuable exception
#define EXCEPTION_UNWINDING 0x2         // Unwind is in progress
#define EXCEPTION_EXIT_UNWIND 0x4       // Exit unwind is in progress
#define EXCEPTION_STACK_INVALID 0x8     // Stack out of limits or unaligned
#define EXCEPTION_NESTED_CALL 0x10      // Nested exception handler call
#define EXCEPTION_TARGET_UNWIND 0x20    // Target unwind in progress
#define EXCEPTION_COLLIDED_UNWIND 0x40  // Collided exception handler call

#define EXCEPTION_UNWIND (EXCEPTION_UNWINDING | EXCEPTION_EXIT_UNWIND | \
                          EXCEPTION_TARGET_UNWIND | EXCEPTION_COLLIDED_UNWIND)

#define IS_UNWINDING(Flag) ((Flag & EXCEPTION_UNWIND) != 0)
#define IS_DISPATCHING(Flag) ((Flag & EXCEPTION_UNWIND) == 0)
#define IS_TARGET_UNWIND(Flag) (Flag & EXCEPTION_TARGET_UNWIND)

HRESULT SetIPFromSrcToDst(Thread *pThread,
                          IJitManager* pIJM,
                          METHODTOKEN MethodToken,
                          SLOT addrStart,       // base address of method
                          DWORD offFrom,        // native offset
                          DWORD offTo,          // native offset
                          bool fCanSetIPOnly,   // if true, don't do any real work
                          PREGDISPLAY pReg,
                          PCONTEXT pCtx,
                          DWORD methodSize,
                          void *firstExceptionHandler,
                          void *pDji);

BOOL IsInFirstFrameOfHandler(Thread *pThread, 
							 IJitManager *pJitManager,
							 METHODTOKEN MethodToken,
							 DWORD offSet);

//#include "CodeMan.h"

class EHRangeTreeNode;
class EHRangeTree;

typedef CUnorderedArray<EHRangeTreeNode *, 7> EH_CLAUSE_UNORDERED_ARRAY;

class EHRangeTreeNode
{
public:
    EHRangeTreeNode            *m_pContainedBy;
    EHRangeTree                *m_pTree;
    EH_CLAUSE_UNORDERED_ARRAY   m_containees;
    EE_ILEXCEPTION_CLAUSE      *m_clause;
    DWORD                       m_offStart;
    DWORD                       m_offEnd;
    ULONG32                     m_depth; //m_root is zero, is it isn't
                                    // an actual EH

    EHRangeTreeNode(void);
    EHRangeTreeNode(DWORD start, DWORD end);    
    void CommonCtor(DWORD start, DWORD end);
    
    bool Contains(DWORD addrStart, DWORD addrEnd);
    bool TryContains(DWORD addrStart, DWORD addrEnd);
    bool HandlerContains(DWORD addrStart, DWORD addrEnd);
    HRESULT AddContainedRange(EHRangeTreeNode *pContained);
} ;

#define EHRT_INVALID_DEPTH (0xFFFFFFFF)
class EHRangeTree
{
    unsigned                m_EHCount;
    EHRangeTreeNode        *m_rgNodes;
    EE_ILEXCEPTION_CLAUSE  *m_rgClauses;
    ULONG32                 m_maxDepth; // init to EHRT_INVALID_DEPTH
    BOOL                    m_isNative; // else it's IL
    
    // We can get the EH info either from
    // the runtime, in runtime data structures, or from
    // the on-disk image, which we'll examine using the
    // COR_ILMETHOD_DECODERs.  Except for the implicit
    // 'root' node, we'll want to iterate through the rest
    // w/o caring which one it is.
    union TypeFields
    {
        // if which == EHRTT_JIT_MANAGER
        struct _JitManager 
        {
            IJitManager     *pIJM;
            METHODTOKEN      methodToken;

            // @NICE: I can't figure out how to get this stupid field's
            // type to be recognized by the compiler
            void             *pEnumState; //EH_CLAUSE_ENUMERATOR
            } JitManager;

        // if which == EHRTT_ON_DISK
        struct _OnDisk
        {
            const COR_ILMETHOD_SECT_EH  *sectEH;
        } OnDisk;
    };

    struct EHRT_InternalIterator
    {
        enum Type
        {
            EHRTT_JIT_MANAGER, //from the runtime
            EHRTT_ON_DISK, // we'll be using a COR_ILMETHOD_DECODER
        };
        
        enum Type which;
        union TypeFields tf;
    };

public:    
    
    EHRangeTreeNode        *m_root; // This is a sentinel, NOT an actual
                                    // Exception Handler!
    HRESULT                 m_hrInit; // Ctor fills this out.

    EHRangeTree(COR_ILMETHOD_DECODER *pMethodDecoder);
    EHRangeTree(IJitManager* pIJM,
                METHODTOKEN methodToken,
                DWORD methodSize);
    void CommonCtor(EHRT_InternalIterator *pii,
                          DWORD methodSize);
                          
    ~EHRangeTree();

    EHRangeTreeNode *FindContainer(EHRangeTreeNode *pNodeCur);
    EHRangeTreeNode *FindMostSpecificContainer(DWORD addr);
    EHRangeTreeNode *FindNextMostSpecificContainer(EHRangeTreeNode *pNodeCur, 
                                                   DWORD addr);

    ULONG32 MaxDepth();   
    BOOL isNative(); // FALSE ==> It's IL

    // @BUG 59560:  We shouldn't need this - instead, we
    // should get sequence points annotated with whether they're STACK_EMPTY, etc,
    // and then we'll figure out if the destination is ok based on that, instead.
    BOOL isAtStartOfCatch(DWORD offset);
} ;                       

//==========================================================================
// Handy helper functions
//==========================================================================
void ThrowUsingMessage(MethodTable * pMT, const WCHAR *pszMsg);
_inline void ThrowUsingMT(MethodTable * pMT) { ThrowUsingMessage(pMT, NULL); }
void ThrowUsingWin32Message(MethodTable * pMT);
void ThrowUsingResource(MethodTable * pMT, DWORD dwMsgResID);
void ThrowUsingResourceAndWin32(MethodTable * pMT, DWORD dwMsgResID);

#endif // __excep_h__
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\extensibleclassfactory.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*============================================================
**
** Header: ExtensibleClassFactory.cpp
**
** Author: Rudi Martin (rudim)
**
** Purpose: Native methods on System.Runtime.InteropServices.ExtensibleClassFactory
**
** Date:  May 27, 1999
** 
===========================================================*/

#include "common.h"
#include "excep.h"
#include "stackwalk.h"
#include "ExtensibleClassFactory.h"


// Helper function used to walk stack frames looking for a class initializer.
static StackWalkAction FrameCallback(CrawlFrame *pCF, void *pData)
{
    MethodDesc *pMD = pCF->GetFunction();
    _ASSERTE(pMD);
    _ASSERTE(pMD->GetClass());

    // We use the pData context argument to track the class as we move down the
    // stack and to return the class whose initializer is being called. If
    // *ppClass is NULL we are looking at the caller's initial frame and just
    // record the class that the method belongs to. From that point on the class
    // must remain the same until we hit a class initializer or else we must
    // fail (to prevent other classes called from a class initializer from
    // setting the current classes callback). The very first class we will see
    // belongs to RegisterObjectCreationCallback itself, so skip it (we set
    // *ppClass to an initial value of -1 to detect this).
    EEClass **ppClass = (EEClass **)pData;

    if (*ppClass == (EEClass *)-1)
        *ppClass = NULL;
    else if (*ppClass == NULL)
        *ppClass = pMD->GetClass();
    else
        if (pMD->GetClass() != *ppClass) {
            *ppClass = NULL;
            return SWA_ABORT;
        }

    if (pMD->IsStaticInitMethod())
        return SWA_ABORT;

    return SWA_CONTINUE;
}


// Register a delegate that will be called whenever an instance of a
// managed type that extends from an unmanaged type needs to allocate
// the aggregated unmanaged object. This delegate is expected to
// allocate and aggregate the unmanaged object and is called in place
// of a CoCreateInstance. This routine must be called in the context
// of the static initializer for the class for which the callbacks
// will be made.
// It is not legal to register this callback from a class that has any
// parents that have already registered a callback.
void __stdcall RegisterObjectCreationCallback(RegisterObjectCreationCallbackArgs *pArgs)
{
    THROWSCOMPLUSEXCEPTION();

    OBJECTREF orDelegate = pArgs->m_pDelegate;

    // Validate the delegate argument.
    if (orDelegate == 0)
        COMPlusThrowArgumentNull(L"callback");

    // We should have been called in the context of a class static initializer.
    // Walk back up the stack to verify this and to determine just what class
    // we're registering a callback for.
    EEClass *pClass = (EEClass *)-1;
    if (GetThread()->StackWalkFrames(FrameCallback, &pClass, FUNCTIONSONLY, NULL) == SWA_FAILED)
        COMPlusThrow(kInvalidOperationException, IDS_EE_CALLBACK_NOT_CALLED_FROM_CCTOR);

    // If we didn't find a class initializer, we can't continue.
    if (pClass == NULL)
        COMPlusThrow(kInvalidOperationException, IDS_EE_CALLBACK_NOT_CALLED_FROM_CCTOR);

    // The object type must derive at some stage from a COM imported object.
    // Also we must fail the call if some parent class has already registered a
    // callback.
    EEClass *pParent = pClass;
    do 
    {
        pParent = pParent->GetParentClass();
        if (pParent && !pParent->IsComImport() && (pParent->GetMethodTable()->GetObjCreateDelegate() != NULL))
        {
            COMPlusThrow(kInvalidOperationException, IDS_EE_CALLBACK_ALREADY_REGISTERED);
        }
    } 
    while (pParent && !pParent->IsComImport());

    // If the class does not have a COM imported base class then fail the call.
    if (pParent == NULL)
        COMPlusThrow(kInvalidOperationException, IDS_EE_CALLBACK_NOT_CALLED_FROM_CCTOR);

    // Save the delegate in the MethodTable for the class.
    pClass->GetMethodTable()->SetObjCreateDelegate(orDelegate);
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\excep.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*  EXCEP.CPP:
 *
 */

#include "common.h"

#include "tls.h"
#include "frames.h"
#include "threads.h"
#include "excep.h"
#include "object.h"
#include "COMString.h"
#include "field.h"
#include "DbgInterface.h"
#include "cgensys.h"
#include "gcscan.h"
#include "comutilnative.h"
#include "comsystem.h"
#include "commember.h"
#include "SigFormat.h"
#include "siginfo.hpp"
#include "gc.h"
#include "EEDbgInterfaceImpl.h" //so we can clearexception in RealCOMPlusThrow
#include "PerfCounters.h"
#include "NExport.h"
#include "stackwalk.h" //for CrawlFrame, in SetIPFromSrcToDst
#include "ShimLoad.h"
#include "EEConfig.h"

#include "zapmonitor.h"

#define FORMAT_MESSAGE_BUFFER_LENGTH 1024

#define SZ_UNHANDLED_EXCEPTION L"Unhandled Exception:"

LPCWSTR GetHResultSymbolicName(HRESULT hr);


typedef struct {
    OBJECTREF pThrowable;
    STRINGREF s1;
    OBJECTREF pTmpThrowable;
} ProtectArgsStruct;

static VOID RealCOMPlusThrowPreallocated();
LPVOID GetCurrentSEHRecord();
BOOL ComPlusStubSEH(EXCEPTION_REGISTRATION_RECORD*);
BOOL ComPlusFrameSEH(EXCEPTION_REGISTRATION_RECORD*);
BOOL ComPlusCoopFrameSEH(EXCEPTION_REGISTRATION_RECORD*);
BOOL ComPlusCannotThrowSEH(EXCEPTION_REGISTRATION_RECORD*);
VOID RealCOMPlusThrow(OBJECTREF throwable, BOOL rethrow);
VOID RealCOMPlusThrow(OBJECTREF throwable);
VOID RealCOMPlusThrow(RuntimeExceptionKind reKind);
void ThrowUsingMessage(MethodTable * pMT, const WCHAR *pszMsg);
void ThrowUsingWin32Message(MethodTable * pMT);
void ThrowUsingResource(MethodTable * pMT, DWORD dwMsgResID);
void ThrowUsingResourceAndWin32(MethodTable * pMT, DWORD dwMsgResID);
void CreateMessageFromRes (MethodTable * pMT, DWORD dwMsgResID, BOOL win32error, WCHAR * wszMessage);
extern "C" void JIT_WriteBarrierStart();
extern "C" void JIT_WriteBarrierEnd();

// Max # of inserts allowed in a error message.
enum {
    kNumInserts = 3
};

// Stores the IP of the EE's RaiseException call to detect forgeries.
// This variable gets set the first time a COM+ exception is thrown.
LPVOID gpRaiseExceptionIP = 0;

void COMPlusThrowBoot(HRESULT hr)
{
    _ASSERTE(g_fEEShutDown >= ShutDown_Finalize2 || !"Exception during startup");
    ULONG_PTR arg = hr;
    RaiseException(BOOTUP_EXCEPTION_COMPLUS, EXCEPTION_NONCONTINUABLE, 1, &arg);
}

//===========================================================================
// Loads a message from the resource DLL and fills in the inserts.
// Cannot return NULL: always throws a COM+ exception instead.
// NOTE: The returned message is LocalAlloc'd and therefore must be
//       LocalFree'd.
//===========================================================================
LPWSTR CreateExceptionMessage(BOOL fHasResourceID, UINT iResourceID, LPCWSTR wszArg1, LPCWSTR wszArg2, LPCWSTR wszArg3)
{
    THROWSCOMPLUSEXCEPTION();

    LPCWSTR wszArgs[kNumInserts] = {wszArg1, wszArg2, wszArg3};

    WCHAR   wszTemplate[500];
    HRESULT hr;

    if (!fHasResourceID) {
        wcscpy(wszTemplate, L"%1");
    } else {

        hr = LoadStringRC(iResourceID,
                          wszTemplate,
                          sizeof(wszTemplate)/sizeof(wszTemplate[0]),
                          FALSE);
        if (FAILED(hr)) {
            wszTemplate[0] = L'?';
            wszTemplate[1] = L'\0';
        }
    }

    LPWSTR wszFinal = NULL;
    DWORD res = WszFormatMessage(FORMAT_MESSAGE_FROM_STRING|FORMAT_MESSAGE_ARGUMENT_ARRAY|FORMAT_MESSAGE_ALLOCATE_BUFFER,
                                 wszTemplate,
                                 0,
                                 0,
                                 (LPWSTR) &wszFinal,
                                 0,
                                 (va_list*)wszArgs);
    if (res == 0) {
        _ASSERTE(wszFinal == NULL);
        RealCOMPlusThrowPreallocated();
    }
    return wszFinal;
}

//===========================================================================
// Gets the message text from an exception
//===========================================================================
ULONG GetExceptionMessage(OBJECTREF throwable, LPWSTR buffer, ULONG bufferLength)
{
    if (throwable == NULL)
        return 0;

    BinderMethodID sigID=METHOD__EXCEPTION__INTERNAL_TO_STRING;

    if (!IsException(throwable->GetClass()))
        sigID=METHOD__OBJECT__TO_STRING;

    MethodDesc *pMD = g_Mscorlib.GetMethod(sigID);

    STRINGREF pString = NULL; 

    INT64 arg[1] = {ObjToInt64(throwable)};
    pString = Int64ToString(pMD->Call(arg, sigID));
    
    if (pString == NULL) 
        return 0;

    ULONG length = pString->GetStringLength();
    LPWSTR chars = pString->GetBuffer();

    if (length < bufferLength)
    {
        wcsncpy(buffer, chars, length);
        buffer[length] = 0;
    }
    else
    {
        wcsncpy(buffer, chars, bufferLength);
        buffer[bufferLength-1] = 0;
    }

    return length;
}

void GetExceptionMessage(OBJECTREF throwable, CQuickWSTRNoDtor *pBuffer)
{
    if (throwable == NULL)
        return;

    BinderMethodID sigID=METHOD__EXCEPTION__INTERNAL_TO_STRING;

    if (!IsException(throwable->GetClass()))
        sigID=METHOD__OBJECT__TO_STRING;

    MethodDesc *pMD = g_Mscorlib.GetMethod(sigID);

    STRINGREF pString = NULL;

    INT64 arg[1] = {ObjToInt64(throwable)};
    pString = Int64ToString(pMD->Call(arg, sigID));
    if (pString == NULL) 
        return;

    ULONG length = pString->GetStringLength();
    LPWSTR chars = pString->GetBuffer();

    if (SUCCEEDED(pBuffer->ReSize(length+1)))
        wcsncpy(pBuffer->Ptr(), chars, length);
    else
    {
        pBuffer->Maximize();
        _ASSERTE(pBuffer->Size() < length);
        wcsncpy(pBuffer->Ptr(), chars, pBuffer->Size());
    }

    (*pBuffer)[pBuffer->Size()-1] = 0;

    return;
}


//------------------------------------------------------------------------
// Array that is used to retrieve the right exception for a given HRESULT.
//------------------------------------------------------------------------
struct ExceptionHRInfo
{
    int cHRs;
    HRESULT *aHRs;
};

#define EXCEPTION_BEGIN_DEFINE(ns, reKind, hr) static HRESULT s_##reKind##HRs[] = {hr,
#define EXCEPTION_ADD_HR(hr) hr,
#define EXCEPTION_END_DEFINE() };
#include "rexcep.h"
#undef EXCEPTION_BEGIN_DEFINE
#undef EXCEPTION_ADD_HR
#undef EXCEPTION_END_DEFINE

static 
ExceptionHRInfo gExceptionHRInfos[] = {
#define EXCEPTION_BEGIN_DEFINE(ns, reKind, hr) {sizeof(s_##reKind##HRs) / sizeof(HRESULT), s_##reKind##HRs},
#define EXCEPTION_ADD_HR(hr)
#define EXCEPTION_END_DEFINE()
#include "rexcep.h"
#undef EXCEPTION_BEGIN_DEFINE
#undef EXCEPTION_ADD_HR
#undef EXCEPTION_END_DEFINE
};

void CreateExceptionObject(RuntimeExceptionKind reKind, UINT iResourceID, LPCWSTR wszArg1, LPCWSTR wszArg2, LPCWSTR wszArg3, OBJECTREF *pThrowable)
{
    LPWSTR wszMessage = NULL;

    EE_TRY_FOR_FINALLY
    {
        wszMessage = CreateExceptionMessage(TRUE, iResourceID, wszArg1, wszArg2, wszArg3);
        CreateExceptionObject(reKind, wszMessage, pThrowable);
    }
    EE_FINALLY
    {
        if (wszMessage)
            LocalFree(wszMessage);
    } EE_END_FINALLY;
}

void CreateExceptionObject(RuntimeExceptionKind reKind, LPCWSTR pMessage, OBJECTREF *pThrowable)
{
    _ASSERTE(GetThread());

    BEGIN_ENSURE_COOPERATIVE_GC();

    struct _gc {
        OBJECTREF throwable;
        STRINGREF message;
    } gc;
    gc.throwable = NULL;
    gc.message = NULL;

    GCPROTECT_BEGIN(gc);
    CreateExceptionObject(reKind, &gc.throwable);
    gc.message = COMString::NewString(pMessage);
    INT64 args1[] = { ObjToInt64(gc.throwable), ObjToInt64(gc.message)};
    CallConstructor(&gsig_IM_Str_RetVoid, args1);
    *pThrowable = gc.throwable;
    GCPROTECT_END(); //Prot

    END_ENSURE_COOPERATIVE_GC();

}

void CreateExceptionObjectWithResource(RuntimeExceptionKind reKind, LPCWSTR resourceName, OBJECTREF *pThrowable)
{
    THROWSCOMPLUSEXCEPTION();
    _ASSERTE(resourceName);  // You should add a resource.
    _ASSERTE(GetThread());

    BEGIN_ENSURE_COOPERATIVE_GC();

    LPWSTR wszValue = NULL;
    struct _gc {
        OBJECTREF throwable;
        STRINGREF str;
    } gc;
    gc.throwable = NULL;
    gc.str = NULL;

    GCPROTECT_BEGIN(gc);
    ResMgrGetString(resourceName, &gc.str);

    CreateExceptionObject(reKind, &gc.throwable);
    INT64 args1[] = { ObjToInt64(gc.throwable), ObjToInt64(gc.str)};
    CallConstructor(&gsig_IM_Str_RetVoid, args1);
    *pThrowable = gc.throwable;
    GCPROTECT_END(); //Prot

    END_ENSURE_COOPERATIVE_GC();
}

void CreateTypeInitializationExceptionObject(LPCWSTR pTypeThatFailed, OBJECTREF *pException, 
                                             OBJECTREF *pThrowable)
{
    _ASSERTE(IsProtectedByGCFrame(pThrowable));
    _ASSERTE(IsProtectedByGCFrame(pException));

    Thread * pThread  = GetThread();
    _ASSERTE(pThread);

    BOOL fGCDisabled = pThread->PreemptiveGCDisabled();
    if (!fGCDisabled)
        pThread->DisablePreemptiveGC();

     CreateExceptionObject(kTypeInitializationException, pThrowable);
 


    BOOL fDerivesFromException = TRUE;
    if ( (*pException) != NULL ) {
        fDerivesFromException = FALSE;
        MethodTable *pSystemExceptionMT = g_Mscorlib.GetClass(CLASS__EXCEPTION);
        MethodTable *pInnerMT = (*pException)->GetMethodTable();
        while (pInnerMT != NULL) {
           if (pInnerMT == pSystemExceptionMT) {
              fDerivesFromException = TRUE;
              break;
           }
           pInnerMT = pInnerMT->GetParentMethodTable();
        }
    }


    STRINGREF sType = COMString::NewString(pTypeThatFailed);

    if (fDerivesFromException) {
       INT64 args1[] = { ObjToInt64(*pThrowable), ObjToInt64(*pException), ObjToInt64(sType)};
       CallConstructor(&gsig_IM_Str_Exception_RetVoid, args1);    
    } else {
       INT64 args1[] = { ObjToInt64(*pThrowable), ObjToInt64(NULL), ObjToInt64(sType)};
       CallConstructor(&gsig_IM_Str_Exception_RetVoid, args1);    
    }

    if (!fGCDisabled)
        pThread->EnablePreemptiveGC();
}

// Creates derivatives of ArgumentException that need two String parameters 
// in their constructor.
void CreateArgumentExceptionObject(RuntimeExceptionKind reKind, LPCWSTR pArgName, STRINGREF pMessage, OBJECTREF *pThrowable)
{
    Thread * pThread  = GetThread();
    _ASSERTE(pThread);

    BOOL fGCDisabled = pThread->PreemptiveGCDisabled();
    if (!fGCDisabled)
          pThread->DisablePreemptiveGC();

    ProtectArgsStruct prot;
    memset(&prot, 0, sizeof(ProtectArgsStruct));
    prot.s1 = pMessage;
    GCPROTECT_BEGIN(prot);
    CreateExceptionObject(reKind, pThrowable);
    prot.pThrowable = *pThrowable;
    STRINGREF argName = COMString::NewString(pArgName);

    // @BUG 59415.  It's goofy that ArgumentException and its subclasses
    // take arguments to their constructors in reverse order.  
    // Likely, this won't ever get fixed.
    if (reKind == kArgumentException)
    {
        INT64 args1[] = { ObjToInt64(prot.pThrowable),
                          ObjToInt64(argName),
                          ObjToInt64(prot.s1) };
        CallConstructor(&gsig_IM_Str_Str_RetVoid, args1);
    }
    else
    {
        INT64 args1[] = { ObjToInt64(prot.pThrowable),
                          ObjToInt64(prot.s1),
                          ObjToInt64(argName) };
        CallConstructor(&gsig_IM_Str_Str_RetVoid, args1);
    }

    GCPROTECT_END(); //Prot

    if (!fGCDisabled)
        pThread->EnablePreemptiveGC();
}

void CreateFieldExceptionObject(RuntimeExceptionKind reKind, FieldDesc *pField, OBJECTREF *pThrowable)
{
    LPUTF8 szFullName;
    LPCUTF8 szClassName, szMember;
    szMember = pField->GetName();
    DefineFullyQualifiedNameForClass();
    szClassName = GetFullyQualifiedNameForClass(pField->GetEnclosingClass());
    MAKE_FULLY_QUALIFIED_MEMBER_NAME(szFullName, NULL, szClassName, szMember, NULL);
    MAKE_WIDEPTR_FROMUTF8(szwFullName, szFullName);

    CreateExceptionObject(reKind, szwFullName, pThrowable);
}

void CreateMethodExceptionObject(RuntimeExceptionKind reKind, MethodDesc *pMethod, OBJECTREF *pThrowable)
{
    LPUTF8 szFullName;
    LPCUTF8 szClassName, szMember;
    szMember = pMethod->GetName();
    DefineFullyQualifiedNameForClass();
    szClassName = GetFullyQualifiedNameForClass(pMethod->GetClass());
    MetaSig tmp(pMethod->GetSig(), pMethod->GetModule());
    SigFormat sigFormatter(tmp, szMember);
    const char * sigStr = sigFormatter.GetCStringParmsOnly();
    MAKE_FULLY_QUALIFIED_MEMBER_NAME(szFullName, NULL, szClassName, szMember, sigStr);
    MAKE_WIDEPTR_FROMUTF8(szwFullName, szFullName);

    CreateExceptionObject(reKind, szwFullName, pThrowable);
}

BOOL CreateExceptionObject(RuntimeExceptionKind reKind, OBJECTREF *pThrowable)
{
    _ASSERTE(g_pPreallocatedOutOfMemoryException != NULL);
    BOOL success = FALSE;

    if (pThrowable == RETURN_ON_ERROR)
        return success;    
        
    Thread * pThread  = GetThread();

    BOOL fGCDisabled = pThread->PreemptiveGCDisabled();
    if (!fGCDisabled)
        pThread->DisablePreemptiveGC();

    MethodTable *pMT = g_Mscorlib.GetException(reKind);
    OBJECTREF throwable = AllocateObject(pMT);

    if (pThrowable == THROW_ON_ERROR) {
        DEBUG_SAFE_TO_THROW_IN_THIS_BLOCK;
        COMPlusThrow(throwable);
    }

    _ASSERTE(pThrowableAvailable(pThrowable));
    *pThrowable = throwable;

    if (!fGCDisabled)
        pThread->EnablePreemptiveGC();

    return success;
}

BOOL IsException(EEClass *pClass) {
    while (pClass != NULL) {
        if (pClass == g_pExceptionClass->GetClass()) return TRUE;
        pClass = pClass->GetParentClass();
    }
    return FALSE;
}

#ifdef _IA64_
VOID RealCOMPlusThrowWorker(RuntimeExceptionKind reKind,
                            BOOL                 fMessage,
                            BOOL                 fHasResourceID,
                            UINT                 resID,
                            HRESULT              hr,
                            LPCWSTR              wszArg1,
                            LPCWSTR              wszArg2,
                            LPCWSTR              wszArg3,
                            ExceptionData*       pED)
{
    _ASSERTE(!"RealCOMPlusThrowWorker -- NOT IMPLEMENTED");
}
#else // !_IA64_
static
VOID RealCOMPlusThrowWorker(RuntimeExceptionKind reKind,
                            BOOL                 fMessage,
                            BOOL                 fHasResourceID,
                            UINT                 resID,
                            HRESULT              hr,
                            LPCWSTR              wszArg1,
                            LPCWSTR              wszArg2,
                            LPCWSTR              wszArg3,
                            ExceptionData*       pED)
{
    THROWSCOMPLUSEXCEPTION();

    Thread *pThread = GetThread();
    _ASSERTE(pThread);

    // Switch to preemptive GC before we manipulate objectref's.
    if (!pThread->PreemptiveGCDisabled())
        pThread->DisablePreemptiveGC();

    if (!g_fExceptionsOK)
        COMPlusThrowBoot(hr);

    if (reKind == kOutOfMemoryException && hr == S_OK)
        RealCOMPlusThrow(ObjectFromHandle(g_pPreallocatedOutOfMemoryException));

    if (reKind == kExecutionEngineException && hr == S_OK &&
        (!fMessage))
        RealCOMPlusThrow(ObjectFromHandle(g_pPreallocatedExecutionEngineException));

    ProtectArgsStruct prot;
    memset(&prot, 0, sizeof(ProtectArgsStruct));

    Frame * __pFrame = pThread->GetFrame();
    EE_TRY_FOR_FINALLY
    { 
    
    FieldDesc *pFD;
    MethodTable *pMT;

    GCPROTECT_BEGIN(prot);
    LPWSTR wszExceptionMessage = NULL;
    GCPROTECT_BEGININTERIOR(wszArg1);

    pMT = g_Mscorlib.GetException(reKind);      // Will throw if we fail to load the type.

    // If there is a message, copy that in the event that a caller passed in
    // a pointer into the middle of an un-GC-protected String object
    if (fMessage) {
        wszExceptionMessage = CreateExceptionMessage(fHasResourceID, resID, wszArg1, wszArg2, wszArg3);
    }
    GCPROTECT_END();

    prot.pThrowable = AllocateObject(pMT);
    CallDefaultConstructor(prot.pThrowable);

    // If we have exception data then retrieve information from there.
    if (pED)
    {
        // The HR in the exception data better be the same as the one thrown.
        _ASSERTE(hr == pED->hr);

        // Retrieve the description from the exception data.
        if (pED->bstrDescription != NULL)
        {
            // If this truly is a BSTR, we can't guarantee that it is null terminated!
            // call NewString constructor with SysStringLen(bstr) as second param.
            prot.s1 = COMString::NewString(pED->bstrDescription, SysStringLen(pED->bstrDescription));
        }

        // Set the _helpURL field in the exception.
        if (pED->bstrHelpFile) 
        {
            // @MANAGED: Set Exception's _helpURL field
            pFD = g_Mscorlib.GetField(FIELD__EXCEPTION__HELP_URL);

            // Create the helt link from the help file and the help context.
            STRINGREF helpStr = NULL;
            if (pED->dwHelpContext != 0)
            {
                // We have a non 0 help context so use it to form the help link.
                WCHAR strHelpContext[32];
                _ltow(pED->dwHelpContext, strHelpContext, 10);
                helpStr = COMString::NewString((INT32)(SysStringLen(pED->bstrHelpFile) + wcslen(strHelpContext) + 1));
                swprintf(helpStr->GetBuffer(), L"%s#%s", pED->bstrHelpFile, strHelpContext);
            }
            else
            {
                // The help context is 0 so we simply use the help file to from the help link.
                helpStr = COMString::NewString(pED->bstrHelpFile, SysStringLen(pED->bstrHelpFile));
            }

            // Set the value of the help link field.
            pFD->SetRefValue(prot.pThrowable, (OBJECTREF)helpStr);
        } 
        
        // Set the Source field in the exception.
        if (pED->bstrSource) 
        {
            pFD = g_Mscorlib.GetField(FIELD__EXCEPTION__SOURCE);
            STRINGREF sourceStr = COMString::NewString(pED->bstrSource, SysStringLen(pED->bstrSource));
            pFD->SetRefValue(prot.pThrowable, (OBJECTREF)sourceStr);
        }
        else
        {
            // for now set a null source
            pFD = g_Mscorlib.GetField(FIELD__EXCEPTION__SOURCE);
            pFD->SetRefValue(prot.pThrowable, (OBJECTREF)COMString::GetEmptyString());
        }
            
    }
    else if (fMessage) 
    {
        EE_TRY_FOR_FINALLY
        {
            prot.s1 = COMString::NewString(wszExceptionMessage);
        }
        EE_FINALLY
        {
            LocalFree(wszExceptionMessage);
        } EE_END_FINALLY;
    } 

    if (FAILED(hr)) {

        // If we haven't managed to get a message so far then try and get one from the 
        // the OS using format message.
        if (prot.s1 == NULL)
        {
            WCHAR *strMessageBuf;
            if (WszFormatMessage(FORMAT_MESSAGE_ALLOCATE_BUFFER | FORMAT_MESSAGE_FROM_SYSTEM,
                                 0, hr, MAKELANGID(LANG_NEUTRAL, SUBLANG_DEFAULT),
                                 (WCHAR *)&strMessageBuf, 0, 0))
            {
                // System messages contain a trailing \r\n, which we don't want normally.
                int iLen = lstrlenW(strMessageBuf);
                if (iLen > 3 && strMessageBuf[iLen - 2] == '\r' && strMessageBuf[iLen - 1] == '\n')
                    strMessageBuf[iLen - 2] = '\0';
                
                // Use the formated error message.
                prot.s1 = COMString::NewString(strMessageBuf);
                
                // Free the buffer allocated by FormatMessage.
                LocalFree((HLOCAL)strMessageBuf);
            }
        }

        // If we haven't found the message yet and if hr is one of those we know has an
        // entry in mscorrc.rc that doesn't take arguments, print that out.
        if (prot.s1 == NULL && HRESULT_FACILITY(hr) == FACILITY_URT)
        {
            switch(hr)
            {
            case SN_CRYPTOAPI_CALL_FAILED:
            case SN_NO_SUITABLE_CSP:
                wszExceptionMessage = CreateExceptionMessage(TRUE, HRESULT_CODE(hr), 0, 0, 0);
                prot.s1 = COMString::NewString(wszExceptionMessage);
                LocalFree(wszExceptionMessage);
                break;
            }
        }


        // If we still haven't managed to get an error message, use the default error message.
        if (prot.s1 == NULL)
        {
            LPCWSTR wszSymbolicName = GetHResultSymbolicName(hr);
            WCHAR numBuff[140];
            Wszultow(hr, numBuff, 16 /*hex*/);
            if (wszSymbolicName)
            {
                wcscat(numBuff, L" (");
                wcscat(numBuff, wszSymbolicName);
                wcscat(numBuff, L")");
            }
            wszExceptionMessage = CreateExceptionMessage(TRUE, IDS_EE_THROW_HRESULT, numBuff, wszArg2, wszArg3);
            prot.s1 = COMString::NewString(wszExceptionMessage);
            LocalFree(wszExceptionMessage);
        }
    }

    // Set the message field. It is not safe doing this through the constructor
    // since the string constructor for some exceptions add a prefix to the message 
    // which we don't want.
    //
    // We only want to replace whatever the default constructor put there, if we
    // have something meaningful to add.
    if (prot.s1 != NULL)
    {
        pFD = g_Mscorlib.GetField(FIELD__EXCEPTION__MESSAGE);
        pFD->SetRefValue((OBJECTREF) prot.pThrowable, (OBJECTREF) prot.s1);
    }

    // If the HRESULT is not S_OK then set it by setting the field value directly.
    if (hr != S_OK)
    {
        pFD = g_Mscorlib.GetField(FIELD__EXCEPTION__HRESULT);
        pFD->SetValue32(prot.pThrowable, hr);   
    }

    RealCOMPlusThrow(prot.pThrowable);
    GCPROTECT_END(); //Prot

    }
    EE_FINALLY
    {  
        // make sure we pop out the frames. If we don't do this, we will leave some 
        // frames on the thread when ThreadAbortException happens.
        UnwindFrameChain( pThread, __pFrame);
    }
    EE_END_FINALLY    
}

#endif // !_IA64_

static
VOID RealCOMPlusThrowWorker(RuntimeExceptionKind reKind,
                            BOOL                 fMessage,
                            BOOL                 fHasResourceID,
                            UINT                 resID,
                            HRESULT              hr,
                            LPCWSTR              wszArg1,
                            LPCWSTR              wszArg2,
                            LPCWSTR              wszArg3)
{
    THROWSCOMPLUSEXCEPTION();

   RealCOMPlusThrowWorker(reKind, fMessage, fHasResourceID, resID, hr, wszArg1, wszArg2, wszArg3, NULL);
}

//==========================================================================
// Throw the preallocated OutOfMemory object.
//==========================================================================
static
VOID RealCOMPlusThrowPreallocated()
{
    THROWSCOMPLUSEXCEPTION();
    LOG((LF_EH, LL_INFO100, "In RealCOMPlusThrowPreallocated\n"));
    if (g_pPreallocatedOutOfMemoryException != NULL) {
        RealCOMPlusThrow(ObjectFromHandle(g_pPreallocatedOutOfMemoryException));
    } else {
        // Big trouble.
        _ASSERTE(!"Unrecoverable out of memory situation.");
        RaiseException(EXCEPTION_ACCESS_VIOLATION,EXCEPTION_NONCONTINUABLE, 0,0);
        _ASSERTE(!"Cannot continue after COM+ exception");      // Debugger can bring you here.
        SafeExitProcess(0);                                     // We can't continue.
    }
}


// ==========================================================================
// COMPlusComputeNestingLevel
// 
//  This is code factored out of COMPlusThrowCallback to figure out
//  what the number of nested exception handlers is.
// ==========================================================================
DWORD COMPlusComputeNestingLevel( IJitManager *pIJM,
                                  METHODTOKEN mdTok, 
                                  SIZE_T offsNat,
                                  bool fGte)
{
    // Determine the nesting level of EHClause. Just walk the table 
    // again, and find out how many handlers enclose it
    DWORD nestingLevel = 0;
    EH_CLAUSE_ENUMERATOR pEnumState2;
    EE_ILEXCEPTION_CLAUSE EHClause2, *EHClausePtr;
    unsigned EHCount2 = pIJM->InitializeEHEnumeration(mdTok, &pEnumState2);
    
    if (EHCount2 > 1)
        for (unsigned j=0; j<EHCount2; j++)
        {
            EHClausePtr = pIJM->GetNextEHClause(mdTok,&pEnumState2,&EHClause2);
            _ASSERTE(EHClausePtr->HandlerEndPC != -1);  // TODO remove, only protects against a deprecated convention
            
            if (fGte )
            {
                if (offsNat >= EHClausePtr->HandlerStartPC && 
                    offsNat < EHClausePtr->HandlerEndPC)
                    nestingLevel++;
            }
            else
            {
                if (offsNat > EHClausePtr->HandlerStartPC && 
                    offsNat < EHClausePtr->HandlerEndPC)
                    nestingLevel++;
            }
        }

    return nestingLevel;
}


// ******************************* EHRangeTreeNode ************************** //
EHRangeTreeNode::EHRangeTreeNode(void)
{
    CommonCtor(0, 0);
}

EHRangeTreeNode::EHRangeTreeNode(DWORD start, DWORD end)
{
    CommonCtor(start, end);
}

void EHRangeTreeNode::CommonCtor(DWORD start, DWORD end)
{
    m_depth = 0;
    m_pTree = NULL;
    m_pContainedBy = NULL;
    m_clause = NULL;
    m_offStart = start;
    m_offEnd = end;
}

bool EHRangeTreeNode::Contains(DWORD addrStart, DWORD addrEnd)
{
    return ( addrStart >= m_offStart && addrEnd < m_offEnd );
}

bool EHRangeTreeNode::TryContains(DWORD addrStart, DWORD addrEnd)
{
    if (m_clause != NULL &&
        addrStart >= m_clause->TryStartPC && 
        addrEnd < m_clause->TryEndPC)
        return true;

    return false;
}

bool EHRangeTreeNode::HandlerContains(DWORD addrStart, DWORD addrEnd)
{
    if (m_clause != NULL &&
        addrStart >= m_clause->HandlerStartPC && 
        addrEnd < m_clause->HandlerEndPC )
        return true;

    return false;
}

HRESULT EHRangeTreeNode::AddContainedRange(EHRangeTreeNode *pContained)
{
    _ASSERTE(pContained != NULL);

    EHRangeTreeNode **ppEH = m_containees.Append();

    if (ppEH == NULL)
        return E_OUTOFMEMORY;
        
    (*ppEH) = pContained;
    return S_OK;
}

// ******************************* EHRangeTree ************************** //
EHRangeTree::EHRangeTree(COR_ILMETHOD_DECODER *pMethodDecoder)
{
    LOG((LF_CORDB, LL_INFO10000, "EHRT::ERHT: on disk!\n"));

    _ASSERTE(pMethodDecoder!=NULL);
    m_EHCount = 0xFFFFFFFF;
    m_isNative = FALSE;
    
    // !!! THIS ISN"T ON THE HEAP - it's only a covienient packaging for
    // the core constructor method, so don't save pointers to it !!!
    EHRT_InternalIterator ii;
    
    ii.which = EHRT_InternalIterator::EHRTT_ON_DISK;

    if(pMethodDecoder->EH == NULL)
    {
        m_EHCount = 0;
    }
    else
    {
        const COR_ILMETHOD_SECT_EH *sectEH = pMethodDecoder->EH;
        m_EHCount = sectEH->EHCount();
        ii.tf.OnDisk.sectEH = sectEH;
    }

    DWORD methodSize = pMethodDecoder->GetCodeSize();
    CommonCtor(&ii, methodSize);
}

EHRangeTree::EHRangeTree(IJitManager* pIJM,
            METHODTOKEN methodToken,
            DWORD methodSize)
{
    LOG((LF_CORDB, LL_INFO10000, "EHRT::ERHT: already loaded!\n"));

    m_EHCount = 0xFFFFFFFF;
    m_isNative = TRUE;
    
    // !!! THIS ISN"T ON THE HEAP - it's only a covienient packaging for
    // the core constructor method, so don't save pointers to it !!!
    EHRT_InternalIterator ii;
    ii.which = EHRT_InternalIterator::EHRTT_JIT_MANAGER;
    ii.tf.JitManager.pIJM = pIJM;
    ii.tf.JitManager.methodToken = methodToken;
    
    m_EHCount = pIJM->InitializeEHEnumeration(methodToken, 
                 (EH_CLAUSE_ENUMERATOR*)&ii.tf.JitManager.pEnumState);

    CommonCtor(&ii, methodSize);
}

EHRangeTree::~EHRangeTree()
{
    if (m_rgNodes != NULL)
        delete [] m_rgNodes;

    if (m_rgClauses != NULL)
        delete [] m_rgClauses;
} //Dtor

// Before calling this, m_EHCount must be filled in.
void EHRangeTree::CommonCtor(EHRT_InternalIterator *pii,
                      DWORD methodSize)
{
    _ASSERTE(m_EHCount != 0xFFFFFFFF);

    ULONG i = 0;

    m_maxDepth = EHRT_INVALID_DEPTH;
    m_rgClauses = NULL;
    m_rgNodes = NULL;
    m_root = NULL;
    m_hrInit = S_OK;
    
    if (m_EHCount > 0)
    {
        m_rgClauses = new EE_ILEXCEPTION_CLAUSE[m_EHCount];
        if (m_rgClauses == NULL)
        {
           m_hrInit = E_OUTOFMEMORY;
           goto LError;
        }
    }

    LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: m_ehcount:0x%x, m_rgClauses:0%x\n",
        m_EHCount, m_rgClauses));
    
    m_rgNodes = new EHRangeTreeNode[m_EHCount+1];
    if (m_rgNodes == NULL)
    {
       m_hrInit = E_OUTOFMEMORY;
       goto LError;
    }

    //this contains everything, even stuff on the last IP
    m_root = &(m_rgNodes[m_EHCount]);
    m_root->m_offEnd = methodSize+1; 

    LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: rgNodes:0x%x\n",
        m_rgNodes));
    
    if (m_EHCount ==0)
    {
        LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: About to leave!\n"));
        m_maxDepth = 0; // we don't actually have any EH clauses
        return;
    }

    LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: Sticking around!\n"));

    EE_ILEXCEPTION_CLAUSE  *pEHClause = NULL;
    EHRangeTreeNode *pNodeCur;

    // First, load all the EH clauses into the object.
    for(i=0; i < m_EHCount; i++) 
    {  
        LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: i:0x%x!\n", i));

        switch(pii->which)
        {
            case EHRT_InternalIterator::EHRTT_JIT_MANAGER:
            {
                LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: EHRTT_JIT_MANAGER\n", i));

                pEHClause = pii->tf.JitManager.pIJM->GetNextEHClause(
                                        pii->tf.JitManager.methodToken,
                 (EH_CLAUSE_ENUMERATOR*)&(pii->tf.JitManager.pEnumState), 
                                        &(m_rgClauses[i]) );
                                        
                LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: EHRTT_JIT_MANAGER got clause\n", i));

                // What's actually
                // happening is that the JIT ignores m_rgClauses[i], and simply
                // hands us back a pointer to their internal data structure.
                // So copy it over, THEN muck with it.
                m_rgClauses[i] = (*pEHClause); //bitwise copy
                pEHClause = &(m_rgClauses[i]);

                LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: clause 0x%x,"
                    "addrof:0x%x\n", i, &(m_rgClauses[i]) ));

                break;
            }
            
            case EHRT_InternalIterator::EHRTT_ON_DISK:
            {
                LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: EHRTT_ON_DISK\n"));

                IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT clause;
                const IMAGE_COR_ILMETHOD_SECT_EH_CLAUSE_FAT *pClause;
                pClause = pii->tf.OnDisk.sectEH->EHClause(i, &clause);
                
                LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: EHRTT_ON_DISK got clause\n"));

                // Convert between data structures.
                pEHClause = &(m_rgClauses[i]);  // DON'T DELETE THIS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
                pEHClause->Flags = pClause->Flags;
                pEHClause->TryStartPC = pClause->TryOffset;
                pEHClause->TryEndPC = pClause->TryOffset+pClause->TryLength;
                pEHClause->HandlerStartPC = pClause->HandlerOffset;
                pEHClause->HandlerEndPC = pClause->HandlerOffset+pClause->HandlerLength;

                LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: post disk get\n"));

                break;
            }
#ifdef _DEBUG
            default:
            {
                _ASSERTE( !"Debugger is trying to analyze an unknown "
                    "EH format!");
            }
#endif //_DEBUG                
        }

        LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: got da clause!\n"));

        _ASSERTE(pEHClause->HandlerEndPC != -1);  // TODO remove, only protects against a deprecated convention
        
        pNodeCur = &(m_rgNodes[i]);
        
        pNodeCur->m_pTree = this;
        pNodeCur->m_clause = pEHClause;

        // Since the filter doesn't have a start/end FilterPC, the only
        // way we can know the size of the filter is if it's located
        // immediately prior to it's handler.  We assume that this is,
        // and if it isn't, we're so amazingly hosed that we can't
        // continue
        if (pEHClause->Flags == COR_ILEXCEPTION_CLAUSE_FILTER &&
            (pEHClause->FilterOffset >= pEHClause->HandlerStartPC ||
             pEHClause->FilterOffset < pEHClause->TryEndPC))
        {
            m_hrInit = CORDBG_E_SET_IP_IMPOSSIBLE;
            goto LError;
        }
        
        pNodeCur->m_offStart = pEHClause->TryStartPC;
        pNodeCur->m_offEnd = pEHClause->HandlerEndPC;
    }

    LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: about to do the second pass\n"));


    // Second, for each EH, find it's most limited, containing clause
    for(i=0; i < m_EHCount; i++) 
    {  
        LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: SP:0x%x\n", i));
        
        pNodeCur = &(m_rgNodes[i]);

        EHRangeTreeNode *pNodeCandidate = NULL;
        pNodeCandidate = FindContainer(pNodeCur);
        _ASSERTE(pNodeCandidate != NULL);
        
        pNodeCur->m_pContainedBy = pNodeCandidate;

        LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: SP: about to add to tree\n"));

        HRESULT hr = pNodeCandidate->AddContainedRange(pNodeCur);
        if (FAILED(hr))
        {
            m_hrInit = hr;
            goto LError;
        }
    }

    return;
LError:
    LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: LError - something went wrong!\n"));

    if (m_rgClauses != NULL)
    {
        delete [] m_rgClauses;
        m_rgClauses = NULL;
    }
    
    if (m_rgNodes != NULL)
    {
        delete [] m_rgNodes;
        m_rgNodes = NULL;
    }

    LOG((LF_CORDB, LL_INFO10000, "EHRT::CC: Falling off of LError!\n"));
            
} // Ctor Core


EHRangeTreeNode *EHRangeTree::FindContainer(EHRangeTreeNode *pNodeCur)
{
    EHRangeTreeNode *pNodeCandidate = NULL;

    // Examine the root, too.
    for(ULONG iInner=0; iInner < m_EHCount+1; iInner++) 
    {  
        EHRangeTreeNode *pNodeSearch = &(m_rgNodes[iInner]);

        if (pNodeSearch->Contains(pNodeCur->m_offStart, pNodeCur->m_offEnd)
            && pNodeCur != pNodeSearch)
        {
            if (pNodeCandidate == NULL)
            {
                pNodeCandidate = pNodeSearch;
            }
            else if (pNodeSearch->m_offStart > pNodeCandidate->m_offStart &&
                     pNodeSearch->m_offEnd < pNodeCandidate->m_offEnd)
            {
                pNodeCandidate = pNodeSearch;
            }
        }
    }

    return pNodeCandidate;
}

EHRangeTreeNode *EHRangeTree::FindMostSpecificContainer(DWORD addr)
{
    EHRangeTreeNode node(addr, addr);
    return FindContainer(&node);
}

EHRangeTreeNode *EHRangeTree::FindNextMostSpecificContainer(
                            EHRangeTreeNode *pNodeCur, 
                            DWORD addr)
{
    EHRangeTreeNode **rgpNodes = pNodeCur->m_containees.Table();

    if (NULL == rgpNodes)
        return pNodeCur;

    // It's possible that no subrange contains the desired address, so
    // keep a reasonable default around.
    EHRangeTreeNode *pNodeCandidate = pNodeCur;
    
    USHORT cSubRanges = pNodeCur->m_containees.Count();
    EHRangeTreeNode **ppNodeSearch = pNodeCur->m_containees.Table();

    for (int i = 0; i < cSubRanges; i++, ppNodeSearch++)
    {
        if ((*ppNodeSearch)->Contains(addr, addr) &&
            (*ppNodeSearch)->m_offStart >= pNodeCandidate->m_offStart &&
            (*ppNodeSearch)->m_offEnd < pNodeCandidate->m_offEnd)
        {
            pNodeCandidate = (*ppNodeSearch);
        }
    }

    return pNodeCandidate;
}

BOOL EHRangeTree::isNative()
{
    return m_isNative;
}

ULONG32 EHRangeTree::MaxDepth()
{
    // If we haven't asked for the depth before, then we'll
    // have to compute it now.
    if (m_maxDepth == EHRT_INVALID_DEPTH)
    {
        INT32   i;
        INT32   iMax;
        EHRangeTreeNode *pNodeCur = NULL;
        // Create a queue that will eventually hold all the nodes
        EHRangeTreeNode **rgNodes = new EHRangeTreeNode*[m_EHCount+1];
        if (rgNodes == NULL)
        {
            return EHRT_INVALID_DEPTH;
        }

        // Prime the queue by adding the root node.
        rgNodes[0] = m_root;
        m_root->m_depth = 0;
        m_maxDepth = 0;

        // iMax = count of elements in the rgNodes queue.  This will
        // increase as we put more in.
        for(i = 0, iMax = 1; i < iMax;i++)
        {
            // For all the children of the current element that we're processing.
            EHRangeTreeNode **rgChildNodes = rgNodes[i]->m_containees.Table();

            if (NULL != rgChildNodes)
            {
                USHORT cKids = rgNodes[i]->m_containees.Count();
                pNodeCur = rgChildNodes[0];

                // Loop over the children - iKid just keeps track of 
                // how many we've done.
                for (int iKid = 0; iKid < cKids; iKid++, pNodeCur++)
                {
                    // The depth of children of node i is
                    // the depth of node i + 1, since they're one deeper.
                    pNodeCur->m_depth = rgNodes[i]->m_depth+1;

                    // Put the children into the queue, so we can get
                    // their children, as well.
                    rgNodes[iMax++] = pNodeCur;
                }
             }
        }

        i--; //back up to the last element
        m_maxDepth = rgNodes[i]->m_depth;

        // Clean up the queue.
        delete [] rgNodes;
        rgNodes = NULL;
    }

    return m_maxDepth;
}

// @BUG 59560:
BOOL EHRangeTree::isAtStartOfCatch(DWORD offset)
{
    if (NULL != m_rgNodes && m_EHCount != 0)
    {
        for(unsigned i = 0; i < m_EHCount;i++)
        {
            if (m_rgNodes[i].m_clause->HandlerStartPC == offset &&
                (m_rgNodes[i].m_clause->Flags != COR_ILEXCEPTION_CLAUSE_FILTER &&
                 m_rgNodes[i].m_clause->Flags != COR_ILEXCEPTION_CLAUSE_FINALLY &&
                 m_rgNodes[i].m_clause->Flags != COR_ILEXCEPTION_CLAUSE_FAULT))
                return TRUE;
        }
    }

    return FALSE;
}

enum TRY_CATCH_FINALLY
{
    TCF_NONE= 0,
    TCF_TRY,
    TCF_FILTER,
    TCF_CATCH,
    TCF_FINALLY,
    TCF_COUNT, //count of all elements, not an element itself
};

#ifdef LOGGING
char *TCFStringFromConst(TRY_CATCH_FINALLY tcf)
{
    switch( tcf )
    {
        case TCF_NONE:
            return "TCFS_NONE";
            break;
        case TCF_TRY:
            return "TCFS_TRY";
            break;
        case TCF_FILTER:
            return "TCF_FILTER";
            break;
        case TCF_CATCH:
            return "TCFS_CATCH";
            break;
        case TCF_FINALLY:
            return "TCFS_FINALLY";
            break;
        case TCF_COUNT:
            return "TCFS_COUNT";
            break;
        default:
            return "INVALID TCFS VALUE";
            break;
    }
}
#endif //LOGGING

// We're unwinding if we'll return to the EE's code.  Otherwise
// we'll return to someplace in the current code.  Anywhere outside
// this function is "EE code".
bool FinallyIsUnwinding(EHRangeTreeNode *pNode,
                        ICodeManager* pEECM,
                        PREGDISPLAY pReg,
                        SLOT addrStart)
{
    const BYTE *pbRetAddr = pEECM->GetFinallyReturnAddr(pReg);

    if (pbRetAddr < (const BYTE *)addrStart)
        return true;
        
    DWORD offset = (DWORD)(size_t)(pbRetAddr - addrStart);
    EHRangeTreeNode *pRoot = pNode->m_pTree->m_root;
    
    if(!pRoot->Contains((DWORD)offset, (DWORD)offset))
        return true;
    else
        return false;
}

BOOL LeaveCatch(ICodeManager* pEECM,
                Thread *pThread, 
                CONTEXT *pCtx,
                void *firstException,
                void *methodInfoPtr,
                unsigned offset)
{
    // We can assert these things here, and skip a call
    // to COMPlusCheckForAbort later.
    
            // If no abort has been requested,
    _ASSERTE((pThread->GetThrowable() != NULL) ||         
            // or if there is a pending exception.
            (!pThread->IsAbortRequested()) );

    DWORD esp = ::COMPlusEndCatch(pThread, pCtx, firstException);

    // Do JIT-specific work
    pEECM->LeaveCatch(methodInfoPtr, offset, pCtx);

#ifdef _X86_
    pCtx->Esp = esp;
#elif defined(CHECK_PLATFORM_BUILD)
    #error "Platform NYI"
#else
    _ASSERTE(!"Platform NYI");
#endif
    
    return TRUE;
}


TRY_CATCH_FINALLY GetTcf(EHRangeTreeNode *pNode, 
                         ICodeManager* pEECM,
                         void *methodInfoPtr,
                         unsigned offset,    
                         PCONTEXT pCtx,
                         DWORD nl)
{
    _ASSERTE(pNode != NULL);

    TRY_CATCH_FINALLY tcf;

    if (!pNode->Contains(offset,offset))
    {
        tcf = TCF_NONE;
    }
    else if (pNode->TryContains(offset, offset))
    {
        tcf = TCF_TRY;
    }
    else
    {
        _ASSERTE(pNode->m_clause);
        if (IsFilterHandler(pNode->m_clause) && 
            offset >= pNode->m_clause->FilterOffset &&
            offset < pNode->m_clause->HandlerStartPC)
            tcf = TCF_FILTER;
        else if (IsFaultOrFinally(pNode->m_clause))
            tcf = TCF_FINALLY;
        else
            tcf = TCF_CATCH;
    }

    return tcf;
}

const DWORD bEnter = 0x01;
const DWORD bLeave = 0x02;

HRESULT IsLegalTransition(Thread *pThread,
                          bool fCanSetIPOnly,
                          DWORD fEnter,
                          EHRangeTreeNode *pNode,
                          DWORD offFrom, 
                          DWORD offTo,
                          ICodeManager* pEECM,
                          PREGDISPLAY pReg,
                          SLOT addrStart,
                          void *firstException,
                          void *methodInfoPtr,
                          PCONTEXT pCtx,
                          DWORD nlFrom,
                          DWORD nlTo)
{
#ifdef _DEBUG
    if (fEnter & bEnter)
    {
        _ASSERTE(pNode->Contains(offTo, offTo));
    }
    if (fEnter & bLeave)
    {
        _ASSERTE(pNode->Contains(offFrom, offFrom));
    }
#endif //_DEBUG

    // First, figure out where we're coming from/going to
    TRY_CATCH_FINALLY tcfFrom = GetTcf(pNode, 
                                       pEECM,
                                       methodInfoPtr,
                                       offFrom,
                                       pCtx,
                                       nlFrom);
                                       
    TRY_CATCH_FINALLY tcfTo =  GetTcf(pNode, 
                                       pEECM,
                                      methodInfoPtr,
                                      offTo,
                                      pCtx,
                                      nlTo);

    LOG((LF_CORDB, LL_INFO10000, "ILT: from %s to %s\n",
        TCFStringFromConst(tcfFrom), 
        TCFStringFromConst(tcfTo)));

    // Now we'll consider, case-by-case, the various permutations that
    // can arise
    switch(tcfFrom)
    {
        case TCF_NONE:
        case TCF_TRY:
        {
            switch(tcfTo)
            {
                case TCF_NONE:
                case TCF_TRY:
                {
                    return S_OK;
                    break;
                }

                case TCF_FILTER:
                {
                    return CORDBG_E_CANT_SETIP_INTO_OR_OUT_OF_FILTER;
                    break;
                }
                
                case TCF_CATCH:
                {
                    return CORDBG_E_CANT_SET_IP_INTO_CATCH;
                    break;
                }
                
                case TCF_FINALLY:
                {
                    return CORDBG_E_CANT_SET_IP_INTO_FINALLY;
                    break;
                }
            }
            break;
        }

        case TCF_FILTER:
        {
            switch(tcfTo)
            {
                case TCF_NONE:
                case TCF_TRY:
                case TCF_CATCH:
                case TCF_FINALLY:
                {
                    return CORDBG_E_CANT_SETIP_INTO_OR_OUT_OF_FILTER;
                    break;
                }
                case TCF_FILTER:
                {
                    return S_OK;
                    break;
                }
                
            }
            break;
        }
        
        case TCF_CATCH:
        {
            switch(tcfTo)
            {
                case TCF_NONE:
                case TCF_TRY:
                {
                    CONTEXT *pCtx = pThread->GetFilterContext();
                    if (pCtx == NULL)
                        return CORDBG_E_SET_IP_IMPOSSIBLE;
                    
                    if (!fCanSetIPOnly)
                    {
                        if (!LeaveCatch(pEECM,
                                        pThread, 
                                        pCtx,
                                        firstException,
                                        methodInfoPtr,
                                        offFrom))
                            return E_FAIL;
                    }
                    return S_OK;
                    break;
                }
                
                case TCF_FILTER:
                {
                    return CORDBG_E_CANT_SETIP_INTO_OR_OUT_OF_FILTER;
                    break;
                }
                
                case TCF_CATCH:
                {
                    return S_OK;
                    break;
                }
                
                case TCF_FINALLY:
                {
                    return CORDBG_E_CANT_SET_IP_INTO_FINALLY;
                    break;
                }
            }
            break;
        }
        
        case TCF_FINALLY:
        {
            switch(tcfTo)
            {
                case TCF_NONE:
                case TCF_TRY:
                {                    
                    if (!FinallyIsUnwinding(pNode, pEECM, pReg, addrStart))
                    {
                        CONTEXT *pCtx = pThread->GetFilterContext();
                        if (pCtx == NULL)
                            return CORDBG_E_SET_IP_IMPOSSIBLE;
                            
                        if (!fCanSetIPOnly)
                        {
                            if (!pEECM->LeaveFinally(methodInfoPtr,
                                                     offFrom,    
                                                     pCtx,
                                                     nlFrom))
                                return E_FAIL;
                        }
                        return S_OK;
                    }
                    else
                    {
                        return CORDBG_E_CANT_SET_IP_OUT_OF_FINALLY;
                    }
                    
                    break;
                }
                
                case TCF_FILTER:
                {
                    return CORDBG_E_CANT_SETIP_INTO_OR_OUT_OF_FILTER;
                    break;
                }
                
                case TCF_CATCH:
                {
                    return CORDBG_E_CANT_SET_IP_INTO_CATCH;
                    break;
                }
                
                case TCF_FINALLY:
                {
                    return S_OK;
                    break;
                }
            }
            break;
        }
       break;
    }

    _ASSERTE( !"IsLegalTransition: We should never reach this point!" );

    return CORDBG_E_SET_IP_IMPOSSIBLE;
}

// @BUG 59560: We need this to determine what
// to do based on whether the stack in general is empty, not
// this kludgey hack that's just a hotfix.
HRESULT DestinationIsValid(void *pDjiToken,
                           DWORD offTo,
                           EHRangeTree *pERHT)
{
    // We'll add a call to the DebugInterface that takes this
    // & tells us if the destination is a stack empty point.
//    DebuggerJitInfo *pDji = (DebuggerJitInfo *)pDjiToken;

    if (pERHT->isAtStartOfCatch(offTo))
        return CORDBG_S_BAD_START_SEQUENCE_POINT;
    else
        return S_OK;
}

// We want to keep the 'worst' HRESULT - if one has failed (..._E_...) & the
// other hasn't, take the failing one.  If they've both/neither failed, then
// it doesn't matter which we take.
// Note that this macro favors retaining the first argument
#define WORST_HR(hr1,hr2) (FAILED(hr1)?hr1:hr2)
HRESULT SetIPFromSrcToDst(Thread *pThread,
                          IJitManager* pIJM,
                          METHODTOKEN MethodToken,
                          SLOT addrStart,       // base address of method
                          DWORD offFrom,        // native offset
                          DWORD offTo,          // native offset
                          bool fCanSetIPOnly,   // if true, don't do any real work
                          PREGDISPLAY pReg,
                          PCONTEXT pCtx,
                          DWORD methodSize,
                          void *firstExceptionHandler,
                          void *pDji)
{
    LPVOID          methodInfoPtr;
    HRESULT         hr = S_OK;
    HRESULT         hrReturn = S_OK;
    BYTE           *EipReal = *(pReg->pPC);
    EHRangeTree    *pERHT = NULL;
    DWORD           nlFrom;
    DWORD           nlTo;
    bool            fCheckOnly = true;

    nlFrom = COMPlusComputeNestingLevel(pIJM,
                                        MethodToken, 
                                        offFrom,
                                        true);
                                        
    nlTo = COMPlusComputeNestingLevel(pIJM,
                                      MethodToken, 
                                      offTo,
                                      true);

    // Make sure that the start point is GC safe
    *(pReg->pPC) = (BYTE *)(addrStart+offFrom);
    // This seems redundant? For now, I'm just putting the assert, MikePa should remove 
    // this after reviewing
    IJitManager* pEEJM = ExecutionManager::FindJitMan(*(pReg->pPC)); 
    _ASSERTE(pEEJM);
    _ASSERTE(pEEJM == pIJM);

    methodInfoPtr = pEEJM->GetGCInfo(MethodToken);

    ICodeManager * pEECM = pEEJM->GetCodeManager();

    EECodeInfo codeInfo(MethodToken, pEEJM);

    // Make sure that the end point is GC safe
    *(pReg->pPC) = (BYTE *)(addrStart + offTo);
    IJitManager* pEEJMDup;
    pEEJMDup = ExecutionManager::FindJitMan(*(pReg->pPC)); 
    _ASSERTE(pEEJMDup == pEEJM);

    // Undo this here so stack traces, etc, don't look weird
    *(pReg->pPC) = EipReal;

    methodInfoPtr = pEEJM->GetGCInfo(MethodToken);

    ICodeManager * pEECMDup;
    pEECMDup = pEEJM->GetCodeManager();
    _ASSERTE(pEECMDup == pEECM);

    EECodeInfo codeInfoDup(MethodToken, pEEJM);

    // Do both checks here so compiler doesn't complain about skipping
    // initialization b/c of goto.
    if (!pEECM->IsGcSafe(pReg, methodInfoPtr, &codeInfo,0) && fCanSetIPOnly)
    {
        hrReturn = WORST_HR(hrReturn, CORDBG_E_SET_IP_IMPOSSIBLE);
    }
    
    if (!pEECM->IsGcSafe(pReg, methodInfoPtr, &codeInfoDup,0) && fCanSetIPOnly)
    {
        hrReturn = WORST_HR(hrReturn, CORDBG_E_SET_IP_IMPOSSIBLE);
    }

    // Create our structure for analyzing this.
    // @PERF: optimize - hold on to this so we don't rebuild it for both
    // CanSetIP & SetIP.
    pERHT = new EHRangeTree(pEEJM,
                            MethodToken,
                            methodSize);
    
    if (FAILED(pERHT->m_hrInit))
    {
        hrReturn = WORST_HR(hrReturn, pERHT->m_hrInit);
        delete pERHT;
        goto LExit;
    }

    if ((hr = DestinationIsValid(pDji, offTo, pERHT)) != S_OK 
        && fCanSetIPOnly)
    {
        hrReturn = WORST_HR(hrReturn,hr);
    }
    
    // The basic approach is this:  We'll start with the most specific (smallest)
    // EHClause that contains the starting address.  We'll 'back out', to larger
    // and larger ranges, until we either find an EHClause that contains both
    // the from and to addresses, or until we reach the root EHRangeTreeNode,
    // which contains all addresses within it.  At each step, we check/do work
    // that the various transitions (from inside to outside a catch, etc).
    // At that point, we do the reverse process  - we go from the EHClause that
    // encompasses both from and to, and narrow down to the smallest EHClause that
    // encompasses the to point.  We use our nifty data structure to manage
    // the tree structure inherent in this process.
    //
    // NOTE:  We do this process twice, once to check that we're not doing an
    //        overall illegal transition, such as ultimately set the IP into
    //        a catch, which is never allowed.  We're doing this because VS
    //        calls SetIP without calling CanSetIP first, and so we should be able
    //        to return an error code and have the stack in the same condition
    //        as the start of the call, and so we shouldn't back out of clauses
    //        or move into them until we're sure that can be done.

retryForCommit:

    EHRangeTreeNode *node;
    EHRangeTreeNode *nodeNext;
    node = pERHT->FindMostSpecificContainer(offFrom);

    while (!node->Contains(offTo, offTo))
    {
        hr = IsLegalTransition(pThread,
                               fCheckOnly, 
                               bLeave,
                               node, 
                               offFrom, 
                               offTo, 
                               pEECM,
                               pReg,
                               addrStart,
                               firstExceptionHandler,
                               methodInfoPtr,
                               pCtx,
                               nlFrom,
                               nlTo);
        if (FAILED(hr))
        {
            hrReturn = WORST_HR(hrReturn,hr);
        }
        
        node = node->m_pContainedBy;                
        // m_root prevents node from ever being NULL.
    }

    if (node != pERHT->m_root)
    {
        hr = IsLegalTransition(pThread,
                           fCheckOnly, 
                           bEnter|bLeave,
                           node, 
                           offFrom, 
                           offTo, 
                           pEECM, 
                           pReg,
                           addrStart,
                           firstExceptionHandler,
                           methodInfoPtr,
                           pCtx,
                           nlFrom,
                           nlTo);
        if (FAILED(hr))
        {
            hrReturn = WORST_HR(hrReturn,hr);
        }
    }
    
    nodeNext = pERHT->FindNextMostSpecificContainer(node,
                                                    offTo);

    while(nodeNext != node)
    {
        hr = IsLegalTransition(pThread,
                               fCheckOnly, 
                               bEnter,
                               nodeNext, 
                               offFrom, 
                               offTo, 
                               pEECM, 
                               pReg,
                               addrStart,
                               firstExceptionHandler,
                               methodInfoPtr,
                               pCtx,
                               nlFrom,
                               nlTo);
        if (FAILED(hr))
        {
            hrReturn = WORST_HR(hrReturn, hr);
        }
        
        node = nodeNext;
        nodeNext = pERHT->FindNextMostSpecificContainer(node,
                                                        offTo);
    }

    // If it was the intention to actually set the IP and the above transition checks succeeded,
    // then go back and do it all again but this time widen and narrow the thread's actual scope
    if (!fCanSetIPOnly && fCheckOnly)
    {
        fCheckOnly = false;
        goto retryForCommit;
    }
    
LExit:
    if (pERHT != NULL)
        delete pERHT;

    return hrReturn;
}
 

// This function should only be called if the thread is suspended and sitting in jitted code
BOOL IsInFirstFrameOfHandler(Thread *pThread, IJitManager *pJitManager, METHODTOKEN MethodToken, DWORD offset)
{
    // if don't have a throwable the aren't processing an exception
    if (IsHandleNullUnchecked(pThread->GetThrowableAsHandle()))
        return FALSE;

    EH_CLAUSE_ENUMERATOR pEnumState;
    unsigned EHCount = pJitManager->InitializeEHEnumeration(MethodToken, &pEnumState);

    if (EHCount == 0)
        return FALSE;

    EE_ILEXCEPTION_CLAUSE EHClause, *EHClausePtr;

    for(ULONG i=0; i < EHCount; i++) 
    {  
         EHClausePtr = pJitManager->GetNextEHClause(MethodToken, &pEnumState, &EHClause);
         _ASSERTE(IsValidClause(EHClausePtr));

        if ( offset >= EHClausePtr->HandlerStartPC && offset < EHClausePtr->HandlerEndPC)
            return TRUE;

        // check if it's in the filter itself if we're not in the handler
        if (IsFilterHandler(EHClausePtr) && offset >= EHClausePtr->FilterOffset && offset < EHClausePtr->HandlerStartPC)
            return TRUE;
    }
    return FALSE;
}



LFH LookForHandler(const EXCEPTION_POINTERS *pExceptionPointers, Thread *pThread, ThrowCallbackType *tct)
{
    if (pExceptionPointers->ExceptionRecord->ExceptionCode == EXCEPTION_COMPLUS &&
                GetIP(pExceptionPointers->ContextRecord) != gpRaiseExceptionIP)
    {
        // normally we would return LFH_NOT_FOUND, however, there is one case where we
        // throw a COMPlusException (from ThrowControlForThread), we need to check for that case
        // The check relies on the fact that when we do throw control for thread, we record the 
        // ip of managed code into m_OSContext. So just check to see that the IP in the context record 
        // matches it.
        if ((pThread->m_OSContext == NULL)  ||
            (GetIP(pThread->m_OSContext) != GetIP(pExceptionPointers->ContextRecord)))
            return LFH_NOT_FOUND; //will cause continue_search
    }
 
    // Make sure that the stack depth counter is set ro zero.
    COUNTER_ONLY(GetPrivatePerfCounters().m_Excep.cThrowToCatchStackDepth=0);
    COUNTER_ONLY(GetGlobalPerfCounters().m_Excep.cThrowToCatchStackDepth=0);
    // go through to find if anyone handles the exception
    // @PERF: maybe can skip stackwalk code here and go directly to StackWalkEx.
    StackWalkAction action = 
        pThread->StackWalkFrames((PSTACKWALKFRAMESCALLBACK)COMPlusThrowCallback,
                                 tct,
                                 0,     //can't use FUNCTIONSONLY because the callback uses non-function frames to stop the walk
                                 tct->pBottomFrame
                                );
    // if someone handles it, the action will be SWA_ABORT with pFunc and dHandler indicating the
        // function and handler that is handling the exception. Debugger can put a hook in here.

    if (action == SWA_ABORT && tct->pFunc != NULL)
        return LFH_FOUND;

    // nobody is handling it

    return LFH_NOT_FOUND;
}

StackWalkAction COMPlusUnwindCallback (CrawlFrame *pCf, ThrowCallbackType *pData);

void UnwindFrames(Thread *pThread, ThrowCallbackType *tct)
{
    // Make sure that the stack depth counter is set ro zero.
    COUNTER_ONLY(GetPrivatePerfCounters().m_Excep.cThrowToCatchStackDepth=0);
    COUNTER_ONLY(GetGlobalPerfCounters().m_Excep.cThrowToCatchStackDepth=0);
    pThread->StackWalkFrames((PSTACKWALKFRAMESCALLBACK)COMPlusUnwindCallback,
                         tct,
                         POPFRAMES | (tct->pFunc ? FUNCTIONSONLY : 0),  // can only use FUNCTIONSONLY here if know will stop
                         tct->pBottomFrame);
}

void SaveStackTraceInfo(ThrowCallbackType *pData, ExInfo *pExInfo, OBJECTHANDLE *hThrowable, BOOL bReplaceStack, BOOL bSkipLastElement)
{

    // if have bSkipLastElement, must also keep the stack
    _ASSERTE(! bSkipLastElement || ! bReplaceStack);

    EEClass *pClass = ObjectFromHandle(*hThrowable)->GetTrueClass();

    if (! pData->bAllowAllocMem || pExInfo->m_dFrameCount == 0) {
        pExInfo->ClearStackTrace();
        if (bReplaceStack && IsException(pClass)) {
            FieldDesc *pStackTraceFD = g_Mscorlib.GetField(FIELD__EXCEPTION__STACK_TRACE);
            FieldDesc *pStackTraceStringFD = g_Mscorlib.GetField(FIELD__EXCEPTION__STACK_TRACE_STRING);
            pStackTraceFD->SetRefValue(ObjectFromHandle(*hThrowable), (OBJECTREF)(size_t)NULL);

            pStackTraceStringFD->SetRefValue(ObjectFromHandle(*hThrowable), (OBJECTREF)(size_t)NULL);
        }
        return;
    }

    // the stack trace info is now filled in so copy it to the exception object
    I1ARRAYREF arr = NULL;
    I1 *pI1 = NULL;

    // Only save stack trace info on exceptions
    if (!IsException(pClass))
        return;

    FieldDesc *pStackTraceFD = g_Mscorlib.GetField(FIELD__EXCEPTION__STACK_TRACE);

    int cNewTrace = pExInfo->m_dFrameCount*sizeof(SystemNative::StackTraceElement);
    _ASSERTE(pStackTraceFD != NULL);
    if (bReplaceStack) {
        // nuke previous info
        arr = (I1ARRAYREF)AllocatePrimitiveArray(ELEMENT_TYPE_I1, cNewTrace);
        if (! arr)
            RealCOMPlusThrowOM();
        pI1 = (I1 *)arr->GetDirectPointerToNonObjectElements();
    } else {
        // append to previous info
        unsigned cOrigTrace = 0;    // this is total size of array since each elem is 1 byte
        I1ARRAYREF pOrigTrace = NULL;
        GCPROTECT_BEGIN(pOrigTrace);
        pOrigTrace = (I1ARRAYREF)((size_t)pStackTraceFD->GetValue32(ObjectFromHandle(*hThrowable)));
        if (pOrigTrace != NULL) {
            cOrigTrace = pOrigTrace->GetNumComponents();
        }
        if (bSkipLastElement && cOrigTrace!=0) {            
            cOrigTrace -= sizeof(SystemNative::StackTraceElement);
        }
        arr = (I1ARRAYREF)AllocatePrimitiveArray(ELEMENT_TYPE_I1, cOrigTrace + cNewTrace);
        _ASSERTE(arr->GetNumComponents() % sizeof(SystemNative::StackTraceElement) == 0); 
        if (! arr)
            RealCOMPlusThrowOM();
        pI1 = (I1 *)arr->GetDirectPointerToNonObjectElements();
        if (cOrigTrace && pOrigTrace!=NULL) {
            I1* pI1Orig = (I1 *)pOrigTrace->GetDirectPointerToNonObjectElements();
            memcpyNoGCRefs(pI1, pI1Orig, cOrigTrace);
            pI1 += cOrigTrace;
        }
        GCPROTECT_END();
    }
    memcpyNoGCRefs(pI1, pExInfo->m_pStackTrace, cNewTrace);
    pExInfo->ClearStackTrace();
    pStackTraceFD->SetRefValue(ObjectFromHandle(*hThrowable), (OBJECTREF)arr);

    FieldDesc *pStackTraceStringFD = g_Mscorlib.GetField(FIELD__EXCEPTION__STACK_TRACE_STRING);
    pStackTraceStringFD->SetRefValue(ObjectFromHandle(*hThrowable), (OBJECTREF)(size_t)NULL);
}

// Copy a context record, being careful about whether or not the target
// is large enough to support CONTEXT_EXTENDED_REGISTERS.


// High 2 bytes are machine type.  Low 2 bytes are register subset.
#define CONTEXT_EXTENDED_BIT (CONTEXT_EXTENDED_REGISTERS & 0xffff)

VOID
ReplaceExceptionContextRecord(CONTEXT *pTarget, CONTEXT *pSource) {
    _ASSERTE(pTarget);
    _ASSERTE(pSource);

    // Source must be a full register set except, perhaps, the extended registers.
    _ASSERTE(
        (pSource->ContextFlags 
         & (CONTEXT_FULL | CONTEXT_FLOATING_POINT | CONTEXT_DEBUG_REGISTERS)) 
        == (CONTEXT_FULL | CONTEXT_FLOATING_POINT | CONTEXT_DEBUG_REGISTERS));

#ifdef CONTEXT_EXTENDED_REGISTERS
    if (pSource->ContextFlags & CONTEXT_EXTENDED_BIT) {
        if (pTarget->ContextFlags & CONTEXT_EXTENDED_BIT) {
            *pTarget = *pSource;
        } else {
            memcpy(pTarget, pSource, offsetof(CONTEXT, ExtendedRegisters));
            pTarget->ContextFlags &= ~CONTEXT_EXTENDED_BIT;  // Target was short.  Reset the extended bit.
        }
    } else {
        memcpy(pTarget, pSource, offsetof(CONTEXT, ExtendedRegisters));
    }
#else // !CONTEXT_EXTENDED_REGISTERS
    *pTarget = *pSource;
#endif // !CONTEXT_EXTENDED_REGISTERS
}

VOID FixupOnRethrow(Thread *pCurThread, EXCEPTION_POINTERS *pExceptionPointers)
{
    ExInfo *pExInfo = pCurThread->GetHandlerInfo();

    // Don't allow rethrow of a STATUS_STACK_OVERFLOW -- it's a new throw of the COM+ exception.
    if (pExInfo->m_ExceptionCode == STATUS_STACK_OVERFLOW) {
        gpRaiseExceptionIP = GetIP(pExceptionPointers->ContextRecord);
        return;
    }

    // For COMPLUS exceptions, we don't need the original context for our rethrow.
    if (pExInfo->m_ExceptionCode != EXCEPTION_COMPLUS) {
        _ASSERTE(pExInfo->m_pExceptionRecord);
        _ASSERTE(pExInfo->m_pContext);

        // don't copy parm args as have already supplied them on the throw
        memcpy((void *)pExceptionPointers->ExceptionRecord, (void *)pExInfo->m_pExceptionRecord, offsetof(EXCEPTION_RECORD, ExceptionInformation));

        // Restore original context if available.
        if (pExInfo->m_pContext) {
            ReplaceExceptionContextRecord(pExceptionPointers->ContextRecord,
                                          pExInfo->m_pContext);
        }
    }

    pExInfo->SetIsRethrown();
}

//==========================================================================
// Throw an object.
//==========================================================================
#ifdef _IA64_
VOID RealCOMPlusThrow(OBJECTREF throwable, BOOL rethrow)
{
    _ASSERTE(!"RealCOMPlusThrow - NOT IMPLEMENTED");
}
#else // !_IA64_

VOID RaiseTheException(OBJECTREF throwable, BOOL rethrow)
{
    THROWSCOMPLUSEXCEPTION();

    LOG((LF_EH, LL_INFO100, "RealCOMPlusThrow throwing %s\n", 
        throwable->GetTrueClass()->m_szDebugClassName));

    if (throwable == NULL) {
        _ASSERTE(!"RealCOMPlusThrow(OBJECTREF) called with NULL argument. Somebody forgot to post an exception!");
        FATAL_EE_ERROR();
    }

    Thread *pThread = GetThread();
    _ASSERTE(pThread);
    ExInfo *pExInfo = pThread->GetHandlerInfo();
    _ASSERTE(pExInfo);

    // raise
    __try {
        //_ASSERTE(! rethrow || pExInfo->m_pExceptionRecord);
        ULONG_PTR *args;
        ULONG argCount;
        ULONG flags;
        ULONG code;

       
        // always save the current object in the handle so on rethrow we can reuse it. This
        // is important as it contains stack trace info.

        pThread->SetLastThrownObject(throwable);

        if (!rethrow 
                || pExInfo->m_ExceptionCode == EXCEPTION_COMPLUS
                || pExInfo->m_ExceptionCode == STATUS_STACK_OVERFLOW) {
            args = NULL;
            argCount = 0;
            flags = EXCEPTION_NONCONTINUABLE;
            code = EXCEPTION_COMPLUS;
        } else {
            // Exception code should be consistent.
            _ASSERTE(pExInfo->m_pExceptionRecord->ExceptionCode == pExInfo->m_ExceptionCode);

            args = pExInfo->m_pExceptionRecord->ExceptionInformation;
            argCount = pExInfo->m_pExceptionRecord->NumberParameters;
            flags = pExInfo->m_pExceptionRecord->ExceptionFlags;
            code = pExInfo->m_pExceptionRecord->ExceptionCode;
        }
        // enable preemptive mode before call into OS
        pThread->EnablePreemptiveGC();

        RaiseException(code, flags, argCount, args);
    } __except(
        // need to reset the EH info back to the original thrown exception
        rethrow ? FixupOnRethrow(pThread, GetExceptionInformation()) : 

        // We want to use the IP of the exception context to distinguish
        // true COM+ throws (exceptions raised from this function) from
        // forgeries (exceptions thrown by other code using our exception
        // code.) To do, so we need to save away the eip of this call site -
        // the easiest way to do this is to intercept our own exception
        // and suck the ip out of the exception context.
        gpRaiseExceptionIP = GetIP((GetExceptionInformation())->ContextRecord),

        EXCEPTION_CONTINUE_SEARCH

      ) {
    }

    _ASSERTE(!"Cannot continue after COM+ exception");      // Debugger can bring you here.
    SafeExitProcess(0);                                     // We can't continue.
}

VOID RealCOMPlusThrow(OBJECTREF throwable, BOOL rethrow) {
    INSTALL_COMPLUS_EXCEPTION_HANDLER();
    RaiseTheException(throwable, rethrow);
    UNINSTALL_COMPLUS_EXCEPTION_HANDLER();
}

#endif // !_IA64_

VOID RealCOMPlusThrow(OBJECTREF throwable)
{
    RealCOMPlusThrow(throwable, FALSE);
}

//==========================================================================
// Throw an undecorated runtime exception.
//==========================================================================
VOID RealCOMPlusThrow(RuntimeExceptionKind reKind)
{
    if (reKind == kExecutionEngineException)
        FATAL_EE_ERROR();
    
    THROWSCOMPLUSEXCEPTION();
    RealCOMPlusThrowWorker(reKind, FALSE, FALSE, 0,0,0,0,0);
}

//==========================================================================
// Throw a decorated runtime exception.
// Try using RealCOMPlusThrow(reKind, wszResourceName) instead.
//==========================================================================
VOID RealCOMPlusThrowNonLocalized(RuntimeExceptionKind reKind, LPCWSTR wszTag)
{
    THROWSCOMPLUSEXCEPTION();
    RealCOMPlusThrowWorker(reKind, TRUE, FALSE, 0,0,wszTag,0,0);
}

//==========================================================================
// Throw a decorated runtime exception with a localized message.
// Queries the ResourceManager for a corresponding resource value.
//==========================================================================
VOID RealCOMPlusThrow(RuntimeExceptionKind reKind, LPCWSTR wszResourceName)
{
    THROWSCOMPLUSEXCEPTION();
    _ASSERTE(wszResourceName);  // You should add a resource.

    LPWSTR wszValue = NULL;
    STRINGREF str = NULL;
    ResMgrGetString(wszResourceName, &str);
    if (str != NULL) {
        int len;
        RefInterpretGetStringValuesDangerousForGC(str, (LPWSTR*)&wszValue, &len);
    }
    RealCOMPlusThrowWorker(reKind, TRUE, FALSE, 0,0, wszValue,0,0);
}


// This function does poentially a LOT of work (loading possibly 50 classes).
// The return value is an un-GC-protected string ref, or possibly NULL.
void ResMgrGetString(LPCWSTR wszResourceName, STRINGREF * ppMessage)
{
    _ASSERTE(ppMessage != NULL);
    OBJECTREF ResMgr = NULL;
    STRINGREF name = NULL;

    MethodDesc* pInitResMgrMeth = g_Mscorlib.GetMethod(METHOD__ENVIRONMENT__INIT_RESOURCE_MANAGER);

    ResMgr = Int64ToObj(pInitResMgrMeth->Call((INT64*)NULL, 
                                              METHOD__ENVIRONMENT__INIT_RESOURCE_MANAGER));

    GCPROTECT_BEGIN(ResMgr);

    // Call ResourceManager::GetString(String name).  Returns String value (or maybe null)
    MethodDesc* pMeth = g_Mscorlib.GetMethod(METHOD__RESOURCE_MANAGER__GET_STRING);

    // No GC-causing actions occur after this line.
    name = COMString::NewString(wszResourceName);
    
    LPCWSTR wszValue = wszResourceName;
    if (ResMgr == NULL || wszResourceName == NULL)
        goto exit;

    {
         // Don't need to GCPROTECT pArgs, since it's not used after the function call.
        INT64 pArgs[2] = { ObjToInt64(ResMgr), ObjToInt64(name) };
        STRINGREF value = (STRINGREF) Int64ToObj(pMeth->Call(pArgs, 
                                                             METHOD__RESOURCE_MANAGER__GET_STRING));
        _ASSERTE(value!=NULL || !"Resource string lookup failed - possible misspelling or .resources missing or out of date?");
        *ppMessage = value;
    }

exit:
    GCPROTECT_END();
}

//==========================================================================
// Throw a decorated runtime exception.
//==========================================================================
VOID __cdecl RealCOMPlusThrow(RuntimeExceptionKind  reKind,
                              UINT                  resID)
{
    THROWSCOMPLUSEXCEPTION();
    RealCOMPlusThrow(reKind, resID, NULL, NULL, NULL);
}


//==========================================================================
// Throw a decorated runtime exception.
//==========================================================================
VOID __cdecl RealCOMPlusThrow(RuntimeExceptionKind  reKind,
                              UINT                  resID,
                              LPCWSTR               wszArg1)
{
    THROWSCOMPLUSEXCEPTION();
    RealCOMPlusThrow(reKind, resID, wszArg1, NULL, NULL);
}

//==========================================================================
// Throw a decorated runtime exception.
//==========================================================================
VOID __cdecl RealCOMPlusThrow(RuntimeExceptionKind  reKind,
                              UINT                  resID,
                              LPCWSTR               wszArg1,
                              LPCWSTR               wszArg2)
{
    THROWSCOMPLUSEXCEPTION();
    RealCOMPlusThrow(reKind, resID, wszArg1, wszArg2, NULL);
}

//==========================================================================
// Throw a decorated runtime exception.
//==========================================================================
VOID __cdecl RealCOMPlusThrow(RuntimeExceptionKind  reKind,
                              UINT                  resID,
                              LPCWSTR               wszArg1,
                              LPCWSTR               wszArg2,
                              LPCWSTR               wszArg3)

{
    THROWSCOMPLUSEXCEPTION();
    RealCOMPlusThrowWorker(reKind, TRUE, TRUE, resID, 0, wszArg1, wszArg2, wszArg3);
}


void FreeExceptionData(ExceptionData *pedata)
{
    _ASSERTE(pedata != NULL);

    // @NICE: At one point, we had the comment:
    //     (DM) Remove this when shutdown works better.
    // This test may no longer be necessary.  Remove at own peril.
    Thread *pThread = GetThread();
    if (!pThread)
        return;

    if (pedata->bstrSource)
        SysFreeString(pedata->bstrSource);
    if (pedata->bstrDescription)
        SysFreeString(pedata->bstrDescription);
    if (pedata->bstrHelpFile)
        SysFreeString(pedata->bstrHelpFile);
}


VOID RealCOMPlusThrowHRWorker(HRESULT hr, ExceptionData *pData, UINT resourceID, LPCWSTR wszArg1, LPCWSTR wszArg2)
{
    THROWSCOMPLUSEXCEPTION();

    if (!g_fExceptionsOK)
        COMPlusThrowBoot(hr);

    _ASSERTE(pData == NULL || hr == pData->hr);

    RuntimeExceptionKind  reKind = kException;
    BOOL bMatch = FALSE;
    int i;

    for (i = 0; i < kLastException; i++)
    {
        for (int j = 0; j < gExceptionHRInfos[i].cHRs; j++)
        {
            if (gExceptionHRInfos[i].aHRs[j] == hr)
            {
                bMatch = TRUE;
                break;
            }
        }

        if (bMatch)
            break;
    }

    reKind = (i != kLastException) ? (RuntimeExceptionKind)i : kCOMException;
    
    if (pData != NULL)
    {
        RealCOMPlusThrowWorker(reKind, FALSE, FALSE, 0, hr, NULL, NULL, NULL, pData);
    }
    else
    {   
        WCHAR   numBuff[40];
        Wszultow(hr, numBuff, 16 /*hex*/);
        if(resourceID == 0) 
        {
            bool fMessage;
            fMessage = wszArg1 ? TRUE : FALSE;
            // It doesn't make sense to have a second string without a ResourceID.
            _ASSERTE (!wszArg2);
            RealCOMPlusThrowWorker(reKind, fMessage, FALSE, 0, hr, wszArg1, NULL, NULL);
        }
        else 
        {
            RealCOMPlusThrowWorker(reKind, TRUE, TRUE, 
                                   resourceID, 
                                   hr, 
                                   numBuff,
                                   wszArg1,
                                   wszArg2);
        }            
    }
}

VOID RealCOMPlusThrowHRWorker(HRESULT hr, ExceptionData *pData,  LPCWSTR wszArg1 = NULL)
{
    RealCOMPlusThrowHRWorker(hr, pData, 0, wszArg1, NULL);
}

//==========================================================================
// Throw a runtime exception based on an HResult
//==========================================================================
VOID RealCOMPlusThrowHR(HRESULT hr, IErrorInfo* pErrInfo )
{
    THROWSCOMPLUSEXCEPTION();
    
    if (!g_fExceptionsOK)
        COMPlusThrowBoot(hr);

    // check for complus created IErrorInfo pointers
    if (pErrInfo != NULL && IsComPlusTearOff(pErrInfo))
    {
        OBJECTREF oref = GetObjectRefFromComIP(pErrInfo);
        _ASSERTE(oref != NULL);
        GCPROTECT_BEGIN (oref);
        ULONG cbRef= SafeRelease(pErrInfo);
        LogInteropRelease(pErrInfo, cbRef, "IErrorInfo release");
        RealCOMPlusThrow(oref);
        GCPROTECT_END ();
    }
   
    if (pErrInfo != NULL)
    {           
        ExceptionData edata;
        edata.hr = hr;
        edata.bstrDescription = NULL;
        edata.bstrSource = NULL;
        edata.bstrHelpFile = NULL;
        edata.dwHelpContext = NULL;
        edata.guid = GUID_NULL;
    
        FillExceptionData(&edata, pErrInfo);

        EE_TRY_FOR_FINALLY
        {
            RealCOMPlusThrowHRWorker(hr, &edata);
        }
        EE_FINALLY
        {
            FreeExceptionData(&edata); // free the BStrs
        } 
        EE_END_FINALLY;
    }
    else
    {
        RealCOMPlusThrowHRWorker(hr, NULL);
    }
}


VOID RealCOMPlusThrowHR(HRESULT hr)
{
    THROWSCOMPLUSEXCEPTION();
    IErrorInfo *pErrInfo = NULL;
    if (GetErrorInfo(0, &pErrInfo) != S_OK)
        pErrInfo = NULL;
    RealCOMPlusThrowHR(hr, pErrInfo);        
}


VOID RealCOMPlusThrowHR(HRESULT hr, LPCWSTR wszArg1)
{
    THROWSCOMPLUSEXCEPTION();
    RealCOMPlusThrowHRWorker(hr, NULL, wszArg1);
}


VOID RealCOMPlusThrowHR(HRESULT hr, UINT resourceID, LPCWSTR wszArg1, LPCWSTR wszArg2)
{
    THROWSCOMPLUSEXCEPTION();
    RealCOMPlusThrowHRWorker(hr, NULL, resourceID, wszArg1, wszArg2);
}


//==========================================================================
// Throw a runtime exception based on an HResult, check for error info
//==========================================================================
VOID RealCOMPlusThrowHR(HRESULT hr, IUnknown *iface, REFIID riid)
{
    THROWSCOMPLUSEXCEPTION();
    IErrorInfo *info = NULL;
    GetSupportedErrorInfo(iface, riid, &info);
    RealCOMPlusThrowHR(hr, info);
}


//==========================================================================
// Throw a runtime exception based on an EXCEPINFO. This function will free
// the strings in the EXCEPINFO that is passed in.
//==========================================================================
VOID RealCOMPlusThrowHR(EXCEPINFO *pExcepInfo)
{
    THROWSCOMPLUSEXCEPTION();

    // If there is a fill in function then call it to retrieve the filled in EXCEPINFO.
    EXCEPINFO FilledInExcepInfo;
    if (pExcepInfo->pfnDeferredFillIn)
    {
        HRESULT hr = pExcepInfo->pfnDeferredFillIn(&FilledInExcepInfo);
        if (SUCCEEDED(hr))
        {
            // Free the strings in the original EXCEPINFO.
            if (pExcepInfo->bstrDescription)
            {
                SysFreeString(pExcepInfo->bstrDescription);
                pExcepInfo->bstrDescription = NULL;
            }
            if (pExcepInfo->bstrSource)
            {
                SysFreeString(pExcepInfo->bstrSource);
                pExcepInfo->bstrSource = NULL;
            }
            if (pExcepInfo->bstrHelpFile)
            {
                SysFreeString(pExcepInfo->bstrHelpFile);
                pExcepInfo->bstrHelpFile = NULL;
            }

            // Set the ExcepInfo pointer to the filled in one.
            pExcepInfo = &FilledInExcepInfo;
        }
    }

    // Extract the required information from the EXCEPINFO.
    ExceptionData edata;
    edata.hr = pExcepInfo->scode;
    edata.bstrDescription = pExcepInfo->bstrDescription;
    edata.bstrSource = pExcepInfo->bstrSource;
    edata.bstrHelpFile = pExcepInfo->bstrHelpFile;
    edata.dwHelpContext = pExcepInfo->dwHelpContext;
    edata.guid = GUID_NULL;

    // Zero the EXCEPINFO.
    memset(pExcepInfo, NULL, sizeof(EXCEPINFO));

    // Call the RealCOMPlusThrowHRWorker to do the actual work of throwing the exception.
    EE_TRY_FOR_FINALLY
    {
        RealCOMPlusThrowHRWorker(edata.hr, &edata);
    }
    EE_FINALLY
    {
        FreeExceptionData(&edata); // free the BStrs
    } 
    EE_END_FINALLY;
}


//==========================================================================
// Throw a runtime exception based on the last Win32 error (GetLastError())
//==========================================================================
VOID RealCOMPlusThrowWin32()
{
    THROWSCOMPLUSEXCEPTION();

    // before we do anything else...
    DWORD   err = ::GetLastError();
    WCHAR   wszBuff[FORMAT_MESSAGE_BUFFER_LENGTH];
    WCHAR  *wszFinal = wszBuff;

    DWORD res = WszFormatMessage(FORMAT_MESSAGE_FROM_SYSTEM | FORMAT_MESSAGE_IGNORE_INSERTS,
                                 NULL         /*ignored msg source*/,
                                 err,
                                 0            /*pick appropriate languageId*/,
                                 wszFinal,
                                 FORMAT_MESSAGE_BUFFER_LENGTH-1,
                                 0            /*arguments*/);
    if (res == 0) 
        RealCOMPlusThrowPreallocated();

    // Either way, we now have the formatted string from the system.
    RealCOMPlusThrowNonLocalized(kApplicationException, wszFinal);
}

//==========================================================================
// Throw a runtime exception based on the last Win32 error (GetLastError())
// with one string argument.  Note that the number & kind & interpretation
// of each error message is specific to each HResult.  
// This is a nasty hack done wrong, but it doesn't matter since this should
// be used extremely infrequently.
// As of 8/98, in winerror.h, there are 24 HResult messages with %1's in them,
// and only 2 with %2's.  There is only one with a %3 in it.  This is out of
// 1472 error messages.
//==========================================================================
VOID RealCOMPlusThrowWin32(DWORD hr, WCHAR* arg)
{
    THROWSCOMPLUSEXCEPTION();

    // before we do anything else...
    WCHAR   wszBuff[FORMAT_MESSAGE_BUFFER_LENGTH];

    DWORD res = WszFormatMessage(FORMAT_MESSAGE_FROM_SYSTEM | FORMAT_MESSAGE_ARGUMENT_ARRAY,
                                 NULL         /*ignored msg source*/,
                                 hr,
                                 0            /*pick appropriate languageId*/,
                                 wszBuff,
                                 FORMAT_MESSAGE_BUFFER_LENGTH-1,
                                 (va_list*)(char**) &arg            /*arguments*/);
    if (res == 0) {
        RealCOMPlusThrowPreallocated();
    }

    // Either way, we now have the formatted string from the system.
    RealCOMPlusThrowNonLocalized(kApplicationException, wszBuff);
}

//==========================================================================
// Throw an OutOfMemoryError
//==========================================================================
VOID RealCOMPlusThrowOM()
{
    THROWSCOMPLUSEXCEPTION();
    RealCOMPlusThrow(ObjectFromHandle(g_pPreallocatedOutOfMemoryException));
}



//==========================================================================
// Throw an ArithmeticException
//==========================================================================
VOID RealCOMPlusThrowArithmetic()
{
    THROWSCOMPLUSEXCEPTION();
    RealCOMPlusThrow(kArithmeticException);
}


//==========================================================================
// Throw an ArgumentNullException
//==========================================================================
VOID RealCOMPlusThrowArgumentNull(LPCWSTR argName, LPCWSTR wszResourceName)
{
    THROWSCOMPLUSEXCEPTION();
    _ASSERTE(wszResourceName);

    ProtectArgsStruct prot;
    memset(&prot, 0, sizeof(ProtectArgsStruct));
    GCPROTECT_BEGIN(prot);
    ResMgrGetString(wszResourceName, &prot.s1);

    CreateArgumentExceptionObject(kArgumentNullException, argName, prot.s1, &prot.pThrowable);
    RealCOMPlusThrow(prot.pThrowable);
    GCPROTECT_END();
}


VOID RealCOMPlusThrowArgumentNull(LPCWSTR argName)
{
    THROWSCOMPLUSEXCEPTION();

    OBJECTREF throwable = NULL;
    GCPROTECT_BEGIN(throwable);
    // This will work - the ArgumentNullException constructor that takes one string takes an 
    // argument name, not a message.  While this next method expects a message, we'll live just fine.
    CreateExceptionObject(kArgumentNullException, argName, &throwable);
    RealCOMPlusThrow(throwable);
    GCPROTECT_END();
}


//==========================================================================
// Throw an ArgumentOutOfRangeException
//==========================================================================
VOID RealCOMPlusThrowArgumentOutOfRange(LPCWSTR argName, LPCWSTR wszResourceName)
{
    THROWSCOMPLUSEXCEPTION();

    ProtectArgsStruct prot;
    memset(&prot, 0, sizeof(ProtectArgsStruct));
    GCPROTECT_BEGIN(prot);
    ResMgrGetString(wszResourceName, &prot.s1);

    CreateArgumentExceptionObject(kArgumentOutOfRangeException, argName, prot.s1, &prot.pThrowable);
    RealCOMPlusThrow(prot.pThrowable);
    GCPROTECT_END();
}

//==========================================================================
// Throw an ArgumentException
//==========================================================================
VOID RealCOMPlusThrowArgumentException(LPCWSTR argName, LPCWSTR wszResourceName)
{
    THROWSCOMPLUSEXCEPTION();

    ProtectArgsStruct prot;
    memset(&prot, 0, sizeof(ProtectArgsStruct));
    GCPROTECT_BEGIN(prot);
    ResMgrGetString(wszResourceName, &prot.s1);

    CreateArgumentExceptionObject(kArgumentException, argName, prot.s1, &prot.pThrowable);
    RealCOMPlusThrow(prot.pThrowable);
    GCPROTECT_END();
}


//==========================================================================
// Re-Throw the last error. Don't call this - use EE_FINALLY instead
//==========================================================================
VOID RealCOMPlusRareRethrow()
{
    THROWSCOMPLUSEXCEPTION();
    LOG((LF_EH, LL_INFO100, "RealCOMPlusRareRethrow\n"));

    OBJECTREF throwable = GETTHROWABLE();
    if (throwable != NULL)
        RealCOMPlusThrow(throwable, TRUE);
    else
        // This can only be the result of bad IL (or some internal EE failure).
        RealCOMPlusThrow(kInvalidProgramException, (UINT)IDS_EE_RETHROW_NOT_ALLOWED);
}

//
// Maps a Win32 fault to a COM+ Exception enumeration code
//
// Returns 0xFFFFFFFF if it cannot be mapped.
//
DWORD MapWin32FaultToCOMPlusException(DWORD Code)
{
    switch (Code)
    {
        case STATUS_FLOAT_INEXACT_RESULT:
        case STATUS_FLOAT_INVALID_OPERATION:
        case STATUS_FLOAT_STACK_CHECK:
        case STATUS_FLOAT_UNDERFLOW:
            return (DWORD) kArithmeticException;
        case STATUS_FLOAT_OVERFLOW:
        case STATUS_INTEGER_OVERFLOW:
            return (DWORD) kOverflowException;

        case STATUS_FLOAT_DIVIDE_BY_ZERO:
        case STATUS_INTEGER_DIVIDE_BY_ZERO:
            return (DWORD) kDivideByZeroException;

        case STATUS_FLOAT_DENORMAL_OPERAND:
            return (DWORD) kFormatException;

        case STATUS_ACCESS_VIOLATION:
            return (DWORD) kNullReferenceException;

        case STATUS_ARRAY_BOUNDS_EXCEEDED:
            return (DWORD) kIndexOutOfRangeException;

        case STATUS_NO_MEMORY:
            return (DWORD) kOutOfMemoryException;

        case STATUS_STACK_OVERFLOW:
            return (DWORD) kStackOverflowException;

        default:
            return kSEHException;
    }
}


TRtlUnwind GetRtlUnwind()
{
    static TRtlUnwind pRtlUnwind = NULL;
    if (! pRtlUnwind)
    {
        //  We will load the Kernel32.DLL and look for RtlUnwind.
        //  If this is avaialble we can proceed with the excption handling scenario,
        //  in the other case we will fail

        HINSTANCE   hiKernel32;         // the handle to Kernel32

        hiKernel32 = WszGetModuleHandle(L"Kernel32.DLL");
        if (hiKernel32)
        {
            // we got the handle now let's get the address
            pRtlUnwind = (TRtlUnwind) GetProcAddress(hiKernel32, "RtlUnwind");
            // everything is supposed to be fine if we got a pointer to the function...
            if (! pRtlUnwind)
            {
                _ASSERTE(0); // RtlUnwind was not found
                return NULL;
            }
        }
    }
    return pRtlUnwind;
}

// on x86 at least, RtlUnwind always returns, but provide this so can catch
// otherwise
void RtlUnwindCallback()
{
    _ASSERTE(!"Should not get here");
}


#ifdef _DEBUG
// check if anyone has written to the stack above the handler which would wipe out the EH registration
void CheckStackBarrier(EXCEPTION_REGISTRATION_RECORD *exRecord)
{ 
    if (exRecord->Handler != COMPlusFrameHandler)
        return;
    DWORD *stackOverwriteBarrier = (DWORD *)(((char *)exRecord) - STACK_OVERWRITE_BARRIER_SIZE * sizeof(DWORD)); 
    for (int i =0; i < STACK_OVERWRITE_BARRIER_SIZE; i++) { 
        if (*(stackOverwriteBarrier+i) != STACK_OVERWRITE_BARRIER_VALUE) {
            // to debug this error, you must determine who erroneously overwrote the stack
            _ASSERTE(!"Fatal error: the stack has been overwritten");
        }
    }
}
#endif

//
//-------------------------------------------------------------------------
// This is installed to indicate a function that cannot allow a COMPlus exception
// to be thrown past it.
//-------------------------------------------------------------------------
#ifdef _DEBUG
EXCEPTION_DISPOSITION __cdecl COMPlusCannotThrowExceptionHandler(EXCEPTION_RECORD *pExceptionRecord, 
                         EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame,
                         CONTEXT *pContext,
                         void *DispatcherContext)
{
    if (pExceptionRecord->ExceptionCode != STATUS_BREAKPOINT
        && pExceptionRecord->ExceptionCode != STATUS_ACCESS_VIOLATION)
        _ASSERTE(!"Exception thrown past CANNOTTHROWCOMPLUSEXCEPTION boundary");
    return ExceptionContinueSearch;
}

EXCEPTION_DISPOSITION __cdecl COMPlusCannotThrowExceptionMarker(EXCEPTION_RECORD *pExceptionRecord, 
                         EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame,
                         CONTEXT *pContext,
                         void *DispatcherContext)
{
    return ExceptionContinueSearch;
}
#endif

//-------------------------------------------------------------------------
// A marker for unmanaged -> EE transition when we know we're in cooperative
// gc mode.  As we leave the EE, we fix a few things:
//
//      - the gc state must be set back to co-operative
//      - the COM+ frame chain must be rewound to what it was on entry
//      - ExInfo()->m_pSearchBoundary must be adjusted
//        if we popped the frame that is identified as begnning the next
//        crawl.
//-------------------------------------------------------------------------
EXCEPTION_DISPOSITION __cdecl COMPlusCooperativeTransitionHandler(
    EXCEPTION_RECORD *pExceptionRecord, 
    EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame,
    CONTEXT *pContext,
    void *pDispatcherContext)
{
    if (IS_UNWINDING(pExceptionRecord->ExceptionFlags)) {

        LOG((LF_EH, LL_INFO1000, "COMPlusCooprativeTransitionHandler unwinding\n"));

        // Fetch a few things we need.
        Thread* pThread = GetThread();
        _ASSERTE(pThread);

        // Restore us to cooperative gc mode.
        if (!pThread->PreemptiveGCDisabled())
            pThread->DisablePreemptiveGC();

        // 3rd dword is the frame to which we must unwind.
        Frame *pFrame = (Frame*)((size_t*)pEstablisherFrame)[2];

        // Pop the frame chain.
        UnwindFrameChain(pThread, pFrame);

        _ASSERTE(pFrame == pThread->GetFrame());

        // An exception is being thrown through here.  The COM+ exception
        // info keeps a pointer to a frame that is used by the next
        // COM+ Exception Handler as the starting point of its crawl.
        // We may have popped this marker -- in which case, we need to
        // update it to the current frame.
        // 
        ExInfo *pExInfo = pThread->GetHandlerInfo();
        if (   pExInfo 
                && pExInfo->m_pSearchBoundary 
                && pExInfo->m_pSearchBoundary < pFrame) {

        LOG((LF_EH, LL_INFO1000, "\tpExInfo->m_pSearchBoundary = %08x\n", (void*)pFrame));
            pExInfo->m_pSearchBoundary = pFrame;
        }
    }

    return ExceptionContinueSearch;     // Same for both DISPATCH and UNWIND
}
  
//
//-------------------------------------------------------------------------
// This is installed when we call COMPlusFrameHandler to provide a bound to
// determine when are within a nested exception
//-------------------------------------------------------------------------
EXCEPTION_DISPOSITION __cdecl COMPlusNestedExceptionHandler(EXCEPTION_RECORD *pExceptionRecord, 
                         EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame,
                         CONTEXT *pContext,
                         void *pDispatcherContext)
{
    if (pExceptionRecord->ExceptionFlags & (EXCEPTION_UNWINDING | EXCEPTION_EXIT_UNWIND)) {

        LOG((LF_EH, LL_INFO100, "    COMPlusNestedHandler(unwind) with %x at %x\n", pExceptionRecord->ExceptionCode, GetIP(pContext)));

        // We're unwinding past a nested exception record, which means that we've thrown
        // a new excecption out of a region in which we're handling a previous one.  The
        // previous exception is overridden -- and needs to be unwound.

        // The preceding is ALMOST true.  There is one more case, where we use setjmp/longjmp
        // from withing a nested handler.  We won't have a nested exception in that case -- just
        // the unwind.

        Thread *pThread = GetThread();
        _ASSERTE(pThread);
        ExInfo *pExInfo = pThread->GetHandlerInfo();
        ExInfo *pPrevNestedInfo = pExInfo->m_pPrevNestedInfo;

        if (pPrevNestedInfo == &((NestedHandlerExRecord*)pEstablisherFrame)->m_handlerInfo) {

            _ASSERTE(pPrevNestedInfo);

            if (pPrevNestedInfo->m_pThrowable != NULL) {
                DestroyHandle(pPrevNestedInfo->m_pThrowable);
            }
            pPrevNestedInfo->FreeStackTrace();
            pExInfo->m_pPrevNestedInfo = pPrevNestedInfo->m_pPrevNestedInfo;

        } else {
            // The whacky setjmp/longjmp case.  Nothing to do.
        }

    } else {
        LOG((LF_EH, LL_INFO100, "    InCOMPlusNestedHandler with %x at %x\n", pExceptionRecord->ExceptionCode, GetIP(pContext)));
    }


    // There is a nasty "gotcha" in the way exception unwinding, finally's, and nested exceptions
    // interact.  Here's the scenario ... it involves two exceptions, one normal one, and one
    // raised in a finally.
    //
    // The first exception occurs, and is caught by some handler way up the stack.  That handler
    // calls RtlUnwind -- and handlers that didn't catch this first exception are called again, with
    // the UNWIND flag set.  If, one of the handlers throws an exception during
    // unwind (like, a throw from a finally) -- then that same handler is not called during
    // the unwind pass of the second exception.  [ASIDE: It is called on first-pass.]
    //
    // What that means is -- the COMPlusExceptionHandler, can't count on unwinding itself correctly
    // if an exception is thrown from a finally.  Instead, it relies on the NestedExceptionHandler
    // that it pushes for this. 
    //

    EXCEPTION_DISPOSITION retval = COMPlusFrameHandler(pExceptionRecord, pEstablisherFrame, pContext, pDispatcherContext);
    LOG((LF_EH, LL_INFO100, "Leaving COMPlusNestedExceptionHandler with %d\n", retval));
    return retval;
}

EXCEPTION_REGISTRATION_RECORD *FindNestedEstablisherFrame(EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame)
{
    while (pEstablisherFrame->Handler != COMPlusNestedExceptionHandler) {
        pEstablisherFrame = pEstablisherFrame->Next;
        _ASSERTE((SSIZE_T)pEstablisherFrame != -1);   // should always find one
    }
    return pEstablisherFrame;
}

ExInfo *FindNestedExInfo(EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame)
{
    while (pEstablisherFrame->Handler != COMPlusNestedExceptionHandler) {
        pEstablisherFrame = pEstablisherFrame->Next;
        _ASSERTE((SSIZE_T)pEstablisherFrame != -1);   // should always find one
    }
    return &((NestedHandlerExRecord*)pEstablisherFrame)->m_handlerInfo;
}


ExInfo& ExInfo::operator=(const ExInfo &from)
{
    LOG((LF_EH, LL_INFO100, "In ExInfo::operator=()\n"));

    // The throwable, and the stack address are handled differently.  Save the original 
    // values.
    OBJECTHANDLE pThrowable = m_pThrowable;
    void *stackAddress = this->m_StackAddress;

    // Blast the entire record.
    memcpy(this, &from, sizeof(ExInfo));

    // Preserve the stack address.  It should never change.
    m_StackAddress = stackAddress;

    // memcpy doesnt work for handles ... copy the handle the right way.
    if (pThrowable != NULL)
        DestroyHandle(pThrowable);

    if (from.m_pThrowable != NULL) {
        HHANDLETABLE table = HndGetHandleTable(from.m_pThrowable);
        m_pThrowable = CreateHandle(table, ObjectFromHandle(from.m_pThrowable));
    }

    return *this;
}

void ExInfo::Init()
{
    m_pSearchBoundary = NULL;
    m_pBottomMostHandler = NULL;
    m_pPrevNestedInfo = NULL;
    m_pStackTrace = NULL;
    m_cStackTrace = 0;
    m_dFrameCount = 0;
    m_ExceptionCode = 0xcccccccc;
    m_pExceptionRecord = NULL;
    m_pContext = NULL;
    m_pShadowSP = NULL;
    m_flags = 0;    
    m_StackAddress = this;
    if (m_pThrowable != NULL)
        DestroyHandle(m_pThrowable);
    m_pThrowable = NULL;
}

ExInfo::ExInfo()
{
    m_pThrowable = NULL;
    Init();
}


void ExInfo::FreeStackTrace()
{
    if (m_pStackTrace) {
        delete [] m_pStackTrace;
        m_pStackTrace = NULL;
        m_cStackTrace = 0;
        m_dFrameCount = 0;
    }
}

void ExInfo::ClearStackTrace()
{
    m_dFrameCount = 0;
}


// When hit an endcatch or an unwind and have nested handler info, either 1) have contained a nested exception
// and will continue handling the original or 2) the nested exception was not contained and was 
// thrown beyond the original bounds where the first exception occurred. The way we can tell this
// is from the stack pointer. The topmost nested handler is installed at the point where the exception
// occurred. For a nested exception to be contained, it must be caught within the scope of any code that 
// is called after the nested handler is installed. If it is caught by anything earlier on the stack, it was 
// not contained. So we unwind the nested handlers until we get to one that is higher on the stack than
// esp we will unwind to. If we still have a nested handler, then we have successfully handled a nested 
// exception and should restore the exception settings that we saved so that processing of the 
// original exception can continue. Otherwise the nested exception has gone beyond where the original 
// exception was thrown and therefore replaces the original exception. Will always remove the current
// exception info from the chain.

void UnwindExInfo(ExInfo *pExInfo, VOID* limit) 
{
    // We must be in cooperative mode to do the chaining below
    Thread * pThread = GetThread();

    // The debugger thread will be using this, even though it has no
    // Thread object associated with it.
    _ASSERT((pThread != NULL && pThread->PreemptiveGCDisabled()) ||
            g_pDebugInterface->GetRCThreadId() == GetCurrentThreadId());
            
    ExInfo *pPrevNestedInfo = pExInfo->m_pPrevNestedInfo;

    // At first glance, you would think that each nested exception has
    // been unwound by it's corresponding NestedExceptionHandler.  But that's
    // not necessarily the case.  The following assertion cannot be made here,
    // and the loop is necessary.
    //
    //_ASSERTE(pPrevNestedInfo == 0 || (DWORD)pPrevNestedInfo >= limit);
    //
    // Make sure we've unwound any nested exceptions that we're going to skip over.
    //
    while (pPrevNestedInfo && pPrevNestedInfo->m_StackAddress < limit) {
        if (pPrevNestedInfo->m_pThrowable != NULL)  {
            DestroyHandle(pPrevNestedInfo->m_pThrowable);
        }
        pPrevNestedInfo->FreeStackTrace();
        ExInfo *pPrev = pPrevNestedInfo->m_pPrevNestedInfo;
        if (pPrevNestedInfo->IsHeapAllocated())
            delete pPrevNestedInfo;
        pPrevNestedInfo = pPrev;
    }

    // either clear the one we're about to copy over or the topmost one
    pExInfo->FreeStackTrace();

    if (pPrevNestedInfo) {
        // found nested handler info that is above the esp restore point so succesfully caught nested
        LOG((LF_EH, LL_INFO100, "UnwindExInfo: resetting nested info to 0x%08x\n", pPrevNestedInfo));
        *pExInfo = *pPrevNestedInfo;
        pPrevNestedInfo->Init();        // Clear out the record, freeing handles.
        if (pPrevNestedInfo->IsHeapAllocated())
            delete pPrevNestedInfo;
    } else {
        LOG((LF_EH, LL_INFO100, "UnwindExInfo: m_pBottomMostHandler gets NULL\n"));
        pExInfo->Init();
    }
}

void UnwindFrameChain(Thread* pThread, Frame* limit) {
    BEGIN_ENSURE_COOPERATIVE_GC();
    Frame* pFrame = pThread->m_pFrame;
    while (pFrame != limit) {
        _ASSERTE(pFrame != 0);      
        pFrame->ExceptionUnwind(); 
        pFrame->Pop(pThread);
        pFrame = pThread->GetFrame();
    }                                     
    END_ENSURE_COOPERATIVE_GC();
}

BOOL ComPlusFrameSEH(EXCEPTION_REGISTRATION_RECORD* pEHR)
{
    return (pEHR->Handler == COMPlusFrameHandler || pEHR->Handler == COMPlusNestedExceptionHandler);
}

BOOL ComPlusCoopFrameSEH(EXCEPTION_REGISTRATION_RECORD* pEHR)
{
    return (pEHR->Handler == COMPlusCooperativeTransitionHandler);
}


#ifdef _DEBUG
BOOL ComPlusCannotThrowSEH(EXCEPTION_REGISTRATION_RECORD* pEHR)
{
    return (   pEHR->Handler == COMPlusCannotThrowExceptionHandler
            || pEHR->Handler == COMPlusCannotThrowExceptionMarker);
}
#endif


#ifdef _DEBUG
//-------------------------------------------------------------------------
// Decide if we are in the scope of a COMPLUS_TRY block.
//
// WARNING: This routine is only used for debug assertions and
// it will fail to detect some cases where COM+ exceptions are
// not actually allowed. In particular, if you execute native code
// from a COMPLUS_TRY block without inserting some kind of frame
// that can signal the transition, ComPlusExceptionsAllowed()
// will return TRUE even within the native code.
//
// The COMPlusFilter() is designed to be precise so that
// it won't be fooled by COM+-looking exceptions thrown from outside
// code (although they shouldn't be doing this anyway because
// COM+ exception code has the "reserved by Microsoft" bit on.)
//-------------------------------------------------------------------------
VOID ThrowsCOMPlusExceptionWorker()
{
    if (!g_fExceptionsOK) {
        // We're at bootup time: diagnostic services suspended.
        return;
    } else {
        Thread *pThread = GetThread();

        EXCEPTION_REGISTRATION_RECORD* pEHR = (EXCEPTION_REGISTRATION_RECORD*) GetCurrentSEHRecord();
        void* pCatchLocation = pThread->m_ComPlusCatchDepth;

        while (pEHR != (void *)-1) { 
            _ASSERTE(pEHR != 0);
            if (ComPlusStubSEH(pEHR) || ComPlusFrameSEH(pEHR) || ComPlusCoopFrameSEH(pEHR)
                                     || NExportSEH(pEHR) || FastNExportSEH(pEHR))
                return;
            if ((void*) pEHR > pCatchLocation)
                return;
            if (ComPlusCannotThrowSEH(pEHR))
                _ASSERTE(!"Throwing an exception here will go through a function with CANNOTTHROWCOMPLUSEXCEPTION");
            pEHR = pEHR->Next;
        }
        // No handlers on the stack.  Might be ok if there are no frames on the stack.
        _ASSERTE(   pThread->m_pFrame == FRAME_TOP 
                 || !"Potential COM+ Exception not guarded by a COMPLUS_TRY." );
    }
}

BOOL IsCOMPlusExceptionHandlerInstalled()
{
    EXCEPTION_REGISTRATION_RECORD* pEHR = (EXCEPTION_REGISTRATION_RECORD*) GetCurrentSEHRecord();
    while (pEHR != (void *)-1) { 
        _ASSERTE(pEHR != 0);
        if (   ComPlusStubSEH(pEHR) 
            || ComPlusFrameSEH(pEHR) 
            || ComPlusCoopFrameSEH(pEHR)
            || NExportSEH(pEHR) 
            || FastNExportSEH(pEHR)
            || ComPlusCannotThrowSEH(pEHR)
           )
            return TRUE;
        pEHR = pEHR->Next;
    }
    // no handlers on the stack
    return FALSE;
}

#endif //_DEBUG



//==========================================================================
// Generate a managed string representation of a method or field.
//==========================================================================
STRINGREF CreatePersistableMemberName(IMDInternalImport *pInternalImport, mdToken token)
{
    THROWSCOMPLUSEXCEPTION();

    LPCUTF8 pName = "<unknownmember>";
    LPCUTF8 tmp;
    PCCOR_SIGNATURE psig;
    DWORD       csig;

    switch (TypeFromToken(token))
    {
        case mdtMemberRef:
            if ((tmp = pInternalImport->GetNameAndSigOfMemberRef(token, &psig, &csig)) != NULL)
                pName = tmp;
            break;

        case mdtMethodDef:
            if ((tmp = pInternalImport->GetNameOfMethodDef(token)) != NULL)
                pName = tmp;
            break;

        default:
            ;
    }
    return COMString::NewString(pName);
}


//==========================================================================
// Generate a managed string representation of a full classname.
//==========================================================================
STRINGREF CreatePersistableClassName(IMDInternalImport *pInternalImport, mdToken token)
{
    THROWSCOMPLUSEXCEPTION();

    LPCUTF8     szFullName;
    CQuickBytes qb;
    LPCUTF8     szClassName = "<unknownclass>";
    LPCUTF8     szNameSpace = NULL;

    switch (TypeFromToken(token)) {
    case mdtTypeRef:
        pInternalImport->GetNameOfTypeRef(token, &szNameSpace, &szClassName);
        break;
    case mdtTypeDef:
        pInternalImport->GetNameOfTypeDef(token, &szClassName, &szNameSpace);
    default:
        ;
        // leave it as "<unknown>"
    };

    if (szNameSpace && *szNameSpace) {
        if (!ns::MakePath(qb, szNameSpace, szClassName))
            RealCOMPlusThrowOM();
        szFullName = (LPCUTF8) qb.Ptr();
    } else {
        szFullName = szClassName;
    }

    return COMString::NewString(szFullName);
}


//==========================================================================
// Used by the classloader to record a managed exception object to explain
// why a classload got botched.
//
// - Can be called with gc enabled or disabled.
// - pThrowable must point to a buffer protected by a GCFrame.
// - If pThrowable is NULL, this function does nothing.
// - If (*pThrowable) is non-NULL, this function does nothing.
//   This allows a catch-all error path to post a generic catchall error
//   message w/out bonking more specific error messages posted by inner functions.
// - If pThrowable != NULL, this function is guaranteed to leave
//   a valid managed exception in it on exit.
//==========================================================================
VOID PostTypeLoadException(LPCUTF8 pszNameSpace, LPCUTF8 pTypeName,
                           LPCWSTR pAssemblyName, LPCUTF8 pMessageArg,
                           UINT resIDWhy, OBJECTREF *pThrowable)
{
    _ASSERTE(IsProtectedByGCFrame(pThrowable));

    if (pThrowable == RETURN_ON_ERROR)
        return;
        // we already have a pThrowable filled in.  
    if (pThrowableAvailable(pThrowable) && *((Object**) pThrowable) != NULL) 
        return;

    Thread *pThread = GetThread();
    BOOL toggleGC = !(pThread->PreemptiveGCDisabled());
    if (toggleGC)
        pThread->DisablePreemptiveGC();

        LPUTF8 pszFullName;
        if(pszNameSpace) {
            MAKE_FULL_PATH_ON_STACK_UTF8(pszFullName, 
                                         pszNameSpace,
                                         pTypeName);
        }
        else
            pszFullName = (LPUTF8) pTypeName;

        COUNTER_ONLY(GetPrivatePerfCounters().m_Loading.cLoadFailures++);
        COUNTER_ONLY(GetGlobalPerfCounters().m_Loading.cLoadFailures++);


        COMPLUS_TRY {

            MethodTable *pMT = g_Mscorlib.GetException(kTypeLoadException);

            struct _gc {
                OBJECTREF pNewException;
                STRINGREF pNewAssemblyString;
                STRINGREF pNewClassString;
                STRINGREF pNewMessageArgString;
            } gc;
            ZeroMemory(&gc, sizeof(gc));
            GCPROTECT_BEGIN(gc);

            gc.pNewClassString = COMString::NewString(pszFullName);

            if (pMessageArg)
                gc.pNewMessageArgString = COMString::NewString(pMessageArg);

            if (pAssemblyName)
                gc.pNewAssemblyString = COMString::NewString(pAssemblyName);

            gc.pNewException = AllocateObject(pMT);

            __int64 args[] = {
                ObjToInt64(gc.pNewException),
                (__int64)resIDWhy,
                ObjToInt64(gc.pNewMessageArgString),
                ObjToInt64(gc.pNewAssemblyString),
                ObjToInt64(gc.pNewClassString),
            };
            CallConstructor(&gsig_IM_Str_Str_Str_Int_RetVoid, args);

        if (pThrowable == THROW_ON_ERROR) {
            DEBUG_SAFE_TO_THROW_IN_THIS_BLOCK;
            COMPlusThrow(gc.pNewException);
        }

        *pThrowable = gc.pNewException;

        GCPROTECT_END();

    } COMPLUS_CATCH {
        UpdateThrowable(pThrowable);
    } COMPLUS_END_CATCH;

    if (toggleGC)
            pThread->EnablePreemptiveGC();
}

//@TODO: security: It would be nice for debugging purposes if the
// user could have the full path, if the user has the right permission.
VOID PostFileLoadException(LPCSTR pFileName, BOOL fRemovePath,
                           LPCWSTR pFusionLog, HRESULT hr, OBJECTREF *pThrowable)
{
    _ASSERTE(IsProtectedByGCFrame(pThrowable));
    if (pThrowable == RETURN_ON_ERROR)
        return;

    // we already have a pThrowable filled in.  
    if (pThrowableAvailable(pThrowable) && *((Object**) pThrowable) != NULL) 
        return;

    if (fRemovePath) {
        // Strip path for security reasons
        LPCSTR pTemp = strrchr(pFileName, '\\');
        if (pTemp)
            pFileName = pTemp+1;

        pTemp = strrchr(pFileName, '/');
        if (pTemp)
            pFileName = pTemp+1;
    }

    Thread *pThread = GetThread();
    BOOL toggleGC = !(pThread->PreemptiveGCDisabled());
    if (toggleGC)
        pThread->DisablePreemptiveGC();

    COUNTER_ONLY(GetPrivatePerfCounters().m_Loading.cLoadFailures++);
    COUNTER_ONLY(GetGlobalPerfCounters().m_Loading.cLoadFailures++);

    COMPLUS_TRY {
        RuntimeExceptionKind type;

        if (Assembly::ModuleFound(hr)) {

            //@BUG: this can be removed when the fileload/badimage stress bugs have been fixed
            STRESS_ASSERT(0);

            if ((hr == COR_E_BADIMAGEFORMAT) ||
                (hr == CLDB_E_FILE_OLDVER)   ||
                (hr == CLDB_E_FILE_CORRUPT)   ||
                (hr == HRESULT_FROM_WIN32(ERROR_BAD_EXE_FORMAT)) ||
                (hr == HRESULT_FROM_WIN32(ERROR_EXE_MARKED_INVALID)) ||
                (hr == CORSEC_E_INVALID_IMAGE_FORMAT))
                type = kBadImageFormatException;
            else {
                if ((hr == E_OUTOFMEMORY) || (hr == NTE_NO_MEMORY))
                    RealCOMPlusThrowOM();

                type = kFileLoadException;
            }
        }
        else
            type = kFileNotFoundException;

        struct _gc {
            OBJECTREF pNewException;
            STRINGREF pNewFileString;
            STRINGREF pFusLogString;
        } gc;
        ZeroMemory(&gc, sizeof(gc));
        GCPROTECT_BEGIN(gc);

        gc.pNewFileString = COMString::NewString(pFileName);
        gc.pFusLogString = COMString::NewString(pFusionLog);
        gc.pNewException = AllocateObject(g_Mscorlib.GetException(type));

        __int64 args[] = {
            ObjToInt64(gc.pNewException),
            (__int64) hr,
            ObjToInt64(gc.pFusLogString),
            ObjToInt64(gc.pNewFileString),
        };
        CallConstructor(&gsig_IM_Str_Str_Int_RetVoid, args);

        if (pThrowable == THROW_ON_ERROR) {
            DEBUG_SAFE_TO_THROW_IN_THIS_BLOCK;
            COMPlusThrow(gc.pNewException);
        }

        *pThrowable = gc.pNewException;
        
        GCPROTECT_END();

    } COMPLUS_CATCH {
        UpdateThrowable(pThrowable);
    } COMPLUS_END_CATCH

    if (toggleGC)
        pThread->EnablePreemptiveGC();
}

//==========================================================================
// Used by the classloader to post illegal layout
//==========================================================================
HRESULT PostFieldLayoutError(mdTypeDef cl,                // cl of the NStruct being loaded
                             Module* pModule,             // Module that defines the scope, loader and heap (for allocate FieldMarshalers)
                             DWORD   dwOffset,            // Offset of field
                             DWORD   dwID,                // Message id
                             OBJECTREF *pThrowable)
{
    IMDInternalImport *pInternalImport = pModule->GetMDImport();    // Internal interface for the NStruct being loaded.
    
    
    LPCUTF8 pszName, pszNamespace;
    pInternalImport->GetNameOfTypeDef(cl, &pszName, &pszNamespace);
    
    LPUTF8 offsetBuf = (LPUTF8)alloca(100);
    sprintf(offsetBuf, "%d", dwOffset);
    
    pModule->GetAssembly()->PostTypeLoadException(pszNamespace,
                                                  pszName,
                                                  offsetBuf,
                                                  dwID,
                                                  pThrowable);
    return COR_E_TYPELOAD;
}

//==========================================================================
// Used by the classloader to post an out of memory.
//==========================================================================
VOID PostOutOfMemoryException(OBJECTREF *pThrowable)
{
    _ASSERTE(IsProtectedByGCFrame(pThrowable));

    if (pThrowable == RETURN_ON_ERROR)
        return;

        // we already have a pThrowable filled in.  
    if (pThrowableAvailable(pThrowable) && *((Object**) pThrowable) != NULL)
        return;

        Thread *pThread = GetThread();
        BOOL toggleGC = !(pThread->PreemptiveGCDisabled());

        if (toggleGC) {
            pThread->DisablePreemptiveGC();
        }

        COMPLUS_TRY {
            RealCOMPlusThrowOM();
        } COMPLUS_CATCH {
        UpdateThrowable(pThrowable);
    } COMPLUS_END_CATCH;

        if (toggleGC) {
            pThread->EnablePreemptiveGC();
        }
    
}


//==========================================================================
// Private helper for TypeLoadException. 
//==========================================================================
struct _FormatTypeLoadExceptionMessageArgs
{
    UINT32      resId;
    STRINGREF   messageArg;
    STRINGREF   assemblyName;
    STRINGREF   typeName;
};

LPVOID __stdcall FormatTypeLoadExceptionMessage(struct _FormatTypeLoadExceptionMessageArgs *args)
{
    LPVOID rv = NULL;
    DWORD ncType = args->typeName->GetStringLength();

    CQuickBytes qb;
    CQuickBytes qb2;
    CQuickBytes qb3;

    LPWSTR wszType = (LPWSTR) qb.Alloc( (ncType+1)*2 );
    CopyMemory(wszType, args->typeName->GetBuffer(), ncType*2);
    wszType[ncType] = L'\0';

    LPWSTR wszAssembly;
    if (args->assemblyName == NULL)
        wszAssembly = NULL;
    else {
        DWORD ncAsm = args->assemblyName->GetStringLength();
        
        wszAssembly = (LPWSTR) qb2.Alloc( (ncAsm+1)*2 );
        CopyMemory(wszAssembly, args->assemblyName->GetBuffer(), ncAsm*2);
        wszAssembly[ncAsm] = L'\0';
    }

    LPWSTR wszMessageArg;
    if (args->messageArg == NULL)
        wszMessageArg = NULL;
    else {
        DWORD ncMessageArg = args->messageArg->GetStringLength();
        
        wszMessageArg = (LPWSTR) qb3.Alloc( (ncMessageArg+1)*2 );
        CopyMemory(wszMessageArg, args->messageArg->GetBuffer(), ncMessageArg*2);
        wszMessageArg[ncMessageArg] = L'\0';
    }

    LPWSTR wszMessage = CreateExceptionMessage(TRUE,
                                               args->resId ? args->resId : IDS_CLASSLOAD_GENERIC,
                                               wszType,
                                               wszAssembly,
                                               wszMessageArg);

    *((OBJECTREF*)(&rv)) = (OBJECTREF) COMString::NewString(wszMessage);
    if (wszMessage)
        LocalFree(wszMessage);
    return rv;
}


//==========================================================================
// Private helper for FileLoadException and FileNotFoundException.
//==========================================================================
struct _FormatFileLoadExceptionMessageArgs
{
    UINT32      hresult;
    STRINGREF   fileName;
};

LPVOID __stdcall FormatFileLoadExceptionMessage(struct _FormatFileLoadExceptionMessageArgs *args)
{
    LPVOID rv = NULL;
    LPWSTR wszFile;
    CQuickBytes qb;

    if (args->fileName == NULL)
        wszFile = NULL;
    else {
        DWORD ncFile = args->fileName->GetStringLength();
        wszFile = (LPWSTR) qb.Alloc( (ncFile+1)*2 );
        CopyMemory(wszFile, args->fileName->GetBuffer(), ncFile*2);
        wszFile[ncFile] = L'\0';
    }

    switch (args->hresult) {

    case COR_E_FILENOTFOUND:
    case HRESULT_FROM_WIN32(ERROR_MOD_NOT_FOUND):
    case HRESULT_FROM_WIN32(ERROR_PATH_NOT_FOUND):
    case HRESULT_FROM_WIN32(ERROR_INVALID_NAME):
    case CTL_E_FILENOTFOUND:
    case HRESULT_FROM_WIN32(ERROR_BAD_NET_NAME):
    case HRESULT_FROM_WIN32(ERROR_BAD_NETPATH):
    case HRESULT_FROM_WIN32(ERROR_NOT_READY):
    case HRESULT_FROM_WIN32(ERROR_WRONG_TARGET_NAME):
        args->hresult = IDS_EE_FILE_NOT_FOUND;

    case FUSION_E_REF_DEF_MISMATCH:
    case FUSION_E_INVALID_PRIVATE_ASM_LOCATION:
    case FUSION_E_PRIVATE_ASM_DISALLOWED:
    case FUSION_E_SIGNATURE_CHECK_FAILED:
    case FUSION_E_ASM_MODULE_MISSING:
    case FUSION_E_INVALID_NAME:
    case FUSION_E_CODE_DOWNLOAD_DISABLED:
    case COR_E_MODULE_HASH_CHECK_FAILED:
    case COR_E_FILELOAD:
    case SECURITY_E_INCOMPATIBLE_SHARE:
    case SECURITY_E_INCOMPATIBLE_EVIDENCE:
    case SECURITY_E_UNVERIFIABLE:
    case CORSEC_E_INVALID_STRONGNAME:
    case CORSEC_E_NO_EXEC_PERM:
    case E_ACCESSDENIED:
    case COR_E_BADIMAGEFORMAT:
    case COR_E_ASSEMBLYEXPECTED:
    case COR_E_FIXUPSINEXE:
    case HRESULT_FROM_WIN32(ERROR_TOO_MANY_OPEN_FILES):
    case HRESULT_FROM_WIN32(ERROR_SHARING_VIOLATION):
    case HRESULT_FROM_WIN32(ERROR_LOCK_VIOLATION):
    case HRESULT_FROM_WIN32(ERROR_OPEN_FAILED):
    case HRESULT_FROM_WIN32(ERROR_UNRECOGNIZED_VOLUME):
    case HRESULT_FROM_WIN32(ERROR_FILE_INVALID):
    case HRESULT_FROM_WIN32(ERROR_DLL_INIT_FAILED):
    case HRESULT_FROM_WIN32(ERROR_DISK_CORRUPT):
    case HRESULT_FROM_WIN32(ERROR_FILE_CORRUPT):
    case IDS_EE_PROC_NOT_FOUND:
    case IDS_EE_PATH_TOO_LONG:
    case IDS_EE_INTERNET_D: //TODO: find name
        break;

    case CLDB_E_FILE_OLDVER:
    case CLDB_E_FILE_CORRUPT:
    case HRESULT_FROM_WIN32(ERROR_BAD_EXE_FORMAT):
    case HRESULT_FROM_WIN32(ERROR_EXE_MARKED_INVALID):
    case CORSEC_E_INVALID_IMAGE_FORMAT:
        args->hresult = COR_E_BADIMAGEFORMAT;
        break;

    case COR_E_DEVICESNOTSUPPORTED:
    case HRESULT_FROM_WIN32(ERROR_INVALID_HANDLE):
    case HRESULT_FROM_WIN32(ERROR_UNSUPPORTED_TYPE):
    case MK_E_SYNTAX:
        args->hresult = FUSION_E_INVALID_NAME;
        break;

    case 0x800c000b:  //TODO: find name
        args->hresult = IDS_EE_INTERNET_B;
        break;

    case NTE_BAD_HASH:
    case NTE_BAD_LEN:
    case NTE_BAD_KEY:
    case NTE_BAD_DATA:
    case NTE_BAD_ALGID:
    case NTE_BAD_FLAGS:
    case NTE_BAD_HASH_STATE:
    case NTE_BAD_UID:
    case NTE_FAIL:
    case NTE_BAD_TYPE:
    case NTE_BAD_VER:
    case NTE_BAD_SIGNATURE:
    case NTE_SIGNATURE_FILE_BAD:
    case CRYPT_E_HASH_VALUE:
        args->hresult = IDS_EE_HASH_VAL_FAILED;
        break;

    case CORSEC_E_NO_SUITABLE_CSP:
        args->hresult = SN_NO_SUITABLE_CSP_NAME;
        break;

    default:
        _ASSERTE(!"Unknown hresult");
        args->hresult = COR_E_FILELOAD;
    }

    LPWSTR wszMessage = CreateExceptionMessage(TRUE,
                                               args->hresult,
                                               wszFile, NULL, NULL);

    *((OBJECTREF*)(&rv)) = (OBJECTREF) COMString::NewString(wszMessage);
    if (wszMessage)
        LocalFree(wszMessage);
    return rv;
}


//==========================================================================
// Persists a data type member of sig.
//==========================================================================
VOID PersistDataType(SigPointer *psp, IMDInternalImport *pInternalImport, StubLinker *psl)
{
    THROWSCOMPLUSEXCEPTION();

    CorElementType typ = (CorElementType)(psp->GetElemType());
    psl->Emit8((INT8)typ);

    if (!CorIsPrimitiveType(typ)) {
        switch (typ)
        {
                        case ELEMENT_TYPE_FNPTR:
            default:
                _ASSERTE(!"Illegal or unimplement type in COM+ sig.");
                break;
            case ELEMENT_TYPE_TYPEDBYREF:
                break;

            case ELEMENT_TYPE_BYREF: 
            case ELEMENT_TYPE_PTR:
                PersistDataType(psp, pInternalImport, psl); 
                break;

            case ELEMENT_TYPE_STRING:
                {
                    psl->EmitUtf8(g_StringClassName);
                    psl->Emit8('\0');
                    break;
                }
    
            case ELEMENT_TYPE_VAR:
            case ELEMENT_TYPE_OBJECT:
                {
                    psl->EmitUtf8(g_ObjectClassName);
                    psl->Emit8('\0');
                    break;
                }
    
            case ELEMENT_TYPE_VALUETYPE: //fallthru
            case ELEMENT_TYPE_CLASS:
                {
                    LPCUTF8 szNameSpace;
                    LPCUTF8 szClassName;

                    mdToken token = psp->GetToken();
                    if (TypeFromToken(token) == mdtTypeRef)
                        pInternalImport->GetNameOfTypeRef(token, &szNameSpace, &szClassName);
                    else
                        pInternalImport->GetNameOfTypeDef(token, &szNameSpace, &szClassName);

                    if (*szNameSpace) {
                        psl->EmitUtf8(szNameSpace);
                        psl->Emit8(NAMESPACE_SEPARATOR_CHAR);
                    }

                    psl->EmitUtf8(szClassName);
                    psl->Emit8('\0');
                }
                break;

            case ELEMENT_TYPE_SZARRAY:
                PersistDataType(psp, pInternalImport, psl);      // persist element type
                psl->Emit32(psp->GetData());    // persist array size
                break;

            case ELEMENT_TYPE_ARRAY: //fallthru
                {
                    PersistDataType(psp, pInternalImport, psl); // persist element type
                    UINT32 rank = psp->GetData();    // Get rank
                    psl->Emit32(rank);
                    if (rank)
                    {
                        UINT32 nsizes = psp->GetData(); // Get # of sizes
                        psl->Emit32(nsizes);
                        while (nsizes--)
                        {
                            psl->Emit32(psp->GetData());           // Persist size
                        }

                        UINT32 nlbounds = psp->GetData(); // Get # of lower bounds
                        psl->Emit32(nlbounds);
                        while (nlbounds--)
                        {
                            psl->Emit32(psp->GetData());           // Persist lower bounds
                        }
                    }

                }
                break;
        }
    }
}


StubLinker *NewStubLinker()
{
    return new StubLinker();
}

//==========================================================================
// Convert a signature into a persistable byte array format.
//
// This format mirrors that of the metadata signature format with
// two exceptions:
//
//   1. Any metadata token is replaced with a utf8 string describing
//      the actual class.
//   2. No compression is done on 32-bit ints.
//==========================================================================
I1ARRAYREF CreatePersistableSignature(const VOID *pSig, IMDInternalImport *pInternalImport)
{
    THROWSCOMPLUSEXCEPTION();

    StubLinker *psl = NULL;
    Stub       *pstub = NULL;
    I1ARRAYREF pArray = NULL;

    if (pSig != NULL) {

        COMPLUS_TRY {
    
            psl = NewStubLinker();
            if (!psl) {
                RealCOMPlusThrowOM();
            }
            SigPointer sp((PCCOR_SIGNATURE)pSig);
            DWORD nargs;

            UINT32 cc = sp.GetData();
            psl->Emit32(cc);
            if (cc == IMAGE_CEE_CS_CALLCONV_FIELD) {
                PersistDataType(&sp, pInternalImport, psl);
            } else {
                psl->Emit32(nargs = sp.GetData());  //Persist arg count
                PersistDataType(&sp, pInternalImport, psl);  //Persist return type
                for (DWORD i = 0; i < nargs; i++) {
                    PersistDataType(&sp, pInternalImport, psl);
                }
            }
            UINT cbSize;
            pstub = psl->Link(&cbSize);

            *((OBJECTREF*)&pArray) = AllocatePrimitiveArray(ELEMENT_TYPE_I1, cbSize);
            memcpyNoGCRefs(pArray->GetDirectPointerToNonObjectElements(), pstub->GetEntryPoint(), cbSize);
    
        } COMPLUS_CATCH {
            delete psl;
            if (pstub) {
                pstub->DecRef();
            }
            RealCOMPlusThrow(GETTHROWABLE());
        } COMPLUS_END_CATCH
        delete psl;
        if (pstub) {
            pstub->DecRef();
        }
        _ASSERTE(pArray != NULL);
    }
    return pArray;

}




//==========================================================================
// Unparses an individual type.
//==========================================================================
const BYTE *UnparseType(const BYTE *pType, StubLinker *psl)
{
    THROWSCOMPLUSEXCEPTION();

    switch ( (CorElementType) *(pType++) ) {
        case ELEMENT_TYPE_VOID:
            psl->EmitUtf8("void");
            break;

        case ELEMENT_TYPE_BOOLEAN:
            psl->EmitUtf8("boolean");
            break;

        case ELEMENT_TYPE_CHAR:
            psl->EmitUtf8("char");
            break;

        case ELEMENT_TYPE_U1:
            psl->EmitUtf8("unsigned ");
            //fallthru
        case ELEMENT_TYPE_I1:
            psl->EmitUtf8("byte");
            break;

        case ELEMENT_TYPE_U2:
            psl->EmitUtf8("unsigned ");
            //fallthru
        case ELEMENT_TYPE_I2:
            psl->EmitUtf8("short");
            break;

        case ELEMENT_TYPE_U4:
            psl->EmitUtf8("unsigned ");
            //fallthru
        case ELEMENT_TYPE_I4:
            psl->EmitUtf8("int");
            break;

        case ELEMENT_TYPE_I:
            psl->EmitUtf8("native int");
            break;
        case ELEMENT_TYPE_U:
            psl->EmitUtf8("native unsigned");
            break;

        case ELEMENT_TYPE_U8:
            psl->EmitUtf8("unsigned ");
            //fallthru
        case ELEMENT_TYPE_I8:
            psl->EmitUtf8("long");
            break;


        case ELEMENT_TYPE_R4:
            psl->EmitUtf8("float");
            break;

        case ELEMENT_TYPE_R8:
            psl->EmitUtf8("double");
            break;

        case ELEMENT_TYPE_STRING:
            psl->EmitUtf8(g_StringName);
            break;
    
        case ELEMENT_TYPE_VAR:
        case ELEMENT_TYPE_OBJECT:
            psl->EmitUtf8(g_ObjectName);
            break;
    
        case ELEMENT_TYPE_PTR:
            pType = UnparseType(pType, psl);
            psl->EmitUtf8("*");
            break;

        case ELEMENT_TYPE_BYREF:
            pType = UnparseType(pType, psl);
            psl->EmitUtf8("&");
            break;
    
        case ELEMENT_TYPE_VALUETYPE:
        case ELEMENT_TYPE_CLASS:
            psl->EmitUtf8((LPCUTF8)pType);
            while (*(pType++)) {
                //nothing
            }
            break;

    
        case ELEMENT_TYPE_SZARRAY:
            {
                pType = UnparseType(pType, psl);
                psl->EmitUtf8("[]");
            }
            break;

        case ELEMENT_TYPE_ARRAY:
            {
                pType = UnparseType(pType, psl);
                DWORD rank = *((DWORD*)pType);
                pType += sizeof(DWORD);
                if (rank)
                {
                    UINT32 nsizes = *((UINT32*)pType); // Get # of sizes
                    pType += 4 + nsizes*4;
                    UINT32 nlbounds = *((UINT32*)pType); // Get # of lower bounds
                    pType += 4 + nlbounds*4;


                    while (rank--) {
                        psl->EmitUtf8("[]");
                    }

                }

            }
            break;

        case ELEMENT_TYPE_TYPEDBYREF:
            psl->EmitUtf8("&");
            break;

        case ELEMENT_TYPE_FNPTR:
            psl->EmitUtf8("ftnptr");
            break;

        default:
            psl->EmitUtf8("?");
            break;
    }

    return pType;
}



//==========================================================================
// Helper for MissingMethodException.
//==========================================================================
struct MissingMethodException_FormatSignature_Args {
    I1ARRAYREF pPersistedSig;
};


LPVOID __stdcall MissingMethodException_FormatSignature(struct MissingMethodException_FormatSignature_Args *args)
{
    THROWSCOMPLUSEXCEPTION();

    STRINGREF pString = NULL;
    DWORD csig = 0;

    if (args->pPersistedSig != NULL)
        csig = args->pPersistedSig->GetNumComponents();

    if (csig == 0)
    {
        pString = COMString::NewString("Unknown signature");
        return *((LPVOID*)&pString);
    } 

    const BYTE *psig = (const BYTE*)_alloca(csig);
    CopyMemory((BYTE*)psig,
               args->pPersistedSig->GetDirectPointerToNonObjectElements(),
               csig);

    StubLinker *psl = NewStubLinker();
    Stub       *pstub = NULL;
    if (!psl) {
        RealCOMPlusThrowOM();
    }
    COMPLUS_TRY {

        UINT32 cconv = *((UINT32*)psig);
        psig += 4;

        if (cconv == IMAGE_CEE_CS_CALLCONV_FIELD) {
            psig = UnparseType(psig, psl);
        } else {
            UINT32 nargs = *((UINT32*)psig);
            psig += 4;

            // Unparse return type
            psig = UnparseType(psig, psl);
            psl->EmitUtf8("(");
            while (nargs--) {
                psig = UnparseType(psig, psl);
                if (nargs) {
                    psl->EmitUtf8(", ");
                }
            }
            psl->EmitUtf8(")");
        }
        psl->Emit8('\0');
        pstub = psl->Link();
        pString = COMString::NewString( (LPCUTF8)(pstub->GetEntryPoint()) );
    } COMPLUS_CATCH {
        delete psl;
        if (pstub) {
            pstub->DecRef();
        }
        RealCOMPlusThrow(GETTHROWABLE());
    } COMPLUS_END_CATCH
    delete psl;
    if (pstub) {
        pstub->DecRef();
    }

    return *((LPVOID*)&pString);

}


//==========================================================================
// Throw a MissingMethodException
//==========================================================================
VOID RealCOMPlusThrowMissingMethod(IMDInternalImport *pInternalImport,
                               mdToken mdtoken)
{
    THROWSCOMPLUSEXCEPTION();
    RealCOMPlusThrowMember(kMissingMethodException, pInternalImport, mdtoken);
}


//==========================================================================
// Throw an exception pertaining to member access.
//==========================================================================
VOID RealCOMPlusThrowMember(RuntimeExceptionKind reKind, IMDInternalImport *pInternalImport, mdToken mdtoken)
{
    THROWSCOMPLUSEXCEPTION();



    mdToken tk      = TypeFromToken(mdtoken);
    mdToken tkclass = mdTypeDefNil;
    const void *psig = NULL;

    MethodTable *pMT = g_Mscorlib.GetException(reKind);

    mdToken tmp;
    PCCOR_SIGNATURE tmpsig;
    DWORD tmpcsig;
    LPCUTF8 tmpname;
    switch (tk) {
        case mdtMethodDef:
            if (SUCCEEDED(pInternalImport->GetParentToken(mdtoken, &tmp))) {
                tkclass = tmp;
            }
            psig = pInternalImport->GetSigOfMethodDef(mdtoken, &tmpcsig);
            break;

        case mdtMemberRef:
            tkclass = pInternalImport->GetParentOfMemberRef(mdtoken);
            tmpname = pInternalImport->GetNameAndSigOfMemberRef(mdtoken,&tmpsig,&tmpcsig);
            break;

        default:
            ;
            // leave tkclass as mdTypeDefNil;
    }


    struct _gc {
        OBJECTREF   pException;
        STRINGREF   pFullClassName;
        STRINGREF   pMethodName;
        I1ARRAYREF  pSig;
    } gc;

    FillMemory(&gc, sizeof(gc), 0);

    GCPROTECT_BEGIN(gc);


    gc.pException     = AllocateObject(pMT);
    gc.pFullClassName = CreatePersistableClassName(pInternalImport, tkclass);
    gc.pMethodName    = CreatePersistableMemberName(pInternalImport, mdtoken);
    gc.pSig           = CreatePersistableSignature(psig ,pInternalImport);

    __int64 args[] =
    {
       ObjToInt64(gc.pException),
       ObjToInt64(gc.pSig),
       ObjToInt64(gc.pMethodName),
       ObjToInt64(gc.pFullClassName),

    };
    CallConstructor(&gsig_IM_Str_Str_ArrByte_RetVoid, args);

    RealCOMPlusThrow(gc.pException);

    GCPROTECT_END();

}

//==========================================================================
// Throw an exception pertaining to member access.
//==========================================================================
VOID RealCOMPlusThrowMember(RuntimeExceptionKind reKind, IMDInternalImport *pInternalImport, MethodTable *pClassMT, LPCWSTR name, PCCOR_SIGNATURE pSig)
{
    THROWSCOMPLUSEXCEPTION();

    MethodTable *pMT = g_Mscorlib.GetException(reKind);

    struct _gc {
        OBJECTREF   pException;
        STRINGREF   pFullClassName;
        STRINGREF   pMethodName;
        I1ARRAYREF  pSig;
    } gc;

    FillMemory(&gc, sizeof(gc), 0);

    GCPROTECT_BEGIN(gc);


    gc.pException     = AllocateObject(pMT);
    gc.pFullClassName = CreatePersistableClassName(pInternalImport, pClassMT->GetClass()->GetCl());
    gc.pMethodName    = COMString::NewString(name);
    gc.pSig           = CreatePersistableSignature(pSig, pInternalImport);

    __int64 args[] =
    {
       ObjToInt64(gc.pException),
       ObjToInt64(gc.pSig),
       ObjToInt64(gc.pMethodName),
       ObjToInt64(gc.pFullClassName),

    };
    CallConstructor(&gsig_IM_Str_Str_ArrByte_RetVoid, args);

    RealCOMPlusThrow(gc.pException);

    GCPROTECT_END();

}

BOOL IsExceptionOfType(RuntimeExceptionKind reKind, OBJECTREF *pThrowable)
{
    _ASSERTE(pThrowableAvailable(pThrowable));

    if (*pThrowable == NULL)
        return FALSE;

    MethodTable *pThrowableMT = (*pThrowable)->GetTrueMethodTable();

    return g_Mscorlib.IsException(pThrowableMT, reKind); 
}

BOOL IsAsyncThreadException(OBJECTREF *pThrowable) {
    if (  IsExceptionOfType(kThreadStopException, pThrowable)
        ||IsExceptionOfType(kThreadAbortException, pThrowable)
        ||IsExceptionOfType(kThreadInterruptedException, pThrowable)) {
        return TRUE;
    } else {
    return FALSE;
    }
}

BOOL IsUncatchable(OBJECTREF *pThrowable) {
    if (IsAsyncThreadException(pThrowable)
        || IsExceptionOfType(kExecutionEngineException, pThrowable)) {
        return TRUE;
    } else {
        return FALSE;
    }
}

#ifdef _DEBUG
BOOL IsValidClause(EE_ILEXCEPTION_CLAUSE *EHClause)
{
    DWORD valid = COR_ILEXCEPTION_CLAUSE_FILTER | COR_ILEXCEPTION_CLAUSE_FINALLY | 
        COR_ILEXCEPTION_CLAUSE_FAULT | COR_ILEXCEPTION_CLAUSE_CACHED_CLASS;

#if 0
    // @NICE: enable this when VC stops generatng a bogus 0x8000.
    if (EHClause->Flags & ~valid)
        return FALSE;
#endif
    if (EHClause->TryStartPC > EHClause->TryEndPC)
        return FALSE;
    return TRUE;
}
#endif


//===========================================================================================
//
// UNHANDLED EXCEPTION HANDLING
//

//=====================
// FailFast is called to take down the process when we discover some kind of unrecoverable
// error.  It can be called from an exception handler, or from any other place we discover
// a fatal error.  If called from a handler, and pExceptionRecord and pContext are not null,
// we'll first give the debugger a chance to handle the exception.
// the exception first.
#pragma warning(disable:4702)
void
FailFast(Thread* pThread, UnhandledExceptionLocation reason, EXCEPTION_RECORD *pExceptionRecord, CONTEXT *pContext) {

    HRESULT hr;

    if (   g_pConfig 
        && g_pConfig->ContinueAfterFatalError()
        && reason != FatalExecutionEngineException)
        return;

    LOG((LF_EH, LL_INFO100, "FailFast() called.\n"));

    g_fFatalError = 1;

#ifdef _DEBUG
    // If this exception interupted a ForbidGC, we need to restore it so that the
    // recovery code is in a semi stable state.
    if (pThread != NULL) {
        while(pThread->GCForbidden()) {
            pThread->EndForbidGC();
        }
    }
#endif
    

    // We sometimes get some other fatal error as a result of a stack overflow.  For example,
    // inside the OS heap allocation routines ... they return a NULL if they get a stack
    // overflow.  If we call it out-of-memory it's misleading, as the machine may have lots of 
    // memory remaining
    //
    if (pThread && !pThread->GuardPageOK()) 
        reason = FatalStackOverflow;

    if (pThread && pContext && pExceptionRecord) {

        BEGIN_ENSURE_COOPERATIVE_GC();

        // Managed debugger gets a chance if we passed in a context and exception record.
        switch(reason) {
        case FatalStackOverflow:
            pThread->SetThrowable(ObjectFromHandle(g_pPreallocatedStackOverflowException));
            break;
        case FatalOutOfMemory:
            pThread->SetThrowable(ObjectFromHandle(g_pPreallocatedOutOfMemoryException));
            break;
        default:
            pThread->SetThrowable(ObjectFromHandle(g_pPreallocatedExecutionEngineException));
            break;
        }

        if  (g_pDebugInterface &&
             g_pDebugInterface->
             LastChanceManagedException(
                 pExceptionRecord, 
                 pContext,
                 pThread,
                 ProcessWideHandler) == ExceptionContinueExecution) {
            LOG((LF_EH, LL_INFO100, "FailFast: debugger ==> EXCEPTION_CONTINUE_EXECUTION\n"));
            _ASSERTE(!"Debugger should not have returned ContinueExecution");
        }

        END_ENSURE_COOPERATIVE_GC();
    }

    // Debugger didn't stop us.  We're out of here.

    // Would use the resource file, but we can't do that without running managed code.
    switch (reason) {
    case FatalStackOverflow:
        // LoadStringRC(IDS_EE_FATAL_STACK_OVERFLOW, buf, buf_size);
        PrintToStdErrA("\nFatal stack overflow error.\n");
        hr = COR_E_STACKOVERFLOW;
        break;
    case FatalOutOfMemory:
        //LoadStringRC(IDS_EE_FATAL_OUT_OF_MEMORY, buf, buf_size);
        PrintToStdErrA("\nFatal out of memory error.\n");
        hr = COR_E_OUTOFMEMORY;
        break;
    default:
        //LoadStringRC(IDS_EE_FATAL_ERROR, buf, buf_size);
        PrintToStdErrA("\nFatal execution engine error.\n");
        hr = COR_E_EXECUTIONENGINE;
        _ASSERTE(0);    // These we want to look at.
        break;
    }

    g_fForbidEnterEE = 1;


    //DebugBreak();    // Give the guy a chance to attack a debugger before we die.  Note ...
                       // func-evals are probably not going to work well any more.

    ::ExitProcess(hr);

    _ASSERTE(!"::ExitProcess(hr) should not return");        

    ::TerminateProcess(GetCurrentProcess(), hr);
}
#pragma warning(default:4702)

//
// Used to get the current instruction pointer value
//
#pragma inline_depth ( 0 )
DWORD __declspec(naked) GetEIP()
{
    __asm
    {
        mov eax, [esp]
        ret
    }
}
#pragma inline_depth()



//
// Log an error to the event log if possible, then throw up a dialog box.
//

#define FatalErrorStringLength 50
#define FatalErrorAddressLength 20
WCHAR lpWID[FatalErrorAddressLength];
WCHAR lpMsg[FatalErrorStringLength];

void
LogFatalError(DWORD id)
{
    // Id is currently an 8 char memory address
    // Error string is 31 characters
    
    // Create the error message
    Wszwsprintf(lpWID, L"0x%x", id);
        
    Wszlstrcpy (lpMsg, L"Fatal Execution Engine Error (");
    Wszlstrcat (lpMsg, lpWID);
    Wszlstrcat(lpMsg, L")");

    // Write to the event log and/or display
    WszMessageBoxInternal(NULL, lpMsg, NULL, 
        MB_OK | MB_ICONERROR | MB_SETFOREGROUND | MB_TOPMOST);

    if (IsDebuggerPresent()) {
        DebugBreak();
    }
}


static void
FatalErrorFilter(Thread *pThread, UnhandledExceptionLocation reason, EXCEPTION_POINTERS *pExceptionPointers) {
    CONTEXT *pContext = pExceptionPointers->ContextRecord;
    EXCEPTION_RECORD *pExceptionRecord = pExceptionPointers->ExceptionRecord;
    FailFast(pThread, reason, pExceptionRecord, pContext);
}

static void 
FatalError(UnhandledExceptionLocation reason, OBJECTHANDLE hException) {
    Thread *pThread = GetThread();

    if (!pThread || !g_fExceptionsOK) {
        FailFast(NULL, reason, NULL, NULL);
    } else {
        __try {
            pThread->SetLastThrownObjectHandleAndLeak(hException);
            RaiseException(EXCEPTION_COMPLUS, EXCEPTION_NONCONTINUABLE, 0, NULL);
        } __except((FatalErrorFilter(pThread, reason, GetExceptionInformation()), 
                    COMPLUS_EXCEPTION_EXECUTE_HANDLER)) {
            /* do nothing */;
        }
    }
}

void 
FatalOutOfMemoryError() {
    if (g_pConfig && g_pConfig->ContinueAfterFatalError())
        return;

    FatalError(FatalOutOfMemory, g_pPreallocatedOutOfMemoryException);
}

void 
FatalInternalError() {
    FatalError(FatalExecutionEngineException, g_pPreallocatedExecutionEngineException);
}

int
UserBreakpointFilter(EXCEPTION_POINTERS* pEP) {
    int result = UnhandledExceptionFilter(pEP);
    if (result == EXCEPTION_CONTINUE_SEARCH) {
        // A debugger got attached.  Instead of allowing the exception to continue
        // up, and hope for the second-chance, we cause it to happen again.  The 
        // debugger snags all int3's on first-chance.
        return EXCEPTION_CONTINUE_EXECUTION;
    } else {
        TerminateProcess(GetCurrentProcess(), STATUS_BREAKPOINT);
        // Shouldn't get here ...
        return EXCEPTION_CONTINUE_EXECUTION;
    }
}


// We keep a pointer to the previous unhandled exception filter.  After we install, we use
// this to call the previous guy.  When we un-install, we put them back.  Putting them back
// is a bug -- we have no guarantee that the DLL unload order matches the DLL load order -- we
// may in fact be putting back a pointer to a DLL that has been unloaded.
//

// initialize to -1 because NULL won't detect difference between us not having installed our handler
// yet and having installed it but the original handler was NULL.
static LPTOP_LEVEL_EXCEPTION_FILTER g_pOriginalUnhandledExceptionFilter = (LPTOP_LEVEL_EXCEPTION_FILTER)-1;
#define FILTER_NOT_INSTALLED (LPTOP_LEVEL_EXCEPTION_FILTER) -1

void InstallUnhandledExceptionFilter() {
    if (g_pOriginalUnhandledExceptionFilter == FILTER_NOT_INSTALLED) {
        g_pOriginalUnhandledExceptionFilter =
              SetUnhandledExceptionFilter(COMUnhandledExceptionFilter);
        // make sure is set (ie. is not our special value to indicate unset)
    }
    _ASSERTE(g_pOriginalUnhandledExceptionFilter != FILTER_NOT_INSTALLED);
}

void UninstallUnhandledExceptionFilter() {
    if (g_pOriginalUnhandledExceptionFilter != FILTER_NOT_INSTALLED) {
        SetUnhandledExceptionFilter(g_pOriginalUnhandledExceptionFilter);
        g_pOriginalUnhandledExceptionFilter = FILTER_NOT_INSTALLED;
    }
}

//
// COMUnhandledExceptionFilter is used to catch all unhandled exceptions.
// The debugger will either handle the exception, attach a debugger, or
// notify an existing attached debugger.
//

BOOL LaunchJITDebugger();

LONG InternalUnhandledExceptionFilter(struct _EXCEPTION_POINTERS  *pExceptionInfo, BOOL isTerminating)
{
    if (g_fFatalError)
        return EXCEPTION_CONTINUE_SEARCH;

    LOG((LF_EH, LL_INFO100, "InternalUnhandledExceptionFilter: Called\n"));

    Thread *pThread = GetThread();
    if (!pThread) {
        return   g_pOriginalUnhandledExceptionFilter 
               ? g_pOriginalUnhandledExceptionFilter(pExceptionInfo) 
               : EXCEPTION_CONTINUE_SEARCH;
    }

#ifdef _DEBUG
    static bool bBreakOnUncaught = false;
    static int fBreakOnUncaught = 0;
    if (!bBreakOnUncaught) {
        fBreakOnUncaught = g_pConfig->GetConfigDWORD(L"BreakOnUncaughtException", 0);
        bBreakOnUncaught = true;
    }
    // static fBreakOnUncaught mad file global due to VC7 bug
    if (fBreakOnUncaught) {
        //fprintf(stderr, "Attach debugger now.  Sleeping for 1 minute.\n");
        //Sleep(60 * 1000);
        _ASSERTE(!"BreakOnUnCaughtException");
    }

#endif

#ifdef _DEBUG_ADUNLOAD
    printf("%x InternalUnhandledExceptionFilter: Called for %x\n", GetThread()->GetThreadId(), pExceptionInfo->ExceptionRecord->ExceptionCode);
    fflush(stdout);
#endif

    if (!pThread->GuardPageOK())
        g_fFatalError = TRUE;

    if (g_fNoExceptions) // This shouldn't be possible, but MSVC re-installs us ...
        return EXCEPTION_CONTINUE_SEARCH;   // ... for now, just bail if this happens.

    LOG((LF_EH, LL_INFO100, "InternalUnhandledExceptionFilter: Handling\n"));

    _ASSERTE(g_pOriginalUnhandledExceptionFilter != FILTER_NOT_INSTALLED);

    SLOT ExceptionEIP = 0;

    __try {

        // Debugger does func-evals inside this call, which may take nested exceptions.  We
        // need a nested exception handler to allow this.

#ifdef DEBUGGING_SUPPORTED
        LOG((LF_EH, LL_INFO100, "InternalUnhandledExceptionFilter: Notifying Debugger...\n"));

        INSTALL_NESTED_EXCEPTION_HANDLER(pThread->GetFrame());

        if  (g_pDebugInterface && 
             g_pDebugInterface->
             LastChanceManagedException(pExceptionInfo->ExceptionRecord, 
                                  pExceptionInfo->ContextRecord,
                                  pThread,
                                  ProcessWideHandler) == ExceptionContinueExecution) {
            LOG((LF_EH, LL_INFO100, "InternalUnhandledExceptionFilter: debugger ==> EXCEPTION_CONTINUE_EXECUTION\n"));
            return EXCEPTION_CONTINUE_EXECUTION;
        }

        LOG((LF_EH, LL_INFO100, "InternalUnhandledExceptionFilter: ... returned.\n"));
        UNINSTALL_NESTED_EXCEPTION_HANDLER();
#endif // DEBUGGING_SUPPORTED

        // Except for notifying debugger, ignore exception if thread is NULL, or if it's
        // a debugger-generated exception.
        if (
               pExceptionInfo->ExceptionRecord->ExceptionCode == STATUS_BREAKPOINT
            || pExceptionInfo->ExceptionRecord->ExceptionCode == STATUS_SINGLE_STEP) {
            LOG((LF_EH, LL_INFO100, "InternalUnhandledExceptionFilter, ignoring the exception\n"));
            return g_pOriginalUnhandledExceptionFilter ? g_pOriginalUnhandledExceptionFilter(pExceptionInfo) : EXCEPTION_CONTINUE_SEARCH;
        }

        LOG((LF_EH, LL_INFO100, "InternalUnhandledExceptionFilter: Calling DefaultCatchHandler\n"));

        BOOL toggleGC = ! pThread->PreemptiveGCDisabled();
        if (toggleGC)
            pThread->DisablePreemptiveGC();

        DefaultCatchHandler(NULL, TRUE);

        if (toggleGC)
            pThread->EnablePreemptiveGC();
    } __except(
        (ExceptionEIP = (SLOT)GetIP((GetExceptionInformation())->ContextRecord)),
        COMPLUS_EXCEPTION_EXECUTE_HANDLER) {

        // Should never get here.
#ifdef _DEBUG
        char buffer[200];
        g_fFatalError = 1;
        sprintf(buffer, "\nInternal error: Uncaught exception was thrown from IP = 0x%08x in UnhandledExceptionFilter on thread %x\n", ExceptionEIP, GetThread()->GetThreadId());
        PrintToStdErrA(buffer);
        _ASSERTE(!"Unexpected exception in UnhandledExceptionFilter");
#endif
        FreeBuildDebugBreak();

    }

    LOG((LF_EH, LL_INFO100, "InternalUnhandledExceptionFilter: call next handler\n"));
    return g_pOriginalUnhandledExceptionFilter ? g_pOriginalUnhandledExceptionFilter(pExceptionInfo) : EXCEPTION_CONTINUE_SEARCH;
}

LONG COMUnhandledExceptionFilter(struct _EXCEPTION_POINTERS  *pExceptionInfo) {
    return InternalUnhandledExceptionFilter(pExceptionInfo, TRUE);
}


void PrintStackTraceToStdout();


void STDMETHODCALLTYPE
DefaultCatchHandler(OBJECTREF *pThrowableIn, BOOL isTerminating)
{
    // TODO: The strings in here should be translatable.
    LOG((LF_ALL, LL_INFO10, "In DefaultCatchHandler\n"));

    const int buf_size = 128;
    WCHAR buf[buf_size];

#ifdef _DEBUG
    static bool bBreakOnUncaught = false;
    static int fBreakOnUncaught = 0;
    if (!bBreakOnUncaught) {
        fBreakOnUncaught = g_pConfig->GetConfigDWORD(L"BreakOnUncaughtException", 0);
        bBreakOnUncaught = true;
    }
    // static fBreakOnUncaught mad file global due to VC7 bug
    if (fBreakOnUncaught) {
        _ASSERTE(!"BreakOnUnCaughtException");
        //fprintf(stderr, "Attach debugger now.  Sleeping for 1 minute.\n");
        //Sleep(60 * 1000);
    }
#endif

    Thread *pThread = GetThread();

    // @NICE:  At one point, we had the comment:
    //     The following hack reduces a window for a race during shutdown.  IT DOES NOT FIX
    //     THE PROBLEM.  ONLY MAKES IT LESS LIKELY.  UGH...
    // It may no longer be necessary ... but remove at own peril.
    if (!pThread) {
        _ASSERTE(g_fEEShutDown);
        return;
    }

    _ASSERTE(pThread);

    ExInfo* pExInfo = pThread->GetHandlerInfo();           
    BOOL ExInUnmanagedHandler = pExInfo->IsInUnmanagedHandler(); 
    pExInfo->ResetIsInUnmanagedHandler();

    BOOL fWasGCEnabled = !pThread->PreemptiveGCDisabled();
    if (fWasGCEnabled)
        pThread->DisablePreemptiveGC();

    OBJECTREF throwable;

    // If we can't run managed code, can't deliver the event, nor can we print a string.  Just silently let
    // the exception go.
    if (!CanRunManagedCode())
        goto exit;

    if (pThrowableAvailable(pThrowableIn))
        throwable = *pThrowableIn;
    else
        throwable = GETTHROWABLE();

    if (throwable == NULL) {
        LOG((LF_ALL, LL_INFO10, "Unhangled exception, throwable == NULL\n"));
        goto exit;
    }
    GCPROTECT_BEGIN(throwable);
    BOOL IsStackOverflow = (throwable->GetTrueMethodTable() == g_pStackOverflowExceptionClass);
    BOOL IsOutOfMemory = (throwable->GetTrueMethodTable() == g_pOutOfMemoryExceptionClass);

    // Notify the AppDomain that we have taken an unhandled exception.  Can't notify
    // of stack overflow -- guard page is not yet reset.

    BOOL SentEvent = FALSE;

    if (!IsStackOverflow && !IsOutOfMemory && pThread->GuardPageOK()) {

        INSTALL_NESTED_EXCEPTION_HANDLER(pThread->GetFrame());

        // This guy will never throw, but it will need a spot to store
        // any nested exceptions it might find.
        SentEvent = pThread->GetDomain()->OnUnhandledException(&throwable, isTerminating);

        UNINSTALL_NESTED_EXCEPTION_HANDLER();

    }
#ifndef _IA64_
    COMPLUS_TRY {
#endif
       
        // if this isn't ThreadStopException, we want to print a stack trace to
        // indicate why this thread abruptly terminated.  Exceptions kill threads
        // rarely enough that an uncached name check is reasonable.
        BOOL        dump = TRUE;

        if (IsStackOverflow || !pThread->GuardPageOK() || IsOutOfMemory) {
            // We have to be very careful.  If we walk off the end of the stack, the process
            // will just die.  e.g. IsAsyncThreadException() and Exception.ToString both consume
            // too much stack -- and can't be called here.
            //
            // @BUG 26505: See if we can't find a way to print a partial stack trace.
            dump = FALSE;
            PrintToStdErrA("\n");
            if (FAILED(LoadStringRC(IDS_EE_UNHANDLED_EXCEPTION, buf, buf_size)))
                wcscpy(buf, SZ_UNHANDLED_EXCEPTION);
            PrintToStdErrW(buf);
            if (IsOutOfMemory) {
                PrintToStdErrA(" OutOfMemoryException.\n");
            } else {
                PrintToStdErrA(" StackOverflowException.\n");
            }
        } else if (SentEvent || IsAsyncThreadException(&throwable)) {
            dump = FALSE;
        }

        if (dump)
        {
            CQuickWSTRNoDtor message;

            if (throwable != NULL) {
                PrintToStdErrA("\n");
                if (FAILED(LoadStringRC(IDS_EE_UNHANDLED_EXCEPTION, buf, buf_size)))
                    wcscpy(buf, SZ_UNHANDLED_EXCEPTION);
                PrintToStdErrW(buf);
                PrintToStdErrA(" ");

            INSTALL_NESTED_EXCEPTION_HANDLER(pThread->GetFrame());

            GetExceptionMessage(throwable, &message);

            UNINSTALL_NESTED_EXCEPTION_HANDLER();

            if (message.Size() > 0) {
                    NPrintToStdErrW(message.Ptr(), message.Size());
                }

                PrintToStdErrA("\n");
            }

            message.Destroy();
        }
        
#ifndef _IA64_
    } COMPLUS_CATCH {
        LOG((LF_ALL, LL_INFO10, "Exception occured while processing uncaught exception\n"));
        if (FAILED(LoadStringRC(IDS_EE_EXCEPTION_TOSTRING_FAILED, buf, buf_size)))
            wcscpy(buf, L"Exception.ToString() failed.");
        PrintToStdErrA("\n   ");
        PrintToStdErrW(buf);
        PrintToStdErrA("\n");
    } COMPLUS_END_CATCH
#endif
    FlushLogging();     // Flush any logging output
    GCPROTECT_END();
exit:
    if (fWasGCEnabled)
        pThread->EnablePreemptiveGC();
    
    if (ExInUnmanagedHandler)               
        pExInfo->SetIsInUnmanagedHandler();  
}

BOOL COMPlusIsMonitorException(struct _EXCEPTION_POINTERS *pExceptionInfo)
{
    return COMPlusIsMonitorException(pExceptionInfo->ExceptionRecord,
                                     pExceptionInfo->ContextRecord);
}

BOOL COMPlusIsMonitorException(EXCEPTION_RECORD *pExceptionRecord, 
                               CONTEXT *pContext)
{
#if ZAPMONITOR_ENABLED
    //get the fault address and hand it to monitor. 
    if (pExceptionRecord->ExceptionCode == EXCEPTION_ACCESS_VIOLATION)
    {
        void* f_address = (void*)pExceptionRecord->ExceptionInformation [1];
        if (ZapMonitor::HandleAccessViolation ((BYTE*)f_address, pContext))
            return true;
    } 
#endif
    return false;
}


LONG
ThreadBaseExceptionFilter(struct _EXCEPTION_POINTERS *pExceptionInfo,
                          Thread* pThread, 
                          UnhandledExceptionLocation location) {

    LOG((LF_EH, LL_INFO100, "ThreadBaseExceptionFilter: Enter\n"));

    if (COMPlusIsMonitorException(pExceptionInfo))
        return EXCEPTION_CONTINUE_EXECUTION;

#ifdef _DEBUG
    if (g_pConfig->GetConfigDWORD(L"BreakOnUncaughtException", 0))
        _ASSERTE(!"BreakOnUnCaughtException");
#endif

    _ASSERTE(!g_fNoExceptions);
    _ASSERTE(pThread);

    // Debugger does func-evals inside this call, which may take nested exceptions.  We
    // need a nested exception handler to allow this.

    if (!pThread->IsAbortRequested()) {

        LONG result = EXCEPTION_CONTINUE_SEARCH;
        // A thread-abort is "expected".  No debugger popup required.
        INSTALL_NESTED_EXCEPTION_HANDLER(pThread->GetFrame());

        if  (g_pDebugInterface && g_pDebugInterface->
                 LastChanceManagedException(pExceptionInfo->ExceptionRecord, 
                                      pExceptionInfo->ContextRecord,
                                      GetThread(), 
                                      location) == ExceptionContinueExecution) {
            LOG((LF_EH, LL_INFO100, "COMUnhandledExceptionFilter: EXCEPTION_CONTINUE_EXECUTION\n"));
            result = EXCEPTION_CONTINUE_EXECUTION;
        }

        UNINSTALL_NESTED_EXCEPTION_HANDLER();
        if (result == EXCEPTION_CONTINUE_EXECUTION)
            return result;

    }

    // ignore breakpoint exceptions
    if (   location != ClassInitUnhandledException
        && pExceptionInfo->ExceptionRecord->ExceptionCode != STATUS_BREAKPOINT
        && pExceptionInfo->ExceptionRecord->ExceptionCode != STATUS_SINGLE_STEP) {
        LOG((LF_EH, LL_INFO100, "ThreadBaseExceptionFilter: Calling DefaultCatchHandler\n"));
        DefaultCatchHandler(NULL, FALSE);
    }

    return EXCEPTION_CONTINUE_SEARCH;
}

#ifndef _X86_
BOOL InitializeExceptionHandling() {
    return TRUE;
}

#ifdef SHOULD_WE_CLEANUP
VOID TerminateExceptionHandling() {
}
#endif /* SHOULD_WE_CLEANUP */
#endif

//==========================================================================
// Handy helper functions
//==========================================================================
#define FORMAT_MESSAGE_BUFFER_LENGTH 1024
#define RES_BUFF_LEN   128
#define EXCEP_BUFF_LEN FORMAT_MESSAGE_BUFFER_LENGTH + RES_BUFF_LEN + 3

static void GetWin32Error(DWORD err, WCHAR *wszBuff, int len)
{
    THROWSCOMPLUSEXCEPTION();

    DWORD res = WszFormatMessage(FORMAT_MESSAGE_FROM_SYSTEM | 
                               FORMAT_MESSAGE_IGNORE_INSERTS,
                               NULL     /*ignored msg source*/,
                               err,
                               0        /*pick appropriate languageId*/,
                               wszBuff,
                               len-1,
                               0        /*arguments*/);
    if (res == 0)
        COMPlusThrowOM();

}

static void ThrowException(OBJECTREF *pThrowable, STRINGREF message)
{
    THROWSCOMPLUSEXCEPTION();

    _ASSERTE(pThrowableAvailable(pThrowable));
    _ASSERTE(*pThrowable);

    if (message != NULL)
    {
        INT64 args[] = {
            ObjToInt64(*pThrowable),
            ObjToInt64(message)
        };
        CallConstructor(&gsig_IM_Str_RetVoid, args);
    } 
    else 
    {
        INT64 args[] = {
            ObjToInt64(*pThrowable)
        };
        CallConstructor(&gsig_IM_RetVoid, args);
    }

    COMPlusThrow(*pThrowable);

    _ASSERTE(!"Should never reach here !");
}

/*
 *  Given a class name and a "message string", an object of the throwable class 
 *  is created and the constructor  is called using the "message string".
 *  Use this method if the constructor takes a string as parameter.
 *  If the string is NULL, the constructor is called without any
 *  parameters.
 *
 *  After successfully creating an object of the class, it is thrown.
 *
 *  If the message is not null and the object constructor does not take a 
 *  string input, a method  not found exception is thrown.
 */
void ThrowUsingMessage(MethodTable * pMT, const WCHAR *pszMsg)
{
    THROWSCOMPLUSEXCEPTION();

    struct _gc {
        OBJECTREF throwable;
        STRINGREF message;
        _gc() : throwable(OBJECTREF((size_t)NULL)), message(STRINGREF((size_t)NULL)) {}
    } gc;

    GCPROTECT_BEGIN(gc);

    gc.throwable = AllocateObject(pMT);

    if (pszMsg)
    {
        gc.message = COMString::NewString(pszMsg);
        if (gc.message == NULL) COMPlusThrowOM();
    }

    ThrowException(&gc.throwable, gc.message);

    _ASSERTE(!"Should never reach here !");
    GCPROTECT_END();
}

/*
 *  Given a class name, an object of the throwable class is created and the 
 *  constructor is called using a string created using Win32 error message.
 *  This function uses GetLastError() to obtain the Win32 error code.
 *  Use this method if the constructor takes a string as parameter.
 *  After successfully creating an object of the class, it is thrown.
 *
 *  If the object constructor does not take a string input, a method 
 *  not found exception is thrown.
 */
void ThrowUsingWin32Message(MethodTable * pMT)
{
    THROWSCOMPLUSEXCEPTION();

    WCHAR wszBuff[FORMAT_MESSAGE_BUFFER_LENGTH];

    GetWin32Error(GetLastError(), wszBuff, FORMAT_MESSAGE_BUFFER_LENGTH);
    wszBuff[FORMAT_MESSAGE_BUFFER_LENGTH - 1] = 0;

    struct _gc {
        OBJECTREF throwable;
        STRINGREF message;
        _gc() : throwable(OBJECTREF((size_t)NULL)), message(STRINGREF((size_t)NULL)) {}
    } gc;

    GCPROTECT_BEGIN(gc);

    gc.message = COMString::NewString(wszBuff);

    gc.throwable = AllocateObject(pMT);

    ThrowException(&gc.throwable, gc.message);

    _ASSERTE(!"Should never reach here !");
    GCPROTECT_END();
}

/*
 * An attempt is made to get the Win32 Error message. 
 * If the Win32 message is available, it is appended to the given message.
 * The ResourceID is used to lookup a string to get the message
 */
void ThrowUsingResourceAndWin32(MethodTable * pMT, DWORD dwMsgResID)
{
    THROWSCOMPLUSEXCEPTION();

    WCHAR wszMessage[EXCEP_BUFF_LEN];
    CreateMessageFromRes (pMT, dwMsgResID, TRUE, wszMessage);
    struct _gc {
        OBJECTREF throwable;
        STRINGREF message;
        _gc() : throwable(OBJECTREF((size_t)NULL)), message(STRINGREF((size_t)NULL)) {}
    } gc;

    GCPROTECT_BEGIN(gc);

    gc.message = COMString::NewString(wszMessage);

    gc.throwable = AllocateObject(pMT);

    ThrowException(&gc.throwable, gc.message);

    _ASSERTE(!"Should never reach here !");
    GCPROTECT_END();
}

void ThrowUsingResource(MethodTable * pMT, DWORD dwMsgResID)
{
    THROWSCOMPLUSEXCEPTION();

    CreateMessageFromRes (pMT, dwMsgResID, FALSE, NULL);
}

// wszMessage is, when affected, RCString[WIN32_Error]
void CreateMessageFromRes (MethodTable * pMT, DWORD dwMsgResID, BOOL win32error, WCHAR * wszMessage)
{
    // First get the Win32 error code
    DWORD err = GetLastError();
    
    // Create the exception message by looking up the resource
    WCHAR wszBuff[EXCEP_BUFF_LEN];
    wszBuff[0] = 0;

    if (FAILED(LoadStringRC(dwMsgResID, wszBuff, RES_BUFF_LEN)))
    {
        wcsncpy(wszBuff, L"[EEInternal : LoadResource Failed]", EXCEP_BUFF_LEN);
        ThrowUsingMessage(pMT, wszBuff);
    }

    wszBuff[RES_BUFF_LEN] = 0;

    // get the Win32 error code
    err = (win32error ?  err : 0);
    if (err == 0)
    {
        ThrowUsingMessage(pMT, wszBuff);
        _ASSERTE(!"Should never reach here !");
    }

    wcscat(wszBuff, L"[");

    // FORMAT_MESSAGE_BUFFER_LENGTH is guaranteed to work because we used
    // wcslen(wszBuff) <= RES_BUFF_LEN + 1

    _ASSERTE(wcslen(wszBuff) <= (RES_BUFF_LEN + 1));

    GetWin32Error(err, &wszBuff[wcslen(wszBuff)], FORMAT_MESSAGE_BUFFER_LENGTH);
    wszBuff[EXCEP_BUFF_LEN - 1] = 0;

    wcscat(wszBuff, L"]");
    _ASSERTE(wcslen(wszBuff) < EXCEP_BUFF_LEN);        
    wcscpy(wszMessage, wszBuff);
}



LPCWSTR GetHResultSymbolicName(HRESULT hr)
{
#define CASE_HRESULT(hrname) case hrname: return L#hrname;


    switch (hr)
    {
        CASE_HRESULT(E_UNEXPECTED)//                     0x8000FFFFL
        CASE_HRESULT(E_NOTIMPL)//                        0x80004001L
        CASE_HRESULT(E_OUTOFMEMORY)//                    0x8007000EL
        CASE_HRESULT(E_INVALIDARG)//                     0x80070057L
        CASE_HRESULT(E_NOINTERFACE)//                    0x80004002L
        CASE_HRESULT(E_POINTER)//                        0x80004003L
        CASE_HRESULT(E_HANDLE)//                         0x80070006L
        CASE_HRESULT(E_ABORT)//                          0x80004004L
        CASE_HRESULT(E_FAIL)//                           0x80004005L
        CASE_HRESULT(E_ACCESSDENIED)//                   0x80070005L

        CASE_HRESULT(CO_E_INIT_TLS)//                    0x80004006L
        CASE_HRESULT(CO_E_INIT_SHARED_ALLOCATOR)//       0x80004007L
        CASE_HRESULT(CO_E_INIT_MEMORY_ALLOCATOR)//       0x80004008L
        CASE_HRESULT(CO_E_INIT_CLASS_CACHE)//            0x80004009L
        CASE_HRESULT(CO_E_INIT_RPC_CHANNEL)//            0x8000400AL
        CASE_HRESULT(CO_E_INIT_TLS_SET_CHANNEL_CONTROL)// 0x8000400BL
        CASE_HRESULT(CO_E_INIT_TLS_CHANNEL_CONTROL)//    0x8000400CL
        CASE_HRESULT(CO_E_INIT_UNACCEPTED_USER_ALLOCATOR)// 0x8000400DL
        CASE_HRESULT(CO_E_INIT_SCM_MUTEX_EXISTS)//       0x8000400EL
        CASE_HRESULT(CO_E_INIT_SCM_FILE_MAPPING_EXISTS)// 0x8000400FL
        CASE_HRESULT(CO_E_INIT_SCM_MAP_VIEW_OF_FILE)//   0x80004010L
        CASE_HRESULT(CO_E_INIT_SCM_EXEC_FAILURE)//       0x80004011L
        CASE_HRESULT(CO_E_INIT_ONLY_SINGLE_THREADED)//   0x80004012L

// ******************
// FACILITY_ITF
// ******************

        CASE_HRESULT(S_OK)//                             0x00000000L
        CASE_HRESULT(S_FALSE)//                          0x00000001L
        CASE_HRESULT(OLE_E_OLEVERB)//                    0x80040000L
        CASE_HRESULT(OLE_E_ADVF)//                       0x80040001L
        CASE_HRESULT(OLE_E_ENUM_NOMORE)//                0x80040002L
        CASE_HRESULT(OLE_E_ADVISENOTSUPPORTED)//         0x80040003L
        CASE_HRESULT(OLE_E_NOCONNECTION)//               0x80040004L
        CASE_HRESULT(OLE_E_NOTRUNNING)//                 0x80040005L
        CASE_HRESULT(OLE_E_NOCACHE)//                    0x80040006L
        CASE_HRESULT(OLE_E_BLANK)//                      0x80040007L
        CASE_HRESULT(OLE_E_CLASSDIFF)//                  0x80040008L
        CASE_HRESULT(OLE_E_CANT_GETMONIKER)//            0x80040009L
        CASE_HRESULT(OLE_E_CANT_BINDTOSOURCE)//          0x8004000AL
        CASE_HRESULT(OLE_E_STATIC)//                     0x8004000BL
        CASE_HRESULT(OLE_E_PROMPTSAVECANCELLED)//        0x8004000CL
        CASE_HRESULT(OLE_E_INVALIDRECT)//                0x8004000DL
        CASE_HRESULT(OLE_E_WRONGCOMPOBJ)//               0x8004000EL
        CASE_HRESULT(OLE_E_INVALIDHWND)//                0x8004000FL
        CASE_HRESULT(OLE_E_NOT_INPLACEACTIVE)//          0x80040010L
        CASE_HRESULT(OLE_E_CANTCONVERT)//                0x80040011L
        CASE_HRESULT(OLE_E_NOSTORAGE)//                  0x80040012L
        CASE_HRESULT(DV_E_FORMATETC)//                   0x80040064L
        CASE_HRESULT(DV_E_DVTARGETDEVICE)//              0x80040065L
        CASE_HRESULT(DV_E_STGMEDIUM)//                   0x80040066L
        CASE_HRESULT(DV_E_STATDATA)//                    0x80040067L
        CASE_HRESULT(DV_E_LINDEX)//                      0x80040068L
        CASE_HRESULT(DV_E_TYMED)//                       0x80040069L
        CASE_HRESULT(DV_E_CLIPFORMAT)//                  0x8004006AL
        CASE_HRESULT(DV_E_DVASPECT)//                    0x8004006BL
        CASE_HRESULT(DV_E_DVTARGETDEVICE_SIZE)//         0x8004006CL
        CASE_HRESULT(DV_E_NOIVIEWOBJECT)//               0x8004006DL
        CASE_HRESULT(DRAGDROP_E_NOTREGISTERED)//         0x80040100L
        CASE_HRESULT(DRAGDROP_E_ALREADYREGISTERED)//     0x80040101L
        CASE_HRESULT(DRAGDROP_E_INVALIDHWND)//           0x80040102L
        CASE_HRESULT(CLASS_E_NOAGGREGATION)//            0x80040110L
        CASE_HRESULT(CLASS_E_CLASSNOTAVAILABLE)//        0x80040111L
        CASE_HRESULT(VIEW_E_DRAW)//                      0x80040140L
        CASE_HRESULT(REGDB_E_READREGDB)//                0x80040150L
        CASE_HRESULT(REGDB_E_WRITEREGDB)//               0x80040151L
        CASE_HRESULT(REGDB_E_KEYMISSING)//               0x80040152L
        CASE_HRESULT(REGDB_E_INVALIDVALUE)//             0x80040153L
        CASE_HRESULT(REGDB_E_CLASSNOTREG)//              0x80040154L
        CASE_HRESULT(CACHE_E_NOCACHE_UPDATED)//          0x80040170L
        CASE_HRESULT(OLEOBJ_E_NOVERBS)//                 0x80040180L
        CASE_HRESULT(INPLACE_E_NOTUNDOABLE)//            0x800401A0L
        CASE_HRESULT(INPLACE_E_NOTOOLSPACE)//            0x800401A1L
        CASE_HRESULT(CONVERT10_E_OLESTREAM_GET)//        0x800401C0L
        CASE_HRESULT(CONVERT10_E_OLESTREAM_PUT)//        0x800401C1L
        CASE_HRESULT(CONVERT10_E_OLESTREAM_FMT)//        0x800401C2L
        CASE_HRESULT(CONVERT10_E_OLESTREAM_BITMAP_TO_DIB)// 0x800401C3L
        CASE_HRESULT(CONVERT10_E_STG_FMT)//              0x800401C4L
        CASE_HRESULT(CONVERT10_E_STG_NO_STD_STREAM)//    0x800401C5L
        CASE_HRESULT(CONVERT10_E_STG_DIB_TO_BITMAP)//    0x800401C6L
        CASE_HRESULT(CLIPBRD_E_CANT_OPEN)//              0x800401D0L
        CASE_HRESULT(CLIPBRD_E_CANT_EMPTY)//             0x800401D1L
        CASE_HRESULT(CLIPBRD_E_CANT_SET)//               0x800401D2L
        CASE_HRESULT(CLIPBRD_E_BAD_DATA)//               0x800401D3L
        CASE_HRESULT(CLIPBRD_E_CANT_CLOSE)//             0x800401D4L
        CASE_HRESULT(MK_E_CONNECTMANUALLY)//             0x800401E0L
        CASE_HRESULT(MK_E_EXCEEDEDDEADLINE)//            0x800401E1L
        CASE_HRESULT(MK_E_NEEDGENERIC)//                 0x800401E2L
        CASE_HRESULT(MK_E_UNAVAILABLE)//                 0x800401E3L
        CASE_HRESULT(MK_E_SYNTAX)//                      0x800401E4L
        CASE_HRESULT(MK_E_NOOBJECT)//                    0x800401E5L
        CASE_HRESULT(MK_E_INVALIDEXTENSION)//            0x800401E6L
        CASE_HRESULT(MK_E_INTERMEDIATEINTERFACENOTSUPPORTED)// 0x800401E7L
        CASE_HRESULT(MK_E_NOTBINDABLE)//                 0x800401E8L
        CASE_HRESULT(MK_E_NOTBOUND)//                    0x800401E9L
        CASE_HRESULT(MK_E_CANTOPENFILE)//                0x800401EAL
        CASE_HRESULT(MK_E_MUSTBOTHERUSER)//              0x800401EBL
        CASE_HRESULT(MK_E_NOINVERSE)//                   0x800401ECL
        CASE_HRESULT(MK_E_NOSTORAGE)//                   0x800401EDL
        CASE_HRESULT(MK_E_NOPREFIX)//                    0x800401EEL
        CASE_HRESULT(MK_E_ENUMERATION_FAILED)//          0x800401EFL
        CASE_HRESULT(CO_E_NOTINITIALIZED)//              0x800401F0L
        CASE_HRESULT(CO_E_ALREADYINITIALIZED)//          0x800401F1L
        CASE_HRESULT(CO_E_CANTDETERMINECLASS)//          0x800401F2L
        CASE_HRESULT(CO_E_CLASSSTRING)//                 0x800401F3L
        CASE_HRESULT(CO_E_IIDSTRING)//                   0x800401F4L
        CASE_HRESULT(CO_E_APPNOTFOUND)//                 0x800401F5L
        CASE_HRESULT(CO_E_APPSINGLEUSE)//                0x800401F6L
        CASE_HRESULT(CO_E_ERRORINAPP)//                  0x800401F7L
        CASE_HRESULT(CO_E_DLLNOTFOUND)//                 0x800401F8L
        CASE_HRESULT(CO_E_ERRORINDLL)//                  0x800401F9L
        CASE_HRESULT(CO_E_WRONGOSFORAPP)//               0x800401FAL
        CASE_HRESULT(CO_E_OBJNOTREG)//                   0x800401FBL
        CASE_HRESULT(CO_E_OBJISREG)//                    0x800401FCL
        CASE_HRESULT(CO_E_OBJNOTCONNECTED)//             0x800401FDL
        CASE_HRESULT(CO_E_APPDIDNTREG)//                 0x800401FEL
        CASE_HRESULT(CO_E_RELEASED)//                    0x800401FFL

        CASE_HRESULT(OLE_S_USEREG)//                     0x00040000L
        CASE_HRESULT(OLE_S_STATIC)//                     0x00040001L
        CASE_HRESULT(OLE_S_MAC_CLIPFORMAT)//             0x00040002L
        CASE_HRESULT(DRAGDROP_S_DROP)//                  0x00040100L
        CASE_HRESULT(DRAGDROP_S_CANCEL)//                0x00040101L
        CASE_HRESULT(DRAGDROP_S_USEDEFAULTCURSORS)//     0x00040102L
        CASE_HRESULT(DATA_S_SAMEFORMATETC)//             0x00040130L
        CASE_HRESULT(VIEW_S_ALREADY_FROZEN)//            0x00040140L
        CASE_HRESULT(CACHE_S_FORMATETC_NOTSUPPORTED)//   0x00040170L
        CASE_HRESULT(CACHE_S_SAMECACHE)//                0x00040171L
        CASE_HRESULT(CACHE_S_SOMECACHES_NOTUPDATED)//    0x00040172L
        CASE_HRESULT(OLEOBJ_S_INVALIDVERB)//             0x00040180L
        CASE_HRESULT(OLEOBJ_S_CANNOT_DOVERB_NOW)//       0x00040181L
        CASE_HRESULT(OLEOBJ_S_INVALIDHWND)//             0x00040182L
        CASE_HRESULT(INPLACE_S_TRUNCATED)//              0x000401A0L
        CASE_HRESULT(CONVERT10_S_NO_PRESENTATION)//      0x000401C0L
        CASE_HRESULT(MK_S_REDUCED_TO_SELF)//             0x000401E2L
        CASE_HRESULT(MK_S_ME)//                          0x000401E4L
        CASE_HRESULT(MK_S_HIM)//                         0x000401E5L
        CASE_HRESULT(MK_S_US)//                          0x000401E6L
        CASE_HRESULT(MK_S_MONIKERALREADYREGISTERED)//    0x000401E7L

// ******************
// FACILITY_WINDOWS
// ******************

        CASE_HRESULT(CO_E_CLASS_CREATE_FAILED)//         0x80080001L
        CASE_HRESULT(CO_E_SCM_ERROR)//                   0x80080002L
        CASE_HRESULT(CO_E_SCM_RPC_FAILURE)//             0x80080003L
        CASE_HRESULT(CO_E_BAD_PATH)//                    0x80080004L
        CASE_HRESULT(CO_E_SERVER_EXEC_FAILURE)//         0x80080005L
        CASE_HRESULT(CO_E_OBJSRV_RPC_FAILURE)//          0x80080006L
        CASE_HRESULT(MK_E_NO_NORMALIZED)//               0x80080007L
        CASE_HRESULT(CO_E_SERVER_STOPPING)//             0x80080008L
        CASE_HRESULT(MEM_E_INVALID_ROOT)//               0x80080009L
        CASE_HRESULT(MEM_E_INVALID_LINK)//               0x80080010L
        CASE_HRESULT(MEM_E_INVALID_SIZE)//               0x80080011L

// ******************
// FACILITY_DISPATCH
// ******************

        CASE_HRESULT(DISP_E_UNKNOWNINTERFACE)//          0x80020001L
        CASE_HRESULT(DISP_E_MEMBERNOTFOUND)//            0x80020003L
        CASE_HRESULT(DISP_E_PARAMNOTFOUND)//             0x80020004L
        CASE_HRESULT(DISP_E_TYPEMISMATCH)//              0x80020005L
        CASE_HRESULT(DISP_E_UNKNOWNNAME)//               0x80020006L
        CASE_HRESULT(DISP_E_NONAMEDARGS)//               0x80020007L
        CASE_HRESULT(DISP_E_BADVARTYPE)//                0x80020008L
        CASE_HRESULT(DISP_E_EXCEPTION)//                 0x80020009L
        CASE_HRESULT(DISP_E_OVERFLOW)//                  0x8002000AL
        CASE_HRESULT(DISP_E_BADINDEX)//                  0x8002000BL
        CASE_HRESULT(DISP_E_UNKNOWNLCID)//               0x8002000CL
        CASE_HRESULT(DISP_E_ARRAYISLOCKED)//             0x8002000DL
        CASE_HRESULT(DISP_E_BADPARAMCOUNT)//             0x8002000EL
        CASE_HRESULT(DISP_E_PARAMNOTOPTIONAL)//          0x8002000FL
        CASE_HRESULT(DISP_E_BADCALLEE)//                 0x80020010L
        CASE_HRESULT(DISP_E_NOTACOLLECTION)//            0x80020011L
        CASE_HRESULT(TYPE_E_BUFFERTOOSMALL)//            0x80028016L
        CASE_HRESULT(TYPE_E_INVDATAREAD)//               0x80028018L
        CASE_HRESULT(TYPE_E_UNSUPFORMAT)//               0x80028019L
        CASE_HRESULT(TYPE_E_REGISTRYACCESS)//            0x8002801CL
        CASE_HRESULT(TYPE_E_LIBNOTREGISTERED)//          0x8002801DL
        CASE_HRESULT(TYPE_E_UNDEFINEDTYPE)//             0x80028027L
        CASE_HRESULT(TYPE_E_QUALIFIEDNAMEDISALLOWED)//   0x80028028L
        CASE_HRESULT(TYPE_E_INVALIDSTATE)//              0x80028029L
        CASE_HRESULT(TYPE_E_WRONGTYPEKIND)//             0x8002802AL
        CASE_HRESULT(TYPE_E_ELEMENTNOTFOUND)//           0x8002802BL
        CASE_HRESULT(TYPE_E_AMBIGUOUSNAME)//             0x8002802CL
        CASE_HRESULT(TYPE_E_NAMECONFLICT)//              0x8002802DL
        CASE_HRESULT(TYPE_E_UNKNOWNLCID)//               0x8002802EL
        CASE_HRESULT(TYPE_E_DLLFUNCTIONNOTFOUND)//       0x8002802FL
        CASE_HRESULT(TYPE_E_BADMODULEKIND)//             0x800288BDL
        CASE_HRESULT(TYPE_E_SIZETOOBIG)//                0x800288C5L
        CASE_HRESULT(TYPE_E_DUPLICATEID)//               0x800288C6L
        CASE_HRESULT(TYPE_E_INVALIDID)//                 0x800288CFL
        CASE_HRESULT(TYPE_E_TYPEMISMATCH)//              0x80028CA0L
        CASE_HRESULT(TYPE_E_OUTOFBOUNDS)//               0x80028CA1L
        CASE_HRESULT(TYPE_E_IOERROR)//                   0x80028CA2L
        CASE_HRESULT(TYPE_E_CANTCREATETMPFILE)//         0x80028CA3L
        CASE_HRESULT(TYPE_E_CANTLOADLIBRARY)//           0x80029C4AL
        CASE_HRESULT(TYPE_E_INCONSISTENTPROPFUNCS)//     0x80029C83L
        CASE_HRESULT(TYPE_E_CIRCULARTYPE)//              0x80029C84L

// ******************
// FACILITY_STORAGE
// ******************

        CASE_HRESULT(STG_E_INVALIDFUNCTION)//            0x80030001L
        CASE_HRESULT(STG_E_FILENOTFOUND)//               0x80030002L
        CASE_HRESULT(STG_E_PATHNOTFOUND)//               0x80030003L
        CASE_HRESULT(STG_E_TOOMANYOPENFILES)//           0x80030004L
        CASE_HRESULT(STG_E_ACCESSDENIED)//               0x80030005L
        CASE_HRESULT(STG_E_INVALIDHANDLE)//              0x80030006L
        CASE_HRESULT(STG_E_INSUFFICIENTMEMORY)//         0x80030008L
        CASE_HRESULT(STG_E_INVALIDPOINTER)//             0x80030009L
        CASE_HRESULT(STG_E_NOMOREFILES)//                0x80030012L
        CASE_HRESULT(STG_E_DISKISWRITEPROTECTED)//       0x80030013L
        CASE_HRESULT(STG_E_SEEKERROR)//                  0x80030019L
        CASE_HRESULT(STG_E_WRITEFAULT)//                 0x8003001DL
        CASE_HRESULT(STG_E_READFAULT)//                  0x8003001EL
        CASE_HRESULT(STG_E_SHAREVIOLATION)//             0x80030020L
        CASE_HRESULT(STG_E_LOCKVIOLATION)//              0x80030021L
        CASE_HRESULT(STG_E_FILEALREADYEXISTS)//          0x80030050L
        CASE_HRESULT(STG_E_INVALIDPARAMETER)//           0x80030057L
        CASE_HRESULT(STG_E_MEDIUMFULL)//                 0x80030070L
        CASE_HRESULT(STG_E_ABNORMALAPIEXIT)//            0x800300FAL
        CASE_HRESULT(STG_E_INVALIDHEADER)//              0x800300FBL
        CASE_HRESULT(STG_E_INVALIDNAME)//                0x800300FCL
        CASE_HRESULT(STG_E_UNKNOWN)//                    0x800300FDL
        CASE_HRESULT(STG_E_UNIMPLEMENTEDFUNCTION)//      0x800300FEL
        CASE_HRESULT(STG_E_INVALIDFLAG)//                0x800300FFL
        CASE_HRESULT(STG_E_INUSE)//                      0x80030100L
        CASE_HRESULT(STG_E_NOTCURRENT)//                 0x80030101L
        CASE_HRESULT(STG_E_REVERTED)//                   0x80030102L
        CASE_HRESULT(STG_E_CANTSAVE)//                   0x80030103L
        CASE_HRESULT(STG_E_OLDFORMAT)//                  0x80030104L
        CASE_HRESULT(STG_E_OLDDLL)//                     0x80030105L
        CASE_HRESULT(STG_E_SHAREREQUIRED)//              0x80030106L
        CASE_HRESULT(STG_E_NOTFILEBASEDSTORAGE)//        0x80030107L
        CASE_HRESULT(STG_S_CONVERTED)//                  0x00030200L

// ******************
// FACILITY_RPC
// ******************

        CASE_HRESULT(RPC_E_CALL_REJECTED)//              0x80010001L
        CASE_HRESULT(RPC_E_CALL_CANCELED)//              0x80010002L
        CASE_HRESULT(RPC_E_CANTPOST_INSENDCALL)//        0x80010003L
        CASE_HRESULT(RPC_E_CANTCALLOUT_INASYNCCALL)//    0x80010004L
        CASE_HRESULT(RPC_E_CANTCALLOUT_INEXTERNALCALL)// 0x80010005L
        CASE_HRESULT(RPC_E_CONNECTION_TERMINATED)//      0x80010006L
        CASE_HRESULT(RPC_E_SERVER_DIED)//                0x80010007L
        CASE_HRESULT(RPC_E_CLIENT_DIED)//                0x80010008L
        CASE_HRESULT(RPC_E_INVALID_DATAPACKET)//         0x80010009L
        CASE_HRESULT(RPC_E_CANTTRANSMIT_CALL)//          0x8001000AL
        CASE_HRESULT(RPC_E_CLIENT_CANTMARSHAL_DATA)//    0x8001000BL
        CASE_HRESULT(RPC_E_CLIENT_CANTUNMARSHAL_DATA)//  0x8001000CL
        CASE_HRESULT(RPC_E_SERVER_CANTMARSHAL_DATA)//    0x8001000DL
        CASE_HRESULT(RPC_E_SERVER_CANTUNMARSHAL_DATA)//  0x8001000EL
        CASE_HRESULT(RPC_E_INVALID_DATA)//               0x8001000FL
        CASE_HRESULT(RPC_E_INVALID_PARAMETER)//          0x80010010L
        CASE_HRESULT(RPC_E_CANTCALLOUT_AGAIN)//          0x80010011L
        CASE_HRESULT(RPC_E_SERVER_DIED_DNE)//            0x80010012L
        CASE_HRESULT(RPC_E_SYS_CALL_FAILED)//            0x80010100L
        CASE_HRESULT(RPC_E_OUT_OF_RESOURCES)//           0x80010101L
        CASE_HRESULT(RPC_E_ATTEMPTED_MULTITHREAD)//      0x80010102L
        CASE_HRESULT(RPC_E_NOT_REGISTERED)//             0x80010103L
        CASE_HRESULT(RPC_E_FAULT)//                      0x80010104L
        CASE_HRESULT(RPC_E_SERVERFAULT)//                0x80010105L
        CASE_HRESULT(RPC_E_CHANGED_MODE)//               0x80010106L
        CASE_HRESULT(RPC_E_INVALIDMETHOD)//              0x80010107L
        CASE_HRESULT(RPC_E_DISCONNECTED)//               0x80010108L
        CASE_HRESULT(RPC_E_RETRY)//                      0x80010109L
        CASE_HRESULT(RPC_E_SERVERCALL_RETRYLATER)//      0x8001010AL
        CASE_HRESULT(RPC_E_SERVERCALL_REJECTED)//        0x8001010BL
        CASE_HRESULT(RPC_E_INVALID_CALLDATA)//           0x8001010CL
        CASE_HRESULT(RPC_E_CANTCALLOUT_ININPUTSYNCCALL)// 0x8001010DL
        CASE_HRESULT(RPC_E_WRONG_THREAD)//               0x8001010EL
        CASE_HRESULT(RPC_E_THREAD_NOT_INIT)//            0x8001010FL
        CASE_HRESULT(RPC_E_UNEXPECTED)//                 0x8001FFFFL   

// ******************
// FACILITY_CTL
// ******************

        CASE_HRESULT(CTL_E_ILLEGALFUNCTIONCALL)       
        CASE_HRESULT(CTL_E_OVERFLOW)                  
        CASE_HRESULT(CTL_E_OUTOFMEMORY)               
        CASE_HRESULT(CTL_E_DIVISIONBYZERO)            
        CASE_HRESULT(CTL_E_OUTOFSTRINGSPACE)          
        CASE_HRESULT(CTL_E_OUTOFSTACKSPACE)           
        CASE_HRESULT(CTL_E_BADFILENAMEORNUMBER)       
        CASE_HRESULT(CTL_E_FILENOTFOUND)              
        CASE_HRESULT(CTL_E_BADFILEMODE)               
        CASE_HRESULT(CTL_E_FILEALREADYOPEN)           
        CASE_HRESULT(CTL_E_DEVICEIOERROR)             
        CASE_HRESULT(CTL_E_FILEALREADYEXISTS)         
        CASE_HRESULT(CTL_E_BADRECORDLENGTH)           
        CASE_HRESULT(CTL_E_DISKFULL)                  
        CASE_HRESULT(CTL_E_BADRECORDNUMBER)           
        CASE_HRESULT(CTL_E_BADFILENAME)               
        CASE_HRESULT(CTL_E_TOOMANYFILES)              
        CASE_HRESULT(CTL_E_DEVICEUNAVAILABLE)         
        CASE_HRESULT(CTL_E_PERMISSIONDENIED)          
        CASE_HRESULT(CTL_E_DISKNOTREADY)              
        CASE_HRESULT(CTL_E_PATHFILEACCESSERROR)       
        CASE_HRESULT(CTL_E_PATHNOTFOUND)              
        CASE_HRESULT(CTL_E_INVALIDPATTERNSTRING)      
        CASE_HRESULT(CTL_E_INVALIDUSEOFNULL)          
        CASE_HRESULT(CTL_E_INVALIDFILEFORMAT)         
        CASE_HRESULT(CTL_E_INVALIDPROPERTYVALUE)      
        CASE_HRESULT(CTL_E_INVALIDPROPERTYARRAYINDEX) 
        CASE_HRESULT(CTL_E_SETNOTSUPPORTEDATRUNTIME)  
        CASE_HRESULT(CTL_E_SETNOTSUPPORTED)           
        CASE_HRESULT(CTL_E_NEEDPROPERTYARRAYINDEX)    
        CASE_HRESULT(CTL_E_SETNOTPERMITTED)           
        CASE_HRESULT(CTL_E_GETNOTSUPPORTEDATRUNTIME)  
        CASE_HRESULT(CTL_E_GETNOTSUPPORTED)           
        CASE_HRESULT(CTL_E_PROPERTYNOTFOUND)          
        CASE_HRESULT(CTL_E_INVALIDCLIPBOARDFORMAT)    
        CASE_HRESULT(CTL_E_INVALIDPICTURE)            
        CASE_HRESULT(CTL_E_PRINTERERROR)              
        CASE_HRESULT(CTL_E_CANTSAVEFILETOTEMP)        
        CASE_HRESULT(CTL_E_SEARCHTEXTNOTFOUND)        
        CASE_HRESULT(CTL_E_REPLACEMENTSTOOLONG)       

        default:
            return NULL;
    }
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\expandsig.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==

#ifndef __EXPANDSIG_H__
#define __EXPANDSIG_H__

#include "siginfo.hpp"

//------------------------------------------------------------------------
// Encapsulates the format and simplifies walking of MetaData sigs.
//------------------------------------------------------------------------
#define ARG_OFFSET          2       // Offset to the start of the Arguments
#define FLAG_OFFSET         0       // Offset to flags
#define RETURN_TYPE_OFFSET  1       // Offset of where the return type starts

#define VALUE_RETBUF_ARG           0x10    // Value Return
#define HEAP_ALLOCATED             0x20    // Signature allocated on heap

class ExpandSig // public MetaSig
{
friend class MetaSig;

private:
    ExpandSig(PCCOR_SIGNATURE sig, Module* pModule);

public:

    void operator delete(void *p) { if (((ExpandSig*)p)->m_flags & HEAP_ALLOCATED) ::delete [] (BYTE*)p; }

    static ExpandSig* GetReflectSig(PCCOR_SIGNATURE sig, Module* pModule);
    static ExpandSig* GetSig(PCCOR_SIGNATURE sig, Module* pModule);

    BOOL IsEquivalent(ExpandSig *pOther);

    // Some MetaSig services, exposed here for convenience
    UINT NumFixedArgs()
    {
        return m_MetaSig.NumFixedArgs();
    }
    BYTE GetCallingConvention()
    {
        return m_MetaSig.GetCallingConvention();
    }
    BYTE GetCallingConventionInfo()
    {
        return m_MetaSig.GetCallingConventionInfo();
    }
    BOOL IsVarArg()
    {
        return m_MetaSig.IsVarArg();
    }
#ifdef COMPLUS_EE
    UINT GetFPReturnSize()
    {
        return m_MetaSig.GetFPReturnSize();
    }
    UINT SizeOfActualFixedArgStack(BOOL fIsStatic)
    {
        return m_MetaSig.SizeOfActualFixedArgStack(fIsStatic);
    }
    UINT SizeOfVirtualFixedArgStack(BOOL fIsStatic)
    {
        return m_MetaSig.SizeOfVirtualFixedArgStack(fIsStatic);
    }

	BOOL IsRetBuffArg()
    {
		return (m_flags & VALUE_RETBUF_ARG) ? 1 : 0;
	}

    Module* GetModule() const
    {
        return m_MetaSig.GetModule();
    }
#endif
    
    EEClass* GetReturnValueClass();
    EEClass* GetReturnClass();

    // Iterators.  There are two types of iterators, the first will return everything
    //  known about an argument.  The second simply returns the type.  Reset should
    //  be called before either of these are called.
    void Reset(void** ppEnum)
    {
        *ppEnum = 0;
    }

	// Return the type handle for the signature element
	TypeHandle NextArgExpanded(void** pEnum);
	TypeHandle GetReturnTypeHandle() {
		return m_Data[RETURN_TYPE_OFFSET];
	}

    UINT GetStackElemSize(CorElementType type,EEClass* pEEC);
    UINT GetStackElemSize(TypeHandle th);
    UINT GetElemSizes(TypeHandle th, UINT *structSize);
    
    void ExtendSkip(void** pEnum)
    {
        AdvanceEnum(pEnum);
    }

    DWORD Hash();

    BOOL AreOffsetsInitted()
    {
        return (m_MetaSig.m_fCacheInitted & SIG_OFFSETS_INITTED);
    }

    void GetInfoForArg(int argNum, short *offset, short *structSize, BYTE *pType)
    {
        _ASSERTE(m_MetaSig.m_fCacheInitted & SIG_OFFSETS_INITTED);
        _ASSERTE(argNum <= MAX_CACHED_SIG_SIZE);
        *offset = m_MetaSig.m_offsets[argNum];
        *structSize = m_MetaSig.m_sizes[argNum];
        *pType = m_MetaSig.m_types[argNum];
    }

private:
    TypeHandle AdvanceEnum(void** pEnum);

    MetaSig     m_MetaSig;
	int			m_flags;

    // The following is variable size (placement operator 'new' is used to
    // allocate) so it must come last.
    TypeHandle	m_Data[1];         // Expanded representation
};

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\expandsig.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==

#include "common.h"
#include "ExpandSig.h"
#include "ComClass.h"

// ExpandSig
// Constructor
ExpandSig::ExpandSig(PCCOR_SIGNATURE sig, Module* pModule) : m_MetaSig(sig, pModule)
{
}

ExpandSig* ExpandSig::GetReflectSig(PCCOR_SIGNATURE sig, Module* pModule)
{
    THROWSCOMPLUSEXCEPTION();

	int nArgs;
	MetaSig msig(sig, pModule);
    BYTE callConv = msig.GetCallingConvention();

	if (callConv == IMAGE_CEE_CS_CALLCONV_FIELD)
		nArgs = 0;
	else
	    nArgs = msig.NumFixedArgs();

    TypeHandle* b = (TypeHandle*) _alloca((2 + nArgs) * sizeof(TypeHandle));
	ZeroMemory(b,(2 + nArgs) * sizeof(TypeHandle));
	*((DWORD*) b) = (DWORD) callConv;

    OBJECTREF pThrowable = NULL;
    GCPROTECT_BEGIN(pThrowable);
    {
	    b[1] = msig.GetRetTypeHandle(&pThrowable);
        if (b[1].IsNull())
            COMPlusThrow(pThrowable);
	    msig.Reset();
	    for (int i=0;i<nArgs;i++) {
            msig.NextArg();
		    b[i+ARG_OFFSET] = msig.GetTypeHandle(&pThrowable);
            if (b[i+ARG_OFFSET].IsNull())
                COMPlusThrow(pThrowable);
	    }
    }
    GCPROTECT_END();

    void *retSig = pModule->GetDomain()->GetReflectionHeap()->AllocMem(sizeof(ExpandSig) + ((1 + nArgs) * sizeof(TypeHandle)));
    if (!retSig)
        COMPlusThrowOM();

	// Set the flags...

    ExpandSig* pSig = new (retSig) ExpandSig(sig,pModule);
    _ASSERTE(pSig != NULL);

    memcpy(pSig->m_Data,b,(2 + nArgs) * sizeof(TypeHandle));
	pSig->m_flags = 0;
	if (msig.HasRetBuffArg())
		pSig->m_flags |= VALUE_RETBUF_ARG;
    return pSig;
}

// Similar to GetReflectSig, but uses the static heap for allocation and doesn't
// throw an exception on allocation failure or on type load failures.
ExpandSig* ExpandSig::GetSig(PCCOR_SIGNATURE sig, Module* pModule)
{
    int nArgs;
    MetaSig msig(sig, pModule);
    BYTE callConv = msig.GetCallingConvention();

    if (callConv == IMAGE_CEE_CS_CALLCONV_FIELD)
        nArgs = 0;
    else
        nArgs = msig.NumFixedArgs();

    TypeHandle* b = (TypeHandle*) _alloca((2 + nArgs) * sizeof(TypeHandle));
    ZeroMemory(b,(2 + nArgs) * sizeof(TypeHandle));
    *((DWORD*) b) = (DWORD) callConv;
    b[1] = msig.GetRetTypeHandle();
    msig.Reset();
    for (int i=0;i<nArgs;i++) {
        msig.NextArg();
        b[i+ARG_OFFSET] = msig.GetTypeHandle();
    }

    void *retSig = new BYTE[sizeof(ExpandSig) + ((1 + nArgs) * sizeof(TypeHandle))];
    if (!retSig)
        return NULL;

    ExpandSig* pSig = new (retSig) ExpandSig(sig,pModule);

    memcpy(pSig->m_Data,b,(2 + nArgs) * sizeof(TypeHandle));
    pSig->m_flags = HEAP_ALLOCATED;
    if (msig.HasRetBuffArg())
        pSig->m_flags |= VALUE_RETBUF_ARG;

    return pSig;
}

BOOL ExpandSig::IsEquivalent(ExpandSig *pOther)
{
    ULONG uCallConv = m_MetaSig.GetCallingConvention();
    if (uCallConv != pOther->m_MetaSig.GetCallingConvention())
        return FALSE;

    if (m_Data[RETURN_TYPE_OFFSET] != pOther->m_Data[RETURN_TYPE_OFFSET])
        return FALSE;

    if (uCallConv == IMAGE_CEE_CS_CALLCONV_FIELD)
        return TRUE;

    ULONG cArgs = m_MetaSig.NumFixedArgs();
    for (ULONG i = ARG_OFFSET; i < (ARG_OFFSET + cArgs); i++)
        if (m_Data[i] != pOther->m_Data[i])
            return FALSE;

    return TRUE;
}

//@TODO: Check this???
EEClass* ExpandSig::GetReturnValueClass() 
{
	_ASSERTE(m_Data[RETURN_TYPE_OFFSET].IsUnsharedMT());
	return m_Data[RETURN_TYPE_OFFSET].AsClass();
}

EEClass* ExpandSig::GetReturnClass()
{
	_ASSERTE(m_Data[RETURN_TYPE_OFFSET].IsUnsharedMT());
	return m_Data[RETURN_TYPE_OFFSET].AsClass();
}

TypeHandle ExpandSig::NextArgExpanded(void** pEnum)
{
	TypeHandle t = AdvanceEnum(pEnum);
	return t;
}


TypeHandle ExpandSig::AdvanceEnum(void** pEnum)
{
	int i = *((int*) pEnum);

    if (i == 0)
		i=ARG_OFFSET;

	*((int*) pEnum) = i+1;

	return m_Data[i];
}


UINT ExpandSig::GetStackElemSize(CorElementType type,EEClass* pEEC)
{
    _ASSERTE(type >= 0 && type < ELEMENT_TYPE_MAX);
    DWORD dwSize = gElementTypeInfo[type].m_cbSize;
    if (dwSize != -1)
        return StackElemSize(dwSize);
    return StackElemSize(pEEC->GetAlignedNumInstanceFieldBytes());
}

UINT ExpandSig::GetStackElemSize(TypeHandle th)
{
    UINT structSize = 0;
    return GetElemSizes(th, &structSize);
}

UINT ExpandSig::GetElemSizes(TypeHandle th, UINT *structSize)
{
	CorElementType type = th.GetNormCorElementType();
    _ASSERTE(type >= 0 && type < ELEMENT_TYPE_MAX);
    *structSize = gElementTypeInfo[type].m_cbSize;
    if (*structSize != -1)
        return StackElemSize(*structSize);

	if (th.IsByRef())
		return(GetElemSizes(th.AsTypeDesc()->GetTypeParam(), structSize));

    *structSize = th.GetClass()->GetAlignedNumInstanceFieldBytes();
	return StackElemSize(*structSize);
}

DWORD ExpandSig::Hash()
{
    DWORD dwHash = m_MetaSig.GetCallingConvention();

    dwHash ^= (DWORD)(size_t)m_Data[RETURN_TYPE_OFFSET].AsPtr(); // @TODO WIN64 - pointer truncation

    if (m_MetaSig.GetCallingConvention() == IMAGE_CEE_CS_CALLCONV_FIELD)
        return dwHash;

    ULONG cArgs = m_MetaSig.NumFixedArgs();
    for (ULONG i = ARG_OFFSET; i < (ARG_OFFSET + cArgs); i++)
        dwHash ^= (DWORD)(size_t)m_Data[i].AsPtr(); // @TODO WIN64 - pointer truncation

    return dwHash;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\exceptmacros.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
// EXCEPTMACROS.H -
//
// This header file exposes mechanisms to:
//
//    1. Throw COM+ exceptions using the COMPlusThrow() function
//    2. Guard a block of code using COMPLUS_TRY, and catch
//       COM+ exceptions using COMPLUS_CATCH
//
// from the *unmanaged* portions of the EE. Much of the EE runs
// in a hybrid state where it runs like managed code but the code
// is produced by a classic unmanaged-code C++ compiler.
//
// THROWING A COM+ EXCEPTION
// -------------------------
// To throw a COM+ exception, call the function:
//
//      COMPlusThrow(OBJECTREF pThrowable);
//
// This function does not return. There are also various functions
// that wrap COMPlusThrow for convenience.
//
// COMPlusThrow() must only be called within the scope of a COMPLUS_TRY
// block. See below for more information.
//
//
// THROWING A RUNTIME EXCEPTION
// ----------------------------
// COMPlusThrow() is overloaded to take a constant describing
// the common EE-generated exceptions, e.g.
//
//    COMPlusThrow(kOutOfMemoryException);
//
// See rexcep.h for list of constants (prepend "k" to get the actual
// constant name.)
//
// You can also add a descriptive error string as follows:
//
//    - Add a descriptive error string and resource id to
//      COM99\src\dlls\mscorrc\resource.h and mscorrc.rc.
//      Embed "%1", "%2" or "%3" to leave room for runtime string
//      inserts.
//
//    - Pass the resource ID and inserts to COMPlusThrow, i.e.
//
//      COMPlusThrow(kSecurityException,
//                   IDS_CANTREFORMATCDRIVEBECAUSE,
//                   L"Formatting C drive permissions not granted.");
//
//
//
// TO CATCH COMPLUS EXCEPTIONS:
// ----------------------------
//
// Use the following syntax:
//
//      #include "exceptmacros.h"
//
//
//      OBJECTREF pThrownObject;
//
//      COMPLUS_TRY {
//          ...guarded code...
//      } COMPLUS_CATCH {
//          ...handler...
//      } COMPLUS_END_CATCH
//
//
// COMPLUS_TRY has a variant:
//
//      Thread *pCurThread = GetThread();
//      COMPLUS_TRYEX(pCurThread);
//
// The only difference is that COMPLUS_TRYEX requires you to give it
// the current Thread structure. If your code already has a pointer
// to the current Thread for other purposes, it's more efficient to
// call COMPLUS_TRYEX (COMPLUS_TRY generates a second call to GetThread()).
//
// COMPLUS_TRY blocks can be nested. 
//
// From within the handler, you can call the GETTHROWABLE() macro to
// obtain the object that was thrown.
//
// CRUCIAL POINTS
// --------------
// In order to call COMPlusThrow(), you *must* be within the scope
// of a COMPLUS_TRY block. Under _DEBUG, COMPlusThrow() will assert
// if you call it out of scope. This implies that just about every
// external entrypoint into the EE has to have a COMPLUS_TRY, in order
// to convert uncaught COM+ exceptions into some error mechanism
// more understandable to its non-COM+ caller.
//
// Any function that can throw a COM+ exception out to its caller
// has the same requirement. ALL such functions should be tagged
// with the following macro statement:
//
//      THROWSCOMPLUSEXCEPTION();
//
// at the start of the function body. Aside from making the code
// self-document its contract, the checked version of this will fire
// an assert if the function is ever called without being in scope.
//
//
// AVOIDING COMPLUS_TRY GOTCHAS
// ----------------------------
// COMPLUS_TRY/COMPLUS_CATCH actually expands into a Win32 SEH
// __try/__except structure. It does a lot of goo under the covers
// to deal with pre-emptive GC settings.
//
//    1. Do not use C++ or SEH try/__try use COMPLUS_TRY instead.
//
//    2. Remember that any function marked THROWSCOMPLUSEXCEPTION()
//       has the potential not to return. So be wary of allocating
//       non-gc'd objects around such calls because ensuring cleanup
//       of these things is not simple (you can wrap another COMPLUS_TRY
//       around the call to simulate a COM+ "try-finally" but COMPLUS_TRY
//       is relatively expensive compared to the real thing.)
//
//

#ifndef __exceptmacros_h__
#define __exceptmacros_h__

struct _EXCEPTION_REGISTRATION_RECORD;
class Thread;
VOID RealCOMPlusThrowOM();

#include <ExcepCpu.h>

// Forward declaration used in COMPLUS_CATCHEX
void Profiler_ExceptionCLRCatcherExecute();

#ifdef _DEBUG
//----------------------------------------------------------------------------------
// Code to generate a compile-time error if COMPlusThrow statement appears in a 
// function that doesn't have a THROWSCOMPLUSEXCEPTION macro.
//
// Here's the way it works...
//
// We create two classes with a safe_to_throw() method.  The method is static,
// returns void, and does nothing.  One class has the method as public, the other
// as private.  We introduce a global scope typedef for __IsSafeToThrow that refers to
// the class with the private method.  So, by default, the expression
//
//      __IsSafeToThrow::safe_to_throw()
//
// gives you a compile time error.  When we enter a block in which we want to
// allow COMPlusThrow, we introduce a new typedef that defines __IsSafeToThrow as the
// class with the public method.  Inside this scope,
//
//      __IsSafeToThrow::safe_to_throw()
//
// is not an error.
//
//
class __ThrowOK {
public:
    static void safe_to_throw() {};
};

class __YouCannotUseCOMPlusThrowWithoutTHROWSCOMPLUSEXCEPTION {
private:
    // If you got here, and you're wondering what you did wrong -- you're using
    // COMPlusThrow without tagging your function with the THROWSCOMPLUSEXCEPTION
    // macro.  In general, just add THROWSCOMPLUSEXCEPTION to the beginning
    // of your function.  
    // 
    static void safe_to_throw() {};
};

typedef __YouCannotUseCOMPlusThrowWithoutTHROWSCOMPLUSEXCEPTION __IsSafeToThrow;

// Unfortunately, the only way to make this work is to #define all return statements --
// even the ones at global scope.  This actually generates better code that appears.
// The call is dead, and does not appear in the generated code, even in a checked
// build.  (And, in fastchecked, there is no penalty at all.)
//
#define DEBUG_SAFE_TO_THROW_BEGIN { typedef __ThrowOK __IsSafeToThrow;
#define DEBUG_SAFE_TO_THROW_END   }
#define DEBUG_SAFE_TO_THROW_IN_THIS_BLOCK typedef __ThrowOK __IsSafeToThrow;

extern char *g_ExceptionFile;
extern DWORD g_ExceptionLine;

inline BOOL THROWLOG() {g_ExceptionFile = __FILE__; g_ExceptionLine = __LINE__; return 1;}

#define COMPlusThrow             if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusThrow
#define COMPlusThrowNonLocalized if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusThrowNonLocalized
#define COMPlusThrowHR           if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusThrowHR
#define COMPlusThrowWin32        if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusThrowWin32
#define COMPlusThrowOM           if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusThrowOM
#define COMPlusThrowArithmetic   if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusThrowArithmetic
#define COMPlusThrowArgumentNull if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusThrowArgumentNull
#define COMPlusThrowArgumentOutOfRange if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusThrowArgumentOutOfRange
#define COMPlusThrowMissingMethod if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusThrowMissingMethod
#define COMPlusThrowMember if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusThrowMember
#define COMPlusThrowArgumentException if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusThrowArgumentException

#define COMPlusRareRethrow if(THROWLOG() && 0) __IsSafeToThrow::safe_to_throw(); else RealCOMPlusRareRethrow

#else

#define DEBUG_SAFE_TO_THROW_IN_THIS_BLOCK
#define DEBUG_SAFE_TO_THROW_BEGIN
#define DEBUG_SAFE_TO_THROW_END
#define THIS_FUNCTION_CONTAINS_A_BUGGY_THROW

#define COMPlusThrow                    RealCOMPlusThrow
#define COMPlusThrowNonLocalized        RealCOMPlusThrowNonLocalized
#define COMPlusThrowHR                  RealCOMPlusThrowHR
#define COMPlusThrowWin32               RealCOMPlusThrowWin32
#define COMPlusThrowOM                  RealCOMPlusThrowOM
#define COMPlusThrowArithmetic          RealCOMPlusThrowArithmetic
#define COMPlusThrowArgumentNull        RealCOMPlusThrowArgumentNull
#define COMPlusThrowArgumentOutOfRange  RealCOMPlusThrowArgumentOutOfRange
#define COMPlusThrowMissingMethod       RealCOMPlusThrowMissingMethod
#define COMPlusThrowMember              RealCOMPlusThrowMember
#define COMPlusThrowArgumentException   RealCOMPlusThrowArgumentException
#define COMPlusRareRethrow              RealCOMPlusRareRethrow

#endif


//==========================================================================
// Macros to allow catching exceptions from within the EE. These are lightweight
// handlers that do not install the managed frame handler. 
//
//      EE_TRY_FOR_FINALLY {
//          ...<guarded code>...
//      } EE_FINALLY {
//          ...<handler>...
//      } EE_END_FINALLY
//
//      EE_TRY(filter expr) {
//          ...<guarded code>...
//      } EE_CATCH {
//          ...<handler>...
//      }
//==========================================================================

// __GotException will only be FALSE if got all the way through the code
// guarded by the try, otherwise it will be TRUE, so we know if we got into the
// finally from an exception or not. In which case need to reset the GC state back
// to what it was for the finally to run in that state and then disable it again
// for return to OS. If got an exception, preemptive GC should always be enabled

#define EE_TRY_FOR_FINALLY                                    \
    BOOL __fGCDisabled = GetThread()->PreemptiveGCDisabled(); \
    BOOL __fGCDisabled2 = FALSE;                                      \
    BOOL __GotException = TRUE;                               \
    __try { 

#define GOT_EXCEPTION() __GotException

#define EE_LEAVE             \
    __GotException = FALSE;  \
    __leave;

#define EE_FINALLY                                                \
        __GotException = FALSE;                                   \
    } __finally {                                                 \
        if (__GotException) {                                     \
            __fGCDisabled2 = GetThread()->PreemptiveGCDisabled(); \
            if (! __fGCDisabled) {                                \
                if (__fGCDisabled2)                               \
                    GetThread()->EnablePreemptiveGC();            \
            } else if (! __fGCDisabled2) {                        \
                Thread* ___pCurThread = GetThread();              \
                ExInfo* ___pExInfo = ___pCurThread->GetHandlerInfo(); \
                ___pCurThread->DisablePreemptiveGC();               \
               }                                                  \
        }

#define EE_END_FINALLY                                            \
        if (__GotException) {                                     \
            if (! __fGCDisabled) {                                \
                if (__fGCDisabled2) {                             \
                    Thread* ___pCurThread = GetThread();              \
                    ExInfo* ___pExInfo = ___pCurThread->GetHandlerInfo(); \
                    ___pCurThread->DisablePreemptiveGC();           \
                }                                                 \
            } else if (! __fGCDisabled2)                          \
                GetThread()->EnablePreemptiveGC();                \
            }                                                     \
        }                                                         


//==========================================================================
// Macros to allow catching COM+ exceptions from within unmanaged code:
//
//      COMPLUS_TRY {
//          ...<guarded code>...
//      } COMPLUS_CATCH {
//          ...<handler>...
//      }
//==========================================================================

#define COMPLUS_CATCH_GCCHECK()                                                \
    if (___fGCDisabled && ! ___pCurThread->PreemptiveGCDisabled())             \
        ___pCurThread->DisablePreemptiveGC();                                  

#define SAVE_HANDLER_TYPE()                                               \
        BOOL ___ExInUnmanagedHandler = ___pExInfo->IsInUnmanagedHandler(); \
        ___pExInfo->ResetIsInUnmanagedHandler();
    
#define RESTORE_HANDLER_TYPE()                    \
    if (___ExInUnmanagedHandler) {              \
        ___pExInfo->SetIsInUnmanagedHandler();  \
    }

#define COMPLUS_CATCH_NEVER_CATCH -1
#define COMPLUS_CATCH_CHECK_CATCH 0
#define COMPLUS_CATCH_ALWAYS_CATCH 1

#define COMPLUS_TRY  COMPLUS_TRYEX(GetThread())

#ifdef _DEBUG
#define DEBUG_CATCH_DEPTH_SAVE \
    void* ___oldCatchDepth = ___pCurThread->m_ComPlusCatchDepth;        \
    ___pCurThread->m_ComPlusCatchDepth = __limit;                       
#else
#define DEBUG_CATCH_DEPTH_SAVE
#endif

#ifdef _DEBUG
#define DEBUG_CATCH_DEPTH_RESTORE \
    ___pCurThread->m_ComPlusCatchDepth = ___oldCatchDepth;
#else
#define DEBUG_CATCH_DEPTH_RESTORE
#endif


#define COMPLUS_TRYEX(/* Thread* */ pCurThread)                               \
    {                                                                         \
    Thread* ___pCurThread = (pCurThread);                                     \
    _ASSERTE(___pCurThread);                                                  \
    Frame *___pFrame = ___pCurThread->GetFrame();                             \
    BOOL ___fGCDisabled = ___pCurThread->PreemptiveGCDisabled();              \
    int ___filterResult = -2;  /* An invalid value to mark non-exception path */ \
    int ___exception_unwind = 0;  /* An invalid value to mark non-exception path */ \
    ExInfo* ___pExInfo = ___pCurThread->GetHandlerInfo();                     \
    void* __limit = GetSP();                                                  \
    DEBUG_CATCH_DEPTH_SAVE                                                    \
    __try {                                                                   \
        __try {                                                               \
            __try {                                                           \
                DEBUG_SAFE_TO_THROW_IN_THIS_BLOCK

                // <TRY-BLOCK> //

#define COMPLUS_CATCH_PROLOG(flag)                                  \
            } __except((___exception_unwind = 1, EXCEPTION_CONTINUE_SEARCH)) {  \
            }                                                                   \
        } __finally {                                               \
            if(___exception_unwind) {                               \
                UnwindFrameChain(___pCurThread, ___pFrame);         \
            }                                                       \
            if(___filterResult != EXCEPTION_CONTINUE_SEARCH) {      \
                COMPLUS_CATCH_GCCHECK();                            \
            }                                                       \
            DEBUG_CATCH_DEPTH_RESTORE                               \
        }                                                           

#define COMPLUS_CATCH COMPLUS_CATCHEX(COMPLUS_CATCH_CHECK_CATCH)

// This complus passes bool indicating if should catch
#define COMPLUS_CATCHEX(flag)                                                      \
    COMPLUS_CATCH_PROLOG(flag)                                                     \
    } __except(___filterResult = COMPlusFilter(GetExceptionInformation(), flag, __limit)) { \
        ___pExInfo->m_pBottomMostHandler = NULL;                                   \
        ___exception_unwind = 0;                                                   \
        __try {                                                                    \
            __try {                                                                \
                        ___pCurThread->FixGuardPage();                             \
                        Profiler_ExceptionCLRCatcherExecute();

                // <CATCH-BLOCK> //

#define COMPLUS_END_CATCH                       \
            } __except((___exception_unwind = 1, EXCEPTION_CONTINUE_SEARCH)) {  \
            }                                                                   \
        } __finally {                                                           \
            if (!___exception_unwind) {                                         \
					bool fToggle = false;                                       \
					Thread * pThread = GetThread();                             \
					if (!pThread->PreemptiveGCDisabled())  {                    \
						fToggle = true;                                         \
						pThread->DisablePreemptiveGC();                         \
					}                                                           \
                                                                                \
                UnwindExInfo(___pExInfo, __limit);                   \
                                                                                \
					if (fToggle) {                                              \
						pThread->EnablePreemptiveGC();                          \
					}                                                           \
                                                                                \
				}                                                               \
        }                                                                       \
    }                                                                           \
    }


// we set pCurThread to NULL to indicate if an exception occured as otherwise it is asserted
// to be non-null.
#define COMPLUS_FINALLY                                                 \
            } __except((___exception_unwind = 1, EXCEPTION_CONTINUE_SEARCH)) {  \
            }                                                                   \
        } __finally {                                                   \
            if(___exception_unwind) {                                   \
                UnwindFrameChain(___pCurThread, ___pFrame);             \
            }                                                           \
        }                                                               \
        ___pCurThread = NULL;                                           \
    } __finally {                                                       \
            BOOL ___fGCDisabled2 = GetThread()->PreemptiveGCDisabled(); \
            if (___pCurThread) {                                        \
            if (! ___fGCDisabled) {                                     \
                if (___fGCDisabled2) {                                  \
                    GetThread()->EnablePreemptiveGC();                  \
                }                                                       \
            } else if (! ___fGCDisabled2) {                             \
                GetThread()->DisablePreemptiveGC();                     \
                }                                                       \
        }                                                               \
                              

#define COMPLUS_END_FINALLY                                                                        \
        if (___pCurThread) {                              \
            if (! ___fGCDisabled) {                       \
                           if (___fGCDisabled2) {         \
                                GetThread()->DisablePreemptiveGC();           \
                           }                                                  \
            } else if (! ___fGCDisabled2) {               \
                GetThread()->EnablePreemptiveGC();        \
            }                                             \
        }                                                 \
    }                                                     \
    }

// @BUG 59559: These need to be implemented.
#define COMPLUS_END_CATCH_MIGHT_RETHROW    COMPLUS_END_CATCH
#define COMPLUS_END_CATCH_NO_RETHROW       COMPLUS_END_CATCH

#define GETTHROWABLE()              (GetThread()->GetThrowable())
#define SETTHROWABLE(or)            (GetThread()->SetThrowable(or))


//==========================================================================
// Declares that a function can throw an uncaught COM+ exception.
//==========================================================================
#ifdef _DEBUG

#define THROWSCOMPLUSEXCEPTION()                        \
    ThrowsCOMPlusExceptionWorker();                     \
    DEBUG_SAFE_TO_THROW_IN_THIS_BLOCK

#define BUGGY_THROWSCOMPLUSEXCEPTION()  \
    ThrowsCOMPlusExceptionWorker();                     \
    DEBUG_SAFE_TO_THROW_IN_THIS_BLOCK

#else

#define THROWSCOMPLUSEXCEPTION()
#define BUGGY_THROWSCOMPLUSEXCEPTION() 

#endif //_DEBUG

//================================================
// Declares a COM+ frame handler that can be used to make sure that
// exceptions that should be handled from within managed code 
// are handled within and don't leak out to give other handlers a 
// chance at them.
//=================================================== 
#define INSTALL_COMPLUS_EXCEPTION_HANDLER() INSTALL_COMPLUS_EXCEPTION_HANDLEREX(GetThread())
#define INSTALL_COMPLUS_EXCEPTION_HANDLEREX(pCurThread) \
  {                                            \
    Thread* ___pCurThread = (pCurThread);      \
    _ASSERTE(___pCurThread);                   \
    COMPLUS_TRY_DECLARE_EH_RECORD();           \
    InsertCOMPlusFrameHandler(___pExRecord);    

#define UNINSTALL_COMPLUS_EXCEPTION_HANDLER()  \
    RemoveCOMPlusFrameHandler(___pExRecord);    \
  }

#define INSTALL_NESTED_EXCEPTION_HANDLER(frame)                           \
   NestedHandlerExRecord *__pNestedHandlerExRecord = (NestedHandlerExRecord*) _alloca(sizeof(NestedHandlerExRecord));       \
   __pNestedHandlerExRecord->m_handlerInfo.m_pThrowable = NULL;               \
   __pNestedHandlerExRecord->Init(0, COMPlusNestedExceptionHandler, frame);   \
   InsertCOMPlusFrameHandler(__pNestedHandlerExRecord);

#define UNINSTALL_NESTED_EXCEPTION_HANDLER()    \
   RemoveCOMPlusFrameHandler(__pNestedHandlerExRecord);

class Frame;

struct FrameHandlerExRecord {
    _EXCEPTION_REGISTRATION_RECORD *m_pNext;
    LPVOID m_pvFrameHandler;
    Frame *m_pEntryFrame;
    Frame *GetCurrFrame() {
        return m_pEntryFrame;
    }
};

#ifdef _DEBUG
VOID ThrowsCOMPlusExceptionWorker();
#endif // _DEBUG
//==========================================================================
// Declares that a function cannot throw a COM+ exception.
// We add an exception handler to the stack and THROWSCOMPLUSEXCEPTION will
// search for it. If it finds this handler before one that can handle
// the exception, then it asserts.
//==========================================================================
#ifdef _DEBUG
EXCEPTION_DISPOSITION __cdecl COMPlusCannotThrowExceptionHandler(EXCEPTION_RECORD *pExceptionRecord,

                         _EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame,
                         CONTEXT *pContext,
                         void *DispatcherContext);

EXCEPTION_DISPOSITION __cdecl COMPlusCannotThrowExceptionMarker(EXCEPTION_RECORD *pExceptionRecord,

                         _EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame,
                         CONTEXT *pContext,
                         void *DispatcherContext);

class _DummyClass {
    FrameHandlerExRecord m_exRegRecord;
    public:
        _DummyClass() {
            m_exRegRecord.m_pvFrameHandler = COMPlusCannotThrowExceptionHandler;
            m_exRegRecord.m_pEntryFrame = 0;
            InsertCOMPlusFrameHandler(&m_exRegRecord);
        }
        ~_DummyClass()
        {
            RemoveCOMPlusFrameHandler(&m_exRegRecord);
        }
};

#define CANNOTTHROWCOMPLUSEXCEPTION() _DummyClass _dummyvariable

static int CheckException(int code) {
    if (code != STATUS_BREAKPOINT)
        _ASSERTE(!"Exception thrown past CANNOTTHROWCOMPLUSEXCEPTION boundary");
    return EXCEPTION_CONTINUE_SEARCH;
}

// use this version if you need to use COMPLUS_TRY or EE_TRY in your function
// as the above version uses C++ EH for the destructor of _DummyClass
#define BEGINCANNOTTHROWCOMPLUSEXCEPTION()                                      \
    {                                                                           \
        FrameHandlerExRecord m_exRegRecord;                                     \
        m_exRegRecord.m_pvFrameHandler = COMPlusCannotThrowExceptionMarker;     \
        m_exRegRecord.m_pEntryFrame = 0;                                        \
        __try {                                                                 \
            InsertCOMPlusFrameHandler(&m_exRegRecord);                           \
            __try {

                // ... <code> ...


#define ENDCANNOTTHROWCOMPLUSEXCEPTION()                                        \
            ;                                                                   \
            } __except(CheckException(GetExceptionCode())) {                    \
                ;                                                               \
            }                                                                   \
        } __finally {                                                           \
            RemoveCOMPlusFrameHandler(&m_exRegRecord);                          \
        }                                                                       \
    }

#else // !_DEBUG
#define CANNOTTHROWCOMPLUSEXCEPTION()
#define BEGINCANNOTTHROWCOMPLUSEXCEPTION()                                          
#define ENDCANNOTTHROWCOMPLUSEXCEPTION()                                      
#endif // _DEBUG

//======================================================
// Used when we're entering the EE from unmanaged code
// and we can assert that the gc state is cooperative.
//
// If an exception is thrown through this transition
// handler, it will clean up the EE appropriately.  See
// the definition of COMPlusCooperativeTransitionHandler
// for the details.
//======================================================
EXCEPTION_DISPOSITION __cdecl COMPlusCooperativeTransitionHandler(
        EXCEPTION_RECORD *pExceptionRecord,
        _EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame,
        CONTEXT *pContext,
        void *DispatcherContext);

#define COOPERATIVE_TRANSITION_BEGIN() COOPERATIVE_TRANSITION_BEGIN_EX(GetThread())

#define COOPERATIVE_TRANSITION_BEGIN_EX(pThread)                          \
  {                                                                       \
    _ASSERTE(GetThread() && GetThread()->PreemptiveGCDisabled() == TRUE); \
    DEBUG_ASSURE_NO_RETURN_IN_THIS_BLOCK                                  \
    Frame *__pFrame = pThread->m_pFrame;                                  \
    INSTALL_FRAME_HANDLING_FUNCTION(COMPlusCooperativeTransitionHandler, __pFrame)

#define COOPERATIVE_TRANSITION_END()                                      \
    UNINSTALL_FRAME_HANDLING_FUNCTION                                     \
  }

extern int UserBreakpointFilter(EXCEPTION_POINTERS *ep);

static int 
CatchIt(EXCEPTION_POINTERS *ep)
{
    PEXCEPTION_RECORD er = ep->ExceptionRecord;
    int code = er->ExceptionCode;
    if (code == STATUS_SINGLE_STEP || code == STATUS_BREAKPOINT)
        return UserBreakpointFilter(ep);
    else
        return EXCEPTION_EXECUTE_HANDLER;
}

#if ZAPMONITOR_ENABLED

#define COMPLUS_EXCEPTION_EXECUTE_HANDLER \
    (COMPlusIsMonitorException(GetExceptionInformation()) \
     ? EXCEPTION_CONTINUE_EXECUTION : CatchIt(GetExceptionInformation()))

#else

#define COMPLUS_EXCEPTION_EXECUTE_HANDLER CatchIt(GetExceptionInformation())

#endif


#endif // __exceptmacros_h__
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\fcall.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
// FCALL.CPP -
//
//

#include "common.h"
#include "vars.hpp"
#include "fcall.h"
#include "excep.h"
#include "frames.h"
#include "gms.h"
#include "ecall.h"
#include "eeconfig.h"


VOID __stdcall __FCThrow(LPVOID __me, RuntimeExceptionKind reKind, UINT resID, LPCWSTR arg1, LPCWSTR arg2, LPCWSTR arg3)
{
    THROWSCOMPLUSEXCEPTION();

#ifdef _DEBUG
        // calling the ENDFORBIDGC conditionally allows it to be called from 
        // generated stubs (like box) which
    if (GetThread()->GCForbidden())   
         ENDFORBIDGC();

    FCallCheck __fCallCheck;
#endif

    FC_GC_POLL_NOT_NEEDED();    // throws always open up for GC
    HELPER_METHOD_FRAME_BEGIN_ATTRIB_NOPOLL(Frame::FRAME_ATTR_CAPUTURE_DEPTH_2);
    // Now, we can construct & throw.

    if (reKind == kExecutionEngineException)
    {
    	FATAL_EE_ERROR();
    }
    
    if (resID == 0)
    {
        // If we have an string to add use NonLocalized otherwise just throw the exception.
        if (arg1)
            COMPlusThrowNonLocalized(reKind, arg1); //COMPlusThrow(reKind,arg1);
        else
            COMPlusThrow(reKind);
    }
    else 
        COMPlusThrow(reKind, resID, arg1, arg2, arg3);
    
    HELPER_METHOD_FRAME_END();
    _ASSERTE(!"Throw returned");
}

VOID __stdcall __FCThrowArgument(LPVOID __me, RuntimeExceptionKind reKind, LPCWSTR argName, LPCWSTR resourceName)
{
    THROWSCOMPLUSEXCEPTION();
	ENDFORBIDGC();
    INDEBUG(FCallCheck __fCallCheck);

    FC_GC_POLL_NOT_NEEDED();    // throws always open up for GC
    HELPER_METHOD_FRAME_BEGIN_ATTRIB_NOPOLL(Frame::FRAME_ATTR_CAPUTURE_DEPTH_2);
    switch (reKind) {
    case kArgumentNullException:
        if (resourceName) {
            COMPlusThrowArgumentNull(argName, resourceName);
        } else {
            COMPlusThrowArgumentNull(argName);
        }
        break;

    case kArgumentOutOfRangeException:
        COMPlusThrowArgumentOutOfRange(argName, resourceName);
        break;

    case kArgumentException:
        COMPlusThrowArgumentException(argName, resourceName);
        break;

    default:
        // If you see this assert, add a case for your exception kind above.
        _ASSERTE(argName == NULL);
        COMPlusThrow(reKind, resourceName);
    }        
        
    HELPER_METHOD_FRAME_END();
    _ASSERTE(!"Throw returned");
}

/**************************************************************************************/
/* erect a frame in the FCALL and then poll the GC, objToProtect will be protected
   during the poll and the updated object returned.  */

Object* FC_GCPoll(void* __me, Object* objToProtect) {
    ENDFORBIDGC();
    INDEBUG(FCallCheck __fCallCheck);

    HELPER_METHOD_FRAME_BEGIN_RET_ATTRIB_1(Frame::FRAME_ATTR_CAPUTURE_DEPTH_2, objToProtect);

#ifdef _DEBUG
    BOOL GCOnTransition = FALSE;
    if (g_pConfig->FastGCStressLevel()) {
        GCOnTransition = GC_ON_TRANSITIONS (FALSE);
    }
#endif
    CommonTripThread();					
#ifdef _DEBUG
    if (g_pConfig->FastGCStressLevel()) {
        GC_ON_TRANSITIONS (GCOnTransition);
    }
#endif

    HELPER_METHOD_FRAME_END();
    BEGINFORBIDGC();
	return(OBJECTREFToObject(objToProtect));
}

#ifdef _DEBUG

/**************************************************************************************/
#ifdef _X86_
static __int64 getCycleCount() {
    if ((GetSpecificCpuType() & 0x0000FFFF) > 4) 
        return getPentiumCycleCount();
    else    
        return(0);
}
#else 
static __int64 getCycleCount() { return(0); }
#endif

/**************************************************************************************/
ForbidGC::ForbidGC() { 
    GetThread()->BeginForbidGC(); 
}

/**************************************************************************************/
ForbidGC::~ForbidGC() { 
       // IF EH happens, this is still called, in which case
       // we should not bother 
    Thread* pThread = GetThread();
    if (pThread->GCForbidden())
        pThread->EndForbidGC(); 
}

/**************************************************************************************/
FCallCheck::FCallCheck() {
	didGCPoll = false;
    notNeeded = false;
	startTicks = getCycleCount();
}

unsigned FcallTimeHist[11];

/**************************************************************************************/
FCallCheck::~FCallCheck() {

		// Confirm that we don't starve the GC or thread-abort.
        // Basically every control flow path through an FCALL must
        // to a poll.   If you hit the assert below, you can fix it by
        //
        // If you erect a HELPER_METHOD_FRAME, you can
        //
        //      Call    HELPER_METHOD_POLL()
        //      or use  HELPER_METHOD_FRAME_END_POLL
        //
        // If you don't have a helper frame you can used
        //
        //      FC_GC_POLL_AND_RETURN_OBJREF        or
        //      FC_GC_POLL                          or
        //      FC_GC_POLL_RET              
        //
        // Note that these must be at GC safe points.  In particular
        // all object references that are NOT protected will be trashed. 
    
    
        // There is a special poll called FC_GC_POLL_NOT_NEEDED
        // which says the code path is short enough that a GC poll is not need
        // you should not use this in most cases.  

    if (notNeeded) {

        /***    TODO, we want to actually measure the time to make certain we are not too far off

		unsigned delta  = unsigned(getCycleCount() - startTicks);
        ***/
    }
    else if (!didGCPoll) {
        // TODO turn this on!!! _ASSERTE(!"FCALL without a GC poll in it somewhere!");
    }

}

#endif // _DEBUG
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\extensibleclassfactory.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*============================================================
**
** Header: ExtensibleClassFactory.h
**
** Author: Rudi Martin (rudim)
**
** Purpose: Native methods on System.Runtime.InteropServices.ExtensibleClassFactory
**
** Date:  May 27, 1999
** 
===========================================================*/

#ifndef _EXTENSIBLECLASSFACTORY_H
#define _EXTENSIBLECLASSFACTORY_H


// Each function that we call through native only gets one argument,
// which is actually a pointer to it's stack of arguments. Our structs
// for accessing these are defined below.

struct RegisterObjectCreationCallbackArgs
{
    DECLARE_ECALL_OBJECTREF_ARG(OBJECTREF, m_pDelegate);
};


// Register a delegate that will be called whenever an instance of a
// managed type that extends from an unmanaged type needs to allocate
// the aggregated unmanaged object. This delegate is expected to
// allocate and aggregate the unmanaged object and is called in place
// of a CoCreateInstance. This routine must be called in the context
// of the static initializer for the class for which the callbacks
// will be made.
// It is not legal to register this callback from a class that has any
// parents that have already registered a callback.
void __stdcall RegisterObjectCreationCallback(RegisterObjectCreationCallbackArgs *pArgs);


#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\fcall.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
// FCall.H -
//
//
// FCall is a high-performance alternative to ECall. Unlike ECall, FCall
// methods do not necessarily create a frame.   Jitted code calls directly
// to the FCall entry point.   It is possible to do operations that need
// to have a frame within an FCall, you need to manually set up the frame
// before you do such operations.

// It is illegal to cause a GC or EH to happen in an FCALL before setting
// up a frame.  To prevent accidentally violating this rule, FCALLs turn
// on BEGINGCFORBID, which insures that these things can't happen in a 
// checked build without causing an ASSERTE.  Once you set up a frame,
// this state is turned off as long as the frame is active, and then is
// turned on again when the frame is torn down.   This mechanism should
// be sufficient to insure that the rules are followed.

// In general you set up a frame by using the following macros

//		HELPER_METHOD_FRAME_BEGIN_RET*()  	// Use If the FCALL has a return value
//		HELPER_METHOD_FRAME_BEGIN*()  		// Use If FCALL does not return a value
//		HELPER_METHOD_FRAME_END*()  			

// These macros introduce a scope in which is protected by an HelperMethodFrame.
// in this scope you can do EH or GC.   There are rules associated with 
// their use.  In particular

//		1) These macros can only be used in the body of a FCALL (that is
//		   something using the FCIMPL* or HCIMPL* macros for their decaration.

//		2) You may not perform a 'return' with this scope..

// Compile time errors occur if you try to violate either of these rules.

// The frame that is set up does NOT protect any GC variables (in particular the
// arguments of the FCALL.  THus you need to do an explicit GCPROTECT once the
// frame is established is you need to protect an argument.  There are flavors
// of HELPER_METHOD_FRAME that protect a certain number of GC variable.  For
// example

//		HELPER_METHOD_FRAME_BEGIN_RET_2(arg1, arg2)

// will protect the GC variables arg1, and arg2 as well as erect the frame.  

// Another invariant that you must be aware of is the need to poll to see if
// a GC is needed by some other thread.   Unless the FCALL is VERY short, 
// every code path through the FCALL must do such a poll.  The important 
// thing here is that a poll will cause a GC, and thus you can only do it
// when all you GC variables are protected.   To make things easier 
// HELPER_METHOD_FRAMES that protect things automatically do this poll.
// If you don't need to protect anything HELPER_METHOD_FRAME_BEGIN_0
// will also do the poll. 

// Sometimes it is convinient to do the poll a the end of the frame, you 
// can used HELPER_METHOD_FRAME_BEGIN_NOPOLL and HELPER_METHOD_FRAME_END_POLL
// to do the poll at the end.   If somewhere in the middle is the best
// place you can do that to with HELPER_METHOD_POLL()

// You don't need to erect a helper method frame to do a poll.  FC_GC_POLL
// can do this (remember all your GC refs will be trashed).  

// Finally if your method is VERY small, you can get away without a poll,
// you have to use FC_GC_POLL_NOT_NEEDED can be used to mark this.  
// Use sparingly!

// It is possible to set up the frame as the first operation in the FCALL and
// tear it down as the last operation before returning.  This works and is 
// reasonably efficient (as good as an ECall), however, if it is the case that
// you can defer the setup of the frame to an unlikely code path (exception path)
// that is much better.   

// TODO: we should have a way of doing a trial allocation (an allocation that
// will fail if it would cause a GC).  That way even FCALLs that need to allocate
// would not necessarily need to set up a frame.  

// It is common to only need to set up a frame in order to throw an exception.
// While this can be done by doing 

//		HELPER_METHOD_FRAME_BEGIN()  		// Use If FCALL does not return a value
//		COMPlusThrow(execpt);
//		HELPER_METHOD_FRAME_END()  			

// It is more efficient (in space) to use conviniece macro FCTHROW that does 
// this for you (sets up a frame, and does the throw).

//		FCTHROW(except)

// Since FCALLS have to conform to the EE calling conventions and not to C
// calling conventions, FCALLS, need to be declared using special macros (FCIMPL*) 
// that implement the correct calling conventions.  There are variants of these
// macros depending on the number of args, and sometimes the types of the 
// arguments. 

//------------------------------------------------------------------------
//    A very simple example:
//
//      FCIMPL2(INT32, Div, INT32 x, INT32 y)
//          if (y == 0) 
//              FCThrow(kDivideByZeroException);
//          return x/y;
//      FCIMPLEND
//
//
// *** WATCH OUT FOR THESE GOTCHAS: ***
// ------------------------------------
//  - In your FCDECL & FCIMPL protos, don't declare a param as type OBJECTREF
//    or any of its deriveds. This will break on the checked build because
//    __fastcall doesn't enregister C++ objects (which OBJECTREF is).
//    Instead, you need to do something like;
//
//      FCIMPL(.., .., Object* pObject0)
//          OBJECTREF pObject = ObjectToOBJECTREF(pObject0);
//      FCIMPL
//
//    For similar reasons, use Object* rather than OBJECTREF as a return type.  
//    Consider either using ObjectToOBJECTREF or calling VALIDATEOBJECTREF
//    to make sure your Object* is valid.
//
//  - FCThrow() must be called directly from your FCall impl function: it
//    cannot be called from a subfunction. Calling from a subfunction breaks
//    the VC code parsing hack that lets us recover the callee saved registers.
//    Fortunately, you'll get a compile error complaining about an
//    unknown variable "__me".
//
//  - If your FCall returns VOID, you must use FCThrowVoid() rather than
//    FCThrow(). This is because FCThrow() has to generate an unexecuted
//    "return" statement for the code parser's evil purposes.
//
//  - Unlike ECall, the parameters aren't gc-promoted. Of course, if you
//    cause a GC in an FCall, you're already breaking the rules.
//
//  - On 32 bit machines, the first two arguments MUST NOT be 64 bit values
//    or larger, because they are enregistered.  If you mess up here, you'll
//    likely corrupt the stack then corrupt the heap so quickly you won't 
//    be able to debug it easily.
//

// How FCall works:
// ----------------
//   An FCall target uses __fastcall or some other calling convention to
//   match the IL calling convention exactly. Thus, a call to FCall is a direct
//   call to the target w/ no intervening stub or frame.
//
//   The tricky part is when FCThrow is called. FCThrow must generate
//   a proper method frame before allocating and throwing the exception.
//   To do this, it must recover several things:
//
//      - The location of the FCIMPL's return address (since that's
//        where the frame will be based.)
//
//      - The on-entry values of the callee-saved regs; which must
//        be recorded in the frame so that GC can update them.
//        Depending on how VC compiles your FCIMPL, those values are still
//        in the original registers or saved on the stack.
//
//        To figure out which, FCThrow() generates the code:
//
//              __FCThrow(__me, ...);
//              return 0;
//
//        The "return" statement will never execute; but its presence guarantees
//        that VC will follow the __FCThrow() call with a VC epilog
//        that restores the callee-saved registers using a pretty small
//        and predictable set of Intel opcodes. __FCThrow() parses this
//        epilog and simulates its execution to recover the callee saved
//        registers.
//
//      - The MethodDesc* that this FCall implements. This MethodDesc*
//        is part of the frame and ensures that the FCall will appear
//        in the exception's stack trace. To get this, FCDECL declares
//        a static local __me, initialized to point to the FC target itself.
//        This address is exactly what's stored in the ECall lookup tables;
//        so __FCThrow() simply does a reverse lookup on that table to recover
//        the MethodDesc*.
//
//   After scarfing all this data, __FCThrow bashes an FCallMethodFrame
//   right onto the stack where the FCall target originally entered.
//   Then it passes control to COMPlusThrow.
//

#ifndef __FCall_h__
#define __FCall_h__

void* FindImplForMethod(MethodDesc* pMD);
MethodDesc *MapTargetBackToMethod(const void* pTarg);

DWORD GetIDForMethod(MethodDesc *pMD);
void *FindImplForID(DWORD ID);

#include "gms.h"
#if defined(_X86_) || defined(_ALPHA_) || defined(_IA64_)

//==============================================================================================
// This is where FCThrow ultimately ends up. Never call this directly.
// Use the FCThrow() macros. __FCThrowArgument is the helper to throw ArgumentExceptions
// with a resource taken from the managed resource manager.
//==============================================================================================
VOID __stdcall __FCThrow(LPVOID me, enum RuntimeExceptionKind reKind, UINT resID, LPCWSTR arg1, LPCWSTR arg2, LPCWSTR arg3);
VOID __stdcall __FCThrowArgument(LPVOID me, enum RuntimeExceptionKind reKind, LPCWSTR argumentName, LPCWSTR resourceName);

//==============================================================================================
// FDECLn: A set of macros for generating header declarations for FC targets.
// Use FIMPLn for the actual body.
//==============================================================================================

// Note: on the x86, these defs reverse all but the first two arguments
// (IL stack calling convention is reversed from __fastcall.)

#define FCDECL0(rettype, funcname) rettype __fastcall funcname()
#define FCDECL1(rettype, funcname, a1) rettype __fastcall funcname(a1)
#define FCDECL2(rettype, funcname, a1, a2) rettype __fastcall funcname(a1, a2)
#define FCDECL2_RR(rettype, funcname, a1, a2) rettype __fastcall funcname(a2, a1)
#define FCDECL3(rettype, funcname, a1, a2, a3) rettype __fastcall funcname(a1, a2, a3)
#define FCDECL3_IRR(rettype, funcname, a1, a2, a3) rettype __fastcall funcname(a1, a3, a2)
#define FCDECL4(rettype, funcname, a1, a2, a3, a4) rettype __fastcall funcname(a1, a2, a4, a3)
#define FCDECL5(rettype, funcname, a1, a2, a3, a4, a5) rettype __fastcall funcname(a1, a2, a5, a4, a3)
#define FCDECL6(rettype, funcname, a1, a2, a3, a4, a5, a6) rettype __fastcall funcname(a1, a2, a6, a5, a4, a3)
#define FCDECL7(rettype, funcname, a1, a2, a3, a4, a5, a6, a7) rettype __fastcall funcname(a1, a2, a7, a6, a5, a4, a3)
#define FCDECL8(rettype, funcname, a1, a2, a3, a4, a5, a6, a7, a8) rettype __fastcall funcname(a1, a2, a8, a7, a6, a5, a4, a3)

#ifdef _DEBUG

// Code to generate a compile-time error if return statements appear where they
// shouldn't.
//
// Here's the way it works...
//
// We create two classes with a safe_to_return() method.  The method is static,
// returns void, and does nothing.  One class has the method as public, the other
// as private.  We introduce a global scope typedef for __ReturnOK that refers to
// the class with the public method.  So, by default, the expression
//
//      __ReturnOK::safe_to_return()
//
// quietly compiles and does nothing.  When we enter a block in which we want to
// inhibit returns, we introduce a new typedef that defines __ReturnOK as the
// class with the private method.  Inside this scope,
//
//      __ReturnOK::safe_to_return()
//
// generates a compile-time error.
//
// To cause the method to be called, we have to #define the return keyword.
// The simplest working version would be
//
//   #define return if (0) __ReturnOK::safe_to_return(); else return
//
// but we've used
//
//   #define return for (;1;__ReturnOK::safe_to_return()) return
//
// because it happens to generate somewhat faster code in a checked build.  (They
// both introduce no overhead in a fastchecked build.)
//
class __SafeToReturn {
public:
    static int safe_to_return() {return 0;};
};

class __YouCannotUseAReturnStatementHere {
private:
    // If you got here, and you're wondering what you did wrong -- you're using
    // a return statement where it's not allowed.  Likely, it's inside one of:
    //     GCPROTECT_BEGIN ... GCPROTECT_END
    //     HELPER_METHOD_FRAME_BEGIN ... HELPER_METHOD_FRAME_END
    //
    static int safe_to_return() {return 0;};
};

typedef __SafeToReturn __ReturnOK;


// Unfortunately, the only way to make this work is to #define all return statements --
// even the ones at global scope.  This actually generates better code that appears.
// The call is dead, and does not appear in the generated code, even in a checked
// build.  (And, in fastchecked, there is no penalty at all.)
//
#define return if (0 && __ReturnOK::safe_to_return()) { } else return

#define DEBUG_ASSURE_NO_RETURN_BEGIN { typedef __YouCannotUseAReturnStatementHere __ReturnOK; 
#define DEBUG_ASSURE_NO_RETURN_END   }
#define DEBUG_ASSURE_NO_RETURN_IN_THIS_BLOCK typedef __YouCannotUseAReturnStatementHere __ReturnOK;
#else

#define DEBUG_ASSURE_NO_RETURN_IN_THIS_BLOCK
#define DEBUG_ASSURE_NO_RETURN_BEGIN
#define DEBUG_ASSURE_NO_RETURN_END
#endif

	// used to define the two macros below
#define HELPER_METHOD_FRAME_BEGIN_EX(capture, helperFrame, gcpoll)  int alwaysZero;     \
                                     do {                                               \
                                     LazyMachState __ms;                                \
                                     capture;					        				\
                                     helperFrame; 		       			        		\
                                     gcpoll;                                            \
                                     DEBUG_ASSURE_NO_RETURN_BEGIN                       \
                                     ENDFORBIDGC(); 

    // Use this one if you return void
#define HELPER_METHOD_FRAME_BEGIN_ATTRIB_NOPOLL(attribs)                                \
            HELPER_METHOD_FRAME_BEGIN_EX(	                                            \
				CAPTURE_STATE(__ms),											        \
				HelperMethodFrame __helperframe(__me, &__ms, attribs),                  \
                {})

#define HELPER_METHOD_FRAME_BEGIN_ATTRIB_1(attribs, arg1) 								        \
			HELPER_METHOD_FRAME_BEGIN_EX(									        			\
				CAPTURE_STATE(__ms),												            \
				HelperMethodFrame_1OBJ __helperframe(__me, &__ms, attribs, (OBJECTREF*) &arg1), \
                HELPER_METHOD_POLL())

#define HELPER_METHOD_FRAME_BEGIN_NOPOLL()  HELPER_METHOD_FRAME_BEGIN_ATTRIB_NOPOLL(Frame::FRAME_ATTR_NONE)

#define HELPER_METHOD_FRAME_BEGIN_0()                                                   \
            HELPER_METHOD_FRAME_BEGIN_EX(	                                            \
				CAPTURE_STATE(__ms),											        \
				HelperMethodFrame __helperframe(__me, &__ms, Frame::FRAME_ATTR_NONE),   \
                HELPER_METHOD_POLL())

#define HELPER_METHOD_FRAME_BEGIN_1(arg1)  HELPER_METHOD_FRAME_BEGIN_ATTRIB_1(Frame::FRAME_ATTR_NONE, arg1)

#define HELPER_METHOD_FRAME_BEGIN_2(arg1, arg2) 									                       \
			HELPER_METHOD_FRAME_BEGIN_EX(												                   \
				CAPTURE_STATE(__ms),												                       \
				HelperMethodFrame_2OBJ __helperframe(__me, &__ms, (OBJECTREF*) &arg1, (OBJECTREF*) &arg2), \
                HELPER_METHOD_POLL())

#define HELPER_METHOD_FRAME_BEGIN_3(arg1, arg2, arg3) 								    \
			HELPER_METHOD_FRAME_BEGIN_EX(												\
				CAPTURE_STATE(__ms),												    \
				HelperMethodFrame_3OBJ __helperframe(__me, &__ms, 						\
					(OBJECTREF*) &arg1, (OBJECTREF*) &arg2, (OBJECTREF*) &arg3),        \
                HELPER_METHOD_POLL())

#define HELPER_METHOD_FRAME_BEGIN_4(arg1, arg2, arg3, arg4) 						    \
			HELPER_METHOD_FRAME_BEGIN_EX(												\
				CAPTURE_STATE(__ms),												    \
				HelperMethodFrame_4OBJ __helperframe(__me, &__ms, 						\
					(OBJECTREF*) &arg1, (OBJECTREF*) &arg2, (OBJECTREF*) &arg3, (OBJECTREF*) &arg4), \
                HELPER_METHOD_POLL())

    // Use this one if you return a value
#define HELPER_METHOD_FRAME_BEGIN_RET_ATTRIB_NOPOLL(attribs)                            \
            HELPER_METHOD_FRAME_BEGIN_EX(			                                    \
				CAPTURE_STATE_RET(__ms),					                            \
				HelperMethodFrame __helperframe(__me, &__ms, attribs),                  \
                {})

#define HELPER_METHOD_FRAME_BEGIN_RET_ATTRIB_1(attribs, arg1) 							        \
			HELPER_METHOD_FRAME_BEGIN_EX(												        \
				CAPTURE_STATE_RET(__ms),												        \
				HelperMethodFrame_1OBJ __helperframe(__me, &__ms, attribs, (OBJECTREF*) &arg1), \
                HELPER_METHOD_POLL())

#define HELPER_METHOD_FRAME_BEGIN_RET_ATTRIB_2(attribs, arg1, arg2) 					\
			HELPER_METHOD_FRAME_BEGIN_EX(												\
				CAPTURE_STATE_RET(__ms),												\
				HelperMethodFrame_2OBJ __helperframe(__me, &__ms, attribs, (OBJECTREF*) &arg1, (OBJECTREF*) &arg2), \
                HELPER_METHOD_POLL())

#define HELPER_METHOD_FRAME_BEGIN_RET_NOPOLL()                                          \
            HELPER_METHOD_FRAME_BEGIN_RET_ATTRIB_NOPOLL(Frame::FRAME_ATTR_NONE)

#define HELPER_METHOD_FRAME_BEGIN_RET_0()                                                   \
            HELPER_METHOD_FRAME_BEGIN_EX(	                                            \
				CAPTURE_STATE_RET(__ms),											    \
				HelperMethodFrame __helperframe(__me, &__ms, Frame::FRAME_ATTR_NONE),   \
                HELPER_METHOD_POLL())

#define HELPER_METHOD_FRAME_BEGIN_RET_1(arg1)                                           \
            HELPER_METHOD_FRAME_BEGIN_RET_ATTRIB_1(Frame::FRAME_ATTR_NONE, arg1)

#define HELPER_METHOD_FRAME_BEGIN_RET_2(arg1, arg2)                                     \
            HELPER_METHOD_FRAME_BEGIN_RET_ATTRIB_2(Frame::FRAME_ATTR_NONE, arg1, arg2)

#define HELPER_METHOD_FRAME_BEGIN_RET_3(arg1, arg2, arg3) 								\
			HELPER_METHOD_FRAME_BEGIN_EX(												\
				CAPTURE_STATE_RET(__ms),												\
				HelperMethodFrame_2OBJ __helperframe(__me, &__ms, 						\
					(OBJECTREF*) &arg1, (OBJECTREF*) &arg2, (OBJECTREF*) &arg3),        \
                HELPER_METHOD_POLL())

#define HELPER_METHOD_FRAME_BEGIN_RET_4(arg1, arg2, arg3, arg4) 						\
			HELPER_METHOD_FRAME_BEGIN_EX(												\
				CAPTURE_STATE_RET(__ms),												\
				HelperMethodFrame_2OBJ __helperframe(__me, &__ms, 						\
					(OBJECTREF*) &arg1, (OBJECTREF*) &arg2, (OBJECTREF*) &arg3, (OBJECTREF*) &arg4), \
                HELPER_METHOD_POLL())

    // The while(__helperframe.RestoreState() needs a bit of explanation.
    // The issue is insuring that the same machine state (which registers saved)
    // exists when the machine state is probed (when the frame is created, and
    // when it is actually used (when the frame is popped.  We do this by creating
    // a flow of control from use to def.  Note that 'RestoreState' always returns false
    // we never actually loop, but the compiler does not know that, and thus
    // will be forced to make the keep the state of register spills the same at
    // the two locations.
#define HELPER_METHOD_FRAME_END_EX(gcpoll)              \
            DEBUG_ASSURE_NO_RETURN_END                  \
            gcpoll;                                     \
            BEGINFORBIDGC(); 							\
            __helperframe.Pop();                        \
            alwaysZero = __helperframe.RestoreState();  \
            } while(alwaysZero)

#define HELPER_METHOD_FRAME_END()        HELPER_METHOD_FRAME_END_EX({})  
#define HELPER_METHOD_FRAME_END_POLL()   HELPER_METHOD_FRAME_END_EX(HELPER_METHOD_POLL())  

    // This is the fastest way to do a GC poll if you have already erected a HelperMethodFrame
// TODO TURN THIS ON!!!  vancem 
// #define HELPER_METHOD_POLL()             { __helperframe.Poll(); INDEBUG(__fCallCheck.SetDidPoll()); }

#define HELPER_METHOD_POLL()             { }

    // Very short routines, or routines that are guarenteed to force GC or EH 
    // don't need to poll the GC.  USE VERY SPARINGLY!!!
#define FC_GC_POLL_NOT_NEEDED()    INDEBUG(__fCallCheck.SetNotNeeded()) 

Object* FC_GCPoll(void* me, Object* objToProtect = NULL);

#define FC_GC_POLL_EX(pThread, ret)               {		\
    INDEBUG(Thread::TriggersGC(pThread);)               \
	INDEBUG(__fCallCheck.SetDidPoll();)					\
    if (pThread->CatchAtSafePoint())    				\
		if (FC_GCPoll(__me))							\
			return ret;									\
	}

#define FC_GC_POLL()        FC_GC_POLL_EX(GetThread(), ;)
#define FC_GC_POLL_RET()    FC_GC_POLL_EX(GetThread(), 0)

#define FC_GC_POLL_AND_RETURN_OBJREF(obj)   { 			\
	INDEBUG(__fCallCheck.SetDidPoll();)					\
	Object* __temp = obj;	                    		\
    INDEBUG(Thread::ObjectRefProtected((OBJECTREF*) &__temp);) \
    if (GetThread()->CatchAtSafePoint())    			\
		return(FC_GCPoll(__me, __temp));				\
	return(__temp);					                    \
	}

#if defined(_DEBUG)
	// turns on forbidGC for the lifetime of the instance
class ForbidGC {
public:
    ForbidGC();
    ~ForbidGC();
};

	// this little helper class checks to make certain
	// 1) ForbidGC is set throughout the routine.
	// 2) Sometime during the routine, a GC poll is done

class FCallCheck : public ForbidGC {
public:
    FCallCheck();
    ~FCallCheck();
	void SetDidPoll() 		{ didGCPoll = true; }
	void SetNotNeeded() 	{ notNeeded = true; }

private:
	bool 		  didGCPoll;			// GC poll was done
    bool          notNeeded;            // GC poll not needed
	unsigned __int64 startTicks;		// tick count at begining of FCall
};

#define FC_TRIGGERS_GC(curThread) Thread::TriggersGC(curThread)

		// FC_COMMMON_PROLOG is used for both FCalls and HCalls
#define FC_COMMON_PROLOG 				    	\
		Thread::ObjectRefFlush(GetThread());    \
		FCallCheck __fCallCheck; 				\

		// FCIMPL_ASSERT is only for FCALL
void FCallAssert(void*& cache, void* target);		
#define FCIMPL_ASSERT 							\
		static void* __cache = 0;				\
		FCallAssert(__cache, __me);				

		// HCIMPL_ASSERT is only for JIT helper calls
void HCallAssert(void*& cache, void* target);
#define HCIMPL_ASSERT(target)					\
		static void* __cache = 0;				\
		HCallAssert(__cache, target);		

#else
#define FC_COMMON_PROLOG 	
#define FC_TRIGGERS_GC(curThread) 
#define FC_COMMMON_PROLOG 
#define FCIMPL_ASSERT
#define HCIMPL_ASSERT(target)
#endif // _DEBUG


//==============================================================================================
// FIMPLn: A set of macros for generating the proto for the actual
// implementation (use FDECLN for header protos.)
//
// The hidden "__me" variable lets us recover the original MethodDesc*
// so any thrown exceptions will have the correct stack trace. FCThrow()
// passes this along to __FCThrowInternal(). 
//==============================================================================================
#define FCIMPL_PROLOG(funcname)  LPVOID __me = (LPVOID)funcname; FCIMPL_ASSERT FC_COMMON_PROLOG

#define FCIMPL0(rettype, funcname) rettype __fastcall funcname() { FCIMPL_PROLOG(funcname)
#define FCIMPL1(rettype, funcname, a1) rettype __fastcall funcname(a1) {  FCIMPL_PROLOG(funcname)
#define FCIMPL2(rettype, funcname, a1, a2) rettype __fastcall funcname(a1, a2) {  FCIMPL_PROLOG(funcname)
#define FCIMPL2_RR(rettype, funcname, a1, a2) rettype __fastcall funcname(a2, a1) {  FCIMPL_PROLOG(funcname)
#define FCIMPL3(rettype, funcname, a1, a2, a3) rettype __fastcall funcname(a1, a2, a3) {  FCIMPL_PROLOG(funcname)
#define FCIMPL3_IRR(rettype, funcname, a1, a2, a3) rettype __fastcall funcname(a1, a3, a2) {  FCIMPL_PROLOG(funcname)
#define FCIMPL4(rettype, funcname, a1, a2, a3, a4) rettype __fastcall funcname(a1, a2, a4, a3) {  FCIMPL_PROLOG(funcname)
#define FCIMPL5(rettype, funcname, a1, a2, a3, a4, a5) rettype __fastcall funcname(a1, a2, a5, a4, a3) {  FCIMPL_PROLOG(funcname)
#define FCIMPL6(rettype, funcname, a1, a2, a3, a4, a5, a6) rettype __fastcall funcname(a1, a2, a6, a5, a4, a3) {  FCIMPL_PROLOG(funcname)
#define FCIMPL7(rettype, funcname, a1, a2, a3, a4, a5, a6, a7) rettype __fastcall funcname(a1, a2, a7, a6, a5, a4, a3) {  FCIMPL_PROLOG(funcname)
#define FCIMPL8(rettype, funcname, a1, a2, a3, a4, a5, a6, a7, a8) rettype __fastcall funcname(a1, a2, a8, a7, a6, a5, a4, a3) {  FCIMPL_PROLOG(funcname)

//==============================================================================================
// Use this to terminte an FCIMPLEND.
//==============================================================================================
#define FCIMPLEND   }

#define HCIMPL_PROLOG(funcname) LPVOID __me = 0; HCIMPL_ASSERT(funcname) FC_COMMON_PROLOG

	// HCIMPL macros are just like their FCIMPL counterparts, however
	// they do not remember the function they come from. Thus they will not
	// show up in a stack trace.  This is what you want for JIT helpers and the like
#define HCIMPL0(rettype, funcname) rettype __fastcall funcname() { HCIMPL_PROLOG(funcname) 
#define HCIMPL1(rettype, funcname, a1) rettype __fastcall funcname(a1) { HCIMPL_PROLOG(funcname)
#define HCIMPL2(rettype, funcname, a1, a2) rettype __fastcall funcname(a1, a2) { HCIMPL_PROLOG(funcname)
#define HCIMPL3(rettype, funcname, a1, a2, a3) rettype __fastcall funcname(a1, a2, a3) { HCIMPL_PROLOG(funcname)
#define HCIMPL4(rettype, funcname, a1, a2, a3, a4) rettype __fastcall funcname(a1, a2, a4, a3) { HCIMPL_PROLOG(funcname)
#define HCIMPL5(rettype, funcname, a1, a2, a3, a4, a5) rettype __fastcall funcname(a1, a2, a5, a4, a3) { HCIMPL_PROLOG(funcname)
#define HCIMPL6(rettype, funcname, a1, a2, a3, a4, a5, a6) rettype __fastcall funcname(a1, a2, a6, a5, a4, a3) { HCIMPL_PROLOG(funcname)
#define HCIMPLEND   }


//==============================================================================================
// Throws an exception from an FCall. See rexcep.h for a list of valid
// exception codes.
//==============================================================================================
#define FCThrow(reKind) FCThrowEx(reKind, 0, 0, 0, 0)

//==============================================================================================
// This version lets you attach a message with inserts (similar to
// COMPlusThrow()).
//==============================================================================================
#define FCThrowEx(reKind, resID, arg1, arg2, arg3) \
    {                                                \
        __FCThrow(__me, reKind, resID, arg1, arg2, arg3);     \
        return 0;                                         \
    }

//==============================================================================================
// Like FCThrow but can be used for a VOID-returning FCall. The only
// difference is in the "return" statement.
//==============================================================================================
#define FCThrowVoid(reKind) FCThrowExVoid(reKind, 0, 0, 0, 0)

//==============================================================================================
// This version lets you attach a message with inserts (similar to
// COMPlusThrow()).
//==============================================================================================
#define FCThrowExVoid(reKind, resID, arg1, arg2, arg3) \
    {                                                \
        __FCThrow(__me, reKind, resID, arg1, arg2, arg3);     \
        return;                                         \
    }

// Use FCThrowRes to throw an exception with a localized error message from the
// ResourceManager in managed code.
#define FCThrowRes(reKind, resourceName) FCThrowArgumentEx(reKind, NULL, resourceName)
#define FCThrowArgumentNull(argName) FCThrowArgumentEx(kArgumentNullException, argName, NULL)
#define FCThrowArgumentOutOfRange(argName, message) FCThrowArgumentEx(kArgumentOutOfRangeException, argName, message)
#define FCThrowArgument(argName, message) FCThrowArgumentEx(kArgumentException, argName, message)

#define FCThrowArgumentEx(reKind, argName, resourceName)        \
    {                                                       \
        __FCThrowArgument(__me, reKind, argName, resourceName); \
        return 0;                                              \
    }

// Use FCThrowRes to throw an exception with a localized error message from the
// ResourceManager in managed code.
#define FCThrowResVoid(reKind, resourceName) FCThrowArgumentVoidEx(reKind, NULL, resourceName)
#define FCThrowArgumentNullVoid(argName) FCThrowArgumentVoidEx(kArgumentNullException, argName, NULL)
#define FCThrowArgumentOutOfRangeVoid(argName, message) FCThrowArgumentVoidEx(kArgumentOutOfRangeException, argName, message)
#define FCThrowArgumentVoid(argName, message) FCThrowArgumentVoidEx(kArgumentException, argName, message)

#define FCThrowArgumentVoidEx(reKind, argName, resourceName)    \
    {                                                       \
        __FCThrowArgument(__me, reKind, argName, resourceName); \
        return;                                                \
    }

#endif //_X86_ || _ALPHA_ || _IA64_
#endif //__FCall_h__
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\field.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
// ===========================================================================
// File: Field.cpp
//
// ===========================================================================
// This file contains the implementation for FieldDesc methods.
// ===========================================================================
//

#include "common.h"

#include "EnCEE.h"
#include "field.h"
#include "remoting.h"

// Return whether the field is a GC ref type
BOOL FieldDesc::IsObjRef()
{
    return CorTypeInfo::IsObjRef(GetFieldType());
}

// Return the type of the field, as a class.
TypeHandle FieldDesc::LoadType()
{
    PCCOR_SIGNATURE pSig;
    DWORD           cSig;

    GetSig(&pSig, &cSig);

    FieldSig        sig(pSig, GetModule());

    return sig.GetTypeHandle();
}

// Return the type of the field, as a class, but only if it's loaded.
TypeHandle FieldDesc::FindType()
{
    // Caller should have handled all the non-class cases, already.
    _ASSERTE(GetFieldType() == ELEMENT_TYPE_CLASS ||
             GetFieldType() == ELEMENT_TYPE_VALUETYPE);

    PCCOR_SIGNATURE pSig;
    DWORD           cSig;

    GetSig(&pSig, &cSig);

    FieldSig        sig(pSig, GetModule());

    // This may be the real type which includes other things
    //  beside class and value class such ass arrays
    _ASSERTE(sig.GetFieldType() == ELEMENT_TYPE_CLASS ||
             sig.GetFieldType() == ELEMENT_TYPE_VALUETYPE ||
             sig.GetFieldType() == ELEMENT_TYPE_STRING ||
             sig.GetFieldType() == ELEMENT_TYPE_VALUEARRAY ||
             sig.GetFieldType() == ELEMENT_TYPE_SZARRAY
             );

    return sig.GetTypeHandle(NULL, TRUE, TRUE);
}


void* FieldDesc::GetStaticAddress(void *base)
{

    void* ret = GetStaticAddressHandle(base);       // Get the handle

        // For value classes, the handle points at an OBJECTREF
        // which holds the boxed value class, so derefernce and unbox.  
    if (GetFieldType() == ELEMENT_TYPE_VALUETYPE && !IsRVA())
    {
        OBJECTREF obj = ObjectToOBJECTREF(*(Object**) ret);
        ret = obj->UnBox();
    }
    return ret;
}

    // static value classes are actually stored in their boxed form.  
    // this means that their address moves.  
void* FieldDesc::GetStaticAddressHandle(void *base)
{
    _ASSERTE(IsStatic());
#ifdef EnC_SUPPORTED
    if (GetOffset() == FIELD_OFFSET_NEW_ENC)
    {
        EnCFieldDesc * pFD = (EnCFieldDesc *)this;
        _ASSERTE(pFD->GetMethodTableOfEnclosingClass()->GetClass()->GetMethodTable() == pFD->GetMethodTableOfEnclosingClass());
        _ASSERTE(pFD->GetModule()->IsEditAndContinue());

        EditAndContinueModule *pModule = (EditAndContinueModule*)pFD->GetModule();
        _ASSERTE(pModule->IsEditAndContinue());
        return (void *)pModule->ResolveField(NULL, pFD, TRUE);
    }
#endif // EnC_SUPPORTED

    if (IsRVA()) 
    {
        Module* pModule = GetModule();

        BYTE *pAltBase = pModule->GetAlternateRVAStaticBase();
        void *ret;
        if (pAltBase != NULL)
            ret = pAltBase + GetOffset();
        else
            ret = pModule->ResolveILRVA(GetOffset(), TRUE);

        _ASSERTE(!pModule->IsPEFile() || !pModule->GetPEFile()->IsTLSAddress(ret));
        return(ret);
    }

    void *ret = ((BYTE *) base + GetOffset());

    // Since static object fields are handles, we need to dereferece to get the actual object
    // pointer, after first checking that the handle for this field exists.
    if (GetFieldType() == ELEMENT_TYPE_CLASS || GetFieldType() == ELEMENT_TYPE_VALUETYPE)
    {
        // Make sure the class's static handles & boxed structs are allocated
        GetMethodTableOfEnclosingClass()->CheckRestore();
        
        OBJECTREF *pObjRef = *((OBJECTREF**) ret);
        _ASSERTE(pObjRef);
        ret = (void*) pObjRef;
    }

    return ret;
}

// These routines encapsulate the operation of getting and setting
// fields.
void    FieldDesc::GetInstanceField(LPVOID o, VOID * pOutVal)
{
    THROWSCOMPLUSEXCEPTION();

    // Check whether we are getting a field value on a proxy or a marshalbyref
    // class. If so, then ask remoting services to extract the value from the 
    //instance
    if(((Object*)o)->GetClass()->IsMarshaledByRef() ||
       ((Object*)o)->IsThunking())
    {

        if (CTPMethodTable::IsTPMethodTable(((Object*)o)->GetMethodTable()))
        {
            Object *puo = (Object *) CRemotingServices::AlwaysUnwrap((Object*) o);
            OBJECTREF unwrapped = ObjectToOBJECTREF(puo);
            
#ifdef PROFILING_SUPPORTED

         GCPROTECT_BEGIN(unwrapped); // protect from RemotingClientInvocationStarted

			BOOL fIsRemoted = FALSE;

            // If profiling is active, notify it that remoting stuff is kicking in,
            // if AlwaysUnwrap returned an identical object pointer which means that
            // we are definitely going through remoting for this access.
            if (CORProfilerTrackRemoting())
            {
                fIsRemoted = ((LPVOID)puo == o);
                if (fIsRemoted)
                {
                    g_profControlBlock.pProfInterface->RemotingClientInvocationStarted(
                        reinterpret_cast<ThreadID>(GetThread()));
                }
            }
#endif // PROFILING_SUPPORTED

            CRemotingServices::FieldAccessor(this, unwrapped, pOutVal, TRUE);

#ifdef PROFILING_SUPPORTED
            if (CORProfilerTrackRemoting() && fIsRemoted)
                g_profControlBlock.pProfInterface->RemotingClientInvocationFinished(
                    reinterpret_cast<ThreadID>(GetThread()));

         GCPROTECT_END();           // protect from RemotingClientInvocationStarted
			
#endif // PROFILING_SUPPORTED


        }
        else
        {
            CRemotingServices::FieldAccessor(this, ObjectToOBJECTREF((Object*)o),
                                             pOutVal, TRUE);
        }
         

    }
    else
    {
        _ASSERTE(GetEnclosingClass()->IsValueClass() || !((Object*) o)->IsThunking());

        // Unbox the value class
        if(GetEnclosingClass()->IsValueClass())
        {
            o = ObjectToOBJECTREF((Object *)o)->UnBox();
        }
        LPVOID pFieldAddress = GetAddress(o);
        UINT cbSize = GetSize();
           
        switch (cbSize)
        {
            case 1:
                *(INT8*)pOutVal = *(INT8*)pFieldAddress;
                break;
        
            case 2:
                *(INT16*)pOutVal = *(INT16*)pFieldAddress;
                break;
        
            case 4:
                *(INT32*)pOutVal = *(INT32*)pFieldAddress;
                break;
        
            case 8:
                *(INT64*)pOutVal = *(INT64*)pFieldAddress;
                break;
        
            default:
                CopyMemory(pOutVal, pFieldAddress, cbSize);
                break;
        }
    }
}

void    FieldDesc::SetInstanceField(LPVOID o, const VOID * pInVal)
{
    THROWSCOMPLUSEXCEPTION();

    // Check whether we are setting a field value on a proxy or a marshalbyref
    // class. If so, then ask remoting services to set the value on the 
    // instance

    if(((Object*)o)->IsThunking())
    {
        Object *puo = (Object *) CRemotingServices::AlwaysUnwrap((Object*) o);
        OBJECTREF unwrapped = ObjectToOBJECTREF(puo);

#ifdef PROFILING_SUPPORTED

        GCPROTECT_BEGIN(unwrapped);

        BOOL fIsRemoted = FALSE;

        // If profiling is active, notify it that remoting stuff is kicking in,
        // if AlwaysUnwrap returned an identical object pointer which means that
        // we are definitely going through remoting for this access.

        if (CORProfilerTrackRemoting())
        {
            fIsRemoted = ((LPVOID)puo == o);
            if (fIsRemoted)
            {
                g_profControlBlock.pProfInterface->RemotingClientInvocationStarted(
                    reinterpret_cast<ThreadID>(GetThread()));
            }
        }
#endif // PROFILING_SUPPORTED

        CRemotingServices::FieldAccessor(this, unwrapped, (void *)pInVal, FALSE);

#ifdef PROFILING_SUPPORTED
        if (CORProfilerTrackRemoting() && fIsRemoted)
            g_profControlBlock.pProfInterface->RemotingClientInvocationFinished(
                reinterpret_cast<ThreadID>(GetThread()));

		GCPROTECT_END();

#endif // PROFILING_SUPPORTED


    }
    else
    {
        _ASSERTE(GetEnclosingClass()->IsValueClass() || !((Object*) o)->IsThunking());
    
        Object *oParmIn = (Object*)o;
        // Unbox the value class
        if(GetEnclosingClass()->IsValueClass())
        {
            o = ObjectToOBJECTREF((Object *)o)->UnBox();
        }
        LPVOID pFieldAddress = GetAddress(o);
    
    
        CorElementType fieldType = GetFieldType();
    
        if (fieldType == ELEMENT_TYPE_CLASS)
        {
            OBJECTREF ref = ObjectToOBJECTREF(*(Object**)pInVal);

            SetObjectReference((OBJECTREF*)pFieldAddress, ref, 
                               (oParmIn)->GetAppDomain());
        }
        else
        {
            UINT cbSize = GetSize();
    
            switch (cbSize)
            {
                case 1:
                    *(INT8*)pFieldAddress = *(INT8*)pInVal;
                    break;
    
                case 2:
                    *(INT16*)pFieldAddress = *(INT16*)pInVal;
                    break;
    
                case 4:
                    *(INT32*)pFieldAddress = *(INT32*)pInVal;
                    break;
    
                case 8:
                    *(INT64*)pFieldAddress = *(INT64*)pInVal;
                    break;
    
                default:
                    CopyMemory(pFieldAddress, pInVal, cbSize);
                    break;
            }
        }
    }
}

// This function is used for BYREF support of fields.  Since it generates
// interior pointers, you really have to watch the lifetime of the pointer
// so that GCs don't happen while you have the reference active
void *FieldDesc::GetAddress( void *o)
{
    if (GetEnclosingClass()->IsValueClass())
        return ((*((BYTE**) &o)) + GetOffset());
    else
        return ((*((BYTE**) &o)) + GetOffset() + sizeof(Object));
}

// And here's the equivalent, when you are guaranteed that the enclosing instance of
// the field is in the GC Heap.  So if the enclosing instance is a value type, it had
// better be boxed.  We ASSERT this.
void *FieldDesc::GetAddressGuaranteedInHeap(void *o, BOOL doValidate)
{
#ifdef _DEBUG
    Object *pObj = (Object *)o;
    if (doValidate)
        pObj->Validate();
#endif
    return ((*((BYTE**) &o)) + GetOffset() + sizeof(Object));
}


DWORD   FieldDesc::GetValue32(OBJECTREF o)
{
    THROWSCOMPLUSEXCEPTION();

    DWORD val;
    GetInstanceField(o, (LPVOID)&val);
    return val;
}

VOID    FieldDesc::SetValue32(OBJECTREF o, DWORD dwValue)
{
    THROWSCOMPLUSEXCEPTION();

    SetInstanceField(o, (LPVOID)&dwValue);
}

void*   FieldDesc::GetValuePtr(OBJECTREF o)
{
    THROWSCOMPLUSEXCEPTION();

    void* val;
    GetInstanceField(o, (LPVOID)&val);
    return val;
}

VOID    FieldDesc::SetValuePtr(OBJECTREF o, void* pValue)
{
    THROWSCOMPLUSEXCEPTION();

    SetInstanceField(o, (LPVOID)&pValue);
}

OBJECTREF FieldDesc::GetRefValue(OBJECTREF o)
{
    THROWSCOMPLUSEXCEPTION();

    OBJECTREF val = NULL;

#ifdef PROFILING_SUPPORTED
    GCPROTECT_BEGIN(val);
#endif

    GetInstanceField(o, (LPVOID)&val);

#ifdef PROFILING_SUPPORTED
    GCPROTECT_END();
#endif

    return val;
}

VOID    FieldDesc::SetRefValue(OBJECTREF o, OBJECTREF orValue)
{
    THROWSCOMPLUSEXCEPTION();
    VALIDATEOBJECTREF(o);
    VALIDATEOBJECTREF(orValue);

    SetInstanceField(o, (LPVOID)&orValue);
}

USHORT  FieldDesc::GetValue16(OBJECTREF o)
{
    THROWSCOMPLUSEXCEPTION();

    USHORT val;
    GetInstanceField(o, (LPVOID)&val);
    return val;
}

VOID    FieldDesc::SetValue16(OBJECTREF o, DWORD dwValue)
{
    THROWSCOMPLUSEXCEPTION();

    USHORT val = (USHORT)dwValue;
    SetInstanceField(o, (LPVOID)&val);
}

BYTE    FieldDesc::GetValue8(OBJECTREF o)
{
    THROWSCOMPLUSEXCEPTION();

    BYTE val;
    GetInstanceField(o, (LPVOID)&val);
    return val;

}

VOID    FieldDesc::SetValue8(OBJECTREF o, DWORD dwValue)
{
    THROWSCOMPLUSEXCEPTION();

    BYTE val = (BYTE)dwValue;
    SetInstanceField(o, (LPVOID)&val);
}

__int64 FieldDesc::GetValue64(OBJECTREF o)
{
    THROWSCOMPLUSEXCEPTION();
    __int64 val;
    GetInstanceField(o, (LPVOID)&val);
    return val;

}

VOID    FieldDesc::SetValue64(OBJECTREF o, __int64 value)
{
    THROWSCOMPLUSEXCEPTION();
    SetInstanceField(o, (LPVOID)&value);
}


HRESULT FieldDesc::SaveContents(DataImage *image)
{
    HRESULT hr;

    //
    // If we are compiling and IL only image, and our RVA fits
    // in the designated range, copy the RVA data over to the prejit
    // image.
    // 

    if (IsRVA() 
        && (GetModule()->GetCORHeader()->Flags & COMIMAGE_FLAGS_ILONLY)
        && m_dwOffset != FIELD_OFFSET_BIG_RVA)
    {
        //
        // Move the RVA data into the prejit image.
        //
            
        BYTE *pRVAData = (BYTE*) GetStaticAddressHandle(NULL);

        UINT size = GetSize();

        // 
        // Compute an alignment for the data based on the alignment
        // of the RVA.  We'll align up to 8 bytes, which is the 
        // current max alignment supported by dataimage.
        //

        UINT align = 1;
        DWORD rva = GetOffset();

        while ((rva&1) == 0 && align < 8 && align < size)
        {
            align <<= 1;
            rva >>= 1;
        }

        if (image->IsAnyStored(pRVAData, size))
        {
            // Cannot handle overlapping RVA statics with this scheme - 
            // fail the prejit image store.
            return VLDTR_E_FRVA_DUPRVA;
        }

        IfFailRet(image->StoreStructure(pRVAData, size,
                                        DataImage::SECTION_RVA_STATICS,
                                        DataImage::DESCRIPTION_FIELD_DESC,
                                        GetMemberDef(), align));
    }

    image->ReattributeStructure(GetMemberDef(), sizeof(FieldDesc));

    return S_OK;
}

HRESULT FieldDesc::Fixup(DataImage *image)
{
    HRESULT hr;

    IfFailRet(image->FixupPointerField(&m_pMTOfEnclosingClass));

    if (IsRVA() 
        && (GetModule()->GetCORHeader()->Flags & COMIMAGE_FLAGS_ILONLY)
        && m_dwOffset != FIELD_OFFSET_BIG_RVA)
    {
        FieldDesc *pNewFD = (FieldDesc *) image->GetImagePointer(this);

        BYTE *pRVAData = (BYTE *) GetStaticAddressHandle(NULL);

        //
        // Store the RVA relative to the first RVA static in the prejit file
        // (rather than to the zap base), so we don't have to worry about 
        // overflowing the rva field.
        //

        BYTE *pNewRVAData = (BYTE *) image->GetImagePointer(pRVAData);

        pNewFD->SetOffset(pNewRVAData - image->GetImageBase() 
                          - image->GetSectionBaseOffset(DataImage::SECTION_RVA_STATICS));
    }

    return S_OK;
}

UINT FieldDesc::GetSize()
{
    CorElementType type = GetFieldType();
    UINT size = GetSizeForCorElementType(type);
    if (size == (UINT) -1)
    {
        _ASSERTE(GetFieldType() == ELEMENT_TYPE_VALUETYPE);
        size = GetTypeOfField()->GetNumInstanceFieldBytes();
    }

    return size;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\field.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
//
// COM+ Data Field Abstraction
// 

#ifndef _FIELD_H_
#define _FIELD_H_

#include "objecthandle.h"
#include "excep.h"
#include <member-offset-info.h>

// Temporary values stored in FieldDesc m_dwOffset during loading
// The high 5 bits must be zero (because in field.h we steal them for other uses), so we must choose values > 0
#define FIELD_OFFSET_MAX              ((1<<27)-1)
#define FIELD_OFFSET_UNPLACED         FIELD_OFFSET_MAX
#define FIELD_OFFSET_UNPLACED_GC_PTR  (FIELD_OFFSET_MAX-1)
#define FIELD_OFFSET_VALUE_CLASS      (FIELD_OFFSET_MAX-2)
#define FIELD_OFFSET_NOT_REAL_FIELD   (FIELD_OFFSET_MAX-3)

// Offset to indicate an EnC added field. They don't have offsets as aren't placed in the object.
#define FIELD_OFFSET_NEW_ENC          (FIELD_OFFSET_MAX-4)
#define FIELD_OFFSET_BIG_RVA          (FIELD_OFFSET_MAX-5)
#define FIELD_OFFSET_LAST_REAL_OFFSET (FIELD_OFFSET_MAX-6)    // real fields have to be smaller than this

// Bits stolen from the MethodTable pointer, assuming 8-byte memory alignment.
#define FIELD_STOLEN_MT_BITS            0x1
#define FIELD_UNUSED_MT_BIT             0x1


//
// This describes a field - one of this is allocated for every field, so don't make this structure any larger.
//
class FieldDesc
{
    friend HRESULT EEClass::BuildMethodTable(Module *pModule, 
                                             mdToken cl, 
                                             BuildingInterfaceInfo_t *pBuildingInterfaceList, 
                                             const LayoutRawFieldInfo *pLayoutRawFieldInfos,
                                             OBJECTREF *pThrowable);

    friend HRESULT EEClass::InitializeFieldDescs(FieldDesc *,const LayoutRawFieldInfo*, bmtInternalInfo*, bmtMetaDataInfo*, 
                                                 bmtEnumMethAndFields*, bmtErrorInfo*, EEClass***, bmtMethAndFieldDescs*, 
                                                 bmtFieldPlacement*, unsigned * totalDeclaredSize);
    friend HRESULT EEClass::PlaceStaticFields(bmtVtable*, bmtFieldPlacement*, bmtEnumMethAndFields*);
    friend HRESULT EEClass::PlaceInstanceFields(bmtFieldPlacement*, bmtEnumMethAndFields*, bmtParentInfo*, bmtErrorInfo*, EEClass***);
    friend HRESULT EEClass::SetupMethodTable(bmtVtable*, bmtInterfaceInfo*, bmtInternalInfo*, bmtProperties*, bmtMethAndFieldDescs*, bmtEnumMethAndFields*, bmtErrorInfo*, bmtMetaDataInfo*, bmtParentInfo*);
    friend DWORD EEClass::GetFieldSize(FieldDesc *pFD);
#ifdef EnC_SUPPORTED
    friend HRESULT EEClass::FixupFieldDescForEnC(EnCFieldDesc *, mdFieldDef);
#endif // EnC_SUPPORTED
    friend struct MEMBER_OFFSET_INFO(FieldDesc);

  protected:
    MethodTable *m_pMTOfEnclosingClass; // Note, 2 bits of info are stolen from this pointer

    // Strike needs to be able to determine the offset of certain bitfields.
    // Bitfields can't be used with /offsetof/.
    // Thus, the union/structure combination is used to determine where the
    // bitfield begins, without adding any additional space overhead.
    union {
        unsigned char m_mb_begin;
        struct {
            unsigned m_mb               : 24; 

            // 8 bits...
            unsigned m_isStatic         : 1;
            unsigned m_isThreadLocal    : 1;
            unsigned m_isContextLocal   : 1;
            unsigned m_isRVA            : 1;
            unsigned m_prot             : 3;
            unsigned m_isDangerousAppDomainAgileField : 1; // Note: this is used in checked only
        };
    };

    // Strike needs to be able to determine the offset of certain bitfields.
    // Bitfields can't be used with /offsetof/.
    // Thus, the union/structure combination is used to determine where the
    // bitfield begins, without adding any additional space overhead.
    union {
        unsigned char m_dwOffset_begin;
        struct {
            // Note: this has been as low as 22 bits in the past & seemed to be OK.
            // we can steal some more bits here if we need them.
            unsigned m_dwOffset         : 27;
            unsigned m_type             : 5;
        };
    };

#ifdef _DEBUG
    const char* m_debugName;
#endif

    // Allocated by special heap means, don't construct me
    FieldDesc() {};

  public:
    // This should be called.  It was added so that Reflection
    // can create FieldDesc's for the static primitive fields that aren't
    // stored with the EEClass.
    //NOTE: Any information that might have been stored using the 2 stolen
    // bits from the MethodDesc* will be lost when calling this method,
    // so the MethodTable should be set before any other state.
    void SetMethodTable(MethodTable* mt)
    {
        m_pMTOfEnclosingClass = mt;
    }

    VOID Init(mdFieldDef mb, CorElementType FieldType, DWORD dwMemberAttrs, BOOL fIsStatic, BOOL fIsRVA, BOOL fIsThreadLocal, BOOL fIsContextLocal, LPCSTR pszFieldName)
    { 
        // We allow only a subset of field types here - all objects must be set to TYPE_CLASS
        // By-value classes are ELEMENT_TYPE_VALUETYPE
        _ASSERTE(
            FieldType == ELEMENT_TYPE_I1 ||
            FieldType == ELEMENT_TYPE_BOOLEAN ||
            FieldType == ELEMENT_TYPE_U1 ||
            FieldType == ELEMENT_TYPE_I2 ||
            FieldType == ELEMENT_TYPE_U2 ||
            FieldType == ELEMENT_TYPE_CHAR ||
            FieldType == ELEMENT_TYPE_I4 ||
            FieldType == ELEMENT_TYPE_U4 ||
            FieldType == ELEMENT_TYPE_I8 ||
            FieldType == ELEMENT_TYPE_U8 ||
            FieldType == ELEMENT_TYPE_R4 ||
            FieldType == ELEMENT_TYPE_R8 ||
            FieldType == ELEMENT_TYPE_CLASS ||
            FieldType == ELEMENT_TYPE_VALUETYPE ||
            FieldType == ELEMENT_TYPE_PTR ||
            FieldType == ELEMENT_TYPE_FNPTR
        );
        _ASSERTE(fIsStatic || (!fIsRVA && !fIsThreadLocal && !fIsContextLocal));
        _ASSERTE(fIsRVA + fIsThreadLocal + fIsContextLocal <= 1);

        m_mb = RidFromToken(mb);
        m_type = FieldType;
        m_prot = fdFieldAccessMask & dwMemberAttrs;
        m_isDangerousAppDomainAgileField = 0;
        m_isStatic = fIsStatic != 0;
        m_isRVA = fIsRVA != 0;
        m_isThreadLocal = fIsThreadLocal != 0;
        m_isContextLocal = fIsContextLocal != 0;

#ifdef _DEBUG
        m_debugName = pszFieldName;
#endif
        _ASSERTE(GetMemberDef() == mb);                 // no truncation
        _ASSERTE(GetFieldType() == FieldType); 
        _ASSERTE(GetFieldProtection() == (fdFieldAccessMask & dwMemberAttrs));
        _ASSERTE((BOOL) IsStatic() == (fIsStatic != 0));
    }

    mdFieldDef GetMemberDef()
    {
        return TokenFromRid(m_mb, mdtFieldDef);
    }

    CorElementType GetFieldType()
    {
        return (CorElementType) m_type;
    }
    
    DWORD GetFieldProtection()
    {
        return m_prot;
    }
    
        // Please only use this in a path that you have already guarenteed
        // the assert is true
    DWORD GetOffsetUnsafe()
    {
        _ASSERTE(m_dwOffset <= FIELD_OFFSET_LAST_REAL_OFFSET);
        return m_dwOffset;
    }

    DWORD GetOffset()
    {
        if (m_dwOffset != FIELD_OFFSET_BIG_RVA) {
            return m_dwOffset;
        }

        return OutOfLine_BigRVAOffset();
    }

    DWORD OutOfLine_BigRVAOffset()
    {
        DWORD   rva;

        _ASSERTE(m_dwOffset == FIELD_OFFSET_BIG_RVA);

        // I'm discarding a potential error here.  According to the code in MDInternalRO.cpp,
        // we won't get an error if we initially found the RVA.  So I'm going to just
        // assert it never happens.
        //
        // This is a small sin, but I don't see a good alternative. --cwb.
#ifdef _DEBUG
        HRESULT     hr =
#endif
        GetMDImport()->GetFieldRVA(GetMemberDef(), &rva); 
        _ASSERTE(SUCCEEDED(hr));

        return rva;
    }

    HRESULT SetOffset(DWORD dwOffset)
    {
        m_dwOffset = dwOffset;
        return((dwOffset > FIELD_OFFSET_LAST_REAL_OFFSET) ? E_FAIL : S_OK);
    }

    // Okay, we've stolen too many bits from FieldDescs.  In the RVA case, there's no
    // reason to believe they will be limited to 22 bits.  So use a sentinel for the
    // huge cases, and recover them from metadata on-demand.
    HRESULT SetOffsetRVA(DWORD dwOffset)
    {
        m_dwOffset = (dwOffset > FIELD_OFFSET_LAST_REAL_OFFSET)
                      ? FIELD_OFFSET_BIG_RVA
                      : dwOffset;
        return S_OK;
    }

    DWORD   IsStatic()                          
    { 
        return m_isStatic;
    }

    BOOL   IsSpecialStatic()
    {
        return m_isStatic && (m_isRVA || m_isThreadLocal || m_isContextLocal);
    }

#if CHECK_APP_DOMAIN_LEAKS
    BOOL   IsDangerousAppDomainAgileField()
    {
        return m_isDangerousAppDomainAgileField;
    }

    void    SetDangerousAppDomainAgileField()
    {
        m_isDangerousAppDomainAgileField = TRUE;
    }
#endif

    BOOL   IsRVA()                     // Has an explicit RVA associated with it
    { 
        return m_isRVA;
    }

    BOOL   IsThreadStatic()            // Static relative to a thread
    { 
        return m_isThreadLocal;
    }

    DWORD   IsContextStatic()           // Static relative to a context
    { 
        return m_isContextLocal;
    }

    void SetEnCNew() 
    {
        SetOffset(FIELD_OFFSET_NEW_ENC);
    }

    BOOL IsEnCNew() 
    {
        return GetOffset() == FIELD_OFFSET_NEW_ENC;
    }

    BOOL IsByValue()
    {
        return GetFieldType() == ELEMENT_TYPE_VALUETYPE;
    }

    BOOL IsPrimitive()
    {
        return (CorIsPrimitiveType(GetFieldType()) != FALSE);
    }

    BOOL IsObjRef();

    HRESULT SaveContents(DataImage *image);
    HRESULT Fixup(DataImage *image);

    UINT GetSize();

    void    GetInstanceField(OBJECTREF o, VOID * pOutVal)
    {
        THROWSCOMPLUSEXCEPTION();
        GetInstanceField(*(LPVOID*)&o, pOutVal);
    }

    void    SetInstanceField(OBJECTREF o, const VOID * pInVal)
    {
        THROWSCOMPLUSEXCEPTION();
        SetInstanceField(*(LPVOID*)&o, pInVal);
    }

    // These routines encapsulate the operation of getting and setting
    // fields.
    void    GetInstanceField(LPVOID o, VOID * pOutVal);
    void    SetInstanceField(LPVOID o, const VOID * pInVal);



        // Get the address of a field within object 'o'
    void*   GetAddress(void *o);
    void*   GetAddressGuaranteedInHeap(void *o, BOOL doValidate=TRUE);

    void*   GetValuePtr(OBJECTREF o);
    VOID    SetValuePtr(OBJECTREF o, void* pValue);
    DWORD   GetValue32(OBJECTREF o);
    VOID    SetValue32(OBJECTREF o, DWORD dwValue);
    OBJECTREF GetRefValue(OBJECTREF o);
    VOID    SetRefValue(OBJECTREF o, OBJECTREF orValue);
    USHORT  GetValue16(OBJECTREF o);
    VOID    SetValue16(OBJECTREF o, DWORD dwValue);  
    BYTE    GetValue8(OBJECTREF o);               
    VOID    SetValue8(OBJECTREF o, DWORD dwValue);  
    __int64 GetValue64(OBJECTREF o);               
    VOID    SetValue64(OBJECTREF o, __int64 value);  

    MethodTable *GetMethodTableOfEnclosingClass()
    {
        return (MethodTable*)(((size_t)m_pMTOfEnclosingClass) & ~FIELD_STOLEN_MT_BITS);
    }

    EEClass *GetEnclosingClass()
    {
        return  GetMethodTableOfEnclosingClass()->GetClass();
    }

    // OBSOLETE:
    EEClass *GetTypeOfField() { return LoadType().AsClass(); }
    // OBSOLETE:
    EEClass *FindTypeOfField()  { return FindType().AsClass(); }

    TypeHandle LoadType();
    TypeHandle FindType();

    // returns the address of the field
    void* GetSharedBase(DomainLocalClass *pLocalClass)
      { 
          _ASSERTE(GetMethodTableOfEnclosingClass()->IsShared());
          return pLocalClass->GetStaticSpace(); 
      }

    // returns the address of the field (boxed object for value classes)
    void* GetUnsharedBase()
      { 
          _ASSERTE(!GetMethodTableOfEnclosingClass()->IsShared() || IsRVA());
          return GetMethodTableOfEnclosingClass()->GetVtable(); 
      }

    void* GetBase(DomainLocalClass *pLocalClass)
      { 
          if (GetMethodTableOfEnclosingClass()->IsShared())
              return GetSharedBase(pLocalClass);
          else
              return GetUnsharedBase();
      }

    void* GetBase()
      { 
          THROWSCOMPLUSEXCEPTION();

          MethodTable *pMT = GetMethodTableOfEnclosingClass();

          if (pMT->IsShared() && !IsRVA())
          {
              DomainLocalClass *pLocalClass;
              OBJECTREF throwable;
              if (pMT->CheckRunClassInit(&throwable, &pLocalClass))
                  return GetSharedBase(pLocalClass);
              else
                  COMPlusThrow(throwable);
          }

          return GetUnsharedBase();
      }

    // returns the address of the field
    void* GetStaticAddress(void *base);

    // In all cases except Value classes, the AddressHandle is
    // simply the address of the static.  For the case of value
    // types, however, it is the address of OBJECTREF that holds
    // the boxed value used to hold the value type.  This is needed
    // because the OBJECTREF moves, and the JIT needs to embed something
    // in the code that does not move.  Thus the jit has to 
    // dereference and unbox before the access.  
    void* GetStaticAddressHandle(void *base);

    OBJECTREF GetStaticOBJECTREF()
    {
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());

        OBJECTREF *pObjRef = NULL;
        if (IsContextStatic()) 
            pObjRef = (OBJECTREF*)Context::GetStaticFieldAddress(this);
        else if (IsThreadStatic()) 
            pObjRef = (OBJECTREF*)Thread::GetStaticFieldAddress(this);
        else {
            void *base = 0;
            if (!IsRVA()) // for RVA the base is ignored
                base = GetBase(); 
            pObjRef = (OBJECTREF*)GetStaticAddressHandle(base); 
        }
        _ASSERTE(pObjRef);
        return *pObjRef;
    }

    VOID SetStaticOBJECTREF(OBJECTREF or)
    {
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());

        OBJECTREF *pObjRef = NULL;
        GCPROTECT_BEGIN(or);
        if (IsContextStatic()) 
            pObjRef = (OBJECTREF*)Context::GetStaticFieldAddress(this);
        else if (IsThreadStatic()) 
            pObjRef = (OBJECTREF*)Thread::GetStaticFieldAddress(this);
        else {
            void *base = 0;
            if (!IsRVA()) // for RVA the base is ignored
                base = GetBase(); 
            pObjRef = (OBJECTREF*)GetStaticAddress(base); 
        }
        _ASSERTE(pObjRef);
        GCPROTECT_END();
        SetObjectReference(pObjRef, or, GetAppDomain());
    }

    void*   GetStaticValuePtr()               
    { 
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());
        return *(void**)GetPrimitiveOrValueClassStaticAddress();
    }
    
    VOID    SetStaticValuePtr(void *value)  
    { 
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());

        void **pLocation = (void**)GetPrimitiveOrValueClassStaticAddress();
        _ASSERTE(pLocation);
        *pLocation = value;
    }

    DWORD   GetStaticValue32()               
    { 
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());
        return *(DWORD*)GetPrimitiveOrValueClassStaticAddress(); 
    }
    
    VOID    SetStaticValue32(DWORD dwValue)  
    { 
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());
        *(DWORD*)GetPrimitiveOrValueClassStaticAddress() = dwValue; 
    }

    USHORT  GetStaticValue16()               
    { 
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());
        return *(USHORT*)GetPrimitiveOrValueClassStaticAddress(); 
    }
    
    VOID    SetStaticValue16(DWORD dwValue)  
    { 
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());
        *(USHORT*)GetPrimitiveOrValueClassStaticAddress() = (USHORT)dwValue; 
    }

    BYTE    GetStaticValue8()               
    { 
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());
        return *(BYTE*)GetPrimitiveOrValueClassStaticAddress(); 
    }
    
    VOID    SetStaticValue8(DWORD dwValue)  
    { 
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());
        *(USHORT*)GetPrimitiveOrValueClassStaticAddress() = (BYTE)dwValue; 
    }

    __int64 GetStaticValue64()
    { 
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());
        return *(__int64*)GetPrimitiveOrValueClassStaticAddress();
    }
    
    VOID    SetStaticValue64(__int64 qwValue)  
    { 
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());
        *(__int64*)GetPrimitiveOrValueClassStaticAddress() = qwValue;
    }

    void* GetPrimitiveOrValueClassStaticAddress()
    {
        THROWSCOMPLUSEXCEPTION();
        _ASSERTE(IsStatic());

        if (IsContextStatic()) 
            return Context::GetStaticFieldAddress(this);
        else if (IsThreadStatic()) 
            return Thread::GetStaticFieldAddress(this);
        else {
            void *base = 0;
            if (!IsRVA()) // for RVA the base is ignored
                base = GetBase(); 
            return GetStaticAddress(base); 
        }
    }

    Module *GetModule()
    {
        return GetMethodTableOfEnclosingClass()->GetModule();
    }

    void GetSig(PCCOR_SIGNATURE *ppSig, DWORD *pcSig)
    {
        *ppSig = GetMDImport()->GetSigOfFieldDef(GetMemberDef(), pcSig);
    }

    // @TODO: This is slow, don't use it!
    LPCUTF8  GetName()
    {
        return GetMDImport()->GetNameOfFieldDef(GetMemberDef());
    }

    // @TODO: This is slow, don't use it!
    DWORD   GetAttributes()
    {
        return GetMDImport()->GetFieldDefProps(GetMemberDef());
    }

    // Mini-Helpers
    DWORD   IsPublic()
    {
        return IsFdPublic(GetFieldProtection());
    }

    DWORD   IsPrivate()
    {
        return IsFdPrivate(GetFieldProtection());
    }

    IMDInternalImport *GetMDImport()
    {
        return GetMethodTableOfEnclosingClass()->GetModule()->GetMDImport();
    }

    IMetaDataImport *GetImporter()
    {
        return GetMethodTableOfEnclosingClass()->GetModule()->GetImporter();
    }
};

#endif _FIELD_H_
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\fjit_eetwain.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#include "common.h"
#include "EjitMgr.h"
#include "EETwain.h"
#include "FJIT_EETwain.h"
#include "CGENSYS.H"
#include "DbgInterface.h"

//@TODO: this file has alot of x86 specific code in it, when we start porting, this needs to be refactored

#if CHECK_APP_DOMAIN_LEAKS
#define CHECK_APP_DOMAIN    GC_CALL_CHECK_APP_DOMAIN
#else
#define CHECK_APP_DOMAIN    0
#endif

/*****************************************************************************
 *
 *  Decodes the methodInfoPtr and returns the decoded information
 *  in the hdrInfo struct.  The EIP parameter is the PC location
 *  within the active method.  Answer the number of bytes read.
 */
static size_t   crackMethodInfoHdr(unsigned char* compressed,
                                   SLOT    methodStart,
                                    Fjit_hdrInfo   * infoPtr)
{
//@TODO: for now we are just passing the full struct back.  Change when the compressor is built to decompress
    memcpy(infoPtr, compressed, sizeof(Fjit_hdrInfo));
    return sizeof(Fjit_hdrInfo);
};

/********************************************************************************
 *  Promote the object, checking interior pointers on the stack is supposed to be done
 *  in pCallback
 */
void promote(GCEnumCallback pCallback, LPVOID hCallBack, OBJECTREF* pObj, DWORD flags /* interior or pinned */
#ifdef _DEBUG
        ,char* why  = NULL     
#endif
             ) 
{
    LOG((LF_GC, INFO3, "    Value %x at %x being Promoted to ", *pObj, pObj));
    pCallback(hCallBack, pObj, flags | CHECK_APP_DOMAIN);
    LOG((LF_GC, INFO3, "%x ", *pObj ));
#ifdef _DEBUG
    LOG((LF_GC, INFO3, " because %s\n", why));
#endif 
}


/* It looks like the implementation 
in EETwain never looks at methodInfoPtr, so we can use it.
bool Fjit_EETwain::FilterException (
                PCONTEXT        pContext,
                unsigned        win32Fault,
                LPVOID          methodInfoPtr,
                LPVOID          methodStart)
{
    _ASSERTE(!"NYI");
    return FALSE;
} */

/*
    Last chance for the runtime support to do fixups in the context
    before execution continues inside a catch handler.
*/
void Fjit_EETwain::FixContext(
                ContextType     ctxType,
                EHContext      *ctx,
                LPVOID          methodInfoPtr,
                LPVOID          methodStart,
                DWORD           nestingLevel,
                OBJECTREF       thrownObject,
                CodeManState   *pState,
                size_t       ** ppShadowSP,
                size_t       ** ppEndRegion)
{
    assert((ctxType == FINALLY_CONTEXT) == (thrownObject == NULL));

#ifdef _X86_
    unsigned char* compressed = (unsigned char*) methodInfoPtr;
    Fjit_hdrInfo hdrInfo;
    crackMethodInfoHdr(compressed, (SLOT)methodStart, &hdrInfo);
    // at this point we need to also zero out the counter and operand base for all handlers
    // that were stored in the hidden locals

    DWORD* pFrameBase; pFrameBase = (DWORD*)(((size_t) ctx->Ebp)+prolog_bias);
    pFrameBase--;       // since stack grows down from here

    if (nestingLevel > 0)
    {
        pFrameBase -= nestingLevel*2;       // there are two slots per EH clause
        if  (*(pFrameBase-1))       // check to see if there has been a localloc 
        {
            ctx->Esp = *(pFrameBase-1);     // yes, use the localloc slot
        }
        else 
        {
            ctx->Esp =  *pFrameBase & ~1;        // else, use normal slot, zero out filter bit
        }

        // zero out the next slot, @TODO: Is this redundant
        *(pFrameBase-2) = 0;
    }
    else 
    {
        ctx->Esp = pFrameBase[JIT_GENERATED_LOCAL_LOCALLOC_OFFSET] ?
                    pFrameBase[JIT_GENERATED_LOCAL_LOCALLOC_OFFSET] :
                    ctx->Ebp - hdrInfo.methodFrame*sizeof(void*);
    }

    *((OBJECTREF*)&(ctx->Eax)) = thrownObject;

#else // !_X86_
    _ASSERTE(!"@TODO Alpha - FixContext (FJIT_EETwain.cpp)");
#endif // _X86_

    return;
}

/*
    Last chance for the runtime support to do fixups in the context
    before execution continues inside an EnC updated function.
*/
ICodeManager::EnC_RESULT   Fjit_EETwain::FixContextForEnC(
                                       void           *pMethodDescToken,
                                       PCONTEXT        pCtx,
                                       LPVOID          oldMethodInfoPtr,
                                       SIZE_T          oldMethodOffset,
                  const ICorDebugInfo::NativeVarInfo * oldMethodVars,
                                       SIZE_T          oldMethodVarsCount,
                                       LPVOID          newMethodInfoPtr,
                                       SIZE_T          newMethodOffset,
                  const ICorDebugInfo::NativeVarInfo * newMethodVars,
                                       SIZE_T          newMethodVarsCount)
{
    //_ASSERTE(!"NYI");
#ifdef _X86_
    LOG((LF_ENC, LL_INFO100, "EjitCM::FixContextForEnC\n"));

    /* extract necessary info from the method info headers */

    Fjit_hdrInfo oldHdrInfo, newHdrInfo;
    crackMethodInfoHdr((unsigned char*)oldMethodInfoPtr,
                       (SLOT)oldMethodOffset,
                       &oldHdrInfo);

    crackMethodInfoHdr((unsigned char*)newMethodInfoPtr,
                       (SLOT)newMethodOffset,
                       &newHdrInfo);

    /* Some basic checks */
    if (!oldHdrInfo.EnCMode || !newHdrInfo.EnCMode) 
    {
        LOG((LF_ENC, LL_INFO100, "**Error** EjitCM::FixContextForEnC EnC_INFOLESS_METHOD\n"));
        return EnC_INFOLESS_METHOD;
    }

    if (pCtx->Esp != pCtx->Ebp - (oldHdrInfo.methodFrame) * sizeof(void*))
    {
        LOG((LF_ENC, LL_INFO100, "**Error** EjitCM::FixContextForEnC stack should be empty\n"));
        return EnC_FAIL; // stack should be empty - @TODO : Barring localloc
    }

    DWORD* pFrameBase; pFrameBase = (DWORD*)(size_t)(pCtx->Ebp + prolog_bias);
    pFrameBase--;       // since stack grows down from here
    
    if (pFrameBase[JIT_GENERATED_LOCAL_LOCALLOC_OFFSET] != 0)
    {
        LOG((LF_ENC, LL_INFO100, "**Error** EjitCM::FixContextForEnC localloc is not allowed\n"));
        return EnC_LOCALLOC; // stack should be empty - @TODO : Barring localloc
    }

    _ASSERTE(oldHdrInfo.methodJitGeneratedLocalsSize == sizeof(void*) || //either no EH clauses
             pFrameBase[JIT_GENERATED_LOCAL_NESTING_COUNTER] == 0); //  or not inside a filter, handler, or finally

    if (oldHdrInfo.methodJitGeneratedLocalsSize != newHdrInfo.methodJitGeneratedLocalsSize)
    {
        LOG((LF_ENC, LL_INFO100,"**Error** EjitCM::FixContextForEnC number of handlers must not change\n"));
        // @TODO: Allow numbers to reduce or increase upto a maximum (say, 2)
        return EnC_FAIL;
    }
    /*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*
     *  IMPORTANT : Once we start munging on the context, we cannot return
     *  EnC_FAIL, as this should be a transacted commit,
     *
     **=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*/

    /* Since caller's registers are saved before locals, no need to update them */

    unsigned deltaEsp = (newHdrInfo.methodFrame - oldHdrInfo.methodFrame)*sizeof(void*);
    /* Adjust the stack height */
    if (deltaEsp > PAGE_SIZE)
    {
        // grow the stack a page at a time.
        unsigned delta = PAGE_SIZE;
        while (delta <= deltaEsp)
        {
            *(DWORD*)((BYTE*)(size_t)(pCtx->Esp) - delta) = 0;
            delta += PAGE_SIZE;
        }

    }
    pCtx->Esp -= deltaEsp;

    // Zero-init the new locals, if any
    if (deltaEsp > 0)
    {
        memset((void*)(size_t)(pCtx->Esp),0,deltaEsp);
    }

#else
    _ASSERTE(!"@TODO ALPHA - EjitCM::FixContextForEnC");
#endif
    return ICodeManager::EnC_OK;
}

/*
    Unwind the current stack frame, i.e. update the virtual register
    set in pContext.
    Returns success of operation.
*/
bool Fjit_EETwain::UnwindStackFrame(
                PREGDISPLAY     ctx,
                LPVOID          methodInfoPtr,
                ICodeInfo      *pCodeInfo,
                unsigned        flags,
                CodeManState   *pState)
{
#ifdef _X86_
    if (methodInfoPtr == NULL)
    { // this can only happen if we got stopped in the ejit stub and the GC info for the method has been pitched

        _ASSERTE(EconoJitManager::IsInStub( (BYTE*) (*(ctx->pPC)), FALSE));
        ctx->pPC = (SLOT*)(size_t)ctx->Esp;
        ctx->Esp += sizeof(void *);
        return true;
    }

    unsigned char* compressed = (unsigned char*) methodInfoPtr;
    Fjit_hdrInfo hdrInfo;
    crackMethodInfoHdr(compressed, (SLOT)pCodeInfo, &hdrInfo);

    /* since Fjit methods have exactly one prolog and one epilog (at end), 
    we can determine if we are in them without needing a map of the entire generated code.
    */

    //@TODO: rework to be chip independent, or have a version for each chip?
    //       and make which regs are saved optional based on method contents??
        /*prolog/epilog sequence is:
            push ebp        (1byte) 
            mov  ebp,esp    (2byte)
            push esi        (1byte) //callee saved
            xor  esi,esi            //used for newobj, initialize for liveness
            push 0                  //security obj == NULL
            push edx                //next enregistered arg
            push ecx                //enregisted this or first arg enregistered
            ... note that Fjit does not use any of the callee saved registers in prolog
            ...
            mov esi,[ebp-4]
            mov esp,ebp     
            pop ebp         
            ret [xx]        

        NOTE: This needs to match the code in fjit\x86fjit.h 
              for the emit_prolog and emit_return macros
        */
    //LPVOID methodStart = pCodeInfo->getStartAddress();
    SLOT   breakPC = (SLOT) *(ctx->pPC);
    DWORD    offset; // = (unsigned)(breakPC) - (unsigned) methodStart;
    METHODTOKEN ignored;
    EconoJitManager::JitCode2MethodTokenAndOffsetStatic(breakPC,&ignored,&offset);

    _ASSERTE(offset < hdrInfo.methodSize);
    if (offset <= hdrInfo.prologSize) {
        if(offset == 0) goto ret_XX;
        if(offset == 1) goto pop_ebp;
        if(offset == 3) goto mov_esp;
        /* callee saved regs have been pushed, frame is complete */
        goto mov_esi;
    }
    /* if the next instr is: ret [xx] -- 0xC2 or 0xC3, we are at the end of the epilog.
       we need to call an API to examine the instruction scheme, since the debugger might
       have placed a breakpoint. For now, the ret can be determined by figuring out 
       distance from the end of the epilog. NOTE: This works because we have only one
       epilog per method. 
       if not, ebp is still valid 
       */
    //if ((*((unsigned char*) breakPC) & 0xfe) == 0xC2) {
    unsigned offsetFromEnd;
    offsetFromEnd = (unsigned) (hdrInfo.methodSize-offset-(hdrInfo.methodArgsSize?2:0));
    if (offsetFromEnd <= 10) 
    {
        if (offsetFromEnd  ==   1) {
            //sanity check, we are at end of method
            //_ASSERTE(hdrInfo.methodSize-offset-(hdrInfo.methodArgsSize?2:0)==1);
            goto ret_XX;
        }
        else 
        {// we are in the epilog
            goto mov_esi;
        }
    }
    /* check if we might be in a filter */
    DWORD* pFrameBase; pFrameBase = (DWORD*)(size_t)(((DWORD)*(ctx->pEbp))+prolog_bias);
    pFrameBase--;       // since stack grows down from here
    
    unsigned EHClauseNestingDepth;
    if ((hdrInfo.methodJitGeneratedLocalsSize > sizeof(void*)) && // do we have any EH at all
        (EHClauseNestingDepth = pFrameBase[JIT_GENERATED_LOCAL_NESTING_COUNTER]) != 0) // are we in an EH?
    {
        int indexCurrentClause = EHClauseNestingDepth-1;
        /* search for clause whose esp base is greater/equal to ctx->Esp */
        while (pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause] < ctx->Esp)
        { 
            indexCurrentClause--;
            if (indexCurrentClause < 0) 
                break;
        }
        /* we are in the (index+1)'th handler */
        /* are we in a filter? */
        if (indexCurrentClause >= 0 && 
            (1 & pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause])) { // lowest bit set => filter 

            // only unwind this funclet
            DWORD baseSP =  pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause] & 0xfffffffe;
            _ASSERTE(pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause-1]==0); // no localloc in a filter
            ctx->pPC = (SLOT*)(size_t)baseSP;
            ctx->Esp = baseSP + sizeof(void*);
            return true;
        }
    }

    /* we still have a set up frame, simulate the epilog by loading saved registers
       off of ebp.
       */
    //@TODO: make saved regs optional and flag which are actually saved.  For now we saved them all
mov_esi:
    //restore esi, simulate mov esi,[ebp-4]
    ctx->pEsi =  (DWORD*) (size_t)((*(ctx->pEbp))+prolog_bias+offsetof(prolog_data,callee_saved_esi));  
mov_esp:
    //simulate mov esp,ebp
    ctx->Esp = *(ctx->pEbp);
pop_ebp:
    //simulate pop ebp
    ctx->pEbp = (DWORD*)(size_t)ctx->Esp;
    ctx->Esp += sizeof(void *);
ret_XX:
    //simulate the ret XX
    ctx->pPC = (SLOT*)(size_t)ctx->Esp;
    ctx->Esp += sizeof(void *) + hdrInfo.methodArgsSize;
    return true;
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - UnwindStackFrame (FJIT_EETwain.cpp)");
    return false;
#endif // _X86_
}


void Fjit_EETwain::HijackHandlerReturns(PREGDISPLAY     ctx,
                                        LPVOID          methodInfoPtr,
                                        ICodeInfo      *pCodeInfo,
                                        IJitManager*    jitmgr,
                                        CREATETHUNK_CALLBACK pCallBack
                                        )
{
#ifdef _X86_
    unsigned char* compressed = (unsigned char*) methodInfoPtr;
    Fjit_hdrInfo hdrInfo;
    crackMethodInfoHdr(compressed, (SLOT)pCodeInfo, &hdrInfo);
    if (hdrInfo.methodJitGeneratedLocalsSize <= sizeof(void*)) 
        return; // there are no exception clauses, so just return
    DWORD* pFrameBase = (DWORD*)(size_t)(((DWORD)*(ctx->pEbp))+prolog_bias);
    pFrameBase--;
    unsigned EHClauseNestingDepth = pFrameBase[JIT_GENERATED_LOCAL_NESTING_COUNTER];
    while (EHClauseNestingDepth) 
    {
        EHClauseNestingDepth--;
        size_t pHandler = (size_t) pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*EHClauseNestingDepth];
        if (!(pHandler & 1))   // only call back if not a filter, since filters are called by the EE
        {
            pCallBack(jitmgr,(LPVOID*) pHandler, pCodeInfo);
        }
    }
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - HijackHandlerReturns (FJIT_EETwain.cpp)");
#endif // _X86_

    return;
}

/*
    Is the function currently at a "GC safe point" ?
    For Fjitted methods this is always false.
*/

#define OPCODE_SEQUENCE_POINT 0x90
bool Fjit_EETwain::IsGcSafe(  PREGDISPLAY     pContext,
                LPVOID          methodInfoPtr,
                ICodeInfo      *pCodeInfo,
                unsigned        flags)

{
#ifdef DEBUGGING_SUPPORTED
    // Only the debugger cares to get a more precise answer. If there
    // is no debugger attached, just return false.
    if (!CORDebuggerAttached())
        return false;

    BYTE* IP = (BYTE*) (*(pContext->pPC));

    // Has the method been pitched? If it has, then of course we are
    // at a GC safe point.
    if (EconoJitManager::IsCodePitched(IP))
        return true;

    // Does the IP point to the start of a thunk? If so, then the
    // method was really moved/preserved, but we still know we were at
    // a GC safe point.
    if (EconoJitManager::IsThunk(IP))
        return true;
    
    // Else see if we are at a sequence point.
    DWORD dw = g_pDebugInterface->GetPatchedOpcode(IP);
    BYTE Opcode = *(BYTE*) &dw;
    return (Opcode == OPCODE_SEQUENCE_POINT);
#else // !DEBUGGING_SUPPORTED
    return false;
#endif // !DEBUGGING_SUPPORTED
}

/* report all the args (but not THIS if present), to the GC. 'framePtr' points at the 
   frame (promote doesn't assume anthing about its structure).  'msig' describes the
   arguments, and 'ctx' has the GC reporting info.  'stackArgsOffs' is the byte offset
   from 'framePtr' where the arguments start (args start at last and grow bacwards).
   Simmilarly, 'regArgsOffs' is the offset to find the register args to promote,
   This also handles the varargs case */
void promoteArgs(BYTE* framePtr, MetaSig* msig, GCCONTEXT* ctx, int stackArgsOffs, int regArgsOffs) {

    LPVOID pArgAddr;
    if (msig->GetCallingConvention() == IMAGE_CEE_CS_CALLCONV_VARARG) {
        // For varargs, look up the signature using the varArgSig token passed on the stack
        VASigCookie* varArgSig = *((VASigCookie**) (framePtr + stackArgsOffs));
        MetaSig varArgMSig(varArgSig->mdVASig, varArgSig->pModule);
        msig = &varArgMSig;

        ArgIterator argit(framePtr, msig, stackArgsOffs, regArgsOffs);
        while (NULL != (pArgAddr = argit.GetNextArgAddr()))
            msig->GcScanRoots(pArgAddr, ctx->f, ctx->sc);
        return;
    }
    
    ArgIterator argit(framePtr, msig, stackArgsOffs, regArgsOffs);
    while (NULL != (pArgAddr = argit.GetNextArgAddr()))
        msig->GcScanRoots(pArgAddr, ctx->f, ctx->sc);
}


/*
    Enumerate all live object references in that function using
    the virtual register set.
    Returns success of operation.
*/

static DWORD gcFlag[4] = {0, GC_CALL_INTERIOR, GC_CALL_PINNED, GC_CALL_INTERIOR | GC_CALL_PINNED}; 

bool Fjit_EETwain::EnumGcRefs(PREGDISPLAY     ctx,
                LPVOID          methodInfoPtr,
                ICodeInfo      *pCodeInfo,
                unsigned        pcOffset,
                unsigned        flags,
                GCEnumCallback  pCallback,      //@TODO: talk to Jeff about exact type
                LPVOID          hCallBack)

{
#ifdef _X86_

    DWORD*   breakPC = (DWORD*) *(ctx->pPC); 
    
    //unsigned pcOffset = (unsigned)relPCOffset;
    unsigned char* compressed = (unsigned char*) methodInfoPtr;
    Fjit_hdrInfo hdrInfo;
    //@TODO: there has to be a better way of getting the jitMgr at this point
    IJitManager* jitMgr = ExecutionManager::GetJitForType(miManaged_IL_EJIT);
    compressed += crackMethodInfoHdr(compressed, (SLOT)pCodeInfo, &hdrInfo);

    /* if execution aborting, we don't have to report any args or temps or regs
        so we are done */
    if ((flags & ExecutionAborted) && (flags & ActiveStackFrame) ) 
        return true;

    /* if in prolog or epilog, we must be the active frame and we must be aborting execution,
       since fjit runs uninterrupted only, and we wouldn't get here */
    if (((unsigned)pcOffset < hdrInfo.prologSize) || ((unsigned)pcOffset > (hdrInfo.methodSize-hdrInfo.epilogSize))) {
        _ASSERTE(!"interrupted in prolog/epilog and not aborting");
        return false;
    }

#ifdef _DEBUG
    LPCUTF8 cls, name = pCodeInfo->getMethodName(&cls);
    LOG((LF_GC, INFO3, "GC reporting from %s.%s\n",cls,name ));
#endif

    size_t* pFrameBase = (size_t*)(((size_t)*(ctx->pEbp))+prolog_bias);
    pFrameBase--;       // since stack grows down from here
    OBJECTREF* pArgPtr;
    /* TODO: when this class is moved into the same dll as fjit, clean this up
       For now, an ugly way of getting an instance of a FJit_Encoder
    */
    IFJitCompiler* fjit = (IFJitCompiler*) jitMgr->m_jit;
    FJit_Encode* encoder = fjit->getEncoder();

    /* report enregistered values, fjit only enregisters ESI */
    /* always report ESI since it is only used by new obj and is forced NULL when not in use */
    promote(pCallback,hCallBack, (OBJECTREF*)(ctx->pEsi), 0
#ifdef _DEBUG
      ,"ESI"  
#endif
        );

    /* report security object */
    OBJECTREF* pSecurityObject; pSecurityObject = (OBJECTREF*) (size_t)(((DWORD)*(ctx->pEbp))+prolog_bias+offsetof(prolog_data, security_obj));
    promote(pCallback,hCallBack, pSecurityObject, 0
#ifdef _DEBUG
      ,"Security object"  
#endif
        );


    unsigned EHClauseNestingDepth=0; 
    int indexCurrentClause = 0;
    // check if we can be in a EH clause
    if (hdrInfo.methodJitGeneratedLocalsSize > sizeof(void*)) {
        EHClauseNestingDepth = (unsigned)pFrameBase[JIT_GENERATED_LOCAL_NESTING_COUNTER];
        indexCurrentClause = EHClauseNestingDepth-1;
    }
    if (EHClauseNestingDepth)
    {
        /* first find out which handler we are in. if pFrameBase[2i+1] <= ctx->pEsp < pFrameBase[2i+3]
           then we are in the i'th handler */
        if ((indexCurrentClause >= 0) && 
            pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP - 2*indexCurrentClause] < ctx->Esp)
        { /* search for clause whose esp base is greater/equal to ctx->Esp */
            while (pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause] < ctx->Esp)
            { 
                indexCurrentClause--;
                if (indexCurrentClause < 0) 
                break;
            }
        }
        /* we are in the (index+1)'th handler */
        /* are we in a filter? */
        if (indexCurrentClause >= 0 && 
            (1 & pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause])) { // lowest bit set => filter 
            /* if there's been a localloc in the handler, use the value in the localloc slot for that handler */
            pArgPtr = (OBJECTREF*) (pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause-1] ? 
                                    pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause-1] :
                                    pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause]);
            pArgPtr = (OBJECTREF*) ((SSIZE_T) pArgPtr & -2);      // zero out the last bit
            /* now report the pending operands */
            unsigned num;
            num = encoder->decode_unsigned(&compressed);    //number of bytes of bits to read for non-interior locals
            compressed += num;
            num = encoder->decode_unsigned(&compressed);    //number of bytes of bits to read for interior locals
            compressed += num;
            num = encoder->decode_unsigned(&compressed);    //number of bytes of bits to read for non-interior pinned locals
            compressed += num;
            num = encoder->decode_unsigned(&compressed);    //number of bytes of bits to read for interior pinned locals
            compressed += num;
            goto REPORT_PENDING_ARGUMENTS;
        }
    }
    /* if we come here, we are not in a filter, so report all locals, arguments, etc. */
     
    /* report <this> for non-static methods */
     if (hdrInfo.hasThis) {
        /* fjit always places <this> on the stack, and it is always enregistered on the call */
        int offsetIntoArgumentRegisters;
        int numRegistersUsed = 0;
        if (!IsArgumentInRegister(&numRegistersUsed, ELEMENT_TYPE_CLASS, 0, TRUE, IMAGE_CEE_CS_CALLCONV_DEFAULT/*@todo: support varargs*/, &offsetIntoArgumentRegisters)) {
            _ASSERTE(!"this was not enregistered");
            return false;
        }
        pArgPtr = (OBJECTREF*)(((size_t)*(ctx->pEbp))+prolog_bias+offsetIntoArgumentRegisters);
        BOOL interior; interior = (pCodeInfo->getClassAttribs() & CORINFO_FLG_VALUECLASS) != 0;
        promote(pCallback,hCallBack, pArgPtr, (interior ? GC_CALL_INTERIOR : 0)
#ifdef _DEBUG
      ," THIS ptr "  
#endif
        );
     }

    {    // enumerate the argument list, looking for GC references
        
         // Note that I really want to say hCallBack is a GCCONTEXT, but this is pretty close
    
#ifdef _DEBUG
    extern void GcEnumObject(LPVOID pData, OBJECTREF *pObj, DWORD flags);
    _ASSERTE((void*) GcEnumObject == pCallback);
#endif
    GCCONTEXT   *pCtx = (GCCONTEXT *) hCallBack;

    // @TODO : Fix the EJIT to not use the MethodDesc directly.
    MethodDesc * pFD = (MethodDesc *)pCodeInfo->getMethodDesc_HACK();

    MetaSig msig(pFD->GetSig(),pFD->GetModule());
        
    if (msig.HasRetBuffArg()) {
        ArgIterator argit((BYTE*)(size_t) *ctx->pEbp, &msig, sizeof(prolog_frame),        // where the args start (skip varArgSig token)
                prolog_bias + offsetof(prolog_data, enregisteredArg_2)); // where the register args start 
        promote(pCallback, hCallBack, (OBJECTREF*) argit.GetRetBuffArgAddr(), GC_CALL_INTERIOR
#ifdef _DEBUG
      ,"RetBuffArg"  
#endif
        );
    }

    promoteArgs((BYTE*)(size_t) *ctx->pEbp, &msig, pCtx,
                sizeof(prolog_frame),                                    // where the args start (skip varArgSig token)
                prolog_bias + offsetof(prolog_data, enregisteredArg_2)); // where the register args start 
    }

#ifdef _DEBUG
    /* compute the TOS so we can detect spinning off the top */
    unsigned pTOS; pTOS = ctx->Esp;
#endif //_DEBUG

    /* report locals 
       NOTE: fjit lays out the locals in the same order as specified in the il, 
              sizes of locals are (I4,I,REF = sizeof(void*); I8 = 8)
       */

    //number of void* sized words in the locals
    unsigned num_local_words; num_local_words= hdrInfo.methodFrame -  ((hdrInfo.methodJitGeneratedLocalsSize+sizeof(prolog_data))/sizeof(void*));
    
    //point to start of the locals frame, note that the locals grow down from here on x86
    pArgPtr = (OBJECTREF*)(((size_t)*(ctx->pEbp))+prolog_bias-hdrInfo.methodJitGeneratedLocalsSize);
    
    /* at this point num_local_words is the number of void* words defined by the local tail sig
       and pArgPtr points to the start of the tail sig locals
       and compressed points to the compressed form of the localGCRef booleans
       */

    //handle gc refs and interiors in the tail sig (compressed localGCRef booleans followed by interiorGC booleans followed by pinned GC and pinned interior GC)
    bool interior;
    interior = false;
    OBJECTREF* pLocals;
    pLocals = pArgPtr;
    unsigned i;
    for (i = 0; i<4; i++) {
        unsigned num;
        unsigned char bits;
        OBJECTREF* pReport;
        pArgPtr = pLocals-1;    //-1 since the stack grows down on x86
        pReport = pLocals-1;    // ...ditto
        num = encoder->decode_unsigned(&compressed);    //number of bytes of bits to read
        while (num > 1) {
            pReport = pArgPtr;
            bits = *compressed++;
            while (bits > 0) {
                if (bits & 1) {
                    //report the gc ref
          promote(pCallback,hCallBack, pReport, gcFlag[i]
#ifdef _DEBUG
                    ,"LOCALS"  
#endif
                        );
                }
                pReport--;
                bits = bits >> 1;
            }
            num--;
            pArgPtr -=8;
        }
        bits = 0;
        if (num) bits = *compressed++;
        while (bits > 0) {
            if (bits & 1) {
                //report the gc ref
                promote(pCallback,hCallBack, pArgPtr, gcFlag[i]
#ifdef _DEBUG
                    ,"LOCALS"  
#endif
                        );
            }
            pArgPtr--;
            bits = bits >> 1;
        }
    } ;
    
    if (EHClauseNestingDepth && indexCurrentClause >= 0) { 
        /* we are in a finally/catch handler.
           NOTE: This code relies on the assumption that the pending argument stack is empty
           at the start of a try
        */
        /* we are in a handler, but not a nested handler because of the check already made */
        pArgPtr = (OBJECTREF*) (pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause-1] ? 
                                pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause-1] :
                                pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*indexCurrentClause]);
                     // NOTE: the pending operands before the handler is entered is dead and is not reported
    }
    else { /* we are not in a handler, but there might have been a localloc */
        pArgPtr =  (pFrameBase[JIT_GENERATED_LOCAL_LOCALLOC_OFFSET] ?   // was there a localloc
                    (OBJECTREF*) pFrameBase[JIT_GENERATED_LOCAL_LOCALLOC_OFFSET] :          // yes, use the value in this slot as the operand base
                    pLocals - num_local_words); // no, the base operand is past the locals
    }
REPORT_PENDING_ARGUMENTS:

    /* report pending args on the operand stack (those not covered by the call itself)
       Note: at this point pArgPtr points to the start of the operand stack, 
             on x86 it grows down from here.
             compressed points to start of labeled stacks */
    /* read down to the nearest labeled stack that is before currect pcOffset */
    struct labeled_stacks {
        unsigned pcOffset;
        unsigned int stackToken;
    };
    //set up a zero stack as the current entry
    labeled_stacks current;
    current.pcOffset = 0;
    current.stackToken = 0;
    unsigned size = encoder->decode_unsigned(&compressed);  //#bytes in labeled stacks
    unsigned char* nextTableStart  = compressed + size;      //start of compressed stacks

    if (flags & ExecutionAborted)  // if we have been interrupted we don't have to report pending arguments
        goto INTEGRITY_CHECK;      // since we don't support resumable exceptions

    unsigned num; num= encoder->decode_unsigned(&compressed);   //#labeled entries
    while (num--) {
        labeled_stacks next;
        next.pcOffset = encoder->decode_unsigned(&compressed);
        next.stackToken = encoder->decode_unsigned(&compressed);
        if (next.pcOffset >= pcOffset) break;
        current = next;
    }
    compressed = nextTableStart;
    size = encoder->decode_unsigned(&compressed);           //#bytes in compressed stacks
#ifdef _DEBUG
    //point past the stacks, in case we have more data for debug
    nextTableStart = compressed + size; 
#endif //_DEBUG             
    if (current.stackToken) {
        //we have a non-empty stack, so we have to walk it
        compressed += current.stackToken;                           //point to the compressed stack
        //handle gc refs and interiors in the tail sig (compressed localGCRef booleans followed by interiorGC booleans)
        bool interior = false;
        DWORD gcFlag = 0;

        OBJECTREF* pLocals = pArgPtr;
        do {
            unsigned num;
            unsigned char bits;
            OBJECTREF* pReport;
            pArgPtr = pLocals-1;    //-1 since the stack grows down on x86
            pReport = pLocals-1;    // ...ditto
            num = encoder->decode_unsigned(&compressed);    //number of bytes of bits to read
            while (num > 1) {
                bits = *compressed++;
                while (bits > 0) {
                    if (bits & 1) {
                        //report the gc ref
                        promote(pCallback,hCallBack, pReport, gcFlag
#ifdef _DEBUG
                    ,"PENDING ARGUMENT"  
#endif
                        );
                    }
                    pReport--;
                    bits = bits >> 1;
                }
                num--;
                pArgPtr -=8;
                pReport = pArgPtr;
            }
            bits = 0;
            if (num) bits = *compressed++;
            while (bits > 0) {
                if (bits & 1) {
                    //report the gc ref
                    promote(pCallback,hCallBack, pArgPtr, gcFlag
#ifdef _DEBUG
                    ,"PENDING ARGUMENT"  
#endif
                        );
                }
                pArgPtr--;
                bits = bits >> 1;
            }
            interior = !interior;
            gcFlag = GC_CALL_INTERIOR;
        } while (interior);
    }

INTEGRITY_CHECK:    

#ifdef _DEBUG
    /* for an intergrity check, we placed a map of pc offsets to an il offsets at the end,
        lets try and map the current pc of the method*/
    compressed = nextTableStart;
    encoder->decompress(compressed);
    unsigned pcInILOffset;
    //if we are not the active stack frame, we must be in a call instr, back up the pc so the we return the il of the call
    // However, we have to watch out for synchronized methods.  The call to synchronize
    // is the last instruction of the prolog.  The instruction we return to is legitimately
    // outside the prolog.  We don't want to back up into the prolog to sit on that call.

//  signed ilOffset = encoder->ilFromPC((flags & ActiveStackFrame) ? pcOffset : pcOffset-1, &pcInILOffset);
    signed ilOffset = encoder->ilFromPC((unsigned)pcOffset, &pcInILOffset);

    //fix up the pcInILOffset if necessary
    pcInILOffset += (flags & ActiveStackFrame) ? 0 : 1;
    if (ilOffset < 0 ) {
        _ASSERTE(!"bad il offset");
        return false;
    }
#endif //_DEBUG

    delete encoder;

    return true;
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - EnumGcRefs (FJIT_EETwain.cpp)");
    return false;
#endif // _X86_
}

/*
    Return the address of the local security object reference
    (if available).
*/
OBJECTREF* Fjit_EETwain::GetAddrOfSecurityObject(
                PREGDISPLAY     ctx,
                LPVOID          methodInfoPtr,
                size_t          relOffset,
                CodeManState   *pState)

{
#ifdef _X86_
    unsigned char* compressed = (unsigned char*) methodInfoPtr;
    Fjit_hdrInfo hdrInfo;
    crackMethodInfoHdr(compressed,  (SLOT)relOffset, &hdrInfo);

    //if (!hdrInfo.hasSecurity)
    //  return NULL;

    return (OBJECTREF*)(((size_t)*ctx->pEbp)+prolog_bias+offsetof(prolog_data, security_obj));
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - GetAddrOfSecurityObject (FJIT_EETwain.cpp)");
    return NULL;
#endif // _X86_
}

/*
    Returns "this" pointer if it is a non-static method AND
    the object is still alive.
    Returns NULL in all other cases.
*/
OBJECTREF Fjit_EETwain::GetInstance(
                PREGDISPLAY     ctx,
                LPVOID          methodInfoPtr,
                ICodeInfo      *pCodeInfo,
                size_t          relOffset)

{
#ifdef _X86_
    unsigned char* compressed = (unsigned char*) methodInfoPtr;
    Fjit_hdrInfo hdrInfo;
    crackMethodInfoHdr(compressed, (SLOT)relOffset, &hdrInfo);

    if(!hdrInfo.hasThis) 
        return NULL;
    //note, this is always the first enregistered
    return (OBJECTREF) * (size_t*) (((size_t)(*ctx->pEbp)+prolog_bias+offsetof(prolog_data, enregisteredArg_1)));
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - GetInstance (FJIT_EETwain.cpp)");
    return NULL;
#endif // _X86_
}

/*
  Returns true if the given IP is in the given method's prolog or an epilog.
*/
bool Fjit_EETwain::IsInPrologOrEpilog(
                BYTE*  pcOffset,
                LPVOID methodInfoPtr,
                size_t* prologSize)

{
    unsigned char* compressed = (unsigned char*) methodInfoPtr;
    Fjit_hdrInfo hdrInfo;
    if (EconoJitManager::IsInStub(pcOffset, FALSE))
        return true;
    crackMethodInfoHdr(compressed, pcOffset, &hdrInfo);  //@todo WIN64 - is pcOffset truely an offset or is it an address?
    _ASSERTE((SIZE_T)pcOffset < hdrInfo.methodSize);
    if (((SIZE_T)pcOffset <= hdrInfo.prologSize) || ((SIZE_T)pcOffset > (hdrInfo.methodSize-hdrInfo.epilogSize))) {
        return true;
    }
    return false;
}

/*
  Returns the size of a given function.
*/
size_t Fjit_EETwain::GetFunctionSize(
                LPVOID methodInfoPtr)
{
    unsigned char* compressed = (unsigned char*) methodInfoPtr;
    Fjit_hdrInfo hdrInfo;
    crackMethodInfoHdr(compressed, 0, &hdrInfo);
    return hdrInfo.methodSize;
}

/*
  Returns the size of the frame of the function.
*/
unsigned int Fjit_EETwain::GetFrameSize(
                LPVOID methodInfoPtr)
{
    unsigned char* compressed = (unsigned char*) methodInfoPtr;
    Fjit_hdrInfo hdrInfo;
    crackMethodInfoHdr(compressed, 0, &hdrInfo);
    return hdrInfo.methodFrame;
}

/**************************************************************************************/
// returns the address of the most deeply nested finally and the IP is currently in that finally.
const BYTE* Fjit_EETwain::GetFinallyReturnAddr(PREGDISPLAY pReg)
{
#ifdef _X86_
    // @TODO: won't work if there is a localloc inside finally
    return *(const BYTE**)(size_t)(pReg->Esp);
#else
    _ASSERTE( !"EconoJitManager::GetFinallyReturnAddr NYI!");
    return NULL;
#endif    
}

//@todo SETIP
BOOL Fjit_EETwain::IsInFilter(void *methodInfoPtr,
                                 unsigned offset,    
                                 PCONTEXT pCtx,
                                 DWORD nestingLevel)
{
#ifdef _X86_
    //_ASSERTE(nestingLevel > 0);
    size_t* pFrameBase = (size_t*)(((size_t) pCtx->Ebp)+prolog_bias);
    pFrameBase -= (1+ nestingLevel*2);       // 1 because the stack grows down, there are two slots per EH clause

#ifdef _DEBUG
    if (*pFrameBase & 1)  
        LOG((LF_CORDB, LL_INFO10000, "EJM::IsInFilter")); 
#endif 
    return ((*pFrameBase & 1) != 0);                // the lsb indicates if this is a filter or not
#else // !_X86_
    _ASSERTE(!"@todo - port");
    return FALSE;
#endif // _X86_
}        

// Only called in normal path, not during exception
BOOL Fjit_EETwain::LeaveFinally(void *methodInfoPtr,
                                   unsigned offset,    
                                   PCONTEXT pCtx,
                                   DWORD nestingLevel    // = current nesting level = most deeply nested finally
                                   )
{
#ifdef _X86_

    _ASSERTE(nestingLevel > 0);
    size_t* pFrameBase = (size_t*)(((size_t) pCtx->Ebp)+prolog_bias - sizeof(void*)); // 1 because the stack grows down
    size_t ctr = pFrameBase[JIT_GENERATED_LOCAL_NESTING_COUNTER];
    pFrameBase[JIT_GENERATED_LOCAL_NESTING_COUNTER] = --ctr;

    // zero out the base esp and localloc slots
    pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*ctr] = 0;
    pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*ctr-1] = 0;

    // @TODO: Won't work if there is a localloc inside finally
    pCtx->Esp += sizeof(void*);
    LOG((LF_CORDB, LL_INFO10000, "EJM::LeaveFinally: fjit sets esp to "
        "0x%x, was 0x%x\n", pCtx->Esp, pCtx->Esp - sizeof(void*)));
    return TRUE;
#else
    _ASSERTE( !"EconoJitManager::LeaveFinally NYI!");
    return FALSE;
#endif    
}

//@todo SETIP
void Fjit_EETwain::LeaveCatch(void *methodInfoPtr,
                                  unsigned offset,    
                                  PCONTEXT pCtx)
{    
#ifdef _X86_
    size_t* pFrameBase = (size_t*)(((size_t) pCtx->Ebp)+prolog_bias - sizeof(void*) /* 1 for localloc, 1 because the stack grows down*/);
    size_t ctr = pFrameBase[JIT_GENERATED_LOCAL_NESTING_COUNTER];

    pFrameBase[JIT_GENERATED_LOCAL_NESTING_COUNTER] = --ctr;
    
    // set esp to right value depending on whether there was a localloc or not
    pCtx->Esp = (DWORD) ( pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*(ctr-1)-1] ? 
                    pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*(ctr-1)-1] :
                    pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*(ctr-1)]
                    );

    // zero out the base esp and localloc slots
    pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*ctr] = 0;
    pFrameBase[JIT_GENERATED_LOCAL_FIRST_ESP-2*ctr-1] = 0;


    return;
#else // !_X86_
    _ASSERTE(!"@todo port");
    return;
#endif // _X86_
}

HRESULT Fjit_EETwain::JITCanCommitChanges(LPVOID methodInfoPtr,
                                    DWORD oldMaxEHLevel,
                                    DWORD newMaxEHLevel)
{
    // This will be more fleshed out as we add things here. :)
    if(oldMaxEHLevel != newMaxEHLevel)
    {
        return CORDBG_E_ENC_EH_MAX_NESTING_LEVEL_CANT_INCREASE;
    }

    return S_OK;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\fjit_eetwain.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#ifndef _FJIT_EETWAIN_H
#define _FJIT_EETWAIN_H

#include "eetwain.h"

//@TODO: move this into the FJIT dll. When that is done, the following can be eliminated
#include "corjit.h"
#include "..\fjit\IFJitCompiler.h"
#ifdef _X86_
#define MAX_ENREGISTERED 2
#endif
// end todo stuff

class Fjit_EETwain : public EECodeManager{

public:
/* looks like we use the one in the super class
virtual bool FilterException (
                PCONTEXT        pContext,
                unsigned        win32Fault,
                LPVOID          methodInfoPtr,
                LPVOID          methodStart);
*/				

/*
    Last chance for the runtime support to do fixups in the context
    before execution continues inside a filter, catch handler, or finally
*/
virtual void FixContext(
                ContextType     ctxType,
                EHContext      *ctx,
                LPVOID          methodInfoPtr,
                LPVOID          methodStart,
                DWORD           nestingLevel,
                OBJECTREF       thrownObject,
                CodeManState   *pState,
                size_t       ** ppShadowSP,             // OUT
                size_t       ** ppEndRegion);           // OUT

/*
    Last chance for the runtime support to do fixups in the context
    before execution continues inside an EnC updated function.
*/
virtual EnC_RESULT FixContextForEnC(void           *pMethodDescToken,
                                    PCONTEXT        ctx,
                                    LPVOID          oldMethodInfoPtr,
                                    SIZE_T          oldMethodOffset,
               const ICorDebugInfo::NativeVarInfo * oldMethodVars,
                                    SIZE_T          oldMethodVarsCount,
                                    LPVOID          newMethodInfoPtr,
                                    SIZE_T          newMethodOffset,
               const ICorDebugInfo::NativeVarInfo * newMethodVars,
                                    SIZE_T          newMethodVarsCount);


/*
    Unwind the current stack frame, i.e. update the virtual register
    set in pContext. This will be similar to the state after the function
    returns back to caller (IP points to after the call, Frame and Stack
    pointer has been reset, callee-saved registers restored 
    (if UpdateAllRegs), callee-UNsaved registers are trashed)
    Returns success of operation.
*/
virtual bool UnwindStackFrame(
                PREGDISPLAY     pContext,
                LPVOID          methodInfoPtr,
                ICodeInfo      *pCodeInfo,
                unsigned        flags,
				CodeManState   *pState);

typedef void (*CREATETHUNK_CALLBACK)(IJitManager* jitMgr,
                                     LPVOID* pHijackLocation,
                                     ICodeInfo *pCodeInfo
                                     );

static void HijackHandlerReturns(PREGDISPLAY     ctx,
                                        LPVOID          methodInfoPtr,
                                        ICodeInfo      *pCodeInfo,
                                        IJitManager*    jitmgr,
                                        CREATETHUNK_CALLBACK pCallBack
                                        );

/*
    Is the function currently at a "GC safe point" ?
    Can call EnumGcRefs() successfully
*/
virtual bool IsGcSafe(  PREGDISPLAY     pContext,
                LPVOID          methodInfoPtr,
                ICodeInfo      *pCodeInfo,
                unsigned        flags);

/*
    Enumerate all live object references in that function using
    the virtual register set. Same reference location cannot be enumerated 
    multiple times (but all differenct references pointing to the same
    object have to be individually enumerated).
    Returns success of operation.
*/
virtual bool EnumGcRefs(PREGDISPLAY     pContext,
                LPVOID          methodInfoPtr,
                ICodeInfo      *pCodeInfo,
                unsigned        pcOffset,
                unsigned        flags,
                GCEnumCallback  pCallback,
                LPVOID          hCallBack);

/*
    Return the address of the local security object reference
    (if available).
*/
virtual OBJECTREF* GetAddrOfSecurityObject(
                PREGDISPLAY     pContext,
                LPVOID          methodInfoPtr,
                unsigned        relOffset,
				CodeManState   *pState);

/*
    Returns "this" pointer if it is a non-static method AND
    the object is still alive.
    Returns NULL in all other cases.
*/
virtual OBJECTREF GetInstance(
                PREGDISPLAY     pContext,
                LPVOID          methodInfoPtr,
                ICodeInfo      *pCodeInfo,
                unsigned        relOffset);

/*
  Returns true if the given IP is in the given method's prolog or an epilog.
*/
virtual bool IsInPrologOrEpilog(
                BYTE*  pcOffset,
                LPVOID methodInfoPtr,
                size_t* prologSize);

/*
  Returns the size of a given function.
*/
virtual size_t GetFunctionSize(
                LPVOID methodInfoPtr);

/*
  Returns the size of the frame (barring localloc)
*/
virtual unsigned int GetFrameSize(
                LPVOID methodInfoPtr);

virtual const BYTE* GetFinallyReturnAddr(PREGDISPLAY pReg);
virtual BOOL LeaveFinally(void *methodInfoPtr,
                          unsigned offset,    
                          PCONTEXT pCtx,
                          DWORD curNestLevel);
virtual BOOL IsInFilter(void *methodInfoPtr,
                        unsigned offset,    
                        PCONTEXT pCtx,
                        DWORD nestingLevel);
virtual void LeaveCatch(void *methodInfoPtr,
                         unsigned offset,    
                         PCONTEXT pCtx);

virtual HRESULT JITCanCommitChanges(LPVOID methodInfoPtr,
                              DWORD oldMaxEHLevel,
                              DWORD newMaxEHLevel);
};



#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\frames.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*  FRAMES.CPP:
 *
 */

#include "common.h"
#include "log.h"
#include "frames.h"
#include "threads.h"
#include "object.h"
#include "method.hpp"
#include "class.h"
#include "excep.h"
#include "security.h"
#include "stublink.h"
#include "comcall.h"
#include "nstruct.h"
#include "objecthandle.h"
#include "siginfo.hpp"
#include "comstringbuffer.h"
#include "gc.h"
#include "nexport.h"
#include "COMVariant.h"
#include "stackwalk.h"
#include "DbgInterface.h"
#include "gms.h"
#include "EEConfig.h"
#include "remoting.h"
#include "ecall.h"
#include "marshaler.h"
#include "clsload.hpp"

#if CHECK_APP_DOMAIN_LEAKS
#define CHECK_APP_DOMAIN    GC_CALL_CHECK_APP_DOMAIN
#else
#define CHECK_APP_DOMAIN    0
#endif

#ifndef NUM_ARGUMENT_REGISTERS
#define DEFINE_ARGUMENT_REGISTER_NOTHING
#include "eecallconv.h"
#endif
#if NUM_ARGUMENT_REGISTERS != 2
#pragma message("ComMethodFrame::PromoteCalleeStack() only handles one enregistered arg.")
#endif


#ifdef _DEBUG
#define OBJECTREFToBaseObject(objref)     (*( (Object **) &(objref) ))
#define BaseObjectToOBJECTREF(obj)        (OBJECTREF((obj),0))
#else
#define OBJECTREFToBaseObject(objref)       (objref)
#define BaseObjectToOBJECTREF(obj)          ((OBJECTREF)(obj))
#endif

#if _DEBUG
unsigned dbgStubCtr = 0;
unsigned dbgStubTrip = 0xFFFFFFFF;

void Frame::Log() {
    dbgStubCtr++;
    if (dbgStubCtr > dbgStubTrip) {
        dbgStubCtr++;      // basicly a nop to put a breakpoint on.
    }

    MethodDesc* method = GetFunction();
	STRESS_LOG3(LF_STUBS, LL_INFO10000, "STUBS: In Stub with Frame %p assoc Method %pM FrameType = %pV\n", this, method, *((void**) this));

    if (!LoggingOn(LF_STUBS, LL_INFO10000))
        return;

    char buff[64];
    char* frameType;
    if (GetVTablePtr() == PrestubMethodFrame::GetMethodFrameVPtr())
        frameType = "PreStub";
    else if (GetVTablePtr() == NDirectMethodFrameGeneric::GetMethodFrameVPtr() ||
             GetVTablePtr() == NDirectMethodFrameSlim::GetMethodFrameVPtr() ||
             GetVTablePtr() == NDirectMethodFrameStandalone::GetMethodFrameVPtr() ||
             GetVTablePtr() == NDirectMethodFrameStandaloneCleanup::GetMethodFrameVPtr()
             ) {
        // Right now, compiled COM interop stubs actually build NDirect frames
        // so have to test for this separately
        if (method->IsNDirect())
        {
            sprintf(buff, "PInvoke target 0x%x", ((NDirectMethodDesc*) method)->GetNDirectTarget());
            frameType = buff;
        }
        else
        {
            frameType = "Interop";
        }
    }
    else if (GetVTablePtr() == ECallMethodFrame::GetMethodFrameVPtr())
        frameType = "ECall";
    else if (GetVTablePtr() == PInvokeCalliFrame::GetMethodFrameVPtr()) {
        sprintf(buff, "PInvoke CALLI target 0x%x", ((PInvokeCalliFrame*)this)->NonVirtual_GetPInvokeCalliTarget());
        frameType = buff;
    }
    else 
        frameType = "Unknown";

    if (method != 0)
        LOG((LF_STUBS, LL_INFO10000, "IN %s Stub Method = %s::%s SIG %s ESP of return = 0x%x\n", frameType, 
        method->m_pszDebugClassName,
        method->m_pszDebugMethodName,
        method->m_pszDebugMethodSignature,
        GetReturnAddressPtr()));
    else 
        LOG((LF_STUBS, LL_INFO10000, "IN %s Stub Method UNKNOWN ESP of return = 0x%x\n", frameType, GetReturnAddressPtr()));

    _ASSERTE(GetThread()->PreemptiveGCDisabled());
}

    // returns TRUE if retAddr, is a return address that can call managed code
bool isLegalManagedCodeCaller(void* retAddr) {

#ifdef _X86_

        // we expect to be called from JITTED code or from special code sites inside
        // mscorwks like callDescr which we have put a NOP (0x90) so we know that they
        // are specially blessed.   See vancem for details
    if (retAddr != 0 && ExecutionManager::FindJitMan(SLOT(retAddr)) == 0 && ((*((BYTE*) retAddr) != 0x90) &&
                                                                             (*((BYTE*) retAddr) != 0xcc)))
    {
        LOG((LF_GC, LL_INFO10, "Bad caller to managed code: retAddr=0x%08x, *retAddr=0x%x\n",
             retAddr, *((BYTE*) retAddr)));
        
        _ASSERTE(!"Bad caller to managed code");
    }

        // it better be a return address of some kind 
	size_t dummy;
	if (isRetAddr(size_t(retAddr), &dummy))
		return true;

			// The debugger could have dropped an INT3 on the instruction that made the call
			// Calls can be 2 to 7 bytes long
	if (CORDebuggerAttached()) {
		BYTE* ptr = (BYTE*) retAddr;
		for (int i = -2; i >= -7; --i)
			if (ptr[i] == 0xCC)
				return true;
		return true;
	}

	_ASSERTE("Bad return address on stack");
#endif
	return true;
}

#endif

//-----------------------------------------------------------------------
// Link and Unlink this frame.
//-----------------------------------------------------------------------
VOID Frame::Push()
{
    Push(GetThread());
}
VOID Frame::Push(Thread *pThread)
{
    m_Next = pThread->GetFrame();
    pThread->SetFrame(this);
}

VOID Frame::Pop()
{
    _ASSERTE(GetThread()->GetFrame() == this && "Popping a frame out of order ?");
    Pop(GetThread());
}
VOID Frame::Pop(Thread *pThread)
{
    _ASSERTE(pThread->GetFrame() == this && "Popping a frame out of order ?");
    pThread->SetFrame(m_Next);
}

//---------------------------------------------------------------
// Get the offset of the stored "this" pointer relative to the frame.
//---------------------------------------------------------------
/*static*/ int FramedMethodFrame::GetOffsetOfThis()
{
    int offsetIntoArgumentRegisters;
    int numRegistersUsed = 0;
    // !!! Abstraction violation. Not passing the proper callconv to IsArgumentInRegister because
    // we don't have a specific frame to query. This only works because all callconv's use the same register for
    // "this"
    if (IsArgumentInRegister(&numRegistersUsed, ELEMENT_TYPE_CLASS, 0, TRUE, IMAGE_CEE_CS_CALLCONV_DEFAULT, &offsetIntoArgumentRegisters)) {
        return FramedMethodFrame::GetOffsetOfArgumentRegisters() + offsetIntoArgumentRegisters;
    } else {
        return (int)(sizeof(FramedMethodFrame));
    }
}


//-----------------------------------------------------------------------
// If an exception unwinds a FramedMethodFrame that is marked as synchronized,
// we need to leave the Object Monitor on the way.
//-----------------------------------------------------------------------
VOID
FramedMethodFrame::UnwindSynchronized()
{
    _ASSERTE(GetFunction()->IsSynchronized());

    MethodDesc    *pMD = GetFunction();
    OBJECTREF      orUnwind = 0;

    if (pMD->IsStatic())
    {
        EEClass    *pClass = pMD->GetClass();
        orUnwind = pClass->GetExposedClassObject();
    }
    else
    {
        orUnwind = GetThis();
    }

    _ASSERTE(orUnwind);
    if (orUnwind != NULL)
        orUnwind->LeaveObjMonitorAtException();
}


//-----------------------------------------------------------------------
// A rather specialized routine for the exclusive use of the PreStub.
//-----------------------------------------------------------------------
VOID
PrestubMethodFrame::Push()
{
    Thread *pThread = GetThread();

    // Initializes the frame's VPTR. This assumes C++ puts the vptr
    // at offset 0 for a class not using MI, but this is no different
    // than the assumption that COM Classic makes.
    *((LPVOID*)this) = GetMethodFrameVPtr();

    // Link frame into the chain.
    m_Next = pThread->GetFrame();
    pThread->SetFrame(this);

}

BOOL PrestubMethodFrame::TraceFrame(Thread *thread, BOOL fromPatch,
                                    TraceDestination *trace, REGDISPLAY *regs)
{
    //
    // We want to set a frame patch, unless we're already at the
    // frame patch, in which case we'll trace addrof_code which 
    // should be set by now.
    //

    trace->type = TRACE_STUB;
    if (fromPatch)
        trace->address = GetFunction()->GetUnsafeAddrofCode();
    else
        trace->address = ThePreStub()->GetEntryPoint();

    LOG((LF_CORDB, LL_INFO10000,
         "PrestubMethodFrame::TraceFrame: ip=0x%08x\n", trace->address));
    
    return TRUE;
}

BOOL SecurityFrame::TraceFrame(Thread *thread, BOOL fromPatch,
                               TraceDestination *trace, REGDISPLAY *regs)
{
    //
    // We should only be called when we're in security code (i.e., in
    // DoDeclarativeActions. We're also claiming that the security
    // stub will only do work _before_ calling the true stub and not
    // after, so we know the wrapped stub hasn't been called yet and
    // we'll be able to trace to it. If this were to get called after
    // the wrapped stub had been called, then we're try to trace to it
    // again and never hit the new patch.
    //
    _ASSERTE(!fromPatch);

    //
    // We're assuming that the security frame is a) the only interceptor
    // or b) at least the first one. This is always true for V1.
    //
    MethodDesc *pMD = GetFunction();
    BYTE *prestub = (BYTE*) pMD - METHOD_CALL_PRESTUB_SIZE;
    INT32 stubOffset = *((UINT32*)(prestub+1));
    const BYTE* pStub = prestub + METHOD_CALL_PRESTUB_SIZE + stubOffset;
    Stub *stub = Stub::RecoverStub(pStub);

    //
    // This had better be an intercept stub, since that what we wanted!
    //
    _ASSERTE(stub->IsIntercept());
    
    while (stub->IsIntercept())
    {
        //
        // Grab the wrapped stub.
        //
        InterceptStub *is = (InterceptStub*)stub;
        if (*is->GetInterceptedStub() == NULL)
        {
            trace->type = TRACE_STUB;
            trace->address = *is->GetRealAddr();
            return TRUE;
        }

        stub = *is->GetInterceptedStub();
    }

    //
    // The wrapped sub better not be another interceptor. (See the
    // comment above.)
    //
    _ASSERTE(!stub->IsIntercept());
    
    LOG((LF_CORDB, LL_INFO10000,
         "SecurityFrame::TraceFrame: intercepted "
         "stub=0x%08x, ep=0x%08x\n",
         stub, stub->GetEntryPoint()));

    trace->type = TRACE_STUB;
    trace->address = stub->GetEntryPoint();
    
    return TRUE;
}

Frame::Interception PrestubMethodFrame::GetInterception()
{
    //
    // The only direct kind of interception done by the prestub 
    // is class initialization.
    //

    return INTERCEPTION_CLASS_INIT;
}

Frame::Interception InterceptorFrame::GetInterception()
{
    // The SecurityDesc gets set just before calling the intercepted target
    // We may have turned on preemptive-GC for SendEvent(). So cast away the OBJECTREF

    bool isNull = (NULL == *(size_t*)GetAddrOfSecurityDesc());

    return (isNull ? INTERCEPTION_SECURITY : INTERCEPTION_NONE);
}

//-----------------------------------------------------------------------
// A rather specialized routine for the exclusive use of the COM PreStub.
//-----------------------------------------------------------------------
VOID
ComPrestubMethodFrame::Push()
{
    Thread *pThread = GetThread();

    // Initializes the frame's VPTR. This assumes C++ puts the vptr
    // at offset 0 for a class not using MI, but this is no different
    // than the assumption that COM Classic makes.
    *((LPVOID*)this) = GetMethodFrameVPtr();

    // Link frame into the chain.
    m_Next = pThread->GetFrame();
    pThread->SetFrame(this);
}


//-----------------------------------------------------------------------
// GCFrames
//-----------------------------------------------------------------------


//--------------------------------------------------------------------
// This constructor pushes a new GCFrame on the frame chain.
//--------------------------------------------------------------------
GCFrame::GCFrame(OBJECTREF *pObjRefs, UINT numObjRefs, BOOL maybeInterior)
{
    Init(GetThread(), pObjRefs, numObjRefs, maybeInterior);
}

void GCFrame::Init(Thread *pThread, OBJECTREF *pObjRefs, UINT numObjRefs, BOOL maybeInterior)
{
#ifdef _DEBUG
    if (!maybeInterior) {
        for(UINT i = 0; i < numObjRefs; i++)
            Thread::ObjectRefProtected(&pObjRefs[i]);
        
        for (i = 0; i < numObjRefs; i++) {
            pObjRefs[i]->Validate();
        }
    }

    if (g_pConfig->GetGCStressLevel() != 0 && IsProtectedByGCFrame(pObjRefs)) {
        _ASSERTE(!"This objectref is already protected by a GCFrame. Protecting it twice will corrupt the GC.");
    }

#endif

    m_pObjRefs      = pObjRefs;
    m_numObjRefs    = numObjRefs;
    m_pCurThread    = pThread;
    m_MaybeInterior = maybeInterior;
    m_Next          = m_pCurThread->GetFrame();
    m_pCurThread->SetFrame(this);
}



//
// GCFrame Object Scanning
//
// This handles scanning/promotion of GC objects that were
// protected by the programmer explicitly protecting it in a GC Frame
// via the GCPROTECTBEGIN / GCPROTECTEND facility...
//

void GCFrame::GcScanRoots(promote_func *fn, ScanContext* sc)
{
    Object **pRefs;

    pRefs = (Object**) m_pObjRefs;

    for (UINT i = 0;i < m_numObjRefs; i++)  {

        LOG((LF_GC, INFO3, "GC Protection Frame Promoting %x to ", m_pObjRefs[i] ));
        if (m_MaybeInterior)
            PromoteCarefully(fn, pRefs[i], sc, GC_CALL_INTERIOR|CHECK_APP_DOMAIN);
        else
            (*fn)(pRefs[i], sc);
        LOG((LF_GC, INFO3, "%x\n", m_pObjRefs[i] ));
    }
}


//--------------------------------------------------------------------
// Pops the GCFrame and cancels the GC protection.
//--------------------------------------------------------------------
VOID GCFrame::Pop()
{
    m_pCurThread->SetFrame(m_Next);
#ifdef _DEBUG
    m_pCurThread->EnableStressHeap();
    for(UINT i = 0; i < m_numObjRefs; i++)
        Thread::ObjectRefNew(&m_pObjRefs[i]);       // Unprotect them
#endif
}

#ifdef _DEBUG

struct IsProtectedByGCFrameStruct
{
    OBJECTREF       *ppObjectRef;
    UINT             count;
};

static StackWalkAction IsProtectedByGCFrameStackWalkFramesCallback(
    CrawlFrame      *pCF,
    VOID            *pData
)
{
    IsProtectedByGCFrameStruct *pd = (IsProtectedByGCFrameStruct*)pData;
    Frame *pFrame = pCF->GetFrame();
    if (pFrame) {
        if (pFrame->Protects(pd->ppObjectRef)) {
            pd->count++;
        }
    }
    return SWA_CONTINUE;
}

BOOL IsProtectedByGCFrame(OBJECTREF *ppObjectRef)
{
    // Just report TRUE if GCStress is not on.  This satisfies the asserts that use this
    // code without the cost of actually determining it.
    if (g_pConfig->GetGCStressLevel() == 0)
        return TRUE;

    if (!pThrowableAvailable(ppObjectRef)) {
        return TRUE;
    }
    IsProtectedByGCFrameStruct d = {ppObjectRef, 0};
    GetThread()->StackWalkFrames(IsProtectedByGCFrameStackWalkFramesCallback, &d);
    if (d.count > 1) {
        _ASSERTE(!"Multiple GCFrames protecting the same pointer. This will cause GC corruption!");
    }
    return d.count != 0;
}
#endif

void ProtectByRefsFrame::GcScanRoots(promote_func *fn, ScanContext *sc)
{
    ByRefInfo *pByRefInfos = m_brInfo;
    while (pByRefInfos)
    {
        if (!CorIsPrimitiveType(pByRefInfos->typ))
        {
            if (pByRefInfos->pClass->IsValueClass())
            {
                ProtectValueClassFrame::PromoteValueClassEmbeddedObjects(fn, sc, pByRefInfos->pClass, 
                                                 pByRefInfos->data);
            }
            else
            {
                Object *pObject = *((Object **)&pByRefInfos->data);

                LOG((LF_GC, INFO3, "ProtectByRefs Frame Promoting %x to ", pObject));

                (*fn)(pObject, sc, CHECK_APP_DOMAIN);

                *((Object **)&pByRefInfos->data) = pObject;

                LOG((LF_GC, INFO3, "%x\n", pObject));
            }
        }
        pByRefInfos = pByRefInfos->pNext;
    }
}


ProtectByRefsFrame::ProtectByRefsFrame(Thread *pThread, ByRefInfo *brInfo) : 
    m_brInfo(brInfo),  m_pThread(pThread)
{
    m_Next = m_pThread->GetFrame();
    m_pThread->SetFrame(this);
}

void ProtectByRefsFrame::Pop()
{
    m_pThread->SetFrame(m_Next);
}




void ProtectValueClassFrame::GcScanRoots(promote_func *fn, ScanContext *sc)
{
    ValueClassInfo *pVCInfo = m_pVCInfo;
    while (pVCInfo != NULL)
    {
        if (!CorIsPrimitiveType(pVCInfo->typ))
        {
            _ASSERTE(pVCInfo->pClass->IsValueClass());
            PromoteValueClassEmbeddedObjects(
                fn, 
                sc, 
                pVCInfo->pClass, 
                pVCInfo->pData);
        }
        pVCInfo = pVCInfo->pNext;
    }
}


ProtectValueClassFrame::ProtectValueClassFrame(Thread *pThread, ValueClassInfo *pVCInfo) : 
    m_pVCInfo(pVCInfo),  m_pThread(pThread)
{
    m_Next = m_pThread->GetFrame();
    m_pThread->SetFrame(this);
}

void ProtectValueClassFrame::Pop()
{
    m_pThread->SetFrame(m_Next);
}

void ProtectValueClassFrame::PromoteValueClassEmbeddedObjects(promote_func *fn, ScanContext *sc, 
                                                          EEClass *pClass, PVOID pvObject)
{
    FieldDescIterator fdIterator(pClass, FieldDescIterator::INSTANCE_FIELDS);
    FieldDesc* pFD;

    while ((pFD = fdIterator.Next()) != NULL)
    {
        if (!CorIsPrimitiveType(pFD->GetFieldType()))
        {
            if (pFD->IsByValue())
            {
                // recurse
                PromoteValueClassEmbeddedObjects(fn, sc, pFD->GetTypeOfField(),
                                                pFD->GetAddress(pvObject));
            }
            else
            {
                fn(*((Object **) pFD->GetAddress(pvObject)), sc, 
                   CHECK_APP_DOMAIN);
            }
        }
    }
}

//
// Promote Caller Stack
//
//

void FramedMethodFrame::PromoteCallerStackWorker(promote_func* fn, 
                                                 ScanContext* sc, BOOL fPinArgs)
{
    PCCOR_SIGNATURE pCallSig;
    MethodDesc   *pFunction;

    LOG((LF_GC, INFO3, "    Promoting method caller Arguments\n" ));

    // We're going to have to look at the signature to determine
    // which arguments a are pointers....First we need the function
    pFunction = GetFunction();
    if (! pFunction)
        return;

    // Now get the signature...
    pCallSig = pFunction->GetSig();
    if (! pCallSig)
        return;

    //If not "vararg" calling convention, assume "default" calling convention
    if (MetaSig::GetCallingConvention(GetModule(),pCallSig) != IMAGE_CEE_CS_CALLCONV_VARARG)
    {
        MetaSig msig (pCallSig,pFunction->GetModule());
        ArgIterator argit (this, &msig);
        PromoteCallerStackHelper (fn, sc, fPinArgs, &argit, &msig);
    }
    else
    {   
        VASigCookie* varArgSig = *((VASigCookie**) ((BYTE*)this + sizeof(FramedMethodFrame)));
        MetaSig msig (varArgSig->mdVASig, varArgSig->pModule);
        ArgIterator argit ((BYTE*)this, &msig, sizeof(FramedMethodFrame),
            FramedMethodFrame::GetOffsetOfArgumentRegisters());
        PromoteCallerStackHelper (fn, sc, fPinArgs, &argit, &msig);
    }

}

void FramedMethodFrame::PromoteCallerStackHelper(promote_func* fn, 
                                                 ScanContext* sc, BOOL fPinArgs,
                                                 ArgIterator *pargit, MetaSig *pmsig)
{
    MethodDesc      *pFunction;
    UINT32          NumArguments;
    DWORD           GcFlags;

    pFunction = GetFunction();
    // promote 'this' for non-static methods
    if (! pFunction->IsStatic())
    {
        BOOL interior = pFunction->GetClass()->IsValueClass();

        StackElemType  *pThis = (StackElemType*)GetAddrOfThis();
        LOG((LF_GC, INFO3, "    'this' Argument promoted from %x to ", *pThis ));
        if (interior)
            PromoteCarefully(fn, *(Object **)pThis, sc, GC_CALL_INTERIOR|CHECK_APP_DOMAIN);
        else
            (fn)( *(Object **)pThis, sc, CHECK_APP_DOMAIN);
        LOG((LF_GC, INFO3, "%x\n", *pThis ));
    }

    if (pmsig->HasRetBuffArg())
    {
        LPVOID* pRetBuffArg = pargit->GetRetBuffArgAddr();
        GcFlags = GC_CALL_INTERIOR;
        if (fPinArgs)
        {
            GcFlags |= GC_CALL_PINNED;
            LOG((LF_GC, INFO3, "    ret buf Argument pinned at %x\n", *pRetBuffArg));
        }
        LOG((LF_GC, INFO3, "    ret buf Argument promoted from %x to ", *pRetBuffArg));
        PromoteCarefully(fn, *(Object**) pRetBuffArg, sc, GcFlags|CHECK_APP_DOMAIN);
    }

    NumArguments = pmsig->NumFixedArgs();

    if (fPinArgs)
    {

        CorElementType typ;
        LPVOID pArgAddr;
        while (typ = pmsig->PeekArg(), NULL != (pArgAddr = pargit->GetNextArgAddr()))
        {
            if (typ == ELEMENT_TYPE_SZARRAY)
            {
                ArrayBase *pArray = *((ArrayBase**)pArgAddr);

#if 0
                if (pArray && pArray->GetNumComponents() > ARRAYPINLIMIT)
                {
                    (fn)(*(Object**)pArgAddr, sc, 
                         GC_CALL_PINNED | CHECK_APP_DOMAIN);
                }
                else
                {
                    pmsig->GcScanRoots(pArgAddr, fn, sc);
                }
#else
                if (pArray)
                {
                    (fn)(*(Object**)pArgAddr, sc, 
                         GC_CALL_PINNED | CHECK_APP_DOMAIN);
                }
#endif
            }
            else if (typ == ELEMENT_TYPE_BYREF)
            {
                if (!( *(Object**)pArgAddr <= Thread::GetNonCurrentStackBase(sc) &&
                       *(Object**)pArgAddr >  Thread::GetNonCurrentStackLimit(sc) ))
                {
                    (fn)(*(Object**)pArgAddr, sc, 
                         GC_CALL_PINNED | GC_CALL_INTERIOR | CHECK_APP_DOMAIN);
                }


            }
            else if (typ == ELEMENT_TYPE_STRING || (typ == ELEMENT_TYPE_CLASS && pmsig->IsStringType()))
            {
                (fn)(*(Object**)pArgAddr, sc, GC_CALL_PINNED);

            }
            else if (typ == ELEMENT_TYPE_CLASS || typ == ELEMENT_TYPE_OBJECT || typ == ELEMENT_TYPE_VAR)
            {
                Object *pObj = *(Object**)pArgAddr;
                if (pObj != NULL)
                {
                    MethodTable *pMT = pObj->GetMethodTable();
                    _ASSERTE(sizeof(ULONG) == sizeof(MethodTable*));
                    ( *((ULONG*)&pMT) ) &= ~((ULONG)3);
                    EEClass *pcls = pMT->GetClass();
                    if (pcls->IsObjectClass() || pcls->IsBlittable() || pcls->HasLayout())
                    {
                        (fn)(*(Object**)pArgAddr, sc, 
                             GC_CALL_PINNED | CHECK_APP_DOMAIN);
                    }
                    else
                    {
                        (fn)(*(Object**)pArgAddr, sc, 
                             CHECK_APP_DOMAIN);
                    }
                }
            }
            else
            {
                pmsig->GcScanRoots(pArgAddr, fn, sc);
            }
        }
    }
    else
    {
        LPVOID pArgAddr;
    
        while (NULL != (pArgAddr = pargit->GetNextArgAddr()))
        {
            pmsig->GcScanRoots(pArgAddr, fn, sc);
        }
    }

}

//+----------------------------------------------------------------------------
//
//  Method:     TPMethodFrame::GcScanRoots    public
//
//  Synopsis:   GC protects arguments on the stack
//
//  History:    29-Dec-00   Gopalk      Created
//
//+----------------------------------------------------------------------------
void ComPlusMethodFrameGeneric::GcScanRoots(promote_func *fn, ScanContext* sc)
{
	ComPlusMethodFrame::GcScanRoots(fn, sc);

    // Promote the returned object
    if(GetFunction()->ReturnsObject() == MethodDesc::RETOBJ)
        (*fn)(GetReturnObject(), sc, CHECK_APP_DOMAIN);
    else if (GetFunction()->ReturnsObject() == MethodDesc::RETBYREF)
        PromoteCarefully(fn, GetReturnObject(), sc, GC_CALL_INTERIOR|CHECK_APP_DOMAIN);

    return;
}
	
//+----------------------------------------------------------------------------
//
//  Method:     TPMethodFrame::GcScanRoots    public
//
//  Synopsis:   GC protects arguments on the stack
//
//  History:    17-Feb-99   Gopalk      Created
//
//+----------------------------------------------------------------------------
void TPMethodFrame::GcScanRoots(promote_func *fn, ScanContext* sc)
{
    // Delegate to FramedMethodFrame
    FramedMethodFrame::GcScanRoots(fn, sc);
    FramedMethodFrame::PromoteCallerStack(fn, sc);

    // Promote the returned object
    if(GetFunction()->ReturnsObject() == MethodDesc::RETOBJ)
        (*fn)(GetReturnObject(), sc, CHECK_APP_DOMAIN);
    else if (GetFunction()->ReturnsObject() == MethodDesc::RETBYREF)
        PromoteCarefully(fn, GetReturnObject(), sc, GC_CALL_INTERIOR|CHECK_APP_DOMAIN);

    return;
}

//+----------------------------------------------------------------------------
//
//  Method:     TPMethodFrame::GcScanRoots    public
//
//  Synopsis:   Return where the frame will execute next - the result is filled
//              into the given "trace" structure.  The frame is responsible for
//              detecting where it is in its execution lifetime.
//
//
//  History:    26-Jun-99   TarunA     Created
//
//+----------------------------------------------------------------------------
BOOL TPMethodFrame::TraceFrame(Thread *thread, BOOL fromPatch, 
                               TraceDestination *trace, REGDISPLAY *regs)
{
    // Sanity check
    _ASSERTE(NULL != TheTPStub());

    // We want to set a frame patch, unless we're already at the
    // frame patch, in which case we'll trace addrof_code which 
    // should be set by now.

    trace->type = TRACE_STUB;
    if (fromPatch)
        trace->address = GetFunction()->GetUnsafeAddrofCode();
    else
        trace->address = TheTPStub()->GetEntryPoint() + TheTPStub()->GetPatchOffset();
    
    return TRUE;

}


//+----------------------------------------------------------------------------
//
//  Method:     TPMethodFrame::GcScanRoots    public
//
//  Synopsis:   Return only a valid method descriptor. TPMethodFrame has slot
//              number in the prolog and bytes to pop in the epilog portions of
//              the stub. It should not allow crawling during such weird periods.
//
//  History:    17-Feb-99   Gopalk      Created
//
//+----------------------------------------------------------------------------
MethodDesc *TPMethodFrame::GetFunction()
{
    return((MethodDesc *)((((size_t) m_Datum) & 0xFFFF0000) ? m_Datum : 0));
}


/*virtual*/ void ECallMethodFrame::GcScanRoots(promote_func *fn, ScanContext* sc)
{
    FramedMethodFrame::GcScanRoots(fn, sc);
    MethodDesc *pFD = GetFunction();
    PromoteShadowStack( ((LPBYTE)this) - GetNegSpaceSize() - pFD->SizeOfVirtualFixedArgStack(),
                        pFD,
                        fn,
                        sc);


}

VOID SecurityFrame::GcScanRoots(promote_func *fn, ScanContext* sc)
{
    Object         **pObject;

    // Do all parent scans first
    FramedMethodFrame::GcScanRoots(fn, sc);

    // Promote Security Object
    pObject = (Object **) GetAddrOfSecurityDesc();
    if (*pObject != NULL)
        {
            LOG((LF_GC, INFO3, "        Promoting Security Object from %x to ", *pObject ));
            (*fn)( *pObject, sc, CHECK_APP_DOMAIN );
            LOG((LF_GC, INFO3, "%x\n", *pObject ));
        }
    return;
}


// used by PromoteCalleeStack to get the destination function sig
// NOTE: PromoteCalleeStack only promotes bona-fide arguments, and not
// the "this" reference. The whole purpose of PromoteCalleeStack is
// to protect the partially constructed argument array during
// the actual process of argument marshaling.
PCCOR_SIGNATURE ComMethodFrame::GetTargetCallSig()
{
    return ((ComCallMethodDesc *)GetDatum())->GetSig();
}

// Same for destination function module.
Module *ComMethodFrame::GetTargetModule()
{
    return ((ComCallMethodDesc *)GetDatum())->GetModule();
}


// Return the # of stack bytes pushed by the unmanaged caller.
UINT ComMethodFrame::GetNumCallerStackBytes()
{
    ComCallMethodDesc *pCMD = (ComCallMethodDesc *)GetDatum();
    // assumes __stdcall
    // compute the callee pop stack bytes
    return pCMD->GetNumStackBytes();
}


// Convert a thrown COM+ exception to an unmanaged result.
UINT32 ComMethodFrame::ConvertComPlusException(OBJECTREF pException)
{
#ifdef _X86_
    HRESULT ComCallExceptionCleanup(UnmanagedToManagedCallFrame* pCurrFrame);
    return (UINT32)ComCallExceptionCleanup(this);


#else
    _ASSERTE(!"Exception mapping NYI on non-x86.");
    return (UINT32)E_FAIL;
#endif
}


// ComCalls are prepared on the stack in our internal (enregistered) calling
// convention, to speed up the calls.  This means we must use special care to
// walk the args in the buffer.
void ComMethodFrame::PromoteCalleeStack(promote_func *fn, ScanContext *sc)
{
    PCCOR_SIGNATURE pCallSig;
    Module         *pModule;
    UINT32          NumArguments;
    DWORD           i;
    BYTE           *pArgument, *pFirstArg;
    CorElementType  type;
    BOOL            canEnregister;

    LOG((LF_GC, INFO3, "    Promoting Com method frame marshalled arguments\n" ));

    // the marshalled objects are placed above the frame above the locals
    //@nice  put some useful information into the locals
    // such as number of arguments and a bitmap of where the args are.

    // get pointer to GCInfo flag
     ComCallGCInfo*   pGCInfo = (ComCallGCInfo*)GetGCInfoFlagPtr();

     // no object args in the frame or not in the middle of marshalling args
     if (pGCInfo == NULL ||
         !(IsArgsGCProtectionEnabled(pGCInfo) || IsRetGCProtectionEnabled(pGCInfo)))
         return; // no protection required

     // for interpreted case, the args were _alloca'ed , so find the pointer
    // to args from the header offset info
    pFirstArg = pArgument = (BYTE *)GetPointerToDstArgs(); // pointer to the args

    _ASSERTE(pArgument != NULL);
    // For Now get the signature...
    pCallSig = GetTargetCallSig();
    if (! pCallSig)
        return;

    pModule = GetTargetModule();
    _ASSERTE(pModule);      // or value classes won't promote correctly

    MetaSig msig(pCallSig,pModule);
    //
    // We currently only support __stdcall calling convention from COM
    //

    NumArguments = msig.NumFixedArgs();
    canEnregister = TRUE;

    i = 0;
    if (msig.HasRetBuffArg())
    {
        canEnregister = FALSE;
        if (IsRetGCProtectionEnabled(pGCInfo))
            (*fn)((Object*&)(**(Object**)pFirstArg), sc, 
                  CHECK_APP_DOMAIN);
    }

    if (!IsArgsGCProtectionEnabled(pGCInfo))
        return;

    for (;i<NumArguments;i++)
    {
        type = msig.NextArg();
        if (canEnregister && gElementTypeInfo[type].m_enregister)
        {
            msig.GcScanRoots(pFirstArg, fn, sc);
            canEnregister = FALSE;
        }
        else
        {
            pArgument -= StackElemSize(msig.GetLastTypeSize());
            msig.GcScanRoots(pArgument, fn, sc);
        }
    }
}
















//
// UMThkCalFrame::Promote Callee Stack
// If we are the topmost frame, and if GC is in progress
// then there might be some arguments that we have to
// during marshalling

void UMThkCallFrame::PromoteCalleeStack(promote_func* fn, 
                                       ScanContext* sc)
{
    PCCOR_SIGNATURE pCallSig;
    Module         *pModule;
    BYTE           *pArgument;

    LOG((LF_GC, INFO3, "    Promoting UMThk call frame marshalled arguments\n" ));

    // the marshalled objects are placed above the frame above the locals
    //@nice  put some useful information into the locals
    // such as number of arguments and a bitmap of where the args are.


    if (!GCArgsProtectionOn())
    {
        return;
    }

    // for interpreted case, the args were _alloca'ed , so find the pointer
    // to args from the header offset info
    pArgument = (BYTE *)GetPointerToDstArgs(); // pointer to the args

    _ASSERTE(pArgument != NULL);
    // For Now get the signature...
    pCallSig = GetTargetCallSig();
    if (! pCallSig)
        return;

    pModule = GetTargetModule();
    _ASSERTE(pModule);      // or value classes won't promote correctly

    MetaSig msig(pCallSig,pModule);
    MetaSig msig2(pCallSig,pModule);
    ArgIterator argit(NULL, &msig2, GetUMEntryThunk()->GetUMThunkMarshInfo()->IsStatic());

    //
    // We currently only support __stdcall calling convention from COM
    //

    int ofs;
    while (0 != (ofs = argit.GetNextOffset()))
    {
        msig.NextArg();
        msig.GcScanRoots(pArgument + ofs, fn, sc);
    }
}






// used by PromoteCalleeStack to get the destination function sig
// NOTE: PromoteCalleeStack only promotes bona-fide arguments, and not
// the "this" reference. The whole purpose of PromoteCalleeStack is
// to protect the partially constructed argument array during
// the actual process of argument marshaling.
PCCOR_SIGNATURE UMThkCallFrame::GetTargetCallSig()
{
    return GetUMEntryThunk()->GetUMThunkMarshInfo()->GetSig();
}

// Same for destination function module.
Module *UMThkCallFrame::GetTargetModule()
{
    return GetUMEntryThunk()->GetUMThunkMarshInfo()->GetModule();
}


// Return the # of stack bytes pushed by the unmanaged caller.
UINT UMThkCallFrame::GetNumCallerStackBytes()
{
    return GetUMEntryThunk()->GetUMThunkMarshInfo()->GetCbRetPop();
}


// Convert a thrown COM+ exception to an unmanaged result.
UINT32 UMThkCallFrame::ConvertComPlusException(OBJECTREF pException)
{
    return 0;
}


const BYTE* UMThkCallFrame::GetManagedTarget()
{
    UMEntryThunk *umet = GetUMEntryThunk();
    
    if (umet)
    {
        // Ensure that the thunk is completely initialized. Note:
        // can't do this from the debugger helper thread, so we
        // assert that here.
        _ASSERTE(GetThread() != NULL);

        umet->RunTimeInit();

        return umet->GetManagedTarget();
    }
    else
        return NULL;
}





//-------------------------------------------------------------------
// Executes each stored cleanup task and resets the worklist back
// to empty. Some task types are conditional based on the
// "fBecauseOfException" flag. This flag distinguishes between
// cleanups due to normal method termination and cleanups due to
// an exception.
//-------------------------------------------------------------------
#pragma warning(disable:4702)
VOID CleanupWorkList::Cleanup(BOOL fBecauseOfException)
{

    CleanupNode *pnode = m_pNodes;

    // Make one non-gc-triggering pass to chop off protected marshalers.
    // We don't want to be calling a marshaler that's already been
    // deallocated due to a GC happening during the cleanup itself.
    while (pnode) {
        if (pnode->m_type == CL_PROTECTEDMARSHALER)
        {
            pnode->m_pMarshaler = NULL;
        }
        pnode = pnode->m_next;
    }

    pnode = m_pNodes;

    ULONG cbRef;
    if (pnode == NULL)
        return;

#ifdef _DEBUG
    DWORD thisDomainId = GetAppDomain()->GetId();
#endif

    while (pnode) {

        // Should never end up in cleanup from another domain. Should always call cleanup prior to returning from
        // the domain where the cleanup list was created.
        _ASSERTE(thisDomainId == pnode->m_dwDomainId);

        switch(pnode->m_type) {

            case CL_GCHANDLE:
                if (pnode->m_oh)
                    DestroyHandle(pnode->m_oh);
                break;

            case CL_COTASKFREE:
                CoTaskMemFree(pnode->m_pv);
                break;

            case CL_FASTFREE:
                // This collapse will actually deallocate the node itself (it
                // should always be the last node in the list anyway). Make sure
                // we don't attempt to read the next pointer after the
                // deallocation.
                _ASSERTE(pnode->m_next == NULL);
                GetThread()->m_MarshalAlloc.Collapse(pnode->m_pv);
                m_pNodes = NULL;
                return;

            case CL_RELEASE:
                cbRef = SafeRelease(pnode->m_ip);
                LogInteropRelease(pnode->m_ip, cbRef, "Cleanup release");
                break;

            case CL_NSTRUCTDESTROY: 
                pnode->nd.m_pFieldMarshaler->DestroyNative(pnode->nd.m_pNativeData);
                break;

            case CL_RESTORECULTURE:
                BEGIN_ENSURE_COOPERATIVE_GC()
                {
                    GetThread()->SetCulture(ObjectFromHandle(pnode->m_oh), FALSE);
                    DestroyHandle(pnode->m_oh);
                }
                END_ENSURE_COOPERATIVE_GC();
                break;

            case CL_NEWLAYOUTDESTROYNATIVE:
                FmtClassDestroyNative(pnode->nlayout.m_pnative, pnode->nlayout.m_pMT->GetClass());
                break;

            case CL_PROTECTEDOBJREF: //fallthru
            case CL_ISVISIBLETOGC:
			case CL_PROTECTEDMARSHALER:
                // nothing to do here.
                break;

            case CL_MARSHALER_EXCEP:        //fallthru
            case CL_MARSHALERREINIT_EXCEP:
                if (fBecauseOfException)
                {
#ifdef _DEBUG
                    //If this assert fires, contact IanCarm. We are checking
                    // that esp still lies below the marshaler we are about
                    // to cleanup.
                    //It means the exception architecture changed so that
                    // the stack now gets popped before the frame's unwind
                    // methods are called. This code is royally screwed if that happens.
                    // If the architecture changed so that only StackOverflow
                    // exceptions pop the stack first, then the architecture
                    // should be changed so that StackOverflow exceptions
                    // bypass this code (we're just cleaning marshaler-created
                    // crud; in a stackoverflow case, we can reasonably neglect
                    // this cleanup.)
                    // If the architecture changed so that all exceptions
                    // pop the stack first, we have a big problem as marshaler.h
                    // will have to be rearchitected to allocate the marshalers
                    // somewhere else (such as the thread allocator.) But that's
                    // a really big perf hit for interop.

                    int dummy;
                    _ASSERTE( sizeof(size_t) >= sizeof(void*) );  //ensure the assert expression below is doing a safe cast
                    _ASSERTE( ((size_t)&dummy) < (size_t) (pnode->m_pMarshaler) );
#endif
                    if (pnode->m_type == CL_MARSHALER_EXCEP)
                    {
                        pnode->m_pMarshaler->DoExceptionCleanup();
                    }
                    else
                    {
                        _ASSERTE(pnode->m_type == CL_MARSHALERREINIT_EXCEP);
                        pnode->m_pMarshaler->DoExceptionReInit();
                    }
                }
                break;

            default:
                //__assume(0);

                _ASSERTE(!"Bad CleanupNode type.");
        }

        pnode = pnode->m_next;
    }
    m_pNodes = NULL;

    // We should never get here (the last element should be a CL_FASTFREE which
    // exits directly).
    _ASSERTE(FALSE);
}
#pragma warning(default:4702)

//-------------------------------------------------------------------
// Inserts a new task of the given type and datum.
//-------------------------------------------------------------------
CleanupWorkList::CleanupNode*
CleanupWorkList::Schedule(CleanupType ct, LPVOID pv)
{
    CleanupNode *pnode = (CleanupNode *)(GetThread()->m_MarshalAlloc.Alloc(sizeof(CleanupNode)));
    if (pnode != NULL)
    {
        pnode->m_type = ct;
        pnode->m_pv   = pv;
        pnode->m_next = m_pNodes;
#ifdef _DEBUG
        pnode->m_dwDomainId = GetAppDomain()->GetId();
#endif
        m_pNodes      = pnode;
    }
    return pnode;
}



//-------------------------------------------------------------------
// Schedules an unconditional release of a COM IP
// Throws a COM+ exception if failed.
//-------------------------------------------------------------------
VOID CleanupWorkList::ScheduleUnconditionalRelease(IUnknown *ip)
{
    THROWSCOMPLUSEXCEPTION();
    if (ip != NULL) {
        if (!Schedule(CL_RELEASE, ip)) {
            ULONG cbRef = SafeRelease(ip);
            LogInteropRelease(ip, cbRef, "Schedule failed");
            COMPlusThrowOM();
        }
    }
}



//-------------------------------------------------------------------
// Schedules an unconditional free of the native version
// of an NStruct reference field. Note that pNativeData points into
// the middle of the external part of the NStruct, so someone
// has to hold a gc reference to the wrapping NStruct until
// the destroy is done.
//-------------------------------------------------------------------
VOID CleanupWorkList::ScheduleUnconditionalNStructDestroy(const FieldMarshaler *pFieldMarshaler, LPVOID pNativeData)
{
    THROWSCOMPLUSEXCEPTION();

    CleanupNode *pnode = (CleanupNode *)(GetThread()->m_MarshalAlloc.Alloc(sizeof(CleanupNode)));
    if (!pnode) {
        pFieldMarshaler->DestroyNative(pNativeData);
        COMPlusThrowOM();
    }
    pnode->m_type               = CL_NSTRUCTDESTROY;
    pnode->m_next               = m_pNodes;
    pnode->nd.m_pFieldMarshaler = pFieldMarshaler;
    pnode->nd.m_pNativeData     = pNativeData;
#ifdef _DEBUG
    pnode->m_dwDomainId         = GetAppDomain()->GetId();
#endif
    m_pNodes                    = pnode;

}




//-------------------------------------------------------------------
// CleanupWorkList::ScheduleLayoutDestroyNative
// schedule cleanup of marshaled struct fields and of the struct itself.
// Throws a COM+ exception if failed.
//-------------------------------------------------------------------
LPVOID CleanupWorkList::NewScheduleLayoutDestroyNative(MethodTable *pMT)
{
    THROWSCOMPLUSEXCEPTION();
    CleanupNode *pnode = NULL;
    LPVOID       pNative = NULL;

    pNative = GetThread()->m_MarshalAlloc.Alloc(pMT->GetNativeSize());
    FillMemory(pNative, pMT->GetNativeSize(), 0);

    pnode = (CleanupNode *)(GetThread()->m_MarshalAlloc.Alloc(sizeof(CleanupNode)));
    if (!pnode)
        COMPlusThrowOM();

    pnode->m_type               = CL_NEWLAYOUTDESTROYNATIVE;
    pnode->m_next               = m_pNodes;
    pnode->nlayout.m_pnative    = pNative;
    pnode->nlayout.m_pMT        = pMT;
#ifdef _DEBUG
    pnode->m_dwDomainId         = GetAppDomain()->GetId();
#endif
    m_pNodes                    = pnode;

    return pNative;
}



//-------------------------------------------------------------------
// CoTaskFree memory unconditionally
//-------------------------------------------------------------------
VOID CleanupWorkList::ScheduleCoTaskFree(LPVOID pv)
{
    THROWSCOMPLUSEXCEPTION();

    if( pv != NULL)
    {
        if (!Schedule(CL_COTASKFREE, pv))
        {
            CoTaskMemFree(pv);
            COMPlusThrowOM();
        }
    }
}



//-------------------------------------------------------------------
// StackingAllocator.Collapse during exceptions
//-------------------------------------------------------------------
VOID CleanupWorkList::ScheduleFastFree(LPVOID checkpoint)
{
    THROWSCOMPLUSEXCEPTION();

    if (!Schedule(CL_FASTFREE, checkpoint))
    {
        GetThread()->m_MarshalAlloc.Collapse(checkpoint);
        COMPlusThrowOM();
    }
}





//-------------------------------------------------------------------
// Allocates a gc-protected handle. This handle is unconditionally
// destroyed on a Cleanup().
// Throws a COM+ exception if failed.
//-------------------------------------------------------------------
OBJECTHANDLE CleanupWorkList::NewScheduledProtectedHandle(OBJECTREF oref)
{
    // _ASSERTE(oref != NULL);
    THROWSCOMPLUSEXCEPTION();

    OBJECTHANDLE oh = GetAppDomain()->CreateHandle(NULL);
    if(oh != NULL)
    {
        StoreObjectInHandle(oh, oref);
        if (Schedule(CL_GCHANDLE, oh))
        {
           return oh;
        }
        //else
        DestroyHandle(oh);
    }
    COMPlusThrowOM();
    return NULL; // should never get here
}





//-------------------------------------------------------------------
// Schedule restoring thread's current culture to the specified 
// culture.
//-------------------------------------------------------------------
VOID CleanupWorkList::ScheduleUnconditionalCultureRestore(OBJECTREF CultureObj)
{
    _ASSERTE(CultureObj != NULL);
    THROWSCOMPLUSEXCEPTION();

    OBJECTHANDLE oh = GetAppDomain()->CreateHandle(NULL);
    if (oh == NULL)
        COMPlusThrowOM();

    StoreObjectInHandle(oh, CultureObj);
    if (!Schedule(CL_RESTORECULTURE, oh))
    {
        DestroyHandle(oh);
        COMPlusThrowOM();
    }
}



//-------------------------------------------------------------------
// CleanupWorkList::NewProtectedObjRef()
// holds a protected objref (used for creating the buffer for
// an unmanaged->managed byref object marshal. We can't use an
// objecthandle because modifying those without using the handle
// api opens up writebarrier violations.
//
// Must have called IsVisibleToGc() first.
//-------------------------------------------------------------------
OBJECTREF* CleanupWorkList::NewProtectedObjectRef(OBJECTREF oref)
{
    THROWSCOMPLUSEXCEPTION();

    CleanupNode *pNew;
    GCPROTECT_BEGIN(oref);

#ifdef _DEBUG
    {
        CleanupNode *pNode = m_pNodes;
        while (pNode)
        {
            if (pNode->m_type == CL_ISVISIBLETOGC)
            {
                break;
            }
            pNode = pNode->m_next;
        }

        if (pNode == NULL)
        {
            _ASSERTE(!"NewProtectedObjectRef called without proper gc-scanning. The big comment right after this assert says a lot more. Read it.");
            // READ THIS! When you use a node of this type, you must
            // invoke CleanupWorklist::GCScanRoots() as part of
            // your gcscan net. Because this node type was added
            // late in the project and cleanup lists did
            // not have a GC-scanning requirement prior to
            // this, we've added this assert to remind
            // you of this requirement. You will not be permitted
            // to add this node type to a cleanuplist until
            // you make a one-time call to "IsVisibleToGc()" on
            // the cleanup list. That call certifies that
            // you've read and understood this warning and have
            // implemented the gc-scanning required.
        }
    }
#endif


    pNew = Schedule(CL_PROTECTEDOBJREF, NULL);
    if (!pNew)
    {
        COMPlusThrowOM();
    }
    pNew->m_oref = OBJECTREFToObject(oref);    

    GCPROTECT_END();
    return (OBJECTREF*)&(pNew->m_oref);
}

CleanupWorkList::MarshalerCleanupNode * CleanupWorkList::ScheduleMarshalerCleanupOnException(Marshaler *pMarshaler)
{
    THROWSCOMPLUSEXCEPTION();

    CleanupNode *pNew = Schedule(CL_MARSHALER_EXCEP, pMarshaler);
    if (!pNew)
    {
        COMPlusThrowOM();
    }

    // make sure some idiot didn't ignore the warning not to add fields to
    // MarshalerCleanupNode.
    _ASSERTE(sizeof(CleanupNode) == sizeof(MarshalerCleanupNode));
    return *(CleanupWorkList::MarshalerCleanupNode**)&pNew;
}

//-------------------------------------------------------------------
// CleanupWorkList::NewProtectedObjRef()
// holds a Marshaler. The cleanupworklist will own the task
// of calling the marshaler's GcScanRoots fcn.
//
// It makes little architectural sense for the CleanupWorkList to
// own this item. But it's late in the project to be adding
// fields to frames, and it so happens everyplace we need this thing,
// there's alreay a cleanuplist. So it's elected.
//
// Must have called IsVisibleToGc() first.
//-------------------------------------------------------------------
VOID CleanupWorkList::NewProtectedMarshaler(Marshaler *pMarshaler)
{
    THROWSCOMPLUSEXCEPTION();

    CleanupNode *pNew;

#ifdef _DEBUG
    {
        CleanupNode *pNode = m_pNodes;
        while (pNode)
        {
            if (pNode->m_type == CL_ISVISIBLETOGC)
            {
                break;
            }
            pNode = pNode->m_next;
        }

        if (pNode == NULL)
        {
            _ASSERTE(!"NewProtectedObjectRef called without proper gc-scanning. The big comment right after this assert says a lot more. Read it.");
            // READ THIS! When you use a node of this type, you must
            // invoke CleanupWorklist::GCScanRoots() as part of
            // your gcscan net. Because this node type was added
            // late in the project and cleanup lists did
            // not have a GC-scanning requirement prior to
            // this, we've added this assert to remind
            // you of this requirement. You will not be permitted
            // to add this node type to a cleanuplist until
            // you make a one-time call to "IsVisibleToGc()" on
            // the cleanup list. That call certifies that
            // you've read and understood this warning and have
            // implemented the gc-scanning required.
        }
    }
#endif


    pNew = Schedule(CL_PROTECTEDMARSHALER, pMarshaler);
    if (!pNew)
    {
        COMPlusThrowOM();
    }
}




//-------------------------------------------------------------------
// If you've called IsVisibleToGc(), must call this.
//-------------------------------------------------------------------
void CleanupWorkList::GcScanRoots(promote_func *fn, ScanContext* sc)
{
    CleanupNode *pnode = m_pNodes;

    while (pnode) {

        switch(pnode->m_type) 
		{
            case CL_PROTECTEDOBJREF: 
                if (pnode->m_oref != NULL)
                {
                    LOG((LF_GC, INFO3, "GC Protection Frame Promoting %x to ", pnode->m_oref ));
                    (*fn)(pnode->m_oref, sc);
                    LOG((LF_GC, INFO3, "%x\n", pnode->m_oref ));
                }
                break;

            case CL_PROTECTEDMARSHALER:
                if (pnode->m_pMarshaler)
                {
                    pnode->m_pMarshaler->GcScanRoots(fn, sc);
                }
                break;

            default:
				;
        }

        pnode = pnode->m_next;
    }
}


//-------------------------------------------------------------------
// Destructor (calls Cleanup(FALSE))
//-------------------------------------------------------------------
CleanupWorkList::~CleanupWorkList()
{
    Cleanup(FALSE);
}



// m_sig((LPCUTF8)pFrame->GetFunction()->GetSig(),pFrame->GetModule()->getScope())
//------------------------------------------------------------
// Constructor
//------------------------------------------------------------
ArgIterator::ArgIterator(FramedMethodFrame *pFrame, MetaSig* pSig)
{
    BOOL fStatic = pFrame->GetFunction()->IsStatic();   
    m_curOfs     = sizeof(FramedMethodFrame) + pSig->SizeOfActualFixedArgStack(fStatic);
    m_pSig       = pSig;
    m_pSig->Reset();                // Reset the enum so we are at the beginning of the signature
    m_pFrameBase = (LPBYTE)pFrame;
    m_regArgsOfs = FramedMethodFrame::GetOffsetOfArgumentRegisters();
    m_numRegistersUsed = 0;
    if (!(fStatic)) {
        IsArgumentInRegister(&m_numRegistersUsed, ELEMENT_TYPE_CLASS, 0, TRUE, pSig->GetCallingConvention(), NULL);
    }
    if (pSig->HasRetBuffArg())
        m_numRegistersUsed++;
    m_argNum = -1;
    _ASSERTE(m_numRegistersUsed <= NUM_ARGUMENT_REGISTERS);
}

//------------------------------------------------------------
// Another constructor, when you have a FramedMethodFrame, but you don't have an active instance
//------------------------------------------------------------
ArgIterator::ArgIterator(LPBYTE pFrameBase, MetaSig* pSig, BOOL fIsStatic)
{
    m_curOfs     = sizeof(FramedMethodFrame) + pSig->SizeOfActualFixedArgStack(fIsStatic);
    m_pSig       = pSig;
    m_pSig->Reset();                // Reset the enum so we are at the beginning of the signature
    m_pFrameBase = pFrameBase;
    m_regArgsOfs = FramedMethodFrame::GetOffsetOfArgumentRegisters();
    m_numRegistersUsed = 0;
    if (!(fIsStatic)) {
        IsArgumentInRegister(&m_numRegistersUsed, ELEMENT_TYPE_CLASS, 0, TRUE, pSig->GetCallingConvention(), NULL);
    }

    if (pSig->HasRetBuffArg())
        m_numRegistersUsed++;
    m_argNum = -1;
    _ASSERTE(m_numRegistersUsed <= NUM_ARGUMENT_REGISTERS);
}

//------------------------------------------------------------
// Another constructor
//------------------------------------------------------------
ArgIterator::ArgIterator(LPBYTE pFrameBase, MetaSig* pSig, int stackArgsOfs, int regArgsOfs)
{
    m_curOfs     = stackArgsOfs + pSig->SizeOfActualFixedArgStack(!pSig->HasThis());
    m_pSig       = pSig;    
    m_pSig->Reset();                // Reset the enum so we are at the beginning of the signature   
    m_pFrameBase = pFrameBase;
    m_regArgsOfs = regArgsOfs;
    m_numRegistersUsed = 0;
    if (pSig->HasThis()) {
        IsArgumentInRegister(&m_numRegistersUsed, ELEMENT_TYPE_CLASS, 0, TRUE, pSig->GetCallingConvention(), NULL);
    }
    if (pSig->HasRetBuffArg())
        m_numRegistersUsed++;
    m_argNum = -1;
    _ASSERTE(m_numRegistersUsed <= NUM_ARGUMENT_REGISTERS);
}


int ArgIterator::GetThisOffset() 
{
    _ASSERTE(NUM_ARGUMENT_REGISTERS > 0);

        // The this pointer is the first register argument. 
        // Thus it is at the 'top' of the array of register aruments
    return(m_regArgsOfs + (NUM_ARGUMENT_REGISTERS - 1)* sizeof(void*));
}

int ArgIterator::GetRetBuffArgOffset(UINT *pRegStructOfs/* = NULL*/)
{
    _ASSERTE(NUM_ARGUMENT_REGISTERS > 1);
    _ASSERTE(m_pSig->HasRetBuffArg());     // Feel free to remove this and return failure if needed

        // Assume it is the first register argument
    int ret = m_regArgsOfs + (NUM_ARGUMENT_REGISTERS - 1)* sizeof(void*);
    if (pRegStructOfs)
        *pRegStructOfs = (NUM_ARGUMENT_REGISTERS - 1) * sizeof(void*);
        
        // if non-static, however, it is the next argument
    if (m_pSig->HasThis()) {
        ret -= sizeof(void*);
        if (pRegStructOfs) {
            (*pRegStructOfs) -= sizeof(void*);
        }
    }
    return(ret);
}

/*---------------------------------------------------------------------------------
    Same as GetNextOffset, but uses cached argument type and size info.
    DOES NOT ALTER state of MetaSig::m_pLastType etc !!
-----------------------------------------------------------------------------------*/

int ArgIterator::GetNextOffsetFaster(BYTE *pType, UINT32 *pStructSize, UINT *pRegStructOfs/* = NULL*/)
{
    if (m_pSig->m_fCacheInitted & SIG_OFFSETS_INITTED)
    {
        if (m_curOfs != 0) {
            m_argNum++;
            _ASSERTE(m_argNum <= MAX_CACHED_SIG_SIZE);
            BYTE typ = m_pSig->m_types[m_argNum];
            *pType = typ;

            if (typ == ELEMENT_TYPE_END) {
                m_curOfs = 0;
            } else {
                *pStructSize = m_pSig->m_sizes[m_argNum];

                if (m_pSig->m_offsets[m_argNum] != -1)
                {
                    if (pRegStructOfs) {
                        *pRegStructOfs = m_pSig->m_offsets[m_argNum];
                    }
                    return m_regArgsOfs + m_pSig->m_offsets[m_argNum];
                } else 
                {
                    if (pRegStructOfs) {
                        *pRegStructOfs = (UINT)(-1);
                    }
                    m_curOfs -= StackElemSize(*pStructSize);
                }
            }
        }
    }
    else 
    {
            
        UINT32 structSize;
        if (m_curOfs != 0) {
            BYTE typ = m_pSig->NextArgNormalized(&structSize);
            *pType = typ;
            int offsetIntoArgumentRegisters;

            if (typ == ELEMENT_TYPE_END) {
                m_curOfs = 0;
            } else {
                *pStructSize = structSize;
                BYTE callingconvention = m_pSig->GetCallingConvention();


                if (IsArgumentInRegister(&m_numRegistersUsed, typ, structSize, FALSE, callingconvention, &offsetIntoArgumentRegisters)) {
                    if (pRegStructOfs) {
                        *pRegStructOfs = offsetIntoArgumentRegisters;
                    }
                    return  m_regArgsOfs + offsetIntoArgumentRegisters;
                } else {
                    if (pRegStructOfs) {
                        *pRegStructOfs = (UINT)(-1);
                    }
                    m_curOfs -= StackElemSize(structSize);
                }
            }
        }
    }   // Cache initted or not
    
    return m_curOfs;
}

//------------------------------------------------------------
// Same as GetNextArgAddr() but returns a byte offset from
// the Frame* pointer. This offset can be positive *or* negative.
//
// Returns 0 once you've hit the end of the list. Since the
// the offset is relative to the Frame* pointer itself, 0 can
// never point to a valid argument.
//------------------------------------------------------------
int ArgIterator::GetNextOffset(BYTE *pType, UINT32 *pStructSize, UINT *pRegStructOfs/* = NULL*/)
{
    if (m_curOfs != 0) {
        BYTE typ = m_pSig->NextArgNormalized();
        *pType = typ;
        int offsetIntoArgumentRegisters;
        UINT32 structSize;

        if (typ == ELEMENT_TYPE_END) {
            m_curOfs = 0;
        } else {
            structSize = m_pSig->GetLastTypeSize();
            *pStructSize = structSize;



            if (IsArgumentInRegister(&m_numRegistersUsed, typ, structSize, FALSE, m_pSig->GetCallingConvention(), &offsetIntoArgumentRegisters)) {
                if (pRegStructOfs) {
                    *pRegStructOfs = offsetIntoArgumentRegisters;
                }
                return m_regArgsOfs + offsetIntoArgumentRegisters;
            } else {
                if (pRegStructOfs) {
                    *pRegStructOfs = (UINT)(-1);
                }
                m_curOfs -= StackElemSize(structSize);
            }
        }
    }

    return m_curOfs;
}

//------------------------------------------------------------
// Each time this is called, this returns a pointer to the next
// argument. This pointer points directly into the caller's stack.
// Whether or not object arguments returned this way are gc-protected
// depends on the exact type of frame.
//
// Returns NULL once you've hit the end of the list.
//------------------------------------------------------------
LPVOID ArgIterator::GetNextArgAddr(BYTE *pType, UINT32 *pStructSize)
{
    int ofs = GetNextOffset(pType, pStructSize);
    if (ofs) {
        return ofs + m_pFrameBase;
    } else {
        return NULL;
    }
}

//------------------------------------------------------------
// Returns the type of the last arg visited
//------------------------------------------------------------
TypeHandle ArgIterator::GetArgType()
{ 
    if (m_pSig->m_fCacheInitted & SIG_OFFSETS_INITTED)
    {
        // 
        // Sync the sig walker with the arg number
        //

        m_pSig->Reset();
        for (int i=0; i<=m_argNum; i++)
            m_pSig->NextArg();
    }

    return m_pSig->GetTypeHandle(); 
}

//------------------------------------------------------------
// GC-promote all arguments in the shadow stack. pShadowStackVoid points
// to the lowest addressed argument (which points to the "this" reference
// for instance methods and the *rightmost* argument for static methods.)
//------------------------------------------------------------
VOID PromoteShadowStack(LPVOID pShadowStackVoid, MethodDesc *pFD, promote_func* fn, ScanContext* sc)
{
    LPBYTE pShadowStack = (LPBYTE)pShadowStackVoid;
    if (!(pFD->IsStatic())) {
        OBJECTREF *pThis = (OBJECTREF*)pShadowStack;
        LOG((LF_GC, INFO3, "    'this' Argument promoted from %x to ", *pThis ));
        DoPromote(fn, sc, pThis, pFD->GetMethodTable()->IsValueClass());
        LOG((LF_GC, INFO3, "%x\n", *pThis ));
    }

    pShadowStack += pFD->SizeOfVirtualFixedArgStack();
    MetaSig msig(pFD->GetSig(),pFD->GetModule());

    if (msig.HasRetBuffArg())
    {
        pShadowStack -= sizeof(void*);
        OBJECTREF *pThis = (OBJECTREF*)pShadowStack;
        PromoteCarefully(fn, *(Object **)pThis, sc, GC_CALL_INTERIOR|CHECK_APP_DOMAIN);
    }
    //_ASSERTE(!"Hit PromoteShadowStack.");

    while (ELEMENT_TYPE_END != msig.NextArg()) {
        pShadowStack -= StackElemSize(msig.GetLastTypeSize());
        msig.GcScanRoots(pShadowStack, fn, sc);
    }
}



//------------------------------------------------------------
// Copy pFrame's arguments into pShadowStackOut using the virtual calling
// convention format. The size of the buffer must be equal to
// pFrame->GetFunction()->SizeOfVirtualFixedArgStack().
// This function also copies the "this" reference if applicable.
//------------------------------------------------------------
VOID FillinShadowStack(FramedMethodFrame *pFrame, LPVOID pShadowStackOut_V)
{
    LPBYTE pShadowStackOut = (LPBYTE)pShadowStackOut_V;
    MethodDesc *pFD = pFrame->GetFunction();
    if (!(pFD->IsStatic())) {
        *((OBJECTREF*)pShadowStackOut) = pFrame->GetThis();
    }

    MetaSig Sig(pFrame->GetFunction()->GetSig(),
                pFrame->GetModule());   
    pShadowStackOut += Sig.SizeOfVirtualFixedArgStack(pFD->IsStatic());
    ArgIterator argit(pFrame,&Sig);
    
    if (Sig.HasRetBuffArg()) {
        pShadowStackOut -= 4;
        *((INT32*)pShadowStackOut) = *((INT32*) argit.GetRetBuffArgAddr());
    }

    BYTE   typ;
    UINT32 cbSize;
    LPVOID psrc;
    while (NULL != (psrc = argit.GetNextArgAddr(&typ, &cbSize))) {
        switch (StackElemSize(cbSize)) {
            case 4:
                pShadowStackOut -= 4;
                *((INT32*)pShadowStackOut) = *((INT32*)psrc);
                break;

            case 8:
                pShadowStackOut -= 8;
                *((INT64*)pShadowStackOut) = *((INT64*)psrc);
                break;

            default:
                pShadowStackOut -= StackElemSize(cbSize);
                CopyMemory(pShadowStackOut, psrc, StackElemSize(cbSize));
                break;

        }
    }
}

HelperMethodFrame::HelperMethodFrame(struct MachState* ms, MethodDesc* pMD, ArgumentRegisters * regArgs)
{
    Init(GetThread(), ms, pMD, regArgs);
}

void HelperMethodFrame::LazyInit(void* FcallFtnEntry, struct LazyMachState* ms)
{
    _ASSERTE(!ms->isValid());

    m_MachState = ms;
    m_FCallEntry = FcallFtnEntry;
    m_pThread = GetThread();
    Push(m_pThread);

#ifdef _DEBUG
    static Frame* stopFrame = 0;
    static int ctr = 0;
    if (m_Next == stopFrame)
        ctr++;

        // Want to excersize both code paths.  do it half the time.
    if (DbgRandomOnExe(.5))
        InsureInit();
#endif
}

MethodDesc::RETURNTYPE HelperMethodFrame::ReturnsObject() {
    if (m_Datum != 0)
        return(GetFunction()->ReturnsObject());
    
    unsigned attrib = GetFrameAttribs();
    if (attrib & FRAME_ATTR_RETURNOBJ)
        return(MethodDesc::RETOBJ);
    if (attrib & FRAME_ATTR_RETURN_INTERIOR)
        return(MethodDesc::RETBYREF);
    
    return(MethodDesc::RETNONOBJ);
}

void HelperMethodFrame::Init(Thread *pThread, struct MachState* ms, MethodDesc* pMD, ArgumentRegisters * regArgs)
{  
    m_MachState = ms;
    ms->getState(1);

    _ASSERTE(isLegalManagedCodeCaller(*ms->pRetAddr()));

    m_Datum = pMD;
    m_Attribs = FRAME_ATTR_NONE;
#ifdef _DEBUG
    m_ReturnAddress = (LPVOID)POISONC;
#endif
    m_RegArgs = regArgs;
    m_pThread = pThread;
    m_FCallEntry = 0;
    
    // Link new frame onto frame chain.
    Push(pThread);


#ifdef STRESS_HEAP
    // TODO show we leave this?

    if (g_pConfig->GetGCStressLevel() != 0
#ifdef _DEBUG
        && !g_pConfig->FastGCStressLevel()
#endif
        )
        g_pGCHeap->StressHeap();
#endif

}


void HelperMethodFrame::InsureInit() 
{
    if (m_MachState->isValid())
        return;

    m_Datum = MapTargetBackToMethod(m_FCallEntry);
    _ASSERTE(m_FCallEntry == 0 || m_Datum != 0);    // if this is an FCall, we should find it
    _ASSERTE(m_Attribs != 0xCCCCCCCC);
    m_RegArgs = 0;

    // because TRUE FCalls can be called from via reflection, com-interop etc.
    // we cant rely on the fact that we are called from jitted code to find the
    // caller of the FCALL.   Thus FCalls must erect the frame directly in the
    // FCall.  For JIT helpers, however, we can rely on this, and so they can
    // be sneaker and defer the HelperMethodFrame setup to a helper etc
   
    if (m_FCallEntry == 0 && !(m_Attribs & Frame::FRAME_ATTR_EXACT_DEPTH)) // Jit Helper
        m_MachState->getState(3, (MachState::TestFtn) ExecutionManager::FindJitManPCOnly);
    else if (m_Attribs & Frame::FRAME_ATTR_CAPUTURE_DEPTH_2)
        m_MachState->getState(2);       // explictly told depth
    else
        m_MachState->getState(1);       // True FCall 

    _ASSERTE(isLegalManagedCodeCaller(*m_MachState->pRetAddr()));
}

void HelperMethodFrame::GcScanRoots(promote_func *fn, ScanContext* sc) 
{
    _ASSERTE(m_MachState->isValid());       // we have calle InsureInit
#ifdef _X86_

    // Note that if we don't have a MD or registe args, then do dont to GC promotion of args
    if (m_Datum == 0 || m_RegArgs == 0)
        return;

    GCCONTEXT ctx;
    ctx.f = fn;
    ctx.sc = sc;

    MethodDesc* pMD = (MethodDesc*) m_Datum;
    MetaSig msig(pMD->GetSig(), pMD->GetModule());
    if (msig.HasThis()) {
        DoPromote(fn, sc, (OBJECTREF *) &m_RegArgs->THIS_REG,
                  pMD->GetMethodTable()->IsValueClass());
    }

    if (msig.HasRetBuffArg()) {
        INT32 * pRetBuffArg = msig.HasThis() ? &m_RegArgs->ARGUMENT_REG2 : &m_RegArgs->ARGUMENT_REG1;
        DoPromote(fn, sc, (OBJECTREF *) pRetBuffArg, TRUE);
    }

    int argRegsOffs = 0;
    promoteArgs((BYTE*) m_MachState->pRetAddr(), &msig, &ctx, 
        sizeof(void*),                  // arguments start right above the return address
        ((BYTE*) m_RegArgs) - ((BYTE*) m_MachState->pRetAddr())   // convert reg args pointer to offset
        );
#else // !_X86_
    _ASSERTE(!"PLATFORM NYI - HelperMethodFrame::GcScanRoots (frames.cpp)");
#endif // _X86_
}

#if defined(_X86_) && defined(_DEBUG)
        // Confirm that if the machine state was not initialized, then
        // any unspilled callee saved registers did not change
MachState* HelperMethodFrame::ConfirmState(HelperMethodFrame* frame, void* esiVal, void* ediVal, void* ebxVal, void* ebpVal) {

	MachState* state = frame->m_MachState;
    if (state->isValid())
        return(state);
    
	frame->InsureInit();
    _ASSERTE(state->_pEsi != &state->_esi || state->_esi  == esiVal);
    _ASSERTE(state->_pEdi != &state->_edi || state->_edi  == ediVal);
    _ASSERTE(state->_pEbx != &state->_ebx || state->_ebx  == ebxVal);
    _ASSERTE(state->_pEbp != &state->_ebp || state->_ebp  == ebpVal);
    return(state);
}

void* getConfirmState() {       // Silly helper because inline asm does not allow syntax for member functions
    return(HelperMethodFrame::ConfirmState);
}

#endif

#ifdef _X86_
__declspec(naked)
#endif // _X86_
int HelperMethodFrame::RestoreState() 
{ 
#ifdef _X86_

    // restore the registers from the m_MachState stucture.  Note that
    // we only do this for register that where not saved on the stack
    // at the time the machine state snapshot was taken.  

    __asm {
    mov EAX, [ECX]HelperMethodFrame.m_MachState;

    // We don't do anything if m_MachState was never initialized.
    // The assumption here is that a GC could not have happened and thus
    // the current values of the register are fine. (ConfirmState checks this under debug build)
    cmp [EAX]MachState._pRetAddr, 0

#ifdef _DEBUG
    jnz noConfirm
        push EBP
        push EBX
        push EDI
        push ESI
		push ECX
        call getConfirmState
        call EAX
    noConfirm:
#endif
    jz doRet;

    lea EDX, [EAX]MachState._esi         // Did we have to spill ESI
    cmp [EAX]MachState._pEsi, EDX
    jnz SkipESI
        mov ESI, [EAX]MachState._esi     // Then restore it
    SkipESI:
    
    lea EDX, [EAX]MachState._edi         // Did we have to spill EDI
    cmp [EAX]MachState._pEdi, EDX
    jnz SkipEDI
        mov EDI, [EAX]MachState._edi     // Then restore it
    SkipEDI:
    
    lea EDX, [EAX]MachState._ebx         // Did we have to spill EBX
    cmp [EAX]MachState._pEbx, EDX
    jnz SkipEBX
        mov EBX, [EAX]MachState._ebx     // Then restore it
    SkipEBX:
    
    lea EDX, [EAX]MachState._ebp         // Did we have to spill EBP
    cmp [EAX]MachState._pEbp, EDX
    jnz SkipEBP
        mov EBP, [EAX]MachState._ebp // Then restore it
    SkipEBP:
    
    doRet:

    xor EAX, EAX
    ret
    }
#else // !_X86_
    _ASSERTE(!"PLATFORM NYI - HelperMethodFrame::PopFrame (Frames.cpp)");
    return(0);
#endif // _X86_
}

#include "COMDelegate.h"
BOOL MulticastFrame::TraceFrame(Thread *thread, BOOL fromPatch, 
                                TraceDestination *trace, REGDISPLAY *regs)
{
#ifdef _X86_ // references to Regs->pEdi, & Regs->pEsi make this x86 specific
    LOG((LF_CORDB,LL_INFO10000, "MulticastFrame::TF FromPatch:0x%x, at 0x%x\n",
        fromPatch, *regs->pPC));

    // The technique is borrowed from SecurityFrame::TraceFrame
    // Basically, we'll walk the list, take advantage of the fact that
    // the GetMCDStubSize is the (DelegateStubManager's) size of the
    // code, and see where the regs->EIP is.  
    MethodDesc *pMD = GetFunction();
    BYTE *prestub = (BYTE*) pMD - METHOD_CALL_PRESTUB_SIZE;
    INT32 stubOffset = *((UINT32*)(prestub+1));
    const BYTE* pStub = prestub + METHOD_CALL_PRESTUB_SIZE + stubOffset;
    Stub *stub = Stub::RecoverStub(pStub);

    if (stub->IsIntercept())
    {
        _ASSERTE( !stub->IsMulticastDelegate() );

        LOG((LF_CORDB, LL_INFO1000, "MF::TF: Intercept stub found @ 0x%x\n", stub));

        InterceptStub *is = (InterceptStub*)stub;
        while ( *(is->GetInterceptedStub()) != NULL)
        {
            stub = *(is->GetInterceptedStub());
            LOG((LF_CORDB, LL_INFO1000, "MF::TF: InterceptSTub 0x%x leads to stub 0x%x\n",
                is, stub));
        }

        stub = AscertainMCDStubness( *(is->GetRealAddr()) );

        LOG((LF_CORDB, LL_INFO1000, "MF::TF: Mulitcast delegate stub is:0x%x\n", stub));

        if (stub == NULL)
            return FALSE;   // Haven't got a clue - hope TrapStepOut 
                            // finds another spot to stop.
        _ASSERTE( stub->IsMulticastDelegate() );
    }

    ULONG32 StubSize = stub->GetMCDStubSize();
    ULONG32 MCDPatch = stub->GetMCDPatchOffset();
    
    if (stub->IsMulticastDelegate() &&
        (BYTE *)stub + sizeof(Stub) <= (BYTE *)(*regs->pPC) &&
        (BYTE *)(*regs->pPC) <=  (BYTE *)stub + sizeof(Stub) + StubSize)
    {
        LOG((LF_CORDB, LL_INFO1000, "MF::TF: 0x%x appears to be a multicast delegate\n",stub));
        
        if (fromPatch)
        {
            _ASSERTE( (BYTE *)(*regs->pPC) ==  (BYTE *)stub + sizeof(Stub) + MCDPatch);

            if (*regs->pEdi == 0)
            {
                LOG((LF_CORDB, LL_INFO1000, "MF::TF: Executed all stubs, should return\n"));
                // We've executed all the stubs, so we should return
                return FALSE;
            }
            else
            {
                // We're going to execute stub EDI-1 next, so go and grab it.
                BYTE *pbDel = *(BYTE **)( (size_t)*(regs->pEsi) + MulticastFrame::GetOffsetOfThis());
                BYTE *pbDelPrev = *(BYTE **)(pbDel + 
                                              Object::GetOffsetOfFirstField() 
                                              + COMDelegate::m_pPRField->GetOffset());

                DWORD iTargetDelegate;
                DWORD iCurDelegate;

                iTargetDelegate = *(regs->pEdi) - 1;
                for (iCurDelegate = 0;
                     iCurDelegate < iTargetDelegate;
                     iCurDelegate++)
                {
                    LOG((LF_CORDB, LL_INFO1000, "MF::TF: pbDel:0x%x prev:0x%x\n", pbDel, pbDelPrev));
                    pbDel = pbDelPrev;
                    pbDelPrev = *(BYTE**)(pbDel + 
                                            Object::GetOffsetOfFirstField() 
                                            + COMDelegate::m_pPRField->GetOffset());
                    _ASSERTE( pbDel != NULL );
                }

                BYTE **ppbDest = NULL;

                if (StubLinkStubManager::g_pManager->IsStaticDelegate(pbDel))
                {
                    // Then what we've got is actually a static delegate, meaning that the
                    // REAL function pointer is hidden away in another field of the delegate.
                    ppbDest = StubLinkStubManager::g_pManager->GetStaticDelegateRealDest(pbDel);

                    LOG((LF_CORDB,LL_INFO10000, "MF::TF (StaticMultiDel) ppbDest: 0x%x "
                        "*ppbDest:0x%x (%s::%s)\n", ppbDest, *ppbDest,
                        ((MethodDesc*)((*ppbDest)+5))->m_pszDebugClassName,
                        ((MethodDesc*)((*ppbDest)+5))->m_pszDebugMethodName));
                    
                }
                else
                {
                    // "single" multicast delegate - no frames, just a direct call
                    ppbDest = StubLinkStubManager::g_pManager->GetSingleDelegateRealDest(pbDel);

                    LOG((LF_CORDB,LL_INFO10000, "MF::TF (MultiDel)ppbDest: 0x%x "
                        "*ppbDest:0x%x (%s::%s)\n", ppbDest, *ppbDest));
                }

                LOG((LF_CORDB, LL_INFO1000, "MF::TF: Ended up at 0x%x, dest is 0x%x\n", pbDel,
                    *ppbDest));
                return StubManager::TraceStub(*ppbDest,trace);
            }
        }
        else
        {
            // Put a BP down for us to hit - when we hit it, we'll execute the if part
            // of this if..else statement.
            trace->type = TRACE_FRAME_PUSH;
            trace->address = (BYTE *)stub + sizeof(Stub) + MCDPatch;

            LOG((LF_CORDB, LL_INFO1000, "MF::TF: FRAME_PUSH to 0x%x ("
                "intermediate offset:0x%x sizeof(Stub):0x%x)\n", trace->address, 
                MCDPatch, sizeof(Stub)));
            return TRUE;
        }
    }
#else // !_X86_
    _ASSERTE(!"PLATFORM NYI - MulticastFrame::TraceFrame (frames.cpp)");
#endif // _X86_

    return FALSE;
}

// Checks to see if we've got a Multicast delegate stub, based on what we know
// about it.
Stub *MulticastFrame::AscertainMCDStubness(BYTE *pbAddr)
{
    if (UpdateableMethodStubManager::g_pManager->CheckIsStub(pbAddr))
        return NULL;

    if (MethodDescPrestubManager::g_pManager->CheckIsStub(pbAddr))
        return NULL;
    
    if (!StubLinkStubManager::g_pManager->CheckIsStub(pbAddr))
        return NULL;

    return Stub::RecoverStub(pbAddr);
}

void UnmanagedToManagedCallFrame::GcScanRoots(promote_func *fn, ScanContext* sc)
{

    if (GetCleanupWorkList())
    {
        GetCleanupWorkList()->GcScanRoots(fn, sc);
    }


    // don't need to worry about the object moving as it is stored in a weak handle
    // but do need to report it so it doesn't get collected if the only reference to
    // it is in this frame. So only do something if are in promotion phase. And if are
    // in reloc phase this could cause invalid refs as the object may have been moved.
    if (! sc->promotion)
        return;

    if (GetReturnContext())
    {
        _ASSERTE(GetReturnContext()->GetDomain());    // this will make sure is a valid pointer
    
        Object *pRef = OBJECTREFToObject(GetReturnContext()->GetExposedObjectRaw());
        if (pRef == NULL)
            return;
    
        LOG((LF_GC, INFO3, "UnmanagedToManagedCallFrame Protection Frame Promoting %x to ", pRef));
        (*fn)(pRef, sc, CHECK_APP_DOMAIN);
        LOG((LF_GC, INFO3, "%x\n", pRef ));
    }


}

void ContextTransitionFrame::GcScanRoots(promote_func *fn, ScanContext* sc)
{
    (*fn) (m_ReturnLogicalCallContext, sc);
    LOG((LF_GC, INFO3, "    %x\n", m_ReturnLogicalCallContext));

    (*fn) (m_ReturnIllogicalCallContext, sc);
    LOG((LF_GC, INFO3, "    %x\n", m_ReturnIllogicalCallContext));

    // don't need to worry about the object moving as it is stored in a weak handle
    // but do need to report it so it doesn't get collected if the only reference to
    // it is in this frame. So only do something if are in promotion phase. And if are
    // in reloc phase this could cause invalid refs as the object may have been moved.
    if (! sc->promotion)
        return;

    _ASSERTE(GetReturnContext());
    _ASSERTE(GetReturnContext()->GetDomain());    // this will make sure is a valid pointer

    Object *pRef = OBJECTREFToObject(GetReturnContext()->GetExposedObjectRaw());
    if (pRef == NULL)
        return;

    LOG((LF_GC, INFO3, "ContextTransitionFrame Protection Frame Promoting %x to ", pRef));
    // Don't check app domains here - the objects are in the parent frame's app domain

    (*fn)(pRef, sc);
    LOG((LF_GC, INFO3, "%x\n", pRef ));
}

//void ContextTransitionFrame::UninstallExceptionHandler() {}

//void ContextTransitionFrame::InstallExceptionHandler() {}

// this is used to handle the unwind out of the last frame that has entered an
// appdomain that is being unloaded. If we get a thread abort exception, then
// we will catch it, reset and turn into an unload exception
void ContextTransitionFrame::InstallExceptionHandler()
{
	// this doesn't actually throw, but the handler will, so make sure the EH stack is ok
	THROWSCOMPLUSEXCEPTION();	
	
    exRecord.Handler = ContextTransitionFrameHandler;
    EXCEPTION_REGISTRATION_RECORD *pCurrExRecord = (EXCEPTION_REGISTRATION_RECORD *)GetCurrentSEHRecord();
    EXCEPTION_REGISTRATION_RECORD *pExRecord = &exRecord;
    //LOG((LF_APPDOMAIN, LL_INFO100, "ContextTransitionFrame::InstallExceptionHandler: frame, %8.8x, exRecord %8.8x\n", this, pExRecord));
    if (pCurrExRecord > pExRecord)
    {
        //LOG((LF_APPDOMAIN, LL_INFO100, "____extTransitionFrame::InstallExceptionHandler: install on top\n"));
        INSTALL_EXCEPTION_HANDLING_RECORD(pExRecord);
        return;
    }

    // there may have been other EH frames installed between the allocation of this frame and
    // the arrival at this point, so insert ourselves in stack order in the right spot.
    while (pCurrExRecord != (EXCEPTION_REGISTRATION_RECORD*) -1 && pCurrExRecord->Next < pExRecord) {
        pCurrExRecord = pCurrExRecord->Next;
    }
    _ASSERTE(pCurrExRecord != (EXCEPTION_REGISTRATION_RECORD*) -1 && pCurrExRecord->Next != (EXCEPTION_REGISTRATION_RECORD*) -1);
    //LOG((LF_APPDOMAIN, LL_INFO100, "____extTransitionFrame::InstallExceptionHandler: install in middle\n"));
    pExRecord->Next = pCurrExRecord->Next;
    pCurrExRecord->Next = pExRecord;
}

void ContextTransitionFrame::UninstallExceptionHandler()
{
    EXCEPTION_REGISTRATION_RECORD *pCurrExRecord = (EXCEPTION_REGISTRATION_RECORD *)GetCurrentSEHRecord();
    EXCEPTION_REGISTRATION_RECORD *pExRecord = &exRecord;
    //LOG((LF_APPDOMAIN, LL_INFO100, "ContextTransitionFrame::UninstallExceptionHandler: frame, %8.8x, exRecord %8.8x\n", this, pExRecord));
    if (pCurrExRecord == pExRecord)
    {
        UNINSTALL_EXCEPTION_HANDLING_RECORD(pExRecord);
        //LOG((LF_APPDOMAIN, LL_INFO100, "____extTransitionFrame::UninstallExceptionHandler: uninstall from top\n"));
        return;
    }
    // there may have been other EH frames installed between the insertion of this frame and
    // the arrival at this point, so remove ourselves from the right spot.
    while (pCurrExRecord != (EXCEPTION_REGISTRATION_RECORD*) -1 && pCurrExRecord->Next < pExRecord) 
    {
        pCurrExRecord = pCurrExRecord->Next;
    }
    if (pCurrExRecord == (EXCEPTION_REGISTRATION_RECORD*) -1 || pCurrExRecord->Next > pExRecord)
    {
        // if we were already unwound off, so just return. This will happen if we didn't catch the exception
        // because it wasn't the type we cared about so someone above us caught it and then called rtlunwind
        // which unwound us.
        //LOG((LF_APPDOMAIN, LL_INFO100, "____extTransitionFrame::UninstallExceptionHandler: already unwound\n"));
        return;
    }

    //LOG((LF_APPDOMAIN, LL_INFO100, "____extTransitionFrame::UninstallExceptionHandler: uninstall from middle\n"));
    pCurrExRecord->Next = pExRecord->Next;
#ifdef _DEBUG
    pExRecord->Handler = NULL;
    pExRecord->Next = NULL;
#endif
} 

void UnmanagedToManagedCallFrame::ExceptionUnwind()
{
    UnmanagedToManagedFrame::ExceptionUnwind();
    GetCleanupWorkList()->Cleanup(TRUE);
    AppDomain::ExceptionUnwind(this);
}

void ContextTransitionFrame::ExceptionUnwind()
{
    THROWSCOMPLUSEXCEPTION();

    Thread *pThread = GetThread();
    // turn the abort request into an AD unloaded exception when go past the boundary
    if (pThread->ShouldChangeAbortToUnload(this))
    {
		LOG((LF_APPDOMAIN, LL_INFO10, "ContextTransitionFrame::ExceptionUnwind turning abort into unload\n"));
        COMPlusThrow(kAppDomainUnloadedException, L"Remoting_AppDomainUnloaded_ThreadUnwound");
    }
}

#ifdef _SECURITY_FRAME_FOR_DISPEX_CALLS
//---------------------------------------------------------------------------
//  ClientSecurityFrame
//---------------------------------------------------------------------------
ComClientSecurityFrame::ComClientSecurityFrame(IServiceProvider *pISP)
{
    m_pISP = pISP;
    m_pSD  = NULL;
}

SecurityDescriptor* ComClientSecurityFrame::GetSecurityDescriptor()
{
    // Add code here post v1
    return NULL;
}

//---------------------------------------------------------------------------
#endif  // _SECURITY_FRAME_FOR_DISPEX_CALLS
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gc.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*++

Module Name:

    gc.h

--*/

#ifndef __GC_H
#define __GC_H

#ifdef PROFILING_SUPPORTED
#define GC_PROFILING       //Turn on profiling
#endif // PROFILING_SUPPORTED

/*
 * Promotion Function Prototypes
 */
typedef void enum_func (Object*);


/* forward declerations */
class gc_heap;
class CFinalize;
class CObjectHeader;
class Object;


/* misc defines */
#define LARGE_OBJECT_SIZE   85000

extern "C" BYTE* g_lowest_address;
extern "C" BYTE* g_highest_address;
extern "C" DWORD* g_card_table;

#ifdef _DEBUG
#define  _LOGALLOC
#if defined(SERVER_GC) && defined(WRITE_BARRIER_CHECK)
#undef WRITE_BARRIER_CHECK      // Does not work on SERVER_GC
#endif
#endif

#if WRITE_BARRIER_CHECK
extern BYTE* g_GCShadow;
extern BYTE* g_GCShadowEnd;
void initGCShadow();
void deleteGCShadow();
void updateGCShadow(Object** ptr, Object* val);
void checkGCWriteBarrier();
#else
inline void initGCShadow() {}
inline void deleteGCShadow() {}
inline void updateGCShadow(Object** ptr, Object* val) {}
inline void checkGCWriteBarrier() {}
#endif


void setCardTableEntryInterlocked(BYTE* location, BYTE* ref);

//server specific settings. 

#ifdef SERVER_GC

#define MULTIPLE_HEAPS
//#define INCREMENTAL_MEMCLR
#define MP_LOCKS

#endif //SERVER_GC

#ifdef MULTIPLE_HEAPS

#define PER_HEAP

#else //MULTIPLE_HEAPS

#define PER_HEAP static

#endif // MULTIPLE_HEAPS

#ifdef ISOLATED_HEAPS 

#define PER_HEAP_ISOLATED

#else //PER_HEAP_ISOLATED

#define PER_HEAP_ISOLATED static

#endif //PER_HEAP_ISOLATED

extern "C" BYTE* g_ephemeral_low;
extern "C" BYTE* g_ephemeral_high;


/*
 * Ephemeral Garbage Collected Heap Interface
 */

struct alloc_context 
{
    BYTE*          alloc_ptr;
    BYTE*          alloc_limit;
    __int64         alloc_bytes; //Number of bytes allocated by this context
#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
    GCHeap*        alloc_heap;
    GCHeap*        home_heap;
    int            alloc_count;
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS

    alloc_context()
    {
        init();
    }

    void init() 
    {
        alloc_ptr = 0;
        alloc_limit = 0;
        alloc_bytes = 0;
#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
        alloc_heap = 0;
        home_heap = 0;
        alloc_count = 0;
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS    
    }
};

struct ScanContext
{
    Thread* thread_under_crawl;
    int thread_number;
    BOOL promotion; //TRUE: Promotion, FALSE: Relocation.
    BOOL concurrent; //TRUE: concurrent scanning 
#if CHECK_APP_DOMAIN_LEAKS
    AppDomain *pCurrentDomain;
#endif
    
    ScanContext()
    {
        thread_under_crawl = 0;
        thread_number = -1;
        promotion = FALSE;
        concurrent = FALSE;
    }
};

#ifdef GC_PROFILING

struct ProfilingScanContext : ScanContext
{
    void *pHeapId;

    ProfilingScanContext() : ScanContext()
    {
        pHeapId = NULL;
    }
};

typedef BOOL (* walk_fn)(Object*, void*);
void walk_object (Object* obj, walk_fn fn, void* context);

#endif //GC_PROFILING

//dynamic data interface
struct gc_counters
{
    size_t current_size;
    size_t promoted_size;
    size_t collection_count;
};

//constants for the flags parameter to the gc call back

#define GC_CALL_INTERIOR            0x1
#define GC_CALL_PINNED              0x2
#define GC_CALL_CHECK_APP_DOMAIN    0x4

//flags for GCHeap::Alloc(...)
#define GC_ALLOC_FINALIZE 0x1
#define GC_ALLOC_CONTAINS_REF 0x2


class GCHeap
{
    friend HRESULT InitializeMiniDumpBlock();

protected:

#ifdef MULTIPLE_HEAPS
    gc_heap*    pGenGCHeap;
#else
    #define pGenGCHeap ((gc_heap*)0)
#endif //MULTIPLE_HEAPS
    
    friend class CFinalize;
    friend class gc_heap;
    friend void EnterAllocLock();
    friend void LeaveAllocLock();
    friend void EEShutDown(BOOL fIsDllUnloading);
    friend void ProfScanRootsHelper(Object*& object, ScanContext *pSC, DWORD dwFlags);
    friend void GCProfileWalkHeap();

    //In order to keep gc.cpp cleaner, ugly EE specific code is relegated to methods. 
    PER_HEAP_ISOLATED   void UpdatePreGCCounters();
    PER_HEAP_ISOLATED   void UpdatePostGCCounters();

public:
    GCHeap(){};
    ~GCHeap(){};

    /* BaseGCHeap Methods*/
    PER_HEAP_ISOLATED   HRESULT Shutdown ();

    PER_HEAP_ISOLATED   size_t  GetTotalBytesInUse ();

    PER_HEAP_ISOLATED   BOOL    IsGCInProgress ()    
    { return GcInProgress; }

    PER_HEAP    Thread* GetGCThread ()       
    { return GcThread; };

    PER_HEAP    Thread* GetGCThreadAttemptingSuspend()
    {
        return m_GCThreadAttemptingSuspend;
    }

    PER_HEAP_ISOLATED   void    WaitUntilGCComplete ();

    PER_HEAP            HRESULT Initialize ();

    //flags can be GC_ALLOC_CONTAINS_REF GC_ALLOC_FINALIZE
    PER_HEAP_ISOLATED Object*  Alloc (DWORD size, DWORD flags);
    PER_HEAP_ISOLATED Object*  AllocLHeap (DWORD size, DWORD flags);
    
    PER_HEAP_ISOLATED Object* Alloc (alloc_context* acontext, 
                                         DWORD size, DWORD flags);

    PER_HEAP_ISOLATED void FixAllocContext (alloc_context* acontext,
                                            BOOL lockp, void* arg);

#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
    static void AssignHeap (alloc_context* acontext);
    static GCHeap* GetHeap (int);
    static int GetNumberOfHeaps ();
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS
    
    static BOOL IsLargeObject(MethodTable *mt);

    static BOOL IsObjectInFixedHeap(Object *pObj);

    PER_HEAP_ISOLATED       HRESULT GarbageCollect (int generation = -1, 
                                        BOOL collectClasses=FALSE);
    PER_HEAP_ISOLATED       HRESULT GarbageCollectPing (int generation = -1, 
                                        BOOL collectClasses=FALSE);

    // Drain the queue of objects waiting to be finalized.
    PER_HEAP_ISOLATED    void    FinalizerThreadWait(int timeout = INFINITE);

    ////
    // GC callback functions
    // Check if an argument is promoted (ONLY CALL DURING
    // THE PROMOTIONSGRANTED CALLBACK.)
    PER_HEAP_ISOLATED    BOOL    IsPromoted (Object *object, 
                                             ScanContext* sc);

    // promote an object
    PER_HEAP_ISOLATED    void    Promote (Object*& object, 
                                          ScanContext* sc,
                                          DWORD flags=0);

    // Find the relocation address for an object
    PER_HEAP_ISOLATED    void    Relocate (Object*& object,
                                           ScanContext* sc, 
                                           DWORD flags=0);


    PER_HEAP            HRESULT Init (size_t heapSize);

    //Register an object for finalization
    PER_HEAP_ISOLATED    void    RegisterForFinalization (int gen, Object* obj); 
    
    //Unregister an object for finalization
    PER_HEAP_ISOLATED    void    SetFinalizationRun (Object* obj); 
    
    //returns the generation number of an object (not valid during relocation)
    PER_HEAP_ISOLATED    unsigned WhichGeneration (Object* object);
    // returns TRUE is the object is ephemeral 
    PER_HEAP_ISOLATED    BOOL    IsEphemeral (Object* object);
#ifdef VERIFY_HEAP
    PER_HEAP_ISOLATED    BOOL    IsHeapPointer (void* object, BOOL small_heap_only = FALSE);
#endif //_DEBUG

    PER_HEAP    size_t  ApproxTotalBytesInUse(BOOL small_heap_only = FALSE);
    PER_HEAP    size_t  ApproxFreeBytes();
    

    static      BOOL    HandlePageFault(void*);//TRUE handled, FALSE propagate

    PER_HEAP_ISOLATED   unsigned GetCondemnedGeneration()
    { return GcCondemnedGeneration;}


    PER_HEAP_ISOLATED     unsigned GetMaxGeneration();
 
    //suspend all threads

    typedef enum
    {
        SUSPEND_OTHER                   = 0,
        SUSPEND_FOR_GC                  = 1,
        SUSPEND_FOR_APPDOMAIN_SHUTDOWN  = 2,
        SUSPEND_FOR_CODE_PITCHING       = 3,
        SUSPEND_FOR_SHUTDOWN            = 4,
        SUSPEND_FOR_DEBUGGER            = 5,
        SUSPEND_FOR_INPROC_DEBUGGER     = 6,
        SUSPEND_FOR_GC_PREP             = 7
    } SUSPEND_REASON;

    PER_HEAP_ISOLATED void SuspendEE(SUSPEND_REASON reason);

    PER_HEAP_ISOLATED void RestartEE(BOOL bFinishedGC, BOOL SuspendSucceded); //resume threads. 

    PER_HEAP_ISOLATED inline SUSPEND_REASON GetSuspendReason()
    { return (m_suspendReason); }

    PER_HEAP_ISOLATED inline void SetSuspendReason(SUSPEND_REASON suspendReason)
    { m_suspendReason = suspendReason; }

    PER_HEAP_ISOLATED  Thread* GetFinalizerThread();

        //  Returns TRUE if the current thread is the finalizer thread.
    PER_HEAP_ISOLATED   BOOL    IsCurrentThreadFinalizer();

    // allow finalizer thread to run
    PER_HEAP_ISOLATED    void    EnableFinalization( void );

    // Start unloading app domain
    PER_HEAP_ISOLATED   void    UnloadAppDomain( AppDomain *pDomain, BOOL fRunFinalizers ) 
      { UnloadingAppDomain = pDomain; fRunFinalizersOnUnload = fRunFinalizers; }

    // Return current unloading app domain (NULL when unload is finished.)
    PER_HEAP_ISOLATED   AppDomain*  GetUnloadingAppDomain() { return UnloadingAppDomain; }

    // Lock for allocation Public because of the 
    // fast allocation helper
#ifdef ISOLATED_HEAPS
    PER_HEAP    volatile LONG m_GCLock;
#endif

    PER_HEAP_ISOLATED unsigned GetGcCount() { return GcCount; }

    PER_HEAP_ISOLATED HRESULT GetGcCounters(int gen, gc_counters* counters);

    static BOOL IsValidSegmentSize(size_t cbSize);

    static BOOL IsValidGen0MaxSize(size_t cbSize);

    static size_t GetValidSegmentSize();

    static size_t GetValidGen0MaxSize(size_t seg_size);

    PER_HEAP_ISOLATED void SetReservedVMLimit (size_t vmlimit);

    PER_HEAP_ISOLATED Object* GetNextFinalizableObject();
    PER_HEAP_ISOLATED size_t GetNumberFinalizableObjects();
    PER_HEAP_ISOLATED size_t GetFinalizablePromotedCount();
    PER_HEAP_ISOLATED BOOL FinalizeAppDomain(AppDomain *pDomain, BOOL fRunFinalizers);
    PER_HEAP_ISOLATED void SetFinalizeQueueForShutdown(BOOL fHasLock);

protected:

    // Lock for finalization
    PER_HEAP_ISOLATED   
        volatile        LONG    m_GCFLock;

    PER_HEAP_ISOLATED   BOOL    GcCollectClasses;
    PER_HEAP_ISOLATED
        volatile        BOOL    GcInProgress;       // used for syncing w/GC
    PER_HEAP_ISOLATED
              SUSPEND_REASON    m_suspendReason;    // This contains the reason
                                                    // that the runtime was suspended
public:                                                    
    PER_HEAP_ISOLATED   Thread* GcThread;           // thread running GC
protected:    
    PER_HEAP_ISOLATED   Thread* m_GCThreadAttemptingSuspend;
    PER_HEAP_ISOLATED   unsigned GcCount;
    PER_HEAP_ISOLATED   unsigned GcCondemnedGeneration;

    
    // Use only for GC tracing.
    PER_HEAP    unsigned long GcDuration;



    // Interface with gc_heap
    PER_HEAP_ISOLATED   int     GarbageCollectTry (int generation, 
                                        BOOL collectClasses=FALSE);
    PER_HEAP_ISOLATED   void    GarbageCollectGeneration (unsigned int gen=0, 
                                                  BOOL collectClasses = FALSE);
    // Finalizer thread stuff.


    
    PER_HEAP_ISOLATED   BOOL    FinalizerThreadWatchDog();
    PER_HEAP_ISOLATED   BOOL    FinalizerThreadWatchDogHelper();
    PER_HEAP_ISOLATED   DWORD   FinalizerThreadCreate();
    PER_HEAP_ISOLATED   ULONG   __stdcall FinalizerThreadStart(void *args);
    PER_HEAP_ISOLATED   HANDLE  WaitForGCEvent;     // used for syncing w/GC
    PER_HEAP_ISOLATED   HANDLE  hEventFinalizer;
    PER_HEAP_ISOLATED   HANDLE  hEventFinalizerDone;
    PER_HEAP_ISOLATED   HANDLE  hEventFinalizerToShutDown;
    PER_HEAP_ISOLATED   HANDLE  hEventShutDownToFinalizer;
    PER_HEAP_ISOLATED   BOOL    fQuitFinalizer;
public:    
    PER_HEAP_ISOLATED   Thread *FinalizerThread;
protected:    
    PER_HEAP_ISOLATED   AppDomain *UnloadingAppDomain;
    PER_HEAP_ISOLATED   BOOL    fRunFinalizersOnUnload;

    PER_HEAP_ISOLATED    CFinalize* m_Finalize;

    PER_HEAP_ISOLATED   gc_heap* Getgc_heap();

#ifdef STRESS_HEAP 
public:
    PER_HEAP_ISOLATED   void    StressHeap(alloc_context * acontext = 0);
protected:

#if !defined(MULTIPLE_HEAPS)
    // handles to hold the string objects that will force GC movement
    enum { NUM_HEAP_STRESS_OBJS = 8 };
    PER_HEAP OBJECTHANDLE m_StressObjs[NUM_HEAP_STRESS_OBJS];
    PER_HEAP int m_CurStressObj;
#endif  // !defined(MULTIPLE_HEAPS)
#endif  // STRESS_HEAP 

#if 0

#ifdef COLLECT_CLASSES
    PER_HEAP    HRESULT QueueClassForFinalization (EEClass*);
    PER_HEAP    void    GCPromoteFinalizableClasses( ScanContext* sc);
    PER_HEAP    BOOL    QueueClassForDeletion( EEClass* pClass );
#endif

    static      BOOL    SizeRequiresBigObject (DWORD size)
    {
        return size > LARGE_OBJECT_SIZE;
    }

    // Maximum possible allocation size
    static      size_t  MaxAllocationSize()
    {
        // See request2size, malloc_extend_top, and wsbrk in gmheap.cpp.
        // Heap requires 2 pages to extend top chunk and gc requires block header/alignment.
        // <BUGBUG>BUGBUG this isn't accurate to the bit but close enough</BUGBUG>
        return (size_t)((unsigned)(1<<31) - 3*OS_PAGE_SIZE);
    }
#endif

};

#ifndef ISOLATED_HEAPS
    extern volatile LONG m_GCLock;
#endif

void SetCardsAfterBulkCopy( Object**, size_t );



//#define TOUCH_ALL_PINNED_OBJECTS  // Force interop to touch all pages straddled by pinned objects.


// Go through and touch (read) each page straddled by a memory block.
void TouchPages(LPVOID pStart, UINT cb);

#ifdef VERIFY_HEAP
void    ValidateObjectMember (Object *obj);
#endif

#endif // __GC_H
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\frames.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
// FRAMES.H -
//
// These C++ classes expose activation frames to the rest of the EE.
// Activation frames are actually created by JIT-generated or stub-generated
// code on the machine stack. Thus, the layout of the Frame classes and
// the JIT/Stub code generators are tightly interwined.
//
// IMPORTANT: Since frames are not actually constructed by C++,
// don't try to define constructor/destructor functions. They won't get
// called.
//
// IMPORTANT: Not all methods have full-fledged activation frames (in
// particular, the JIT may create frameless methods.) This is one reason
// why Frame doesn't expose a public "Next()" method: such a method would
// skip frameless method calls. You must instead use one of the
// StackWalk methods.
//
//
// The following is the hierarchy of frames:
//
//    Frame                   - the root class. There are no actual instances
//    |                         of Frames.
//    |
//    +-ComPrestubMethodFrame - prestub frame for calls from com to com+
//    |
//    |
//    +-GCFrame               - this frame doesn't represent a method call.
//    |                         it's sole purpose is to let the EE gc-protect
//    |                         object references that it is manipulating.
//    |
//    +-HijackFrame           - if a method's return address is hijacked, we
//    |                         construct one of these to allow crawling back
//    |                         to where the return should have gone.
//    |
//    +-InlinedCallFrame      - if a call to unmanaged code is hoisted into
//    |                         a JIT'ted caller, the calling method keeps
//    |                         this frame linked throughout its activation.
//    |
//    +-ResumableFrame        - this frame provides the context necessary to
//    | |                       allow garbage collection during handling of
//    | |                       a resumable exception (e.g. during edit-and-continue,
//    | |                       or under GCStress4).
//    | |
//    | +-RedirectedThreadFrame - this frame is used for redirecting threads during suspension
//    |
//    +-TransitionFrame       - this frame represents a transition from
//    | |                       one or more nested frameless method calls
//    | |                       to either a EE runtime helper function or
//    | |                       a framed method.
//    | |
//    | +-ExceptionFrame        - this frame has caused an exception
//    | | |
//    | | |
//    | | +- FaultingExceptionFrame - this frame was placed on a method which faulted
//    | |                              to save additional state information
//    | |
//    | +-FuncEvalFrame         - frame for debugger function evaluation
//    | |
//    | |
//    | +-HelperMethodFrame     - frame used allow stack crawling inside jit helpers and fcalls
//    | |
//    | |
//    | +-FramedMethodFrame   - this frame represents a call to a method
//    |   |                     that generates a full-fledged frame.
//    |   |
//    |   +-ECallMethodFrame     - represents a direct call to the EE.
//    |   |
//    |   +-FCallMethodFrame     - represents a fast direct call to the EE.
//    |   |
//    |   +-NDirectMethodFrame   - represents an N/Direct call.
//    |   | |
//    |   | +-NDirectMethodFrameEx - represents an N/Direct call w/ cleanup
//    |   |
//    |   +-ComPlusMethodFrame   - represents a complus to com call
//    |   | |
//    |   | +-ComPlusMethodFrameEx - represents an complus to com call w/ cleanup
//    |   |
//    |   +-PrestubFrame         - represents a call to a prestub
//    |   |
//    |   +-CtxCrossingFrame     - this frame marks a call across a context
//    |   |                        boundary
//    |   |
//    |   +-MulticastFrame       - this frame protects arguments to a MulticastDelegate
//    |   |                         Invoke() call while calling each subscriber.
//    |   |
//    |   +-PInovkeCalliFrame   - represents a calli to unmanaged target
//    |  
//    |  
//    +-UnmanagedToManagedFrame - this frame represents a transition from
//    | |                         unmanaged code back to managed code. It's
//    | |                         main functions are to stop COM+ exception
//    | |                         propagation and to expose unmanaged parameters.
//    | |
//    | +-UnmanagedToManagedCallFrame - this frame is used when the target
//    |   |                             is a COM+ function or method call. it
//    |   |                             adds the capability to gc-promote callee
//    |   |                             arguments during marshaling.
//    |   |
//    |   +-ComMethodFrame      - this frame represents a transition from
//    |   |                       com to com+
//    |   |
//    |   +-UMThunkCallFrame    - this frame represents an unmanaged->managed
//    |                           transition through N/Direct
//    |
//    +-CtxMarshaledFrame  - this frame represent a cross context marshalled call
//    |                      (cross thread,inter-process, cross m/c scenarios)
//    |
//    +-CtxByValueFrame    - this frame is used to support protection of a by-
//    |                      value marshaling session, even though the thread is
//    |                      not pushing a call across a context boundary.
//    |
//    +-ContextTransitionFrame - this frame is used to mark an appdomain transition
//    |
//    +-NativeClientSecurityFrame -  this frame is used to capture the security 
//       |                           context in a Native -> Managed call. Code 
//       |                           Acess security stack walk use caller 
//       |                           information in  this frame to apply 
//       |                           security policy to the  native client.
//       |
//       +-ComClientSecurityFrame -  Security frame for Com clients. 
//                                   VBScript, JScript, IE ..


#ifndef __frames_h__
#define __frames_h__

#include "util.hpp"
#include "vars.hpp"
#include "method.hpp"
#include "object.h"
#include "objecthandle.h"
#include "regdisp.h"
#include <stddef.h>
#include "gcscan.h"
#include "siginfo.hpp"
// context headers
#include "context.h"
#include "stubmgr.h"
#include "gms.h"
#include "threads.h"
// remoting headers
//#include "remoting.h"

// Forward references
class Frame;
class FieldMarshaler;
class FramedMethodFrame;
struct HijackArgs;
class UMEntryThunk;
class UMThunkMarshInfo;
class Marshaler;
class SecurityDescriptor;

// Security Frame for all IDispEx::InvokeEx calls. Enable in security.h also
// Consider post V.1
// #define _SECURITY_FRAME_FOR_DISPEX_CALLS

//------------------------------------------------------------
// GC-promote all arguments in the shadow stack. pShadowStackVoid points
// to the lowest addressed argument (which points to the "this" reference
// for instance methods and the *rightmost* argument for static methods.)
//------------------------------------------------------------
VOID PromoteShadowStack(LPVOID pShadowStackVoid, MethodDesc *pFD, promote_func* fn, ScanContext* sc);


//------------------------------------------------------------
// Copy pFrame's arguments into pShadowStackOut using the virtual calling
// convention format. The size of the buffer must be equal to
// pFrame->GetFunction()->SizeOfVirtualFixedArgStack().
// This function also copies the "this" reference if applicable.
//------------------------------------------------------------
VOID FillinShadowStack(FramedMethodFrame *pFrame, LPVOID pShadowStackOut_V);


//------------------------------------------------------------------------
// CleanupWorkList
//
// A CleanupWorkList is a LIFO list of tasks to be carried out at a later
// time. It's designed for use in managed->unmanaged calls.
//
// NOTE: CleanupWorkList's are designed to be embedded inside method frames,
// hence they can be constructed by stubs. Thus, any changes to the layout
// or constructor will also require changing some stubs.
//
// CleanupWorkLists are not synchronized for multithreaded use.
//
// The current layout of a CleanupWorkList is simply a pointer to a linked
// list of tasks (CleanupNodes.) This makes it very easy for a stub to
// stack-allocate an empty CleanupWorkList (just push a NULL pointer)
// and equally easy for it to inline a test to see if any cleanup needs
// to be done.
//
// NOTE: CleanupTasks can execute during exception handling so they
// shouldn't be calling other managed code or throwing COM+ exceptions.
//------------------------------------------------------------------------
class CleanupWorkList
{
    public:
        //-------------------------------------------------------------------
        // Constructor.
        //-------------------------------------------------------------------
        CleanupWorkList()
        {
            // NOTE: IF YOU CHANGE THIS, YOU WILL ALSO HAVE TO CHANGE SOME
            // STUBS.
            m_pNodes = NULL;
        }

        //-------------------------------------------------------------------
        // Destructor (calls Cleanup(FALSE))
        //-------------------------------------------------------------------
        ~CleanupWorkList();


        //-------------------------------------------------------------------
        // Executes each stored cleanup task and resets the worklist back
        // to empty. Some task types are conditional based on the
        // "fBecauseOfException" flag. This flag distinguishes between
        // cleanups due to normal method termination and cleanups due to
        // an exception.
        //-------------------------------------------------------------------
        VOID Cleanup(BOOL fBecauseOfException);


        //-------------------------------------------------------------------
        // Allocates a gc-protected handle. This handle is unconditionally
        // destroyed on a Cleanup().
        // Throws a COM+ exception if failed.
        //-------------------------------------------------------------------
        OBJECTHANDLE NewScheduledProtectedHandle(OBJECTREF oref);


        //-------------------------------------------------------------------
        // CoTaskFree memory unconditionally
        //-------------------------------------------------------------------
        VOID ScheduleCoTaskFree(LPVOID pv);

        //-------------------------------------------------------------------
        // StackingAllocator.Collapse during exceptions
        //-------------------------------------------------------------------
        VOID ScheduleFastFree(LPVOID checkpoint);


        //-------------------------------------------------------------------
        // Schedules an unconditional release of a COM IP
        // Throws a COM+ exception if failed.
        //-------------------------------------------------------------------
        VOID ScheduleUnconditionalRelease(IUnknown *ip);


        //-------------------------------------------------------------------
        // Schedules an unconditional free of the native version
        // of an NStruct reference field. Note that pNativeData points into
        // the middle of the external part of the NStruct, so someone
        // has to hold a gc reference to the wrapping NStruct until
        // the destroy is done.
        //-------------------------------------------------------------------
        VOID ScheduleUnconditionalNStructDestroy(const FieldMarshaler *pFieldMarshaler, LPVOID pNativeData);


        //-------------------------------------------------------------------
        // CleanupWorkList::ScheduleUnconditionalCultureRestore
        // schedule restoring thread's current culture to the specified 
        // culture.
        // Throws a COM+ exception if failed.
        //-------------------------------------------------------------------
        VOID ScheduleUnconditionalCultureRestore(OBJECTREF CultureObj);

        //-------------------------------------------------------------------
        // CleanupWorkList::ScheduleLayoutDestroy
        // schedule cleanup of marshaled struct fields and of the struct itself.
        // Throws a COM+ exception if failed.
        //-------------------------------------------------------------------
        LPVOID NewScheduleLayoutDestroyNative(MethodTable *pMT);


        //-------------------------------------------------------------------
        // CleanupWorkList::NewProtectedObjRef()
        // holds a protected objref (used for creating the buffer for
        // an unmanaged->managed byref object marshal. We can't use an
        // objecthandle because modifying those without using the handle
        // api opens up writebarrier violations.
        //
        // Must have called IsVisibleToGc() first.
        //-------------------------------------------------------------------
        OBJECTREF* NewProtectedObjectRef(OBJECTREF oref);

        //-------------------------------------------------------------------
        // CleanupWorkList::NewProtectedObjRef()
        // holds a Marshaler. The cleanupworklist will own the task
        // of calling the marshaler's GcScanRoots fcn.
        //
        // It makes little architectural sense for the CleanupWorkList to
        // own this item. But it's late in the project to be adding
        // fields to frames, and it so happens everyplace we need this thing,
        // there's alreay a cleanuplist. So it's elected.
        //
        // Must have called IsVisibleToGc() first.
        //-------------------------------------------------------------------
        VOID NewProtectedMarshaler(Marshaler *pMarshaler);


        //-------------------------------------------------------------------
        // CleanupWorkList::ScheduleMarshalerCleanupOnException()
        // holds a Marshaler. The cleanupworklist will own the task
        // of calling the marshaler's DoExceptionCleanup() if an exception
        // occurs.
        //
        // The return value is a cookie thru which the marshaler can
        // cancel this item. It must do this once to avoid double
        // destruction if the marshaler cleanups normally.
        //-------------------------------------------------------------------
        class MarshalerCleanupNode;
        MarshalerCleanupNode *ScheduleMarshalerCleanupOnException(Marshaler *pMarshaler);


        //-------------------------------------------------------------------
        // CleanupWorkList::IsVisibleToGc()
        //-------------------------------------------------------------------
        VOID IsVisibleToGc()
        {
#ifdef _DEBUG
            Schedule(CL_ISVISIBLETOGC, NULL);
#endif
        }



        //-------------------------------------------------------------------
        // If you've called IsVisibleToGc(), must call this.
        //-------------------------------------------------------------------
        void GcScanRoots(promote_func *fn, ScanContext* sc);




    private:
        //-------------------------------------------------------------------
        // Cleanup task types.
        //-------------------------------------------------------------------
        enum CleanupType {
            CL_GCHANDLE,    //gc protected handle,
            CL_COTASKFREE,      // unconditional cotaskfree
            CL_FASTFREE,       // unconditionally StackingAllocator.Collapse
            CL_RELEASE,        // IUnknown::Release
            CL_NSTRUCTDESTROY, // unconditionally do a DestroyNative on an NStruct ref field
            CL_RESTORECULTURE, // unconditionally restore the culture
            CL_NEWLAYOUTDESTROYNATIVE,

            CL_PROTECTEDOBJREF, // holds a GC protected OBJECTREF - similar to CL_GCHANDLE
                             // but can be safely written into without updating
                             // write barrier.
                             //
                             // Must call IsVisibleToGc() before using this nodetype.
                             //
            CL_PROTECTEDMARSHALER, // holds a GC protected marshaler
                             // Must call IsVisibleToGc() before using this nodetype.


            CL_ISVISIBLETOGC,// a special do-nothing nodetype that simply
                             // records that "IsVisibleToGc()" was called on this.


            CL_MARSHALER_EXCEP, // holds a marshaler for cleanup on exception
            CL_MARSHALERREINIT_EXCEP, // holds a marshaler for reiniting on exception
        };

        //-------------------------------------------------------------------
        // These are linked into a list.
        //-------------------------------------------------------------------
        struct CleanupNode {
            CleanupType     m_type;       // See CleanupType enumeration
            CleanupNode    *m_next;       // pointer to next task
#ifdef _DEBUG
            DWORD m_dwDomainId;           // domain Id of list. 
#endif
            union {
                BSTR        m_bstr;       // bstr for CL_SYSFREE_EXCEP
                OBJECTHANDLE m_oh;        // CL_GCHANDLE
                Object*     m_oref;       // CL_PROTECTEDOBJREF
                IUnknown   *m_ip;         // if CL_RELEASE
                LPVOID      m_pv;         // CleanupType-dependent contents.
                SAFEARRAY  *m_safearray;
                Marshaler  *m_pMarshaler;

                struct {                  // if CL_NSTRUCTDESTROY
                    const FieldMarshaler *m_pFieldMarshaler;
                    LPVOID                m_pNativeData;
                } nd;

                struct {
                    LPVOID  m_pnative;
                    MethodTable *m_pMT;
                } nlayout;
            };

        };


        //-------------------------------------------------------------------
        // Inserts a new task of the given type and datum.
        // Returns non NULL on success.
        //-------------------------------------------------------------------
        CleanupNode* Schedule(CleanupType ct, LPVOID pv);

    public:
        class MarshalerCleanupNode : private CleanupNode
        {
            // DO NOT ADD ANY FIELDS!
            public:
                void CancelCleanup()
                {
                    m_type = CL_MARSHALERREINIT_EXCEP;
                }
    
        };


    private:
        // NOTE: If you change the layout of this structure, you will
        // have to change some stubs which build and manipulate
        // CleanupWorkLists.
        CleanupNode     *m_pNodes;   //Pointer to first task.
};




// Note: the value (-1) is used to generate the largest
// possible pointer value: this keeps frame addresses
// increasing upward.
#define FRAME_TOP ((Frame*)(-1))

#define RETURNFRAMEVPTR(classname) \
    classname boilerplate;      \
    return *((LPVOID*)&boilerplate)

#define DEFINE_VTABLE_GETTER(klass)             \
    public:                                     \
        friend struct MEMBER_OFFSET_INFO(klass);\
        static LPVOID GetFrameVtable() {        \
            klass boilerplate(false);           \
            return *((LPVOID*)&boilerplate);    \
        }                                       \
        klass(bool dummy) { }

#define DEFINE_VTABLE_GETTER_AND_CTOR(klass)    \
        DEFINE_VTABLE_GETTER(klass)             \
    protected:                                  \
        klass() { }

//------------------------------------------------------------------------
// Frame defines methods common to all frame types. There are no actual
// instances of root frames.
//------------------------------------------------------------------------

class Frame
{
public:
    virtual void GcScanRoots(promote_func *fn, ScanContext* sc) = 0;

    //------------------------------------------------------------------------
    // Special characteristics of a frame
    //------------------------------------------------------------------------
    enum FrameAttribs {
        FRAME_ATTR_NONE = 0,
        FRAME_ATTR_EXCEPTION = 1,           // This frame caused an exception
        FRAME_ATTR_OUT_OF_LINE = 2,         // The exception out of line (IP of the frame is not correct)
        FRAME_ATTR_FAULTED = 4,             // Exception caused by Win32 fault
        FRAME_ATTR_RESUMABLE = 8,           // We may resume from this frame
        FRAME_ATTR_RETURNOBJ = 0x10,        // Frame returns an object (helperFrame only)
        FRAME_ATTR_RETURN_INTERIOR = 0x20,  // Frame returns an interior GC poitner (helperFrame only)
        FRAME_ATTR_CAPUTURE_DEPTH_2 = 0x40, // This is a helperMethodFrame and the capture occured at depth 2
        FRAME_ATTR_EXACT_DEPTH = 0x80,      // This is a helperMethodFrame and a jit helper, but only crawl to the given depth
    };
    virtual unsigned GetFrameAttribs()
    {
        return FRAME_ATTR_NONE;
    }

    //------------------------------------------------------------------------
    // Performs cleanup on an exception unwind
    //------------------------------------------------------------------------
    virtual void ExceptionUnwind()
    {
        // Nothing to do here.
    }

    //------------------------------------------------------------------------
    // Is this a frame used on transition to native code from jitted code?
    //------------------------------------------------------------------------
    virtual BOOL IsTransitionToNativeFrame()
    {
        return FALSE;
    }

    virtual MethodDesc *GetFunction()
    {
        return NULL;
    }

    virtual MethodDesc::RETURNTYPE ReturnsObject()
    {
        MethodDesc* pMD = GetFunction();
        if (pMD == 0)
            return(MethodDesc::RETNONOBJ);
        return(pMD->ReturnsObject());
    }

    // indicate the current X86 IP address within the current method
    // return 0 if the information is not available
    virtual const BYTE* GetIP()
    {
        return NULL;
    }

    virtual LPVOID GetReturnAddress()
    {
        return NULL;
    }

    virtual LPVOID* GetReturnAddressPtr()
    {
        return NULL;
    }

    virtual Context **GetReturnContextAddr()
    {
        return NULL;
    }

    virtual Object **GetReturnLogicalCallContextAddr()
    {
        return NULL;
    }

    virtual Object **GetReturnIllogicalCallContextAddr()
    {
        return NULL;
    }

    virtual ULONG_PTR* GetWin32ContextAddr()
    {
        return NULL;
    }

    void SetReturnContext(Context *pReturnContext)
    {
        Context **ppReturnContext = GetReturnContextAddr();
        _ASSERTE(ppReturnContext);
        *ppReturnContext = pReturnContext;
    }

    Context *GetReturnContext()
    {
        Context **ppReturnContext = GetReturnContextAddr();
        if (! ppReturnContext)
            return NULL;
        return *ppReturnContext;
    }

    AppDomain *GetReturnDomain()
    {
        if (! GetReturnContext())
            return NULL;
        return GetReturnContext()->GetDomain();
    }

    void SetReturnLogicalCallContext(OBJECTREF ref)
    {
        Object **pRef = GetReturnLogicalCallContextAddr();
        if (pRef != NULL)
            *pRef = OBJECTREFToObject(ref);
    }

    OBJECTREF GetReturnLogicalCallContext()
    {
        Object **pRef = GetReturnLogicalCallContextAddr();
        if (pRef == NULL)
            return NULL;
        else
            return ObjectToOBJECTREF(*pRef);
    }

    void SetReturnIllogicalCallContext(OBJECTREF ref)
    {
        Object **pRef = GetReturnIllogicalCallContextAddr();
        if (pRef != NULL)
            *pRef = OBJECTREFToObject(ref);
    }

    OBJECTREF GetReturnIllogicalCallContext()
    {
        Object **pRef = GetReturnIllogicalCallContextAddr();
        if (pRef == NULL)
            return NULL;
        else
            return ObjectToOBJECTREF(*pRef);
    }

    void SetWin32Context(ULONG_PTR cookie)
    {
        ULONG_PTR* pAddr = GetWin32ContextAddr();
        if(pAddr != NULL)
            *pAddr = cookie;
    }

    ULONG_PTR GetWin32Context()
    {
        ULONG_PTR* pAddr = GetWin32ContextAddr();
        if(pAddr == NULL)
            return NULL;
        else
            return *pAddr;
    }

    virtual void UpdateRegDisplay(const PREGDISPLAY)
    {
        return;
    }

    //------------------------------------------------------------------------
    // Debugger support
    //------------------------------------------------------------------------

    enum
    {
        TYPE_INTERNAL,
        TYPE_ENTRY,
        TYPE_EXIT,
        TYPE_CONTEXT_CROSS,
        TYPE_INTERCEPTION,
        TYPE_SECURITY,
        TYPE_CALL,
        TYPE_FUNC_EVAL,
        TYPE_TP_METHOD_FRAME,
        TYPE_MULTICAST,
        
        TYPE_COUNT
    };

    virtual int GetFrameType()
    {
        return TYPE_INTERNAL;
    };

    // When stepping into a method, various other methods may be called.
    // These are refererred to as interceptors. They are all invoked
    // with frames of various types. GetInterception() indicates whether
    // the frame was set up for execution of such interceptors

    enum Interception
    {
        INTERCEPTION_NONE,
        INTERCEPTION_CLASS_INIT,
        INTERCEPTION_EXCEPTION,
        INTERCEPTION_CONTEXT,
        INTERCEPTION_SECURITY,
        INTERCEPTION_OTHER,

        INTERCEPTION_COUNT
    };

    virtual Interception GetInterception()
    {
        return INTERCEPTION_NONE;
    }

    // Return information about an unmanaged call the frame
    // will make.
    // ip - the unmanaged routine which will be called
    // returnIP - the address in the stub which the unmanaged routine
    //            will return to.
    // returnSP - the location returnIP is pushed onto the stack
    //            during the call.
    //
    virtual void GetUnmanagedCallSite(void **ip,
                                      void **returnIP,
                                      void **returnSP)
    {
        if (ip)
            *ip = NULL;

        if (returnIP)
            *returnIP = NULL;

        if (returnSP)
            *returnSP = NULL;
    }

    // Return where the frame will execute next - the result is filled
    // into the given "trace" structure.  The frame is responsible for
    // detecting where it is in its execution lifetime.
    virtual BOOL TraceFrame(Thread *thread, BOOL fromPatch,
                            TraceDestination *trace, REGDISPLAY *regs)
    {
        LOG((LF_CORDB, LL_INFO10000,
             "Default TraceFrame always returns false.\n"));
        return FALSE;
    }

#if _DEBUG
    static void CheckExitFrameDebuggerCalls();
    static void CheckExitFrameDebuggerCallsWrap();
#endif

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static BYTE GetOffsetOfNextLink()
    {
        size_t ofs = offsetof(class Frame, m_Next);
        _ASSERTE(FitsInI1(ofs));
        return (BYTE)ofs;
    }

    // get your VTablePointer (can be used to check what type the frame is)
    LPVOID GetVTablePtr()
    {
        return(*((LPVOID*) this));
    }

    // Change the type of frame (pretty dangerous),
    void SetVTablePtr(LPVOID val)
    {
        *((LPVOID*) this) = val;
    }

#ifdef _DEBUG
    virtual BOOL Protects(OBJECTREF *ppObjectRef)
    {
        return FALSE;
    }
#endif

    // Link and Unlink this frame
    VOID Push();
    VOID Pop();
    VOID Push(Thread *pThread);
    VOID Pop(Thread *pThread);

#ifdef _DEBUG
    virtual void Log();
    static void LogTransition(Frame* frame) { frame->Log(); }
#endif

    // Returns true only for frames derived from FramedMethodFrame.
    virtual BOOL IsFramedMethodFrame() { return FALSE; }

private:
    // Pointer to the next frame up the stack.

protected:
    Frame   *m_Next;        // offset +4

private:
    // Because JIT-method activations cannot be expressed as Frames,
    // everyone must use the StackCrawler to walk the frame chain
    // reliably. We'll expose the Next method only to the StackCrawler
    // to prevent mistakes.
    /*@NICE: Restrict "friendship" again to the StackWalker method;
      not done because of circular dependency with threads.h
    */
    //        friend Frame* Thread::StackWalkFrames(PSTACKWALKFRAMESCALLBACK pCallback, VOID *pData);
    friend Thread;
    friend void CrawlFrame::GotoNextFrame();
    friend VOID RealCOMPlusThrow(OBJECTREF);
    Frame   *Next()
    {
        return m_Next;
    }


protected:
    // Frame is considered an abstract class: this protected constructor
    // causes any attempt to instantiate one to fail at compile-time.
    Frame() {}

    friend struct MEMBER_OFFSET_INFO(Frame);
};


#ifdef _DEBUG
class FCallInProgressFrame : public Frame
{
public:
    FCallInProgressFrame()
    {
        m_Next = FRAME_TOP;
    }

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {}

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER(FCallInProgressFrame)
};
#endif


//-------------------------------------------
// This frame provides context for a frame that
// took an exception that is going to be resumed.
//
// It is necessary to create this frame if garbage
// collection may happen during handling of the
// exception.  The FRAME_ATTR_RESUMABLE flag tells
// the GC that the preceding frame needs to be treated
// like the top of stack (with the important implication that
// caller-save-regsiters will be potential roots).
class ResumableFrame : public Frame
{
public:
    ResumableFrame(CONTEXT* regs) {
        m_Regs = regs;
    }

    virtual LPVOID* GetReturnAddressPtr();
    virtual LPVOID GetReturnAddress();
    virtual void UpdateRegDisplay(const PREGDISPLAY pRD);

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc) {
        // Nothing to do.
    }

    virtual unsigned GetFrameAttribs() {
        return FRAME_ATTR_RESUMABLE;    // Treat the next frame as the top frame.
    }

    virtual CONTEXT *GetContext() { return (m_Regs); }

private:
    CONTEXT* m_Regs;

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(ResumableFrame)
};

class RedirectedThreadFrame : public ResumableFrame
{
public:
    RedirectedThreadFrame(CONTEXT *regs) : ResumableFrame(regs) {}

    static LPVOID GetRedirectedThreadFrameVPtr()
    {
        RETURNFRAMEVPTR(RedirectedThreadFrame);
    }

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(RedirectedThreadFrame)
};

#define ISREDIRECTEDTHREAD(thread)                                                      \
    (thread->GetFrame() != FRAME_TOP &&                                                 \
     thread->GetFrame()->GetVTablePtr() == RedirectedThreadFrame::GetRedirectedThreadFrameVPtr())

//------------------------------------------------------------------------
// This frame represents a transition from one or more nested frameless
// method calls to either a EE runtime helper function or a framed method.
// Because most stackwalks from the EE start with a full-fledged frame,
// anything but the most trivial call into the EE has to push this
// frame in order to prevent the frameless methods inbetween from
// getting lost.
//------------------------------------------------------------------------
class TransitionFrame : public Frame
{
    public:

        virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
        {
            // Nothing to protect here.
        }


        // Retrieves the return address into the code that called the
        // helper or method.
        virtual LPVOID GetReturnAddress()
        {
            return m_ReturnAddress;
        }

        virtual void SetReturnAddress(LPVOID val)
        {
            m_ReturnAddress = val;
        }

        static BYTE GetOffsetOfReturnAddress()
        {
            size_t ofs = offsetof(class TransitionFrame, m_ReturnAddress);
            _ASSERTE(FitsInI1(ofs));
            return (BYTE)ofs;
        }


        virtual void UpdateRegDisplay(const PREGDISPLAY) = 0;

        virtual LPVOID* GetReturnAddressPtr()
        {
            return (&m_ReturnAddress);
        }


    protected:
        LPVOID  m_Datum;          // offset +8: contents depend on subclass
                                  // type.
        LPVOID  m_ReturnAddress;  // return address into JIT'ted code

        friend struct MEMBER_OFFSET_INFO(TransitionFrame);
};


//-----------------------------------------------------------------------
// TransitionFrames for exceptions
//-----------------------------------------------------------------------

class ExceptionFrame : public TransitionFrame
{
public:
    static LPVOID GetMethodFrameVPtr()
    {
        _ASSERTE(!"This is a pure virtual class");
        return 0;
    }

    Interception GetInterception()
    {
        return INTERCEPTION_EXCEPTION;
    }

    unsigned GetFrameAttribs()
    {
        return FRAME_ATTR_EXCEPTION;
    }

    friend struct MEMBER_OFFSET_INFO(ExceptionFrame);
};

class FaultingExceptionFrame : public ExceptionFrame
{
#ifdef _X86_
    DWORD m_Esp;
    CalleeSavedRegisters m_regs;
#endif

public:
    FaultingExceptionFrame() { m_Next = NULL; }
    void InitAndLink(CONTEXT *pContext);

    CalleeSavedRegisters *GetCalleeSavedRegisters()
    {
        return &m_regs;
    }
    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(FaultingExceptionFrame);
    }

    unsigned GetFrameAttribs()
    {
        return FRAME_ATTR_EXCEPTION | FRAME_ATTR_FAULTED;
    }

    virtual void UpdateRegDisplay(const PREGDISPLAY);

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER(FaultingExceptionFrame)
};



//-----------------------------------------------------------------------
// TransitionFrame for debugger function evaluation
//
// m_Datum holds a ptr to a DebuggerEval object which contains a copy
// of the thread's context at the time it was hijacked for the func
// eval.
//
// UpdateRegDisplay updates all registers inthe REGDISPLAY, not just
// the callee saved registers, because we can hijack for a func eval
// at any point in a thread's execution.
//
// No callee saved registers are held in the negative space for this
// type of frame.
//
//-----------------------------------------------------------------------

class FuncEvalFrame : public TransitionFrame
{
public:
    FuncEvalFrame(void *pDebuggerEval, LPVOID returnAddress)
    {
        m_Datum = pDebuggerEval;
        m_ReturnAddress = returnAddress;
    }

    virtual BOOL IsTransitionToNativeFrame()
    {
        return FALSE; 
    }

    virtual int GetFrameType()
    {
        return TYPE_FUNC_EVAL;
    };

    virtual unsigned GetFrameAttribs();

    virtual void UpdateRegDisplay(const PREGDISPLAY);

    virtual void *GetDebuggerEval()
    {
        return (void*)m_Datum;
    }
    
    virtual LPVOID GetReturnAddress(); 

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(FuncEvalFrame)
};


//-----------------------------------------------------------------------
// Provides access to the caller's arguments from a FramedMethodFrame.
// Does *not* include the "this" pointer.
//-----------------------------------------------------------------------
class ArgIterator
{
    public:
        //------------------------------------------------------------
        // Constructor
        //------------------------------------------------------------
        ArgIterator(FramedMethodFrame *pFrame, MetaSig* pSig);

        //------------------------------------------------------------
        // Another constructor when you dont have active frame FramedMethodFrame
        //------------------------------------------------------------
        ArgIterator(LPBYTE pFrameBase, MetaSig* pSig, BOOL fIsStatic);

        // An even more primitive constructor when dont have have a
        // a FramedMethodFrame
        ArgIterator(LPBYTE pFrameBase, MetaSig* pSig, int stackArgsOfs, int regArgsOfs);

        //------------------------------------------------------------
        // Each time this is called, this returns a pointer to the next
        // argument. This pointer points directly into the caller's stack.
        // Whether or not object arguments returned this way are gc-protected
        // depends on the exact type of frame.
        //
        // Returns NULL once you've hit the end of the list.
        //------------------------------------------------------------
        LPVOID GetNextArgAddr()
        {
            BYTE   typeDummy;
            UINT32 structSizeDummy;
            return GetNextArgAddr(&typeDummy, &structSizeDummy);
        }

        int GetThisOffset();
        int GetRetBuffArgOffset(UINT *pRegStructOfs = NULL);
        LPVOID* GetThisAddr()   {
            return((LPVOID*) (m_pFrameBase + GetThisOffset()));
        }
        LPVOID* GetRetBuffArgAddr() {
            return((LPVOID*) (m_pFrameBase + GetRetBuffArgOffset()));
        }

        //------------------------------------------------------------
        // Like GetNextArgAddr but returns information about the
        // param type (IMAGE_CEE_CS_*) and the structure size if apropos.
        //------------------------------------------------------------
        LPVOID GetNextArgAddr(BYTE *pType, UINT32 *pStructSize);

        //------------------------------------------------------------
        // Same as GetNextArgAddr() but returns a byte offset from
        // the Frame* pointer. This offset can be positive *or* negative.
        //
        // Returns 0 once you've hit the end of the list. Since the
        // the offset is relative to the Frame* pointer itself, 0 can
        // never point to a valid argument.
        //------------------------------------------------------------
        int    GetNextOffset()
        {
            BYTE   typeDummy;
            UINT32 structSizeDummy;
            return GetNextOffset(&typeDummy, &structSizeDummy);
        }

        //------------------------------------------------------------
        // Like GetNextArgOffset but returns information about the
        // param type (IMAGE_CEE_CS_*) and the structure size if apropos.
        // The optional pRegStructOfs param points to a buffer which receives
        // either the appropriate offset into the ArgumentRegisters struct or
        // -1 if the argument is on the stack.
        //------------------------------------------------------------
        int    GetNextOffset(BYTE *pType, UINT32 *pStructSize, UINT *pRegStructOfs = NULL);

        int    GetNextOffsetFaster(BYTE *pType, UINT32 *pStructSize, UINT *pRegStructOfs = NULL);

        // Must be called after all the args.  returns the offset of the
        // argument passed to methods implementing parameterized types.
        // If it is in a register pRegStructOfs is set, otherwise it is -1
        // In either case it returns off offset in the frame of the arg (assuming
        // it is framed).

        int     GetParamTypeArgOffset(INT *pRegStructOfs)
        {
            if (IsArgumentInRegister(&m_numRegistersUsed,
                                     ELEMENT_TYPE_I,
                                     sizeof(void*), FALSE,
                                     m_pSig->GetCallingConvention(),
                                     pRegStructOfs))
            {
                return m_regArgsOfs + *pRegStructOfs;
            }
            *pRegStructOfs = -1;
            return m_curOfs - sizeof(void*);
        }

        TypeHandle GetArgType();

    private:
        MetaSig*           m_pSig;
        int                m_curOfs;
        LPBYTE             m_pFrameBase;
        int                m_numRegistersUsed;
        int                m_regArgsOfs;        // add this to pFrameBase to find the the pointer
                                                // to where the last register based argument has
                                                // been saved in the frame (again stack grows down
                                                // first arg pushed first).  0 is an illegal value
                                                // than means the register args are not saved on the
                                                // stack.
                int                                     m_argNum;
};

//------------------------------------------------------------------------
// A HelperMethodFrame is created by jit helper (Modified slightly it could be used
// for native routines).   This frame just does the callee saved register
// fixup, it does NOT protect arguments (you can use GCPROTECT or the HelperMethodFrame subclases)
// see JitInterface for sample use, YOU CAN'T RETURN STATEMENT WHILE IN THE PROTECTED STATE!
//------------------------------------------------------------------------

class HelperMethodFrame : public TransitionFrame
{
public:
        // Lazy initialization of HelperMethodFrame.  Need to
        // call InsureInit to complete initialization
        // If this is an FCall, the second param is the entry point for the FCALL.
        // The MethodDesc will be looked up form this (lazily), and this method
        // will be used in stack reporting, if this is not an FCall pass a 0
    HelperMethodFrame(void* fCallFtnEntry, struct LazyMachState* ms, unsigned attribs = 0) {
         INDEBUG(memset(&m_Attribs, 0xCC, sizeof(HelperMethodFrame) - offsetof(HelperMethodFrame, m_Attribs));)
         m_Attribs = attribs;
         LazyInit(fCallFtnEntry, ms);
    }
       
        // If you give the optional MethodDesc parameter, then the frame
        // will act like like the given method for stack tracking purposes.
        // If you also give regArgs != 0, then the helper frame will
        // will also promote the arguments for you (Pretty cool huh?)
    HelperMethodFrame(struct MachState* ms, MethodDesc* pMD, ArgumentRegisters* regArgs=0);

    
    virtual void SetReturnAddress(LPVOID val)   { *GetReturnAddressPtr() = val; }
    virtual LPVOID GetReturnAddress()           { return *GetReturnAddressPtr(); }
    LPVOID* GetReturnAddressPtr() {
        _ASSERTE(m_MachState->isValid());
        return m_MachState->_pRetAddr;
    }
    virtual MethodDesc* GetFunction() {
        InsureInit();
        return((MethodDesc*) m_Datum);
    }
    virtual MethodDesc::RETURNTYPE ReturnsObject();
    virtual void UpdateRegDisplay(const PREGDISPLAY);
    virtual void GcScanRoots(promote_func *fn, ScanContext* sc);
    virtual Interception GetInterception() {
        if (GetFrameAttribs() & FRAME_ATTR_EXCEPTION)
            return(INTERCEPTION_EXCEPTION);
        return(INTERCEPTION_NONE);
    }
    virtual unsigned GetFrameAttribs() {
        return(m_Attribs);
    }
    void SetFrameAttribs(unsigned attribs) {
        m_Attribs = attribs;
    }
    void Pop() {
        Frame::Pop(m_pThread);
    }
    void Poll() { 
        if (m_pThread->CatchAtSafePoint())
            CommonTripThread();
    }
    int RestoreState();                     // Restores registers saved in m_MachState
    void InsureInit();
    void Init(Thread *pThread, struct MachState* ms, MethodDesc* pMD, ArgumentRegisters * regArgs);
    inline void Init(struct LazyMachState* ms)
    {
        LazyInit(0, ms);
    }

    INDEBUG(static MachState* ConfirmState(HelperMethodFrame*, void* esiVal, void* ediVal, void* ebxVal, void* ebpVal);)
    INDEBUG(static LPVOID GetMethodFrameVPtr() { RETURNFRAMEVPTR(HelperMethodFrame); })
protected:
    
    HelperMethodFrame::HelperMethodFrame() {
        INDEBUG(memset(&m_Attribs, 0xCC, sizeof(HelperMethodFrame) - offsetof(HelperMethodFrame, m_Attribs));)
    }
    void LazyInit(void* FcallFtnEntry, struct LazyMachState* ms);

protected:
    unsigned m_Attribs;
    MachState* m_MachState;         // pRetAddr points to the return address and the stack arguments
    ArgumentRegisters * m_RegArgs;  // if non-zero we also report these as the register arguments 
    Thread *m_pThread;
    void* m_FCallEntry;             // used to determine our identity for stack traces

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER(HelperMethodFrame)
};

/* report all the args (but not THIS if present), to the GC. 'framePtr' points at the
   frame (promote doesn't assume anthing about its structure).  'msig' describes the
   arguments, and 'ctx' has the GC reporting info.  'stackArgsOffs' is the byte offset
   from 'framePtr' where the arguments start (args start at last and grow bacwards).
   Simmilarly, 'regArgsOffs' is the offset to find the register args to promote */
void promoteArgs(BYTE* framePtr, MetaSig* msig, GCCONTEXT* ctx, int stackArgsOffs, int regArgsOffs);

// workhorse for our promotion efforts
inline void DoPromote(promote_func *fn, ScanContext* sc, OBJECTREF *address, BOOL interior)
{
    LOG((LF_GC, INFO3, "    Promoting pointer argument at %x from %x to ", address, *address));
    if (interior)
        PromoteCarefully(fn, *((Object**)address), sc);
    else
        (*fn) (*((Object **)address), sc);
    LOG((LF_GC, INFO3, "    %x\n", *address));
}

/***********************************************************************************/
/* a HelplerMethodFrames that also report additional object references */

class HelperMethodFrame_1OBJ : public HelperMethodFrame {
public:
    HelperMethodFrame_1OBJ() { INDEBUG(gcPtrs[0] = (OBJECTREF*)POISONC;) }

    HelperMethodFrame_1OBJ(void* fCallFtnEntry, struct LazyMachState* ms, unsigned attribs, OBJECTREF* aGCPtr1)
        : HelperMethodFrame(fCallFtnEntry, ms, attribs) {
        gcPtrs[0] = aGCPtr1; 
        INDEBUG(Thread::ObjectRefProtected(aGCPtr1);)
        }

    HelperMethodFrame_1OBJ(void* fCallFtnEntry, struct LazyMachState* ms, OBJECTREF* aGCPtr1)
        : HelperMethodFrame(fCallFtnEntry, ms) { 
        gcPtrs[0] = aGCPtr1; 
        INDEBUG(Thread::ObjectRefProtected(aGCPtr1);)
        }
        
    void SetProtectedObject(OBJECTREF* objPtr) {
        gcPtrs[0] = objPtr; 
        INDEBUG(Thread::ObjectRefProtected(objPtr);)
        }

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc) {
        DoPromote(fn, sc, gcPtrs[0], FALSE);
        HelperMethodFrame::GcScanRoots(fn, sc);
    }

#ifdef _DEBUG
    void Pop() {
        HelperMethodFrame::Pop();
        Thread::ObjectRefNew(gcPtrs[0]);   
    }

    BOOL Protects(OBJECTREF *ppORef)
    {
        return (ppORef == gcPtrs[0]) ? TRUE : FALSE;
    }

#endif

private:
    OBJECTREF*  gcPtrs[1];

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER(HelperMethodFrame_1OBJ)
};

class HelperMethodFrame_2OBJ : public HelperMethodFrame {
public:
    HelperMethodFrame_2OBJ(void* fCallFtnEntry, struct LazyMachState* ms, OBJECTREF* aGCPtr1, OBJECTREF* aGCPtr2)
        : HelperMethodFrame(fCallFtnEntry, ms) {
        gcPtrs[0] = aGCPtr1; 
        gcPtrs[1] = aGCPtr2; 
        INDEBUG(Thread::ObjectRefProtected(aGCPtr1);)
        INDEBUG(Thread::ObjectRefProtected(aGCPtr2);)
        }
        
    HelperMethodFrame_2OBJ(void* fCallFtnEntry, struct LazyMachState* ms, unsigned attribs, OBJECTREF* aGCPtr1, OBJECTREF* aGCPtr2)
        : HelperMethodFrame(fCallFtnEntry, ms, attribs) { 
        gcPtrs[0] = aGCPtr1; 
        gcPtrs[1] = aGCPtr2; 
        INDEBUG(Thread::ObjectRefProtected(aGCPtr1);)
        INDEBUG(Thread::ObjectRefProtected(aGCPtr2);)
        }

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc) {
        DoPromote(fn, sc, gcPtrs[0], FALSE);
        DoPromote(fn, sc, gcPtrs[1], FALSE);
        HelperMethodFrame::GcScanRoots(fn, sc);
    }

#ifdef _DEBUG
    void Pop() {
        HelperMethodFrame::Pop();
        Thread::ObjectRefNew(gcPtrs[0]); 
        Thread::ObjectRefNew(gcPtrs[1]); 
    }

    BOOL Protects(OBJECTREF *ppORef)
    {
        return (ppORef == gcPtrs[0] || ppORef == gcPtrs[1]) ? TRUE : FALSE;
    }
#endif

private:
    OBJECTREF*  gcPtrs[2];

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(HelperMethodFrame_2OBJ)
};

class HelperMethodFrame_4OBJ : public HelperMethodFrame {
public:
    HelperMethodFrame_4OBJ(void* fCallFtnEntry, struct LazyMachState* ms, 
        OBJECTREF* aGCPtr1, OBJECTREF* aGCPtr2, OBJECTREF* aGCPtr3, OBJECTREF* aGCPtr4 = NULL)
        : HelperMethodFrame(fCallFtnEntry, ms) { 
        gcPtrs[0] = aGCPtr1; gcPtrs[1] = aGCPtr2; gcPtrs[2] = aGCPtr3; gcPtrs[3] = aGCPtr4; 
        INDEBUG(Thread::ObjectRefProtected(aGCPtr1);)
        INDEBUG(Thread::ObjectRefProtected(aGCPtr2);)
        INDEBUG(Thread::ObjectRefProtected(aGCPtr3);)
        INDEBUG(if (aGCPtr4) Thread::ObjectRefProtected(aGCPtr4);)
    }
        
    virtual void GcScanRoots(promote_func *fn, ScanContext* sc) {
        DoPromote(fn, sc, gcPtrs[0], FALSE);
        DoPromote(fn, sc, gcPtrs[1], FALSE);
        DoPromote(fn, sc, gcPtrs[2], FALSE);
        if (gcPtrs[3] != 0) DoPromote(fn, sc, gcPtrs[3], FALSE);
        HelperMethodFrame::GcScanRoots(fn, sc);
    }

#ifdef _DEBUG
    void Pop() {
        HelperMethodFrame::Pop();
        Thread::ObjectRefNew(gcPtrs[0]); 
        Thread::ObjectRefNew(gcPtrs[1]); 
        Thread::ObjectRefNew(gcPtrs[2]); 
        if (gcPtrs[3] != 0) Thread::ObjectRefNew(gcPtrs[3]); 
    }

    virtual BOOL Protects(OBJECTREF *ppORef)
    {
        for (UINT i = 0; i < 4; i++) {
            if (ppORef == gcPtrs[i]) {
                return TRUE;
            }
        }
        return FALSE;
    }
#endif

private:
    OBJECTREF*  gcPtrs[4];

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(HelperMethodFrame_4OBJ)
};

//------------------------------------------------------------------------
// This frame represents a method call. No actual instances of this
// frame exist: there are subclasses for each method type.
//
// However, they all share a similar image ...
//
//              +...    stack-based arguments here
//              +12     return address
//              +8      datum (typically a MethodDesc*)
//              +4      m_Next
//              +0      the frame vptr
//              -...    preserved CalleeSavedRegisters
//              -...    VC5Frame (debug only)
//              -...    ArgumentRegisters
//
//------------------------------------------------------------------------
class FramedMethodFrame : public TransitionFrame
{
    public:

        // TransitionFrames must store some fields at negative offset.
        // This method exposes the size for people needing to allocate
        // TransitionFrames.
        static UINT32 GetNegSpaceSize()
        {
            return PLATFORM_FRAME_ALIGN(sizeof(CalleeSavedRegisters) + VC5FRAME_SIZE + ARGUMENTREGISTERS_SIZE);
        }

        // Exposes an offset for stub generation.
        static BYTE GetOffsetOfArgs()
        {
            size_t ofs = sizeof(TransitionFrame);
            _ASSERTE(FitsInI1(ofs));
            return (BYTE)ofs;
        }

        //---------------------------------------------------------------
        // Expose key offsets and values for stub generation.
        //---------------------------------------------------------------
        static int GetOffsetOfArgumentRegisters()
        {
            return -((int)(sizeof(CalleeSavedRegisters) + VC5FRAME_SIZE + ARGUMENTREGISTERS_SIZE));
        }


        CalleeSavedRegisters *GetCalleeSavedRegisters()
        {
            return (CalleeSavedRegisters*)( ((BYTE*)this) - sizeof(CalleeSavedRegisters) );
        }

        virtual MethodDesc *GetFunction()
        {
            return (MethodDesc*)m_Datum;
        }

        virtual void UpdateRegDisplay(const PREGDISPLAY);

        //------------------------------------------------------------------------
        // Returns the address of a security object or
        // null if there is no space for an object on this frame.
        //------------------------------------------------------------------------
        virtual OBJECTREF *GetAddrOfSecurityDesc()
        {
            return NULL;
        }

        // Get return value address
        virtual INT64 *GetReturnValuePtr()
        {
            return NULL;
        }
        
        //------------------------------------------------------------------------
        // Performs cleanup on an exception unwind
        //------------------------------------------------------------------------
        virtual void ExceptionUnwind()
        {
            if (GetFunction() && GetFunction()->IsSynchronized())
                UnwindSynchronized();

            TransitionFrame::ExceptionUnwind();
        }

        IMDInternalImport *GetMDImport()
        {
            _ASSERTE(GetFunction());
            return GetFunction()->GetMDImport();
        }

        Module *GetModule()
        {
            _ASSERTE(GetFunction());
            return GetFunction()->GetModule();
        }

        //---------------------------------------------------------------
        // Get the "this" object.
        //---------------------------------------------------------------
        OBJECTREF GetThis()
        {
            return *GetAddrOfThis();
        }


        //---------------------------------------------------------------
        // Get the address of the "this" object. WARNING!!! Whether or not "this"
        // is gc-protected is depends on the frame type!!!
        //---------------------------------------------------------------
        OBJECTREF* GetAddrOfThis()
        {
            return (OBJECTREF*)(GetOffsetOfThis() + (LPBYTE)this);
        }

        //---------------------------------------------------------------
        // Get the offset of the stored "this" pointer relative to the frame.
        //---------------------------------------------------------------
        static int GetOffsetOfThis();


        //---------------------------------------------------------------
        // Expose key offsets and values for stub generation.
        //---------------------------------------------------------------
        static BYTE GetOffsetOfMethod()
        {
            size_t ofs = offsetof(class FramedMethodFrame, m_Datum);
            _ASSERTE(FitsInI1(ofs));
            return (BYTE)ofs;
        }

        //---------------------------------------------------------------
        // For vararg calls, return cookie.
        //---------------------------------------------------------------
        VASigCookie *GetVASigCookie()
        {
            return *((VASigCookie**)(this + 1));
        }

        int GetFrameType()
        {
            return TYPE_CALL;
        }

        virtual BOOL IsFramedMethodFrame() { return TRUE; }

    protected:
        // For use by classes deriving from FramedMethodFrame.
        void PromoteCallerStack(promote_func* fn, ScanContext* sc)
        {
            PromoteCallerStackWorker(fn, sc, FALSE);
        }

        // For use by classes deriving from FramedMethodFrame.
        void PromoteCallerStackWithPinning(promote_func* fn, ScanContext* sc)
        {
            PromoteCallerStackWorker(fn, sc, TRUE);
        }

        void UnwindSynchronized();

                // Helper for ComPlus and NDirect method calls that are implemented via
                // compiled stubs. This function retrieves the stub (after unwrapping
                // interceptors) and asks it for the stack count computed by the stublinker.
                void AskStubForUnmanagedCallSite(void **ip,
                                         void **returnIP, void **returnSP);


    private:
        // For use by classes deriving from FramedMethodFrame.
        void PromoteCallerStackWorker(promote_func* fn, ScanContext* sc, BOOL fPinArrays);

        void PromoteCallerStackHelper(promote_func* fn, ScanContext* sc, BOOL fPinArrays,
            ArgIterator *pargit, MetaSig *pmsig);


        // Keep as last entry in class
        DEFINE_VTABLE_GETTER_AND_CTOR(FramedMethodFrame)
};



//+----------------------------------------------------------------------------
//
//  Class:      TPMethodFrame            private
//
//  Synopsis:   This frame is pushed onto the stack for calls on transparent
//              proxy
//
//  History:    17-Feb-99   Gopalk      Created
//
//+----------------------------------------------------------------------------
class TPMethodFrame : public FramedMethodFrame
{
    public:
        virtual int GetFrameType()
        {
            return TYPE_TP_METHOD_FRAME;
        }
        
        // GC protect arguments
        virtual void GcScanRoots(promote_func *fn, ScanContext* sc);

        // Return only a valid Method Descriptor
        virtual MethodDesc *GetFunction();

        // For proxy calls m_Datum contains the number of stack bytes containing arguments.
        void SetFunction(void *pMD)
        {
            m_Datum = pMD;
        }

        // Return value is stored here
        Object *&GetReturnObject()
        {
            Object *&pReturn = *(Object **) (((BYTE *) this) - GetNegSpaceSize() - sizeof(INT64));
            // This assert is too strong, it does not work for byref returns!
            _ASSERTE(pReturn == NULL || pReturn->GetMethodTable()->GetClass());
            return(pReturn);
        }

        // Get return value address
        virtual INT64 *GetReturnValuePtr()
        {
            return (INT64*) (((BYTE *) this) - GetNegSpaceSize() - sizeof(INT64));
        }

        // Get slot number on which we were called
        INT32 GetSlotNumber()
        {
            return GetSlotNumber(m_Datum);
        }

        static INT32 GetSlotNumber(PVOID MDorSlot)
        {

            if(( ((size_t)MDorSlot) & ~0xFFFF) == 0)
            {
                // The slot number was pushed on the stack
                return (INT32)(size_t)MDorSlot;
            }
            else
            {
                // The method descriptor was pushed on the stack
                return -1;
            }
        }

        // Get offset used during stub generation
        static LPVOID GetMethodFrameVPtr()
        {
            RETURNFRAMEVPTR(TPMethodFrame);
        }

        // Aid the debugger in finding the actual address of callee
        virtual BOOL TraceFrame(Thread *thread, BOOL fromPatch,
                                TraceDestination *trace, REGDISPLAY *regs);

        // Keep as last entry in class
        DEFINE_VTABLE_GETTER_AND_CTOR(TPMethodFrame)
};


//------------------------------------------------------------------------
// This represents a call to a ECall method.
//------------------------------------------------------------------------
class ECallMethodFrame : public FramedMethodFrame
{
public:

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc);


    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(ECallMethodFrame);
    }

    int GetFrameType()
    {
        return TYPE_EXIT;
    };

    virtual void GetUnmanagedCallSite(void **ip,
                                      void **returnIP, void **returnSP);
    virtual BOOL TraceFrame(Thread *thread, BOOL fromPatch,
                            TraceDestination *trace, REGDISPLAY *regs);

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(ECallMethodFrame)
};


//------------------------------------------------------------------------
// This represents a call to a FCall method.
// Note that this frame is pushed only if the FCall throws an exception.
// For normal execution, FCall methods run frameless. That's the whole
// reason for FCall's existence.
//------------------------------------------------------------------------
class FCallMethodFrame : public FramedMethodFrame
{
public:

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(FCallMethodFrame);
    }

    int GetFrameType()
    {
        return TYPE_EXIT;
    };

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(FCallMethodFrame)
};



//------------------------------------------------------------------------
// This represents a call to a NDirect method.
//------------------------------------------------------------------------
class NDirectMethodFrame : public FramedMethodFrame
{
    public:

        virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
        {
            FramedMethodFrame::GcScanRoots(fn, sc);
            PromoteCallerStackWithPinning(fn, sc);
            if (GetCleanupWorkList())
            {
                GetCleanupWorkList()->GcScanRoots(fn, sc);
            }

        }

        virtual BOOL IsTransitionToNativeFrame()
        {
            return TRUE;
        }

        virtual CleanupWorkList *GetCleanupWorkList()
        {
            return NULL;
        }

        //---------------------------------------------------------------
        // Expose key offsets and values for stub generation.
        //---------------------------------------------------------------

        int GetFrameType()
        {
            return TYPE_EXIT;
        };

        void GetUnmanagedCallSite(void **ip,
                                  void **returnIP, void **returnSP) = 0;

        BOOL TraceFrame(Thread *thread, BOOL fromPatch,
                        TraceDestination *trace, REGDISPLAY *regs);

        friend struct MEMBER_OFFSET_INFO(NDirectMethodFrame);
};






//------------------------------------------------------------------------
// This represents a call to a NDirect method with cleanup.
//------------------------------------------------------------------------
class NDirectMethodFrameEx : public NDirectMethodFrame
{
public:

    //------------------------------------------------------------------------
    // Performs cleanup on an exception unwind
    //------------------------------------------------------------------------
    virtual void ExceptionUnwind()
    {
        NDirectMethodFrame::ExceptionUnwind();
        GetCleanupWorkList()->Cleanup(TRUE);
    }


    //------------------------------------------------------------------------
    // Gets the cleanup worklist for this method call.
    //------------------------------------------------------------------------
    virtual CleanupWorkList *GetCleanupWorkList()
    {
        return (CleanupWorkList*)( ((LPBYTE)this) + GetOffsetOfCleanupWorkList() );
    }

    static INT GetOffsetOfCleanupWorkList()
    {
            return 0 - GetNegSpaceSize();
    }

    // This frame must store some fields at negative offset.
    // This method exposes the size for people needing to allocate
    // TransitionFrames.
    static UINT32 GetNegSpaceSize()
    {
        return PLATFORM_FRAME_ALIGN(FramedMethodFrame::GetNegSpaceSize() + sizeof(CleanupWorkList));
    }

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------

    void GetUnmanagedCallSite(void **ip,
                              void **returnIP, void **returnSP) = 0;

    friend struct MEMBER_OFFSET_INFO(NDirectMethodFrameEx);
};

//------------------------------------------------------------------------
// This represents a call to a NDirect method with the generic worker
// (the subclass is so the debugger can tell the difference)
//------------------------------------------------------------------------
class NDirectMethodFrameGeneric : public NDirectMethodFrameEx
{
public:
    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(NDirectMethodFrameGeneric);
    }

    void GetUnmanagedCallSite(void **ip,
                              void **returnIP, void **returnSP);

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(NDirectMethodFrameGeneric)
};


//------------------------------------------------------------------------
// This represents a call to a NDirect method with the slimstub
// (the subclass is so the debugger can tell the difference)
//------------------------------------------------------------------------
class NDirectMethodFrameSlim : public NDirectMethodFrameEx
{
public:
    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(NDirectMethodFrameSlim);
    }

    void GetUnmanagedCallSite(void **ip,
                              void **returnIP, void **returnSP);

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(NDirectMethodFrameSlim)
};




//------------------------------------------------------------------------
// This represents a call to a NDirect method with the standalone stub (no cleanup)
// (the subclass is so the debugger can tell the difference)
//------------------------------------------------------------------------
class NDirectMethodFrameStandalone : public NDirectMethodFrame
{
public:
    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(NDirectMethodFrameStandalone);
    }

    void GetUnmanagedCallSite(void **ip,
                              void **returnIP, void **returnSP)
    {
            AskStubForUnmanagedCallSite(ip, returnIP, returnSP);
    }

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(NDirectMethodFrameStandalone)
};



//------------------------------------------------------------------------
// This represents a call to a NDirect method with the standalone stub (with cleanup)
// (the subclass is so the debugger can tell the difference)
//------------------------------------------------------------------------
class NDirectMethodFrameStandaloneCleanup : public NDirectMethodFrameEx
{
public:
    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(NDirectMethodFrameStandaloneCleanup);
    }

    void GetUnmanagedCallSite(void **ip,
                              void **returnIP, void **returnSP)
            {
                    AskStubForUnmanagedCallSite(ip, returnIP, returnSP);
            }

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(NDirectMethodFrameStandaloneCleanup)
};



//------------------------------------------------------------------------
// This represents a call Multicast.Invoke. It's only used to gc-protect
// the arguments during the iteration.
//------------------------------------------------------------------------
class MulticastFrame : public FramedMethodFrame
{
public:
    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        FramedMethodFrame::GcScanRoots(fn, sc);
        PromoteCallerStack(fn, sc);
    }

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(MulticastFrame);
    }


    int GetFrameType()
    {
        return TYPE_MULTICAST;
    }

    virtual BOOL TraceFrame(Thread *thread, BOOL fromPatch,
                            TraceDestination *trace, REGDISPLAY *regs);

    Stub *AscertainMCDStubness(BYTE *pbAddr);

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(MulticastFrame)
};








//-----------------------------------------------------------------------
// Transition frame from unmanaged to managed
//-----------------------------------------------------------------------
class UnmanagedToManagedFrame : public Frame
{
public:

    // Retrieves the return address into the code that called the
    // helper or method.
    virtual LPVOID GetReturnAddress()
    {
        return m_ReturnAddress;
    }

    virtual LPVOID* GetReturnAddressPtr()
    {
        return &m_ReturnAddress;
    }

    // Retrives pointer to the lowest-addressed argument on
    // the stack. Depending on the calling convention, this
    // may or may not be the first argument.
    LPVOID GetPointerToArguments()
    {
        return (LPVOID)(this + 1);
    }

    // Exposes an offset for stub generation.
    static BYTE GetOffsetOfArgs()
    {
        size_t ofs = sizeof(UnmanagedToManagedFrame);
        _ASSERTE(FitsInI1(ofs));
        return (BYTE)ofs;
    }

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static BYTE GetOffsetOfReturnAddress()
    {
        size_t ofs = offsetof(class UnmanagedToManagedFrame, m_ReturnAddress);
        _ASSERTE(FitsInI1(ofs));
        return (BYTE)ofs;
    }

    // depends on the sub frames to return approp. type here
    virtual LPVOID GetDatum()
    {
        return m_pvDatum;
    }

    static UINT GetOffsetOfDatum()
    {
        return (UINT)offsetof(class UnmanagedToManagedFrame, m_pvDatum);
    }

    int GetFrameType()
    {
        return TYPE_ENTRY;
    };

    virtual const BYTE* GetManagedTarget()
    {
        return NULL;
    }

    // Return the # of stack bytes pushed by the unmanaged caller.
    virtual UINT GetNumCallerStackBytes() = 0;

     virtual void UpdateRegDisplay(const PREGDISPLAY);

protected:
    LPVOID    m_pvDatum;       // type depends on the sub class
    LPVOID    m_ReturnAddress;  // return address into unmanaged code

    friend struct MEMBER_OFFSET_INFO(UnmanagedToManagedFrame);
};


//-----------------------------------------------------------------------
// Transition frame from unmanaged to managed
//
// this frame contains some object reference at negative
// offset which need to be promoted, the references could be [in] args during
// in the middle of marshalling or [out], [in,out] args that need to be tracked
//------------------------------------------------------------------------
class UnmanagedToManagedCallFrame : public UnmanagedToManagedFrame
{
public:
    struct NegInfo
    {
        CleanupWorkList m_List;
        Context *m_pReturnContext;
        LPVOID      m_pArgs;
        ULONG       m_fGCEnabled;
        Marshaler  *m____NOLONGERUSED____; // marshaler structures that want to be notified of GC promotes
    };

    // Return the # of stack bytes pushed by the unmanaged caller.
    virtual UINT GetNumCallerStackBytes()
    {
        return 0;
    }


    // Should managed exceptions be passed thru?
    virtual BOOL CatchManagedExceptions() = 0;

    // Convert a thrown COM+ exception to an unmanaged result.
    virtual UINT32 ConvertComPlusException(OBJECTREF pException) = 0;

    //------------------------------------------------------------------------
    // Performs cleanup on an exception unwind
    //------------------------------------------------------------------------
    virtual void ExceptionUnwind();

    // ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING
    //
    // !! We shave some cycles in ComToComPlusWorker() & ComToComPlusSimpleWorker()
    // !! by asserting that we are operating on a ComMethodFrame and then we
    // !! bypass virtualization.  Regrettably, this means that there are some
    // !! non-virtual implementations of the following three methods in ComMethodFrame.
    // !!
    // !! If you edit the following 3 methods, please propagate your changes to
    // !! those implementations, also.
    //
    // ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING

    virtual CleanupWorkList *GetCleanupWorkList()
    {
        return &GetNegInfo()->m_List;
    }
    virtual NegInfo* GetNegInfo()
    {
        return (NegInfo*)( ((LPBYTE)this) - GetNegSpaceSize());
    }
    virtual ULONG* GetGCInfoFlagPtr()
    {
        return &GetNegInfo()->m_fGCEnabled;
    }

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc);

    virtual Context **GetReturnContextAddr()
    {
        return &(GetNegInfo()->m_pReturnContext);
    }

    // ********************** END OF WARNING *************************



    virtual LPVOID GetPointerToDstArgs()
    {
        return GetNegInfo()->m_pArgs;
    }

    virtual void SetDstArgsPointer(LPVOID pv)
    {
        _ASSERTE(pv != NULL);
        GetNegInfo()->m_pArgs = pv;
    }


    // UnmanagedToManagedCallFrames must store some fields at negative offset.
    // This method exposes the size
    static UINT32 GetNegSpaceSize()
    {
        return PLATFORM_FRAME_ALIGN(sizeof (NegInfo) + sizeof(CalleeSavedRegisters) + VC5FRAME_SIZE);
    }

    friend struct MEMBER_OFFSET_INFO(UnmanagedToManagedCallFrame);
};

//------------------------------------------------------------------------
// This frame represents a transition from COM to COM+
// this frame contains some object reference at negative
// offset which need to be promoted, the references could be [in] args during
// in the middle of marshalling or [out], [in,out] args that need to be tracked
//------------------------------------------------------------------------
class ComMethodFrame : public UnmanagedToManagedCallFrame
{
public:

    // ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING
    //
    // !! We shave some cycles in ComToComPlusWorker() & ComToComPlusSimpleWorker()
    // !! by asserting that we are operating on a ComMethodFrame and then we
    // !! bypass virtualization.  Stay away from these NonVirtual methods unless
    // !! you **really** need this optimization.
    //
    // ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING

    NegInfo *NonVirtual_GetNegInfo()
    {
        return (NegInfo*)( ((LPBYTE)this) - GetNegSpaceSize());
    }
    ULONG *NonVirtual_GetGCInfoFlagPtr()
    {
        return &NonVirtual_GetNegInfo()->m_fGCEnabled;
    }
    void NonVirtual_SetDstArgsPointer(LPVOID pv)
    {
        _ASSERTE(pv != NULL);
        NonVirtual_GetNegInfo()->m_pArgs = pv;
    }
    CleanupWorkList *NonVirtual_GetCleanupWorkList()
    {
        return &NonVirtual_GetNegInfo()->m_List;
    }

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        //@PERF: optimize
        // if we are the topmost frame, we might have some marshalled argument objects
        // in the stack that needs our protection, lets save 'em.
        PromoteCalleeStack(fn, sc);
        UnmanagedToManagedCallFrame::GcScanRoots(fn,sc);
    }

    // promote callee stack, if we are the topmost frame
    void PromoteCalleeStack(promote_func *fn, ScanContext* sc);


    // used by PromoteCalleeStack to get the destination function sig and module
    // NOTE: PromoteCalleeStack only promotes bona-fide arguments, and not
    // the "this" reference. The whole purpose of PromoteCalleeStack is
    // to protect the partially constructed argument array during
    // the actual process of argument marshaling.
    virtual PCCOR_SIGNATURE GetTargetCallSig();
    virtual Module *GetTargetModule();

    // Return the # of stack bytes pushed by the unmanaged caller.
    UINT GetNumCallerStackBytes();


    // Should managed exceptions be passed thru?
    virtual BOOL CatchManagedExceptions()
    {
        return TRUE;
    }

    // Convert a thrown COM+ exception to an unmanaged result.
    virtual UINT32 ConvertComPlusException(OBJECTREF pException);

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
         RETURNFRAMEVPTR(ComMethodFrame);
    }

protected:
    friend INT64 __stdcall ComToComPlusWorker(Thread *pThread, ComMethodFrame* pFrame);
    friend INT64 __stdcall FieldCallWorker(Thread *pThread, ComMethodFrame* pFrame);

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(ComMethodFrame)
};


//------------------------------------------------------------------------
// This represents a call from ComPlus to COM
//------------------------------------------------------------------------
class ComPlusMethodFrame : public FramedMethodFrame
{
public:

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        FramedMethodFrame::GcScanRoots(fn, sc);
        PromoteCallerStackWithPinning(fn, sc);
        if (GetCleanupWorkList())
        {
            GetCleanupWorkList()->GcScanRoots(fn, sc);
        }
    }

    virtual BOOL IsTransitionToNativeFrame()
    {
        return TRUE;
    }

    virtual CleanupWorkList *GetCleanupWorkList()
    {
        return NULL;
    }

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    int GetFrameType()
    {
        return TYPE_EXIT;
    };

    void GetUnmanagedCallSite(void **ip,
                              void **returnIP, void **returnSP) = 0;

    BOOL TraceFrame(Thread *thread, BOOL fromPatch,
                    TraceDestination *trace, REGDISPLAY *regs);

    friend struct MEMBER_OFFSET_INFO(ComPlusMethodFrame);
};






//------------------------------------------------------------------------
// This represents a call from COM+ to COM with cleanup.
//------------------------------------------------------------------------
class ComPlusMethodFrameEx : public ComPlusMethodFrame
{
public:


    //------------------------------------------------------------------------
    // Performs cleanup on an exception unwind
    //------------------------------------------------------------------------
    virtual void ExceptionUnwind()
    {
        ComPlusMethodFrame::ExceptionUnwind();
        GetCleanupWorkList()->Cleanup(TRUE);
    }


    //------------------------------------------------------------------------
    // Gets the cleanup worklist for this method call.
    //------------------------------------------------------------------------
    virtual CleanupWorkList *GetCleanupWorkList()
    {
        return (CleanupWorkList*)( ((LPBYTE)this) - GetNegSpaceSize() );
    }

    // This frame must store some fields at negative offset.
    // This method exposes the size for people needing to allocate
    // TransitionFrames.
    static UINT32 GetNegSpaceSize()
    {
        return PLATFORM_FRAME_ALIGN(FramedMethodFrame::GetNegSpaceSize() + sizeof(CleanupWorkList));
    }

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    void GetUnmanagedCallSite(void **ip,
                              void **returnIP, void **returnSP) = 0;

    friend struct MEMBER_OFFSET_INFO(ComPlusMethodFrameEx);
};





//------------------------------------------------------------------------
// This represents a call from COM+ to COM using the generic worker
//------------------------------------------------------------------------
class ComPlusMethodFrameGeneric : public ComPlusMethodFrameEx
{
public:
    virtual void GcScanRoots(promote_func *fn, ScanContext* sc);
    
    // Return value is stored here
    Object *&GetReturnObject()
    {
        Object *&pReturn = *(Object **) (((BYTE *) this) - FramedMethodFrame::GetNegSpaceSize() - sizeof(INT64));
        // This assert is too strong, it does not work for byref returns!
        _ASSERTE(pReturn == NULL || pReturn->GetMethodTable()->GetClass());
        return(pReturn);
    }
    
    // Get return value address
    virtual INT64 *GetReturnValuePtr()
    {
        return (INT64*) (((BYTE *) this) - FramedMethodFrame::GetNegSpaceSize() - sizeof(INT64));
    }

    //------------------------------------------------------------------------
    // Gets the cleanup worklist for this method call.
    //------------------------------------------------------------------------
    virtual CleanupWorkList *GetCleanupWorkList()
    {
        return (CleanupWorkList*)( ((LPBYTE)this) - GetNegSpaceSize() - sizeof(INT64));
    }
    
    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(ComPlusMethodFrameGeneric);
    }

    void GetUnmanagedCallSite(void **ip,
                              void **returnIP, void **returnSP);

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(ComPlusMethodFrameGeneric)
};




//------------------------------------------------------------------------
// This represents a call from COM+ to COM using the standalone stub (no cleanup)
//------------------------------------------------------------------------
class ComPlusMethodFrameStandalone : public ComPlusMethodFrame
{
public:
    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(ComPlusMethodFrameStandalone);
    }

    void GetUnmanagedCallSite(void **ip,
                              void **returnIP, void **returnSP)
    {
            AskStubForUnmanagedCallSite(ip, returnIP, returnSP);
    }

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(ComPlusMethodFrameStandalone)
};


//------------------------------------------------------------------------
// This represents a call from COM+ to COM using the standalone stub using cleanup
//------------------------------------------------------------------------
class ComPlusMethodFrameStandaloneCleanup : public ComPlusMethodFrameEx
{
public:
    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(ComPlusMethodFrameStandaloneCleanup);
    }

    void GetUnmanagedCallSite(void **ip,
                              void **returnIP, void **returnSP)
    {
            AskStubForUnmanagedCallSite(ip, returnIP, returnSP);
    }

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(ComPlusMethodFrameStandaloneCleanup)
};





//------------------------------------------------------------------------
// This represents a call from ComPlus to COM
//------------------------------------------------------------------------
class PInvokeCalliFrame : public FramedMethodFrame
{
public:
    //------------------------------------------------------------------------
    // Performs cleanup on an exception unwind
    //------------------------------------------------------------------------
    virtual void ExceptionUnwind()
    {
         FramedMethodFrame::ExceptionUnwind();
         GetCleanupWorkList()->Cleanup(TRUE);
    }

    //------------------------------------------------------------------------
    // Gets the cleanup worklist for this method call.
    //------------------------------------------------------------------------
    virtual CleanupWorkList *GetCleanupWorkList()
    {
        return (CleanupWorkList*)( ((LPBYTE)this) - GetNegSpaceSize() );
    }

    // This frame must store some fields at negative offset.
    // This method exposes the size for people needing to allocate
    // PInvokeCalliFrames.
    static UINT32 GetNegSpaceSize()
    {
        return PLATFORM_FRAME_ALIGN(FramedMethodFrame::GetNegSpaceSize() + sizeof(CleanupWorkList));
    }


    virtual BOOL IsTransitionToNativeFrame()
    {
        return TRUE;
    }

    // not a method
    virtual MethodDesc *GetFunction()
    {
        return NULL;
    }

    // Update the datum
    void NonVirtual_SetFunction(void *pMD)
    {
        m_Datum = pMD;
    }

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        if (GetCleanupWorkList())
        {
            GetCleanupWorkList()->GcScanRoots(fn, sc);
        }
    }

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(PInvokeCalliFrame);
    }

    int GetFrameType()
    {
        return TYPE_EXIT;
    };

    LPVOID NonVirtual_GetPInvokeCalliTarget()
    {
        return FramedMethodFrame::GetFunction();
    }


    LPVOID NonVirtual_GetCookie()
    {
        return (LPVOID) *(LPVOID *)NonVirtual_GetPointerToArguments();
    }

    // Retrives pointer to the lowest-addressed argument on
    // the stack.
    LPVOID NonVirtual_GetPointerToArguments()
    {
        return (LPVOID)(this + 1);
    }

        virtual void UpdateRegDisplay(const PREGDISPLAY);
    void GetUnmanagedCallSite(void **ip,
                              void **returnIP, void **returnSP);

    BOOL TraceFrame(Thread *thread, BOOL fromPatch,
                    TraceDestination *trace, REGDISPLAY *regs);

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(PInvokeCalliFrame)
};


// Some context-related forwards.

//------------------------------------------------------------------------
// This frame represents a hijacked return.  If we crawl back through it,
// it gets us back to where the return should have gone (and eventually will
// go).
//------------------------------------------------------------------------
class HijackFrame : public Frame
{
public:

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
    }

    // Retrieves the return address into the code that called the
    // helper or method.
    virtual LPVOID GetReturnAddress()
    {
        return m_ReturnAddress;
    }

    virtual LPVOID* GetReturnAddressPtr()
    {
        return &m_ReturnAddress;
    }

    virtual void UpdateRegDisplay(const PREGDISPLAY);


    // HijackFrames are created by trip functions. See OnHijackObjectTripThread()
    // and OnHijackScalarTripThread().  They are real C++ objects on the stack.  So
    // it's a public function -- but that doesn't mean you should make some.
    HijackFrame(LPVOID returnAddress, Thread *thread, HijackArgs *args);

protected:

    VOID        *m_ReturnAddress;
    Thread      *m_Thread;
    HijackArgs  *m_Args;

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(HijackFrame)
};

//------------------------------------------------------------------------
// This represents a declarative secuirty check. This frame is inserted
// prior to calls on methods that have declarative security defined for
// the class or the specific method that is being called. This frame
// is only created when the prestubworker creates a real stub.
//------------------------------------------------------------------------
class SecurityFrame : public FramedMethodFrame
{
public:
    struct ISecurityState {
        // Additional field for referencing per-frame security information.
        // This field is not necessary for most frames, so it is a costly
        // addition for all frames. Leave it here for M3 after which we can
        // be smarter about when to insert this extra field. This field should
        // not always be added to the negative offset
        OBJECTREF   m_securityData;
    };

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(SecurityFrame);
    }

    //-----------------------------------------------------------
    // Returns the address of the frame security descriptor ref
    //-----------------------------------------------------------

    virtual OBJECTREF *GetAddrOfSecurityDesc()
    {
        return & GetISecurityState()->m_securityData;
    }

    static UINT32 GetNegSpaceSize()
    {
        return PLATFORM_FRAME_ALIGN(FramedMethodFrame::GetNegSpaceSize() + sizeof(ISecurityState));
    }

    VOID GcScanRoots(promote_func *fn, ScanContext* sc);

    BOOL TraceFrame(Thread *thread, BOOL fromPatch,
                    TraceDestination *trace, REGDISPLAY *regs);

    int GetFrameType()
    {
        return TYPE_SECURITY;
    }

private:
    ISecurityState *GetISecurityState()
    {
        return (ISecurityState*)( ((BYTE*)this) - GetNegSpaceSize() );
    }

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(SecurityFrame)
};


//------------------------------------------------------------------------
// This represents a call to a method prestub. Because the prestub
// can do gc and throw exceptions while building the replacement
// stub, we need this frame to keep things straight.
//------------------------------------------------------------------------
class PrestubMethodFrame : public FramedMethodFrame
{
public:
    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        FramedMethodFrame::GcScanRoots(fn, sc);
        PromoteCallerStack(fn, sc);
    }


    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(PrestubMethodFrame);
    }

    BOOL TraceFrame(Thread *thread, BOOL fromPatch,
                    TraceDestination *trace, REGDISPLAY *regs);

    int GetFrameType()
    {
        return TYPE_INTERCEPTION;
    }

    Interception GetInterception();

    // Link this frame, setting the vptr
    VOID Push();

private:
    friend const BYTE * __stdcall PreStubWorker(PrestubMethodFrame *pPFrame);

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(PrestubMethodFrame)
};

//------------------------------------------------------------------------
// This represents a call to a method prestub. Because the prestub
// can do gc and throw exceptions while building the replacement
// stub, we need this frame to keep things straight.
//------------------------------------------------------------------------
class InterceptorFrame : public SecurityFrame
{
public:
    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        SecurityFrame::GcScanRoots(fn, sc);
        PromoteCallerStack(fn, sc);
    }


    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(InterceptorFrame);
    }

    Interception GetInterception();

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(InterceptorFrame)
};

//------------------------------------------------------------------------
// This represents a com to com+ call method prestub.
// we need to catch exceptions etc. so this frame is not the same
// as the prestub method frame
// Note that in rare IJW cases, the immediate caller could be a managed method
// which pinvoke-inlined a call to a COM interface, which happenned to be
// implemented by a managed function via COM-interop.
//------------------------------------------------------------------------
class ComPrestubMethodFrame : public Frame
{
public:
    // Retrieves the return address into the code that called the
    // helper or method.
    virtual LPVOID GetReturnAddress()
    {
        return m_ReturnAddress;
    }

    virtual LPVOID* GetReturnAddressPtr()
    {
        return &m_ReturnAddress;
    }

    CalleeSavedRegisters *GetCalleeSavedRegisters()
    {
        return (CalleeSavedRegisters*)( ((BYTE*)this) - sizeof(CalleeSavedRegisters) );
    }

    virtual void UpdateRegDisplay(const PREGDISPLAY pRD);

    // Retrives pointer to the lowest-addressed argument on
    // the stack. Depending on the calling convention, this
    // may or may not be the first argument.
    LPVOID GetPointerToArguments()
    {
        return (LPVOID)(this + 1);
    }

    // Prestub frames must store some fields at negative offset.
    // This method exposes the size for people needing to allocate
    // TransitionFrames.
    static UINT32 GetNegSpaceSize()
    {
        return PLATFORM_FRAME_ALIGN(sizeof(CalleeSavedRegisters) + VC5FRAME_SIZE);
    }


    // Exposes an offset for stub generation.
    static BYTE GetOffsetOfArgs()
    {
        size_t ofs = sizeof(ComPrestubMethodFrame);
        _ASSERTE(FitsInI1(ofs));
        return (BYTE)ofs;
    }

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static BYTE GetOffsetOfReturnAddress()
    {
        size_t ofs = offsetof(class ComPrestubMethodFrame, m_ReturnAddress);
        _ASSERTE(FitsInI1(ofs));
        return (BYTE)ofs;
    }

    // okay this function is only used by the COM stubs
    // so don't name this the same as GetFunction
    MethodDesc *GetMethodDesc()
    {
        return m_pFuncDesc;
    }

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        // nothing to do here
    }

    // Link this frame, setting the vptr
    VOID Push();


    //------------------------------------------------------------------------
    // Performs cleanup on an exception unwind
    //------------------------------------------------------------------------
    virtual void ExceptionUnwind()
    {
        //nothing to do here
    }

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(ComPrestubMethodFrame);
    }

    int GetFrameType()
    {
        return TYPE_INTERCEPTION;
    }

protected:
    MethodDesc*     m_pFuncDesc;      // func desc of the function being called
    LPVOID          m_ReturnAddress;  // return address into Com code

private:
    friend const BYTE * __stdcall ComPreStubWorker(ComPrestubMethodFrame *pPFrame);

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(ComPrestubMethodFrame)
};





//------------------------------------------------------------------------
// These macros GC-protect OBJECTREF pointers on the EE's behalf.
// In between these macros, the GC can move but not discard the protected
// objects. If the GC moves an object, it will update the guarded OBJECTREF's.
// Typical usage:
//
//   OBJECTREF or = <some valid objectref>;
//   GCPROTECT_BEGIN(or);
//
//   ...<do work that can trigger GC>...
//
//   GCPROTECT_END();
//
//
// These macros can also protect multiple OBJECTREF's if they're packaged
// into a structure:
//
//   struct xx {
//      OBJECTREF o1;
//      OBJECTREF o2;
//   } gc;
//
//   GCPROTECT_BEGIN(gc);
//   ....
//   GCPROTECT_END();
//
//
// Notes:
//
//   - GCPROTECT_BEGININTERIOR() can be used in place of GCPROTECT_BEGIN()
//     to handle the case where one or more of the OBJECTREFs is potentially
//     an interior pointer.  This is a rare situation, because boxing would
//     normally prevent us from encountering it.  Be aware that the OBJECTREFs
//     we protect are not validated in this situation.
//
//   - GCPROTECT_ARRAY_BEGIN() can be used when an array of object references
//     is allocated on the stack.  The pointer to the first element is passed
//     along with the number of elements in the array.
//
//   - The argument to GCPROTECT_BEGIN should be an lvalue because it
//     uses "sizeof" to count the OBJECTREF's.
//
//   - GCPROTECT_BEGIN spiritually violates our normal convention of not passing
//     non-const refernce arguments. Unfortunately, this is necessary in
//     order for the sizeof thing to work.
//
//   - GCPROTECT_BEGIN does _not_ zero out the OBJECTREF's. You must have
//     valid OBJECTREF's when you invoke this macro.
//
//   - GCPROTECT_BEGIN begins a new C nesting block. Besides allowing
//     GCPROTECT_BEGIN's to nest, it also has the advantage of causing
//     a compiler error if you forget to code a maching GCPROTECT_END.
//
//   - If you are GCPROTECTing something, it means you are expecting a GC to occur.
//     So we assert that GC is not forbidden. If you hit this assert, you probably need
//     a HELPER_METHOD_FRAME to protect the region that can cause the GC.
//------------------------------------------------------------------------
#define GCPROTECT_BEGIN(ObjRefStruct)           do {            \
                GCFrame __gcframe((OBJECTREF*)&(ObjRefStruct),  \
                sizeof(ObjRefStruct)/sizeof(OBJECTREF),         \
                FALSE);                                         \
                _ASSERTE(!GetThread()->GCForbidden());          \
                DEBUG_ASSURE_NO_RETURN_IN_THIS_BLOCK

#define GCPROTECT_ARRAY_BEGIN(ObjRefArray,cnt) do {             \
                GCFrame __gcframe((OBJECTREF*)&(ObjRefArray),   \
                cnt,                                            \
                FALSE);                                         \
                _ASSERTE(!GetThread()->GCForbidden());          \
                DEBUG_ASSURE_NO_RETURN_IN_THIS_BLOCK

#define GCPROTECT_BEGININTERIOR(ObjRefStruct)           do {    \
                GCFrame __gcframe((OBJECTREF*)&(ObjRefStruct),  \
                sizeof(ObjRefStruct)/sizeof(OBJECTREF),         \
                TRUE);                                          \
                _ASSERTE(!GetThread()->GCForbidden());          \
                DEBUG_ASSURE_NO_RETURN_IN_THIS_BLOCK

#define GCPROTECT_END()                     __gcframe.Pop(); } while(0)

//------------------------------------------------------------------------
// This frame protects object references for the EE's convenience.
// This frame type actually is created from C++.
//------------------------------------------------------------------------
class GCFrame : public Frame
{
public:


    //--------------------------------------------------------------------
    // This constructor pushes a new GCFrame on the frame chain.
    //--------------------------------------------------------------------
    GCFrame() { };
    GCFrame(OBJECTREF *pObjRefs, UINT numObjRefs, BOOL maybeInterior);
    void Init(Thread *pThread, OBJECTREF *pObjRefs, UINT numObjRefs, BOOL maybeInterior);


    //--------------------------------------------------------------------
    // Pops the GCFrame and cancels the GC protection. Also
    // trashes the contents of pObjRef's in _DEBUG.
    //--------------------------------------------------------------------
    VOID Pop();

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc);

#ifdef _DEBUG
    virtual BOOL Protects(OBJECTREF *ppORef)
    {
        for (UINT i = 0; i < m_numObjRefs; i++) {
            if (ppORef == m_pObjRefs + i) {
                return TRUE;
            }
        }
        return FALSE;
    }
#endif

private:
    OBJECTREF *m_pObjRefs;
    UINT       m_numObjRefs;
    Thread    *m_pCurThread;
    BOOL       m_MaybeInterior;

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER(GCFrame)
};

struct ByRefInfo
{
    ByRefInfo *pNext;
    INT32      argIndex;
    CorElementType typ;
    EEClass   *pClass;
    char       data[1];
};

class ProtectByRefsFrame : public Frame
{
public:
    ProtectByRefsFrame(Thread *pThread, ByRefInfo *brInfo);
    void Pop();

    virtual void GcScanRoots(promote_func *fn, ScanContext *sc);

private:
    ByRefInfo *m_brInfo;
    Thread    *m_pThread;

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(ProtectByRefsFrame)
};

struct ValueClassInfo
{
    ValueClassInfo  *pNext;
    INT32           argIndex;
    CorElementType  typ;
    EEClass         *pClass;
    LPVOID          pData;
};

class ProtectValueClassFrame : public Frame
{
public:
    ProtectValueClassFrame(Thread *pThread, ValueClassInfo *vcInfo);
    void Pop();

    static void PromoteValueClassEmbeddedObjects(promote_func *fn, ScanContext *sc,
                                          EEClass *pClass, PVOID pvObject);    
    virtual void GcScanRoots(promote_func *fn, ScanContext *sc);

private:

    ValueClassInfo *m_pVCInfo;
    Thread    *m_pThread;

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(ProtectValueClassFrame)
};


#ifdef _DEBUG
BOOL IsProtectedByGCFrame(OBJECTREF *ppObjectRef);
#endif













void DoPromote(promote_func *fn, ScanContext* sc, OBJECTREF *address, BOOL interior);





//------------------------------------------------------------------------
// DebuggerClassInitMarkFrame is a small frame whose only purpose in
// life is to mark for the debugger that "class initialization code" is
// being run. It does nothing useful except return good values from
// GetFrameType and GetInterception.
//------------------------------------------------------------------------
class DebuggerClassInitMarkFrame : public Frame
{
public:
    DebuggerClassInitMarkFrame()
    {
        Push();
    };

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        // Nothing to do here.
    }

    virtual int GetFrameType()
    {
        return TYPE_INTERCEPTION;
    }

    virtual Interception GetInterception()
    {
        return INTERCEPTION_CLASS_INIT;
    }

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER(DebuggerClassInitMarkFrame)
};

//------------------------------------------------------------------------
// DebuggerSecurityCodeMarkFrame is a small frame whose only purpose in
// life is to mark for the debugger that "security code" is
// being run. It does nothing useful except return good values from
// GetFrameType and GetInterception.
//------------------------------------------------------------------------
class DebuggerSecurityCodeMarkFrame : public Frame
{
public:
    DebuggerSecurityCodeMarkFrame()
    {
        Push();
    };

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        // Nothing to do here.
    }

    virtual int GetFrameType()
    {
        return TYPE_INTERCEPTION;
    }

    virtual Interception GetInterception()
    {
        return INTERCEPTION_SECURITY;
    }

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER(DebuggerSecurityCodeMarkFrame)
};

//------------------------------------------------------------------------
// DebuggerExitFrame is a small frame whose only purpose in
// life is to mark for the debugger that there is an exit transiton on
// the stack.  This is special cased for the "break" IL instruction since
// it is an fcall using a helper frame which returns TYPE_CALL instead of
// an ecall (as in System.Diagnostics.Debugger.Break()) which returns
// TYPE_EXIT.  This just makes the two consistent for debugging services.
//------------------------------------------------------------------------
class DebuggerExitFrame : public Frame
{
public:
    DebuggerExitFrame()
    {
        Push();
    };

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        // Nothing to do here.
    }

    virtual int GetFrameType()
    {
        return TYPE_EXIT;
    }

    // Return information about an unmanaged call the frame
    // will make.
    // ip - the unmanaged routine which will be called
    // returnIP - the address in the stub which the unmanaged routine
    //            will return to.
    // returnSP - the location returnIP is pushed onto the stack
    //            during the call.
    //
    virtual void GetUnmanagedCallSite(void **ip,
                                      void **returnIP,
                                      void **returnSP)
    {
        if (ip)
            *ip = NULL;

        if (returnIP)
            *returnIP = NULL;

        if (returnSP)
            *returnSP = NULL;
    }

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER(DebuggerExitFrame)
};




//------------------------------------------------------------------------
// This frame guards an unmanaged->managed transition thru a UMThk
//------------------------------------------------------------------------
class UMThkCallFrame : public UnmanagedToManagedCallFrame
{
    friend class UMThunkStubCache;

public:

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        //@PERF: optimize
        // if we are the topmost frame, we might have some marshalled argument objects
        // in the stack that needs our protection, lets save 'em.
        PromoteCalleeStack(fn, sc);
        UnmanagedToManagedCallFrame::GcScanRoots(fn,sc);
    }

    // promote callee stack, if we are the topmost frame
    void PromoteCalleeStack(promote_func *fn, ScanContext* sc);


    // Manipulate the GCArgsprotection enable bit. For us, it's just a simple boolean!
    BOOL GCArgsProtectionOn()
    {
        return !!*(GetGCInfoFlagPtr());
    }

    VOID SetGCArgsProtectionState(BOOL fEnable)
    {
        *(GetGCInfoFlagPtr()) = !!fEnable;
    }

    // used by PromoteCalleeStack to get the destination function sig and module
    // NOTE: PromoteCalleeStack only promotes bona-fide arguments, and not
    // the "this" reference. The whole purpose of PromoteCalleeStack is
    // to protect the partially constructed argument array during
    // the actual process of argument marshaling.
    virtual PCCOR_SIGNATURE GetTargetCallSig();
    virtual Module *GetTargetModule();

    // Return the # of stack bytes pushed by the unmanaged caller.
    UINT GetNumCallerStackBytes();


    // Should managed exceptions be passed thru?
    virtual BOOL CatchManagedExceptions()
    {
        return FALSE;
    }


    // Convert a thrown COM+ exception to an unmanaged result.
    virtual UINT32 ConvertComPlusException(OBJECTREF pException);


    UMEntryThunk *GetUMEntryThunk()
    {
        return (UMEntryThunk*)GetDatum();
    }


    static UINT GetOffsetOfUMEntryThunk()
    {
        return GetOffsetOfDatum();
    }

    const BYTE* GetManagedTarget();

    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetUMThkCallFrameVPtr()
    {
        RETURNFRAMEVPTR(UMThkCallFrame);
    }

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(UMThkCallFrame)
};




//------------------------------------------------------------------------
// This frame is pushed by any JIT'ted method that contains one or more
// inlined N/Direct calls. Note that the JIT'ted method keeps it pushed
// the whole time to amortize the pushing cost across the entire method.
//------------------------------------------------------------------------
class InlinedCallFrame : public Frame
{
public:
    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)
    {
        // Nothing to protect here.
    }


    virtual MethodDesc *GetFunction()
    {
        if (FrameHasActiveCall(this) && (((size_t)m_Datum) & ~0xffff) != 0)
            return (MethodDesc*)m_Datum;
        else
            return NULL;
    }

    // Retrieves the return address into the code that called out
    // to managed code
    virtual LPVOID GetReturnAddress()
    {
        /* m_pCallSiteTracker contains the ESP just before the call, i.e.*/
        /* the return address pushed by the call is just in front of it  */

        if (FrameHasActiveCall(this))
            return m_pCallerReturnAddress;
        else
            return NULL;
    }

    virtual LPVOID* GetReturnAddressPtr()
    {
        if (FrameHasActiveCall(this))
            return &m_pCallerReturnAddress;
        else
            return NULL;
    }

    virtual void UpdateRegDisplay(const PREGDISPLAY);

protected:
    MethodDesc*          m_Datum;   // func desc of the function being called
                                    // or stack argument size (for calli)
    LPVOID               m_pCallSiteTracker;
    LPVOID               m_pCallerReturnAddress;
    CalleeSavedRegisters m_pCalleeSavedRegisters;

public:
    //---------------------------------------------------------------
    // Expose key offsets and values for stub generation.
    //---------------------------------------------------------------
    static LPVOID GetInlinedCallFrameFrameVPtr()
    {
        RETURNFRAMEVPTR(InlinedCallFrame);
    }

    static unsigned GetOffsetOfCallSiteTracker()
    {
        return (unsigned)(offsetof(InlinedCallFrame, m_pCallSiteTracker));
    }

    static unsigned GetOffsetOfCallSiteTarget()
    {
        return (unsigned)(offsetof(InlinedCallFrame, m_Datum));
    }

    static unsigned GetOffsetOfCallerReturnAddress()
    {
        return (unsigned)(offsetof(InlinedCallFrame, m_pCallerReturnAddress));
    }

    static unsigned GetOffsetOfCalleeSavedRegisters()
    {
        return (unsigned)(offsetof(InlinedCallFrame, m_pCalleeSavedRegisters));
    }

    // Is the specified frame an InlinedCallFrame which has an active call
    // inside it right now?
    static BOOL FrameHasActiveCall(Frame *pFrame)
    {
        return (pFrame &&
                (pFrame != FRAME_TOP) &&
                (GetInlinedCallFrameFrameVPtr() == pFrame->GetVTablePtr()) &&
                (((InlinedCallFrame *)pFrame)->m_pCallSiteTracker != 0));
    }

    int GetFrameType()
    {
        return TYPE_INTERNAL; // will have to revisit this case later
    }

    virtual BOOL IsTransitionToNativeFrame()
    {
        return TRUE;
    }

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(InlinedCallFrame)
};

//------------------------------------------------------------------------
// This frame is used to mark a Context/AppDomain Transition
//------------------------------------------------------------------------
class ContextTransitionFrame : public Frame
{
    friend EXCEPTION_DISPOSITION __cdecl ContextTransitionFrameHandler(EXCEPTION_RECORD *pExceptionRecord, 
                         EXCEPTION_REGISTRATION_RECORD *pEstablisherFrame,
                         CONTEXT *pContext,
                         void *DispatcherContext);

    // !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    // WARNING: If you change this structure, you must also change
    // System.Runtime.Remoting.ContextTransitionFrame to match it.
    // You must also change CORCOMPILE_DOMAIN_TRANSITION_FRAME in
    // corcompile.h
    // !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    // exRecord field must always go first
    EXCEPTION_REGISTRATION_RECORD exRecord;
    Context *m_pReturnContext;
    Object *m_ReturnLogicalCallContext;
    Object *m_ReturnIllogicalCallContext;
    ULONG_PTR m_ReturnWin32Context;

public:

    virtual void GcScanRoots(promote_func *fn, ScanContext* sc);

    static LPVOID GetMethodFrameVPtr()
    {
        RETURNFRAMEVPTR(ContextTransitionFrame);
    }

    virtual Context **GetReturnContextAddr()
    {
        return &m_pReturnContext;
    }

    virtual Object **GetReturnLogicalCallContextAddr()
    {
        return (Object **) &m_ReturnLogicalCallContext;
    }

    virtual Object **GetReturnIllogicalCallContextAddr()
    {
        return (Object **) &m_ReturnIllogicalCallContext;
    }

    virtual ULONG_PTR* GetWin32ContextAddr()
    {
        return &m_ReturnWin32Context;
    }

    virtual void ExceptionUnwind();

        // Install an EH handler so we can unwind properly
    void InstallExceptionHandler();

    void UninstallExceptionHandler();

    static ContextTransitionFrame* CurrFrame(EXCEPTION_REGISTRATION_RECORD *pRec) {
        return (ContextTransitionFrame*)((char *)pRec - offsetof(ContextTransitionFrame, exRecord));
    }

    ContextTransitionFrame() {}

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER(ContextTransitionFrame)
};

#define DECLARE_ALLOCA_CONTEXT_TRANSITION_FRAME(pFrame) \
    void *_pBlob = _alloca(sizeof(ContextTransitionFrame)); \
    ContextTransitionFrame *pFrame = new (_pBlob) ContextTransitionFrame();

INDEBUG(bool isLegalManagedCodeCaller(void* retAddr));
bool isRetAddr(size_t retAddr, size_t* whereCalled);

#ifdef _SECURITY_FRAME_FOR_DISPEX_CALLS
//------------------------------------------------------------------------
// This frame is used to capture the Security Description of a Native
// client.
//------------------------------------------------------------------------
class NativeClientSecurityFrame : public Frame
{
public:
    virtual SecurityDescriptor* GetSecurityDescriptor() = 0;
    virtual void GcScanRoots(promote_func *fn, ScanContext* sc)  { }
};

//------------------------------------------------------------------------
// This frame is used to capture the Security Description of a COM client
//------------------------------------------------------------------------
class ComClientSecurityFrame : public NativeClientSecurityFrame
{
public:
    ComClientSecurityFrame(IServiceProvider *pISP);
    virtual SecurityDescriptor* GetSecurityDescriptor();

private:
    IServiceProvider *m_pISP;
    SecurityDescriptor *m_pSD;

    // Keep as last entry in class
    DEFINE_VTABLE_GETTER_AND_CTOR(ComClientSecurityFrame)
};
#endif  // _SECURITY_FRAME_FOR_DISPEX_CALLS

#endif  //__frames_h__
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gcdecode.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#include "common.h"

#define FPO_INTERRUPTIBLE 0

/* Precompiled header nonsense requires that we do it this way  */

/* GCDecoder.cpp is a common source file bewtween VM and JIT/IL */
/* GCDecoder.cpp is located in $COM99/inc                       */

#include "GCDecoder.cpp"
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gcdesc.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
//
//
// GC Object Pointer Location Series Stuff
//

#ifndef _GCDESC_H_
#define _GCDESC_H_

#if defined(_X86_)
typedef unsigned short HALF_SIZE_T;
#elif defined(_WIN64)
typedef DWORD HALF_SIZE_T;
#endif

typedef DWORD *JSlot;


//
// These two classes make up the apparatus with which the object references
// within an object can be found.
//
// CGCDescSeries:
//
// The CGCDescSeries class describes a series of object references within an
// object by describing the size of the series (which has an adjustment which
// will be explained later) and the starting point of the series.
//
// The series size is adjusted when the map is created by subtracting the
// GetBaseSize() of the object.   On retieval of the size the total size
// of the object is added back.   For non-array objects the total object
// size is equal to the base size, so this returns the same value.   For
// array objects this will yield the size of the data portion of the array.
// Since arrays containing object references will contain ONLY object references
// this is a fast way of handling arrays and normal objects without a
// conditional test
//
//
//
// CGCDesc:
//
// The CGCDesc is a collection of CGCDescSeries objects to describe all the
// different runs of pointers in a particular object.   @TODO [add more on the strange
// way the CGCDesc grows backwards in memory behind the MethodTable]
//

struct val_serie_item
{
	unsigned short nptrs;
	unsigned short skip;
	void set_val_serie_item (unsigned short nptrs, unsigned short skip)
	{
		this->nptrs = nptrs;
		this->skip = skip;
	}
};

class CGCDescSeries
{
public:
	union 
	{
		DWORD seriessize;       		// adjusted length of series (see above) in bytes
		val_serie_item val_serie[1];    //coded serie for value class array
	};

    DWORD startoffset;

    DWORD GetSeriesCount () 
    { 
        return seriessize/sizeof(JSlot); 
    }

    VOID SetSeriesCount (DWORD newcount)
    {
        seriessize = newcount * sizeof(JSlot);
    }

    VOID IncSeriesCount (DWORD increment = 1)
    {
        seriessize += increment * sizeof(JSlot);
    }

    DWORD GetSeriesSize ()
    {
        return seriessize;
    }

    VOID SetSeriesSize (DWORD newsize)
    {
        seriessize = newsize;
    }

    VOID SetSeriesValItem (val_serie_item item, int index)
    {
        val_serie [index] = item;
    }

    VOID SetSeriesOffset (DWORD newoffset)
    {
        startoffset = newoffset;
    }

    DWORD GetSeriesOffset ()
    {
        return startoffset;
    }
};





class CGCDesc
{
    // Don't construct me, you have to hand me a ptr to the *top* of my storage in Init.
    CGCDesc () {}

public:
    static DWORD ComputeSize (DWORD NumSeries)
    {
		_ASSERTE (NumSeries > 0);
        return sizeof(DWORD)+NumSeries*sizeof(CGCDescSeries);
    }

    static VOID Init (PVOID mem, DWORD NumSeries)
    {
        *((DWORD*)mem-1) = NumSeries;
    }

    static CGCDesc *GetCGCDescFromMT (MethodTable *pMT)
    {
        // If it doesn't contain pointers, there isn't a GCDesc
        _ASSERTE(pMT->ContainsPointers());
        return (CGCDesc *) pMT;
    }

    DWORD GetNumSeries ()
    {
        return *((DWORD*)this-1);
    }

    // Returns lowest series in memory.
    CGCDescSeries *GetLowestSeries ()
    {
		_ASSERTE (GetNumSeries() > 0);
        return (CGCDescSeries*)((BYTE*)this-GetSize());
    }

    // Returns highest series in memory.
    CGCDescSeries *GetHighestSeries ()
    {
        return (CGCDescSeries*)((DWORD*)this-1)-1;
    }

    // Size of the entire slot map.
    DWORD GetSize ()
    {
        return ComputeSize(GetNumSeries());
    }

};


#endif _GCDESC_H_
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gccover.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/****************************************************************************/
/*                              gccover.cpp                                 */
/****************************************************************************/

/* This file holds code that is designed to test GC pointer tracking in
   fully interruptable code.  We basically do a GC everywhere we can in
   jitted code 
 */
/****************************************************************************/

#include "common.h"
#include "EEConfig.h"
#include "gms.h"
#include "utsem.h"

#if defined(STRESS_HEAP) && defined(_DEBUG)

//
// Hacks to keep msdis.h include file happy
//
typedef int fpos_t; 
static unsigned long strtoul(const char *, char **, int) { _ASSERTE(!"HACK");  return(0); }
static unsigned long strtol(const char *, char **, int) { _ASSERTE(!"HACK");  return(0); }
static void clearerr(FILE *) { _ASSERTE(!"HACK");  return; }
static int fclose(FILE *) { _ASSERTE(!"HACK");  return(0); }
static int feof(FILE *) { _ASSERTE(!"HACK");  return(0); }
static int ferror(FILE *) { _ASSERTE(!"HACK");  return(0); }
static int fgetc(FILE *) { _ASSERTE(!"HACK");  return(0); }
static int fgetpos(FILE *, fpos_t *) { _ASSERTE(!"HACK");  return(0); }
static char * fgets(char *, int, FILE *) { _ASSERTE(!"HACK");  return(0); }
static int fputc(int, FILE *) { _ASSERTE(!"HACK");  return(0); }
static int fputs(const char *, FILE *) { _ASSERTE(!"HACK");  return(0); }
static size_t fread(void *, size_t, size_t, FILE *) { _ASSERTE(!"HACK");  return(0); }
static FILE * freopen(const char *, const char *, FILE *) { _ASSERTE(!"HACK");  return(0); }
static int fscanf(FILE *, const char *, ...) { _ASSERTE(!"HACK");  return(0); }
static int fsetpos(FILE *, const fpos_t *) { _ASSERTE(!"HACK");  return(0); }
static int fseek(FILE *, long, int) { _ASSERTE(!"HACK");  return(0); }
static int getc(FILE *) { _ASSERTE(!"HACK");  return(0); }
static int getchar(void) { _ASSERTE(!"HACK");  return(0); }
static char * gets(char *) { _ASSERTE(!"HACK");  return(0); }
static void perror(const char *) { _ASSERTE(!"HACK");  return; }
static int putc(int, FILE *) { _ASSERTE(!"HACK");  return(0); }
static int putchar(int) { _ASSERTE(!"HACK");  return(0); }
static int puts(const char *) { _ASSERTE(!"HACK");  return(0); }
static int remove(const char *) { _ASSERTE(!"HACK");  return(0); }
static int rename(const char *, const char *) { _ASSERTE(!"HACK");  return(0); }
static void rewind(FILE *) { _ASSERTE(!"HACK");  return; }
static int scanf(const char *, ...) { _ASSERTE(!"HACK");  return(0); }
static void setbuf(FILE *, char *) { _ASSERTE(!"HACK");  return; }
static int setvbuf(FILE *, char *, int, size_t) { _ASSERTE(!"HACK");  return(0); }
static int sscanf(const char *, const char *, ...) { _ASSERTE(!"HACK");  return(0); }
static FILE * tmpfile(void) { _ASSERTE(!"HACK");  return(0); }
static char * tmpnam(char *) { _ASSERTE(!"HACK");  return(0); }
static int ungetc(int, FILE *) { _ASSERTE(!"HACK");  return(0); }
static int vfprintf(FILE *, const char *, va_list) { _ASSERTE(!"HACK");  return(0); }
typedef int div_t;
typedef int ldiv_t;
static void   abort(void) { _ASSERTE(!"HACK");  return; }
static void   exit(int) { _ASSERTE(!"HACK");  return; }
static void * bsearch(const void *, const void *, size_t, size_t, int (__cdecl *)(const void *, const void *)) { _ASSERTE(!"HACK");  return(0); }
static div_t  div(int, int) { _ASSERTE(!"HACK");  return(0); }
static char * getenv(const char *) { _ASSERTE(!"HACK");  return(0); }
static ldiv_t ldiv(long, long) { _ASSERTE(!"HACK");  return(0); }
static int    mblen(const char *, size_t) { _ASSERTE(!"HACK");  return(0); }
static int    mbtowc(wchar_t *, const char *, size_t) { _ASSERTE(!"HACK");  return(0); }
static size_t mbstowcs(wchar_t *, const char *, size_t) { _ASSERTE(!"HACK");  return(0); }
static int    rand(void) { _ASSERTE(!"HACK");  return(0); }
static void   srand(unsigned int) { _ASSERTE(!"HACK");  return; }
static double strtod(const char *, char **) { _ASSERTE(!"HACK");  return(0); }
static int    system(const char *) { _ASSERTE(!"HACK");  return(0); }
static int    wctomb(char *, wchar_t) { _ASSERTE(!"HACK");  return(0); }
static size_t wcstombs(char *, const wchar_t *, size_t) { _ASSERTE(!"HACK");  return(0); }
static __int64 _strtoi64(const char *, char **, int) { _ASSERTE(!"HACK");  return(0); }
static unsigned __int64 _strtoui64(const char *, char **, int) { _ASSERTE(!"HACK");  return(0); }

#include "msdis.h"

    // We need a X86 instruction walker (disassembler), here are some
    // routines for caching such a disassembler in a concurrent environment. 
static DIS* g_Disasm = 0;

static DIS* GetDisasm() {
#ifdef _X86_
	DIS* myDisasm = (DIS*)(size_t) FastInterlockExchange((LONG*) &g_Disasm, 0);
	if (myDisasm == 0)
		myDisasm = DIS::PdisNew(DIS::distX86);
	_ASSERTE(myDisasm);
	return(myDisasm);
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - GetDisasm (GcCover.cpp)");
    return NULL;
#endif // _X86_
}

static void ReleaseDisasm(DIS* myDisasm) {
#ifdef _X86_
	myDisasm = (DIS*)(size_t) FastInterlockExchange((LONG*) &g_Disasm, (LONG)(size_t) myDisasm);
	delete myDisasm;
#else
    _ASSERTE(!"@TODO Port - ReleaseDisasm (GcCover.cpp)");
    return;
#endif // _X86_
}


#define INTERRUPT_INSTR	    0xF4				// X86 HLT instruction (any 1 byte illegal instruction will do)
#define INTERRUPT_INSTR_CALL   0xFA        		// X86 CLI instruction 
#define INTERRUPT_INSTR_PROTECT_RET   0xFB      // X86 STI instruction 

/****************************************************************************/
/* GCCOverageInfo holds the state of which instructions have been visited by
   a GC and which ones have not */

#pragma warning(push)
#pragma warning(disable : 4200 )  // zero-sized array

class GCCoverageInfo {
public:
    BYTE* methStart;
	BYTE* curInstr;					// The last instruction that was able to execute 
    MethodDesc* lastMD;     		// Used to quickly figure out the culprite

		// Following 6 variables are for prolog / epilog walking coverage		
	ICodeManager* codeMan;			// CodeMan for this method
	void* gcInfo;					// gcInfo for this method

	Thread* callerThread;			// Thread associated with context callerRegs
	CONTEXT callerRegs;				// register state when method was entered
    unsigned gcCount;               // GC count at the time we caputured the regs
	bool    doingEpilogChecks;		// are we doing epilog unwind checks? (do we care about callerRegs?)

	enum { hasExecutedSize = 4 };
	unsigned hasExecuted[hasExecutedSize];
	unsigned totalCount;
	BYTE savedCode[0];				// really variable sized

		// Sloppy bitsets (will wrap, and not threadsafe) but best effort is OK
        // since we just need half decent coverage.  
	BOOL IsBitSetForOffset(unsigned offset) {
		unsigned dword = hasExecuted[(offset >> 5) % hasExecutedSize];
		return(dword & (1 << (offset & 0x1F)));
	}

    void SetBitForOffset(unsigned offset) {
		unsigned* dword = &hasExecuted[(offset >> 5) % hasExecutedSize];
		*dword |= (1 << (offset & 0x1F)) ;
	}
};

#pragma warning(pop)

/****************************************************************************/
/* called when a method is first jitted when GCStress level 4 is on */

void SetupGcCoverage(MethodDesc* pMD, BYTE* methodStartPtr) {
#ifdef _DEBUG
    if (!g_pConfig->ShouldGcCoverageOnMethod(pMD->m_pszDebugMethodName)) {
        return;
    }
#endif    
    // Look directly at m_CodeOrIL, since GetUnsafeAddrofCode() will return the
    // prestub in case of EnC (bug #71613)
    SLOT methodStart = (SLOT) methodStartPtr;
#ifdef _X86_
    // When profiler exists, the instruction at methodStart is "jmp XXXXXXXX".
    // Our JitManager for normal JIT does not maintain this address.
    SLOT p = methodStart;
    if (p[0] == 0xE9)
    {
        SLOT addr = p+5;
        int iVal = *(int*)((char*)p+1);
        methodStart = addr + iVal;
    }
#else
    _ASSERTE(!"NYI for platform");
#endif

    /* get the GC info */	
    IJitManager* jitMan = ExecutionManager::FindJitMan(methodStart);
	ICodeManager* codeMan = jitMan->GetCodeManager();
	METHODTOKEN methodTok;
	DWORD methodOffs;
    jitMan->JitCode2MethodTokenAndOffset(methodStart, &methodTok, &methodOffs);
	_ASSERTE(methodOffs == 0);
	void* gcInfo = jitMan->GetGCInfo(methodTok);
	REGDISPLAY regs;
	SLOT cur;
	regs.pPC = (SLOT*) &cur;
    unsigned methodSize = (unsigned)codeMan->GetFunctionSize(gcInfo);
	EECodeInfo codeInfo(methodTok, jitMan);

        // Allocate room for the GCCoverageInfo and copy of the method instructions
	unsigned memSize = sizeof(GCCoverageInfo) + methodSize;
	GCCoverageInfo* gcCover = (GCCoverageInfo*) pMD->GetModule()->GetClassLoader()->GetHighFrequencyHeap()->AllocMem(memSize);	

	memset(gcCover, 0, sizeof(GCCoverageInfo));
	memcpy(gcCover->savedCode, methodStart, methodSize);
    gcCover->methStart = methodStart;
    gcCover->codeMan = codeMan;
    gcCover->gcInfo = gcInfo;
    gcCover->callerThread = 0;
    gcCover->doingEpilogChecks = true;	

	/* sprinkle interupt instructions that will stop on every GCSafe location */

    // @CONSIDER: do this for prejitted code (We don't now because it is pretty slow)
	// If we do prejitted code, we need to remove write protection 
    // DWORD oldProtect;
    // VirtualProtect(methodStart, methodSize, PAGE_READWRITE, &oldProtect);

	cur = methodStart;
	DIS* pdis = GetDisasm();
	BYTE* methodEnd = methodStart + methodSize;
	size_t dummy;
	int instrsPlaced = 0;
	while (cur < methodEnd) {
		unsigned len = (unsigned)pdis->CbDisassemble(0, cur, methodEnd-cur);
		_ASSERTE(len > 0);

        switch (pdis->Trmt()) {
			case DIS::trmtCallInd:
			   *cur = INTERRUPT_INSTR_CALL;        // return value.  May need to protect
				instrsPlaced++;

				// Fall through
		    case DIS::trmtCall:
					// We need to have two interrupt instructions placed before the
					// first call (one at the start, and one to catch us before we
					// can call ourselves again when we remove the first instruction
					// if we don't have this, bail on the epilog checks. 
				if (instrsPlaced < 2)
					gcCover->doingEpilogChecks = false;
		}

            // For fully interruptable code, we end up wacking every instrction
            // to INTERRUPT_INSTR.  For non-fully interrupable code, we end
            // up only touching the call instructions (specially so that we
            // can really do the GC on the instruction just after the call).  
        if (codeMan->IsGcSafe(&regs, gcInfo, &codeInfo, 0))
            *cur = INTERRUPT_INSTR;

			// we will wack every instruction in the prolog an epilog to make certain
			// our unwinding logic works there.  
        if (codeMan->IsInPrologOrEpilog(cur-methodStart, gcInfo, &dummy)) {
			instrsPlaced++;
            *cur = INTERRUPT_INSTR;
		}
        cur += len;
	}

        // If we are not able to place a interrupt at the first instruction, this means that
        // we are partially interrupable with no prolog.  Just don't bother to confirm that
		// the epilog since it will be trival (a single return instr)
    assert(methodSize > 0);
	if (*methodStart != INTERRUPT_INSTR)
        gcCover->doingEpilogChecks = false;

    ReleaseDisasm(pdis);

	pMD->m_GcCover = gcCover;
}

static bool isMemoryReadable(const void* start, unsigned len) 
{
    void* buff = _alloca(len);
    return(ReadProcessMemory(GetCurrentProcess(), start, buff, len, 0) != 0);
}

static bool isValidObject(Object* obj) {
    if (!isMemoryReadable(obj, sizeof(Object)))
        return(false);

    MethodTable* pMT = obj->GetMethodTable();
    if (!isMemoryReadable(pMT, sizeof(MethodTable)))
        return(false);

    EEClass* cls = pMT->GetClass();
    if (!isMemoryReadable(cls, sizeof(EEClass)))
        return(false);

    return(cls->GetMethodTable() == pMT);
}

static DWORD getRegVal(unsigned regNum, PCONTEXT regs)
{
#ifdef _X86_
    switch(regNum) {    
    case 0:
        return(regs->Eax);
    case 1:
        return(regs->Ecx);
    case 2:
        return(regs->Edx);
    case 3:
        return(regs->Ebx);
    case 4:
        return(regs->Esp);
    case 5:
        return(regs->Ebp);
    case 6:
        return(regs->Esi);
    case 7:
        return(regs->Edi);
    default:
        _ASSERTE(!"Bad Register");
    }
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - getRegVal (GcCover.cpp)");
#endif // _X86_
    return(0);
}

/****************************************************************************/
static SLOT getTargetOfCall(SLOT instrPtr, PCONTEXT regs, SLOT*nextInstr) {

    if (instrPtr[0] == 0xE8) {
        *nextInstr = instrPtr + 5;
        return((SLOT)(*((size_t*) &instrPtr[1]) + (size_t) instrPtr + 5)); 
    }

    if (instrPtr[0] == 0xFF) {
        if (instrPtr[1] == 025) {               // call [XXXXXXXX]
            *nextInstr = instrPtr + 6;
            size_t* ptr = *((size_t**) &instrPtr[2]);
            return((SLOT)*ptr);
        }

        int reg = instrPtr[1] & 7;
        if ((instrPtr[1] & ~7) == 0320)    {       // call REG
            *nextInstr = instrPtr + 2;
            return((SLOT)(size_t)getRegVal(reg, regs));
        }
        if ((instrPtr[1] & ~7) == 0020)    {     // call [REG]
            *nextInstr = instrPtr + 2;
            return((SLOT)(*((size_t*)(size_t) getRegVal(reg, regs))));
        }
        if ((instrPtr[1] & ~7) == 0120)    {    // call [REG+XX]
            *nextInstr = instrPtr + 3;
            return((SLOT)(*((size_t*)(size_t) (getRegVal(reg, regs) + *((char*) &instrPtr[2])))));
        }
        if ((instrPtr[1] & ~7) == 0220)    {   // call [REG+XXXX]
            *nextInstr = instrPtr + 6;
            return((SLOT)(*((size_t*)(size_t) (getRegVal(reg, regs) + *((int*) &instrPtr[2])))));
        }
    }
    return(0);      // Fail
}

/****************************************************************************/
void checkAndUpdateReg(DWORD& origVal, DWORD curVal, bool gcHappened) {
    if (origVal == curVal)
        return;

		// You can come and see me if these asserts go off -
		// They indicate either that unwinding out of a epilog is wrong or that
		// the harness is got a bug.  -vancem

    _ASSERTE(gcHappened);	// If the register values are different, a GC must have happened
    _ASSERTE(g_pGCHeap->IsHeapPointer((BYTE*) size_t(origVal)));	// And the pointers involved are on the GCHeap
    _ASSERTE(g_pGCHeap->IsHeapPointer((BYTE*) size_t(curVal)));
    origVal = curVal;       // this is now the best estimate of what should be returned. 
}

static int GCcoverCount = 0;

MethodDesc* AsMethodDesc(size_t addr);
void* forceStack[8];

/****************************************************************************/
BOOL OnGcCoverageInterrupt(PCONTEXT regs) 
{
#ifdef _X86_
		// So that you can set counted breakpoint easily;
	GCcoverCount++;
	forceStack[0]= &regs;				// This is so I can see it fastchecked

    volatile BYTE* instrPtr = (BYTE*)(size_t) regs->Eip;
	forceStack[4] = &instrPtr;		    // This is so I can see it fastchecked
	
    MethodDesc* pMD = IP2MethodDesc((SLOT)(size_t) regs->Eip);
	forceStack[1] = &pMD;				// This is so I can see it fastchecked
    if (pMD == 0)  
        return(FALSE);

    GCCoverageInfo* gcCover = pMD->m_GcCover;
	forceStack[2] = &gcCover;			// This is so I can see it fastchecked
    if (gcCover == 0)  
        return(FALSE);		// we aren't doing code gcCoverage on this function

    BYTE* methodStart = gcCover->methStart;
    _ASSERTE(methodStart <= instrPtr);

    /****
    if (gcCover->curInstr != 0)
        *gcCover->curInstr = INTERRUPT_INSTR;
    ****/
  
    unsigned offset = instrPtr - methodStart;
	forceStack[3] = &offset;				// This is so I can see it fastchecked

	BYTE instrVal = *instrPtr;
	forceStack[6] = &instrVal;			// This is so I can see it fastchecked
	
    if (instrVal != INTERRUPT_INSTR && instrVal != INTERRUPT_INSTR_CALL && instrVal != INTERRUPT_INSTR_PROTECT_RET) {
        _ASSERTE(instrVal == gcCover->savedCode[offset]);  // some one beat us to it.
		return(TRUE);       // Someone beat us to it, just go on running
    }

    bool atCall = (instrVal == INTERRUPT_INSTR_CALL);
    bool afterCallProtect = (instrVal == INTERRUPT_INSTR_PROTECT_RET);

	/* are we at the very first instruction?  If so, capture the register state */

    Thread* pThread = GetThread();
    if (gcCover->doingEpilogChecks) {
        if (offset == 0) {
            if (gcCover->callerThread == 0) {
                if (VipInterlockedCompareExchange((LPVOID*) &gcCover->callerThread, pThread, 0) == 0) {
                    gcCover->callerRegs = *regs;
                    gcCover->gcCount = GCHeap::GetGcCount();
                }
            }	
            else {
                // We have been in this routine before.  Give up on epilog checking because
                // it is hard to insure that the saved caller register state is correct 
                // This also has the effect of only doing the checking once per routine
                // (Even if there are multiple epilogs) 
                gcCover->doingEpilogChecks = false;
            }
        } 
        else {
            _ASSERTE(gcCover->callerThread != 0);	// we should have hit something at offset 0
            // We need to insure that the caputured caller state cooresponds to the
            // method we are currently epilog testing.  To insure this, we put the
            // barrier back up, after we are in.  If we reenter this routine we simply
            // give up. (since we assume we will get enough coverage with non-recursive functions).
            
            // This works because we insure in the SetupGcCover that there will be at least
            // one more interrupt (which will put back the barrier), before we can call 
            // back into this routine 
            if (gcCover->doingEpilogChecks)
                *methodStart = INTERRUPT_INSTR;
        }

        // If some other thread removes interrupt points, we abandon epilog testing
        // for this routine since the barrier at the begining of the routine may not
        // be up anymore, and thus the caller context is now not guarenteed to be correct.  
        // This should happen only very rarely so is not a big deal.
        if (gcCover->callerThread != pThread)
            gcCover->doingEpilogChecks = false;
    }
    

    /* remove the interrupt instruction */
    *instrPtr = instrVal = gcCover->savedCode[offset];
	

	/* are we in a prolog or epilog?  If so just test the unwind logic
	   but don't actually do a GC since the prolog and epilog are not
	   GC safe points */
	size_t dummy;
	if (gcCover->codeMan->IsInPrologOrEpilog(instrPtr-methodStart, gcCover->gcInfo, &dummy)) {
		REGDISPLAY regDisp;
        CONTEXT copyRegs = *regs;
		pThread->Thread::InitRegDisplay(&regDisp, &copyRegs, true);
        pThread->UnhijackThread();

		IJitManager* jitMan = ExecutionManager::FindJitMan(methodStart);
		METHODTOKEN methodTok;
		DWORD methodOffs;
		jitMan->JitCode2MethodTokenAndOffset(methodStart, &methodTok, &methodOffs);
		_ASSERTE(methodOffs == 0);

	    CodeManState codeManState;
        codeManState.dwIsSet = 0;

		EECodeInfo codeInfo(methodTok, jitMan, pMD);

			// unwind out of the prolog or epilog
		gcCover->codeMan->UnwindStackFrame(&regDisp, gcCover->gcInfo,  &codeInfo, UpdateAllRegs, &codeManState);
	
			// Note we always doing the unwind, since that at does some checking (that we 
			// unwind to a valid return address), but we only do the precise checking when
			// we are certain we have a good caller state 
		if (gcCover->doingEpilogChecks) {
				// Confirm that we recovered our register state properly
			_ASSERTE((PBYTE*) size_t(gcCover->callerRegs.Esp) == regDisp.pPC);

                // If a GC happened in this function, then the registers will not match
                // precisely.  However there is still checks we can do.  Also we can update
                // the saved register to its new value so that if a GC does not happen between
                // instructions we can recover (and since GCs are not allowed in the 
                // prologs and epilogs, we get get complete coverage except for the first
                // instruction in the epilog  (TODO: fix it for the first instr Case)

			_ASSERTE(pThread->PreemptiveGCDisabled());	// Epilogs should be in cooperative mode, no GC can happen right now. 
            bool gcHappened = gcCover->gcCount != GCHeap::GetGcCount();
            checkAndUpdateReg(gcCover->callerRegs.Edi, *regDisp.pEdi, gcHappened);
            checkAndUpdateReg(gcCover->callerRegs.Esi, *regDisp.pEsi, gcHappened);
            checkAndUpdateReg(gcCover->callerRegs.Ebx, *regDisp.pEbx, gcHappened);
            checkAndUpdateReg(gcCover->callerRegs.Ebp, *regDisp.pEbp, gcHappened);

            gcCover->gcCount = GCHeap::GetGcCount();    
		}
		return(TRUE);
	}


    /* In non-fully interrruptable code, if the EIP is just after a call instr
       means something different because it expects that that we are IN the 
       called method, not actually at the instruction just after the call. This
       is important, because until the called method returns, IT is responcible
       for protecting the return value.  Thus just after a call instruction
       we have to protect EAX if the method being called returns a GC pointer.

       To figure this out, we need to stop AT the call so we can determine the
       target (and thus whether it returns a GC pointer), and then place the
       a different interrupt instruction so that the GCCover harness protects
       EAX before doing the GC).  This effectively simulates a hijack in 
       non-fully interrupable code */

    if (atCall) {
        BYTE* nextInstr;
        SLOT target = getTargetOfCall((BYTE*) instrPtr, regs, &nextInstr);
        if (target == 0)
            return(TRUE);
        MethodDesc* targetMD = IP2MethodDesc((SLOT) target);
        if (targetMD == 0) {
            if (*((BYTE*) target) != 0xE8)  // target is a CALL, could be a stub
                return(TRUE);      // Dont know what target it is, don't do anything
            
            targetMD = AsMethodDesc(size_t(target + 5));    // See if it is
            if (targetMD == 0)
                return(TRUE);       // Dont know what target it is, don't do anything
        }

            // OK, we have the MD, mark the instruction afer the CALL
            // appropriately
        if (targetMD->ReturnsObject(true) != MethodDesc::RETNONOBJ)
            *nextInstr = INTERRUPT_INSTR_PROTECT_RET;  
        else
            *nextInstr = INTERRUPT_INSTR;
        return(TRUE);    // we just needed to set the next instruction correctly, we are done now.  
    }

    
    bool enableWhenDone = false;
    if (!pThread->PreemptiveGCDisabled()) {
        // We are in preemtive mode in JITTed code. currently this can only
        // happen in a couple of instructions when we have an inlined PINVOKE
        // method. 

            // Better be a CALL (direct or indirect), or a MOV instruction (three flavors)
            // pop ECX or add ESP xx (for cdecl pops)
            // or cmp, je (for the PINVOKE ESP checks 
        if (!(instrVal == 0xE8 || instrVal == 0xFF || 
                 instrVal == 0x89 || instrVal == 0x8B || instrVal == 0xC6 ||
                 instrVal == 0x59 || instrVal == 0x83) || instrVal == 0x3B ||
                 instrVal == 0x74)
            _ASSERTE(!"Unexpected instruction in preemtive JITTED code");
        pThread->DisablePreemptiveGC();
        enableWhenDone = true;
    }


#if 0
    // TODO currently disableed.  we only do a GC once per instruction location.  

  /* note that for multiple threads, we can loose track and
       forget to set reset the interrupt after we executed
       an instruction, so some instruction points will not be
       executed twice, but we still ge350t very good coverage 
       (perfect for single threaded cases) */

    /* if we have not run this instruction in the past */
    /* remember to wack it to an INTERUPT_INSTR again */

    if (!gcCover->IsBitSetForOffset(offset))  {
        // gcCover->curInstr = instrPtr;
        gcCover->SetBitForOffset(offset);
    }
#endif 

	Thread* curThread = GetThread();
	_ASSERTE(curThread);
	
    ResumableFrame frame(regs);

	frame.Push(curThread);

    GCFrame gcFrame;
    DWORD retVal = 0;
    if (afterCallProtect) {         // Do I need to protect return value?
        retVal = regs->Eax;
        gcFrame.Init(curThread, (OBJECTREF*) &retVal, 1, TRUE);
    }

	if (gcCover->lastMD != pMD) {
		LOG((LF_GCROOTS, LL_INFO100000, "GCCOVER: Doing GC at method %s::%s offset 0x%x\n",
				 pMD->m_pszDebugClassName, pMD->m_pszDebugMethodName, offset));
		gcCover->lastMD =pMD;
	} else {
		LOG((LF_GCROOTS, LL_EVERYTHING, "GCCOVER: Doing GC at method %s::%s offset 0x%x\n",
				pMD->m_pszDebugClassName, pMD->m_pszDebugMethodName, offset));
	}

	g_pGCHeap->StressHeap();

    if (afterCallProtect) {
        regs->Eax = retVal;
		gcFrame.Pop();
    }

	frame.Pop(curThread);

    if (enableWhenDone) {
        BOOL b = GC_ON_TRANSITIONS(FALSE);      // Don't do a GCStress 3 GC here
        pThread->EnablePreemptiveGC();
        GC_ON_TRANSITIONS(b);
    }

    return(TRUE);
#else // !_X86_
    _ASSERTE(!"@TODO Alpha - OnGcCoverageInterrupt (GcCover.cpp)");
    return(FALSE);
#endif // _X86_
}

#endif // STRESS_HEAP && _DEBUG
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gcee.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==

#include "common.h"
#include "DbgInterface.h"
#include "gcpriv.h"
#include "remoting.h"
#include "comsynchronizable.h"
#include "comsystem.h"
#include "compluswrapper.h"
#include "SyncClean.hpp"

// The contract between GC and the EE, for starting and finishing a GC is as follows:
//
//      LockThreadStore
//      SetGCInProgress
//      SuspendEE
//
//      ... perform the GC ...
//
//      SetGCDone
//      RestartEE
//      UnlockThreadStore
//
// Note that this is intentionally *not* symmetrical.  The EE will assert that the
// GC does most of this stuff in the correct sequence.

// sets up vars for GC

COUNTER_ONLY(PERF_COUNTER_TIMER_PRECISION g_TotalTimeInGC = 0);
COUNTER_ONLY(PERF_COUNTER_TIMER_PRECISION g_TotalTimeSinceLastGCEnd = 0);

void GCHeap::UpdatePreGCCounters()
{

#if defined(ENABLE_PERF_COUNTERS)
    size_t allocation_0 = 0;
    size_t allocation_3 = 0; 
    
    // Publish perf stats
    g_TotalTimeInGC = GET_CYCLE_COUNT();

#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
    int hn = 0;
    for (hn = 0; hn < gc_heap::n_heaps; hn++)
    {
        gc_heap* hp = gc_heap::g_heaps [hn];
            
        allocation_0 += 
            dd_desired_allocation (hp->dynamic_data_of (0))-
            dd_new_allocation (hp->dynamic_data_of (0));
        allocation_3 += 
            dd_desired_allocation (hp->dynamic_data_of (max_generation+1))-
            dd_new_allocation (hp->dynamic_data_of (max_generation+1));
    }
#else
    gc_heap* hp = pGenGCHeap;
    allocation_0 = 
        dd_desired_allocation (hp->dynamic_data_of (0))-
        dd_new_allocation (hp->dynamic_data_of (0));
    allocation_3 = 
        dd_desired_allocation (hp->dynamic_data_of (max_generation+1))-
        dd_new_allocation (hp->dynamic_data_of (max_generation+1));
        
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS

    GetGlobalPerfCounters().m_GC.cbAlloc += allocation_0;
    GetPrivatePerfCounters().m_GC.cbAlloc += allocation_0;
        
    GetGlobalPerfCounters().m_GC.cbAlloc += allocation_3;
    GetPrivatePerfCounters().m_GC.cbAlloc += allocation_3;
    
    GetGlobalPerfCounters().m_GC.cbLargeAlloc += allocation_3;
    GetPrivatePerfCounters().m_GC.cbLargeAlloc += allocation_3;
    
    GetGlobalPerfCounters().m_GC.cPinnedObj = 0;
    GetPrivatePerfCounters().m_GC.cPinnedObj = 0;

    // The following two counters are not a part of the memory object
    // They are reset here due to the lack of a heartbeat mechanism in the CLR
    // We use GCs as a hearbeat, since if the app is not doing gc maybe the perf of it
    // is not interesting?
    GetPrivatePerfCounters().m_Jit.timeInJit = 0;
    GetGlobalPerfCounters().m_Jit.timeInJit = 0;
    GetPrivatePerfCounters().m_Jit.timeInJitBase = 1; // To avoid divide by zero
    GetGlobalPerfCounters().m_Jit.timeInJitBase = 1; // To avoid divide by zero
    GetGlobalPerfCounters().m_Security.timeRTchecks = 0;
    GetPrivatePerfCounters().m_Security.timeRTchecks = 0;
    GetGlobalPerfCounters().m_Security.timeRTchecksBase = 1; // To avoid divide by zero
    GetPrivatePerfCounters().m_Security.timeRTchecksBase = 1; // To avoid divide by zero

#endif //ENABLE_PERF_COUNTERS
}   

void GCHeap::UpdatePostGCCounters()
{

#if defined(ENABLE_PERF_COUNTERS)
    // Publish Perf Data

    int xGen;
#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
    //take the first heap....
    gc_heap* hp = gc_heap::g_heaps[0];
#else
    gc_heap* hp = pGenGCHeap;
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS

//Generation 0 is empty (if there isn't demotion) so its size is 0
//It is more interesting to report the desired size before next collection.
    for (xGen = 0; xGen < MAX_TRACKED_GENS; xGen++)
    {
        size_t gensize = 0;

#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
        int hn = 0;

        for (hn = 0; hn < gc_heap::n_heaps; hn++)
        {
            gc_heap* hp = gc_heap::g_heaps [hn];
            gensize += (xGen == 0) ? 
                dd_desired_allocation (hp->dynamic_data_of (xGen)) :
                hp->generation_size(xGen);          
        }
#else
        gensize = ((xGen == 0) ? 
                   dd_desired_allocation (hp->dynamic_data_of (xGen)) :
                   hp->generation_size(xGen));    
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS


        GetGlobalPerfCounters().m_GC.cGenHeapSize[xGen] = gensize;
        GetPrivatePerfCounters().m_GC.cGenHeapSize[xGen] = gensize;

        GetGlobalPerfCounters().m_GC.cGenCollections[xGen] =
            dd_collection_count (hp->dynamic_data_of (xGen));
        GetPrivatePerfCounters().m_GC.cGenCollections[xGen] =
            dd_collection_count (hp->dynamic_data_of (xGen));
        
    }

    for (xGen = 0; xGen <= (int)GcCondemnedGeneration; xGen++)
    {
        size_t promoted_mem = 0; 
        size_t promoted_finalization_mem = 0;
#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
        int hn = 0;
        for (hn = 0; hn < gc_heap::n_heaps; hn++)
        {
            gc_heap* hp = gc_heap::g_heaps [hn];
            promoted_mem += dd_promoted_size (hp->dynamic_data_of (xGen));
            promoted_finalization_mem += dd_freach_previous_promotion (hp->dynamic_data_of (xGen));
        }
#else
        promoted_mem =  dd_promoted_size (hp->dynamic_data_of (xGen));
        promoted_finalization_mem =  dd_freach_previous_promotion (hp->dynamic_data_of (xGen));
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS
        if (xGen < (MAX_TRACKED_GENS - 1))
        {
            GetGlobalPerfCounters().m_GC.cbPromotedMem[xGen] = promoted_mem;
            GetPrivatePerfCounters().m_GC.cbPromotedMem[xGen] = promoted_mem;
            
            GetGlobalPerfCounters().m_GC.cbPromotedFinalizationMem[xGen] = promoted_finalization_mem;
            GetPrivatePerfCounters().m_GC.cbPromotedFinalizationMem[xGen] = promoted_finalization_mem;
        }
    }
    for (xGen = (int)GcCondemnedGeneration + 1 ; xGen < MAX_TRACKED_GENS-1; xGen++)
    {
        // Reset the promoted mem for generations higer than the condemned one.
        GetGlobalPerfCounters().m_GC.cbPromotedMem[xGen] = 0;
        GetPrivatePerfCounters().m_GC.cbPromotedMem[xGen] = 0;
        
        GetGlobalPerfCounters().m_GC.cbPromotedFinalizationMem[xGen] = 0;
        GetPrivatePerfCounters().m_GC.cbPromotedFinalizationMem[xGen] = 0;
    }

    
    //Committed memory 
    {
        size_t committed_mem = 0;
        size_t reserved_mem = 0;
#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
        int hn = 0;
        for (hn = 0; hn < gc_heap::n_heaps; hn++)
        {
            gc_heap* hp = gc_heap::g_heaps [hn];
#else
            {
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS
                heap_segment* seg = 
                    generation_start_segment (hp->generation_of (max_generation));
                while (seg)
                {
                    committed_mem += heap_segment_committed (seg) - 
                        heap_segment_mem (seg);
                    reserved_mem += heap_segment_reserved (seg) - 
                        heap_segment_mem (seg);
                    seg = heap_segment_next (seg);
                }
                //same for large segments
                seg = 
                    generation_start_segment (hp->generation_of (max_generation + 1));
                while (seg)
                {
                    committed_mem += heap_segment_committed (seg) - 
                        heap_segment_mem (seg);
                    reserved_mem += heap_segment_reserved (seg) - 
                        heap_segment_mem (seg);
                    seg = heap_segment_next (seg);
                }
#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
            }
#else
        }
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS

        GetGlobalPerfCounters().m_GC.cTotalCommittedBytes =
            committed_mem;
        GetPrivatePerfCounters().m_GC.cTotalCommittedBytes = 
            committed_mem;

        GetGlobalPerfCounters().m_GC.cTotalReservedBytes =
            reserved_mem;
        GetPrivatePerfCounters().m_GC.cTotalReservedBytes = 
            reserved_mem;

    }
            
#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
    size_t gensize = 0;
    int hn = 0;

    for (hn = 0; hn < gc_heap::n_heaps; hn++)
    {
        gc_heap* hp = gc_heap::g_heaps [hn];
        gensize += hp->generation_size (max_generation + 1);          
    }
#else
    size_t gensize = hp->generation_size (max_generation + 1);    
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS

    GetGlobalPerfCounters().m_GC.cLrgObjSize = gensize;       
    GetPrivatePerfCounters().m_GC.cLrgObjSize = gensize;      
    GetGlobalPerfCounters().m_GC.cSurviveFinalize =  GetFinalizablePromotedCount();

    GetPrivatePerfCounters().m_GC.cSurviveFinalize = GetFinalizablePromotedCount();
    
    // Compute Time in GC
    PERF_COUNTER_TIMER_PRECISION _currentPerfCounterTimer = GET_CYCLE_COUNT();
    
    g_TotalTimeInGC = _currentPerfCounterTimer - g_TotalTimeInGC;
    PERF_COUNTER_TIMER_PRECISION _timeInGCBase = (_currentPerfCounterTimer - g_TotalTimeSinceLastGCEnd);

    _ASSERTE (_timeInGCBase >= g_TotalTimeInGC);
    while (_timeInGCBase > UINT_MAX) 
    {
        _timeInGCBase = _timeInGCBase >> 8;
        g_TotalTimeInGC = g_TotalTimeInGC >> 8;
        _ASSERTE (_timeInGCBase >= g_TotalTimeInGC);
    }

    // Update Total Time    
    GetGlobalPerfCounters().m_GC.timeInGC = (DWORD)g_TotalTimeInGC;
    GetPrivatePerfCounters().m_GC.timeInGC = (DWORD)g_TotalTimeInGC;

    GetGlobalPerfCounters().m_GC.timeInGCBase = (DWORD)_timeInGCBase;
    GetPrivatePerfCounters().m_GC.timeInGCBase = (DWORD)_timeInGCBase;
    
    g_TotalTimeSinceLastGCEnd = _currentPerfCounterTimer;

#endif //ENABLE_PERF_COUNTERS
}

void ProfScanRootsHelper(Object*& object, ScanContext *pSC, DWORD dwFlags)
{
#ifdef GC_PROFILING
    Object *pObj = object;
#ifdef INTERIOR_POINTERS
    if (dwFlags & GC_CALL_INTERIOR)
    {
        BYTE *o = (BYTE*)object;
        gc_heap* hp = gc_heap::heap_of (o
#ifdef _DEBUG
                                        , !(dwFlags & GC_CALL_INTERIOR)
#endif //_DEBUG
                                       );

        if ((o < hp->gc_low) || (o >= hp->gc_high))
        {
            return;
        }
        pObj = (Object*) hp->find_object(o, hp->gc_low);
    }
#endif //INTERIOR_POINTERS
    ScanRootsHelper(pObj, pSC, dwFlags);
#endif // GC_PROFILING
}

void GCProfileWalkHeap()
{
#if defined (GC_PROFILING)
    if (CORProfilerTrackGC())
    {
        // Indicate that inproc debugging is permitted for the duration of the heap walk
        g_profControlBlock.inprocState = ProfControlBlock::INPROC_PERMITTED;

        ProfilingScanContext SC;

#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
        int hn;

        // Must emulate each GC thread number so we can hit each
        // heap for enumerating the roots.
        for (hn = 0; hn < gc_heap::n_heaps; hn++)
        {
            // Ask the vm to go over all of the roots for this specific
            // heap.
            gc_heap* hp = gc_heap::g_heaps [hn];
            SC.thread_number = hn;
            CNameSpace::GcScanRoots(&ProfScanRootsHelper, max_generation, max_generation, &SC);

            // The finalizer queue is also a source of roots
            hp->finalize_queue->GcScanRoots(&ScanRootsHelper, hn, &SC);
        }
#else
        // Ask the vm to go over all of the roots
        CNameSpace::GcScanRoots(&ProfScanRootsHelper, max_generation, max_generation, &SC);

        // The finalizer queue is also a source of roots
        pGenGCHeap->finalize_queue->GcScanRoots(&ScanRootsHelper, 0, &SC);

#endif // (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)

        // Handles are kept independent of wks/svr/concurrent builds
        CNameSpace::GcScanHandlesForProfiler(max_generation, &SC);

        // Indicate that root scanning is over, so we can flush the buffered roots
        // to the profiler
        g_profControlBlock.pProfInterface->EndRootReferences(&SC.pHeapId);

#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
        // Walk the heap and provide the objref to the profiler
        for (hn = 0; hn < gc_heap::n_heaps; hn++)
        {
            gc_heap* hp = gc_heap::g_heaps [hn];
            hp->walk_heap (&HeapWalkHelper, 0, max_generation, hn == 0);
        }
#else
        gc_heap::walk_heap (&HeapWalkHelper, 0, max_generation, TRUE);
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS

        // Indicate that inproc debugging is no longer permitted
        g_profControlBlock.inprocState = ProfControlBlock::INPROC_FORBIDDEN;
    }
#endif //GC_PROFILING
}

void GCHeap::RestartEE(BOOL bFinishedGC, BOOL SuspendSucceded)
{
    if (g_fSuspendOnShutdown) {
        // We are shutting down.  The finalizer thread has suspended EE.
        // There will only be one thread running inside EE: either the shutdown
        // thread or the finalizer thread.

        g_profControlBlock.inprocState = ProfControlBlock::INPROC_PERMITTED;

        _ASSERTE (g_fEEShutDown);
        m_suspendReason = SUSPEND_FOR_SHUTDOWN;
        return;
    }

#ifdef TIME_CPAUSE
    printf ("Pause time: %d\n", GetCycleCount32() - cstart);
#endif //TIME_CPAUSE

    // SetGCDone();
    SyncClean::CleanUp();
    GcInProgress= FALSE;
    ThreadStore::TrapReturningThreads(FALSE);
    GcThread    = 0;
    SetEvent( WaitForGCEvent );
    _ASSERTE(ThreadStore::HoldingThreadStore());

    Thread::SysResumeFromGC(bFinishedGC, SuspendSucceded);
}

void GCHeap::SuspendEE(SUSPEND_REASON reason)
{        
#ifdef TIME_CPAUSE
    cstart = GetCycleCount32();
#endif //TIME_CPAUSE

    if (g_fSuspendOnShutdown) {
        // We are shutting down.  The finalizer thread has suspended EE.
        // There will only be one thread running inside EE: either the shutdown
        // thread or the finalizer thread.
        if (reason == GCHeap::SUSPEND_FOR_GC || reason == GCHeap::SUSPEND_FOR_GC_PREP)
            g_profControlBlock.inprocState = ProfControlBlock::INPROC_FORBIDDEN;

        _ASSERTE (g_fEEShutDown);
        m_suspendReason = reason;
        return;
    }

    LOG((LF_SYNC, INFO3, "Suspending the runtime for reason %d\n", reason));

    // lock the thread store which could take us out of our must
    // complete
    // Need the thread store lock here.  We take this lock before the thread
    // lock to avoid a deadlock condition where another thread suspends this
    // thread while it holds the heap lock.  While the thread store lock is
    // held, threads cannot be suspended.
    BOOL gcOnTransitions = GC_ON_TRANSITIONS(FALSE);        // dont do GC for GCStress 3
    Thread* pCurThread = GetThread();
    _ASSERTE(pCurThread==NULL || pCurThread->PreemptiveGCDisabled());

    // Note: we need to make sure to re-set m_GCThreadAttemptingSuspend when we retry
    // due to the debugger case below!
retry_for_debugger:
    
    // Set variable to indicate that this thread is preforming a true GC
    // This is needed to overcome deadlock in taking the ThreadStore lock
    if (reason == GCHeap::SUSPEND_FOR_GC || reason == GCHeap::SUSPEND_FOR_GC_PREP)
    {
        m_GCThreadAttemptingSuspend = pCurThread;

    }

    ThreadStore::LockThreadStore(reason);

    if (ThreadStore::s_hAbortEvt != NULL &&
        (reason == GCHeap::SUSPEND_FOR_GC || reason == GCHeap::SUSPEND_FOR_GC_PREP))
    {
        LOG((LF_SYNC, INFO3, "GC thread is backing out the suspend abort event.\n"));
        ThreadStore::s_hAbortEvt = NULL;

        LOG((LF_SYNC, INFO3, "GC thread is signalling the suspend abort event.\n"));
        SetEvent(ThreadStore::s_hAbortEvtCache);
    }

    // Set variable to indicate that this thread is attempting the suspend because it
    // needs to perform a GC and, as such, it holds GC locks.
    if (reason == GCHeap::SUSPEND_FOR_GC || reason == GCHeap::SUSPEND_FOR_GC_PREP)
    {
        m_GCThreadAttemptingSuspend = NULL;
    }

    {
        // suspend for GC, set in progress after suspending
        // threads which have no must complete
        ResetEvent( WaitForGCEvent );
        // SetGCInProgress();
        {
            GcThread = pCurThread;
            ThreadStore::TrapReturningThreads(TRUE);
            m_suspendReason = reason;

#ifdef PROFILING_SUPPORTED
            if (reason == GCHeap::SUSPEND_FOR_GC || reason == GCHeap::SUSPEND_FOR_GC_PREP)
                g_profControlBlock.inprocState = ProfControlBlock::INPROC_FORBIDDEN;
#endif // PROFILING_SUPPORTED

            GcInProgress= TRUE;
        }

        HRESULT hr;
        {
            _ASSERTE(ThreadStore::HoldingThreadStore() || g_fProcessDetach);
            hr = Thread::SysSuspendForGC(reason);
            ASSERT( hr == S_OK || hr == ERROR_TIMEOUT);
        }

        // If the debugging services are attached, then its possible
        // that there is a thread which appears to be stopped at a gc
        // safe point, but which really is not. If that is the case,
        // back off and try again.

        // If this is not the GC thread and another thread has triggered
        // a GC, then we may have bailed out of SysSuspendForGC, so we
        // must resume all of the threads and tell the GC that we are
        // at a safepoint - since this is the exact same behaviour
        // that the debugger needs, just use it's code.
        if ((hr == ERROR_TIMEOUT)
#ifdef DEBUGGING_SUPPORTED
             || (CORDebuggerAttached() && 
                 g_pDebugInterface->ThreadsAtUnsafePlaces())
#endif // DEBUGGING_SUPPORTED
            )
        {
            // In this case, the debugger has stopped at least one
            // thread at an unsafe place.  The debugger will usually
            // have already requested that we stop.  If not, it will 
            // either do so shortly -- or resume the thread that is
            // at the unsafe place.
            //
            // Either way, we have to wait for the debugger to decide
            // what it wants to do.
            //
            // Note: we've still got the gc_lock lock held.

            LOG((LF_GCROOTS | LF_GC | LF_CORDB,
                 LL_INFO10,
                 "***** Giving up on current GC suspension due "
                 "to debugger or timeout *****\n"));            

            if (ThreadStore::s_hAbortEvtCache == NULL)
            {
                LOG((LF_SYNC, INFO3, "Creating suspend abort event.\n"));
                ThreadStore::s_hAbortEvtCache = CreateEvent(NULL, TRUE, FALSE, NULL);
                if (!ThreadStore::s_hAbortEvtCache) 
                {
                    FailFast(GetThread(), FatalOutOfMemory);
                }
            }

            LOG((LF_SYNC, INFO3, "Using suspend abort event.\n"));
            ThreadStore::s_hAbortEvt = ThreadStore::s_hAbortEvtCache;
            ResetEvent(ThreadStore::s_hAbortEvt);
            
            // Mark that we're done with the gc, just like at the
            // end of this method.
            RestartEE(FALSE, FALSE);            
            
            LOG((LF_GCROOTS | LF_GC | LF_CORDB,
                 LL_INFO10, "The EE is free now...\n"));
            
            // Check if we're ready to go to suspend.
            if (pCurThread && pCurThread->CatchAtSafePoint())
            {
                _ASSERTE(pCurThread->PreemptiveGCDisabled());
                pCurThread->PulseGCMode();  // Go suspend myself.
            }
            else
            {
                __SwitchToThread (0); // Wait a little while, before retrying.
            }

            goto retry_for_debugger;
        }
    }
    GC_ON_TRANSITIONS(gcOnTransitions);
}

void CallFinalizer(Object* obj)
{

    MethodTable     *pMT = obj->GetMethodTable();
    STRESS_LOG2(LF_GC, LL_INFO1000, "Finalizing object %x MT %pT\n", obj, pMT);
    LOG((LF_GC, LL_INFO1000, "Finalizing object %s\n", pMT->GetClass()->m_szDebugClassName));

    _ASSERTE(GetThread()->PreemptiveGCDisabled());
    // if we don't have a class, we can't call the finalizer
    // if the object has been marked run as finalizer run don't call either
    if (pMT)
    {
        if (!((obj->GetHeader()->GetBits()) & BIT_SBLK_FINALIZER_RUN))
        {
            if (pMT->IsContextful())
            {
                Object *proxy = OBJECTREFToObject(CRemotingServices::GetProxyFromObject(ObjectToOBJECTREF(obj)));

                _ASSERTE(proxy && "finalizing an object that was never wrapped?????");                
                if (proxy == NULL)
                {
                    // Quite possibly the app abruptly shutdown while a proxy
                    // was being setup for a contextful object. We will skip
                    // finalizing this object.
                    _ASSERTE (g_fEEShutDown);
                    return;
                }
                else
                {
                    // This saves us from the situation where an object gets GC-ed 
                    // after its Context. 
                    Object* stub = (Object *)proxy->GetPtrOffset(CTPMethodTable::GetOffsetOfStubData());
                    Context *pServerCtx = (Context *) stub->UnBox();
                    // Check if the context is valid             
                    if (!Context::ValidateContext(pServerCtx))
                    {
                        // Since the server context is gone (GC-ed)
                        // we will associate the server with the default 
                        // context for a good faith attempt to run 
                        // the finalizer
                        // We want to do this only if we are using RemotingProxy
                        // and not for other types of proxies (eg. SvcCompPrxy)
                        OBJECTREF orRP = ObjectToOBJECTREF(CRemotingServices::GetRealProxy(OBJECTREFToObject(proxy)));
                        if(CTPMethodTable::IsInstanceOfRemotingProxy(
                            orRP->GetMethodTable()))
                        {
                            *((Context **)stub->UnBox()) = (Context*) GetThread()->GetContext();
                        }
                    }
                    // call Finalize on the proxy of the server object.
                    obj = proxy;
                }
            }
            _ASSERTE(pMT->HasFinalizer());
            MethodTable::CallFinalizer(obj);
        }
        else
        {
            //reset the bit so the object can be put on the list 
            //with RegisterForFinalization
            obj->GetHeader()->ClrBit (BIT_SBLK_FINALIZER_RUN);
        }
    }
}

#ifndef GOLDEN
static char s_FinalizeObjectName[MAX_CLASSNAME_LENGTH+MAX_NAMESPACE_LENGTH+2];
static BOOL s_fSaveFinalizeObjectName = FALSE;
#endif

void  CallFinalizer(Thread* FinalizerThread, 
                    Object* fobj)
{
#ifndef GOLDEN
    if (s_fSaveFinalizeObjectName) {
#if ZAPMONITOR_ENABLED
        INSTALL_COMPLUS_EXCEPTION_HANDLER();
#endif
        DefineFullyQualifiedNameForClass();
        LPCUTF8 name = GetFullyQualifiedNameForClass(fobj->GetClass());
        strcat (s_FinalizeObjectName, name);
#if ZAPMONITOR_ENABLED
        UNINSTALL_COMPLUS_EXCEPTION_HANDLER();
#endif
    }
#endif
    CallFinalizer(fobj);
#ifndef GOLDEN
    if (s_fSaveFinalizeObjectName) {
        s_FinalizeObjectName[0] = '\0';
    }
#endif
    // we might want to do some extra work on the finalizer thread
    // check and do it
    if (FinalizerThread->HaveExtraWorkForFinalizer())
    {
        FinalizerThread->DoExtraWorkForFinalizer();
    }

    // if someone is trying to stop us, open the gates
    FinalizerThread->PulseGCMode();
}

struct FinalizeAllObjects_Args {
    struct {
        Object* fobj;
        Object *retObj;
    } gcArgs;
    int bitToCheck;
};

static Object *FinalizeAllObjects(Object* fobj, int bitToCheck);

static void FinalizeAllObjects_Wrapper(FinalizeAllObjects_Args *args)
{
    _ASSERTE(args->gcArgs.fobj);
    args->gcArgs.retObj = FinalizeAllObjects(args->gcArgs.fobj, args->bitToCheck);
    // clear out the fobj as we no longer need it so don't want to pin it
    args->gcArgs.fobj = NULL;
}

static Object *FinalizeAllObjects(Object* fobj, int bitToCheck)
{
    if (fobj == NULL)
        fobj = GCHeap::GetNextFinalizableObject();

    // Finalize everyone
    while (fobj)
    {
        if (fobj->GetHeader()->GetBits() & bitToCheck)
        {
            fobj = GCHeap::GetNextFinalizableObject();
            continue;
        }

        COMPLUS_TRY 
        {
            Thread *pThread = GetThread();
            AppDomain* targetAppDomain = fobj->GetAppDomain();
            AppDomain* currentDomain = pThread->GetDomain();
            if (! targetAppDomain || ! targetAppDomain->CanThreadEnter(pThread))
            {
                // if can't get into domain to finalize it, then it must be agile so finalize in current domain
                targetAppDomain = currentDomain;
#if CHECK_APP_DOMAIN_LEAKS
                 // object must be agile if can't get into it's domain
                if (g_pConfig->AppDomainLeaks() && !fobj->SetAppDomainAgile(FALSE))   
                    _ASSERTE(!"Found non-agile GC object which should have been finalized during app domain unload.");
#endif
            }
            if (targetAppDomain == currentDomain)
            {
                CallFinalizer(fobj);
                fobj = GCHeap::GetNextFinalizableObject();
            } 
            else 
            {
                if (! targetAppDomain->GetDefaultContext())
                {
                    // can no longer enter domain becuase the handle containing the context has been
                    // nuked so just bail. Should only get this if are at the stage of nuking the
                    // handles in the domain if it's still open.
                    _ASSERTE(targetAppDomain->IsUnloading() && targetAppDomain->ShouldHaveRoots());
                    fobj = GCHeap::GetNextFinalizableObject();
                    continue;
                }
                if (currentDomain != SystemDomain::System()->DefaultDomain())
                {
                    // this means we are in some other domain, so need to return back out through the DoADCallback
                    // and handle the object from there in another domain.
                    return(fobj);
                } 
                else
                {
                    // otherwise call back to ourselves to process as many as we can in that other domain
                    FinalizeAllObjects_Args args = { {fobj, NULL}, bitToCheck};
                    Object *dummy = fobj;
                    GCPROTECT_BEGIN(args.gcArgs);
                    pThread->DoADCallBack(targetAppDomain->GetDefaultContext(), FinalizeAllObjects_Wrapper, &args);
                    // process the object we got back or be done if we got back null
                    fobj = args.gcArgs.retObj;
                    GCPROTECT_END();
#ifdef _DEBUG
                    // clear the dangerous objects table as don't care about anything earlier. If don't clear, then will
                    // get assert in next GCPROTECT because the GCPROTECT_END will have put the ref adddresses in the dangerous
                    // object table as unprotected and will flag the refs as invalid becuase a GC occured since now and
                    // next time we call GCPROTECT with the same address
                    Thread::ObjectRefFlush(pThread);
#endif
                }
            }
        }    
        COMPLUS_CATCH
        {
            // Should be an out of memory from Thread::EnterDomain.  Swallow,
            // no where to report this, and get the next object
            fobj = GCHeap::GetNextFinalizableObject();
        }
        COMPLUS_END_CATCH
    }
    return NULL;
}

void GCHeap::WaitUntilGCComplete()
{
    DWORD dwWaitResult = NOERROR;

    if (GcInProgress) {
        ASSERT( WaitForGCEvent );
        ASSERT( GetThread() != GcThread );

#ifdef DETECT_DEADLOCK
        // wait for GC to complete
BlockAgain:
        dwWaitResult = WaitForSingleObject( WaitForGCEvent,
                                            DETECT_DEADLOCK_TIMEOUT );

        if (dwWaitResult == WAIT_TIMEOUT) {
            //  Even in retail, stop in the debugger if available.  Ideally, the
            //  following would use DebugBreak, but debspew.h makes this a null
            //  macro in retail.  Note that in debug, we don't use the debspew.h
            //  macros because these take a critical section that may have been
            //  taken by a suspended thread.
            RetailDebugBreak();
            goto BlockAgain;
        }

#else  //DETECT_DEADLOCK

        
        if (g_fEEShutDown) {
            Thread *pThread = GetThread();
            if (pThread) {
                dwWaitResult = pThread->DoAppropriateAptStateWait(1, &WaitForGCEvent, FALSE, INFINITE, TRUE);
            } else {
                dwWaitResult = WaitForSingleObject( WaitForGCEvent, INFINITE );
            }

        } else {
            dwWaitResult = WaitForSingleObject( WaitForGCEvent, INFINITE );
        }
        
#endif //DETECT_DEADLOCK

    }
}

HANDLE MHandles[2];


void WaitForFinalizerEvent (HANDLE event)
{
    if (MHandles[0] && g_fEEStarted)
    {
        //give a chance to the finalizer event (2s)
        switch (WaitForSingleObject(event, 2000))
        {
        case (WAIT_OBJECT_0):
            return;
        case (WAIT_ABANDONED):
            return;
        case (WAIT_TIMEOUT):
            break;
        }
        while (1)
        {
            MHandles [1] = event;
            switch (WaitForMultipleObjects (2, MHandles, FALSE, INFINITE))
            {
            case (WAIT_OBJECT_0):
                dprintf (2, ("Async low memory notification"));
                //short on memory GC immediately
                g_pGCHeap->GetFinalizerThread()->DisablePreemptiveGC();
                g_pGCHeap->GarbageCollect(0);
                g_pGCHeap->GetFinalizerThread()->EnablePreemptiveGC();
                //wait only on the event for 2s 
                switch (WaitForSingleObject(event, 2000))
                {
                case (WAIT_OBJECT_0):
                    return;
                case (WAIT_ABANDONED):
                    return;
                case (WAIT_TIMEOUT):
                    break;
                }
                break;
            case (WAIT_OBJECT_0+1):
                return;
            default:
                //what's wrong?
                _ASSERTE (!"Bad return code from WaitForMultipleObjects");
                return;
            }
        }
    }
    else
        WaitForSingleObject(event, INFINITE);
}





ULONG GCHeap::FinalizerThreadStart(void *args)
{
    ASSERT(args == 0);
    ASSERT(GCHeap::hEventFinalizer);

    LOG((LF_GC, LL_INFO10, "Finalizer thread starting..."));

    AppDomain* defaultDomain = FinalizerThread->GetDomain();
    _ASSERTE(defaultDomain == SystemDomain::System()->DefaultDomain());

    BOOL    ok = FinalizerThread->HasStarted();

    _ASSERTE(ok);
    _ASSERTE(GetThread() == FinalizerThread);

    // finalizer should always park in default domain

    if (ok)
    {
        EE_TRY_FOR_FINALLY
        {
            FinalizerThread->SetBackground(TRUE);

            BOOL noUnloadedObjectsRegistered = FALSE;

            while (!fQuitFinalizer)
            {
                UINT nGen = 0;

                // Wait for work to do...

                _ASSERTE(FinalizerThread->PreemptiveGCDisabled());
#ifdef _DEBUG
                if (g_pConfig->FastGCStressLevel()) {
                    FinalizerThread->m_GCOnTransitionsOK = FALSE;
                }
#endif
                FinalizerThread->EnablePreemptiveGC();
#ifdef _DEBUG
                if (g_pConfig->FastGCStressLevel()) {
                    FinalizerThread->m_GCOnTransitionsOK = TRUE;
                }
#endif
#if 0
                // Setting the event here, instead of at the bottom of the loop, could
                // cause us to skip draining the Q, if the request is made as soon as
                // the app starts running.
                SetEvent(GCHeap::hEventFinalizerDone);
#endif //0
                WaitForFinalizerEvent (GCHeap::hEventFinalizer);
                FinalizerThread->DisablePreemptiveGC();

#ifdef _DEBUG
                    // TODO: HACK.  make finalization very lazy for gcstress 3 or 4.  
                    // only do finalization if the system is quiescent
                if (g_pConfig->GetGCStressLevel() > 1)
                {
                    int last_gc_count;
                    do {
                        last_gc_count = gc_count;
                        FinalizerThread->m_GCOnTransitionsOK = FALSE; 
                        FinalizerThread->EnablePreemptiveGC();
                        __SwitchToThread (0);
                        FinalizerThread->DisablePreemptiveGC();             
                            // If no GCs happended, then we assume we are quiescent
                        FinalizerThread->m_GCOnTransitionsOK = TRUE; 
                    } while (gc_count - last_gc_count > 0);
                }
#endif //_DEBUG
                
                // we might want to do some extra work on the finalizer thread
                // check and do it
                if (FinalizerThread->HaveExtraWorkForFinalizer())
                {
                    FinalizerThread->DoExtraWorkForFinalizer();
                }

                LOG((LF_GC, LL_INFO100, "***** Calling Finalizers\n"));
                FinalizeAllObjects(NULL, 0);
                _ASSERTE(FinalizerThread->GetDomain() == SystemDomain::System()->DefaultDomain());

#ifdef COLLECT_CLASSES
                // finalize all finalizable classes
                ClassListEntry  *cur = 0;

                for( cur = GCHeap::m_Finalize->GetNextFinalizableClassAndDeleteCurrent( cur );
                     cur;
                     cur = GCHeap::m_Finalize->GetNextFinalizableClassAndDeleteCurrent( cur ) )
                {
                    CallClassFinalizer( cur->GetClass() );

                    pTB->LeaveMC();
                    pTB->EnterMC();
                }

                GCHeap::m_Finalize->DeleteDeletableClasses();
#endif //COLLECT_CLASSES

                if (GCHeap::UnloadingAppDomain != NULL)
                {
                    // Now schedule any objects from an unloading app domain for finalization 
                    // on the next pass (even if they are reachable.)
                    // Note that it may take several passes to complete the unload, if new objects are created during
                    // finalization.

                    if (!FinalizeAppDomain(GCHeap::UnloadingAppDomain, GCHeap::fRunFinalizersOnUnload))
                    {
                        if (!noUnloadedObjectsRegistered)
                        {
                            //
                            // There is nothing left to schedule.  However, there are possibly still objects
                            // left in the finalization queue.  We might be done after the next pass, assuming
                            // we don't see any new finalizable objects in the domain.
                            noUnloadedObjectsRegistered = TRUE;
                        }
                        else
                        {
                            // We've had 2 passes seeing no objects - we're done.
                            GCHeap::UnloadingAppDomain = NULL;
                            noUnloadedObjectsRegistered = FALSE;
                        }
                    }
                    else
                        noUnloadedObjectsRegistered = FALSE;
                }

                // Anyone waiting to drain the Q can now wake up.  Note that there is a
                // race in that another thread starting a drain, as we leave a drain, may
                // consider itself satisfied by the drain that just completed.  This is
                // acceptable.
                SetEvent(GCHeap::hEventFinalizerDone);
            }
            
            // Tell shutdown thread we are done with finalizing dead objects.
            SetEvent (GCHeap::hEventFinalizerToShutDown);
            
            // Wait for shutdown thread to signal us.
            FinalizerThread->EnablePreemptiveGC();
            WaitForSingleObject(GCHeap::hEventShutDownToFinalizer, INFINITE);
            FinalizerThread->DisablePreemptiveGC();
            
            AppDomain::RaiseExitProcessEvent();

            SetEvent(GCHeap::hEventFinalizerToShutDown);
            
            // Phase 1 ends.
            // Now wait for Phase 2 signal.

            // Wait for shutdown thread to signal us.
            FinalizerThread->EnablePreemptiveGC();
            WaitForSingleObject(GCHeap::hEventShutDownToFinalizer, INFINITE);
            FinalizerThread->DisablePreemptiveGC();
            
            SetFinalizeQueueForShutdown (FALSE);
            
            // Finalize all registered objects during shutdown, even they are still reachable.
            // we have been asked to quit, so must be shutting down      
            _ASSERTE(g_fEEShutDown);
            _ASSERTE(FinalizerThread->PreemptiveGCDisabled());
            FinalizeAllObjects(NULL, BIT_SBLK_FINALIZER_RUN);
            _ASSERTE(FinalizerThread->GetDomain() == SystemDomain::System()->DefaultDomain());

            // we might want to do some extra work on the finalizer thread
            // check and do it
            if (FinalizerThread->HaveExtraWorkForFinalizer())
            {
                FinalizerThread->DoExtraWorkForFinalizer();
            }

            SetEvent(GCHeap::hEventFinalizerToShutDown);

            // Wait for shutdown thread to signal us.
            FinalizerThread->EnablePreemptiveGC();
            WaitForSingleObject(GCHeap::hEventShutDownToFinalizer, INFINITE);
            FinalizerThread->DisablePreemptiveGC();

            // Do extra cleanup for part 1 of shutdown.
            // If we hang here (bug 87809) shutdown thread will
            // timeout on us and will proceed normally
            CoEEShutDownCOM();

            SetEvent(GCHeap::hEventFinalizerToShutDown);
        }
        EE_FINALLY
        {
            if (GOT_EXCEPTION())
                _ASSERTE(!"Exception in the finalizer thread!");
        }
        EE_END_FINALLY;
    }
    // finalizer should always park in default domain
    _ASSERTE(GetThread()->GetDomain() == SystemDomain::System()->DefaultDomain());

    LOG((LF_GC, LL_INFO10, "Finalizer thread done."));

    // Enable pre-emptive GC before we leave so that anybody trying to suspend
    // us will not end up waiting forever. Don't do a DestroyThread because this
    // will happen soon when we tear down the thread store.
    FinalizerThread->EnablePreemptiveGC();

    // We do not want to tear Finalizer thread,
    // since doing so will cause OLE32 to CoUninitalize.
    ::Sleep(INFINITE);
    
    return 0;
}

DWORD GCHeap::FinalizerThreadCreate()
{
    DWORD   dwRet = 0;
    HANDLE  h;
    DWORD   newThreadId;

    hEventFinalizerDone = CreateEvent(0, TRUE, FALSE, 0);
    if (hEventFinalizerDone)
    {
        hEventFinalizer = CreateEvent(0, FALSE, FALSE, 0);
        hEventFinalizerToShutDown = CreateEvent(0, FALSE, FALSE, 0);
        hEventShutDownToFinalizer = CreateEvent(0, FALSE, FALSE, 0);
        if (hEventFinalizer && hEventFinalizerToShutDown && hEventShutDownToFinalizer)
        {
            _ASSERTE(FinalizerThread == 0);
            FinalizerThread = SetupUnstartedThread();
            if (FinalizerThread == 0) {
                return 0;
            }

            // We don't want the thread block disappearing under us -- even if the
            // actual thread terminates.
            FinalizerThread->IncExternalCount();

            h = FinalizerThread->CreateNewThread(4096, FinalizerThreadStart, 0, &newThreadId);
            if (h)
            {
                ::SetThreadPriority(h, THREAD_PRIORITY_HIGHEST);

                // Before we do the resume, we need to take note of the new ThreadId.  This
                // is necessary because -- before the thread starts executing at KickofThread --
                // it may perform some DllMain DLL_THREAD_ATTACH notifications.  These could
                // call into managed code.  During the consequent SetupThread, we need to
                // perform the Thread::HasStarted call instead of going through the normal
                // 'new thread' pathway.
                _ASSERTE(FinalizerThread->GetThreadId() == 0);
                _ASSERTE(newThreadId != 0);

                FinalizerThread->SetThreadId(newThreadId);

                dwRet = ::ResumeThread(h);
                _ASSERTE(dwRet == 1);
            }
        }
    }
    return dwRet;
}

// Wait for the finalizer thread to complete one pass.
void GCHeap::FinalizerThreadWait(int timeout)
{
    ASSERT(hEventFinalizerDone);
    ASSERT(hEventFinalizer);
    ASSERT(FinalizerThread);

    // Can't call this from within a finalized method.
    if (!IsCurrentThreadFinalizer())
    {
        // To help combat finalizer thread starvation, we check to see if there are any wrappers
        // scheduled to be cleaned up for our context.  If so, we'll do them here to avoid making
        // the finalizer thread do a transition.
        if (g_pRCWCleanupList != NULL)
            g_pRCWCleanupList->CleanUpCurrentWrappers();

        Thread  *pCurThread = GetThread();
        BOOL     toggleGC = pCurThread->PreemptiveGCDisabled();

        if (toggleGC)
            pCurThread->EnablePreemptiveGC();

        ::ResetEvent(hEventFinalizerDone);
        ::SetEvent(hEventFinalizer);

        //----------------------------------------------------
        // Do appropriate wait and pump messages if necessary
        //----------------------------------------------------
        //WaitForSingleObject(hEventFinalizerDone, INFINITE);

        pCurThread->DoAppropriateWait(1, &hEventFinalizerDone, FALSE, timeout, TRUE, NULL);

        if (toggleGC)
            pCurThread->DisablePreemptiveGC();
    }
}


#ifdef _DEBUG
#define FINALIZER_WAIT_TIMEOUT 250
#else
#define FINALIZER_WAIT_TIMEOUT 200
#endif
#define FINALIZER_TOTAL_WAIT 2000

static BOOL s_fRaiseExitProcessEvent = FALSE;
static DWORD dwBreakOnFinalizeTimeOut = -1;

BOOL GCHeap::FinalizerThreadWatchDog()
{
#if 0
#ifdef CONCURRENT_GC
    if (pGenGCHeap->concurrent_gc_p)
        pGenGCHeap->kill_gc_thread();
#endif //CONCURRENT_GC
#endif //0
    
    Thread *pThread = GetThread();
    HANDLE      h = FinalizerThread->GetThreadHandle();

    if (dwBreakOnFinalizeTimeOut == -1) {
        dwBreakOnFinalizeTimeOut = g_pConfig->GetConfigDWORD(L"BreakOnFinalizeTimeOut", 0);
    }

    // Do not wait for FinalizerThread if the current one is FinalizerThread.
    if (pThread == FinalizerThread)
        return TRUE;

    // If finalizer thread is gone, just return.
    if (h == INVALID_HANDLE_VALUE || WaitForSingleObject (h, 0) != WAIT_TIMEOUT)
        return TRUE;

    // *** This is the first call ShutDown -> Finalizer to Finilize dead objects ***
    if ((g_fEEShutDown & ShutDown_Finalize1) &&
        !(g_fEEShutDown & ShutDown_Finalize2)) {
        // Wait for the finalizer...
        LOG((LF_GC, LL_INFO10, "Signalling finalizer to quit..."));

        fQuitFinalizer = TRUE;
        ResetEvent(hEventFinalizerDone);
        SetEvent(hEventFinalizer);

        LOG((LF_GC, LL_INFO10, "Waiting for finalizer to quit..."));
        
        pThread->EnablePreemptiveGC();
        BOOL fTimeOut = FinalizerThreadWatchDogHelper();
        
        if (!fTimeOut) {
            SetEvent(hEventShutDownToFinalizer);

            // Wait for finalizer thread to finish raising ExitProcess Event.
            s_fRaiseExitProcessEvent = TRUE;
            fTimeOut = FinalizerThreadWatchDogHelper();
            if (fTimeOut) {
                s_fRaiseExitProcessEvent = FALSE;
            }
        }
        
        pThread->DisablePreemptiveGC();
        
        // Can not call ExitProcess here if we are in a hosting environment.
        // The host does not expect that we terminate the process.
        //if (fTimeOut)
        //{
            //::ExitProcess (GetLatchedExitCode());
        //}
        
        return !fTimeOut;
    }

    // *** This is the second call ShutDown -> Finalizer to ***
    // suspend the Runtime and Finilize live objects
    if ( g_fEEShutDown & ShutDown_Finalize2 &&
        !(g_fEEShutDown & ShutDown_COM) ) {

#ifdef CONCURRENT_GC
        // From this point on, we have made SuspendEE and ResumeEE no-op.
        // We need to turn off Concurrent GC to make the shutdown work.
        gc_heap::gc_can_use_concurrent = FALSE;

        if (pGenGCHeap->settings.concurrent)
           pGenGCHeap->gc_wait();
#endif //CONCURRENT_GC
        
        _ASSERTE (g_fEEShutDown & ShutDown_Finalize1);
        SuspendEE(GCHeap::SUSPEND_FOR_SHUTDOWN);
        g_fSuspendOnShutdown = TRUE;
        
        GcThread = FinalizerThread;
        // !!! We will not resume EE from now on.  But we are setting the finslizer thread
        // !!! to be the thread that SuspendEE, so that it will be blocked.
        // !!! Before we wake up Finalizer thread, we need to enable preemptive gc on the
        // !!! finalizer thread.  Otherwise we may see a deadlock during debug test.
        pThread->EnablePreemptiveGC();
        
        g_fFinalizerRunOnShutDown = TRUE;
        
        // Wait for finalizer thread to finish finalizing all objects.
        SetEvent(GCHeap::hEventShutDownToFinalizer);
        BOOL fTimeOut = FinalizerThreadWatchDogHelper();

        if (!fTimeOut) {
            // We only switch back GcThread if we do not timeout.
            // We check these to decide if we want to enter EE when processing DLL_PROCESS_DETACH.
            GcThread = pThread;
            g_fFinalizerRunOnShutDown = FALSE;
        }
        
        // Can not call ExitProcess here if we are in a hosting environment.
        // The host does not expect that we terminate the process.
        //if (fTimeOut) {
        //    ::ExitProcess (GetLatchedExitCode());
        //}

        pThread->DisablePreemptiveGC();
        return !fTimeOut;
    }

    // *** This is the third call ShutDown -> Finalizer ***
    // to do additional cleanup
    if (g_fEEShutDown & ShutDown_COM) {
        _ASSERTE (g_fEEShutDown & (ShutDown_Finalize2 | ShutDown_Finalize1));

        GcThread = FinalizerThread;
        pThread->EnablePreemptiveGC();
        g_fFinalizerRunOnShutDown = TRUE;
        
        SetEvent(GCHeap::hEventShutDownToFinalizer);
        DWORD status = pThread->DoAppropriateAptStateWait(1, &hEventFinalizerToShutDown,
                                                FALSE, FINALIZER_WAIT_TIMEOUT,
                                                TRUE);
        
        BOOL fTimeOut = (status == WAIT_TIMEOUT) ? TRUE : FALSE;

#ifndef GOLDEN
        if (fTimeOut) 
        {
            if (dwBreakOnFinalizeTimeOut) {
                LOG((LF_GC, LL_INFO10, "Finalizer took too long to clean up COM IP's.\n"));
                DebugBreak();
            }
        }
#endif // GOLDEN

        if (!fTimeOut) {
            GcThread = pThread;
            g_fFinalizerRunOnShutDown = FALSE;
        }
        pThread->DisablePreemptiveGC();

        return !fTimeOut;
    }

    _ASSERTE(!"Should never reach this point");
    return FALSE;
}

BOOL GCHeap::FinalizerThreadWatchDogHelper()
{
    Thread *pThread = GetThread();
    _ASSERTE (!pThread->PreemptiveGCDisabled());
    
    DWORD dwBeginTickCount = GetTickCount();
    
    size_t prevNumFinalizableObjects = GetNumberFinalizableObjects();
    size_t curNumFinalizableObjects;
    BOOL fTimeOut = FALSE;
    DWORD nTry = 0;
    DWORD maxTry = (DWORD)(FINALIZER_TOTAL_WAIT*1.0/FINALIZER_WAIT_TIMEOUT + 0.5);
    DWORD maxTotalWait = (s_fRaiseExitProcessEvent?3000:40000);
    BOOL bAlertable = TRUE; //(g_fEEShutDown & ShutDown_Finalize2) ? FALSE:TRUE;

    if (dwBreakOnFinalizeTimeOut == -1) {
        dwBreakOnFinalizeTimeOut = g_pConfig->GetConfigDWORD(L"BreakOnFinalizeTimeOut", 0);
    }

    DWORD dwTimeout = FINALIZER_WAIT_TIMEOUT;

    // This used to set the dwTimeout to infinite, but this can cause a hang when shutting down
    // if a finalizer tries to take a lock that another suspended managed thread already has.
    // This results in the hang because the other managed thread is never going to be resumed
    // because we're in shutdown.  So we make a compromise here - make the timeout for every
    // iteration 10 times longer and make the total wait infinite - so if things hang we will
    // eventually shutdown but we also give things a chance to finish if they're running slower
    // because of the profiler.
    if (CORProfilerPresent())
    {
        dwTimeout *= 10;
        maxTotalWait = INFINITE;
    }

    while (1) {
        DWORD status = 0;
        COMPLUS_TRY
        {
            status = pThread->DoAppropriateAptStateWait(1,&hEventFinalizerToShutDown,FALSE, dwTimeout, bAlertable);
        }
        COMPLUS_CATCH
        {
            status = WAIT_TIMEOUT;
        }
        COMPLUS_END_CATCH

        if (status != WAIT_TIMEOUT) {
            break;
        }
        nTry ++;
        curNumFinalizableObjects = GetNumberFinalizableObjects();
        if ((prevNumFinalizableObjects <= curNumFinalizableObjects || s_fRaiseExitProcessEvent)
#ifdef _DEBUG
            && gc_heap::gc_lock.lock == -1
#else
            && gc_heap::gc_lock == -1
#endif
            && !(pThread->m_State & (Thread::TS_UserSuspendPending | Thread::TS_DebugSuspendPending))){
            if (nTry == maxTry) {
                if (!s_fRaiseExitProcessEvent) {
                LOG((LF_GC, LL_INFO10, "Finalizer took too long on one object.\n"));
                }
                else
                    LOG((LF_GC, LL_INFO10, "Finalizer took too long to process ExitProcess event.\n"));

                fTimeOut = TRUE;
                if (dwBreakOnFinalizeTimeOut != 2) {
                    break;
                }
            }
        }
        else
        {
            nTry = 0;
            prevNumFinalizableObjects = curNumFinalizableObjects;
        }
        DWORD dwCurTickCount = GetTickCount();
        if (pThread->m_State & (Thread::TS_UserSuspendPending | Thread::TS_DebugSuspendPending)) {
            dwBeginTickCount = dwCurTickCount;
        }
        if (dwCurTickCount - dwBeginTickCount >= maxTotalWait
            || (dwBeginTickCount > dwCurTickCount && dwBeginTickCount - dwCurTickCount <= (~0) - maxTotalWait)) {
            LOG((LF_GC, LL_INFO10, "Finalizer took too long on shutdown.\n"));
            fTimeOut = TRUE;
            if (dwBreakOnFinalizeTimeOut != 2) {
                break;
            }
        }
    }

#ifndef GOLDEN
    if (fTimeOut) 
    {
        if (dwBreakOnFinalizeTimeOut){
            DebugBreak();
        }
        if (!s_fRaiseExitProcessEvent && s_FinalizeObjectName[0] != '\0') {
            LOG((LF_GC, LL_INFO10, "Currently running finalizer on object of %s\n", 
                 s_FinalizeObjectName));
        }
    }
#endif
    return fTimeOut;
}


void gc_heap::user_thread_wait (HANDLE event)
{
    Thread* pCurThread = GetThread();
    BOOL mode = pCurThread->PreemptiveGCDisabled();
    if (mode)
    {
        pCurThread->EnablePreemptiveGC();
    }

    WaitForSingleObject(event, INFINITE);

    if (mode)
    {
        pCurThread->DisablePreemptiveGC();
    }
}


#ifdef CONCURRENT_GC
// Wait for gc to finish
void gc_heap::gc_wait()
{
    dprintf(2, ("Waiting end of concurrent gc"));
    assert (gc_done_event);
    assert (gc_start_event);

    Thread *pCurThread = GetThread();
    BOOL mode = pCurThread->PreemptiveGCDisabled();
    if (mode)
    {
        pCurThread->EnablePreemptiveGC();
    }

    //ResetEvent(gc_done_event);
    WaitForSingleObject(gc_done_event, INFINITE);

    if (mode)
    {
        pCurThread->DisablePreemptiveGC();
    }
        dprintf(2, ("Waiting end of concurrent gc is done"));
}

// Wait for gc to finish marking part of mark_phase
void gc_heap::gc_wait_lh()
{
    Thread *pCurThread = GetThread();
    BOOL mode = pCurThread->PreemptiveGCDisabled();
    if (mode)
    {
        pCurThread->EnablePreemptiveGC();
    }

    //ResetEvent(gc_done_event);
    WaitForSingleObject(gc_lh_block_event, INFINITE);

    if (mode)
    {
        pCurThread->DisablePreemptiveGC();
    }
        dprintf(2, ("Waiting end of concurrent large sweep is done"));

}

#endif //CONCURRENT_GC

#ifdef _DEBUG

// Normally, any thread we operate on has a Thread block in its TLS.  But there are
// a few special threads we don't normally execute managed code on.
//
// There is a scenario where we run managed code on such a thread, which is when the
// DLL_THREAD_ATTACH notification of an (IJW?) module calls into managed code.  This
// is incredibly dangerous.  If a GC is provoked, the system may have trouble performing
// the GC because its threads aren't available yet.  This is survivable in the
// concurrent case (we perform the GC synchronously).  This is catastrophic in the
// server GC case.
static DWORD SpecialEEThreads[64];
static LONG  cnt_SpecialEEThreads = 64;
static CRITICAL_SECTION SpecialEEThreadsLock;
static CRITICAL_SECTION *pSpecialEEThreadsLock = NULL;
static LONG EEThreadsLockInitialized = 0;

inline void dbgOnly_EnsureInit()
{
    if (pSpecialEEThreadsLock == NULL)
    {   
        if (InterlockedCompareExchange(&EEThreadsLockInitialized, 1, 0) == 0)
        {
            // first one to get in does the initialization
            ZeroMemory(SpecialEEThreads,sizeof(SpecialEEThreads));
            InitializeCriticalSection(&SpecialEEThreadsLock);
            pSpecialEEThreadsLock = &SpecialEEThreadsLock;
        }
        else 
        {
            while (pSpecialEEThreadsLock == NULL)
                ::SwitchToThread();
        }
    }
}

void dbgOnly_IdentifySpecialEEThread()
{
    dbgOnly_EnsureInit();
    EnterCriticalSection(pSpecialEEThreadsLock);

    DWORD   ourId = ::GetCurrentThreadId();
    for (LONG i=0; i<cnt_SpecialEEThreads; i++)
    {
        if (0 == SpecialEEThreads[i])
        {
            SpecialEEThreads[i] = ourId;
            LeaveCriticalSection(pSpecialEEThreadsLock);
            return;
        }
    }      
    _ASSERTE(!"SpecialEEThreads array is too small");
    LeaveCriticalSection(pSpecialEEThreadsLock);
}

void dbgOnly_RemoveSpecialEEThread()
{
    dbgOnly_EnsureInit();
    EnterCriticalSection(pSpecialEEThreadsLock);
    DWORD   ourId = ::GetCurrentThreadId();
    for (LONG i=0; i<cnt_SpecialEEThreads; i++)
    {
        if (ourId == SpecialEEThreads[i])
        {
            SpecialEEThreads[i] = 0;
            LeaveCriticalSection(pSpecialEEThreadsLock);
            return;
        }
    }        
    _ASSERTE(!"Failed to find our thread ID");
    LeaveCriticalSection(pSpecialEEThreadsLock);
}

BOOL dbgOnly_IsSpecialEEThread()
{
    dbgOnly_EnsureInit();
    DWORD   ourId = ::GetCurrentThreadId();

    for (LONG i=0; i<cnt_SpecialEEThreads; i++)
        if (ourId == SpecialEEThreads[i])
            return TRUE;

    return FALSE;
}

#endif // _DEBUG
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gcportpriv.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
// optimize for speed
#ifndef _DEBUG
#pragma optimize( "t", on )
#endif
#define inline __forceinline


#include "wsperf.h"
#include "PerfCounters.h"
#include "gmheap.hpp"
#include "log.h"
#include "eeconfig.h"
#include "gc.h"


#ifdef GC_PROFILING
#include "profilepriv.h"
#endif

#ifdef _IA64_
#define RetailDebugBreak()  DebugBreak()
#elif defined(_X86_)

#ifdef _DEBUG
inline void RetailDebugBreak()   {_asm int 3}
#else
inline void RetailDebugBreak()   FATAL_EE_ERROR()
#endif

#else // _X86_
#define RetailDebugBreak()    
#endif

#pragma inline_depth(20)
/* the following section defines the optional features */


#define INTERIOR_POINTERS   //Allow interior pointers in the code manager

#define FREE_LIST_0         //Generation 0 can allocate from free list

#define FFIND_OBJECT        //faster find_object, slower allocation
#define FFIND_DECAY  7      //Number of GC for which fast find will be active

//#define STRESS_PINNING    //Stress pinning by pinning randomly

//#define DUMP_OBJECTS      //Dump objects from the heap

//#define TRACE_GC          //debug trace gc operation

// Make verify heap available for free build, but not for retail build.

//#define CATCH_GC          //catches exception during GC

//#define TIME_GC           //time allocation and garbage collection
//#define TIME_WRITE_WATCH  //time GetWriteWatch and ResetWriteWatch calls
//#define COUNT_CYCLES  //Use cycle counter for timing
//#define TIME_CPAUSE     //Use cycle counter to time pauses.

//#define DEBUG_CONCURRENT 

/* End of optional features */

#ifdef _DEBUG
#define TRACE_GC
#endif

#define NUMBERGENERATIONS   5               //Max number of generations

//Please leave these definitions intact.

#ifdef CreateFileMapping

#undef CreateFileMapping

#endif //CreateFileMapping

#define CreateFileMapping WszCreateFileMapping

#ifdef CreateSemaphore

#undef CreateSemaphore

#endif //CreateSemaphore

#define CreateSemaphore WszCreateSemaphore

#ifdef CreateEvent

#undef CreateEvent

#endif //ifdef CreateEvent

#define CreateEvent WszCreateEvent

#ifdef memcpy
#undef memcpy
#endif //memcpy


#define THREAD_NUMBER_DCL
#define THREAD_NUMBER_ARG 
#define THREAD_NUMBER_FROM_CONTEXT
#define THREAD_FROM_HEAP 
#define HEAP_FROM_THREAD  gc_heap* hpt = 0;

#ifdef TRACE_GC


extern int     print_level;
extern int     gc_count;
extern BOOL    trace_gc;


class hlet 
{
    static hlet* bindings;
    int prev_val;
    int* pval;
    hlet* prev_let;
public:
    hlet (int& place, int value)
    {
        prev_val = place;
        pval = &place;
        place = value;
        prev_let = bindings;
        bindings = this;
    }
    ~hlet ()
    {
        *pval = prev_val;
        bindings = prev_let;
    }
};


#define let(p,v) hlet __x = hlet (p, v);

#else //TRACE_GC

#define gc_count    -1
#define let(s,v)

#endif //TRACE_GC

#ifdef TRACE_GC
//#include "log.h"
//#define dprintf(l,x) {if (trace_gc &&(l<=print_level)) {LogSpewAlways x;LogSpewAlways ("\n");}}
#define dprintf(l,x) {if (trace_gc && (l<=print_level)) {printf ("\n");printf x ; fflush(stdout);}}
#else //TRACE_GC
#define dprintf(l,x)
#endif //TRACE_GC


#undef  assert
#define assert _ASSERTE
#undef  ASSERT
#define ASSERT _ASSERTE


#ifdef _DEBUG

struct GCDebugSpinLock {
    long    lock;           // -1 if free, 0 if held
    Thread* holding_thread; // -1 if no thread holds the lock.
};
typedef GCDebugSpinLock GCSpinLock;
#define SPIN_LOCK_INITIALIZER {-1, (Thread*) -1}

#else

typedef long GCSpinLock;
#define SPIN_LOCK_INITIALIZER -1

#endif

HANDLE MHandles[];

class mark;
class heap_segment;
class CObjectHeader;
class large_object_block;
class segment_manager;
class l_heap;
class sorted_table;
class f_page_list;
class page_manager;
class c_synchronize;

class generation
{
public:
    // Don't move these first two fields without adjusting the references
    // from the __asm in jitinterface.cpp.
    alloc_context   allocation_context;
    heap_segment*   allocation_segment;
    BYTE*           free_list;
    heap_segment*   start_segment;
    BYTE*           allocation_start;
    BYTE*           plan_allocation_start;
    BYTE*           last_gap;
    size_t          free_list_space;
    size_t          allocation_size;

};

class dynamic_data
{
public:
    ptrdiff_t new_allocation;
    ptrdiff_t gc_new_allocation; // new allocation at beginning of gc
    ptrdiff_t c_new_allocation;  // keep track of allocation during c_gc
    size_t    current_size;
    size_t    previous_size;
    size_t    desired_allocation;
    size_t    collection_count;
    size_t    promoted_size;
    size_t    fragmentation;    //fragmentation when we don't compact
    size_t    min_gc_size;
    size_t    max_size;
    size_t    min_size;
    size_t    default_new_allocation;
    size_t    fragmentation_limit;
    float           fragmentation_burden_limit;
    float           limit;
    float           max_limit;
};


//class definition of the internal class
class gc_heap
{
   friend GCHeap;
   friend CFinalize;
   friend void ProfScanRootsHelper(Object*& object, ScanContext *pSC, DWORD dwFlags);
   friend void GCProfileWalkHeap();
#ifdef DUMP_OBJECTS
friend void print_all();
#endif // DUMP_OBJECTS


    typedef void (* card_fn) (BYTE**);
#define call_fn(fn) (*fn)
#define __this (gc_heap*)0

public:
    PER_HEAP
    void verify_heap();

    static
    heap_segment* make_heap_segment (BYTE* new_pages, size_t size);
    static 
    l_heap* make_large_heap (BYTE* new_pages, size_t size, BOOL managed);
    
    static 
    gc_heap* make_gc_heap(
);
    
    static 
    void destroy_gc_heap(gc_heap* heap);

    static 
    HRESULT initialize_gc  (size_t segment_size,
                            size_t heap_size
);

    static
    void shutdown_gc();

    PER_HEAP
    CObjectHeader* allocate (size_t jsize,
                             alloc_context* acontext
        );

    CObjectHeader* try_fast_alloc (size_t jsize);

    PER_HEAP
    CObjectHeader* allocate_large_object (size_t size, BOOL pointerp, alloc_context* acontext);

    PER_HEAP
    int garbage_collect (int n
                        );
    static 
    int grow_brick_card_tables (BYTE* start, BYTE* end);
    
    static 
        DWORD __stdcall gc_thread_stub (void* arg);

    PER_HEAP
    BOOL is_marked (BYTE* o);
    
protected:

    static void user_thread_wait (HANDLE event);


#if defined (GC_PROFILING) || defined (DUMP_OBJECTS)

    PER_HEAP
    void walk_heap (walk_fn fn, void* context, int gen_number, BOOL walk_large_object_heap_p);

    PER_HEAP
    void walk_relocation (int condemned_gen_number,
                                   BYTE* first_condemned_address, void *pHeapId);

    PER_HEAP
    void walk_relocation_in_brick (BYTE* tree,  BYTE*& last_plug, size_t& last_plug_relocation, void *pHeapId);

#endif //GC_PROFILING || DUMP_OBJECTS


    PER_HEAP
    int generation_to_condemn (int n, BOOL& not_enough_memory);

    PER_HEAP
    void gc1();

    PER_HEAP
    size_t limit_from_size (size_t size, size_t room);
    PER_HEAP
    BOOL allocate_more_space (alloc_context* acontext, size_t jsize);

    PER_HEAP_ISOLATED
    int init_semi_shared();
    PER_HEAP
    int init_gc_heap ();
    PER_HEAP
    void self_destroy();
    PER_HEAP_ISOLATED
    void destroy_semi_shared();
    PER_HEAP
    void fix_youngest_allocation_area (BOOL for_gc_p);
    PER_HEAP
    void fix_allocation_context (alloc_context* acontext, BOOL for_gc_p);
    PER_HEAP
    void fix_older_allocation_area (generation* older_gen);
    PER_HEAP
    void set_allocation_heap_segment (generation* gen);
    PER_HEAP
    void reset_allocation_pointers (generation* gen, BYTE* start);
    PER_HEAP
    unsigned int object_gennum (BYTE* o);
    PER_HEAP
    void delete_heap_segment (heap_segment* seg);
    PER_HEAP_ISOLATED
    void delete_large_heap (l_heap* hp);
    PER_HEAP
    void reset_heap_segment_pages (heap_segment* seg);
    PER_HEAP
    void decommit_heap_segment_pages (heap_segment* seg);
    PER_HEAP
    void rearrange_heap_segments();
    PER_HEAP
    void reset_write_watch ();
    PER_HEAP
    void adjust_ephemeral_limits ();
    PER_HEAP
    void make_generation (generation& gen, heap_segment* seg,
                          BYTE* start, BYTE* pointer);
    PER_HEAP_ISOLATED
    int heap_grow_hook (BYTE* mem, size_t memsize, ptrdiff_t delta);

    PER_HEAP_ISOLATED
    int heap_pregrow_hook (size_t memsize);

    PER_HEAP
    BOOL size_fit_p (size_t size, BYTE* alloc_pointer, BYTE* alloc_limit);
    PER_HEAP
    BOOL a_size_fit_p (size_t size, BYTE* alloc_pointer, BYTE* alloc_limit);
    PER_HEAP
    size_t card_of ( BYTE* object);
    PER_HEAP
    BYTE* brick_address (size_t brick);
    PER_HEAP
    size_t brick_of (BYTE* add);
    PER_HEAP
    BYTE* card_address (size_t card);
    PER_HEAP
    size_t card_to_brick (size_t card);
    PER_HEAP
    void clear_card (size_t card);
    PER_HEAP
    void set_card (size_t card);
    PER_HEAP
    BOOL  card_set_p (size_t card);
    PER_HEAP
    void card_table_set_bit (BYTE* location);
    PER_HEAP
    int grow_heap_segment (heap_segment* seg, size_t size);
    PER_HEAP
    void copy_brick_card_range (BYTE* la, DWORD* old_card_table,
                                short* old_brick_table,
                                heap_segment* seg,
                                BYTE* start, BYTE* end, BOOL heap_expand);
    PER_HEAP
    void copy_brick_card_table_l_heap ();
    PER_HEAP
    void copy_brick_card_table(BOOL heap_expand);
    PER_HEAP
    void clear_brick_table (BYTE* from, BYTE* end);
    PER_HEAP
    void set_brick (size_t index, ptrdiff_t val);

    PER_HEAP
    void adjust_limit (BYTE* start, size_t limit_size, generation* gen, 
                       int gen_number);
    PER_HEAP
    void adjust_limit_clr (BYTE* start, size_t limit_size, 
                           alloc_context* acontext, heap_segment* seg);
    PER_HEAP
    BYTE* allocate_in_older_generation (generation* gen, size_t size, 
                                        int from_gen_number);
    PER_HEAP
    generation*  ensure_ephemeral_heap_segment (generation* consing_gen);
    PER_HEAP
    BYTE* allocate_in_condemned_generations (generation* gen,
                                             size_t size,
                                             int from_gen_number);
#if defined (INTERIOR_POINTERS) || defined (_DEBUG)
    PER_HEAP
    heap_segment* find_segment (BYTE* interior);
    PER_HEAP
    BYTE* find_object_for_relocation (BYTE* o, BYTE* low, BYTE* high);
#endif //INTERIOR_POINTERS

    PER_HEAP_ISOLATED
    gc_heap* heap_of (BYTE* object, BOOL verify_p =
#ifdef _DEBUG
                      TRUE
#else
                      FALSE
#endif //_DEBUG
);

    PER_HEAP
    BYTE* find_object (BYTE* o, BYTE* low);

    PER_HEAP
    dynamic_data* dynamic_data_of (int gen_number);
    PER_HEAP
    ptrdiff_t  get_new_allocation (int gen_number);
    PER_HEAP
    void ensure_new_allocation (int size);
    PER_HEAP
    size_t deque_pinned_plug ();
    PER_HEAP
    mark* pinned_plug_of (size_t bos);
    PER_HEAP
    mark* oldest_pin ();
    PER_HEAP
    mark* before_oldest_pin();
    PER_HEAP
    BOOL pinned_plug_que_empty_p ();
    PER_HEAP
    void make_mark_stack (mark* arr);
    PER_HEAP
    generation* generation_of (int  n);
    PER_HEAP
    BOOL gc_mark1 (BYTE* o);
    PER_HEAP
    BOOL gc_mark (BYTE* o, BYTE* low, BYTE* high);
    PER_HEAP
    BYTE* mark_object(BYTE* o THREAD_NUMBER_DCL);
    PER_HEAP
    BYTE* mark_object_class (BYTE* o THREAD_NUMBER_DCL);
    PER_HEAP
    void mark_object_simple (BYTE** o THREAD_NUMBER_DCL);
    PER_HEAP
    void mark_object_simple1 (BYTE* o THREAD_NUMBER_DCL);
    PER_HEAP
    BYTE* next_end (heap_segment* seg, BYTE* f);
    PER_HEAP
    void fix_card_table ();
    PER_HEAP
    void verify_card_table ();
    PER_HEAP
    BYTE* mark_object_internal (BYTE* o THREAD_NUMBER_DCL);
    PER_HEAP
    void mark_object_internal1 (BYTE* o THREAD_NUMBER_DCL);
    PER_HEAP
    void mark_through_object (BYTE* oo THREAD_NUMBER_DCL);
    PER_HEAP
    BOOL process_mark_overflow (int condemned_gen_number);
    PER_HEAP
    void mark_phase (int condemned_gen_number, BOOL mark_only_p);

    PER_HEAP
    void pin_object (BYTE* o, BYTE* low, BYTE* high);
    PER_HEAP
    void reset_mark_stack ();
    PER_HEAP
    BYTE* insert_node (BYTE* new_node, size_t sequence_number,
                       BYTE* tree, BYTE* last_node);
    PER_HEAP
    size_t update_brick_table (BYTE* tree, size_t current_brick,
                               BYTE* x, BYTE* plug_end);
    PER_HEAP
    void set_allocator_next_pin (generation* gen);
    PER_HEAP
    void enque_pinned_plug (generation* gen,
                            BYTE* plug, size_t len);

    PER_HEAP
    void plan_generation_start (generation*& consing_gen);
    PER_HEAP
    void process_ephemeral_boundaries(BYTE* x, int& active_new_gen_number,
                                      int& active_old_gen_number,
                                      generation*& consing_gen,
                                      BOOL& allocate_in_condemned,
                                      BYTE*& free_gap, BYTE* zero_limit=0);
    PER_HEAP
    void plan_phase (int condemned_gen_number);
    PER_HEAP
    void fix_generation_bounds (int condemned_gen_number, 
                                generation* consing_gen, 
                                BOOL demoting);
    PER_HEAP
    BYTE* generation_limit (int gen_number);

    struct make_free_args
    {
        int free_list_gen_number;
        BYTE* current_gen_limit;
        generation* free_list_gen;
        BYTE* highest_plug; 
        BYTE* free_top;
    };
    PER_HEAP
    BYTE* allocate_at_end (size_t size);
    PER_HEAP
    void make_free_lists (int condemned_gen_number
                         );
    PER_HEAP
    void make_free_list_in_brick (BYTE* tree, make_free_args* args);
    PER_HEAP
    void thread_gap (BYTE* gap_start, size_t size, generation*  gen);
    PER_HEAP
    void make_unused_array (BYTE* x, size_t size);
    PER_HEAP
    void relocate_address (BYTE** old_address THREAD_NUMBER_DCL);

    struct relocate_args
    {
        BYTE* last_plug;
        BYTE* low;
        BYTE* high;
        BYTE* demoted_low;
        BYTE* demoted_high;
    };

    PER_HEAP
    void reloc_survivor_helper (relocate_args* args, BYTE** pval);

    PER_HEAP
    void relocate_survivors_in_plug (BYTE* plug, BYTE* plug_end, 
                                     relocate_args* args);
    PER_HEAP
    void relocate_survivors_in_brick (BYTE* tree, 
                                       relocate_args* args);
    PER_HEAP
    void relocate_survivors (int condemned_gen_number,
                             BYTE* first_condemned_address );
    PER_HEAP
    void relocate_phase (int condemned_gen_number,
                         BYTE* first_condemned_address);
    
    struct compact_args
    {
        BOOL copy_cards_p;
        BYTE* last_plug;
        size_t last_plug_relocation;
        BYTE* before_last_plug;
        size_t current_compacted_brick;
    };

    PER_HEAP
    void  gcmemcopy (BYTE* dest, BYTE* src, size_t len, BOOL copy_cards_p);
    PER_HEAP
    void compact_plug (BYTE* plug, size_t size, compact_args* args);
    PER_HEAP
    void compact_in_brick (BYTE* tree, compact_args* args);

    PER_HEAP
    void compact_phase (int condemned_gen_number, BYTE* 
                        first_condemned_address, BOOL clear_cards);
    PER_HEAP
    void clear_cards (size_t start_card, size_t end_card);
    PER_HEAP
    void clear_card_for_addresses (BYTE* start_address, BYTE* end_address);
    PER_HEAP
    void copy_cards (size_t dst_card, size_t src_card,
                     size_t end_card, BOOL nextp);
    PER_HEAP
    void copy_cards_for_addresses (BYTE* dest, BYTE* src, size_t len);
    PER_HEAP
    BOOL ephemeral_pointer_p (BYTE* o);
    PER_HEAP
    void fix_brick_to_highest (BYTE* o, BYTE* next_o);
    PER_HEAP
    BYTE* find_first_object (BYTE* start,  size_t brick, BYTE* min_address);
    PER_HEAP
    BYTE* compute_next_boundary (BYTE* low, int gen_number, BOOL relocating);
    PER_HEAP
    void mark_through_cards_helper (BYTE** poo, unsigned int& ngen, 
                                    unsigned int& cg_pointers_found, 
                                    card_fn fn, BYTE* nhigh, 
                                    BYTE* next_boundary);
    PER_HEAP
    void mark_through_cards_for_segments (card_fn fn, BOOL relocating);
    PER_HEAP
    void realloc_plug (size_t last_plug_size, BYTE*& last_plug,
                       generation* gen, BYTE* start_address, 
                       unsigned int& active_new_gen_number,
                       BYTE*& last_pinned_gap, BOOL& leftp, size_t page);
    PER_HEAP
    void realloc_in_brick (BYTE* tree, BYTE*& last_plug, BYTE* start_address,
                           generation* gen,
                           unsigned int& active_new_gen_number,
                           BYTE*& last_pinned_gap, BOOL& leftp, size_t page);
    PER_HEAP
    void realloc_plugs (generation* consing_gen, heap_segment* seg,
                        BYTE* start_address, BYTE* end_address,
                        unsigned active_new_gen_number);


    PER_HEAP
    generation* expand_heap (int condemned_generation,
                             generation* consing_gen, 
                             heap_segment* new_heap_segment);
    PER_HEAP
    void init_dynamic_data ();
    PER_HEAP
    float surv_to_growth (float cst, float limit, float max_limit);
    PER_HEAP
    size_t desired_new_allocation (dynamic_data* dd, size_t in, size_t out, 
                                   float& cst, int gen_number);
    PER_HEAP
    size_t generation_size (int gen_number);
    PER_HEAP
    size_t  compute_promoted_allocation (int gen_number);
    PER_HEAP
    void compute_new_dynamic_data (int gen_number);
    PER_HEAP
    size_t new_allocation_limit (size_t size, size_t free_size);
    PER_HEAP
    size_t generation_fragmentation (generation* gen,
                                     generation* consing_gen, 
                                     BYTE* end);
    PER_HEAP
    size_t generation_sizes (generation* gen);
    PER_HEAP
    BOOL decide_on_compacting (int condemned_gen_number,
                               generation* consing_gen,
                               size_t fragmentation, 
                               BOOL& should_expand);
    PER_HEAP
    BOOL ephemeral_gen_fit_p (BOOL compacting=FALSE);
    PER_HEAP
    void RemoveBlock (large_object_block* item, BOOL pointerp);
    PER_HEAP
    void InsertBlock (large_object_block** after, large_object_block* item,
                      BOOL pointerp);
    PER_HEAP
    void insert_large_pblock (large_object_block* bl);
    PER_HEAP
    void reset_large_object (BYTE* o);
    PER_HEAP
    void sweep_large_objects ();
    PER_HEAP
    void relocate_in_large_objects ();
    PER_HEAP
    void mark_through_cards_for_large_objects (card_fn fn, BOOL relocating);
    PER_HEAP
    void descr_segment (heap_segment* seg);
    PER_HEAP
    void descr_card_table ();
    PER_HEAP
    void descr_generations ();

    /* ------------------- per heap members --------------------------*/ 
public:

    PER_HEAP
    DWORD* card_table;

    PER_HEAP
    short* brick_table;


    PER_HEAP_ISOLATED
    BOOL demotion;  //Some demotion is happening

    PER_HEAP
    BYTE* demotion_low;

    PER_HEAP
    BYTE* demotion_high;


#define concurrent_gc_p 0


    PER_HEAP
    BYTE* lowest_address;

    PER_HEAP
    BYTE* highest_address;

protected:
    #define vm_heap ((GCHeap*)0)
    #define heap_number (0)
    PER_HEAP
    heap_segment* ephemeral_heap_segment;

    PER_HEAP
    int         condemned_generation_num;

    PER_HEAP
    BYTE*       gc_low; // lowest address being condemned

    PER_HEAP
    BYTE*       gc_high; //highest address being condemned

    PER_HEAP
    size_t      mark_stack_tos;

    PER_HEAP
    size_t      mark_stack_bos;

    PER_HEAP
    size_t    mark_stack_array_length;

    PER_HEAP
    mark*       mark_stack_array;


#ifdef MARK_LIST
    PER_HEAP
    BYTE** mark_list;

    PER_HEAP_ISOLATED
    size_t mark_list_size;

    PER_HEAP
    BYTE** mark_list_end;

    PER_HEAP
    BYTE** mark_list_index;

    PER_HEAP_ISOLATED
    BYTE** g_mark_list;
#endif //MARK_LIST

    PER_HEAP
    BYTE*  min_overflow_address;

    PER_HEAP
    BYTE*  max_overflow_address;

    PER_HEAP
    BYTE*  shigh; //keeps track of the highest marked object

    PER_HEAP
    BYTE*  slow; //keeps track of the lowest marked object

    PER_HEAP
    int   allocation_quantum;

    PER_HEAP
    int   alloc_contexts_used;

#define youngest_generation (generation_of (0))

    PER_HEAP
    BYTE* alloc_allocated; //keeps track of the highest 
                                //address allocated by alloc

    // The more_space_lock is used for 3 purposes:
    //
    // 1) to coordinate threads that exceed their quantum (UP & MP)
    // 2) to synchronize allocations of large objects
    // 3) to synchronize the GC itself
    //
    // As such, it has 3 clients:
    //
    // 1) Threads that want to extend their quantum.  This always takes the lock 
    //    and sometimes provokes a GC.
    // 2) Threads that want to perform a large allocation.  This always takes 
    //    the lock and sometimes provokes a GC.
    // 3) GarbageCollect takes the lock and then unconditionally provokes a GC by 
    //    calling GarbageCollectGeneration.
    //
    PER_HEAP_ISOLATED
    GCSpinLock more_space_lock; //lock while allocating more space


    PER_HEAP
    dynamic_data dynamic_data_table [NUMBERGENERATIONS+1];


    //Large object support

    PER_HEAP_ISOLATED
    l_heap* lheap;


    PER_HEAP_ISOLATED
    DWORD* lheap_card_table;

    PER_HEAP_ISOLATED
    gmallocHeap* gheap;

    PER_HEAP_ISOLATED
    large_object_block* large_p_objects;

    PER_HEAP_ISOLATED
    large_object_block** last_large_p_object;

    PER_HEAP_ISOLATED
    large_object_block* large_np_objects;

    PER_HEAP_ISOLATED
    size_t large_objects_size;

    PER_HEAP_ISOLATED
    size_t large_blocks_size;

    PER_HEAP
    int generation_skip_ratio;//in %

    PER_HEAP
    BOOL gen0_bricks_cleared;
#ifdef FFIND_OBJECT
    PER_HEAP
    int gen0_must_clear_bricks;
#endif //FFIND_OBJECT


    PER_HEAP
    CFinalize* finalize_queue;

    /* ----------------------- global members ----------------------- */
public:

    static
    segment_manager* seg_manager;

    static 
    int g_max_generation;
    


    static 
    size_t reserved_memory;
    static
    size_t reserved_memory_limit;

}; // class gc_heap


class CFinalize
{
private:

    Object** m_Array;
    Object** m_FillPointers[NUMBERGENERATIONS+2];
    Object** m_EndArray;
    int m_PromotedCount;
    long lock;


    BOOL GrowArray();
    void MoveItem (Object** fromIndex,
                   unsigned int fromSeg,
                   unsigned int toSeg);

    BOOL IsSegEmpty ( unsigned int i)
    {
        ASSERT ( i <= NUMBERGENERATIONS+1);
        return ((i==0) ?
                (m_FillPointers[0] == m_Array):
                (m_FillPointers[i] == m_FillPointers[i-1]));
    }

public:
    CFinalize ();
    ~CFinalize();
    void EnterFinalizeLock();
    void LeaveFinalizeLock();
    void RegisterForFinalization (int gen, Object* obj);
    Object* GetNextFinalizableObject ();
    BOOL ScanForFinalization (int gen, int passnum, BOOL mark_only_p,
                              gc_heap* hp);
    void RelocateFinalizationData (int gen, gc_heap* hp);
    void GcScanRoots (promote_func* fn, int hn, ScanContext *pSC);
    void UpdatePromotedGenerations (int gen, BOOL gen_0_empty_p);
    int  GetPromotedCount();

    //Methods used by the shutdown code to call every finalizer
    void SetSegForShutDown(BOOL fHasLock);
    size_t GetNumberFinalizableObjects();
    
    //Methods used by the app domain unloading call to finalize objects in an app domain
    BOOL FinalizeAppDomain (AppDomain *pDomain, BOOL fRunFinalizers);

    void CheckFinalizerObjects();
};

inline
 size_t& dd_current_size (dynamic_data* inst)
{
  return inst->current_size;
}
inline
size_t& dd_previous_size (dynamic_data* inst)
{
  return inst->previous_size;
}
inline
size_t& dd_desired_allocation (dynamic_data* inst)
{
  return inst->desired_allocation;
}
inline
size_t& dd_collection_count (dynamic_data* inst)
{
    return inst->collection_count;
}
inline
size_t& dd_promoted_size (dynamic_data* inst)
{
    return inst->promoted_size;
}
inline
float& dd_limit (dynamic_data* inst)
{
  return inst->limit;
}
inline
float& dd_max_limit (dynamic_data* inst)
{
  return inst->max_limit;
}
inline
size_t& dd_min_gc_size (dynamic_data* inst)
{
  return inst->min_gc_size;
}
inline
size_t& dd_max_size (dynamic_data* inst)
{
  return inst->max_size;
}
inline
size_t& dd_min_size (dynamic_data* inst)
{
  return inst->min_size;
}
inline
ptrdiff_t& dd_new_allocation (dynamic_data* inst)
{
  return inst->new_allocation;
}
inline
ptrdiff_t& dd_gc_new_allocation (dynamic_data* inst)
{
  return inst->gc_new_allocation;
}
inline
ptrdiff_t& dd_c_new_allocation (dynamic_data* inst)
{
  return inst->c_new_allocation;
}
inline
size_t& dd_default_new_allocation (dynamic_data* inst)
{
  return inst->default_new_allocation;
}
inline
size_t& dd_fragmentation_limit (dynamic_data* inst)
{
  return inst->fragmentation_limit;
}
inline
float& dd_fragmentation_burden_limit (dynamic_data* inst)
{
  return inst->fragmentation_burden_limit;
}

inline
size_t& dd_fragmentation (dynamic_data* inst)
{
  return inst->fragmentation;
}

#define max_generation          gc_heap::g_max_generation

inline 
alloc_context* generation_alloc_context (generation* inst)
{
    return &(inst->allocation_context);
}

inline
BYTE*& generation_allocation_start (generation* inst)
{
  return inst->allocation_start;
}
inline
BYTE*& generation_allocation_pointer (generation* inst)
{
  return inst->allocation_context.alloc_ptr;
}
inline
BYTE*& generation_allocation_limit (generation* inst)
{
  return inst->allocation_context.alloc_limit;
}
inline
BYTE*& generation_free_list (generation* inst)
{
  return inst->free_list;
}
inline
heap_segment*& generation_start_segment (generation* inst)
{
  return inst->start_segment;
}
inline
heap_segment*& generation_allocation_segment (generation* inst)
{
  return inst->allocation_segment;
}
inline
BYTE*& generation_plan_allocation_start (generation* inst)
{
  return inst->plan_allocation_start;
}
inline
BYTE*& generation_last_gap (generation* inst)
{
  return inst->last_gap;
}
inline
size_t& generation_free_list_space (generation* inst)
{
  return inst->free_list_space;
}
inline
size_t& generation_allocation_size (generation* inst)
{
  return inst->allocation_size;
}

#define plug_skew           sizeof(DWORD)   // syncblock size. 
#define min_obj_size        (sizeof(BYTE*)+plug_skew+sizeof(size_t))//syncblock + vtable+ first field
#define min_free_list       (sizeof(BYTE*)+min_obj_size) //Need one slot more
//Note that this encodes the fact that plug_skew is a multiple of BYTE*.
struct plug
{
    BYTE *  skew[sizeof(plug_skew) / sizeof(BYTE *)];
};


//need to be careful to keep enough pad items to fit a relocation node 
//padded to QuadWord before the plug_skew
class heap_segment
{
public:
    BYTE*           allocated;
    BYTE*           committed;
    BYTE*           reserved;
    BYTE*           used;
    BYTE*           mem;
    heap_segment*   next;
    BYTE*           plan_allocated;


    BYTE*           pad0;
#if (SIZEOF_OBJHEADER % 8) != 0
    BYTE            pad1[8 - (SIZEOF_OBJHEADER % 8)];   // Must pad to quad word
#endif
    plug            plug;
};

inline
BYTE*& heap_segment_reserved (heap_segment* inst)
{
  return inst->reserved;
}
inline
BYTE*& heap_segment_committed (heap_segment* inst)
{
  return inst->committed;
}
inline
BYTE*& heap_segment_used (heap_segment* inst)
{
  return inst->used;
}
inline
BYTE*& heap_segment_allocated (heap_segment* inst)
{
  return inst->allocated;
}
inline
heap_segment*& heap_segment_next (heap_segment* inst)
{
  return inst->next;
}
inline
BYTE*& heap_segment_mem (heap_segment* inst)
{
  return inst->mem;
}
inline
BYTE*& heap_segment_plan_allocated (heap_segment* inst)
{
  return inst->plan_allocated;
}



extern "C" {
extern generation   generation_table [NUMBERGENERATIONS];
}


inline
generation* gc_heap::generation_of (int  n)
{
    assert (((n <= max_generation) && (n >= 0)));
    return &generation_table [ n ];
}


inline
dynamic_data* gc_heap::dynamic_data_of (int gen_number)
{
    return &dynamic_data_table [ gen_number ];
}

//This is a hack to avoid changing gcee.cpp for now. 
#if defined (CONCURRENT_GC)
#undef CONCURRENT_GC
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gcpriv.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
// optimize for speed
#ifndef _DEBUG
#pragma optimize( "t", on )
#endif
#define inline __forceinline


#include "wsperf.h"
#include "PerfCounters.h"
#include "gmheap.hpp"
#include "log.h"
#include "eeconfig.h"
#include "gc.h"
#include <member-offset-info.h>


#ifdef GC_PROFILING
#include "profilepriv.h"
#endif

#ifdef _IA64_
#define RetailDebugBreak()  DebugBreak()
#elif defined(_X86_)

#ifdef _DEBUG
inline void RetailDebugBreak()   {_asm int 3}
#else
inline void RetailDebugBreak()   FATAL_EE_ERROR()
#endif

#else // _X86_
#define RetailDebugBreak()    
#endif

#pragma inline_depth(20)
/* the following section defines the optional features */

//BUGBUG to get us started on multiple heaps. 

#define MARK_LIST         //used sorted list to speed up plan phase


#ifndef SERVER_GC

#define CONCURRENT_GC
#define MARK_ARRAY      //Mark bit in an array
#define WRITE_WATCH     //Write Watch feature

//#define CONCURRENT_COMPACT  //Compact concurrently
//#define MAP_VIEW          //Use MapViewOfFile instead of VirtualAlloc
//#define ALIAS_MEM         //Use Win9x VXD instead of MapViewOfFile

#endif // !SERVER_GC

//#define MULTIPLE_HEAPS         //Allow multiple heaps for servers
//#define ISOLATED_HEAPS    //Heaps are totally independent. 
//#define INCREMENTAL_MEMCLR  //objects are cleared on allocation

#define INTERIOR_POINTERS   //Allow interior pointers in the code manager

//#define SHORT_PLUGS           //keep plug short

#define FREE_LIST_0         //Generation 0 can allocate from free list

#define FFIND_OBJECT        //faster find_object, slower allocation
#define FFIND_DECAY  7      //Number of GC for which fast find will be active

//#define COLLECT_CLASSES   //Collect classes.

//#define NO_WRITE_BARRIER  //no write barrier, use Write Watch feature

#ifdef _IA64_
//#ifndef SERVER_GC
#define NO_WRITE_BARRIER  //no write barrier, use Write Watch feature
// @TODO: Implement JIT_WriteBarrier for IA64
//#endif
#undef WRITE_WATCH
#define WRITE_WATCH
#endif


//#define DEBUG_WRITE_WATCH //Additional debug for write watch

//#define STRESS_PINNING    //Stress pinning by pinning randomly

//#define DUMP_OBJECTS      //Dump objects from the heap

//#define TRACE_GC          //debug trace gc operation

//#define CATCH_GC          //catches exception during GC

//#define TIME_GC           //time allocation and garbage collection
//#define TIME_WRITE_WATCH  //time GetWriteWatch and ResetWriteWatch calls
//#define COUNT_CYCLES  //Use cycle counter for timing
//#define TIME_CPAUSE     //Use cycle counter to time pauses.

//#define DEBUG_CONCURRENT 

/* End of optional features */

#ifdef _DEBUG
#define TRACE_GC
#endif

#define NUMBERGENERATIONS   5               //Max number of generations

//Please leave these definitions intact.

#ifdef CreateFileMapping

#undef CreateFileMapping

#endif //CreateFileMapping

#define CreateFileMapping WszCreateFileMapping

#ifdef CreateSemaphore

#undef CreateSemaphore

#endif //CreateSemaphore

#define CreateSemaphore WszCreateSemaphore

#ifdef CreateEvent

#undef CreateEvent

#endif //ifdef CreateEvent

#define CreateEvent WszCreateEvent

#ifdef memcpy
#undef memcpy
#endif //memcpy


#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
#define THREAD_NUMBER_DCL ,int thread
#define THREAD_NUMBER_ARG ,thread
#define THREAD_NUMBER_FROM_CONTEXT int thread = sc->thread_number;
#define THREAD_FROM_HEAP  int thread = heap_number;
#define HEAP_FROM_THREAD  gc_heap* hpt = gc_heap::g_heaps[thread];
//These constants are ordered
const int policy_sweep = 0;
const int policy_compact = 1;
const int policy_expand  = 2;

#else
#define THREAD_NUMBER_DCL
#define THREAD_NUMBER_ARG 
#define THREAD_NUMBER_FROM_CONTEXT
#define THREAD_FROM_HEAP 
#define HEAP_FROM_THREAD  gc_heap* hpt = 0;
#endif //MULTIPLE_HEAPS &&!ISOLATED_HEAPS

#ifdef TRACE_GC


extern int     print_level;
extern int     gc_count;
extern BOOL    trace_gc;


class hlet 
{
    static hlet* bindings;
    int prev_val;
    int* pval;
    hlet* prev_let;
public:
    hlet (int& place, int value)
    {
        prev_val = place;
        pval = &place;
        place = value;
        prev_let = bindings;
        bindings = this;
    }
    ~hlet ()
    {
        *pval = prev_val;
        bindings = prev_let;
    }
};


#define let(p,v) hlet __x = hlet (p, v);

#else //TRACE_GC

#define gc_count    -1
#define let(s,v)

#endif //TRACE_GC

#ifdef TRACE_GC
//#include "log.h"
//#define dprintf(l,x) {if (trace_gc &&(l<=print_level)) {LogSpewAlways x;LogSpewAlways ("\n");}}
#define dprintf(l,x) {if (trace_gc && (l<=print_level)) {printf ("\n");printf x ; fflush(stdout);}}
#else //TRACE_GC
#define dprintf(l,x)
#endif //TRACE_GC


#undef  assert
#define assert _ASSERTE
#undef  ASSERT
#define ASSERT _ASSERTE


#ifdef _DEBUG

struct GCDebugSpinLock {
    volatile LONG lock;     // -1 if free, 0 if held
    Thread* holding_thread; // -1 if no thread holds the lock.
};
typedef GCDebugSpinLock GCSpinLock;
#define SPIN_LOCK_INITIALIZER {-1, (Thread*) -1}

#else

typedef long GCSpinLock;
#define SPIN_LOCK_INITIALIZER -1

#endif

HANDLE MHandles[];

class mark;
class heap_segment;
class CObjectHeader;
class segment_manager;
class l_heap;
class sorted_table;
class f_page_list;
class page_manager;
class c_synchronize;


//encapsulates the mechanism for the current gc
class gc_mechanisms
{
public:
    int condemned_generation;
    BOOL promotion;
    BOOL compaction;
    BOOL heap_expansion;
    DWORD concurrent;
    BOOL concurrent_compaction; //concurrent marking, stopping compaction
    BOOL demotion;
    int  gen0_reduction_count;
    void init_mechanisms();
};

class generation
{
    friend struct MEMBER_OFFSET_INFO(generation);
public:
    // Don't move these first two fields without adjusting the references
    // from the __asm in jitinterface.cpp.
    alloc_context   allocation_context;
    heap_segment*   allocation_segment;
    BYTE*           free_list;
    heap_segment*   start_segment;
    BYTE*           allocation_start;
    BYTE*           plan_allocation_start;
    BYTE*           last_gap;
    size_t          free_list_space;
    size_t          allocation_size;

};

class dynamic_data
{
public:
    ptrdiff_t new_allocation;
    ptrdiff_t gc_new_allocation; // new allocation at beginning of gc
    ptrdiff_t c_new_allocation;  // keep track of allocation during c_gc
    size_t    current_size;
    size_t    previous_size;
    size_t    desired_allocation;
    size_t    collection_count;
    size_t    promoted_size;
    size_t    fragmentation;    //fragmentation when we don't compact
    size_t    min_gc_size;
    size_t    max_size;
    size_t    min_size;
    size_t    default_new_allocation;
    size_t	  freach_previous_promotion;
    size_t    fragmentation_limit;
    float           fragmentation_burden_limit;
    float           limit;
    float           max_limit;
};


//class definition of the internal class
class gc_heap
{
#ifndef NOVM
   friend GCHeap;
   friend CFinalize;
   friend void ProfScanRootsHelper(Object*& object, ScanContext *pSC, DWORD dwFlags);
   friend void GCProfileWalkHeap();
#endif
#ifdef DUMP_OBJECTS
friend void print_all();
#endif // DUMP_OBJECTS

   friend struct MEMBER_OFFSET_INFO(gc_heap);

#ifdef WRITE_BARRIER_CHECK
friend void checkGCWriteBarrier();
friend void initGCShadow();
#endif 

#ifdef MULTIPLE_HEAPS
    typedef void (gc_heap::* card_fn) (BYTE**, int);
#define call_fn(fn) (this->*fn)
#define __this this
#else
    typedef void (* card_fn) (BYTE**);
#define call_fn(fn) (*fn)
#define __this (gc_heap*)0
#endif

public:
    PER_HEAP
    void verify_heap();

    static
    heap_segment* make_heap_segment (BYTE* new_pages, size_t size);
    static 
    l_heap* make_large_heap (BYTE* new_pages, size_t size, BOOL managed);
    
    static 
    gc_heap* make_gc_heap(
#if !defined (NOVM) && defined (MULTIPLE_HEAPS)
                          GCHeap* vm_heap, 
                          int heap_number
#endif //MULTIPLE_HEAPS
);
    
    static 
    void destroy_gc_heap(gc_heap* heap);

    static 
    HRESULT initialize_gc  (size_t segment_size,
                            size_t heap_size
#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
                            , unsigned number_of_heaps
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS
);

    static
    void shutdown_gc();

    PER_HEAP
    CObjectHeader* allocate (size_t jsize,
                             alloc_context* acontext
#ifdef NOVM
                             , BOOL pointerp = FALSE
#endif
        );

#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
    static void gc_heap::balance_heaps (alloc_context* acontext);
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS

    CObjectHeader* try_fast_alloc (size_t jsize);

    PER_HEAP
    CObjectHeader* allocate_large_object (size_t size, BOOL pointerp);

    PER_HEAP
    int garbage_collect (int n
#ifdef CONCURRENT_GC
                         , BOOL concurrent_p
#endif //CONCURRENT_GC
                        );
    static 
    int grow_brick_card_tables (BYTE* start, BYTE* end);
    
    static 
        DWORD __stdcall gc_thread_stub (void* arg);

    PER_HEAP
    BOOL is_marked (BYTE* o);
    
protected:

    static void user_thread_wait (HANDLE event);


#if defined (GC_PROFILING) || defined (DUMP_OBJECTS)

    PER_HEAP
    void walk_heap (walk_fn fn, void* context, int gen_number, BOOL walk_large_object_heap_p);

    PER_HEAP
    void walk_relocation (int condemned_gen_number,
                                   BYTE* first_condemned_address, void *pHeapId);

    PER_HEAP
    void walk_relocation_in_brick (BYTE* tree,  BYTE*& last_plug, size_t& last_plug_relocation, void *pHeapId);

#endif //GC_PROFILING || DUMP_OBJECTS


    PER_HEAP
    int generation_to_condemn (int n, BOOL& not_enough_memory);

    PER_HEAP
    void gc1();

    PER_HEAP
    size_t limit_from_size (size_t size, size_t room, int gen_number, 
                            int align_const);
    PER_HEAP
    BOOL allocate_more_space (alloc_context* acontext, size_t jsize, 
                              int alloc_generation_number);

    PER_HEAP_ISOLATED
    int init_semi_shared();
    PER_HEAP
    int init_gc_heap (int heap_number);
    PER_HEAP
    void self_destroy();
    PER_HEAP_ISOLATED
    void destroy_semi_shared();
    PER_HEAP
    void fix_youngest_allocation_area (BOOL for_gc_p);
    PER_HEAP
    void fix_allocation_context (alloc_context* acontext, BOOL for_gc_p, 
                                 int align_const);
    PER_HEAP
    void fix_large_allocation_area (BOOL for_gc_p);
    PER_HEAP
    void fix_older_allocation_area (generation* older_gen);
    PER_HEAP
    void set_allocation_heap_segment (generation* gen);
    PER_HEAP
    void reset_allocation_pointers (generation* gen, BYTE* start);
    PER_HEAP
    unsigned int object_gennum (BYTE* o);
    PER_HEAP
    void delete_heap_segment (heap_segment* seg);
    PER_HEAP
    heap_segment* 
    get_large_segment (size_t size);
    PER_HEAP
    void reset_heap_segment_pages (heap_segment* seg);
    PER_HEAP
    void decommit_heap_segment_pages (heap_segment* seg, size_t extra_space);
    PER_HEAP
    void rearrange_heap_segments();
    PER_HEAP
    void reset_write_watch ();
    PER_HEAP
    void adjust_ephemeral_limits ();
    PER_HEAP
    void make_generation (generation& gen, heap_segment* seg,
                          BYTE* start, BYTE* pointer);


    PER_HEAP
    BOOL size_fit_p (size_t size, BYTE* alloc_pointer, BYTE* alloc_limit);
    PER_HEAP
    BOOL a_size_fit_p (size_t size, BYTE* alloc_pointer, BYTE* alloc_limit, 
                       int align_const);
    PER_HEAP
    size_t card_of ( BYTE* object);
    PER_HEAP
    BYTE* brick_address (size_t brick);
    PER_HEAP
    size_t brick_of (BYTE* add);
    PER_HEAP
    BYTE* card_address (size_t card);
    PER_HEAP
    size_t card_to_brick (size_t card);
    PER_HEAP
    void clear_card (size_t card);
    PER_HEAP
    void set_card (size_t card);
    PER_HEAP
    BOOL  card_set_p (size_t card);
    PER_HEAP
    void card_table_set_bit (BYTE* location);
    PER_HEAP
    int grow_heap_segment (heap_segment* seg, BYTE* high_address);
    PER_HEAP
    void copy_brick_card_range (BYTE* la, DWORD* old_card_table,
#ifdef CONCURRENT_COMPACT
                                BYTE** old_page_table,
#endif //CONCURRENT_COMPACT
                                short* old_brick_table,
                                heap_segment* seg,
                                BYTE* start, BYTE* end, BOOL heap_expand);
    PER_HEAP
    void copy_brick_card_table_l_heap ();
    PER_HEAP
    void copy_brick_card_table(BOOL heap_expand);
    PER_HEAP
    void clear_brick_table (BYTE* from, BYTE* end);
    PER_HEAP
    void set_brick (size_t index, ptrdiff_t val);
#ifdef MARK_ARRAY
    PER_HEAP_ISOLATED
    size_t mark_word_of (BYTE* add);
    PER_HEAP_ISOLATED
    BYTE* gc_heap::mark_word_address (size_t wd);
    PER_HEAP_ISOLATED
    size_t mark_bit_of (BYTE* add);
    PER_HEAP_ISOLATED
    unsigned int mark_array_marked (BYTE* add);
    PER_HEAP_ISOLATED
    void mark_array_set_marked (BYTE* add);
    PER_HEAP_ISOLATED
    void mark_array_clear_marked (BYTE* add, int& index);
    PER_HEAP_ISOLATED
    void clear_mark_array (BYTE* from, BYTE* end);
#endif //MARK_ARRAY

    PER_HEAP
    BOOL large_object_marked (BYTE* o, BOOL clearp, int pin_finger);

    PER_HEAP
    void adjust_limit (BYTE* start, size_t limit_size, generation* gen, 
                       int gen_number);
    PER_HEAP
    void adjust_limit_clr (BYTE* start, size_t limit_size, 
                           alloc_context* acontext, heap_segment* seg, 
                           int align_const);
    PER_HEAP
    BYTE* allocate_in_older_generation (generation* gen, size_t size, 
                                        int from_gen_number);
    PER_HEAP
    generation*  ensure_ephemeral_heap_segment (generation* consing_gen);
    PER_HEAP
    BYTE* allocate_in_condemned_generations (generation* gen,
                                             size_t size,
                                             int from_gen_number);
#if defined (INTERIOR_POINTERS) || defined (_DEBUG)
    PER_HEAP
    heap_segment* find_segment (BYTE* interior, BOOL small_segment_only_p);
    PER_HEAP
    BYTE* find_object_for_relocation (BYTE* o, BYTE* low, BYTE* high);
#endif //INTERIOR_POINTERS

    PER_HEAP_ISOLATED
    gc_heap* heap_of (BYTE* object, BOOL verify_p =
#ifdef _DEBUG
                      TRUE
#else
                      FALSE
#endif //_DEBUG
);


    PER_HEAP_ISOLATED
    size_t&  promoted_bytes (int);

    PER_HEAP
    BYTE* find_object (BYTE* o, BYTE* low);

    PER_HEAP
    dynamic_data* dynamic_data_of (int gen_number);
    PER_HEAP
    ptrdiff_t  get_new_allocation (int gen_number);
    PER_HEAP
    void ensure_new_allocation (int size);
    PER_HEAP
    size_t deque_pinned_plug ();
    PER_HEAP
    mark* pinned_plug_of (size_t bos);
    PER_HEAP
    mark* oldest_pin ();
    PER_HEAP
    mark* before_oldest_pin();
    PER_HEAP
    BOOL pinned_plug_que_empty_p ();
    PER_HEAP
    void make_mark_stack (mark* arr);
#ifdef MARK_ARRAY
    PER_HEAP
    void make_pin_list (BYTE** arr);
    PER_HEAP
    void make_c_mark_list (BYTE** arr);
#endif //MARK_ARRAY
    PER_HEAP
    generation* generation_of (int  n);
    PER_HEAP
    BOOL gc_mark1 (BYTE* o);
    PER_HEAP
    BOOL gc_mark (BYTE* o, BYTE* low, BYTE* high);
    PER_HEAP
    BYTE* mark_object(BYTE* o THREAD_NUMBER_DCL);
    PER_HEAP
    BYTE* mark_object_class (BYTE* o THREAD_NUMBER_DCL);
    PER_HEAP
    void mark_object_simple (BYTE** o THREAD_NUMBER_DCL);
    PER_HEAP
    void mark_object_simple1 (BYTE* o THREAD_NUMBER_DCL);
    PER_HEAP
    BYTE* next_end (heap_segment* seg, BYTE* f);
    PER_HEAP
    void fix_card_table ();
    PER_HEAP
    void verify_card_table ();
    PER_HEAP
    BYTE* mark_object_internal (BYTE* o THREAD_NUMBER_DCL);
    PER_HEAP
    void mark_object_internal1 (BYTE* o THREAD_NUMBER_DCL);
    PER_HEAP
    void mark_through_object (BYTE* oo THREAD_NUMBER_DCL);
    PER_HEAP
    BOOL process_mark_overflow (int condemned_gen_number);
    PER_HEAP
    void mark_phase (int condemned_gen_number, BOOL mark_only_p);

    PER_HEAP
    void pin_object (BYTE* o, BYTE* low, BYTE* high);
    PER_HEAP
    void reset_mark_stack ();
    PER_HEAP
    BYTE* insert_node (BYTE* new_node, size_t sequence_number,
                       BYTE* tree, BYTE* last_node);
    PER_HEAP
    size_t update_brick_table (BYTE* tree, size_t current_brick,
                               BYTE* x, BYTE* plug_end);
    PER_HEAP
    void set_allocator_next_pin (generation* gen);
    PER_HEAP
    void enque_pinned_plug (generation* gen,
                            BYTE* plug, size_t len);

    PER_HEAP
    void plan_generation_start (generation*& consing_gen);
    PER_HEAP
    void process_ephemeral_boundaries(BYTE* x, int& active_new_gen_number,
                                      int& active_old_gen_number,
                                      generation*& consing_gen,
                                      BOOL& allocate_in_condemned,
                                      BYTE* zero_limit=0);
    PER_HEAP
    void plan_phase (int condemned_gen_number);
    PER_HEAP
    void fix_generation_bounds (int condemned_gen_number, 
                                generation* consing_gen, 
                                BOOL demoting);
    PER_HEAP
    BYTE* generation_limit (int gen_number);

    struct make_free_args
    {
        int free_list_gen_number;
        BYTE* current_gen_limit;
        generation* free_list_gen;
        BYTE* highest_plug; 
        BYTE* free_top;
    };
#ifdef CONCURRENT_GC
    PER_HEAP
    BYTE* allocate_from_free_top (size_t size, BYTE*& free_top);
#endif //CONCURRENT_GC
    PER_HEAP
    BYTE* allocate_at_end (size_t size);
    PER_HEAP
    void make_free_lists (int condemned_gen_number
#ifdef CONCURRENT_GC
                          ,BYTE* free_top
#endif //CONCURRENT_GC
                         );
    PER_HEAP
    void make_free_list_in_brick (BYTE* tree, make_free_args* args);
    PER_HEAP
    void thread_gap (BYTE* gap_start, size_t size, generation*  gen);
    PER_HEAP
    void make_unused_array (BYTE* x, size_t size, BOOL clearp=FALSE);
#if 0
    PER_HEAP
    BYTE* tree_search (BYTE* tree, BYTE* old_address);
#endif
    PER_HEAP
    void relocate_address (BYTE** old_address THREAD_NUMBER_DCL);

    struct relocate_args
    {
        BYTE* last_plug;
        BYTE* low;
        BYTE* high;
        BYTE* demoted_low;
        BYTE* demoted_high;
    };

    PER_HEAP
    void reloc_survivor_helper (relocate_args* args, BYTE** pval);

    PER_HEAP
    void relocate_survivors_in_plug (BYTE* plug, BYTE* plug_end, 
                                     relocate_args* args);
    PER_HEAP
    void relocate_survivors_in_brick (BYTE* tree, 
                                       relocate_args* args);
    PER_HEAP
    void relocate_survivors (int condemned_gen_number,
                             BYTE* first_condemned_address );
    PER_HEAP
    void relocate_phase (int condemned_gen_number,
                         BYTE* first_condemned_address);
    
    struct compact_args
    {
        BOOL copy_cards_p;
        BYTE* last_plug;
        size_t last_plug_relocation;
        BYTE* before_last_plug;
        size_t current_compacted_brick;
    };

    PER_HEAP
    void  gcmemcopy (BYTE* dest, BYTE* src, size_t len, BOOL copy_cards_p);
    PER_HEAP
    void compact_plug (BYTE* plug, size_t size, compact_args* args);
    PER_HEAP
    void compact_in_brick (BYTE* tree, compact_args* args);

    PER_HEAP
    void compact_phase (int condemned_gen_number, BYTE* 
                        first_condemned_address, BOOL clear_cards);
    PER_HEAP
    void clear_cards (size_t start_card, size_t end_card);
    PER_HEAP
    void clear_card_for_addresses (BYTE* start_address, BYTE* end_address);
    PER_HEAP
    void copy_cards (size_t dst_card, size_t src_card,
                     size_t end_card, BOOL nextp);
    PER_HEAP
    void copy_cards_for_addresses (BYTE* dest, BYTE* src, size_t len);
    PER_HEAP
    BOOL ephemeral_pointer_p (BYTE* o);
    PER_HEAP
    void fix_brick_to_highest (BYTE* o, BYTE* next_o);
    PER_HEAP
    BYTE* find_first_object (BYTE* start,  size_t brick, BYTE* min_address);
    PER_HEAP
    BYTE* compute_next_boundary (BYTE* low, int gen_number, BOOL relocating);
    PER_HEAP
    void mark_through_cards_helper (BYTE** poo, unsigned int& ngen, 
                                    unsigned int& cg_pointers_found, 
                                    card_fn fn, BYTE* nhigh, 
                                    BYTE* next_boundary);
    PER_HEAP
    void mark_through_cards_for_segments (card_fn fn, BOOL relocating);
    PER_HEAP
    void realloc_plug (size_t last_plug_size, BYTE*& last_plug,
                       generation* gen, BYTE* start_address, 
                       unsigned int& active_new_gen_number,
                       BYTE*& last_pinned_gap, BOOL& leftp, size_t page);
    PER_HEAP
    void realloc_in_brick (BYTE* tree, BYTE*& last_plug, BYTE* start_address,
                           generation* gen,
                           unsigned int& active_new_gen_number,
                           BYTE*& last_pinned_gap, BOOL& leftp, size_t page);
    PER_HEAP
    void realloc_plugs (generation* consing_gen, heap_segment* seg,
                        BYTE* start_address, BYTE* end_address,
                        unsigned active_new_gen_number);


    PER_HEAP
    generation* expand_heap (int condemned_generation,
                             generation* consing_gen, 
                             heap_segment* new_heap_segment);
    PER_HEAP
    void init_dynamic_data ();
    PER_HEAP
    float surv_to_growth (float cst, float limit, float max_limit);
    PER_HEAP
    size_t desired_new_allocation (dynamic_data* dd, size_t in, size_t out, 
                                   int gen_number);
    PER_HEAP
    size_t generation_size (int gen_number);
    PER_HEAP
    size_t  compute_promoted_allocation (int gen_number);
    PER_HEAP
    void compute_new_dynamic_data (int gen_number);
    PER_HEAP
    size_t new_allocation_limit (size_t size, size_t free_size, int gen_number);
    PER_HEAP
    size_t generation_fragmentation (generation* gen,
                                     generation* consing_gen, 
                                     BYTE* end);
    PER_HEAP
    size_t generation_sizes (generation* gen);
    PER_HEAP
    BOOL decide_on_compacting (int condemned_gen_number,
                               generation* consing_gen,
                               size_t fragmentation, 
                               BOOL& should_expand);
    PER_HEAP
    BOOL ephemeral_gen_fit_p (BOOL compacting=FALSE);
    PER_HEAP
    void reset_large_object (BYTE* o);
    PER_HEAP
    void sweep_large_objects ();
    PER_HEAP
    void relocate_in_large_objects ();
    PER_HEAP
    void mark_through_cards_for_large_objects (card_fn fn, BOOL relocating);
    PER_HEAP
    void descr_segment (heap_segment* seg);
    PER_HEAP
    void descr_card_table ();
    PER_HEAP
    void descr_generations ();

#ifdef CONCURRENT_COMPACT

    PER_HEAP
    size_t page_of (BYTE* add);
    PER_HEAP
    size_t plug_page_of (BYTE* add);
    PER_HEAP
    BYTE* page_address (size_t p);
    PER_HEAP
    void set_page (size_t p, BYTE* val);
    PER_HEAP
    BYTE* get_page (size_t p);
    PER_HEAP
    BYTE* gc_heap::page_plug (size_t p);
    PER_HEAP
    BOOL page_compacted (size_t p);
    PER_HEAP
    BOOL page_relocated (size_t p);
    PER_HEAP
    void set_page_relocated (size_t p);
    PER_HEAP
    void set_page_faulted (size_t p);
    PER_HEAP
    void set_alt_page (size_t p, ptrdiff_t adelta);
    PER_HEAP 
    void set_page_compacted (size_t p, BOOL gcthread_p);
    PER_HEAP
    BOOL page_contain_pinned (size_t p);
    PER_HEAP 
    void set_page_contain_pinned (size_t p);
    PER_HEAP
    int gc_heap::page_c_i (size_t p);
    PER_HEAP
    void gc_heap::set_page_c_i (size_t p, int c_i);

    PER_HEAP
    void update_page_table (BYTE* new_address, BYTE* plug_start, 
                            BYTE* plug_end, size_t& last_page);
    PER_HEAP
    void expand_update_page_table (BYTE* new_address, BYTE* plug_start, 
                                   BYTE* plug_end, size_t& last_page);
    PER_HEAP
    void update_page_table_pinned (generation* gen,  BYTE* plug_start, 
                                   BYTE* plug_end);
#endif //CONCURRENT_COMPACT

    /*------------ Multiple non isolated heaps ----------------*/
#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
    PER_HEAP_ISOLATED
    BOOL   create_thread_support (unsigned number_of_heaps);
    PER_HEAP_ISOLATED
    void destroy_thread_support ();
    PER_HEAP
    HANDLE create_gc_thread();
    PER_HEAP
    DWORD gc_thread_function();
#ifdef MARK_LIST
    PER_HEAP_ISOLATED
    void combine_mark_lists();
#endif
#endif //MULTIPLE_HEAPS && !ISOLATED_HEAPS

    /*------------ End of Multiple non isolated heaps ---------*/

#if defined (CONCURRENT_COMPACT) || (defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS))

    PER_HEAP_ISOLATED
    heap_segment* segment_of (BYTE* add,  ptrdiff_t & delta, 
                              BOOL verify_p = FALSE);

#endif //CONCURRENT_COMPACT || (MULTIPLE_HEAPS && !ISOLATED_HEAPS)

#ifdef CONCURRENT_GC
#ifdef CONCURRENT_COMPACT
    PER_HEAP
    ptrdiff_t gc_delta (BYTE* add);
    PER_HEAP
    BYTE* address_gc (BYTE* add, ptrdiff_t delta=~0);
    PER_HEAP
    BYTE* address_prg (BYTE* add, ptrdiff_t delta);

    PER_HEAP
    BOOL page_faulted (size_t page);
    PER_HEAP
    ptrdiff_t alt_delta (size_t page);
    PER_HEAP
    BOOL handle_fault (BYTE* add);
    PER_HEAP
    void duplicate_page (BYTE* add, int c_i, ptrdiff_t delta);

    PER_HEAP
    void c_expand_node (BYTE* node, int c_i, exp_node* exp, ptrdiff_t delta, 
                        ptrdiff_t adelta = ~0);
#endif //CONCURRENT_COMPACT

    PER_HEAP
    void revisit_written_page (BYTE* page, BYTE* beg,
                               BYTE* end,  BYTE*& last_page, 
                               BYTE*& last_object, BOOL large_objects_p);

    PER_HEAP
    void revisit_written_pages (BOOL concurrent_p);

    PER_HEAP
    void c_mark_phase();

    PER_HEAP
    void c_drain_mark_list();

    PER_HEAP
    void c_promote_callback(Object*& object, ScanContext* sc, DWORD flags);

    PER_HEAP
    void mark_absorb_new_alloc ();

    PER_HEAP
    void plan_absorb_new_alloc (BYTE* plug_end, BYTE* tree, 
                                size_t sequence_number, size_t curr_brick,
                                BYTE* last_node, 
                                generation*& consing_gen, 
                                int& active_new_gen_number, 
                                int& active_old_gen_number);

#ifdef CONCURRENT_COMPACT

    PER_HEAP
    void c_relocate_address (BYTE** address, BYTE* low, BYTE* high, 
                              int c_i);
    PER_HEAP
    void c_check_reloc_large (BYTE* gcadd, BYTE*& page_end, 
                              BOOL& reloc_page, ptrdiff_t delta);

    PER_HEAP
    void c_relocate_in_large_objects ();
    struct c_relocate_survivors_args
    {
        BYTE* low;
        BYTE* high; 
        ptrdiff_t delta;
        ptrdiff_t wpage_adelta; 
        BYTE* wpage_end;
    };
    PER_HEAP
    void
    gc_heap::c_check_reloc_surv (BYTE* gcadd, BYTE*& page_end, 
                                 ptrdiff_t& newadelta, ptrdiff_t delta);

    PER_HEAP
    void c_relocate_survivors_in_plug (BYTE* plug, BYTE* plug_end, 
                                       c_relocate_survivors_args* args);
    PER_HEAP
    BYTE* c_relocate_survivors_in_brick (BYTE* tree, BYTE* last_plug,
                                         c_relocate_survivors_args* args);

    PER_HEAP
    void c_relocate_survivors (int condemned_gen_number,
                               BYTE* first_condemned_address);

    struct c_compact_args 
    {
        BOOL copy_cards_p;
        BYTE* last_reloc_plug;
        size_t current_compacted_brick;
        ptrdiff_t src_delta;
        ptrdiff_t dst_delta;
        ptrdiff_t src_adelta;
        BYTE* src_page_end;
        BYTE* dst_page_end;
        BYTE* dst_start;
    };

    PER_HEAP
    void  c_gcmemcopy (BYTE* dest, BYTE* src, size_t len, 
                       c_compact_args* args);
    PER_HEAP
    void c_compact_plug (BYTE* plug, size_t size, 
                         c_compact_args* args);
    PER_HEAP
    void c_compact_in_brick (BYTE* tree, BYTE*& last_plug, 
                             c_compact_args* args);
    PER_HEAP
    void c_compact_phase (int condemned_gen_number, BYTE* 
                          first_condemned_address, BOOL clear_cards);

    struct c_compact_page_args 
    {
        BYTE* first_plug; 
        size_t size;
        BYTE* start_c_address;
        BYTE* end_c_address;
        ptrdiff_t delta;
        ptrdiff_t page_delta;
        heap_segment* seg;
        int c_i;
        int skip_pin_index;
        BYTE* last_plug;
    };

    PER_HEAP
    BOOL c_compact_page_plug (c_compact_page_args* args);
    PER_HEAP
    BOOL c_compact_page_in_brick (BYTE* tree, c_compact_page_args* args);
    PER_HEAP
    void c_compact_in_page (BYTE* add, int c_i, BYTE* plug, BOOL pinned_p);
    PER_HEAP
    void c_compact_from_plug_in_page (BYTE* add, BYTE* o, int c_i, BOOL skip_pin);
    PER_HEAP
    BOOL
    c_relocate_pinned_plug_in_brick (BYTE* tree, c_compact_page_args* args);
    PER_HEAP
    void c_relocate_pinned_plug_in_page (BYTE* add, BYTE* pplug, int c_i);
    PER_HEAP
    void c_relocate_pinned_plugs_in_page (BYTE* add, int c_i);
    PER_HEAP
    BOOL c_pinned_plug_may_reach (BYTE* plug, size_t len, BYTE* add);
    PER_HEAP
    void c_relocate_plug_in_page (BYTE* add, int c_i);
    PER_HEAP
    void c_relocate_large_object_page (BYTE* add, int c_i);
    PER_HEAP
    BOOL src_page_relocated_p (heap_segment* seg, BYTE* gcadd);
    PER_HEAP
    void unmap_segment_region (heap_segment* seg, BYTE* start, BYTE* end);
    PER_HEAP
    void unmap_prg_addresses ();
    PER_HEAP
    void remap_prg_addresses ();
#endif //CONCURRENT_COMPACT
    PER_HEAP
    void restart_vm();
    PER_HEAP
    void c_adjust_limits (int generation);
    PER_HEAP
    BOOL c_prepare_youngest_generation (BOOL user_thread_p);
    PER_HEAP
    BOOL gc_heap::prepare_gc_thread();
    PER_HEAP
    BOOL gc_heap::create_gc_thread();
    PER_HEAP
    void gc_heap::gc_wait_return();
    PER_HEAP
    void gc_heap::gc_wait_lh();
    PER_HEAP
    void gc_heap::gc_wait();
    PER_HEAP
    void gc_heap::start_c_gc();
    PER_HEAP
    void gc_heap::kill_gc_thread();
    PER_HEAP
    DWORD gc_heap::gc_thread_function();

#ifdef CONCURRENT_COMPACT
/*--------------- Beginning of synchronization functions ------------------*/

#define GC_THREAD 0
    PER_HEAP
    void acquire_write (BYTE* page, int c_i);
    PER_HEAP
    void release_write (int c_i);
    PER_HEAP
    ptrdiff_t acquire_fast_read (BYTE* page, int c_i, ptrdiff_t delta);
    PER_HEAP
    void release_fast_read (int c_i, ptrdiff_t adelta);
    PER_HEAP
    ptrdiff_t acquire_slow_read (BYTE* page, int c_i, ptrdiff_t delta);
    PER_HEAP
    void release_slow_read (int c_i, ptrdiff_t adelta);
    PER_HEAP
    ptrdiff_t acquire_gc_relocation (BYTE* page, ptrdiff_t delta);
    PER_HEAP
    void release_gc_write(); //for relocation and compaction
    PER_HEAP 
    void set_gc_current_page (BYTE* page);
    PER_HEAP
    int acquire_concurrency_index ();
    PER_HEAP
    void release_concurrency_index(int c_i);
    PER_HEAP
    void c_wait_for_writers ();
    PER_HEAP
    void c_wait_end_fault (BYTE* gcpage, ptrdiff_t delta);
    PER_HEAP
    void disable_faults (); //make user thread wait on faults. 
    PER_HEAP
    void allow_faults (); //allow the faults to proceed
    
/*------------------ End of synchonization functions ----------------------*/
#endif //CONCURRENT_COMPACT

#ifdef _DEBUG
    PER_HEAP
    void verify_page_table ();
    PER_HEAP
    BYTE* gc_heap::find_node (BYTE* old_address);
#endif //_DEBUG

    // All of these take segments as program address 
    PER_HEAP
    BYTE*& c_heap_segment_reserved (heap_segment* inst, ptrdiff_t delta=~0);
    PER_HEAP
    BYTE*& c_heap_segment_committed (heap_segment* inst, ptrdiff_t delta=~0);
    PER_HEAP
    BYTE*& c_heap_segment_allocated (heap_segment* inst, ptrdiff_t delta=~0);
    PER_HEAP
    heap_segment*& c_heap_segment_next (heap_segment* inst, ptrdiff_t delta=~0);
    PER_HEAP
    BYTE*& c_heap_segment_mem (heap_segment* inst, ptrdiff_t delta=~0);
    PER_HEAP
    BYTE*& c_heap_segment_plan_allocated (heap_segment* inst, 
                                               ptrdiff_t delta=~0);
    PER_HEAP
    BOOL c_heap_segment_relocated (heap_segment* inst, ptrdiff_t delta=~0);
    PER_HEAP
    BOOL c_heap_segment_compacted (heap_segment* inst, ptrdiff_t delta=~0);
    PER_HEAP
    BOOL c_heap_segment_being_relocated (heap_segment* inst, ptrdiff_t delta=~0);
    PER_HEAP
    BOOL c_heap_segment_being_compacted (heap_segment* inst, ptrdiff_t delta=~0);
    PER_HEAP
    void set_heap_segment_relocated (heap_segment* inst, ptrdiff_t delta=~0);
    PER_HEAP
    void set_heap_segment_compacted (heap_segment* inst, ptrdiff_t delta=~0);
    PER_HEAP
    void set_heap_segment_being_relocated (heap_segment* inst,ptrdiff_t delta=~0);
    PER_HEAP
    void set_heap_segment_being_compacted (heap_segment* inst,ptrdiff_t delta=~0);

#endif //CONCURRENT_COMPACT

    /* ------------------- per heap members --------------------------*/ 
public:

#ifdef MULTIPLE_HEAPS
    PER_HEAP
    BYTE*  ephemeral_low;      //lowest ephemeral address

    PER_HEAP
    BYTE*  ephemeral_high;     //highest ephemeral address
#endif //MULTIPLE_HEAPS

    PER_HEAP
    DWORD* card_table;

    PER_HEAP
    short* brick_table;

#ifdef MARK_ARRAY
    PER_HEAP_ISOLATED
    DWORD* mark_array;
#endif //MARK_ARRAY


#if defined (CONCURRENT_COMPACT) || (defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS))

    PER_HEAP_ISOLATED
    sorted_table* seg_table;

#endif //CONCURRENT_COMPACT || (MULTIPLE_HEAPS && !ISOLATED_HEAPS)

#if defined (CONCURRENT_GC) || (defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS))

    PER_HEAP_ISOLATED
    HANDLE gc_start_event;

    PER_HEAP_ISOLATED
    HANDLE gc_done_event;

#endif //CONCURRENT_GC || (MULTIPLE_HEAPS && !ISOLATED_HEAPS)

    PER_HEAP
    BYTE* demotion_low;

    PER_HEAP
    BYTE* demotion_high;


#if (defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS))
    PER_HEAP
    int gc_policy;  //sweep, compact, expand

    PER_HEAP
    heap_segment* new_heap_segment;

    PER_HEAP
    int spin_count;
#endif (MULTIPLE_HEAPS && !ISOLATED_HEAPS)

    PER_HEAP_ISOLATED
    gc_mechanisms settings;


#ifdef CONCURRENT_GC

    enum c_lh_state 
    {
        lh_state_marking,
        lh_state_planning,
        lh_state_free
    };

    PER_HEAP_ISOLATED
    c_lh_state c_allocate_lh;     //tells the large object allocator to 
    //mark the object as new since the start of gc.

    PER_HEAP
    HANDLE gc_thread;

    //This is necessary because the OS loader lock prevents
    //the gc thread from starting. a full gc under the OS loader lock would
    //deadlock waiting for the gc thread to proceed. 
    PER_HEAP
    BOOL gc_thread_running; // gc thread is its main loop

    PER_HEAP
    CRITICAL_SECTION gc_thread_timeout_cs; 

    PER_HEAP
    HANDLE gc_proceed_event;

    PER_HEAP
    HANDLE gc_lh_block_event;

    // this flag is used to turn off concurrent GC during EEshutdown
    PER_HEAP
    BOOL gc_can_use_concurrent;

#else //CONCURRENT_GC

#define concurrent_gc_p 0

#endif //CONCURRENT_GC


#ifdef CONCURRENT_COMPACT
    PER_HEAP_ISOLATED
    page_manager* p_manager;

    PER_HEAP
    f_page_list* faulted_pages;

    PER_HEAP
    c_synchronize* c_sync;

    PER_HEAP
    BYTE* last_pinned_plug_end;
#endif //CONCURRENT_COMPACT

    PER_HEAP
    BYTE* lowest_address;

    PER_HEAP
    BYTE* highest_address;

protected:
#if !defined (NOVM) && defined (MULTIPLE_HEAPS)
    PER_HEAP
    GCHeap* vm_heap;
    PER_HEAP
    int heap_number;
    PER_HEAP
    volatile int alloc_context_count;
#else
    #define vm_heap ((GCHeap*)0)
    #define heap_number (0)
#endif //MULTIPLE_HEAPS
    PER_HEAP
    heap_segment* ephemeral_heap_segment;

    PER_HEAP
    BYTE*       gc_low; // lowest address being condemned

    PER_HEAP
    BYTE*       gc_high; //highest address being condemned

    PER_HEAP
    size_t      mark_stack_tos;

    PER_HEAP
    size_t      mark_stack_bos;

    PER_HEAP
    size_t    mark_stack_array_length;

    PER_HEAP
    mark*       mark_stack_array;

#ifdef MARK_ARRAY
    PER_HEAP
    BYTE**      pin_list;

    PER_HEAP
    int         pin_list_length;

    PER_HEAP
    int         pin_index;

    PER_HEAP
    BYTE**      c_mark_list;

    PER_HEAP
    size_t         c_mark_list_length;

    PER_HEAP
    size_t         c_mark_list_index;
#endif //MARK_ARRAY

#ifdef MARK_LIST
    PER_HEAP
    BYTE** mark_list;

    PER_HEAP_ISOLATED
    size_t mark_list_size;

    PER_HEAP
    BYTE** mark_list_end;

    PER_HEAP
    BYTE** mark_list_index;

    PER_HEAP_ISOLATED
    BYTE** g_mark_list;
#endif //MARK_LIST

    PER_HEAP
    BYTE*  min_overflow_address;

    PER_HEAP
    BYTE*  max_overflow_address;

    PER_HEAP
    BYTE*  shigh; //keeps track of the highest marked object

    PER_HEAP
    BYTE*  slow; //keeps track of the lowest marked object

    PER_HEAP
    int   allocation_quantum;

    PER_HEAP
    int   alloc_contexts_used;

#define youngest_generation (generation_of (0))
#define large_object_generation (generation_of (max_generation+1))

    PER_HEAP
    BYTE* alloc_allocated; //keeps track of the highest 
                                //address allocated by alloc

    // The more_space_lock and gc_lock is used for 3 purposes:
    //
    // 1) to coordinate threads that exceed their quantum (UP & MP) (more_spacee_lock)
    // 2) to synchronize allocations of large objects (more_space_lock)
    // 3) to synchronize the GC itself (gc_lock)
    //
    // As such, it has 3 clients:
    //
    // 1) Threads that want to extend their quantum.  This always takes the lock 
    //    and sometimes provokes a GC.
    // 2) Threads that want to perform a large allocation.  This always takes 
    //    the lock and sometimes provokes a GC.
    // 3) GarbageCollect takes the lock and then unconditionally provokes a GC by 
    //    calling GarbageCollectGeneration.
    //
    PER_HEAP_ISOLATED
    GCSpinLock gc_lock; //lock while allocating more space

    PER_HEAP
    GCSpinLock more_space_lock; //lock while allocating more space

#ifdef MULTIPLE_HEAPS
    PER_HEAP
    generation generation_table [NUMBERGENERATIONS];
#endif

    PER_HEAP
    dynamic_data dynamic_data_table [NUMBERGENERATIONS+1];


    //Large object support

#ifdef CONCURRENT_GC
    PER_HEAP_ISOLATED
    GCSpinLock lheap_lock;
#endif //CONCURRENT_GC

    PER_HEAP
    int generation_skip_ratio;//in %

    PER_HEAP
    BOOL gen0_bricks_cleared;
#ifdef FFIND_OBJECT
    PER_HEAP
    int gen0_must_clear_bricks;
#endif //FFIND_OBJECT


    PER_HEAP
    CFinalize* finalize_queue;

    /* ----------------------- global members ----------------------- */
public:

    static
    segment_manager* seg_manager;

    static 
    int g_max_generation;
    
#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)

    PER_HEAP
    int         condemned_generation_num;

    static 
    int       n_heaps;
    static
    gc_heap** g_heaps;  //keeps all of the heaps in an array
    static    
    HANDLE*   g_gc_threads; // keep all of the gc threads.
    static 
    size_t*   g_promoted;
#else
    static 
    size_t    g_promoted;

#endif //MULTIPLE_HEAPS &&!ISOLATED_HEAPS

    static 
    size_t reserved_memory;
    static
    size_t reserved_memory_limit;

}; // class gc_heap


class CFinalize
{
    friend struct MEMBER_OFFSET_INFO(CFinalize);
private:

    Object** m_Array;
    Object** m_FillPointers[NUMBERGENERATIONS+2];
    Object** m_EndArray;
    int m_PromotedCount;
    volatile LONG lock;

#ifdef COLLECT_CLASSES
    ListSingle  listFinalizableClasses;
    ListSingle  listDeletableClasses;
#endif //COLLECT_CLASSES

    BOOL GrowArray();
    void MoveItem (Object** fromIndex,
                   unsigned int fromSeg,
                   unsigned int toSeg);

    BOOL IsSegEmpty ( unsigned int i)
    {
        ASSERT ( i <= NUMBERGENERATIONS+1);
        return ((i==0) ?
                (m_FillPointers[0] == m_Array):
                (m_FillPointers[i] == m_FillPointers[i-1]));
    }

public:
    CFinalize ();
    ~CFinalize();
    void EnterFinalizeLock();
    void LeaveFinalizeLock();
    void RegisterForFinalization (int gen, Object* obj);
    Object* GetNextFinalizableObject ();
    BOOL ScanForFinalization (int gen, int passnum, BOOL mark_only_p,
                              gc_heap* hp);
    void RelocateFinalizationData (int gen, gc_heap* hp);
    void GcScanRoots (promote_func* fn, int hn, ScanContext *pSC);
    void UpdatePromotedGenerations (int gen, BOOL gen_0_empty_p);
    int  GetPromotedCount();

    //Methods used by the shutdown code to call every finalizer
    void SetSegForShutDown(BOOL fHasLock);
    size_t GetNumberFinalizableObjects();
    
    //Methods used by the app domain unloading call to finalize objects in an app domain
    BOOL FinalizeAppDomain (AppDomain *pDomain, BOOL fRunFinalizers);

#ifdef COLLECT_CLASSES
    void GCPromoteFinalizableClasses( BYTE *low, BYTE *high );

    ClassListEntry  *GetNextFinalizableClassAndDeleteCurrent( ClassListEntry *cur );
    HRESULT QueueClassForFinalization( PjvmClass pClass );
    BOOL QueueClassForDeletion( PjvmClass pClass );
    void DeleteDeletableClasses( void );
#endif //COLLECT_CLASSES
    void CheckFinalizerObjects();
};

inline
 size_t& dd_current_size (dynamic_data* inst)
{
  return inst->current_size;
}
inline
size_t& dd_previous_size (dynamic_data* inst)
{
  return inst->previous_size;
}
inline
size_t& dd_freach_previous_promotion (dynamic_data* inst)
{
  return inst->freach_previous_promotion;
}
inline
size_t& dd_desired_allocation (dynamic_data* inst)
{
  return inst->desired_allocation;
}
inline
size_t& dd_collection_count (dynamic_data* inst)
{
    return inst->collection_count;
}
inline
size_t& dd_promoted_size (dynamic_data* inst)
{
    return inst->promoted_size;
}
inline
float& dd_limit (dynamic_data* inst)
{
  return inst->limit;
}
inline
float& dd_max_limit (dynamic_data* inst)
{
  return inst->max_limit;
}
inline
size_t& dd_min_gc_size (dynamic_data* inst)
{
  return inst->min_gc_size;
}
inline
size_t& dd_max_size (dynamic_data* inst)
{
  return inst->max_size;
}
inline
size_t& dd_min_size (dynamic_data* inst)
{
  return inst->min_size;
}
inline
ptrdiff_t& dd_new_allocation (dynamic_data* inst)
{
  return inst->new_allocation;
}
inline
ptrdiff_t& dd_gc_new_allocation (dynamic_data* inst)
{
  return inst->gc_new_allocation;
}
inline
ptrdiff_t& dd_c_new_allocation (dynamic_data* inst)
{
  return inst->c_new_allocation;
}
inline
size_t& dd_default_new_allocation (dynamic_data* inst)
{
  return inst->default_new_allocation;
}
inline
size_t& dd_fragmentation_limit (dynamic_data* inst)
{
  return inst->fragmentation_limit;
}
inline
float& dd_fragmentation_burden_limit (dynamic_data* inst)
{
  return inst->fragmentation_burden_limit;
}

inline
size_t& dd_fragmentation (dynamic_data* inst)
{
  return inst->fragmentation;
}

#define max_generation          gc_heap::g_max_generation

inline 
alloc_context* generation_alloc_context (generation* inst)
{
    return &(inst->allocation_context);
}

inline
BYTE*& generation_allocation_start (generation* inst)
{
  return inst->allocation_start;
}
inline
BYTE*& generation_allocation_pointer (generation* inst)
{
  return inst->allocation_context.alloc_ptr;
}
inline
BYTE*& generation_allocation_limit (generation* inst)
{
  return inst->allocation_context.alloc_limit;
}
inline
BYTE*& generation_free_list (generation* inst)
{
  return inst->free_list;
}
inline
heap_segment*& generation_start_segment (generation* inst)
{
  return inst->start_segment;
}
inline
heap_segment*& generation_allocation_segment (generation* inst)
{
  return inst->allocation_segment;
}
inline
BYTE*& generation_plan_allocation_start (generation* inst)
{
  return inst->plan_allocation_start;
}
inline
BYTE*& generation_last_gap (generation* inst)
{
  return inst->last_gap;
}
inline
size_t& generation_free_list_space (generation* inst)
{
  return inst->free_list_space;
}
inline
size_t& generation_allocation_size (generation* inst)
{
  return inst->allocation_size;
}

#define plug_skew           sizeof(ObjHeader)
#define min_obj_size        (sizeof(BYTE*)+plug_skew+sizeof(size_t))//syncblock + vtable+ first field
#define min_free_list       (sizeof(BYTE*)+min_obj_size) //Need one slot more
//Note that this encodes the fact that plug_skew is a multiple of BYTE*.
struct plug
{
    BYTE *  skew[plug_skew / sizeof(BYTE *)];
};


//need to be careful to keep enough pad items to fit a relocation node 
//padded to QuadWord before the plug_skew
class heap_segment
{
public:
    BYTE*           allocated;
    BYTE*           committed;
    BYTE*           reserved;
    BYTE*           used;
    BYTE*           mem;
    heap_segment*   next;
    BYTE*           plan_allocated;
#ifdef CONCURRENT_COMPACT
    int             status;
#ifdef ALIAS_MEM
    BYTE*           aliased;
#else
    BYTE*           padx;
#endif //ALIAS_MEM
#endif //CONCURRENT_COMPACT

#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
    gc_heap*        heap;
    BYTE*           padx;
#endif //MULTIPLE_HEAPS ISOLATED_HEAPS

    BYTE*           pad0;
#if (SIZEOF_OBJHEADER % 8) != 0
    BYTE            pad1[8 - (SIZEOF_OBJHEADER % 8)];   // Must pad to quad word
#endif
    plug            plug;
};

inline
BYTE*& heap_segment_reserved (heap_segment* inst)
{
  return inst->reserved;
}
inline
BYTE*& heap_segment_committed (heap_segment* inst)
{
  return inst->committed;
}
inline
BYTE*& heap_segment_used (heap_segment* inst)
{
  return inst->used;
}
inline
BYTE*& heap_segment_allocated (heap_segment* inst)
{
  return inst->allocated;
}
inline
heap_segment*& heap_segment_next (heap_segment* inst)
{
  return inst->next;
}
inline
BYTE*& heap_segment_mem (heap_segment* inst)
{
  return inst->mem;
}
inline
BYTE*& heap_segment_plan_allocated (heap_segment* inst)
{
  return inst->plan_allocated;
}

#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
inline
gc_heap*& heap_segment_heap (heap_segment* inst)
{
  return inst->heap;
}
#endif //MULTIPLE_HEAPS ISOLATED_HEAPS

#ifndef MULTIPLE_HEAPS

extern "C" {
extern generation   generation_table [NUMBERGENERATIONS];
}
#else //MULTIPLE_HEAPS

//This is used by jit helpers. Set it later to the 
//generation table of heap 0
extern "C" {
extern generation*  generation_table;
}

#endif //MULTIPLE_HEAPS


inline
generation* gc_heap::generation_of (int  n)
{
    assert (((n <= max_generation+1) && (n >= 0)));
    return &generation_table [ n ];
}


inline
dynamic_data* gc_heap::dynamic_data_of (int gen_number)
{
    return &dynamic_data_table [ gen_number ];
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gcport.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*++



Module Name:

    gc.cpp

Abstract:

   Automatic memory manager.


Todo:
Provide API for tuning parameters and statistics. 
Provide API for segment size (before startup). 
Support for multiple heaps. 
    Mark stack sharing?
    Clean heap functionality Done.
    shutdown -> destroy everything Done
    release stuff when we are finished with the heap Done.
Support for very large segments:
    virtual allocation of card, mark and brick tables?
Allocation in oldest generation
Concurrent GC
    relocate, compact, need to work on gc_addresses Done
    New relocation entry that branches on copied pages. Done
    release all side pages when gc is finished. Done
    Write src_page_relocated_p Done.
    Synchronization with the collector thread and other faults at 
    overlapping addresses. Done.
    Fix the update_page_table to care only about straddling plugs Done
    Add plug_skew constant, fix the code to use the constant. Done 
    Add plug_page_of to take plug_skew into account. Done
    Correct page_table in the face of pinned plugs. Done
    Handle pinning (pinned plugs mixed with normal plugs on a page) Done
    Fix the page_table entries during heap expansion.Done
    Use the alias VXD for Win9x Done.
    Consider locking sread across plugs Done. 
    Allocate gap at start of concurrent gc Done.
    Prepare to restart(all gc structures ready at beginning of mark phase) Done
    Wait until all threads are out of page fault before remapping segements Done.
    Finish the mark array implementation Done.
    Large Object marking not dependent on mark array
    See if allocator can always leave room for Align(min_obj_size) before 
    it GC() (perf impact?) Done.
    Verify that the page is valid in handle_fault Done.
    Fix allocation from free_list in allocate. Done. 
    Implement pinned plug demoting for non concurrent gc only. Done
    Tell the relevant weak pointers about pinned plug demoting. Done
    Reduce cost of clearing gen1 card table (do it only when many 
    1->0 pointers were found).(almost done)
    Consider clearing the promoted gen 0 card table during sweep
    Straighten our copy and merge of card table for concurrent GC Done.
    Detect mark stack overflow better? Nothing to be done.
    Handle out of memory during heap expansion Done.
    Handle out of memory during concurrent gc Almost Done, (pinned faults ...)
    Reconciliate SHORT_PLUGS and concurrent gc
    Reconciliate FREE_LIST_0 and concurrent gc Done. 
    Verify memory leaks with concurrent gc Done. 
    Handle running out of reserved space during plan phase. Done
    Clear the brick table before concurrent mark phase efficiently

--*/
#include "common.h"
#include "object.inl"
#include "gcportpriv.h"

#ifdef GC_PROFILING
#include "profilepriv.h"
#endif




#ifdef COUNT_CYCLES
#pragma warning(disable:4035)

static
unsigned        GetCycleCount32()        // enough for about 40 seconds
{
__asm   push    EDX
__asm   _emit   0x0F
__asm   _emit   0x31
__asm   pop     EDX
};

#pragma warning(default:4035)

long cstart = 0;

#endif //COUNT_CYCLES

#ifdef TIME_GC
long mark_time, plan_time, sweep_time, reloc_time, compact_time;
#endif //TIME_GC



#define ephemeral_low           g_ephemeral_low
#define ephemeral_high          g_ephemeral_high






extern void StompWriteBarrierEphemeral();
extern void StompWriteBarrierResize(BOOL bReqUpperBoundsCheck);

#ifdef TRACE_GC

int     print_level     = DEFAULT_GC_PRN_LVL;  //level of detail of the debug trace
int     gc_count        = 0;
BOOL    trace_gc        = FALSE;

hlet* hlet::bindings = 0;

#endif //TRACE_GC




#define collect_classes         GCHeap::GcCollectClasses

DWORD gfNewGcEnable;
DWORD gfDisableClassCollection;

void mark_class_of (BYTE*);




COUNTER_ONLY(PERF_COUNTER_TIMER_PRECISION g_TotalTimeInGC);

//Alignment constant for allocation
#define ALIGNCONST (DATA_ALIGNMENT-1)


#define mem_reserve (MEM_RESERVE)


//check if the low memory notification is supported

enum MEMORY_RESOURCE_NOTIFICATION_TYPE {
    LowMemoryResourceNotification,
    HighMemoryResourceNotification
};

typedef
WINBASEAPI
HANDLE
(WINAPI *CreateMemoryResourceNotification_t)(MEMORY_RESOURCE_NOTIFICATION_TYPE);

typedef
WINBASEAPI
HANDLE
(WINAPI *QueryMemoryResourceNotification_t)(HANDLE, BOOL*);

QueryMemoryResourceNotification_t QueryMemNotification;

BOOL low_mem_api_supported()
{
    HINSTANCE hstat = WszGetModuleHandle(L"Kernel32.dll");

    if (hstat == 0)
        return FALSE;

    FARPROC maddr = GetProcAddress (hstat, "CreateMemoryResourceNotification");
    FARPROC qaddr = GetProcAddress (hstat, "QueryMemoryResourceNotification");


    if (maddr)
    {
        dprintf (2, ("Low Memory API supported"));
        MHandles[0] = ((CreateMemoryResourceNotification_t)maddr) (LowMemoryResourceNotification);
        QueryMemNotification = ((QueryMemoryResourceNotification_t)qaddr);

        return TRUE;
    }
    else
        return FALSE;
}

int GetProcessCpuCount() 
{
    static int CPuCount = 0;

    if (CPuCount != 0)
        return CPuCount;
    else
    {

        DWORD pmask, smask;

        if (!GetProcessAffinityMask(GetCurrentProcess(), &pmask, &smask))
            return 1;

        if (pmask == 1)
            return 1;

   
        int count = 0;

        pmask &= smask;

        //counts the number of high bits in a 32 bit word.
        count = (pmask & 0x55555555) + ((pmask >> 1) &  0x55555555);
        count = (count & 0x33333333) + ((count >> 2) &  0x33333333);
        count = (count & 0x0F0F0F0F) + ((count >> 4) &  0x0F0F0F0F);
        count = (count & 0x00FF00FF) + ((count >> 8) &  0x00FF00FF);
        count = (count & 0x0000FFFF) + ((count >> 16)&  0x0000FFFF);
        assert (count > 0);
        return count;
    }
}


//This function clears a piece of memory
// size has to be Dword aligned


inline
void memclr ( BYTE* mem, size_t size)
{
   assert ((size & (sizeof (DWORD)-1)) == 0);
   DWORD* m= (DWORD*)mem;
   for (size_t i = 0; i < size / sizeof(DWORD); i++)
       *(m++) = 0;
}                                                               



void memcopy (BYTE* dmem, BYTE* smem, size_t size)
{
    assert ((size & (sizeof (DWORD)-1)) == 0);
    // copy 16 bytes at a time
    if (size >= 16)
    {
        do
        {
            ((DWORD *)dmem)[0] = ((DWORD *)smem)[0];
            ((DWORD *)dmem)[1] = ((DWORD *)smem)[1];
            ((DWORD *)dmem)[2] = ((DWORD *)smem)[2];
            ((DWORD *)dmem)[3] = ((DWORD *)smem)[3];
            dmem += 16;
            smem += 16;
        }
        while ((size -= 16) >= 16);
    }

    // still 8 bytes or more left to copy?
    if (size & 8)
    {
        ((DWORD *)dmem)[0] = ((DWORD *)smem)[0];
        ((DWORD *)dmem)[1] = ((DWORD *)smem)[1];
        dmem += 8;
        smem += 8;
    }

    // still 4 bytes or more left to copy?
    if (size & 4)
    {
        ((DWORD *)dmem)[0] = ((DWORD *)smem)[0];
        dmem += 4;
        smem += 4;
    }

}
inline
ptrdiff_t round_down (ptrdiff_t add, int pitch)
{
    return ((add / pitch) * pitch);
}

inline
size_t Align (size_t nbytes) 
{
    return (nbytes + ALIGNCONST) & ~ALIGNCONST;
}

inline
size_t AlignQword (size_t nbytes)
{
    return (nbytes + 7) & ~7;
}

inline
BOOL Aligned (size_t n)
{
    return (n & ALIGNCONST) == 0;
}

//CLR_SIZE  is the max amount of bytes from gen0 that is set to 0 in one chunk
#ifdef SERVER_GC
#define CLR_SIZE ((size_t)(8*1024))
#else //SERVER_GC
#define CLR_SIZE ((size_t)(8*1024))
#endif //SERVER_GC

#ifdef SERVER_GC

#define INITIAL_ALLOC (1024*1024*64)
#define LHEAP_ALLOC (1024*1024*64)

#else //SERVER_GC

#define INITIAL_ALLOC (1024*1024*16)
#define LHEAP_ALLOC (1024*1024*16)

#endif //SERVER_GC


#define page_size OS_PAGE_SIZE

inline
size_t align_on_page (size_t add)
{
    return ((add + page_size - 1) & - (page_size));
}

inline
BYTE* align_on_page (BYTE* add)
{
    return (BYTE*)align_on_page ((size_t) add);
}

inline
size_t align_lower_page (size_t add)
{
    return (add & - (page_size));
}

inline
BYTE* align_lower_page (BYTE* add)
{
    return (BYTE*)align_lower_page ((size_t)add);
}

inline
BOOL power_of_two_p (size_t integer)
{
    return !(integer & (integer-1));
}

inline
BOOL oddp (size_t integer)
{
    return (integer & 1) != 0;
}

size_t logcount (size_t word)
{
    //counts the number of high bits in a 16 bit word.
    assert (word < 0x10000);
    size_t count;
    count = (word & 0x5555) + ( (word >> 1 ) & 0x5555);
    count = (count & 0x3333) + ( (count >> 2) & 0x3333);
    count = (count & 0x0F0F) + ( (count >> 4) & 0x0F0F);
    count = (count & 0x00FF) + ( (count >> 8) & 0x00FF);
    return count;
}


class mark;
class generation;
class heap_segment;
class CObjectHeader;
class dynamic_data;
class large_object_block;
class segment_manager;
class l_heap;
class sorted_table;
class f_page_list;
class page_manager;
class c_synchronize;

static
HRESULT AllocateCFinalize(CFinalize **pCFinalize);

BYTE* tree_search (BYTE* tree, BYTE* old_address);
c_synchronize* make_c_synchronize(int max_ci);
void delete_c_synchronize(c_synchronize*);

void qsort1(BYTE** low, BYTE** high);

//no constructors because we initialize in make_generation

/* per heap static initialization */




#ifdef MARK_LIST
BYTE**      gc_heap::g_mark_list;
size_t      gc_heap::mark_list_size;
#endif //MARK_LIST




size_t      gc_heap::reserved_memory = 0;
size_t      gc_heap::reserved_memory_limit = 0;





BYTE*       g_ephemeral_low = (BYTE*)1; 

BYTE*       g_ephemeral_high = (BYTE*)~0;

BOOL        gc_heap::demotion;




int         gc_heap::generation_skip_ratio = 100;

int         gc_heap::condemned_generation_num = 0;


BYTE*       gc_heap::lowest_address;

BYTE*       gc_heap::highest_address;

short*      gc_heap::brick_table;

DWORD*      gc_heap::card_table;

BYTE*       gc_heap::gc_low;

BYTE*       gc_heap::gc_high;

BYTE*       gc_heap::demotion_low;

BYTE*       gc_heap::demotion_high;

heap_segment* gc_heap::ephemeral_heap_segment = 0;


size_t      gc_heap::mark_stack_tos = 0;

size_t      gc_heap::mark_stack_bos = 0;

size_t      gc_heap::mark_stack_array_length = 0;

mark*       gc_heap::mark_stack_array = 0;


#ifdef MARK_LIST
BYTE**      gc_heap::mark_list;
BYTE**      gc_heap::mark_list_index;
BYTE**      gc_heap::mark_list_end;
#endif //MARK_LIST


BYTE*       gc_heap::min_overflow_address = (BYTE*)~0;

BYTE*       gc_heap::max_overflow_address = 0;

BYTE*       gc_heap::shigh = 0;

BYTE*       gc_heap::slow = (BYTE*)~0;


GCSpinLock gc_heap::more_space_lock = SPIN_LOCK_INITIALIZER;

long m_GCLock = -1;


extern "C" {
generation  generation_table [NUMBERGENERATIONS];
}
dynamic_data gc_heap::dynamic_data_table [NUMBERGENERATIONS+1];

BYTE* gc_heap::alloc_allocated = 0;

int   gc_heap::allocation_quantum = CLR_SIZE;

int   gc_heap::alloc_contexts_used = 0;



l_heap*      gc_heap::lheap = 0;


DWORD*       gc_heap::lheap_card_table = 0;

gmallocHeap* gc_heap::gheap = 0;

large_object_block* gc_heap::large_p_objects = 0;

large_object_block** gc_heap::last_large_p_object = &large_p_objects;

large_object_block* gc_heap::large_np_objects = 0;

size_t      gc_heap::large_objects_size = 0;

size_t      gc_heap::large_blocks_size = 0;



BOOL        gc_heap::gen0_bricks_cleared = FALSE;

#ifdef FFIND_OBJECT
int         gc_heap::gen0_must_clear_bricks = 0;
#endif FFIND_OBJECT

CFinalize* gc_heap::finalize_queue = 0;


/* end of per heap static initialization */


/* static initialization */ 
int max_generation = 2;

segment_manager* gc_heap::seg_manager;

BYTE* g_lowest_address = 0;

BYTE* g_highest_address = 0;

/* global versions of the card table and brick table */ 
DWORD*  g_card_table;


/* end of static initialization */ 




size_t gcard_of ( BYTE*);
void gset_card (size_t card);

#define memref(i) *(BYTE**)(i)

//GC Flags
#define GC_MARKED       0x1
#define GC_PINNED       0x2
#define slot(i, j) ((BYTE**)(i))[j+1]
class CObjectHeader : public Object
{
public:
    /////
    //
    // Header Status Information
    //

    MethodTable    *GetMethodTable() const 
    { 
        return( (MethodTable *) (((size_t) m_pMethTab) & (~(GC_MARKED | GC_PINNED))));
    }

    void SetMarked()
    { 
        m_pMethTab = (MethodTable *) (((size_t) m_pMethTab) | GC_MARKED); 
    }
    
    BOOL IsMarked() const
    { 
        return !!(((size_t)m_pMethTab) & GC_MARKED); 
    }

    void SetPinned()
    { 
        m_pMethTab = (MethodTable *) (((size_t) m_pMethTab) | GC_PINNED); 
    }

    BOOL IsPinned() const
    {
        return !!(((size_t)m_pMethTab) & GC_PINNED); 
    }

    void ClearMarkedPinned()
    { 
        SetMethodTable( GetMethodTable() ); 
    }

    CGCDesc *GetSlotMap ()
    {
        ASSERT(GetMethodTable()->ContainsPointers());
        return CGCDesc::GetCGCDescFromMT(GetMethodTable());
    }

    void SetFree(size_t size)
    {
        I1Array     *pNewFreeObject;
        
        _ASSERTE( size >= sizeof(ArrayBase));
        assert (size == (size_t)(DWORD)size);
        pNewFreeObject = (I1Array *) this;
        pNewFreeObject->SetMethodTable( g_pFreeObjectMethodTable );
        int base_size = g_pFreeObjectMethodTable->GetBaseSize();
        assert (g_pFreeObjectMethodTable->GetComponentSize() == 1);
        ((ArrayBase *)pNewFreeObject)->m_NumComponents = (DWORD)(size - base_size);
#ifdef _DEBUG
        ((DWORD*) this)[-1] = 0;    // clear the sync block, 
#endif //_DEBUG
#ifdef VERIFY_HEAP
        assert ((DWORD)(((ArrayBase *)pNewFreeObject)->m_NumComponents) >= 0);
        if (g_pConfig->IsHeapVerifyEnabled())
            memset (((DWORD*)this)+2, 0xcc, 
                    ((ArrayBase *)pNewFreeObject)->m_NumComponents);
#endif //VERIFY_HEAP
    }

    BOOL IsFree () const
    {
        return (GetMethodTable() == g_pFreeObjectMethodTable);
    }
    
    // get the next header
    CObjectHeader* Next()
    { return (CObjectHeader*)(Align ((size_t)((char*)this + GetSize()))); }

    BOOL ContainsPointers() const
    {
        return GetMethodTable()->ContainsPointers();
    }

    Object* GetObjectBase() const
    {
        return (Object*) this;
    }

#ifdef _DEBUG
    friend BOOL IsValidCObjectHeader (CObjectHeader*);
#endif
};

inline CObjectHeader* GetObjectHeader(Object* object)
{
    return (CObjectHeader*)object;
}




#define free_list_slot(x) ((BYTE**)(x))[2]


//represents a section of the large object heap
class l_heap
{
public:
    void*  heap;  // pointer to the heap
    l_heap* next; //next heap
    size_t size;  //Total size (including l_heap)
    DWORD flags;  // 1: allocated from the segment_manager
};

/* flags definitions */

#define L_MANAGED 1 //heap has been allocated via segment_manager.

inline 
l_heap*& l_heap_next (l_heap* h)
{
    return h->next;
}

inline
size_t& l_heap_size (l_heap* h)
{
    return h->size;
}

inline
DWORD& l_heap_flags (l_heap* h)
{
    return h->flags;
}

inline 
void*& l_heap_heap (l_heap* h)
{
    return h->heap;
}

inline
BOOL l_heap_managed (l_heap* h)
{
    return l_heap_flags (h) & L_MANAGED;
}


inline
unsigned get_heap_dwcount (size_t n_heaps)

{
    return unsigned((n_heaps+31)/32);
}

inline
static unsigned get_segment_dwcount (size_t n_segs)
{
    return unsigned((n_segs+31)/32);
}


/* heap manager helper classes */
class vm_block
{
public:
    void* start_address;
    size_t length;
    size_t delta; // of the gc addresses. 
    size_t n_segs;
    size_t n_heaps;
    vm_block* next;
    DWORD* get_segment_bitmap ()
    {
        return (DWORD*)(this + 1);
    }
    DWORD* get_heap_bitmap ()
    {
        return ((DWORD*)(this + 1)) + get_segment_dwcount (n_segs);
    }
    vm_block(size_t nsegs, size_t nheaps)
    {
        next = 0;
        n_segs = nsegs;
        n_heaps = nheaps;
        //clear the arrays
        DWORD* ar = get_segment_bitmap ();
        for (size_t i = 0; i < get_segment_dwcount(n_segs); i++)
            ar[i] = 0;
        ar = get_heap_bitmap ();
        for (i = 0; i < get_heap_dwcount(n_heaps); i++)
            ar[i] = 0;
    }

};


class segment_manager 
{
public:
    char* lowest_segment;
    vm_block* segment_vm_block;
    char* highest_heap;
    vm_block* heap_vm_block;
    ptrdiff_t segment_size; /* always power of 2 MB */
    size_t heap_size; /* always power of 2 MB */
    size_t vm_block_size;
    size_t n_segs; /* number of segment in the block */
    size_t n_heaps; /* number of heaps in the block */

    vm_block* first_vm_block;
    /* beginning of API */
    l_heap* get_heap (ptrdiff_t& delta);
    heap_segment* get_segment(ptrdiff_t& delta);
    void release_heap (l_heap*);
    void release_segment (heap_segment*);
    /* end of API */

    segment_manager (size_t vm_block_size, size_t segment_size, 
                     size_t heap_size, size_t n_segs, size_t n_heaps,
                     vm_block* first_vm_block)
    {
        assert (segment_size >> 20);
        assert (power_of_two_p (segment_size >> 20));
        assert (heap_size >> 20);
        assert (power_of_two_p (heap_size >> 20));
        this->vm_block_size = vm_block_size;
        this->segment_size = segment_size;
        this->heap_size = heap_size;
        this->first_vm_block = first_vm_block;
        this->n_segs = n_segs;
        this->n_heaps = n_heaps;
        lowest_segment = (char*)((char*)first_vm_block->start_address + 
                                 segment_size * n_segs)
;
        segment_vm_block = first_vm_block;
        highest_heap = lowest_segment;
        heap_vm_block = first_vm_block;
        
    } 
    size_t bit_to_segment_offset (unsigned bit, size_t nsegs)
    {
        return (segment_size * nsegs - segment_size*(bit+1));
    }

    size_t bit_to_heap_offset (unsigned bit, size_t nsegs)
    {
        return (segment_size * nsegs +
                heap_size*bit);
    }
    
    unsigned segment_offset_to_bit (size_t offset, size_t nsegs)
    {
        int bit = (((int)(segment_size * nsegs) - (int)offset) / segment_size) - 1;
        assert (bit >= 0);
        return (unsigned) bit;
    }
                
    unsigned heap_offset_to_bit (size_t offset, size_t nsegs)
    {
        int bit = int((offset-(segment_size * nsegs)) / heap_size);
        assert (bit >= 0); 
        return (unsigned) bit;
    }
                

    unsigned get_segment_dwcount ()
    {
        return ::get_segment_dwcount (n_segs);
    }
    
    unsigned get_heap_dwcount ()
    {
        return ::get_heap_dwcount (n_heaps);
    }

    vm_block* vm_block_of (void*);

    heap_segment* find_free_segment(ptrdiff_t& delta)
    {
        vm_block* vb = first_vm_block;
        while (vb)
        {
            DWORD* free = vb->get_segment_bitmap();
            unsigned i;
            for (i = 0; i < ::get_segment_dwcount (vb->n_segs);i++){
                if (free[i])
                {
                    for (unsigned j = 0; j < 32; j++)
                    {
                        DWORD x = 1 << j;
                        if (free[i] & x)
                        {
                            free[i] &= ~x;
                            break;
                        }
                    }
                    assert (j < 32);
                    delta = vb->delta;
                    return (heap_segment*)((char*)vb->start_address +
                                           bit_to_segment_offset (32*i+j, vb->n_segs));
                }
            }
            vb = vb->next;
        }
        return 0;
    }

    void* find_free_heap(ptrdiff_t& delta)
    {
        vm_block* vb = first_vm_block;
        while (vb)
        {
            DWORD* free = vb->get_heap_bitmap();
            unsigned i;
            for (i = 0; i < ::get_heap_dwcount (vb->n_heaps);i++){
                if (free[i])
                {
                    for (unsigned j = 0; j < 32; j++)
                    {
                        DWORD x = 1 << j;
                        if (free[i] & x)
                        {
                            free[i] &= ~x;
                            break;
                        }
                    }
                    assert (j < 32);
                    delta = vb->delta;
                    return (void*)((char*)vb->start_address +
                                   bit_to_heap_offset (32*i+j, vb->n_segs));
                }
            }
            vb = vb->next;
        }
        return 0;
    }

};

void* virtual_alloc (size_t size, ptrdiff_t& delta)
{

    if ((gc_heap::reserved_memory + size) > gc_heap::reserved_memory_limit)
    {
        gc_heap::reserved_memory_limit = 
            CNameSpace::AskForMoreReservedMemory (gc_heap::reserved_memory_limit, size);
        if ((gc_heap::reserved_memory + size) > gc_heap::reserved_memory_limit)
            return 0;
    }
    gc_heap::reserved_memory += size;
    delta = 0;
    void* prgmem = VirtualAlloc (0, size, mem_reserve, PAGE_READWRITE);
    WS_PERF_LOG_PAGE_RANGE(0, prgmem, (unsigned char *)prgmem + size - OS_PAGE_SIZE, size);
    return prgmem;
    
}

void virtual_free (void* add, size_t size, ptrdiff_t delta)
{
    VirtualFree (add, 0, MEM_RELEASE);
    WS_PERF_LOG_PAGE_RANGE(0, add, 0, 0);
    gc_heap::reserved_memory -= size;
}

segment_manager* create_segment_manager (size_t segment_size,
                                         size_t heap_size, 
                                         size_t n_segs,
                                         size_t n_heaps)
{
    
    segment_manager* newseg = 0;
    ptrdiff_t sdelta;

    size_t vm_block_size = (segment_size*n_segs) + (heap_size*n_heaps);

    // allocate the virtual necessary virtual memory
    void* newvm = virtual_alloc (vm_block_size, sdelta);
//    assert (newvm);
    if (!newvm)
        return 0;

    // allocate memory for the segment manager and the first vm block
    size_t segment_dwcount = ::get_segment_dwcount (n_segs);

    size_t heap_dwcount = ::get_heap_dwcount (n_heaps);
    size_t bsize = (sizeof (segment_manager) + sizeof (vm_block) +
                    (segment_dwcount + heap_dwcount)* sizeof (DWORD));

    WS_PERF_SET_HEAP(GC_HEAP);
    newseg = (segment_manager*)new(char[bsize]);

    if (!newseg)
        return 0;
    
    WS_PERF_UPDATE("GC:create_segment_manager", bsize, newseg);
    
    // initialize the segment manager.
    vm_block* newvm_block = (vm_block*)(newseg+1);
    newvm_block->vm_block::vm_block(n_segs, n_heaps);
    newvm_block->start_address = newvm;
    newvm_block->length = vm_block_size;
    newvm_block->delta = sdelta;
    newseg->segment_manager::segment_manager (vm_block_size, segment_size, 
                                              heap_size, n_segs, n_heaps, newvm_block);
    g_lowest_address = (BYTE*)newvm;
    g_highest_address = (BYTE*)newvm + vm_block_size;
    return newseg;
}

void destroy_segment_manager (segment_manager* sm)
{
    //all vm block have been decommitted. 

    //delete all vm blocks except first
    vm_block* vmb = sm->first_vm_block->next;
    while (vmb)
    {
        vm_block* vmbnext = vmb->next;
        virtual_free (vmb->start_address, vmb->length, vmb->delta);
        delete vmb;
        vmb = vmbnext;
    }

        virtual_free (sm->first_vm_block->start_address, 
                      sm->first_vm_block->length,
                      sm->first_vm_block->delta);

    // delete the first vm block and the segment manager
    delete (char*)sm;
}

vm_block* segment_manager::vm_block_of (void* add)
{
    vm_block* vb = first_vm_block;
    while (vb)
    {
        if ((vb->start_address <= add ) && 
            (add < ((char*)vb->start_address + vb->length)))
            break;
        vb = vb->next;
    }
    return vb;
}

//returns 0 in case of allocation failure
heap_segment* segment_manager::get_segment(ptrdiff_t& delta)
{
    heap_segment* result;
    /* Find free segment */
    result = find_free_segment(delta);
    if (result)
    {
        result = gc_heap::make_heap_segment ((BYTE*)result, segment_size);
        return result;
    }

    /* create new segment in current vm block */
    if (((char*)segment_vm_block->start_address + segment_size ) <= lowest_segment)
    {
        lowest_segment -= segment_size;
        delta = segment_vm_block->delta;
        result = gc_heap::make_heap_segment ((BYTE*)lowest_segment, segment_size);
        return result;
    }

    /* take the next vm block if there */
    vm_block* new_block = segment_vm_block->next;
    /* create a new vm block */
    //Allocate new vm_block
    if (!new_block)
    {
        WS_PERF_SET_HEAP(GC_HEAP);
        // Allocate Virtual Memory
        // Allocate space for the bitmaps (one bit per segment, one
        // bit per heap)
        new_block = (vm_block*)new char[sizeof (vm_block) + 
                              (get_segment_dwcount()+get_heap_dwcount())*
                               sizeof (DWORD)];
        if (!new_block)
            return 0;
        WS_PERF_UPDATE("GC:segment_manager:get_segment", sizeof (vm_block) + (get_segment_dwcount()+get_heap_dwcount())*sizeof (DWORD), new_block);
        
        new_block->vm_block::vm_block(1,0);
        ptrdiff_t sdelta;
        new_block->start_address = virtual_alloc (segment_size, sdelta);
        new_block->delta = sdelta;

        if (!new_block->start_address)
        {
            delete new_block;
            return 0;
        }
        new_block->length = segment_size;
        
        BYTE* start;
        BYTE* end;
        if (new_block->start_address < g_lowest_address)
        {
            start =  (BYTE*)new_block->start_address;
        } 
        else
        {
            start = (BYTE*)g_lowest_address;
        }

        if (((BYTE*)new_block->start_address + vm_block_size)> g_highest_address)
        {
            end = (BYTE*)new_block->start_address + vm_block_size;
        }
        else
        {
            end = (BYTE*)g_highest_address;
        }

        if (gc_heap::grow_brick_card_tables (start, end) != 0)
        {
            virtual_free (new_block->start_address, vm_block_size, sdelta);
            delete new_block;
            return 0;
        }

        //chain it to the end of vm_blocks;
        segment_vm_block->next = new_block;
    }
    //Make it the current vm block
    segment_vm_block = new_block;
    lowest_segment = (char*)((char*)new_block->start_address + 
                             segment_size * new_block->n_segs);
    //get segment
    lowest_segment -= segment_size;
    delta = segment_vm_block->delta;
    result = gc_heap::make_heap_segment ((BYTE*)lowest_segment, segment_size);
    return result;
}

l_heap* segment_manager::get_heap(ptrdiff_t& delta)
{
    void* bresult;
    l_heap* hresult;
    /* Find free segment */
    bresult = find_free_heap(delta);
    if (bresult)
    {
        hresult = gc_heap::make_large_heap ((BYTE*)bresult, heap_size,
                                            TRUE);
        return hresult;
    }

    /* create new heap in current vm block */
    if ((highest_heap + heap_size) <=
        ((char*)heap_vm_block->start_address + heap_vm_block->length))
    {
        delta = heap_vm_block->delta;
        hresult = gc_heap::make_large_heap ((BYTE*)highest_heap, heap_size,
                                           TRUE);
        highest_heap += heap_size;
        return hresult;
    }

    /* take the next vm block if there */
    vm_block* new_block = heap_vm_block->next;
    /* create a new vm block */
    //Allocate new vm_block
    if (!new_block)
    {
        WS_PERF_SET_HEAP(GC_HEAP);
        // Allocate Virtual Memory
        // Allocate space for the bitmaps (one bit per heap, one
        // bit per heap)
        new_block = (vm_block*)new char[(sizeof (vm_block) + 
                              (get_segment_dwcount()+get_heap_dwcount()*
                               sizeof (DWORD)))];
        if (!new_block)
            return 0;
        WS_PERF_UPDATE("GC:segment_manager:get_heap", sizeof (vm_block) + (get_segment_dwcount()+get_heap_dwcount())*sizeof (DWORD), new_block);
        
        new_block->vm_block::vm_block(0, 1);

        ptrdiff_t sdelta;
        new_block->start_address = virtual_alloc (heap_size, sdelta);
        new_block->delta = sdelta;

        if (!new_block->start_address)
        {
            delete new_block;
            return 0;
        }
        new_block->length = heap_size;
        
        BYTE* start;
        BYTE* end;
        if (new_block->start_address < g_lowest_address)
        {
            start =  (BYTE*)new_block->start_address;
            end = (BYTE*)g_highest_address;
        } 
        else
        {
            start = (BYTE*)g_lowest_address;
            end = (BYTE*)new_block->start_address + vm_block_size;
        }

        if (gc_heap::grow_brick_card_tables (start, end) != 0)
        {
            virtual_free (new_block->start_address, vm_block_size, sdelta);
            delete new_block;
            return 0;
        }
        //chain it to the end of vm_blocks;
        heap_vm_block->next = new_block;
    }
    //Make it the current vm block
    heap_vm_block = new_block;
    highest_heap = (char*)((char*)new_block->start_address + 
                           segment_size * new_block->n_segs);
    //get heap
    delta = heap_vm_block->delta;
    hresult = gc_heap::make_large_heap ((BYTE*)highest_heap, heap_size, TRUE);
    highest_heap += heap_size;
    return hresult;
}
    

void segment_manager::release_segment (heap_segment* sg)
{

    //find the right vm_block
    vm_block* vb = vm_block_of (sg);
    assert (vb);
    // set the bitmap
    unsigned bit = segment_offset_to_bit ((char*)sg - (char*)vb->start_address,
                                          vb->n_segs);
    vb->get_segment_bitmap ()[bit/32] |= ( 1 << (bit % 32));
}

void segment_manager::release_heap (l_heap* hp)
{
    //release all of the chained heaps. 
    //some of them aren't part of the heap manager. 
    do 
    {
        l_heap* nhp = l_heap_next (hp);
        if (l_heap_managed(hp))
        {

            void* bheap = l_heap_heap (hp);
            //find the right vm_block
            vm_block* vb = vm_block_of (bheap);
            assert (vb);
            // set the bitmap
            unsigned bit = heap_offset_to_bit ((char*)bheap - (char*)vb->start_address,
                                               vb->n_segs);
            vb->get_heap_bitmap ()[bit/32] |= ( 1 << (bit % 32));
        }
        delete hp;
        hp = nhp;
    } while (hp != 0);
}
    

/* End of heap manager */ 



class large_object_block
{
public:
    large_object_block*    next;      // Points to the next block
    large_object_block**   prev;      // Points to &(prev->next) where prev is the previous block
#if (SIZEOF_OBJHEADER % 8) != 0
    BYTE                   pad1[8 - (SIZEOF_OBJHEADER % 8)];    // Must pad to quad word
#endif
    plug                   plug;      // space for ObjHeader
};

inline
BYTE* block_object (large_object_block* b)
{
    assert ((BYTE*)AlignQword ((size_t)(b+1)) == (BYTE*)((size_t)(b+1)));
    return (BYTE*)AlignQword ((size_t)(b+1));
}

inline 
large_object_block* object_block (BYTE* o)
{
    return (large_object_block*)o - 1;
}

inline 
large_object_block* large_object_block_next (large_object_block* bl)
{
    return bl->next;
}

inline
BYTE* next_large_object (BYTE* o)
{
    large_object_block* x = large_object_block_next (object_block (o));
    if (x != 0)
        return block_object (x);
    else
        return 0;
}



class mark
{
public:
    BYTE* first;
    union 
    {
        BYTE* last;
        size_t len;
    };
};
inline
BYTE*& mark_first (mark* inst)
{
  return inst->first;
}
inline
BYTE*& mark_last (mark* inst)
{
  return inst->last;
}

/**********************************
   called at the beginning of GC to fix the allocated size to 
   what is really allocated, or to turn the free area into an unused object
   It needs to be called after all of the other allocation contexts have been 
   fixed since it relies on alloc_allocated.
 ********************************/

//for_gc_p indicates that the work is being done for GC, 
//as opposed to concurrent heap verification 
void gc_heap::fix_youngest_allocation_area (BOOL for_gc_p)
{
    assert (alloc_allocated);
    alloc_context* acontext = generation_alloc_context (youngest_generation);
    dprintf (3, ("generation 0 alloc context: ptr: %x, limit %x", 
                 (size_t)acontext->alloc_ptr, (size_t)acontext->alloc_limit));
    fix_allocation_context (acontext, for_gc_p);
    if (for_gc_p)
    {
        acontext->alloc_ptr = alloc_allocated;
        acontext->alloc_limit = acontext->alloc_ptr;
        heap_segment_allocated (ephemeral_heap_segment) =
            alloc_allocated;
    }
}

//for_gc_p indicates that the work is being done for GC, 
//as opposed to concurrent heap verification 
void gc_heap::fix_allocation_context (alloc_context* acontext, BOOL for_gc_p)
{
    dprintf (3, ("Fixing allocation context %x: ptr: %x, limit: %x",
                  (size_t)acontext, 
                  (size_t)acontext->alloc_ptr, (size_t)acontext->alloc_limit));
    if (((acontext->alloc_limit + Align (min_obj_size)) < alloc_allocated)||
		!for_gc_p)
    {

        BYTE*  point = acontext->alloc_ptr;
        if (point != 0)
        {
            size_t  size = (acontext->alloc_limit - acontext->alloc_ptr);
            // the allocation area was from the free list
            // it was shortened by Align (min_obj_size) to make room for 
            // at least the shortest unused object
            size += Align (min_obj_size);
            assert ((size >= Align (min_obj_size)));

            dprintf(3,("Making unused area [%x, %x[", (size_t)point, 
                       (size_t)point + size ));
            make_unused_array (point, size);

            if (for_gc_p)
                alloc_contexts_used ++; 

        }
    }
    else if (for_gc_p)
    {
        alloc_allocated = acontext->alloc_ptr;
        assert (heap_segment_allocated (ephemeral_heap_segment) <= 
                heap_segment_committed (ephemeral_heap_segment));
        alloc_contexts_used ++; 
    }


    if (for_gc_p)
    {
        acontext->alloc_ptr = 0;
        acontext->alloc_limit = acontext->alloc_ptr;
    }
}

//used by the heap verification for concurrent gc.
//it nulls out the words set by fix_allocation_context for heap_verification
void repair_allocation (alloc_context* acontext, void* arg)
{
    BYTE* alloc_allocated = (BYTE*)arg;
    BYTE*  point = acontext->alloc_ptr;

    if (point != 0)
    {
        memclr (acontext->alloc_ptr - sizeof(ObjHeader), 
                (acontext->alloc_limit - acontext->alloc_ptr)+Align (min_obj_size));
    }
}


void gc_heap::fix_older_allocation_area (generation* older_gen)
{
    heap_segment* older_gen_seg = generation_allocation_segment (older_gen);
    if (generation_allocation_limit (older_gen) !=
        heap_segment_plan_allocated (older_gen_seg))
    {
        BYTE*  point = generation_allocation_pointer (older_gen);
    
        size_t  size = (generation_allocation_limit (older_gen) -
                               generation_allocation_pointer (older_gen));
        if (size != 0)
        {
            assert ((size >= Align (min_obj_size)));
            dprintf(3,("Making unused area [%x, %x[", (size_t)point, (size_t)point+size));
            make_unused_array (point, size);
        }
    }
    else
    {
        assert (older_gen_seg != ephemeral_heap_segment);
        heap_segment_plan_allocated (older_gen_seg) =
            generation_allocation_pointer (older_gen);
        generation_allocation_limit (older_gen) =
            generation_allocation_pointer (older_gen);
    }
}

void gc_heap::set_allocation_heap_segment (generation* gen)
{
    BYTE* p = generation_allocation_start (gen);
    assert (p);
    heap_segment* seg = generation_allocation_segment (gen);
    if ((p <= heap_segment_reserved (seg)) &&
        (p >= heap_segment_mem (seg)))
        return;

    // try ephemeral heap segment in case of heap expansion
    seg = ephemeral_heap_segment;
    if (!((p <= heap_segment_reserved (seg)) &&
          (p >= heap_segment_mem (seg))))
    {
        seg = generation_start_segment (gen);
        while (!((p <= heap_segment_reserved (seg)) &&
                 (p >= heap_segment_mem (seg))))
        {
            seg = heap_segment_next (seg);
            assert (seg);
        }
    }
    generation_allocation_segment (gen) = seg;
}

void gc_heap::reset_allocation_pointers (generation* gen, BYTE* start)
{
    assert (start);
    assert (Align ((size_t)start) == (size_t)start);
    generation_allocation_start (gen) = start;
    generation_allocation_pointer (gen) =  0;//start + Align (min_obj_size);
    generation_allocation_limit (gen) = 0;//generation_allocation_pointer (gen);
    set_allocation_heap_segment (gen);

}

inline
ptrdiff_t  gc_heap::get_new_allocation (int gen_number)
{
    return dd_new_allocation (dynamic_data_of (gen_number));
}

void gc_heap::ensure_new_allocation (int size)
{
    if (dd_new_allocation (dynamic_data_of (0)) <= size)
        dd_new_allocation (dynamic_data_of (0)) = size+Align(1);
}

size_t gc_heap::deque_pinned_plug ()
{
    size_t m = mark_stack_bos;
    mark_stack_bos++;
    return m;
}

inline
mark* gc_heap::pinned_plug_of (size_t bos)
{
    return &mark_stack_array [ bos ];
}

inline
mark* gc_heap::oldest_pin ()
{
    return pinned_plug_of (mark_stack_bos);
}

inline
BOOL gc_heap::pinned_plug_que_empty_p ()
{
    return (mark_stack_bos == mark_stack_tos);
}

inline
BYTE* pinned_plug (mark* m)
{
   return mark_first (m);
}

inline
size_t& pinned_len (mark* m)
{
    return m->len;
}


inline
mark* gc_heap::before_oldest_pin()
{
    if (mark_stack_bos >= 1)
        return pinned_plug_of (mark_stack_bos-1);
    else
        return 0;
}

inline
BOOL gc_heap::ephemeral_pointer_p (BYTE* o)
{
    return ((o >= ephemeral_low) && (o < ephemeral_high));
}

void gc_heap::make_mark_stack (mark* arr)
{
    mark_stack_tos = 0;
    mark_stack_bos = 0;
    mark_stack_array = arr;
    mark_stack_array_length = 100;
}


#define brick_size 2048

inline
size_t gc_heap::brick_of (BYTE* add)
{
    return (size_t)(add - lowest_address) / brick_size;
}

inline
BYTE* gc_heap::brick_address (size_t brick)
{
    return lowest_address + (brick_size * brick);
}


void gc_heap::clear_brick_table (BYTE* from, BYTE* end)
{
    for (size_t i = brick_of (from);i < brick_of (end); i++)
        brick_table[i] = -32768;
}


inline
void gc_heap::set_brick (size_t index, ptrdiff_t val)
{
    if (val < -32767)
        val = -32767;
    assert (val < 32767);
    brick_table [index] = (short)val;
}

inline
BYTE* align_on_brick (BYTE* add)
{
    return (BYTE*)((size_t)(add + brick_size - 1) & - (brick_size));
}

inline
BYTE* align_lower_brick (BYTE* add)
{
    return (BYTE*)(((size_t)add) & - (brick_size));
}

size_t size_brick_of (BYTE* from, BYTE* end)
{
    assert (((size_t)from & (brick_size-1)) == 0);
    assert (((size_t)end  & (brick_size-1)) == 0);

    return ((end - from) / brick_size) * sizeof (short);
}



#define card_size 128

#define card_word_width 32

inline
BYTE* gc_heap::card_address (size_t card)
{
    return  lowest_address + card_size * card;
}

inline
size_t gc_heap::card_of ( BYTE* object)
{
    return (size_t)(object - lowest_address) / card_size;
}

inline
size_t gcard_of ( BYTE* object)
{
    return (size_t)(object - g_lowest_address) / card_size;
}



inline
size_t card_word (size_t card)
{
    return card / card_word_width;
}

inline
unsigned card_bit (size_t card)
{
    return (unsigned)(card % card_word_width);
}

inline
size_t gc_heap::card_to_brick (size_t card)
{
    return brick_of (card_address (card));
}

inline
BYTE* align_on_card (BYTE* add)
{
    return (BYTE*)((size_t)(add + card_size - 1) & - (card_size));
}
inline
BYTE* align_on_card_word (BYTE* add)
{
    return (BYTE*) ((size_t)(add + (card_size*card_word_width)-1) & -(card_size*card_word_width));
}

inline
BYTE* align_lower_card (BYTE* add)
{
    return (BYTE*)((size_t)add & - (card_size));
}

inline
void gc_heap::clear_card (size_t card)
{
    card_table [card_word (card)] =
        (card_table [card_word (card)] & ~(1 << card_bit (card)));
    dprintf (3,("Cleared card %x [%x, %x[", card, (size_t)card_address (card),
              (size_t)card_address (card+1)));
}

inline
void gc_heap::set_card (size_t card)
{
    card_table [card_word (card)] =
        (card_table [card_word (card)] | (1 << card_bit (card)));
}

inline
void gset_card (size_t card)
{
    g_card_table [card_word (card)] |= (1 << card_bit (card));
}

inline
BOOL  gc_heap::card_set_p (size_t card)
{
    return ( card_table [ card_word (card) ] & (1 << card_bit (card)));
}

inline
void gc_heap::card_table_set_bit (BYTE* location)
{
    set_card (card_of (location));
}

size_t size_card_of (BYTE* from, BYTE* end)
{
    assert (((size_t)from & ((card_size*card_word_width)-1)) == 0);
    assert (((size_t)end  & ((card_size*card_word_width)-1)) == 0);

    return ((end - from) / (card_size*card_word_width)) * sizeof (DWORD);
}

class card_table_info
{
public:
    unsigned    recount;
    BYTE*       lowest_address;
    BYTE*       highest_address;
    short*      brick_table;



    DWORD*      next_card_table;
};



//These are accessors on untranslated cardtable
inline 
unsigned& card_table_refcount (DWORD* c_table)
{
    return *(unsigned*)((char*)c_table - sizeof (card_table_info));
}

inline 
BYTE*& card_table_lowest_address (DWORD* c_table)
{
    return ((card_table_info*)((BYTE*)c_table - sizeof (card_table_info)))->lowest_address;
}

inline 
BYTE*& card_table_highest_address (DWORD* c_table)
{
    return ((card_table_info*)((BYTE*)c_table - sizeof (card_table_info)))->highest_address;
}

inline 
short*& card_table_brick_table (DWORD* c_table)
{
    return ((card_table_info*)((BYTE*)c_table - sizeof (card_table_info)))->brick_table;
}



//These work on untranslated card tables
inline 
DWORD*& card_table_next (DWORD* c_table)
{
    return ((card_table_info*)((BYTE*)c_table - sizeof (card_table_info)))->next_card_table;
}

inline 
size_t old_card_of (BYTE* object, DWORD* c_table)
{
    return (size_t) (object - card_table_lowest_address (c_table))/card_size;
}
void own_card_table (DWORD* c_table)
{
    card_table_refcount (c_table) += 1;
}

void destroy_card_table (DWORD* c_table);

void delete_next_card_table (DWORD* c_table)
{
    DWORD* n_table = card_table_next (c_table);
    if (n_table)
    {
        if (card_table_next (n_table))
        {
            delete_next_card_table (n_table);
        }
        if (card_table_refcount (n_table) == 0)
        {
            destroy_card_table (n_table);
            card_table_next (c_table) = 0;
        }
    }
}

void release_card_table (DWORD* c_table)
{
    assert (card_table_refcount (c_table) >0);
    card_table_refcount (c_table) -= 1;
    if (card_table_refcount (c_table) == 0)
    {
        delete_next_card_table (c_table);
        if (card_table_next (c_table) == 0)
        {
            destroy_card_table (c_table);
            // sever the link from the parent
            if (g_card_table == c_table)
                g_card_table = 0;
            DWORD* p_table = g_card_table;
            if (p_table)
            {
                while (p_table && (card_table_next (p_table) != c_table))
                    p_table = card_table_next (p_table);
                card_table_next (p_table) = 0;
            }
            
        }
    }
}


void destroy_card_table (DWORD* c_table)
{


//  delete (DWORD*)&card_table_refcount(c_table);
    VirtualFree (&card_table_refcount(c_table), 0, MEM_RELEASE);
}


DWORD* make_card_table (BYTE* start, BYTE* end)
{


    assert (g_lowest_address == start);
    assert (g_highest_address == end);

    size_t bs = size_brick_of (start, end);
    size_t cs = size_card_of (start, end);
    size_t ms = 0;

    WS_PERF_SET_HEAP(GC_HEAP);
    DWORD* ct = (DWORD*)VirtualAlloc (0, sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)),
                                      MEM_COMMIT, PAGE_READWRITE);

    if (!ct)
        return 0;

    WS_PERF_LOG_PAGE_RANGE(0, ct, (unsigned char *)ct + sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)) - OS_PAGE_SIZE, sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)));
    WS_PERF_UPDATE("GC:make_card_table", bs + cs + ms + sizeof (card_table_info), ct);

    // initialize the ref count
    ct = (DWORD*)((BYTE*)ct+sizeof (card_table_info));
    card_table_refcount (ct) = 0;
    card_table_lowest_address (ct) = start;
    card_table_highest_address (ct) = end;
    card_table_brick_table (ct) = (short*)((BYTE*)ct + cs);
    card_table_next (ct) = 0;
/*
    //clear the card table 
    memclr ((BYTE*)ct, cs);
*/



    return ct;
}


//returns 0 for success, -1 otherwise

int gc_heap::grow_brick_card_tables (BYTE* start, BYTE* end)
{
    BYTE* la = g_lowest_address;
    BYTE* ha = g_highest_address;
    g_lowest_address = min (start, g_lowest_address);
    g_highest_address = max (end, g_highest_address);
    // See if the address is already covered
    if ((la != g_lowest_address ) || (ha != g_highest_address))
    {
        DWORD* ct = 0;
        short* bt = 0;


        size_t cs = size_card_of (g_lowest_address, g_highest_address);
        size_t bs = size_brick_of (g_lowest_address, g_highest_address);

        size_t ms = 0;

        WS_PERF_SET_HEAP(GC_HEAP);
//      ct = (DWORD*)new (BYTE[cs + bs + sizeof(card_table_info)]);
        ct = (DWORD*)VirtualAlloc (0, sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)),
                                      MEM_COMMIT, PAGE_READWRITE);


        if (!ct)
            goto fail;
        
        WS_PERF_LOG_PAGE_RANGE(0, ct, (unsigned char *)ct + sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)) - OS_PAGE_SIZE, sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)));
        WS_PERF_UPDATE("GC:gc_heap:grow_brick_card_tables", cs + bs + ms + sizeof(card_table_info), ct);

        ct = (DWORD*)((BYTE*)ct + sizeof (card_table_info));
        card_table_refcount (ct) = 0;
        card_table_lowest_address (ct) = g_lowest_address;
        card_table_highest_address (ct) = g_highest_address;
        card_table_next (ct) = g_card_table;

        //clear the card table 
/*
        memclr ((BYTE*)ct, 
                (((g_highest_address - g_lowest_address)*sizeof (DWORD) / 
                  (card_size * card_word_width))
                 + sizeof (DWORD)));
*/

        bt = (short*)((BYTE*)ct + cs);

        // No initialization needed, will be done in copy_brick_card
#ifdef INTERIOR_POINTERS
        //But for interior pointers the entire brick table 
        //needs to be initialized. 
        {
            for (int i = 0;i < ((g_highest_address - g_lowest_address) / brick_size); i++)
            bt[i] = -32768;
        }

#endif //INTERIOR_POINTERS
        card_table_brick_table (ct) = bt;



        g_card_table = ct;
        // This passes a bool telling whether we need to switch to the post
        // grow version of the write barrier.  This test tells us if the new
        // segment was allocated at a lower address than the old, requiring
        // that we start doing an upper bounds check in the write barrier.
        StompWriteBarrierResize(la != g_lowest_address);
        return 0;
    fail:
        //cleanup mess and return -1;
        g_lowest_address = la;
        g_highest_address = ha;

        if (ct)
        {
            //delete (DWORD*)((BYTE*)ct - sizeof(card_table_info));
            VirtualFree (((BYTE*)ct - sizeof(card_table_info)), 0, MEM_RELEASE);
        }


        return -1;
    }
    return 0;


}

//copy all of the arrays managed by the card table for a page aligned range
void gc_heap::copy_brick_card_range (BYTE* la, DWORD* old_card_table,

                                     short* old_brick_table,
                                     heap_segment* seg,
                                     BYTE* start, BYTE* end, BOOL heap_expand)
{
    ptrdiff_t brick_offset = brick_of (start) - brick_of (la);


    dprintf (2, ("copying tables for range [%x %x[", (size_t)start, (size_t)end)); 
        
    // copy brick table
    short* brick_start = &brick_table [brick_of (start)];
    if (old_brick_table)
    {
        // segments are always on page boundaries 
        memcpy (brick_start, &old_brick_table[brick_offset], 
                ((end - start)/brick_size)*sizeof (short));

    }
    else 
    {
        assert (seg == 0);
        // This is a large heap, just clear the brick table
        clear_brick_table (start, end);
    }




    // n way merge with all of the card table ever used in between
    DWORD* c_table = card_table_next (card_table);
    assert (c_table);
    while (card_table_next (old_card_table) != c_table)
    {
        //copy if old card table contained [start, end[ 
        if ((card_table_highest_address (c_table) >= end) &&
            (card_table_lowest_address (c_table) <= start))
        {
            // or the card_tables
            DWORD* dest = &card_table [card_word (card_of (start))];
            DWORD* src = &c_table [card_word (old_card_of (start, c_table))];
            ptrdiff_t count = ((end - start)/(card_size*card_word_width));
            for (int x = 0; x < count; x++)
            {
                *dest |= *src;
                dest++;
                src++;
            }
        }
        c_table = card_table_next (c_table);
    }

}
void gc_heap::copy_brick_card_table(BOOL heap_expand) 
{


    BYTE* la = lowest_address;
    BYTE* ha = highest_address;
    DWORD* old_card_table = card_table;
    short* old_brick_table = brick_table;


    assert (la == card_table_lowest_address (old_card_table));
    assert (ha == card_table_highest_address (old_card_table));

    /* todo: Need a global lock for this */
    own_card_table (g_card_table);
    card_table = g_card_table;
    /* End of global lock */    

    highest_address = card_table_highest_address (card_table);
    lowest_address = card_table_lowest_address (card_table);

    brick_table = card_table_brick_table (card_table);





    // for each of the segments and heaps, copy the brick table and 
    // or the card table 
    heap_segment* seg = generation_start_segment (generation_of (max_generation));
    while (seg)
    {
        BYTE* end = align_on_page (heap_segment_allocated (seg));
        copy_brick_card_range (la, old_card_table, 
                               old_brick_table, seg,
                               (BYTE*)seg, end, heap_expand);
        seg = heap_segment_next (seg);
    }

    copy_brick_card_table_l_heap();

    release_card_table (old_card_table);

    

}

void gc_heap::copy_brick_card_table_l_heap ()
{

    if (lheap_card_table != g_card_table)
    {

        DWORD* old_card_table = lheap_card_table;

        BYTE* la = card_table_lowest_address (old_card_table);



        // Do the same thing for l_heaps 
        l_heap* h = lheap;
        while (h)
        {
            BYTE* hm = (BYTE*)l_heap_heap (h);
            BYTE* end = hm + align_on_page (l_heap_size (h));
            copy_brick_card_range (la, old_card_table, 
                                   0, 0,
                                   hm, end, FALSE);
            h = l_heap_next (h);
        }
        lheap_card_table = g_card_table;
    }
}
#ifdef MARK_LIST

BYTE** make_mark_list (size_t size)
{
    WS_PERF_SET_HEAP(GC_HEAP);
    BYTE** mark_list = new BYTE* [size];
    WS_PERF_UPDATE("GC:make_mark_list", sizeof(BYTE*) * size, mark_list);
    return mark_list;
}

#define swap(a,b){BYTE* t; t = a; a = b; b = t;}

void qsort1( BYTE* *low, BYTE* *high)
{
    if ((low + 16) >= high)
    {
        //insertion sort
        BYTE **i, **j;
        for (i = low+1; i <= high; i++)
        {
            BYTE* val = *i;
            for (j=i;j >low && val<*(j-1);j--)
            {
                *j=*(j-1);
            }
            *j=val;
        }
    }
    else
    {
        BYTE *pivot, **left, **right;

        //sort low middle and high
        if (*(low+((high-low)/2)) < *low)
            swap (*(low+((high-low)/2)), *low);
        if (*high < *low)
            swap (*low, *high);
        if (*high < *(low+((high-low)/2)))
            swap (*(low+((high-low)/2)), *high);
            
        swap (*(low+((high-low)/2)), *(high-1));
        pivot =  *(high-1); 
        left = low; right = high-1;
        while (1) {
            while (*(--right) > pivot);
            while (*(++left)  < pivot);
            if (left < right)
            {
                swap(*left, *right);
            }
            else
                break;
        }
        swap (*left, *(high-1));
        qsort1(low, left-1);
        qsort1(left+1, high);
    }
}

#endif //MARK_LIST

#define header(i) ((CObjectHeader*)(i))


//Depending on the implementation of the mark and pin bit, it can 
//be more efficient to clear both of them at the same time, 
//or it can be better to clear the pinned bit only on pinned objects
//The code calls both clear_pinned(o) and clear_marked_pinned(o)
//but only one implementation will clear the pinned bit


#define marked(i) header(i)->IsMarked()
#define set_marked(i) header(i)->SetMarked()
#define clear_marked_pinned(i) header(i)->ClearMarkedPinned()
#define pinned(i) header(i)->IsPinned()
#define set_pinned(i) header(i)->SetPinned()

#define clear_pinned(i)



inline DWORD my_get_size (Object* ob)                          
{ 
    MethodTable* mT = ob->GetMethodTable();
    mT = (MethodTable *) (((ULONG_PTR) mT) & ~3);
    return (mT->GetBaseSize() + 
            (mT->GetComponentSize()? 
             (ob->GetNumComponents() * mT->GetComponentSize()) : 0));
}



//#define size(i) header(i)->GetSize()
#define size(i) my_get_size (header(i))

#define contain_pointers(i) header(i)->ContainsPointers()


BOOL gc_heap::is_marked (BYTE* o)
{
    return marked (o);
}


// return the generation number of an object.
// It is assumed that the object is valid.
unsigned int gc_heap::object_gennum (BYTE* o)
{
    if ((o < heap_segment_reserved (ephemeral_heap_segment)) && 
        (o >= heap_segment_mem (ephemeral_heap_segment)))
    {
        // in an ephemeral generation.
        // going by decreasing population volume
        for (unsigned int i = max_generation; i > 0 ; i--)
        {
            if ((o < generation_allocation_start (generation_of (i-1))))
                return i;
        }
        return 0;
    }
    else
        return max_generation;
}


heap_segment* gc_heap::make_heap_segment (BYTE* new_pages, size_t size)
{
    void * res;

    size_t initial_commit = OS_PAGE_SIZE;

    WS_PERF_SET_HEAP(GC_HEAP);      
    //Commit the first page
    if ((res = VirtualAlloc (new_pages, initial_commit, 
                              MEM_COMMIT, PAGE_READWRITE)) == 0)
        return 0;
    WS_PERF_UPDATE("GC:gc_heap:make_heap_segment", initial_commit, res);

    //overlay the heap_segment
    heap_segment* new_segment = (heap_segment*)new_pages;
    heap_segment_mem (new_segment) = new_pages + Align (sizeof (heap_segment));
    heap_segment_reserved (new_segment) = new_pages + size;
    heap_segment_committed (new_segment) = new_pages + initial_commit;
    heap_segment_next (new_segment) = 0;
    heap_segment_plan_allocated (new_segment) = heap_segment_mem (new_segment);
    heap_segment_allocated (new_segment) = heap_segment_mem (new_segment);
    heap_segment_used (new_segment) = heap_segment_allocated (new_segment);



    dprintf (2, ("Creating heap segment %x", (size_t)new_segment));
    return new_segment;
}

l_heap* gc_heap::make_large_heap (BYTE* new_pages, size_t size, BOOL managed)
{

    l_heap* new_heap = new l_heap();
    if (!new_heap)
        return 0;
    l_heap_size (new_heap) = size;
    l_heap_next (new_heap) = 0;
    l_heap_heap (new_heap) = new_pages;
    l_heap_flags (new_heap) = (size_t)new_pages | (managed ? L_MANAGED : 0);
    dprintf (2, ("Creating large heap %x", (size_t)new_heap));
    return new_heap;
}


void gc_heap::delete_large_heap (l_heap* hp)
{

    l_heap* h = hp;
    do 
    {
        BYTE* hphp = (BYTE*)l_heap_heap (h);


        //for now, also decommit the non heap-managed heaps. 
        //This is conservative because the whole heap may not be committed. 
        VirtualFree (hphp, l_heap_size (h), MEM_DECOMMIT);


        h = l_heap_next (h);

    } while (h);
    
    seg_manager->release_heap (hp);

}

//Releases the segment to the OS.

void gc_heap::delete_heap_segment (heap_segment* seg)
{
    dprintf (2, ("Destroying segment [%x, %x[", (size_t)seg,
                 (size_t)heap_segment_reserved (seg)));

    VirtualFree (seg, heap_segment_committed(seg) - (BYTE*)seg, MEM_DECOMMIT);

    seg_manager->release_segment (seg);

}

//resets the pages beyond alloctes size so they won't be swapped out and back in

void gc_heap::reset_heap_segment_pages (heap_segment* seg)
{
}


void gc_heap::decommit_heap_segment_pages (heap_segment* seg)
{
    BYTE*  page_start = align_on_page (heap_segment_allocated (seg));
    size_t size = heap_segment_committed (seg) - page_start;
    if (size >= 100*OS_PAGE_SIZE){
        page_start += 32*OS_PAGE_SIZE;
        size -= 32*OS_PAGE_SIZE;
        VirtualFree (page_start, size, MEM_DECOMMIT);
        heap_segment_committed (seg) = page_start;
        if (heap_segment_used (seg) > heap_segment_committed (seg))
            heap_segment_used (seg) = heap_segment_committed (seg);

    }
}

void gc_heap::rearrange_heap_segments()
{
    heap_segment* seg =
        generation_start_segment (generation_of (max_generation));
    heap_segment* prev_seg = 0;
    heap_segment* next_seg = 0;
    while (seg && (seg != ephemeral_heap_segment))
    {
        next_seg = heap_segment_next (seg);

        // check if the segment was reached by allocation
        if (heap_segment_plan_allocated (seg) ==
            heap_segment_mem (seg))
        {
            //if not, unthread and delete
            assert (prev_seg);
            heap_segment_next (prev_seg) = next_seg;
            delete_heap_segment (seg);
        }
        else
        {
            heap_segment_allocated (seg) =
                heap_segment_plan_allocated (seg);
            // reset the pages between allocated and committed.
            decommit_heap_segment_pages (seg);
            prev_seg = seg;

        }

        seg = next_seg;
    }
    //Heap expansion, thread the ephemeral segment.
    if (prev_seg && !seg)
    {
        prev_seg->next = ephemeral_heap_segment;
    }
}






void gc_heap::make_generation (generation& gen, heap_segment* seg,
                      BYTE* start, BYTE* pointer)
{
    gen.allocation_start = start;
    gen.allocation_context.alloc_ptr = pointer;
    gen.allocation_context.alloc_limit = pointer;
    gen.free_list = 0;
    gen.start_segment = seg;
    gen.allocation_segment = seg;
    gen.last_gap = 0;
    gen.plan_allocation_start = 0;
    gen.free_list_space = 0;
    gen.allocation_size = 0;
}



void gc_heap::adjust_ephemeral_limits ()
{
    ephemeral_low = generation_allocation_start (generation_of ( max_generation -1));
    ephemeral_high = heap_segment_reserved (ephemeral_heap_segment);

    // This updates the write barrier helpers with the new info.
    StompWriteBarrierEphemeral();
}

HRESULT gc_heap::initialize_gc (size_t segment_size,
                                size_t heap_size
)
{


    HRESULT hres = S_OK;



#if 1 //def LOW_MEM_NOTIFICATION
    low_mem_api_supported();
#endif //LOW_MEM_NOTIFICATION

    reserved_memory = 0;

    reserved_memory_limit = segment_size + heap_size;

    seg_manager = create_segment_manager (segment_size,
                                          heap_size, 
                                          1,
                                          1);
    if (!seg_manager)
        return E_OUTOFMEMORY;

    g_card_table = make_card_table (g_lowest_address, g_highest_address);

    if (!g_card_table)
        return E_OUTOFMEMORY;






#ifdef TRACE_GC
    print_level = g_pConfig->GetGCprnLvl();
#endif


    init_semi_shared();
    
    return hres;
}


//Initializes PER_HEAP_ISOLATED data members.
int
gc_heap::init_semi_shared()
{

    ptrdiff_t hdelta;

    lheap = seg_manager->get_heap (hdelta);
    if (!lheap)
        return 0;

    lheap_card_table = g_card_table;

    gheap = new  gmallocHeap;
    if (!gheap)
        return 0;

    gheap->Init ("GCHeap", (DWORD*)l_heap_heap (lheap), 
                 (unsigned long)l_heap_size (lheap), heap_grow_hook, 
                 heap_pregrow_hook);


#ifdef MARK_LIST
    size_t gen0size = GCHeap::GetValidGen0MaxSize(GCHeap::GetValidSegmentSize());


    mark_list_size = gen0size / 400;
    g_mark_list = make_mark_list (mark_list_size);


    dprintf (3, ("gen0 size: %d, mark_list_size: %d",
                 gen0size, mark_list_size));

    if (!g_mark_list)
        return 0;
#endif //MARK_LIST



    return 1;
}

gc_heap* gc_heap::make_gc_heap (
                                )
{
    gc_heap* res = 0;


    if (res->init_gc_heap ()==0)
    {
        return 0;
    }


    return (gc_heap*)1;

    
}


int
gc_heap::init_gc_heap ()
{


    ptrdiff_t sdelta;
    heap_segment* seg = seg_manager->get_segment (sdelta);
    if (!seg)
        return 0;





    /* todo: Need a global lock for this */
    own_card_table (g_card_table);
    card_table = g_card_table;
    /* End of global lock */    

    brick_table = card_table_brick_table (g_card_table);
    highest_address = card_table_highest_address (g_card_table);
    lowest_address = card_table_lowest_address (g_card_table);

#ifndef INTERIOR_POINTERS
    //set the brick_table for large objects
    clear_brick_table ((BYTE*)l_heap_heap (lheap), 
                       (BYTE*)l_heap_heap (lheap) + l_heap_size (lheap));

#else //INTERIOR_POINTERS

    //Because of the interior pointer business, we have to clear 
    //the whole brick table
    //TODO: remove this code when code manager is fixed. 
    clear_brick_table (lowest_address, highest_address);
#endif //INTERIOR_POINTERS



    BYTE*  start = heap_segment_mem (seg);

    for (int i = 0; i < 1 + max_generation; i++)
    {
        make_generation (generation_table [ (max_generation - i) ],
                         seg, start, 0);
        start += Align (min_obj_size); 
    }

    heap_segment_allocated (seg) = start;
    alloc_allocated = start;

    ephemeral_heap_segment = seg;

    for (int gen_num = 0; gen_num < 1 + max_generation; gen_num++)
    {
        generation*  gen = generation_of (gen_num);
        make_unused_array (generation_allocation_start (gen), Align (min_obj_size));
    }


    init_dynamic_data();

    mark* arr = new (mark [100]);
    if (!arr)
        return 0;

    make_mark_stack(arr);

    adjust_ephemeral_limits();




    HRESULT hr = AllocateCFinalize(&finalize_queue);
    if (FAILED(hr))
        return 0;

    return 1;
}

void 
gc_heap::destroy_semi_shared()
{
    delete gheap;
    
    delete_large_heap (lheap);

    

#ifdef MARK_LIST
    if (g_mark_list)
        delete g_mark_list;
#endif //MARK_LIST


    
}


void
gc_heap::self_destroy()
{

    // destroy every segment.
    heap_segment* seg = generation_start_segment (generation_of (max_generation));
    heap_segment* next_seg;
    while (seg)
    {
        next_seg = heap_segment_next (seg);
        delete_heap_segment (seg);
        seg = next_seg;
    }

    // get rid of the card table
    release_card_table (card_table);

    // destroy the mark stack

    delete mark_stack_array;

    if (finalize_queue)
        delete finalize_queue;


    
}

void 
gc_heap::destroy_gc_heap(gc_heap* heap)
{
    heap->self_destroy();
    delete heap;
}

// Destroys resources owned by gc. It is assumed that a last GC has been performed and that
// the finalizer queue has been drained.
void gc_heap::shutdown_gc()
{
    destroy_semi_shared();


    //destroy seg_manager
    destroy_segment_manager (seg_manager);


}


//In the concurrent version, the Enable/DisablePreemptiveGC is optional because
//the gc thread call WaitLonger.
void WaitLonger (int i)
{
    // every 8th attempt:
    Thread *pCurThread = GetThread();
    BOOL bToggleGC = FALSE;

    {
        bToggleGC = pCurThread->PreemptiveGCDisabled();
        if (bToggleGC)
            pCurThread->EnablePreemptiveGC();    
    }

    if  (g_SystemInfo.dwNumberOfProcessors > 1)
    {
		pause();			// indicate to the processor that we are spining 
        if  (i & 0x01f)
            __SwitchToThread (0);
        else
            __SwitchToThread (5);
    }
    else
        __SwitchToThread (5);


    {
        if (bToggleGC)
            pCurThread->DisablePreemptiveGC();
    }
}

#ifdef  MP_LOCKS
inline
static void enter_spin_lock (volatile long* lock)
{
retry:

    if (FastInterlockExchange ((long *)lock, 0) >= 0)
    {
        unsigned int i = 0;
        while (*lock >= 0)
        {
            if ((++i & 7) && !GCHeap::IsGCInProgress())
            {
                if  (g_SystemInfo.dwNumberOfProcessors > 1)
                {
                    for (int j = 0; j < 1000; j++)
                    {
                        if  (*lock < 0 || GCHeap::IsGCInProgress())
                            break;
						pause();			// indicate to the processor that we are spining 
                    }
                    if  (*lock >= 0 && !GCHeap::IsGCInProgress())
                        ::SwitchToThread();
                }
                else
                    ::SwitchToThread();
            }
            else
            {
                WaitLonger(i);
            }
        }
        goto retry;
    }
}
#else
inline
static void enter_spin_lock (long* lock)
{
retry:

    if (FastInterlockExchange (lock, 0) >= 0)
    {
        unsigned int i = 0;
        while (*lock >= 0)
        {
            if (++i & 7)
                __SwitchToThread (0);
            else
            {
                WaitLonger(i);
            }
        }
        goto retry;
    }
}
#endif

inline
static void leave_spin_lock(long *lock) 
{
    *lock = -1;
}


#ifdef _DEBUG

inline
static void enter_spin_lock(GCDebugSpinLock *pSpinLock) {
    enter_spin_lock(&pSpinLock->lock);
    pSpinLock->holding_thread = GetThread();
}

inline
static void leave_spin_lock(GCDebugSpinLock *pSpinLock) {
    _ASSERTE(pSpinLock->holding_thread == GetThread());
    pSpinLock->holding_thread = (Thread*) -1;
    if (pSpinLock->lock != -1)
        leave_spin_lock(&pSpinLock->lock);
}

#define ASSERT_HOLDING_SPIN_LOCK(pSpinLock) \
    _ASSERTE((pSpinLock)->holding_thread == GetThread());

#else

#define ASSERT_HOLDING_SPIN_LOCK(pSpinLock)

#endif


//BUGBUG these function should not be static. and the 
//gmheap object should keep some context. 
int gc_heap::heap_pregrow_hook (size_t memsize)
{
    if ((gc_heap::reserved_memory + memsize) > gc_heap::reserved_memory_limit)
    {
        gc_heap::reserved_memory_limit = 
            CNameSpace::AskForMoreReservedMemory (gc_heap::reserved_memory_limit, memsize);
        if ((gc_heap::reserved_memory + memsize) > gc_heap::reserved_memory_limit)
            return E_OUTOFMEMORY;
    }

    gc_heap::reserved_memory += memsize;

    return 0;
}

int gc_heap::heap_grow_hook (BYTE* mem, size_t memsize, ptrdiff_t delta)
{
    int hres = 0;


    l_heap* new_heap = gc_heap::make_large_heap ((BYTE*)mem, memsize, FALSE);
    if (!new_heap)
    {
        hres = E_OUTOFMEMORY;
        return hres;
    }


    if (lheap)
        l_heap_next (new_heap) = lheap;
    
    lheap = new_heap;
    
    
    hres = grow_brick_card_tables ((BYTE*)mem, (BYTE*)mem + memsize);

    return hres;
}




inline
BOOL gc_heap::size_fit_p (size_t size, BYTE* alloc_pointer, BYTE* alloc_limit)
{
    return ((alloc_pointer + size + Align(min_obj_size)) <= alloc_limit) ||
            ((alloc_pointer + size) == alloc_limit);
}

inline
BOOL gc_heap::a_size_fit_p (size_t size, BYTE* alloc_pointer, BYTE* alloc_limit)
{
    return ((alloc_pointer + size + Align(min_obj_size)) <= alloc_limit) ||
            ((alloc_pointer + size) == alloc_limit);
}

// Grow by committing more pages
int gc_heap::grow_heap_segment (heap_segment* seg, size_t size)
{
    
    assert (size == align_on_page (size));

    size_t c_size = max (size, 16*OS_PAGE_SIZE);

    c_size = min (c_size, (size_t)(heap_segment_reserved (seg) -
                                   heap_segment_committed (seg)));
    if (c_size == 0)
        return 0;
    assert (c_size >= size);
    WS_PERF_SET_HEAP(GC_HEAP);      
    if (!VirtualAlloc (heap_segment_committed (seg), c_size,
                       MEM_COMMIT, PAGE_READWRITE))
    {
        return 0;
    }
    WS_PERF_UPDATE("GC:gc_heap:grow_heap_segment", c_size, heap_segment_committed (seg));
    assert (heap_segment_committed (seg) <= 
            heap_segment_reserved (seg));
    heap_segment_committed (seg) += c_size;
    return 1;

}

//used only in older generation allocation (i.e during gc). 
void gc_heap::adjust_limit (BYTE* start, size_t limit_size, generation* gen, 
                            int gennum)
{
    dprintf(3,("gc Expanding segment allocation"));
    if (generation_allocation_limit (gen) != start)
    {
        BYTE*  hole = generation_allocation_pointer (gen);
        size_t  size = (generation_allocation_limit (gen) - generation_allocation_pointer (gen));
        if (size != 0)
        {
            dprintf(3,("gc filling up hole"));
            make_unused_array (hole, size);
            //increment the fragmentation since size isn't used.
            dd_fragmentation (dynamic_data_of (gennum)) += size;
        }
        generation_allocation_pointer (gen) = start;
    }
    generation_allocation_limit (gen) = (start + limit_size);
}


void gc_heap::adjust_limit_clr (BYTE* start, size_t limit_size, 
                                alloc_context* acontext, heap_segment* seg)
{
    assert (seg == ephemeral_heap_segment);
    assert (heap_segment_used (seg) <= heap_segment_committed (seg));

    dprintf(3,("Expanding segment allocation [%x, %x[", (size_t)start, 
               (size_t)start + limit_size - Align (min_obj_size)));
    if ((acontext->alloc_limit != start) &&
        (acontext->alloc_limit + Align (min_obj_size))!= start)
    {
        BYTE*  hole = acontext->alloc_ptr;
        if (hole != 0)
        {
            size_t  size = (acontext->alloc_limit - acontext->alloc_ptr);
            dprintf(3,("filling up hole [%x, %x[", (size_t)hole, (size_t)hole + size + Align (min_obj_size)));
            // when we are finishing an allocation from a free list
            // we know that the free area was Align(min_obj_size) larger
            make_unused_array (hole, size + Align (min_obj_size));
        }
        acontext->alloc_ptr = start;
    }
    acontext->alloc_limit = (start + limit_size - Align (min_obj_size));
    acontext->alloc_bytes += limit_size;

#ifdef FFIND_OBJECT
    if (gen0_must_clear_bricks > 0)
    {
        //set the brick table to speed up find_object
        size_t b = brick_of (acontext->alloc_ptr);
        set_brick (b++, acontext->alloc_ptr - brick_address (b));
        dprintf (3, ("Allocation Clearing bricks [%x, %x[", 
                     b, brick_of (align_on_brick (start + limit_size))));
        short* x = &brick_table [b];
        short* end_x = &brick_table [brick_of (align_on_brick (start + limit_size))];
        
        for (;x < end_x;x++)
            *x = -1;
    }
    else
#endif //FFIND_OBJECT
    {
        gen0_bricks_cleared = FALSE;
    }

    //Sometimes the allocated size is advanced without clearing the 
    //memory. Let's catch up here
    if (heap_segment_used (seg) < alloc_allocated)
    {
        heap_segment_used (seg) = alloc_allocated;

    }
    if ((start + limit_size) <= heap_segment_used (seg))
    {
        leave_spin_lock (&more_space_lock);
        memclr (start - plug_skew, limit_size);
    }
    else
    {
        BYTE* used = heap_segment_used (seg);
        heap_segment_used (seg) = start + limit_size;

        leave_spin_lock (&more_space_lock);
        memclr (start - plug_skew, used - start);
    }

}

/* in order to make the allocator faster, allocate returns a 
 * 0 filled object. Care must be taken to set the allocation limit to the 
 * allocation pointer after gc
 */

size_t gc_heap::limit_from_size (size_t size, size_t room)
{
#pragma warning(disable:4018)
    return new_allocation_limit ((size + Align (min_obj_size)),
                                 min (room,max (size + Align (min_obj_size), 
                                                allocation_quantum)));
#pragma warning(default:4018)
}




BOOL gc_heap::allocate_more_space (alloc_context* acontext, size_t size)
{
    generation* gen = youngest_generation;
    enter_spin_lock (&more_space_lock);
    {
        BOOL ran_gc = FALSE;
        if ((get_new_allocation (0) <= 0))
        {
            vm_heap->GarbageCollectGeneration();
        }
    try_free_list:
        {
#ifdef FREE_LIST_0
            BYTE* free_list = generation_free_list (gen);
// dformat (t, 3, "considering free area %x ", free_list);
            if (0 == free_list)
#endif //FREE_LIST_0
            {
                heap_segment* seg = generation_allocation_segment (gen);
                if (a_size_fit_p (size, alloc_allocated,
                                  heap_segment_committed (seg)))
                {
                    size_t limit = limit_from_size (size, (heap_segment_committed (seg) - 
                                                           alloc_allocated));
                    BYTE* old_alloc = alloc_allocated;
                    alloc_allocated += limit;
                    adjust_limit_clr (old_alloc, limit, acontext, seg);
                }
                else if (a_size_fit_p (size, alloc_allocated,
                                       heap_segment_reserved (seg)))
                {
                    size_t limit = limit_from_size (size, (heap_segment_reserved (seg) -
                                                           alloc_allocated));
                    if (!grow_heap_segment (seg,
                                            align_on_page (alloc_allocated + limit) - 
                                 heap_segment_committed(seg)))
                    {
                        // assert (!"Memory exhausted during alloc");
                        leave_spin_lock (&more_space_lock);
                        return 0;
                    }
                    BYTE* old_alloc = alloc_allocated;
                    alloc_allocated += limit;
                    adjust_limit_clr (old_alloc, limit, acontext, seg);
                }
                else
                {
                    dprintf (2, ("allocate more space: No space to allocate"));
                    if (!ran_gc)
                    {
                        dprintf (2, ("Running full gc"));
                        ran_gc = TRUE;
                        vm_heap->GarbageCollectGeneration(max_generation);
                        goto try_free_list;
                    }
                    else
                    {
                        dprintf (2, ("Out of memory"));
                        leave_spin_lock (&more_space_lock);
                        return 0;
                    }
                }
            }
#ifdef FREE_LIST_0
            else
            {
                dprintf (3, ("grabbing free list %x", (size_t)free_list));
                generation_free_list (gen) = (BYTE*)free_list_slot(free_list);
                if ((size + Align (min_obj_size)) <= size (free_list))
                {
                    // We ask for more Align (min_obj_size)
                    // to make sure that we can insert a free object
                    // in adjust_limit will set the limit lower 
                    size_t limit = limit_from_size (size, size (free_list));

                    BYTE*  remain = (free_list + limit);
                    size_t remain_size = (size (free_list) - limit);
                    if (remain_size >= min_free_list)
                    {
                        make_unused_array (remain, remain_size);
                        free_list_slot (remain) = generation_free_list (gen);
                        generation_free_list (gen) = remain;
                        assert (remain_size >= Align (min_obj_size));
                    }
                    else 
                    {
                        //absorb the entire free list 
                        limit += remain_size;
                    }
                    adjust_limit_clr (free_list, limit, 
                                      acontext, ephemeral_heap_segment);

                }
                else
                {
                    goto try_free_list;
                }
            }
#endif //FREE_LIST_0
        }
    }
    return TRUE;
}

inline
CObjectHeader* gc_heap::allocate (size_t jsize, alloc_context* acontext
)
{
    size_t size = Align (jsize);
    assert (size >= Align (min_obj_size));
    {
    retry:
        BYTE*  result = acontext->alloc_ptr;
        acontext->alloc_ptr+=size;
        if (acontext->alloc_ptr <= acontext->alloc_limit)
        
        {
            CObjectHeader* obj = (CObjectHeader*)result;
            return obj;
        }
        else
        {
            acontext->alloc_ptr -= size;


#pragma inline_depth(0)

            if (! allocate_more_space (acontext, size))             
                return 0;

#pragma inline_depth(20)
            goto retry;
        }
    }
}


inline
CObjectHeader* gc_heap::try_fast_alloc (size_t jsize)
{
    size_t size = Align (jsize);
    assert (size >= Align (min_obj_size));
    generation* gen = generation_of (0);
    BYTE*  result = generation_allocation_pointer (gen);
    generation_allocation_pointer (gen) += size;
    if (generation_allocation_pointer (gen) <= 
        generation_allocation_limit (gen))
    {
        return (CObjectHeader*)result;
    }
    else
    {
        generation_allocation_pointer (gen) -= size;
        return 0;
    }
}




BYTE* gc_heap::allocate_in_older_generation (generation* gen, size_t size,
                                             int from_gen_number)
{
    size = Align (size);
    assert (size >= Align (min_obj_size));
    assert (from_gen_number < max_generation);
    assert (from_gen_number >= 0);
    assert (generation_of (from_gen_number + 1) == gen);
    if (! (size_fit_p (size, generation_allocation_pointer (gen),
                       generation_allocation_limit (gen))))
    {
        while (1)
        {
            BYTE* free_list = generation_free_list (gen);
// dformat (t, 3, "considering free area %x", free_list);
            if (0 == free_list)
            {
            retry:
                heap_segment* seg = generation_allocation_segment (gen);
                if (seg != ephemeral_heap_segment)
                {
                    if (size_fit_p(size, heap_segment_plan_allocated (seg),
                                   heap_segment_committed (seg)))
                    {
                        adjust_limit (heap_segment_plan_allocated (seg),
                                      heap_segment_committed (seg) -
                                      heap_segment_plan_allocated (seg),
                                      gen, from_gen_number+1);
// dformat (t, 3, "Expanding segment allocation");
                            heap_segment_plan_allocated (seg) =
                                heap_segment_committed (seg);
                        break;
                    }
                    else
                    {
#if 0 //don't commit new pages
                        if (size_fit_p (size, heap_segment_plan_allocated (seg),
                                        heap_segment_reserved (seg)) &&
                            grow_heap_segment (seg, align_on_page (size)))
                        {
                            adjust_limit (heap_segment_plan_allocated (seg),
                                          heap_segment_committed (seg) -
                                          heap_segment_plan_allocated (seg),
                                          gen, from_gen_number+1);
                            heap_segment_plan_allocated (seg) =
                                heap_segment_committed (seg);

                            break;
                        }
                        else
#endif //0
                        {
                            if (generation_allocation_limit (gen) !=
                                heap_segment_plan_allocated (seg))
                            {
                                BYTE*  hole = generation_allocation_pointer (gen);
                                size_t hsize = (generation_allocation_limit (gen) -
                                                generation_allocation_pointer (gen));
                                if (! (0 == hsize))
                                {
// dformat (t, 3, "filling up hole");
                                    make_unused_array (hole, hsize);
                                }
                            }
                            else
                            {
                                assert (generation_allocation_pointer (gen) >=
                                        heap_segment_mem (seg));
                                assert (generation_allocation_pointer (gen) <=
                                        heap_segment_committed (seg));
                                heap_segment_plan_allocated (seg) =
                                    generation_allocation_pointer (gen);
                                generation_allocation_limit (gen) =
                                    generation_allocation_pointer (gen);
                            }
                            heap_segment*   next_seg = heap_segment_next (seg);
                            if (next_seg)
                            {
                                generation_allocation_segment (gen) = next_seg;
                                generation_allocation_pointer (gen) = heap_segment_mem (next_seg);
                                generation_allocation_limit (gen) = generation_allocation_pointer (gen);
                                goto retry;
                            }
                            else
                            {
                                size = 0;
                                break;
                            }

                        }
                    }
                }
                else
                {
                    //No need to fix the last region. Will be done later
                    size = 0;
                    break;
                }
            }
            else
            {
                generation_free_list (gen) = (BYTE*)free_list_slot (free_list);
            }
            if (size_fit_p (size, free_list, (free_list + size (free_list))))
            {
                dprintf (3, ("Found adequate unused area: %x, size: %d", 
                             (size_t)free_list, size (free_list)));
                dd_fragmentation (dynamic_data_of (from_gen_number+1)) -=
                    size (free_list);
                adjust_limit (free_list, size (free_list), gen, from_gen_number+1);
                break;
            }
        }
    }
    if (0 == size)
        return 0;
    else
    {
        BYTE*  result = generation_allocation_pointer (gen);
        generation_allocation_pointer (gen) += size;
        assert (generation_allocation_pointer (gen) <= generation_allocation_limit (gen));
        generation_allocation_size (gen) += size;
        return result;
    }
}

generation*  gc_heap::ensure_ephemeral_heap_segment (generation* consing_gen)
{
    heap_segment* seg = generation_allocation_segment (consing_gen);
    if (seg != ephemeral_heap_segment)
    {
        assert (generation_allocation_pointer (consing_gen)>= heap_segment_mem (seg));
        assert (generation_allocation_pointer (consing_gen)<= heap_segment_committed (seg));

        //fix the allocated size of the segment.
        heap_segment_plan_allocated (seg) = generation_allocation_pointer (consing_gen);

        generation* new_consing_gen = generation_of (max_generation - 1);
        generation_allocation_pointer (new_consing_gen) =
                heap_segment_mem (ephemeral_heap_segment);
        generation_allocation_limit (new_consing_gen) =
            generation_allocation_pointer (new_consing_gen);
        generation_allocation_segment (new_consing_gen) = ephemeral_heap_segment;
        return new_consing_gen;
    }
    else
        return consing_gen;
}


BYTE* gc_heap::allocate_in_condemned_generations (generation* gen,
                                         size_t size,
                                         int from_gen_number)
{
    // Make sure that the youngest generation gap hasn't been allocated
    {
        assert (generation_plan_allocation_start (youngest_generation) == 0);
    }
    size = Align (size);
    assert (size >= Align (min_obj_size));
    if ((from_gen_number != -1) && (from_gen_number != (int)max_generation))
    {
        generation_allocation_size (generation_of (1 + from_gen_number)) += size;
    }
retry:
    {
        if (! (size_fit_p (size, generation_allocation_pointer (gen),
                           generation_allocation_limit (gen))))
        {
            heap_segment* seg = generation_allocation_segment (gen);
            if ((! (pinned_plug_que_empty_p()) &&
                 (generation_allocation_limit (gen) ==
                  pinned_plug (oldest_pin()))))
            {
                size_t entry = deque_pinned_plug();
                size_t len = pinned_len (pinned_plug_of (entry));
                BYTE* plug = pinned_plug (pinned_plug_of(entry));
                mark_stack_array [ entry ].len = plug - generation_allocation_pointer (gen);
                assert(mark_stack_array[entry].len == 0 ||
                       mark_stack_array[entry].len >= Align(min_obj_size));
                generation_allocation_pointer (gen) = plug + len;
                generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
                set_allocator_next_pin (gen);
                goto retry;
            }
            if (size_fit_p (size, generation_allocation_pointer (gen),
                            heap_segment_plan_allocated (seg)))
            {
                generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
            }
            else
            {
                if (size_fit_p (size,  heap_segment_plan_allocated (seg),
                                heap_segment_committed (seg)))
                {
// dformat (t, 3, "Expanding segment allocation");
                    heap_segment_plan_allocated (seg) = heap_segment_committed (seg);
                    generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
                }
                else
                {
                    if (size_fit_p (size,  heap_segment_plan_allocated (seg),
                                    heap_segment_reserved (seg)))
                    {
// dformat (t, 3, "Expanding segment allocation");
                        if (!grow_heap_segment
                               (seg, 
                                align_on_page (heap_segment_plan_allocated (seg) + size) - 
                                heap_segment_committed (seg)))
                            {
                                //assert (!"Memory exhausted during alloc_con");
                                return 0;
                            }
                        heap_segment_plan_allocated (seg) += size;
                        generation_allocation_limit (gen) = heap_segment_plan_allocated (seg);
                    }
                    else
                    {
                        heap_segment*   next_seg = heap_segment_next (seg);
                        assert (generation_allocation_pointer (gen)>=
                                heap_segment_mem (seg));
                        // Verify that all pinned plugs for this segment are consumed
                        if (!pinned_plug_que_empty_p() &&
                            ((pinned_plug (oldest_pin()) <
                              heap_segment_allocated (seg)) &&
                             (pinned_plug (oldest_pin()) >=
                              generation_allocation_pointer (gen))))
                        {
                            LOG((LF_GC, LL_INFO10, "remaining pinned plug %x while leaving segment on allocation",
                                         pinned_plug (oldest_pin())));
                            RetailDebugBreak();
                        }
                        assert (generation_allocation_pointer (gen)>=
                                heap_segment_mem (seg));
                        assert (generation_allocation_pointer (gen)<=
                                heap_segment_committed (seg));
                        heap_segment_plan_allocated (seg) = generation_allocation_pointer (gen);

                        if (next_seg)
                        {
                            generation_allocation_segment (gen) = next_seg;
                            generation_allocation_pointer (gen) = heap_segment_mem (next_seg);
                            generation_allocation_limit (gen) = generation_allocation_pointer (gen);
                        }
                        else
                            return 0; //should only happen during allocation of generation 0 gap
                            // in that case we are going to grow the heap anyway
                    }
                }
            }
            set_allocator_next_pin (gen);
            goto retry;
        }
    }
    {
        assert (generation_allocation_pointer (gen)>=
                heap_segment_mem (generation_allocation_segment (gen)));
        BYTE* result = generation_allocation_pointer (gen);
        generation_allocation_pointer (gen) += size;
        assert (generation_allocation_pointer (gen) <= generation_allocation_limit (gen));
        return result;
    }
}


int 
gc_heap::generation_to_condemn (int n, BOOL& blocking_collection)
{

    dprintf (2, ("Asked to condemned generation %d", n));
    blocking_collection = FALSE;
    int i = 0; 
    BOOL low_memory_detected = FALSE;

    if (MHandles [0])
    {
        QueryMemNotification (MHandles [0], &low_memory_detected);
    }

    //save new_allocation
    for (i = 0; i <= max_generation+1; i++)
    {
        dynamic_data* dd = dynamic_data_of (i);
        dd_gc_new_allocation (dd) = dd_new_allocation (dd);
        dd_c_new_allocation (dd) = 0;
    }

    /* promote into max-generation if the card table has too many 
     * generation faults besides the n -> 0 
     */
    if (generation_skip_ratio < 70)
    {
        n = max (n, max_generation - 1);
        dprintf (1, ("generation_skip_ratio %d under spec, collecting %d",
                     generation_skip_ratio, n));
    }

    generation_skip_ratio = 100;

    if (!ephemeral_gen_fit_p())
    {
        n = max (n, max_generation - 1);
    }

    //figure out if large objects need to be collected.
    if (get_new_allocation (max_generation+1) <= 0)
        n = max_generation;

    //figure out which generation ran out of allocation
    for (i = n+1; i <= max_generation; i++)
    {
        if (get_new_allocation (i) <= 0)
        {
            n = min (i, max_generation);
        }
        else
            break;
    }

    if ((n >= 1) || low_memory_detected)
    {
        //find out if we are short on memory
        MEMORYSTATUS ms;
        GlobalMemoryStatus (&ms);
        dprintf (2, ("Memory load: %d", ms.dwMemoryLoad));
        if (ms.dwMemoryLoad >= 95)
        {
            dprintf (2, ("Memory load too high on entry"));
            //will prevent concurrent collection
            blocking_collection = TRUE;
            //we are tight in memory, see if we have allocated enough 
            //since last time we did a full gc to justify another one. 
            dynamic_data* dd = dynamic_data_of (max_generation);
            if ((dd_fragmentation (dd) + dd_desired_allocation (dd) - dd_new_allocation (dd)) >=
                ms.dwTotalPhys/32)
            {
                dprintf (2, ("Collecting max_generation early"));
                n = max_generation;
            }
        } 
        else if (ms.dwMemoryLoad >= 90)
        {
            dprintf (2, ("Memory load too high on entry"));
            //trying to estimate if it worth collecting 2
            dynamic_data* dd = dynamic_data_of (max_generation);
            int est_frag = dd_fragmentation (dd) +
                (dd_desired_allocation (dd) - dd_new_allocation (dd)) *
                (dd_current_size (dd) ? 
                 dd_fragmentation (dd) / (dd_fragmentation (dd) + dd_current_size (dd)) :
                 0);
            dprintf (2, ("Estimated gen2 fragmentation %d", est_frag));
            if (est_frag >= (int)ms.dwTotalPhys/16)
            {
                dprintf (2, ("Collecting max_generation early"));
                n = max_generation;
            }
        }

    }

    return n;
}



enum {
CORINFO_EXCEPTION_GC = ('GC' | 0xE0000000)
};

//internal part of gc used by the serial and concurrent version
void gc_heap::gc1()
{
#ifdef TIME_GC
    mark_time = plan_time = reloc_time = compact_time = sweep_time = 0;
#endif //TIME_GC

    int n = condemned_generation_num;

    __try 
    {

    vm_heap->GcCondemnedGeneration = condemned_generation_num;


#ifdef NO_WRITE_BARRIER
    fix_card_table();
#endif //NO_WRITE_BARRIER

    {

        if (n == max_generation)
        {
            gc_low = lowest_address;
            gc_high = highest_address;
        }
        else
        {
            gc_low = generation_allocation_start (generation_of (n));
            gc_high = heap_segment_reserved (ephemeral_heap_segment);
        }
        {
            mark_phase (n, FALSE);
        }

        plan_phase (n);

    }
    for (int gen_number = 0; gen_number <= n; gen_number++)
    {
        compute_new_dynamic_data (gen_number);
    }
    if (n < max_generation)
        compute_promoted_allocation (1 + n);
    adjust_ephemeral_limits();

    {
        for (int gen_number = 0; gen_number <= max_generation+1; gen_number++)
        {
            dynamic_data* dd = dynamic_data_of (gen_number);
            dd_new_allocation(dd) = dd_gc_new_allocation (dd) +  
                dd_c_new_allocation (dd);
        }
    }




    //decide on the next allocation quantum
    if (alloc_contexts_used >= 1)
    {
        allocation_quantum = (int)Align (min (CLR_SIZE, 
                                         max (1024, get_new_allocation (0) / (2 * alloc_contexts_used))));
        dprintf (3, ("New allocation quantum: %d(0x%x)", allocation_quantum, allocation_quantum));
    }

#ifdef NO_WRITE_BARRIER
        reset_write_watch();
#endif

    descr_generations();
    descr_card_table();

#ifdef TIME_GC
    fprintf (stdout, "%d,%d,%d,%d,%d,%d\n", 
             n, mark_time, plan_time, reloc_time, compact_time, sweep_time);
#endif

#if defined (VERIFY_HEAP)

    if (g_pConfig->IsHeapVerifyEnabled())
    {

        verify_heap();
    }


#endif //VERIFY_HEAP

    }
    __except (COMPLUS_EXCEPTION_EXECUTE_HANDLER)
    {
        _ASSERTE(!"Exception during gc1()");
        ::ExitProcess (CORINFO_EXCEPTION_GC);
    }
}

int gc_heap::garbage_collect (int n
                             )
{

    __try {
    //reset the number of alloc contexts
    alloc_contexts_used = 0;

    {
#ifdef TRACE_GC

        gc_count++;

        if (gc_count >= g_pConfig->GetGCtraceStart())
            trace_gc = 1;
        if (gc_count >=  g_pConfig->GetGCtraceEnd())
            trace_gc = 0;

#endif

    dprintf(1,(" ****Garbage Collection**** %d", gc_count));
        


        // fix all of the allocation contexts.
        CNameSpace::GcFixAllocContexts ((void*)TRUE);


    }



    fix_youngest_allocation_area(TRUE);

    // check for card table growth
    if (g_card_table != card_table)
        copy_brick_card_table (FALSE);

    BOOL blocking_collection = FALSE;

    condemned_generation_num = generation_to_condemn (n, blocking_collection);


#ifdef GC_PROFILING

        // If we're tracking GCs, then we need to walk the first generation
        // before collection to track how many items of each class has been
        // allocated.
        if (CORProfilerTrackGC())
        {
            size_t heapId = 0;

            // When we're walking objects allocated by class, then we don't want to walk the large
            // object heap because then it would count things that may have been around for a while.
            gc_heap::walk_heap (&AllocByClassHelper, (void *)&heapId, 0, FALSE);

            // Notify that we've reached the end of the Gen 0 scan
            g_profControlBlock.pProfInterface->EndAllocByClass(&heapId);
        }

#endif // GC_PROFILING


    //update counters
    {
        int i; 
        for (i = 0; i <= condemned_generation_num;i++)
        {
            dd_collection_count (dynamic_data_of (i))++;
        }
    }




    descr_generations();
//    descr_card_table();

    dprintf(1,("generation %d condemned", condemned_generation_num));

#if defined (VERIFY_HEAP)
    if (g_pConfig->IsHeapVerifyEnabled())
    {
        verify_heap();
        checkGCWriteBarrier();
    }
#endif // VERIFY_HEAP


    {
        gc1();
    }
    }
    __except (COMPLUS_EXCEPTION_EXECUTE_HANDLER)
    {
        _ASSERTE(!"Exception during garbage_collect()");
        ::ExitProcess (CORINFO_EXCEPTION_GC);
    }


    return condemned_generation_num;
}

#define mark_stack_empty_p() (mark_stack_base == mark_stack_tos)

#define push_mark_stack(object,add,num)                             \
{                                                                   \
    dprintf(3,("pushing mark for %x ", object));                    \
    if (mark_stack_tos < mark_stack_limit)                          \
    {                                                               \
        mark_stack_tos->first = (add);                              \
        mark_stack_tos->last= (add) + (num);                      \
        mark_stack_tos++;                                           \
    }                                                               \
    else                                                            \
    {                                                               \
        dprintf(3,("mark stack overflow for object %x ", object));  \
        min_overflow_address = min (min_overflow_address, object);  \
        max_overflow_address = max (max_overflow_address, object);  \
    }                                                               \
}

#define push_mark_stack_unchecked(add,num)                      \
{                                                               \
    mark_stack_tos->first = (add);                              \
    mark_stack_tos->last= (add) + (num);                      \
    mark_stack_tos++;                                           \
}


#define pop_mark_stack()(*(--mark_stack_tos))

#if defined ( INTERIOR_POINTERS ) || defined (_DEBUG)

heap_segment* gc_heap::find_segment (BYTE* interior)
{
    if ((interior < ephemeral_high ) && (interior >= ephemeral_low))
    {
        return ephemeral_heap_segment;
    }
    else
    {
        heap_segment* seg = generation_start_segment (generation_of (max_generation));
        do 
        {
            if ((interior >= heap_segment_mem (seg)) &&
                (interior < heap_segment_reserved (seg)))
                return seg;
        }while ((seg = heap_segment_next (seg)) != 0);
           
        return 0;
    }
}

#endif //_DEBUG || INTERIOR_POINTERS
inline
gc_heap* gc_heap::heap_of (BYTE* o, BOOL verify_p)
{
    return __this;
}

#ifdef INTERIOR_POINTERS
// will find all heap objects (large and small)
BYTE* gc_heap::find_object (BYTE* interior, BYTE* low)
{
    if (!gen0_bricks_cleared)
    {
//BUGBUG to get started on multiple heaps.
        gen0_bricks_cleared = TRUE;
        //initialize brick table for gen 0
        for (size_t b = brick_of (generation_allocation_start (generation_of (0)));
             b < brick_of (align_on_brick 
                           (heap_segment_allocated (ephemeral_heap_segment)));
             b++)
        {
            set_brick (b, -1);
        }
    }
#ifdef FFIND_OBJECT
    //indicate that in the future this needs to be done during allocation
    gen0_must_clear_bricks = FFIND_DECAY;
#endif //FFIND_OBJECT

    int brick_entry = brick_table [brick_of (interior)];
    if (brick_entry == -32768)
    {
        // this is a pointer to a large object
        large_object_block* bl = large_p_objects;
        while (bl)
        {
            large_object_block* next_bl = large_object_block_next (bl);
            BYTE* o = block_object (bl);
            if ((o <= interior) && (interior < o + size (o)))
            {
                return o;
            }
            bl = next_bl;
        }
        bl = large_np_objects;
        while (bl)
        {
            large_object_block* next_bl = large_object_block_next (bl);
            BYTE* o = block_object (bl);
            if ((o <= interior) && (interior < o + size (o)))
            {
                return o;
            }
            bl = next_bl;
        }
        return 0;

    }
    else if (interior >= low)
    {
        heap_segment* seg = find_segment (interior);
        if (seg)
        {
            assert (interior < heap_segment_allocated (seg));
            BYTE* o = find_first_object (interior, brick_of (interior), heap_segment_mem (seg));
            return o;
        } 
        else
            return 0;
    }
    else
        return 0;
}

BYTE* 
gc_heap::find_object_for_relocation (BYTE* interior, BYTE* low, BYTE* high)
{
    BYTE* old_address = interior;
    if (!((old_address >= low) && (old_address < high)))
        return 0;
    BYTE* plug = 0;
    size_t  brick = brick_of (old_address);
    int    brick_entry =  brick_table [ brick ];
    int    orig_brick_entry = brick_entry;
    if (brick_entry != -32768)
    {
    retry:
        {
            while (brick_entry < 0)
            {
                brick = (brick + brick_entry);
                brick_entry =  brick_table [ brick ];
            }
            BYTE* old_loc = old_address;
            BYTE* node = tree_search ((brick_address (brick) + brick_entry),
                                      old_loc);
            if (node <= old_loc)
                plug = node;
            else
            {
                brick = brick - 1;
                brick_entry =  brick_table [ brick ];
                goto retry;
            }

        }
        assert (plug);
        //find the object by going along the plug
        BYTE* o = plug;
        while (o <= interior)
        {
            BYTE* next_o = o + Align (size (o));
            if (next_o > interior)
            {
                break;
            }
            o = next_o;
        }
        assert ((o <= interior) && ((o + Align (size (o))) > interior));
        return o;
    } 
    else
    {
        // this is a pointer to a large object
        large_object_block* bl = large_p_objects;
        while (bl)
        {
            large_object_block* next_bl = large_object_block_next (bl);
            BYTE* o = block_object (bl);
            if ((o <= interior) && (interior < o + size (o)))
            {
                return o;
            }
            bl = next_bl;
        }
        bl = large_np_objects;
        while (bl)
        {
            large_object_block* next_bl = large_object_block_next (bl);
            BYTE* o = block_object (bl);
            if ((o <= interior) && (interior < o + size (o)))
            {
                return o;
            }
            bl = next_bl;
        }
        return 0;

    }

}
    
#else //INTERIOR_POINTERS
inline
BYTE* gc_heap::find_object (BYTE* o, BYTE* low)
{
    return o;
}
#endif //INTERIOR_POINTERS


#ifdef MARK_LIST
#define m_boundary(o) {if (mark_list_index <= mark_list_end) {*mark_list_index = o;mark_list_index++;}if (slow > o) slow = o; if (shigh < o) shigh = o;}
#else //MARK_LIST
#define m_boundary(o) {if (slow > o) slow = o; if (shigh < o) shigh = o;}
#endif //MARK_LIST

inline
BOOL gc_heap::gc_mark1 (BYTE* o)
{
    dprintf(3,("*%x*", (size_t)o));

#if 0 //def MARK_ARRAY
    DWORD* x = &mark_array[mark_word_of (o)];
    BOOL  marked = !(*x & (1 << mark_bit_of (o)));
    *x |= 1 << mark_bit_of (o);
#else
    BOOL marked = !marked (o);
    set_marked (o);
#endif

    return marked;
}

inline
BOOL gc_heap::gc_mark (BYTE* o, BYTE* low, BYTE* high)
{
    BOOL marked = FALSE;
    if ((o >= low) && (o < high))
        marked = gc_mark1 (o);
    return marked;
}

inline
BYTE* gc_heap::next_end (heap_segment* seg, BYTE* f)
{
    if (seg == ephemeral_heap_segment)
        return  f;
    else
        return  heap_segment_allocated (seg);
}

#define method_table(o) ((CObjectHeader*)(o))->GetMethodTable()

#define go_through_object(mt,o,size,parm,exp)                               \
{                                                                           \
    CGCDesc* map = CGCDesc::GetCGCDescFromMT((MethodTable*)(mt));           \
    CGCDescSeries* cur = map->GetHighestSeries();                           \
    CGCDescSeries* last = map->GetLowestSeries();                           \
                                                                            \
    if (cur >= last)                                                        \
    {                                                                       \
        do                                                                  \
        {                                                                   \
            BYTE** parm = (BYTE**)((o) + cur->GetSeriesOffset());           \
            BYTE** ppstop =                                                 \
                (BYTE**)((BYTE*)parm + cur->GetSeriesSize() + (size));      \
            while (parm < ppstop)                                           \
            {                                                               \
                {exp}                                                       \
                parm++;                                                     \
            }                                                               \
            cur--;                                                          \
                                                                            \
        } while (cur >= last);                                              \
    }                                                                       \
    else                                                                    \
    {                                                                       \
        int cnt = map->GetNumSeries();                                      \
        BYTE** parm = (BYTE**)((o) + cur->startoffset);                     \
        while ((BYTE*)parm < ((o)+(size)-plug_skew))                        \
        {                                                                   \
            for (int __i = 0; __i > cnt; __i--)                             \
            {                                                               \
                unsigned skip =  cur->val_serie[__i].skip;                  \
                unsigned nptrs = cur->val_serie[__i].nptrs;                 \
                BYTE** ppstop = parm + nptrs;                               \
                do                                                          \
                {                                                           \
                   {exp}                                                    \
                   parm++;                                                  \
                } while (parm < ppstop);                                    \
                parm = (BYTE**)((BYTE*)parm + skip);                        \
            }                                                               \
        }                                                                   \
    }                                                                       \
}

/* small objects and array of value classes have to be treated specially because sometime a 
 * cross generation pointer can exist without the corresponding card being set. This can happen if 
 * a previous card covering the object (or value class) is set. This works because the object 
 * (or embedded value class) is always scanned completely if any of the cards covering it is set. */ 

void gc_heap::verify_card_table ()
{
    int         curr_gen_number = max_generation;
    generation* gen = generation_of (curr_gen_number);
    heap_segment*    seg = generation_start_segment (gen);
    BYTE*       x = generation_allocation_start (gen);
    BYTE*       last_end = 0;
    BYTE*       last_x = 0;
    BYTE*       last_last_x = 0;
    BYTE*       f = generation_allocation_start (generation_of (0));
    BYTE*       end = next_end (seg, f);
    BYTE*       next_boundary = generation_allocation_start (generation_of (curr_gen_number - 1));


    dprintf (2,("Verifying card table from %x to %x", (size_t)x, (size_t)end));

    while (1)
    {
        if (x >= end)
        {
            if ((seg = heap_segment_next(seg)) != 0)
            {
                x = heap_segment_mem (seg);
                last_end = end;
                end = next_end (seg, f);
                dprintf (3,("Verifying card table from %x to %x", (size_t)x, (size_t)end));
                continue;
            } else
            {
                break;
            }
        }

        if ((seg == ephemeral_heap_segment) && (x >= next_boundary))
        {
            curr_gen_number--;
            assert (curr_gen_number > 0);
            next_boundary = generation_allocation_start (generation_of (curr_gen_number - 1));
        }

        size_t s = size (x);
        BOOL need_card_p = FALSE;
        if (contain_pointers (x))
        {
            size_t crd = card_of (x);
            BOOL found_card_p = card_set_p (crd);
            go_through_object 
                (method_table(x), x, s, oo, 
                 {
                     if ((*oo < ephemeral_high) && (*oo >= next_boundary))
                     {
                         need_card_p = TRUE;
                     }
                     if ((crd != card_of ((BYTE*)oo)) && !found_card_p)
                     {
                         crd = card_of ((BYTE*)oo);
                         found_card_p = card_set_p (crd);
                     }
                     if (need_card_p && !found_card_p)
                     {
                         RetailDebugBreak();
                     }
                 }
                    );
            if (need_card_p && !found_card_p)
            {
                printf ("Card not set, x = [%x:%x, %x:%x[",
                        card_of (x), (size_t)x,
                        card_of (x+Align(s)), (size_t)x+Align(s));
                RetailDebugBreak();
            }

        }

        last_last_x = last_x;
        last_x = x;
        x = x + Align (s);
    }

    // go through large object
    large_object_block* bl = large_p_objects;
    while (bl)
    {
        large_object_block* next_bl = bl->next;
        BYTE* o = block_object (bl);
        MethodTable* mt = method_table (o);
        {                                                                           
            CGCDesc* map = CGCDesc::GetCGCDescFromMT((MethodTable*)(mt));
            CGCDescSeries* cur = map->GetHighestSeries();
            CGCDescSeries* last = map->GetLowestSeries();

            if (cur >= last)
            {
                do
                {
                    BYTE** oo = (BYTE**)((o) + cur->GetSeriesOffset());
                    BYTE** ppstop =
                        (BYTE**)((BYTE*)oo + cur->GetSeriesSize() + (size (o)));
                    while (oo < ppstop)
                    {
                        if ((*oo < ephemeral_high) && (*oo >= ephemeral_low)) 
                        { 
                            if (!card_set_p (card_of ((BYTE*)oo))) 
                            { 
                                RetailDebugBreak(); 
                            } 
                        }
                        oo++;
                    }
                    cur--;

                } while (cur >= last);
            }
            else
            {
                BOOL need_card_p = FALSE;
                size_t crd = card_of (o);
                BOOL found_card_p = card_set_p (crd);
                int cnt = map->GetNumSeries();
                BYTE** oo = (BYTE**)((o) + cur->startoffset);
                while ((BYTE*)oo < ((o)+(size (o))-plug_skew))
                {
                    for (int __i = 0; __i > cnt; __i--)
                    {
                        unsigned skip =  cur->val_serie[__i].skip;
                        unsigned nptrs = cur->val_serie[__i].nptrs;
                        BYTE** ppstop = oo + nptrs;
                        do
                        {
                            if ((*oo < ephemeral_high) && (*oo >= next_boundary))
                            {
                                need_card_p = TRUE;
                            }
                            if ((crd != card_of ((BYTE*)oo)) && !found_card_p)
                            {
                                crd = card_of ((BYTE*)oo);
                                found_card_p = card_set_p (crd);
                            }
                            if (need_card_p && !found_card_p)
                            {
                                RetailDebugBreak();
                            }
                            oo++;
                        } while (oo < ppstop);
                        oo = (BYTE**)((BYTE*)oo + skip);
                    }
                }
            }                                                                       
        }
        bl = next_bl;
    }
}


void gc_heap::mark_object_internal1 (BYTE* oo THREAD_NUMBER_DCL)
{

    BYTE** mark_stack_tos = (BYTE**)mark_stack_array;
    BYTE** mark_stack_limit = (BYTE**)&mark_stack_array[mark_stack_array_length];
    BYTE** mark_stack_base = mark_stack_tos;

    while (1)
    {
        size_t s = size (oo);       
        if (mark_stack_tos + (s) /sizeof (BYTE*) < mark_stack_limit)
        {
            dprintf(3,("pushing mark for %x ", (size_t)oo));
            go_through_object (method_table(oo), oo, s, po, 
                               {
                                   BYTE* o = *po;
                                   if (gc_mark (o, gc_low, gc_high))
                                   {
                                       m_boundary (o);

//#ifdef COLLECT_CLASSES
//                                     if (collect_classes)
//                                     {
//                                         CObjectHeader* pheader = GetObjectHeader((Object*)o);
//                                         Object** clp = pheader->GetMethodTable()->
//                                             GetClass()->
//                                             GetManagedClassSlot();
//                                         if (clp && gc_mark (clp, low, high) && contain_pointers (clp))
//                                             *(mark_stack_tos++) = clp;
//                                     }
//#endif //COLLECT_CLASSES
                                       if (contain_pointers (o)) 
                                       {
                                           *(mark_stack_tos++) = o;

                                       }
                                   } 
                               }
                    );
        }
        else
        {
            dprintf(3,("mark stack overflow for object %x ", (size_t)oo));
            min_overflow_address = min (min_overflow_address, oo);
            max_overflow_address = max (max_overflow_address, oo);
        }

        if (!(mark_stack_empty_p()))
        {
            oo = *(--mark_stack_tos);
        }
        else
            break;
    }
}

BYTE* gc_heap::mark_object_internal (BYTE* o THREAD_NUMBER_DCL)
{
    if (gc_mark (o, gc_low, gc_high))
    {
        m_boundary (o);
        if (contain_pointers (o))
        {
            size_t s = size (o);
            go_through_object (method_table (o), o, s, poo,
                               {
                                   BYTE* oo = *poo;
                                   if (gc_mark (oo, gc_low, gc_high))
                                   {
                                       m_boundary (oo);
                                       if (contain_pointers (oo))
                                           mark_object_internal1 (oo THREAD_NUMBER_ARG);
                                   }
                               }
                );

        }
    }
    return o;
}

//#define SORT_MARK_STACK
void gc_heap::mark_object_simple1 (BYTE* oo THREAD_NUMBER_DCL)
{
    BYTE** mark_stack_tos = (BYTE**)mark_stack_array;
    BYTE** mark_stack_limit = (BYTE**)&mark_stack_array[mark_stack_array_length];
    BYTE** mark_stack_base = mark_stack_tos;
#ifdef SORT_MARK_STACK
    BYTE** sorted_tos = mark_stack_base;
#endif //SORT_MARK_STACK
    while (1)
    {
        size_t s = size (oo);       
        if (mark_stack_tos + (s) /sizeof (BYTE*) < mark_stack_limit)
        {
            dprintf(3,("pushing mark for %x ", (size_t)oo));

            go_through_object (method_table(oo), oo, s, ppslot, 
                               {
                                   BYTE* o = *ppslot;
                                   if (gc_mark (o, gc_low, gc_high))
                                   {
                                       m_boundary (o);
                                       if (contain_pointers (o))
                                       {
                                           *(mark_stack_tos++) = o;

                                       }
                                   }
                               }
                              );

        }
        else
        {
            dprintf(3,("mark stack overflow for object %x ", (size_t)oo));
            min_overflow_address = min (min_overflow_address, oo);
            max_overflow_address = max (max_overflow_address, oo);
        }               
#ifdef SORT_MARK_STACK
        if (mark_stack_tos > sorted_tos + mark_stack_array_length/8)
        {
            qsort1 (sorted_tos, mark_stack_tos-1);
            sorted_tos = mark_stack_tos-1;
        }
#endif //SORT_MARK_STACK
        if (!(mark_stack_empty_p()))
        {
            oo = *(--mark_stack_tos);
#ifdef SORT_MARK_STACK
            sorted_tos = (BYTE**)min ((size_t)sorted_tos, (size_t)mark_stack_tos);
#endif //SORT_MARK_STACK
        }
        else
            break;
    }
}

//this method assumes that *po is in the [low. high[ range
void 
gc_heap::mark_object_simple (BYTE** po THREAD_NUMBER_DCL)
{
    BYTE* o = *po;
    {
        if (gc_mark1 (o))
        {
            m_boundary (o);
            if (contain_pointers (o))
            {
                size_t s = size (o);
                go_through_object (method_table(o), o, s, poo,
                                   {
                                       BYTE* oo = *poo;
                                       if (gc_mark (oo, gc_low, gc_high))
                                       {
                                           m_boundary (oo);
                                           if (contain_pointers (oo))
                                               mark_object_simple1 (oo THREAD_NUMBER_ARG);
                                       }
                                   }
                    );

            }
        }
    }
}

//We know we are collecting classes. 
BYTE* gc_heap::mark_object_class (BYTE* o THREAD_NUMBER_DCL)
{
    mark_object_internal (o THREAD_NUMBER_ARG);
    if (o)
    {
        CObjectHeader* pheader = GetObjectHeader((Object*)o);

    }
    return o;
}

// We may collect classes. 
inline
BYTE* gc_heap::mark_object (BYTE* o THREAD_NUMBER_DCL)
{
    if (collect_classes)
    {
        mark_object_internal (o THREAD_NUMBER_ARG);
    }
    else
        if ((o >= gc_low) && (o < gc_high))
            mark_object_simple (&o THREAD_NUMBER_ARG);

    return o;
}


void gc_heap::fix_card_table ()
{

}

void gc_heap::mark_through_object (BYTE* oo THREAD_NUMBER_DCL)
{
    if (contain_pointers (oo))
        {
            dprintf(3,( "Marking through %x", (size_t)oo));
            size_t s = size (oo);
            go_through_object (method_table(oo), oo, s, po,
                               BYTE* o = *po;
                               mark_object (o THREAD_NUMBER_ARG);
                              );
        }
}

//returns TRUE is an overflow happened.
BOOL gc_heap::process_mark_overflow(int condemned_gen_number)
{
    BOOL  full_p = (condemned_gen_number == max_generation);
    BOOL  overflow_p = FALSE;
recheck:
    if ((! ((max_overflow_address == 0)) ||
         ! ((min_overflow_address == (BYTE*)(SSIZE_T)-1))))
    {
        overflow_p = TRUE;
        // Try to grow the array.
        size_t new_size =
            max (100, 2*mark_stack_array_length);
        mark* tmp = new (mark [new_size]);
        if (tmp)
        {
            delete mark_stack_array;
            mark_stack_array = tmp;
            mark_stack_array_length = new_size;
        }

        BYTE*  min_add = min_overflow_address;
        BYTE*  max_add = max_overflow_address;
        max_overflow_address = 0;
        min_overflow_address = (BYTE*)(SSIZE_T)-1;


        dprintf(3,("Processing Mark overflow [%x %x]", (size_t)min_add, (size_t)max_add));
        {
            {
                generation*   gen = generation_of (condemned_gen_number);

                heap_segment* seg = generation_start_segment (gen);
                BYTE*  o = max (generation_allocation_start (gen), min_add);
                while (1)
                {
                    BYTE*  end = heap_segment_allocated (seg);
                    while ((o < end) && (o <= max_add))
                    {
                        assert ((min_add <= o) && (max_add >= o));
                        dprintf (3, ("considering %x", (size_t)o));
                        if (marked (o))
                        {
                            mark_through_object (o THREAD_NUMBER_ARG);
                        }
                        o = o + Align (size (o));
                    }
                    if (( seg = heap_segment_next (seg)) == 0)
                    {
                        break;
                    } else
                    {
                        o = max (heap_segment_mem (seg), min_add);
                        continue;
                    }
                }
            }
            if (full_p)
            {
                // If full_gc, look in large object list as well
                large_object_block* bl = large_p_objects;
                while (bl)
                {
                    BYTE* o = block_object (bl);
                    if ((min_add <= o) && (max_add >= o) && marked (o))
                    {
                        mark_through_object (o THREAD_NUMBER_ARG);
                    }
                    bl = large_object_block_next (bl);
                }
            }

        }
        goto recheck;
    }
    return overflow_p;
}

void gc_heap::mark_phase (int condemned_gen_number, BOOL mark_only_p)

{

    ScanContext sc;
    sc.thread_number = heap_number;
    sc.promotion = TRUE;
    sc.concurrent = FALSE;

    dprintf(2,("---- Mark Phase condemning %d ----", condemned_gen_number));
    BOOL  full_p = (condemned_gen_number == max_generation);

#ifdef TIME_GC
    unsigned start;
    unsigned finish;
    start = GetCycleCount32();
#endif

#ifdef FFIND_OBJECT
    if (gen0_must_clear_bricks > 0)
        gen0_must_clear_bricks--;
#endif //FFIND_OBJECT



    reset_mark_stack();

//BUGBUG: hack to get started on multiple heaps. 


    {

#ifdef MARK_LIST
        //set up the mark lists from g_mark_list
        assert (g_mark_list);
        mark_list = g_mark_list;
        //dont use the mark list for full gc 
        //because multiple segments are more complex to handle and the list 
        //is likely to overflow
        if (condemned_gen_number != max_generation)
            mark_list_end = &mark_list [mark_list_size-1];
        else
            mark_list_end = &mark_list [0];
        mark_list_index = &mark_list [0];
#endif //MARK_LIST


        shigh = (BYTE*) 0;
        slow  = (BYTE*)~0;

        //%type%  category = quote (mark);
        generation*   gen = generation_of (condemned_gen_number);


        if (! (full_p))
        {
            dprintf(3,("Marking cross generation pointers"));
            assert (!collect_classes);
            mark_through_cards_for_segments (mark_object_simple, FALSE);
        }

        dprintf(3,("Marking Roots"));
        CNameSpace::GcScanRoots(GCHeap::Promote, 
                                condemned_gen_number, max_generation, 
                                &sc, 0);

        dprintf(3,("Marking finalization data"));
        finalize_queue->GcScanRoots(GCHeap::Promote, heap_number, 0);

        {

            if (! (full_p))
            {
                dprintf(3,("Marking cross generation pointers for large objects"));
                mark_through_cards_for_large_objects (mark_object_simple, FALSE);
            }
            dprintf(3,("Marking handle table"));
            CNameSpace::GcScanHandles(GCHeap::Promote, 
                                      condemned_gen_number, max_generation, 
                                      &sc);
        }
    }

    {


    }

    process_mark_overflow(condemned_gen_number);




    {
        // scan for deleted short weak pointers
        CNameSpace::GcShortWeakPtrScan(condemned_gen_number, max_generation,&sc);



    }


    //Handle finalization.

    finalize_queue->ScanForFinalization (condemned_gen_number, 1, mark_only_p, __this);



    // make sure everything is promoted
    process_mark_overflow (condemned_gen_number);

    finalize_queue->ScanForFinalization (condemned_gen_number, 2, mark_only_p, __this);

    {   

    
        // scan for deleted weak pointers
        CNameSpace::GcWeakPtrScan (condemned_gen_number, max_generation, &sc);



        if (condemned_gen_number == max_generation)
            sweep_large_objects();
    }

#ifdef TIME_GC
        finish = GetCycleCount32();
        mark_time = finish - start;
#endif //TIME_GC

    dprintf(2,("---- End of mark phase ----"));
}



inline
void gc_heap::pin_object (BYTE* o, BYTE* low, BYTE* high)
{
    dprintf (3, ("Pinning %x", (size_t)o));
    if ((o >= low) && (o < high))
    {
        dprintf(3,("^%x^", (size_t)o));
        set_pinned (o);
    }
}


void gc_heap::reset_mark_stack ()
{
    mark_stack_tos = 0;
    mark_stack_bos = 0;
    max_overflow_address = 0;
    min_overflow_address = (BYTE*)(SSIZE_T)-1;
}

class pair
{
public:
    short left;
    short right;
};

//Note that these encode the fact that plug_skew is a multiple of BYTE*.
// Each of new field is prepended to the prior struct.

struct plug_and_pair
{
    pair        pair;
    plug        plug;
};

struct plug_and_reloc
{
    ptrdiff_t   reloc;
    pair        pair;
    plug        plug;
};
    
struct plug_and_gap
{
    ptrdiff_t   gap;
    ptrdiff_t   reloc;
    union
    {
        pair    pair;
        int     lr;  //for clearing the entire pair in one instruction
    };
    plug        plug;
};

inline
short node_left_child(BYTE* node)
{
    return ((plug_and_pair*)node)[-1].pair.left;
}

inline
void set_node_left_child(BYTE* node, ptrdiff_t val)
{
    ((plug_and_pair*)node)[-1].pair.left = (short)val;
}

inline
short node_right_child(BYTE* node)
{
    return ((plug_and_pair*)node)[-1].pair.right;
}

inline 
pair node_children (BYTE* node)
{
    return ((plug_and_pair*)node)[-1].pair;
}

inline
void set_node_right_child(BYTE* node, ptrdiff_t val)
{
    ((plug_and_pair*)node)[-1].pair.right = (short)val;
}

inline 
ptrdiff_t node_relocation_distance (BYTE* node) 
{
    return (((plug_and_reloc*)(node))[-1].reloc & ~3);
}

inline
void set_node_relocation_distance(BYTE* node, ptrdiff_t val)
{
    assert (val == (val & ~3));
    ptrdiff_t* place = &(((plug_and_reloc*)node)[-1].reloc);
    //clear the left bit and the relocation field
    *place &= 1;
    // store the value
    *place |= val;
}

#define node_left_p(node) (((plug_and_reloc*)(node))[-1].reloc & 2)
    
#define set_node_left(node) ((plug_and_reloc*)(node))[-1].reloc |= 2;
    
#define node_small_gap(node)    (((plug_and_reloc*)(node))[-1].reloc & 1)
    
#define set_node_small_gap(node) ((plug_and_reloc*)(node))[-1].reloc |= 1;
    
inline
size_t  node_gap_size (BYTE* node)
{
    if (! (node_small_gap (node)))
        return ((plug_and_gap *)node)[-1].gap;
    else
        return sizeof(plug_and_reloc);
}


void set_gap_size (BYTE* node, size_t size)
{
    assert (Aligned (size));
    // clear the 2 DWORD used by the node.
    ((plug_and_gap *)node)[-1].reloc = 0;
    ((plug_and_gap *)node)[-1].lr =0;
    if (size > sizeof(plug_and_reloc))
    {
        ((plug_and_gap *)node)[-1].gap = size;
    }
    else
        set_node_small_gap (node);
}


BYTE* gc_heap::insert_node (BYTE* new_node, size_t sequence_number,
                   BYTE* tree, BYTE* last_node)
{
    dprintf (3, ("insert node %x, tree: %x, last_node: %x, seq_num: %x",
                 (size_t)new_node, (size_t)tree, (size_t)last_node, sequence_number));
    if (power_of_two_p (sequence_number))
    {
        set_node_left_child (new_node, (tree - new_node));
        tree = new_node;
        dprintf (3, ("New tree: %x", (size_t)tree));
    }
    else
    {
        if (oddp (sequence_number))
        {
            set_node_right_child (last_node, (new_node - last_node));
        }
        else
        {
            BYTE*  earlier_node = tree;
            size_t imax = logcount(sequence_number) - 2;
            for (size_t i = 0; i != imax; i++)
            {
                earlier_node = earlier_node + node_right_child (earlier_node);
            }
            int tmp_offset = node_right_child (earlier_node);
            assert (tmp_offset); // should never be empty
            set_node_left_child (new_node, ((earlier_node + tmp_offset ) - new_node));
            set_node_right_child (earlier_node, (new_node - earlier_node));
        }
    }
    return tree;
}


size_t gc_heap::update_brick_table (BYTE* tree, size_t current_brick,
                                    BYTE* x, BYTE* plug_end)
{
    if (tree > 0)
        set_brick (current_brick, (tree - brick_address (current_brick)));
    else
    {
        brick_table [ current_brick ] = (short)-1;
    }
    size_t  b = 1 + current_brick;
    short  offset = 0;
    size_t last_br = brick_of (plug_end-1);
    dprintf(3,(" Fixing brick [%x, %x] to point to %x", current_brick, last_br, (size_t)tree));
    current_brick = brick_of (x-1);
    while (b <= current_brick)
    {
        if (b <= last_br)
            set_brick (b, --offset);
        else
            set_brick (b,-1);
        b++;
    }
    return brick_of (x);
}



void gc_heap::set_allocator_next_pin (generation* gen)
{
    if (! (pinned_plug_que_empty_p()))
    {
        mark*  oldest_entry = oldest_pin();
        BYTE* plug = pinned_plug (oldest_entry);
        if ((plug >= generation_allocation_pointer (gen)) &&
            (plug <  generation_allocation_limit (gen)))
        {
            generation_allocation_limit (gen) = pinned_plug (oldest_entry);
        }
        else
            assert (!((plug < generation_allocation_pointer (gen)) &&
                      (plug >= heap_segment_mem (generation_allocation_segment (gen)))));
    }
}


void gc_heap::enque_pinned_plug (generation* gen,
                        BYTE* plug, size_t len)
{
    assert(len >= Align(min_obj_size));

    if (mark_stack_array_length <= mark_stack_tos)
    {
        // Mark stack overflow. No choice but grow the stack
        size_t new_size = max (100, 2*mark_stack_array_length);
        mark* tmp = new (mark [new_size]);
        if (tmp)
        {
            memcpy (tmp, mark_stack_array,
                    mark_stack_array_length*sizeof (mark));
            delete mark_stack_array;
            mark_stack_array = tmp;
            mark_stack_array_length = new_size;
        }
        else
        {
            assert (tmp);
            // Throw an out of memory error.
        }
    }
    mark& m = mark_stack_array [ mark_stack_tos ];
    m.first = plug;
    m.len = len;
    mark_stack_tos++;
    set_allocator_next_pin (gen);
}

void gc_heap::plan_generation_start (generation*& consing_gen)
{
    consing_gen = ensure_ephemeral_heap_segment (consing_gen);      
    {
        //make sure that every generation has a planned allocation start
        int  gen_number = condemned_generation_num;
        while (gen_number>= 0)
        {
            generation* gen = generation_of (gen_number);
            if (0 == generation_plan_allocation_start (gen))
            {
                generation_plan_allocation_start (gen) =
                    allocate_in_condemned_generations 
                        (consing_gen, Align (min_obj_size),-1);
            }
            gen_number--;
        }
    }
    // now we know the planned allocation size
    heap_segment_plan_allocated (ephemeral_heap_segment) =
        generation_allocation_pointer (consing_gen);
}

void gc_heap::process_ephemeral_boundaries (BYTE* x, 
                                            int& active_new_gen_number,
                                            int& active_old_gen_number,
                                            generation*& consing_gen,
                                            BOOL& allocate_in_condemned, 
                                            BYTE*& free_gap, BYTE* zero_limit)
                                  
{
retry:
    if ((active_old_gen_number > 0) &&
        (x >= generation_allocation_start (generation_of (active_old_gen_number - 1))))
    {
        active_old_gen_number--;
    }
    if ((zero_limit && (active_new_gen_number == 1) && (x >= zero_limit)) ||
        (x >= generation_limit (active_new_gen_number)))
    {
        //Go past all of the pinned plugs for this generation.
        while (!pinned_plug_que_empty_p() &&
               (!ephemeral_pointer_p (pinned_plug (oldest_pin())) ||
                (pinned_plug (oldest_pin()) <
                 generation_limit (active_new_gen_number))))
        {
            size_t  entry = deque_pinned_plug();
            mark*  m = pinned_plug_of (entry);
            BYTE*  plug = pinned_plug (m);
            size_t  len = pinned_len (m);
            // detect pinned block in different segment (later) than
            // allocation segment
            heap_segment* nseg = generation_allocation_segment (consing_gen);
            while ((plug < generation_allocation_pointer (consing_gen)) ||
                   (plug >= heap_segment_allocated (nseg)))
            {
                //adjust the end of the segment to be the end of the plug
                assert (generation_allocation_pointer (consing_gen)>=
                        heap_segment_mem (nseg));
                assert (generation_allocation_pointer (consing_gen)<=
                        heap_segment_committed (nseg));

                heap_segment_plan_allocated (nseg) =
                    generation_allocation_pointer (consing_gen);
                //switch allocation segment
                nseg = heap_segment_next (nseg);
                generation_allocation_segment (consing_gen) = nseg;
                //reset the allocation pointer and limits
                generation_allocation_pointer (consing_gen) =
                    heap_segment_mem (nseg);
            }
            pinned_len(m) = (plug - generation_allocation_pointer (consing_gen));
            assert(pinned_len(m) == 0 ||
                   pinned_len(m) >= Align(min_obj_size));
            generation_allocation_pointer (consing_gen) = plug + len;
            generation_allocation_limit (consing_gen) =
                generation_allocation_pointer (consing_gen);
        }
        allocate_in_condemned = TRUE;
        consing_gen = ensure_ephemeral_heap_segment (consing_gen);
        set_allocator_next_pin(consing_gen);
        active_new_gen_number--;

        assert (active_new_gen_number>0);

        {
            generation_plan_allocation_start (generation_of (active_new_gen_number)) =
                allocate_in_condemned_generations (consing_gen, Align (min_obj_size), -1);
        }
        goto retry;
    }
}




void gc_heap::plan_phase (int condemned_gen_number)
{
    // %type%  category = quote (plan);
#ifdef TIME_GC
    unsigned start;
    unsigned finish;
    start = GetCycleCount32();
#endif

    dprintf (2,("---- Plan Phase ---- Condemned generation %d",
                condemned_gen_number));

    generation*  condemned_gen = generation_of (condemned_gen_number);








#ifdef MARK_LIST 
    BOOL use_mark_list = FALSE;
    BYTE** mark_list_next = &mark_list[0];
    dprintf (3, ("mark_list length: %d", 
                 mark_list_index - &mark_list[0]));
    if ((condemned_gen_number < max_generation) &&
        (mark_list_index <= mark_list_end))
    {
        qsort1 (&mark_list[0], mark_list_index-1);
        use_mark_list = TRUE;
    }else
        dprintf (3, ("mark_list not used"));
        
#endif //MARK_LIST

    if (shigh != (BYTE*)0)
    {
        heap_segment* seg = generation_start_segment (condemned_gen);
        do 
        {
            if (slow >= heap_segment_mem (seg) && 
                slow < heap_segment_reserved (seg))
            {
                if (seg == generation_start_segment (condemned_gen))
                {
                    BYTE* o = generation_allocation_start (condemned_gen) +
                        Align (size (generation_allocation_start (condemned_gen)));
                    if (slow > o)
                    {
                        assert ((slow - o) >= (int)Align (min_obj_size));
                        make_unused_array (o, slow - o);
                    }
                } else 
                {
                    assert (condemned_gen_number == max_generation);
                    make_unused_array (heap_segment_mem (seg),
                                       slow - heap_segment_mem (seg));
                }
            }
            if (shigh >= heap_segment_mem (seg) && 
                shigh < heap_segment_reserved (seg))
            {
                heap_segment_allocated (seg) =
                    shigh + Align (size (shigh));

            }
            // test if the segment is in the range of [slow, shigh]
            if (!((heap_segment_reserved (seg) >= slow) && 
                  (heap_segment_mem (seg) <= shigh)))
            {
                // shorten it to minimum
                heap_segment_allocated (seg) =  heap_segment_mem (seg);
            }
            seg = heap_segment_next (seg);
        } while (seg);
    } 
    else
    {
        heap_segment* seg = generation_start_segment (condemned_gen);
        do 
        {
            // shorten it to minimum
            if (seg == generation_start_segment (condemned_gen))
            {
                // no survivors make all generations look empty
                heap_segment_allocated (seg) =
                    generation_allocation_start (condemned_gen) + 
                    Align (size (generation_allocation_start (condemned_gen)));

            }
            else
            {
                assert (condemned_gen_number == max_generation);
                {
                    heap_segment_allocated (seg) =  heap_segment_mem (seg);
                }
            }
            seg = heap_segment_next (seg);
        } while (seg);
    }           


    heap_segment*  seg = generation_start_segment (condemned_gen);
    BYTE*  end = heap_segment_allocated (seg);
    BYTE*  first_condemned_address = generation_allocation_start (condemned_gen);
    BYTE*  x = first_condemned_address;

    assert (!marked (x));
    BYTE*  plug_end = x;
    BYTE*  tree = 0;
    size_t  sequence_number = 0;
    BYTE*  last_node = 0;
    size_t  current_brick = brick_of (x);
    BOOL  allocate_in_condemned = (condemned_gen_number == max_generation);
    int  active_old_gen_number = condemned_gen_number;
    int  active_new_gen_number = min (max_generation,
                                      1 + condemned_gen_number);
    generation*  older_gen = 0;
    generation* consing_gen = condemned_gen;
    BYTE*  r_free_list = 0;
    BYTE*  r_allocation_pointer = 0;
    BYTE*  r_allocation_limit = 0;
    heap_segment*  r_allocation_segment = 0;




    if ((condemned_gen_number < max_generation))
    {
        older_gen = generation_of (min (max_generation, 1 + condemned_gen_number));
        r_free_list = generation_free_list (older_gen);
        r_allocation_limit = generation_allocation_limit (older_gen);
        r_allocation_pointer = generation_allocation_pointer (older_gen);
        r_allocation_segment = generation_allocation_segment (older_gen);
        heap_segment* start_seg = generation_start_segment (older_gen);
        if (start_seg != ephemeral_heap_segment)
        {
            assert (condemned_gen_number == (max_generation - 1));
            while (start_seg && (start_seg != ephemeral_heap_segment))
            {
                assert (heap_segment_allocated (start_seg) >=
                        heap_segment_mem (start_seg));
                assert (heap_segment_allocated (start_seg) <=
                        heap_segment_reserved (start_seg));
                heap_segment_plan_allocated (start_seg) =
                    heap_segment_allocated (start_seg);
                start_seg = heap_segment_next (start_seg);
            }
        }

    }

    //reset all of the segment allocated sizes
    {
        heap_segment*  seg = generation_start_segment (condemned_gen);
        while (seg)
        {
            heap_segment_plan_allocated (seg) =
                heap_segment_mem (seg);
            seg = heap_segment_next (seg);
        }
    }
    int  condemned_gn = condemned_gen_number;
    int bottom_gen = 0;

    while (condemned_gn >= bottom_gen)
    {
        generation*  condemned_gen = generation_of (condemned_gn);
        generation_free_list (condemned_gen) = 0;
        generation_last_gap (condemned_gen) = 0;
        generation_free_list_space (condemned_gen) = 0;
        generation_allocation_size (condemned_gen) = 0;
        generation_plan_allocation_start (condemned_gen) = 0;
        generation_allocation_segment (condemned_gen) = generation_start_segment (condemned_gen);
        generation_allocation_pointer (condemned_gen) = generation_allocation_start (condemned_gen);
        generation_allocation_limit (condemned_gen) = generation_allocation_pointer (condemned_gen);
        condemned_gn--;
    }
    if ((condemned_gen_number == max_generation))
    {
        generation_plan_allocation_start (condemned_gen) = allocate_in_condemned_generations (consing_gen, Align (min_obj_size), -1);
// dformat (t, 3, "Reserving generation gap for max-generation at %x", generation_plan_allocation_start (condemned_gen));
    }
    dprintf(3,( " From %x to %x", (size_t)x, (size_t)end));

    BYTE* free_gap = 0; //keeps tracks of the last gap inserted for short plugs


    while (1)
    {
        if (x >= end)
        {
            assert (x == end);
            {
                heap_segment_allocated (seg) = plug_end;

                current_brick = update_brick_table (tree, current_brick, x, plug_end);
                sequence_number = 0;
                tree = 0;
            }
            if (heap_segment_next (seg))
            {
                seg = heap_segment_next (seg);
                end = heap_segment_allocated (seg);
                plug_end = x = heap_segment_mem (seg);
                current_brick = brick_of (x);
                dprintf(3,( " From %x to %x", (size_t)x, (size_t)end));
                continue;
            }
            else
            {
                break;
            }
        }
        if (marked (x))
        {
            BYTE*  plug_start = x;
            BOOL  pinned_plug_p = FALSE;
            if (seg == ephemeral_heap_segment)
                process_ephemeral_boundaries (x, active_new_gen_number,
                                              active_old_gen_number,
                                              consing_gen, 
                                              allocate_in_condemned,
                                              free_gap);
                
            if (current_brick != brick_of (x))
            {
                current_brick = update_brick_table (tree, current_brick, x, plug_end);
                sequence_number = 0;
                tree = 0;
            }
            set_gap_size (plug_start, plug_start - plug_end);
            dprintf(3,( "Gap size: %d before plug [%x,",
                        node_gap_size (plug_start), (size_t)plug_start));
            {
                BYTE* xl = x;
                while ((xl < end) && marked (xl))
                {
                    assert (xl < end);
                    if (pinned(xl))
                    {
                        pinned_plug_p = TRUE;
                        clear_pinned (xl);
                    }

                    clear_marked_pinned (xl);

                    dprintf(4, ("+%x+", (size_t)xl));
                    assert ((size (xl) > 0));
                    assert ((size (xl) <= LARGE_OBJECT_SIZE));

                    xl = xl + Align (size (xl));
                }
                assert (xl <= end);
                x = xl;
            }
            dprintf(3,( "%x[", (size_t)x));
            plug_end = x;
            BYTE*  new_address = 0;

            if (pinned_plug_p)
            {
                dprintf(3,( "[%x,%x[ pinned", (size_t)plug_start, (size_t)plug_end));
                dprintf(3,( "Gap: [%x,%x[", (size_t)plug_start - node_gap_size (plug_start),
                            (size_t)plug_start));
                enque_pinned_plug (consing_gen, plug_start, plug_end - plug_start);
                new_address = plug_start;
            }
            else
            {
                size_t ps = plug_end - plug_start;
                if (allocate_in_condemned)
                    new_address =
                        allocate_in_condemned_generations (consing_gen,
                                                           ps,
                                                           active_old_gen_number);
                else
                {
                    if (0 ==  (new_address = allocate_in_older_generation (older_gen, ps, active_old_gen_number)))
                    {
                        allocate_in_condemned = TRUE;
                        new_address = allocate_in_condemned_generations (consing_gen, ps, active_old_gen_number);
                    }
                }

                if (!new_address)
                {
                    //verify that we are at then end of the ephemeral segment
                    assert (generation_allocation_segment (consing_gen) ==
                            ephemeral_heap_segment);
                    //verify that we are near the end 
                    assert ((generation_allocation_pointer (consing_gen) + Align (ps)) < 
                            heap_segment_allocated (ephemeral_heap_segment));
                    assert ((generation_allocation_pointer (consing_gen) + Align (ps)) >
                            (heap_segment_allocated (ephemeral_heap_segment) + Align (min_obj_size)));
                }

            }

            dprintf(3,(" New address: [%x, %x[: %d", 
                       (size_t)new_address, (size_t)new_address + (plug_end - plug_start),
                       plug_end - plug_start));
#ifdef _DEBUG
            // detect forward allocation in the same segment
            if ((new_address > plug_start) &&
                (new_address < heap_segment_allocated (seg)))
                RetailDebugBreak();
#endif
            set_node_relocation_distance (plug_start, (new_address - plug_start));
            if (last_node && (node_relocation_distance (last_node) ==
                              (node_relocation_distance (plug_start) +
                               (int)node_gap_size (plug_start))))
            {
                dprintf(3,( " L bit set"));
                set_node_left (plug_start);
            }
            if (0 == sequence_number)
            {
                tree = plug_start;
            }
            tree = insert_node (plug_start, ++sequence_number, tree, last_node);
            last_node = plug_start;
        }
        else
        {
#ifdef MARK_LIST
            if (use_mark_list)
            {
               while ((mark_list_next < mark_list_index) && 
                      (*mark_list_next <= x))
               {
                   mark_list_next++;
               }
               if ((mark_list_next < mark_list_index) 
                   )
                   x = *mark_list_next;
               else
                   x = end;
            }
            else
#endif //MARK_LIST
            {
                BYTE* xl = x;
                while ((xl < end) && !marked (xl))
                {
                    dprintf (4, ("-%x-", (size_t)xl));
                    assert ((size (xl) > 0));
                    xl = xl + Align (size (xl));
                }
                assert (xl <= end);
                x = xl;
            }
        }
    }

    size_t fragmentation =
        generation_fragmentation (generation_of (condemned_gen_number),
                                  consing_gen, 
                                  heap_segment_allocated (ephemeral_heap_segment)
                                  );

    dprintf (2,("Fragmentation: %d", fragmentation));
    
    BOOL found_demoted_plug = FALSE;
    demotion_low = (BYTE*)(SSIZE_T)-1;
    demotion_high = heap_segment_allocated (ephemeral_heap_segment);  

    while (!pinned_plug_que_empty_p())
    {

#ifdef FREE_LIST_0 
        {
            BYTE* pplug = pinned_plug (oldest_pin());
            if ((found_demoted_plug == FALSE) && ephemeral_pointer_p (pplug))
            {
                dprintf (3, ("Demoting all pinned plugs beyond %x", (size_t)pplug));
                found_demoted_plug = TRUE;
                consing_gen = ensure_ephemeral_heap_segment (consing_gen);
                //allocate all of the generation gaps
                set_allocator_next_pin (consing_gen);
                while (active_new_gen_number > 0)
                {
                    active_new_gen_number--;
                    generation* gen = generation_of (active_new_gen_number);
                    generation_plan_allocation_start (gen) = 
                        allocate_in_condemned_generations (consing_gen, 
                                                           Align(min_obj_size),
                                                           -1);
                }
                consing_gen = generation_of (0);
                generation_allocation_pointer (consing_gen) = 
                    generation_plan_allocation_start (consing_gen) +
                    Align (min_obj_size);
                generation_allocation_limit (consing_gen) =
                    generation_allocation_pointer (consing_gen);
                //set the demote boundaries. 
                demotion_low = pplug;
                if (pinned_plug_que_empty_p())
                    break;
            }
        }
#endif //FREE_LIST_0

        size_t  entry = deque_pinned_plug();
        mark*  m = pinned_plug_of (entry);
        BYTE*  plug = pinned_plug (m);
        size_t  len = pinned_len (m);


        // detect pinned block in different segment (later) than
        // allocation segment
        heap_segment* nseg = generation_allocation_segment (consing_gen);
        while ((plug < generation_allocation_pointer (consing_gen)) ||
               (plug >= heap_segment_allocated (nseg)))
        {
            assert ((plug < heap_segment_mem (nseg)) ||
                    (plug > heap_segment_reserved (nseg)));
            //adjust the end of the segment to be the end of the plug
            assert (generation_allocation_pointer (consing_gen)>=
                    heap_segment_mem (nseg));
            assert (generation_allocation_pointer (consing_gen)<=
                    heap_segment_committed (nseg));

            heap_segment_plan_allocated (nseg) =
                generation_allocation_pointer (consing_gen);
            //switch allocation segment
            nseg = heap_segment_next (nseg);
            generation_allocation_segment (consing_gen) = nseg;
            //reset the allocation pointer and limits
            generation_allocation_pointer (consing_gen) =
                heap_segment_mem (nseg);
        }

        pinned_len(m) = (plug - generation_allocation_pointer (consing_gen));
        generation_allocation_pointer (consing_gen) = plug + len;
        generation_allocation_limit (consing_gen) =
            generation_allocation_pointer (consing_gen);
    }

    plan_generation_start (consing_gen);

    dprintf (2,("Fragmentation with pinned objects: %d",
                generation_fragmentation (generation_of (condemned_gen_number),
                                          consing_gen, 
                                          (generation_plan_allocation_start (youngest_generation)))));
    dprintf(2,("---- End of Plan phase ----"));
    BOOL should_expand = FALSE;
    BOOL should_compact= FALSE;

#ifdef TIME_GC
    finish = GetCycleCount32();
    plan_time = finish - start;
#endif

    should_compact = decide_on_compacting (condemned_gen_number, consing_gen,
                                           fragmentation, should_expand);



    demotion = ((demotion_high >= demotion_low) ? TRUE : FALSE);

        
    if (should_compact)
    {
        dprintf(1,( "**** Doing Compacting GC ****"));
        {
            if (should_expand)
            {
                ptrdiff_t sdelta;
                heap_segment* new_heap_segment = seg_manager->get_segment(sdelta);
                if (new_heap_segment)
                {
                    consing_gen = expand_heap(condemned_gen_number, 
                                              consing_gen, 
                                              new_heap_segment);
                    demotion_low = (BYTE*)(SSIZE_T)-1;
                    demotion_high = 0;
                    demotion = FALSE;
                }
                else
                {
                    should_expand = FALSE;
                }
            }
        }
        generation_allocation_limit (condemned_gen) = 
            generation_allocation_pointer (condemned_gen);
        if ((condemned_gen_number < max_generation))
        {
            // Fix the allocation area of the older generation
            fix_older_allocation_area (older_gen);

        }
        assert (generation_allocation_segment (consing_gen) ==
                ephemeral_heap_segment);

        relocate_phase (condemned_gen_number, first_condemned_address);
        {
            compact_phase (condemned_gen_number, first_condemned_address,
                           !demotion);
        }
        fix_generation_bounds (condemned_gen_number, consing_gen,
                               demotion);
        {
            assert (generation_allocation_limit (youngest_generation) ==
                    generation_allocation_pointer (youngest_generation));
        }
        if (condemned_gen_number >= (max_generation -1))
        {
            rearrange_heap_segments();

            if (should_expand)
            {
                    
                //fix the start_segment for the ephemeral generations
                for (int i = 0; i < max_generation; i++)
                {
                    generation* gen = generation_of (i);
                    generation_start_segment (gen) = ephemeral_heap_segment;
                    generation_allocation_segment (gen) = ephemeral_heap_segment;
                }
            }
                
            if (condemned_gen_number == max_generation)
            {
                decommit_heap_segment_pages (ephemeral_heap_segment);
                //reset_heap_segment_pages (ephemeral_heap_segment);
            }
        }
    
        {

            finalize_queue->UpdatePromotedGenerations (condemned_gen_number, !demotion);

            {
                ScanContext sc;
                sc.thread_number = heap_number;
                sc.promotion = FALSE;
                sc.concurrent = FALSE;
                // new generations bounds are set can call this guy
                if (!demotion)
                {
                    CNameSpace::GcPromotionsGranted(condemned_gen_number, 
                                                    max_generation, &sc);
                }
                else
                {
                    CNameSpace::GcDemote (&sc);
                }

            }

        }

        {
            mark_stack_bos = 0;
            unsigned int  gen_number = min (max_generation, 1 + condemned_gen_number);
            generation*  gen = generation_of (gen_number);
            BYTE*  low = generation_allocation_start (generation_of (gen_number-1));
            BYTE*  high =  heap_segment_allocated (ephemeral_heap_segment);
            while (!pinned_plug_que_empty_p())
            {
                mark*  m = pinned_plug_of (deque_pinned_plug());
                size_t len = pinned_len (m);
                BYTE*  arr = (pinned_plug (m) - len);
                dprintf(3,("Making unused array [%x %x[ before pin",
                           (size_t)arr, (size_t)arr + len));
                if (len != 0)
                {
                    assert (len >= Align (min_obj_size));
                    make_unused_array (arr, len);
                    // fix fully contained bricks + first one
                    // if the array goes beyong the first brick
                    size_t start_brick = brick_of (arr);
                    size_t end_brick = brick_of (arr + len);
                    if (end_brick != start_brick)
                    {
                        dprintf (3,
                                 ("Fixing bricks [%x, %x[ to point to unused array %x",
                                  start_brick, end_brick, (size_t)arr));
                        set_brick (start_brick,
                                   arr - brick_address (start_brick));
                        size_t brick = start_brick+1;
                        while (brick < end_brick)
                        {
                            set_brick (brick, start_brick - brick);
                            brick++;
                        }
                    }
                    while ((low <= arr) && (high > arr))
                    {
                        gen_number--;
                        assert ((gen_number >= 1) || found_demoted_plug);

                        gen = generation_of (gen_number);
                        if (gen_number >= 1)
                            low = generation_allocation_start (generation_of (gen_number-1));
                        else
                            low = high;
                    }

                    dprintf(3,("Putting it into generation %d", gen_number));
                    thread_gap (arr, len, gen);
                }
            }
        }

#ifdef _DEBUG
        for (int x = 0; x <= max_generation; x++)
        {
            assert (generation_allocation_start (generation_of (x)));
        }
#endif //_DEBUG



        {
#if 1 //maybe obsolete in the future.  clear cards during relocation
            if (!demotion)
            {
                //clear card for generation 1. generation 0 is empty
                clear_card_for_addresses (
                    generation_allocation_start (generation_of (1)),
                    generation_allocation_start (generation_of (0)));
            }
#endif
        }
        {
            if (!found_demoted_plug)
            {
                BYTE* start = generation_allocation_start (youngest_generation);
                assert (heap_segment_allocated (ephemeral_heap_segment) ==
                        (start + Align (size (start))));
            }
        }
    }
    else
    {
        ScanContext sc;
        sc.thread_number = heap_number;
        sc.promotion = FALSE;
        sc.concurrent = FALSE;

        dprintf(1,("**** Doing Mark and Sweep GC****"));
        if ((condemned_gen_number < max_generation))
        {
            generation_free_list (older_gen) = r_free_list;
            generation_allocation_limit (older_gen) = r_allocation_limit;
            generation_allocation_pointer (older_gen) = r_allocation_pointer;
            generation_allocation_segment (older_gen) = r_allocation_segment;
        }
        make_free_lists (condemned_gen_number
                        );
        {

            finalize_queue->UpdatePromotedGenerations (condemned_gen_number, TRUE);

            {
                CNameSpace::GcPromotionsGranted (condemned_gen_number, 
                                                 max_generation, &sc);

            }
        }



#ifdef _DEBUG
        for (int x = 0; x <= max_generation; x++)
        {
            assert (generation_allocation_start (generation_of (x)));
        }
#endif //_DEBUG
        {
            //clear card for generation 1. generation 0 is empty
            clear_card_for_addresses (
                generation_allocation_start (generation_of (1)),
                generation_allocation_start (generation_of (0)));
            assert ((heap_segment_allocated (ephemeral_heap_segment) ==
                     (generation_allocation_start (youngest_generation) +
                      Align (min_obj_size))));
        }
    }

}

/*****************************
Called after compact phase to fix all generation gaps
********************************/
void gc_heap::fix_generation_bounds (int condemned_gen_number, 
                                     generation* consing_gen, 
                                     BOOL demoting)
{
    assert (generation_allocation_segment (consing_gen) ==
            ephemeral_heap_segment);

    //assign the planned allocation start to the generation 
    int gen_number = condemned_gen_number;
    int bottom_gen = 0;

    while (gen_number >= bottom_gen)
    {
        generation*  gen = generation_of (gen_number);
        dprintf(3,("Fixing generation pointers for %x", gen_number));
        reset_allocation_pointers (gen, generation_plan_allocation_start (gen));
        make_unused_array (generation_allocation_start (gen), Align (min_obj_size));
        dprintf(3,(" start %x", (size_t)generation_allocation_start (gen)));
        gen_number--;
    }
    {
        alloc_allocated = heap_segment_plan_allocated(ephemeral_heap_segment);
        //reset the allocated size 
        BYTE* start = generation_allocation_start (youngest_generation);
        if (!demoting)
            assert ((start + Align (size (start))) ==
                    heap_segment_plan_allocated(ephemeral_heap_segment));
        heap_segment_allocated(ephemeral_heap_segment)=
            heap_segment_plan_allocated(ephemeral_heap_segment);
    }
}


BYTE* gc_heap::generation_limit (int gen_number)
{
    if ((gen_number <= 1))
        return heap_segment_reserved (ephemeral_heap_segment);
    else
        return generation_allocation_start (generation_of ((gen_number - 2)));
}

BYTE* gc_heap::allocate_at_end (size_t size)
{
    BYTE* start = heap_segment_allocated (ephemeral_heap_segment);
    size = Align (size);
    BYTE* result = start;
    {
        assert ((start + size) <= 
                heap_segment_reserved (ephemeral_heap_segment));
        if ((start + size) > 
            heap_segment_committed (ephemeral_heap_segment))
        {
            if (!grow_heap_segment (ephemeral_heap_segment, 
                                    align_on_page (start + size) -
                                    heap_segment_committed (ephemeral_heap_segment)))
            {
                assert (!"Memory exhausted during alloc_at_end");
                return 0;
            }

        }
    }
    heap_segment_allocated (ephemeral_heap_segment) += size;
    return result;
}



void gc_heap::make_free_lists (int condemned_gen_number
                               )
{

#ifdef TIME_GC
    unsigned start;
    unsigned finish;
    start = GetCycleCount32();
#endif


    generation* condemned_gen = generation_of (condemned_gen_number);
    BYTE* start_address = generation_allocation_start (condemned_gen);
    size_t  current_brick = brick_of (start_address);
    heap_segment* current_heap_segment = generation_start_segment (condemned_gen);
    BYTE*  end_address = heap_segment_allocated (current_heap_segment);
    size_t  end_brick = brick_of (end_address-1);
    make_free_args args;
    args.free_list_gen_number = min (max_generation, 1 + condemned_gen_number);
    args.current_gen_limit = (((condemned_gen_number == max_generation)) ?
                              (BYTE*)~0 :
                              (generation_limit (args.free_list_gen_number)));
    args.free_list_gen = generation_of (args.free_list_gen_number);
    args.highest_plug = 0;

    if ((start_address < end_address) || 
        (condemned_gen_number == max_generation))
    {
        while (1)
        {
            if ((current_brick > end_brick))
            {
                if (args.current_gen_limit == (BYTE*)~0)
                {
                    //We had an empty segment
                    //need to allocate the generation start
                    generation* gen = generation_of (max_generation);
                    BYTE* gap = heap_segment_mem (generation_start_segment (gen));
                    generation_allocation_start (gen) = gap;
                    heap_segment_allocated (generation_start_segment (gen)) = 
                        gap + Align (min_obj_size);
                    make_unused_array (gap, Align (min_obj_size));
                    reset_allocation_pointers (gen, gap);
                    dprintf (3, ("Start segment empty, fixin generation start of %d to: %x",
                                 max_generation, (size_t)gap));
                    args.current_gen_limit = generation_limit (args.free_list_gen_number);
                }                   
                if (heap_segment_next (current_heap_segment))
                {
                    current_heap_segment = heap_segment_next (current_heap_segment);
                    current_brick = brick_of (heap_segment_mem (current_heap_segment));
                    end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
                        
                    continue;
                }
                else
                {
                    break;
                }
            }
            {
                int brick_entry =  brick_table [ current_brick ];
                if ((brick_entry >= 0))
                {
                    make_free_list_in_brick (brick_address (current_brick) + brick_entry, &args);
                    dprintf(3,("Fixing brick entry %x to %x",
                               current_brick, (size_t)args.highest_plug));
                    set_brick (current_brick,
                               (args.highest_plug - brick_address (current_brick)));
                }
                else
                {
                    if ((brick_entry > -32768))
                    {

#ifdef _DEBUG
                        short offset = (short)(brick_of (args.highest_plug) - current_brick);
                        if (! ((offset == brick_entry)))
                        {
                            assert ((brick_entry == -1));
                        }
#endif //_DEBUG
                        //init to -1 for faster find_first_object
                        set_brick (current_brick, -1);
                    }
                }
            }
            current_brick++;
        }

    }
    {
        int bottom_gen = 0;
        BYTE*  start = heap_segment_allocated (ephemeral_heap_segment);
        generation* gen = generation_of (args.free_list_gen_number);
        args.free_list_gen_number--; 
        while (args.free_list_gen_number >= bottom_gen)
        {
            BYTE*  gap = 0;
            generation* gen = generation_of (args.free_list_gen_number);
            {
                gap = allocate_at_end (Align(min_obj_size));
            }
            //check out of memory
            if (gap == 0)
                return;
            generation_allocation_start (gen) = gap;
            {
                reset_allocation_pointers (gen, gap);
            }
            dprintf(3,("Fixing generation start of %d to: %x",
                       args.free_list_gen_number, (size_t)gap));
            make_unused_array (gap, Align (min_obj_size));

            args.free_list_gen_number--;
        }
        {
            //reset the allocated size 
            BYTE* start = generation_allocation_start (youngest_generation);
            alloc_allocated = start + Align (size (start));
        }
    }

#ifdef TIME_GC
        finish = GetCycleCount32();
        sweep_time = finish - start;

#endif

}


void gc_heap::make_free_list_in_brick (BYTE* tree, make_free_args* args)
{
    assert ((tree >= 0));
    {
        int  right_node = node_right_child (tree);
        int left_node = node_left_child (tree);
        args->highest_plug = 0;
        if (! (0 == tree))
        {
            if (! (0 == left_node))
            {
                make_free_list_in_brick (tree + left_node, args);

            }
            {
                BYTE*  plug = tree;
                size_t  gap_size = node_gap_size (tree);
                BYTE*  gap = (plug - gap_size);
                dprintf (3,("Making free list %x len %d in %d",
                        (size_t)gap, gap_size, args->free_list_gen_number));
                args->highest_plug = tree;
				//if we are consuming the free_top 
				if (gap == args->free_top)
					args->free_top = 0;
            gen_crossing:
                {
                    if ((args->current_gen_limit == (BYTE*)~0) ||
                        ((plug >= args->current_gen_limit) &&
                         ephemeral_pointer_p (plug)))
                    {
                        dprintf(3,(" Crossing Generation boundary at %x",
                               (size_t)args->current_gen_limit));
                        if (!(args->current_gen_limit == (BYTE*)~0))
                        {
                            args->free_list_gen_number--;
                            args->free_list_gen = generation_of (args->free_list_gen_number);
                        }
                        dprintf(3,( " Fixing generation start of %d to: %x",
                                args->free_list_gen_number, (size_t)gap));
                        {
                            reset_allocation_pointers (args->free_list_gen, gap);
                        }
                        {
                            args->current_gen_limit = generation_limit (args->free_list_gen_number);
                        }

                        if ((gap_size >= (2*Align (min_obj_size))))
                        {
                            dprintf(3,(" Splitting the gap in two %d left",
                                   gap_size));
                            make_unused_array (gap, Align(min_obj_size));
                            gap_size = (gap_size - Align(min_obj_size));
                            gap = (gap + Align(min_obj_size));
                        }
                        else 
                        {
                            make_unused_array (gap, gap_size);
                            gap_size = 0;
                        }
                        goto gen_crossing;
                    }
                }
#if defined (_DEBUG) && defined (CONCURRENT_GC)
                //this routine isn't thread safe if free_list isn't empty
                if ((args->free_list_gen == youngest_generation) &&
                    concurrent_gc_p)
                {
                    assert (generation_free_list(args->free_list_gen) == 0);
                }
#endif //_DEBUG

                thread_gap (gap, gap_size, args->free_list_gen);

            }
            if (! (0 == right_node))
            {
                make_free_list_in_brick (tree + right_node, args);
            }
        }
    }
}


void gc_heap::thread_gap (BYTE* gap_start, size_t size, generation*  gen)
{
    assert (generation_allocation_start (gen));
    if ((size > 0))
    {
        assert ((generation_start_segment (gen) != ephemeral_heap_segment) ||
                (gap_start > generation_allocation_start (gen)));
        // The beginning of a segment gap is not aligned
        assert (size >= Align (min_obj_size));
        make_unused_array (gap_start, size);
        dprintf(3,("Free List: [%x, %x[", (size_t)gap_start, (size_t)gap_start+size));
        if ((size >= min_free_list))
        {
			generation_free_list_space (gen) += size;

            free_list_slot (gap_start) = 0;

            BYTE* first = generation_free_list (gen);

            assert (gap_start != first);
            if (first == 0)
            {
                generation_free_list (gen) = gap_start;
            }
            //the following is necessary because the last free element
            //may have been truncated, and last_gap isn't updated. 
            else if (free_list_slot (first) == 0)
            {
                free_list_slot (first) = gap_start;
            }
            else
            {
                assert (gap_start != generation_last_gap (gen));
                assert (free_list_slot(generation_last_gap (gen)) == 0);
                free_list_slot (generation_last_gap (gen)) = gap_start;
            }
            generation_last_gap (gen) = gap_start;
        }
    }
}


void gc_heap::make_unused_array (BYTE* x, size_t size)
{
    assert (size >= Align (min_obj_size));
    ((CObjectHeader*)x)->SetFree(size);
    clear_card_for_addresses (x, x + Align(size));
}


inline
BYTE* tree_search (BYTE* tree, BYTE* old_address)
{
    BYTE* candidate = 0;
    int cn;
    while (1)
    {
        if (tree < old_address)
        {
            if ((cn = node_right_child (tree)) != 0)
            {
                assert (candidate < tree);
                candidate = tree;
                tree = tree + cn;
                continue;
            }
            else
                break;
        }
        else if (tree > old_address)
        {
            if ((cn = node_left_child (tree)) != 0)
            {
                tree = tree + cn;
                continue;
            }
            else
                break;
        } else
            break;
    }
    if (tree <= old_address)
        return tree;
    else if (candidate)
        return candidate;
    else
        return tree;
}


inline
void gc_heap::relocate_address (BYTE** pold_address THREAD_NUMBER_DCL)
{
    0 THREAD_NUMBER_ARG;
    BYTE* old_address = *pold_address;
    if (!((old_address >= gc_low) && (old_address < gc_high)))
        return ;
    // delta translates old_address into address_gc (old_address);
    size_t  brick = brick_of (old_address);
    int    brick_entry =  brick_table [ brick ];
    int    orig_brick_entry = brick_entry;
    BYTE*  new_address = old_address;
    if (! ((brick_entry == -32768)))
    {
    retry:
        {
            while (brick_entry < 0)
            {
                brick = (brick + brick_entry);
                brick_entry =  brick_table [ brick ];
            }
            BYTE* old_loc = old_address;
            BYTE* node = tree_search ((brick_address (brick) + brick_entry),
                                      old_loc);
            if ((node <= old_loc))
                new_address = (old_address + node_relocation_distance (node));
            else
            {
                if (node_left_p (node))
                {
                    dprintf(3,(" using L optimization for %x", (size_t)node));
                    new_address = (old_address + 
                                   (node_relocation_distance (node) + 
                                    node_gap_size (node)));
                }
                else
                {
                    brick = brick - 1;
                    brick_entry =  brick_table [ brick ];
                    goto retry;
                }
            }
        }
    }
    dprintf(3,(" %x->%x", (size_t)old_address, (size_t)new_address));

    *pold_address = new_address;
}

inline void
gc_heap::reloc_survivor_helper (relocate_args* args, BYTE** pval)
{
    THREAD_FROM_HEAP;
    relocate_address (pval THREAD_NUMBER_ARG);

    // detect if we are demoting an object
    if ((*pval < args->demoted_high) && 
        (*pval >= args->demoted_low))
    {
        dprintf(3, ("setting card %x:%x",
                    card_of((BYTE*)pval),
                    (size_t)pval));

        set_card (card_of ((BYTE*)pval));
    }
}


void gc_heap::relocate_survivors_in_plug (BYTE* plug, BYTE* plug_end, 
                                          relocate_args* args)

{
    dprintf(3,("Relocating pointers in Plug [%x,%x[", (size_t)plug, (size_t)plug_end));

    THREAD_FROM_HEAP;

    BYTE*  x = plug;
    while (x < plug_end)
    {
        size_t s = size (x);
        if (contain_pointers (x))
        {
            dprintf (3,("$%x$", (size_t)x));

            go_through_object (method_table(x), x, s, pval, 
                               {
                                   reloc_survivor_helper (args, pval);
                               });

        }
        assert (s > 0);
        x = x + Align (s);
    }
}


void gc_heap::relocate_survivors_in_brick (BYTE* tree,  relocate_args* args)
{
    assert ((tree != 0));
    if (node_left_child (tree))
    {
        relocate_survivors_in_brick (tree + node_left_child (tree), args);
    }
    {
        BYTE*  plug = tree;
        size_t  gap_size = node_gap_size (tree);
        BYTE*  gap = (plug - gap_size);
        if (args->last_plug)
        {
            BYTE*  last_plug_end = gap;
            relocate_survivors_in_plug (args->last_plug, last_plug_end, args);
        }
        args->last_plug = plug;
    }
    if (node_right_child (tree))
    {
        relocate_survivors_in_brick (tree + node_right_child (tree), args);

    }
}


void gc_heap::relocate_survivors (int condemned_gen_number,
                                  BYTE* first_condemned_address)
{
    generation* condemned_gen = generation_of (condemned_gen_number);
    BYTE*  start_address = first_condemned_address;
    size_t  current_brick = brick_of (start_address);
    heap_segment*  current_heap_segment = generation_start_segment (condemned_gen);
    size_t  end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
    relocate_args args;
    args.low = gc_low;
    args.high = gc_high;
    args.demoted_low = demotion_low;
    args.demoted_high = demotion_high;
    args.last_plug = 0;
    while (1)
    {
        if (current_brick > end_brick)
        {
            if (args.last_plug)
            {
                relocate_survivors_in_plug (args.last_plug, 
                                            heap_segment_allocated (current_heap_segment),
                                            &args);
                args.last_plug = 0;
            }
            if (heap_segment_next (current_heap_segment))
            {
                current_heap_segment = heap_segment_next (current_heap_segment);
                current_brick = brick_of (heap_segment_mem (current_heap_segment));
                end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
                continue;
            }
            else
            {
                break;
            }
        }
        {
            int brick_entry =  brick_table [ current_brick ];
            if (brick_entry >= 0)
            {
                relocate_survivors_in_brick (brick_address (current_brick) +
                                             brick_entry,
                                             &args);
            }
        }
        current_brick++;
    }
}

#ifdef GC_PROFILING
void gc_heap::walk_relocation_in_brick (BYTE* tree, BYTE*& last_plug, size_t& last_plug_relocation, void *pHeapId)
{
    assert ((tree != 0));
    if (node_left_child (tree))
    {
        walk_relocation_in_brick (tree + node_left_child (tree), last_plug, last_plug_relocation, pHeapId);
    }

    {
        BYTE*  plug = tree;
        size_t  gap_size = node_gap_size (tree);
        BYTE*  gap = (plug - gap_size);
        if (last_plug)
        {
            BYTE*  last_plug_end = gap;

            // Now notify the profiler of this particular memory block move
            g_profControlBlock.pProfInterface->MovedReference(last_plug,
                                                              last_plug_end,
                                                              last_plug_relocation,
                                                              pHeapId);
        }

        // Store the information for the next plug
        last_plug = plug;
        last_plug_relocation = node_relocation_distance (tree);
    }

    if (node_right_child (tree))
    {
        walk_relocation_in_brick (tree + node_right_child (tree), last_plug, last_plug_relocation, pHeapId);

    }
}


void gc_heap::walk_relocation (int condemned_gen_number,
                               BYTE* first_condemned_address,
                               void *pHeapId)

{
    generation* condemned_gen = generation_of (condemned_gen_number);
    BYTE*  start_address = first_condemned_address;
    size_t  current_brick = brick_of (start_address);
    heap_segment*  current_heap_segment = generation_start_segment (condemned_gen);
    size_t  end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
    BYTE* last_plug = 0;
    size_t last_plug_relocation = 0;
    while (1)
    {
        if (current_brick > end_brick)
        {
            if (last_plug)
            {
                BYTE *tree = brick_address(current_brick) +
                                brick_table [ current_brick ];

                // Now notify the profiler of this particular memory block move
                HRESULT hr = g_profControlBlock.pProfInterface->
                                    MovedReference(last_plug,
                                                   heap_segment_allocated (current_heap_segment),
                                                   last_plug_relocation,
                                                   pHeapId);
                                                
                last_plug = 0;
            }
            if (heap_segment_next (current_heap_segment))
            {
                current_heap_segment = heap_segment_next (current_heap_segment);
                current_brick = brick_of (heap_segment_mem (current_heap_segment));
                end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
                continue;
            }
            else
            {
                break;
            }
        }
        {
            int brick_entry =  brick_table [ current_brick ];
            if (brick_entry >= 0)
            {
                walk_relocation_in_brick (brick_address (current_brick) +
                                          brick_entry,
                                          last_plug,
                                          last_plug_relocation,
                                          pHeapId);
            }
        }
        current_brick++;
    }

    // Notify the EE-side profiling code that all the references have been traced for
    // this heap, and that it needs to flush all cached data it hasn't sent to the
    // profiler and release resources it no longer needs.
    g_profControlBlock.pProfInterface->EndMovedReferences(pHeapId);
}

#endif //GC_PROFILING

void gc_heap::relocate_phase (int condemned_gen_number,
                              BYTE* first_condemned_address)
{
    ScanContext sc;
    sc.thread_number = heap_number;
    sc.promotion = FALSE;
    sc.concurrent = FALSE;


#ifdef TIME_GC
        unsigned start;
        unsigned finish;
        start = GetCycleCount32();
#endif

//  %type%  category = quote (relocate);
    dprintf(2,("---- Relocate phase -----"));




    {
    }

    dprintf(3,("Relocating roots"));
    CNameSpace::GcScanRoots(GCHeap::Relocate,
                            condemned_gen_number, max_generation, &sc);


    if (condemned_gen_number != max_generation)
    {
        dprintf(3,("Relocating cross generation pointers"));
        mark_through_cards_for_segments (relocate_address, TRUE);
    }
    {
        dprintf(3,("Relocating survivors"));
        relocate_survivors (condemned_gen_number, 
                            first_condemned_address);
#ifdef GC_PROFILING

        // This provides the profiler with information on what blocks of
        // memory are moved during a gc.
        if (CORProfilerTrackGC())
        {
            size_t heapId = 0;

            // Now walk the portion of memory that is actually being relocated.
            walk_relocation(condemned_gen_number, first_condemned_address, (void *)&heapId);
        }
#endif GC_PROFILING
    }

        dprintf(3,("Relocating finalization data"));
        finalize_queue->RelocateFinalizationData (condemned_gen_number,
                                                       __this);


    {
        dprintf(3,("Relocating handle table"));
        CNameSpace::GcScanHandles(GCHeap::Relocate,
                                  condemned_gen_number, max_generation, &sc);

        if (condemned_gen_number != max_generation)
        {
            dprintf(3,("Relocating cross generation pointers for large objects"));
            mark_through_cards_for_large_objects (relocate_address, TRUE);
        } 
        else
        {
            {
                relocate_in_large_objects ();
            }
        }
    }


#ifdef TIME_GC
        finish = GetCycleCount32();
        reloc_time = finish - start;
#endif

    dprintf(2,( "---- End of Relocate phase ----"));

}

inline
void  gc_heap::gcmemcopy (BYTE* dest, BYTE* src, size_t len, BOOL copy_cards_p)
{
    if (dest != src)
    {
        dprintf(3,(" Memcopy [%x->%x, %x->%x[", (size_t)src, (size_t)dest, (size_t)src+len, (size_t)dest+len));
        memcopy (dest - plug_skew, src - plug_skew, len);
        if (copy_cards_p)
            copy_cards_for_addresses (dest, src, len);
        else
            clear_card_for_addresses (dest, dest + len);
    }
}




void gc_heap::compact_plug (BYTE* plug, size_t size, compact_args* args)
{
    dprintf (3, ("compact_plug [%x, %x[", (size_t)plug, (size_t)plug+size));
    BYTE* reloc_plug = plug + args->last_plug_relocation;
    gcmemcopy (reloc_plug, plug, size, args->copy_cards_p);
    size_t current_reloc_brick = args->current_compacted_brick;

    if (brick_of (reloc_plug) != current_reloc_brick)
    {
        if (args->before_last_plug)
        {
            dprintf(3,(" fixing brick %x to point to plug %x",
                     current_reloc_brick, (size_t)args->last_plug));
            set_brick (current_reloc_brick,
                       args->before_last_plug - brick_address (current_reloc_brick));
        }
        current_reloc_brick = brick_of (reloc_plug);
    }
    size_t end_brick = brick_of (reloc_plug + size-1);
    if (end_brick != current_reloc_brick)
    {
        // The plug is straddling one or more bricks
        // It has to be the last plug of its first brick
        dprintf (3,("Fixing bricks [%x, %x[ to point to plug %x",
                 current_reloc_brick, end_brick, (size_t)reloc_plug));
        set_brick (current_reloc_brick,
                   reloc_plug - brick_address (current_reloc_brick));
        // update all intervening brick
        size_t brick = current_reloc_brick + 1;
        while (brick < end_brick)
        {
            set_brick (brick, -1);
            brick++;
        }
        // code last brick offset as a plug address
        args->before_last_plug = brick_address (end_brick) -1;
        current_reloc_brick = end_brick;
    } else
        args->before_last_plug = reloc_plug;
    args->current_compacted_brick = current_reloc_brick;
}


void gc_heap::compact_in_brick (BYTE* tree, compact_args* args)
{
    assert (tree >= 0);
    int   left_node = node_left_child (tree);
    int   right_node = node_right_child (tree);
    size_t relocation = node_relocation_distance (tree);
    if (left_node)
    {
        compact_in_brick ((tree + left_node), args);
    }
    BYTE*  plug = tree;
    if (args->last_plug != 0)
    {
        size_t gap_size = node_gap_size (tree);
        BYTE*   gap = (plug - gap_size);
        BYTE*  last_plug_end = gap;
        size_t  last_plug_size = (last_plug_end - args->last_plug);
        compact_plug (args->last_plug, last_plug_size, args);
    }
    args->last_plug = plug;
    args->last_plug_relocation = relocation;
    if (right_node)
    {
        compact_in_brick ((tree + right_node), args);

    }

}


void gc_heap::compact_phase (int condemned_gen_number, 
                             BYTE*  first_condemned_address,
                             BOOL clear_cards)
{
//  %type%  category = quote (compact);
#ifdef TIME_GC
        unsigned start;
        unsigned finish;
        start = GetCycleCount32();
#endif
    generation*   condemned_gen = generation_of (condemned_gen_number);
    BYTE*  start_address = first_condemned_address;
    size_t   current_brick = brick_of (start_address);
    heap_segment*  current_heap_segment = generation_start_segment (condemned_gen);
    BYTE*  end_address = heap_segment_allocated (current_heap_segment);
    size_t  end_brick = brick_of (end_address-1);
    compact_args args;
    args.last_plug = 0;
    args.before_last_plug = 0;
    args.current_compacted_brick = ~1u;
    args.copy_cards_p =  (condemned_gen_number >= 1) || !clear_cards;
    dprintf(2,("---- Compact Phase ----"));



    if ((start_address < end_address) ||
        (condemned_gen_number == max_generation))
    {
        while (1)
        {
            if (current_brick > end_brick)
            {
                if (args.last_plug != 0)
                {
                    compact_plug (args.last_plug,
                                  (heap_segment_allocated (current_heap_segment) - args.last_plug),
                                  &args);
                }
                if (heap_segment_next (current_heap_segment))
                {
                    current_heap_segment = heap_segment_next (current_heap_segment);
                    current_brick = brick_of (heap_segment_mem (current_heap_segment));
                    end_brick = brick_of (heap_segment_allocated (current_heap_segment)-1);
                    args.last_plug = 0;
                    continue;
                }
                else
                {
                    dprintf (3,("Fixing last brick %x to point to plug %x",
                              args.current_compacted_brick, (size_t)args.before_last_plug));
                    set_brick (args.current_compacted_brick,
                               args.before_last_plug - brick_address (args.current_compacted_brick));
                    break;
                }
            }
            {
                int  brick_entry =  brick_table [ current_brick ];
                if (brick_entry >= 0)
                {
                    compact_in_brick ((brick_entry + brick_address (current_brick)),
                                      &args);

                }
            }
            current_brick++;
        }
    }
#ifdef TIME_GC
    finish = GetCycleCount32();
    compact_time = finish - start;
#endif

        dprintf(2,("---- End of Compact phase ----"));
}






/*------------------ Concurrent GC ----------------------------*/



//extract the low bits [0,low[ of a DWORD
#define lowbits(wrd, bits) ((wrd) & ((1 << (bits))-1))
//extract the high bits [high, 32] of a DWORD
#define highbits(wrd, bits) ((wrd) & ~((1 << (bits))-1))


//Clear the cards [start_card, end_card[

void gc_heap::clear_cards (size_t start_card, size_t end_card)
{
    if (start_card < end_card)
    {
        size_t start_word = card_word (start_card);
        size_t end_word = card_word (end_card);
        if (start_word < end_word)
        {
            unsigned bits = card_bit (start_card);
            card_table [start_word] &= lowbits (~0, bits);
            for (size_t i = start_word+1; i < end_word; i++)
                card_table [i] = 0;
            bits = card_bit (end_card);
            card_table [end_word] &= highbits (~0, bits);
        }
        else
        {
            card_table [start_word] &= (lowbits (~0, card_bit (start_card)) | 
                                        highbits (~0, card_bit (end_card)));
        }
#ifdef VERYSLOWDEBUG
        size_t  card = start_card;
        while (card < end_card)
        {
            assert (! (card_set_p (card)));
            card++;
        }
#endif
        dprintf (3,("Cleared cards [%x:%x, %x:%x[",
                  start_card, (size_t)card_address (start_card),
                  end_card, (size_t)card_address (end_card)));
    }
}


void gc_heap::clear_card_for_addresses (BYTE* start_address, BYTE* end_address)
{
    size_t   start_card = card_of (align_on_card (start_address));
    size_t   end_card = card_of (align_lower_card (end_address));
    clear_cards (start_card, end_card);
}

// copy [srccard, ...[ to [dst_card, end_card[
inline
void gc_heap::copy_cards (size_t dst_card, size_t src_card,
                 size_t end_card, BOOL nextp)
{
    unsigned int srcbit = card_bit (src_card);
    unsigned int dstbit = card_bit (dst_card);
    size_t srcwrd = card_word (src_card);
    size_t dstwrd = card_word (dst_card);
    unsigned int srctmp = card_table[srcwrd];
    unsigned int dsttmp = card_table[dstwrd];
    for (size_t card = dst_card; card < end_card; card++)
    {
        if (srctmp & (1 << srcbit))
            dsttmp |= 1 << dstbit;
        else
            dsttmp &= ~(1 << dstbit);
        if (!(++srcbit % 32))
        {
            srctmp = card_table[++srcwrd];
            srcbit = 0;
        }
        if (nextp)
        {
            if (srctmp & (1 << srcbit))
                dsttmp |= 1 << dstbit;
        }
        if (!(++dstbit % 32))
        {
            card_table[dstwrd] = dsttmp;
            dstwrd++;
            dsttmp = card_table[dstwrd];
            dstbit = 0;
        }
    }
    card_table[dstwrd] = dsttmp;
}




void gc_heap::copy_cards_for_addresses (BYTE* dest, BYTE* src, size_t len)
{
    ptrdiff_t relocation_distance = src - dest;
    size_t start_dest_card = card_of (align_on_card (dest));
    size_t end_dest_card = card_of (dest + len - 1);
    size_t dest_card = start_dest_card;
    size_t src_card = card_of (card_address (dest_card)+relocation_distance);
    dprintf (3,("Copying cards [%x:%x->%x:%x, %x->%x:%x[",
              src_card, (size_t)src, dest_card, (size_t)dest,
              (size_t)src+len, end_dest_card, (size_t)dest+len));

    //First card has two boundaries
    if (start_dest_card != card_of (dest))
        if (card_set_p (card_of (card_address (start_dest_card) + relocation_distance)))
            set_card (card_of (dest));

    if (card_set_p (card_of (src)))
        set_card (card_of (dest));


    copy_cards (dest_card, src_card, end_dest_card,
                ((dest - align_lower_card (dest)) != (src - align_lower_card (src))));


    //Last card has two boundaries.
    if (card_set_p (card_of (card_address (end_dest_card) + relocation_distance)))
        set_card (end_dest_card);

    if (card_set_p (card_of (src + len - 1)))
        set_card (end_dest_card);
}


void gc_heap::fix_brick_to_highest (BYTE* o, BYTE* next_o)
{
    size_t new_current_brick = brick_of (o);
    dprintf(3,(" fixing brick %x to point to object %x",
               new_current_brick, (size_t)o));
    set_brick (new_current_brick,
               (o - brick_address (new_current_brick)));
    size_t b = 1 + new_current_brick;
    size_t limit = brick_of (next_o);
    while (b < limit)
    {
        set_brick (b,(new_current_brick - b));
        b++;
    }

}


BYTE* gc_heap::find_first_object (BYTE* start,  size_t brick, BYTE* min_address)
{

    assert (brick == brick_of (start));
    ptrdiff_t  min_brick = (ptrdiff_t)brick_of (min_address);
    ptrdiff_t  prev_brick = (ptrdiff_t)brick - 1;
    int         brick_entry = 0;
    while (1)
    {
        if (prev_brick < min_brick)
        {
            break;
        }
        if ((brick_entry =  brick_table [ prev_brick ]) >= 0)
        {
            break;
        }
        assert (! ((brick_entry == -32768)));
        prev_brick = (brick_entry + prev_brick);

    }

    BYTE* o = ((prev_brick < min_brick) ? min_address :
                      brick_address (prev_brick) + brick_entry);
    assert (o <= start);
    assert (size (o) >= Align (min_obj_size));
    BYTE*  next_o = o + Align (size (o));
    size_t curr_cl = (size_t)next_o / brick_size;
    size_t min_cl = (size_t)min_address / brick_size;

    dprintf (3,( "Looking for intersection with %x from %x", (size_t)start, (size_t)o));
#ifdef TRACE_GC
    unsigned int n_o = 1;
#endif

    BYTE* next_b = min (align_lower_brick (next_o) + brick_size, start+1);

    while (next_o <= start)
    {
        do 
        {
#ifdef TRACE_GC
            n_o++;
#endif
            o = next_o;
            assert (size (o) >= Align (min_obj_size));
            next_o = o + Align (size (o));
        }while (next_o < next_b);

        if (((size_t)next_o / brick_size) != curr_cl)
        {
            if (curr_cl >= min_cl)
            {
                fix_brick_to_highest (o, next_o);
            }
            curr_cl = (size_t) next_o / brick_size;
        }
        next_b = min (align_lower_brick (next_o) + brick_size, start+1);
    }

    dprintf (3, ("Looked at %d objects", n_o));
    size_t bo = brick_of (o);
    if (bo < brick)
    {
        set_brick (bo, (o - brick_address(bo)));
        size_t b = 1 + bo;
        size_t limit = brick - 1;
        int x = -1;
        while (b < brick)
        {
            set_brick (b,x--);
            b++;
        }
    }

    return o;
}




void find_card (DWORD* card_table, size_t& card, 
                size_t card_word_end, size_t& end_card)
{
    DWORD* last_card_word;
    DWORD y;
    DWORD z;
    // Find the first card which is set

    last_card_word = &card_table [card_word (card)];
    z = card_bit (card);
    y = (*last_card_word) >> z;
    if (!y)
    {
        z = 0;
        do
        {
            ++last_card_word;
        }
        while ((last_card_word < &card_table [card_word_end]) && 
               !(*last_card_word));
        if (last_card_word < &card_table [card_word_end])
            y = *last_card_word;
    }

    // Look for the lowest bit set
    if (y)
        while (!(y & 1))
        {
            z++;
            y = y / 2;
        }
    card = (last_card_word - &card_table [0])* card_word_width + z;
    do 
    {
        z++;
        y = y / 2;
    } while (y & 1);
    end_card = (last_card_word - &card_table [0])* card_word_width + z;
}


    //because of heap expansion, computing end is complicated. 
BYTE* compute_next_end (heap_segment* seg, BYTE* low)
{
    if ((low >=  heap_segment_mem (seg)) && 
        (low < heap_segment_allocated (seg)))
        return low;
    else
        return heap_segment_allocated (seg);
}


BYTE* 
gc_heap::compute_next_boundary (BYTE* low, int gen_number, 
                                BOOL relocating)
{
    //when relocating, the fault line is the plan start of the younger 
    //generation because the generation is promoted. 
    if (relocating && (gen_number == (condemned_generation_num+1)))
    {
        generation* gen = generation_of (gen_number - 1);
        BYTE* gen_alloc = generation_plan_allocation_start (gen);
        assert (gen_alloc);
        return gen_alloc;
    } 
    else
    {
        assert (gen_number > condemned_generation_num);
        return generation_allocation_start (generation_of (gen_number - 1 ));
    }
            
}

inline void 
gc_heap::mark_through_cards_helper (BYTE** poo, unsigned int& n_gen, 
                                    unsigned int& cg_pointers_found, 
                                    card_fn fn, BYTE* nhigh, 
                                    BYTE* next_boundary)
{
    THREAD_FROM_HEAP;
    if ((gc_low <= *poo) && (gc_high > *poo))
    {
        n_gen++;
        call_fn(fn) (poo THREAD_NUMBER_ARG);
    }
    if ((next_boundary <= *poo) && (nhigh > *poo))
    {
        cg_pointers_found ++;
    }
}

void gc_heap::mark_through_cards_for_segments (card_fn fn, BOOL relocating)
{
    size_t  card;
    BYTE* low = gc_low;
    BYTE* high = gc_high;
    size_t  end_card          = 0;
    generation*   oldest_gen        = generation_of (max_generation);
    int           curr_gen_number   = max_generation;
    BYTE*         gen_boundary      = generation_allocation_start 
                                       (generation_of (curr_gen_number - 1));
    BYTE*         next_boundary     = (compute_next_boundary 
                                        (gc_low, curr_gen_number, relocating));
    heap_segment* seg               = generation_start_segment (oldest_gen);
    BYTE*         beg               = generation_allocation_start (oldest_gen);
    BYTE*         end               = compute_next_end (seg, low);
    size_t        last_brick        = ~1u;
    BYTE*         last_object       = beg;

    unsigned int  cg_pointers_found = 0;

    size_t  card_word_end = (card_of (align_on_card_word (end)) / 
                                   card_word_width);

    unsigned int  n_eph             = 0;
    unsigned int  n_gen             = 0;
    unsigned int  n_card_set        = 0;
    BYTE*         nhigh             = relocating ? 
        heap_segment_plan_allocated (ephemeral_heap_segment) : high;

    THREAD_FROM_HEAP;

    dprintf(3,( "scanning from %x to %x", (size_t)beg, (size_t)end));
    card        = card_of (beg);
    while (1)
    {
        if (card >= end_card)
            find_card (card_table, card, card_word_end, end_card);
        if ((last_object >= end) || (card_address (card) >= end))
        {
            if ( (seg = heap_segment_next (seg)) != 0)
            {
                beg = heap_segment_mem (seg);
                end = compute_next_end (seg, low);
                card_word_end = card_of (align_on_card_word (end)) / card_word_width;
                card = card_of (beg);
                last_object = beg;
                last_brick = ~1u;
                end_card = 0;
                continue;
            }
            else
            {
                break;
            }
        }
        n_card_set++;
        assert (card_set_p (card));
        {
            BYTE*   start_address = max (beg, card_address (card));
            size_t  brick         = brick_of (start_address);
            BYTE*   o;

            // start from the last object if in the same brick or
            // if the last_object already intersects the card
            if ((brick == last_brick) || (start_address <= last_object))
            {
                o = last_object;
            }
            else if (brick_of (beg) == brick)
                    o = beg;
            else
            {
                o = find_first_object (start_address, brick, last_object);
                //Never visit an object twice.
                assert (o >= last_object);
            }

            BYTE* limit             = min (end, card_address (end_card));
            dprintf(3,("Considering card %x start object: %x, %x[ ",
                       card, (size_t)o, (size_t)limit));
            while (o < limit)
            {
                if ((o >= gen_boundary) &&
                    (seg == ephemeral_heap_segment))
                {
                    curr_gen_number--;
                    assert ((curr_gen_number > 0));
                    gen_boundary = generation_allocation_start
                        (generation_of (curr_gen_number - 1));
                    next_boundary = (compute_next_boundary 
                                     (low, curr_gen_number, relocating));
                }
                assert (size (o) >= Align (min_obj_size));
                size_t s = size (o);

                BYTE* next_o =  o + Align (s);

// dformat (t, 4, "|%x|", o);
                if (card_of (o) > card)
                {
                    if (cg_pointers_found == 0)
                    {
                        dprintf(3,(" Clearing cards [%x, %x[ ", (size_t)card_address(card), (size_t)o));
                        clear_cards (card, card_of(o));
                    }
                    n_eph +=cg_pointers_found;
                    cg_pointers_found = 0;
                    card = card_of (o);
                }
                if ((next_o >= start_address) && contain_pointers (o))
                {
                    dprintf(3,("Going through %x", (size_t)o));
                    

                    go_through_object (method_table(o), o, s, poo,
                       {
                           mark_through_cards_helper (poo, n_gen,
                                                      cg_pointers_found, fn,
                                                      nhigh, next_boundary);
                       }
                        );
                    dprintf (3, ("Found %d cg pointers", cg_pointers_found));
                }
                if (((size_t)next_o / brick_size) != ((size_t) o / brick_size))
                {
                    if (brick_table [brick_of (o)] <0)
                        fix_brick_to_highest (o, next_o);
                }
                o = next_o;
            }
            if (cg_pointers_found == 0)
            {
                dprintf(3,(" Clearing cards [%x, %x[ ", (size_t)o, (size_t)limit));
                clear_cards (card, card_of (limit));
            }

            n_eph +=cg_pointers_found;
            cg_pointers_found = 0;

            card = card_of (o);
            last_object = o;
            last_brick = brick;
        }
    }
    // compute the efficiency ratio of the card table
    if (!relocating)
    {
        dprintf (2, ("cross pointers: %d, useful ones: %d", n_eph, n_gen));
        generation_skip_ratio = (((n_card_set > 60) && (n_eph > 0))? 
                                 (n_gen*100) / n_eph : 100);
        dprintf (2, ("generation_skip_ratio: %d %", generation_skip_ratio));
    }

}

void gc_heap::realloc_plug (size_t last_plug_size, BYTE*& last_plug,
                            generation* gen, BYTE* start_address,
                            unsigned int& active_new_gen_number,
                            BYTE*& last_pinned_gap, BOOL& leftp,
                            size_t current_page)
{
    // We know that all plugs are contiguous in memory, except for first plug in generation.
    // detect generation boundaries
    // make sure that active_new_gen_number is not the youngest generation.
    // because the generation_limit wouldn't return the right thing in this case.
    if ((active_new_gen_number > 1) &&
        (last_plug >= generation_limit (active_new_gen_number)))
    {
        assert (last_plug >= start_address);
        active_new_gen_number--;
        generation_plan_allocation_start (generation_of (active_new_gen_number)) =
            allocate_in_condemned_generations (gen, Align(min_obj_size), -1);
        leftp = FALSE;
    }
    // detect pinned plugs
    if (!pinned_plug_que_empty_p() && (last_plug == oldest_pin()->first))
    {
        size_t  entry = deque_pinned_plug();
        mark*  m = pinned_plug_of (entry);
        BYTE*  plug = pinned_plug (m);
        size_t  len = pinned_len (m);
        dprintf(3,("Adjusting pinned gap: [%x, %x[", (size_t)last_pinned_gap, (size_t)last_plug));
        pinned_len(m) = last_plug - last_pinned_gap;
        last_pinned_gap = last_plug + last_plug_size;
        leftp = FALSE;
    }
    else if (last_plug >= start_address)
    {

        BYTE* new_address = allocate_in_condemned_generations (gen, last_plug_size, -1);
        assert (new_address);
        set_node_relocation_distance (last_plug, new_address - last_plug);
        if (leftp)
            set_node_left (last_plug);
        dprintf(3,(" Re-allocating %x->%x len %d", (size_t)last_plug, (size_t)new_address, last_plug_size));

            leftp = TRUE;


    }

}

void gc_heap::realloc_in_brick (BYTE* tree, BYTE*& last_plug, 
                                BYTE* start_address,
                                generation* gen,
                                unsigned int& active_new_gen_number,
                                BYTE*& last_pinned_gap, BOOL& leftp, 
                                size_t current_page)
{
    assert (tree >= 0);
    int   left_node = node_left_child (tree);
    int   right_node = node_right_child (tree);
    if (left_node)
    {
        realloc_in_brick ((tree + left_node), last_plug, start_address,
                          gen, active_new_gen_number, last_pinned_gap, 
                          leftp, current_page);
    }

    if (last_plug != 0)
    {
        BYTE*  plug = tree;
        size_t gap_size = node_gap_size (plug);
        BYTE*   gap = (plug - gap_size);
        BYTE*  last_plug_end = gap;
        size_t  last_plug_size = (last_plug_end - last_plug);
        realloc_plug (last_plug_size, last_plug, gen, start_address,
                      active_new_gen_number, last_pinned_gap, 
                      leftp, current_page);
    }
    last_plug = tree;

    if (right_node)
    {
        realloc_in_brick ((tree + right_node), last_plug, start_address,
                          gen, active_new_gen_number, last_pinned_gap,
                          leftp, current_page);
    }

}

void
gc_heap::realloc_plugs (generation* consing_gen, heap_segment* seg,
                        BYTE* start_address, BYTE* end_address,
                        unsigned active_new_gen_number)
{
    BYTE* first_address = start_address;
    //when there is demotion, we need to fix the pinned plugs 
    //starting at demotion if they were in gen1 (not normally considered here)
    if (demotion)
    {
        if (demotion_low < first_address)
            first_address = demotion_low;
    }
    size_t  current_brick = brick_of (first_address);
    size_t  end_brick = brick_of (end_address-1);
    BYTE*  last_plug = 0;
    //Look for the right pinned plug to start from.
    mark_stack_bos = 0;
    while (!pinned_plug_que_empty_p())
    {
        mark* m = oldest_pin();
        if ((m->first >= first_address) && (m->first < end_address))
            break;
        else
            deque_pinned_plug();
    }

    BYTE* last_pinned_gap = heap_segment_plan_allocated (seg);
    BOOL leftp = FALSE;
    size_t current_page = ~0;

    while (current_brick <= end_brick)
    {
        int   brick_entry =  brick_table [ current_brick ];
        if (brick_entry >= 0)
        {
            realloc_in_brick ((brick_entry + brick_address (current_brick)),
                              last_plug, start_address, consing_gen,
                              active_new_gen_number, last_pinned_gap, 
                              leftp, current_page);
        }
        current_brick++;
    }

    if (last_plug !=0)
    {
        realloc_plug (end_address - last_plug, last_plug, consing_gen,
                      start_address, 
                      active_new_gen_number, last_pinned_gap, 
                      leftp, current_page);

    }

    //Fix the old segment allocated size
    assert (last_pinned_gap >= heap_segment_mem (seg));
    assert (last_pinned_gap <= heap_segment_committed (seg));
    heap_segment_plan_allocated (seg) = last_pinned_gap;
}

generation* gc_heap::expand_heap (int condemned_generation, 
                                  generation* consing_gen, 
                                  heap_segment* new_heap_segment)
{
    assert (condemned_generation >= (max_generation -1));
    unsigned int active_new_gen_number = max_generation; //Set one too high to get generation gap
    BYTE*  start_address = generation_limit (max_generation);
    size_t   current_brick = brick_of (start_address);
    BYTE*  end_address = heap_segment_allocated (ephemeral_heap_segment);
    size_t  end_brick = brick_of (end_address-1);
    BYTE*  last_plug = 0;
    dprintf(2,("---- Heap Expansion ----"));


    heap_segment* new_seg = new_heap_segment;


    if (!new_seg)
        return consing_gen;

    //copy the card and brick tables
    if (g_card_table!= card_table)
        copy_brick_card_table (TRUE);

    
    assert (generation_plan_allocation_start (generation_of (max_generation-1)));
    assert (generation_plan_allocation_start (generation_of (max_generation-1)) >=
            heap_segment_mem (ephemeral_heap_segment));
    assert (generation_plan_allocation_start (generation_of (max_generation-1)) <=
            heap_segment_committed (ephemeral_heap_segment));

    //compute the size of the new ephemeral heap segment. 
    ptrdiff_t eph_size = 
        heap_segment_plan_allocated (ephemeral_heap_segment) -
        generation_plan_allocation_start (generation_of (max_generation-1));
    //compute the size of the new pages to commit
    eph_size = eph_size - (heap_segment_committed (new_seg)-heap_segment_mem (new_seg));

    // commit the new ephemeral segment all at once. 
    if (eph_size > 0)
    {
        if (grow_heap_segment (new_seg, align_on_page (eph_size)) == 0)
        return consing_gen;
    }

    //initialize the first brick
    size_t first_brick = brick_of (heap_segment_mem (new_seg));
    set_brick (first_brick,
               heap_segment_mem (new_seg) - brick_address (first_brick));

    //From this point on, we cannot run out of memory 

    //Fix the end of the old ephemeral heap segment
    heap_segment_plan_allocated (ephemeral_heap_segment) =
        generation_plan_allocation_start (generation_of (max_generation-1));

    
    dprintf (3, ("Old ephemeral allocated set to %p", 
                 heap_segment_plan_allocated (ephemeral_heap_segment)));

    //reset the allocation of the consing generation back to the end of the 
    //old ephemeral segment
    generation_allocation_limit (consing_gen) =
        heap_segment_plan_allocated (ephemeral_heap_segment);
    generation_allocation_pointer (consing_gen) = generation_allocation_limit (consing_gen);
    generation_allocation_segment (consing_gen) = ephemeral_heap_segment;


    //clear the generation gap for all of the ephemeral generations
    {
        int generation_num = max_generation-1;
        while (generation_num >= 0)
        {
            generation* gen = generation_of (generation_num);
            generation_plan_allocation_start (gen) = 0;
            generation_num--;
        }
    }

    heap_segment* old_seg = ephemeral_heap_segment;
    ephemeral_heap_segment = new_seg;

    //Note: the ephemeral segment shouldn't be threaded onto the segment chain
    //because the relocation and compact phases shouldn't see it

    // set the generation members used by allocate_in_condemned_generations
    // and switch to ephemeral generation
    consing_gen = ensure_ephemeral_heap_segment (consing_gen);

    realloc_plugs (consing_gen, old_seg, start_address, end_address,
                   active_new_gen_number);

    plan_generation_start (consing_gen);

    dprintf(2,("---- End of Heap Expansion ----"));
    return consing_gen;
}



void gc_heap::init_dynamic_data ()
{
  
  // get the registry setting for generation 0 size
  size_t gen0size = GCHeap::GetValidGen0MaxSize(GCHeap::GetValidSegmentSize());

  dprintf (2, ("gen 0 size: %d", gen0size));

  dynamic_data* dd = dynamic_data_of (0);
  dd->current_size = 0;
  dd->previous_size = 0;
  dd->promoted_size = 0;
  dd->collection_count = 0;
//  dd->limit = 3.0f; 
  dd->limit = 2.5f;
//  dd->max_limit = 15.0f; //10.0f;
  dd->max_limit = 10.0f;
  dd->min_gc_size = Align(gen0size / 8 * 5);
  dd->min_size = dd->min_gc_size;
  //dd->max_size = Align (gen0size);
  dd->max_size = Align (12*gen0size/2);
  dd->new_allocation = dd->min_gc_size;
  dd->gc_new_allocation = dd->new_allocation;
  dd->c_new_allocation = dd->new_allocation;
  dd->desired_allocation = dd->new_allocation;
  dd->default_new_allocation = dd->min_gc_size;
  dd->fragmentation = 0;
  dd->fragmentation_limit = 40000;
  dd->fragmentation_burden_limit = 0.5f;

  dd =  dynamic_data_of (1);
  dd->current_size = 0;
  dd->previous_size = 0;
  dd->promoted_size = 0;
  dd->collection_count = 0;
  dd->limit = 2.0f;
//  dd->max_limit = 15.0f;
  dd->max_limit = 7.0f;
  dd->min_gc_size = 9*32*1024;
  dd->min_size = dd->min_gc_size;
//  dd->max_size = 2397152;
  dd->max_size = (dynamic_data_of (0))->max_size;
  dd->new_allocation = dd->min_gc_size;
  dd->gc_new_allocation = dd->new_allocation;
  dd->c_new_allocation = dd->new_allocation;
  dd->desired_allocation = dd->new_allocation;
  dd->default_new_allocation = dd->min_gc_size;
  dd->fragmentation = 0;
  dd->fragmentation_limit = 80000;
  dd->fragmentation_burden_limit = 0.5f;

  dd =  dynamic_data_of (2);
  dd->current_size = 0;
  dd->previous_size = 0;
  dd->promoted_size = 0;
  dd->collection_count = 0;
  dd->limit = 1.2f;
  dd->max_limit = 1.8f;
  dd->min_gc_size = 256*1024;
  dd->min_size = dd->min_gc_size;
  dd->max_size = 0x7fffffff;
  dd->new_allocation = dd->min_gc_size;
  dd->gc_new_allocation = dd->new_allocation;
  dd->c_new_allocation = dd->new_allocation;
  dd->desired_allocation = dd->new_allocation;
  dd->default_new_allocation = dd->min_gc_size;
  dd->fragmentation = 0;
  dd->fragmentation_limit = 200000;
  dd->fragmentation_burden_limit = 0.25f;

  //dynamic data for large objects
  dd =  dynamic_data_of (3);
  dd->current_size = 0;
  dd->previous_size = 0;
  dd->promoted_size = 0;
  dd->collection_count = 0;
  dd->limit = 1.25f;
  dd->max_limit = 2.0f;
  dd->min_gc_size = 1024*1024;
  dd->min_size = dd->min_gc_size;
  dd->max_size = 0x7fffffff;
  dd->new_allocation = dd->min_gc_size;
  dd->gc_new_allocation = dd->new_allocation;
  dd->c_new_allocation = dd->new_allocation;
  dd->desired_allocation = dd->new_allocation;
  dd->default_new_allocation = dd->min_gc_size;
  dd->fragmentation = 0;
  dd->fragmentation_limit = 0;
  dd->fragmentation_burden_limit = 0.0f;


#if 0 //def SERVER_GC
  for (int gennum = 0; gennum < 4; gennum++)
  {

      dynamic_data* dd = dynamic_data_of (gennum);
      dd->min_gc_size *=2;
      dd->min_size *=2;
      if (dd->max_size < 0x7fffffff)
          dd->max_size *= 2;
      dd->fragmentation_limit *=2;
  }
#endif //SERVER_GC

}

float gc_heap::surv_to_growth (float cst, float limit, float max_limit)
{
    if (cst < ((max_limit - limit ) / (limit * (max_limit-1.0f))))
        return ((limit - limit*cst) / (1.0f - (cst * limit)));
    else
        return max_limit;
}

size_t gc_heap::desired_new_allocation (dynamic_data* dd, size_t in, size_t out, float& cst, 
                               int gen_number)
{
    //update counter
    dd_promoted_size (dd) = out;

    if ((0 == dd_current_size (dd)) || (0 == dd_previous_size (dd)))
    {
        return dd_default_new_allocation (dd);
    }
    else
    {
        ptrdiff_t allocation = (dd_desired_allocation (dd) - dd_gc_new_allocation (dd));
        size_t    current_size = dd_current_size (dd);
        size_t    previous_size = dd_previous_size (dd);
        float     max_limit = dd_max_limit (dd);
        float     limit = dd_limit (dd);
        ptrdiff_t min_gc_size = dd_min_gc_size (dd);
        float     f = 0;
        ptrdiff_t max_size = dd_max_size (dd);
        size_t    new_size = 0;
        size_t    new_allocation = 0; 
        if (gen_number >= max_generation)
        {
            if (allocation > 0)
//              cst = min (1.0f, float (current_size-previous_size) / float (allocation));
                cst = min (1.0f, float (current_size) / float (previous_size + allocation));
            else
                cst = 0;
            //f = limit * (1 - cst) + max_limit * cst;
            f = surv_to_growth (cst, limit, max_limit);
            new_size = min (max (ptrdiff_t (f * current_size), min_gc_size), max_size);
            new_allocation  =  max((SSIZE_T)(new_size - current_size),
                                   (SSIZE_T)dd_desired_allocation (dynamic_data_of (max_generation-1)));
            //new_allocation = min (max (int (f * out), min_gc_size), max_size);
            //new_size =  (current_size + new_allocation);
        }
        else
        {
            cst = float (out) / float (previous_size+allocation-in);
            f = surv_to_growth (cst, limit, max_limit);
            new_allocation = min (max (ptrdiff_t (f * out), min_gc_size), max_size);
            new_size =  (current_size + new_allocation);
        }

        dprintf (1,("gen: %d in: %d out: %d prev: %d current: %d alloc: %d surv: %d%% f: %d%% new-size: %d new-alloc: %d", gen_number,
                    in, out, previous_size, current_size, allocation,
                    (int)(cst*100), (int)(f*100), new_size, new_allocation));
        return Align (new_allocation);
    }
}

size_t gc_heap::generation_size (int gen_number)
{
    if (0 == gen_number)
        return max((heap_segment_allocated (ephemeral_heap_segment) -
                    generation_allocation_start (generation_of (gen_number))),
                   (int)Align (min_obj_size));
    else
    {
        generation* gen = generation_of (gen_number);
        if (generation_start_segment (gen) == ephemeral_heap_segment)
            return (generation_allocation_start (generation_of (gen_number - 1)) -
                    generation_allocation_start (generation_of (gen_number)));
        else
        {
            assert (gen_number == max_generation);
            size_t gensize = (generation_allocation_start (generation_of (gen_number - 1)) - 
                              heap_segment_mem (ephemeral_heap_segment));
            heap_segment* seg = generation_start_segment (gen);
            while (seg != ephemeral_heap_segment)
            {
                gensize += heap_segment_allocated (seg) -
                           heap_segment_mem (seg);
                seg = heap_segment_next (seg);
            }
            return gensize;
        }
    }

}


size_t  gc_heap::compute_promoted_allocation (int gen_number)
{
  dynamic_data* dd = dynamic_data_of (gen_number);
  size_t  in = generation_allocation_size (generation_of (gen_number));
  dd_gc_new_allocation (dd) -= in;
  generation_allocation_size (generation_of (gen_number)) = 0;
  return in;
}


void gc_heap::compute_new_dynamic_data (int gen_number)
{
    dynamic_data* dd = dynamic_data_of (gen_number);
    generation*   gen = generation_of (gen_number);
    size_t        in = compute_promoted_allocation (gen_number);
    dd_previous_size (dd) = dd_current_size (dd);
    dd_current_size (dd) = (generation_size (gen_number) - generation_free_list_space (gen));
	//keep track of fragmentation
	dd_fragmentation (dd) = generation_free_list_space (gen);

    size_t         out = (((gen_number == max_generation)) ?
                         (dd_current_size(dd) - in) :
                         (generation_allocation_size (generation_of (1 + gen_number))));
    float surv;
    dd_desired_allocation (dd) = desired_new_allocation (dd, in, out, surv, gen_number);
    dd_gc_new_allocation (dd) = dd_desired_allocation (dd);
    if (gen_number == max_generation)
    {
        dd = dynamic_data_of (max_generation+1);
        dd_previous_size (dd) = dd_current_size (dd);
        dd_current_size (dd) = large_objects_size;
        dd_desired_allocation (dd) = desired_new_allocation (dd, 0, large_objects_size, surv, max_generation+1);
        dd_gc_new_allocation (dd) = dd_desired_allocation (dd);
    }

}



size_t gc_heap::new_allocation_limit (size_t size, size_t free_size)
{
    dynamic_data* dd        = dynamic_data_of (0);
    ptrdiff_t           new_alloc = dd_new_allocation (dd);
    assert (new_alloc == (ptrdiff_t)Align (new_alloc));
    size_t        limit     = min (max (new_alloc, (int)size), (int)free_size);
    assert (limit == Align (limit));
    dd_new_allocation (dd) = (new_alloc - limit );
    return limit;
}

//This is meant to be called by decide_on_compacting.

size_t gc_heap::generation_fragmentation (generation* gen, 
                                          generation* consing_gen,
                                          BYTE* end)
{
    size_t frag;
    BYTE* alloc = generation_allocation_pointer (consing_gen);
    // If the allocation pointer has reached the ephemeral segment
    // fine, otherwise the whole ephemeral segment is considered
    // fragmentation
    if ((alloc < heap_segment_reserved (ephemeral_heap_segment)) &&
        (alloc >= heap_segment_mem (ephemeral_heap_segment)))
        {
            if (alloc <= heap_segment_allocated(ephemeral_heap_segment))
                frag = end - alloc;
            else
            {
                // case when no survivors, allocated set to beginning
                frag = 0;
            }
        }
    else
        frag = (heap_segment_allocated (ephemeral_heap_segment) -
                heap_segment_mem (ephemeral_heap_segment));
    heap_segment* seg = generation_start_segment (gen);
    while (seg != ephemeral_heap_segment)
    {
        frag += (heap_segment_allocated (seg) -
                 heap_segment_plan_allocated (seg));
        seg = heap_segment_next (seg);
        assert (seg);
    }
    return frag;
}

// return the total sizes of the generation gen and younger

size_t gc_heap::generation_sizes (generation* gen)
{
    size_t result = 0;
    if (generation_start_segment (gen ) == ephemeral_heap_segment)
        result = (heap_segment_allocated (ephemeral_heap_segment) -
                  generation_allocation_start (gen));
    else
    {
        heap_segment* seg = generation_start_segment (gen);
        while (seg)
        {
            result += (heap_segment_allocated (seg) -
                       heap_segment_mem (seg));
            seg = heap_segment_next (seg);
        }
    }
    return result;
}




BOOL gc_heap::decide_on_compacting (int condemned_gen_number, 
                                    generation* consing_gen,
                                    size_t fragmentation, 
                                    BOOL& should_expand)
{
    BOOL should_compact = FALSE;
    should_expand = FALSE;
    generation*   gen = generation_of (condemned_gen_number);
    dynamic_data* dd = dynamic_data_of (condemned_gen_number);
    size_t gen_sizes     = generation_sizes(gen);
    float  fragmentation_burden = ( ((0 == fragmentation) || (0 == gen_sizes)) ? (0.0f) :
                                    (float (fragmentation) / gen_sizes) );
#ifdef STRESS_HEAP
    if (g_pConfig->GetGCStressLevel() != 0 && !concurrent_gc_p)  
        should_compact = TRUE;
#endif //STRESS_HEAP

    if (g_pConfig->GetGCForceCompact() && !concurrent_gc_p)
        should_compact = TRUE;
    

    dprintf (1,(" Fragmentation: %d Fragmentation burden %d%%",
                fragmentation, (int) (100*fragmentation_burden)));
    // check if there is enough room for the generation 0
    BOOL space_exceeded = ((size_t)(heap_segment_reserved (ephemeral_heap_segment) -
                            heap_segment_allocated (ephemeral_heap_segment)) <=
                           dd_min_size (dynamic_data_of (0)));
    if (space_exceeded && !concurrent_gc_p)
    {
        dprintf(2,("Not enough space for generation 0 without compaction"));
        should_compact = TRUE;
        if (condemned_gen_number >= (max_generation -1))
        {
            assert (generation_allocation_pointer (consing_gen) >=
                    heap_segment_mem (ephemeral_heap_segment));
            assert (generation_allocation_pointer (consing_gen) <=
                    heap_segment_reserved (ephemeral_heap_segment));
            if ((size_t)(heap_segment_reserved (ephemeral_heap_segment) -
                 generation_allocation_pointer (consing_gen)) <=
                dd_min_size (dynamic_data_of (0)))
            {
                dprintf(2,("Not enough space for generation 0 with compaction"));
                should_expand = TRUE;
            }
        }
    }
    if (!ephemeral_gen_fit_p () && !concurrent_gc_p)
    {
        dprintf(2,("Not enough space for all ephemeral generations without compaction"));
        should_compact = TRUE;
        if ((condemned_gen_number >= (max_generation - 1)) && 
             !ephemeral_gen_fit_p (TRUE))
        {
            dprintf(2,("Not enough space for all ephemeral generations with compaction"));
            should_expand = TRUE;
        }


    }

    BOOL frag_exceeded =((fragmentation >= (int)dd_fragmentation_limit (dd)) &&
                         (fragmentation_burden >= dd_fragmentation_burden_limit (dd)));
    if (frag_exceeded)
    {

        if (concurrent_gc_p)
        {
        }
        else
        {
            dprintf(2,("Fragmentation limits exceeded"));
            should_compact = TRUE;
        }
    }

    return should_compact;

}


BOOL gc_heap::ephemeral_gen_fit_p (BOOL compacting)
{
    size_t  sum = Align (LARGE_OBJECT_SIZE);
    BYTE* start_ephemeral = compacting ?
        generation_plan_allocation_start (generation_of (max_generation -1)) :
        generation_allocation_start (generation_of (max_generation -1));
    if (start_ephemeral == 0) // empty ephemeral generations
    {
        assert (compacting);
        start_ephemeral = generation_allocation_pointer (generation_of (max_generation));
    }

    for (int i = 0; i < max_generation; i++)
    {
        sum += max (generation_size (i), dd_min_size (dynamic_data_of (i)));
    }
    return ((start_ephemeral + sum) < heap_segment_reserved (ephemeral_heap_segment));
}

inline
large_object_block* get_object_block (large_object_block* bl)
{
    return bl;
}

inline
void gc_heap::RemoveBlock (large_object_block* item, BOOL pointerp)
{
    *(item->prev) = get_object_block (item->next);
    if (get_object_block (item->next))
        get_object_block (item->next)->prev = item->prev;
    else if (pointerp)
        last_large_p_object = item->prev;

}

inline
void gc_heap::InsertBlock (large_object_block** after, large_object_block* item, BOOL pointerp)
{
    ptrdiff_t lowest = (ptrdiff_t)(*after) & 1;
    item->next = get_object_block (*after);
    item->prev = after;
    if (get_object_block (*after))
        (get_object_block (*after))->prev = &(item->next);
    else if (pointerp)
        last_large_p_object = &(item->next);
    //preserve the lowest bit used as a marker during concurrentgc
    *((ptrdiff_t*)after) = (ptrdiff_t)item | lowest;
}
#define block_head(blnext)((large_object_block*)((BYTE*)(blnext)-\
                           &((large_object_block*)0)->next))

//Large object support



// sorted insertion. Blocks are likely to be allocated
// in increasing addresses so sort from the end.

void gc_heap::insert_large_pblock (large_object_block* bl)
{
    large_object_block** i = last_large_p_object;
    while (((size_t)bl < (size_t)i) &&
           (i != &large_p_objects))
    {
        i = block_head(i)->prev;


    }
    InsertBlock (i, bl, TRUE);
}



CObjectHeader* gc_heap::allocate_large_object (size_t size, BOOL pointerp, 
                                               alloc_context* acontext)
{
    //gmheap cannot allocate more than 2GB
    if (size >= 0x80000000 - 8 - AlignQword (sizeof (large_object_block)))
        return 0;

    size_t memsize = AlignQword (size) + AlignQword (sizeof (large_object_block));

    ptrdiff_t allocsize = dd_new_allocation (dynamic_data_of (max_generation+1));
    if (allocsize < 0)
    {
        vm_heap->GarbageCollectGeneration(max_generation);
    }



    void* mem = gheap->Alloc ((unsigned)memsize);

    if (!mem)
    {


        return 0;
    }

    assert (mem < g_highest_address);

    large_object_block* bl = (large_object_block*)mem;
    
    CObjectHeader* obj = (CObjectHeader*)block_object (bl);
    //store the pointer to the block before the object. 
    *((BYTE**)obj - 2) = (BYTE*)bl;

    dprintf (3,("New large object: %x, lower than %x", (size_t)obj, (size_t)highest_address));

    if (pointerp)
    {
        insert_large_pblock (bl);
    }
    else
    {
        InsertBlock (&large_np_objects, bl, FALSE);
    }

    //Increment the max_generation allocation counter to trigger
    //Full GC if necessary

    dd_new_allocation (dynamic_data_of (max_generation+1)) -= size;

    large_blocks_size += size;

    acontext->alloc_bytes += size;



    return obj;
}


void gc_heap::reset_large_object (BYTE* o)
{
}


void gc_heap::sweep_large_objects ()
{

    //this min value is for the sake of the dynamic tuning.
    //so we know that we are not starting even if we have no 
    //survivors. 
    large_objects_size = min_obj_size;
    large_object_block* bl = large_p_objects;
    int pin_finger = 0;
    while (bl)
    {
        large_object_block* next_bl = large_object_block_next (bl);
        {
            BYTE* o = block_object (bl);
            if (marked (o))
            {
                clear_marked_pinned (o);
                large_objects_size += size(o);
            }
            else
            {
                RemoveBlock (bl, TRUE);
                large_blocks_size -= size(o);
                gheap->Free (bl);
                reset_large_object (o);
            }
        }
        bl = next_bl;
    }
    bl = large_np_objects;
    while (bl)
    {
        large_object_block* next_bl = large_object_block_next (bl);
        {
            BYTE* o = block_object (bl);
            if ((size_t)(bl->next) & 1)
            {
                //this is a new object allocated since the start of gc
                //leave it alone
                bl->next = next_bl;
            }
            if (marked (o))
            {
                clear_marked_pinned (o);
                large_objects_size += size(o);
            }
            else
            {
                RemoveBlock (bl, FALSE);
                large_blocks_size -= size(o);
                reset_large_object (o);
                gheap->Free (bl);
            }
        }
        bl = next_bl;
    }

}


void gc_heap::relocate_in_large_objects ()
{
    THREAD_FROM_HEAP;
    relocate_args args;
    args.low = gc_low;
    args.high = gc_high;
    args.demoted_low = demotion_low;
    args.demoted_high = demotion_high;
    args.last_plug = 0;
    large_object_block* bl = large_p_objects;
    while (bl)
    {
        large_object_block* next_bl = bl->next;
        BYTE* o = block_object (bl);
        dprintf(3, ("Relocating through large object %x", (size_t)o));
        go_through_object (method_table (o), o, size(o), pval,
                           {
                               reloc_survivor_helper (&args, pval);
                           });
        bl = next_bl;
    }
}


void gc_heap::mark_through_cards_for_large_objects (card_fn fn,
                                                    BOOL relocating)
{
    THREAD_FROM_HEAP;
    //This function relies on the list to be sorted.
    large_object_block* bl = large_p_objects;
    size_t last_card = ~1u;
    BOOL         last_cp   = FALSE;
    unsigned n_eph = 0;
    unsigned n_gen = 0;
    BYTE*    next_boundary = (relocating ?
                              generation_plan_allocation_start (generation_of (max_generation -1)) :
                              ephemeral_low);
                                                                 
    BYTE*    nhigh         = (relocating ? 
                              heap_segment_plan_allocated (ephemeral_heap_segment) : 
                              ephemeral_high);

    while (bl)
    {
        BYTE* ob = block_object (bl);
        //object should not be marked
        assert (!marked (ob));

        CGCDesc* map = ((CObjectHeader*)ob)->GetSlotMap();
        CGCDescSeries* cur = map->GetHighestSeries();
        CGCDescSeries* last = map->GetLowestSeries();
        size_t s = size (ob);

        if (cur >= last) do 
        {
            BYTE* o = ob  + cur->GetSeriesOffset();
            BYTE* end_o = o  + cur->GetSeriesSize() + s;

            //Look for a card set within the range

            size_t card     = card_of (o);
            size_t end_card = 0;
            size_t card_word_end =  card_of (align_on_card_word (end_o)) / card_word_width;
            while (card_address (card) < end_o)
            {
                if (card >= end_card)
                    find_card (card_table, card, card_word_end, end_card);
                if (card_address (card) < end_o)
                {
                    if ((last_card != ~1u) && (card != last_card))
                    {
                        if (!last_cp)
                            clear_card (last_card);
                        else
                            last_cp = FALSE;
                    }
                    // Look at the portion of the pointers within the card.
                    BYTE** end =(BYTE**) min (end_o, card_address (card+1));
                    BYTE** beg =(BYTE**) max (o, card_address (card));
                    unsigned  markedp = 0;
                    dprintf (3,("Considering large object %x [%x,%x[", (size_t)ob, (size_t)beg, (size_t)end));

                    do 
                    {
                        mark_through_cards_helper (beg, n_gen, 
                                                   markedp, fn, nhigh, next_boundary);
                    } while (++beg < end);

                    n_eph += markedp;

                    last_card = card;
                    last_cp |= (markedp != 0);
                    card++;
                }
            }

            cur--;
        } while (cur >= last);
        else
        {
            //array of value classes. 
            int          cnt = map->GetNumSeries();
            BYTE**       ppslot = (BYTE**)(ob + cur->GetSeriesOffset());
            BYTE*        end_o = ob + s - plug_skew;
            size_t card = card_of ((BYTE*)ppslot);
            size_t end_card = 0;
            size_t card_word_end =  card_of (align_on_card_word (end_o)) / card_word_width;
            int pitch = 0;
            //compute the total size of the value element. 
            for (int _i = 0; _i > cnt; _i--)
            {
                unsigned nptrs = cur->val_serie[_i].nptrs;
                unsigned skip =  cur->val_serie[_i].skip;
                assert ((skip & (sizeof (BYTE*)-1)) == 0);
                pitch += nptrs*sizeof(BYTE*) + skip;
            }
            
            do 
            {
                if (card >= end_card)
                {
                    find_card (card_table, card, card_word_end, end_card);
                    if (card_address (card) > (BYTE*)ppslot)
                    {
                        ptrdiff_t min_offset = card_address (card) - (BYTE*)ppslot;
                        ppslot = (BYTE**)((BYTE*)ppslot + round_down (min_offset,pitch));
                    }
                }
                if (card_address (card) < end_o)
                {
                    unsigned     markedp = 0;
                    if ((last_card != ~1u) && (card != last_card))
                    {
                        if (!last_cp)
                            clear_card (last_card);
                        else
                            last_cp = FALSE;
                    }
                    BYTE** end =(BYTE**) min (card_address(card+1), end_o);
                    while (ppslot < end)
                    {
                        for (int __i = 0; __i > cnt; __i--)
                        {
                            unsigned nptrs = cur->val_serie[__i].nptrs;
                            unsigned skip =  cur->val_serie[__i].skip;
                            BYTE** ppstop = ppslot + nptrs;
                            do
                            {
                                mark_through_cards_helper (ppslot, n_gen, markedp, 
                                                           fn, nhigh, next_boundary);
                                ppslot++;
                            } while (ppslot < ppstop);
                            ppslot = (BYTE**)((BYTE*)ppslot + skip);
                        }
                    }
                    n_eph += markedp;
                    last_card = card;
                    last_cp |= (markedp != 0);
                    card++;
                }
            }while (card_address (card) < end_o);
        }
        bl = bl->next;
    }
    //clear last card. 
    if (last_card != ~1u) 
    {
        if (!last_cp)
            clear_card (last_card);
    }

    // compute the efficiency ratio of the card table
    if (!relocating)
    {
        generation_skip_ratio = min (((n_eph > 800) ? 
                                      (int)((n_gen*100) /n_eph) : 100),
                                     generation_skip_ratio);
        dprintf (2, ("Large Objects: n_eph: %d, n_gen: %d, ratio %d %", n_eph, n_gen, 
                     generation_skip_ratio));

    }

}


void gc_heap::descr_segment (heap_segment* seg )
{

#ifdef TRACE_GC
    BYTE*  x = heap_segment_mem (seg);
    while (x < heap_segment_allocated (seg))
    {
        dprintf(2, ( "%x: %d ", (size_t)x, size (x)));
        x = x + Align(size (x));
    }

#endif
}


void gc_heap::descr_card_table ()
{

#ifdef TRACE_GC
    if (trace_gc && (print_level >= 4))
    {
        ptrdiff_t  min = -1;
        dprintf(3,("Card Table set at: "));
        for (size_t i = card_of (lowest_address); i < card_of (highest_address); i++)
        {
            if (card_set_p (i))
            {
                if ((min == -1))
                {
                    min = i;
                }
            }
            else
            {
                if (! ((min == -1)))
                {
                    dprintf (3,("[%x %x[, ",
                            (size_t)card_address (min), (size_t)card_address (i)));
                    min = -1;
                }
            }
        }
    }
#endif
}

void gc_heap::descr_generations ()
{

#ifdef TRACE_GC
    int curr_gen_number = max_generation;
    while (curr_gen_number >= 0)
    {
        size_t gen_size = 0;
        if (curr_gen_number == max_generation)
        {
            heap_segment* curr_seg = generation_start_segment (generation_of (max_generation));
            while (curr_seg != ephemeral_heap_segment )
            {
                gen_size += (heap_segment_allocated (curr_seg) -
                             heap_segment_mem (curr_seg));
                curr_seg = heap_segment_next (curr_seg);
            }
            gen_size += (generation_allocation_start (generation_of (curr_gen_number -1)) - 
                         heap_segment_mem (ephemeral_heap_segment));
        }
        else if (curr_gen_number != 0)
        {
            gen_size += (generation_allocation_start (generation_of (curr_gen_number -1)) - 
                         generation_allocation_start (generation_of (curr_gen_number)));
        } else
        {
            gen_size += (heap_segment_allocated (ephemeral_heap_segment) - 
                         generation_allocation_start (generation_of (curr_gen_number)));
        }
                    
        dprintf (2,( "Generation %x: [%x %x[, gap size: %d, gen size: %d",
                 curr_gen_number,
                 (size_t)generation_allocation_start (generation_of (curr_gen_number)),
                 (((curr_gen_number == 0)) ?
                  (size_t)(heap_segment_allocated
                   (generation_start_segment
                    (generation_of (curr_gen_number)))) :
                  (size_t)(generation_allocation_start
                   (generation_of (curr_gen_number - 1)))),
                 size (generation_allocation_start
                           (generation_of (curr_gen_number))),
                     gen_size));
        curr_gen_number--;
    }

#endif

}


#undef TRACE_GC

//#define TRACE_GC 

//-----------------------------------------------------------------------------
//
//                                  VM Specific support
//
//-----------------------------------------------------------------------------

#include "excep.h"


#ifdef TRACE_GC

 unsigned long PromotedObjectCount  = 0;
 unsigned long CreatedObjectCount       = 0;
 unsigned long AllocDuration            = 0;
 unsigned long AllocCount               = 0;
 unsigned long AllocBigCount            = 0;
 unsigned long AllocSmallCount      = 0;
 unsigned long AllocStart             = 0;
#endif //TRACE_GC

//Static member variables.
volatile    BOOL    GCHeap::GcInProgress            = FALSE;
GCHeap::SUSPEND_REASON GCHeap::m_suspendReason      = GCHeap::SUSPEND_OTHER;
Thread*             GCHeap::GcThread                = 0;
Thread*             GCHeap::m_GCThreadAttemptingSuspend = 0;
//GCTODO
//CMCSafeLock*      GCHeap::fGcLock;
HANDLE              GCHeap::WaitForGCEvent          = 0;
unsigned            GCHeap::GcCount                 = 0;
//GCTODO
#ifdef TRACE_GC
unsigned long       GCHeap::GcDuration;
#endif //TRACE_GC
unsigned            GCHeap::GcCondemnedGeneration   = 0;
CFinalize*          GCHeap::m_Finalize              = 0;
BOOL                GCHeap::GcCollectClasses        = FALSE;
long                GCHeap::m_GCFLock               = 0;

#if defined (STRESS_HEAP) && !defined (MULTIPLE_HEAPS)
OBJECTHANDLE        GCHeap::m_StressObjs[NUM_HEAP_STRESS_OBJS];
int                 GCHeap::m_CurStressObj          = 0;
#endif //STRESS_HEAP && !MULTIPLE_HEAPS



HANDLE              GCHeap::hEventFinalizer     = 0;
HANDLE              GCHeap::hEventFinalizerDone = 0;
HANDLE              GCHeap::hEventFinalizerToShutDown     = 0;
HANDLE              GCHeap::hEventShutDownToFinalizer     = 0;
BOOL                GCHeap::fQuitFinalizer          = FALSE;
Thread*             GCHeap::FinalizerThread         = 0;
AppDomain*          GCHeap::UnloadingAppDomain  = 0;
BOOL                GCHeap::fRunFinalizersOnUnload  = FALSE;

inline
static void spin_lock ()
{
    enter_spin_lock (&m_GCLock);
}

inline
static void EnterAllocLock()
{
#ifdef _X86_

    
    __asm {
        inc dword ptr m_GCLock
        jz gotit
        call spin_lock
            gotit:
    }
#else //_X86_
    spin_lock();
#endif //_X86_
}

inline
static void LeaveAllocLock()
{
    // Trick this out
    leave_spin_lock (&m_GCLock);
}


// An explanation of locking for finalization:
//
// Multiple threads allocate objects.  During the allocation, they are serialized by
// the AllocLock above.  But they release that lock before they register the object
// for finalization.  That's because there is much contention for the alloc lock, but
// finalization is presumed to be a rare case.
//
// So registering an object for finalization must be protected by the FinalizeLock.
//
// There is another logical queue that involves finalization.  When objects registered
// for finalization become unreachable, they are moved from the "registered" queue to
// the "unreachable" queue.  Note that this only happens inside a GC, so no other
// threads can be manipulating either queue at that time.  Once the GC is over and
// threads are resumed, the Finalizer thread will dequeue objects from the "unreachable"
// queue and call their finalizers.  This dequeue operation is also protected with
// the finalize lock.
//
// At first, this seems unnecessary.  Only one thread is ever enqueuing or dequeuing
// on the unreachable queue (either the GC thread during a GC or the finalizer thread
// when a GC is not in progress).  The reason we share a lock with threads enqueuing
// on the "registered" queue is that the "registered" and "unreachable" queues are
// interrelated.
//
// They are actually two regions of a longer list, which can only grow at one end.
// So to enqueue an object to the "registered" list, you actually rotate an unreachable
// object at the boundary between the logical queues, out to the other end of the
// unreachable queue -- where all growing takes place.  Then you move the boundary
// pointer so that the gap we created at the boundary is now on the "registered"
// side rather than the "unreachable" side.  Now the object can be placed into the
// "registered" side at that point.  This is much more efficient than doing moves
// of arbitrarily long regions, but it causes the two queues to require a shared lock.
//
// Notice that Enter/LeaveFinalizeLock is not a GC-aware spin lock.  Instead, it relies
// on the fact that the lock will only be taken for a brief period and that it will
// never provoke or allow a GC while the lock is held.  This is critical.  If the
// FinalizeLock used enter_spin_lock (and thus sometimes enters preemptive mode to
// allow a GC), then the Alloc client would have to GC protect a finalizable object
// to protect against that eventuality.  That is too slow!



BOOL IsValidObject99(BYTE *pObject)
{
#if defined (VERIFY_HEAP)
    if (!((CObjectHeader*)pObject)->IsFree())
        ((Object *) pObject)->Validate();
#endif
    return(TRUE);
}

#if defined (VERIFY_HEAP)

void
gc_heap::verify_heap()
{
    size_t          last_valid_brick = 0;
    BOOL            bCurrentBrickInvalid = FALSE;
    size_t          curr_brick = 0;
    size_t          prev_brick = -1;
    heap_segment*   seg = generation_start_segment( generation_of( max_generation ) );;
    BYTE*           curr_object = generation_allocation_start(generation_of(max_generation));
    BYTE*           prev_object = 0;
    BYTE*           begin_youngest = generation_allocation_start(generation_of(0));
    BYTE*           end_youngest = heap_segment_allocated (ephemeral_heap_segment);
    int             curr_gen_num = max_generation;
    BYTE*           curr_free_item = generation_free_list (generation_of (curr_gen_num));

    dprintf (2,("Verifying heap"));
    //verify that the generation structures makes sense
    generation* gen = generation_of (max_generation);
    
    assert (generation_allocation_start (gen) == 
            heap_segment_mem (generation_start_segment (gen)));
    int gen_num = max_generation-1;
    generation* prev_gen = gen;
    while (gen_num >= 0)
    {
        gen = generation_of (gen_num);
        assert (generation_allocation_segment (gen) == ephemeral_heap_segment);
        assert (generation_allocation_start (gen) >= heap_segment_mem (ephemeral_heap_segment));
        assert (generation_allocation_start (gen) < heap_segment_allocated (ephemeral_heap_segment));

        if (generation_start_segment (prev_gen ) == 
            generation_start_segment (gen))
        {
            assert (generation_allocation_start (prev_gen) < 
                    generation_allocation_start (gen));
        }
        prev_gen = gen;
        gen_num--;
    }


    while (1)
    {
        // Handle segment transitions
        if (curr_object >= heap_segment_allocated (seg))
        {
            if (curr_object > heap_segment_allocated(seg))
            {
                printf ("curr_object: %x > heap_segment_allocated (seg: %x)",
                        (size_t)curr_object, (size_t)seg);
                RetailDebugBreak();
            }
            seg = heap_segment_next(seg);
            if (seg)
            {
                curr_object = heap_segment_mem(seg);
                continue;
            }
            else
                break;  // Done Verifying Heap -- no more segments
        }

        // Are we at the end of the youngest_generation?
        if ((seg == ephemeral_heap_segment) && (curr_object >= end_youngest))
        {
            // prev_object length is too long if we hit this int3
            if (curr_object > end_youngest)
            {
                printf ("curr_object: %x > end_youngest: %x",
                        (size_t)curr_object, (size_t)end_youngest);
                RetailDebugBreak();
            }
            break;
        }
        dprintf (4, ("curr_object: %x", (size_t)curr_object));
        size_t s = size (curr_object);
        if (s == 0)
        {
            printf ("s == 0");
            RetailDebugBreak();
        }

        //verify that the free list makes sense.
        if ((curr_free_item >= heap_segment_mem (seg)) &&
            (curr_free_item < heap_segment_allocated (seg)))
        {
            if (curr_free_item < curr_object)
            {
                printf ("Current free item %x is invalid (inside %x)",
                        (size_t)prev_object,
                        0);
                RetailDebugBreak();
            }
            else if (curr_object == curr_free_item) 
            {
                curr_free_item = free_list_slot (curr_free_item);
                if ((curr_free_item == 0) && (curr_gen_num > 0))
                {
                    curr_gen_num--;
                    curr_free_item = generation_free_list (generation_of (curr_gen_num));
                }
                //verify that the free list is its own generation
                if (curr_free_item != 0)
                {
                    if ((curr_free_item >= heap_segment_mem (ephemeral_heap_segment)) &&
                        (curr_free_item < heap_segment_allocated (ephemeral_heap_segment)))
                    {
                        if (curr_free_item < generation_allocation_start (generation_of (curr_gen_num)))
                        {
                            printf ("Current free item belongs to previous gen");
                            RetailDebugBreak();
                        } 
                        else if ((curr_gen_num > 0) && 
                                 ((curr_free_item + Align (size (curr_free_item)))>
                                  generation_allocation_start (generation_of (curr_gen_num-1))))
                        {
                            printf ("Current free item belongs to next gen");
                            RetailDebugBreak();
                        }
                    }
                }

                    
            }
        }



        // If object is not in the youngest generation, then lets
        // verify that the brick table is correct....
        if ((seg != ephemeral_heap_segment) || 
            (brick_of(curr_object) < brick_of(begin_youngest)))
        {
            curr_brick = brick_of(curr_object);

            // Brick Table Verification...
            //
            // On brick transition
            //     if brick is negative
            //          verify that brick indirects to previous valid brick
            //     else
            //          set current brick invalid flag to be flipped if we
            //          encounter an object at the correct place
            //
            if (curr_brick != prev_brick)
            {
                // If the last brick we were examining had positive
                // entry but we never found the matching object, then
                // we have a problem
                // If prev_brick was the last one of the segment 
                // it's ok for it to be invalid because it is never looked at
                if (bCurrentBrickInvalid && 
                    (curr_brick != brick_of (heap_segment_mem (seg))))
                {
                    printf ("curr brick %x invalid", curr_brick);
                    RetailDebugBreak();
                }

                // If the current brick contains a negative value make sure
                // that the indirection terminates at the last  valid brick
                if (brick_table[curr_brick] < 0)
                {
                    if (brick_table [curr_brick] == -32768)
                    {
                        printf ("curr_brick %x for object %x set to -32768",
                                curr_brick, (size_t)curr_object);
                        RetailDebugBreak();
                    }
                    ptrdiff_t i = curr_brick;
                    while ((i >= ((ptrdiff_t) brick_of (heap_segment_mem (seg)))) &&
                           (brick_table[i] < 0))
                    {
                        i = i + brick_table[i];
                    }
                    if (i <  ((ptrdiff_t)(brick_of (heap_segment_mem (seg))) - 1))
                    {
                        printf ("i: %x < brick_of (heap_segment_mem (seg)):%x - 1. curr_brick: %x",
                                i, brick_of (heap_segment_mem (seg)), 
                                curr_brick);
                        RetailDebugBreak();
                    }
                    // if (i != last_valid_brick)
                    //  RetailDebugBreak();
                    bCurrentBrickInvalid = FALSE;
                }
                else
                {
                    bCurrentBrickInvalid = TRUE;
                }
            }

            if (bCurrentBrickInvalid)
            {
                if (curr_object == (brick_address(curr_brick) + brick_table[curr_brick]))
                {
                    bCurrentBrickInvalid = FALSE;
                    last_valid_brick = curr_brick;
                }
            }
        }


        // Free Objects are not really valid method tables in the sense that
        // IsValidObject will not work, so we special case this
        if (*((BYTE**)curr_object) != (BYTE *) g_pFreeObjectMethodTable)
        {
            ((Object*)curr_object)->ValidateHeap((Object*)curr_object);
            if (contain_pointers(curr_object))
                go_through_object(method_table (curr_object), curr_object, s, oo,  
                                  { 
                                      if (*oo) 
                                          ((Object*)(*oo))->ValidateHeap((Object*)curr_object); 
                                  } );
        }

        prev_object = curr_object;
        prev_brick = curr_brick;
        curr_object = curr_object + Align(s);
    }

    verify_card_table();

    {
        //uninit the unused portions of segments. 
        seg = generation_start_segment (generation_of (max_generation));
        while (seg)
        {
            memset (heap_segment_allocated (seg), 0xcc,
                    heap_segment_committed (seg) - heap_segment_allocated (seg));
            seg = heap_segment_next (seg);
        }
    }

    finalize_queue->CheckFinalizerObjects();

    SyncBlockCache::GetSyncBlockCache()->VerifySyncTableEntry();
}


void ValidateObjectMember (Object* obj)
{
    if (contain_pointers(obj))
    {
        size_t s = size (obj);
        go_through_object(method_table (obj), (BYTE*)obj, s, oo,  
                          { 
                              if (*oo)
                              {
                                  MethodTable *pMT = method_table (*oo);
                                  if (pMT->GetClass()->GetMethodTable() != pMT) {
                                      RetailDebugBreak();
                                  }
                              }
                          } );
    }
}
#endif  //VERIFY_HEAP






void DestructObject (CObjectHeader* hdr)
{
    hdr->~CObjectHeader();
}

void GCHeap::EnableFinalization( void )
{
    SetEvent( hEventFinalizer );
}

BOOL GCHeap::IsCurrentThreadFinalizer()
{
    return GetThread() == FinalizerThread;
}

Thread* GCHeap::GetFinalizerThread()
{
    _ASSERTE(FinalizerThread != 0);
    return FinalizerThread;
}


BOOL    GCHeap::HandlePageFault(void* add)
{
    return FALSE;
}

unsigned GCHeap::GetMaxGeneration()
{ 
    return max_generation;
}


HRESULT GCHeap::Shutdown ()
{ 
    deleteGCShadow();

    // Cannot assert this, since we use SuspendEE as the mechanism to quiesce all
    // threads except the one performing the shutdown.
    // ASSERT( !GcInProgress );

    // Guard against any more GC occurring and against any threads blocking
    // for GC to complete when the GC heap is gone.  This fixes a race condition
    // where a thread in GC is destroyed as part of process destruction and
    // the remaining threads block for GC complete.

    //GCTODO
    //EnterAllocLock();
    //Enter();
    //EnterFinalizeLock();
    //SetGCDone();

    //Shutdown finalization should already have happened
    //_ASSERTE(FinalizerThread == 0);

    // during shutdown lot of threads are suspended 
    // on this even, we don't want to wake them up just yet
    // rajak
    //CloseHandle (WaitForGCEvent);

    //find out if the global card table hasn't been used yet
    if (card_table_refcount (g_card_table) == 0)
    {
        destroy_card_table (g_card_table);
        g_card_table = 0;
    }

    gc_heap::destroy_gc_heap (pGenGCHeap);

    gc_heap::shutdown_gc();

    if (MHandles[0])
    {
        CloseHandle (MHandles[0]);
        MHandles[0] = 0;
    }

    return S_OK; 
}



//used by static variable implementation
void CGCDescGcScan(LPVOID pvCGCDesc, promote_func* fn, ScanContext* sc)
{
    CGCDesc* map = (CGCDesc*)pvCGCDesc;

    CGCDescSeries *last = map->GetLowestSeries();
    CGCDescSeries *cur = map->GetHighestSeries();

    assert (cur >= last);
    do
    {
        BYTE** ppslot = (BYTE**)((PBYTE)pvCGCDesc + cur->GetSeriesOffset());
        BYTE**ppstop = (BYTE**)((PBYTE)ppslot + cur->GetSeriesSize());

        while (ppslot < ppstop)
        {
            if (*ppslot)
            {
                (fn) ((Object*&) *ppslot, sc);
            }

            ppslot++;
        }

        cur--;
    }
    while (cur >= last);


}

// Wait until a garbage collection is complete
// returns NOERROR if wait was OK, other error code if failure.
// WARNING: This will not undo the must complete state. If you are
// in a must complete when you call this, you'd better know what you're
// doing.

// Routine factored out of ::Init functions to avoid using COMPLUS_TRY (__try) and C++ EH in same
// function.

static CFinalize* AllocateCFinalize() {
    return new CFinalize();
}

static
HRESULT AllocateCFinalize(CFinalize **pCFinalize) {
    COMPLUS_TRY 
    {
        *pCFinalize = AllocateCFinalize();
    } 
    COMPLUS_CATCH 
    {
        return E_OUTOFMEMORY;
    } 
    COMPLUS_END_CATCH

    if (!*pCFinalize)
        return E_OUTOFMEMORY;

    return S_OK;
}

// init the instance heap 
HRESULT GCHeap::Init( size_t )
{
    HRESULT hres = S_OK;

    //Initialize all of the instance members.


    // Rest of the initialization

    if (!gc_heap::make_gc_heap())
        hres = E_OUTOFMEMORY;
    
    // Failed.
    return hres;
}



//System wide initialization
HRESULT GCHeap::Initialize ()
{

    HRESULT hr = S_OK;

//Initialize the static members. 
#ifdef TRACE_GC
    GcDuration = 0;
    CreatedObjectCount = 0;
#endif

    size_t seg_size = GCHeap::GetValidSegmentSize();

    hr = gc_heap::initialize_gc (seg_size, seg_size /*LHEAP_ALLOC*/);


    if (hr != S_OK)
        return hr;

    if ((WaitForGCEvent = CreateEvent( 0, TRUE, TRUE, 0 )) != 0)
    {
        // Thread for running finalizers...
        if (FinalizerThreadCreate() != 1)
        {
            hr = E_OUTOFMEMORY;
            return hr;
        }
    }
    else
    {
        return E_OUTOFMEMORY;
    }

    StompWriteBarrierResize(FALSE);

#if defined (STRESS_HEAP) && !defined (MULTIPLE_HEAPS)
    if (g_pConfig->GetGCStressLevel() != 0)  {
        for(int i = 0; i < GCHeap::NUM_HEAP_STRESS_OBJS; i++)
            m_StressObjs[i] = CreateGlobalHandle(0);
        m_CurStressObj = 0;
    }
#endif //STRESS_HEAP && !MULTIPLE_HEAPS


// Setup for "%Time in GC" Perf Counter
    COUNTER_ONLY(g_TotalTimeInGC = 0);

    initGCShadow();         // If we are debugging write barriers, initialize heap shadow

    return Init (0);
};

////
// GC callback functions

BOOL GCHeap::IsPromoted(Object* object, ScanContext* sc)
{
#if defined (_DEBUG) 
    object->Validate();
#endif //_DEBUG
    BYTE* o = (BYTE*)object;
    gc_heap* hp = gc_heap::heap_of (o);
    return (!((o < hp->gc_high) && (o >= hp->gc_low)) || 
            hp->is_marked (o));
}

unsigned int GCHeap::WhichGeneration (Object* object)
{
    gc_heap* hp = gc_heap::heap_of ((BYTE*)object);
    return hp->object_gennum ((BYTE*)object);
}


#ifdef VERIFY_HEAP

// returns TRUE if the pointer is in one of the GC heaps. 
BOOL GCHeap::IsHeapPointer (void* vpObject, BOOL small_heap_only)
{
    BYTE* object = (BYTE*) vpObject;
    gc_heap* h = gc_heap::heap_of(object, FALSE);

    if ((object < g_highest_address) && (object >= g_lowest_address))
    {
        if (!small_heap_only) {
            l_heap* lh = h->lheap;
            while (lh)
            {
                if ((object < (BYTE*)lh->heap + lh->size) && (object >= lh->heap))
                    return TRUE;
                lh = lh->next;
            }
        }

        if (h->find_segment (object))
            return TRUE;
        else
            return FALSE;
    }
    else
        return FALSE;
}

#endif //VERIFY_HEAP


#ifdef STRESS_PINNING
static n_promote = 0;
#endif
// promote an object
void GCHeap::Promote(Object*& object, ScanContext* sc, DWORD flags)
{
    THREAD_NUMBER_FROM_CONTEXT;
    BYTE* o = (BYTE*)object;

    if (object == 0)
        return;

    HEAP_FROM_THREAD;

#if 0 //def CONCURRENT_GC
    if (sc->concurrent_p)
    {
        hpt->c_promote_callback (object, sc, flags);
        return;
    }
#endif //CONCURRENT_GC

    gc_heap* hp = gc_heap::heap_of (o
#ifdef _DEBUG
                                    , !(flags & GC_CALL_INTERIOR)
#endif //_DEBUG
                                   );
    
    dprintf (3, ("Promote %x", (size_t)o));

#ifdef INTERIOR_POINTERS
    if (flags & GC_CALL_INTERIOR)
    {
        if ((o < hp->gc_low) || (o >= hp->gc_high))
        {
            return;
        }
        o = hp->find_object (o, hp->gc_low);
    }
#endif //INTERIOR_POINTERS


#if defined (_DEBUG)
    ((Object*)o)->ValidatePromote(sc, flags);
#endif

    if (flags & GC_CALL_PINNED)
    {
        hp->pin_object (o, hp->gc_low, hp->gc_high);
        COUNTER_ONLY(GetGlobalPerfCounters().m_GC.cPinnedObj ++);
        COUNTER_ONLY(GetPrivatePerfCounters().m_GC.cPinnedObj ++);
    }

#ifdef STRESS_PINNING
    if ((++n_promote % 20) == 1)
            hp->pin_object (o, hp->gc_low, hp->gc_high);
#endif //STRESS_PINNING

    if ( o && collect_classes)
        hpt->mark_object_class (o THREAD_NUMBER_ARG);
    else
        if ((o >= hp->gc_low) && (o < hp->gc_high))
        {
            hpt->mark_object_simple (&o THREAD_NUMBER_ARG);
        }

    LOG((LF_GC|LF_GCROOTS, LL_INFO1000000, "Promote GC Root %#x = %#x\n", &object, object));
}


void GCHeap::Relocate (Object*& object, ScanContext* sc,
                       DWORD flags)
{

    flags;
    THREAD_NUMBER_FROM_CONTEXT;

    if (object == 0)
        return;
    gc_heap* hp = gc_heap::heap_of ((BYTE*)object
#ifdef _DEBUG
                                    , !(flags & GC_CALL_INTERIOR)
#endif //_DEBUG
                                    );

#if defined (_DEBUG)
    if (!(flags & GC_CALL_INTERIOR))
        object->Validate(FALSE);
#endif

    dprintf (3, ("Relocate %x\n", (size_t)object));

#if defined(_DEBUG)
    BYTE* old_loc = (BYTE*)object;
#endif

    BYTE* pheader = (BYTE*)object;
    hp->relocate_address (&pheader THREAD_NUMBER_ARG);
    object = (Object*)pheader;

#if defined(_DEBUG)
    if (old_loc != (BYTE*)object)
        LOG((LF_GC|LF_GCROOTS, LL_INFO10000, "GC Root %#x updated %#x -> %#x\n", &object, old_loc, object));
    else
        LOG((LF_GC|LF_GCROOTS, LL_INFO100000, "GC Root %#x updated %#x -> %#x\n", &object, old_loc, object));
#endif
}



/*static*/ BOOL GCHeap::IsLargeObject(MethodTable *mt)
{
    return mt->GetBaseSize() >= LARGE_OBJECT_SIZE;
}

/*static*/ BOOL GCHeap::IsObjectInFixedHeap(Object *pObj)
{
    // For now we simply look at the size of the object to determine if it in the
    // fixed heap or not. If the bit indicating this gets set at some point
    // we should key off that instead.
    return pObj->GetSize() >= LARGE_OBJECT_SIZE;
}

#ifdef STRESS_HEAP

size_t StressHeapPreIP = -1;
size_t StressHeapPostIP = -1;

void StressHeapDummy ();

static LONG GCStressStartCount = EEConfig::GetConfigDWORD(L"GCStressStart", 0);
static LONG GCStressCurCount = 0;

    // free up object so that things will move and then do a GC
void GCHeap::StressHeap(alloc_context * acontext) 
{
#ifdef _DEBUG
    if (g_pConfig->FastGCStressLevel() && !GetThread()->StressHeapIsEnabled()) {
        return;
    }
#endif

    if ((g_pConfig->GetGCStressLevel() & EEConfig::GCSTRESS_UNIQUE)
#ifdef _DEBUG
        || g_pConfig->FastGCStressLevel() > 1
#endif
        ) {
        if (StressHeapPreIP == -1) {
#ifdef _X86_
            _asm {
                lea eax, PreCheck
                mov StressHeapPreIP, eax
                lea eax, PostCheck
                mov StressHeapPostIP, eax
            }
#else
            StressHeapPreIP = (size_t)GCHeap::StressHeap;
            StressHeapPostIP = (size_t)StressHeapDummy;
#endif
        }
#ifdef _X86_
PreCheck:
#endif
        if (!Thread::UniqueStack()) {
            return;
        }
#ifdef _X86_
PostCheck:  
        0;
#endif
    }

    COMPLUS_TRY 
    {

        // Allow programmer to skip the first N Stress GCs so that you can 
        // get to the interesting ones faster.  
        FastInterlockIncrement(&GCStressCurCount);
        if (GCStressCurCount < GCStressStartCount)
            return;
    
        static LONG OneAtATime = -1;

        if (acontext == 0)
            acontext = generation_alloc_context (pGenGCHeap->generation_of(0));

        // Only bother with this if the stress level is big enough and if nobody else is
        // doing it right now.  Note that some callers are inside the AllocLock and are
        // guaranteed synchronized.  But others are using AllocationContexts and have no
        // particular synchronization.
        //
        // For this latter case, we want a very high-speed way of limiting this to one
        // at a time.  A secondary advantage is that we release part of our StressObjs
        // buffer sparingly but just as effectively.

        if (g_pStringClass == 0)
        {
            // If the String class has not been loaded, dont do any stressing. This should
            // be kept to a minimum to get as complete coverage as possible.
            _ASSERTE(g_fEEInit);
            return;
        }

        if (FastInterlockIncrement((LONG *) &OneAtATime) == 0)
        {
            StringObject* str;
        
            // If the current string is used up 
            if (ObjectFromHandle(m_StressObjs[m_CurStressObj]) == 0)
            {       // Populate handles with strings
                int i = m_CurStressObj;
                while(ObjectFromHandle(m_StressObjs[i]) == 0)
                {
                    _ASSERTE(m_StressObjs[i] != 0);
                    unsigned strLen = (LARGE_OBJECT_SIZE - 32) / sizeof(WCHAR);
                    unsigned strSize = g_pStringClass->GetBaseSize() + strLen * sizeof(WCHAR);
                    str = (StringObject*) pGenGCHeap->allocate (strSize, acontext);
                    str->SetMethodTable (g_pStringClass);
                    str->SetArrayLength (strLen);

#if CHECK_APP_DOMAIN_LEAKS
                    if (g_pConfig->AppDomainLeaks())
                        str->SetAppDomain();
#endif

                    StoreObjectInHandle(m_StressObjs[i], ObjectToOBJECTREF(str));
                    
                    i = (i + 1) % NUM_HEAP_STRESS_OBJS;
                    if (i == m_CurStressObj) break;
                }
                // advance the current handle to the next string
                m_CurStressObj = (m_CurStressObj + 1) % NUM_HEAP_STRESS_OBJS;
            }
            
            // Get the current string
            str = (StringObject*) OBJECTREFToObject(ObjectFromHandle(m_StressObjs[m_CurStressObj]));
            
            // Chop off the end of the string and form a new object out of it.  
            // This will 'free' an object at the begining of the heap, which will
            // force data movement.  Note that we can only do this so many times.
            // before we have to move on to the next string.  
            unsigned sizeOfNewObj = (unsigned)Align(min_obj_size);
            if (str->GetArrayLength() > sizeOfNewObj / sizeof(WCHAR))
            {
                unsigned sizeToNextObj = (unsigned)Align(size(str));
                CObjectHeader* freeObj = (CObjectHeader*) (((BYTE*) str) + sizeToNextObj - sizeOfNewObj);
                freeObj->SetFree(sizeOfNewObj);
                str->SetArrayLength(str->GetArrayLength() - (sizeOfNewObj / sizeof(WCHAR)));

            }
            else {
                StoreObjectInHandle(m_StressObjs[m_CurStressObj], 0);       // Let the string itself become garbage.  
                // will be realloced next time around
            }
        }
        FastInterlockDecrement((LONG *) &OneAtATime);

        GarbageCollect(max_generation, FALSE);

        // Do nothing if MULTIPLE_HEAPS is enabled
    }
    COMPLUS_CATCH
    {
        _ASSERTE (!"Exception happens during StressHeap");
    }
    COMPLUS_END_CATCH
}

static void StressHeapDummy()
{
}
#endif // STRESS_HEAP

//
// Small Object Allocator
//
//

Object *
GCHeap::Alloc( DWORD size, DWORD flags)
{
    THROWSCOMPLUSEXCEPTION();

    TRIGGERSGC();
#ifdef _DEBUG
    Thread* pThread = GetThread();
    if (pThread)
    {
        pThread->SetReadyForSuspension();
    }
#endif
    
    Object* newAlloc;

#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    AllocStart = GetCycleCount32();
    unsigned finish;
#elif defined(ENABLE_INSTRUMENTATION)
    unsigned AllocStart = GetInstLogTime();
    unsigned finish;
#endif
#endif

    EnterAllocLock();

    gc_heap* hp = pGenGCHeap;

#ifdef STRESS_HEAP
    // GCStress Testing
    if (g_pConfig->GetGCStressLevel() & EEConfig::GCSTRESS_ALLOC)  
        StressHeap(generation_alloc_context(hp->generation_of(0)));

#ifdef _DEBUG
    if (pThread) {
        pThread->EnableStressHeap();
    }
#endif
#endif //STRESS_HEAP

    alloc_context* acontext = generation_alloc_context (hp->generation_of(0));

    if (size < LARGE_OBJECT_SIZE)
    {
        
#ifdef TRACE_GC
        AllocSmallCount++;
#endif
        newAlloc = (Object*) hp->allocate (size, acontext);
        LeaveAllocLock();
//        ASSERT (newAlloc);
        if (newAlloc != 0)
        {
            if (flags & GC_ALLOC_FINALIZE)
                hp->finalize_queue->RegisterForFinalization (0, newAlloc);
        } else
            COMPlusThrowOM();
    }
    else
    {
        enter_spin_lock (&hp->more_space_lock);
        newAlloc = (Object*) hp->allocate_large_object 
            (size, (flags & GC_ALLOC_CONTAINS_REF ), acontext); 
        leave_spin_lock (&hp->more_space_lock);
        LeaveAllocLock();
        if (newAlloc != 0)
        {
            //Clear the object
            memclr ((BYTE*)newAlloc - plug_skew, Align(size));
            if (flags & GC_ALLOC_FINALIZE)
                hp->finalize_queue->RegisterForFinalization (0, newAlloc);
        } else
            COMPlusThrowOM();
    }
   
#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    finish = GetCycleCount32();
#elif defined(ENABLE_INSTRUMENTATION)
    finish = GetInstLogTime();
#endif
    AllocDuration += finish - AllocStart;
    AllocCount++;
#endif
    return newAlloc;
}

Object *
GCHeap::AllocLHeap( DWORD size, DWORD flags)
{
    THROWSCOMPLUSEXCEPTION();

    TRIGGERSGC();
#ifdef _DEBUG
    Thread* pThread = GetThread();
    if (pThread)
    {
        pThread->SetReadyForSuspension();
    }
#endif
    
    Object* newAlloc;

#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    AllocStart = GetCycleCount32();
    unsigned finish;
#elif defined(ENABLE_INSTRUMENTATION)
    unsigned AllocStart = GetInstLogTime();
    unsigned finish;
#endif
#endif

    gc_heap* hp = pGenGCHeap;

#ifdef STRESS_HEAP
    // GCStress Testing
    if (g_pConfig->GetGCStressLevel() & EEConfig::GCSTRESS_ALLOC)  
        StressHeap(generation_alloc_context (hp->generation_of(0)));

#ifdef _DEBUG
    if (pThread) {
        pThread->EnableStressHeap();
    }
#endif
#endif //STRESS_HEAP

    alloc_context* acontext = generation_alloc_context (hp->generation_of(0));
    enter_spin_lock (&hp->more_space_lock);
    newAlloc = (Object*) hp->allocate_large_object 
        (size, (flags & GC_ALLOC_CONTAINS_REF), acontext); 
    leave_spin_lock (&hp->more_space_lock);
    if (newAlloc != 0)
    {
        //Clear the object
        memclr ((BYTE*)newAlloc - plug_skew, Align(size));

        if (flags & GC_ALLOC_FINALIZE)
            hp->finalize_queue->RegisterForFinalization (0, newAlloc);
    } else
        COMPlusThrowOM();
   
#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    finish = GetCycleCount32();
#elif defined(ENABLE_INSTRUMENTATION)
    finish = GetInstLogTime();
#endif
    AllocDuration += finish - AllocStart;
    AllocCount++;
#endif
    return newAlloc;
}

Object*
GCHeap::Alloc(alloc_context* acontext, DWORD size, DWORD flags )
{
    THROWSCOMPLUSEXCEPTION();

    TRIGGERSGC();
#ifdef _DEBUG
    Thread* pThread = GetThread();
    if (pThread) {
        pThread->SetReadyForSuspension();
    }
#endif
    
    Object* newAlloc;

#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    AllocStart = GetCycleCount32();
    unsigned finish;
#elif defined(ENABLE_INSTRUMENTATION)
    unsigned AllocStart = GetInstLogTime();
    unsigned finish;
#endif
#endif


#if defined (STRESS_HEAP)
    // GCStress Testing
    if (g_pConfig->GetGCStressLevel() & EEConfig::GCSTRESS_ALLOC)  
        StressHeap(acontext);

#ifdef _DEBUG
    if (pThread) {
        pThread->EnableStressHeap();
    }
#endif
#endif //STRESS_HEAP

    gc_heap* hp = pGenGCHeap;


    if (size < LARGE_OBJECT_SIZE)
    {
        
#ifdef TRACE_GC
        AllocSmallCount++;
#endif
        newAlloc = (Object*) hp->allocate (size, acontext);
//        ASSERT (newAlloc);
        if (newAlloc != 0)
        {
            if (flags & GC_ALLOC_FINALIZE)
                hp->finalize_queue->RegisterForFinalization (0, newAlloc);
        } else
            COMPlusThrowOM();
    }
    else
    {
        enter_spin_lock (&hp->more_space_lock);
        newAlloc = (Object*) hp->allocate_large_object 
                        (size, (flags & GC_ALLOC_CONTAINS_REF), acontext); 
        leave_spin_lock (&hp->more_space_lock);
        if (newAlloc != 0)
        {
            //Clear the object
            memclr ((BYTE*)newAlloc - plug_skew, Align(size));

            if (flags & GC_ALLOC_FINALIZE)
                hp->finalize_queue->RegisterForFinalization (0, newAlloc);
        } else
            COMPlusThrowOM();
    }
   
#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    finish = GetCycleCount32();
#elif defined(ENABLE_INSTRUMENTATION)
    finish = GetInstLogTime();
#endif
    AllocDuration += finish - AllocStart;
    AllocCount++;
#endif
    return newAlloc;
}

void 
GCHeap::FixAllocContext (alloc_context* acontext, BOOL lockp, void* arg)
{

    gc_heap* hp = pGenGCHeap;

    if (lockp)
        enter_spin_lock (&hp->more_space_lock);
    hp->fix_allocation_context (acontext, ((arg != 0)? TRUE : FALSE));
    if (lockp)
        leave_spin_lock (&hp->more_space_lock);
}
    


HRESULT
GCHeap::GarbageCollect (int generation, BOOL collect_classes_p)
{

    UINT GenerationAtEntry = GcCount;
    //This loop is necessary for concurrent GC because 
    //during concurrent GC we get in and out of 
    //GarbageCollectGeneration without doing an independent GC
    do 
    {
        enter_spin_lock (&gc_heap::more_space_lock);


        COUNTER_ONLY(GetGlobalPerfCounters().m_GC.cInducedGCs ++);
        COUNTER_ONLY(GetPrivatePerfCounters().m_GC.cInducedGCs ++);

        int gen = (generation < 0) ? max_generation : 
            min (generation, max_generation);
        GarbageCollectGeneration (gen, collect_classes_p);

        leave_spin_lock (&gc_heap::more_space_lock);


    }
    while (GenerationAtEntry == GcCount);
    return S_OK;
}

void
GCHeap::GarbageCollectGeneration (unsigned int gen, BOOL collect_classes_p)
{
    ASSERT_HOLDING_SPIN_LOCK(&gc_heap::more_space_lock);

// Perf Counter "%Time in GC" support
    COUNTER_ONLY(PERF_COUNTER_TIMER_START());

#ifdef COUNT_CYCLES 
    long gc_start = GetCycleCount32();
#endif //COUNT_CYCLES
    
#if defined ( _DEBUG) && defined (CATCH_GC)
    __try
#endif // _DEBUG && CATCH_GC
    {
    
        LOG((LF_GCROOTS|LF_GC|LF_GCALLOC, LL_INFO10, 
             "{ =========== BEGINGC %d, (gen = %lu, collect_classes = %lu) ==========\n",
             gc_count,
             (ULONG)gen,
             (ULONG)collect_classes_p));

retry:

#ifdef TRACE_GC
#ifdef COUNT_CYCLES
        AllocDuration += GetCycleCount32() - AllocStart;
#else
        AllocDuration += clock() - AllocStart;
#endif
#endif

    
        {
            SuspendEE(GCHeap::SUSPEND_FOR_GC);
            GcCount++;
        }
    
    // MAP_EVENT_MONITORS(EE_MONITOR_GARBAGE_COLLECTIONS, NotifyEvent(EE_EVENT_TYPE_GC_STARTED, 0));
    
    
#ifdef TRACE_GC
#ifdef COUNT_CYCLES
        unsigned start;
        unsigned finish;
        start = GetCycleCount32();
#else
        clock_t start;
        clock_t finish;
        start = clock();
#endif
        PromotedObjectCount = 0;
#endif
        GcCollectClasses = collect_classes_p;
    
        unsigned int condemned_generation_number = gen;
    

        gc_heap* hp = pGenGCHeap;

        UpdatePreGCCounters();

    
        condemned_generation_number = hp->garbage_collect 
            (condemned_generation_number
            );

    
        if (condemned_generation_number == -1)
            goto retry;
    
#ifdef TRACE_GC
#ifdef COUNT_CYCLES
        finish = GetCycleCount32();
#else
        finish = clock();
#endif
        GcDuration += finish - start;
        dprintf (1, 
                 ("<GC# %d> Condemned: %d, Duration: %d, total: %d Alloc Avg: %d, Small Objects:%d Large Objects:%d",
                  GcCount, condemned_generation_number,
                  finish - start, GcDuration,
                  AllocCount ? (AllocDuration / AllocCount) : 0,
                  AllocSmallCount, AllocBigCount));
        AllocCount = 0;
        AllocDuration = 0;
#endif // TRACE_GC

        {
            GCProfileWalkHeap();
        }

#ifdef JIT_COMPILER_SUPPORTED
        ScavengeJitHeaps();
#endif
    
        //GCTODO
        //CNameSpace::TimeToGC (FALSE);
    
        GcCollectClasses = FALSE;
    
        //MAP_EVENT_MONITORS(EE_MONITOR_GARBAGE_COLLECTIONS, NotifyEvent(EE_EVENT_TYPE_GC_FINISHED, 0));
        {
            initGCShadow();
        }
    
#ifdef TRACE_GC
#ifdef COUNT_CYCLES
        AllocStart = GetCycleCount32();
#else
        AllocStart = clock();
#endif
#endif

        {
            UpdatePostGCCounters();
        }

        {

            //LeaveFinalizeLock();

            // no longer in progress
            RestartEE(TRUE, TRUE);
        }
    
        //CNameSpace::GcDoneAndThreadsResumed();

        LOG((LF_GCROOTS|LF_GC|LF_GCALLOC, LL_INFO10, 
             "========== ENDGC (gen = %lu, collect_classes = %lu) ===========}\n",
             (ULONG)gen,
            (ULONG)collect_classes_p));
    
    }
#if defined (_DEBUG) && defined (CATCH_GC)
    __except (COMPLUS_EXCEPTION_EXECUTE_HANDLER)
    {
        _ASSERTE(!"Exception during GarbageCollectGeneration()");
    }
#endif // _DEBUG && CATCH_GC



#if defined(ENABLE_PERF_COUNTERS)
// Compute Time in GC, do we really need a global var?
    PERF_COUNTER_TIMER_STOP(g_TotalTimeInGC);

// Update Total Time    
    GetGlobalPerfCounters().m_GC.timeInGC += g_TotalTimeInGC;
    GetPrivatePerfCounters().m_GC.timeInGC += g_TotalTimeInGC;
    g_TotalTimeInGC = 0;

#endif

#ifdef COUNT_CYCLES
    printf ("GC: %d Time: %d\n", GcCondemnedGeneration, 
            GetCycleCount32() - gc_start);
#endif //COUNT_CYCLES

}

size_t      GCHeap::GetTotalBytesInUse ()
{
    return ApproxTotalBytesInUse ();
}

size_t GCHeap::ApproxTotalBytesInUse(BOOL small_heap_only)
{
    size_t totsize = 0;
    //GCTODO
    //ASSERT(InMustComplete());
    enter_spin_lock (&pGenGCHeap->more_space_lock);

    heap_segment* eph_seg = generation_allocation_segment (pGenGCHeap->generation_of (0));
    // Get small block heap size info
    totsize = (pGenGCHeap->alloc_allocated - heap_segment_mem (eph_seg));
    heap_segment* seg = generation_start_segment (pGenGCHeap->generation_of (max_generation));
    while (seg != eph_seg)
    {
        totsize += heap_segment_allocated (seg) -
            heap_segment_mem (seg);
        seg = heap_segment_next (seg);
    }

    //discount the fragmentation 
    for (int i = 0; i <= max_generation; i++)
    {
        totsize -= dd_fragmentation (pGenGCHeap->dynamic_data_of (i));
    }

    if (!small_heap_only)
    {
        // Add size of large objects
        ASSERT(pGenGCHeap->large_blocks_size >= 0);
        totsize += pGenGCHeap->large_blocks_size;
    }
    leave_spin_lock (&pGenGCHeap->more_space_lock);
    return totsize;
}


// The spec for this one isn't clear. This function
// returns the size that can be allocated without
// triggering a GC of any kind.
size_t GCHeap::ApproxFreeBytes()
{
    //GCTODO
    //ASSERT(InMustComplete());
    enter_spin_lock (&pGenGCHeap->more_space_lock);

    generation* gen = pGenGCHeap->generation_of (0);
    size_t res = generation_allocation_limit (gen) - generation_allocation_pointer (gen);

    leave_spin_lock (&pGenGCHeap->more_space_lock);

    return res;
}

HRESULT GCHeap::GetGcCounters(int gen, gc_counters* counters)
{
    if ((gen < 0) || (gen > max_generation))
        return E_FAIL;
    dynamic_data* dd = pGenGCHeap->dynamic_data_of (gen);
    counters->current_size = dd_current_size (dd);
    counters->promoted_size = dd_promoted_size (dd);
    counters->collection_count = dd_collection_count (dd);
    return S_OK;
}

// Verify the segment size is valid.
BOOL GCHeap::IsValidSegmentSize(size_t cbSize)
{
    return (power_of_two_p(cbSize) && (cbSize >> 20));
}

// Verify that gen0 size is at least large enough.
BOOL GCHeap::IsValidGen0MaxSize(size_t cbSize)
{
    return (cbSize >= 64*1024);
}

// Get the segment size to use, making sure it conforms.
size_t GCHeap::GetValidSegmentSize()
{
    size_t seg_size = g_pConfig->GetSegmentSize();
    if (!GCHeap::IsValidSegmentSize(seg_size))
    {
    
    	seg_size = ((g_SystemInfo.dwNumberOfProcessors > 4) ?
					INITIAL_ALLOC / 2 :
					INITIAL_ALLOC);
    }

  	return (seg_size);
}

// Get the max gen0 heap size, making sure it conforms.
size_t GCHeap::GetValidGen0MaxSize(size_t seg_size)
{
    size_t gen0size = g_pConfig->GetGCgen0size();

    if ((gen0size == 0) || !GCHeap::IsValidGen0MaxSize(gen0size))
    {
#ifdef SERVER_GC
        gen0size =  ((g_SystemInfo.dwNumberOfProcessors < 4) ? 1 : 2) *
            max(GetL2CacheSize(), (512*1024));
                     
#else //SERVER_GC
        gen0size = max((4*GetL2CacheSize()/5),(256*1024));
#endif //SERVER_GC
    }

    // Generation 0 must never be more than 1/2 the segment size.
    if (gen0size >= (seg_size / 2))
        gen0size = seg_size / 2;

    return (gen0size);   
}


void GCHeap::SetReservedVMLimit (size_t vmlimit)
{
    gc_heap::reserved_memory_limit = vmlimit;
}


//versions of same method on each heap

Object* GCHeap::GetNextFinalizableObject()
{

    return pGenGCHeap->finalize_queue->GetNextFinalizableObject();
    
}

size_t GCHeap::GetNumberFinalizableObjects()
{
    return pGenGCHeap->finalize_queue->GetNumberFinalizableObjects();
}

size_t GCHeap::GetFinalizablePromotedCount()
{
    return pGenGCHeap->finalize_queue->GetPromotedCount();
}

BOOL GCHeap::FinalizeAppDomain(AppDomain *pDomain, BOOL fRunFinalizers)
{
    return pGenGCHeap->finalize_queue->FinalizeAppDomain (pDomain, fRunFinalizers);
}

void GCHeap::SetFinalizeQueueForShutdown(BOOL fHasLock)
{
    pGenGCHeap->finalize_queue->SetSegForShutDown(fHasLock);
}





//---------------------------------------------------------------------------
// Finalized class tracking
//---------------------------------------------------------------------------


void GCHeap::RegisterForFinalization (int gen, Object* obj)
{
    if (gen == -1) 
        gen = 0;
    if (((obj->GetHeader()->GetBits()) & BIT_SBLK_FINALIZER_RUN))
    {
        //just reset the bit
        obj->GetHeader()->ClrBit(BIT_SBLK_FINALIZER_RUN);
    }
    else 
    {
        gc_heap* hp = gc_heap::heap_of ((BYTE*)obj);
        hp->finalize_queue->RegisterForFinalization (gen, obj);
    }
}

void GCHeap::SetFinalizationRun (Object* obj)
{
    obj->GetHeader()->SetBit(BIT_SBLK_FINALIZER_RUN);
}
    


//----------------------------------------------------------------------------
//
// Write Barrier Support for bulk copy ("Clone") operations
//
// StartPoint is the target bulk copy start point
// len is the length of the bulk copy (in bytes)
//
//
// Performance Note:
//
// This is implemented somewhat "conservatively", that is we
// assume that all the contents of the bulk copy are object
// references.  If they are not, and the value lies in the
// ephemeral range, we will set false positives in the card table.
//
// We could use the pointer maps and do this more accurately if necessary

VOID
SetCardsAfterBulkCopy( Object **StartPoint, size_t len )
{
    Object **rover;
    Object **end;

    // Target should aligned
    assert(Aligned ((size_t)StartPoint));

        
    // Don't optimize the Generation 0 case if we are checking for write barrier voilations
    // since we need to update the shadow heap even in the generation 0 case.
    // If destination is in Gen 0 don't bother
    if (GCHeap::WhichGeneration( (Object*) StartPoint ) == 0)
        return;

    rover = StartPoint;
    end = StartPoint + (len/sizeof(Object*));
    while (rover < end)
    {
        if ( (((BYTE*)*rover) >= g_ephemeral_low) && (((BYTE*)*rover) < g_ephemeral_high) )
        {
            // Set Bit For Card and advance to next card
            size_t card = gcard_of ((BYTE*)rover);

            FastInterlockOr (&g_card_table[card/card_word_width],
                             (1 << (DWORD)(card % card_word_width)));
            // Skip to next card for the object
            rover = (Object**) (g_lowest_address + (card_size * (card+1)));
        }
        else
        {
            rover++;
        }
    }
}




//--------------------------------------------------------------------
//
//          Support for finalization
//
//--------------------------------------------------------------------

inline
unsigned int gen_segment (int gen)
{
    return (NUMBERGENERATIONS - gen - 1);
}

CFinalize::CFinalize()
{
    THROWSCOMPLUSEXCEPTION();

    m_Array = new(Object*[100]);

    if (!m_Array)
    {
        ASSERT (m_Array);
        COMPlusThrowOM();;
    }
    m_EndArray = &m_Array[100];

    for (unsigned int i =0; i < NUMBERGENERATIONS+2; i++)
    {
        m_FillPointers[i] = m_Array;
    }
    m_PromotedCount = 0;
    lock = -1;
}

CFinalize::~CFinalize()
{
    delete m_Array;
}

int CFinalize::GetPromotedCount ()
{
    return m_PromotedCount;
}

inline
void CFinalize::EnterFinalizeLock()
{
    _ASSERTE(dbgOnly_IsSpecialEEThread() ||
             GetThread() == 0 ||
             GetThread()->PreemptiveGCDisabled());

retry:
    if (FastInterlockExchange (&lock, 0) >= 0)
    {
        unsigned int i = 0;
        while (lock >= 0)
        {
            if (++i & 7)
                __SwitchToThread (0);
            else
                __SwitchToThread (5);
        }
        goto retry;
    }
}

inline
void CFinalize::LeaveFinalizeLock()
{
    _ASSERTE(dbgOnly_IsSpecialEEThread() ||
             GetThread() == 0 ||
             GetThread()->PreemptiveGCDisabled());

    lock = -1;
}

void
CFinalize::RegisterForFinalization (int gen, Object* obj)
{
    THROWSCOMPLUSEXCEPTION();


    EnterFinalizeLock();
    // Adjust gen
    unsigned int dest = gen_segment (gen);
    //We don't maintain the fill pointer for NUMBERGENERATIONS+1 
    //because it is temporary. 
    //  m_FillPointers[NUMBERGENERATIONS+1]++;
    Object*** s_i = &m_FillPointers [NUMBERGENERATIONS]; 
    if ((*s_i) == m_EndArray)
    {
        if (!GrowArray())
        {
            LeaveFinalizeLock();
            COMPlusThrowOM();;
        }
    }
    Object*** end_si = &m_FillPointers[dest];
    do 
    {
        //is the segment empty? 
        if (!(*s_i == *(s_i-1)))
        {
            //no, swap the end elements. 
            *(*s_i) = *(*(s_i-1));
        }
        //increment the fill pointer
        (*s_i)++;
        //go to the next segment. 
        s_i--;
    } while (s_i > end_si);

    // We have reached the destination segment
    // store the object
    **s_i = obj;
    // increment the fill pointer
    (*s_i)++;

    if (g_fFinalizerRunOnShutDown) {
        // Adjust boundary for segments so that GC will keep objects alive.
        SetSegForShutDown(TRUE);
    }

    LeaveFinalizeLock();

}

Object*
CFinalize::GetNextFinalizableObject ()
{
    Object* obj = 0;
    //serialize
    EnterFinalizeLock();
    if (!IsSegEmpty(NUMBERGENERATIONS))
    {
        obj =  *(--m_FillPointers [NUMBERGENERATIONS]);

    }
    LeaveFinalizeLock();
    return obj;
}

void
CFinalize::SetSegForShutDown(BOOL fHasLock)
{
    int i;

    if (!fHasLock)
        EnterFinalizeLock();
    for (i = 0; i < NUMBERGENERATIONS; i++) {
        m_FillPointers[i] = m_Array;
    }
    if (!fHasLock)
        LeaveFinalizeLock();
}

size_t 
CFinalize::GetNumberFinalizableObjects()
{
    return m_FillPointers[NUMBERGENERATIONS] - 
        (g_fFinalizerRunOnShutDown?m_Array:m_FillPointers[NUMBERGENERATIONS-1]);
}

BOOL
CFinalize::FinalizeAppDomain (AppDomain *pDomain, BOOL fRunFinalizers)
{
    BOOL finalizedFound = FALSE;

    unsigned int startSeg = gen_segment (max_generation);

    EnterFinalizeLock();

    //reset the N+2 segment to empty
    m_FillPointers[NUMBERGENERATIONS+1] = m_FillPointers[NUMBERGENERATIONS];
    
    for (unsigned int Seg = startSeg; Seg < NUMBERGENERATIONS; Seg++)
    {
        Object** endIndex = Seg ? m_FillPointers [Seg-1] : m_Array;
        for (Object** i = m_FillPointers [Seg]-1; i >= endIndex ;i--)
        {
            Object* obj = *i;

            // Objects are put into the finalization queue before they are complete (ie their methodtable 
            // may be null) so we must check that the object we found has a method table before checking 
            // if it has the index we are looking for. If the methodtable is null, it can't be from the 
            // unloading domain, so skip it.
            if (obj->GetMethodTable() == NULL)
                continue;

            // eagerly finalize all objects except those that may be agile. 
            if (obj->GetAppDomainIndex() != pDomain->GetIndex())
                continue;

            if (obj->GetMethodTable()->IsAgileAndFinalizable())
            {
                // If an object is both agile & finalizable, we leave it in the
                // finalization queue during unload.  This is OK, since it's agile.
                // Right now only threads can be this way, so if that ever changes, change
                // the assert to just continue if not a thread.
                _ASSERTE(obj->GetMethodTable() == g_pThreadClass);

                // However, an unstarted thread should be finalized. It could be holding a delegate 
                // in the domain we want to unload. Once the thread has been started, its 
                // delegate is cleared so only unstarted threads are a problem.
                Thread *pThread = ((THREADBASEREF)ObjectToOBJECTREF(obj))->GetInternal();
                if (! pThread || ! pThread->IsUnstarted())
                    continue;
            }

            if (!fRunFinalizers || (obj->GetHeader()->GetBits()) & BIT_SBLK_FINALIZER_RUN)
            {
                //remove the object because we don't want to 
                //run the finalizer
                MoveItem (i, Seg, NUMBERGENERATIONS+2);
                //Reset the bit so it will be put back on the queue
                //if resurrected and re-registered.
                obj->GetHeader()->ClrBit (BIT_SBLK_FINALIZER_RUN);
            }
            else
            {
                finalizedFound = TRUE;
                MoveItem (i, Seg, NUMBERGENERATIONS);
            }
        }
    }

    LeaveFinalizeLock();

    return finalizedFound;
}

void
CFinalize::MoveItem (Object** fromIndex,
                     unsigned int fromSeg,
                     unsigned int toSeg)
{

    int step;
    ASSERT (fromSeg != toSeg);
    if (fromSeg > toSeg)
        step = -1;
    else
        step = +1;
    // Place the element at the boundary closest to dest
    Object** srcIndex = fromIndex;
    for (unsigned int i = fromSeg; i != toSeg; i+= step)
    {
        Object**& destFill = m_FillPointers[i+(step - 1 )/2];
        Object** destIndex = destFill - (step + 1)/2;
        if (srcIndex != destIndex)
        {
            Object* tmp = *srcIndex;
            *srcIndex = *destIndex;
            *destIndex = tmp;
        }
        destFill -= step;
        srcIndex = destIndex;
    }
}

void
CFinalize::GcScanRoots (promote_func* fn, int hn, ScanContext *pSC)
{

    ScanContext sc;
    if (pSC == 0)
        pSC = &sc;

    pSC->thread_number = hn;
    //scan the finalization queue
    Object** startIndex = m_FillPointers[NUMBERGENERATIONS-1];
    Object** stopIndex  = m_FillPointers[NUMBERGENERATIONS];
    for (Object** po = startIndex; po < stopIndex; po++)
    {
        (*fn)(*po, pSC, 0);

    }
}


BOOL
CFinalize::ScanForFinalization (int gen, int passNumber, BOOL mark_only_p,
                                gc_heap* hp)
{
    ScanContext sc;
    sc.promotion = TRUE;

    BOOL finalizedFound = FALSE;

    //start with gen and explore all the younger generations.
    unsigned int startSeg = gen_segment (gen);
    if (passNumber == 1)
    {
        m_PromotedCount = 0;
        //reset the N+2 segment to empty
        m_FillPointers[NUMBERGENERATIONS+1] = m_FillPointers[NUMBERGENERATIONS];
        unsigned int max_seg = gen_segment (max_generation);
        for (unsigned int Seg = startSeg; Seg < NUMBERGENERATIONS; Seg++)
        {
            Object** endIndex = Seg ? m_FillPointers [Seg-1] : m_Array;
            for (Object** i = m_FillPointers [Seg]-1; i >= endIndex ;i--)
            {
                Object* obj = *i;
                if (!GCHeap::IsPromoted (obj, &sc))
                {
                    if ((obj->GetHeader()->GetBits()) & BIT_SBLK_FINALIZER_RUN)
                    {
                        //remove the object because we don't want to 
                        //run the finalizer
                        MoveItem (i, Seg, NUMBERGENERATIONS+2);
                        //Reset the bit so it will be put back on the queue
                        //if resurrected and re-registered.
                        obj->GetHeader()->ClrBit (BIT_SBLK_FINALIZER_RUN);

                    }
                    else
                    {
                        m_PromotedCount++;
                        finalizedFound = TRUE;
                        MoveItem (i, Seg, NUMBERGENERATIONS);
                    }

                }
            }
        }
    }
    else
    {
        // Second pass, get rid of the non promoted NStructs.
        ASSERT (passNumber == 2 );
        Object** startIndex = m_FillPointers[NUMBERGENERATIONS];
        Object** stopIndex  = m_FillPointers[NUMBERGENERATIONS+1];
        for (Object** i = startIndex; i < stopIndex; i++)
        {
            assert (!"Should never get here. NStructs are gone!");
            if (!GCHeap::IsPromoted (*i, &sc))
            {
                assert (!"Should never get here. NStructs are gone!");
            }
            else
            {
                unsigned int Seg = gen_segment (GCHeap::WhichGeneration (*i));
                MoveItem (i, NUMBERGENERATIONS+1, Seg);
            }
        }
        //reset the N+2 segment to empty
        m_FillPointers[NUMBERGENERATIONS+1] = m_FillPointers[NUMBERGENERATIONS];
    }

    if (finalizedFound)
    {
        //Promote the f-reachable objects
        GcScanRoots (GCHeap::Promote,
                     0
                     , 0);

        if (!mark_only_p)
            SetEvent(GCHeap::hEventFinalizer);
    }

    return finalizedFound;
}

//Relocates all of the objects in the finalization array
void
CFinalize::RelocateFinalizationData (int gen, gc_heap* hp)
{
    ScanContext sc;
    sc.promotion = FALSE;

    unsigned int Seg = gen_segment (gen);

    Object** startIndex = Seg ? m_FillPointers [Seg-1] : m_Array;
    for (Object** po = startIndex; po < m_FillPointers [NUMBERGENERATIONS];po++)
    {
        GCHeap::Relocate (*po, &sc);
    }
}

void
CFinalize::UpdatePromotedGenerations (int gen, BOOL gen_0_empty_p)
{
    // update the generation fill pointers. 
    // if gen_0_empty is FALSE, test each object to find out if
    // it was promoted or not
    int last_gen = gen_0_empty_p ? 0 : 1;
    if (gen_0_empty_p)
    {
        for (int i = min (gen+1, max_generation); i > 0; i--)
        {
            m_FillPointers [gen_segment(i)] = m_FillPointers [gen_segment(i-1)];
        }
    }
    else
    {
        //Look for demoted or promoted plugs 

        for (int i = gen; i >= 0; i--)
        {
            unsigned int Seg = gen_segment (i);
            Object** startIndex = Seg ? m_FillPointers [Seg-1] : m_Array;

            for (Object** po = startIndex;
                 po < m_FillPointers [gen_segment(i)]; po++)
            {
                int new_gen = GCHeap::WhichGeneration (*po);
                if (new_gen != i)
                {
                    if (new_gen > i)
                    {
                        //promotion
                        MoveItem (po, gen_segment (i), gen_segment (new_gen));
                    }
                    else
                    {
                        //demotion
                        MoveItem (po, gen_segment (i), gen_segment (new_gen));
                        //back down in order to see all objects. 
                        po--;
                    }
                }

            }
        }
    }
}

BOOL
CFinalize::GrowArray()
{
    size_t oldArraySize = (m_EndArray - m_Array);
    size_t newArraySize =  (oldArraySize* 12)/10;
    WS_PERF_SET_HEAP(GC_HEAP);
    Object** newArray = new(Object*[newArraySize]);
    if (!newArray)
    {
        // It's not safe to throw here, because of the FinalizeLock.  Tell our caller
        // to throw for us.
//        ASSERT (newArray);
        return FALSE;
    }
    WS_PERF_UPDATE("GC:CRFinalizeGrowArray", sizeof(Object*)*newArraySize, newArray);
    memcpy (newArray, m_Array, oldArraySize*sizeof(Object*));

    //adjust the fill pointers
    for (unsigned i = 0; i <= NUMBERGENERATIONS+1; i++)
    {
        m_FillPointers [i] += (newArray - m_Array);
    }
    delete m_Array;
    m_Array = newArray;
    m_EndArray = &m_Array [newArraySize];

    return TRUE;
}



#if defined (VERIFY_HEAP)
void CFinalize::CheckFinalizerObjects()
{
    for (unsigned int i = 0; i < NUMBERGENERATIONS; i++)
    {
        Object **startIndex = (i > 0) ? m_Array : m_FillPointers[i-1];
        Object **stopIndex  = m_FillPointers[i];

        for (Object **po = startIndex; po > stopIndex; po++)
        {
            if (GCHeap::WhichGeneration (*po) < (NUMBERGENERATIONS - i -1))
                RetailDebugBreak ();
            (*po)->Validate();

        }
    }
}
#endif




//------------------------------------------------------------------------------
//
//                      End of VM specific support
//
//------------------------------------------------------------------------------




#if defined (GC_PROFILING) 
void gc_heap::walk_heap (walk_fn fn, void* context, int gen_number, BOOL walk_large_object_heap_p)
{
    generation* gen = gc_heap::generation_of (gen_number);
    heap_segment*    seg = generation_start_segment (gen);
    BYTE*       x = generation_allocation_start (gen);
    BYTE*       end = heap_segment_allocated (seg);

    while (1)
    {
        if (x >= end)
        {
            if ((seg = heap_segment_next(seg)) != 0)
            {
                x = heap_segment_mem (seg);
                end = heap_segment_allocated (seg);
                continue;
            } else
            {
                break;
            }
        }
        size_t s = size (x);
        CObjectHeader* o = (CObjectHeader*)x;
        if (!o->IsFree())
        {
            _ASSERTE(((size_t)o & 0x3) == 0); // Last two bits should never be set at this point
            if (!fn (o->GetObjectBase(), context))
                return;
        }
        x = x + Align (s);
    }

    if (walk_large_object_heap_p)
    {
        // go through large objects
        large_object_block* bl = gc_heap::large_p_objects;
        while (bl)
        {
            large_object_block* next_bl = large_object_block_next (bl);
            BYTE* x = block_object (bl);
            CObjectHeader* o = (CObjectHeader*)x;
            _ASSERTE(((size_t)o & 0x3) == 0); // Last two bits should never be set at this point
            if (!fn (o->GetObjectBase(), context))
                return;
            bl = next_bl;
        }

        bl = gc_heap::large_np_objects;
        while (bl)
        {
            large_object_block* next_bl = large_object_block_next (bl);
            BYTE* x = block_object (bl);
            CObjectHeader* o = (CObjectHeader*)x;
            _ASSERTE(((size_t)o & 0x3) == 0); // Last two bits should never be set at this point
            if (!fn (o->GetObjectBase(), context))
                return;
            bl = next_bl;
        }
    }
}

void ::walk_object (Object* obj, walk_fn fn, void* context)
{
    BYTE* o = (BYTE*)obj;
    if (o && contain_pointers (o))
    {
        go_through_object (method_table (o), o, size(o), oo,
                           {
                               if (*oo)
                               {
                                   Object *oh = (Object*)*oo;
                                   if (!fn (oh, context))
                                       return;
                               }
                           }
            );

    }
}



#endif //GC_PROFILING 



// Go through and touch (read) each page straddled by a memory block.
void TouchPages(LPVOID pStart, UINT cb)
{
    const UINT pagesize = OS_PAGE_SIZE;
    _ASSERTE(0 == (pagesize & (pagesize-1))); // Must be a power of 2.
    if (cb)
    {
        volatile char *pEnd = cb + (char*)pStart;
        volatile char *p = ((char*)pStart) -  (((size_t)pStart) & (pagesize-1));
        while (p < pEnd)
        {
            char a = *p;
            //printf("Touching page %lxh\n", (ULONG)p);
            p += pagesize;
        }

    }
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gchost.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
//*****************************************************************************
// gchost.cpp
//
// This module contains the implementation for the IGCController interface.
// This interface is published through the gchost.idl file.  It allows a host
// environment to set config values for the GC.
//
//*****************************************************************************

//********** Includes *********************************************************
#include "common.h"
#include "vars.hpp"
#include "EEConfig.h"
#include "PerfCounters.h"
#include "gchost.h"
#include "corhost.h"
#include "excep.h"
#include "Field.h"
#include "gc.h"


inline size_t SizeInKBytes(size_t cbSize)
{
    size_t cb = (cbSize % 1024) ? 1 : 0;
    return ((cbSize / 1024) + cb);
}

// IGCController

HRESULT CorHost::SetGCStartupLimits( 
    DWORD SegmentSize,
    DWORD MaxGen0Size)
{
    // Set default overrides if specified by caller.
    if (SegmentSize != ~0 && SegmentSize > 0)
    {
        // Sanity check the value, it must be a power of two and big enough.
        if (!GCHeap::IsValidSegmentSize(SegmentSize))
            return (E_INVALIDARG);
        g_pConfig->SetSegmentSize(SegmentSize);
    }
    
    if (MaxGen0Size != ~0 && MaxGen0Size > 0)
    {
        // Sanity check the value is at least large enough.
        if (!GCHeap::IsValidGen0MaxSize(MaxGen0Size))
            return (E_INVALIDARG);
        g_pConfig->SetGCgen0size(MaxGen0Size);
    }

    return (S_OK);
}


// Collect the requested generation.
HRESULT CorHost::Collect( 
    long       Generation)
{
    HRESULT     hr = E_FAIL;
    
    if (Generation > (long) g_pGCHeap->GetMaxGeneration())
        hr = E_INVALIDARG;
    else
    {
        Thread *pThread = GetThread();
        BOOL bIsCoopMode = pThread->PreemptiveGCDisabled();

        // Put thread into co-operative mode, which is how GC must run.
        if (!bIsCoopMode)
            pThread->DisablePreemptiveGC();
        
        COMPLUS_TRY
        {
            hr = g_pGCHeap->GarbageCollect(Generation);
        }
        COMPLUS_CATCH
        {
    		hr = SetupErrorInfo(GETTHROWABLE());
        }
        COMPLUS_END_CATCH
    
        // Return mode as requird.
        if (!bIsCoopMode)
            pThread->EnablePreemptiveGC();
    }
    return (hr);
}


// Return GC counters in the gchost format.
HRESULT CorHost::GetStats( 
    COR_GC_STATS *pStats)
{
#if defined(ENABLE_PERF_COUNTERS)
    Perf_GC		*pgc = &GetGlobalPerfCounters().m_GC;

    if (!pStats)
        return (E_INVALIDARG);

    if (pStats->Flags & COR_GC_COUNTS)
    {
        pStats->ExplicitGCCount = pgc->cInducedGCs;
        for (int idx=0; idx<3; idx++)
        {
            pStats->GenCollectionsTaken[idx] = pgc->cGenCollections[idx];
        }
    }
    
    if (pStats->Flags & COR_GC_MEMORYUSAGE)
    {
        pStats->CommittedKBytes = SizeInKBytes(pgc->cTotalCommittedBytes);
        pStats->ReservedKBytes = SizeInKBytes(pgc->cTotalReservedBytes);
        pStats->Gen0HeapSizeKBytes = SizeInKBytes(pgc->cGenHeapSize[0]);
        pStats->Gen1HeapSizeKBytes = SizeInKBytes(pgc->cGenHeapSize[1]);
        pStats->Gen2HeapSizeKBytes = SizeInKBytes(pgc->cGenHeapSize[2]);
        pStats->LargeObjectHeapSizeKBytes = SizeInKBytes(pgc->cLrgObjSize);
        pStats->KBytesPromotedFromGen0 = SizeInKBytes(pgc->cbPromotedMem[0]);
        pStats->KBytesPromotedFromGen1 = SizeInKBytes(pgc->cbPromotedMem[1]);
    }
    return (S_OK);
#else
    return (E_NOTIMPL);
#endif // ENABLE_PERF_COUNTERS
}

// Return per-thread allocation information.
HRESULT CorHost::GetThreadStats( 
    DWORD *pFiberCookie,
    COR_GC_THREAD_STATS *pStats)
{
    Thread      *pThread;

    // Get the thread from the caller or the current thread.
    if (!pFiberCookie)
        pThread = GetThread();
    else
        pThread = (Thread *) pFiberCookie;
    if (!pThread)
        return (E_INVALIDARG);
    
    // Get the allocation context which contains this counter in it.
    alloc_context *p = &pThread->m_alloc_context;
    pStats->PerThreadAllocation = p->alloc_bytes;
    if (pThread->GetHasPromotedBytes())
        pStats->Flags = COR_GC_THREAD_HAS_PROMOTED_BYTES;
    return (S_OK);
}

// Return per-thread allocation information.
HRESULT CorHost::SetVirtualMemLimit(
    SIZE_T sztMaxVirtualMemMB)
{
    GCHeap::SetReservedVMLimit (sztMaxVirtualMemMB);
    return (S_OK);
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gcscan.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*
 * GCSCAN.CPP 
 *
 * GC Root Scanning
 */

#include "common.h"
#include "object.h"
#include "threads.h"
#include "eetwain.h"
#include "eeconfig.h"
#include "gcscan.h"
#include "gc.h"
#include "corhost.h"
#include "threads.h"
#include "nstruct.h"
#include "interoputil.h"

#include "excep.h"
#include "compluswrapper.h"
#include "comclass.h"

//#define CATCH_GC  //catches exception during GC

//This is to allow inlines in gcscan.h to access GetThread 
//(gcscan.h is included before GetThread gets declared)
#ifdef _DEBUG
void Assert_GCDisabled()
{
    _ASSERTE (GetThread()->PreemptiveGCDisabled());
}
#endif //_DEBUG

//set the number of processors required to trigger the use of thread based allocation contexts
#ifdef MULTIPLE_HEAPS
#define MP_PROC_COUNT 1
#else
#define MP_PROC_COUNT 2
#endif //MULTIPLE_HEAPS


inline alloc_context* GetThreadAllocContext()
{
    assert(g_SystemInfo.dwNumberOfProcessors >= MP_PROC_COUNT);

    return & GetThread()->m_alloc_context;
}


#ifdef MAXALLOC
AllocRequestManager g_gcAllocManager(L"AllocMaxGC");
#endif


inline Object* Alloc(DWORD size, BOOL bFinalize, BOOL bContainsPointers )
{
#ifdef MAXALLOC
    THROWSCOMPLUSEXCEPTION();

    if (! g_gcAllocManager.CheckRequest(size))
        COMPlusThrowOM();
#endif
    DWORD flags = ((bContainsPointers ? GC_ALLOC_CONTAINS_REF : 0) |
                   (bFinalize ? GC_ALLOC_FINALIZE : 0));
    if (g_SystemInfo.dwNumberOfProcessors >= MP_PROC_COUNT)
        return g_pGCHeap->Alloc(GetThreadAllocContext(), size, flags);
    else
        return g_pGCHeap->Alloc(size, flags);
}

inline Object* AllocLHeap(DWORD size, BOOL bFinalize, BOOL bContainsPointers )
{
#ifdef MAXALLOC
    THROWSCOMPLUSEXCEPTION();

    if (! g_gcAllocManager.CheckRequest(size))
        COMPlusThrowOM();
#endif
    DWORD flags = ((bContainsPointers ? GC_ALLOC_CONTAINS_REF : 0) |
                   (bFinalize ? GC_ALLOC_FINALIZE : 0));
    return g_pGCHeap->AllocLHeap(size, flags);
}


#ifdef  _LOGALLOC
int g_iNumAllocs = 0;

bool ToLogOrNotToLog(DWORD size, char *typeName)
{
    g_iNumAllocs++;

    if (g_iNumAllocs > g_pConfig->AllocNumThreshold())
        return true;

    if ((int)size > g_pConfig->AllocSizeThreshold())
        return true;

    if (g_pConfig->ShouldLogAlloc(typeName))
        return true;

    return false;

}


inline void LogAlloc(DWORD size, MethodTable *pMT, Object* object)
{
#ifdef LOGGING
    if (LoggingOn(LF_GCALLOC, LL_INFO10))
    {
        LogSpewAlways("Allocated %s_TYPE %#x %5d bytes for %s\n", pMT->GetClass()->IsValueClass() ? "VAL" : "REF", object, size, pMT->GetClass()->m_szDebugClassName ? pMT->GetClass()->m_szDebugClassName : "<Null>");
        if (LoggingOn(LF_GCALLOC, LL_INFO1000) || (LoggingOn(LF_GCALLOC, LL_INFO100) && ToLogOrNotToLog(size, pMT->GetClass()->m_szDebugClassName ? pMT->GetClass()->m_szDebugClassName : "<Null>")))
        {
            void LogStackTrace();
            LogStackTrace();
        }
    }
#endif
}
#else
#define LogAlloc(size, pMT, object)
#endif


/*
 * GcEnumObject()
 *
 * This is the JIT compiler (or any remote code manager)
 * GC enumeration callback
 */

void GcEnumObject(LPVOID pData, OBJECTREF *pObj, DWORD flags)
{
    Object ** ppObj = (Object **)pObj;
    GCCONTEXT   * pCtx  = (GCCONTEXT *) pData;

    //
    // Sanity check that the flags contain only these three values
    //
    assert((flags & ~(GC_CALL_INTERIOR|GC_CALL_PINNED|GC_CALL_CHECK_APP_DOMAIN)) == 0);

    // for interior pointers, we optimize the case in which
    //  it points into the current threads stack area
    //
    if (flags & GC_CALL_INTERIOR)
        PromoteCarefully (pCtx->f, *ppObj, pCtx->sc, flags);
    else
        (pCtx->f)( *ppObj, pCtx->sc, flags);
}



StackWalkAction GcStackCrawlCallBack(CrawlFrame* pCF, VOID* pData)
{
    Frame       *pFrame;
    GCCONTEXT   *gcctx = (GCCONTEXT*) pData;

#if CHECK_APP_DOMAIN_LEAKS
    gcctx->sc->pCurrentDomain = pCF->GetAppDomain();
#endif

    if ((pFrame = pCF->GetFrame()) != NULL)
    {
        STRESS_LOG3(LF_GCROOTS, LL_INFO1000, "Scaning ExplictFrame %p AssocMethod = %pM frameVTable = %pV\n", pFrame, pFrame->GetFunction(), *((void**) pFrame));
        pFrame->GcScanRoots( gcctx->f, gcctx->sc);
    }
    else
    {
        ICodeManager * pCM = pCF->GetCodeManager();
        _ASSERTE(pCM != NULL);

        unsigned flags = pCF->GetCodeManagerFlags();
        STRESS_LOG3(LF_GCROOTS, LL_INFO1000, "Scaning Frameless method %pM EIP = %p &EIP = %p\n", 
            pCF->GetFunction(), *pCF->GetRegisterSet()->pPC, pCF->GetRegisterSet()->pPC);

        if (pCF->GetFunction() != 0)  
        {
            LOG((LF_GCROOTS, LL_INFO1000, "Scaning Frame for method %s:%s\n",
                 pCF->GetFunction()->m_pszDebugClassName, pCF->GetFunction()->m_pszDebugMethodName));
        }

        EECodeInfo codeInfo(pCF->GetMethodToken(), pCF->GetJitManager());

        pCM->EnumGcRefs(pCF->GetRegisterSet(),
                        pCF->GetInfoBlock(),
                        &codeInfo,
                        pCF->GetRelOffset(),
                        flags,
                        GcEnumObject,
                        pData);
    }
    return SWA_CONTINUE;
}


/*
 * Scan for dead weak pointers
 */

VOID CNameSpace::GcWeakPtrScan( int condemned, int max_gen, ScanContext* sc )
{
    Ref_CheckReachable(condemned, max_gen, (LPARAM)sc);
}

VOID CNameSpace::GcShortWeakPtrScan( int condemned, int max_gen, 
                                     ScanContext* sc)
{
    Ref_CheckAlive(condemned, max_gen, (LPARAM)sc);
}



/*
 * Scan all stack roots in this 'namespace'
 */
 
VOID CNameSpace::GcScanRoots(promote_func* fn,  int condemned, int max_gen, 
                             ScanContext* sc, GCHeap* Hp )
{
    Hp;
    GCCONTEXT   gcctx;
    Thread*     pThread;

    gcctx.f  = fn;
    gcctx.sc = sc;

#if defined ( _DEBUG) && defined (CATCH_GC)
    //note that we can't use COMPLUS_TRY because the gc_thread isn't known
    __try
#endif // _DEBUG && CATCH_GC
    {
        STRESS_LOG1(LF_GCROOTS, LL_INFO10, "GCScan: Promotion Phase = %d\n", sc->promotion);

        // Either we are in a concurrent situation (in which case the thread is unknown to
        // us), or we are performing a synchronous GC and we are the GC thread, holding
        // the threadstore lock.
        
        _ASSERTE(dbgOnly_IsSpecialEEThread() ||
                 GetThread() == NULL ||
                 (GetThread() == g_pGCHeap->GetGCThread() && ThreadStore::HoldingThreadStore()));
        
        pThread = NULL;
        while ((pThread = ThreadStore::GetThreadList(pThread)) != NULL)
        {
            STRESS_LOG2(LF_GC|LF_GCROOTS, LL_INFO100, "{ Starting scan of Thread 0x%x ID = %x\n", pThread, pThread->GetThreadId());
#if defined (MULTIPLE_HEAPS) && !defined (ISOLATED_HEAPS)
            if  (pThread->m_alloc_context.home_heap == GCHeap::GetHeap(sc->thread_number) ||
                 pThread->m_alloc_context.home_heap == 0 && sc->thread_number == 0)
#endif
            {
                pThread->SetHasPromotedBytes();
                pThread->UpdateCachedStackInfo(gcctx.sc);
                pThread->StackWalkFrames( GcStackCrawlCallBack, &gcctx, 0);
            }
            STRESS_LOG2(LF_GC|LF_GCROOTS, LL_INFO100, "Ending scan of Thread 0x%x ID = %x }\n", pThread, pThread->GetThreadId());
        }
    }

#if defined ( _DEBUG) && defined (CATCH_GC)
    __except (EXCEPTION_EXECUTE_HANDLER)
    {
        _ASSERTE (!"We got an exception during scan roots");
    }
#endif //_DEBUG
}

/*
 * Scan all handle roots in this 'namespace'
 */


VOID CNameSpace::GcScanHandles (promote_func* fn,  int condemned, int max_gen, 
                                ScanContext* sc)
{

#if defined ( _DEBUG) && defined (CATCH_GC)
    //note that we can't use COMPLUS_TRY because the gc_thread isn't known
    __try
#endif // _DEBUG && CATCH_GC
    {
        STRESS_LOG1(LF_GC|LF_GCROOTS, LL_INFO10, "GcScanHandles (Promotion Phase = %d)\n", sc->promotion);
        if (sc->promotion == TRUE)
        {
            Ref_TracePinningRoots(condemned, max_gen, (LPARAM)sc);
            Ref_TraceNormalRoots(condemned, max_gen, (LPARAM)sc);
        }
        else
        {
            Ref_UpdatePointers(condemned, max_gen, (LPARAM)sc);
            Ref_UpdatePinnedPointers(condemned, max_gen, (LPARAM)sc);
        }
    }
    
#if defined ( _DEBUG) && defined (CATCH_GC)
    __except (EXCEPTION_EXECUTE_HANDLER)
    {
        _ASSERTE (!"We got an exception during scan roots");
    }
#endif //_DEBUG
}

#ifdef GC_PROFILING

/*
 * Scan all handle roots in this 'namespace' for profiling
 */

VOID CNameSpace::GcScanHandlesForProfiler (int max_gen, ScanContext* sc)
{

#if defined ( _DEBUG) && defined (CATCH_GC)
    //note that we can't use COMPLUS_TRY because the gc_thread isn't known
    __try
#endif // _DEBUG && CATCH_GC
    {
        LOG((LF_GC|LF_GCROOTS, LL_INFO10, "Profiler Root Scan Phase, Handles\n"));
        Ref_ScanPointersForProfiler(max_gen, (LPARAM)sc);
    }
    
#if defined ( _DEBUG) && defined (CATCH_GC)
    __except (EXCEPTION_EXECUTE_HANDLER)
    {
        _ASSERTE (!"We got an exception during scan roots for the profiler");
    }
#endif //_DEBUG
}

#endif // GC_PROFILING

void CNameSpace::GcDemote (ScanContext* )
{
    Ref_RejuvenateHandles ();
    SyncBlockCache::GetSyncBlockCache()->GCDone(TRUE);
}

void CNameSpace::GcPromotionsGranted (int condemned, int max_gen, ScanContext* sc)
{
    
    Ref_AgeHandles(condemned, max_gen, (LPARAM)sc);
    SyncBlockCache::GetSyncBlockCache()->GCDone(FALSE);
}


void CNameSpace::GcFixAllocContexts (void* arg)
{
    if (g_SystemInfo.dwNumberOfProcessors >= MP_PROC_COUNT)
    {
        Thread  *thread;

        thread = NULL;
        while ((thread = ThreadStore::GetThreadList(thread)) != NULL)
        {
            g_pGCHeap->FixAllocContext (&thread->m_alloc_context, FALSE, arg);
        }
    }
}

void CNameSpace::GcEnumAllocContexts (enum_alloc_context_func* fn, void* arg)
{
    if (g_SystemInfo.dwNumberOfProcessors >= MP_PROC_COUNT)
    {
        Thread  *thread;

        thread = NULL;
        while ((thread = ThreadStore::GetThreadList(thread)) != NULL)
        {
            (*fn) (&thread->m_alloc_context, arg);
        }
    }
}


size_t CNameSpace::AskForMoreReservedMemory (size_t old_size, size_t need_size)
{
    //Abhi, call the host....

    IGCHostControl *pGCHostControl = CorHost::GetGCHostControl();

    if (pGCHostControl)
    {
        size_t new_max_limit_size = need_size;
        pGCHostControl->RequestVirtualMemLimit (old_size, 
                                                (SIZE_T*)&new_max_limit_size);
        return new_max_limit_size;
    }
    else
        return old_size + need_size;
}


// PromoteCarefully
//
// Clients who know they MAY have an interior pointer should come through here.  We
// can efficiently check whether our object lives on the current stack.  If so, our
// reference to it is not an interior pointer.  This is more efficient than asking
// the heap to verify whether our reference is interior, since it would have to
// check all the heap segments, including those containing large objects.
//
// Note that we only have to check against the thread we are currently crawling.  It
// would be illegal for us to have a ByRef from someone else's stack.  And this will
// be asserted if we pass this reference to the heap as a potentially interior pointer.
//
// But the thread we are currently crawling is not the currently executing thread (in
// the general case).  We rely on fragile caching of the interesting thread, in our
// call to UpdateCachedStackInfo() where we initiate the crawl in GcScanRoots() above.
//
// The flags must indicate that the have an interior pointer GC_CALL_INTERIOR
// additionally the flags may indicate that we also have a pinned local byref
// 
void PromoteCarefully(promote_func  fn, 
                      Object *& obj, 
                      ScanContext*  sc, 
                      DWORD         flags /* = GC_CALL_INTERIOR*/ )
{
    //
    // Sanity check that the flags contain only these three values
    //
    assert((flags & ~(GC_CALL_INTERIOR|GC_CALL_PINNED|GC_CALL_CHECK_APP_DOMAIN)) == 0);

    //
    // Sanity check that GC_CALL_INTERIOR FLAG is set
    //
    assert(flags & GC_CALL_INTERIOR);

    // Note that the base is at a higher address than the limit, since the stack
    // grows downwards.
    if (obj <= Thread::GetNonCurrentStackBase(sc) &&
        obj >  Thread::GetNonCurrentStackLimit(sc))
    {
        return;
    }

    (*fn) (obj, sc, flags);
}


//
// Handles arrays of arbitrary dimensions
//
// If dwNumArgs is set to greater than 1 for a SZARRAY this function will recursively 
// allocate sub-arrays and fill them in.  
//
// For arrays with lower bounds, pBounds is <lower bound 1>, <count 1>, <lower bound 2>, ...

OBJECTREF AllocateArrayEx(TypeHandle arrayType, DWORD *pArgs, DWORD dwNumArgs, BOOL bAllocateInLargeHeap) 
{
    THROWSCOMPLUSEXCEPTION();

    ArrayTypeDesc* arrayDesc = arrayType.AsArray();
    MethodTable* pArrayMT = arrayDesc->GetMethodTable();
    CorElementType kind = arrayType.GetNormCorElementType();
    _ASSERTE(kind == ELEMENT_TYPE_ARRAY || kind == ELEMENT_TYPE_SZARRAY);
    
    // Calculate the total number of elements int the array
    unsigned cElements = pArgs[0];
    bool providedLowerBounds = false;
    unsigned rank;
    if (kind == ELEMENT_TYPE_ARRAY)
    {
        rank = arrayDesc->GetRank();
        _ASSERTE(dwNumArgs == rank || dwNumArgs == 2*rank);

        // Morph a ARRAY rank 1 with 0 lower bound into an SZARRAY
        if (rank == 1 && (dwNumArgs == 1 || pArgs[0] == 0)) {  // lower bound is zero
            TypeHandle szArrayType = arrayDesc->GetModule()->GetClassLoader()->FindArrayForElem(arrayDesc->GetElementTypeHandle(), ELEMENT_TYPE_SZARRAY, 1, 0);
            if (szArrayType.IsNull())
            {
                _ASSERTE(!"Unable to load array class");
                return 0;
            }
            return AllocateArrayEx(szArrayType, &pArgs[dwNumArgs - 1], 1, bAllocateInLargeHeap);
        }

        providedLowerBounds = (dwNumArgs == 2*rank);
        cElements = 1;
        for (unsigned i = 0; i < dwNumArgs; i++)
        {
            int lowerBound = 0;
            if (providedLowerBounds)
            {
                lowerBound = pArgs[i];
                i++;
            }
            int length = pArgs[i];

            if (int(pArgs[i]) < 0)
                COMPlusThrow(kOverflowException);
            if (lowerBound + length < lowerBound)
                COMPlusThrow(kArgumentOutOfRangeException, L"ArgumentOutOfRange_ArrayLBAndLength");

            unsigned __int64 temp = (unsigned __int64) cElements * pArgs[i];
            if ((temp >> 32) != 0)              // watch for wrap around
                COMPlusThrowOM();
            cElements = (unsigned) temp;
        }
    } 
    else if (int(cElements) < 0)
        COMPlusThrow(kOverflowException);

    // Allocate the space from the GC heap
    unsigned __int64 totalSize = (unsigned __int64) cElements * pArrayMT->GetComponentSize() + pArrayMT->GetBaseSize();
    if ((totalSize >> 32) != 0)         // watch for wrap around
        COMPlusThrowOM();
    ArrayBase* orObject;
    if (bAllocateInLargeHeap)
        orObject = (ArrayBase *) AllocLHeap((unsigned) totalSize, FALSE, pArrayMT->ContainsPointers());
    else
        orObject = (ArrayBase *) Alloc((unsigned) totalSize, FALSE, pArrayMT->ContainsPointers());

        // Initialize Object
    orObject->SetMethodTable(pArrayMT);
    orObject->m_NumComponents = cElements;

    if (pArrayMT->HasSharedMethodTable())
    {
#ifdef  _LOGALLOC
#ifdef LOGGING
        if (LoggingOn(LF_GCALLOC, LL_INFO10))
        {
            CQuickBytes qb;
            LPSTR buffer = (LPSTR) qb.Alloc(MAX_CLASSNAME_LENGTH * sizeof(CHAR));
            arrayDesc->GetElementTypeHandle().GetName(buffer, MAX_CLASSNAME_LENGTH * sizeof(CHAR));

            LogSpewAlways("Allocated %s_TYPE %#x %5d bytes for %s[]\n", pArrayMT->GetClass()->IsValueClass() ? "VAL" : "REF", orObject, (DWORD)totalSize, buffer);
            if (LoggingOn(LF_GCALLOC, LL_INFO1000) || (LoggingOn(LF_GCALLOC, LL_INFO100) && ToLogOrNotToLog((DWORD)totalSize, buffer)))
            {
                void LogStackTrace();
                LogStackTrace();
            }
        }
#endif
#endif
        orObject->SetElementTypeHandle(arrayDesc->GetElementTypeHandle());
    }
    else
        LogAlloc((DWORD)totalSize, pArrayMT, orObject);

#ifdef PROFILING_SUPPORTED
    // Notify the profiler of the allocation
    if (CORProfilerTrackAllocations())
    {
        g_profControlBlock.pProfInterface->ObjectAllocated(
            (ObjectID)orObject, (ClassID) orObject->GetTypeHandle().AsPtr());
    }
#endif // PROFILING_SUPPORTED

#if CHECK_APP_DOMAIN_LEAKS
    if (g_pConfig->AppDomainLeaks())
        orObject->SetAppDomain();
#endif

    if (kind == ELEMENT_TYPE_ARRAY)
    {
        DWORD *pCountsPtr = (DWORD*) orObject->GetBoundsPtr();
        DWORD *pLowerBoundsPtr = (DWORD*) orObject->GetLowerBoundsPtr();
        for (unsigned i = 0; i < dwNumArgs; i++)
        {
            if (providedLowerBounds)
                *pLowerBoundsPtr++ = pArgs[i++];        // if not stated, lower bound becomes 0
            *pCountsPtr++ = pArgs[i];
        }
    }
    else
    {
                        // Handle allocating multiple jagged array dimensions at once
        if (dwNumArgs > 1)
        {
            PTRARRAYREF pOuterArray = (PTRARRAYREF) ObjectToOBJECTREF((Object*)orObject);
            PTRARRAYREF ret;
            GCPROTECT_BEGIN(pOuterArray);

            #ifdef STRESS_HEAP
            // Turn off GC stress, it is of little value here
            int gcStress = g_pConfig->GetGCStressLevel();
            g_pConfig->SetGCStressLevel(0);
            #endif //STRESS_HEAP
            
            // Allocate dwProvidedBounds arrays
            if (!arrayDesc->GetElementTypeHandle().IsArray()) {
                ret = NULL;
            } else {
                TypeHandle subArrayType = arrayDesc->GetElementTypeHandle().AsArray();
                for (unsigned i = 0; i < cElements; i++)
                {
                    OBJECTREF or = AllocateArrayEx(subArrayType, &pArgs[1], dwNumArgs-1, bAllocateInLargeHeap);
                    pOuterArray->SetAt(i, or);
                }
                
                #ifdef STRESS_HEAP
                g_pConfig->SetGCStressLevel(gcStress);      // restore GCStress
                #endif // STRESS_HEAP
                
                ret = pOuterArray;                          // have to pass it in another var since GCPROTECTE_END zaps it
            }
            GCPROTECT_END();
            return (OBJECTREF) ret;
        }
    }

    return( ObjectToOBJECTREF((Object*)orObject) );
}

/*
 * Allocates a single dimensional array of primitive types.
 */
OBJECTREF   AllocatePrimitiveArray(CorElementType type, DWORD cElements, BOOL bAllocateInLargeHeap)
{
    _ASSERTE(type >= ELEMENT_TYPE_BOOLEAN && type <= ELEMENT_TYPE_R8);

    // Fetch the proper array type
    if (g_pPredefinedArrayTypes[type] == NULL)
    {
        TypeHandle elemType = ElementTypeToTypeHandle(type);
        if (elemType.IsNull())
            return(NULL);
        g_pPredefinedArrayTypes[type] = SystemDomain::Loader()->FindArrayForElem(elemType, ELEMENT_TYPE_SZARRAY).AsArray();
        if (g_pPredefinedArrayTypes[type] == NULL) {
            _ASSERTE(!"Failed to load primitve array class");
            return NULL;
        }
    }
    return FastAllocatePrimitiveArray(g_pPredefinedArrayTypes[type]->GetMethodTable(), cElements, bAllocateInLargeHeap);
}

/*
 * Allocates a single dimensional array of primitive types.
 */

OBJECTREF   FastAllocatePrimitiveArray(MethodTable* pMT, DWORD cElements, BOOL bAllocateInLargeHeap)
{
    THROWSCOMPLUSEXCEPTION();

    _ASSERTE(pMT && pMT->IsArray());

    unsigned __int64 AlignSize = (((unsigned __int64) cElements) * pMT->GetComponentSize()) + pMT->GetBaseSize();
    if ((AlignSize >> 32) != 0)              // watch for wrap around
        COMPlusThrowOM();

    ArrayBase* orObject;
    if (bAllocateInLargeHeap)
        orObject = (ArrayBase*) AllocLHeap((DWORD) AlignSize, FALSE, FALSE);
    else
        orObject = (ArrayBase*) Alloc((DWORD) AlignSize, FALSE, FALSE);

    // Initialize Object
    orObject->SetMethodTable( pMT );
    _ASSERTE(orObject->GetMethodTable() != NULL);
    orObject->m_NumComponents = cElements;

#ifdef PROFILING_SUPPORTED
    // Notify the profiler of the allocation
    if (CORProfilerTrackAllocations())
    {
        g_profControlBlock.pProfInterface->ObjectAllocated(
            (ObjectID)orObject, (ClassID) orObject->GetTypeHandle().AsPtr());
    }
#endif // PROFILING_SUPPORTED

    LogAlloc((DWORD) AlignSize, pMT, orObject);

#if CHECK_APP_DOMAIN_LEAKS
    if (g_pConfig->AppDomainLeaks())
        orObject->SetAppDomain();
#endif

    return( ObjectToOBJECTREF((Object*)orObject) );
}

//
// Allocate an array which is the same size as pRef.  However, do not zero out the array.
//
OBJECTREF   DupArrayForCloning(BASEARRAYREF pRef, BOOL bAllocateInLargeHeap)
{
    THROWSCOMPLUSEXCEPTION();

    ArrayTypeDesc arrayType(pRef->GetMethodTable(), pRef->GetElementTypeHandle());
    unsigned rank = arrayType.GetRank();

    DWORD numArgs =  rank*2;
    DWORD* args = (DWORD*) _alloca(sizeof(DWORD)*numArgs);

    if (arrayType.GetNormCorElementType() == ELEMENT_TYPE_ARRAY)
    {
        const DWORD* bounds = pRef->GetBoundsPtr();
        const DWORD* lowerBounds = pRef->GetLowerBoundsPtr();
        for(unsigned int i=0; i < rank; i++) 
        {
            args[2*i]   = lowerBounds[i];
            args[2*i+1] = bounds[i];
        }
    }
    else
    {
        numArgs = 1;
        args[0] = pRef->GetNumComponents();
    }
    return AllocateArrayEx(TypeHandle(&arrayType), args, numArgs, bAllocateInLargeHeap);
}

//
// Helper for parts of the EE which are allocating arrays
//
OBJECTREF   AllocateObjectArray(DWORD cElements, TypeHandle elementType, BOOL bAllocateInLargeHeap)
{
    // The object array class is loaded at startup.
    _ASSERTE(g_pPredefinedArrayTypes[ELEMENT_TYPE_OBJECT] != NULL);

    ArrayTypeDesc arrayType(g_pPredefinedArrayTypes[ELEMENT_TYPE_OBJECT]->GetMethodTable(), elementType);
    return AllocateArrayEx(TypeHandle(&arrayType), &cElements, 1, bAllocateInLargeHeap);
}


STRINGREF SlowAllocateString( DWORD cchArrayLength )
{
    StringObject    *orObject  = NULL;
    DWORD           ObjectSize;

    THROWSCOMPLUSEXCEPTION();

    _ASSERTE( GetThread()->PreemptiveGCDisabled() );
    
    ObjectSize = g_pStringClass->GetBaseSize() + (cchArrayLength * sizeof(WCHAR));

    //Check for overflow.
    if (ObjectSize < cchArrayLength) 
        COMPlusThrowOM();

    orObject = (StringObject *)Alloc( ObjectSize, FALSE, FALSE );

    // Object is zero-init already
    _ASSERTE( ! orObject->HasSyncBlockIndex() );

    // Initialize Object
    //@TODO need to build a LARGE g_pStringMethodTable before
    orObject->SetMethodTable( g_pStringClass );
    orObject->SetArrayLength( cchArrayLength );

#ifdef PROFILING_SUPPORTED
    // Notification to the profiler of the allocation
    if (CORProfilerTrackAllocations())
    {
        g_profControlBlock.pProfInterface->ObjectAllocated(
            (ObjectID)orObject, (ClassID) orObject->GetTypeHandle().AsPtr());
    }
#endif // PROFILILNG_SUPPORTED

    LogAlloc(ObjectSize, g_pStringClass, orObject);

#if CHECK_APP_DOMAIN_LEAKS
    if (g_pConfig->AppDomainLeaks())
        orObject->SetAppDomain(); 
#endif

    return( ObjectToSTRINGREF(orObject) );
}



// OBJECTREF AllocateComClassObject(ComClassFactory* pComClsFac)
void AllocateComClassObject(ComClassFactory* pComClsFac, OBJECTREF* ppRefClass)
{
    THROWSCOMPLUSEXCEPTION();

    _ASSERTE(pComClsFac != NULL);
    COMClass::EnsureReflectionInitialized();
    // Create a COM+ Class object.  Since these may be proxied, go through AllocateObject
    // rather than FastAllocateObject.
    MethodTable *pMT = COMClass::GetRuntimeType();
    _ASSERTE(pMT != NULL);
    *ppRefClass= AllocateObject(pMT);
    
    if (*ppRefClass != NULL)
    {
        SyncBlock* pSyncBlock = (*((REFLECTCLASSBASEREF*) ppRefClass))->GetSyncBlockSpecial();
        if (pSyncBlock != NULL)
        {
            // @TODO: This needs to support a COM version of ReflectClass.  Right now we 
            //  still work as we used to <darylo>
            EEClass* pClass = SystemDomain::GetDefaultComObject()->GetClass();
            _ASSERTE(pClass != NULL);
            ReflectClass* p = (ReflectClass*) new (pClass->GetDomain()) ReflectBaseClass();
            if (!p)
                COMPlusThrowOM();
            // class for ComObject
            p->Init(pClass);
            p->SetCOMObject(pComClsFac);
            (*((REFLECTCLASSBASEREF*) ppRefClass))->SetData(p);
            // @todo HACK, this will get cleanedup when we have full reflection
            // on COM
            // set the EEClass to 1, as this is special type of Class
            //(*ppRefClass)->SetReflClass((EEClass *)1);
            // Set the data in the COM+ object
            //(*ppRefClass)->SetData(pComClsFac);
            // store the ComClassFactory wrapper in syncblock of the class
            // the low bits are set to differentiate this pointer from ComCallWrappers
            // which are also stored in the sync block
            pSyncBlock->SetComClassFactory((LPVOID)((size_t)pComClsFac | 0x3));
            
        }
    }   
}

void AllocateComClassObject(ReflectClass* pRef, OBJECTREF* ppRefClass)
{
    COMClass::EnsureReflectionInitialized();
    // Create a COM+ Class object.  Since these may be proxied, go through AllocateObject
    // rather than FastAllocateObject.
    // @TODO context cwb: revisit if we change Class objects to Agile.
    MethodTable *pMT = COMClass::GetRuntimeType();
    _ASSERTE(pMT != NULL);
    *ppRefClass= AllocateObject(pMT);
    
    if (*ppRefClass != NULL)
    {
        SyncBlock* pSyncBlock = (*((REFLECTCLASSBASEREF*) ppRefClass))->GetSyncBlockSpecial();
        if (pSyncBlock != NULL)
        {
            (*((REFLECTCLASSBASEREF*) ppRefClass))->SetData(pRef);
            // @todo HACK, this will get cleanedup when we have full reflection
            // on COM
            // set the EEClass to 1, as this is special type of Class
            //(*ppRefClass)->SetReflClass((EEClass *)1);
            // Set the data in the COM+ object
            //(*ppRefClass)->SetData(pComClsFac);
            // store the ComClassFactory wrapper in syncblock of the class
            // the low bits are set to differentiate this pointer from ComCallWrappers
            // which are also stored in the sync block
            pSyncBlock->SetComClassFactory((LPVOID)((size_t)(pRef->GetCOMObject()) | 0x3));
            
        }
    }   
}


// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
// ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING
// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
//
//  As FastAllocateObject and AllocateObject drift apart, be sure to update
//  CEEJitInfo::canUseFastNew() so that it understands when to use which service
//
// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
// ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING
// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

// AllocateObjectSpecial will throw OutOfMemoryException so don't need to check
// for NULL return value from it.
OBJECTREF AllocateObjectSpecial( MethodTable *pMT )
{
    THROWSCOMPLUSEXCEPTION();

    Object     *orObject = NULL;
    // use unchecked oref here to avoid triggering assert in Validate that the AD is
    // not set becuase it isn't until near the end of the fcn at which point we can allow
    // the check.
    _UNCHECKED_OBJECTREF oref;

    _ASSERTE( GetThread()->PreemptiveGCDisabled() );

    if (!pMT->IsComObjectType())
    {   
        orObject = (Object *) Alloc(pMT->GetBaseSize(),
                                    pMT->HasFinalizer(),
                                    pMT->ContainsPointers());

        // verify zero'd memory (at least for sync block)
        _ASSERTE( ! orObject->HasSyncBlockIndex() );

        orObject->SetMethodTable(pMT);

#if CHECK_APP_DOMAIN_LEAKS
        if (g_pConfig->AppDomainLeaks())
            orObject->SetAppDomain(); 
        else
#endif
        if (pMT->HasFinalizer())
            orObject->SetAppDomain(); 

#ifdef PROFILING_SUPPORTED
        // Notify the profiler of the allocation
        if (CORProfilerTrackAllocations())
        {
            g_profControlBlock.pProfInterface->ObjectAllocated(
                (ObjectID)orObject, (ClassID) orObject->GetTypeHandle().AsPtr());
        }
#endif // PROFILING_SUPPORTED

        LogAlloc(pMT->GetBaseSize(), pMT, orObject);

        oref = OBJECTREF_TO_UNCHECKED_OBJECTREF(orObject);
    }
    else
    {
        oref = OBJECTREF_TO_UNCHECKED_OBJECTREF(AllocateComObject_ForManaged(pMT));
    }

    return UNCHECKED_OBJECTREF_TO_OBJECTREF(oref);
}

// AllocateObject will throw OutOfMemoryException so don't need to check
// for NULL return value from it.
OBJECTREF AllocateObject( MethodTable *pMT )
{
    THROWSCOMPLUSEXCEPTION();

    _ASSERTE(pMT != NULL);
    OBJECTREF oref = AllocateObjectSpecial(pMT);

    // Note that we only consider contexts on normal objects.  Strings are context
    // agile; arrays are marshaled by value.  Neither needs to consider the context
    // during instantiation.

    return oref;
}


// The JIT compiles calls to FastAllocateObject instead of AllocateObject if it
// can prove that the caller and calleee are guaranteed to be in the same context.
//
// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
// ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING
// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
//
//  As FastAllocateObject and AllocateObject drift apart, be sure to update
//  CEEJitInfo::canUseFastNew() so that it understands when to use which service
//
// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
// ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING ** WARNING
// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

// FastAllocateObject will throw OutOfMemoryException so don't need to check
// for NULL return value from it.
OBJECTREF FastAllocateObject( MethodTable *pMT )
{
    THROWSCOMPLUSEXCEPTION();

    Object     *orObject = NULL;

    _ASSERTE( GetThread()->PreemptiveGCDisabled() );

    orObject = (Object *) Alloc(pMT->GetBaseSize(),
                                pMT->HasFinalizer(),
                                pMT->ContainsPointers());

    // verify zero'd memory (at least for sync block)
    _ASSERTE( ! orObject->HasSyncBlockIndex() );

    orObject->SetMethodTable(pMT);

#ifdef PROFILING_SUPPORTED
    // Notify the profiler of the allocation
    if (CORProfilerTrackAllocations())
    {
        g_profControlBlock.pProfInterface->ObjectAllocated(
            (ObjectID)orObject, (ClassID) orObject->GetTypeHandle().AsPtr());
    }
#endif // PROFILING_SUPPORTED

    LogAlloc(pMT->GetBaseSize(), pMT, orObject);

#if CHECK_APP_DOMAIN_LEAKS
    if (g_pConfig->AppDomainLeaks())
        orObject->SetAppDomain(); 
    else
#endif
    if (pMT->HasFinalizer())
        orObject->SetAppDomain(); 

    return( ObjectToOBJECTREF(orObject) );
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gcscan.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*
 * GCSCAN.H
 *
 * GC Root Scanning
 *
 */

#ifndef _GCSCAN_H_
#define _GCSCAN_H_

#include "gc.h"
typedef void promote_func(Object*&, ScanContext*, DWORD=0);

void PromoteCarefully(promote_func fn,
                      Object*& obj, 
                      ScanContext* sc, 
                      DWORD        flags = GC_CALL_INTERIOR);

typedef struct
{
    promote_func*  f;
    ScanContext*   sc;
} GCCONTEXT;


/*
 * @TODO (JSW): For compatibility with the existing GC code we use CNamespace
 * as the name of this class.   I'm planning on changing it to
 * something like GCDomain....
 */

typedef void enum_alloc_context_func(alloc_context*, void*); 

class CNameSpace
{
  public:

    // Regular stack Roots
    static void GcScanRoots (promote_func* fn, int condemned, int max_gen, 
                             ScanContext* sc, GCHeap* Hp=0);
    //
    static void GcScanHandles (promote_func* fn, int condemned, int max_gen, 
                               ScanContext* sc);

#ifdef GC_PROFILING
    //
    static void GcScanHandlesForProfiler (int max_gen, ScanContext* sc);
#endif // GC_PROFILING

    // scan for dead weak pointers
    static void GcWeakPtrScan (int condemned, int max_gen, ScanContext*sc );

    // scan for dead weak pointers
    static void GcShortWeakPtrScan (int condemned, int max_gen, 
                                    ScanContext* sc);

    // post-promotions callback
    static void GcPromotionsGranted (int condemned, int max_gen, 
                                     ScanContext* sc);

    // post-promotions callback some roots were demoted
    static void GcDemote (ScanContext* sc);

    static void GcEnumAllocContexts (enum_alloc_context_func* fn, void* arg);

    static void GcFixAllocContexts (void* arg);

    // post-gc callback.
    static void GcDoneAndThreadsResumed ()
    {
        _ASSERTE(0);
    }
	static size_t AskForMoreReservedMemory (size_t old_size, size_t need_size);
};



/*
 * Allocation Helpers
 *
 */

    // The main Array allocation routine, can do multi-dimensional
OBJECTREF AllocateArrayEx(TypeHandle arrayClass, DWORD *pArgs, DWORD dwNumArgs, BOOL bAllocateInLargeHeap = FALSE); 
    // Optimized verion of above
OBJECTREF FastAllocatePrimitiveArray(MethodTable* arrayType, DWORD cElements, BOOL bAllocateInLargeHeap = FALSE);


#ifdef _DEBUG
extern void  Assert_GCDisabled();
#endif //_DEBUG




#ifdef _X86_

    // for x86, we generate efficient allocators for some special cases
    // these are called via inline wrappers that call the generated allocators
    // via function pointers.


    // Create a SD array of primitive types

typedef Object* (__fastcall * FastPrimitiveArrayAllocatorFuncPtr)(CorElementType type, DWORD cElements);

extern FastPrimitiveArrayAllocatorFuncPtr fastPrimitiveArrayAllocator;

    // The fast version always allocates in the normal heap
inline OBJECTREF AllocatePrimitiveArray(CorElementType type, DWORD cElements)
{

#ifdef _DEBUG
    Assert_GCDisabled();
#endif //_DEBUG

    return OBJECTREF( fastPrimitiveArrayAllocator(type, cElements) );
}

    // The slow version is distinguished via overloading by an additional parameter
OBJECTREF AllocatePrimitiveArray(CorElementType type, DWORD cElements, BOOL bAllocateInLargeHeap);


    // Allocate SD array of object pointers

typedef Object* (__fastcall * FastObjectArrayAllocatorFuncPtr)(MethodTable *ElementType, DWORD cElements);

extern FastObjectArrayAllocatorFuncPtr fastObjectArrayAllocator;

    // The fast version always allocates in the normal heap
inline OBJECTREF AllocateObjectArray(DWORD cElements, TypeHandle ElementType)
{

#ifdef _DEBUG
    Assert_GCDisabled();
#endif //_DEBUG

    return OBJECTREF( fastObjectArrayAllocator(ElementType.AsMethodTable(), cElements) );
}

    // The slow version is distinguished via overloading by an additional parameter
OBJECTREF AllocateObjectArray(DWORD cElements, TypeHandle ElementType, BOOL bAllocateInLargeHeap);


    // Allocate string

typedef StringObject* (__fastcall * FastStringAllocatorFuncPtr)(DWORD cchArrayLength);

extern FastStringAllocatorFuncPtr fastStringAllocator;

inline STRINGREF AllocateString( DWORD cchArrayLength )
{
#ifdef _DEBUG
    Assert_GCDisabled();
#endif //_DEBUG
    return STRINGREF(fastStringAllocator(cchArrayLength-1));
}

    // The slow version, implemented in gcscan.cpp
STRINGREF SlowAllocateString( DWORD cchArrayLength );

#else

// On other platforms, go to the (somewhat less efficient) implementations in gcscan.cpp

    // Create a SD array of primitive types
OBJECTREF AllocatePrimitiveArray(CorElementType type, DWORD cElements, BOOL bAllocateInLargeHeap = FALSE);

    // Allocate SD array of object pointers
OBJECTREF AllocateObjectArray(DWORD cElements, TypeHandle ElementType, BOOL bAllocateInLargeHeap = FALSE);

STRINGREF SlowAllocateString( DWORD cchArrayLength );

inline STRINGREF AllocateString( DWORD cchArrayLength )
{
    return SlowAllocateString( cchArrayLength );
}

#endif

OBJECTREF DupArrayForCloning(BASEARRAYREF pRef, BOOL bAllocateInLargeHeap = FALSE);

OBJECTREF AllocateUninitializedStruct(MethodTable *pMT);

// The JIT requests the EE to specify an allocation helper to use at each new-site.
// The EE makes this choice based on whether context boundaries may be involved,
// whether the type is a COM object, whether it is a large object,
// whether the object requires finalization.
// These functions will throw OutOfMemoryException so don't need to check
// for NULL return value from them.

OBJECTREF AllocateObject( MethodTable *pMT );
OBJECTREF AllocateObjectSpecial( MethodTable *pMT );
OBJECTREF FastAllocateObject( MethodTable *pMT );

#endif _GCSCAN_H_
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gcsmp.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*++



Module Name:

    gc.cpp

Abstract:

   Automatic memory manager.

--*/
#include "common.h"
#include "object.inl"
#include "gcsmppriv.h"

#ifdef GC_PROFILING
#include "profilepriv.h"
#endif


#ifdef COUNT_CYCLES
#pragma warning(disable:4035)

static
unsigned        GetCycleCount32()        // enough for about 40 seconds
{
__asm   push    EDX
__asm   _emit   0x0F
__asm   _emit   0x31
__asm   pop     EDX
};

#pragma warning(default:4035)

long cstart = 0;

#endif //COUNT_CYCLES

#ifdef TIME_GC
long mark_time, plan_time, sweep_time, reloc_time, compact_time;
#endif //TIME_GC

#define ephemeral_low           g_ephemeral_low
#define ephemeral_high          g_ephemeral_high


extern void StompWriteBarrierEphemeral();
extern void StompWriteBarrierResize(BOOL bReqUpperBoundsCheck);

#ifdef TRACE_GC

int     print_level     = DEFAULT_GC_PRN_LVL;  //level of detail of the debug trace
int     gc_count        = 0;
BOOL    trace_gc        = FALSE;

hlet* hlet::bindings = 0;

#endif //TRACE_GC


DWORD gfNewGcEnable;
DWORD gfDisableClassCollection;


void mark_class_of (BYTE*);


//Alignment constant for allocation
#define ALIGNCONST (DATA_ALIGNMENT-1)



#define mem_reserve (MEM_RESERVE)


//This function clears a piece of memory
// size has to be Dword aligned
inline
void memclr ( BYTE* mem, size_t size)
{
   assert ((size & (sizeof (DWORD)-1)) == 0);
   DWORD* m= (DWORD*)mem;
   for (size_t i = 0; i < size / sizeof(DWORD); i++)
       *(m++) = 0;
}                                                               

inline 
void memcopy (BYTE* dmem, BYTE* smem, size_t size)
{

   assert ((size & (sizeof (DWORD)-1)) == 0);
   DWORD* dm= (DWORD*)dmem;
   DWORD* sm= (DWORD*)smem;
   DWORD* smlimit = (DWORD*)(smem + size);
   do { 
       *(dm++) = *(sm++);
   } while  (sm < smlimit);
}

inline
ptrdiff_t round_down (ptrdiff_t add, int pitch)
{
    return ((add / pitch) * pitch);
}

inline
size_t Align (size_t nbytes) 
{
    return (nbytes + ALIGNCONST) & ~ALIGNCONST;
}

inline
size_t AlignQword (size_t nbytes)
{
    return (nbytes + 7) & ~7;
}

inline
BOOL Aligned (size_t n)
{
    return (n & ALIGNCONST) == 0;
}

//CLR_SIZE  is the max amount of bytes from gen0 that is set to 0 in one chunk

#define CLR_SIZE ((size_t)(8*1024))


#define INITIAL_ALLOC (1024*1024*16)
#define LHEAP_ALLOC (1024*1024*16)


#define page_size OS_PAGE_SIZE

inline
size_t align_on_page (size_t add)
{
    return ((add + page_size - 1) & - (page_size));
}

inline
BYTE* align_on_page (BYTE* add)
{
    return (BYTE*)align_on_page ((size_t) add);
}

inline
size_t align_lower_page (size_t add)
{
    return (add & - (page_size));
}

inline
BYTE* align_lower_page (BYTE* add)
{
    return (BYTE*)align_lower_page ((size_t)add);
}

inline
BOOL power_of_two_p (size_t integer)
{
    return !(integer & (integer-1));
}

inline
BOOL oddp (size_t integer)
{
    return (integer & 1) != 0;
}



class mark;
class generation;
class heap_segment;
class CObjectHeader;
class dynamic_data;
class large_object_block;
class segment_manager;
class l_heap;
class sorted_table;

static
HRESULT AllocateCFinalize(CFinalize **pCFinalize);

void qsort1(BYTE** low, BYTE** high);

//no constructors because we initialize in make_generation

/* per heap static initialization */


size_t      gc_heap::reserved_memory = 0;
size_t      gc_heap::reserved_memory_limit = 0;




BYTE*       g_ephemeral_low = (BYTE*)1; 

BYTE*       g_ephemeral_high = (BYTE*)~0;


int         gc_heap::condemned_generation_num = 0;


BYTE*       gc_heap::lowest_address;

BYTE*       gc_heap::highest_address;

short*      gc_heap::brick_table;

BYTE*      gc_heap::card_table;

BYTE* 		gc_heap::scavenge_list;

BYTE* 		gc_heap::last_scavenge;

BOOL 		gc_heap::pinning;

BYTE*       gc_heap::gc_low;

BYTE*       gc_heap::gc_high;

size_t 		gc_heap::segment_size;

size_t 		gc_heap::lheap_size;

heap_segment* gc_heap::ephemeral_heap_segment = 0;


size_t      gc_heap::mark_stack_tos = 0;

size_t      gc_heap::mark_stack_bos = 0;

size_t      gc_heap::mark_stack_array_length = 0;

mark*       gc_heap::mark_stack_array = 0;



BYTE*       gc_heap::min_overflow_address = (BYTE*)~0;

BYTE*       gc_heap::max_overflow_address = 0;



GCSpinLock gc_heap::more_space_lock = SPIN_LOCK_INITIALIZER;

long m_GCLock = -1;

extern "C" {
generation  generation_table [NUMBERGENERATIONS];
}

dynamic_data gc_heap::dynamic_data_table [NUMBERGENERATIONS+1];

size_t   gc_heap::allocation_quantum = CLR_SIZE;

int   gc_heap::alloc_contexts_used = 0;



l_heap*      gc_heap::lheap = 0;

BYTE*       gc_heap::lheap_card_table = 0;

gmallocHeap* gc_heap::gheap = 0;

large_object_block* gc_heap::large_p_objects = 0;

large_object_block** gc_heap::last_large_p_object = &large_p_objects;

large_object_block* gc_heap::large_np_objects = 0;

size_t      gc_heap::large_objects_size = 0;

size_t      gc_heap::large_blocks_size = 0;



BOOL        gc_heap::gen0_bricks_cleared = FALSE;

CFinalize* gc_heap::finalize_queue = 0;

/* end of per heap static initialization */


/* static initialization */ 
int max_generation = 1;


BYTE* g_lowest_address = 0;

BYTE* g_highest_address = 0;

/* global versions of the card table and brick table */ 
DWORD*  g_card_table;

/* end of static initialization */ 


size_t gcard_of ( BYTE*);
void gset_card (size_t card);

#define memref(i) *(BYTE**)(i)

#define GC_MARKED       0x1
#define GC_PINNED       0x2
#define slot(i, j) ((BYTE**)(i))[j+1]
class CObjectHeader : public Object
{
public:
    /////
    //
    // Header Status Information
    //

    MethodTable    *GetMethodTable() const 
    { 
        return( (MethodTable *) (((size_t) m_pMethTab) & (~(GC_MARKED | GC_PINNED))));
    }

    void SetMarked()
    { 
        m_pMethTab = (MethodTable *) (((size_t) m_pMethTab) | GC_MARKED); 
    }
    
    BOOL IsMarked() const
    { 
        return !!(((size_t)m_pMethTab) & GC_MARKED); 
    }

    void SetPinned()
    { 
        m_pMethTab = (MethodTable *) (((size_t) m_pMethTab) | GC_PINNED); 
    }

    BOOL IsPinned() const
    {
        return !!(((size_t)m_pMethTab) & GC_PINNED); 
    }

    void ClearMarkedPinned()
    { 
        SetMethodTable( GetMethodTable() ); 
    }

    CGCDesc *GetSlotMap ()
    {
        ASSERT(GetMethodTable()->ContainsPointers());
        return CGCDesc::GetCGCDescFromMT(GetMethodTable());
    }

    void SetFree(size_t size)
    {
        I1Array     *pNewFreeObject;
        
        _ASSERTE( size >= sizeof(ArrayBase));
        assert (size == (size_t)(DWORD)size);
        pNewFreeObject = (I1Array *) this;
        pNewFreeObject->SetMethodTable( g_pFreeObjectMethodTable );
        int base_size = g_pFreeObjectMethodTable->GetBaseSize();
        assert (g_pFreeObjectMethodTable->GetComponentSize() == 1);
        ((ArrayBase *)pNewFreeObject)->m_NumComponents = (DWORD)(size - base_size);
#ifdef _DEBUG
        ((DWORD*) this)[-1] = 0;    // clear the sync block, 
#endif //_DEBUG
#ifdef VERIFY_HEAP
        assert ((DWORD)(((ArrayBase *)pNewFreeObject)->m_NumComponents) >= 0);
        if (g_pConfig->IsHeapVerifyEnabled())
            memset (((DWORD*)this)+2, 0xcc, 
                    ((ArrayBase *)pNewFreeObject)->m_NumComponents);
#endif //VERIFY_HEAP
    }

    BOOL IsFree () const
    {
        return (GetMethodTable() == g_pFreeObjectMethodTable);
    }
    
    // get the next header
    CObjectHeader* Next()
    { return (CObjectHeader*)(Align ((size_t)((char*)this + GetSize()))); }

    BOOL ContainsPointers() const
    {
        return GetMethodTable()->ContainsPointers();
    }

    Object* GetObjectBase() const
    {
        return (Object*) this;
    }

	void SetRelocation(BYTE* newlocation)
	{
			*(BYTE**)(((DWORD**)this)-1) = newlocation;
	}
	BYTE* GetRelocated() const
	{
		if (!IsPinned())
			return (BYTE*)*(((DWORD**)this)-1);
		else
			return (BYTE*)this;
	}

#ifdef _DEBUG
    friend BOOL IsValidCObjectHeader (CObjectHeader*);
#endif
};

inline CObjectHeader* GetObjectHeader(Object* object)
{
    return (CObjectHeader*)object;
}


#define free_list_slot(x) ((BYTE**)(x))[2]


//represents a section of the large object heap
class l_heap
{
public:
    void*  heap;  // pointer to the heap
    l_heap* next; //next heap
    size_t size;  //Total size (including l_heap)
    DWORD flags;  // 1: allocated from the segment_manager
};

/* flags definitions */

#define L_MANAGED 1 //heap has been allocated via segment_manager.

inline 
l_heap*& l_heap_next (l_heap* h)
{
    return h->next;
}

inline
size_t& l_heap_size (l_heap* h)
{
    return h->size;
}

inline
DWORD& l_heap_flags (l_heap* h)
{
    return h->flags;
}

inline 
void*& l_heap_heap (l_heap* h)
{
    return h->heap;
}

inline
BOOL l_heap_managed (l_heap* h)
{
    return l_heap_flags (h) & L_MANAGED;
}



void* virtual_alloc (size_t size)
{

    if ((gc_heap::reserved_memory + size) > gc_heap::reserved_memory_limit)
    {
        gc_heap::reserved_memory_limit = 
            CNameSpace::AskForMoreReservedMemory (gc_heap::reserved_memory_limit, size);
        if ((gc_heap::reserved_memory + size) > gc_heap::reserved_memory_limit)
            return 0;
    }
    gc_heap::reserved_memory += size;

    void* prgmem = VirtualAlloc (0, size, mem_reserve, PAGE_READWRITE);

    WS_PERF_LOG_PAGE_RANGE(0, prgmem, (unsigned char *)prgmem + size - OS_PAGE_SIZE, size);
    return prgmem;

}

void virtual_free (void* add, size_t size)
{

    VirtualFree (add, 0, MEM_RELEASE);

    WS_PERF_LOG_PAGE_RANGE(0, add, 0, 0);
    gc_heap::reserved_memory -= size;
}


//returns 0 in case of allocation failure
heap_segment* gc_heap::get_segment()
{
    heap_segment* result;
    BYTE* alloced = (BYTE*)virtual_alloc (segment_size);
	if (!alloced)
		return 0;

	if (gc_heap::grow_brick_card_tables (alloced, alloced + segment_size) != 0)
	{
		virtual_free (alloced, segment_size);
		return 0;
	}

    result = make_heap_segment (alloced, segment_size);
    return result;
}

//returns 0 in case of allocation failure
l_heap* gc_heap::get_heap()
{
    l_heap* result;
    BYTE* alloced = (BYTE*)virtual_alloc (lheap_size);
	if (!alloced)
		return 0;

	if (gc_heap::grow_brick_card_tables (alloced, alloced + lheap_size) != 0)
	{
		virtual_free (alloced, segment_size);
		return 0;
	}

    result = make_large_heap (alloced, lheap_size, TRUE);
    return result;
}

class large_object_block
{
public:
    large_object_block*    next;      // Points to the next block
    large_object_block**   prev;      // Points to &(prev->next) where prev is the previous block
#if (SIZEOF_OBJHEADER % 8) != 0
    BYTE                   pad1[8 - (SIZEOF_OBJHEADER % 8)];    // Must pad to quad word
#endif
    plug                   plug;      // space for ObjHeader
};

inline
BYTE* block_object (large_object_block* b)
{
    assert ((BYTE*)AlignQword ((size_t)(b+1)) == (BYTE*)((size_t)(b+1)));
    return (BYTE*)AlignQword ((size_t)(b+1));
}

inline 
large_object_block* object_block (BYTE* o)
{
    return (large_object_block*)o - 1;
}

inline 
large_object_block* large_object_block_next (large_object_block* bl)
{
    return bl->next;
}

inline
BYTE* next_large_object (BYTE* o)
{
    large_object_block* x = large_object_block_next (object_block (o));
    if (x != 0)
        return block_object (x);
    else
        return 0;
}


class mark
{
public:
    BYTE* first;
    union 
    {
        BYTE* last;
        size_t len;
    };
};
inline
BYTE*& mark_first (mark* inst)
{
  return inst->first;
}
inline
BYTE*& mark_last (mark* inst)
{
  return inst->last;
}

/**********************************
   called at the beginning of GC to fix the allocated size to 
   what is really allocated, or to turn the free area into an unused object
   It needs to be called after all of the other allocation contexts have been 
   fixed 
 ********************************/

void gc_heap::fix_youngest_allocation_area ()
{
    alloc_context* acontext = generation_alloc_context (youngest_generation);
    dprintf (3, ("generation 0 alloc context: ptr: %p, limit %p", 
                 acontext->alloc_ptr, acontext->alloc_limit));
    fix_allocation_context (acontext);

	acontext->alloc_ptr = heap_segment_allocated (ephemeral_heap_segment);
	acontext->alloc_limit = acontext->alloc_ptr;
}


void gc_heap::fix_allocation_context (alloc_context* acontext)
{
    dprintf (3, ("Fixing allocation context %p: ptr: %p, limit: %p",
                  acontext, 
                  acontext->alloc_ptr, acontext->alloc_limit));
    if ((acontext->alloc_limit + Align (min_obj_size)) < heap_segment_allocated (ephemeral_heap_segment))
    {

        BYTE*  point = acontext->alloc_ptr;
        if (point != 0)
        {
            size_t  size = (acontext->alloc_limit - acontext->alloc_ptr);
            // the allocation area was from the free list
            // it was shortened by Align (min_obj_size) to make room for 
            // at least the shortest unused object
            size += Align (min_obj_size);
            assert ((size >= Align (min_obj_size)));

            dprintf(3,("Making unused area [%p, %p[", point, 
                       point + size ));
            make_unused_array (point, size);

			alloc_contexts_used ++; 

        }
    }
    else 
    {
        heap_segment_allocated (ephemeral_heap_segment) = acontext->alloc_ptr;
        assert (heap_segment_allocated (ephemeral_heap_segment) <= 
                heap_segment_committed (ephemeral_heap_segment));
        alloc_contexts_used ++; 
    }

	acontext->alloc_ptr = 0;
	acontext->alloc_limit = acontext->alloc_ptr;
}

void gc_heap::fix_older_allocation_area (generation* older_gen)
{
    heap_segment* older_gen_seg = generation_allocation_segment (older_gen);
	BYTE*  point = generation_allocation_pointer (older_gen);
    
	size_t  size = (generation_allocation_limit (older_gen) -
                               generation_allocation_pointer (older_gen));
	if (size != 0)
	{
		assert ((size >= Align (min_obj_size)));
		dprintf(3,("Making unused area [%p, %p[", point, point+size));
		make_unused_array (point, size);
	}

    if (generation_allocation_limit (older_gen) !=
        heap_segment_plan_allocated (older_gen_seg))
    {
		// return the unused portion to the free list
		if (size > Align (min_free_list))
		{
			free_list_slot (point) = generation_free_list (older_gen);
			generation_free_list (older_gen) = point;
            generation_free_list_space (older_gen) += 
				((size-Align (min_free_list))/LARGE_OBJECT_SIZE)*LARGE_OBJECT_SIZE;

		}

    }
    else
    {
        heap_segment_plan_allocated (older_gen_seg) =
            generation_allocation_pointer (older_gen);
    }
	generation_allocation_pointer (older_gen) = 0;
	generation_allocation_limit (older_gen) = 0;

}


inline
ptrdiff_t  gc_heap::get_new_allocation (int gen_number)
{
    return dd_new_allocation (dynamic_data_of (gen_number));
}


inline
BOOL gc_heap::ephemeral_pointer_p (BYTE* o)
{
    return ((o >= ephemeral_low) && (o < ephemeral_high));
}

void gc_heap::make_mark_stack (mark* arr)
{
    mark_stack_tos = 0;
    mark_stack_bos = 0;
    mark_stack_array = arr;
    mark_stack_array_length = 100;
}

#define brick_size 2048

inline
size_t gc_heap::brick_of (BYTE* add)
{
    return (size_t)(add - lowest_address) / brick_size;
}

inline
BYTE* gc_heap::brick_address (size_t brick)
{
    return lowest_address + (brick_size * brick);
}


void gc_heap::clear_brick_table (BYTE* from, BYTE* end)
{
    for (size_t i = brick_of (from);i < brick_of (end); i++)
        brick_table[i] = -32768;
}


inline
void gc_heap::set_brick (size_t index, ptrdiff_t val)
{
    if (val < -32767)
        val = -32767;
    assert (val < 32767);
    brick_table [index] = (short)val;
}

inline
BYTE* align_on_brick (BYTE* add)
{
    return (BYTE*)((size_t)(add + brick_size - 1) & - (brick_size));
}

inline
BYTE* align_lower_brick (BYTE* add)
{
    return (BYTE*)(((size_t)add) & - (brick_size));
}

size_t size_brick_of (BYTE* from, BYTE* end)
{
    assert (((size_t)from & (brick_size-1)) == 0);
    assert (((size_t)end  & (brick_size-1)) == 0);

    return ((end - from) / brick_size) * sizeof (short);
}


#define card_size 1024

inline
BYTE* gc_heap::card_address (size_t card)
{
    return  lowest_address + card_size * card;
}

inline
size_t gc_heap::card_of ( BYTE* object)
{
    return (size_t)(object - lowest_address) / card_size;
}

inline
size_t gcard_of ( BYTE* object)
{
    return (size_t)(object - g_lowest_address) / card_size;
}



inline
size_t gc_heap::card_to_brick (size_t card)
{
    return brick_of (card_address (card));
}

inline
BYTE* align_on_card (BYTE* add)
{
    return (BYTE*)((size_t)(add + card_size - 1) & - (card_size));
}

inline
BYTE* align_lower_card (BYTE* add)
{
    return (BYTE*)((size_t)add & - (card_size));
}

inline
void gc_heap::clear_card (size_t card)
{
    card_table [card] = 0;
    dprintf (3,("Cleared card %p [%p, %p[", card, card_address (card),
              card_address (card+1)));
}

inline
void gc_heap::set_card (size_t card)
{
    card_table [card] = (BYTE)~0;
}

inline
void gset_card (size_t card)
{
    ((BYTE*)g_card_table) [card] = (BYTE)~0;
}

inline
BOOL  gc_heap::card_set_p (size_t card)
{
    return card_table [ card ];
}


size_t size_card_of (BYTE* from, BYTE* end)
{
    assert (((size_t)from & ((card_size)-1)) == 0);
    assert (((size_t)end  & ((card_size)-1)) == 0);

    return ((end - from) /card_size);
}

class card_table_info
{
public:
    BYTE*       lowest_address;
    BYTE*       highest_address;
    short*      brick_table;
    BYTE*      next_card_table;
};



//These are accessors on untranslated cardtable

inline 
BYTE*& card_table_lowest_address (BYTE* c_table)
{
    return ((card_table_info*)((BYTE*)c_table - sizeof (card_table_info)))->lowest_address;
}

inline 
BYTE*& card_table_highest_address (BYTE* c_table)
{
    return ((card_table_info*)((BYTE*)c_table - sizeof (card_table_info)))->highest_address;
}

inline 
short*& card_table_brick_table (BYTE* c_table)
{
    return ((card_table_info*)((BYTE*)c_table - sizeof (card_table_info)))->brick_table;
}

//These work on untranslated card tables
inline 
BYTE*& card_table_next (BYTE* c_table)
{
    return ((card_table_info*)((BYTE*)c_table - sizeof (card_table_info)))->next_card_table;
}

inline 
size_t old_card_of (BYTE* object, BYTE* c_table)
{
    return (size_t) (object - card_table_lowest_address (c_table))/card_size;
}

void destroy_card_table (BYTE* c_table)
{

    VirtualFree ((BYTE*)c_table - sizeof(card_table_info), 0, MEM_RELEASE);
}


BYTE* make_card_table (BYTE* start, BYTE* end)
{


    assert (g_lowest_address == start);
    assert (g_highest_address == end);

    size_t bs = size_brick_of (start, end);
    size_t cs = size_card_of (start, end);
    size_t ms = 0;

    WS_PERF_SET_HEAP(GC_HEAP);
    BYTE* ct = (BYTE*)VirtualAlloc (0, sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)),
                                      MEM_COMMIT, PAGE_READWRITE);

    if (!ct)
        return 0;

    WS_PERF_LOG_PAGE_RANGE(0, ct, (unsigned char *)ct + sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)) - OS_PAGE_SIZE, sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)));
    WS_PERF_UPDATE("GC:make_card_table", bs + cs + ms + sizeof (card_table_info), ct);

    // initialize the ref count
    ct = ((BYTE*)ct+sizeof (card_table_info));
    card_table_lowest_address (ct) = start;
    card_table_highest_address (ct) = end;
    card_table_brick_table (ct) = (short*)((BYTE*)ct + cs);
    card_table_next (ct) = 0;
/*
    //clear the card table 
    memclr ((BYTE*)ct, cs);
*/

    return ct;
}


//returns 0 for success, -1 otherwise

int gc_heap::grow_brick_card_tables (BYTE* start, BYTE* end)
{
    BYTE* la = g_lowest_address;
    BYTE* ha = g_highest_address;
    g_lowest_address = min (start, g_lowest_address);
    g_highest_address = max (end, g_highest_address);
    // See if the address is already covered
    if ((la != g_lowest_address ) || (ha != g_highest_address))
    {
        BYTE* ct = 0;
        short* bt = 0;

        size_t cs = size_card_of (g_lowest_address, g_highest_address);
        size_t bs = size_brick_of (g_lowest_address, g_highest_address);


        size_t ms = 0;

        WS_PERF_SET_HEAP(GC_HEAP);
        ct = (BYTE*)VirtualAlloc (0, sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)),
                                      MEM_COMMIT, PAGE_READWRITE);


        if (!ct)
            goto fail;
        
        WS_PERF_LOG_PAGE_RANGE(0, ct, (unsigned char *)ct + sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)) - OS_PAGE_SIZE, sizeof (BYTE)*(bs + cs + ms + sizeof (card_table_info)));
        WS_PERF_UPDATE("GC:gc_heap:grow_brick_card_tables", cs + bs + ms + sizeof(card_table_info), ct);

        ct = (BYTE*)((BYTE*)ct + sizeof (card_table_info));
        card_table_lowest_address (ct) = g_lowest_address;
        card_table_highest_address (ct) = g_highest_address;
        card_table_next (ct) = (BYTE*)g_card_table;

        //clear the card table 

        bt = (short*)((BYTE*)ct + cs);

        // No initialization needed, will be done in copy_brick_card
#ifdef INTERIOR_POINTERS
        //But for interior pointers the entire brick table 
        //needs to be initialized. 
        {
            for (int i = 0;i < ((g_highest_address - g_lowest_address) / brick_size); i++)
            bt[i] = -32768;
        }

#endif //INTERIOR_POINTERS
        card_table_brick_table (ct) = bt;


        g_card_table = (DWORD*)ct;

        // This passes a bool telling whether we need to switch to the post
        // grow version of the write barrier.  This test tells us if the new
        // segment was allocated at a lower address than the old, requiring
        // that we start doing an upper bounds check in the write barrier.
        StompWriteBarrierResize(la != g_lowest_address);
        return 0;
    fail:
        //cleanup mess and return -1;
        g_lowest_address = la;
        g_highest_address = ha;

        if (ct)
        {
            VirtualFree (((BYTE*)ct - sizeof(card_table_info)), 0, MEM_RELEASE);
        }

        return -1;
    }
    return 0;


}

//copy all of the arrays managed by the card table for a page aligned range
void gc_heap::copy_brick_card_range (BYTE* la, BYTE* old_card_table,

                                     short* old_brick_table,
                                     heap_segment* seg,
                                     BYTE* start, BYTE* end, BOOL heap_expand)
{
    ptrdiff_t brick_offset = brick_of (start) - brick_of (la);

    dprintf (2, ("copying tables for range [%p %p[", start, end)); 
        
    // copy brick table
    short* brick_start = &brick_table [brick_of (start)];
    if (old_brick_table)
    {
        // segments are always on page boundaries 
        memcpy (brick_start, &old_brick_table[brick_offset], 
                ((end - start)/brick_size)*sizeof (short));

    }
    else 
    {
        assert (seg == 0);
        // This is a large heap, just clear the brick table
        clear_brick_table (start, end);
    }

    // n way merge with all of the card table ever used in between
    BYTE* c_table = card_table_next (card_table);
    assert (c_table);
	assert (card_table_next (old_card_table) == 0);
    while (c_table)
    {
        //copy if old card table contained [start, end[ 
        if ((card_table_highest_address (c_table) >= end) &&
            (card_table_lowest_address (c_table) <= start))
        {
            // or the card_tables
            BYTE* dest = &card_table [card_of (start)];
            BYTE* src = &c_table [old_card_of (start, c_table)];
            ptrdiff_t count = ((end - start)/(card_size));
            for (int x = 0; x < count; x++)
            {
                *dest |= *src;
                dest++;
                src++;
            }
        }
        c_table = card_table_next (c_table);
    }

}
void gc_heap::copy_brick_card_table(BOOL heap_expand) 
{

    BYTE* la = lowest_address;
    BYTE* ha = highest_address;
    BYTE* old_card_table = card_table;
    short* old_brick_table = brick_table;

    assert (la == card_table_lowest_address (old_card_table));
    assert (ha == card_table_highest_address (old_card_table));


    card_table = (BYTE*)g_card_table;

    highest_address = card_table_highest_address (card_table);
    lowest_address = card_table_lowest_address (card_table);

    brick_table = card_table_brick_table (card_table);


    // for each of the segments and heaps, copy the brick table and 
    // or the card table 
    heap_segment* seg = generation_start_segment (generation_of (max_generation));
    while (seg)
    {
        BYTE* end = align_on_page (heap_segment_allocated (seg));
        copy_brick_card_range (la, old_card_table, 
                               old_brick_table, seg,
                               (BYTE*)seg, end, heap_expand);
        seg = heap_segment_next (seg);
    }

    copy_brick_card_table_l_heap();

	// delete all accumulated card tables
	BYTE* cd = card_table_next ((BYTE*)g_card_table);
	while (cd)
	{
		BYTE* next_cd = card_table_next (cd);
		destroy_card_table (cd);
		cd = next_cd;
	}
	card_table_next ((BYTE*)g_card_table) = 0;

}

void gc_heap::copy_brick_card_table_l_heap ()
{

    if (lheap_card_table != (BYTE*)g_card_table)
    {

        BYTE* la = lowest_address;

        BYTE* old_card_table = lheap_card_table;

        assert (la == card_table_lowest_address (old_card_table));

        // Do the same thing for l_heaps 
        l_heap* h = lheap;
        while (h)
        {
            BYTE* hm = (BYTE*)l_heap_heap (h);
            BYTE* end = hm + align_on_page (l_heap_size (h));
            copy_brick_card_range (la, old_card_table, 
                                   0, 0,
                                   hm, end, FALSE);
            h = l_heap_next (h);
        }
        lheap_card_table = (BYTE*)g_card_table;
    }
}


#define header(i) ((CObjectHeader*)(i))


//Depending on the implementation of the mark and pin bit, it can 
//be more efficient to clear both of them at the same time, 
//or it can be better to clear the pinned bit only on pinned objects
//The code calls both clear_pinned(o) and clear_marked_pinned(o)
//but only one implementation will clear the pinned bit


#define marked(i) header(i)->IsMarked()
#define set_marked(i) header(i)->SetMarked()
#define clear_marked_pinned(i) header(i)->ClearMarkedPinned()
#define pinned(i) header(i)->IsPinned()
#define set_pinned(i) header(i)->SetPinned()

#define clear_pinned(i)

inline DWORD my_get_size (Object* ob)                          
{ 
    MethodTable* mT = ob->GetMethodTable();
    mT = (MethodTable *) (((ULONG_PTR) mT) & ~3);
    return (mT->GetBaseSize() + 
            (mT->GetComponentSize()? 
             (ob->GetNumComponents() * mT->GetComponentSize()) : 0));
}



//#define size(i) header(i)->GetSize()
#define size(i) my_get_size (header(i))

#define contain_pointers(i) header(i)->ContainsPointers()


BOOL gc_heap::is_marked (BYTE* o)
{
    return marked (o);
}


// return the generation number of an object.
// It is assumed that the object is valid.
inline
unsigned int gc_heap::object_gennum (BYTE* o)
{
    if ((o < heap_segment_reserved (ephemeral_heap_segment)) && 
        (o >= heap_segment_mem (ephemeral_heap_segment)))
    {
        // in an ephemeral generation.
        // going by decreasing population volume
        for (unsigned int i = max_generation; i > 0 ; i--)
        {
            if ((o < generation_allocation_start (generation_of (i-1))))
                return i;
        }
        return 0;
    }
    else
        return max_generation;
}


heap_segment* gc_heap::make_heap_segment (BYTE* new_pages, size_t size)
{
    void * res;

    size_t initial_commit = OS_PAGE_SIZE;

    WS_PERF_SET_HEAP(GC_HEAP);      
    //Commit the first page
    if ((res = VirtualAlloc (new_pages, initial_commit, 
                              MEM_COMMIT, PAGE_READWRITE)) == 0)
        return 0;
    WS_PERF_UPDATE("GC:gc_heap:make_heap_segment", initial_commit, res);

    //overlay the heap_segment
    heap_segment* new_segment = (heap_segment*)new_pages;
    heap_segment_mem (new_segment) = new_pages + Align (sizeof (heap_segment));
    heap_segment_reserved (new_segment) = new_pages + size;
    heap_segment_committed (new_segment) = new_pages + initial_commit;
    heap_segment_next (new_segment) = 0;
    heap_segment_plan_allocated (new_segment) = heap_segment_mem (new_segment);
    heap_segment_allocated (new_segment) = heap_segment_mem (new_segment);
    heap_segment_used (new_segment) = heap_segment_allocated (new_segment);


    dprintf (2, ("Creating heap segment %p", new_segment));
    return new_segment;
}

l_heap* gc_heap::make_large_heap (BYTE* new_pages, size_t size, BOOL managed)
{

    l_heap* new_heap = new l_heap();
    if (!new_heap)
        return 0;
    l_heap_size (new_heap) = size;
    l_heap_next (new_heap) = 0;
    l_heap_heap (new_heap) = new_pages;
    l_heap_flags (new_heap) = (size_t)new_pages | (managed ? L_MANAGED : 0);
    dprintf (2, ("Creating large heap %p", new_heap));
    return new_heap;
}


void gc_heap::delete_large_heap (l_heap* hp)
{

    l_heap* h = hp;
    do 
    {
        BYTE* hphp = (BYTE*)l_heap_heap (h);

        //for now, also decommit the non heap-managed heaps. 
        //This is conservative because the whole heap may not be committed. 
        VirtualFree (hphp, l_heap_size (h), MEM_RELEASE);

        h = l_heap_next (h);

    } while (h);
    
}

//Releases the segment to the OS.

void gc_heap::delete_heap_segment (heap_segment* seg)
{
    dprintf (2, ("Destroying segment [%p, %p[", seg,
                 heap_segment_reserved (seg)));

    VirtualFree (seg, heap_segment_committed(seg) - (BYTE*)seg, MEM_DECOMMIT);
    VirtualFree (seg, heap_segment_reserved(seg) - (BYTE*)seg, MEM_RELEASE);

}

//resets the pages beyond alloctes size so they won't be swapped out and back in

void gc_heap::reset_heap_segment_pages (heap_segment* seg)
{
#if 0
    size_t page_start = align_on_page ((size_t)heap_segment_allocated (seg));
    size_t size = (size_t)heap_segment_committed (seg) - page_start;
    if (size != 0)
        VirtualAlloc ((char*)page_start, size, MEM_RESET, PAGE_READWRITE);
#endif
}


void gc_heap::decommit_heap_segment_pages (heap_segment* seg)
{
#if 1
    BYTE*  page_start = align_on_page (heap_segment_allocated (seg));
    size_t size = heap_segment_committed (seg) - page_start;
    if (size >= 100*OS_PAGE_SIZE){
        page_start += 32*OS_PAGE_SIZE;
        size -= 32*OS_PAGE_SIZE;
        VirtualFree (page_start, size, MEM_DECOMMIT);
        heap_segment_committed (seg) = page_start;
    }
#endif
}

void gc_heap::rearrange_heap_segments()
{
    heap_segment* seg =
        generation_start_segment (generation_of (max_generation));
    heap_segment* prev_seg = 0;
    heap_segment* next_seg = 0;
    while (seg && (seg != ephemeral_heap_segment))
    {
        next_seg = heap_segment_next (seg);

        // check if the segment was reached by allocation
        if (heap_segment_plan_allocated (seg) ==
            heap_segment_mem (seg))
        {
            //if not, unthread and delete
            assert (prev_seg);
            heap_segment_next (prev_seg) = next_seg;
            delete_heap_segment (seg);
        }
        else
        {
            heap_segment_allocated (seg) =
                heap_segment_plan_allocated (seg);
            // reset the pages between allocated and committed.
            decommit_heap_segment_pages (seg);
            prev_seg = seg;

        }

        seg = next_seg;
    }
    //Heap expansion, thread the ephemeral segment.
    if (prev_seg && !seg)
    {
        prev_seg->next = ephemeral_heap_segment;
    }
}


void gc_heap::make_generation (generation& gen, heap_segment* seg,
                      BYTE* start, BYTE* pointer)
{
    gen.allocation_start = start;
    gen.allocation_context.alloc_ptr = pointer;
    gen.allocation_context.alloc_limit = pointer;
    gen.free_list = 0;
    gen.start_segment = seg;
    gen.allocation_segment = seg;
    gen.last_gap = 0;
    gen.plan_allocation_start = 0;
    gen.free_list_space = 0;
    gen.allocation_size = 0;
}

void gc_heap::reset_allocation_pointers (generation* gen, BYTE* start)
{
    assert (start);
    assert (Align ((size_t)start) == (size_t)start);
    generation_allocation_start (gen) = start;
    generation_allocation_pointer (gen) =  0;
    generation_allocation_limit (gen) = 0;
}

void gc_heap::adjust_ephemeral_limits ()
{
    ephemeral_low = generation_allocation_start (generation_of ( max_generation -1));
    ephemeral_high = heap_segment_reserved (ephemeral_heap_segment);

    // This updates the write barrier helpers with the new info.

    StompWriteBarrierEphemeral();

}

HRESULT gc_heap::initialize_gc (size_t vm_block_size,
                                size_t segment_size,
                                size_t heap_size)
{

    HRESULT hres = S_OK;

    reserved_memory = 0;
    reserved_memory_limit = vm_block_size;

	lheap_size = heap_size;
	gc_heap::segment_size = segment_size;

	BYTE* allocated = (BYTE*)virtual_alloc (segment_size + heap_size);
	if (!allocated)
        return E_OUTOFMEMORY;

    heap_segment* seg = make_heap_segment (allocated, segment_size);

    lheap = make_large_heap (allocated+segment_size, lheap_size, TRUE);

	g_lowest_address = allocated;
	g_highest_address = allocated + segment_size + lheap_size;

    g_card_table = (DWORD*)make_card_table (g_lowest_address, g_highest_address);

    if (!g_card_table)
        return E_OUTOFMEMORY;


#ifdef TRACE_GC
    print_level = g_pConfig->GetGCprnLvl();
#endif

    lheap_card_table = (BYTE*)g_card_table;

    gheap = new  gmallocHeap;
    if (!gheap)
        return E_OUTOFMEMORY;

    gheap->Init ("GCHeap", (DWORD*)l_heap_heap (lheap), 
                 (unsigned long)l_heap_size (lheap), heap_grow_hook, 
                 heap_pregrow_hook);

    card_table = (BYTE*)g_card_table;


    brick_table = card_table_brick_table ((BYTE*)g_card_table);
    highest_address = card_table_highest_address ((BYTE*)g_card_table);
    lowest_address = card_table_lowest_address ((BYTE*)g_card_table);

#ifndef INTERIOR_POINTERS
    //set the brick_table for large objects
    clear_brick_table ((BYTE*)l_heap_heap (lheap), 
                       (BYTE*)l_heap_heap (lheap) + l_heap_size (lheap));

#else //INTERIOR_POINTERS

    //Because of the interior pointer business, we have to clear 
    //the whole brick table
    //TODO: remove this code when code manager is fixed. 
    clear_brick_table (lowest_address, highest_address);
#endif //INTERIOR_POINTERS



    BYTE*  start = heap_segment_mem (seg);

	make_generation (generation_table [ (max_generation) ],
					 seg, start, 0);
	start += Align (min_obj_size); 
	
	generation*  gen = generation_of (max_generation);
	make_unused_array (generation_allocation_start (gen), Align (min_obj_size));	
    heap_segment_allocated (seg) = start;


    ephemeral_heap_segment = seg;

    init_dynamic_data();

	BYTE* start0 = allocate_semi_space (dd_desired_allocation (dynamic_data_of(0)));
	make_generation (generation_table [ (0) ],
					 seg, start0, 0);


    mark* arr = new (mark [100]);
    if (!arr)
        return E_OUTOFMEMORY;

    make_mark_stack(arr);

    adjust_ephemeral_limits();

    HRESULT hr = AllocateCFinalize(&finalize_queue);

    if (FAILED(hr))
        return hr;

    return hres;
}



void 
gc_heap::destroy_gc_heap(gc_heap* heap)
{

    // destroy every segment.
    heap_segment* seg = generation_start_segment (generation_of (max_generation));
    heap_segment* next_seg;
    while (seg)
    {
        next_seg = heap_segment_next (seg);
        delete_heap_segment (seg);
        seg = next_seg;
    }

    // get rid of the card table
    destroy_card_table (card_table);

    // destroy the mark stack

    delete mark_stack_array;

    if (finalize_queue)
        delete finalize_queue;

    delete heap;

    delete gheap;
    
    delete_large_heap (lheap);

    
}


//In the concurrent version, the Enable/DisablePreemptiveGC is optional because
//the gc thread call WaitLonger.
void WaitLonger (int i)
{
    // every 8th attempt:
    Thread *pCurThread = GetThread();
    BOOL bToggleGC;
    {
        bToggleGC = pCurThread->PreemptiveGCDisabled();
        if (bToggleGC)
            pCurThread->EnablePreemptiveGC();    
    }

    if  (g_SystemInfo.dwNumberOfProcessors > 1)
    {
		pause();			// indicate to the processor that we are spining 
        if  (i & 0x01f)
            __SwitchToThread (0);
        else
            __SwitchToThread (5);
    }
    else
        __SwitchToThread (5);

    {
        if (bToggleGC)
            pCurThread->DisablePreemptiveGC();
    }
}

#ifdef  MP_LOCKS
inline
static void enter_spin_lock (volatile long* lock)
{
retry:

    if (FastInterlockExchange ((long *)lock, 0) >= 0)
    {
        unsigned int i = 0;
        while (*lock >= 0)
        {
            if ((++i & 7) && !GCHeap::IsGCInProgress())
            {
                if  (g_SystemInfo.dwNumberOfProcessors > 1)
                {
                    for (int j = 0; j < 1000; j++)
                    {
                        if  (*lock < 0 || GCHeap::IsGCInProgress())
                            break;
						pause();			// indicate to the processor that we are spining 
                    }
                    if  (*lock >= 0 && !GCHeap::IsGCInProgress())
                        ::SwitchToThread();
                }
                else
                    ::SwitchToThread();
            }
            else
            {
                WaitLonger(i);
            }
        }
        goto retry;
    }
}
#else
inline
static void enter_spin_lock (long* lock)
{
retry:

    if (FastInterlockExchange (lock, 0) >= 0)
    {
        unsigned int i = 0;
        while (*lock >= 0)
        {
            if (++i & 7)
                __SwitchToThread (0);
            else
            {
                WaitLonger(i);
            }
        }
        goto retry;
    }
}
#endif

inline
static void leave_spin_lock(long *lock) 
{
    *lock = -1;
}


#ifdef _DEBUG

inline
static void enter_spin_lock(GCDebugSpinLock *pSpinLock) {
    enter_spin_lock(&pSpinLock->lock);
    pSpinLock->holding_thread = GetThread();
}

inline
static void leave_spin_lock(GCDebugSpinLock *pSpinLock) {
    _ASSERTE(pSpinLock->holding_thread == GetThread());
    pSpinLock->holding_thread = (Thread*) -1;
    if (pSpinLock->lock != -1)
        leave_spin_lock(&pSpinLock->lock);
}

#define ASSERT_HOLDING_SPIN_LOCK(pSpinLock) \
    _ASSERTE((pSpinLock)->holding_thread == GetThread());

#else

#define ASSERT_HOLDING_SPIN_LOCK(pSpinLock)

#endif


//BUGBUG this function should not be static. and the 
//gmheap object should keep some context. 
int gc_heap::heap_pregrow_hook (size_t memsize)
{
    if ((gc_heap::reserved_memory + memsize) > gc_heap::reserved_memory_limit)
    {
        gc_heap::reserved_memory_limit = 
            CNameSpace::AskForMoreReservedMemory (gc_heap::reserved_memory_limit, memsize);
        if ((gc_heap::reserved_memory + memsize) > gc_heap::reserved_memory_limit)
            return E_OUTOFMEMORY;
    }

    gc_heap::reserved_memory += memsize;

    return 0;
}

int gc_heap::heap_grow_hook (BYTE* mem, size_t memsize, ptrdiff_t ignore)
{
    int hres = 0;

    l_heap* new_heap = gc_heap::make_large_heap ((BYTE*)mem, memsize, FALSE);
    if (!new_heap)
    {
        hres = E_OUTOFMEMORY;
        return hres;
    }

    if (lheap)
        l_heap_next (new_heap) = lheap;
    
    lheap = new_heap;
    
    
    hres = grow_brick_card_tables ((BYTE*)mem, (BYTE*)mem + memsize);

    return hres;
}




inline
BOOL gc_heap::size_fit_p (size_t size, BYTE* alloc_pointer, BYTE* alloc_limit)
{
    return ((alloc_pointer + size + Align(min_obj_size)) <= alloc_limit) ||
            ((alloc_pointer + size) == alloc_limit);
}

inline
BOOL gc_heap::a_size_fit_p (size_t size, BYTE* alloc_pointer, BYTE* alloc_limit)
{
    return ((alloc_pointer + size + Align(min_obj_size)) <= alloc_limit) ||
            ((alloc_pointer + size) == alloc_limit);
}

// Grow by committing more pages
int gc_heap::grow_heap_segment (heap_segment* seg, size_t size)
{
    
    assert (size == align_on_page (size));

    size_t c_size = max (size, 16*OS_PAGE_SIZE);

    c_size = min (c_size, (size_t)(heap_segment_reserved (seg) -
                                   heap_segment_committed (seg)));
    if (c_size == 0)
        return 0;
    assert (c_size >= size);
    WS_PERF_SET_HEAP(GC_HEAP);      
    if (!VirtualAlloc (heap_segment_committed (seg), c_size,
                       MEM_COMMIT, PAGE_READWRITE))
    {
        return 0;
    }
    WS_PERF_UPDATE("GC:gc_heap:grow_heap_segment", c_size, heap_segment_committed (seg));

    assert (heap_segment_committed (seg) <= 
            heap_segment_reserved (seg));
    heap_segment_committed (seg) += c_size;
    return 1;

}

//used only in older generation allocation (i.e during gc). 
void gc_heap::adjust_limit (BYTE* start, size_t limit_size, generation* gen)
{
    dprintf(3,("gc Expanding segment allocation"));
    if (generation_allocation_limit (gen) != start)
    {
        BYTE*  hole = generation_allocation_pointer (gen);
        size_t  size = (generation_allocation_limit (gen) - generation_allocation_pointer (gen));
        if (size != 0)
        {
            dprintf(3,("gc filling up hole"));
            make_unused_array (hole, size);
        }
        generation_allocation_pointer (gen) = start;
    }
    generation_allocation_limit (gen) = (start + limit_size);
}


void gc_heap::adjust_limit_clr (BYTE* start, size_t limit_size, 
                                alloc_context* acontext, heap_segment* seg)
{
    assert (seg == ephemeral_heap_segment);
    assert (heap_segment_used (seg) <= heap_segment_committed (seg));

    dprintf(3,("Expanding segment allocation [%p, %p[", start, 
               start + limit_size - Align (min_obj_size)));
    if ((acontext->alloc_limit != start) &&
        (acontext->alloc_limit + Align (min_obj_size))!= start)
    {
        BYTE*  hole = acontext->alloc_ptr;
        if (hole != 0)
        {
            size_t  size = (acontext->alloc_limit - acontext->alloc_ptr);
            dprintf(3,("filling up hole [%p, %p[", hole, hole + size + Align (min_obj_size)));
            // when we are finishing an allocation from a free list
            // we know that the free area was Align(min_obj_size) larger
            make_unused_array (hole, size + Align (min_obj_size));
        }
        acontext->alloc_ptr = start;
    }
    acontext->alloc_limit = (start + limit_size - Align (min_obj_size));
    acontext->alloc_bytes += limit_size;

    {
        gen0_bricks_cleared = FALSE;
    }

    //Sometimes the allocated size is advanced without clearing the 
    //memory. Let's catch up here
    if (heap_segment_used (seg) < heap_segment_allocated (ephemeral_heap_segment))
    {
        heap_segment_used (seg) = heap_segment_allocated (ephemeral_heap_segment);

    }
    if ((start + limit_size) <= heap_segment_used (seg))
    {
        leave_spin_lock (&more_space_lock);
        memclr (start - plug_skew, limit_size);
    }
    else
    {
        BYTE* used = heap_segment_used (seg);
        heap_segment_used (seg) = start + limit_size;

        leave_spin_lock (&more_space_lock);
        memclr (start - plug_skew, used - start);
    }

}

/* in order to make the allocator faster, allocate returns a 
 * 0 filled object. Care must be taken to set the allocation limit to the 
 * allocation pointer after gc
 */

size_t gc_heap::limit_from_size (size_t size, size_t room)
{
#pragma warning(disable:4018)
    return new_allocation_limit ((size + Align (min_obj_size)),
                                 min (room,max (size + Align (min_obj_size), 
                                                allocation_quantum)));
#pragma warning(default:4018)
}


BYTE* gc_heap::allocate_semi_space (size_t dsize)
{

	size_t old_size = generation_free_list_space (generation_of (max_generation));

	dsize = Align (dsize);

again:
	heap_segment* seg = ephemeral_heap_segment;
	BYTE* start = heap_segment_allocated (ephemeral_heap_segment);

	BYTE* result = start;
	//do we have enough room for the whole gen0 allocation 
	// plus the promoted gen 0 in the current segment.
	//In the worst case this is 2 * dsize
	if (a_size_fit_p (2*dsize, start, heap_segment_reserved (seg)))
	{
		if (old_size < (dsize))
		{	
			// size computation include the size of the generation gap
			// and the size of the free list overhead
			size_t size = (((Align (dsize - old_size) + LARGE_OBJECT_SIZE-1) /LARGE_OBJECT_SIZE)*
						   LARGE_OBJECT_SIZE +
						   Align (min_obj_size) + 
						   Align (min_free_list));	
			if (a_size_fit_p (size, start, 
							  heap_segment_committed (seg)))
			{
				heap_segment_allocated (ephemeral_heap_segment) += size;
			}
			else
			{
				if (!grow_heap_segment (seg,
										align_on_page (start + size) - 
										heap_segment_committed(seg)))
				{
					assert (!"Memory exhausted during alloc");
					return 0;
				}
				heap_segment_allocated (ephemeral_heap_segment) += size;
			}
			//put it on the free list of gen1
			thread_gap (start, size - Align(min_obj_size));
			start += size - Align (min_obj_size);
		}
		else
		{
			start = heap_segment_allocated (ephemeral_heap_segment);
			heap_segment_allocated (ephemeral_heap_segment) += Align (min_obj_size);
			assert (heap_segment_allocated (ephemeral_heap_segment) <=
					heap_segment_committed (ephemeral_heap_segment));

		}
	}
	else
	{
		//We have to expand into another segment 
		start= expand_heap (get_segment(), dsize);
		generation_allocation_segment (youngest_generation) = 
			ephemeral_heap_segment;
		goto again;

	}
	//allocate the generation gap
	make_unused_array (start, Align (min_obj_size));
	return 	start;
}

BOOL gc_heap::allocate_more_space (alloc_context* acontext, size_t size)
{
    generation* gen = youngest_generation;
    enter_spin_lock (&more_space_lock);
    {
        BOOL ran_gc = FALSE;
        if (get_new_allocation (0) <=
			(ptrdiff_t)max (size + Align (min_obj_size), allocation_quantum))
        {
            if (!ran_gc)
            {
                ran_gc = TRUE;
                vm_heap->GarbageCollectGeneration();
            }
            else
            {
                assert(!"Out of memory");
                leave_spin_lock (&more_space_lock);
                return 0;
            }
        }
    try_again:
        {
            {
                heap_segment* seg = generation_allocation_segment (gen);
                if (a_size_fit_p (size, heap_segment_allocated (seg),
                                  heap_segment_committed (seg)))
                {
                    size_t limit = limit_from_size (size, (heap_segment_committed (seg) - 
                                                           heap_segment_allocated (seg)));
                    BYTE* old_alloc = heap_segment_allocated (seg);
                    heap_segment_allocated (seg) += limit;
                    adjust_limit_clr (old_alloc, limit, acontext, seg);
                }
                else if (a_size_fit_p (size, heap_segment_allocated (seg),
                                       heap_segment_reserved (seg)))
                {
                    size_t limit = limit_from_size (size, (heap_segment_reserved (seg) -
                                                           heap_segment_allocated (seg)));
                    if (!grow_heap_segment (seg,
                                            align_on_page (heap_segment_allocated (seg) + limit) - 
                                 heap_segment_committed(seg)))
                    {
                        assert (!"Memory exhausted during alloc");
                        leave_spin_lock (&more_space_lock);
                        return 0;
                    }
                    BYTE* old_alloc = heap_segment_allocated (seg);
                    heap_segment_allocated (seg) += limit;
                    adjust_limit_clr (old_alloc, limit, acontext, seg);
                }
                else
                {
                    if (!ran_gc)
                    {
                        ran_gc = TRUE;
                        vm_heap->GarbageCollectGeneration();
                        goto try_again;
                    }
                    else
                    {
                        assert(!"Out of memory");
                        leave_spin_lock (&more_space_lock);
                        return 0;
                    }
                }
            }
        }
    }
    return TRUE;
}

inline
CObjectHeader* gc_heap::allocate (size_t jsize, alloc_context* acontext)
{
    size_t size = Align (jsize);
    assert (size >= Align (min_obj_size));
    {
    retry:
        BYTE*  result = acontext->alloc_ptr;
        acontext->alloc_ptr+=size;
        if (acontext->alloc_ptr <= acontext->alloc_limit)
        
        {
            CObjectHeader* obj = (CObjectHeader*)result;
            return obj;
        }
        else
        {
            acontext->alloc_ptr -= size;

#pragma inline_depth(0)

            if (! allocate_more_space (acontext, size))             
                return 0;

#pragma inline_depth(20)
            goto retry;
        }
    }
}


inline
CObjectHeader* gc_heap::try_fast_alloc (size_t jsize)
{
    size_t size = Align (jsize);
    assert (size >= Align (min_obj_size));
    generation* gen = generation_of (0);
    BYTE*  result = generation_allocation_pointer (gen);
    generation_allocation_pointer (gen) += size;
    if (generation_allocation_pointer (gen) <= 
        generation_allocation_limit (gen))
    {
        return (CObjectHeader*)result;
    }
    else
    {
        generation_allocation_pointer (gen) -= size;
        return 0;
    }
}


void
gc_heap::thread_scavenge_list (BYTE* list)
{
	free_list_slot (list) = 0;

	if (last_scavenge == 0)
	{
		scavenge_list  = list;
	}
	else 
	{
		free_list_slot (last_scavenge) = list;
	}
	last_scavenge = list;
}


BYTE* gc_heap::allocate_in_older_generation (size_t size)
{
    size = Align (size);
    assert (size >= Align (min_obj_size));
	generation* gen = generation_of (max_generation);
    if (! (size_fit_p (size, generation_allocation_pointer (gen),
                       generation_allocation_limit (gen))))
    {
        while (1)
        {
            BYTE* free_list = generation_free_list (gen);
// dformat (t, 3, "considering free area %x", free_list);

			assert (free_list);
			generation_free_list (gen) = (BYTE*)free_list_slot (free_list);
			size_t flsize = size (free_list) - Align (min_free_list);

            if (size_fit_p (size, free_list, (free_list + flsize)))
            {
                dprintf (3, ("Found adequate unused area: %p, size: %d", 
                             free_list, flsize));
				generation_free_list_space (gen) -= 
					(flsize/LARGE_OBJECT_SIZE)*LARGE_OBJECT_SIZE;
				assert ((int)generation_free_list_space (gen) >=0);
                adjust_limit (free_list+Align (min_free_list), flsize, gen);
				thread_scavenge_list (free_list);
                break;
            }
        }
    }
    if (0 == size)
        return 0;
    else
    {
        BYTE*  result = generation_allocation_pointer (gen);
        generation_allocation_pointer (gen) += size;
        assert (generation_allocation_pointer (gen) <= generation_allocation_limit (gen));
        generation_allocation_size (gen) += size;
        return result;
    }
}


int 
gc_heap::generation_to_condemn (int n)
{
    int i = 0; 

    //figure out which generation ran out of allocation
    for (i = n+1; i <= max_generation+1; i++)
    {
        if (get_new_allocation (i) <= 0)
            n = min (i, max_generation);
    }

    return n;
}

LONG HandleGCException(int code)
{
    // need this to avoid infinite loop if no debugger attached as will keep calling
    // our handler as it's still on the stack.
    if (code == STATUS_BREAKPOINT)
        return EXCEPTION_CONTINUE_SEARCH;

	_ASSERTE_ALL_BUILDS(!"Exception during GC");
	// Set the debugger to break on AV and return a value of EXCEPTION_CONTINUE_EXECUTION (-1)
	// here and you will bounce back to the point of the AV.
	return EXCEPTION_EXECUTE_HANDLER;
}

//internal part of gc used by the serial and concurrent version
void gc_heap::gc1()
{
#ifdef TIME_GC
    mark_time = plan_time = reloc_time = compact_time = sweep_time = 0;
#endif //TIME_GC
    int n = condemned_generation_num;

    {
		//always do a gen 0 collection first
		vm_heap->GcCondemnedGeneration = 0;
		gc_low = generation_allocation_start (youngest_generation);
		gc_high = heap_segment_reserved (ephemeral_heap_segment);
		heap_segment_plan_allocated (ephemeral_heap_segment) = 
			generation_allocation_start (youngest_generation);

		copy_phase (0);

		heap_segment_allocated (ephemeral_heap_segment) = 
			heap_segment_plan_allocated (ephemeral_heap_segment);
	}

	if (n == max_generation)
	{
		vm_heap->GcCondemnedGeneration = condemned_generation_num;
		gc_low = lowest_address;
		gc_high = highest_address;
		mark_phase (n, FALSE);
		sweep_phase (n);
	}

	for (int gen_number = 0; gen_number <= n; gen_number++)
	{
		compute_new_dynamic_data (gen_number);
	}
	if (n < max_generation)
		compute_promoted_allocation (1 + n);
	//prepare the semi space for gen 0
	BYTE* start = allocate_semi_space (dd_desired_allocation (dynamic_data_of (0)));
	if (start)
	{
		make_unused_array (start, Align (min_obj_size));
		reset_allocation_pointers (youngest_generation, start);
		set_brick (brick_of (start), start - brick_address (brick_of (start)));
	}

	//clear card for generation 1. generation 0 is empty
	clear_card_for_addresses (
		generation_allocation_start (generation_of (1)),
		generation_allocation_start (generation_of (0)));



    adjust_ephemeral_limits();

    //decide on the next allocation quantum
    if (alloc_contexts_used >= 1)
    {
        allocation_quantum = (int)Align (min (CLR_SIZE, 
											  max (1024, get_new_allocation (0) / (2 * alloc_contexts_used))));
        dprintf (3, ("New allocation quantum: %d(0x%x)", allocation_quantum, allocation_quantum));
    }


    descr_generations();
    descr_card_table();

#ifdef TIME_GC
    fprintf (stdout, "%d,%d,%d,%d,%d,%d\n", 
             n, mark_time, plan_time, reloc_time, compact_time, sweep_time);
#endif

#if defined (VERIFY_HEAP)

    if (g_pConfig->IsHeapVerifyEnabled())
    {

        verify_heap();
    }


#endif //VERIFY_HEAP


}

int gc_heap::garbage_collect (int n
                             )
{

    //reset the number of alloc contexts
    alloc_contexts_used = 0;

    {
#ifdef TRACE_GC

        gc_count++;

        if (gc_count >= g_pConfig->GetGCtraceStart())
            trace_gc = 1;
        if (gc_count >=  g_pConfig->GetGCtraceEnd())
            trace_gc = 0;

#endif

        
        // fix all of the allocation contexts.
        CNameSpace::GcFixAllocContexts ((void*)TRUE);

    }



    fix_youngest_allocation_area();

    // check for card table growth

    if ((BYTE*)g_card_table != card_table)
        copy_brick_card_table (FALSE);

    n = generation_to_condemn (n);

    condemned_generation_num = n;

#ifdef GC_PROFILING

        // If we're tracking GCs, then we need to walk the first generation
        // before collection to track how many items of each class has been
        // allocated.
        if (CORProfilerTrackGC())
        {
            size_t heapId = 0;

            // When we're walking objects allocated by class, then we don't want to walk the large
            // object heap because then it would count things that may have been around for a while.
            gc_heap::walk_heap (&AllocByClassHelper, (void *)&heapId, 0, FALSE);

            // Notify that we've reached the end of the Gen 0 scan
            g_profControlBlock.pProfInterface->EndAllocByClass(&heapId);
        }

#endif // GC_PROFILING


    //update counters
    {
        int i; 
        for (i = 0; i <= condemned_generation_num;i++)
        {
            dd_collection_count (dynamic_data_of (i))++;
        }
    }

    dprintf(1,(" ****Garbage Collection**** %d", gc_count));

    descr_generations();
//    descr_card_table();

    dprintf(1,("generation %d condemned", condemned_generation_num));

#if defined (VERIFY_HEAP)
    if (g_pConfig->IsHeapVerifyEnabled())
    {
        verify_heap();
    }
#endif


    {
        gc1();
    }


    return condemned_generation_num;
}

#define mark_stack_empty_p() (mark_stack_base == mark_stack_tos)

#define push_mark_stack(object,add,num)                             \
{                                                                   \
    dprintf(3,("pushing mark for %p ", object));                    \
    if (mark_stack_tos < mark_stack_limit)                          \
    {                                                               \
        mark_stack_tos->first = (add);                              \
        mark_stack_tos->last= (add) + (num);                      \
        mark_stack_tos++;                                           \
    }                                                               \
    else                                                            \
    {                                                               \
        dprintf(3,("mark stack overflow for object %p ", object));  \
        min_overflow_address = min (min_overflow_address, object);  \
        max_overflow_address = max (max_overflow_address, object);  \
    }                                                               \
}

#define push_mark_stack_unchecked(add,num)                      \
{                                                               \
    mark_stack_tos->first = (add);                              \
    mark_stack_tos->last= (add) + (num);                      \
    mark_stack_tos++;                                           \
}


#define pop_mark_stack()(*(--mark_stack_tos))

#if defined ( INTERIOR_POINTERS ) || defined (_DEBUG)

heap_segment* gc_heap::find_segment (BYTE* interior)
{
    if ((interior < ephemeral_high ) && (interior >= ephemeral_low))
    {
        return ephemeral_heap_segment;
    }
    else
    {
        heap_segment* seg = generation_start_segment (generation_of (max_generation));
        do 
        {
            if ((interior >= heap_segment_mem (seg)) &&
                (interior < heap_segment_reserved (seg)))
                return seg;
        }while ((seg = heap_segment_next (seg))!= 0);
           
        return 0;
    }
}

#endif //_DEBUG || INTERIOR_POINTERS

#ifdef INTERIOR_POINTERS
// will find all heap objects (large and small)
BYTE* gc_heap::find_object (BYTE* interior, BYTE* low)
{
    if (!gen0_bricks_cleared)
    {
        gen0_bricks_cleared = TRUE;
        //initialize brick table for gen 0
        for (size_t b = brick_of (generation_allocation_start (generation_of (0)));
             b < brick_of (align_on_brick 
                           (heap_segment_allocated (ephemeral_heap_segment)));
             b++)
        {
            set_brick (b, -1);
        }
    }

    int brick_entry = brick_table [brick_of (interior)];
    if (brick_entry == -32768)
    {
        // this is a pointer to a large object
        large_object_block* bl = large_p_objects;
        while (bl)
        {
            large_object_block* next_bl = large_object_block_next (bl);
            BYTE* o = block_object (bl);
            if ((o <= interior) && (interior < o + size (o)))
            {
                return o;
            }
            bl = next_bl;
        }
        bl = large_np_objects;
        while (bl)
        {
            large_object_block* next_bl = large_object_block_next (bl);
            BYTE* o = block_object (bl);
            if ((o <= interior) && (interior < o + size (o)))
            {
                return o;
            }
            bl = next_bl;
        }
        return 0;

    }
    else if (interior >= low)
    {
        heap_segment* seg = find_segment (interior);
        if (seg)
        {
            assert (interior < heap_segment_allocated (seg));
            BYTE* o = find_first_object (interior, brick_of (interior), heap_segment_mem (seg));
            return o;
        } 
        else
            return 0;
    }
    else
        return 0;
}

#endif //INTERIOR_POINTERS



inline
BOOL gc_heap::gc_mark1 (BYTE* o)
{
    dprintf(3,("*%p*", o));

    BOOL marked = !marked (o);
    set_marked (o);
    return marked;
}

inline
BOOL gc_heap::gc_mark (BYTE* o, BYTE* low, BYTE* high)
{
    BOOL marked = FALSE;
    if ((o >= low) && (o < high))
        marked = gc_mark1 (o);
    return marked;
}

inline
BYTE* gc_heap::next_end (heap_segment* seg, BYTE* f)
{
    if (seg == ephemeral_heap_segment)
        return  f;
    else
        return  heap_segment_allocated (seg);
}

#define method_table(o) ((CObjectHeader*)(o))->GetMethodTable()

#define go_through_object(mt,o,size,parm,exp)                               \
{                                                                           \
    CGCDesc* map = CGCDesc::GetCGCDescFromMT((MethodTable*)(mt));           \
    CGCDescSeries* cur = map->GetHighestSeries();                           \
    CGCDescSeries* last = map->GetLowestSeries();                           \
                                                                            \
    if (cur >= last)                                                        \
    {                                                                       \
        do                                                                  \
        {                                                                   \
            BYTE** parm = (BYTE**)((o) + cur->GetSeriesOffset());           \
            BYTE** ppstop =                                                 \
                (BYTE**)((BYTE*)parm + cur->GetSeriesSize() + (size));      \
            while (parm < ppstop)                                           \
            {                                                               \
                {exp}                                                       \
                parm++;                                                     \
            }                                                               \
            cur--;                                                          \
                                                                            \
        } while (cur >= last);                                              \
    }                                                                       \
    else                                                                    \
    {                                                                       \
        int cnt = map->GetNumSeries();                                      \
        BYTE** parm = (BYTE**)((o) + cur->startoffset);                     \
        while ((BYTE*)parm < ((o)+(size)-plug_skew))                        \
        {                                                                   \
            for (int __i = 0; __i > cnt; __i--)                             \
            {                                                               \
                unsigned skip =  cur->val_serie[__i].skip;                  \
                unsigned nptrs = cur->val_serie[__i].nptrs;                 \
                BYTE** ppstop = parm + nptrs;                               \
                do                                                          \
                {                                                           \
                   {exp}                                                    \
                   parm++;                                                  \
                } while (parm < ppstop);                                    \
                parm = (BYTE**)((BYTE*)parm + skip);                        \
            }                                                               \
        }                                                                   \
    }                                                                       \
}

/* small objects and array of value classes have to be treated specially because sometime a 
 * cross generation pointer can exist without the corresponding card being set. This can happen if 
 * a previous card covering the object (or value class) is set. This works because the object 
 * (or embedded value class) is always scanned completely if any of the cards covering it is set. */ 

void gc_heap::verify_card_table ()
{
    int         curr_gen_number = max_generation;
    generation* gen = generation_of (curr_gen_number);
    heap_segment*    seg = generation_start_segment (gen);
    BYTE*       x = generation_allocation_start (gen);
    BYTE*       last_end = 0;
    BYTE*       last_x = 0;
    BYTE*       last_last_x = 0;
    BYTE*       f = generation_allocation_start (generation_of (0));
    BYTE*       end = next_end (seg, f);
    BYTE*       next_boundary = generation_allocation_start (generation_of (curr_gen_number - 1));


    dprintf (2,("Verifying card table from %p to %p", x, end));

    while (1)
    {
        if (x >= end)
        {
            if ((seg = heap_segment_next(seg)) != 0)
            {
                x = heap_segment_mem (seg);
                last_end = end;
                end = next_end (seg, f);
                dprintf (3,("Verifying card table from %p to %p", x, end));
                continue;
            } else
            {
                break;
            }
        }

        if ((seg == ephemeral_heap_segment) && (x >= next_boundary))
        {
            curr_gen_number--;
            assert (curr_gen_number > 0);
            next_boundary = generation_allocation_start (generation_of (curr_gen_number - 1));
        }

        size_t s = size (x);
        BOOL need_card_p = FALSE;
        if (contain_pointers (x))
        {
            size_t crd = card_of (x);
            BOOL found_card_p = card_set_p (crd);
            go_through_object 
                (method_table(x), x, s, oo, 
                 {
                     if ((*oo < ephemeral_high) && (*oo >= next_boundary))
                     {
                         need_card_p = TRUE;
                     }
                     if ((crd != card_of ((BYTE*)oo)) && !found_card_p)
                     {
                         crd = card_of ((BYTE*)oo);
                         found_card_p = card_set_p (crd);
                     }
                     if (need_card_p && !found_card_p)
                     {
                         RetailDebugBreak();
                     }
                 }
                    );
            if (need_card_p && !found_card_p)
            {
                printf ("Card not set, x = [%p:%p, %p:%p[",
                        card_of (x), x,
                        card_of (x+Align(s)), x+Align(s));
                RetailDebugBreak();
            }

        }

        last_last_x = last_x;
        last_x = x;
        x = x + Align (s);
    }

    // go through large object
    large_object_block* bl = large_p_objects;
    while (bl)
    {
        large_object_block* next_bl = bl->next;
        BYTE* o = block_object (bl);
        MethodTable* mt = method_table (o);
        {                                                                           
            CGCDesc* map = CGCDesc::GetCGCDescFromMT((MethodTable*)(mt));
            CGCDescSeries* cur = map->GetHighestSeries();
            CGCDescSeries* last = map->GetLowestSeries();

            if (cur >= last)
            {
                do
                {
                    BYTE** oo = (BYTE**)((o) + cur->GetSeriesOffset());
                    BYTE** ppstop =
                        (BYTE**)((BYTE*)oo + cur->GetSeriesSize() + (size (o)));
                    while (oo < ppstop)
                    {
                        if ((*oo < ephemeral_high) && (*oo >= ephemeral_low)) 
                        { 
                            if (!card_set_p (card_of ((BYTE*)oo))) 
                            { 
                                RetailDebugBreak(); 
                            } 
                        }
                        oo++;
                    }
                    cur--;

                } while (cur >= last);
            }
            else
            {
                BOOL need_card_p = FALSE;
                size_t crd = card_of (o);
                BOOL found_card_p = card_set_p (crd);
                int cnt = map->GetNumSeries();
                BYTE** oo = (BYTE**)((o) + cur->startoffset);
                while ((BYTE*)oo < ((o)+(size (o))-plug_skew))
                {
                    for (int __i = 0; __i > cnt; __i--)
                    {
                        unsigned skip =  cur->val_serie[__i].skip;
                        unsigned nptrs = cur->val_serie[__i].nptrs;
                        BYTE** ppstop = oo + nptrs;
                        do
                        {
                            if ((*oo < ephemeral_high) && (*oo >= next_boundary))
                            {
                                need_card_p = TRUE;
                            }
                            if ((crd != card_of ((BYTE*)oo)) && !found_card_p)
                            {
                                crd = card_of ((BYTE*)oo);
                                found_card_p = card_set_p (crd);
                            }
                            if (need_card_p && !found_card_p)
                            {
                                RetailDebugBreak();
                            }
                            oo++;
                        } while (oo < ppstop);
                        oo = (BYTE**)((BYTE*)oo + skip);
                    }
                }
            }                                                                       
        }
        bl = next_bl;
    }
}


void gc_heap::mark_object_internal (BYTE* oo)
{
    BYTE** mark_stack_tos = (BYTE**)mark_stack_array;
    BYTE** mark_stack_limit = (BYTE**)&mark_stack_array[mark_stack_array_length];
    BYTE** mark_stack_base = mark_stack_tos;
    while (1)
    {
        size_t s = size (oo);       
        if (mark_stack_tos + (s) /sizeof (BYTE*) < mark_stack_limit)
        {
            dprintf(3,("pushing mark for %p ", oo));

            go_through_object (method_table(oo), oo, s, ppslot, 
                               {
                                   BYTE* o = *ppslot;
                                   if (gc_mark (o, gc_low, gc_high))
                                   {
                                       if (contain_pointers (o))
                                       {
                                           *(mark_stack_tos++) = o;

                                       }
                                   }
                               }
                              );

        }
        else
        {
            dprintf(3,("mark stack overflow for object %p ", oo));
            min_overflow_address = min (min_overflow_address, oo);
            max_overflow_address = max (max_overflow_address, oo);
        }               
        if (!(mark_stack_empty_p()))
        {
            oo = *(--mark_stack_tos);
        }
        else
            break;
    }
}

//this method assumes that *po is in the [low. high[ range
void 
gc_heap::mark_object_simple (BYTE** po)
{
    BYTE* o = *po;
    {
        if (gc_mark1 (o))
        {
            if (contain_pointers (o))
            {
                size_t s = size (o);
                go_through_object (method_table(o), o, s, poo,
                                   {
                                       BYTE* oo = *poo;
                                       if (gc_mark (oo, gc_low, gc_high))
                                       {
                                           if (contain_pointers (oo))
                                               mark_object_internal (oo);
                                       }
                                   }
                    );

            }
        }
    }
}

//this method assumes that *po is in the [low. high[ range
void 
gc_heap::copy_object_simple (BYTE** po)
{
    BYTE* o = *po;
	if (gc_mark1 (o))
	{
		if (!pinned (o))
		{
			//allocate space for it
			BYTE* no = allocate_in_older_generation (size (o));
			dprintf (3, ("Copying %p to %p", o, no));
			//copy it
			memcopy (no - plug_skew, o - plug_skew, Align(size(o)));

			//forward 
			(header(o))->SetRelocation(no);
			*po = no;
		}
		else
		{
			dprintf (3, ("%p Pinned", o));
		}
	}
	else
	{
		*po = header(o)->GetRelocated ();
		dprintf (3, ("%p Already copied to %p", o, *po));
	}
}


//This method does not side effect the argument's data. 
void 
gc_heap::copy_object_simple_const (BYTE** po)
{
	BYTE* o = *po;
	copy_object_simple (&o);
}

void 
gc_heap::get_copied_object (BYTE** po)
{
	assert (marked (*po));
	BYTE* o = *po;
	*po = header(o)->GetRelocated ();
	dprintf (3, ("%p copied to %p", o, *po));
}



void 
gc_heap::scavenge_object_simple (BYTE* o)
{
	assert (marked (o)|| (*((BYTE**)o) == (BYTE *) g_pFreeObjectMethodTable));
	clear_marked_pinned (o);
	if (contain_pointers (o))
	{
		size_t s = size (o);
		dprintf (3, ("Scavenging %p", o));
		go_through_object (method_table(o), o, s, poo,
						   {
							   BYTE* oo = *poo;
							   if ((oo>= gc_low) && (oo < gc_high))
							   {
								   copy_object_simple (poo);
							   }
						   }
			);

	}

}


inline
BYTE* gc_heap::mark_object (BYTE* o)
{
	if ((o >= gc_low) && (o < gc_high))
		mark_object_simple (&o);
    return o;
}


void gc_heap::fix_card_table ()
{

}

void gc_heap::mark_through_object (BYTE* oo)
{
    if (contain_pointers (oo))
        {
            dprintf(3,( "Marking through %p", oo));
            size_t s = size (oo);
            go_through_object (method_table(oo), oo, s, po,
                               BYTE* o = *po;
                               mark_object (o);
                              );
        }
}

//returns TRUE is an overflow happened.
BOOL gc_heap::process_mark_overflow(int condemned_gen_number)
{
    BOOL  full_p = (condemned_gen_number == max_generation);
    BOOL  overflow_p = FALSE;
recheck:
    if ((! ((max_overflow_address == 0)) ||
         ! ((min_overflow_address == (BYTE*)(ptrdiff_t)-1))))
    {
        overflow_p = TRUE;
        // Try to grow the array.
        size_t new_size =
            max (100, 2*mark_stack_array_length);
        mark* tmp = new (mark [new_size]);
        if (tmp)
        {
            delete mark_stack_array;
            mark_stack_array = tmp;
            mark_stack_array_length = new_size;
        }

        BYTE*  min_add = min_overflow_address;
        BYTE*  max_add = max_overflow_address;
        max_overflow_address = 0;
        min_overflow_address = (BYTE*)(ptrdiff_t)-1;


        dprintf(3,("Processing Mark overflow [%p %p]", min_add, max_add));
        {
            {
                generation*   gen = generation_of (condemned_gen_number);

                heap_segment* seg = generation_start_segment (gen);
                BYTE*  o = max (generation_allocation_start (gen), min_add);
                while (1)
                {
                    BYTE*  end = heap_segment_allocated (seg);
                    while ((o < end) && (o <= max_add))
                    {
                        assert ((min_add <= o) && (max_add >= o));
                        dprintf (3, ("considering %p", o));
                        if (marked (o))
                        {
                            mark_through_object (o);
                        }
                        o = o + Align (size (o));
                    }
                    if (( seg = heap_segment_next (seg)) == 0)
                    {
                        break;
                    } else
                    {
                        o = max (heap_segment_mem (seg), min_add);
                        continue;
                    }
                }
            }
            if (full_p)
            {
                // If full_gc, look in large object list as well
                large_object_block* bl = large_p_objects;
                while (bl)
                {
                    BYTE* o = block_object (bl);
                    if ((min_add <= o) && (max_add >= o) && marked (o))
                    {
                        mark_through_object (o);
                    }
                    bl = large_object_block_next (bl);
                }
            }

        }
        goto recheck;
    }
    return overflow_p;
}

void gc_heap::mark_phase (int condemned_gen_number, BOOL mark_only_p)
{

    ScanContext sc;
    sc.thread_number = heap_number;
    sc.promotion = TRUE;
    sc.concurrent = FALSE;

    dprintf(2,("---- Mark Phase condemning %d ----", condemned_gen_number));
    BOOL  full_p = (condemned_gen_number == max_generation);

#ifdef TIME_GC
    unsigned start;
    unsigned finish;
    start = GetCycleCount32();
#endif

    reset_mark_stack();



	//%type%  category = quote (mark);
	generation*   gen = generation_of (condemned_gen_number);


	dprintf(3,("Marking Roots"));
	CNameSpace::GcScanRoots(GCHeap::Promote, 
							condemned_gen_number, max_generation, 
							&sc, 0);



	dprintf(3,("Marking handle table"));
	CNameSpace::GcScanHandles(GCHeap::Promote, 
							  condemned_gen_number, max_generation, 
							  &sc);
	dprintf(3,("Marking finalization data"));
	finalize_queue->GcScanRoots(GCHeap::Promote, heap_number, 0);


    process_mark_overflow(condemned_gen_number);


	// scan for deleted short weak pointers
	CNameSpace::GcShortWeakPtrScan(condemned_gen_number, max_generation,&sc);

	//Handle finalization.

	finalize_queue->ScanForFinalization (condemned_gen_number, 1, mark_only_p, __this);


    // make sure everything is promoted
    process_mark_overflow (condemned_gen_number);


	// scan for deleted weak pointers
	CNameSpace::GcWeakPtrScan (condemned_gen_number, max_generation, &sc);

	sweep_large_objects();


#ifdef TIME_GC
	finish = GetCycleCount32();
	mark_time = finish - start;
#endif //TIME_GC

    dprintf(2,("---- End of mark phase ----"));
}

void gc_heap::copy_phase (int condemned_gen_number)

{

    ScanContext sc;
    sc.thread_number = heap_number;
    sc.promotion = TRUE;
    sc.concurrent = FALSE;

    dprintf(2,("---- Copy Phase condemning %d ----", condemned_gen_number));

#ifdef TIME_GC
    unsigned start;
    unsigned finish;
    start = GetCycleCount32();
#endif

	scavenge_list = 0;
	last_scavenge = 0;
	
	pinning       = FALSE;

	generation*   gen = generation_of (condemned_gen_number);

	dprintf(3,("Copying cross generation pointers"));
	copy_through_cards_for_segments (copy_object_simple_const);

	dprintf(3,("Copying cross generation pointers for large objects"));
	copy_through_cards_for_large_objects (copy_object_simple_const);


	dprintf(3,("Copying Roots"));
	CNameSpace::GcScanRoots(GCHeap::Promote, 
							condemned_gen_number, max_generation, 
							&sc, 0);


	dprintf(3,("Copying handle table"));
	CNameSpace::GcScanHandles(GCHeap::Promote, 
							  condemned_gen_number, max_generation, 
							  &sc);
	dprintf(3,("Copying finalization data"));
	finalize_queue->GcScanRoots(GCHeap::Promote, heap_number, 0);


	if (pinning)
		scavenge_pinned_objects (FALSE);

	scavenge_context scan_c;

	scavenge_phase(&scan_c);

	// scan for deleted short weak pointers
	CNameSpace::GcShortWeakPtrScan(condemned_gen_number, max_generation,
								   &sc);

	//Handle finalization.

	finalize_queue->ScanForFinalization (condemned_gen_number, 1, FALSE, __this);


	scavenge_phase(&scan_c);

	// scan for deleted weak pointers
	CNameSpace::GcWeakPtrScan (condemned_gen_number, max_generation, &sc);


	scavenge_phase(&scan_c);

	//fix the scavenge list so we don't hide our objects under the free
	//array

	if (scavenge_list)
	{
		header(scavenge_list)->SetFree (min_free_list);
	}

	fix_older_allocation_area (generation_of (max_generation));

	sc.promotion = FALSE;

	dprintf(3,("Relocating cross generation pointers"));
	copy_through_cards_for_segments (get_copied_object);

	dprintf(3,("Relocating cross generation pointers for large objects"));
	copy_through_cards_for_large_objects (get_copied_object);

	dprintf(3,("Relocating roots"));
	CNameSpace::GcScanRoots(GCHeap::Relocate,
                            condemned_gen_number, max_generation, &sc);


	dprintf(3,("Relocating handle table"));
	CNameSpace::GcScanHandles(GCHeap::Relocate,
							  condemned_gen_number, max_generation, &sc);

	dprintf(3,("Relocating finalization data"));
	finalize_queue->RelocateFinalizationData (condemned_gen_number,
												   __this);


	if (pinning)
		scavenge_pinned_objects (TRUE);

	finalize_queue->UpdatePromotedGenerations (condemned_gen_number, TRUE);


	CNameSpace::GcPromotionsGranted (condemned_gen_number, 
									 max_generation, &sc);

#ifdef TIME_GC
	finish = GetCycleCount32();
	mark_time = finish - start;
#endif //TIME_GC

	finalize_queue->UpdatePromotedGenerations (condemned_gen_number, TRUE);    dprintf(2,("---- End of Copy phase ----"));
}

inline
void gc_heap::pin_object (BYTE* o, BYTE* low, BYTE* high)
{
    dprintf (3, ("Pinning %p", o));
    if ((o >= low) && (o < high))
    {
		pinning = TRUE;
        dprintf(3,("^%p^", o));
		
		set_pinned (o);
		if (marked (o))
		{
			//undo the copy
			BYTE* no =header(o)->GetRelocated();
			header(o)->SetRelocation(header(no)->GetRelocated());
		}
    }
}


void gc_heap::reset_mark_stack ()
{
    mark_stack_tos = 0;
    mark_stack_bos = 0;
    max_overflow_address = 0;
    min_overflow_address = (BYTE*)(ptrdiff_t)-1;
}


void gc_heap::sweep_phase (int condemned_gen_number)
{
    // %type%  category = quote (plan);
#ifdef TIME_GC
    unsigned start;
    unsigned finish;
    start = GetCycleCount32();
#endif

    dprintf (2,("---- Sweep Phase ---- Condemned generation %d",
                condemned_gen_number));

    generation*  condemned_gen = generation_of (condemned_gen_number);

	//reset the free list
	generation_free_list (condemned_gen) = 0;
	generation_free_list_space (condemned_gen) = 0;

	heap_segment*  seg = generation_start_segment (condemned_gen);
    BYTE*  end = heap_segment_allocated (seg);
    BYTE*  first_condemned_address = generation_allocation_start (condemned_gen);
    BYTE*  x = first_condemned_address;

    assert (!marked (x));
    BYTE*  plug_end = x;
 
    while (1)
    {
        if (x >= end)
        {
            assert (x == end);
			//adjust the end of the segment. 
			heap_segment_allocated (seg) = plug_end;
            if (heap_segment_next (seg))
            {
                seg = heap_segment_next (seg);
                end = heap_segment_allocated (seg);
                plug_end = x = heap_segment_mem (seg);
                dprintf(3,( " From %p to %p", x, end));
                continue;
            }
            else
            {
                break;
            }
        }
        if (marked (x))
        {
            BYTE*  plug_start = x;

			thread_gap (plug_end, plug_start - plug_end);

            dprintf(3,( "Gap size: %d before plug [%p,",
                        plug_start - plug_end, plug_start));
            {
                BYTE* xl = x;
                while (marked (xl) && (xl < end))
                {
                    assert (xl < end);
                    if (pinned(xl))
                    {
                        clear_pinned (xl);
                    }

                    clear_marked_pinned (xl);

                    dprintf(4, ("+%p+", xl));
                    assert ((size (xl) > 0));

                    xl = xl + Align (size (xl));
                }
                assert (xl <= end);
                x = xl;
            }
            dprintf(3,( "%p[", x));
            plug_end = x;
		}
        else
        {
            {
                BYTE* xl = x;
                while ((xl < end) && !marked (xl))
                {
                    dprintf (4, ("-%p-", xl));
                    assert ((size (xl) > 0));
                    xl = xl + Align (size (xl));
                }
                assert (xl <= end);
                x = xl;
            }
        }
    }

    dprintf(2,("---- End of sweep phase ----"));

}

void gc_heap::scavenge_pinned_objects (BOOL free_list_p)
{

    generation*  condemned_gen = youngest_generation;

	heap_segment*  seg = generation_start_segment (condemned_gen);
    BYTE*  end = heap_segment_allocated (seg);
    BYTE*  first_condemned_address = generation_allocation_start (condemned_gen);
    BYTE*  x = first_condemned_address;

    assert (!marked (x));
    BYTE*  plug_end = x;
 
    while (1)
    {
        if (x >= end)
        {
            assert (x == end);

			if (free_list_p)
			{
				//adjust the end of the segment. 
				heap_segment_allocated (seg) = plug_end;
				//adjust the start of new allocation
				heap_segment_plan_allocated (seg) = plug_end;
			}

            if (heap_segment_next (seg))
            {
                seg = heap_segment_next (seg);
                end = heap_segment_allocated (seg);
                plug_end = x = heap_segment_mem (seg);
                dprintf(3,( " From %p to %p", x, end));
                continue;
            }
            else
            {
                break;
            }
        }
        if (pinned (x))
        {
            BYTE*  plug_start = x;

			if (free_list_p)
			{

				thread_gap (plug_end, plug_start - plug_end);

				dprintf(3,( "Gap size: %d before plug [%p,",
							plug_start - plug_end, plug_start));
			}
            {
                BYTE* xl = x;
                while (pinned (xl) && (xl < end))
                {
                    assert (xl < end);
					if (free_list_p)
					{

						if (pinned(xl))
						{
							clear_pinned (xl);
						}

						clear_marked_pinned (xl);
					}


                    dprintf(4, ("#%p#", xl));
                    assert ((size (xl) > 0));

                    xl = xl + Align (size (xl));
                }
                assert (xl <= end);
                x = xl;
            }
            dprintf(3,( "%p[", x));
            plug_end = x;
		}
        else
        {
            {
                BYTE* xl = x;
                while ((xl < end) && !pinned (xl))
                {
                    dprintf (4, ("-%p-", xl));
                    assert ((size (xl) > 0));
                    xl = xl + Align (size (xl));
                }
                assert (xl <= end);
                x = xl;
            }
        }
    }

    dprintf(2,("---- End of sweep phase ----"));

}

void gc_heap::scavenge_phase (scavenge_context* sc)
{
#ifdef TIME_GC
    unsigned start;
    unsigned finish;
    start = GetCycleCount32();
#endif

    dprintf (2,("---- Scavenge Phase ---- Condemned generation %d",
                0));

    generation*  gen = generation_of (max_generation);
	if (scavenge_list)
	{

		BYTE* o = sc->first_object;
		BYTE* limit = sc->limit;
		if (!sc->limit)
		{
			o = scavenge_list + Align (min_free_list);
			limit = scavenge_list + Align (size (scavenge_list));
		}
		while (scavenge_list)
		{
			dprintf (3, ("Scavenging free list %p", scavenge_list));
			while ((o != generation_allocation_pointer (gen)) && (o < limit))
			{
				scavenge_object_simple (o);
				o = o + Align (size (o));
			}
			if (o == generation_allocation_pointer (gen))
			{
				sc->first_object = o;
				sc->limit = limit;
				break;
			}

			BYTE* next_scavenge_list = free_list_slot (scavenge_list);

			//convert the free list into a free object
			//fix the scavenge list so we don't hide our objects under the free
			//array
			(header(scavenge_list))->SetFree (min_free_list);

			scavenge_list = next_scavenge_list;
			o = scavenge_list + Align (min_free_list);
			limit = scavenge_list + Align (size (scavenge_list));
		}
		assert (scavenge_list && "Run out of free list before the end of scavenging");
	}
}


BYTE* gc_heap::generation_limit (int gen_number)
{
    if ((gen_number <= 1))
        return heap_segment_reserved (ephemeral_heap_segment);
    else
        return generation_allocation_start (generation_of ((gen_number - 2)));
}




void gc_heap::thread_gap (BYTE* gap_start, size_t size)
{
	generation* gen = generation_of (max_generation);
    if ((size > 0))
    {
        // The beginning of a segment gap is not aligned
        assert (size >= Align (min_obj_size));
        make_unused_array (gap_start, size);
		//set bricks 
		size_t br = brick_of (gap_start);
		ptrdiff_t begoffset = brick_table [brick_of (gap_start)];
		if (begoffset < (gap_start + size) - brick_address (br))
			set_brick (br, gap_start - brick_address (br));
		br++;
		short offset = 0;
		while (br <= brick_of (gap_start+size-1))
		{
			set_brick (br, --offset);
			br++;
		}

        dprintf(3,("Free List: [%p, %p[", gap_start, gap_start+size));
        if ((size >= min_free_list))
        {
            free_list_slot (gap_start) = 0;
			//The useable size is lower because we keep the 
			//header of the free list until scavenging
            generation_free_list_space (gen) +=
				((size-Align(min_free_list))/LARGE_OBJECT_SIZE)*LARGE_OBJECT_SIZE;

			assert ((int)generation_free_list_space (gen) >=0);
            BYTE* first = generation_free_list (gen);

            assert (gap_start != first);
            if (first == 0)
            {
                generation_free_list (gen) = gap_start;
            }
            //the following is necessary because the last free element
            //may have been truncated, and last_gap isn't updated. 
            else if (free_list_slot (first) == 0)
            {
                free_list_slot (first) = gap_start;
            }
            else
            {
                assert (gap_start != generation_last_gap (gen));
                assert (free_list_slot(generation_last_gap (gen)) == 0);
                free_list_slot (generation_last_gap (gen)) = gap_start;
            }
            generation_last_gap (gen) = gap_start;
        }
    }
}


void gc_heap::make_unused_array (BYTE* x, size_t size)
{
    assert (size >= Align (min_obj_size));
    (header(x))->SetFree(size);
    clear_card_for_addresses (x, x + Align(size));
}


//extract the low bits [0,low[ of a DWORD
#define lowbits(wrd, bits) ((wrd) & ((1 << (bits))-1))
//extract the high bits [high, 32] of a DWORD
#define highbits(wrd, bits) ((wrd) & ~((1 << (bits))-1))


//Clear the cards [start_card, end_card[

void gc_heap::clear_cards (size_t start_card, size_t end_card)
{
    if (start_card < end_card)
    {
		for (size_t i = start_card; i < end_card; i++)
                card_table [i] = 0;
#ifdef VERYSLOWDEBUG
        size_t  card = start_card;
        while (card < end_card)
        {
            assert (! (card_set_p (card)));
            card++;
        }
#endif
        dprintf (3,("Cleared cards [%p:%p, %p:%p[",
                  start_card, card_address (start_card),
                  end_card, card_address (end_card)));
    }
}


void gc_heap::clear_card_for_addresses (BYTE* start_address, BYTE* end_address)
{
    size_t   start_card = card_of (align_on_card (start_address));
    size_t   end_card = card_of (align_lower_card (end_address));
    clear_cards (start_card, end_card);
}

void gc_heap::fix_brick_to_highest (BYTE* o, BYTE* next_o)
{
    size_t new_current_brick = brick_of (o);
    dprintf(3,(" fixing brick %p to point to object %p",
               new_current_brick, o));
    set_brick (new_current_brick,
               (o - brick_address (new_current_brick)));
    size_t b = 1 + new_current_brick;
    size_t limit = brick_of (next_o);
    while (b < limit)
    {
        set_brick (b,(new_current_brick - b));
        b++;
    }

}


BYTE* gc_heap::find_first_object (BYTE* start,  size_t brick, BYTE* min_address)
{

    assert (brick == brick_of (start));
    ptrdiff_t  min_brick = (ptrdiff_t)brick_of (min_address);
    ptrdiff_t  prev_brick = (ptrdiff_t)brick - 1;
    int         brick_entry = 0;
    while (1)
    {
        if (prev_brick < min_brick)
        {
            break;
        }
        if ((brick_entry =  brick_table [ prev_brick ]) >= 0)
        {
            break;
        }
        assert (! ((brick_entry == -32768)));
        prev_brick = (brick_entry + prev_brick);

    }

    BYTE* o = ((prev_brick < min_brick) ? min_address :
                      brick_address (prev_brick) + brick_entry);
    assert (o <= start);
    assert (size (o) >= Align (min_obj_size));
    BYTE*  next_o = o + Align (size (o));
    size_t curr_cl = (size_t)next_o / brick_size;
    size_t min_cl = (size_t)min_address / brick_size;

    dprintf (3,( "Looking for intersection with %p from %p", start, o));
#ifdef TRACE_GC
    unsigned int n_o = 1;
#endif

    BYTE* next_b = min (align_lower_brick (next_o) + brick_size, start+1);

    while (next_o <= start)
    {
        do 
        {
#ifdef TRACE_GC
            n_o++;
#endif
            o = next_o;
            assert (size (o) >= Align (min_obj_size));
            next_o = o + Align (size (o));
        }while (next_o < next_b);

        if (((size_t)next_o / brick_size) != curr_cl)
        {
            if (curr_cl >= min_cl)
            {
                fix_brick_to_highest (o, next_o);
            }
            curr_cl = (size_t) next_o / brick_size;
        }
        next_b = min (align_lower_brick (next_o) + brick_size, start+1);
    }

    dprintf (3, ("Looked at %d objects", n_o));
    size_t bo = brick_of (o);
    if (bo < brick)
    {
        set_brick (bo, (o - brick_address(bo)));
        size_t b = 1 + bo;
        size_t limit = brick - 1;
        int x = -1;
        while (b < brick)
        {
            set_brick (b,x--);
            b++;
        }
    }

    return o;
}




void find_card (BYTE* card_table, size_t& card, 
                size_t card_word_end, size_t& end_card)
{
    size_t last_card;
    // Find the first card which is set

    last_card = card;
	while ((last_card < card_word_end) && !(card_table[last_card]))
	{
            ++last_card;
	}

    card = last_card;

	while ((last_card<card_word_end) && (card_table[last_card]))
		++last_card;

    end_card = last_card;
}


    //because of heap expansion, computing end is complicated. 
BYTE* compute_next_end (heap_segment* seg, BYTE* low)
{
    if ((low >=  heap_segment_mem (seg)) && 
        (low < heap_segment_allocated (seg)))
        return low;
    else
        return heap_segment_allocated (seg);
}


inline void 
gc_heap::copy_through_cards_helper (BYTE** poo, unsigned int& n_gen, 
                                    card_fn fn)
{
    if ((gc_low <= *poo) && (gc_high > *poo))
    {
        n_gen++;
        call_fn(fn) (poo);
    }
}

void gc_heap::copy_through_cards_for_segments (card_fn fn)
{
    size_t  		card;
    BYTE* 			low = gc_low;
    BYTE* 			high = gc_high;
    size_t  		end_card          = 0;
    generation*   	oldest_gen        = generation_of (max_generation);
    int           	curr_gen_number   = max_generation;
    heap_segment* 	seg               = generation_start_segment (oldest_gen);
    BYTE*         	beg               = generation_allocation_start (oldest_gen);
    BYTE*         	end               = compute_next_end (seg, low);
    size_t        	last_brick        = ~1u;
    BYTE*         	last_object       = beg;

    unsigned int  	cg_pointers_found = 0;

    size_t  card_word_end = (card_of (align_on_card (end)));

    dprintf(3,( "scanning from %p to %p", beg, end));
    card        = card_of (beg);
    while (1)
    {
        if (card >= end_card)
            find_card (card_table, card, card_word_end, end_card);
        if ((last_object >= end) || (card_address (card) >= end))
        {
            if ((seg = heap_segment_next (seg)) != 0)
            {
                beg = heap_segment_mem (seg);
                end = compute_next_end (seg, low);
                card_word_end = card_of (align_on_card (end));
                card = card_of (beg);
                last_object = beg;
                last_brick = ~1u;
                end_card = 0;
                continue;
            }
            else
            {
                break;
            }
        }
        assert (card_set_p (card));
        {
            BYTE*   start_address = max (beg, card_address (card));
            size_t  brick         = brick_of (start_address);
            BYTE*   o;

            // start from the last object if in the same brick or
            // if the last_object already intersects the card
            if ((brick == last_brick) || (start_address <= last_object))
            {
                o = last_object;
            }
            else if (brick_of (beg) == brick)
                    o = beg;
            else
            {
                o = find_first_object (start_address, brick, last_object);
                //Never visit an object twice.
                assert (o >= last_object);
            }

            BYTE* limit             = min (end, card_address (end_card));
            dprintf(3,("Considering card %p start object: %p, %p[ ",
                       card, o, limit));
            while (o < limit)
            {
                assert (size (o) >= Align (min_obj_size));
                size_t s = size (o);

                BYTE* next_o =  o + Align (s);

                if (card_of (o) > card)
                {
                    if (cg_pointers_found == 0)
                    {
                        dprintf(3,(" Clearing cards [%p, %p[ ", card_address(card), o));
                        clear_cards (card, card_of(o));
                    }
                    cg_pointers_found = 0;
                    card = card_of (o);
                }
                if ((next_o >= start_address) && contain_pointers (o))
                {
                    dprintf(3,("Going through %p", o));
                    

                    go_through_object (method_table(o), o, s, poo,
                       {
                           copy_through_cards_helper (poo, cg_pointers_found, fn);

                       }
                        );
                    dprintf (3, ("Found %d cg pointers", cg_pointers_found));
                }
                o = next_o;
            }
            if (cg_pointers_found == 0)
            {
                dprintf(3,(" Clearing cards [%p, %p[ ", o, limit));
                clear_cards (card, card_of (limit));
            }

            cg_pointers_found = 0;

            card = card_of (o);
            last_object = o;
            last_brick = brick;
        }
    }

}

BYTE* gc_heap::expand_heap (heap_segment* new_heap_segment, 
								  size_t size)
{
    BYTE*  start_address = generation_limit (max_generation);
    size_t   current_brick = brick_of (start_address);
    BYTE*  end_address = heap_segment_allocated (ephemeral_heap_segment);
    size_t  end_brick = brick_of (end_address-1);
    BYTE*  last_plug = 0;

    dprintf(2,("---- Heap Expansion ----"));

    heap_segment* new_seg = new_heap_segment;

    if (!new_seg)
        return 0;

    //copy the card and brick tables
    if ((BYTE*)g_card_table!= card_table)
        copy_brick_card_table (TRUE);

    //compute the size of the new ephemeral heap segment. 
    size_t eph_size = size;

    // commit the new ephemeral segment all at once. 
    if (grow_heap_segment (new_seg, align_on_page (eph_size)) == 0)
        return 0;

    //initialize the first brick
    size_t first_brick = brick_of (heap_segment_mem (new_seg));
    set_brick (first_brick,
               heap_segment_mem (new_seg) - brick_address (first_brick));

    heap_segment* old_seg = ephemeral_heap_segment;
    ephemeral_heap_segment = new_seg;

	heap_segment_next (old_seg) = new_seg;

    dprintf(2,("---- End of Heap Expansion ----"));
    return heap_segment_mem (new_seg);
}



void gc_heap::init_dynamic_data ()
{
  
  // get the registry setting for generation 0 size

  dynamic_data* dd = dynamic_data_of (0);
  dd->current_size = 0;
  dd->promoted_size = 0;
  dd->collection_count = 0;
  dd->desired_allocation =  800*1024;
  dd->new_allocation = dd->desired_allocation;


  dd =  dynamic_data_of (1);
  dd->current_size = 0;
  dd->promoted_size = 0;
  dd->collection_count = 0;
  dd->desired_allocation = 1024*1024;
  dd->new_allocation = dd->desired_allocation;


  //dynamic data for large objects
  dd =  dynamic_data_of (max_generation+1);
  dd->current_size = 0;
  dd->promoted_size = 0;
  dd->collection_count = 0;
  dd->desired_allocation = 1024*1024;
  dd->new_allocation = dd->desired_allocation;
}

size_t gc_heap::generation_size (int gen_number)
{
	if (gen_number > max_generation)
		return 0;

    if (0 == gen_number)
        return max((heap_segment_allocated (ephemeral_heap_segment) -
                    generation_allocation_start (generation_of (gen_number))),
                   (int)Align (min_obj_size));
    else
    {
        generation* gen = generation_of (gen_number);
        if (generation_start_segment (gen) == ephemeral_heap_segment)
            return (generation_allocation_start (generation_of (gen_number - 1)) -
                    generation_allocation_start (generation_of (gen_number)));
        else
        {
            assert (gen_number == max_generation);
            size_t gensize = (generation_allocation_start (generation_of (gen_number - 1)) - 
                              heap_segment_mem (ephemeral_heap_segment));
            heap_segment* seg = generation_start_segment (gen);
            while (seg != ephemeral_heap_segment)
            {
                gensize += heap_segment_allocated (seg) -
                           heap_segment_mem (seg);
                seg = heap_segment_next (seg);
            }
            return gensize;
        }
    }

}


size_t  gc_heap::compute_promoted_allocation (int gen_number)
{
  dynamic_data* dd = dynamic_data_of (gen_number);
  size_t  in = generation_allocation_size (generation_of (gen_number));
  dd_new_allocation (dd) -= in;
  generation_allocation_size (generation_of (gen_number)) = 0;
  return in;
}


void gc_heap::compute_new_dynamic_data (int gen_number)
{
    dynamic_data* dd = dynamic_data_of (gen_number);
    dd_new_allocation (dd) = dd_desired_allocation (dd);
    if (gen_number == max_generation)
    {
		//do the large object as well
		dynamic_data* dd = dynamic_data_of (max_generation+1);
        dd_new_allocation (dd) = dd_desired_allocation (dd);
    }
	else
	{
		dd_promoted_size (dd) = generation_allocation_size (generation_of (gen_number+1));
	}
}



size_t gc_heap::new_allocation_limit (size_t size, size_t free_size)
{
    dynamic_data* dd        = dynamic_data_of (0);
    ptrdiff_t           new_alloc = dd_new_allocation (dd);
    assert (new_alloc == (ptrdiff_t)Align (new_alloc));
    size_t        limit     = min (max (new_alloc, (int)size), (int)free_size);
    assert (limit == Align (limit));
    dd_new_allocation (dd) = (new_alloc - limit );
    return limit;
}

// return the total sizes of the generation gen and younger

size_t gc_heap::generation_sizes (generation* gen)
{
    size_t result = 0;
    if (generation_start_segment (gen ) == ephemeral_heap_segment)
        result = (heap_segment_allocated (ephemeral_heap_segment) -
                  generation_allocation_start (gen));
    else
    {
        heap_segment* seg = generation_start_segment (gen);
        while (seg)
        {
            result += (heap_segment_allocated (seg) -
                       heap_segment_mem (seg));
            seg = heap_segment_next (seg);
        }
    }
    return result;
}

inline
large_object_block* get_object_block (large_object_block* bl)
{
    return bl;
}

inline
void gc_heap::RemoveBlock (large_object_block* item, BOOL pointerp)
{
    *(item->prev) = get_object_block (item->next);
    if (get_object_block (item->next))
        get_object_block (item->next)->prev = item->prev;
    else if (pointerp)
        last_large_p_object = item->prev;

}

inline
void gc_heap::InsertBlock (large_object_block** after, large_object_block* item, BOOL pointerp)
{
    ptrdiff_t lowest = (ptrdiff_t)(*after) & 1;
    item->next = get_object_block (*after);
    item->prev = after;
    if (get_object_block (*after))
        (get_object_block (*after))->prev = &(item->next);
    else if (pointerp)
        last_large_p_object = &(item->next);
    //preserve the lowest bit used as a marker during concurrentgc
    *((ptrdiff_t*)after) = (ptrdiff_t)item | lowest;
}
#define block_head(blnext)((large_object_block*)((BYTE*)(blnext)-\
                           &((large_object_block*)0)->next))

//Large object support



// sorted insertion. Blocks are likely to be allocated
// in increasing addresses so sort from the end.

void gc_heap::insert_large_pblock (large_object_block* bl)
{
    large_object_block** i = last_large_p_object;
    while (((size_t)bl < (size_t)i) &&
           (i != &large_p_objects))
    {
        i = block_head(i)->prev;


    }
    InsertBlock (i, bl, TRUE);
}



CObjectHeader* gc_heap::allocate_large_object (size_t size, BOOL pointerp, 
                                               alloc_context* acontext)
{
    //gmheap cannot allocate more than 2GB
    if (size >= 0x80000000 - 8 - AlignQword (sizeof (large_object_block)))
        return 0;

    size_t memsize = AlignQword (size) + AlignQword (sizeof (large_object_block));

    ptrdiff_t allocsize = dd_new_allocation (dynamic_data_of (max_generation+1));
    if (allocsize < 0)
    {
        vm_heap->GarbageCollectGeneration(max_generation);
    }


    void* mem = gheap->Alloc ((unsigned)memsize);

    if (!mem)
    {

        return 0;
    }

    assert (mem < g_highest_address);

    large_object_block* bl = (large_object_block*)mem;
    
    CObjectHeader* obj = (CObjectHeader*)block_object (bl);
    //store the pointer to the block before the object. 
    *((BYTE**)obj - 2) = (BYTE*)bl;

    dprintf (3,("New large object: %p, lower than %p", obj, highest_address));

    if (pointerp)
    {
        insert_large_pblock (bl);
    }
    else
    {
        InsertBlock (&large_np_objects, bl, FALSE);
    }

    //Increment the max_generation allocation counter to trigger
    //Full GC if necessary

    dd_new_allocation (dynamic_data_of (max_generation+1)) -= size;

    large_blocks_size += size;

    acontext->alloc_bytes += size;

    return obj;
}


void gc_heap::reset_large_object (BYTE* o)
{
#if 0
    size_t page_start = align_on_page ((size_t)o);
    size_t size = align_lower_page (((BYTE*)page_start - o) + size (o));
    VirtualAlloc ((char*)page_start, size, MEM_RESET, PAGE_READWRITE);
#endif
}


void gc_heap::sweep_large_objects ()
{

    //this min value is for the sake of the dynamic tuning.
    //so we know that we are not starting even if we have no 
    //survivors. 
    large_objects_size = min_obj_size;
    large_object_block* bl = large_p_objects;
    int pin_finger = 0;
    while (bl)
    {
        large_object_block* next_bl = large_object_block_next (bl);
        {
            BYTE* o = block_object (bl);
            if (marked (o))
            {
                clear_marked_pinned (o);
                large_objects_size += size(o);
            }
            else
            {
                RemoveBlock (bl, TRUE);
                large_blocks_size -= size(o);
                gheap->Free (bl);
                reset_large_object (o);
            }
        }
        bl = next_bl;
    }
    bl = large_np_objects;
    while (bl)
    {
        large_object_block* next_bl = large_object_block_next (bl);
        {
            BYTE* o = block_object (bl);
            if ((size_t)(bl->next) & 1)
            {
                //this is a new object allocated since the start of gc
                //leave it alone
                bl->next = next_bl;
            }
            if (marked (o))
            {
                clear_marked_pinned (o);
                large_objects_size += size(o);
            }
            else
            {
                RemoveBlock (bl, FALSE);
                large_blocks_size -= size(o);
                reset_large_object (o);
                gheap->Free (bl);
            }
        }
        bl = next_bl;
    }

}


void gc_heap::copy_through_cards_for_large_objects (card_fn fn)
{
    //This function relies on the list to be sorted.
    large_object_block* bl = large_p_objects;
    size_t last_card = ~1u;
    BOOL         last_cp   = FALSE;

    while (bl)
    {
        BYTE* ob = block_object (bl);
        //object should not be marked
        assert (!marked (ob));

        CGCDesc* map = ((CObjectHeader*)ob)->GetSlotMap();
        CGCDescSeries* cur = map->GetHighestSeries();
        CGCDescSeries* last = map->GetLowestSeries();
        size_t s = size (ob);

        if (cur >= last) do 
        {
            BYTE* o = ob  + cur->GetSeriesOffset();
            BYTE* end_o = o  + cur->GetSeriesSize() + s;

            //Look for a card set within the range

            size_t card     = card_of (o);
            size_t end_card = 0;
            size_t card_word_end =  card_of (align_on_card (end_o));
            while (card_address (card) < end_o)
            {
                if (card >= end_card)
                    find_card (card_table, card, card_word_end, end_card);
                if (card_address (card) < end_o)
                {
                    if ((last_card != ~1u) && (card != last_card))
                    {
                        if (!last_cp)
                            clear_card (last_card);
                        else
                            last_cp = FALSE;
                    }
                    // Look at the portion of the pointers within the card.
                    BYTE** end =(BYTE**) min (end_o, card_address (card+1));
                    BYTE** beg =(BYTE**) max (o, card_address (card));
                    unsigned  markedp = 0;
                    dprintf (3,("Considering large object %p [%p,%p[", ob, beg, end));

                    do 
                    {
                        copy_through_cards_helper (beg, markedp, fn);
                    } while (++beg < end);


                    last_card = card;
                    last_cp |= (markedp != 0);
                    card++;
                }
            }

            cur--;
        } while (cur >= last);
        else
        {
            //array of value classes. 
            int          cnt = map->GetNumSeries();
            BYTE**       ppslot = (BYTE**)(ob + cur->GetSeriesOffset());
            BYTE*        end_o = ob + s - plug_skew;
            size_t card = card_of ((BYTE*)ppslot);
            size_t end_card = 0;
            size_t card_word_end =  card_of (align_on_card (end_o));
            int pitch = 0;
            //compute the total size of the value element. 
            for (int _i = 0; _i > cnt; _i--)
            {
                unsigned nptrs = cur->val_serie[_i].nptrs;
                unsigned skip =  cur->val_serie[_i].skip;
                assert ((skip & (sizeof (BYTE*)-1)) == 0);
                pitch += nptrs*sizeof(BYTE*) + skip;
            }
            
            do 
            {
                if (card >= end_card)
                {
                    find_card (card_table, card, card_word_end, end_card);
                    if (card_address (card) > (BYTE*)ppslot)
                    {
                        ptrdiff_t min_offset = card_address (card) - (BYTE*)ppslot;
                        ppslot = (BYTE**)((BYTE*)ppslot + round_down (min_offset,pitch));
                    }
                }
                if (card_address (card) < end_o)
                {
                    unsigned     markedp = 0;
                    if ((last_card != ~1u) && (card != last_card))
                    {
                        if (!last_cp)
                            clear_card (last_card);
                        else
                            last_cp = FALSE;
                    }
                    BYTE** end =(BYTE**) min (card_address(card+1), end_o);
                    while (ppslot < end)
                    {
                        for (int __i = 0; __i > cnt; __i--)
                        {
                            unsigned nptrs = cur->val_serie[__i].nptrs;
                            unsigned skip =  cur->val_serie[__i].skip;
                            BYTE** ppstop = ppslot + nptrs;
                            do
                            {
                                copy_through_cards_helper (ppslot, markedp, fn);
                                ppslot++;
                            } while (ppslot < ppstop);
                            ppslot = (BYTE**)((BYTE*)ppslot + skip);
                        }
                    }
                    last_card = card;
                    last_cp |= (markedp != 0);
                    card++;
                }
            }while (card_address (card) < end_o);
        }
        bl = bl->next;
    }
    //clear last card. 
    if (last_card != ~1u) 
    {
        if (!last_cp)
            clear_card (last_card);
    }

}


void gc_heap::descr_segment (heap_segment* seg )
{

#ifdef TRACE_GC
    BYTE*  x = heap_segment_mem (seg);
    while (x < heap_segment_allocated (seg))
    {
        dprintf(2, ( "%p: %d ", x, size (x)));
        x = x + Align(size (x));
    }

#endif
}


void gc_heap::descr_card_table ()
{

#ifdef TRACE_GC
    if (trace_gc && (print_level >= 4))
    {
        ptrdiff_t  min = -1;
        dprintf(3,("Card Table set at: "));
        for (size_t i = card_of (lowest_address); i < card_of (highest_address); i++)
        {
            if (card_set_p (i))
            {
                if ((min == -1))
                {
                    min = i;
                }
            }
            else
            {
                if (! ((min == -1)))
                {
                    dprintf (3,("[%p %p[, ",
                            card_address (min), card_address (i)));
                    min = -1;
                }
            }
        }
    }
#endif
}

void gc_heap::descr_generations ()
{

#ifdef TRACE_GC
    int curr_gen_number = max_generation;
    while (curr_gen_number >= 0)
    {
        dprintf (2,( "Generation %d: [%p %p[, gap size: %d",
                 curr_gen_number,
                 generation_allocation_start (generation_of (curr_gen_number)),
                 (((curr_gen_number == 0)) ?
                  (heap_segment_allocated
                   (generation_start_segment
                    (generation_of (curr_gen_number)))) :
                  (generation_allocation_start
                   (generation_of (curr_gen_number - 1)))),
                 size (generation_allocation_start
                       (generation_of (curr_gen_number)))));
        curr_gen_number--;
    }

#endif

}


#undef TRACE_GC

//#define TRACE_GC 

//-----------------------------------------------------------------------------
//
//                                  VM Specific support
//
//-----------------------------------------------------------------------------

#include "excep.h"


#ifdef TRACE_GC

 unsigned long PromotedObjectCount  = 0;
 unsigned long CreatedObjectCount       = 0;
 unsigned long AllocDuration            = 0;
 unsigned long AllocCount               = 0;
 unsigned long AllocBigCount            = 0;
 unsigned long AllocSmallCount      = 0;
 unsigned long AllocStart             = 0;
#endif //TRACE_GC

//Static member variables.

volatile    BOOL    GCHeap::GcInProgress            = FALSE;
GCHeap::SUSPEND_REASON GCHeap::m_suspendReason      = GCHeap::SUSPEND_OTHER;
Thread*             GCHeap::GcThread                = 0;
Thread*             GCHeap::m_GCThreadAttemptingSuspend = NULL;
HANDLE              GCHeap::WaitForGCEvent          = 0;
unsigned            GCHeap::GcCount                 = 0;
#ifdef TRACE_GC
unsigned long       GCHeap::GcDuration;
#endif //TRACE_GC
unsigned            GCHeap::GcCondemnedGeneration   = 0;
CFinalize*          GCHeap::m_Finalize              = 0;
BOOL                GCHeap::GcCollectClasses        = FALSE;
long                GCHeap::m_GCFLock               = 0;

#if defined (STRESS_HEAP) 
OBJECTHANDLE        GCHeap::m_StressObjs[NUM_HEAP_STRESS_OBJS];
int                 GCHeap::m_CurStressObj          = 0;
#endif //STRESS_HEAP


HANDLE              GCHeap::hEventFinalizer     = 0;
HANDLE              GCHeap::hEventFinalizerDone = 0;
HANDLE              GCHeap::hEventFinalizerToShutDown     = 0;
HANDLE              GCHeap::hEventShutDownToFinalizer     = 0;
BOOL                GCHeap::fQuitFinalizer          = FALSE;
Thread*             GCHeap::FinalizerThread         = 0;
AppDomain*          GCHeap::UnloadingAppDomain  = NULL;
BOOL                GCHeap::fRunFinalizersOnUnload  = FALSE;

inline
static void spin_lock ()
{
    enter_spin_lock (&m_GCLock);
}

inline
static void EnterAllocLock()
{
#ifdef _X86_

    
    __asm {
        inc dword ptr m_GCLock
        jz gotit
        call spin_lock
            gotit:
    }
#else //_X86_
    spin_lock();
#endif //_X86_
}

inline
static void LeaveAllocLock()
{
    // Trick this out
    leave_spin_lock (&m_GCLock);
}


// An explanation of locking for finalization:
//
// Multiple threads allocate objects.  During the allocation, they are serialized by
// the AllocLock above.  But they release that lock before they register the object
// for finalization.  That's because there is much contention for the alloc lock, but
// finalization is presumed to be a rare case.
//
// So registering an object for finalization must be protected by the FinalizeLock.
//
// There is another logical queue that involves finalization.  When objects registered
// for finalization become unreachable, they are moved from the "registered" queue to
// the "unreachable" queue.  Note that this only happens inside a GC, so no other
// threads can be manipulating either queue at that time.  Once the GC is over and
// threads are resumed, the Finalizer thread will dequeue objects from the "unreachable"
// queue and call their finalizers.  This dequeue operation is also protected with
// the finalize lock.
//
// At first, this seems unnecessary.  Only one thread is ever enqueuing or dequeuing
// on the unreachable queue (either the GC thread during a GC or the finalizer thread
// when a GC is not in progress).  The reason we share a lock with threads enqueuing
// on the "registered" queue is that the "registered" and "unreachable" queues are
// interrelated.
//
// They are actually two regions of a longer list, which can only grow at one end.
// So to enqueue an object to the "registered" list, you actually rotate an unreachable
// object at the boundary between the logical queues, out to the other end of the
// unreachable queue -- where all growing takes place.  Then you move the boundary
// pointer so that the gap we created at the boundary is now on the "registered"
// side rather than the "unreachable" side.  Now the object can be placed into the
// "registered" side at that point.  This is much more efficient than doing moves
// of arbitrarily long regions, but it causes the two queues to require a shared lock.
//
// Notice that Enter/LeaveFinalizeLock is not a GC-aware spin lock.  Instead, it relies
// on the fact that the lock will only be taken for a brief period and that it will
// never provoke or allow a GC while the lock is held.  This is critical.  If the
// FinalizeLock used enter_spin_lock (and thus sometimes enters preemptive mode to
// allow a GC), then the Alloc client would have to GC protect a finalizable object
// to protect against that eventuality.  That is too slow!


BOOL IsValidObject99(BYTE *pObject)
{
#if defined (VERIFY_HEAP)
    if (!((CObjectHeader*)pObject)->IsFree())
        ((Object *) pObject)->Validate();
#endif
    return(TRUE);
}

#if defined (VERIFY_HEAP)

void
gc_heap::verify_heap()
{
    size_t          last_valid_brick = 0;
    BOOL            bCurrentBrickInvalid = FALSE;
    size_t          curr_brick = 0;
    size_t          prev_brick = -1;
    heap_segment*   seg = generation_start_segment( generation_of( max_generation ) );;
    BYTE*           curr_object = generation_allocation_start(generation_of(max_generation));
    BYTE*           prev_object = 0;
    BYTE*           begin_youngest = generation_allocation_start(generation_of(0));
    BYTE*           end_youngest = heap_segment_allocated (ephemeral_heap_segment);
    int             curr_gen_num = max_generation;
    BYTE*           curr_free_item = generation_free_list (generation_of (curr_gen_num));

    dprintf (2,("Verifying heap"));
    //verify that the generation structures makes sense
    generation* gen = generation_of (max_generation);
    
    assert (generation_allocation_start (gen) == 
            heap_segment_mem (generation_start_segment (gen)));
    int gen_num = max_generation-1;
    generation* prev_gen = gen;
    while (gen_num >= 0)
    {
        gen = generation_of (gen_num);
        assert (generation_allocation_segment (gen) == ephemeral_heap_segment);
        assert (generation_allocation_start (gen) >= heap_segment_mem (ephemeral_heap_segment));
        assert (generation_allocation_start (gen) < heap_segment_allocated (ephemeral_heap_segment));

        if (generation_start_segment (prev_gen ) == 
            generation_start_segment (gen))
        {
            assert (generation_allocation_start (prev_gen) < 
                    generation_allocation_start (gen));
        }
        prev_gen = gen;
        gen_num--;
    }


    while (1)
    {
        // Handle segment transitions
        if (curr_object >= heap_segment_allocated (seg))
        {
            if (curr_object > heap_segment_allocated(seg))
            {
                printf ("curr_object: %p > heap_segment_allocated (seg: %p)",
                        curr_object, seg);
                RetailDebugBreak();
            }
            seg = heap_segment_next(seg);
            if (seg)
            {
                curr_object = heap_segment_mem(seg);
                continue;
            }
            else
                break;  // Done Verifying Heap -- no more segments
        }

        // Are we at the end of the youngest_generation?
        if ((seg == ephemeral_heap_segment) && (curr_object >= end_youngest))
        {
            // prev_object length is too long if we hit this int3
            if (curr_object > end_youngest)
            {
                printf ("curr_object: %p > end_youngest: %p",
                        curr_object, end_youngest);
                RetailDebugBreak();
            }
            break;
        }
        dprintf (4, ("curr_object: %p", curr_object));
        size_t s = size (curr_object);
        if (s == 0)
        {
            printf ("s == 0");
            RetailDebugBreak();
        }

        //verify that the free list makes sense.
        if ((curr_free_item >= heap_segment_mem (seg)) &&
            (curr_free_item < heap_segment_allocated (seg)))
        {
            if (curr_free_item < curr_object)
            {
                printf ("Current free item %p is invalid (inside %p)",
                        prev_object);
                RetailDebugBreak();
            }
            else if (curr_object == curr_free_item) 
            {
                curr_free_item = free_list_slot (curr_free_item);
                if ((curr_free_item == 0) && (curr_gen_num > 0))
                {
                    curr_gen_num--;
                    curr_free_item = generation_free_list (generation_of (curr_gen_num));
                }
                //verify that the free list is its own generation
                if (curr_free_item != 0)
                {
                    if ((curr_free_item >= heap_segment_mem (ephemeral_heap_segment)) &&
                        (curr_free_item < heap_segment_allocated (ephemeral_heap_segment)))
                    {
                        if (curr_free_item < generation_allocation_start (generation_of (curr_gen_num)))
                        {
                            printf ("Current free item belongs to previous gen");
                            RetailDebugBreak();
                        } 
                        else if ((curr_gen_num > 0) && 
                                 ((curr_free_item + Align (size (curr_free_item)))>
                                  generation_allocation_start (generation_of (curr_gen_num-1))))
                        {
                            printf ("Current free item belongs to next gen");
                            RetailDebugBreak();
                        }
                    }
                }

                    
            }
        }



        // If object is not in the youngest generation, then lets
        // verify that the brick table is correct....
        if ((seg != ephemeral_heap_segment) || 
            (brick_of(curr_object) < brick_of(begin_youngest)))
        {
            curr_brick = brick_of(curr_object);

            // Brick Table Verification...
            //
            // On brick transition
            //     if brick is negative
            //          verify that brick indirects to previous valid brick
            //     else
            //          set current brick invalid flag to be flipped if we
            //          encounter an object at the correct place
            //
            if (curr_brick != prev_brick)
            {
                // If the last brick we were examining had positive
                // entry but we never found the matching object, then
                // we have a problem
                // If prev_brick was the last one of the segment 
                // it's ok for it to be invalid because it is never looked at
                if (bCurrentBrickInvalid && 
                    (curr_brick != brick_of (heap_segment_mem (seg))))
                {
                    printf ("curr brick %p invalid", curr_brick);
                    RetailDebugBreak();
                }

                // If the current brick contains a negative value make sure
                // that the indirection terminates at the last  valid brick
                if (brick_table[curr_brick] < 0)
                {
                    if (brick_table [curr_brick] == -32768)
                    {
                        printf ("curr_brick %p for object %p set to -32768",
                                curr_brick, curr_object);
                        RetailDebugBreak();
                    }
                    ptrdiff_t i = curr_brick;
                    while ((i >= ((ptrdiff_t) brick_of (heap_segment_mem (seg)))) &&
                           (brick_table[i] < 0))
                    {
                        i = i + brick_table[i];
                    }
                    if (i <  ((ptrdiff_t)(brick_of (heap_segment_mem (seg))) - 1))
                    {
                        printf ("i: %p < brick_of (heap_segment_mem (seg)):%p - 1. curr_brick: %p",
                                i, brick_of (heap_segment_mem (seg)), 
                                curr_brick);
                        RetailDebugBreak();
                    }
                    // if (i != last_valid_brick)
                    //  RetailDebugBreak();
                    bCurrentBrickInvalid = FALSE;
                }
                else
                {
                    bCurrentBrickInvalid = TRUE;
                }
            }

            if (bCurrentBrickInvalid)
            {
                if (curr_object == (brick_address(curr_brick) + brick_table[curr_brick]))
                {
                    bCurrentBrickInvalid = FALSE;
                    last_valid_brick = curr_brick;
                }
            }
        }


        // Free Objects are not really valid method tables in the sense that
        // IsValidObject will not work, so we special case this
        if (*((BYTE**)curr_object) != (BYTE *) g_pFreeObjectMethodTable)
        {
			assert (!marked (curr_object));
            ((Object*)curr_object)->Validate();
            if (contain_pointers(curr_object))
                go_through_object(method_table (curr_object), curr_object, s, oo,  
                                  { 
                                      if (*oo) 
									  {
										  assert (!marked (*oo));
                                          ((Object*)(*oo))->Validate(); 
									  }
                                  } );
        }

        prev_object = curr_object;
        prev_brick = curr_brick;
        curr_object = curr_object + Align(s);
    }

    verify_card_table();


    {
        //uninit the unused portions of segments. 
        seg = generation_start_segment (generation_of (max_generation));
        while (seg)
        {
            memset (heap_segment_allocated (seg), 0xcc,
                    heap_segment_committed (seg) - heap_segment_allocated (seg));
            seg = heap_segment_next (seg);
        }
    }
    finalize_queue->CheckFinalizerObjects();
}

void ValidateObjectMember (Object* obj)
{
    if (contain_pointers(obj))
    {
        size_t s = size (obj);
        go_through_object(method_table (obj), (BYTE*)obj, s, oo,  
                          { 
                              if (*oo)
                              {
                                  MethodTable *pMT = method_table (*oo);
                                  if (pMT->GetClass()->GetMethodTable() != pMT) {
                                      RetailDebugBreak();
                                  }
                              }
                          } );
    }
}

#endif  //VERIFY_HEAP




void DestructObject (CObjectHeader* hdr)
{
    hdr->~CObjectHeader();
#if 0
    Object* obj = hdr->GetObjectBase();
    // call object destructor
    obj->~Object();
#endif
}

void GCHeap::EnableFinalization( void )
{
    SetEvent( hEventFinalizer );
}

BOOL GCHeap::IsCurrentThreadFinalizer()
{
    return GetThread() == FinalizerThread;
}

Thread* GCHeap::GetFinalizerThread()
{
    _ASSERTE(FinalizerThread != NULL);
    return FinalizerThread;
}


BOOL    GCHeap::HandlePageFault(void* add)
{
    return FALSE;
}

unsigned GCHeap::GetMaxGeneration()
{ 
    return max_generation;
}


HRESULT GCHeap::Shutdown ()
{ 
    deleteGCShadow();

    gc_heap::destroy_gc_heap (pGenGCHeap);


    return S_OK; 
}



//used by static variable implementation
void CGCDescGcScan(LPVOID pvCGCDesc, promote_func* fn, ScanContext* sc)
{
    CGCDesc* map = (CGCDesc*)pvCGCDesc;

    CGCDescSeries *last = map->GetLowestSeries();
    CGCDescSeries *cur = map->GetHighestSeries();

    assert (cur >= last);
    do
    {
        BYTE** ppslot = (BYTE**)((PBYTE)pvCGCDesc + cur->GetSeriesOffset());
        BYTE**ppstop = (BYTE**)((PBYTE)ppslot + cur->GetSeriesSize());

        while (ppslot < ppstop)
        {
            if (*ppslot)
            {
                (fn) ((Object*&) *ppslot, sc);
            }

            ppslot++;
        }

        cur--;
    }
    while (cur >= last);


}

// Wait until a garbage collection is complete
// returns NOERROR if wait was OK, other error code if failure.
// WARNING: This will not undo the must complete state. If you are
// in a must complete when you call this, you'd better know what you're
// doing.


// init the instance heap 
HRESULT GCHeap::Init( size_t )
{
    //Initialize all of the instance members.


    // Rest of the initialization
    HRESULT hres = S_OK;

    return hres;
}

static CFinalize* AllocateCFinalize() {
    return new CFinalize();
}


static
HRESULT AllocateCFinalize(CFinalize **pCFinalize) {
    COMPLUS_TRY 
    {
        *pCFinalize = AllocateCFinalize();
    } 
    COMPLUS_CATCH 
    {
        return E_OUTOFMEMORY;
    } 
    COMPLUS_END_CATCH

    if (!*pCFinalize)
        return E_OUTOFMEMORY;

    return S_OK;
}


//System wide initialization
HRESULT GCHeap::Initialize ()
{

    HRESULT hr = S_OK;
	CFinalize* tmp = 0;

//Initialize the static members. 
#ifdef TRACE_GC
    GcDuration = 0;
    CreatedObjectCount = 0;
#endif

    size_t seg_size = GCHeap::GetValidSegmentSize();

    size_t vmblock_size = seg_size + LHEAP_ALLOC;
    hr = gc_heap::initialize_gc (vmblock_size, seg_size, 
                                 LHEAP_ALLOC);

    if (hr != S_OK)
        return hr;

    if ((WaitForGCEvent = CreateEvent( 0, TRUE, TRUE, 0 )) != 0)
    {
        // Thread for running finalizers...
        if (FinalizerThreadCreate() != 1)
        {
            hr = E_OUTOFMEMORY;
            return hr;
        }
    }
    else
    {
        return E_OUTOFMEMORY;
    }


    StompWriteBarrierResize(FALSE);

#if defined (STRESS_HEAP) 
    if (g_pConfig->GetGCStressLevel() != 0)  {
        for(int i = 0; i < GCHeap::NUM_HEAP_STRESS_OBJS; i++)
            m_StressObjs[i] = CreateGlobalHandle(0);
        m_CurStressObj = 0;
    }
#endif //STRESS_HEAP 



    initGCShadow();         // If we are debugging write barriers, initialize heap shadow

    return Init (0);
};

////
// GC callback functions

BOOL GCHeap::IsPromoted(Object* object, ScanContext* sc)
{
#if defined (_DEBUG) 
    object->Validate(FALSE);
#endif //_DEBUG
    BYTE* o = (BYTE*)object;
    return (!((o < gc_heap::gc_high) && (o >= gc_heap::gc_low)) || 
            gc_heap::is_marked (o));
}

inline
unsigned int GCHeap::WhichGeneration (Object* object)
{
    return gc_heap::object_gennum ((BYTE*)object);
}

BOOL    GCHeap::IsEphemeral (Object* object)
{
    BYTE* o = (BYTE*)object;
    return ((ephemeral_low <= o) && (ephemeral_high > o));
}

#ifdef VERIFY_HEAP

// returns TRUE if the pointer is in one of the GC heaps. 
BOOL GCHeap::IsHeapPointer (void* p, BOOL small_heap_only)
{
	BYTE* object = (BYTE*)p;

    if ((object < gc_heap::highest_address) && (object >= gc_heap::lowest_address))
    {
        if (!small_heap_only) {
            l_heap* lh = gc_heap::lheap;
            while (lh)
            {
                if ((object < (BYTE*)lh->heap + lh->size) && (object >= lh->heap))
                    return TRUE;
                lh = lh->next;
            }
        }

        if (gc_heap::find_segment (object))
            return TRUE;
        else
            return FALSE;
    }
    else
        return FALSE;
}

#endif //VERIFY_HEAP


#ifdef STRESS_PINNING
static n_promote = 0;
#endif
// promote an object
void GCHeap::Promote(Object*& object, ScanContext* sc, DWORD flags)
{

    BYTE* o = (BYTE*)object;

    if (object == 0)
        return;

    dprintf (3, ("Promote %p", o));

#ifdef INTERIOR_POINTERS
    if (flags & GC_CALL_INTERIOR)
    {
        if ((o < gc_heap::gc_low) || (o >= gc_heap::gc_high))
        {
            return;
        }
        o = gc_heap::find_object (o, gc_heap::gc_low);
    }
#endif //INTERIOR_POINTERS


#if defined (_DEBUG)
    ((Object*)o)->Validate(FALSE);
#endif



	//just record pinning for counters
    if (flags & GC_CALL_PINNED)
    {
        COUNTER_ONLY(GetGlobalPerfCounters().m_GC.cPinnedObj ++);
        COUNTER_ONLY(GetPrivatePerfCounters().m_GC.cPinnedObj ++);
    }


	if ((o >= gc_heap::gc_low) && (o < gc_heap::gc_high))
	{
		if (GcCondemnedGeneration == 0)
		{
			if (flags & GC_CALL_PINNED)
			{
				gc_heap::pin_object (o, gc_heap::gc_low, gc_heap::gc_high);

			}

			gc_heap::copy_object_simple_const (&o);
		}
		else
			gc_heap::mark_object_simple (&o);
	}

    LOG((LF_GC|LF_GCROOTS, LL_INFO1000000, "Promote GC Root %#x = %#x\n", &object, object));
}


void GCHeap::Relocate (Object*& object, ScanContext* sc,
                       DWORD flags)
{
	BYTE* o = (BYTE*)object;

	assert (GcCondemnedGeneration == 0);

	ptrdiff_t offset = 0; 

    if (object == 0)
        return;

    dprintf (3, ("Relocate %p\n", object));

#ifdef INTERIOR_POINTERS
    if (flags & GC_CALL_INTERIOR)
    {
        if ((o < gc_heap::gc_low) || (o >= gc_heap::gc_high))
        {
            return;
        }
        o = gc_heap::find_object (o, gc_heap::gc_low);

		offset = (BYTE*)object - o;
    }
#endif //INTERIOR_POINTERS

//#if defined (_DEBUG)
//	((Object*)o)->Validate();
//#endif

	if ((o >= gc_heap::gc_low) && (o < gc_heap::gc_high))
	{
		if (GcCondemnedGeneration == 0)
		{
			gc_heap::get_copied_object (&o);
			object = (Object*)(o + offset);
		}
	}
}


/*static*/ BOOL GCHeap::IsLargeObject(MethodTable *mt)
{
    return mt->GetBaseSize() >= LARGE_OBJECT_SIZE;
}

/*static*/ BOOL GCHeap::IsObjectInFixedHeap(Object *pObj)
{
    // For now we simply look at the size of the object to determine if it in the
    // fixed heap or not. If the bit indicating this gets set at some point
    // we should key off that instead.
    return pObj->GetSize() >= LARGE_OBJECT_SIZE;
}

#ifdef STRESS_HEAP

size_t StressHeapPreIP = -1;
size_t StressHeapPostIP = -1;

    // free up object so that things will move and then do a GC
void GCHeap::StressHeap(alloc_context * acontext) 
{

}
#endif // STRESS_HEAP

//
// Small Object Allocator
//
//

Object *
GCHeap::Alloc( DWORD size, DWORD flags)
{
    THROWSCOMPLUSEXCEPTION();

    TRIGGERSGC();
#ifdef _DEBUG
    Thread* pThread = GetThread();
    if (pThread)
        pThread->SetReadyForSuspension();
#endif
    
    Object* newAlloc;

#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    AllocStart = GetCycleCount32();
    unsigned finish;
#elif defined(ENABLE_INSTRUMENTATION)
    unsigned AllocStart = GetInstLogTime();
    unsigned finish;
#endif
#endif

    EnterAllocLock();

    gc_heap* hp = pGenGCHeap;

    alloc_context* acontext = generation_alloc_context (gc_heap::generation_of(0));

    if (size < LARGE_OBJECT_SIZE)
    {
        
#ifdef TRACE_GC
        AllocSmallCount++;
#endif
        newAlloc = (Object*) gc_heap::allocate (size, acontext);
        LeaveAllocLock();
        ASSERT (newAlloc);
        if (newAlloc != 0)
        {
            if (flags & GC_ALLOC_FINALIZE)
                gc_heap::finalize_queue->RegisterForFinalization (0, newAlloc);
        } else
            COMPlusThrowOM();
    }
    else
    {
        enter_spin_lock (&gc_heap::more_space_lock);
        newAlloc = (Object*) gc_heap::allocate_large_object 
            (size, (flags & GC_ALLOC_CONTAINS_REF ), acontext); 
        leave_spin_lock (&gc_heap::more_space_lock);
        LeaveAllocLock();
        if (newAlloc != 0)
        {
            //Clear the object
            memclr ((BYTE*)newAlloc - plug_skew, Align(size));
            if (flags & GC_ALLOC_FINALIZE)
                gc_heap::finalize_queue->RegisterForFinalization (0, newAlloc);
        } else
            COMPlusThrowOM();
    }
   
#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    finish = GetCycleCount32();
#elif defined(ENABLE_INSTRUMENTATION)
    finish = GetInstLogTime();
#endif
    AllocDuration += finish - AllocStart;
    AllocCount++;
#endif
    return newAlloc;
}

Object *
GCHeap::AllocLHeap( DWORD size, DWORD flags)
{
    THROWSCOMPLUSEXCEPTION();

    TRIGGERSGC();
#ifdef _DEBUG
    Thread* pThread = GetThread();
    if (pThread)
        pThread->SetReadyForSuspension();
#endif
    
    Object* newAlloc;

#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    AllocStart = GetCycleCount32();
    unsigned finish;
#elif defined(ENABLE_INSTRUMENTATION)
    unsigned AllocStart = GetInstLogTime();
    unsigned finish;
#endif
#endif


    gc_heap* hp = pGenGCHeap;

    alloc_context* acontext = generation_alloc_context (gc_heap::generation_of(0));
    enter_spin_lock (&gc_heap::more_space_lock);
    newAlloc = (Object*) gc_heap::allocate_large_object 
        (size, (flags & GC_ALLOC_CONTAINS_REF), acontext); 
    leave_spin_lock (&gc_heap::more_space_lock);
    if (newAlloc != 0)
    {
        //Clear the object
        memclr ((BYTE*)newAlloc - plug_skew, Align(size));

        if (flags & GC_ALLOC_FINALIZE)
            gc_heap::finalize_queue->RegisterForFinalization (0, newAlloc);
    } else
        COMPlusThrowOM();
   
#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    finish = GetCycleCount32();
#elif defined(ENABLE_INSTRUMENTATION)
    finish = GetInstLogTime();
#endif
    AllocDuration += finish - AllocStart;
    AllocCount++;
#endif
    return newAlloc;
}

Object*
GCHeap::Alloc(alloc_context* acontext, DWORD size, DWORD flags )
{
    THROWSCOMPLUSEXCEPTION();

    TRIGGERSGC();
#ifdef _DEBUG
    Thread* pThread = GetThread();
    if (pThread)
        pThread->SetReadyForSuspension();
#endif
    
    Object* newAlloc;

#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    AllocStart = GetCycleCount32();
    unsigned finish;
#elif defined(ENABLE_INSTRUMENTATION)
    unsigned AllocStart = GetInstLogTime();
    unsigned finish;
#endif
#endif

    gc_heap* hp = pGenGCHeap;


    if (size < LARGE_OBJECT_SIZE)
    {
        
#ifdef TRACE_GC
        AllocSmallCount++;
#endif
        newAlloc = (Object*) gc_heap::allocate (size, acontext);
        ASSERT (newAlloc);
        if (newAlloc != 0)
        {
            if (flags & GC_ALLOC_FINALIZE)
                gc_heap::finalize_queue->RegisterForFinalization (0, newAlloc);
        } else
            COMPlusThrowOM();
    }
    else
    {
        enter_spin_lock (&gc_heap::more_space_lock);
        newAlloc = (Object*) gc_heap::allocate_large_object 
                        (size, (flags & GC_ALLOC_CONTAINS_REF), acontext); 
        leave_spin_lock (&gc_heap::more_space_lock);
        if (newAlloc != 0)
        {
            //Clear the object
            memclr ((BYTE*)newAlloc - plug_skew, Align(size));

            if (flags & GC_ALLOC_FINALIZE)
                gc_heap::finalize_queue->RegisterForFinalization (0, newAlloc);
        } else
            COMPlusThrowOM();
    }
   
#ifdef TRACE_GC
#ifdef COUNT_CYCLES
    finish = GetCycleCount32();
#elif defined(ENABLE_INSTRUMENTATION)
    finish = GetInstLogTime();
#endif
    AllocDuration += finish - AllocStart;
    AllocCount++;
#endif
    return newAlloc;
}

void 
GCHeap::FixAllocContext (alloc_context* acontext, BOOL lockp, void* arg)
{


    gc_heap* hp = pGenGCHeap;

    if (lockp)
        enter_spin_lock (&gc_heap::more_space_lock);
    gc_heap::fix_allocation_context (acontext);
    if (lockp)
        leave_spin_lock (&gc_heap::more_space_lock);
}
    


HRESULT
GCHeap::GarbageCollect (int generation, BOOL collect_classes_p)
{

    UINT GenerationAtEntry = GcCount;
    //This loop is necessary for concurrent GC because 
    //during concurrent GC we get in and out of 
    //GarbageCollectGeneration without doing an independent GC
    do 
    {
        enter_spin_lock (&gc_heap::more_space_lock);


        COUNTER_ONLY(GetGlobalPerfCounters().m_GC.cInducedGCs ++);
        COUNTER_ONLY(GetPrivatePerfCounters().m_GC.cInducedGCs ++);

        int gen = (generation < 0) ? max_generation : 
            min (generation, max_generation);
        GarbageCollectGeneration (gen, collect_classes_p);

        leave_spin_lock (&gc_heap::more_space_lock);

    }
    while (GenerationAtEntry == GcCount);
    return S_OK;
}

void
GCHeap::GarbageCollectGeneration (unsigned int gen, BOOL collect_classes_p)
{
    ASSERT_HOLDING_SPIN_LOCK(&gc_heap::more_space_lock);

#ifdef COUNT_CYCLES 
    long gc_start = GetCycleCount32();
#endif //COUNT_CYCLES
    
#if defined ( _DEBUG) && defined (CATCH_GC)
    __try
#endif // _DEBUG && CATCH_GC
    {
    
        LOG((LF_GCROOTS|LF_GC|LF_GCALLOC, LL_INFO10, 
             "{ =========== BEGINGC %d, (gen = %lu, collect_classes = %lu) ==========\n",
             gc_count,
             (ULONG)gen,
             (ULONG)collect_classes_p));

retry:

#ifdef TRACE_GC
#ifdef COUNT_CYCLES
        AllocDuration += GetCycleCount32() - AllocStart;
#else
        AllocDuration += clock() - AllocStart;
#endif
#endif

        {
            SuspendEE(GCHeap::SUSPEND_FOR_GC);
            GcCount++;
        }
    
    // MAP_EVENT_MONITORS(EE_MONITOR_GARBAGE_COLLECTIONS, NotifyEvent(EE_EVENT_TYPE_GC_STARTED, 0));
    
    

#ifdef TRACE_GC
#ifdef COUNT_CYCLES
        unsigned start;
        unsigned finish;
        start = GetCycleCount32();
#else
        clock_t start;
        clock_t finish;
        start = clock();
#endif
        PromotedObjectCount = 0;
#endif
        unsigned int condemned_generation_number = gen;
    

        gc_heap* hp = pGenGCHeap;

        UpdatePreGCCounters();

    
        condemned_generation_number = gc_heap::garbage_collect 
            (condemned_generation_number
            );

    
        if (condemned_generation_number == -1)
            goto retry;
    
#ifdef TRACE_GC
#ifdef COUNT_CYCLES
        finish = GetCycleCount32();
#else
        finish = clock();
#endif
        GcDuration += finish - start;
        dprintf (1, 
                 ("<GC# %d> Condemned: %d, Duration: %d, total: %d Alloc Avg: %d, Small Objects:%d Large Objects:%d",
                  GcCount, condemned_generation_number,
                  finish - start, GcDuration,
                  AllocCount ? (AllocDuration / AllocCount) : 0,
                  AllocSmallCount, AllocBigCount));
        AllocCount = 0;
        AllocDuration = 0;
#endif // TRACE_GC


        {
            GCProfileWalkHeap();
        }

#ifdef JIT_COMPILER_SUPPORTED
        ScavengeJitHeaps();
#endif
    
        //GCTODO
        //CNameSpace::TimeToGC (FALSE);
    
        {
            initGCShadow();
        }
    
#ifdef TRACE_GC
#ifdef COUNT_CYCLES
        AllocStart = GetCycleCount32();
#else
        AllocStart = clock();
#endif
#endif

        {
            UpdatePostGCCounters();
        }

        {

            // no longer in progress
            RestartEE(TRUE, TRUE);
        }
    

        LOG((LF_GCROOTS|LF_GC|LF_GCALLOC, LL_INFO10, 
             "========== ENDGC (gen = %lu, collect_classes = %lu) ===========}\n",
             (ULONG)gen,
            (ULONG)collect_classes_p));
    
    }
#if defined (_DEBUG) && defined (CATCH_GC)
    __except (HandleGCException(GetExceptionCode()))
    {
        _ASSERTE(!"Exception during GarbageCollectGeneration()r");
    }
#endif // _DEBUG && CATCH_GC



#if 0
    if (GcCondemnedGeneration == 2)
    {
        printf ("Finished GC\n");
    }
#endif //0

#ifdef COUNT_CYCLES
    printf ("GC: %d Time: %d\n", GcCondemnedGeneration, 
            GetCycleCount32() - gc_start);
#endif //COUNT_CYCLES

}

size_t      GCHeap::GetTotalBytesInUse ()
{

    return ApproxTotalBytesInUse ();
}

size_t GCHeap::ApproxTotalBytesInUse(BOOL small_heap_only)
{
    size_t totsize = 0;
    //GCTODO
    //ASSERT(InMustComplete());
    enter_spin_lock (&pGenGCHeap->more_space_lock);

    heap_segment* eph_seg = generation_allocation_segment (pGenGCHeap->generation_of (0));
    // Get small block heap size info
    totsize = (heap_segment_allocated (eph_seg) - heap_segment_mem (eph_seg));
    heap_segment* seg = generation_start_segment (pGenGCHeap->generation_of (max_generation));
    while (seg != eph_seg)
    {
        totsize += heap_segment_allocated (seg) -
            heap_segment_mem (seg);
        seg = heap_segment_next (seg);
    }

    if (!small_heap_only)
    {
        // Add size of large objects
        ASSERT(pGenGCHeap->large_blocks_size >= 0);
        totsize += pGenGCHeap->large_blocks_size;
    }
    leave_spin_lock (&pGenGCHeap->more_space_lock);
    return totsize;
}

// The spec for this one isn't clear. This function
// returns the size that can be allocated without
// triggering a GC of any kind.
size_t GCHeap::ApproxFreeBytes()
{
    enter_spin_lock (&pGenGCHeap->more_space_lock);

    generation* gen = pGenGCHeap->generation_of (0);
    size_t res = generation_allocation_limit (gen) - generation_allocation_pointer (gen);

    leave_spin_lock (&pGenGCHeap->more_space_lock);

    return res;
}

HRESULT GCHeap::GetGcCounters(int gen, gc_counters* counters)
{
    if ((gen < 0) || (gen > max_generation))
        return E_FAIL;

    dynamic_data* dd = pGenGCHeap->dynamic_data_of (gen);
    counters->current_size = dd_current_size (dd);
    counters->promoted_size = dd_promoted_size (dd);
    counters->collection_count = dd_collection_count (dd);

    return S_OK;
}

// Verify the segment size is valid.
BOOL GCHeap::IsValidSegmentSize(size_t cbSize)
{
    return (power_of_two_p(cbSize) && (cbSize >> 20));
}

// Verify that gen0 size is at least large enough.
BOOL GCHeap::IsValidGen0MaxSize(size_t cbSize)
{
    return ((cbSize >= 64*1024) && (cbSize % 1024) == 0);
}

// Get the segment size to use, making sure it conforms.
size_t GCHeap::GetValidSegmentSize()
{
    size_t seg_size = g_pConfig->GetSegmentSize();
    if (!GCHeap::IsValidSegmentSize(seg_size))
        seg_size = INITIAL_ALLOC;
    return (seg_size);
}

// Get the max gen0 heap size, making sure it conforms.
size_t GCHeap::GetValidGen0MaxSize(size_t seg_size)
{
    size_t gen0size = g_pConfig->GetGCgen0size();

    if (gen0size == 0)
    {
        gen0size = 800*1024;
    }

    // if it appears to be non-sensical, revert to default
    if (!GCHeap::IsValidGen0MaxSize(gen0size))
        gen0size = 800*1024;

    // Generation 0 must never be more than 1/2 the segment size.
    if (gen0size >= (seg_size / 2))
        gen0size = seg_size / 2;

    return (gen0size);   
}


void GCHeap::SetReservedVMLimit (size_t vmlimit)
{
    gc_heap::reserved_memory_limit = vmlimit;
}

Object* GCHeap::GetNextFinalizableObject()
{
    return pGenGCHeap->finalize_queue->GetNextFinalizableObject();
}

size_t GCHeap::GetNumberFinalizableObjects()
{
    return pGenGCHeap->finalize_queue->GetNumberFinalizableObjects();
}

size_t GCHeap::GetFinalizablePromotedCount()
{
    return pGenGCHeap->finalize_queue->GetPromotedCount();
}

BOOL GCHeap::FinalizeAppDomain(AppDomain *pDomain, BOOL fRunFinalizers)
{
    return pGenGCHeap->finalize_queue->FinalizeAppDomain (pDomain, fRunFinalizers);
}

void GCHeap::SetFinalizeQueueForShutdown(BOOL fHasLock)
{
    pGenGCHeap->finalize_queue->SetSegForShutDown(fHasLock);
}



//---------------------------------------------------------------------------
// Finalized class tracking
//---------------------------------------------------------------------------

void GCHeap::RegisterForFinalization (int gen, Object* obj)
{
    if (gen == -1) 
        gen = 0;
    if (((obj->GetHeader()->GetBits()) & BIT_SBLK_FINALIZER_RUN))
    {
        //just reset the bit
        obj->GetHeader()->ClrBit(BIT_SBLK_FINALIZER_RUN);
    }
    else 
    {
        gc_heap::finalize_queue->RegisterForFinalization (gen, obj);
    }
}

void GCHeap::SetFinalizationRun (Object* obj)
{
    obj->GetHeader()->SetBit(BIT_SBLK_FINALIZER_RUN);
}
    


//----------------------------------------------------------------------------
//
// Write Barrier Support for bulk copy ("Clone") operations
//
// StartPoint is the target bulk copy start point
// len is the length of the bulk copy (in bytes)
//
//
// Performance Note:
//
// This is implemented somewhat "conservatively", that is we
// assume that all the contents of the bulk copy are object
// references.  If they are not, and the value lies in the
// ephemeral range, we will set false positives in the card table.
//
// We could use the pointer maps and do this more accurately if necessary

VOID
SetCardsAfterBulkCopy( Object **StartPoint, size_t len )
{
    Object **rover;
    Object **end;

    // Target should aligned
    assert(Aligned ((size_t)StartPoint));

        
    // Don't optimize the Generation 0 case if we are checking for write barrier voilations
    // since we need to update the shadow heap even in the generation 0 case.
#ifdef WRITE_BARRIER_CHECK
    if (g_pConfig->GetHeapVerifyLevel() > 1) 
        for(unsigned i=0; i < len / sizeof(Object*); i++)
            updateGCShadow(&StartPoint[i], StartPoint[i]);
#endif
    // If destination is in Gen 0 don't bother
    if (GCHeap::WhichGeneration( (Object*) StartPoint ) == 0)
        return;

    rover = StartPoint;
    end = StartPoint + (len/sizeof(Object*));
    while (rover < end)
    {
        if ( (((BYTE*)*rover) >= g_ephemeral_low) && (((BYTE*)*rover) < g_ephemeral_high) )
        {
            // Set Bit For Card and advance to next card
            size_t card = gcard_of ((BYTE*)rover);

            gset_card (card);
            // Skip to next card for the object
            rover = (Object**) (g_lowest_address + (card_size * (card+1)));
        }
        else
        {
            rover++;
        }
    }
}




//--------------------------------------------------------------------
//
//          Support for finalization
//
//--------------------------------------------------------------------

inline
unsigned int gen_segment (int gen)
{
    return (NUMBERGENERATIONS - gen - 1);
}

CFinalize::CFinalize()
{
    BUGGY_THROWSCOMPLUSEXCEPTION();

    m_Array = new(Object*[100]);

    if (!m_Array)
    {
        ASSERT (m_Array);
        COMPlusThrowOM();;
    }
    m_EndArray = &m_Array[100];

    for (unsigned int i =0; i < NUMBERGENERATIONS+2; i++)
    {
        m_FillPointers[i] = m_Array;
    }
    m_PromotedCount = 0;
    lock = -1;
}

CFinalize::~CFinalize()
{
    delete m_Array;
}

int CFinalize::GetPromotedCount ()
{
    return m_PromotedCount;
}


inline
void CFinalize::EnterFinalizeLock()
{
    _ASSERTE(dbgOnly_IsSpecialEEThread() ||
             GetThread() == 0 ||
             GetThread()->PreemptiveGCDisabled());

retry:
    if (FastInterlockExchange (&lock, 0) >= 0)
    {
        unsigned int i = 0;
        while (lock >= 0)
        {
            if (++i & 7)
                __SwitchToThread (0);
            else
                __SwitchToThread (5);
        }
        goto retry;
    }
}

inline
void CFinalize::LeaveFinalizeLock()
{
    _ASSERTE(dbgOnly_IsSpecialEEThread() ||
             GetThread() == 0 ||
             GetThread()->PreemptiveGCDisabled());

    lock = -1;
}


void
CFinalize::RegisterForFinalization (int gen, Object* obj)
{
    THROWSCOMPLUSEXCEPTION();


    EnterFinalizeLock();
    // Adjust gen
    unsigned int dest = gen_segment (gen);

    Object*** s_i = &m_FillPointers [NUMBERGENERATIONS]; 
    if ((*s_i) == m_EndArray)
    {
        if (!GrowArray())
        {
            LeaveFinalizeLock();
            COMPlusThrowOM();;
        }
    }
    Object*** end_si = &m_FillPointers[dest];
    do 
    {
        //is the segment empty? 
        if (!(*s_i == *(s_i-1)))
        {
            //no, swap the end elements. 
            *(*s_i) = *(*(s_i-1));
        }
        //increment the fill pointer
        (*s_i)++;
        //go to the next segment. 
        s_i--;
    } while (s_i > end_si);

    // We have reached the destination segment
    // store the object
    **s_i = obj;
    // increment the fill pointer
    (*s_i)++;

    if (g_fFinalizerRunOnShutDown) {
        // Adjust boundary for segments so that GC will keep objects alive.
        SetSegForShutDown(TRUE);
    }

    LeaveFinalizeLock();

}

Object*
CFinalize::GetNextFinalizableObject ()
{
    Object* obj = 0;
    //serialize
    EnterFinalizeLock();
    if (!IsSegEmpty(NUMBERGENERATIONS))
    {
        obj =  *(--m_FillPointers [NUMBERGENERATIONS]);

    }
    LeaveFinalizeLock();
    return obj;
}

void
CFinalize::SetSegForShutDown(BOOL fHasLock)
{
    int i;

    if (!fHasLock)
        EnterFinalizeLock();
    for (i = 0; i < NUMBERGENERATIONS; i++) {
        m_FillPointers[i] = m_Array;
    }
    if (!fHasLock)
        LeaveFinalizeLock();

}

size_t 
CFinalize::GetNumberFinalizableObjects()
{
    return m_FillPointers[NUMBERGENERATIONS] - 
        (g_fFinalizerRunOnShutDown?m_Array:m_FillPointers[NUMBERGENERATIONS-1]);
}


BOOL
CFinalize::FinalizeAppDomain (AppDomain *pDomain, BOOL fRunFinalizers)
{
    BOOL finalizedFound = FALSE;

    unsigned int startSeg = gen_segment (max_generation);

    EnterFinalizeLock();

    //reset the N+2 segment to empty
    m_FillPointers[NUMBERGENERATIONS+1] = m_FillPointers[NUMBERGENERATIONS];
    
    for (unsigned int Seg = startSeg; Seg < NUMBERGENERATIONS; Seg++)
    {
        Object** endIndex = Seg ? m_FillPointers [Seg-1] : m_Array;
        for (Object** i = m_FillPointers [Seg]-1; i >= endIndex ;i--)
        {
            Object* obj = *i;

            // Objects are put into the finalization queue before they are complete (ie their methodtable 
            // may be null) so we must check that the object we found has a method table before checking 
            // if it has the index we are looking for. If the methodtable is null, it can't be from the 
            // unloading domain, so skip it.
            if (obj->GetMethodTable() == NULL)
                continue;

            // eagerly finalize all objects except those that may be agile. 
            if (obj->GetAppDomainIndex() != pDomain->GetIndex())
                continue;

            if (obj->GetMethodTable()->IsAgileAndFinalizable())
            {
                // If an object is both agile & finalizable, we leave it in the
                // finalization queue during unload.  This is OK, since it's agile.
                // Right now only threads can be this way, so if that ever changes, change
                // the assert to just continue if not a thread.
                _ASSERTE(obj->GetMethodTable() == g_pThreadClass);

                // However, an unstarted thread should be finalized. It could be holding a delegate 
                // in the domain we want to unload. Once the thread has been started, its 
                // delegate is cleared so only unstarted threads are a problem.
                Thread *pThread = ((THREADBASEREF)ObjectToOBJECTREF(obj))->GetInternal();
                if (! pThread || ! pThread->IsUnstarted())
                    continue;
            }

            if (!fRunFinalizers || (obj->GetHeader()->GetBits()) & BIT_SBLK_FINALIZER_RUN)
            {
                //remove the object because we don't want to 
                //run the finalizer
                MoveItem (i, Seg, NUMBERGENERATIONS+2);
                //Reset the bit so it will be put back on the queue
                //if resurrected and re-registered.
                obj->GetHeader()->ClrBit (BIT_SBLK_FINALIZER_RUN);
            }
            else
            {
                finalizedFound = TRUE;
                MoveItem (i, Seg, NUMBERGENERATIONS);
            }
        }
    }

    LeaveFinalizeLock();

    return finalizedFound;
}

void
CFinalize::MoveItem (Object** fromIndex,
                     unsigned int fromSeg,
                     unsigned int toSeg)
{

    int step;
    ASSERT (fromSeg != toSeg);
    if (fromSeg > toSeg)
        step = -1;
    else
        step = +1;
    // Place the element at the boundary closest to dest
    Object** srcIndex = fromIndex;
    for (unsigned int i = fromSeg; i != toSeg; i+= step)
    {
        Object**& destFill = m_FillPointers[i+(step - 1 )/2];
        Object** destIndex = destFill - (step + 1)/2;
        if (srcIndex != destIndex)
        {
            Object* tmp = *srcIndex;
            *srcIndex = *destIndex;
            *destIndex = tmp;
        }
        destFill -= step;
        srcIndex = destIndex;
    }
}

void
CFinalize::GcScanRoots (promote_func* fn, int hn, ScanContext *pSC)
{

    ScanContext sc;
    if (pSC == NULL)
        pSC = &sc;

    pSC->thread_number = hn;
    //scan the finalization queue
    Object** startIndex = m_FillPointers[NUMBERGENERATIONS-1];
    Object** stopIndex  = m_FillPointers[NUMBERGENERATIONS];
    for (Object** po = startIndex; po < stopIndex; po++)
    {
        (*fn)(*po, pSC, 0);

    }
}

BOOL
CFinalize::ScanForFinalization (int gen, int passNumber, BOOL mark_only_p,
                                gc_heap* hp)
{
    ScanContext sc;
    sc.promotion = TRUE;

    BOOL finalizedFound = FALSE;
    m_PromotedCount = 0;

    //start with gen and explore all the younger generations.
    unsigned int startSeg = gen_segment (gen);
    if (passNumber == 1)
    {
        //reset the N+2 segment to empty
        m_FillPointers[NUMBERGENERATIONS+1] = m_FillPointers[NUMBERGENERATIONS];
        unsigned int max_seg = gen_segment (max_generation);
        for (unsigned int Seg = startSeg; Seg < NUMBERGENERATIONS; Seg++)
        {
            Object** endIndex = Seg ? m_FillPointers [Seg-1] : m_Array;
            for (Object** i = m_FillPointers [Seg]-1; i >= endIndex ;i--)
            {
                Object* obj = *i;
                if (!GCHeap::IsPromoted (obj, &sc))
                {
                    if ((obj->GetHeader()->GetBits()) & BIT_SBLK_FINALIZER_RUN)
                    {
                        //remove the object because we don't want to 
                        //run the finalizer
                        MoveItem (i, Seg, NUMBERGENERATIONS+2);
                        //Reset the bit so it will be put back on the queue
                        //if resurrected and re-registered.
                        obj->GetHeader()->ClrBit (BIT_SBLK_FINALIZER_RUN);

                    }
                    else
                    {
                        m_PromotedCount++;
                        finalizedFound = TRUE;
                        MoveItem (i, Seg, NUMBERGENERATIONS);
                    }

                }
            }
        }
    }
    if (finalizedFound)
    {
        //Promote the f-reachable objects
        GcScanRoots (GCHeap::Promote, 0, 0);

        if (!mark_only_p)
            SetEvent(GCHeap::hEventFinalizer);
    }

    return finalizedFound;
}

//Relocates all of the objects in the finalization array
void
CFinalize::RelocateFinalizationData (int gen, gc_heap* hp)
{
    ScanContext sc;
    sc.promotion = FALSE;
    unsigned int Seg = gen_segment (gen);

    Object** startIndex = Seg ? m_FillPointers [Seg-1] : m_Array;
    for (Object** po = startIndex; po < m_FillPointers [NUMBERGENERATIONS];po++)
    {
        GCHeap::Relocate (*po, &sc);
    }
}

void
CFinalize::UpdatePromotedGenerations (int gen, BOOL ignore)
{
    // update the generation fill pointers. 
	for (int i = min (gen+1, max_generation); i > 0; i--)
	{
		m_FillPointers [gen_segment(i)] = m_FillPointers [gen_segment(i-1)];
	}

}

BOOL
CFinalize::GrowArray()
{
    size_t oldArraySize = (m_EndArray - m_Array);
    size_t newArraySize =  (oldArraySize* 12)/10;
    WS_PERF_SET_HEAP(GC_HEAP);
    Object** newArray = new(Object*[newArraySize]);
    if (!newArray)
    {
        // It's not safe to throw here, because of the FinalizeLock.  Tell our caller
        // to throw for us.
        ASSERT (newArray);
        return FALSE;
    }
    WS_PERF_UPDATE("GC:CRFinalizeGrowArray", sizeof(Object*)*newArraySize, newArray);
    memcpy (newArray, m_Array, oldArraySize*sizeof(Object*));

    //adjust the fill pointers
    for (unsigned i = 0; i <= NUMBERGENERATIONS+1; i++)
    {
        m_FillPointers [i] += (newArray - m_Array);
    }
    delete m_Array;
    m_Array = newArray;
    m_EndArray = &m_Array [newArraySize];

    return TRUE;
}


#if defined (VERIFY_HEAP)
void CFinalize::CheckFinalizerObjects()
{
    for (unsigned int i = 0; i < NUMBERGENERATIONS; i++)
    {
        Object **startIndex = (i > 0) ? m_Array : m_FillPointers[i-1];
        Object **stopIndex  = m_FillPointers[i];

        for (Object **po = startIndex; po > stopIndex; po++)
        {
            if (GCHeap::WhichGeneration (*po) < (NUMBERGENERATIONS - i -1))
                RetailDebugBreak ();
            (*po)->Validate();

        }
    }
}
#endif




//------------------------------------------------------------------------------
//
//                      End of VM specific support
//
//------------------------------------------------------------------------------

#if defined (GC_PROFILING)
void gc_heap::walk_heap (walk_fn fn, void* context, int gen_number, BOOL walk_large_object_heap_p)
{
    generation* gen = gc_heap::generation_of (gen_number);
    heap_segment*    seg = generation_start_segment (gen);
    BYTE*       x = generation_allocation_start (gen);
    BYTE*       end = heap_segment_allocated (seg);

    while (1)
    {
        if (x >= end)
        {
            if ((seg = heap_segment_next(seg)) != 0)
            {
                x = heap_segment_mem (seg);
                end = heap_segment_allocated (seg);
                continue;
            } else
            {
                break;
            }
        }
        size_t s = size (x);
        CObjectHeader* o = (CObjectHeader*)x;
        if (!o->IsFree())
        {
            _ASSERTE(((size_t)o & 0x3) == 0); // Last two bits should never be set at this point
            if (!fn (o->GetObjectBase(), context))
                return;
        }
        x = x + Align (s);
    }

    if (walk_large_object_heap_p)
    {
        // go through large objects
        large_object_block* bl = gc_heap::large_p_objects;
        while (bl)
        {
            large_object_block* next_bl = large_object_block_next (bl);
            BYTE* x = block_object (bl);
            CObjectHeader* o = (CObjectHeader*)x;
            _ASSERTE(((size_t)o & 0x3) == 0); // Last two bits should never be set at this point
            if (!fn (o->GetObjectBase(), context))
                return;
            bl = next_bl;
        }

        bl = gc_heap::large_np_objects;
        while (bl)
        {
            large_object_block* next_bl = large_object_block_next (bl);
            BYTE* x = block_object (bl);
            CObjectHeader* o = (CObjectHeader*)x;
            _ASSERTE(((size_t)o & 0x3) == 0); // Last two bits should never be set at this point
            if (!fn (o->GetObjectBase(), context))
                return;
            bl = next_bl;
        }
    }
}

void ::walk_object (Object* obj, walk_fn fn, void* context)
{
    BYTE* o = (BYTE*)obj;
    if (o && contain_pointers (o))
    {
        go_through_object (method_table (o), o, size(o), oo,
                           {
                               if (*oo)
                               {
                                   Object *oh = (Object*)*oo;
                                   if (!fn (oh, context))
                                       return;
                               }
                           }
            );

    }
}



#endif //GC_PROFILING 


// Go through and touch (read) each page straddled by a memory block.
void TouchPages(LPVOID pStart, UINT cb)
{
    const UINT pagesize = OS_PAGE_SIZE;
    _ASSERTE(0 == (pagesize & (pagesize-1))); // Must be a power of 2.
    if (cb)
    {
        volatile char *pEnd = cb + (char*)pStart;
        volatile char *p = ((char*)pStart) -  (((size_t)pStart) & (pagesize-1));
        while (p < pEnd)
        {
            char a = *p;
            //printf("Touching page %lxh\n", (ULONG)p);
            p += pagesize;
        }

    }
}

#ifdef WRITE_BARRIER_CHECK

    // This code is designed to catch the failure to update the write barrier
    // The way it works is to copy the whole heap right after every GC.  The write
    // barrier code has been modified so that it updates the shadow as well as the
    // real GC heap.  Before doing the next GC, we walk the heap, looking for pointers
    // that were updated in the real heap, but not the shadow.  A mismatch indicates
    // an error.  The offending code can be found by breaking after the correct GC, 
    // and then placing a data breakpoint on the Heap location that was updated without
    // going through the write barrier.  

BYTE* g_GCShadow;
BYTE* g_GCShadowEnd;


    // Called at process shutdown
void deleteGCShadow() 
{
}

    // Called at startup and right after a GC, get a snapshot of the GC Heap
void initGCShadow() 
{

}

    // called by the write barrier to update the shadow heap
void updateGCShadow(Object** ptr, Object* val)  
{
}

    // test to see if 'ptr' was only updated via the write barrier. 
inline void testGCShadow(Object** ptr) 
{

}

    // Walk the whole heap, looking for pointers that were not updated with the write barrier. 
void checkGCWriteBarrier() 
{
}

#endif WRITE_BARRIER_CHECK
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gmheap.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*
*/


/* ---------- To make a malloc.h, start cutting here ------------ */

/*
  A version of malloc/free/realloc written by Doug Lea and released to the
  public domain.  Send questions/comments/complaints/performance data
  to dl@cs.oswego.edu

* VERSION 2.6.5  Wed Jun 17 15:55:16 1998  Doug Lea  (dl at gee)
  
   Note: There may be an updated version of this malloc obtainable at
           ftp://g.oswego.edu/pub/misc/malloc.c
         Check before installing!

   Note: This version differs from 2.6.4 only by correcting a
         statement ordering error that could cause failures only
         when calls to this malloc are interposed with calls to
         other memory allocators.

* Why use this malloc?

  This is not the fastest, most space-conserving, most portable, or
  most tunable malloc ever written. However it is among the fastest
  while also being among the most space-conserving, portable and tunable.
  Consistent balance across these factors results in a good general-purpose 
  allocator. For a high-level description, see 
     http://g.oswego.edu/dl/html/malloc.html

* Synopsis of public routines

  (Much fuller descriptions are contained in the program documentation below.)

  malloc(size_t n);
     Return a pointer to a newly allocated chunk of at least n bytes, or null
     if no space is available.
  free(Void_t* p);
     Release the chunk of memory pointed to by p, or no effect if p is null.
  realloc(Void_t* p, size_t n);
     Return a pointer to a chunk of size n that contains the same data
     as does chunk p up to the minimum of (n, p's size) bytes, or null
     if no space is available. The returned pointer may or may not be
     the same as p. If p is null, equivalent to malloc.  Unless the
     #define REALLOC_ZERO_BYTES_FREES below is set, realloc with a
     size argument of zero (re)allocates a minimum-sized chunk.
  memalign(size_t alignment, size_t n);
     Return a pointer to a newly allocated chunk of n bytes, aligned
     in accord with the alignment argument, which must be a power of
     two.
  valloc(size_t n);
     Equivalent to memalign(pagesize, n), where pagesize is the page
     size of the system (or as near to this as can be figured out from
     all the includes/defines below.)
  pvalloc(size_t n);
     Equivalent to valloc(minimum-page-that-holds(n)), that is,
     round up n to nearest pagesize.
  calloc(size_t unit, size_t quantity);
     Returns a pointer to quantity * unit bytes, with all locations
     set to zero.
  cfree(Void_t* p);
     Equivalent to free(p).
  malloc_trim(size_t pad);
     Release all but pad bytes of freed top-most memory back 
     to the system. Return 1 if successful, else 0.
  malloc_usable_size(Void_t* p);
     Report the number usable allocated bytes associated with allocated
     chunk p. This may or may not report more bytes than were requested,
     due to alignment and minimum size constraints.
  malloc_stats();
     Prints brief summary statistics on stderr.
  mallinfo()
     Returns (by copy) a struct containing various summary statistics.
  mallopt(int parameter_number, int parameter_value)
     Changes one of the tunable parameters described below. Returns
     1 if successful in changing the parameter, else 0.

* Vital statistics:

  Alignment:                            8-byte

       8 byte alignment is currently hardwired into the design.  This
       seems to suffice for all current machines and C compilers.

  Assumed pointer representation:       4 or 8 bytes
       Code for 8-byte pointers is untested by me but has worked
       reliably by Wolfram Gloger, who contributed most of the
       changes supporting this.

  Assumed size_t  representation:       4 or 8 bytes
       Note that size_t is allowed to be 4 bytes even if pointers are 8.        

  Minimum overhead per allocated chunk: 4 or 8 bytes
       Each malloced chunk has a hidden overhead of 4 bytes holding size
       and status information.  

  Minimum allocated size: 4-byte ptrs:  16 bytes    (including 4 overhead)
                          8-byte ptrs:  24/32 bytes (including, 4/8 overhead)
                                     
       When a chunk is freed, 12 (for 4byte ptrs) or 20 (for 8 byte
       ptrs but 4 byte size) or 24 (for 8/8) additional bytes are 
       needed; 4 (8) for a trailing size field
       and 8 (16) bytes for free list pointers. Thus, the minimum
       allocatable size is 16/24/32 bytes.

       Even a request for zero bytes (i.e., malloc(0)) returns a
       pointer to something of the minimum allocatable size.

  Maximum allocated size: 4-byte size_t: 2^31 -  8 bytes
                          8-byte size_t: 2^63 - 16 bytes

       It is assumed that (possibly signed) size_t bit values suffice to
       represent chunk sizes. `Possibly signed' is due to the fact
       that `size_t' may be defined on a system as either a signed or
       an unsigned type. To be conservative, values that would appear
       as negative numbers are avoided.  
       Requests for sizes with a negative sign bit will return a
       minimum-sized chunk.

  Maximum overhead wastage per allocated chunk: normally 15 bytes

       Alignnment demands, plus the minimum allocatable size restriction
       make the normal worst-case wastage 15 bytes (i.e., up to 15
       more bytes will be allocated than were requested in malloc), with 
       two exceptions:
         1. Because requests for zero bytes allocate non-zero space,
            the worst case wastage for a request of zero bytes is 24 bytes.
         2. For requests >= mmap_threshold that are serviced via
            mmap(), the worst case wastage is 8 bytes plus the remainder
            from a system page (the minimal mmap unit); typically 4096 bytes.

* Limitations

    Here are some features that are NOT currently supported

    * No user-definable hooks for callbacks and the like.
    * No automated mechanism for fully checking that all accesses
      to malloced memory stay within their bounds.
    * No support for compaction.

* Synopsis of compile-time options:

    People have reported using previous versions of this malloc on all
    versions of Unix, sometimes by tweaking some of the defines
    below. It has been tested most extensively on Solaris and
    Linux. It is also reported to work on WIN32 platforms.
    People have also reported adapting this malloc for use in
    stand-alone embedded systems.

    The implementation is in straight, hand-tuned ANSI C.  Among other
    consequences, it uses a lot of macros.  Because of this, to be at
    all usable, this code should be compiled using an optimizing compiler
    (for example gcc -O2) that can simplify expressions and control
    paths.

  __STD_C                  (default: derived from C compiler defines)
     Nonzero if using ANSI-standard C compiler, a C++ compiler, or
     a C compiler sufficiently close to ANSI to get away with it.
  DEBUG                    (default: NOT defined)
     Define to enable debugging. Adds fairly extensive assertion-based 
     checking to help track down memory errors, but noticeably slows down
     execution.
  REALLOC_ZERO_BYTES_FREES (default: NOT defined) 
     Define this if you think that realloc(p, 0) should be equivalent
     to free(p). Otherwise, since malloc returns a unique pointer for
     malloc(0), so does realloc(p, 0).
  HAVE_MEMCPY               (default: defined)
     Define if you are not otherwise using ANSI STD C, but still 
     have memcpy and memset in your C library and want to use them.
     Otherwise, simple internal versions are supplied.
  USE_MEMCPY               (default: 1 if HAVE_MEMCPY is defined, 0 otherwise)
     Define as 1 if you want the C library versions of memset and
     memcpy called in realloc and calloc (otherwise macro versions are used). 
     At least on some platforms, the simple macro versions usually
     outperform libc versions.
  HAVE_MMAP                 (default: defined as 1)
     Define to non-zero to optionally make malloc() use mmap() to
     allocate very large blocks.  
  HAVE_MREMAP                 (default: defined as 0 unless Linux libc set)
     Define to non-zero to optionally make realloc() use mremap() to
     reallocate very large blocks.  
  malloc_getpagesize        (default: derived from system #includes)
     Either a constant or routine call returning the system page size.
  HAVE_USR_INCLUDE_MALLOC_H (default: NOT defined) 
     Optionally define if you are on a system with a /usr/include/malloc.h
     that declares struct mallinfo. It is not at all necessary to
     define this even if you do, but will ensure consistency.
  INTERNAL_SIZE_T           (default: size_t)
     Define to a 32-bit type (probably `unsigned int') if you are on a 
     64-bit machine, yet do not want or need to allow malloc requests of 
     greater than 2^31 to be handled. This saves space, especially for
     very small chunks.
  INTERNAL_LINUX_C_LIB      (default: NOT defined)
     Defined only when compiled as part of Linux libc.
     Also note that there is some odd internal name-mangling via defines
     (for example, internally, `malloc' is named `mALLOc') needed
     when compiling in this case. These look funny but don't otherwise
     affect anything.
  WIN32                     (default: undefined)
     Define this on MS win (95, nt) platforms to compile in sbrk emulation.
  LACKS_UNISTD_H            (default: undefined)
     Define this if your system does not have a <unistd.h>.
  MORECORE                  (default: sbrk)
     The name of the routine to call to obtain more memory from the system.
  MORECORE_FAILURE          (default: -1)
     The value returned upon failure of MORECORE.
  MORECORE_CLEARS           (default 1)
     True (1) if the routine mapped to MORECORE zeroes out memory (which
     holds for sbrk).
  DEFAULT_TRIM_THRESHOLD
  DEFAULT_TOP_PAD       
  DEFAULT_MMAP_THRESHOLD
  DEFAULT_MMAP_MAX      
     Default values of tunable parameters (described in detail below)
     controlling interaction with host system routines (sbrk, mmap, etc).
     These values may also be changed dynamically via mallopt(). The
     preset defaults are those that give best performance for typical
     programs/systems.


*/

#include "common.h"

#ifndef ASSERT
#define ASSERT _ASSERTE
#endif

//#define FORCEDEBUG
//#define PARANOID_VALIDATION

#if defined(FORCEDEBUG) && !defined(DEBUG)
#undef ASSERT
#define DEBUG
VOID __gmassert (int cond, LPCSTR condstr, DWORD line)
{
    if (!cond)
    {
        CHAR buf[100];
        wsprintf(buf, "failed gmheap assertion at %d: %s\n", line, condstr);
        OutputDebugString(buf);
        RetailDebugBreak();
    }
}
#define ASSERT(cond) __gmassert((int)(cond), #cond, __LINE__)
#else
#define REALDEBUG
#endif


#include "gmheap.hpp"
#include "wsperf.h"
//#include "qdll.h"
//#include "memmon.h"

//#define MAP_VIEW

#ifdef MAP_VIEW
#ifdef CreateFileMapping

#undef CreateFileMapping

#endif //CreateFileMapping

#define CreateFileMapping WszCreateFileMapping
#endif //MAP_VIEW



#if defined(ENABLE_MEMORY_LOG) && !defined(GMALLOCHEAPS)
#define TAGALLOC(mem,size) (LogUserHeapAlloc(this, mem, size, __FILE__, __LINE__), mem)
#define TAGREALLOC(oldmem,newmem,size) (LogUserHeapReAlloc(this, oldmem, newmem, size, __FILE__, __LINE__), newmem)
#define TAGFREE(mem) LogUserHeapFree(this,mem,__FILE__,__LINE__)
#define IGNOREFREE(fn) { SuspendMemoryTagging(); fn; ResumeMemoryTagging(); }
#else
#define TAGALLOC(mem,size) (mem)
#define TAGREALLOC(oldmem,newmem,size) (newmem)
#define TAGFREE(mem) (mem)
#define IGNOREFREE(fn) fn
#endif


/* Preliminaries */


#define __STD_C     1


#ifndef Void_t
#if __STD_C
#define Void_t      void
#else
#define Void_t      char
#endif
#endif /*Void_t*/

#if __STD_C
#include <stddef.h>   /* for size_t */
#else
#include <sys/types.h>
#endif

#ifdef __cplusplus
extern "C" {
#endif

#include <stdio.h>    /* needed for malloc_stats */


/*
  Compile-time options
*/


/*
    Debugging:

    Because freed chunks may be overwritten with link fields, this
    malloc will often die when freed memory is overwritten by user
    programs.  This can be very effective (albeit in an annoying way)
    in helping track down dangling pointers.

    If you compile with -DDEBUG, a number of assertion checks are
    enabled that will catch more memory errors. You probably won't be
    able to make much sense of the actual assertion errors, but they
    should help you locate incorrectly overwritten memory.  The
    checking is fairly extensive, and will slow down execution
    noticeably. Calling malloc_stats or mallinfo with DEBUG set will
    attempt to check every non-mmapped allocated and free chunk in the
    course of computing the summmaries. (By nature, mmapped regions
    cannot be checked very much automatically.)

    Setting DEBUG may also be helpful if you are trying to modify 
    this code. The assertions in the check routines spell out in more 
    detail the assumptions and invariants underlying the algorithms.

*/

#if 0
#if DEBUG 
#include <assert.h>
#else
#define assert(x) ((void)0)
#endif

#endif //0

/*
  INTERNAL_SIZE_T is the word-size used for internal bookkeeping
  of chunk sizes. On a 64-bit machine, you can reduce malloc
  overhead by defining INTERNAL_SIZE_T to be a 32 bit `unsigned int'
  at the expense of not being able to handle requests greater than
  2^31. This limitation is hardly ever a concern; you are encouraged
  to set this. However, the default version is the same as size_t.
*/

#ifndef INTERNAL_SIZE_T
#define INTERNAL_SIZE_T size_t
#endif

/*
  REALLOC_ZERO_BYTES_FREES should be set if a call to
  realloc with zero bytes should be the same as a call to free.
  Some people think it should. Otherwise, since this malloc
  returns a unique pointer for malloc(0), so does realloc(p, 0). 
*/


/*   #define REALLOC_ZERO_BYTES_FREES */


/* 
  WIN32 causes an emulation of sbrk to be compiled in
  mmap-based options are not currently supported in WIN32.
*/

/* #define WIN32 */
#ifdef WIN32
#define MORECORE gmallocHeap::wsbrk
#define HAVE_MMAP 0
#endif


/*
  HAVE_MEMCPY should be defined if you are not otherwise using
  ANSI STD C, but still have memcpy and memset in your C library
  and want to use them in calloc and realloc. Otherwise simple
  macro versions are defined here.

  USE_MEMCPY should be defined as 1 if you actually want to
  have memset and memcpy called. People report that the macro
  versions are often enough faster than libc versions on many
  systems that it is better to use them. 

*/

#define HAVE_MEMCPY 

#ifndef USE_MEMCPY
#ifdef HAVE_MEMCPY
#define USE_MEMCPY 1
#else
#define USE_MEMCPY 0
#endif
#endif

#if (__STD_C || defined(HAVE_MEMCPY)) 

#if __STD_C

#else
Void_t* memset();
Void_t* memcpy();
#endif
#endif

#if USE_MEMCPY

/* The following macros are only invoked with (2n+1)-multiples of
   INTERNAL_SIZE_T units, with a positive integer n. This is exploited
   for fast inline execution when n is small. */

#define MALLOC_ZERO(charp, nbytes)                                            \
do {                                                                          \
  INTERNAL_SIZE_T mzsz = (nbytes);                                            \
  if(mzsz <= 9*sizeof(mzsz)) {                                                \
    INTERNAL_SIZE_T* mz = (INTERNAL_SIZE_T*) (charp);                         \
    if(mzsz >= 5*sizeof(mzsz)) {     *mz++ = 0;                               \
                                     *mz++ = 0;                               \
      if(mzsz >= 7*sizeof(mzsz)) {   *mz++ = 0;                               \
                                     *mz++ = 0;                               \
        if(mzsz >= 9*sizeof(mzsz)) { *mz++ = 0;                               \
                                     *mz++ = 0; }}}                           \
                                     *mz++ = 0;                               \
                                     *mz++ = 0;                               \
                                     *mz   = 0;                               \
  } else memset((charp), 0, mzsz);                                            \
} while(0)

#define MALLOC_COPY(dest,src,nbytes)                                          \
do {                                                                          \
  INTERNAL_SIZE_T mcsz = (nbytes);                                            \
  if(mcsz <= 9*sizeof(mcsz)) {                                                \
    INTERNAL_SIZE_T* mcsrc = (INTERNAL_SIZE_T*) (src);                        \
    INTERNAL_SIZE_T* mcdst = (INTERNAL_SIZE_T*) (dest);                       \
    if(mcsz >= 5*sizeof(mcsz)) {     *mcdst++ = *mcsrc++;                     \
                                     *mcdst++ = *mcsrc++;                     \
      if(mcsz >= 7*sizeof(mcsz)) {   *mcdst++ = *mcsrc++;                     \
                                     *mcdst++ = *mcsrc++;                     \
        if(mcsz >= 9*sizeof(mcsz)) { *mcdst++ = *mcsrc++;                     \
                                     *mcdst++ = *mcsrc++; }}}                 \
                                     *mcdst++ = *mcsrc++;                     \
                                     *mcdst++ = *mcsrc++;                     \
                                     *mcdst   = *mcsrc  ;                     \
  } else memcpy(dest, src, mcsz);                                             \
} while(0)

#else /* !USE_MEMCPY */

/* Use Duff's device for good zeroing/copying performance. */

#define MALLOC_ZERO(charp, nbytes)                                            \
do {                                                                          \
  INTERNAL_SIZE_T* mzp = (INTERNAL_SIZE_T*)(charp);                           \
  long mctmp = (nbytes)/sizeof(INTERNAL_SIZE_T), mcn;                         \
  if (mctmp < 8) mcn = 0; else { mcn = (mctmp-1)/8; mctmp %= 8; }             \
  switch (mctmp) {                                                            \
    case 0: for(;;) { *mzp++ = 0;                                             \
    case 7:           *mzp++ = 0;                                             \
    case 6:           *mzp++ = 0;                                             \
    case 5:           *mzp++ = 0;                                             \
    case 4:           *mzp++ = 0;                                             \
    case 3:           *mzp++ = 0;                                             \
    case 2:           *mzp++ = 0;                                             \
    case 1:           *mzp++ = 0; if(mcn <= 0) break; mcn--; }                \
  }                                                                           \
} while(0)

#define MALLOC_COPY(dest,src,nbytes)                                          \
do {                                                                          \
  INTERNAL_SIZE_T* mcsrc = (INTERNAL_SIZE_T*) src;                            \
  INTERNAL_SIZE_T* mcdst = (INTERNAL_SIZE_T*) dest;                           \
  long mctmp = (nbytes)/sizeof(INTERNAL_SIZE_T), mcn;                         \
  if (mctmp < 8) mcn = 0; else { mcn = (mctmp-1)/8; mctmp %= 8; }             \
  switch (mctmp) {                                                            \
    case 0: for(;;) { *mcdst++ = *mcsrc++;                                    \
    case 7:           *mcdst++ = *mcsrc++;                                    \
    case 6:           *mcdst++ = *mcsrc++;                                    \
    case 5:           *mcdst++ = *mcsrc++;                                    \
    case 4:           *mcdst++ = *mcsrc++;                                    \
    case 3:           *mcdst++ = *mcsrc++;                                    \
    case 2:           *mcdst++ = *mcsrc++;                                    \
    case 1:           *mcdst++ = *mcsrc++; if(mcn <= 0) break; mcn--; }       \
  }                                                                           \
} while(0)

#endif


/*
  Define HAVE_MMAP to optionally make malloc() use mmap() to
  allocate very large blocks.  These will be returned to the
  operating system immediately after a free().
*/

#ifndef HAVE_MMAP
#define HAVE_MMAP 1
#endif

/*
  Define HAVE_MREMAP to make realloc() use mremap() to re-allocate
  large blocks.  This is currently only possible on Linux with
  kernel versions newer than 1.3.77.
*/

#ifndef HAVE_MREMAP
#ifdef INTERNAL_LINUX_C_LIB
#define HAVE_MREMAP 1
#else
#define HAVE_MREMAP 0
#endif
#endif

#if HAVE_MMAP

#include <unistd.h>
#include <fcntl.h>
#include <sys/mman.h>

#if !defined(MAP_ANONYMOUS) && defined(MAP_ANON)
#define MAP_ANONYMOUS MAP_ANON
#endif

#endif /* HAVE_MMAP */

/*
  Access to system page size. To the extent possible, this malloc
  manages memory from the system in page-size units.
  
  The following mechanics for getpagesize were adapted from 
  bsd/gnu getpagesize.h 
*/

#ifndef WIN32
#ifndef LACKS_UNISTD_H
#  include <unistd.h>
#endif


#ifndef malloc_getpagesize
#  ifdef _SC_PAGESIZE         
#    ifndef _SC_PAGE_SIZE
#      define _SC_PAGE_SIZE _SC_PAGESIZE
#    endif
#  endif
#  ifdef _SC_PAGE_SIZE
#    define malloc_getpagesize sysconf(_SC_PAGE_SIZE)
#  else
#    if defined(BSD) || defined(DGUX) || defined(HAVE_GETPAGESIZE)
       extern size_t getpagesize();
#      define malloc_getpagesize getpagesize()
#    else
#      include <sys/param.h>
#      ifdef EXEC_PAGESIZE
#        define malloc_getpagesize EXEC_PAGESIZE
#      else
#        ifdef NBPG
#          ifndef CLSIZE
#            define malloc_getpagesize NBPG
#          else
#            define malloc_getpagesize (NBPG * CLSIZE)
#          endif
#        else 
#          ifdef NBPC
#            define malloc_getpagesize NBPC
#          else
#            ifdef PAGESIZE
#              define malloc_getpagesize PAGESIZE
#            else
#              define malloc_getpagesize (0x10000) 
#            endif
#          endif
#        endif 
#      endif
#    endif 
#  endif
#endif
#endif


/* The following mechanics for getpagesize were adapted from
  bsd/gnu getpagesize.h
*/

#if defined(_ALPHA_)
#define malloc_getpagesize (8192)
#else
#define malloc_getpagesize (0x10000)
#endif


/*

  This version of malloc supports the standard SVID/XPG mallinfo
  routine that returns a struct containing the same kind of
  information you can get from malloc_stats. It should work on
  any SVID/XPG compliant system that has a /usr/include/malloc.h
  defining struct mallinfo. (If you'd like to install such a thing
  yourself, cut out the preliminary declarations as described above
  and below and save them in a malloc.h file. But there's no
  compelling reason to bother to do this.)

  The main declaration needed is the mallinfo struct that is returned
  (by-copy) by mallinfo().  The SVID/XPG malloinfo struct contains a
  bunch of fields, most of which are not even meaningful in this
  version of malloc. Some of these fields are are instead filled by
  mallinfo() with other numbers that might possibly be of interest.

  HAVE_USR_INCLUDE_MALLOC_H should be set if you have a
  /usr/include/malloc.h file that includes a declaration of struct
  mallinfo.  If so, it is included; else an SVID2/XPG2 compliant
  version is declared below.  These must be precisely the same for
  mallinfo() to work.

*/

#define HAVE_USR_INCLUDE_MALLOC_H 1

#if HAVE_USR_INCLUDE_MALLOC_H
#include "malloc.h"
#else

/* SVID2/XPG mallinfo structure */

struct mallinfo {
  int arena;    /* total space allocated from system */
  int ordblks;  /* number of non-inuse chunks */
  int smblks;   /* unused -- always zero */
  int hblks;    /* number of mmapped regions */
  int hblkhd;   /* total space in mmapped regions */
  int usmblks;  /* unused -- always zero */
  int fsmblks;  /* unused -- always zero */
  int uordblks; /* total allocated space */
  int fordblks; /* total non-inuse space */
  int keepcost; /* top-most, releasable (via malloc_trim) space */
};	

/* SVID2/XPG mallopt options */

#define M_MXFAST  1    /* UNUSED in this malloc */
#define M_NLBLKS  2    /* UNUSED in this malloc */
#define M_GRAIN   3    /* UNUSED in this malloc */
#define M_KEEP    4    /* UNUSED in this malloc */

#endif

/* mallopt options that actually do something */

#define M_TRIM_THRESHOLD    -1
#define M_TOP_PAD           -2
#define M_MMAP_THRESHOLD    -3
#define M_MMAP_MAX          -4



#ifndef DEFAULT_TRIM_THRESHOLD

//#define DEFAULT_TRIM_THRESHOLD (128 * 1024)

// Turn off the trimming code.  It has a nasty bug w/ large allocations
// that causes us to lose large amounts of address space every time it
// trims.

#define DEFAULT_TRIM_THRESHOLD ((unsigned long)(-1))
#endif

/*
    M_TRIM_THRESHOLD is the maximum amount of unused top-most memory 
      to keep before releasing via malloc_trim in free().

      Automatic trimming is mainly useful in long-lived programs.
      Because trimming via sbrk can be slow on some systems, and can
      sometimes be wasteful (in cases where programs immediately
      afterward allocate more large chunks) the value should be high
      enough so that your overall system performance would improve by
      releasing.  

      The trim threshold and the mmap control parameters (see below)
      can be traded off with one another. Trimming and mmapping are
      two different ways of releasing unused memory back to the
      system. Between these two, it is often possible to keep
      system-level demands of a long-lived program down to a bare
      minimum. For example, in one test suite of sessions measuring
      the XF86 X server on Linux, using a trim threshold of 128K and a
      mmap threshold of 192K led to near-minimal long term resource
      consumption.  

      If you are using this malloc in a long-lived program, it should
      pay to experiment with these values.  As a rough guide, you
      might set to a value close to the average size of a process
      (program) running on your system.  Releasing this much memory
      would allow such a process to run in memory.  Generally, it's
      worth it to tune for trimming rather tham memory mapping when a
      program undergoes phases where several large chunks are
      allocated and released in ways that can reuse each other's
      storage, perhaps mixed with phases where there are no such
      chunks at all.  And in well-behaved long-lived programs,
      controlling release of large blocks via trimming versus mapping
      is usually faster.

      However, in most programs, these parameters serve mainly as
      protection against the system-level effects of carrying around
      massive amounts of unneeded memory. Since frequent calls to
      sbrk, mmap, and munmap otherwise degrade performance, the default
      parameters are set to relatively high values that serve only as
      safeguards.

      The default trim value is high enough to cause trimming only in
      fairly extreme (by current memory consumption standards) cases.
      It must be greater than page size to have any useful effect.  To
      disable trimming completely, you can set to (unsigned long)(-1);


*/

#ifndef SHAVE_TOP_THRESHOLD
#define SHAVE_TOP_THRESHOLD (1024*1024*4)
#endif

/*
    SHAVE_TOP_THRESHOLD is used in malloc_extend_top. Normally, this function 
    extends the top of the heap by nb bytes, where nb is the size of the new
    allocation. But when nb is large, this can lead to waste because we may
    need to allocate a new non-contiguous region and put top's chunk on the
    free list. It would be better to allocate (nb - top_size) bytes to avoid this.
    This feature is enabled for allocation requests of SHAVE_TOP_THRESHOLD
    or greater, initially set to 4MB.
*/

#ifndef DEFAULT_TOP_PAD
#define DEFAULT_TOP_PAD        (0)
#endif

/*
    M_TOP_PAD is the amount of extra `padding' space to allocate or 
      retain whenever sbrk is called. It is used in two ways internally:

      * When sbrk is called to extend the top of the arena to satisfy
        a new malloc request, this much padding is added to the sbrk
        request.

      * When malloc_trim is called automatically from free(),
        it is used as the `pad' argument.

      In both cases, the actual amount of padding is rounded 
      so that the end of the arena is always a system page boundary.

      The main reason for using padding is to avoid calling sbrk so
      often. Having even a small pad greatly reduces the likelihood
      that nearly every malloc request during program start-up (or
      after trimming) will invoke sbrk, which needlessly wastes
      time. 

      Automatic rounding-up to page-size units is normally sufficient
      to avoid measurable overhead, so the default is 0.  However, in
      systems where sbrk is relatively slow, it can pay to increase
      this value, at the expense of carrying around more memory than 
      the program needs.

*/


#ifndef DEFAULT_MMAP_THRESHOLD
#define DEFAULT_MMAP_THRESHOLD (128 * 1024)
#endif

/*

    M_MMAP_THRESHOLD is the request size threshold for using mmap() 
      to service a request. Requests of at least this size that cannot 
      be allocated using already-existing space will be serviced via mmap.  
      (If enough normal freed space already exists it is used instead.)

      Using mmap segregates relatively large chunks of memory so that
      they can be individually obtained and released from the host
      system. A request serviced through mmap is never reused by any
      other request (at least not directly; the system may just so
      happen to remap successive requests to the same locations).

      Segregating space in this way has the benefit that mmapped space
      can ALWAYS be individually released back to the system, which
      helps keep the system level memory demands of a long-lived
      program low. Mapped memory can never become `locked' between
      other chunks, as can happen with normally allocated chunks, which
      menas that even trimming via malloc_trim would not release them.

      However, it has the disadvantages that:

         1. The space cannot be reclaimed, consolidated, and then
            used to service later requests, as happens with normal chunks. 
         2. It can lead to more wastage because of mmap page alignment
            requirements
         3. It causes malloc performance to be more dependent on host
            system memory management support routines which may vary in
            implementation quality and may impose arbitrary
            limitations. Generally, servicing a request via normal
            malloc steps is faster than going through a system's mmap.

      All together, these considerations should lead you to use mmap
      only for relatively large requests.  


*/



#ifndef DEFAULT_MMAP_MAX
#if HAVE_MMAP
#define DEFAULT_MMAP_MAX       (64)
#else
#define DEFAULT_MMAP_MAX       (0)
#endif
#endif

/*
    M_MMAP_MAX is the maximum number of requests to simultaneously 
      service using mmap. This parameter exists because:

         1. Some systems have a limited number of internal tables for
            use by mmap.
         2. In most systems, overreliance on mmap can degrade overall
            performance.
         3. If a program allocates many large regions, it is probably
            better off using normal sbrk-based allocation routines that
            can reclaim and reallocate normal heap memory. Using a
            small value allows transition into this mode after the
            first few allocations.

      Setting to 0 disables all use of mmap.  If HAVE_MMAP is not set,
      the default value is 0, and attempts to set it to non-zero values
      in mallopt will fail.
*/




/* 

  Special defines for linux libc

  Except when compiled using these special defines for Linux libc
  using weak aliases, this malloc is NOT designed to work in
  multithreaded applications.  No semaphores or other concurrency
  control are provided to ensure that multiple malloc or free calls
  don't run at the same time, which could be disasterous. A single
  semaphore could be used across malloc, realloc, and free (which is
  essentially the effect of the linux weak alias approach). It would
  be hard to obtain finer granularity.

*/
#ifndef Win32LocalAlloc
#define Win32LocalAlloc LocalAlloc
#endif

#ifndef Win32LocalFree
#define Win32LocalFree LocalFree
#endif

#ifdef INTERNAL_LINUX_C_LIB

#if __STD_C

Void_t * __default_morecore_init (ptrdiff_t);
Void_t *(*__morecore)(ptrdiff_t) = __default_morecore_init;

#else

Void_t * __default_morecore_init ();
Void_t *(*__morecore)() = __default_morecore_init;

#endif

#define MORECORE (*__morecore)
// #define MORECORE_FAILURE 0
#define MORECORE_FAILURE -1
#define MORECORE_CLEARS 0 

#else /* INTERNAL_LINUX_C_LIB */

#if __STD_C
extern Void_t*     sbrk(ptrdiff_t);
#else
extern Void_t*     sbrk();
#endif

#ifndef MORECORE
// void* wsbrk (long size);
void* wsbrk (long ContigSize, unsigned long& AppendSize);
#define MORECORE wsbrk
#endif

#ifndef MORECORE_FAILURE
#define MORECORE_FAILURE -1
#endif

#ifndef MORECORE_CLEARS
#define MORECORE_CLEARS 0
#endif

#endif /* INTERNAL_LINUX_C_LIB */

#if defined(INTERNAL_LINUX_C_LIB) && defined(__ELF__)

#define cALLOc		__libc_calloc
#define fREe		__libc_free
#define mALLOc		__libc_malloc
#define mEMALIGn	__libc_memalign
#define rEALLOc		__libc_realloc
#define vALLOc		__libc_valloc
#define pvALLOc		__libc_pvalloc
#define mALLINFo	__libc_mallinfo
#define mALLOPt		__libc_mallopt

#pragma weak calloc = __libc_calloc
#pragma weak free = __libc_free
#pragma weak cfree = __libc_free
#pragma weak malloc = __libc_malloc
#pragma weak memalign = __libc_memalign
#pragma weak realloc = __libc_realloc
#pragma weak valloc = __libc_valloc
#pragma weak pvalloc = __libc_pvalloc
#pragma weak mallinfo = __libc_mallinfo
#pragma weak mallopt = __libc_mallopt

#else

#define cALLOc      gmallocHeap::gcalloc
#define fREe        gmallocHeap::gfree
#define mALLOc      gmallocHeap::gmalloc
#define mEMALIGn    gmallocHeap::gmemalign
#define rEALLOc     gmallocHeap::grealloc
#define vALLOc      gmallocHeap::gvalloc
#define mALLINFo    gmallocHeap::gmallinfo
#define mALLOPt     gmallocHeap::gmallopt

#endif

/* Public routines */

#if !__STD_C
Void_t* mALLOc();
void    fREe();
Void_t* rEALLOc();
Void_t* mEMALIGn();
Void_t* vALLOc();
Void_t* pvALLOc();
Void_t* cALLOc();
void    cfree();
int     malloc_trim();
size_t  malloc_usable_size();
void    malloc_stats();
int     mALLOPt();
struct mallinfo mALLINFo();
#endif


#ifdef __cplusplus
};  /* end of extern "C" */
#endif

/* ---------- To make a malloc.h, end cutting here ------------ */


/* 
  Emulation of sbrk for WIN32
  All code within the ifdef WIN32 is untested by me.
*/


#ifdef WIN32


#define MEM_WRITE_WATCH 0x200000  

DWORD mem_reserve = (MEM_RESERVE | MEM_WRITE_WATCH);

#define AlignPage(add) (((size_t)(add) + (malloc_getpagesize-1)) &~(malloc_getpagesize-1))

/* resrve 64MB to insure large contiguous space */ 
#define RESERVED_SIZE (1024*1024*64)
#define NEXT_SIZE (2048*1024)

struct GmListElement;
typedef struct GmListElement GmListElement;

struct GmListElement 
{
	GmListElement* next;
	void* base;
};

GmListElement* gmallocHeap::makeGmListElement (void* bas)
{
	GmListElement* mthis;
	mthis = (GmListElement*)(void*)LocalAlloc (0, sizeof (GmListElement));
	ASSERT (mthis);
	if (mthis)
	{
		mthis->base = bas;
		mthis->next = head;
		head = mthis;
	}
	return mthis;
}

void gmallocHeap::gcleanup ()
{
	BOOL rval;
	ASSERT ( (head == NULL) || (head->base == gAddressBase));
	if (gAddressBase && ((size_t)gNextAddress - (size_t)gAddressBase))
	{
		rval = VirtualFree (gAddressBase, 
							(size_t)gNextAddress - (size_t)gAddressBase, 
							MEM_DECOMMIT);
        ASSERT (rval);
	}
	while (head)
	{
		GmListElement* next = head->next;
		rval = VirtualFree (head->base, 0, MEM_RELEASE);
		ASSERT (rval);
		LocalFree (head);
		head = next;
	}
}
		
static
void* findRegion (void* start_address, unsigned long size)
{
    if (size >= (size_t)TOP_MEMORY)
        return NULL;
    MEMORY_BASIC_INFORMATION info;
    while (((size_t)start_address + size) < (size_t)TOP_MEMORY)
    {
        VirtualQuery (start_address, &info, sizeof (info));
        if (info.State != MEM_FREE)
            start_address = (char*)info.BaseAddress + info.RegionSize;
        else if (info.RegionSize >= (size + ((0x10000 - ((size_t)start_address & 0xFFFF)) & 0xFFFF)))
            return (void*)(((size_t)start_address + 0xFFFF) & ~0xFFFF); //align on 64K
        else
            start_address = (char*)info.BaseAddress + info.RegionSize;
    }
    return NULL;

}

void* gmallocHeap::wsbrk (long ContigSize, unsigned long& AppendSize)
{
    void* tmp;
    if (ContigSize > 0)
    {
        void* prev_base = gAddressBase;
        void* prev_next = gNextAddress;
        size_t prev_size = gAllocatedSize;
        BOOL ReservedContig = FALSE;
        unsigned long CandidateSize = AppendSize > 0 ? AppendSize : ContigSize;                
        long new_size = (long)gAllocatedSize;

        // Note: the initial allocation will be forced to be as low as possible by falling into the loop below and using
        // findRegion.
        if (gAddressBase == 0)
        {
            assert (gPreVirtualAllocHook == 0);

            new_size = max((long)gInitialReserve, (long)AlignPage (ContigSize));
            gAllocatedSize = 0;

			//TODO: AddName??
            /*if (gName)
                AddName(gAddressBase, gName);*/
        }

        if (AlignPage ((size_t)gNextAddress + CandidateSize) > ((size_t)gAddressBase + gAllocatedSize))
        {            
            // can't add another segment if client requested fixed heap

            if ((gmFlags & GM_FIXED_HEAP) && (gAddressBase != 0))
                return (void*)-1;

            // Our allocation should be large enough for ContigSize, though if we can reserve contiguously to 
            // gNextAddress, we'll only commit AppendSize
            new_size = max (new_size, (long)AlignPage (ContigSize));
            // Better to reserve larger chunks.
            new_size = max(new_size,AlignPage(1024*1024*64));                        
            void* new_address = (void*)((size_t)gAddressBase+gAllocatedSize);            
            void* saved_new_address = new_address;            
            ptrdiff_t delta = 0;

            //check if it is OK to reserve more memory
            if (gPreVirtualAllocHook != 0)
                if ((gPreVirtualAllocHook) ((size_t)new_size))
                    return (void*)-1;
            do
            {
                new_address = findRegion (new_address, new_size);

                if (new_address == 0)
                    return (void*)-1;

#ifdef MAP_VIEW
				if (gVirtualAllocedHook != 0)
				{
					gAddressBase = 0;
					HANDLE hf = CreateFileMapping ((HANDLE)~0u, NULL, 
												   PAGE_READWRITE | SEC_RESERVE,
												   0, new_size, 0);
					if (!hf)
						return (void*)-1;
					void* prgmem = MapViewOfFileEx (hf, FILE_MAP_WRITE, 0, 0, 0, new_address);
					if (!prgmem)
						continue;

					// allocate the read only view 
					void* gcmem = MapViewOfFile (hf, FILE_MAP_WRITE, 0, 0, 0);

					if (!gcmem)
						return (void*)-1;

					delta = (BYTE*)gcmem - (BYTE*)prgmem;
					gAddressBase = gNextAddress =
						(unsigned int) prgmem;
				}
				else

#elif defined (ALIAS_MEM)
				if (gVirtualAllocedHook != 0)
				{
					gAddressBase = 0;
					void* prgmem = VirtualAlloc (new_address, new_size,
												 MEM_RESERVE, PAGE_NOACCESS);
					if (!prgmem)
						continue;
                    
                    WS_PERF_LOG_PAGE_RANGE((void*)0x2, prgmem, (unsigned char *)prgmem + new_size - OS_PAGE_SIZE, new_size);

					void* gcmem = VirtualAlloc (NULL, new_size,
												MEM_RESERVE, PAGE_NOACCESS);
					if (!gcmem)
						return (void*)-1;
                    
                    WS_PERF_LOG_PAGE_RANGE((void*)0x2, gcmem, (unsigned char *)gcmem + new_size - OS_PAGE_SIZE, new_size);

					delta = (BYTE*)gcmem - (BYTE*)prgmem;

					gAddressBase = gNextAddress =
						(unsigned int) prgmem;
				}
				else
#endif //MAP_VIEW
				{
					gAddressBase = 
						VirtualAlloc (new_address, new_size,
						              mem_reserve, PAGE_NOACCESS);
                    
                    WS_PERF_LOG_PAGE_RANGE((void*)0x2, gAddressBase, (void*)((size_t)gAddressBase + new_size - OS_PAGE_SIZE), new_size);
					
                    if ((gAddressBase == 0) && 
						(GetLastError() != ERROR_INVALID_ADDRESS) &&
						mem_reserve == (MEM_RESERVE | MEM_WRITE_WATCH))
					{
						int errc = GetLastError();
						ASSERT ((ERROR_NOT_SUPPORTED == errc )|| 
							    (ERROR_INVALID_PARAMETER == errc));
						mem_reserve = MEM_RESERVE;
						gAddressBase = 
							VirtualAlloc (new_address, new_size,
							              mem_reserve, PAGE_NOACCESS);
                        
                        WS_PERF_LOG_PAGE_RANGE((void*)0x2, gAddressBase, (void*)((size_t)gAddressBase + new_size - OS_PAGE_SIZE), new_size);
					}
				}

                // repeat in case of race condition
                // The region that we found has been snagged
                // by another thread
            }
            while (gAddressBase == 0);


            ASSERT (new_address == gAddressBase);

            gAllocatedSize = new_size;

            if(new_address != saved_new_address)
            {                
                CandidateSize = ContigSize; // Since we are not contiguous, we need ContigSize commitment
                gNextAddress = gAddressBase;
            }            
            else
            {
                ReservedContig = TRUE;
            }
            
            // invoke the virtual allocation hook if present
            if (gVirtualAllocedHook != 0)
                if ((gVirtualAllocedHook) ((BYTE*)gAddressBase, (size_t)gAllocatedSize, delta))
                {
                    //restore old values
                    gAddressBase = prev_base;
                    gNextAddress = prev_next;
                    gAllocatedSize = (DWORD)prev_size;
                    return (void*)-1;
                }

            if (!makeGmListElement (gAddressBase))
            {
                //restore old values
                gAddressBase = prev_base;
                gNextAddress = prev_next;
                gAllocatedSize = (DWORD)prev_size;

                return (void*)-1;
            }

            //TODO: AddName??
            /*if (gName)
                AddName(gAddressBase, gName);*/
        }
        if ((CandidateSize + (size_t)gNextAddress) > AlignPage (gNextAddress))
        {
            void* res;
            if(ReservedContig)
            {
                size_t RegionOneSection = (size_t)gAddressBase - (size_t)gNextAddress; 
                if( RegionOneSection > 0 )
                {
                    res = VirtualAlloc((void*)AlignPage(gNextAddress),
                                            AlignPage(RegionOneSection),
                                            MEM_COMMIT, PAGE_READWRITE);
                    if (res == 0)
                        return (void*)-1;
                }

                res = VirtualAlloc((void*)AlignPage(gAddressBase),
                                        AlignPage(CandidateSize - RegionOneSection),
                                        MEM_COMMIT, PAGE_READWRITE);
            }
            else
            {
                res = VirtualAlloc ((void*)AlignPage (gNextAddress),
                                     AlignPage(CandidateSize),
                                    MEM_COMMIT, PAGE_READWRITE);
            }
            
            if (res == 0)
                return (void*)-1;            
            
            WS_PERF_LOG_PAGE_RANGE((void*)0x2, res, (unsigned char *)res + (CandidateSize + (size_t)gNextAddress - AlignPage (gNextAddress))- OS_PAGE_SIZE, (CandidateSize + (size_t)gNextAddress - AlignPage (gNextAddress)));
        }
        tmp = gNextAddress;
        gNextAddress = (void*)((size_t)tmp + CandidateSize);
        
        // Store actual commitment for caller
        if(AppendSize)
            AppendSize = CandidateSize;
        
        return tmp;
    }
    else if (ContigSize < 0)
    {
        void* alignedGoal = (void*)AlignPage ((size_t)gNextAddress + ContigSize);
        /* Trim by releasing the virtual memory */
        if (alignedGoal >= gAddressBase)
        {
            VirtualFree (alignedGoal, (size_t)gNextAddress - (size_t)alignedGoal,
                         MEM_DECOMMIT);

            gNextAddress = (void*)((size_t)gNextAddress + ContigSize);
            return gNextAddress;
        }
        else
        {
            VirtualFree (gAddressBase, (size_t)gNextAddress - (size_t)gAddressBase,
                         MEM_DECOMMIT);
            gNextAddress = gAddressBase;
            return (void*)-1;
        }
    }
    else
    {
        return gNextAddress;
    }
}


#endif



/*
  Type declarations
*/


struct malloc_chunk
{
  INTERNAL_SIZE_T prev_size; /* Size of previous chunk (if free). */
  INTERNAL_SIZE_T size;      /* Size in bytes, including overhead. */
  struct malloc_chunk* fd;   /* double links -- used only if free. */
  struct malloc_chunk* bk;
};

typedef struct malloc_chunk* mchunkptr;

/*

   malloc_chunk details:

    (The following includes lightly edited explanations by Colin Plumb.)

    Chunks of memory are maintained using a `boundary tag' method as
    described in e.g., Knuth or Standish.  (See the paper by Paul
    Wilson ftp://ftp.cs.utexas.edu/pub/garbage/allocsrv.ps for a
    survey of such techniques.)  Sizes of free chunks are stored both
    in the front of each chunk and at the end.  This makes
    consolidating fragmented chunks into bigger chunks very fast.  The
    size fields also hold bits representing whether chunks are free or
    in use.

    An allocated chunk looks like this:  


    chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Size of previous chunk, if allocated            | |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Size of chunk, in bytes                         |P|
      mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             User data starts here...                          .
            .                                                               .
            .             (malloc_usable_space() bytes)                     .
            .                                                               |
nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Size of chunk                                     |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+


    Where "chunk" is the front of the chunk for the purpose of most of
    the malloc code, but "mem" is the pointer that is returned to the
    user.  "Nextchunk" is the beginning of the next contiguous chunk.

    Chunks always begin on even word boundries, so the mem portion
    (which is returned to the user) is also on an even word boundary, and
    thus double-word aligned.

    Free chunks are stored in circular doubly-linked lists, and look like this:

    chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Size of previous chunk                            |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    `head:' |             Size of chunk, in bytes                         |P|
      mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Forward pointer to next chunk in list             |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Back pointer to previous chunk in list            |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Unused space (may be 0 bytes long)                .
            .                                                               .
            .                                                               |
nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    `foot:' |             Size of chunk, in bytes                           |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    The P (PREV_INUSE) bit, stored in the unused low-order bit of the
    chunk size (which is always a multiple of two words), is an in-use
    bit for the *previous* chunk.  If that bit is *clear*, then the
    word before the current chunk size contains the previous chunk
    size, and can be used to find the front of the previous chunk.
    (The very first chunk allocated always has this bit set,
    preventing access to non-existent (or non-owned) memory.)

    Note that the `foot' of the current chunk is actually represented
    as the prev_size of the NEXT chunk. (This makes it easier to
    deal with alignments etc).

    The two exceptions to all this are 

     1. The special chunk `top', which doesn't bother using the 
        trailing size field since there is no
        next contiguous chunk that would have to index off it. (After
        initialization, `top' is forced to always exist.  If it would
        become less than MINSIZE bytes long, it is replenished via
        malloc_extend_top.)

     2. Chunks allocated via mmap, which have the second-lowest-order
        bit (IS_MMAPPED) set in their size fields.  Because they are
        never merged or traversed from any other chunk, they have no
        foot size or inuse information.

    Available chunks are kept in any of several places (all declared below):

    * `av': An array of chunks serving as bin headers for consolidated
       chunks. Each bin is doubly linked.  The bins are approximately
       proportionally (log) spaced.  There are a lot of these bins
       (128). This may look excessive, but works very well in
       practice.  All procedures maintain the invariant that no
       consolidated chunk physically borders another one. Chunks in
       bins are kept in size order, with ties going to the
       approximately least recently used chunk.

       The chunks in each bin are maintained in decreasing sorted order by
       size.  This is irrelevant for the small bins, which all contain
       the same-sized chunks, but facilitates best-fit allocation for
       larger chunks. (These lists are just sequential. Keeping them in
       order almost never requires enough traversal to warrant using
       fancier ordered data structures.)  Chunks of the same size are
       linked with the most recently freed at the front, and allocations
       are taken from the back.  This results in LRU or FIFO allocation
       order, which tends to give each chunk an equal opportunity to be
       consolidated with adjacent freed chunks, resulting in larger free
       chunks and less fragmentation. 

    * `top': The top-most available chunk (i.e., the one bordering the
       end of available memory) is treated specially. It is never
       included in any bin, is used only if no other chunk is
       available, and is released back to the system if it is very
       large (see M_TRIM_THRESHOLD).

    * `last_remainder': A bin holding only the remainder of the
       most recently split (non-top) chunk. This bin is checked
       before other non-fitting chunks, so as to provide better
       locality for runs of sequentially allocated chunks. 

    *  Implicitly, through the host system's memory mapping tables.
       If supported, requests greater than a threshold are usually 
       serviced via calls to mmap, and then later released via munmap.

*/






/*  sizes, alignments */

#define SIZE_SZ                (sizeof(INTERNAL_SIZE_T))
#define MALLOC_ALIGNMENT       (SIZE_SZ + SIZE_SZ)
#define MALLOC_ALIGN_MASK      (MALLOC_ALIGNMENT - 1)
#define MINSIZE                (sizeof(struct malloc_chunk))

/* conversion from malloc headers to user pointers, and back */

#define chunk2mem(p)   ((Void_t*)((char*)(p) + 2*SIZE_SZ))
#define mem2chunk(mem) ((mchunkptr)((char*)(mem) - 2*SIZE_SZ))

/* pad request bytes into a usable size */

#define request2size(req) \
 (((long)((req) + (SIZE_SZ + MALLOC_ALIGN_MASK)) < \
  (long)(MINSIZE + MALLOC_ALIGN_MASK)) ? MINSIZE : \
   (((req) + (SIZE_SZ + MALLOC_ALIGN_MASK)) & ~(MALLOC_ALIGN_MASK)))

/* Check if m has acceptable alignment */

#define aligned_OK(m)    (((unsigned long)((m)) & (MALLOC_ALIGN_MASK)) == 0)




/* 
  Physical chunk operations  
*/


/* size field is or'ed with PREV_INUSE when previous adjacent chunk in use */

#define PREV_INUSE 0x1 

/* size field is or'ed with IS_MMAPPED if the chunk was obtained with mmap() */

#define IS_MMAPPED 0x2

/* Bits to mask off when extracting size */

#define SIZE_BITS (PREV_INUSE|IS_MMAPPED)


/* Ptr to next physical malloc_chunk. */

#define next_chunk(p) ((mchunkptr)( ((char*)(p)) + ((p)->size & ~PREV_INUSE) ))

/* Ptr to previous physical malloc_chunk */

#define prev_chunk(p)\
   ((mchunkptr)( ((char*)(p)) - ((p)->prev_size) ))


/* Treat space at ptr + offset as a chunk */

#define chunk_at_offset(p, s)  ((mchunkptr)(((char*)(p)) + (s)))




/* 
  Dealing with use bits 
*/

/* extract p's inuse bit */

#define inuse(p)\
((((mchunkptr)(((char*)(p))+((p)->size & ~PREV_INUSE)))->size) & PREV_INUSE)

/* extract inuse bit of previous chunk */

#define prev_inuse(p)  ((p)->size & PREV_INUSE)

/* check for mmap()'ed chunk */

#define chunk_is_mmapped(p) ((p)->size & IS_MMAPPED)

/* set/clear chunk as in use without otherwise disturbing */

#define set_inuse(p)\
((mchunkptr)(((char*)(p)) + ((p)->size & ~PREV_INUSE)))->size |= PREV_INUSE

#define clear_inuse(p)\
((mchunkptr)(((char*)(p)) + ((p)->size & ~PREV_INUSE)))->size &= ~(PREV_INUSE)

/* check/set/clear inuse bits in known places */

#define inuse_bit_at_offset(p, s)\
 (((mchunkptr)(((char*)(p)) + (s)))->size & PREV_INUSE)

#define set_inuse_bit_at_offset(p, s)\
 (((mchunkptr)(((char*)(p)) + (s)))->size |= PREV_INUSE)

#define clear_inuse_bit_at_offset(p, s)\
 (((mchunkptr)(((char*)(p)) + (s)))->size &= ~(PREV_INUSE))




/* 
  Dealing with size fields 
*/

/* Get size, ignoring use bits */

#define chunksize(p)          ((p)->size & ~(SIZE_BITS))

/* Set size at head, without disturbing its use bit */

#define set_head_size(p, s)   ((p)->size = (((p)->size & PREV_INUSE) | (s)))

/* Set size/use ignoring previous bits in header */

#define set_head(p, s)        ((p)->size = (s))

/* Set size at footer (only when chunk is not in use) */

#define set_foot(p, s)   (((mchunkptr)((char*)(p) + (s)))->prev_size = (s))





/*
   Bins

    The bins, `av_' are an array of pairs of pointers serving as the
    heads of (initially empty) doubly-linked lists of chunks, laid out
    in a way so that each pair can be treated as if it were in a
    malloc_chunk. (This way, the fd/bk offsets for linking bin heads
    and chunks are the same).

    Bins for sizes < 512 bytes contain chunks of all the same size, spaced
    8 bytes apart. Larger bins are approximately logarithmically
    spaced. (See the table below.) The `av_' array is never mentioned
    directly in the code, but instead via bin access macros.

    Bin layout:

    64 bins of size       8
    32 bins of size      64
    16 bins of size     512
     8 bins of size    4096
     4 bins of size   32768
     2 bins of size  262144
     1 bin  of size what's left

    There is actually a little bit of slop in the numbers in bin_index
    for the sake of speed. This makes no difference elsewhere.

    The special chunks `top' and `last_remainder' get their own bins,
    (this is implemented via yet more trickery with the av_ array),
    although `top' is never properly linked to its bin since it is
    always handled specially.

*/


/* access macros */

#define bin_at(i)      ((mbinptr)((char*)&(av_[2*(i) + 2]) - 2*SIZE_SZ))
#define next_bin(b)    ((mbinptr)((char*)(b) + 2 * sizeof(mbinptr)))
#define prev_bin(b)    ((mbinptr)((char*)(b) - 2 * sizeof(mbinptr)))

/*
   The first 2 bins are never indexed. The corresponding av_ cells are instead
   used for bookkeeping. This is not to save space, but to simplify
   indexing, maintain locality, and avoid some initialization tests.
*/

#define top            (bin_at(0)->fd)   /* The topmost chunk */
#define last_remainder (bin_at(1))       /* remainder from last split */


/*
   Because top initially points to its own bin with initial
   zero size, thus forcing extension on the first malloc request, 
   we avoid having any special code in malloc to check whether 
   it even exists yet. But we still need to in malloc_extend_top.
*/

#define initial_top    ((mchunkptr)(bin_at(0)))



/* field-extraction macros */

#define first(b) ((b)->fd)
#define last(b)  ((b)->bk)

/* 
  Indexing into bins
*/

#define bin_index(sz)                                                          \
(((((unsigned long)(sz)) >> 9) ==    0) ?       (((unsigned long)(sz)) >>  3): \
 ((((unsigned long)(sz)) >> 9) <=    4) ?  56 + (((unsigned long)(sz)) >>  6): \
 ((((unsigned long)(sz)) >> 9) <=   20) ?  91 + (((unsigned long)(sz)) >>  9): \
 ((((unsigned long)(sz)) >> 9) <=   84) ? 110 + (((unsigned long)(sz)) >> 12): \
 ((((unsigned long)(sz)) >> 9) <=  340) ? 119 + (((unsigned long)(sz)) >> 15): \
 ((((unsigned long)(sz)) >> 9) <= 1364) ? 124 + (((unsigned long)(sz)) >> 18): \
                                          126)                     
/* 
  bins for chunks < 512 are all spaced 8 bytes apart, and hold
  identically sized chunks. This is exploited in malloc.
*/

#define MAX_SMALLBIN         63
#define MAX_SMALLBIN_SIZE   512
#define SMALLBIN_WIDTH        8

#define smallbin_index(sz)  (((unsigned long)(sz)) >> 3)

/* 
   Requests are `small' if both the corresponding and the next bin are small
*/

#define is_small_request(nb) (nb < MAX_SMALLBIN_SIZE - SMALLBIN_WIDTH)



/*
    To help compensate for the large number of bins, a one-level index
    structure is used for bin-by-bin searching.  `binblocks' is a
    one-word bitvector recording whether groups of BINBLOCKWIDTH bins
    have any (possibly) non-empty bins, so they can be skipped over
    all at once during during traversals. The bits are NOT always
    cleared as soon as all bins in a block are empty, but instead only
    when all are noticed to be empty during traversal in malloc.
*/

#define BINBLOCKWIDTH     4   /* bins per block */

#define binblocks      (bin_at(0)->size) /* bitvector of nonempty blocks */

/* bin<->block macros */

#define idx2binblock(ix)    ((unsigned)1 << (ix / BINBLOCKWIDTH))
#define mark_binblock(ii)   (binblocks |= idx2binblock(ii))
#define clear_binblock(ii)  (binblocks &= ~(idx2binblock(ii)))



/* The total memory obtained from system via sbrk */
#define sbrked_mem  (current_mallinfo.arena)

/* Tracking mmaps */




/* 
  Debugging support 
*/

#if DEBUG


/*
  These routines make a number of assertions about the states
  of data structures that should be true at all times. If any
  are not true, it's very likely that a user program has somehow
  trashed memory. (It's also possible that there is a coding error
  in malloc. In which case, please report it!)
*/

static void gmallocHeap::do_check_chunk(mchunkptr p) 
{ 
  INTERNAL_SIZE_T sz = p->size & ~PREV_INUSE;

  /* No checkable chunk is mmapped */
  assert(!chunk_is_mmapped(p));

  /* Check for legal address ... */
  assert((char*)p >= sbrk_base);
  if (p != top) 
    assert((char*)p + sz <= (char*)top);
  else
    assert((char*)p + sz <= sbrk_base + sbrked_mem);

}


void gmallocHeap::do_check_free_chunk(mchunkptr p)
{
  INTERNAL_SIZE_T sz = p->size & ~PREV_INUSE;
  mchunkptr next = chunk_at_offset(p, sz);

  do_check_chunk(p);

  /* Check whether it claims to be free ... */
  assert(!inuse(p));

  /* Unless a special marker, must have OK fields */
  if ((long)sz >= (long)MINSIZE)
  {
    assert((sz & MALLOC_ALIGN_MASK) == 0);
    assert(aligned_OK(chunk2mem(p)));
    /* ... matching footer field */
    assert(next->prev_size == sz);
    /* ... and is fully consolidated */
    assert(prev_inuse(p));
    assert (next == top || inuse(next));
    
    /* ... and has minimally sane links */
    assert(p->fd->bk == p);
    assert(p->bk->fd == p);
  }
  else /* markers are always of size SIZE_SZ */
    assert(sz == SIZE_SZ); 
}

void gmallocHeap::do_check_inuse_chunk(mchunkptr p)
{
  mchunkptr next = next_chunk(p);
  do_check_chunk(p);

  /* Check whether it claims to be in use ... */
  assert(inuse(p));

  /* ... and is surrounded by OK chunks.
    Since more things can be checked with free chunks than inuse ones,
    if an inuse chunk borders them and debug is on, it's worth doing them.
  */
  if (!prev_inuse(p)) 
  {
    mchunkptr prv = prev_chunk(p);
    assert(next_chunk(prv) == p);
    do_check_free_chunk(prv);
  }
  if (next == top)
  {
    assert(prev_inuse(next));
    assert(chunksize(next) >= MINSIZE);
  }
  else if (!inuse(next))
    do_check_free_chunk(next);

}

void gmallocHeap::do_check_malloced_chunk(mchunkptr p, INTERNAL_SIZE_T s)
{
  INTERNAL_SIZE_T sz = p->size & ~PREV_INUSE;
  long room = sz - s;

  do_check_inuse_chunk(p);

  /* Legal size ... */
  assert((long)sz >= (long)MINSIZE);
  assert((sz & MALLOC_ALIGN_MASK) == 0);
  assert(room >= 0);
  assert(room < (long)MINSIZE);

  /* ... and alignment */
  assert(aligned_OK(chunk2mem(p)));


  /* ... and was allocated at front of an available chunk */
  assert(prev_inuse(p));

}


#define check_free_chunk(P)  do_check_free_chunk(P)
#define check_inuse_chunk(P) do_check_inuse_chunk(P)
#define check_chunk(P) do_check_chunk(P)
#define check_malloced_chunk(P,N) do_check_malloced_chunk(P,N)
#else
#define check_free_chunk(P) 
#define check_inuse_chunk(P)
#define check_chunk(P)
#define check_malloced_chunk(P,N)
#endif



/* 
  Macro-based internal utilities
*/


/*  
  Linking chunks in bin lists.
  Call these only with variables, not arbitrary expressions, as arguments.
*/

/* 
  Place chunk p of size s in its bin, in size order,
  putting it ahead of others of same size.
*/


#define frontlink(P, S, IDX, BK, FD)                                          \
{                                                                             \
  if (S < MAX_SMALLBIN_SIZE)                                                  \
  {                                                                           \
    IDX = smallbin_index(S);                                                  \
    mark_binblock(IDX);                                                       \
    BK = bin_at(IDX);                                                         \
    FD = BK->fd;                                                              \
    P->bk = BK;                                                               \
    P->fd = FD;                                                               \
    FD->bk = BK->fd = P;                                                      \
  }                                                                           \
  else                                                                        \
  {                                                                           \
    IDX = bin_index(S);                                                       \
    BK = bin_at(IDX);                                                         \
    FD = BK->fd;                                                              \
    if (FD == BK) mark_binblock(IDX);                                         \
    else                                                                      \
    {                                                                         \
      while (FD != BK && S < chunksize(FD)) FD = FD->fd;                      \
      BK = FD->bk;                                                            \
    }                                                                         \
    P->bk = BK;                                                               \
    P->fd = FD;                                                               \
    FD->bk = BK->fd = P;                                                      \
  }                                                                           \
}


/* take a chunk off a list */

#define unlink(P, BK, FD)                                                     \
{                                                                             \
  BK = P->bk;                                                                 \
  FD = P->fd;                                                                 \
  FD->bk = BK;                                                                \
  BK->fd = FD;                                                                \
}                                                                             \

/* Place p as the last remainder */

#define link_last_remainder(P)                                                \
{                                                                             \
  last_remainder->fd = last_remainder->bk =  P;                               \
  P->fd = P->bk = last_remainder;                                             \
}

/* Clear the last_remainder bin */

#define clear_last_remainder \
  (last_remainder->fd = last_remainder->bk = last_remainder)






/* Routines dealing with mmap(). */

#if HAVE_MMAP

#if __STD_C
static mchunkptr mmap_chunk(size_t size)
#else
static mchunkptr mmap_chunk(size) size_t size;
#endif
{
  size_t page_mask = malloc_getpagesize - 1;
  mchunkptr p;

#ifndef MAP_ANONYMOUS
  static int fd = -1;
#endif

  if(n_mmaps >= n_mmaps_max) return 0; /* too many regions */

  /* For mmapped chunks, the overhead is one SIZE_SZ unit larger, because
   * there is no following chunk whose prev_size field could be used.
   */
  size = (size + SIZE_SZ + page_mask) & ~page_mask;

#ifdef MAP_ANONYMOUS
  p = (mchunkptr)mmap(0, size, PROT_READ|PROT_WRITE,
		      MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
#else /* !MAP_ANONYMOUS */
  if (fd < 0) 
  {
    fd = open("/dev/zero", O_RDWR);
    if(fd < 0) return 0;
  }
  p = (mchunkptr)mmap(0, size, PROT_READ|PROT_WRITE, MAP_PRIVATE, fd, 0);
#endif

  if(p == (mchunkptr)-1) return 0;

  n_mmaps++;
  if (n_mmaps > max_n_mmaps) max_n_mmaps = n_mmaps;
  
  /* We demand that eight bytes into a page must be 8-byte aligned. */
  assert(aligned_OK(chunk2mem(p)));

  /* The offset to the start of the mmapped region is stored
   * in the prev_size field of the chunk; normally it is zero,
   * but that can be changed in memalign().
   */
  p->prev_size = 0;
  set_head(p, size|IS_MMAPPED);
  
  mmapped_mem += size;
  if ((unsigned long)mmapped_mem > (unsigned long)max_mmapped_mem) 
    max_mmapped_mem = mmapped_mem;
  if ((unsigned long)(mmapped_mem + sbrked_mem) > (unsigned long)max_total_mem) 
    max_total_mem = mmapped_mem + sbrked_mem;
  return p;
}

#if __STD_C
static void munmap_chunk(mchunkptr p)
#else
static void munmap_chunk(p) mchunkptr p;
#endif
{
  INTERNAL_SIZE_T size = chunksize(p);
  int ret;

  assert (chunk_is_mmapped(p));
  assert(! ((char*)p >= sbrk_base && (char*)p < sbrk_base + sbrked_mem));
  assert((n_mmaps > 0));
  assert(((p->prev_size + size) & (malloc_getpagesize-1)) == 0);

  n_mmaps--;
  mmapped_mem -= (size + p->prev_size);

  ret = munmap((char *)p - p->prev_size, size + p->prev_size);

  /* munmap returns non-zero on failure */
  assert(ret == 0);
}

#if HAVE_MREMAP

#if __STD_C
static mchunkptr mremap_chunk(mchunkptr p, size_t new_size)
#else
static mchunkptr mremap_chunk(p, new_size) mchunkptr p; size_t new_size;
#endif
{
  size_t page_mask = malloc_getpagesize - 1;
  INTERNAL_SIZE_T offset = p->prev_size;
  INTERNAL_SIZE_T size = chunksize(p);
  char *cp;

  assert (chunk_is_mmapped(p));
  assert(! ((char*)p >= sbrk_base && (char*)p < sbrk_base + sbrked_mem));
  assert((n_mmaps > 0));
  assert(((size + offset) & (malloc_getpagesize-1)) == 0);

  /* Note the extra SIZE_SZ overhead as in mmap_chunk(). */
  new_size = (new_size + offset + SIZE_SZ + page_mask) & ~page_mask;

  cp = (char *)mremap((char *)p - offset, size + offset, new_size, 1);

  if (cp == (char *)-1) return 0;

  p = (mchunkptr)(cp + offset);

  assert(aligned_OK(chunk2mem(p)));

  assert((p->prev_size == offset));
  set_head(p, (new_size - offset)|IS_MMAPPED);

  mmapped_mem -= size + offset;
  mmapped_mem += new_size;
  if ((unsigned long)mmapped_mem > (unsigned long)max_mmapped_mem) 
    max_mmapped_mem = mmapped_mem;
  if ((unsigned long)(mmapped_mem + sbrked_mem) > (unsigned long)max_total_mem)
    max_total_mem = mmapped_mem + sbrked_mem;
  return p;
}

#endif /* HAVE_MREMAP */

#endif /* HAVE_MMAP */




/* 
  Extend the top-most chunk by obtaining memory from system.
  Main interface to sbrk (but see also malloc_trim).
*/

void gmallocHeap::malloc_extend_top(INTERNAL_SIZE_T nb)
{
  char*     brk;                  /* return value from sbrk */
  INTERNAL_SIZE_T front_misalign; /* unusable bytes at front of sbrked space */
  INTERNAL_SIZE_T correction;     /* bytes for 2nd sbrk call */
  char*     new_brk;              /* return of 2nd sbrk call */
  INTERNAL_SIZE_T top_size;       /* new size of top chunk */

  mchunkptr old_top     = top;  /* Record state of old top */
  INTERNAL_SIZE_T old_top_size = chunksize(old_top);
  char*     old_end      = (char*)(chunk_at_offset(old_top, old_top_size));

  /* Pad request with top_pad plus minimal overhead */
  
  INTERNAL_SIZE_T    sbrk_size     = nb + top_pad + MINSIZE;
  unsigned long pagesz    = malloc_getpagesize;

  /* If not the first time through, round to preserve page boundary */
  /* Otherwise, we need to correct to a page size below anyway. */
  /* (We also correct below if an intervening foreign sbrk call.) */

    unsigned long appendSize = 0;
  
  if (sbrk_base != (char*)(-1))
  {
      // sbrk_size = (sbrk_size + (pagesz - 1)) & ~(pagesz - 1);
      sbrk_size = AlignPage(sbrk_size);

        if( sbrk_size > SHAVE_TOP_THRESHOLD )
        {
            appendSize = (unsigned long) (nb-old_top_size) + top_pad + MINSIZE;
            appendSize = (unsigned long) AlignPage(appendSize);
        }
  }      

  brk = (char*)(MORECORE ((long)sbrk_size,appendSize));

    // the appendSize argument is used to store the actual amount allocated.
    if(appendSize)
        sbrk_size = appendSize;

  /* Fail if sbrk failed or if a foreign sbrk call killed our space */
  if (brk == (char*)(MORECORE_FAILURE) || 
      (brk < old_end && old_top != initial_top))
    return;     

    
  sbrked_mem += (int)sbrk_size;

  if (brk == old_end) /* can just add bytes to current top */
  {
    top_size = sbrk_size + old_top_size;
    set_head(top, top_size | PREV_INUSE);
  }
  else
  {
    if (sbrk_base == (char*)(-1))  /* First time through. Record base */
      sbrk_base = brk;
    else  /* Someone else called sbrk().  Count those bytes as sbrked_mem. */
      sbrked_mem += (int)(brk - (char*)old_end);
    
    /* Guarantee alignment of first new chunk made from this space */
    front_misalign = (size_t)chunk2mem(brk) & MALLOC_ALIGN_MASK;
    if (front_misalign > 0) 
    {
      correction = (MALLOC_ALIGNMENT) - front_misalign;
      brk += correction;
    }
    else
      correction = 0;

    /* Guarantee the next brk will be at a page boundary */
    //This code is buggy because it can return pagesz correction += pagesz - ((unsigned long)(brk + sbrk_size) & (pagesz - 1));
	correction += ((((size_t)brk + sbrk_size)+(pagesz-1)) & ~(pagesz - 1)) - ((size_t)brk + sbrk_size);

    /* Allocate correction */
    appendSize = 0; // Not using this feature for a correction.
    new_brk = (char*)(MORECORE ((long)correction,appendSize));
    if (new_brk == (char*)(MORECORE_FAILURE))  return;  

    // test if the allocation was contiguous. 
    if (new_brk != (brk + sbrk_size))  return;  

    sbrked_mem += (int)correction;

    top = (mchunkptr)brk;
    top_size = new_brk - brk + correction;
    set_head(top, top_size | PREV_INUSE);            
    if (old_top != initial_top)
    {

      /* There must have been an intervening foreign sbrk call. */
      /* A double fencepost is necessary to prevent consolidation */

      /* If not enough space to do this, then user did something very wrong */
      if (old_top_size < MINSIZE) 
      {
        set_head(top, PREV_INUSE); /* will force null return from malloc */
        return;
      }

      /* Also keep size a multiple of MALLOC_ALIGNMENT */
      old_top_size = (old_top_size - 3*SIZE_SZ) & ~MALLOC_ALIGN_MASK;
      set_head_size(old_top, old_top_size);
      chunk_at_offset(old_top, old_top_size          )->size =
        SIZE_SZ|PREV_INUSE;
      chunk_at_offset(old_top, old_top_size + SIZE_SZ)->size =
        SIZE_SZ|PREV_INUSE;
      /* If possible, release the rest. */
      if (old_top_size >= MINSIZE) 
        fREe(chunk2mem(old_top));
    }
  }

  if ((unsigned long)sbrked_mem > (unsigned long)max_sbrked_mem) 
    max_sbrked_mem = sbrked_mem;
  if ((unsigned long)(mmapped_mem + sbrked_mem) > (unsigned long)max_total_mem) 
    max_total_mem = mmapped_mem + sbrked_mem;

  /* We always land on a page boundary */
  assert(((size_t)((char*)top + top_size) & (pagesz - 1)) == 0);
}




/* Main public routines */


/*
  Malloc Algorthim:

    The requested size is first converted into a usable form, `nb'.
    This currently means to add 4 bytes overhead plus possibly more to
    obtain 8-byte alignment and/or to obtain a size of at least
    MINSIZE (currently 16 bytes), the smallest allocatable size.
    (All fits are considered `exact' if they are within MINSIZE bytes.)

    From there, the first successful of the following steps is taken:

      1. The bin corresponding to the request size is scanned, and if
         a chunk of exactly the right size is found, it is taken.

      2. The most recently remaindered chunk is used if it is big
         enough.  This is a form of (roving) first fit, used only in
         the absence of exact fits. Runs of consecutive requests use
         the remainder of the chunk used for the previous such request
         whenever possible. This limited use of a first-fit style
         allocation strategy tends to give contiguous chunks
         coextensive lifetimes, which improves locality and can reduce
         fragmentation in the long run.

      3. Other bins are scanned in increasing size order, using a
         chunk big enough to fulfill the request, and splitting off
         any remainder.  This search is strictly by best-fit; i.e.,
         the smallest (with ties going to approximately the least
         recently used) chunk that fits is selected.

      4. If large enough, the chunk bordering the end of memory
         (`top') is split off. (This use of `top' is in accord with
         the best-fit search rule.  In effect, `top' is treated as
         larger (and thus less well fitting) than any other available
         chunk since it can be extended to be as large as necessary
         (up to system limitations).

      5. If the request size meets the mmap threshold and the
         system supports mmap, and there are few enough currently
         allocated mmapped regions, and a call to mmap succeeds,
         the request is allocated via direct memory mapping.

      6. Otherwise, the top of memory is extended by
         obtaining more space from the system (normally using sbrk,
         but definable to anything else via the MORECORE macro).
         Memory is gathered from the system (in system page-sized
         units) in a way that allows chunks obtained across different
         sbrk calls to be consolidated, but does not require
         contiguous memory. Thus, it should be safe to intersperse
         mallocs with other sbrk calls.


      All allocations are made from the the `lowest' part of any found
      chunk. (The implementation invariant is that prev_inuse is
      always true of any allocated chunk; i.e., that each allocated
      chunk borders either a previously allocated and still in-use chunk,
      or the base of its memory arena.)

*/

#if __STD_C
Void_t* mALLOc(size_t bytes)
#else
Void_t* mALLOc(bytes) size_t bytes;
#endif
{
  mchunkptr victim;                  /* inspected/selected chunk */
  INTERNAL_SIZE_T victim_size;       /* its size */
  int       idx;                     /* index for bin traversal */
  mbinptr   bin;                     /* associated bin */
  mchunkptr remainder;               /* remainder from a split */
  long      remainder_size;          /* its size */
  int       remainder_index;         /* its bin index */
  unsigned long block;               /* block traverser bit */
  int       startidx;                /* first bin of a traversed block */
  mchunkptr fwd;                     /* misc temp for linking */
  mchunkptr bck;                     /* misc temp for linking */
  mbinptr q;                         /* misc temp */

#ifdef PARANOID_VALIDATION
  Validate();
#endif

  INTERNAL_SIZE_T nb  = request2size(bytes);  /* padded request size; */
#ifdef DEBUG
  if (nb < bytes)
  {
    WARNING_OUT(("allocation size %d overflow to %d", bytes, nb));
  }
#endif

  /* Check for exact match in a bin */

  if (is_small_request(nb))  /* Faster version for small requests */
  {
    idx = smallbin_index(nb); 

    /* No traversal or size check necessary for small bins.  */

    q = bin_at(idx);
    victim = last(q);

    /* Also scan the next one, since it would have a remainder < MINSIZE */
    if (victim == q)
    {
      q = next_bin(q);
      victim = last(q);
    }
    if (victim != q)
    {
      victim_size = chunksize(victim);
      unlink(victim, bck, fwd);
      set_inuse_bit_at_offset(victim, victim_size);
      check_malloced_chunk(victim, nb);
      return chunk2mem(victim);
    }

    idx += 2; /* Set for bin scan below. We've already scanned 2 bins. */

  }
  else
  {
    idx = bin_index(nb);
    bin = bin_at(idx);

    for (victim = last(bin); victim != bin; victim = victim->bk)
    {
      victim_size = chunksize(victim);
      remainder_size = (long)(victim_size - nb);
      
      if (remainder_size >= (long)MINSIZE) /* too big */
      {
        --idx; /* adjust to rescan below after checking last remainder */
        break;   
      }

      else if (remainder_size >= 0) /* exact fit */
      {
        unlink(victim, bck, fwd);
        set_inuse_bit_at_offset(victim, victim_size);
        check_malloced_chunk(victim, nb);
        return chunk2mem(victim);
      }
    }

    ++idx; 

  }

  /* Try to use the last split-off remainder */

  if ( (victim = last_remainder->fd) != last_remainder)
  {
    victim_size = chunksize(victim);
    remainder_size = (long)(victim_size - nb);

    if (remainder_size >= (long)MINSIZE) /* re-split */
    {
      remainder = chunk_at_offset(victim, nb);
      set_head(victim, nb | PREV_INUSE);
      link_last_remainder(remainder);
      set_head(remainder, remainder_size | PREV_INUSE);
      set_foot(remainder, remainder_size);
      check_malloced_chunk(victim, nb);
      return chunk2mem(victim);
    }

    clear_last_remainder;

    if (remainder_size >= 0)  /* exhaust */
    {
      set_inuse_bit_at_offset(victim, victim_size);
      check_malloced_chunk(victim, nb);
      return chunk2mem(victim);
    }

    /* Else place in bin */

    frontlink(victim, victim_size, remainder_index, bck, fwd);
  }

  /* 
     If there are any possibly nonempty big-enough blocks, 
     search for best fitting chunk by scanning bins in blockwidth units.
  */

  if ( (block = idx2binblock(idx)) <= binblocks)  
  {

    /* Get to the first marked block */

    if ( (block & binblocks) == 0) 
    {
      /* force to an even block boundary */
      idx = (idx & ~(BINBLOCKWIDTH - 1)) + BINBLOCKWIDTH;
      block <<= 1;
      while ((block & binblocks) == 0)
      {
        idx += BINBLOCKWIDTH;
        block <<= 1;
      }
    }
      
    /* For each possibly nonempty block ... */
    for (;;)  
    {
      startidx = idx;          /* (track incomplete blocks) */
      q = bin = bin_at(idx);

      /* For each bin in this block ... */
      do
      {
        /* Find and use first big enough chunk ... */

        for (victim = last(bin); victim != bin; victim = victim->bk)
        {
          victim_size = chunksize(victim);
          remainder_size = (long)(victim_size - nb);

          if (remainder_size >= (long)MINSIZE) /* split */
          {
            remainder = chunk_at_offset(victim, nb);
            set_head(victim, nb | PREV_INUSE);
            unlink(victim, bck, fwd);
            link_last_remainder(remainder);
            set_head(remainder, remainder_size | PREV_INUSE);
            set_foot(remainder, remainder_size);
            check_malloced_chunk(victim, nb);
            return chunk2mem(victim);
          }

          else if (remainder_size >= 0)  /* take */
          {
            set_inuse_bit_at_offset(victim, victim_size);
            unlink(victim, bck, fwd);
            check_malloced_chunk(victim, nb);
            return chunk2mem(victim);
          }

        }

       bin = next_bin(bin);

      } while ((++idx & (BINBLOCKWIDTH - 1)) != 0);

      /* Clear out the block bit. */

      do   /* Possibly backtrack to try to clear a partial block */
      {
        if ((startidx & (BINBLOCKWIDTH - 1)) == 0)
        {
          binblocks &= ~block;
          break;
        }
        --startidx;
       q = prev_bin(q);
      } while (first(q) == q);

      /* Get to the next possibly nonempty block */

      if ( (block <<= 1) <= binblocks && (block != 0) ) 
      {
        while ((block & binblocks) == 0)
        {
          idx += BINBLOCKWIDTH;
          block <<= 1;
        }
      }
      else
        break;
    }
  }


  /* Try to use top chunk */

  /* Require that there be a remainder, ensuring top always exists  */
  if ( (remainder_size = (long)(chunksize(top) - nb)) < (long)MINSIZE)
  {

#if HAVE_MMAP
    /* If big and would otherwise need to extend, try to use mmap instead */
    if ((unsigned long)nb >= (unsigned long)mmap_threshold &&
        (victim = mmap_chunk(nb)) != 0)
      return chunk2mem(victim);
#endif

    /* Try to extend */
    malloc_extend_top(nb);
    if ( (remainder_size = (long)(chunksize(top) - nb)) < (long)MINSIZE)
      return 0; /* propagate failure */
  }

  victim = top;
  set_head(victim, nb | PREV_INUSE);
  top = chunk_at_offset(victim, nb);
  set_head(top, remainder_size | PREV_INUSE);
  check_malloced_chunk(victim, nb);
  return chunk2mem(victim);

}




/*

  free() algorithm :

    cases:

       1. free(0) has no effect.  

       2. If the chunk was allocated via mmap, it is release via munmap().

       3. If a returned chunk borders the current high end of memory,
          it is consolidated into the top, and if the total unused
          topmost memory exceeds the trim threshold, malloc_trim is
          called.

       4. Other chunks are consolidated as they arrive, and
          placed in corresponding bins. (This includes the case of
          consolidating with the current `last_remainder').

*/


#if __STD_C
void fREe(Void_t* mem)
#else
void fREe(mem) Void_t* mem;
#endif
{
  mchunkptr p;         /* chunk corresponding to mem */
  INTERNAL_SIZE_T hd;  /* its head field */
  INTERNAL_SIZE_T sz;  /* its size */
  int       idx;       /* its bin index */
  mchunkptr next;      /* next contiguous chunk */
  INTERNAL_SIZE_T nextsz; /* its size */
  INTERNAL_SIZE_T prevsz; /* size of previous contiguous chunk */
  mchunkptr bck;       /* misc temp for linking */
  mchunkptr fwd;       /* misc temp for linking */
  int       islr;      /* track whether merging with last_remainder */

  if (mem == 0)                              /* free(0) has no effect */
    return;

  p = mem2chunk(mem);
  hd = p->size;

#if HAVE_MMAP
  if (hd & IS_MMAPPED)                       /* release mmapped memory. */
  {
    munmap_chunk(p);
    return;
  }
#endif
  
  check_inuse_chunk(p);
  
  sz = hd & ~PREV_INUSE;
  next = chunk_at_offset(p, sz);
  nextsz = chunksize(next);
  
  if (next == top)                            /* merge with top */
  {
    sz += nextsz;

    if (!(hd & PREV_INUSE))                    /* consolidate backward */
    {
      prevsz = p->prev_size;
#pragma warning( disable : 4146 )       // turn off "no return value" warning
      p = chunk_at_offset(p, -prevsz);
#pragma warning( default : 4146 )
      sz += prevsz;
      unlink(p, bck, fwd);
    }

    set_head(p, sz | PREV_INUSE);
    top = p;
    if ((unsigned long)(sz) >= (unsigned long)trim_threshold) 
      malloc_trim(top_pad); 
    return;
  }

  set_head(next, nextsz);                    /* clear inuse bit */

  islr = 0;

  if (!(hd & PREV_INUSE))                    /* consolidate backward */
  {
    prevsz = p->prev_size;
#pragma warning( disable : 4146 )       // turn off "no return value" warning
    p = chunk_at_offset(p, -prevsz);
#pragma warning( default : 4146 )
    sz += prevsz;
    
    if (p->fd == last_remainder)             /* keep as last_remainder */
      islr = 1;
    else
      unlink(p, bck, fwd);
  }
  
  if (!(inuse_bit_at_offset(next, nextsz)))   /* consolidate forward */
  {
    sz += nextsz;
    
    if (!islr && next->fd == last_remainder)  /* re-insert last_remainder */
    {
      islr = 1;
      link_last_remainder(p);   
    }
    else
      unlink(next, bck, fwd);
  }


  set_head(p, sz | PREV_INUSE);
  set_foot(p, sz);
  if (!islr)
    frontlink(p, sz, idx, bck, fwd);  
}





/*

  Realloc algorithm:

    Chunks that were obtained via mmap cannot be extended or shrunk
    unless HAVE_MREMAP is defined, in which case mremap is used.
    Otherwise, if their reallocation is for additional space, they are
    copied.  If for less, they are just left alone.

    Otherwise, if the reallocation is for additional space, and the
    chunk can be extended, it is, else a malloc-copy-free sequence is
    taken.  There are several different ways that a chunk could be
    extended. All are tried:

       * Extending forward into following adjacent free chunk.
       * Shifting backwards, joining preceding adjacent space
       * Both shifting backwards and extending forward.
       * Extending into newly sbrked space

    Unless the #define REALLOC_ZERO_BYTES_FREES is set, realloc with a
    size argument of zero (re)allocates a minimum-sized chunk.

    If the reallocation is for less space, and the new request is for
    a `small' (<512 bytes) size, then the newly unused space is lopped
    off and freed.

    The old unix realloc convention of allowing the last-free'd chunk
    to be used as an argument to realloc is no longer supported.
    I don't know of any programs still relying on this feature,
    and allowing it would also allow too many other incorrect 
    usages of realloc to be sensible.


*/


#if __STD_C
Void_t* rEALLOc(Void_t* oldmem, size_t bytes)
#else
Void_t* rEALLOc(oldmem, bytes) Void_t* oldmem; size_t bytes;
#endif
{
  INTERNAL_SIZE_T    nb;      /* padded request size */

  mchunkptr oldp;             /* chunk corresponding to oldmem */
  INTERNAL_SIZE_T    oldsize; /* its size */

  mchunkptr newp;             /* chunk to return */
  INTERNAL_SIZE_T    newsize; /* its size */
  Void_t*   newmem;           /* corresponding user mem */

  mchunkptr next;             /* next contiguous chunk after oldp */
  INTERNAL_SIZE_T  nextsize;  /* its size */

  mchunkptr prev;             /* previous contiguous chunk before oldp */
  INTERNAL_SIZE_T  prevsize;  /* its size */

  mchunkptr remainder;        /* holds split off extra space from newp */
  INTERNAL_SIZE_T  remainder_size;   /* its size */

  mchunkptr bck;              /* misc temp for linking */
  mchunkptr fwd;              /* misc temp for linking */

#ifdef REALLOC_ZERO_BYTES_FREES
  if (bytes == 0) { fREe(oldmem); return 0; }
#endif


  /* realloc of null is supposed to be same as malloc */
  if (oldmem == 0) return mALLOc(bytes);

  newp    = oldp    = mem2chunk(oldmem);
  newsize = oldsize = chunksize(oldp);


  nb = request2size(bytes);

#if HAVE_MMAP
  if (chunk_is_mmapped(oldp)) 
  {
#if HAVE_MREMAP
    newp = mremap_chunk(oldp, nb);
    if(newp) return chunk2mem(newp);
#endif
    /* Note the extra SIZE_SZ overhead. */
    if(oldsize - SIZE_SZ >= nb) return oldmem; /* do nothing */
    /* Must alloc, copy, free. */
    newmem = mALLOc(bytes);
    if (newmem == 0) return 0; /* propagate failure */
    MALLOC_COPY(newmem, oldmem, oldsize - 2*SIZE_SZ);
    munmap_chunk(oldp);
    return newmem;
  }
#endif

  check_inuse_chunk(oldp);

  if ((long)(oldsize) < (long)(nb))  
  {

    /* Try expanding forward */

    next = chunk_at_offset(oldp, oldsize);
    if (next == top || !inuse(next)) 
    {
      nextsize = chunksize(next);

      /* Forward into top only if a remainder */
      if (next == top)
      {
        if ((long)(nextsize + newsize) >= (long)(nb + MINSIZE))
        {
          newsize += nextsize;
          top = chunk_at_offset(oldp, nb);
          set_head(top, (newsize - nb) | PREV_INUSE);
          set_head_size(oldp, nb);
          return chunk2mem(oldp);
        }
      }

      /* Forward into next chunk */
      else if (((long)(nextsize + newsize) >= (long)(nb)))
      { 
        unlink(next, bck, fwd);
        newsize  += nextsize;
        goto split;
      }
    }
    else
    {
      next = 0;
      nextsize = 0;
    }

    /* Try shifting backwards. */

    if (!prev_inuse(oldp))
    {
      prev = prev_chunk(oldp);
      prevsize = chunksize(prev);

      /* try forward + backward first to save a later consolidation */

      if (next != 0)
      {
        /* into top */
        if (next == top)
        {
          if ((long)(nextsize + prevsize + newsize) >= (long)(nb + MINSIZE))
          {
            unlink(prev, bck, fwd);
            newp = prev;
            newsize += prevsize + nextsize;
            newmem = chunk2mem(newp);
            MALLOC_COPY(newmem, oldmem, oldsize - SIZE_SZ);
            top = chunk_at_offset(newp, nb);
            set_head(top, (newsize - nb) | PREV_INUSE);
            set_head_size(newp, nb);
            return newmem;
          }
        }

        /* into next chunk */
        else if (((long)(nextsize + prevsize + newsize) >= (long)(nb)))
        {
          unlink(next, bck, fwd);
          unlink(prev, bck, fwd);
          newp = prev;
          newsize += nextsize + prevsize;
          newmem = chunk2mem(newp);
          MALLOC_COPY(newmem, oldmem, oldsize - SIZE_SZ);
          goto split;
        }
      }
      
      /* backward only */
      if (prev != 0 && (long)(prevsize + newsize) >= (long)nb)  
      {
        unlink(prev, bck, fwd);
        newp = prev;
        newsize += prevsize;
        newmem = chunk2mem(newp);
        MALLOC_COPY(newmem, oldmem, oldsize - SIZE_SZ);
        goto split;
      }
    }

    /* Must allocate */

    newmem = mALLOc (bytes);

    if (newmem == 0)  /* propagate failure */
      return 0; 

    /* Avoid copy if newp is next chunk after oldp. */
    /* (This can only happen when new chunk is sbrk'ed.) */

    if ( (newp = mem2chunk(newmem)) == next_chunk(oldp)) 
    {
      newsize += chunksize(newp);
      newp = oldp;
      goto split;
    }

    /* Otherwise copy, free, and exit */
    MALLOC_COPY(newmem, oldmem, oldsize - SIZE_SZ);
    fREe(oldmem);
    return newmem;
  }


 split:  /* split off extra room in old or expanded chunk */

  if (newsize - nb >= MINSIZE) /* split off remainder */
  {
    remainder = chunk_at_offset(newp, nb);
    remainder_size = newsize - nb;
    set_head_size(newp, nb);
    set_head(remainder, remainder_size | PREV_INUSE);
    set_inuse_bit_at_offset(remainder, remainder_size);
    fREe(chunk2mem(remainder)); /* let free() deal with it */
  }
  else
  {
    set_head_size(newp, newsize);
    set_inuse_bit_at_offset(newp, newsize);
  }

  check_inuse_chunk(newp);
  return chunk2mem(newp);
}




/*

  memalign algorithm:

    memalign requests more than enough space from malloc, finds a spot
    within that chunk that meets the alignment request, and then
    possibly frees the leading and trailing space. 

    The alignment argument must be a power of two. This property is not
    checked by memalign, so misuse may result in random runtime errors.

    8-byte alignment is guaranteed by normal malloc calls, so don't
    bother calling memalign with an argument of 8 or less.

    Overreliance on memalign is a sure way to fragment space.

*/


#if __STD_C
Void_t* mEMALIGn(size_t alignment, size_t bytes)
#else
Void_t* mEMALIGn(alignment, bytes) size_t alignment; size_t bytes;
#endif
{
  INTERNAL_SIZE_T    nb;      /* padded  request size */
  char*     m;                /* memory returned by malloc call */
  mchunkptr p;                /* corresponding chunk */
  char*     brk;              /* alignment point within p */
  mchunkptr newp;             /* chunk to return */
  INTERNAL_SIZE_T  newsize;   /* its size */
  INTERNAL_SIZE_T  leadsize;  /* leading space befor alignment point */
  mchunkptr remainder;        /* spare room at end to split off */
  long      remainder_size;   /* its size */

  /* If need less alignment than we give anyway, just relay to malloc */

  if (alignment <= MALLOC_ALIGNMENT) return mALLOc(bytes);

  /* Otherwise, ensure that it is at least a minimum chunk size */
  
  if (alignment <  MINSIZE) alignment = MINSIZE;

  /* Call malloc with worst case padding to hit alignment. */

  nb = request2size(bytes);
  m  = (char*)(mALLOc(nb + alignment + MINSIZE));

  if (m == 0) return 0; /* propagate failure */

  p = mem2chunk(m);

  if (((size_t)m % alignment) == 0) /* aligned */
  {
#if HAVE_MMAP
    if(chunk_is_mmapped(p))
      return chunk2mem(p); /* nothing more to do */
#endif
  }
  else /* misaligned */
  {
    /* 
      Find an aligned spot inside chunk.
      Since we need to give back leading space in a chunk of at 
      least MINSIZE, if the first calculation places us at
      a spot with less than MINSIZE leader, we can move to the
      next aligned spot -- we've allocated enough total room so that
      this is always possible.
    */

    brk = (char*)mem2chunk((size_t)(m + alignment - 1) & ~(alignment-1));
    if ((long)(brk - (char*)(p)) < MINSIZE) brk = brk + alignment;

    newp = (mchunkptr)brk;
    leadsize = brk - (char*)(p);
    newsize = chunksize(p) - leadsize;

#if HAVE_MMAP
    if(chunk_is_mmapped(p)) 
    {
      newp->prev_size = p->prev_size + leadsize;
      set_head(newp, newsize|IS_MMAPPED);
      return chunk2mem(newp);
    }
#endif

    /* give back leader, use the rest */

    set_head(newp, newsize | PREV_INUSE);
    set_inuse_bit_at_offset(newp, newsize);
    set_head_size(p, leadsize);
    fREe(chunk2mem(p));
    p = newp;

    assert (newsize >= nb && ((size_t)chunk2mem(p) % alignment) == 0);
  }

  /* Also give back spare room at the end */

  remainder_size = (long)(chunksize(p) - nb);

  if (remainder_size >= (long)MINSIZE)
  {
    remainder = chunk_at_offset(p, nb);
    set_head(remainder, remainder_size | PREV_INUSE);
    set_head_size(p, nb);
    fREe(chunk2mem(remainder));
  }

  check_inuse_chunk(p);
  return chunk2mem(p);

}



#if DEADCODE
/*
    valloc just invokes memalign with alignment argument equal
    to the page size of the system (or as near to this as can
    be figured out from all the includes/defines above.)
*/

#if __STD_C
Void_t* vALLOc(size_t bytes)
#else
Void_t* vALLOc(bytes) size_t bytes;
#endif
{
  return mEMALIGn (malloc_getpagesize, bytes);
}

/* 
  pvalloc just invokes valloc for the nearest pagesize
  that will accommodate request
*/


#if __STD_C
Void_t* pvALLOc(size_t bytes)
#else
Void_t* pvALLOc(bytes) size_t bytes;
#endif
{
  size_t pagesize = malloc_getpagesize;
  return mEMALIGn (pagesize, (bytes + pagesize - 1) & ~(pagesize - 1));
}

/*

  calloc calls malloc, then zeroes out the allocated chunk.

*/

#if __STD_C
Void_t* cALLOc(size_t n, size_t elem_size)
#else
Void_t* cALLOc(n, elem_size) size_t n; size_t elem_size;
#endif
{
  mchunkptr p;
  INTERNAL_SIZE_T csz;

  INTERNAL_SIZE_T sz = n * elem_size;

  /* check if expand_top called, in which case don't need to clear */
#if MORECORE_CLEARS
  mchunkptr oldtop = top;
  INTERNAL_SIZE_T oldtopsize = chunksize(top);
#endif
  Void_t* mem = mALLOc (sz);

  if (mem == 0) 
    return 0;
  else
  {
    p = mem2chunk(mem);

    /* Two optional cases in which clearing not necessary */


#if HAVE_MMAP
    if (chunk_is_mmapped(p)) return mem;
#endif

    csz = chunksize(p);

#if MORECORE_CLEARS
    if (p == oldtop && csz > oldtopsize) 
    {
      /* clear only the bytes from non-freshly-sbrked memory */
      csz = oldtopsize;
    }
#endif

    MALLOC_ZERO(mem, csz - SIZE_SZ);
    return mem;
  }
}

#endif //DEADCODE
/*
 
  cfree just calls free. It is needed/defined on some systems
  that pair it with calloc, presumably for odd historical reasons.

*/

#ifdef DEAD_CODE
#if !defined(INTERNAL_LINUX_C_LIB) || !defined(__ELF__)
#if __STD_C
void cfree(Void_t *mem)
#else
void cfree(mem) Void_t *mem;
#endif
{
  free(mem);
}
#endif

#endif


/*

    Malloc_trim gives memory back to the system (via negative
    arguments to sbrk) if there is unused memory at the `high' end of
    the malloc pool. You can call this after freeing large blocks of
    memory to potentially reduce the system-level memory requirements
    of a program. However, it cannot guarantee to reduce memory. Under
    some allocation patterns, some large free blocks of memory will be
    locked between two used chunks, so they cannot be given back to
    the system.

    The `pad' argument to malloc_trim represents the amount of free
    trailing space to leave untrimmed. If this argument is zero,
    only the minimum amount of memory to maintain internal data
    structures will be left (one page or less). Non-zero arguments
    can be supplied to maintain enough trailing space to service
    future expected allocations without having to re-obtain memory
    from the system.

    Malloc_trim returns 1 if it actually released any memory, else 0.

*/

int gmallocHeap::malloc_trim(size_t pad)
{
  long  top_size;        /* Amount of top-most memory */
  long  extra;           /* Amount to release */
  char* current_brk;     /* address returned by pre-check sbrk call */
  char* new_brk;         /* address returned by negative sbrk call */

  unsigned long pagesz = malloc_getpagesize;

  top_size = (long)chunksize(top);
  extra = (long)(((top_size - pad - MINSIZE + (pagesz-1)) / pagesz - 1) * pagesz);

  if (extra < (long)pagesz)  /* Not enough memory to release */
    return 0;

  else
  {
    /* Test to make sure no one else called sbrk */
    unsigned long appendSize = 0;
    current_brk = (char*)(MORECORE (0,appendSize));
    if (current_brk != (char*)(top) + top_size)
      return 0;     /* Apparently we don't own memory; must fail */

    else
    {
      new_brk = (char*)(MORECORE (-extra,appendSize));
      
      if (new_brk == (char*)(MORECORE_FAILURE)) /* sbrk failed? */
      {
        /* Try to figure out what we have */
        current_brk = (char*)(MORECORE (0,appendSize));
        top_size = (long)(current_brk - (char*)top);
        if (top_size >= (long)MINSIZE) /* if not, we are very very dead! */
        {
          sbrked_mem = (int)(current_brk - sbrk_base);
          set_head(top, top_size | PREV_INUSE);
        }
        check_chunk(top);
        return 0; 
      }

      else
      {
        /* Success. Adjust top accordingly. */
        set_head(top, (top_size - extra) | PREV_INUSE);
        sbrked_mem -= extra;
        check_chunk(top);
        return 1;
      }
    }
  }
}



/*
  malloc_usable_size:

    This routine tells you how many bytes you can actually use in an
    allocated chunk, which may be more than you requested (although
    often not). You can use this many bytes without worrying about
    overwriting other allocated objects. Not a particularly great
    programming practice, but still sometimes useful.

*/

size_t gmallocHeap::malloc_usable_size(Void_t* mem)
{
  mchunkptr p;
  if (mem == 0)
    return 0;
  else
  {
    p = mem2chunk(mem);
    if(!chunk_is_mmapped(p))
    {
      if (!inuse(p)) return 0;
      check_inuse_chunk(p);
      return chunksize(p) - SIZE_SZ;
    }
    return chunksize(p) - 2*SIZE_SZ;
  }
}




/* Utility to update current_mallinfo for malloc_stats and mallinfo() */

void gmallocHeap::malloc_update_mallinfo()
{
  int i;
  mbinptr b;
  mchunkptr p;
#if DEBUG
  mchunkptr q;
#endif

  INTERNAL_SIZE_T avail = chunksize(top);
  int   navail = ((long)(avail) >= (long)MINSIZE)? 1 : 0;

  for (i = 1; i < NAV; ++i)
  {
    b = bin_at(i);
    for (p = last(b); p != b; p = p->bk) 
    {
#if DEBUG
      check_free_chunk(p);
      for (q = next_chunk(p); 
           q < top && inuse(q) && (long)(chunksize(q)) >= (long)MINSIZE; 
           q = next_chunk(q))
        check_inuse_chunk(q);
#endif
      avail += chunksize(p);
      navail++;
    }
  }

  current_mallinfo.ordblks = navail;
  current_mallinfo.uordblks = (int)(sbrked_mem - avail);
  current_mallinfo.fordblks = (int)avail;
  current_mallinfo.hblks = n_mmaps;
  current_mallinfo.hblkhd = mmapped_mem;
  current_mallinfo.keepcost = (int)chunksize(top);

}



/*

  malloc_stats:

    Prints on stderr the amount of space obtain from the system (both
    via sbrk and mmap), the maximum amount (which may be more than
    current if malloc_trim and/or munmap got called), the maximum
    number of simultaneous mmap regions used, and the current number
    of bytes allocated via malloc (or realloc, etc) but not yet
    freed. (Note that this is the number of bytes allocated, not the
    number requested. It will be larger than the number requested
    because of alignment and bookkeeping overhead.)

*/

#ifdef DEAD_CODE

void gmallocHeap::malloc_stats()
{
  malloc_update_mallinfo();
  fprintf(stderr, "max system bytes = %10u\n", 
          (unsigned int)(max_total_mem));
  fprintf(stderr, "system bytes     = %10u\n", 
          (unsigned int)(sbrked_mem + mmapped_mem));
  fprintf(stderr, "in use bytes     = %10u\n", 
          (unsigned int)(current_mallinfo.uordblks + mmapped_mem));
#if HAVE_MMAP
  fprintf(stderr, "max mmap regions = %10u\n", 
          (unsigned int)max_n_mmaps);
#endif
}

#endif

/*
  mallinfo returns a copy of updated current mallinfo.
*/

struct mallinfo mALLINFo()
{
  malloc_update_mallinfo();
  return current_mallinfo;
}




/*
  mallopt:

    mallopt is the general SVID/XPG interface to tunable parameters.
    The format is to provide a (parameter-number, parameter-value) pair.
    mallopt then sets the corresponding parameter to the argument
    value if it can (i.e., so long as the value is meaningful),
    and returns 1 if successful else 0.

    See descriptions of tunable parameters above.

*/

#if __STD_C
int mALLOPt(int param_number, int value)
#else
int mALLOPt(param_number, value) int param_number; int value;
#endif
{
  switch(param_number) 
  {
    case M_TRIM_THRESHOLD:
      trim_threshold = value; return 1; 
    case M_TOP_PAD:
      top_pad = value; return 1; 
    case M_MMAP_THRESHOLD:
      mmap_threshold = value; return 1;
    case M_MMAP_MAX:
#if HAVE_MMAP
      n_mmaps_max = value; return 1;
#else
      if (value != 0) return 0; else  n_mmaps_max = value; return 1;
#endif

    default:
      return 0;
  }
}

/*

History:

    V2.6.5 Wed Jun 17 15:57:31 1998  Doug Lea  (dl at gee)
      * Fixed ordering problem with boundary-stamping

    V2.6.3 Sun May 19 08:17:58 1996  Doug Lea  (dl at gee)
      * Added pvalloc, as recommended by H.J. Liu
      * Added 64bit pointer support mainly from Wolfram Gloger
      * Added anonymously donated WIN32 sbrk emulation
      * Malloc, calloc, getpagesize: add optimizations from Raymond Nijssen
      * malloc_extend_top: fix mask error that caused wastage after
        foreign sbrks
      * Add linux mremap support code from HJ Liu
   
    V2.6.2 Tue Dec  5 06:52:55 1995  Doug Lea  (dl at gee)
      * Integrated most documentation with the code.
      * Add support for mmap, with help from 
        Wolfram Gloger (Gloger@lrz.uni-muenchen.de).
      * Use last_remainder in more cases.
      * Pack bins using idea from  colin@nyx10.cs.du.edu
      * Use ordered bins instead of best-fit threshhold
      * Eliminate block-local decls to simplify tracing and debugging.
      * Support another case of realloc via move into top
      * Fix error occuring when initial sbrk_base not word-aligned.  
      * Rely on page size for units instead of SBRK_UNIT to
        avoid surprises about sbrk alignment conventions.
      * Add mallinfo, mallopt. Thanks to Raymond Nijssen
        (raymond@es.ele.tue.nl) for the suggestion. 
      * Add `pad' argument to malloc_trim and top_pad mallopt parameter.
      * More precautions for cases where other routines call sbrk,
        courtesy of Wolfram Gloger (Gloger@lrz.uni-muenchen.de).
      * Added macros etc., allowing use in linux libc from
        H.J. Lu (hjl@gnu.ai.mit.edu)
      * Inverted this history list

    V2.6.1 Sat Dec  2 14:10:57 1995  Doug Lea  (dl at gee)
      * Re-tuned and fixed to behave more nicely with V2.6.0 changes.
      * Removed all preallocation code since under current scheme
        the work required to undo bad preallocations exceeds
        the work saved in good cases for most test programs.
      * No longer use return list or unconsolidated bins since
        no scheme using them consistently outperforms those that don't
        given above changes.
      * Use best fit for very large chunks to prevent some worst-cases.
      * Added some support for debugging

    V2.6.0 Sat Nov  4 07:05:23 1995  Doug Lea  (dl at gee)
      * Removed footers when chunks are in use. Thanks to
        Paul Wilson (wilson@cs.texas.edu) for the suggestion.

    V2.5.4 Wed Nov  1 07:54:51 1995  Doug Lea  (dl at gee)
      * Added malloc_trim, with help from Wolfram Gloger 
        (wmglo@Dent.MED.Uni-Muenchen.DE).

    V2.5.3 Tue Apr 26 10:16:01 1994  Doug Lea  (dl at g)

    V2.5.2 Tue Apr  5 16:20:40 1994  Doug Lea  (dl at g)
      * realloc: try to expand in both directions
      * malloc: swap order of clean-bin strategy;
      * realloc: only conditionally expand backwards
      * Try not to scavenge used bins
      * Use bin counts as a guide to preallocation
      * Occasionally bin return list chunks in first scan
      * Add a few optimizations from colin@nyx10.cs.du.edu

    V2.5.1 Sat Aug 14 15:40:43 1993  Doug Lea  (dl at g)
      * faster bin computation & slightly different binning
      * merged all consolidations to one part of malloc proper
         (eliminating old malloc_find_space & malloc_clean_bin)
      * Scan 2 returns chunks (not just 1)
      * Propagate failure in realloc if malloc returns 0
      * Add stuff to allow compilation on non-ANSI compilers 
          from kpv@research.att.com
     
    V2.5 Sat Aug  7 07:41:59 1993  Doug Lea  (dl at g.oswego.edu)
      * removed potential for odd address access in prev_chunk
      * removed dependency on getpagesize.h
      * misc cosmetics and a bit more internal documentation
      * anticosmetics: mangled names in macros to evade debugger strangeness
      * tested on sparc, hp-700, dec-mips, rs6000 
          with gcc & native cc (hp, dec only) allowing
          Detlefs & Zorn comparison study (in SIGPLAN Notices.)

    Trial version Fri Aug 28 13:14:29 1992  Doug Lea  (dl at g.oswego.edu)
      * Based loosely on libg++-1.2X malloc. (It retains some of the overall 
         structure of old version,  but most details differ.)

*/

HRESULT gmallocHeap::Init (char * name, DWORD InitialReserve, DWORD flags)
{
    // Initialize bins
    av_[0] = 0;
	av_[1] = 0;
    mbinptr *cur = av_+2;
    for (int i = 0; i < NAV; i++)
    {
        mbinptr p = bin_at (i);
        *cur = p;
        *(cur+1) = p;
        cur += 2;
    }
    ASSERT(cur == av_+NAV*2+2);

    /*  Other bookkeeping data */

    /* variables holding tunable values */

    trim_threshold   = DEFAULT_TRIM_THRESHOLD;
    top_pad          = DEFAULT_TOP_PAD;
    n_mmaps_max      = DEFAULT_MMAP_MAX;
    mmap_threshold   = DEFAULT_MMAP_THRESHOLD;

    ASSERT((flags & GM_FIXED_HEAP) == flags);

    gmFlags          = (flags & GM_FIXED_HEAP);

    /* The first value returned from sbrk */
    sbrk_base = (char*)(-1);

    /* The maximum memory obtained from system via sbrk */
    max_sbrked_mem = 0;

    /* The maximum via either sbrk or mmap */
    max_total_mem = 0;

#ifdef DEBUG
    /* internal working copy of mallinfo */
    ZeroMemory(&current_mallinfo, sizeof(current_mallinfo));
#endif

#if HAVE_MMAP
    /* Tracking mmaps */
    max_n_mmaps = 0;
    max_mmapped_mem = 0;
#endif
    n_mmaps = 0;
    mmapped_mem = 0;

    head = 0;
    gNextAddress = 0;
    gAddressBase = 0;
    gInitialReserve = InitialReserve;
    gVirtualAllocedHook = 0;
    gPreVirtualAllocHook = 0;

    gName = name;
    //@TODO : fix memmon support
	//OpenMemmon();

    return NOERROR;
}

HRESULT gmallocHeap::Init (char *name, DWORD* start, DWORD Size, gmhook_fn fn,
                           gmprehook_fn pre_fn, DWORD flags)
{
    // Call the other initializer
    HRESULT res = Init (name, Size);
    if (res != NOERROR)
        return res;
    else
    {
        gAddressBase = gNextAddress = start;
        gAllocatedSize = Size;
        gVirtualAllocedHook = fn;
        gPreVirtualAllocHook = pre_fn;
        return NOERROR;
    }
}

VOID gmallocHeap::Finalize ()
{
    gcleanup();
}


#ifdef DEBUG

VOID gmallocHeap::Validate (BOOL dumpleaks)
{
  int i;
  mbinptr b;
  mchunkptr p;
#ifdef DEBUG
  mchunkptr q;
#endif

  size_t avail = chunksize(top);
  int   navail = ((long)(avail) >= (long)MINSIZE)? 1 : 0;

  for (i = 1; i < NAV; ++i)
  {
    b = bin_at(i);
    for (p = last(b); p != b; p = p->bk)
    {
      check_free_chunk(p);
      for (q = next_chunk(p);
           q < top && inuse(q) && (long)(chunksize(q)) >= (long)MINSIZE;
           q = next_chunk(q))
      {
        check_inuse_chunk(q);
        if (dumpleaks)
            WARNING_OUT(("gmallocHeap leak: heap %08x, block %08x, size %08x", this, chunk2mem(q), chunksize(q)));
      }
      avail += chunksize(p);
      navail++;
    }
  }
}

#endif


#ifdef GMALLOCHEAPS

// 'Replacement' for local process heap
gmallocSyncHeap gmallocURTLocalHeap;

#define GMALLOC_LMEM_FLAGS (LMEM_ZEROINIT)

inline HANDLE gmallocHeapCreate (char * name, DWORD flOptions, DWORD dwInitialSize, DWORD dwMaximumSize)
{
    ASSERT(flOptions == 0);
    gmallocSyncHeap *heap = new(gmallocSyncHeap());
    if (heap != NULL)
    {
        if (dwMaximumSize)
        {
            heap->Init(name, dwMaximumSize, GM_FIXED_HEAP);
        }
        else
            heap->Init(name);
    }
    return (HANDLE)heap;
}

inline BOOL gmallocHeapDestroy(HANDLE hHeap)
{
    gmallocSyncHeap *gmheap = (gmallocSyncHeap*)hHeap;
    gmheap->Finalize();
    delete(gmheap);
    return TRUE;
}

inline LPVOID gmallocHeapAlloc (HANDLE hHeap, DWORD dwFlags, DWORD dwBytes)
{
    ASSERT(dwFlags == 0);
    gmallocSyncHeap *gmheap = (gmallocSyncHeap*)hHeap;
    return gmheap->Alloc(dwBytes);
}

inline LPVOID gmallocHeapReAlloc (HANDLE hHeap, DWORD dwFlags, LPVOID lpMem, DWORD dwBytes)
{
    ASSERT(dwFlags == 0 || dwFlags == HEAP_REALLOC_IN_PLACE_ONLY && dwBytes <= HeapSize(hHeap,0,lpMem));
    gmallocSyncHeap *gmheap = (gmallocSyncHeap*)hHeap;
    return gmheap->ReAlloc(lpMem, dwBytes);
}

inline BOOL gmallocHeapFree (HANDLE hHeap, DWORD dwFlags, LPVOID lpMem)
{
    ASSERT(dwFlags == 0);
    gmallocSyncHeap *gmheap = (gmallocSyncHeap*)hHeap;
    gmheap->Free(lpMem);
    return TRUE;
}

inline HLOCAL gmallocLocalAlloc (UINT uFlags, UINT uBytes)
{
    ASSERT((uFlags & (~GMALLOC_LMEM_FLAGS)) == 0);
    return (HLOCAL)((uFlags & LMEM_ZEROINIT) != 0
            ? gmallocURTLocalHeap.ZeroAlloc(uBytes)
            : gmallocURTLocalHeap.    Alloc(uBytes)
            );
}

inline HLOCAL gmallocLocalReAlloc (HLOCAL hMem, UINT uBytes, UINT uFlags)
{
    ASSERT(uFlags == LMEM_MOVEABLE);
    return (HLOCAL)gmallocURTLocalHeap.ReAlloc((LPVOID)hMem, uBytes);
}

inline HLOCAL gmallocLocalFree (HLOCAL hMem)
{
    gmallocURTLocalHeap.Free((LPVOID)hMem);
    return NULL;
}


extern "C" {

HLOCAL
CLocalAlloc(
    UINT uFlags,
    UINT uBytes
    )
{
    return gmallocLocalAlloc(uFlags, uBytes);
}

HLOCAL
CLocalReAlloc(
    HLOCAL hMem,
    UINT uBytes,
    UINT uFlags
    )
{
    return gmallocLocalReAlloc(hMem, uBytes, uFlags);
}

HLOCAL
CLocalFree(
    HLOCAL hMem
    )
{
    return gmallocLocalFree(hMem);
}

HANDLE
CHeapCreate(
    DWORD flOptions,
    DWORD dwInitialSize,
    DWORD dwMaximumSize
    )
{
    return gmallocHeapCreate("URTUnknown", flOptions, dwInitialSize, dwMaximumSize);
}

BOOL
CHeapDestroy(
    HANDLE hHeap
    )
{
    return gmallocHeapDestroy(hHeap);
}

LPVOID
CHeapAlloc(
    HANDLE hHeap,
    DWORD dwFlags,
    DWORD dwBytes
    )
{
    return gmallocHeapAlloc(hHeap, dwFlags, dwBytes);
}

LPVOID
CHeapReAlloc(
    HANDLE hHeap,
    DWORD dwFlags,
    LPVOID lpMem,
    DWORD dwBytes
    )
{
    return gmallocHeapReAlloc(hHeap, dwFlags, lpMem, dwBytes);
}

BOOL
CHeapFree(
    HANDLE hHeap,
    DWORD dwFlags,
    LPVOID lpMem
    )
{
    return gmallocHeapFree(hHeap, dwFlags, lpMem);
}

HANDLE
CGetProcessHeap( VOID )
{
    return gmallocGetProcessHeap();
}

UINT
CLocalSize(
    HLOCAL hMem
    )
{
    return chunksize(mem2chunk(hMem))-SIZE_SZ;
}

DWORD
CHeapSize(
    HANDLE hHeap,
    DWORD dwFlags,
    LPCVOID lpMem
    )
{
    return chunksize(mem2chunk(lpMem))-SIZE_SZ;
}

}

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gmheap.hpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
// gmalloc.hpp
//
// Created 11/16/96
//

#ifndef __GMALLOC_HPP__
#define __GMALLOC_HPP__


#ifndef Void_t
#define Void_t      void
#endif /*Void_t*/

/* SVID2/XPG mallinfo structure */

struct mallinfo {
  int arena;    /* total space allocated from system */
  int ordblks;  /* number of non-inuse chunks */
  int smblks;   /* unused -- always zero */
  int hblks;    /* number of mmapped regions */
  int hblkhd;   /* total space in mmapped regions */
  int usmblks;  /* unused -- always zero */
  int fsmblks;  /* unused -- always zero */
  int uordblks; /* total allocated space */
  int fordblks; /* total non-inuse space */
  int keepcost; /* top-most, releasable (via malloc_trim) space */
};

/* mallopt options that actually do something */

#define M_TRIM_THRESHOLD    -1
#define M_TOP_PAD           -2
#define M_MMAP_THRESHOLD    -3
#define M_MMAP_MAX          -4

#define NAV             128   /* number of bins */


#define GM_FIXED_HEAP        1

#ifndef HAVE_MMAP
#define HAVE_MMAP 0
#endif

typedef struct malloc_chunk* mbinptr;
typedef struct malloc_chunk* mchunkptr;
typedef int (*gmhook_fn) (BYTE*, size_t, ptrdiff_t);
typedef int (*gmprehook_fn) (size_t);

class gmallocHeap
{
    mbinptr av_[NAV*2+2];


    /*  Other bookkeeping data */

    /* variables holding tunable values */

    unsigned long trim_threshold;
    unsigned long top_pad;
    unsigned int  n_mmaps_max;
    unsigned long mmap_threshold;
    unsigned int  gmFlags;

    /* The first value returned from sbrk */
    char* sbrk_base;

    /* The maximum memory obtained from system via sbrk */
    unsigned long max_sbrked_mem;

    /* The maximum via either sbrk or mmap */
    unsigned long max_total_mem;

    /* internal working copy of mallinfo */
    struct mallinfo current_mallinfo;

    struct mallinfo gmallinfo (void);

#if HAVE_MMAP
    /* Tracking mmaps */
    unsigned int max_n_mmaps;
    unsigned long max_mmapped_mem;
#endif
    unsigned int n_mmaps;
    unsigned long mmapped_mem;

    struct GmListElement* head;
    void* gNextAddress;
    void* gAddressBase;
    union
    {
        unsigned int gAllocatedSize;
        DWORD gInitialReserve;
    };
    char *       gName;
    gmhook_fn gVirtualAllocedHook;
    gmprehook_fn gPreVirtualAllocHook;

protected:

    GmListElement* makeGmListElement (void* bas);
    void gcleanup ();


    Void_t* gcalloc (size_t, size_t);
    void    gfree (Void_t*);
    Void_t* gmalloc (size_t bytes);
    Void_t* gmemalign (size_t, size_t);
    Void_t* grealloc (Void_t*, size_t);
    Void_t* gvalloc (size_t);
    int     gmallopt (int, int);

    void do_check_chunk(mchunkptr p);
    void do_check_free_chunk(mchunkptr p);
    void do_check_inuse_chunk(mchunkptr p);
    void malloc_extend_top(size_t nb);
    void do_check_malloced_chunk(mchunkptr p, size_t s);
    int malloc_trim(size_t pad);
    size_t malloc_usable_size(Void_t* mem);
    void malloc_update_mallinfo();
    void malloc_stats();

    // ContigSize - This represents the amount of contiguous space required by the manager.
    // AppendSize - If wsbrk can allocate AppendSize contiguously to already allocated memory,
    //                      it suffices to allocate this amount. This parameter is ignored if ContigSize is <= 0.
    //                      (AppendSize < ContigSize) must hold on entry.
    //                      On return, AppendSize contains the actual amount allocated (either ContigSize
    //                      or AppendSize).
    void* wsbrk (long ContigSize, unsigned long& AppendSize);

public:

    HRESULT Init (char * name, DWORD InitialReserve = 2*1024*1024, DWORD flags = 0);
    HRESULT Init (char * name, DWORD* address, DWORD Size, gmhook_fn fn=0, 
                  gmprehook_fn prefn=0, DWORD flags = 0);

    VOID Finalize ();

    LPVOID Alloc (DWORD Size) { return gmalloc(Size); }
    LPVOID ZeroAlloc (DWORD Size) { return gcalloc(1,Size); }
    LPVOID AlignedAlloc (DWORD Alignment, DWORD Size) { return gmemalign(Alignment, Size); }

    LPVOID ReAlloc (LPVOID pMem, DWORD NewSize ) { return grealloc(pMem, NewSize); }

    VOID Free (LPVOID pMem) { gfree(pMem); }

    VOID DumpHeap () { Validate(TRUE); }
    VOID Validate (BOOL dumpleaks = FALSE);
};


#endif /* __GMALLOC_HPP__ */
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gms.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#include "common.h"
#include "..\\fjit\\helperframe.cpp"
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gms.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#include "..\\fjit\\helperFrame.h"
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\gcsmppriv.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
// optimize for speed
#ifndef _DEBUG
#pragma optimize( "t", on )
#endif
#define inline __forceinline


#include "wsperf.h"
#include "PerfCounters.h"
#include "gmheap.hpp"
#include "log.h"
#include "eeconfig.h"
#include "gc.h"


#ifdef GC_PROFILING
#include "profilepriv.h"
#endif

#ifdef _IA64_
#define RetailDebugBreak()  DebugBreak()
#elif defined(_X86_)

#ifdef _DEBUG
inline void RetailDebugBreak()   {_asm int 3}
#else
inline void RetailDebugBreak()   FATAL_EE_ERROR()
#endif

#else // _X86_
#define RetailDebugBreak()    
#endif

#pragma inline_depth(20)
/* the following section defines the optional features */


#define INTERIOR_POINTERS   //Allow interior pointers in the code manager

#define FREE_LIST_0         //Generation 0 can allocate from free list

#define FFIND_OBJECT        //faster find_object, slower allocation
#define FFIND_DECAY  7      //Number of GC for which fast find will be active

//#define STRESS_PINNING    //Stress pinning by pinning randomly

//#define DUMP_OBJECTS      //Dump objects from the heap

//#define TRACE_GC          //debug trace gc operation

//#define CATCH_GC          //catches exception during GC

//#define TIME_GC           //time allocation and garbage collection
//#define TIME_WRITE_WATCH  //time GetWriteWatch and ResetWriteWatch calls
//#define COUNT_CYCLES  //Use cycle counter for timing
//#define TIME_CPAUSE     //Use cycle counter to time pauses.

/* End of optional features */

#ifdef _DEBUG
#define TRACE_GC
#endif

#define NUMBERGENERATIONS   5               //Max number of generations

//Please leave these definitions intact.

#ifdef CreateFileMapping

#undef CreateFileMapping

#endif //CreateFileMapping

#define CreateFileMapping WszCreateFileMapping

#ifdef CreateSemaphore

#undef CreateSemaphore

#endif //CreateSemaphore

#define CreateSemaphore WszCreateSemaphore

#ifdef CreateEvent

#undef CreateEvent

#endif //ifdef CreateEvent

#define CreateEvent WszCreateEvent

#ifdef memcpy
#undef memcpy
#endif //memcpy



#define THREAD_NUMBER_ARG 
#define THREAD_NUMBER_FROM_CONTEXT
#define THREAD_FROM_HEAP 
#define HEAP_FROM_THREAD  gc_heap* hpt = 0;

#ifdef TRACE_GC


extern int     print_level;
extern int     gc_count;
extern BOOL    trace_gc;


class hlet 
{
    static hlet* bindings;
    int prev_val;
    int* pval;
    hlet* prev_let;
public:
    hlet (int& place, int value)
    {
        prev_val = place;
        pval = &place;
        place = value;
        prev_let = bindings;
        bindings = this;
    }
    ~hlet ()
    {
        *pval = prev_val;
        bindings = prev_let;
    }
};


#define let(p,v) hlet __x = hlet (p, v);

#else //TRACE_GC

#define gc_count    -1
#define let(s,v)

#endif //TRACE_GC

#ifdef TRACE_GC
//#include "log.h"
//#define dprintf(l,x) {if (trace_gc &&(l<=print_level)) {LogSpewAlways x;LogSpewAlways ("\n");}}
#define dprintf(l,x) {if (trace_gc && (l<=print_level)) {printf ("\n");printf x ; fflush(stdout);}}
#else //TRACE_GC
#define dprintf(l,x)
#endif //TRACE_GC


#undef  assert
#define assert _ASSERTE
#undef  ASSERT
#define ASSERT _ASSERTE


#ifdef _DEBUG

struct GCDebugSpinLock {
    long    lock;           // -1 if free, 0 if held
    Thread* holding_thread; // -1 if no thread holds the lock.
};
typedef GCDebugSpinLock GCSpinLock;
#define SPIN_LOCK_INITIALIZER {-1, (Thread*) -1}

#else

typedef long GCSpinLock;
#define SPIN_LOCK_INITIALIZER -1

#endif



class mark;
class heap_segment;
class CObjectHeader;
class large_object_block;
class l_heap;
class sorted_table;
class f_page_list;
class page_manager;
class c_synchronize;

class generation
{
public:
    // Don't move these first two fields without adjusting the references
    // from the __asm in jitinterface.cpp.
    alloc_context   allocation_context;
    heap_segment*   allocation_segment;
    BYTE*           free_list;
    heap_segment*   start_segment;
    BYTE*           allocation_start;
    BYTE*           plan_allocation_start;
    BYTE*           last_gap;
    size_t          free_list_space;
    size_t          allocation_size;

};

class dynamic_data
{
public:
    ptrdiff_t new_allocation;
    size_t    current_size;
    size_t    desired_allocation;
    size_t    collection_count;
    size_t    promoted_size;
};


//class definition of the internal class
class gc_heap
{
   friend GCHeap;
   friend CFinalize;
   friend void ProfScanRootsHelper(Object*&, ScanContext*, DWORD);
   friend void GCProfileWalkHeap();

#ifdef WRITE_BARRIER_CHECK
	friend void checkGCWriteBarrier();
	friend void initGCShadow();
#endif 


	typedef void (* card_fn) (BYTE**);
#define call_fn(fn) (*fn)
#define __this (gc_heap*)0


public:
    static
    void verify_heap();

    static
    heap_segment* make_heap_segment (BYTE* new_pages, size_t size);
    static 
    l_heap* make_large_heap (BYTE* new_pages, size_t size, BOOL managed);
    
    static 
    gc_heap* make_gc_heap();
    
    static 
    void destroy_gc_heap(gc_heap* heap);

    static 
    HRESULT initialize_gc  (size_t vm_block_size,
                            size_t segment_size,
                            size_t heap_size);

    static
    void shutdown_gc();

    static
    CObjectHeader* allocate (size_t jsize,
                             alloc_context* acontext );

    CObjectHeader* try_fast_alloc (size_t jsize);

    static
    CObjectHeader* allocate_large_object (size_t size, BOOL pointerp, alloc_context* acontext);

    static
    int garbage_collect (int n);
    static 
    int grow_brick_card_tables (BYTE* start, BYTE* end);
    
    static 
        DWORD __stdcall gc_thread_stub (void* arg);

    static
    BOOL is_marked (BYTE* o);
    
protected:

    static void user_thread_wait (HANDLE event);


#if defined (GC_PROFILING) || defined (DUMP_OBJECTS)

    static
    void walk_heap (walk_fn fn, void* context, int gen_number, BOOL walk_large_object_heap_p);

    static
    void walk_relocation (int condemned_gen_number,
                                   BYTE* first_condemned_address, void *pHeapId);

    static
    void walk_relocation_in_brick (BYTE* tree,  BYTE*& last_plug, void *pHeapId);

#endif //GC_PROFILING || DUMP_OBJECTS


    static
    int generation_to_condemn (int n);

    static
    void gc1();

	static
	heap_segment* get_segment ();

	static
	l_heap* get_heap();

    static
    size_t limit_from_size (size_t size, size_t room);
    static
    BOOL allocate_more_space (alloc_context* acontext, size_t jsize);

    static
    int init_semi_shared();
    static
    int init_gc_heap ();
    static
    void self_destroy();
    static
    void destroy_semi_shared();
    static
    void fix_youngest_allocation_area ();
    static
    void fix_allocation_context (alloc_context* acontext);
    static
    void fix_older_allocation_area (generation* older_gen);
    static
    void set_allocation_heap_segment (generation* gen);
    static
    void reset_allocation_pointers (generation* gen, BYTE* start);
    static
    unsigned int object_gennum (BYTE* o);
    static
    void delete_heap_segment (heap_segment* seg);
    static
    void delete_large_heap (l_heap* hp);
    static
    void reset_heap_segment_pages (heap_segment* seg);
    static
    void decommit_heap_segment_pages (heap_segment* seg);
    static
    void rearrange_heap_segments();
    static
    void reset_write_watch ();
    static
    void adjust_ephemeral_limits ();
    static
    void make_generation (generation& gen, heap_segment* seg,
                          BYTE* start, BYTE* pointer);
    static
    int heap_grow_hook (BYTE* mem, size_t memsize, ptrdiff_t delta);

    static
    int heap_pregrow_hook (size_t memsize);

    static
    BOOL size_fit_p (size_t size, BYTE* alloc_pointer, BYTE* alloc_limit);
    static
    BOOL a_size_fit_p (size_t size, BYTE* alloc_pointer, BYTE* alloc_limit);
    static
    size_t card_of ( BYTE* object);
    static
    BYTE* brick_address (size_t brick);
    static
    size_t brick_of (BYTE* add);
    static
    BYTE* card_address (size_t card);
    static
    size_t card_to_brick (size_t card);
    static
    void clear_card (size_t card);
    static
    void set_card (size_t card);
    static
    BOOL  card_set_p (size_t card);
    static
    int grow_heap_segment (heap_segment* seg, size_t size);
    static
    void copy_brick_card_range (BYTE* la, BYTE* old_card_table,
                                short* old_brick_table,
                                heap_segment* seg,
                                BYTE* start, BYTE* end, BOOL heap_expand);
    static
    void copy_brick_card_table_l_heap ();
    static
    void copy_brick_card_table(BOOL heap_expand);
    static
    void clear_brick_table (BYTE* from, BYTE* end);
    static
    void set_brick (size_t index, ptrdiff_t val);

    static
    void adjust_limit (BYTE* start, size_t limit_size, generation* gen);

    static
    void adjust_limit_clr (BYTE* start, size_t limit_size, 
                           alloc_context* acontext, heap_segment* seg);
    static
    void thread_scavenge_list (BYTE* list);
    static
    BYTE* allocate_in_older_generation (size_t size);
	static 
	BYTE* gc_heap::allocate_semi_space (size_t size);
    static
    generation*  ensure_ephemeral_heap_segment (generation* consing_gen);
    static
    BYTE* allocate_in_condemned_generations (generation* gen,
                                             size_t size,
                                             int from_gen_number);
#if defined (INTERIOR_POINTERS) || defined (_DEBUG)
    static
    heap_segment* find_segment (BYTE* interior);
    static
    BYTE* find_object_for_relocation (BYTE* o, BYTE* low, BYTE* high);
#endif //INTERIOR_POINTERS

    static
    gc_heap* heap_of (BYTE* object, BOOL verify_p =
#ifdef _DEBUG
                      TRUE
#else
                      FALSE
#endif //_DEBUG
){return (gc_heap*)0;}

    static
    BYTE* find_object (BYTE* o, BYTE* low);

    static
    dynamic_data* dynamic_data_of (int gen_number);
    static
    ptrdiff_t  get_new_allocation (int gen_number);
    static
    void ensure_new_allocation (int size);
    static
    void make_mark_stack (mark* arr);

    static
    generation* generation_of (int  n);
    static
    BOOL gc_mark1 (BYTE* o);
    static
    BOOL gc_mark (BYTE* o, BYTE* low, BYTE* high);
    static
    BYTE* mark_object(BYTE* o );
    static
    BYTE* mark_object_class (BYTE* o );
    static
    void mark_object_simple (BYTE** o );
    static
    void copy_object_simple (BYTE** o );
    static
    void copy_object_simple_const (BYTE** o );
    static
    void get_copied_object (BYTE** o );
    static
    void scavenge_object_simple (BYTE* o );
    static
    void mark_object_internal (BYTE* o );
    static
    BYTE* next_end (heap_segment* seg, BYTE* f);
    static
    void fix_card_table ();
    static
    void verify_card_table ();
    static
    void mark_through_object (BYTE* oo );
    static
    BOOL process_mark_overflow (int condemned_gen_number);
    static
    void mark_phase (int condemned_gen_number, BOOL mark_only_p);

	static
	void copy_phase (int n);

	struct scavenge_context
	{
		BYTE* first_object;
		BYTE* limit;
		scavenge_context(){limit = 0; first_object = 0;}
	};

	static
	void scavenge_phase(scavenge_context* sc);
    static
    void pin_object (BYTE* o, BYTE* low, BYTE* high);
    static
    void reset_mark_stack ();
    static
    BYTE* insert_node (BYTE* new_node, size_t sequence_number,
                       BYTE* tree, BYTE* last_node);
    static
    size_t update_brick_table (BYTE* tree, size_t current_brick,
                               BYTE* x, BYTE* plug_end);

    static
    void plan_generation_start (generation*& consing_gen);
    static
    void process_ephemeral_boundaries(BYTE* x, int& active_new_gen_number,
                                      int& active_old_gen_number,
                                      generation*& consing_gen,
                                      BOOL& allocate_in_condemned,
                                      BYTE*& free_gap, BYTE* zero_limit=0);
    static
    void sweep_phase (int condemned_gen_number);

    static
    void scavenge_pinned_objects (BOOL free_list_p);

    static
    void fix_generation_bounds (int condemned_gen_number, 
                                generation* consing_gen, 
                                BOOL demoting);
    static
    BYTE* generation_limit (int gen_number);

    static
    BYTE* allocate_at_end (size_t size);
	static
    void thread_gap (BYTE* gap_start, size_t size);
    static
    void make_unused_array (BYTE* x, size_t size);

    static
    void  gcmemcopy (BYTE* dest, BYTE* src, size_t len, BOOL copy_cards_p);

    static
    void clear_cards (size_t start_card, size_t end_card);
    static
    void clear_card_for_addresses (BYTE* start_address, BYTE* end_address);
    static
    void copy_cards (size_t dst_card, size_t src_card,
                     size_t end_card, BOOL nextp);
    static
    void copy_cards_for_addresses (BYTE* dest, BYTE* src, size_t len);
    static
    BOOL ephemeral_pointer_p (BYTE* o);
    static
    void fix_brick_to_highest (BYTE* o, BYTE* next_o);
    static
    BYTE* find_first_object (BYTE* start,  size_t brick, BYTE* min_address);
    static
    BYTE* compute_next_boundary (BYTE* low, int gen_number, BOOL relocating);
    static
    void copy_through_cards_helper (BYTE** poo, unsigned int& ngen, card_fn fn);

    static
    void copy_through_cards_for_segments (card_fn fn);

    static
    BYTE* expand_heap (heap_segment* seg, size_t size);
    static
    void init_dynamic_data ();
    static
    float surv_to_growth (float cst, float limit, float max_limit);
    static
    size_t desired_new_allocation (dynamic_data* dd, size_t in, size_t out, 
                                   float& cst, int gen_number);
    static
    size_t generation_size (int gen_number);
    static
    size_t  compute_promoted_allocation (int gen_number);
    static
    void compute_new_dynamic_data (int gen_number);
    static
    size_t new_allocation_limit (size_t size, size_t free_size);

    static
    size_t generation_sizes (generation* gen);
    static
    BOOL decide_on_compacting (int condemned_gen_number,
                               generation* consing_gen,
                               size_t fragmentation, 
                               BOOL& should_expand);
    static
    BOOL ephemeral_gen_fit_p (BOOL compacting=FALSE);
    static
    void RemoveBlock (large_object_block* item, BOOL pointerp);
    static
    void InsertBlock (large_object_block** after, large_object_block* item,
                      BOOL pointerp);
    static
    void insert_large_pblock (large_object_block* bl);
    static
    void reset_large_object (BYTE* o);
    static
    void sweep_large_objects ();
    static
    void copy_through_cards_for_large_objects (card_fn fn);
    static
    void descr_segment (heap_segment* seg);
    static
    void descr_card_table ();
    static
    void descr_generations ();

    /* ------------------- per heap members --------------------------*/ 
public:

    static
    BYTE* card_table;

    static
    short* brick_table;

    static
    BYTE* lowest_address;

    static
    BYTE* highest_address;

protected:
    #define vm_heap ((GCHeap*)0)
    #define heap_number (0)
    static
    heap_segment* ephemeral_heap_segment;

    static
    int         condemned_generation_num;

	static 
	BYTE* 		scavenge_list;

	static 
	BYTE* 		last_scavenge;


	static 
	BOOL 		pinning;

    static
    BYTE*       gc_low; // lowest address being condemned

    static
    BYTE*       gc_high; //highest address being condemned


	static
	size_t		segment_size; //size of each heap segment

	static
	size_t		lheap_size;  //size of each lheap

    static
    size_t      mark_stack_tos;

    static
    size_t      mark_stack_bos;

    static
    size_t    mark_stack_array_length;

    static
    mark*       mark_stack_array;

    static
    BYTE*  min_overflow_address;

    static
    BYTE*  max_overflow_address;

    static
    size_t allocation_quantum;

    static
    int   alloc_contexts_used;

#define youngest_generation (generation_of (0))

    // The more_space_lock is used for 3 purposes:
    //
    // 1) to coordinate threads that exceed their quantum (UP & MP)
    // 2) to synchronize allocations of large objects
    // 3) to synchronize the GC itself
    //
    // As such, it has 3 clients:
    //
    // 1) Threads that want to extend their quantum.  This always takes the lock 
    //    and sometimes provokes a GC.
    // 2) Threads that want to perform a large allocation.  This always takes 
    //    the lock and sometimes provokes a GC.
    // 3) GarbageCollect takes the lock and then unconditionally provokes a GC by 
    //    calling GarbageCollectGeneration.
    //
    static
    GCSpinLock more_space_lock; //lock while allocating more space

    static
    dynamic_data dynamic_data_table [NUMBERGENERATIONS+1];


    //Large object support

    static
    l_heap* lheap;

#ifdef CONCURRENT_GC
    static
    GCSpinLock lheap_lock;
#endif //CONCURRENT_GC

    static
    BYTE* lheap_card_table;

    static
    gmallocHeap* gheap;

    static
    large_object_block* large_p_objects;

    static
    large_object_block** last_large_p_object;

    static
    large_object_block* large_np_objects;

    static
    size_t large_objects_size;

    static
    size_t large_blocks_size;

    static
    BOOL gen0_bricks_cleared;
#ifdef FFIND_OBJECT
    static
    int gen0_must_clear_bricks;
#endif //FFIND_OBJECT


    static
    CFinalize* finalize_queue;

    /* ----------------------- global members ----------------------- */
public:


    static 
    int g_max_generation;
    
    static 
    size_t reserved_memory;
    static
    size_t reserved_memory_limit;

}; // class gc_heap


class CFinalize
{
    friend struct MEMBER_OFFSET_INFO(CFinalize);
private:

    Object** m_Array;
    Object** m_FillPointers[NUMBERGENERATIONS+2];
    Object** m_EndArray;
    int m_PromotedCount;
    long lock;

#ifdef COLLECT_CLASSES
    ListSingle  listFinalizableClasses;
    ListSingle  listDeletableClasses;
#endif //COLLECT_CLASSES

    BOOL GrowArray();
    void MoveItem (Object** fromIndex,
                   unsigned int fromSeg,
                   unsigned int toSeg);

    BOOL IsSegEmpty ( unsigned int i)
    {
        ASSERT ( i <= NUMBERGENERATIONS+1);
        return ((i==0) ?
                (m_FillPointers[0] == m_Array):
                (m_FillPointers[i] == m_FillPointers[i-1]));
    }

public:
    CFinalize ();
    ~CFinalize();
    void EnterFinalizeLock();
    void LeaveFinalizeLock();
    void RegisterForFinalization (int gen, Object* obj);
    Object* GetNextFinalizableObject ();
    BOOL ScanForFinalization (int gen, int passnum, BOOL mark_only_p,
                              gc_heap* hp);
    void RelocateFinalizationData (int gen, gc_heap* hp);
    void GcScanRoots (promote_func* fn, int hn, ScanContext* sc);
    void UpdatePromotedGenerations (int gen, BOOL gen_0_empty_p);
    int  GetPromotedCount();

    //Methods used by the shutdown code to call every finalizer
    void SetSegForShutDown(BOOL fHasLock);
    size_t GetNumberFinalizableObjects();
    
    //Methods used by the app domain unloading call to finalize objects in an app domain
    BOOL FinalizeAppDomain (AppDomain *pDomain, BOOL fRunFinalizers);

    void CheckFinalizerObjects();
};

inline
 size_t& dd_current_size (dynamic_data* inst)
{
  return inst->current_size;
}
inline
size_t& dd_desired_allocation (dynamic_data* inst)
{
  return inst->desired_allocation;
}
inline
size_t& dd_collection_count (dynamic_data* inst)
{
    return inst->collection_count;
}
inline
size_t& dd_promoted_size (dynamic_data* inst)
{
    return inst->promoted_size;
}
inline
ptrdiff_t& dd_new_allocation (dynamic_data* inst)
{
  return inst->new_allocation;
}

#define max_generation          gc_heap::g_max_generation

inline 
alloc_context* generation_alloc_context (generation* inst)
{
    return &(inst->allocation_context);
}
inline
size_t dd_freach_previous_promotion (dynamic_data* inst)
{
  return 0;
}

inline
BYTE*& generation_allocation_start (generation* inst)
{
  return inst->allocation_start;
}
inline
BYTE*& generation_allocation_pointer (generation* inst)
{
  return inst->allocation_context.alloc_ptr;
}
inline
BYTE*& generation_allocation_limit (generation* inst)
{
  return inst->allocation_context.alloc_limit;
}
inline
BYTE*& generation_free_list (generation* inst)
{
  return inst->free_list;
}
inline
heap_segment*& generation_start_segment (generation* inst)
{
  return inst->start_segment;
}
inline
heap_segment*& generation_allocation_segment (generation* inst)
{
  return inst->allocation_segment;
}
inline
BYTE*& generation_plan_allocation_start (generation* inst)
{
  return inst->plan_allocation_start;
}
inline
BYTE*& generation_last_gap (generation* inst)
{
  return inst->last_gap;
}
inline
size_t& generation_free_list_space (generation* inst)
{
  return inst->free_list_space;
}
inline
size_t& generation_allocation_size (generation* inst)
{
  return inst->allocation_size;
}

#define plug_skew           sizeof(DWORD)   // syncblock size. 
#define min_obj_size        (sizeof(BYTE*)+plug_skew+sizeof(size_t))//syncblock + vtable+ first field
#define min_free_list       (sizeof(BYTE*)+min_obj_size) //Need one slot more
//Note that this encodes the fact that plug_skew is a multiple of BYTE*.
struct plug
{
    BYTE *  skew[sizeof(plug_skew) / sizeof(BYTE *)];
};


//need to be careful to keep enough pad items to fit a relocation node 
//padded to QuadWord before the plug_skew
class heap_segment
{
public:
    BYTE*           allocated;
    BYTE*           committed;
    BYTE*           reserved;
    BYTE*           used;
    BYTE*           mem;
    heap_segment*   next;
    BYTE*           plan_allocated;
    BYTE*           padx;

    BYTE*           pad0;
#if (SIZEOF_OBJHEADER % 8) != 0
    BYTE            pad1[8 - (SIZEOF_OBJHEADER % 8)];   // Must pad to quad word
#endif
    plug            plug;
};

inline
BYTE*& heap_segment_reserved (heap_segment* inst)
{
  return inst->reserved;
}
inline
BYTE*& heap_segment_committed (heap_segment* inst)
{
  return inst->committed;
}
inline
BYTE*& heap_segment_used (heap_segment* inst)
{
  return inst->used;
}
inline
BYTE*& heap_segment_allocated (heap_segment* inst)
{
  return inst->allocated;
}
inline
heap_segment*& heap_segment_next (heap_segment* inst)
{
  return inst->next;
}
inline
BYTE*& heap_segment_mem (heap_segment* inst)
{
  return inst->mem;
}
inline
BYTE*& heap_segment_plan_allocated (heap_segment* inst)
{
  return inst->plan_allocated;
}


extern "C" {
extern generation   generation_table [NUMBERGENERATIONS];
}

inline
generation* gc_heap::generation_of (int  n)
{
    assert (((n <= max_generation) && (n >= 0)));
    return &generation_table [ n ];
}


inline
dynamic_data* gc_heap::dynamic_data_of (int gen_number)
{
    return &dynamic_data_table [ gen_number ];
}

//This is a hack to avoid changing gcee.cpp for now. 
#if defined (CONCURRENT_GC)
#undef CONCURRENT_GC
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\guardpagehelper.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*============================================================
 **
 ** Header:  GuardPageHelper.h
 **
 ** Purpose: Routines for resetting the stack after stack overflow.
 **
 ** Date:  Mar 7, 2000
 **
 ===========================================================*/

#ifndef __guardpagehelper_h__
#define __guardpagehelper_h__
class GuardPageHelper {
public:
    // Returns true if guard page can be reset after stack is set to this value.
    static BOOL CanResetStackTo(LPCVOID StackPointer);

    // Called AFTER stack is reset to re-establish guard page.  This function preserves
    // all of the callers scratch registers as well.
    static VOID ResetGuardPage();
};

#endif // __guardpagehelper_h__
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\handletable.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*
 * Generational GC handle manager.  Main Entrypoint Layer.
 *
 * Implements generic support for external roots into a GC heap.
 *
 * francish
 */

#include "common.h"
#include "HandleTablePriv.h"
#include "PerfCounters.h"
#include "EEConfig.h"


/****************************************************************************
 *
 * FORWARD DECLARATIONS
 *
 ****************************************************************************/

#ifdef _DEBUG
void DEBUG_PostGCScanHandler(HandleTable *pTable, const UINT *types, UINT typeCount, UINT condemned, UINT maxgen, ScanCallbackInfo *info);
void DEBUG_LogScanningStatistics(HandleTable *pTable, DWORD level);
void DEBUG_TrackAlloc(OBJECTHANDLE handle);
void DEBUG_TrackFree(OBJECTHANDLE handle);
void DEBUG_TrackInit();
#endif

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * HELPER ROUTINES
 *
 ****************************************************************************/

/*
 * Table
 *
 * Gets and validates the table pointer from a table handle.
 *
 */
__inline HandleTable *Table(HHANDLETABLE hTable)
{
    // convert the handle to a pointer
    HandleTable *pTable = (HandleTable *)hTable;

    // sanity
    _ASSERTE(pTable);

    // return the table pointer
    return pTable;
}


/*
 * CallHandleEnumProc
 *
 * Calls a HNDENUMPROC for the specified handle.
 *
 */
void CALLBACK CallHandleEnumProc(_UNCHECKED_OBJECTREF *pref, LPARAM *pUserData, LPARAM param1, LPARAM param2)
{
    // fetch the enum procedure from param1
    HNDENUMPROC pfnEnum = (HNDENUMPROC)param1;

    // call it with the other params
    pfnEnum((OBJECTHANDLE)pref, pUserData ? *pUserData : NULL, param2);
}

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * MAIN ENTRYPOINTS
 *
 ****************************************************************************/

/*
 * HndCreateHandleTable
 *
 * Alocates and initializes a handle table.
 *
 */
HHANDLETABLE HndCreateHandleTable(UINT *pTypeFlags, UINT uTypeCount, UINT uADIndex)
{
    // sanity
    _ASSERTE(uTypeCount);

    // verify that we can handle the specified number of types
    if (uTypeCount > HANDLE_MAX_PUBLIC_TYPES)
    {
        // may need to increase HANDLE_MAX_INTERNAL_TYPES (by 4)
        _ASSERTE(FALSE);
        return NULL;
    }

    // verify that segment header layout we're using fits expected size
    if (sizeof(_TableSegmentHeader) > HANDLE_HEADER_SIZE)
    {
        // if you hit this then TABLE LAYOUT IS BROKEN - may want to contact FrancisH
        _ASSERTE(FALSE);
        return NULL;
    }

    // compute the size of the handle table allocation
    DWORD32 dwSize = sizeof(HandleTable) + (uTypeCount * sizeof(HandleTypeCache));

    // allocate the table
    HandleTable *pTable = (HandleTable *)LocalAlloc(LPTR, dwSize);

    // if that failed then we are out of business
    if (!pTable)
    {
        // not much we can do really
        _ASSERTE(FALSE);
        return NULL;
    }

    // allocate the initial handle segment
    pTable->pSegmentList = SegmentAlloc(pTable);

    // if that failed then we are also out of business
    if (!pTable->pSegmentList)
    {
        // free the table's memory and get out
        LocalFree((HLOCAL)pTable);
        return NULL;
    }

    // initialize the table's lock
    pTable->pLock = new (pTable->_LockInstance) Crst("GC Heap Handle Table Lock", CrstHandleTable, TRUE, FALSE);

    // remember how many types we are supporting
    pTable->uTypeCount = uTypeCount;

    // Store user data
    pTable->uTableIndex = -1;
    pTable->uADIndex = uADIndex;

    // loop over various arrays an initialize them
    UINT u;

    // initialize the type flags for the types we were passed
    for (u = 0; u < uTypeCount; u++)
        pTable->rgTypeFlags[u] = pTypeFlags[u];

    // preinit the rest to HNDF_NORMAL
    while (u < HANDLE_MAX_INTERNAL_TYPES)
        pTable->rgTypeFlags[u++] = HNDF_NORMAL;

    // initialize the main cache
    for (u = 0; u < uTypeCount; u++)
    {
        // at init time, the only non-zero field in a type cache is the free index
        pTable->rgMainCache[u].lFreeIndex = HANDLES_PER_CACHE_BANK;
    }

#ifdef _DEBUG
    // set up scanning stats
    pTable->_DEBUG_iMaxGen = -1;

    // Set up for tracking, if requested
    DEBUG_TrackInit();
#endif

    // all done - return the newly created table
    return (HHANDLETABLE)pTable;
}


/*
 * HndDestroyHandleTable
 *
 * Cleans up and frees the specified handle table.
 *
 */
void HndDestroyHandleTable(HHANDLETABLE hTable)
{
    // fetch the handle table pointer
    HandleTable *pTable = Table(hTable);

    // We are going to free the memory for this HandleTable.
    // Let us reset the copy in g_pHandleTableArray to NULL.
    // Otherwise, GC will think this HandleTable is still available.
    Ref_RemoveHandleTable (hTable);
    
    // null out the lock pointer and release and free the lock
    delete pTable->pLock;
    pTable->pLock = NULL;

    // fetch the segment list and null out the list pointer
    TableSegment *pSegment = pTable->pSegmentList;
    pTable->pSegmentList = NULL;

    // walk the segment list, freeing the segments as we go
    while (pSegment)
    {
        // fetch the next segment
        TableSegment *pNextSegment = pSegment->pNextSegment;

        // free the current one and advance to the next
        SegmentFree(pSegment);
        pSegment = pNextSegment;
    }

    // free the table's memory
    LocalFree((HLOCAL)pTable);
}

/*
 * HndGetHandleTableIndex
 *
 * Sets the index associated with a handle table at creation
 */
void HndSetHandleTableIndex(HHANDLETABLE hTable, UINT uTableIndex)
{
    // fetch the handle table pointer
    HandleTable *pTable = Table(hTable);

    pTable->uTableIndex = uTableIndex;
}

/*
 * HndGetHandleTableIndex
 *
 * Retrieves the index associated with a handle table at creation
 */
UINT HndGetHandleTableIndex(HHANDLETABLE hTable)
{
    // fetch the handle table pointer
    HandleTable *pTable = Table(hTable);

    return pTable->uTableIndex;
}

/*
 * HndGetHandleTableIndex
 *
 * Retrieves the AppDomain index associated with a handle table at creation
 */
UINT HndGetHandleTableADIndex(HHANDLETABLE hTable)
{
    // fetch the handle table pointer
    HandleTable *pTable = Table(hTable);

    return pTable->uADIndex;
}

/*
 * HndCreateHandle
 *
 * Entrypoint for allocating an individual handle.
 *
 */
OBJECTHANDLE HndCreateHandle(HHANDLETABLE hTable, UINT uType, OBJECTREF object, LPARAM lExtraInfo)
{
    // update perf-counters: track number of handles
    COUNTER_ONLY(GetGlobalPerfCounters().m_GC.cHandles ++);
    COUNTER_ONLY(GetPrivatePerfCounters().m_GC.cHandles ++);

    VALIDATEOBJECTREF(object);

    // fetch the handle table pointer
    HandleTable *pTable = Table(hTable);

    // sanity check the type index
    _ASSERTE(uType < pTable->uTypeCount);

    // get a handle from the table's cache
    OBJECTHANDLE handle = TableAllocSingleHandleFromCache(pTable, uType);

    // did the allocation succeed?
    if (handle)
    {
        // yep - the handle better not point at anything yet
        _ASSERTE(*(_UNCHECKED_OBJECTREF *)handle == NULL);

        // we are not holding the lock - check to see if there is nonzero extra info
        if (lExtraInfo)
        {
            // initialize the user data BEFORE assigning the referent
            // this ensures proper behavior if we are currently scanning
            HandleQuickSetUserData(handle, lExtraInfo);
        }

        // store the referent
        HndAssignHandle(handle, object);
    }
    else
        FailFast(GetThread(), FatalOutOfMemory);

#ifdef _DEBUG
    DEBUG_TrackAlloc(handle);
#endif

    // return the result
    return handle;
}


#ifdef _DEBUG
void ValidateFetchObjrefForHandle(OBJECTREF objref, UINT appDomainIndex)
{
    VALIDATEOBJECTREF (objref);
#if CHECK_APP_DOMAIN_LEAKS
    if (g_pConfig->AppDomainLeaks())
    {
        if (appDomainIndex)
            objref->AssignAppDomain(SystemDomain::GetAppDomainAtIndex(appDomainIndex));
        else if (objref != 0)
            objref->SetAppDomainAgile();
    }
#endif
}

void ValidateAssignObjrefForHandle(OBJECTREF objref, UINT appDomainIndex)
{
    VALIDATEOBJECTREF (objref);
#if CHECK_APP_DOMAIN_LEAKS
    if (g_pConfig->AppDomainLeaks())
    {
        if (appDomainIndex)
            objref->AssignAppDomain(SystemDomain::GetAppDomainAtIndex(appDomainIndex));
        else if (objref != 0)
            objref->SetAppDomainAgile();
    }
#endif
}
#endif

/*
 * HndDestroyHandle
 *
 * Entrypoint for freeing an individual handle.
 *
 */
void HndDestroyHandle(HHANDLETABLE hTable, UINT uType, OBJECTHANDLE handle)
{
#ifdef _DEBUG
    DEBUG_TrackFree(handle);
#endif

    // update perf-counters: track number of handles
    COUNTER_ONLY(GetGlobalPerfCounters().m_GC.cHandles --);
    COUNTER_ONLY(GetPrivatePerfCounters().m_GC.cHandles --);

    // sanity check handle we are being asked to free
    _ASSERTE(handle);

    // fetch the handle table pointer
    HandleTable *pTable = Table(hTable);

    // sanity check the type index
    _ASSERTE(uType < pTable->uTypeCount);

    // return the handle to the table's cache
    TableFreeSingleHandleToCache(pTable, uType, handle);
}


/*
 * HndDestroyHandleOfUnknownType
 *
 * Entrypoint for freeing an individual handle whose type is unknown.
 *
 */
void HndDestroyHandleOfUnknownType(HHANDLETABLE hTable, OBJECTHANDLE handle)
{
    // sanity check handle we are being asked to free
    _ASSERTE(handle);

    // fetch the type and then free normally
    HndDestroyHandle(hTable, HandleFetchType(handle), handle);
}


/*
 * HndCreateHandles
 *
 * Entrypoint for allocating handles in bulk.
 *
 */
UINT HndCreateHandles(HHANDLETABLE hTable, UINT uType, OBJECTHANDLE *pHandles, UINT uCount)
{
    // fetch the handle table pointer
    HandleTable *pTable = Table(hTable);

    // sanity check the type index
    _ASSERTE(uType < pTable->uTypeCount);

    // keep track of the number of handles we've allocated
    UINT uSatisfied = 0;

    // if this is a large number of handles then bypass the cache
    if (uCount > SMALL_ALLOC_COUNT)
    {
        // acquire the handle manager lock
        pTable->pLock->Enter();

        // allocate handles in bulk from the main handle table
        uSatisfied = TableAllocBulkHandles(pTable, uType, pHandles, uCount);

        // release the handle manager lock
        pTable->pLock->Leave();
    }

    // do we still need to get some handles?
    if (uSatisfied < uCount)
    {
        // get some handles from the cache
        uSatisfied += TableAllocHandlesFromCache(pTable, uType, pHandles + uSatisfied, uCount - uSatisfied);
    }

    // update perf-counters: track number of handles
    COUNTER_ONLY(GetGlobalPerfCounters().m_GC.cHandles += uSatisfied);
    COUNTER_ONLY(GetPrivatePerfCounters().m_GC.cHandles += uSatisfied);

    // return the number of handles we allocated
    return uSatisfied;
}


/*
 * HndDestroyHandles
 *
 * Entrypoint for freeing handles in bulk.
 *
 */
void HndDestroyHandles(HHANDLETABLE hTable, UINT uType, const OBJECTHANDLE *pHandles, UINT uCount)
{
    // fetch the handle table pointer
    HandleTable *pTable = Table(hTable);

    // sanity check the type index
    _ASSERTE(uType < pTable->uTypeCount);

    // is this a small number of handles?
    if (uCount <= SMALL_ALLOC_COUNT)
    {
        // yes - free them via the handle cache
        TableFreeHandlesToCache(pTable, uType, pHandles, uCount);
        return;
    }

    // acquire the handle manager lock
    pTable->pLock->Enter();

    // free the unsorted handles in bulk to the main handle table
    TableFreeBulkUnpreparedHandles(pTable, uType, pHandles, uCount);

    // update perf-counters: track number of handles
    COUNTER_ONLY(GetGlobalPerfCounters().m_GC.cHandles -= uCount);
    COUNTER_ONLY(GetPrivatePerfCounters().m_GC.cHandles -= uCount);

    // release the handle manager lock
    pTable->pLock->Leave();
}


/*
 * HndSetHandleExtraInfo
 *
 * Stores owner data with handle.
 *
 */
void HndSetHandleExtraInfo(OBJECTHANDLE handle, UINT uType, LPARAM lExtraInfo)
{
    // fetch the user data slot for this handle if we have the right type
    LPARAM *pUserData = HandleValidateAndFetchUserDataPointer(handle, uType);

    // is there a slot?
    if (pUserData)
    {
        // yes - store the info
        *pUserData = lExtraInfo;
    }
}


/*
 * HndGetHandleExtraInfo
 *
 * Retrieves owner data from handle.
 *
 */
LPARAM HndGetHandleExtraInfo(OBJECTHANDLE handle)
{
    // assume zero until we actually get it
    LPARAM lExtraInfo = 0L;

    // fetch the user data slot for this handle
    LPARAM *pUserData = HandleQuickFetchUserDataPointer(handle);

    // if we did then copy the value
    if (pUserData)
        lExtraInfo = *pUserData;

    // return the value to our caller
    return lExtraInfo;
}

/*
 * HndGetHandleTable
 *
 * Returns the containing table of a handle.
 * 
 */
HHANDLETABLE HndGetHandleTable(OBJECTHANDLE handle)
{
    HandleTable *pTable = HandleFetchHandleTable(handle);

    return (HHANDLETABLE)pTable;
}

/*
 * HndWriteBarrier
 *
 * Resets the generation number for the handle's clump to zero.
 *
 */
void HndWriteBarrier(OBJECTHANDLE handle)
{
    // find the write barrier for this handle
    BYTE *barrier = (BYTE *)((UINT_PTR)handle & HANDLE_SEGMENT_ALIGN_MASK);

    // sanity
    _ASSERTE(barrier);

    // find the offset of this handle into the segment
    UINT_PTR offset = (UINT_PTR)handle & HANDLE_SEGMENT_CONTENT_MASK;

    // make sure it is in the handle area and not the header
    _ASSERTE(offset >= HANDLE_HEADER_SIZE);

    // compute the clump index for this handle
    offset = (offset - HANDLE_HEADER_SIZE) / (HANDLE_SIZE * HANDLE_HANDLES_PER_CLUMP);

    // zero the generation byte for this handle's clump
    barrier[offset] = 0;
}


/*
 * HndEnumHandles
 *
 * Enumerates all handles of the specified type in the handle table.
 *
 * This entrypoint is provided for utility code (debugger support etc) that
 * needs to enumerate all roots in the handle table.
 *
 */
void HndEnumHandles(HHANDLETABLE hTable, const UINT *puType, UINT uTypeCount,
                    HNDENUMPROC pfnEnum, LPARAM lParam, BOOL fAsync)
{
    // fetch the handle table pointer
    HandleTable *pTable = Table(hTable);

    // per-block scanning callback
    BLOCKSCANPROC pfnBlock;

    // do we need to support user data?
    BOOL fEnumUserData = TypesRequireUserDataScanning(pTable, puType, uTypeCount);

    if (fEnumUserData)
    {
        // scan all handles with user data
        pfnBlock = BlockScanBlocksWithUserData;
    }
    else
    {
        // scan all handles without user data
        pfnBlock = BlockScanBlocksWithoutUserData;
    }

    // set up parameters for handle enumeration
    ScanCallbackInfo info;

    info.uFlags          = (fAsync? HNDGCF_ASYNC : HNDGCF_NORMAL);
    info.fEnumUserData   = fEnumUserData;
    info.dwAgeMask       = 0;
    info.pCurrentSegment = NULL;
    info.pfnScan         = CallHandleEnumProc;
    info.param1          = (LPARAM)pfnEnum;
    info.param2          = lParam;

    // choose a scanning method based on the async flag
    TABLESCANPROC pfnScanTable = TableScanHandles;
    if (fAsync)
        pfnScanTable = xxxTableScanHandlesAsync;

    // acquire the handle manager lock
    pTable->pLock->Enter();

    // scan the table
    pfnScanTable(pTable, puType, uTypeCount, FullSegmentIterator, pfnBlock, &info);

    // release the handle manager lock
    pTable->pLock->Leave();
}


/*
 * HndScanHandlesForGC
 *
 * Multiple type scanning entrypoint for GC.
 *
 * This entrypoint is provided for GC-time scnas of the handle table ONLY.  It
 * enables ephemeral scanning of the table, and optionally ages the write barrier
 * as it scans.
 *
 */
void HndScanHandlesForGC(HHANDLETABLE hTable, HANDLESCANPROC scanProc, LPARAM param1, LPARAM param2,
                         const UINT *types, UINT typeCount, UINT condemned, UINT maxgen, UINT flags)
{
    // fetch the table pointer
    HandleTable *pTable = Table(hTable);

    // per-segment and per-block callbacks
    SEGMENTITERATOR pfnSegment;
    BLOCKSCANPROC pfnBlock = NULL;

    // do we need to support user data?
    BOOL enumUserData =
        ((flags & HNDGCF_EXTRAINFO) &&
        TypesRequireUserDataScanning(pTable, types, typeCount));

    // what type of GC are we performing?
    if (condemned >= maxgen)
    {
        // full GC - use our full-service segment iterator
        pfnSegment = FullSegmentIterator;

        // see if there is a callback
        if (scanProc)
        {
            // do we need to scan blocks with user data?
            if (enumUserData)
            {
                // scan all with user data
                pfnBlock = BlockScanBlocksWithUserData;
            }
            else
            {
                // scan all without user data
                pfnBlock = BlockScanBlocksWithoutUserData;
            }
        }
        else if (flags & HNDGCF_AGE)
        {
            // there is only aging to do
            pfnBlock = BlockAgeBlocks;
        }
    }
    else
    {
        // this is an ephemeral GC - is it g0?
        if (condemned == 0)
        {
            // yes - do bare-bones enumeration
            pfnSegment = QuickSegmentIterator;
        }
        else
        {
            // no - do normal enumeration
            pfnSegment = StandardSegmentIterator;
        }

        // see if there is a callback
        if (scanProc)
        {
            // there is a scan callback - scan the condemned generation
            pfnBlock = BlockScanBlocksEphemeral;
        }
        else if (flags & HNDGCF_AGE)
        {
            // there is only aging to do
            pfnBlock = BlockAgeBlocksEphemeral;
        }
    }

    // set up parameters for scan callbacks
    ScanCallbackInfo info;

    info.uFlags          = flags;
    info.fEnumUserData   = enumUserData;
    info.dwAgeMask       = BuildAgeMask(condemned);
    info.pCurrentSegment = NULL;
    info.pfnScan         = scanProc;
    info.param1          = param1;
    info.param2          = param2;

#ifdef _DEBUG
    info.DEBUG_BlocksScanned                = 0;
    info.DEBUG_BlocksScannedNonTrivially    = 0;
    info.DEBUG_HandleSlotsScanned           = 0;
    info.DEBUG_HandlesActuallyScanned       = 0;
#endif

    // choose a scanning method based on the async flag
    TABLESCANPROC pfnScanTable = TableScanHandles;
    if (flags & HNDGCF_ASYNC)
        pfnScanTable = xxxTableScanHandlesAsync;

    // lock the table down
    pTable->pLock->Enter();

    // perform the scan
    pfnScanTable(pTable, types, typeCount, pfnSegment, pfnBlock, &info);

#ifdef _DEBUG
    // update our scanning statistics for this generation
    DEBUG_PostGCScanHandler(pTable, types, typeCount, condemned, maxgen, &info);
#endif

    // unlock the table
    pTable->pLock->Leave();
}


/*
 * HndResetAgeMap
 *
 * Service to forceably reset the age map for a set of handles.
 *
 * Provided for GC-time resetting the handle table's write barrier.  This is not
 * normally advisable, as it increases the amount of work that will be done in
 * subsequent scans.  Under some circumstances, however, this is precisely what is
 * desired.  Generally this entrypoint should only be used under some exceptional
 * condition during garbage collection, like objects being demoted from a higher
 * generation to a lower one.
 *
 */
void HndResetAgeMap(HHANDLETABLE hTable, const UINT *types, UINT typeCount, UINT flags)
{
    // fetch the table pointer
    HandleTable *pTable = Table(hTable);

    // set up parameters for scan callbacks
    ScanCallbackInfo info;

    info.uFlags          = flags;
    info.fEnumUserData   = FALSE;
    info.dwAgeMask       = 0;
    info.pCurrentSegment = NULL;
    info.pfnScan         = NULL;
    info.param1          = 0;
    info.param2          = 0;

    // lock the table down
    pTable->pLock->Enter();

    // perform the scan
    TableScanHandles(pTable, types, typeCount, QuickSegmentIterator, BlockResetAgeMapForBlocks, &info);

    // unlock the table
    pTable->pLock->Leave();
}


/*
 * HndNotifyGcCycleComplete
 *
 * Informs the handle table that a GC has completed.
 *
 */
void HndNotifyGcCycleComplete(HHANDLETABLE hTable, UINT condemned, UINT maxgen)
{
#ifdef _DEBUG
    // fetch the handle table pointer
    HandleTable *pTable = Table(hTable);

    // lock the table down
    pTable->pLock->Enter();

    // if this was a full GC then dump a cumulative log of scanning stats
    if (condemned >= maxgen)
        DEBUG_LogScanningStatistics(pTable, LL_INFO10);

    // unlock the table
    pTable->pLock->Leave();
#endif
}

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * DEBUG SCANNING STATISTICS
 *
 ****************************************************************************/
#ifdef _DEBUG

void DEBUG_PostGCScanHandler(HandleTable *pTable, const UINT *types, UINT typeCount, UINT condemned, UINT maxgen, ScanCallbackInfo *info)
{
    // looks like the GC supports more generations than we expected
    _ASSERTE(condemned < MAXSTATGEN);

    // remember the highest generation we've seen
    if (pTable->_DEBUG_iMaxGen < (int)condemned)
        pTable->_DEBUG_iMaxGen = (int)condemned;

    // update the statistics
    pTable->_DEBUG_TotalBlocksScanned                [condemned] += info->DEBUG_BlocksScanned;
    pTable->_DEBUG_TotalBlocksScannedNonTrivially    [condemned] += info->DEBUG_BlocksScannedNonTrivially;
    pTable->_DEBUG_TotalHandleSlotsScanned           [condemned] += info->DEBUG_HandleSlotsScanned;
    pTable->_DEBUG_TotalHandlesActuallyScanned       [condemned] += info->DEBUG_HandlesActuallyScanned;

    // if this is an ephemeral GC then dump ephemeral stats for this scan right now
    if (condemned < maxgen)
    {
        // dump a header for the stats with the condemned generation number
        LOG((LF_GC, LL_INFO1000, "--------------------------------------------------------------\n"));
        LOG((LF_GC, LL_INFO1000, "Ephemeral Handle Scan Summary:\n"));
        LOG((LF_GC, LL_INFO1000, "    Generation            = %u\n", condemned));

        // dump the handle types we were asked to scan
        LOG((LF_GC, LL_INFO1000, "    Handle Type(s)        = %u", *types));
        for (UINT u = 1; u < typeCount; u++)
            LOG((LF_GC, LL_INFO1000, ",%u", types[u]));
        LOG((LF_GC, LL_INFO1000,  "\n"));

        // dump the number of blocks and slots we scanned
        DWORD32 blockHandles = info->DEBUG_BlocksScanned * HANDLE_HANDLES_PER_BLOCK;
        LOG((LF_GC, LL_INFO1000, "    Blocks Scanned        = %u (%u slots)\n", info->DEBUG_BlocksScanned, blockHandles));

        // if we scanned any blocks then summarize some stats
        if (blockHandles)
        {
            DWORD32 nonTrivialBlockHandles = info->DEBUG_BlocksScannedNonTrivially * HANDLE_HANDLES_PER_BLOCK;
            LOG((LF_GC, LL_INFO1000, "    Blocks Examined       = %u (%u slots)\n", info->DEBUG_BlocksScannedNonTrivially, nonTrivialBlockHandles));

            LOG((LF_GC, LL_INFO1000, "    Slots Scanned         = %u\n", info->DEBUG_HandleSlotsScanned));
            LOG((LF_GC, LL_INFO1000, "    Handles Scanned       = %u\n", info->DEBUG_HandlesActuallyScanned));

            double scanRatio = ((double)info->DEBUG_HandlesActuallyScanned / (double)blockHandles) * 100.0;

            LOG((LF_GC, LL_INFO1000, "    Handle Scanning Ratio = %1.1lf%%\n", scanRatio));
        }

        // dump a footer for the stats
        LOG((LF_GC, LL_INFO1000, "--------------------------------------------------------------\n"));
    }
}

void DEBUG_LogScanningStatistics(HandleTable *pTable, DWORD level)
{
    // have we done any GC's yet?
    if (pTable->_DEBUG_iMaxGen >= 0)
    {
        // dump a header for the stats
        LOG((LF_GC, level, "\n==============================================================\n"));
        LOG((LF_GC, level, " Cumulative Handle Scan Summary:\n"));

        // for each generation we've collected,  dump the current stats
        for (int i = 0; i <= pTable->_DEBUG_iMaxGen; i++)
        {
            __int64 totalBlocksScanned = pTable->_DEBUG_TotalBlocksScanned[i];

            // dump the generation number and the number of blocks scanned
            LOG((LF_GC, level,     "--------------------------------------------------------------\n"));
            LOG((LF_GC, level,     "    Condemned Generation      = %d\n", i));
            LOG((LF_GC, level,     "    Blocks Scanned            = %I64u\n", totalBlocksScanned));

            // if we scanned any blocks in this generation then dump some interesting numbers
            if (totalBlocksScanned)
            {
                LOG((LF_GC, level, "    Blocks Examined           = %I64u\n", pTable->_DEBUG_TotalBlocksScannedNonTrivially[i]));
                LOG((LF_GC, level, "    Slots Scanned             = %I64u\n", pTable->_DEBUG_TotalHandleSlotsScanned       [i]));
                LOG((LF_GC, level, "    Handles Scanned           = %I64u\n", pTable->_DEBUG_TotalHandlesActuallyScanned   [i]));

                double blocksScanned  = (double) totalBlocksScanned;
                double blocksExamined = (double) pTable->_DEBUG_TotalBlocksScannedNonTrivially[i];
                double slotsScanned   = (double) pTable->_DEBUG_TotalHandleSlotsScanned       [i];
                double handlesScanned = (double) pTable->_DEBUG_TotalHandlesActuallyScanned   [i];
                double totalSlots     = (double) (totalBlocksScanned * HANDLE_HANDLES_PER_BLOCK);

                LOG((LF_GC, level, "    Block Scan Ratio          = %1.1lf%%\n", (100.0 * (blocksExamined / blocksScanned)) ));
                LOG((LF_GC, level, "    Clump Scan Ratio          = %1.1lf%%\n", (100.0 * (slotsScanned   / totalSlots))    ));
                LOG((LF_GC, level, "    Scanned Clump Saturation  = %1.1lf%%\n", (100.0 * (handlesScanned / slotsScanned))  ));
                LOG((LF_GC, level, "    Overall Handle Scan Ratio = %1.1lf%%\n", (100.0 * (handlesScanned / totalSlots))    ));
            }
        }

        // dump a footer for the stats
        LOG((LF_GC, level, "==============================================================\n\n"));
    }
}

// Clients sometimes release the same handle multiple times.  The handle cache makes
// it hard to debug these cases, because it defers detection until the point where
// the cache underflows or overflows.  At that point, we have data corruptions and
// the code provoking the underflow / overflow may have nothing to do with the broken
// clients.
//
// Under registry control, the checked build can try to detect these cases a little
// earlier.
#define MAX_TRACK   20

BOOL         fIsTracking;               // controlled via registry
Crst        *pTrackCrst;                // serialize access to TrackedHandles
OBJECTHANDLE TrackedHandles[MAX_TRACK];
int          iCurTrack;                 // current index into TrackedHandles

void DEBUG_TrackInit()
{
    // Determine whether we are tracking handles with a one-shot test.
    fIsTracking = EEConfig::GetConfigDWORD(L"TrackHandles", 0);
    if (fIsTracking)
    {
        // Use the same level as the HandleTable lock, so we'll get a violation if
        // we ever interfere with that lock.
        pTrackCrst = ::new Crst("TrackHandles", CrstHandleTable);
        iCurTrack = 0;
        for (int i=0; i<MAX_TRACK; i++)
            TrackedHandles[i] = 0;
    }
}

void DEBUG_TrackAlloc(OBJECTHANDLE h)
{
    if (fIsTracking)
    {
        pTrackCrst->Enter();

        // If we are tracking this as a freed handle, it is no longer freed and must
        // be removed from the list.  Once we've done so, we don't need to consider
        // the rest of the list since it can only be added once (
        for (int i=0; i<MAX_TRACK; i++)
            if (TrackedHandles[i] == h)
            {
                TrackedHandles[i] = 0;
                break;
            }

        pTrackCrst->Leave();
    }
}

void DEBUG_TrackFree(OBJECTHANDLE h)
{
    if (fIsTracking)
    {
        pTrackCrst->Enter();

        // It better not already be freed
        for (int i=0; i<MAX_TRACK; i++)
            if (TrackedHandles[i] == h)
                _ASSERTE(!"Multiple release of a handle causes data corruption");

        // Now add it.
        TrackedHandles[iCurTrack] = h;
        if (++iCurTrack >= MAX_TRACK)
            iCurTrack = 0;

        pTrackCrst->Leave();
    }
}

#endif

/*--------------------------------------------------------------------------*/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\guardpagehelper.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*============================================================
**
** Header:  GuardPageHelper.cpp
**
** Purpose: Routines for resetting the stack after stack overflow.
**
** Date:  Mar 7, 2000
**
===========================================================*/

#include "common.h"
#include "guardpagehelper.h"

#ifdef _X86_

#define OS_PAGE_SIZE 4096

// Given a stack pointer, calculate where the guard page lives.  The pMbi arg is an 
// output paramater.  It contains the result of the VirtualQuery for the guard page.
//
static LPBYTE
CalcGuardPageBase(MEMORY_BASIC_INFORMATION *pMbi, LPCVOID StackPointer) {
    LPCVOID AllocationBase;
    LPBYTE GuardPageBase;

    //  Based on the snapshot of our stack pointer, get the base address of the
    //  VirtualAlloc'ed thread stack.
    VirtualQuery(StackPointer, pMbi, sizeof(MEMORY_BASIC_INFORMATION));

    //  On both Windows 95 and Windows NT, the base of the thread stack is a
    //  barrier that catches threads that attempt to grow their stack beyond the
    //  final guard page.  To determine the size of this barrier, loop until we
    //  hit committed pages, which should be where we put the guard page.
    AllocationBase = pMbi->AllocationBase;
    GuardPageBase = (LPBYTE) AllocationBase;

    pMbi->RegionSize = 0;

    do {
        GuardPageBase += pMbi->RegionSize;
        VirtualQuery(GuardPageBase, pMbi, sizeof(MEMORY_BASIC_INFORMATION));

        if ((pMbi->State == MEM_FREE) || (pMbi->AllocationBase != AllocationBase)) {
            _ASSERTE(!"Did not find commited region of stack");
            return (LPBYTE) -1;
        }

    }   while (pMbi->State == MEM_RESERVE);

    // Guard page is one more page up the stack.
    GuardPageBase += OS_PAGE_SIZE;

    return GuardPageBase;
}


// A heuristic for deciding if we can reset the guard page. The pMbi is the memory info
// for the guard page region.  Here, we currently back of 4 pages before we continue.
//
static BOOL
InternalCanResetTo(MEMORY_BASIC_INFORMATION *pMbi, LPCVOID StackPointer, LPBYTE GuardPageBase) {
    //  Check if the guard page is too close to the current stack pointer.  If
    //  things look bad, give up and see if somebody further up the stack can
    //  handle the exception.
    if ((StackPointer > (GuardPageBase + (OS_PAGE_SIZE * 4))) &&
        ((pMbi->Protect & (PAGE_READWRITE | PAGE_GUARD)) != 0)) {
        return TRUE;
    } else {
        return FALSE;
    }
}


// Return true if the guard age can be set to this depth.
BOOL 
GuardPageHelper::CanResetStackTo(LPCVOID StackPointer) {

    MEMORY_BASIC_INFORMATION mbi;

    LPBYTE GuardPageBase = CalcGuardPageBase(&mbi, StackPointer);
    return InternalCanResetTo(&mbi, StackPointer, GuardPageBase);
}

//  Resets the stack guard page.  The supplied stack pointer is used to pinpoint
//  the address range used by the stack in order to locate the guard page.
//
//  Stack overflows are the rare case, so performance is not critical.
static VOID
InternalResetGuardPage(LPCVOID StackPointer)
{
    DWORD flOldProtect;
    BOOL fResetFailed;
    MEMORY_BASIC_INFORMATION mbi;

    LPBYTE GuardPageBase = CalcGuardPageBase(&mbi, StackPointer);

    //  Check if the guard page is too close to the current stack pointer.  If
    //  things look bad, give up and see if somebody further up the stack can
    //  handle the exception.
    _ASSERTE(InternalCanResetTo(&mbi, StackPointer, GuardPageBase));

    if (!RunningOnWinNT()) {

        fResetFailed = !VirtualProtect(GuardPageBase, OS_PAGE_SIZE,
            PAGE_NOACCESS, &flOldProtect);

    } else {

        fResetFailed = !VirtualProtect(GuardPageBase, OS_PAGE_SIZE,
            PAGE_READWRITE | PAGE_GUARD, &flOldProtect);

    }

    _ASSERTE(!fResetFailed);
}


static VOID
ResetThreadState() {
    Thread *pThread = GetThread();
    pThread->ResetGuardPageGone();
}


// This function preserves all of the caller's registers.  It does this, because the
// caller's stack is nuked -- to give the caller a little more room to save things,
// we preserve a larger register set than usual.

__declspec(naked)
VOID
GuardPageHelper::ResetGuardPage() {

    __asm {
        // Save caller's registers.
        push eax
        push ebx
        push ecx
        push esi
        push edi
        push edx

        // Calc caller SP.
        mov  ecx, esp           
        add  ecx, ((6 + 1) * 4) // SP of caller was 6 pushes + 1 return address.

        // Call InternalResetGuardPage to do the work.
        push ecx
        call InternalResetGuardPage
    }

    ResetThreadState();

    __asm {
        // Restore registers and return.
        pop edx
        pop edi
        pop esi
        pop ecx
        pop ebx
        pop eax
        ret
    }
}

#else // !_X86_

BOOL GuardPageHelper::CanResetStackTo(LPCVOID StackPointer) 
{ 
    _ASSERTE(!"NYI"); 
    return TRUE; 
}

VOID GuardPageHelper::ResetGuardPage() 
{ 
    _ASSERTE(!"NYI"); 
}

#endif // !_X86_
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\handletablescan.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*
 * Generational GC handle manager.  Table Scanning Routines.
 *
 * Implements support for scanning handles in the table.
 *
 * francish
 */

#include "common.h"
#include "HandleTablePriv.h"



/****************************************************************************
 *
 * DEFINITIONS FOR WRITE-BARRIER HANDLING
 *
 ****************************************************************************/

#define GEN_MAX_AGE                         (0x3F)
#define GEN_CLAMP                           (0x3F3F3F3F)
#define GEN_INVALID                         (0xC0C0C0C0)
#define GEN_FILL                            (0x80808080)
#define GEN_MASK                            (0x40404040)
#define GEN_INC_SHIFT                       (6)

#define PREFOLD_FILL_INTO_AGEMASK(msk)      (1 + (msk) + (~GEN_FILL))

#define GEN_FULLGC                          PREFOLD_FILL_INTO_AGEMASK(GEN_CLAMP)

#define MAKE_CLUMP_MASK_ADDENDS(bytes)      (bytes >> GEN_INC_SHIFT)
#define APPLY_CLUMP_ADDENDS(gen, addend)    (gen + addend)

#define COMPUTE_CLUMP_MASK(gen, msk)        (((gen & GEN_CLAMP) - msk) & GEN_MASK)
#define COMPUTE_CLUMP_ADDENDS(gen, msk)     MAKE_CLUMP_MASK_ADDENDS(COMPUTE_CLUMP_MASK(gen, msk))
#define COMPUTE_AGED_CLUMPS(gen, msk)       APPLY_CLUMP_ADDENDS(gen, COMPUTE_CLUMP_ADDENDS(gen, msk))

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * SUPPORT STRUCTURES FOR ASYNCHRONOUS SCANNING
 *
 ****************************************************************************/

/*
 * ScanRange
 *
 * Specifies a range of blocks for scanning.
 *
 */
struct ScanRange
{
    /*
     * Start Index
     *
     * Specifies the first block in the range.
     */
    UINT uIndex;

    /*
     * Count
     *
     * Specifies the number of blocks in the range.
     */
    UINT uCount;
};


/*
 * ScanQNode
 *
 * Specifies a set of block ranges in a scan queue.
 *
 */
struct ScanQNode
{
    /*
     * Next Node
     *
     * Specifies the next node in a scan list.
     */
    struct ScanQNode *pNext;

    /*
     * Entry Count
     *
     * Specifies how many entries in this block are valid.
     */
    UINT              uEntries;

    /*
     * Range Entries
     *
     * Each entry specifies a range of blocks to process.
     */
    ScanRange         rgRange[HANDLE_BLOCKS_PER_SEGMENT / 4];
};

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * MISCELLANEOUS HELPER ROUTINES AND DEFINES
 *
 ****************************************************************************/

/*
 * INCLUSION_MAP_SIZE
 *
 * Number of elements in a type inclusion map.
 *
 */
#define INCLUSION_MAP_SIZE (HANDLE_MAX_INTERNAL_TYPES + 1)


/*
 * BuildInclusionMap
 *
 * Creates an inclusion map for the specified type array.
 *
 */
void BuildInclusionMap(BOOL *rgTypeInclusion, const UINT *puType, UINT uTypeCount)
{
    // by default, no types are scanned
    ZeroMemory(rgTypeInclusion, INCLUSION_MAP_SIZE * sizeof(BOOL));

    // add the specified types to the inclusion map
    for (UINT u = 0; u < uTypeCount; u++)
    {
        // fetch a type we are supposed to scan
        UINT uType = puType[u];

        // hope we aren't about to trash the stack :)
        _ASSERTE(uType < HANDLE_MAX_INTERNAL_TYPES);

        // add this type to the inclusion map
        rgTypeInclusion[uType + 1] = TRUE;
    }
}


/*
 * IsBlockIncluded
 *
 * Checks a type inclusion map for the inclusion of a particular block.
 *
 */
__inline BOOL IsBlockIncluded(TableSegment *pSegment, UINT uBlock, const BOOL *rgTypeInclusion)
{
    // fetch the adjusted type for this block
    UINT uType = (UINT)(((int)(signed char)pSegment->rgBlockType[uBlock]) + 1);

    // hope the adjusted type was valid
    _ASSERTE(uType <= HANDLE_MAX_INTERNAL_TYPES);

    // return the inclusion value for the block's type
    return rgTypeInclusion[uType];
}


/*
 * TypesRequireUserDataScanning
 *
 * Determines whether the set of types listed should get user data during scans
 *
 * if ALL types passed have user data then this function will enable user data support
 * otherwise it will disable user data support
 *
 * IN OTHER WORDS, SCANNING WITH A MIX OF USER-DATA AND NON-USER-DATA TYPES IS NOT SUPPORTED
 *
 */
BOOL TypesRequireUserDataScanning(HandleTable *pTable, const UINT *types, UINT typeCount)
{
    // count up the number of types passed that have user data associated
    UINT userDataCount = 0;
    for (UINT u = 0; u < typeCount; u++)
    {
        if (TypeHasUserData(pTable, types[u]))
            userDataCount++;
    }

    // if all have user data then we can enum user data
    if (userDataCount == typeCount)
        return TRUE;

    // WARNING: user data is all or nothing in scanning!!!
    // since we have some types which don't support user data, we can't use the user data scanning code
    // this means all callbacks will get NULL for user data!!!!!
    _ASSERTE(userDataCount == 0);

    // no user data
    return FALSE;
}


/*
 * BuildAgeMask
 *
 * Builds an age mask to be used when examining/updating the write barrier.
 *
 */
DWORD32 BuildAgeMask(UINT uGen)
{
    // an age mask is composed of repeated bytes containing the next older generation
    uGen++;

    // clamp the generation to the maximum age we support in our macros
    if (uGen > GEN_MAX_AGE)
        uGen = GEN_MAX_AGE;

    // pack up a word with age bytes and fill bytes pre-folded as well
    return PREFOLD_FILL_INTO_AGEMASK(uGen | (uGen << 8) | (uGen << 16) | (uGen << 24));
}

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * SYNCHRONOUS HANDLE AND BLOCK SCANNING ROUTINES
 *
 ****************************************************************************/

/*
 * ARRAYSCANPROC
 *
 * Prototype for callbacks that implement handle array scanning logic.
 *
 */
typedef void (CALLBACK *ARRAYSCANPROC)(_UNCHECKED_OBJECTREF *pValue, _UNCHECKED_OBJECTREF *pLast,
                                       ScanCallbackInfo *pInfo, LPARAM *pUserData);


/*
 * ScanConsecutiveHandlesWithoutUserData
 *
 * Unconditionally scans a consecutive range of handles.
 *
 * USER DATA PASSED TO CALLBACK PROC IS ALWAYS NULL!
 *
 */
void CALLBACK ScanConsecutiveHandlesWithoutUserData(_UNCHECKED_OBJECTREF *pValue,
                                                    _UNCHECKED_OBJECTREF *pLast,
                                                    ScanCallbackInfo *pInfo,
                                                    LPARAM *)
{
#ifdef _DEBUG
    // update our scanning statistics
    pInfo->DEBUG_HandleSlotsScanned += (int)(pLast - pValue);
#endif

    // get frequently used params into locals
    HANDLESCANPROC pfnScan = pInfo->pfnScan;
    LPARAM         param1  = pInfo->param1;
    LPARAM         param2  = pInfo->param2;

    // scan for non-zero handles
    do
    {
        // call the callback for any we find
        if (*pValue)
        {
#ifdef _DEBUG
            // update our scanning statistics
            pInfo->DEBUG_HandlesActuallyScanned++;
#endif

            // process this handle
            pfnScan(pValue, NULL, param1, param2);
        }

        // on to the next handle
        pValue++;

    } while (pValue < pLast);
}


/*
 * ScanConsecutiveHandlesWithUserData
 *
 * Unconditionally scans a consecutive range of handles.
 *
 * USER DATA IS ASSUMED TO BE CONSECUTIVE!
 *
 */
void CALLBACK ScanConsecutiveHandlesWithUserData(_UNCHECKED_OBJECTREF *pValue,
                                                 _UNCHECKED_OBJECTREF *pLast,
                                                 ScanCallbackInfo *pInfo,
                                                 LPARAM *pUserData)
{
#ifdef _DEBUG
    // this function will crash if it is passed bad extra info
    _ASSERTE(pUserData);

    // update our scanning statistics
    pInfo->DEBUG_HandleSlotsScanned += (int)(pLast - pValue);
#endif

    // get frequently used params into locals
    HANDLESCANPROC pfnScan = pInfo->pfnScan;
    LPARAM         param1  = pInfo->param1;
    LPARAM         param2  = pInfo->param2;

    // scan for non-zero handles
    do
    {
        // call the callback for any we find
        if (*pValue)
        {
#ifdef _DEBUG
            // update our scanning statistics
            pInfo->DEBUG_HandlesActuallyScanned++;
#endif

            // process this handle
            pfnScan(pValue, pUserData, param1, param2);
        }

        // on to the next handle
        pValue++;
        pUserData++;

    } while (pValue < pLast);
}


/*
 * BlockAgeBlocks
 *
 * Ages all clumps in a range of consecutive blocks.
 *
 */
void CALLBACK BlockAgeBlocks(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo)
{
    // set up to update the specified blocks
    DWORD32 *pdwGen     = (DWORD32 *)pSegment->rgGeneration + uBlock;
    DWORD32 *pdwGenLast =            pdwGen                 + uCount;

    // loop over all the blocks, aging their clumps as we go
    do
    {
        // compute and store the new ages in parallel
        *pdwGen = COMPUTE_AGED_CLUMPS(*pdwGen, GEN_FULLGC);

    } while (++pdwGen < pdwGenLast);
}


/*
 * BlockScanBlocksWithoutUserData
 *
 * Calls the specified callback once for each handle in a range of blocks,
 * optionally aging the corresponding generation clumps.
 *
 */
void CALLBACK BlockScanBlocksWithoutUserData(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo)
{
    // get the first and limit handles for these blocks
    _UNCHECKED_OBJECTREF *pValue = pSegment->rgValue + (uBlock * HANDLE_HANDLES_PER_BLOCK);
    _UNCHECKED_OBJECTREF *pLast  = pValue            + (uCount * HANDLE_HANDLES_PER_BLOCK);

    // scan the specified handles
    ScanConsecutiveHandlesWithoutUserData(pValue, pLast, pInfo, NULL);

    // optionally update the clump generations for these blocks too
    if (pInfo->uFlags & HNDGCF_AGE)
        BlockAgeBlocks(pSegment, uBlock, uCount, pInfo);

#ifdef _DEBUG
    // update our scanning statistics
    pInfo->DEBUG_BlocksScannedNonTrivially += uCount;
    pInfo->DEBUG_BlocksScanned += uCount;
#endif
}


/*
 * BlockScanBlocksWithUserData
 *
 * Calls the specified callback once for each handle in a range of blocks,
 * optionally aging the corresponding generation clumps.
 *
 */
void CALLBACK BlockScanBlocksWithUserData(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo)
{
    // iterate individual blocks scanning with user data
    for (UINT u = 0; u < uCount; u++)
    {
        // compute the current block
        UINT uCur = (u + uBlock);

        // fetch the user data for this block
        LPARAM *pUserData = BlockFetchUserDataPointer(pSegment, uCur, TRUE);

        // get the first and limit handles for this block
        _UNCHECKED_OBJECTREF *pValue = pSegment->rgValue + (uCur * HANDLE_HANDLES_PER_BLOCK);
        _UNCHECKED_OBJECTREF *pLast  = pValue            + HANDLE_HANDLES_PER_BLOCK;

        // scan the handles in this block
        ScanConsecutiveHandlesWithUserData(pValue, pLast, pInfo, pUserData);
    }

    // optionally update the clump generations for these blocks too
    if (pInfo->uFlags & HNDGCF_AGE)
        BlockAgeBlocks(pSegment, uBlock, uCount, pInfo);

#ifdef _DEBUG
    // update our scanning statistics
    pInfo->DEBUG_BlocksScannedNonTrivially += uCount;
    pInfo->DEBUG_BlocksScanned += uCount;
#endif
}


/*
 * BlockAgeBlocksEphemeral
 *
 * Ages all clumps within the specified generation.
 *
 */
void CALLBACK BlockAgeBlocksEphemeral(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo)
{
    // get frequently used params into locals
    DWORD32 dwAgeMask = pInfo->dwAgeMask;

    // set up to update the specified blocks
    DWORD32 *pdwGen     = (DWORD32 *)pSegment->rgGeneration + uBlock;
    DWORD32 *pdwGenLast =            pdwGen                 + uCount;

    // loop over all the blocks, aging their clumps as we go
    do
    {
        // compute and store the new ages in parallel
        *pdwGen = COMPUTE_AGED_CLUMPS(*pdwGen, dwAgeMask);

    } while (++pdwGen < pdwGenLast);
}


/*
 * BlockScanBlocksEphemeralWorker
 *
 * Calls the specified callback once for each handle in any clump
 * identified by the clump mask in the specified block.
 *
 */
void BlockScanBlocksEphemeralWorker(DWORD32 *pdwGen, DWORD32 dwClumpMask, ScanCallbackInfo *pInfo)
{
    //
    // OPTIMIZATION: Since we expect to call this worker fairly rarely compared to
    //  the number of times we pass through the outer loop, this function intentionally
    //  does not take pSegment as a param.
    //
    //  We do this so that the compiler won't try to keep pSegment in a register during
    //  the outer loop, leaving more registers for the common codepath.
    //
    //  You might wonder why this is an issue considering how few locals we have in
    //  BlockScanBlocksEphemeral.  For some reason the x86 compiler doesn't like to use
    //  all the registers during that loop, so a little coaxing was necessary to get
    //  the right output.
    //

    // fetch the table segment we are working in
    TableSegment *pSegment = pInfo->pCurrentSegment;

    // if we should age the clumps then do so now (before we trash dwClumpMask)
    if (pInfo->uFlags & HNDGCF_AGE)
        *pdwGen = APPLY_CLUMP_ADDENDS(*pdwGen, MAKE_CLUMP_MASK_ADDENDS(dwClumpMask));

    // compute the index of the first clump in the block
    UINT uClump = (UINT)((BYTE *)pdwGen - pSegment->rgGeneration);

    // compute the first handle in the first clump of this block
    _UNCHECKED_OBJECTREF *pValue = pSegment->rgValue + (uClump * HANDLE_HANDLES_PER_CLUMP);

    // some scans require us to report per-handle extra info - assume this one doesn't
    ARRAYSCANPROC pfnScanHandles = ScanConsecutiveHandlesWithoutUserData;
    LPARAM       *pUserData = NULL;

    // do we need to pass user data to the callback?
    if (pInfo->fEnumUserData)
    {
        // scan with user data enabled
        pfnScanHandles = ScanConsecutiveHandlesWithUserData;

        // get the first user data slot for this block
        pUserData = BlockFetchUserDataPointer(pSegment, (uClump / HANDLE_CLUMPS_PER_BLOCK), TRUE);
    }

    // loop over the clumps, scanning those that are identified by the mask
    do
    {
        // compute the last handle in this clump
        _UNCHECKED_OBJECTREF *pLast = pValue + HANDLE_HANDLES_PER_CLUMP;

        // if this clump should be scanned then scan it
        if (dwClumpMask & GEN_CLUMP_0_MASK)
            pfnScanHandles(pValue, pLast, pInfo, pUserData);

        // skip to the next clump
        dwClumpMask = NEXT_CLUMP_IN_MASK(dwClumpMask);
        pValue = pLast;
        pUserData += HANDLE_HANDLES_PER_CLUMP;

    } while (dwClumpMask);

#ifdef _DEBUG
    // update our scanning statistics
    pInfo->DEBUG_BlocksScannedNonTrivially++;
#endif
}


/*
 * BlockScanBlocksEphemeral
 *
 * Calls the specified callback once for each handle from the specified
 * generation in a block.
 *
 */
void CALLBACK BlockScanBlocksEphemeral(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo)
{
    // get frequently used params into locals
    DWORD32 dwAgeMask = pInfo->dwAgeMask;

    // set up to update the specified blocks
    DWORD32 *pdwGen     = (DWORD32 *)pSegment->rgGeneration + uBlock;
    DWORD32 *pdwGenLast =            pdwGen                 + uCount;

    // loop over all the blocks, checking for elligible clumps as we go
    do
    {
        // determine if any clumps in this block are elligible
        DWORD32 dwClumpMask = COMPUTE_CLUMP_MASK(*pdwGen, dwAgeMask);

        // if there are any clumps to scan then scan them now
        if (dwClumpMask)
        {
            // ok we need to scan some parts of this block
            //
            // OPTIMIZATION: Since we expect to call the worker fairly rarely compared
            //  to the number of times we pass through the loop, the function below
            //  intentionally does not take pSegment as a param.
            //
            //  We do this so that the compiler won't try to keep pSegment in a register
            //  during our loop, leaving more registers for the common codepath.
            //
            //  You might wonder why this is an issue considering how few locals we have
            //  here.  For some reason the x86 compiler doesn't like to use all the
            //  registers available during this loop and instead was hitting the stack
            //  repeatedly, so a little coaxing was necessary to get the right output.
            //
            BlockScanBlocksEphemeralWorker(pdwGen, dwClumpMask, pInfo);
        }

        // on to the next block's generation info
        pdwGen++;

    } while (pdwGen < pdwGenLast);

#ifdef _DEBUG
    // update our scanning statistics
    pInfo->DEBUG_BlocksScanned += uCount;
#endif
}


/*
 * BlockResetAgeMapForBlocks
 *
 * Clears the age maps for a range of blocks.
 *
 */
void CALLBACK BlockResetAgeMapForBlocks(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *)
{
    // zero the age map for the specified range of blocks
    ZeroMemory((DWORD32 *)pSegment->rgGeneration + uBlock, uCount * sizeof(DWORD32));
}


/*
 * BlockLockBlocks
 *
 * Locks all blocks in the specified range.
 *
 */
void CALLBACK BlockLockBlocks(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *)
{
    // loop over the blocks in the specified range and lock them
    for (uCount += uBlock; uBlock < uCount; uBlock++)
        BlockLock(pSegment, uBlock);
}


/*
 * BlockUnlockBlocks
 *
 * Unlocks all blocks in the specified range.
 *
 */
void CALLBACK BlockUnlockBlocks(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *)
{
    // loop over the blocks in the specified range and unlock them
    for (uCount += uBlock; uBlock < uCount; uBlock++)
        BlockUnlock(pSegment, uBlock);
}
    
    
/*
 * BlockQueueBlocksForAsyncScan
 *
 * Queues the specified blocks to be scanned asynchronously.
 *
 */
void CALLBACK BlockQueueBlocksForAsyncScan(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *)
{
    // fetch our async scan information
    AsyncScanInfo *pAsyncInfo = pSegment->pHandleTable->pAsyncScanInfo;

    // sanity
    _ASSERTE(pAsyncInfo);

    // fetch the current queue tail
    ScanQNode *pQNode = pAsyncInfo->pQueueTail;

    // did we get a tail?
    if (pQNode)
    {
        // we got an existing tail - is the tail node full already?
        if (pQNode->uEntries >= ARRAYSIZE(pQNode->rgRange))
        {
            // the node is full - is there another node in the queue?
            if (!pQNode->pNext)
            {
                // no more nodes - allocate a new one
                ScanQNode *pQNodeT = (ScanQNode *)LocalAlloc(LPTR, sizeof(ScanQNode));

                // did it succeed?
                if (!pQNodeT)
                {
                    //
                    // We couldn't allocate another queue node.
                    //
                    // THIS IS NOT FATAL IF ASYNCHRONOUS SCANNING IS BEING USED PROPERLY
                    //
                    // The reason we can survive this is that asynchronous scans are not
                    // guaranteed to enumerate all handles anyway.  Since the table can
                    // change while the lock is released, the caller may assume only that
                    // asynchronous scanning will enumerate a reasonably high percentage
                    // of the handles requested, most of the time.
                    //
                    // The typical use of an async scan is to process as many handles as
                    // possible asynchronously, so as to reduce the amount of time spent
                    // in the inevitable synchronous scan that follows.
                    //
                    // As a practical example, the Concurrent Mark phase of garbage
                    // collection marks as many objects as possible asynchronously, and
                    // subsequently performs a normal, synchronous mark to catch the
                    // stragglers.  Since most of the reachable objects in the heap are
                    // already marked at this point, the synchronous scan ends up doing
                    // very little work.
                    //
                    // So the moral of the story is that yes, we happily drop some of
                    // your blocks on the floor in this out of memory case, and that's
                    // BY DESIGN.
                    //
                    LOG((LF_GC, LL_WARNING, "WARNING: Out of memory queueing for async scan.  Some blocks skipped.\n"));
                    return;
                }

                // link the new node into the queue
                pQNode->pNext = pQNodeT;
            }

            // either way, use the next node in the queue
            pQNode = pQNode->pNext;
        }
    }
    else
    {
        // no tail - this is a brand new queue; start the tail at the head node
        pQNode = pAsyncInfo->pScanQueue;
    }

    // we will be using the last slot after the existing entries
    UINT uSlot = pQNode->uEntries;

    // fetch the slot where we will be storing the new block range
    ScanRange *pNewRange = pQNode->rgRange + uSlot;

    // update the entry count in the node
    pQNode->uEntries = uSlot + 1;

    // fill in the new slot with the block range info
    pNewRange->uIndex = uBlock;
    pNewRange->uCount = uCount;

    // remember the last block we stored into as the new queue tail
    pAsyncInfo->pQueueTail = pQNode;
}

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * ASYNCHRONOUS SCANNING WORKERS AND CALLBACKS
 *
 ****************************************************************************/

/*
 * QNODESCANPROC
 *
 * Prototype for callbacks that implement per ScanQNode scanning logic.
 *
 */
typedef void (CALLBACK *QNODESCANPROC)(AsyncScanInfo *pAsyncInfo, ScanQNode *pQNode, LPARAM lParam);


/*
 * ProcessScanQueue
 *
 * Calls the specified handler once for each node in a scan queue.
 *
 */
void ProcessScanQueue(AsyncScanInfo *pAsyncInfo, QNODESCANPROC pfnNodeHandler, LPARAM lParam, BOOL fCountEmptyQNodes)
{
	if (pAsyncInfo->pQueueTail == NULL && fCountEmptyQNodes == FALSE)
		return;
		
    // if any entries were added to the block list after our initial node, clean them up now
    ScanQNode *pQNode = pAsyncInfo->pScanQueue;
    while (pQNode)
    {
        // remember the next node
        ScanQNode *pNext = pQNode->pNext;

        // call the handler for the current node and then advance to the next
        pfnNodeHandler(pAsyncInfo, pQNode, lParam);
        pQNode = pNext;
    }
}


/*
 * ProcessScanQNode
 *
 * Calls the specified block handler once for each range of blocks in a ScanQNode.
 *
 */
void CALLBACK ProcessScanQNode(AsyncScanInfo *pAsyncInfo, ScanQNode *pQNode, LPARAM lParam)
{
    // get the block handler from our lParam
    BLOCKSCANPROC     pfnBlockHandler = (BLOCKSCANPROC)lParam;

    // fetch the params we will be passing to the handler
    ScanCallbackInfo *pCallbackInfo = pAsyncInfo->pCallbackInfo;
    TableSegment     *pSegment = pCallbackInfo->pCurrentSegment;

    // set up to iterate the ranges in the queue node
    ScanRange *pRange     = pQNode->rgRange;
    ScanRange *pRangeLast = pRange          + pQNode->uEntries;

    // loop over all the ranges, calling the block handler for each one
    while (pRange < pRangeLast) {
        // call the block handler with the current block range
        pfnBlockHandler(pSegment, pRange->uIndex, pRange->uCount, pCallbackInfo);

        // advance to the next range
        pRange++;

    }
}


/*
 * UnlockAndForgetQueuedBlocks
 *
 * Unlocks all blocks referenced in the specified node and marks the node as empty.
 *
 */
void CALLBACK UnlockAndForgetQueuedBlocks(AsyncScanInfo *pAsyncInfo, ScanQNode *pQNode, LPARAM)
{
    // unlock the blocks named in this node
    ProcessScanQNode(pAsyncInfo, pQNode, (LPARAM)BlockUnlockBlocks);

    // reset the node so it looks empty
    pQNode->uEntries = 0;
}


/*
 * FreeScanQNode
 *
 * Frees the specified ScanQNode
 *
 */
void CALLBACK FreeScanQNode(AsyncScanInfo *pAsyncInfo, ScanQNode *pQNode, LPARAM)
{
    // free the node's memory
    LocalFree((HLOCAL)pQNode);
}


/*
 * xxxTableScanQueuedBlocksAsync
 *
 * Performs and asynchronous scan of the queued blocks for the specified segment.
 *
 * N.B. THIS FUNCTION LEAVES THE TABLE LOCK WHILE SCANNING.
 *
 */
void xxxTableScanQueuedBlocksAsync(HandleTable *pTable, TableSegment *pSegment)
{
    //-------------------------------------------------------------------------------
    // PRE-SCAN PREPARATION

    // fetch our table's async and sync scanning info
    AsyncScanInfo    *pAsyncInfo    = pTable->pAsyncScanInfo;
    ScanCallbackInfo *pCallbackInfo = pAsyncInfo->pCallbackInfo;

    // make a note that we are now processing this segment
    pCallbackInfo->pCurrentSegment = pSegment;

    // loop through and lock down all the blocks referenced by the queue
    ProcessScanQueue(pAsyncInfo, ProcessScanQNode, (LPARAM)BlockLockBlocks, FALSE);


    //-------------------------------------------------------------------------------
    // ASYNCHRONOUS SCANNING OF QUEUED BLOCKS
    //

    // leave the table lock
    pTable->pLock->Leave();

    // sanity - this isn't a very asynchronous scan if we don't actually leave
    _ASSERTE(!pTable->pLock->OwnedByCurrentThread());

    // perform the actual scanning of the specified blocks
    ProcessScanQueue(pAsyncInfo, ProcessScanQNode, (LPARAM)pAsyncInfo->pfnBlockHandler, FALSE);

    // re-enter the table lock
    pTable->pLock->Enter();


    //-------------------------------------------------------------------------------
    // POST-SCAN CLEANUP
    //

    // loop through, unlock all the blocks we had locked, and reset the queue nodes
    ProcessScanQueue(pAsyncInfo, UnlockAndForgetQueuedBlocks, NULL, FALSE);

    // we are done processing this segment
    pCallbackInfo->pCurrentSegment = NULL;

    // reset the "queue tail" pointer to indicate an empty queue
    pAsyncInfo->pQueueTail = NULL;
}

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * SEGMENT ITERATORS
 *
 ****************************************************************************/

/*
 * QuickSegmentIterator
 *
 * Returns the next segment to be scanned in a scanning loop.
 *
 */
TableSegment * CALLBACK QuickSegmentIterator(HandleTable *pTable, TableSegment *pPrevSegment)
{
    TableSegment *pNextSegment;

    // do we have a previous segment?
    if (!pPrevSegment)
    {
        // nope - start with the first segment in our list
        pNextSegment = pTable->pSegmentList;
    }
    else
    {
        // yup, fetch the next segment in the list
        pNextSegment = pPrevSegment->pNextSegment;
    }

    // return the segment pointer
    return pNextSegment;
}


/*
 * StandardSegmentIterator
 *
 * Returns the next segment to be scanned in a scanning loop.
 *
 * This iterator performs some maintenance on the segments,
 * primarily making sure the block chains are sorted so that
 * g0 scans are more likely to operate on contiguous blocks.
 *
 */
TableSegment * CALLBACK StandardSegmentIterator(HandleTable *pTable, TableSegment *pPrevSegment)
{
    // get the next segment using the quick iterator
    TableSegment *pNextSegment = QuickSegmentIterator(pTable, pPrevSegment);

    // re-sort the block chains if neccessary
    if (pNextSegment && pNextSegment->fResortChains)
        SegmentResortChains(pNextSegment);

    // return the segment we found
    return pNextSegment;
}


/*
 * FullSegmentIterator
 *
 * Returns the next segment to be scanned in a scanning loop.
 *
 * This iterator performs full maintenance on the segments,
 * including freeing those it notices are empty along the way.
 *
 */
TableSegment * CALLBACK FullSegmentIterator(HandleTable *pTable, TableSegment *pPrevSegment)
{
    // we will be resetting the next segment's sequence number
    UINT uSequence = 0;

    // if we have a previous segment then compute the next sequence number from it
    if (pPrevSegment)
        uSequence = (UINT)pPrevSegment->bSequence + 1;

    // loop until we find an appropriate segment to return
    TableSegment *pNextSegment;
    for (;;)
    {
        // first, call the standard iterator to get the next segment
        pNextSegment = StandardSegmentIterator(pTable, pPrevSegment);

        // if there are no more segments then we're done
        if (!pNextSegment)
            break;

        // check if we should decommit any excess pages in this segment
        SegmentTrimExcessPages(pNextSegment);

        // if the segment has handles in it then it will survive and be returned
        if (pNextSegment->bEmptyLine > 0)
        {
            // update this segment's sequence number
            pNextSegment->bSequence = (BYTE)(uSequence % 0x100);

            // break out and return the segment
            break;
        }

        // this segment is completely empty - can we free it now?
        if (TableCanFreeSegmentNow(pTable, pNextSegment))
        {
            // yup, we probably want to free this one
            TableSegment *pNextNext = pNextSegment->pNextSegment;

            // was this the first segment in the list?
            if (!pPrevSegment)
            {
                // yes - are there more segments?
                if (pNextNext)
                {
                    // yes - unlink the head
                    pTable->pSegmentList = pNextNext;
                }
                else
                {
                    // no - leave this one in the list and enumerate it
                    break;
                }
            }
            else
            {
                // no - unlink this segment from the segment list
                pPrevSegment->pNextSegment = pNextNext;
            }

            // free this segment
            SegmentFree(pNextSegment);
        }
    }

    // return the segment we found
    return pNextSegment;
}


/*
 * xxxAsyncSegmentIterator
 *
 * Implements the core handle scanning loop for a table.
 *
 * This iterator wraps another iterator, checking for queued blocks from the
 * previous segment before advancing to the next.  If there are queued blocks,
 * the function processes them by calling xxxTableScanQueuedBlocksAsync.
 *
 * N.B. THIS FUNCTION LEAVES THE TABLE LOCK WHILE SCANNING.
 *
 */
TableSegment * CALLBACK xxxAsyncSegmentIterator(HandleTable *pTable, TableSegment *pPrevSegment)
{
    // fetch our table's async scanning info
    AsyncScanInfo *pAsyncInfo = pTable->pAsyncScanInfo;

    // sanity
    _ASSERTE(pAsyncInfo);

    // if we have queued some blocks from the previous segment then scan them now
    if (pAsyncInfo->pQueueTail)
        xxxTableScanQueuedBlocksAsync(pTable, pPrevSegment);

    // fetch the underlying iterator from our async info
    SEGMENTITERATOR pfnCoreIterator = pAsyncInfo->pfnSegmentIterator;

    // call the underlying iterator to get the next segment
    return pfnCoreIterator(pTable, pPrevSegment);
}

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * CORE SCANNING LOGIC
 *
 ****************************************************************************/

/*
 * SegmentScanByTypeChain
 *
 * Implements the single-type block scanning loop for a single segment.
 *
 */
void SegmentScanByTypeChain(TableSegment *pSegment, UINT uType, BLOCKSCANPROC pfnBlockHandler, ScanCallbackInfo *pInfo)
{
    // hope we are enumerating a valid type chain :)
    _ASSERTE(uType < HANDLE_MAX_INTERNAL_TYPES);

    // fetch the tail
    UINT uBlock = pSegment->rgTail[uType];
    
    // if we didn't find a terminator then there's blocks to enumerate
    if (uBlock != BLOCK_INVALID)
    {
        // start walking from the head
        uBlock = pSegment->rgAllocation[uBlock];

        // scan until we loop back to the first block
        UINT uHead = uBlock;
        do
        {
            // search forward trying to batch up sequential runs of blocks
            UINT uLast, uNext = uBlock;
            do
            {
                // compute the next sequential block for comparison
                uLast = uNext + 1;

                // fetch the next block in the allocation chain
                uNext = pSegment->rgAllocation[uNext];

            } while ((uNext == uLast) && (uNext != uHead));

            // call the calback for this group of blocks
            pfnBlockHandler(pSegment, uBlock, (uLast - uBlock), pInfo);

            // advance to the next block
            uBlock = uNext;

        } while (uBlock != uHead);
    }
}


/*
 * SegmentScanByTypeMap
 *
 * Implements the multi-type block scanning loop for a single segment.
 *
 */
void SegmentScanByTypeMap(TableSegment *pSegment, const BOOL *rgTypeInclusion,
                          BLOCKSCANPROC pfnBlockHandler, ScanCallbackInfo *pInfo)
{
    // start scanning with the first block in the segment
    UINT uBlock = 0;

    // we don't need to scan the whole segment, just up to the empty line
    UINT uLimit = pSegment->bEmptyLine;

    // loop across the segment looking for blocks to scan
    for (;;)
    {
        // find the first block included by the type map
        for (;;)
        {
            // if we are out of range looking for a start point then we're done
            if (uBlock >= uLimit)
                return;

            // if the type is one we are scanning then we found a start point
            if (IsBlockIncluded(pSegment, uBlock, rgTypeInclusion))
                break;

            // keep searching with the next block
            uBlock++;
        }

        // remember this block as the first that needs scanning
        UINT uFirst = uBlock;

        // find the next block not included in the type map
        for (;;)
        {
            // advance the block index
            uBlock++;

            // if we are beyond the limit then we are done
            if (uBlock >= uLimit)
                break;

            // if the type is not one we are scanning then we found an end point
            if (!IsBlockIncluded(pSegment, uBlock, rgTypeInclusion))
                break;
        }

        // call the calback for the group of blocks we found
        pfnBlockHandler(pSegment, uFirst, (uBlock - uFirst), pInfo);

        // look for another range starting with the next block
        uBlock++;
    }
}


/*
 * TableScanHandles
 *
 * Implements the core handle scanning loop for a table.
 *
 */
void CALLBACK TableScanHandles(HandleTable *pTable,
                               const UINT *puType,
                               UINT uTypeCount,
                               SEGMENTITERATOR pfnSegmentIterator,
                               BLOCKSCANPROC pfnBlockHandler,
                               ScanCallbackInfo *pInfo)
{
    // sanity - caller must ALWAYS provide a valid ScanCallbackInfo
    _ASSERTE(pInfo);

    // we may need a type inclusion map for multi-type scans
    BOOL rgTypeInclusion[INCLUSION_MAP_SIZE];

    // we only need to scan types if we have a type array and a callback to call
    if (!pfnBlockHandler || !puType)
        uTypeCount = 0;

    // if we will be scanning more than one type then initialize the inclusion map
    if (uTypeCount > 1)
        BuildInclusionMap(rgTypeInclusion, puType, uTypeCount);

    // now, iterate over the segments, scanning blocks of the specified type(s)
    TableSegment *pSegment = NULL;
    while ((pSegment = pfnSegmentIterator(pTable, pSegment)) != NULL)
    {
        // if there are types to scan then enumerate the blocks in this segment
        // (we do this test inside the loop since the iterators should still run...)
        if (uTypeCount >= 1)
        {
            // make sure the "current segment" pointer in the scan info is up to date
            pInfo->pCurrentSegment = pSegment;

            // is this a single type or multi-type enumeration?
            if (uTypeCount == 1)
            {
                // single type enumeration - walk the type's allocation chain
                SegmentScanByTypeChain(pSegment, *puType, pfnBlockHandler, pInfo);
            }
            else
            {
                // multi-type enumeration - walk the type map to find eligible blocks
                SegmentScanByTypeMap(pSegment, rgTypeInclusion, pfnBlockHandler, pInfo);
            }

            // make sure the "current segment" pointer in the scan info is up to date
            pInfo->pCurrentSegment = NULL;
        }
    }
}


/*
 * xxxTableScanHandlesAsync
 *
 * Implements asynchronous handle scanning for a table.
 *
 * N.B. THIS FUNCTION LEAVES THE TABLE LOCK WHILE SCANNING.
 *
 */
void CALLBACK xxxTableScanHandlesAsync(HandleTable *pTable,
                                       const UINT *puType,
                                       UINT uTypeCount,
                                       SEGMENTITERATOR pfnSegmentIterator,
                                       BLOCKSCANPROC pfnBlockHandler,
                                       ScanCallbackInfo *pInfo)
{
    // presently only one async scan is allowed at a time
    if (pTable->pAsyncScanInfo)
    {
        // somebody tried to kick off multiple async scans
        _ASSERTE(FALSE);
        return;
    }


    //-------------------------------------------------------------------------------
    // PRE-SCAN PREPARATION

    // we keep an initial scan list node on the stack (for perf)
    ScanQNode initialNode;

    initialNode.pNext    = NULL;
    initialNode.uEntries = 0;

    // initialize our async scanning info
    AsyncScanInfo asyncInfo;

    asyncInfo.pCallbackInfo      = pInfo;
    asyncInfo.pfnSegmentIterator = pfnSegmentIterator;
    asyncInfo.pfnBlockHandler    = pfnBlockHandler;
    asyncInfo.pScanQueue         = &initialNode;
    asyncInfo.pQueueTail         = NULL;

    // link our async scan info into the table
    pTable->pAsyncScanInfo = &asyncInfo;


    //-------------------------------------------------------------------------------
    // PER-SEGMENT ASYNCHRONOUS SCANNING OF BLOCKS
    //

    // call the synchronous scanner with our async callbacks
    TableScanHandles(pTable,
                     puType, uTypeCount,
                     xxxAsyncSegmentIterator,
                     BlockQueueBlocksForAsyncScan,
                     pInfo);


    //-------------------------------------------------------------------------------
    // POST-SCAN CLEANUP
    //

    // if we dynamically allocated more nodes then free them now
    if (initialNode.pNext)
    {
        // adjust the head to point to the first dynamically allocated block
        asyncInfo.pScanQueue = initialNode.pNext;

        // loop through and free all the queue nodes
        ProcessScanQueue(&asyncInfo, FreeScanQNode, NULL, TRUE);
    }

    // unlink our async scanning info from the table
    pTable->pAsyncScanInfo = NULL;
}

/*--------------------------------------------------------------------------*/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\handletable.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*
 * Generational GC handle manager.  Entrypoint Header.
 *
 * Implements generic support for external handles into a GC heap.
 *
 * francish
 */

#ifndef _HANDLETABLE_H
#define _HANDLETABLE_H



/****************************************************************************
 *
 * FLAGS, CONSTANTS AND DATA TYPES
 *
 ****************************************************************************/

/*
 * handle flags used by HndCreateHandleTable
 */
#define HNDF_NORMAL         (0x00)
#define HNDF_EXTRAINFO      (0x01)

/*
 * handle to handle table
 */
DECLARE_HANDLE(HHANDLETABLE);

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * PUBLIC ROUTINES AND MACROS
 *
 ****************************************************************************/

/*
 * handle manager init and shutdown routines
 */
HHANDLETABLE    HndCreateHandleTable(UINT *pTypeFlags, UINT uTypeCount, UINT uADIndex);
void            HndDestroyHandleTable(HHANDLETABLE hTable);

/*
 * retrieve index stored in table at creation 
 */
void            HndSetHandleTableIndex(HHANDLETABLE hTable, UINT uTableIndex);
UINT            HndGetHandleTableIndex(HHANDLETABLE hTable);
UINT            HndGetHandleTableADIndex(HHANDLETABLE hTable);

/*
 * individual handle allocation and deallocation
 */
OBJECTHANDLE    HndCreateHandle(HHANDLETABLE hTable, UINT uType, OBJECTREF object, LPARAM lExtraInfo = 0);
void            HndDestroyHandle(HHANDLETABLE hTable, UINT uType, OBJECTHANDLE handle);

void            HndDestroyHandleOfUnknownType(HHANDLETABLE hTable, OBJECTHANDLE handle);

/*
 * bulk handle allocation and deallocation
 */
UINT            HndCreateHandles(HHANDLETABLE hTable, UINT uType, OBJECTHANDLE *pHandles, UINT uCount);
void            HndDestroyHandles(HHANDLETABLE hTable, UINT uType, const OBJECTHANDLE *pHandles, UINT uCount);

/*
 * owner data associated with handles
 */
void            HndSetHandleExtraInfo(OBJECTHANDLE handle, UINT uType, LPARAM lExtraInfo);
LPARAM          HndGetHandleExtraInfo(OBJECTHANDLE handle);

/*
 * get parent table of handle
 */
HHANDLETABLE    HndGetHandleTable(OBJECTHANDLE handle);

/*
 * write barrier
 */
void            HndWriteBarrier(OBJECTHANDLE handle);

/*
 * NON-GC handle enumeration
 */
typedef void (CALLBACK *HNDENUMPROC)(OBJECTHANDLE handle, LPARAM lExtraInfo, LPARAM lCallerParam);

void HndEnumHandles(HHANDLETABLE hTable, const UINT *puType, UINT uTypeCount,
                    HNDENUMPROC pfnEnum, LPARAM lParam, BOOL fAsync);

/*
 * GC-time handle scanning
 */
#define HNDGCF_NORMAL       (0x00000000)    // normal scan
#define HNDGCF_AGE          (0x00000001)    // age handles while scanning
#define HNDGCF_ASYNC        (0x00000002)    // drop the table lock while scanning
#define HNDGCF_EXTRAINFO    (0x00000004)    // iterate per-handle data while scanning

typedef void (CALLBACK *HANDLESCANPROC)(_UNCHECKED_OBJECTREF *pref, LPARAM *pExtraInfo, LPARAM param1, LPARAM param2);

void            HndScanHandlesForGC(HHANDLETABLE hTable,
                                    HANDLESCANPROC scanProc,
                                    LPARAM param1,
                                    LPARAM param2,
                                    const UINT *types,
                                    UINT typeCount,
                                    UINT condemned,
                                    UINT maxgen,
                                    UINT flags);

void            HndResetAgeMap(HHANDLETABLE hTable, const UINT *types, UINT typeCount, UINT flags);

void            HndNotifyGcCycleComplete(HHANDLETABLE hTable, UINT condemned, UINT maxgen);

/*--------------------------------------------------------------------------*/


#if defined(_DEBUG) && !defined(_NOVM)
#define OBJECTREF_TO_UNCHECKED_OBJECTREF(objref)    (*((_UNCHECKED_OBJECTREF*)&(objref)))
#define UNCHECKED_OBJECTREF_TO_OBJECTREF(obj)       (OBJECTREF(obj))
#else
#define OBJECTREF_TO_UNCHECKED_OBJECTREF(objref)    (objref)
#define UNCHECKED_OBJECTREF_TO_OBJECTREF(obj)       (obj)
#endif

#ifdef _DEBUG
void ValidateAssignObjrefForHandle(OBJECTREF, UINT appDomainIndex);
void ValidateFetchObjrefForHandle(OBJECTREF, UINT appDomainIndex);
#endif

/*
 * handle assignment
 */
__inline void HndAssignHandle(OBJECTHANDLE handle, OBJECTREF objref)
{
    // sanity
    _ASSERTE(handle);

#ifdef _DEBUG
    // Make sure the objref is valid before it is assigned to a handle
    ValidateAssignObjrefForHandle(objref, HndGetHandleTableADIndex(HndGetHandleTable(handle)));
#endif
    // unwrap the objectref we were given
    _UNCHECKED_OBJECTREF value = OBJECTREF_TO_UNCHECKED_OBJECTREF(objref);

    // if we are doing a non-NULL pointer store then invoke the write-barrier
    if (value)
        HndWriteBarrier(handle);

    // store the pointer
    *(_UNCHECKED_OBJECTREF *)handle = value;
}

/*
 * interlocked-exchange assignment
 */
__inline void HndInterlockedCompareExchangeHandle(OBJECTHANDLE handle, OBJECTREF objref, OBJECTREF oldObjref)
{
    // sanity
    _ASSERTE(handle);

#ifdef _DEBUG
    // Make sure the objref is valid before it is assigned to a handle
    ValidateAssignObjrefForHandle(objref, HndGetHandleTableADIndex(HndGetHandleTable(handle)));
#endif
    // unwrap the objectref we were given
    _UNCHECKED_OBJECTREF value = OBJECTREF_TO_UNCHECKED_OBJECTREF(objref);
    _UNCHECKED_OBJECTREF oldValue = OBJECTREF_TO_UNCHECKED_OBJECTREF(oldObjref);

    // if we are doing a non-NULL pointer store then invoke the write-barrier
    if (value)
        HndWriteBarrier(handle);

    // store the pointer
    
    InterlockedCompareExchangePointer((PVOID *)handle, value, oldValue);
}

/*
 * Note that HndFirstAssignHandle is similar to HndAssignHandle, except that it only
 * succeeds if transitioning from NULL to non-NULL.  In other words, if this handle
 * is being initialized for the first time.
 */
__inline BOOL HndFirstAssignHandle(OBJECTHANDLE handle, OBJECTREF objref)
{
    // sanity
    _ASSERTE(handle);

#ifdef _DEBUG
    // Make sure the objref is valid before it is assigned to a handle
    ValidateAssignObjrefForHandle(objref, HndGetHandleTableADIndex(HndGetHandleTable(handle)));
#endif
    // unwrap the objectref we were given
    _UNCHECKED_OBJECTREF value = OBJECTREF_TO_UNCHECKED_OBJECTREF(objref);

    // store the pointer if we are the first ones here
    BOOL success = (NULL == FastInterlockCompareExchange((void **)handle,
                                                         *(void **)&value,
                                                         NULL));

    // if we successfully did a non-NULL pointer store then invoke the write-barrier
    if (value && success)
        HndWriteBarrier(handle);

    // return our result
    return success;
}

/*
 * inline handle dereferencing
 */
FORCEINLINE OBJECTREF HndFetchHandle(OBJECTHANDLE handle)
{
    // sanity
    _ASSERTE(handle);

#ifdef _DEBUG
    // Make sure the objref for handle is valid
    ValidateFetchObjrefForHandle(ObjectToOBJECTREF(*(Object **)handle), 
                            HndGetHandleTableADIndex(HndGetHandleTable(handle)));
#endif
    // wrap the raw objectref and return it
    return UNCHECKED_OBJECTREF_TO_OBJECTREF(*(_UNCHECKED_OBJECTREF *)handle);
}


/*
 * inline null testing (needed in certain cases where we're in the wrong GC mod)
 */
FORCEINLINE BOOL HndIsNull(OBJECTHANDLE handle)
{
    // sanity
    _ASSERTE(handle);

    return NULL == *(Object **)handle;
}



/*
 * inline handle checking
 */
FORCEINLINE BOOL HndCheckForNullUnchecked(OBJECTHANDLE *pHandle)
{
    // sanity
    _ASSERTE(pHandle);

    return (*(_UNCHECKED_OBJECTREF **)pHandle == NULL ||
            (**(_UNCHECKED_OBJECTREF **)pHandle) == NULL);
}
/*--------------------------------------------------------------------------*/


#endif //_HANDLETABLE_H
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\handletablecore.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*
 * Generational GC handle manager.  Core Table Implementation.
 *
 * Implementation of core table management routines.
 *
 * francish
 */

#include "common.h"
#include "HandleTablePriv.h"



/****************************************************************************
 *
 * RANDOM HELPERS
 *
 ****************************************************************************/

//@TODO: put this lookup in a read only data or code section
BYTE c_rgLowBitIndex[256] =
{
    0xff, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x04, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x05, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x04, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x06, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x04, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x05, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x04, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x07, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x04, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x05, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x04, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x06, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x04, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x05, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x04, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
    0x03, 0x00, 0x01, 0x00, 0x02, 0x00, 0x01, 0x00,
};


/*
 * A 32/64 neutral quicksort
 */
//@TODO: move/merge into common util file
typedef int (*PFNCOMPARE)(UINT_PTR p, UINT_PTR q);
void QuickSort(UINT_PTR *pData, int left, int right, PFNCOMPARE pfnCompare)
{
    do
    {
        int i = left;
        int j = right;

        UINT_PTR x = pData[(i + j + 1) / 2];

        do
        {
            while (pfnCompare(pData[i], x) < 0)
                i++;

            while (pfnCompare(x, pData[j]) < 0)
                j--;

            if (i > j)
                break;

            if (i < j)
            {
                UINT_PTR t = pData[i];
                pData[i] = pData[j];
                pData[j] = t;
            }

            i++;
            j--;

        } while (i <= j);

        if ((j - left) <= (right - i))
        {
            if (left < j)
                QuickSort(pData, left, j, pfnCompare);

            left = i;
        }
        else
        {
            if (i < right)
                QuickSort(pData, i, right, pfnCompare);

            right = j;
        }

    } while (left < right);
}


/*
 * CompareHandlesByFreeOrder
 *
 * Returns:
 *  <0 - handle P should be freed before handle Q
 *  =0 - handles are eqivalent for free order purposes
 *  >0 - handle Q should be freed before handle P
 *
 */
int CompareHandlesByFreeOrder(UINT_PTR p, UINT_PTR q)
{
    // compute the segments for the handles
    TableSegment *pSegmentP = (TableSegment *)(p & HANDLE_SEGMENT_ALIGN_MASK);
    TableSegment *pSegmentQ = (TableSegment *)(q & HANDLE_SEGMENT_ALIGN_MASK);

    // are the handles in the same segment?
    if (pSegmentP == pSegmentQ)
    {
        // return the in-segment handle free order
        return (int)((INT_PTR)q - (INT_PTR)p);
    }
    else if (pSegmentP)
    {
        // do we have two valid segments?
        if (pSegmentQ)
        {
            // return the sequence order of the two segments
            return (int)(UINT)pSegmentQ->bSequence - (int)(UINT)pSegmentP->bSequence;
        }
        else
        {
            // only the P handle is valid - free Q first
            return 1;
        }
    }
    else if (pSegmentQ)
    {
        // only the Q handle is valid - free P first
        return -1;
    }

    // neither handle is valid
    return 0;
}


/*
 * ZeroHandles
 *
 * Zeroes the object pointers for an array of handles.
 *
 */
void ZeroHandles(OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // compute our stopping point
    OBJECTHANDLE *pLastHandle = pHandleBase + uCount;

    // loop over the array, zeroing as we go
    while (pHandleBase < pLastHandle)
    {
        // get the current handle from the array
        OBJECTHANDLE handle = *pHandleBase;

        // advance to the next handle
        pHandleBase++;

        // zero the handle's object pointer
        *(_UNCHECKED_OBJECTREF *)handle = NULL;
    }
}

#ifdef _DEBUG
void CALLBACK DbgCountEnumeratedBlocks(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo)
{
    // accumulate the block count in pInfo->param1
    pInfo->param1 += uCount;
}
#endif

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * CORE TABLE MANAGEMENT
 *
 ****************************************************************************/

/*
 * TableCanFreeSegmentNow
 *
 * Determines if it is OK to free the specified segment at this time.
 *
 */
BOOL TableCanFreeSegmentNow(HandleTable *pTable, TableSegment *pSegment)
{
    // sanity
    _ASSERTE(pTable);
    _ASSERTE(pSegment);
    _ASSERTE(pTable->pLock->OwnedByCurrentThread());

    // deterine if any segment is currently being scanned asynchronously
    TableSegment *pSegmentAsync = NULL;

    // do we have async info?
    AsyncScanInfo *pAsyncInfo = pTable->pAsyncScanInfo;
    if (pAsyncInfo)
    {
        // must always have underlying callback info in an async scan
        _ASSERTE(pAsyncInfo->pCallbackInfo);

        // yes - if a segment is being scanned asynchronously it is listed here
        pSegmentAsync = pAsyncInfo->pCallbackInfo->pCurrentSegment;
    }

    // we can free our segment if it isn't being scanned asynchronously right now
    return (pSegment != pSegmentAsync);
}


/*
 * BlockFetchUserDataPointer
 *
 * Gets the user data pointer for the first handle in a block.
 *
 */
LPARAM *BlockFetchUserDataPointer(TableSegment *pSegment, UINT uBlock, BOOL fAssertOnError)
{
    // assume NULL until we actually find the data
    LPARAM *pUserData = NULL;

    // get the user data index for this block
    UINT uData = pSegment->rgUserData[uBlock];

    // is there user data for the block?
    if (uData != BLOCK_INVALID)
    {
        // yes - compute the address of the user data
        pUserData = (LPARAM *)(pSegment->rgValue + (uData * HANDLE_HANDLES_PER_BLOCK));
    }
    else if (fAssertOnError)
    {
        // no user data is associated with this block
        //
        // we probably got here for one of the following reasons:
        //  1) an outside caller tried to do a user data operation on an incompatible handle
        //  2) the user data map in the segment is corrupt
        //  3) the global type flags are corrupt
        //
        _ASSERTE(FALSE);
    }

    // return the result
    return pUserData;
}


/*
 * HandleFetchSegmentPointer
 *
 * Computes the segment pointer for a given handle.
 *
 */
__inline TableSegment *HandleFetchSegmentPointer(OBJECTHANDLE handle)
{
    // find the segment for this handle
    TableSegment *pSegment = (TableSegment *)((UINT_PTR)handle & HANDLE_SEGMENT_ALIGN_MASK);

    // sanity
    _ASSERTE(pSegment);

    // return the segment pointer
    return pSegment;
}


/*
 * HandleValidateAndFetchUserDataPointer
 *
 * Gets the user data pointer for the specified handle.
 * ASSERTs and returns NULL if handle is not of the expected type.
 *
 */
LPARAM *HandleValidateAndFetchUserDataPointer(OBJECTHANDLE handle, UINT uTypeExpected)
{
    // get the segment for this handle
    TableSegment *pSegment = HandleFetchSegmentPointer(handle);

    // find the offset of this handle into the segment
    UINT_PTR offset = (UINT_PTR)handle & HANDLE_SEGMENT_CONTENT_MASK;

    // make sure it is in the handle area and not the header
    _ASSERTE(offset >= HANDLE_HEADER_SIZE);

    // convert the offset to a handle index
    UINT uHandle = (UINT)((offset - HANDLE_HEADER_SIZE) / HANDLE_SIZE);

    // compute the block this handle resides in
    UINT uBlock = uHandle / HANDLE_HANDLES_PER_BLOCK;

    // fetch the user data for this block
    LPARAM *pUserData = BlockFetchUserDataPointer(pSegment, uBlock, TRUE);

    // did we get the user data block?
    if (pUserData)
    {
        // yup - adjust the pointer to be handle-specific
        pUserData += (uHandle - (uBlock * HANDLE_HANDLES_PER_BLOCK));

        // validate the block type before returning the pointer
        if (pSegment->rgBlockType[uBlock] != uTypeExpected)
        {
            // type mismatch - caller error
            _ASSERTE(FALSE);

            // don't return a pointer to the caller
            pUserData = NULL;
        }
    }

    // return the result
    return pUserData;
}


/*
 * HandleQuickFetchUserDataPointer
 *
 * Gets the user data pointer for a handle.
 * Less validation is performed.
 *
 */
LPARAM *HandleQuickFetchUserDataPointer(OBJECTHANDLE handle)
{
    // get the segment for this handle
    TableSegment *pSegment = HandleFetchSegmentPointer(handle);

    // find the offset of this handle into the segment
    UINT_PTR offset = (UINT_PTR)handle & HANDLE_SEGMENT_CONTENT_MASK;

    // make sure it is in the handle area and not the header
    _ASSERTE(offset >= HANDLE_HEADER_SIZE);

    // convert the offset to a handle index
    UINT uHandle = (UINT)((offset - HANDLE_HEADER_SIZE) / HANDLE_SIZE);

    // compute the block this handle resides in
    UINT uBlock = uHandle / HANDLE_HANDLES_PER_BLOCK;

    // fetch the user data for this block
    LPARAM *pUserData = BlockFetchUserDataPointer(pSegment, uBlock, TRUE);

    // if we got the user data block then adjust the pointer to be handle-specific
    if (pUserData)
        pUserData += (uHandle - (uBlock * HANDLE_HANDLES_PER_BLOCK));

    // return the result
    return pUserData;
}


/*
 * HandleQuickSetUserData
 *
 * Stores user data with a handle.
 *
 */
void HandleQuickSetUserData(OBJECTHANDLE handle, LPARAM lUserData)
{
    // fetch the user data slot for this handle
    LPARAM *pUserData = HandleQuickFetchUserDataPointer(handle);

    // is there a slot?
    if (pUserData)
    {
        // yes - store the info
        *pUserData = lUserData;
    }
}


/*
 * HandleFetchType
 *
 * Computes the type index for a given handle.
 *
 */
UINT HandleFetchType(OBJECTHANDLE handle)
{
    // get the segment for this handle
    TableSegment *pSegment = HandleFetchSegmentPointer(handle);

    // find the offset of this handle into the segment
    UINT_PTR offset = (UINT_PTR)handle & HANDLE_SEGMENT_CONTENT_MASK;

    // make sure it is in the handle area and not the header
    _ASSERTE(offset >= HANDLE_HEADER_SIZE);

    // convert the offset to a handle index
    UINT uHandle = (UINT)((offset - HANDLE_HEADER_SIZE) / HANDLE_SIZE);

    // compute the block this handle resides in
    UINT uBlock = uHandle / HANDLE_HANDLES_PER_BLOCK;

    // return the block's type
    return pSegment->rgBlockType[uBlock];
}
    
/*
 * HandleFetchHandleTable
 *
 * Computes the type index for a given handle.
 *
 */
HandleTable *HandleFetchHandleTable(OBJECTHANDLE handle)
{
    // get the segment for this handle
    TableSegment *pSegment = HandleFetchSegmentPointer(handle);

    // return the table
    return pSegment->pHandleTable;
}

/*
 * SegmentInitialize
 *
 * Initializes a segment.
 *
 */
BOOL SegmentInitialize(TableSegment *pSegment, HandleTable *pTable)
{
    // we want to commit enough for the header PLUS some handles
    DWORD32 dwCommit =
        (HANDLE_HEADER_SIZE + g_SystemInfo.dwPageSize) & (~(g_SystemInfo.dwPageSize - 1));

    // commit the header
    if (!VirtualAlloc(pSegment, dwCommit, MEM_COMMIT, PAGE_READWRITE))
    {
        _ASSERTE(FALSE);
        return FALSE;
    }

    // remember how many blocks we commited
    pSegment->bCommitLine = (BYTE)((dwCommit - HANDLE_HEADER_SIZE) / HANDLE_BYTES_PER_BLOCK);

    // now preinitialize the 0xFF guys
    FillMemory(pSegment->rgGeneration, sizeof(pSegment->rgGeneration), 0xFF);
    FillMemory(pSegment->rgTail,       sizeof(pSegment->rgTail),       BLOCK_INVALID);
    FillMemory(pSegment->rgHint,       sizeof(pSegment->rgHint),       BLOCK_INVALID);
    FillMemory(pSegment->rgFreeMask,   sizeof(pSegment->rgFreeMask),   0xFF);
    FillMemory(pSegment->rgBlockType,  sizeof(pSegment->rgBlockType),  TYPE_INVALID);
    FillMemory(pSegment->rgUserData,   sizeof(pSegment->rgUserData),   BLOCK_INVALID);

    // prelink the free chain
    UINT u = 0;
    while (u < (HANDLE_BLOCKS_PER_SEGMENT - 1))
    {
        UINT next = u + 1;
        pSegment->rgAllocation[u] = next;
        u = next;
    }

    // and terminate the last node
    pSegment->rgAllocation[u] = BLOCK_INVALID;

    // store the back pointer from our new segment to its owning table
    pSegment->pHandleTable = pTable;

    // all done
    return TRUE;
}


/*
 * SegmentFree
 *
 * Frees the specified segment.
 *
 */
void SegmentFree(TableSegment *pSegment)
{
    // free the segment's memory
    VirtualFree(pSegment, 0, MEM_RELEASE);
}


/*
 * SegmentAlloc
 *
 * Allocates a new segment.
 *
 */
TableSegment *SegmentAlloc(HandleTable *pTable)
{
    // allocate the segment's address space
    TableSegment *pSegment =
        (TableSegment *)ReserveAlignedMemory(HANDLE_SEGMENT_SIZE, HANDLE_SEGMENT_SIZE);

    // bail out if we couldn't get any memory
    if (!pSegment)
    {
        _ASSERTE(FALSE);
        return NULL;
    }

    // initialize the header
    if (!SegmentInitialize(pSegment, pTable))
    {
        SegmentFree(pSegment);
        pSegment = NULL;
    }

    // all done
    return pSegment;
}


/*
 * SegmentRemoveFreeBlocks
 *
 * Scans a segment for free blocks of the specified type
 * and moves them to the segment's free list.
 *
 */
void SegmentRemoveFreeBlocks(TableSegment *pSegment, UINT uType, BOOL *pfScavengeLater)
{
    // fetch the tail block for the specified chain
    UINT uPrev = pSegment->rgTail[uType];

    // if it's a terminator then there are no blocks in the chain
    if (uPrev == BLOCK_INVALID)
        return;

    // we may need to clean up user data blocks later
    BOOL fCleanupUserData = FALSE;

    // start iterating with the head block
    UINT uStart = pSegment->rgAllocation[uPrev];
    UINT uBlock = uStart;

    // keep track of how many blocks we removed
    UINT uRemoved = 0;

    // we want to preserve the relative order of any blocks we free
    // this is the best we can do until the free list is resorted
    UINT uFirstFreed = BLOCK_INVALID;
    UINT uLastFreed  = BLOCK_INVALID;

    // loop until we've processed the whole chain
    for (;;)
    {
        // fetch the next block index
        UINT uNext = pSegment->rgAllocation[uBlock];

#ifdef HANDLE_OPTIMIZE_FOR_64_HANDLE_BLOCKS
        // determine whether this block is empty
        if (((PUINT64)pSegment->rgFreeMask)[uBlock] == 0xFFFFFFFFFFFFFFFFUL)
#else
        // assume this block is empty until we know otherwise
        BOOL fEmpty = TRUE;

        // get the first mask for this block
        DWORD32 *pdwMask     = pSegment->rgFreeMask + (uBlock * HANDLE_MASKS_PER_BLOCK);
        DWORD32 *pdwMaskLast = pdwMask              + HANDLE_MASKS_PER_BLOCK;

        // loop through the masks until we've processed them all or we've found handles
        do
        {
            // is this mask empty?
            if (*pdwMask != MASK_EMPTY)
            {
                // nope - this block still has handles in it
                fEmpty = FALSE;
                break;
            }

            // on to the next mask
            pdwMask++;

        } while (pdwMask < pdwMaskLast);

        // is this block empty?
        if (fEmpty)
#endif
        {
            // is this block currently locked?
            if (BlockIsLocked(pSegment, uBlock))
            {
                // block cannot be freed, if we were passed a scavenge flag then set it
                if (pfScavengeLater)
                    *pfScavengeLater = TRUE;
            }
            else
            {
                // safe to free - did it have user data associated?
                UINT uData = pSegment->rgUserData[uBlock];
                if (uData != BLOCK_INVALID)
                {
                    // data blocks are 'empty' so we keep them locked
                    // unlock the block so it can be reclaimed below
                    BlockUnlock(pSegment, uData);

                    // unlink the data block from the handle block
                    pSegment->rgUserData[uBlock] = BLOCK_INVALID;

                    // remember that we need to scavenge the data block chain
                    fCleanupUserData = TRUE;
                }

                // mark the block as free
                pSegment->rgBlockType[uBlock] = TYPE_INVALID;

                // have we freed any other blocks yet?
                if (uFirstFreed == BLOCK_INVALID)
                {
                    // no - this is the first one - remember it as the new head
                    uFirstFreed = uBlock;
                }
                else
                {
                    // yes - link this block to the other ones in order
                    pSegment->rgAllocation[uLastFreed] = (BYTE)uBlock;
                }

                // remember this block for later
                uLastFreed = uBlock;

                // are there other blocks in the chain?
                if (uPrev != uBlock)
                {
                    // yes - unlink this block from the chain
                    pSegment->rgAllocation[uPrev] = (BYTE)uNext;

                    // if we are removing the tail then pick a new tail
                    if (pSegment->rgTail[uType] == uBlock)
                        pSegment->rgTail[uType] = (BYTE)uPrev;

                    // if we are removing the hint then pick a new hint
                    if (pSegment->rgHint[uType] == uBlock)
                        pSegment->rgHint[uType] = (BYTE)uNext;

                    // we removed the current block - reset uBlock to a valid block
                    uBlock = uPrev;

                    // N.B. we'll check if we freed uStart later when it's safe to recover
                }
                else
                {
                    // we're removing last block - sanity check the loop condition
                    _ASSERTE(uNext == uStart);

                    // mark this chain as completely empty
                    pSegment->rgAllocation[uBlock] = BLOCK_INVALID;
                    pSegment->rgTail[uType]        = BLOCK_INVALID;
                    pSegment->rgHint[uType]        = BLOCK_INVALID;
                }

                // update the number of blocks we've removed
                uRemoved++;
            }
        }

        // if we are back at the beginning then it is time to stop
        if (uNext == uStart)
            break;

        // now see if we need to reset our start block
        if (uStart == uLastFreed)
            uStart = uNext;

        // on to the next block
        uPrev = uBlock;
        uBlock = uNext;
    }

    // did we remove any blocks?
    if (uRemoved)
    {
        // yes - link the new blocks into the free list
        pSegment->rgAllocation[uLastFreed] = pSegment->bFreeList;
        pSegment->bFreeList = (BYTE)uFirstFreed;

        // update the free count for this chain
        pSegment->rgFreeCount[uType] -= (uRemoved * HANDLE_HANDLES_PER_BLOCK);

        // mark for a resort - the free list (and soon allocation chains) may be out of order
        pSegment->fResortChains = TRUE;

        // if we removed blocks that had user data then we need to reclaim those too
        if (fCleanupUserData)
            SegmentRemoveFreeBlocks(pSegment, HNDTYPE_INTERNAL_DATABLOCK, NULL);
    }
}


/*
 * SegmentInsertBlockFromFreeListWorker
 *
 * Inserts a block into a block list within a segment.  Blocks are obtained from the
 * segment's free list.  Returns the index of the block inserted, or BLOCK_INVALID
 * if no blocks were avaliable.
 *
 * This routine is the core implementation for SegmentInsertBlockFromFreeList.
 *
 */
UINT SegmentInsertBlockFromFreeListWorker(TableSegment *pSegment, UINT uType, BOOL fUpdateHint)
{
    // fetch the next block from the free list
    UINT uBlock = pSegment->bFreeList;

    // if we got the terminator then there are no more blocks
    if (uBlock != BLOCK_INVALID)
    {
        // are we eating out of the last empty range of blocks?
        if (uBlock >= pSegment->bEmptyLine)
        {
            // get the current commit line
            UINT uCommitLine = pSegment->bCommitLine;

            // if this block is uncommitted then commit some memory now
            if (uBlock >= uCommitLine)
            {
                // figure out where to commit next
                LPVOID pvCommit = pSegment->rgValue + (uCommitLine * HANDLE_HANDLES_PER_BLOCK);

                // we should commit one more page of handles
                DWORD32 dwCommit = g_SystemInfo.dwPageSize;

                // commit the memory
                if (!VirtualAlloc(pvCommit, dwCommit, MEM_COMMIT, PAGE_READWRITE))
                    return BLOCK_INVALID;

                // use the previous commit line as the new decommit line
                pSegment->bDecommitLine = (BYTE)uCommitLine;

                // adjust the commit line by the number of blocks we commited
                pSegment->bCommitLine = (BYTE)(uCommitLine + (dwCommit / HANDLE_BYTES_PER_BLOCK));
            }

            // update our empty line
            pSegment->bEmptyLine = uBlock + 1;
        }

        // unlink our block from the free list
        pSegment->bFreeList = pSegment->rgAllocation[uBlock];

        // link our block into the specified chain
        UINT uOldTail = pSegment->rgTail[uType];
        if (uOldTail == BLOCK_INVALID)
        {
            // first block, set as head and link to itself
            pSegment->rgAllocation[uBlock] = (BYTE)uBlock;

            // there are no other blocks - update the hint anyway
            fUpdateHint = TRUE;
        }
        else
        {
            // not first block - link circularly
            pSegment->rgAllocation[uBlock] = pSegment->rgAllocation[uOldTail];
            pSegment->rgAllocation[uOldTail] = (BYTE)uBlock;
        
            // chain may need resorting depending on what we added
            pSegment->fResortChains = TRUE;
        }

        // mark this block with the type we're using it for
        pSegment->rgBlockType[uBlock] = (BYTE)uType;

        // update the chain tail
        pSegment->rgTail[uType] = (BYTE)uBlock;

        // if we are supposed to update the hint, then point it at the new block
        if (fUpdateHint)
            pSegment->rgHint[uType] = (BYTE)uBlock;

        // increment the chain's free count to reflect the additional block
        pSegment->rgFreeCount[uType] += HANDLE_HANDLES_PER_BLOCK;
    }

    // all done
    return uBlock;
}


/*
 * SegmentInsertBlockFromFreeList
 *
 * Inserts a block into a block list within a segment.  Blocks are obtained from the
 * segment's free list.  Returns the index of the block inserted, or BLOCK_INVALID
 * if no blocks were avaliable.
 *
 * This routine does the work of securing a parallel user data block if required.
 *
 */
UINT SegmentInsertBlockFromFreeList(TableSegment *pSegment, UINT uType, BOOL fUpdateHint)
{
    UINT uBlock, uData = 0;

    // does this block type require user data?
    BOOL fUserData = TypeHasUserData(pSegment->pHandleTable, uType);

    // if we need user data then we need to make sure it can go in the same segment as the handles
    if (fUserData)
    {
        // if we can't also fit the user data in this segment then bail
        uBlock = pSegment->bFreeList;
        if ((uBlock == BLOCK_INVALID) || (pSegment->rgAllocation[uBlock] == BLOCK_INVALID))
            return BLOCK_INVALID;

        // allocate our user data block (we do it in this order so that free order is nicer)
        uData = SegmentInsertBlockFromFreeListWorker(pSegment, HNDTYPE_INTERNAL_DATABLOCK, FALSE);
    }

    // now allocate the requested block
    uBlock = SegmentInsertBlockFromFreeListWorker(pSegment, uType, fUpdateHint);

    // should we have a block for user data too?
    if (fUserData)
    {
        // did we get them both?
        if ((uBlock != BLOCK_INVALID) && (uData != BLOCK_INVALID))
        {
            // link the data block to the requested block
            pSegment->rgUserData[uBlock] = (BYTE)uData;

            // no handles are ever allocated out of a data block
            // lock the block so it won't be reclaimed accidentally
            BlockLock(pSegment, uData);
        }
        else
        {
            // NOTE: We pre-screened that the blocks exist above, so we should only
            //       get here under heavy load when a MEM_COMMIT operation fails.

            // if the type block allocation succeeded then scavenge the type block list
            if (uBlock != BLOCK_INVALID)
                SegmentRemoveFreeBlocks(pSegment, uType, NULL);

            // if the user data allocation succeeded then scavenge the user data list
            if (uData != BLOCK_INVALID)
                SegmentRemoveFreeBlocks(pSegment, HNDTYPE_INTERNAL_DATABLOCK, NULL);

            // make sure we return failure
            uBlock = BLOCK_INVALID;
        }
    }

    // all done
    return uBlock;
}


/*
 * SegmentResortChains
 *
 * Sorts the block chains for optimal scanning order.
 * Sorts the free list to combat fragmentation.
 *
 */
void SegmentResortChains(TableSegment *pSegment)
{
    // clear the sort flag for this segment
    pSegment->fResortChains = FALSE;

    // first, do we need to scavenge any blocks?
    if (pSegment->fNeedsScavenging)
    {
        // clear the scavenge flag
        pSegment->fNeedsScavenging = FALSE;

        // we may need to explicitly scan the user data chain too
        BOOL fCleanupUserData = FALSE;

        // fetch the empty line for this segment
        UINT uLast = pSegment->bEmptyLine;

        // loop over all active blocks, scavenging the empty ones as we go
        for (UINT uBlock = 0; uBlock < uLast; uBlock++)
        {
            // fetch the block type of this block
            UINT uType = pSegment->rgBlockType[uBlock];

            // only process public block types - we handle data blocks separately
            if (uType < HANDLE_MAX_PUBLIC_TYPES)
            {
#ifdef HANDLE_OPTIMIZE_FOR_64_HANDLE_BLOCKS
                // determine whether this block is empty
                if (((PUINT64)pSegment->rgFreeMask)[uBlock] == 0xFFFFFFFFFFFFFFFFUL)
#else
                // assume this block is empty until we know otherwise
                BOOL fEmpty = TRUE;
    
                // get the first mask for this block
                DWORD32 *pdwMask     = pSegment->rgFreeMask + (uBlock * HANDLE_MASKS_PER_BLOCK);
                DWORD32 *pdwMaskLast = pdwMask              + HANDLE_MASKS_PER_BLOCK;

                // loop through the masks until we've processed them all or we've found handles
                do
                {
                    // is this mask empty?
                    if (*pdwMask != MASK_EMPTY)
                    {
                        // nope - this block still has handles in it
                        fEmpty = FALSE;
                        break;
                    }

                    // on to the next mask
                    pdwMask++;

                } while (pdwMask < pdwMaskLast);

                // is this block empty?
                if (fEmpty)
#endif
                {
                    // is the block unlocked?
                    if (!BlockIsLocked(pSegment, uBlock))
                    {
                        // safe to free - did it have user data associated?
                        UINT uData = pSegment->rgUserData[uBlock];
                        if (uData != BLOCK_INVALID)
                        {
                            // data blocks are 'empty' so we keep them locked
                            // unlock the block so it can be reclaimed below
                            BlockUnlock(pSegment, uData);

                            // unlink the data block from the handle block
                            pSegment->rgUserData[uBlock] = BLOCK_INVALID;

                            // remember that we need to scavenge the data block chain
                            fCleanupUserData = TRUE;
                        }

                        // mark the block as free
                        pSegment->rgBlockType[uBlock] = TYPE_INVALID;

                        // fix up the free count for the block's type
                        pSegment->rgFreeCount[uType] -= HANDLE_HANDLES_PER_BLOCK;

                        // N.B. we don't update the list linkages here since they are rebuilt below
                    }
                }
            }
        }

        // if we have to clean up user data then do that now
        if (fCleanupUserData)
            SegmentRemoveFreeBlocks(pSegment, HNDTYPE_INTERNAL_DATABLOCK, NULL);
    }

    // keep some per-chain data
    BYTE rgChainCurr[HANDLE_MAX_INTERNAL_TYPES];
    BYTE rgChainHigh[HANDLE_MAX_INTERNAL_TYPES];
    BYTE bChainFree = BLOCK_INVALID;
    UINT uEmptyLine = BLOCK_INVALID;
    BOOL fContiguousWithFreeList = TRUE;

    // preinit the chain data to no blocks
    for (UINT uType = 0; uType < HANDLE_MAX_INTERNAL_TYPES; uType++)
        rgChainHigh[uType] = rgChainCurr[uType] = BLOCK_INVALID;

    // scan back through the block types
    UINT uBlock = HANDLE_BLOCKS_PER_SEGMENT;
    while (uBlock > 0)
    {
        // decrement the block index
        uBlock--;

        // fetch the type for this block
        uType = pSegment->rgBlockType[uBlock];

        // is this block allocated?
        if (uType != TYPE_INVALID)
        {
            // looks allocated
            fContiguousWithFreeList = FALSE;
             
            // hope the segment's not corrupt :)
            _ASSERTE(uType < HANDLE_MAX_INTERNAL_TYPES);

            // remember the first block we see for each type
            if (rgChainHigh[uType] == BLOCK_INVALID)
                rgChainHigh[uType] = uBlock;

            // link this block to the last one we saw of this type
            pSegment->rgAllocation[uBlock] = rgChainCurr[uType];

            // remember this block in type chain
            rgChainCurr[uType] = (BYTE)uBlock;
        }
        else
        {
            // block is free - is it also contiguous with the free list?
            if (fContiguousWithFreeList)
                uEmptyLine = uBlock;

            // link this block to the last one in the free chain
            pSegment->rgAllocation[uBlock] = bChainFree;

            // add this block to the free list
            bChainFree = (BYTE)uBlock;
        }
    }

    // now close the loops and store the tails
    for (uType = 0; uType < HANDLE_MAX_INTERNAL_TYPES; uType++)
    {
        // get the first block in the list
        BYTE bBlock = rgChainCurr[uType];

        // if there is a list then make it circular and save it
        if (bBlock != BLOCK_INVALID)
        {
            // highest block we saw becomes tail
            UINT uTail = rgChainHigh[uType];

            // store tail in segment
            pSegment->rgTail[uType] = (BYTE)uTail;

            // link tail to head
            pSegment->rgAllocation[uTail] = bBlock;
        }
    }

    // store the new free list head
    pSegment->bFreeList = bChainFree;

    // compute the new empty line
    if (uEmptyLine > HANDLE_BLOCKS_PER_SEGMENT)
        uEmptyLine = HANDLE_BLOCKS_PER_SEGMENT;

    // store the updated empty line
    pSegment->bEmptyLine = (BYTE)uEmptyLine;
}


/*
 * SegmentTrimExcessPages
 *
 * Checks to see if any pages can be decommitted from the segment
 *
 */
void SegmentTrimExcessPages(TableSegment *pSegment)
{
    // fetch the empty and decommit lines
    UINT uEmptyLine    = pSegment->bEmptyLine;
    UINT uDecommitLine = pSegment->bDecommitLine;

    // check to see if we can decommit some handles
    // NOTE: we use '<' here to avoid playing ping-pong on page boundaries
    //       this is OK since the zero case is handled elsewhere (segment gets freed)
    if (uEmptyLine < uDecommitLine)
    {
        // derive some useful info about the page size
        DWORD32 dwPageRound = g_SystemInfo.dwPageSize - 1;
        DWORD32 dwPageMask  = ~dwPageRound;

        // compute the address corresponding to the empty line
        size_t dwLo = (size_t)pSegment->rgValue + (uEmptyLine  * HANDLE_BYTES_PER_BLOCK);

        // adjust the empty line address to the start of the nearest whole empty page
        dwLo = (dwLo + dwPageRound) & dwPageMask;

        // compute the address corresponding to the old commit line
        size_t dwHi = (size_t)pSegment->rgValue + ((UINT)pSegment->bCommitLine * HANDLE_BYTES_PER_BLOCK);

        // is there anything to decommit?
        if (dwHi > dwLo)
        {
            // decommit the memory
            VirtualFree((LPVOID)dwLo, dwHi - dwLo, MEM_DECOMMIT);

            // update the commit line
            pSegment->bCommitLine = (BYTE)((dwLo - (size_t)pSegment->rgValue) / HANDLE_BYTES_PER_BLOCK);

            // compute the address for the new decommit line
            size_t dwDecommitAddr = dwLo - g_SystemInfo.dwPageSize;

            // assume a decommit line of zero until we know otheriwse
            uDecommitLine = 0;

            // if the address is within the handle area then compute the line from the address
            if (dwDecommitAddr > (size_t)pSegment->rgValue)
                uDecommitLine = (UINT)((dwDecommitAddr - (size_t)pSegment->rgValue) / HANDLE_BYTES_PER_BLOCK);

            // update the decommit line
            pSegment->bDecommitLine = (BYTE)uDecommitLine;
        }
    }
}


/*
 * BlockAllocHandlesInMask
 *
 * Attempts to allocate the requested number of handes of the specified type,
 * from the specified mask of the specified handle block.
 *
 * Returns the number of available handles actually allocated.
 *
 */
UINT BlockAllocHandlesInMask(TableSegment *pSegment, UINT uBlock,
                             DWORD32 *pdwMask, UINT uHandleMaskDisplacement,
                             OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // keep track of how many handles we have left to allocate
    UINT uRemain = uCount;

    // fetch the free mask into a local so we can play with it
    DWORD32 dwFree = *pdwMask;

    // keep track of our displacement within the mask
    UINT uByteDisplacement = 0;

    // examine the mask byte by byte for free handles
    do
    {
        // grab the low byte of the mask
        DWORD32 dwLowByte = (dwFree & MASK_LOBYTE);

        // are there any free handles here?
        if (dwLowByte)
        {
            // remember which handles we've taken
            DWORD32 dwAlloc = 0;

            // loop until we've allocated all the handles we can from here
            do
            {
                // get the index of the next handle
                UINT uIndex = c_rgLowBitIndex[dwLowByte];

                // compute the mask for the handle we chose
                dwAlloc |= (1 << uIndex);

                // remove this handle from the mask byte
                dwLowByte &= ~dwAlloc;

                // compute the index of this handle in the segment
                uIndex += uHandleMaskDisplacement + uByteDisplacement;

                // store the allocated handle in the handle array
                *pHandleBase = (OBJECTHANDLE)(pSegment->rgValue + uIndex);

                // adjust our count and array pointer
                uRemain--;
                pHandleBase++;

            } while (dwLowByte && uRemain);

            // shift the allocation mask into position
            dwAlloc <<= uByteDisplacement;

            // update the mask to account for the handles we allocated
            *pdwMask &= ~dwAlloc;
        }

        // on to the next byte in the mask
        dwFree >>= BITS_PER_BYTE;
        uByteDisplacement += BITS_PER_BYTE;

    } while (uRemain && dwFree);

    // return the number of handles we got
    return (uCount - uRemain);
}


/*
 * BlockAllocHandlesInitial
 *
 * Allocates a specified number of handles from a newly committed (empty) block.
 *
 */
UINT BlockAllocHandlesInitial(TableSegment *pSegment, UINT uType, UINT uBlock,
                              OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // sanity check
    _ASSERTE(uCount);

    // validate the number of handles we were asked to allocate
    if (uCount > HANDLE_HANDLES_PER_BLOCK)
    {
        _ASSERTE(FALSE);
        uCount = HANDLE_HANDLES_PER_BLOCK;
    }

    // keep track of how many handles we have left to mark in masks
    UINT uRemain = uCount;

    // get the first mask for this block
    DWORD32 *pdwMask = pSegment->rgFreeMask + (uBlock * HANDLE_MASKS_PER_BLOCK);

    // loop through the masks, zeroing the appropriate free bits
    do
    {
        // this is a brand new block - all masks we encounter should be totally free
        _ASSERTE(*pdwMask == MASK_EMPTY);

        // pick an initial guess at the number to allocate
        UINT uAlloc = uRemain;

        // compute the default mask based on that count
        DWORD32 dwNewMask = (MASK_EMPTY << uAlloc);

        // are we allocating all of them?
        if (uAlloc >= HANDLE_HANDLES_PER_MASK)
        {
            // shift above has unpredictable results in this case
            dwNewMask = MASK_FULL;
            uAlloc = HANDLE_HANDLES_PER_MASK;
        }

        // set the free mask
        *pdwMask = dwNewMask;

        // update our count and mask pointer
        uRemain -= uAlloc;
        pdwMask++;

    } while (uRemain);

    // compute the bounds for allocation so we can copy the handles
    _UNCHECKED_OBJECTREF *pValue = pSegment->rgValue + (uBlock * HANDLE_HANDLES_PER_BLOCK);
    _UNCHECKED_OBJECTREF *pLast  = pValue + uCount;

    // loop through filling in the output array with handles
    do
    {
        // store the next handle in the next array slot
        *pHandleBase = (OBJECTHANDLE)pValue;

        // increment our source and destination
        pValue++;
        pHandleBase++;

    } while (pValue < pLast);

    // return the number of handles we allocated
    return uCount;
}


/*
 * BlockAllocHandles
 *
 * Attempts to allocate the requested number of handes of the specified type,
 * from the specified handle block.
 *
 * Returns the number of available handles actually allocated.
 *
 */
UINT BlockAllocHandles(TableSegment *pSegment, UINT uBlock, OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // keep track of how many handles we have left to allocate
    UINT uRemain = uCount;

    // set up our loop and limit mask pointers
    DWORD32 *pdwMask     = pSegment->rgFreeMask + (uBlock * HANDLE_MASKS_PER_BLOCK);
    DWORD32 *pdwMaskLast = pdwMask + HANDLE_MASKS_PER_BLOCK;

    // keep track of the handle displacement for the mask we're scanning
    UINT uDisplacement = uBlock * HANDLE_HANDLES_PER_BLOCK;

    // loop through all the masks, allocating handles as we go
    do
    {
        // if this mask indicates free handles then grab them
        if (*pdwMask)
        {
            // allocate as many handles as we need from this mask
            UINT uSatisfied = BlockAllocHandlesInMask(pSegment, uBlock, pdwMask, uDisplacement, pHandleBase, uRemain);

            // adjust our count and array pointer
            uRemain     -= uSatisfied;
            pHandleBase += uSatisfied;
    
            // if there are no remaining slots to be filled then we are done
            if (!uRemain)
                break;
        }

        // on to the next mask
        pdwMask++;
        uDisplacement += HANDLE_HANDLES_PER_MASK;

    } while (pdwMask < pdwMaskLast);

    // return the number of handles we got
    return (uCount - uRemain);
}


/*
 * SegmentAllocHandlesFromTypeChain
 *
 * Attempts to allocate the requested number of handes of the specified type,
 * from the specified segment's block chain for the specified type.  This routine
 * ONLY scavenges existing blocks in the type chain.  No new blocks are committed.
 *
 * Returns the number of available handles actually allocated.
 *
 */
UINT SegmentAllocHandlesFromTypeChain(TableSegment *pSegment, UINT uType, OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // fetch the number of handles available in this chain
    UINT uAvail = pSegment->rgFreeCount[uType];

    // is the available count greater than the requested count?
    if (uAvail > uCount)
    {
        // yes - all requested handles are available
        uAvail = uCount;
    }
    else
    {
        // no - we can only satisfy some of the request
        uCount = uAvail;
    }

    // did we find that any handles are available?
    if (uAvail)
    {
        // yes - fetch the head of the block chain and set up a loop limit
        UINT uBlock = pSegment->rgHint[uType];
        UINT uLast = uBlock;

        // loop until we have found all handles known to be available
        for (;;)
        {
            // try to allocate handles from the current block
            UINT uSatisfied = BlockAllocHandles(pSegment, uBlock, pHandleBase, uAvail);

            // did we get everything we needed?
            if (uSatisfied == uAvail)
            {
                // yes - update the hint for this type chain and get out
                pSegment->rgHint[uType] = (BYTE)uBlock;
                break;
            }

            // adjust our count and array pointer
            uAvail      -= uSatisfied;
            pHandleBase += uSatisfied;

            // fetch the next block in the type chain
            uBlock = pSegment->rgAllocation[uBlock];

            // are we out of blocks?
            if (uBlock == uLast)
            {
                // free count is corrupt
                _ASSERTE(FALSE);

                // avoid making the problem any worse
                uCount -= uAvail;
                break;
            }
        }

        // update the free count
        pSegment->rgFreeCount[uType] -= uCount;
    }

    // return the number of handles we got
    return uCount;
}


/*
 * SegmentAllocHandlesFromFreeList
 *
 * Attempts to allocate the requested number of handes of the specified type,
 * by committing blocks from the free list to that type's type chain.
 *
 * Returns the number of available handles actually allocated.
 *
 */
UINT SegmentAllocHandlesFromFreeList(TableSegment *pSegment, UINT uType, OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // keep track of how many handles we have left to allocate
    UINT uRemain = uCount;

    // loop allocating handles until we are done or we run out of free blocks
    do
    {
        // start off assuming we can allocate all the handles
        UINT uAlloc = uRemain;

        // we can only get a block-full of handles at a time
        if (uAlloc > HANDLE_HANDLES_PER_BLOCK)
            uAlloc = HANDLE_HANDLES_PER_BLOCK;

        // try to get a block from the free list
        UINT uBlock = SegmentInsertBlockFromFreeList(pSegment, uType, (uRemain == uCount));

        // if there are no free blocks left then we are done
        if (uBlock == BLOCK_INVALID)
            break;

        // initialize the block by allocating the required handles into the array
        uAlloc = BlockAllocHandlesInitial(pSegment, uType, uBlock, pHandleBase, uAlloc);

        // adjust our count and array pointer
        uRemain     -= uAlloc;
        pHandleBase += uAlloc;

    } while (uRemain);

    // compute the number of handles we took
    uCount -= uRemain;

    // update the free count by the number of handles we took
    pSegment->rgFreeCount[uType] -= uCount;

    // return the number of handles we got
    return uCount;
}


/*
 * SegmentAllocHandles
 *
 * Attempts to allocate the requested number of handes of the specified type,
 * from the specified segment.
 *
 * Returns the number of available handles actually allocated.
 *
 */
UINT SegmentAllocHandles(TableSegment *pSegment, UINT uType, OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // first try to get some handles from the existing type chain
    UINT uSatisfied = SegmentAllocHandlesFromTypeChain(pSegment, uType, pHandleBase, uCount);

    // if there are still slots to be filled then we need to commit more blocks to the type chain
    if (uSatisfied < uCount)
    {
        // adjust our count and array pointer
        uCount      -= uSatisfied;
        pHandleBase += uSatisfied;

        // get remaining handles by committing blocks from the free list
        uSatisfied += SegmentAllocHandlesFromFreeList(pSegment, uType, pHandleBase, uCount);
    }

    // return the number of handles we got
    return uSatisfied;
}


/*
 * TableAllocBulkHandles
 *
 * Attempts to allocate the requested number of handes of the specified type.
 *
 * Returns the number of handles that were actually allocated.  This is always
 * the same as the number of handles requested except in out-of-memory conditions,
 * in which case it is the number of handles that were successfully allocated.
 *
 */
UINT TableAllocBulkHandles(HandleTable *pTable, UINT uType, OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // keep track of how many handles we have left to allocate
    UINT uRemain = uCount;

    // start with the first segment and loop until we are done
    TableSegment *pSegment = pTable->pSegmentList;
    for (;;)
    {
        // get some handles from the current segment
        UINT uSatisfied = SegmentAllocHandles(pSegment, uType, pHandleBase, uRemain);

        // adjust our count and array pointer
        uRemain     -= uSatisfied;
        pHandleBase += uSatisfied;

        // if there are no remaining slots to be filled then we are done
        if (!uRemain)
            break;

        // fetch the next segment in the chain.
        TableSegment *pNextSegment = pSegment->pNextSegment;

        // if are no more segments then allocate another
        if (!pNextSegment)
        {
            // ok if this fails then we're out of luck
            pNextSegment = SegmentAlloc(pTable);
            if (!pNextSegment)
            {
                // we ran out of memory allocating a new segment.
                // this may not be catastrophic - if there are still some
                // handles in the cache then some allocations may succeed.
                _ASSERTE(FALSE);
                break;
            }

            // set up the correct sequence number for the new segment
            pNextSegment->bSequence = (BYTE)(((UINT)pSegment->bSequence + 1) % 0x100);

            // link the new segment into the list
            pSegment->pNextSegment = pNextSegment;
        }

        // try again with new segment
        pSegment = pNextSegment;
    }

    // return the number of handles we actually got
    return (uCount - uRemain);
}


/*
 * BlockFreeHandlesInMask
 *
 * Frees some portion of an array of handles of the specified type.
 * The array is scanned forward and handles are freed until a handle
 * from a different mask is encountered.
 *
 * Returns the number of handles that were freed from the front of the array.
 *
 */
UINT BlockFreeHandlesInMask(TableSegment *pSegment, UINT uBlock, UINT uMask, OBJECTHANDLE *pHandleBase, UINT uCount,
                            LPARAM *pUserData, UINT *puActualFreed, BOOL *pfAllMasksFree)
{
    // keep track of how many handles we have left to free
    UINT uRemain = uCount;

    // if this block has user data, convert the pointer to be mask-relative
    if (pUserData)
        pUserData += (uMask * HANDLE_HANDLES_PER_MASK);

    // convert our mask index to be segment-relative
    uMask += (uBlock * HANDLE_MASKS_PER_BLOCK);

    // compute the handle bounds for our mask
    OBJECTHANDLE firstHandle = (OBJECTHANDLE)(pSegment->rgValue + (uMask * HANDLE_HANDLES_PER_MASK));
    OBJECTHANDLE lastHandle  = (OBJECTHANDLE)((_UNCHECKED_OBJECTREF *)firstHandle + HANDLE_HANDLES_PER_MASK);

    // keep a local copy of the free mask to update as we free handles
    DWORD32 dwFreeMask = pSegment->rgFreeMask[uMask];

    // keep track of how many bogus frees we are asked to do
    UINT uBogus = 0;

    // loop freeing handles until we encounter one outside our block or there are none left
    do
    {
        // fetch the next handle in the array
        OBJECTHANDLE handle = *pHandleBase;

        // if the handle is outside our segment then we are done
        if ((handle < firstHandle) || (handle >= lastHandle))
            break;

        // sanity check - the handle should no longer refer to an object here
        _ASSERTE(*(_UNCHECKED_OBJECTREF *)handle == 0);

        // compute the handle index within the mask
        UINT uHandle = (UINT)(handle - firstHandle);

        // if there is user data then clear the user data for this handle
        if (pUserData)
            pUserData[uHandle] = 0L;

        // compute the mask bit for this handle
        DWORD32 dwFreeBit = (1 << uHandle);

        // the handle should not already be free
        if ((dwFreeMask & dwFreeBit) != 0)
        {
            // SOMEONE'S FREEING A HANDLE THAT ISN'T ALLOCATED
            uBogus++;
            _ASSERTE(FALSE);
        }

        // add this handle to the tally of freed handles
        dwFreeMask |= dwFreeBit;

        // adjust our count and array pointer
        uRemain--;
        pHandleBase++;

    } while (uRemain);

    // update the mask to reflect the handles we changed
    pSegment->rgFreeMask[uMask] = dwFreeMask;

    // if not all handles in this mask are free then tell our caller not to check the block
    if (dwFreeMask != MASK_EMPTY)
        *pfAllMasksFree = FALSE;

    // compute the number of handles we processed from the array
    UINT uFreed = (uCount - uRemain);

    // tell the caller how many handles we actually freed
    *puActualFreed += (uFreed - uBogus);

    // return the number of handles we actually freed
    return uFreed;
}


/*
 * BlockFreeHandles
 *
 * Frees some portion of an array of handles of the specified type.
 * The array is scanned forward and handles are freed until a handle
 * from a different block is encountered.
 *
 * Returns the number of handles that were freed from the front of the array.
 *
 */
UINT BlockFreeHandles(TableSegment *pSegment, UINT uBlock, OBJECTHANDLE *pHandleBase, UINT uCount,
                      UINT *puActualFreed, BOOL *pfScanForFreeBlocks)
{
    // keep track of how many handles we have left to free
    UINT uRemain = uCount;

    // fetch the user data for this block, if any
    LPARAM *pBlockUserData = BlockFetchUserDataPointer(pSegment, uBlock, FALSE);

    // compute the handle bounds for our block
    OBJECTHANDLE firstHandle = (OBJECTHANDLE)(pSegment->rgValue + (uBlock * HANDLE_HANDLES_PER_BLOCK));
    OBJECTHANDLE lastHandle  = (OBJECTHANDLE)((_UNCHECKED_OBJECTREF *)firstHandle + HANDLE_HANDLES_PER_BLOCK);

    // this variable will only stay TRUE if all masks we touch end up in the free state
    BOOL fAllMasksWeTouchedAreFree = TRUE;

    // loop freeing handles until we encounter one outside our block or there are none left
    do
    {
        // fetch the next handle in the array
        OBJECTHANDLE handle = *pHandleBase;

        // if the handle is outside our segment then we are done
        if ((handle < firstHandle) || (handle >= lastHandle))
            break;

        // compute the mask that this handle resides in
        UINT uMask = (UINT)((handle - firstHandle) / HANDLE_HANDLES_PER_MASK);

        // free as many handles as this mask owns from the front of the array
        UINT uFreed = BlockFreeHandlesInMask(pSegment, uBlock, uMask, pHandleBase, uRemain,
                                             pBlockUserData, puActualFreed, &fAllMasksWeTouchedAreFree);

        // adjust our count and array pointer
        uRemain     -= uFreed;
        pHandleBase += uFreed;

    } while (uRemain);

    // are all masks we touched free?
    if (fAllMasksWeTouchedAreFree)
    {
        // is the block unlocked?
        if (!BlockIsLocked(pSegment, uBlock))
        {
            // tell the caller it might be a good idea to scan for free blocks
            *pfScanForFreeBlocks = TRUE;
        }
    }

    // return the number of handles we actually freed
    return (uCount - uRemain);
}


/*
 * SegmentFreeHandles
 *
 * Frees some portion of an array of handles of the specified type.
 * The array is scanned forward and handles are freed until a handle
 * from a different segment is encountered.
 *
 * Returns the number of handles that were freed from the front of the array.
 *
 */
UINT SegmentFreeHandles(TableSegment *pSegment, UINT uType, OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // keep track of how many handles we have left to free
    UINT uRemain = uCount;

    // compute the handle bounds for our segment
    OBJECTHANDLE firstHandle = (OBJECTHANDLE)pSegment->rgValue;
    OBJECTHANDLE lastHandle  = (OBJECTHANDLE)((_UNCHECKED_OBJECTREF *)firstHandle + HANDLE_HANDLES_PER_SEGMENT);

    // the per-block free routines will set this if there is a chance some blocks went free
    BOOL fScanForFreeBlocks = FALSE;

    // track the number of handles we actually free
    UINT uActualFreed = 0;

    // loop freeing handles until we encounter one outside our segment or there are none left
    do
    {
        // fetch the next handle in the array
        OBJECTHANDLE handle = *pHandleBase;

        // if the handle is outside our segment then we are done
        if ((handle < firstHandle) || (handle >= lastHandle))
            break;

        // compute the block that this handle resides in
        UINT uBlock = (UINT)((handle - firstHandle) / HANDLE_HANDLES_PER_BLOCK);

        // sanity check that this block is the type we expect to be freeing
        _ASSERTE(pSegment->rgBlockType[uBlock] == uType);

        // free as many handles as this block owns from the front of the array
        UINT uFreed = BlockFreeHandles(pSegment, uBlock, pHandleBase, uRemain, &uActualFreed, &fScanForFreeBlocks);

        // adjust our count and array pointer
        uRemain     -= uFreed;
        pHandleBase += uFreed;

    } while (uRemain);

    // compute the number of handles we actually freed
    UINT uFreed = (uCount - uRemain);

    // update the free count
    pSegment->rgFreeCount[uType] += uActualFreed;

    // if we saw blocks that may have gone totally free then do a free scan
    if (fScanForFreeBlocks)
    {
        // assume we no scavenging is required
        BOOL fNeedsScavenging = FALSE;

        // try to remove any free blocks we may have created
        SegmentRemoveFreeBlocks(pSegment, uType, &fNeedsScavenging);

        // did SegmentRemoveFreeBlocks have to skip over any free blocks?
        if (fNeedsScavenging)
        {
            // yup, arrange to scavenge them later
            pSegment->fResortChains    = TRUE;
            pSegment->fNeedsScavenging = TRUE;
        }
    }

    // return the total number of handles we freed
    return uFreed;
}


/*
 * TableFreeBulkPreparedHandles
 *
 * Frees an array of handles of the specified type.
 *
 * This routine is optimized for a sorted array of handles but will accept any order.
 *
 */
void TableFreeBulkPreparedHandles(HandleTable *pTable, UINT uType, OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // loop until all handles are freed
    do
    {
        // get the segment for the first handle
        TableSegment *pSegment = HandleFetchSegmentPointer(*pHandleBase);

        // sanity
        _ASSERTE(pSegment->pHandleTable == pTable);

        // free as many handles as this segment owns from the front of the array
        UINT uFreed = SegmentFreeHandles(pSegment, uType, pHandleBase, uCount);

        // adjust our count and array pointer
        uCount      -= uFreed;
        pHandleBase += uFreed;

    } while (uCount);
}


/*
 * TableFreeBulkUnpreparedHandlesWorker
 *
 * Frees an array of handles of the specified type by preparing them and calling TableFreeBulkPreparedHandles.
 * Uses the supplied scratch buffer to prepare the handles.
 *
 */
void TableFreeBulkUnpreparedHandlesWorker(HandleTable *pTable, UINT uType, const OBJECTHANDLE *pHandles, UINT uCount,
                                          OBJECTHANDLE *pScratchBuffer)
{
    // copy the handles into the destination buffer
    CopyMemory(pScratchBuffer, pHandles, uCount * sizeof(OBJECTHANDLE));
 
    // sort them for optimal free order
    QuickSort((UINT_PTR *)pScratchBuffer, 0, uCount - 1, CompareHandlesByFreeOrder);
 
    // make sure the handles are zeroed too
    ZeroHandles(pScratchBuffer, uCount);
 
    // prepare and free these handles
    TableFreeBulkPreparedHandles(pTable, uType, pScratchBuffer, uCount);
}
 

/*
 * TableFreeBulkUnpreparedHandles
 *
 * Frees an array of handles of the specified type by preparing them and calling
 * TableFreeBulkPreparedHandlesWorker one or more times.
 *
 */
void TableFreeBulkUnpreparedHandles(HandleTable *pTable, UINT uType, const OBJECTHANDLE *pHandles, UINT uCount)
{
    // preparation / free buffer
    OBJECTHANDLE rgStackHandles[HANDLE_HANDLES_PER_BLOCK];
    OBJECTHANDLE *pScratchBuffer  = rgStackHandles;
    HLOCAL       hScratchBuffer   = NULL;
    UINT         uFreeGranularity = ARRAYSIZE(rgStackHandles);
 
    // if there are more handles than we can put on the stack then try to allocate a sorting buffer
    if (uCount > uFreeGranularity)
    {
        // try to allocate a bigger buffer to work in
        hScratchBuffer = LocalAlloc(LMEM_FIXED, uCount * sizeof(OBJECTHANDLE));
 
        // did we get it?
        if (hScratchBuffer)
        {
            // yes - use this buffer to prepare and free the handles
            pScratchBuffer   = (OBJECTHANDLE *)hScratchBuffer;
            uFreeGranularity = uCount;
        }
    }
 
    // loop freeing handles until we have freed them all
    while (uCount)
    {
        // decide how many we can process in this iteration
        if (uFreeGranularity > uCount)
            uFreeGranularity = uCount;
 
        // prepare and free these handles
        TableFreeBulkUnpreparedHandlesWorker(pTable, uType, pHandles, uFreeGranularity, pScratchBuffer);
 
        // adjust our pointers and move on
        uCount   -= uFreeGranularity;
        pHandles += uFreeGranularity;
    }
 
    // if we allocated a sorting buffer then free it now
    if (hScratchBuffer)
        LocalFree(hScratchBuffer);
}

/*--------------------------------------------------------------------------*/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\handletablepriv.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*
 * Generational GC handle manager.  Internal Implementation Header.
 *
 * Shared defines and declarations for handle table implementation.
 *
 * francish
 */

#include "common.h"
#include "HandleTable.h"

#include "vars.hpp"
#include "crst.h"


/*--------------------------------------------------------------------------*/

//@TODO: find a home for this in a project-level header file
#define BITS_PER_BYTE               (8)
#define ARRAYSIZE(x)                (sizeof(x)/sizeof(x[0]))

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * MAJOR TABLE DEFINITIONS THAT CHANGE DEPENDING ON THE WEATHER
 *
 ****************************************************************************/

#ifndef _WIN64

    // Win32 - 64k reserved per segment with 4k as header
    #define HANDLE_SEGMENT_SIZE     (0x10000)   // MUST be a power of 2
    #define HANDLE_HEADER_SIZE      (0x1000)    // SHOULD be <= OS page size

#else

    // Win64 - 128k reserved per segment with 4k as header
    #define HANDLE_SEGMENT_SIZE     (0x20000)   // MUST be a power of 2
    #define HANDLE_HEADER_SIZE      (0x1000)    // SHOULD be <= OS page size

#endif


#ifndef _BIG_ENDIAN

    // little-endian write barrier mask manipulation
    #define GEN_CLUMP_0_MASK        (0x000000FF)
    #define NEXT_CLUMP_IN_MASK(dw)  (dw >> BITS_PER_BYTE)

#else

    // big-endian write barrier mask manipulation
    #define GEN_CLUMP_0_MASK        (0xFF000000)
    #define NEXT_CLUMP_IN_MASK(dw)  (dw << BITS_PER_BYTE)

#endif


// if the above numbers change than these will likely change as well
#define HANDLE_HANDLES_PER_CLUMP    (16)        // segment write-barrier granularity
#define HANDLE_HANDLES_PER_BLOCK    (64)        // segment suballocation granularity
#define HANDLE_OPTIMIZE_FOR_64_HANDLE_BLOCKS    // flag for certain optimizations

// maximum number of internally supported handle types
#define HANDLE_MAX_INTERNAL_TYPES   (8)                             // should be a multiple of 4

// number of types allowed for public callers
#define HANDLE_MAX_PUBLIC_TYPES     (HANDLE_MAX_INTERNAL_TYPES - 1) // reserve one internal type

// internal block types
#define HNDTYPE_INTERNAL_DATABLOCK  (HANDLE_MAX_INTERNAL_TYPES - 1) // reserve last type for data blocks

// max number of generations to support statistics on
#define MAXSTATGEN                  (5)

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * MORE DEFINITIONS
 *
 ****************************************************************************/

// fast handle-to-segment mapping
#define HANDLE_SEGMENT_CONTENT_MASK     (HANDLE_SEGMENT_SIZE - 1)
#define HANDLE_SEGMENT_ALIGN_MASK       (~HANDLE_SEGMENT_CONTENT_MASK)

// table layout metrics
#define HANDLE_SIZE                     sizeof(_UNCHECKED_OBJECTREF)
#define HANDLE_HANDLES_PER_SEGMENT      ((HANDLE_SEGMENT_SIZE - HANDLE_HEADER_SIZE) / HANDLE_SIZE)
#define HANDLE_BLOCKS_PER_SEGMENT       (HANDLE_HANDLES_PER_SEGMENT / HANDLE_HANDLES_PER_BLOCK)
#define HANDLE_CLUMPS_PER_SEGMENT       (HANDLE_HANDLES_PER_SEGMENT / HANDLE_HANDLES_PER_CLUMP)
#define HANDLE_CLUMPS_PER_BLOCK         (HANDLE_HANDLES_PER_BLOCK / HANDLE_HANDLES_PER_CLUMP)
#define HANDLE_BYTES_PER_BLOCK          (HANDLE_HANDLES_PER_BLOCK * HANDLE_SIZE)
#define HANDLE_HANDLES_PER_MASK         (sizeof(DWORD32) * BITS_PER_BYTE)
#define HANDLE_MASKS_PER_SEGMENT        (HANDLE_HANDLES_PER_SEGMENT / HANDLE_HANDLES_PER_MASK)
#define HANDLE_MASKS_PER_BLOCK          (HANDLE_HANDLES_PER_BLOCK / HANDLE_HANDLES_PER_MASK)
#define HANDLE_CLUMPS_PER_MASK          (HANDLE_HANDLES_PER_MASK / HANDLE_HANDLES_PER_CLUMP)

// cache layout metrics
#define HANDLE_CACHE_TYPE_SIZE          128 // 128 == 63 handles per bank
#define HANDLES_PER_CACHE_BANK          ((HANDLE_CACHE_TYPE_SIZE / 2) - 1)

// cache policy defines
#define REBALANCE_TOLERANCE             (HANDLES_PER_CACHE_BANK / 3)
#define REBALANCE_LOWATER_MARK          (HANDLES_PER_CACHE_BANK - REBALANCE_TOLERANCE)
#define REBALANCE_HIWATER_MARK          (HANDLES_PER_CACHE_BANK + REBALANCE_TOLERANCE)

// bulk alloc policy defines
#define SMALL_ALLOC_COUNT               (HANDLES_PER_CACHE_BANK / 10)

// misc constants
#define MASK_FULL                       (0)
#define MASK_EMPTY                      (0xFFFFFFFF)
#define MASK_LOBYTE                     (0x000000FF)
#define TYPE_INVALID                    ((BYTE)0xFF)
#define BLOCK_INVALID                   ((BYTE)0xFF)

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * CORE TABLE LAYOUT STRUCTURES
 *
 ****************************************************************************/

/*
 * we need byte packing for the handle table layout to work
 */
#pragma pack(push)
#pragma pack(1)


/*
 * Table Segment Header
 *
 * Defines the layout for a segment's header data.
 */
struct _TableSegmentHeader
{
    /*
     * Write Barrier Generation Numbers
     *
     * Each slot holds four bytes.  Each byte corresponds to a clump of handles.
     * The value of the byte corresponds to the lowest possible generation that a
     * handle in that clump could point into.
     *
     * WARNING: Although this array is logically organized as a BYTE[], it is sometimes
     *  accessed as DWORD32[] when processing bytes in parallel.  Code which treats the
     *  array as an array of DWORD32s must handle big/little endian issues itself.
     */
    BYTE rgGeneration[HANDLE_BLOCKS_PER_SEGMENT * sizeof(DWORD32) / sizeof(BYTE)];

    /*
     * Block Allocation Chains
     *
     * Each slot indexes the next block in an allocation chain.
     */
    BYTE rgAllocation[HANDLE_BLOCKS_PER_SEGMENT];

    /*
     * Block Free Masks
     *
     * Masks - 1 bit for every handle in the segment.
     */
    DWORD32 rgFreeMask[HANDLE_MASKS_PER_SEGMENT];

    /*
     * Block Handle Types
     *
     * Each slot holds the handle type of the associated block.
     */
    BYTE rgBlockType[HANDLE_BLOCKS_PER_SEGMENT];

    /*
     * Block User Data Map
     *
     * Each slot holds the index of a user data block (if any) for the associated block.
     */
    BYTE rgUserData[HANDLE_BLOCKS_PER_SEGMENT];

    /*
     * Block Lock Count
     *
     * Each slot holds a lock count for its associated block.
     * Locked blocks are not freed, even when empty.
     */
    BYTE rgLocks[HANDLE_BLOCKS_PER_SEGMENT];

    /*
     * Allocation Chain Tails
     *
     * Each slot holds the tail block index for an allocation chain.
     */
    BYTE rgTail[HANDLE_MAX_INTERNAL_TYPES];

    /*
     * Allocation Chain Hints
     *
     * Each slot holds a hint block index for an allocation chain.
     */
    BYTE rgHint[HANDLE_MAX_INTERNAL_TYPES];

    /*
     * Free Count
     *
     * Each slot holds the number of free handles in an allocation chain.
     */
    UINT rgFreeCount[HANDLE_MAX_INTERNAL_TYPES];

    /*
     * Next Segment
     *
     * Points to the next segment in the chain (if we ran out of space in this one).
     */
    struct TableSegment *pNextSegment;

    /*
     * Handle Table
     *
     * Points to owning handle table for this table segment.
     */
    struct HandleTable *pHandleTable;

    /*
     * Flags
     */
    BYTE fResortChains      : 1;    // allocation chains need sorting
    BYTE fNeedsScavenging   : 1;    // free blocks need scavenging
    BYTE _fUnused           : 6;    // unused

    /*
     * Free List Head
     *
     * Index of the first free block in the segment.
     */
    BYTE bFreeList;

    /*
     * Empty Line
     *
     * Index of the first KNOWN block of the last group of unused blocks in the segment.
     */
    BYTE bEmptyLine;

    /*
     * Commit Line
     *
     * Index of the first uncommited block in the segment.
     */
    BYTE bCommitLine;

    /*
     * Decommit Line
     *
     * Index of the first block in the highest committed page of the segment.
     */
    BYTE bDecommitLine;

    /*
     * Sequence
     *
     * Indicates the segment sequence number.
     */
    BYTE bSequence;
};


/*
 * Table Segment
 *
 * Defines the layout for a handle table segment.
 */
struct TableSegment : public _TableSegmentHeader
{
    /*
     * Filler
     */
    BYTE rgUnused[HANDLE_HEADER_SIZE - sizeof(_TableSegmentHeader)];

    /*
     * Handles
     */
    _UNCHECKED_OBJECTREF rgValue[HANDLE_HANDLES_PER_SEGMENT];
};


/*
 * Handle Type Cache
 *
 * Defines the layout of a per-type handle cache.
 */
struct HandleTypeCache
{
    /*
     * reserve bank
     */
    OBJECTHANDLE rgReserveBank[HANDLES_PER_CACHE_BANK];

    /*
     * index of next available handle slot in the reserve bank
     */
    LONG lReserveIndex;

    /*---------------------------------------------------------------------------------
     * N.B. this structure is split up this way so that when HANDLES_PER_CACHE_BANK is
     * large enough, lReserveIndex and lFreeIndex will reside in different cache lines
     *--------------------------------------------------------------------------------*/

    /*
     * free bank
     */
    OBJECTHANDLE rgFreeBank[HANDLES_PER_CACHE_BANK];

    /*
     * index of next empty slot in the free bank
     */
    LONG lFreeIndex;
};


/*
 * restore default packing
 */
#pragma pack(pop)

/*---------------------------------------------------------------------------*/



/****************************************************************************
 *
 * SCANNING PROTOTYPES
 *
 ****************************************************************************/

/*
 * ScanCallbackInfo
 *
 * Carries parameters for per-segment and per-block scanning callbacks.
 *
 */
struct ScanCallbackInfo
{
    TableSegment  *pCurrentSegment; // segment we are presently scanning, if any
    UINT           uFlags;          // HNDGCF_* flags
    BOOL           fEnumUserData;   // whether user data is being enumerated as well
    HANDLESCANPROC pfnScan;         // per-handle scan callback
    LPARAM         param1;          // callback param 1
    LPARAM         param2;          // callback param 2
    DWORD32        dwAgeMask;       // generation mask for ephemeral GCs

#ifdef _DEBUG
    UINT DEBUG_BlocksScanned;
    UINT DEBUG_BlocksScannedNonTrivially;
    UINT DEBUG_HandleSlotsScanned;
    UINT DEBUG_HandlesActuallyScanned;
#endif
};


/*
 * BLOCKSCANPROC
 *
 * Prototype for callbacks that implement per-block scanning logic.
 *
 */
typedef void (CALLBACK *BLOCKSCANPROC)(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo);


/*
 * SEGMENTITERATOR
 *
 * Prototype for callbacks that implement per-segment scanning logic.
 *
 */
typedef TableSegment * (CALLBACK *SEGMENTITERATOR)(HandleTable *pTable, TableSegment *pPrevSegment);


/*
 * TABLESCANPROC
 *
 * Prototype for TableScanHandles and xxxTableScanHandlesAsync.
 *
 */
typedef void (CALLBACK *TABLESCANPROC)(HandleTable *pTable,
                                       const UINT *puType, UINT uTypeCount,
                                       SEGMENTITERATOR pfnSegmentIterator,
                                       BLOCKSCANPROC pfnBlockHandler,
                                       ScanCallbackInfo *pInfo);

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * ADDITIONAL TABLE STRUCTURES
 *
 ****************************************************************************/

/*
 * AsyncScanInfo
 *
 * Tracks the state of an async scan for a handle table.
 *
 */
struct AsyncScanInfo
{
    /*
     * Underlying Callback Info
     *
     * Specifies callback info for the underlying block handler.
     */
    struct ScanCallbackInfo *pCallbackInfo;

    /*
     * Underlying Segment Iterator
     *
     * Specifies the segment iterator to be used during async scanning.
     */
    SEGMENTITERATOR   pfnSegmentIterator;

    /*
     * Underlying Block Handler
     *
     * Specifies the block handler to be used during async scanning.
     */
    BLOCKSCANPROC     pfnBlockHandler;

    /*
     * Scan Queue
     *
     * Specifies the nodes to be processed asynchronously.
     */
    struct ScanQNode *pScanQueue;

    /*
     * Queue Tail
     *
     * Specifies the tail node in the queue, or NULL if the queue is empty.
     */
    struct ScanQNode *pQueueTail;
};


/*
 * Handle Table
 *
 * Defines the layout of a handle table object.
 */
#pragma warning(push)
#pragma warning(disable : 4200 )  // zero-sized array
struct HandleTable
{
    /*
     * flags describing handle attributes
     *
     * N.B. this is at offset 0 due to frequent access by cache free codepath
     */
    UINT rgTypeFlags[HANDLE_MAX_INTERNAL_TYPES];

    /*
     * memory for lock for this table
     */
    BYTE _LockInstance[sizeof(Crst)];                       // interlocked ops used here

    /*
     * lock for this table
     */
    Crst *pLock;

    /*
     * number of types this table supports
     */
    UINT uTypeCount;

    /*
     * head of segment list for this table
     */
    TableSegment *pSegmentList;

    /*
     * information on current async scan (if any)
     */
    AsyncScanInfo *pAsyncScanInfo;

    /*
     * per-table user info
     */
    UINT uTableIndex;

    /*
     * per-table AppDomain info
     */
    UINT uADIndex;

    /*
     * one-level per-type 'quick' handle cache
     */
    OBJECTHANDLE rgQuickCache[HANDLE_MAX_INTERNAL_TYPES];   // interlocked ops used here

    /*
     * debug-only statistics
     */
#ifdef _DEBUG
    int     _DEBUG_iMaxGen;
    __int64 _DEBUG_TotalBlocksScanned            [MAXSTATGEN];
    __int64 _DEBUG_TotalBlocksScannedNonTrivially[MAXSTATGEN];
    __int64 _DEBUG_TotalHandleSlotsScanned       [MAXSTATGEN];
    __int64 _DEBUG_TotalHandlesActuallyScanned   [MAXSTATGEN];
#endif

    /*
     * primary per-type handle cache
     */
    HandleTypeCache rgMainCache[0];                         // interlocked ops used here
};
#pragma warning(pop)

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * HELPERS
 *
 ****************************************************************************/

/*
 * A 32/64 comparison callback
 *
 * @TODO: move/merge into common util file
 *
 */
typedef int (*PFNCOMPARE)(UINT_PTR p, UINT_PTR q);


/*
 * A 32/64 neutral quicksort
 *
 * @TODO: move/merge into common util file
 *
 */
void QuickSort(UINT_PTR *pData, int left, int right, PFNCOMPARE pfnCompare);


/*
 * CompareHandlesByFreeOrder
 *
 * Returns:
 *  <0 - handle P should be freed before handle Q
 *  =0 - handles are eqivalent for free order purposes
 *  >0 - handle Q should be freed before handle P
 *
 */
int CompareHandlesByFreeOrder(UINT_PTR p, UINT_PTR q);

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * CORE TABLE MANAGEMENT
 *
 ****************************************************************************/

/*
 * TypeHasUserData
 *
 * Determines whether a given handle type has user data.
 *
 */
__inline BOOL TypeHasUserData(HandleTable *pTable, UINT uType)
{
    // sanity
    _ASSERTE(uType < HANDLE_MAX_INTERNAL_TYPES);

    // consult the type flags
    return (pTable->rgTypeFlags[uType] & HNDF_EXTRAINFO);
}


/*
 * TableCanFreeSegmentNow
 *
 * Determines if it is OK to free the specified segment at this time.
 *
 */
BOOL TableCanFreeSegmentNow(HandleTable *pTable, TableSegment *pSegment);


/*
 * BlockIsLocked
 *
 * Determines if the lock count for the specified block is currently non-zero.
 *
 */
__inline BOOL BlockIsLocked(TableSegment *pSegment, UINT uBlock)
{
    // sanity
    _ASSERTE(uBlock < HANDLE_BLOCKS_PER_SEGMENT);

    // fetch the lock count and compare it to zero
    return (pSegment->rgLocks[uBlock] != 0);
}


/*
 * BlockLock
 *
 * Increases the lock count for a block.
 *
 */
__inline void BlockLock(TableSegment *pSegment, UINT uBlock)
{
    // fetch the old lock count
    BYTE bLocks = pSegment->rgLocks[uBlock];

    // assert if we are about to trash the count
    _ASSERTE(bLocks < 0xFF);

    // store the incremented lock count
    pSegment->rgLocks[uBlock] = bLocks + 1;
}


/*
 * BlockUnlock
 *
 * Decreases the lock count for a block.
 *
 */
__inline void BlockUnlock(TableSegment *pSegment, UINT uBlock)
{
    // fetch the old lock count
    BYTE bLocks = pSegment->rgLocks[uBlock];

    // assert if we are about to trash the count
    _ASSERTE(bLocks > 0);

    // store the decremented lock count
    pSegment->rgLocks[uBlock] = bLocks - 1;
}


/*
 * BlockFetchUserDataPointer
 *
 * Gets the user data pointer for the first handle in a block.
 *
 */
LPARAM *BlockFetchUserDataPointer(TableSegment *pSegment, UINT uBlock, BOOL fAssertOnError);


/*
 * HandleValidateAndFetchUserDataPointer
 *
 * Gets the user data pointer for a handle.
 * ASSERTs and returns NULL if handle is not of the expected type.
 *
 */
LPARAM *HandleValidateAndFetchUserDataPointer(OBJECTHANDLE handle, UINT uTypeExpected);


/*
 * HandleQuickFetchUserDataPointer
 *
 * Gets the user data pointer for a handle.
 * Less validation is performed.
 *
 */
LPARAM *HandleQuickFetchUserDataPointer(OBJECTHANDLE handle);


/*
 * HandleQuickSetUserData
 *
 * Stores user data with a handle.
 * Less validation is performed.
 *
 */
void HandleQuickSetUserData(OBJECTHANDLE handle, LPARAM lUserData);


/*
 * HandleFetchType
 *
 * Computes the type index for a given handle.
 *
 */
UINT HandleFetchType(OBJECTHANDLE handle);


/*
 * HandleFetchHandleTable
 *
 * Returns the containing handle table of a given handle.
 *
 */
HandleTable *HandleFetchHandleTable(OBJECTHANDLE handle);


/*
 * SegmentAlloc
 *
 * Allocates a new segment.
 *
 */
TableSegment *SegmentAlloc(HandleTable *pTable);


/*
 * SegmentFree
 *
 * Frees the specified segment.
 *
 */
void SegmentFree(TableSegment *pSegment);


/*
 * SegmentRemoveFreeBlocks
 *
 * Removes a block from a block list in a segment.  The block is returned to
 * the segment's free list.
 *
 */
void SegmentRemoveFreeBlocks(TableSegment *pSegment, UINT uType);


/*
 * SegmentResortChains
 *
 * Sorts the block chains for optimal scanning order.
 * Sorts the free list to combat fragmentation.
 *
 */
void SegmentResortChains(TableSegment *pSegment);


/*
 * SegmentTrimExcessPages
 *
 * Checks to see if any pages can be decommitted from the segment
 *
 */
void SegmentTrimExcessPages(TableSegment *pSegment);


/*
 * TableAllocBulkHandles
 *
 * Attempts to allocate the requested number of handes of the specified type.
 *
 * Returns the number of handles that were actually allocated.  This is always
 * the same as the number of handles requested except in out-of-memory conditions,
 * in which case it is the number of handles that were successfully allocated.
 *
 */
UINT TableAllocBulkHandles(HandleTable *pTable, UINT uType, OBJECTHANDLE *pHandleBase, UINT uCount);


/*
 * TableFreeBulkPreparedHandles
 *
 * Frees an array of handles of the specified type.
 *
 * This routine is optimized for a sorted array of handles but will accept any order.
 *
 */
void TableFreeBulkPreparedHandles(HandleTable *pTable, UINT uType, OBJECTHANDLE *pHandleBase, UINT uCount);


/*
 * TableFreeBulkUnpreparedHandles
 *
 * Frees an array of handles of the specified type by preparing them and calling TableFreeBulkPreparedHandles.
 *
 */
void TableFreeBulkUnpreparedHandles(HandleTable *pTable, UINT uType, const OBJECTHANDLE *pHandles, UINT uCount);

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * HANDLE CACHE
 *
 ****************************************************************************/

/*
 * TableAllocSingleHandleFromCache
 *
 * Gets a single handle of the specified type from the handle table by
 * trying to fetch it from the reserve cache for that handle type.  If the
 * reserve cache is empty, this routine calls TableCacheMissOnAlloc.
 *
 */
OBJECTHANDLE TableAllocSingleHandleFromCache(HandleTable *pTable, UINT uType);


/*
 * TableFreeSingleHandleToCache
 *
 * Returns a single handle of the specified type to the handle table
 * by trying to store it in the free cache for that handle type.  If the
 * free cache is full, this routine calls TableCacheMissOnFree.
 *
 */
void TableFreeSingleHandleToCache(HandleTable *pTable, UINT uType, OBJECTHANDLE handle);


/*
 * TableAllocHandlesFromCache
 *
 * Allocates multiple handles of the specified type by repeatedly
 * calling TableAllocSingleHandleFromCache.
 *
 */
UINT TableAllocHandlesFromCache(HandleTable *pTable, UINT uType, OBJECTHANDLE *pHandleBase, UINT uCount);


/*
 * TableFreeHandlesToCache
 *
 * Frees multiple handles of the specified type by repeatedly
 * calling TableFreeSingleHandleToCache.
 *
 */
void TableFreeHandlesToCache(HandleTable *pTable, UINT uType, const OBJECTHANDLE *pHandleBase, UINT uCount);

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * TABLE SCANNING
 *
 ****************************************************************************/

/*
 * TableScanHandles
 *
 * Implements the core handle scanning loop for a table.
 *
 */
void CALLBACK TableScanHandles(HandleTable *pTable,
                               const UINT *puType,
                               UINT uTypeCount,
                               SEGMENTITERATOR pfnSegmentIterator,
                               BLOCKSCANPROC pfnBlockHandler,
                               ScanCallbackInfo *pInfo);


/*
 * xxxTableScanHandlesAsync
 *
 * Implements asynchronous handle scanning for a table.
 *
 */
void CALLBACK xxxTableScanHandlesAsync(HandleTable *pTable,
                                       const UINT *puType,
                                       UINT uTypeCount,
                                       SEGMENTITERATOR pfnSegmentIterator,
                                       BLOCKSCANPROC pfnBlockHandler,
                                       ScanCallbackInfo *pInfo);


/*
 * TypesRequireUserDataScanning
 *
 * Determines whether the set of types listed should get user data during scans
 *
 * if ALL types passed have user data then this function will enable user data support
 * otherwise it will disable user data support
 *
 * IN OTHER WORDS, SCANNING WITH A MIX OF USER-DATA AND NON-USER-DATA TYPES IS NOT SUPPORTED
 *
 */
BOOL TypesRequireUserDataScanning(HandleTable *pTable, const UINT *types, UINT typeCount);


/*
 * BuildAgeMask
 *
 * Builds an age mask to be used when examining/updating the write barrier.
 *
 */
DWORD32 BuildAgeMask(UINT uGen);


/*
 * QuickSegmentIterator
 *
 * Returns the next segment to be scanned in a scanning loop.
 *
 */
TableSegment * CALLBACK QuickSegmentIterator(HandleTable *pTable, TableSegment *pPrevSegment);


/*
 * StandardSegmentIterator
 *
 * Returns the next segment to be scanned in a scanning loop.
 *
 * This iterator performs some maintenance on the segments,
 * primarily making sure the block chains are sorted so that
 * g0 scans are more likely to operate on contiguous blocks.
 *
 */
TableSegment * CALLBACK StandardSegmentIterator(HandleTable *pTable, TableSegment *pPrevSegment);


/*
 * FullSegmentIterator
 *
 * Returns the next segment to be scanned in a scanning loop.
 *
 * This iterator performs full maintenance on the segments,
 * including freeing those it notices are empty along the way.
 *
 */
TableSegment * CALLBACK FullSegmentIterator(HandleTable *pTable, TableSegment *pPrevSegment);


/*
 * BlockScanBlocksWithoutUserData
 *
 * Calls the specified callback for each handle, optionally aging the corresponding generation clumps.
 * NEVER propagates per-handle user data to the callback.
 *
 */
void CALLBACK BlockScanBlocksWithoutUserData(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo);


/*
 * BlockScanBlocksWithUserData
 *
 * Calls the specified callback for each handle, optionally aging the corresponding generation clumps.
 * ALWAYS propagates per-handle user data to the callback.
 *
 */
void CALLBACK BlockScanBlocksWithUserData(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo);


/*
 * BlockScanBlocksEphemeral
 *
 * Calls the specified callback for each handle from the specified generation.
 * Propagates per-handle user data to the callback if present.
 *
 */
void CALLBACK BlockScanBlocksEphemeral(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo);


/*
 * BlockAgeBlocks
 *
 * Ages all clumps in a range of consecutive blocks.
 *
 */
void CALLBACK BlockAgeBlocks(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo);


/*
 * BlockAgeBlocksEphemeral
 *
 * Ages all clumps within the specified generation.
 *
 */
void CALLBACK BlockAgeBlocksEphemeral(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo);


/*
 * BlockResetAgeMapForBlocks
 *
 * Clears the age maps for a range of blocks.
 *
 */
void CALLBACK BlockResetAgeMapForBlocks(TableSegment *pSegment, UINT uBlock, UINT uCount, ScanCallbackInfo *pInfo);

/*--------------------------------------------------------------------------*/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\handletablecache.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*
 * Generational GC handle manager.  Handle Caching Routines.
 *
 * Implementation of handle table allocation cache.
 *
 * francish
 */

#include "common.h"

#include "HandleTablePriv.h"



/****************************************************************************
 *
 * RANDOM HELPERS
 *
 ****************************************************************************/

/*
 * SpinUntil
 *
 * Spins on a variable until its state matches a desired state.
 *
 * This routine will assert if it spins for a very long time.
 *
 */
void SpinUntil(void *pCond, BOOL fNonZero)
{
    // if we have to sleep then we will keep track of a sleep period
    DWORD dwThisSleepPeriod = 1;    // first just give up our timeslice
    DWORD dwNextSleepPeriod = 10;   // next try a real delay

#ifdef _DEBUG
    DWORD dwTotalSlept = 0;
    DWORD dwNextComplain = 1000;
#endif //_DEBUG

    // on MP machines, allow ourselves some spin time before sleeping
    UINT uNonSleepSpins = 8 * (g_SystemInfo.dwNumberOfProcessors - 1);

    // spin until the specificed condition is met
    while ((*(UINT_PTR *)pCond != 0) != (fNonZero != 0))
    {
        // have we exhausted the non-sleep spin count?
        if (!uNonSleepSpins)
        {
#ifdef _DEBUG
            // yes, missed again - before sleeping, check our current sleep time
            if (dwTotalSlept >= dwNextComplain)
            {
                //
                // THIS SHOULD NOT NORMALLY HAPPEN
                //
                // The only time this assert can be ignored is if you have
                // another thread intentionally suspended in a way that either
                // directly or indirectly leaves a thread suspended in the
                // handle table while the current thread (this assert) is
                // running normally.
                //
                // Otherwise, this assert should be investigated as a bug.
                //
                _ASSERTE(FALSE);

                // slow down the assert rate so people can investigate
                dwNextComplain = 3 * dwNextComplain;
            }

            // now update our total sleep time
            dwTotalSlept += dwThisSleepPeriod;
#endif //_DEBUG

            // sleep for a little while
            Sleep(dwThisSleepPeriod);

            // now update our sleep period
            dwThisSleepPeriod = dwNextSleepPeriod;

            // now increase the next sleep period if it is still small
            if (dwNextSleepPeriod < 1000)
                dwNextSleepPeriod += 10;
        }
        else
        {
            // nope - just spin again
			pause();			// indicate to the processor that we are spining 
            uNonSleepSpins--;
        }
    }
}


/*
 * ReadAndZeroCacheHandles
 *
 * Reads a set of handles from a bank in the handle cache, zeroing them as they are taken.
 *
 * This routine will assert if a requested handle is missing.
 *
 */
OBJECTHANDLE *ReadAndZeroCacheHandles(OBJECTHANDLE *pDst, OBJECTHANDLE *pSrc, UINT uCount)
{
    // set up to loop
    OBJECTHANDLE *pLast = pDst + uCount;

    // loop until we've copied all of them
    while (pDst < pLast)
    {
        // this version assumes we have handles to read
        _ASSERTE(*pSrc);

        // copy the handle and zero it from the source
        *pDst = *pSrc;
        *pSrc = 0;

        // set up for another handle
        pDst++;
        pSrc++;
    }

    // return the next unfilled slot after what we filled in
    return pLast;
}


/*
 * SyncReadAndZeroCacheHandles
 *
 * Reads a set of handles from a bank in the handle cache, zeroing them as they are taken.
 *
 * This routine will spin until all requested handles are obtained.
 *
 */
OBJECTHANDLE *SyncReadAndZeroCacheHandles(OBJECTHANDLE *pDst, OBJECTHANDLE *pSrc, UINT uCount)
{
    // set up to loop
    // we loop backwards since that is the order handles are added to the bank
    // this is designed to reduce the chance that we will have to spin on a handle
    OBJECTHANDLE *pBase = pDst;
    pSrc += uCount;
    pDst += uCount;

    // remember the end of the array
    OBJECTHANDLE *pLast = pDst;

    // loop until we've copied all of them
    while (pDst > pBase)
    {
        // advance to the next slot
        pDst--;
        pSrc--;

        // this version spins if there is no handle to read
        if (!*pSrc)
            SpinUntil(pSrc, TRUE);

        // copy the handle and zero it from the source
        *pDst = *pSrc;
        *pSrc = 0;
    }

    // return the next unfilled slot after what we filled in
    return pLast;
}


/*
 * WriteCacheHandles
 *
 * Writes a set of handles to a bank in the handle cache.
 *
 * This routine will assert if it is about to clobber an existing handle.
 *
 */
void WriteCacheHandles(OBJECTHANDLE *pDst, OBJECTHANDLE *pSrc, UINT uCount)
{
    // set up to loop
    OBJECTHANDLE *pLimit = pSrc + uCount;

    // loop until we've copied all of them
    while (pSrc < pLimit)
    {
        // this version assumes we have space to store the handles
        _ASSERTE(!*pDst);

        // copy the handle
        *pDst = *pSrc;

        // set up for another handle
        pDst++;
        pSrc++;
    }
}


/*
 * SyncWriteCacheHandles
 *
 * Writes a set of handles to a bank in the handle cache.
 *
 * This routine will spin until lingering handles in the cache bank are gone.
 *
 */
void SyncWriteCacheHandles(OBJECTHANDLE *pDst, OBJECTHANDLE *pSrc, UINT uCount)
{
    // set up to loop
    // we loop backwards since that is the order handles are removed from the bank
    // this is designed to reduce the chance that we will have to spin on a handle
    OBJECTHANDLE *pBase = pSrc;
    pSrc += uCount;
    pDst += uCount;

    // loop until we've copied all of them
    while (pSrc > pBase)
    {
        // set up for another handle
        pDst--;
        pSrc--;

        // this version spins if there is no handle to read
        if (*pDst)
            SpinUntil(pDst, FALSE);

        // copy the handle
        *pDst = *pSrc;
    }
}


/*
 * SyncTransferCacheHandles
 *
 * Transfers a set of handles from one bank of the handle cache to another,
 * zeroing the source bank as the handles are removed.
 *
 * The routine will spin until all requested handles can be transferred.
 *
 * This routine is equivalent to SyncReadAndZeroCacheHandles + SyncWriteCacheHandles
 *
 */
void SyncTransferCacheHandles(OBJECTHANDLE *pDst, OBJECTHANDLE *pSrc, UINT uCount)
{
    // set up to loop
    // we loop backwards since that is the order handles are added to the bank
    // this is designed to reduce the chance that we will have to spin on a handle
    OBJECTHANDLE *pBase = pDst;
    pSrc += uCount;
    pDst += uCount;

    // loop until we've copied all of them
    while (pDst > pBase)
    {
        // advance to the next slot
        pDst--;
        pSrc--;

        // this version spins if there is no handle to read or no place to write it
        if (*pDst || !*pSrc)
        {
            SpinUntil(pSrc, TRUE);
            SpinUntil(pDst, FALSE);
        }

        // copy the handle and zero it from the source
        *pDst = *pSrc;
        *pSrc = 0;
    }
}

/*--------------------------------------------------------------------------*/



/****************************************************************************
 *
 * HANDLE CACHE
 *
 ****************************************************************************/

/*
 * TableFullRebalanceCache
 *
 * Rebalances a handle cache by transferring handles from the cache's
 * free bank to its reserve bank.  If the free bank does not provide
 * enough handles to replenish the reserve bank, handles are allocated
 * in bulk from the main handle table.  If too many handles remain in
 * the free bank, the extra handles are returned in bulk to the main
 * handle table.
 *
 * This routine attempts to reduce fragmentation in the main handle
 * table by sorting the handles according to table order, preferring to
 * refill the reserve bank with lower handles while freeing higher ones.
 * The sorting also allows the free routine to operate more efficiently,
 * as it can optimize the case where handles near each other are freed.
 *
 */
void TableFullRebalanceCache(HandleTable *pTable,
                             HandleTypeCache *pCache,
                             UINT uType,
                             LONG lMinReserveIndex,
                             LONG lMinFreeIndex,
                             OBJECTHANDLE *pExtraOutHandle,
                             OBJECTHANDLE extraInHandle)
{
    // we need a temporary space to sort our free handles in
    OBJECTHANDLE rgHandles[HANDLE_CACHE_TYPE_SIZE];

    // set up a base handle pointer to keep track of where we are
    OBJECTHANDLE *pHandleBase = rgHandles;

    // do we have a spare incoming handle?
    if (extraInHandle)
    {
        // remember the extra handle now
        *pHandleBase = extraInHandle;
        pHandleBase++;
    }

    // if there are handles in the reserve bank then gather them up
    // (we don't need to wait on these since they are only put there by this
    //  function inside our own lock)
    if (lMinReserveIndex > 0)
        pHandleBase = ReadAndZeroCacheHandles(pHandleBase, pCache->rgReserveBank, (UINT)lMinReserveIndex);
    else
        lMinReserveIndex = 0;

    // if there are handles in the free bank then gather them up
    if (lMinFreeIndex < HANDLES_PER_CACHE_BANK)
    {
        // this may have underflowed
        if (lMinFreeIndex < 0)
            lMinFreeIndex = 0;

        // here we need to wait for all pending freed handles to be written by other threads
        pHandleBase = SyncReadAndZeroCacheHandles(pHandleBase,
                                                  pCache->rgFreeBank + lMinFreeIndex,
                                                  HANDLES_PER_CACHE_BANK - (UINT)lMinFreeIndex);
    }

    // compute the number of handles we have
    UINT uHandleCount = pHandleBase - rgHandles;

    // do we have enough handles for a balanced cache?
    if (uHandleCount < REBALANCE_LOWATER_MARK)
    {
        // nope - allocate some more
        UINT uAlloc = HANDLES_PER_CACHE_BANK - uHandleCount;

        // if we have an extra outgoing handle then plan for that too
        if (pExtraOutHandle)
            uAlloc++;

        // allocate the new handles - we intentionally don't check for success here
        uHandleCount += TableAllocBulkHandles(pTable, uType, pHandleBase, uAlloc);
    }

    // reset the base handle pointer
    pHandleBase = rgHandles;

    // by default the whole free bank is available
    lMinFreeIndex = HANDLES_PER_CACHE_BANK;

    // if we have handles left over then we need to do some more work
    if (uHandleCount)
    {
        // do we have too many handles for a balanced cache?
        if (uHandleCount > REBALANCE_HIWATER_MARK)
        {
            //
            // sort the array by reverse handle order - this does two things:
            //  (1) combats handle fragmentation by preferring low-address handles to high ones
            //  (2) allows the free routine to run much more efficiently over the ones we free
            //
            QuickSort((UINT_PTR *)pHandleBase, 0, uHandleCount - 1, CompareHandlesByFreeOrder);

            // yup, we need to free some - calculate how many
            UINT uFree = uHandleCount - HANDLES_PER_CACHE_BANK;

            // free the handles - they are already 'prepared' (eg zeroed and sorted)
            TableFreeBulkPreparedHandles(pTable, uType, pHandleBase, uFree);

            // update our array base and length
            uHandleCount -= uFree;
            pHandleBase += uFree;
        }

        // if we have an extra outgoing handle then fill it now
        if (pExtraOutHandle)
        {
            // account for the handle we're giving away
            uHandleCount--;

            // now give it away
            *pExtraOutHandle = pHandleBase[uHandleCount];
        }

        // if we have more than a reserve bank of handles then put some in the free bank
        if (uHandleCount > HANDLES_PER_CACHE_BANK)
        {
            // compute the number of extra handles we need to save away
            UINT uStore = uHandleCount - HANDLES_PER_CACHE_BANK;

            // compute the index to start writing the handles to
            lMinFreeIndex = HANDLES_PER_CACHE_BANK - uStore;

            // store the handles
            // (we don't need to wait on these since we already waited while reading them)
            WriteCacheHandles(pCache->rgFreeBank + lMinFreeIndex, pHandleBase, uStore);

            // update our array base and length
            uHandleCount -= uStore;
            pHandleBase += uStore;
        }
    }

    // update the write index for the free bank
    // NOTE: we use an interlocked exchange here to guarantee relative store order on MP
    // AFTER THIS POINT THE FREE BANK IS LIVE AND COULD RECEIVE NEW HANDLES
    FastInterlockExchange(&pCache->lFreeIndex, lMinFreeIndex);

    // now if we have any handles left, store them in the reserve bank
    if (uHandleCount)
    {
        // store the handles
        // (here we need to wait for all pending allocated handles to be taken
        //  before we set up new ones in their places)
        SyncWriteCacheHandles(pCache->rgReserveBank, pHandleBase, uHandleCount);
    }

    // compute the index to start serving handles from
    lMinReserveIndex = (LONG)uHandleCount;

    // update the read index for the reserve bank
    // NOTE: we use an interlocked exchange here to guarantee relative store order on MP
    // AT THIS POINT THE RESERVE BANK IS LIVE AND HANDLES COULD BE ALLOCATED FROM IT
    FastInterlockExchange(&pCache->lReserveIndex, lMinReserveIndex);
}


/*
 * TableQuickRebalanceCache
 *
 * Rebalances a handle cache by transferring handles from the cache's free bank
 * to its reserve bank.  If the free bank does not provide enough handles to
 * replenish the reserve bank or too many handles remain in the free bank, the
 * routine just punts and calls TableFullRebalanceCache.
 *
 */
void TableQuickRebalanceCache(HandleTable *pTable,
                              HandleTypeCache *pCache,
                              UINT uType,
                              LONG lMinReserveIndex,
                              LONG lMinFreeIndex,
                              OBJECTHANDLE *pExtraOutHandle,
                              OBJECTHANDLE extraInHandle)
{
    // clamp the min free index to be non-negative
    if (lMinFreeIndex < 0)
        lMinFreeIndex = 0;

    // clamp the min reserve index to be non-negative
    if (lMinReserveIndex < 0)
        lMinReserveIndex = 0;

    // compute the number of slots in the free bank taken by handles
    UINT uFreeAvail = HANDLES_PER_CACHE_BANK - (UINT)lMinFreeIndex;

    // compute the number of handles we have to fiddle with
    UINT uHandleCount = (UINT)lMinReserveIndex + uFreeAvail + (extraInHandle != 0);

    // can we rebalance these handles in place?
    if ((uHandleCount < REBALANCE_LOWATER_MARK) ||
        (uHandleCount > REBALANCE_HIWATER_MARK))
    {
        // nope - perform a full rebalance of the handle cache
        TableFullRebalanceCache(pTable, pCache, uType, lMinReserveIndex, lMinFreeIndex,
                                pExtraOutHandle, extraInHandle);

        // all done
        return;
    }

    // compute the number of empty slots in the reserve bank
    UINT uEmptyReserve = HANDLES_PER_CACHE_BANK - lMinReserveIndex;

    // we want to transfer as many handles as we can from the free bank
    UINT uTransfer = uFreeAvail;

    // but only as many as we have room to store in the reserve bank
    if (uTransfer > uEmptyReserve)
        uTransfer = uEmptyReserve;

    // transfer the handles
    SyncTransferCacheHandles(pCache->rgReserveBank + lMinReserveIndex,
                             pCache->rgFreeBank    + lMinFreeIndex,
                             uTransfer);

    // adjust the free and reserve indices to reflect the transfer
    lMinFreeIndex    += uTransfer;
    lMinReserveIndex += uTransfer;

    // do we have an extra incoming handle to store?
    if (extraInHandle)
    {
        //
        // HACKHACK: For code size reasons, we don't handle all cases here.
        // We assume an extra IN handle means a cache overflow during a free.
        //
        // After the rebalance above, the reserve bank should be full, and
        // there may be a few handles sitting in the free bank.  The HIWATER
        // check above guarantees that we have room to store the handle.
        //
        _ASSERTE(!pExtraOutHandle);

        // store the handle in the next available free bank slot
        pCache->rgFreeBank[--lMinFreeIndex] = extraInHandle;
    }
    else if (pExtraOutHandle)   // do we have an extra outgoing handle to satisfy?
    {
        //
        // HACKHACK: For code size reasons, we don't handle all cases here.
        // We assume an extra OUT handle means a cache underflow during an alloc.
        //
        // After the rebalance above, the free bank should be empty, and
        // the reserve bank may not be fully populated.  The LOWATER check above
        // guarantees that the reserve bank has at least one handle we can steal.
        //

        // take the handle from the reserve bank and update the reserve index
        *pExtraOutHandle = pCache->rgReserveBank[--lMinReserveIndex];

        // zero the cache slot we chose
        pCache->rgReserveBank[lMinReserveIndex] = NULL;
    }

    // update the write index for the free bank
    // NOTE: we use an interlocked exchange here to guarantee relative store order on MP
    // AFTER THIS POINT THE FREE BANK IS LIVE AND COULD RECEIVE NEW HANDLES
    FastInterlockExchange(&pCache->lFreeIndex, lMinFreeIndex);

    // update the read index for the reserve bank
    // NOTE: we use an interlocked exchange here to guarantee relative store order on MP
    // AT THIS POINT THE RESERVE BANK IS LIVE AND HANDLES COULD BE ALLOCATED FROM IT
    FastInterlockExchange(&pCache->lReserveIndex, lMinReserveIndex);
}


/*
 * TableCacheMissOnAlloc
 *
 * Gets a single handle of the specified type from the handle table,
 * making the assumption that the reserve cache for that type was
 * recently emptied.  This routine acquires the handle manager lock and
 * attempts to get a handle from the reserve cache again.  If this second
 * get operation also fails, the handle is allocated by means of a cache
 * rebalance.
 *
 */
OBJECTHANDLE TableCacheMissOnAlloc(HandleTable *pTable, HandleTypeCache *pCache, UINT uType)
{
    // assume we get no handle
    OBJECTHANDLE handle = NULL;

    // acquire the handle manager lock
    pTable->pLock->Enter();

    // try again to take a handle (somebody else may have rebalanced)
    LONG lReserveIndex = FastInterlockDecrement(&pCache->lReserveIndex);

    // are we still waiting for handles?
    if (lReserveIndex < 0)
    {
        // yup, suspend free list usage...
        LONG lFreeIndex = FastInterlockExchange(&pCache->lFreeIndex, 0L);

        // ...and rebalance the cache...
        TableQuickRebalanceCache(pTable, pCache, uType, lReserveIndex, lFreeIndex, &handle, NULL);
    }
    else
    {
        // somebody else rebalanced the cache for us - take the handle
        handle = pCache->rgReserveBank[lReserveIndex];

        // zero the handle slot
        pCache->rgReserveBank[lReserveIndex] = 0;
    }

    // release the handle manager lock
    pTable->pLock->Leave();

    // return the handle we got
    return handle;
}


/*
 * TableCacheMissOnFree
 *
 * Returns a single handle of the specified type to the handle table,
 * making the assumption that the free cache for that type was recently
 * filled.  This routine acquires the handle manager lock and attempts
 * to store the handle in the free cache again.  If this second store
 * operation also fails, the handle is freed by means of a cache
 * rebalance.
 *
 */
void TableCacheMissOnFree(HandleTable *pTable, HandleTypeCache *pCache, UINT uType, OBJECTHANDLE handle)
{
    // acquire the handle manager lock
    pTable->pLock->Enter();

    // try again to take a slot (somebody else may have rebalanced)
    LONG lFreeIndex = FastInterlockDecrement(&pCache->lFreeIndex);

    // are we still waiting for free slots?
    if (lFreeIndex < 0)
    {
        // yup, suspend reserve list usage...
        LONG lReserveIndex = FastInterlockExchange(&pCache->lReserveIndex, 0L);

        // ...and rebalance the cache...
        TableQuickRebalanceCache(pTable, pCache, uType, lReserveIndex, lFreeIndex, NULL, handle);
    }
    else
    {
        // somebody else rebalanced the cache for us - free the handle
        pCache->rgFreeBank[lFreeIndex] = handle;
    }

    // release the handle manager lock
    pTable->pLock->Leave();
}


/*
 * TableAllocSingleHandleFromCache
 *
 * Gets a single handle of the specified type from the handle table by
 * trying to fetch it from the reserve cache for that handle type.  If the
 * reserve cache is empty, this routine calls TableCacheMissOnAlloc.
 *
 */
OBJECTHANDLE TableAllocSingleHandleFromCache(HandleTable *pTable, UINT uType)
{
    // we use this in two places
    OBJECTHANDLE handle;

    // first try to get a handle from the quick cache
    if (pTable->rgQuickCache[uType])
    {
        // try to grab the handle we saw
        handle = (OBJECTHANDLE)InterlockedExchangePointer((PVOID*)(pTable->rgQuickCache + uType), (PVOID)NULL);

        // if it worked then we're done
        if (handle)
            return handle;
    }

    // ok, get the main handle cache for this type
    HandleTypeCache *pCache = pTable->rgMainCache + uType;

    // try to take a handle from the main cache
    LONG lReserveIndex = FastInterlockDecrement(&pCache->lReserveIndex);

    // did we underflow?
    if (lReserveIndex < 0)
    {
        // yep - the cache is out of handles
        return TableCacheMissOnAlloc(pTable, pCache, uType);
    }

    // get our handle
    handle = pCache->rgReserveBank[lReserveIndex];

    // zero the handle slot
    pCache->rgReserveBank[lReserveIndex] = 0;

    // sanity
    _ASSERTE(handle);

    // return our handle
    return handle;
}


/*
 * TableFreeSingleHandleToCache
 *
 * Returns a single handle of the specified type to the handle table
 * by trying to store it in the free cache for that handle type.  If the
 * free cache is full, this routine calls TableCacheMissOnFree.
 *
 */
void TableFreeSingleHandleToCache(HandleTable *pTable, UINT uType, OBJECTHANDLE handle)
{
    // zero the handle's object pointer
    *(_UNCHECKED_OBJECTREF *)handle = NULL;

    // if this handle type has user data then clear it - AFTER the referent is cleared!
    if (TypeHasUserData(pTable, uType))
        HandleQuickSetUserData(handle, 0L);

    // is there room in the quick cache?
    if (!pTable->rgQuickCache[uType])
    {
        // yup - try to stuff our handle in the slot we saw
        handle = (OBJECTHANDLE)InterlockedExchangePointer((PVOID*)(pTable->rgQuickCache + uType), (PVOID)handle);

        // if we didn't end up with another handle then we're done
        if (!handle)
            return;
    }

    // ok, get the main handle cache for this type
    HandleTypeCache *pCache = pTable->rgMainCache + uType;

    // try to take a free slot from the main cache
    LONG lFreeIndex = FastInterlockDecrement(&pCache->lFreeIndex);

    // did we underflow?
    if (lFreeIndex < 0)
    {
        // yep - we're out of free slots
        TableCacheMissOnFree(pTable, pCache, uType, handle);
        return;
    }

    // we got a slot - save the handle in the free bank
    pCache->rgFreeBank[lFreeIndex] = handle;
}


/*
 * TableAllocHandlesFromCache
 *
 * Allocates multiple handles of the specified type by repeatedly
 * calling TableAllocSingleHandleFromCache.
 *
 */
UINT TableAllocHandlesFromCache(HandleTable *pTable, UINT uType, OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // loop until we have satisfied all the handles we need to allocate
    UINT uSatisfied = 0;
    while (uSatisfied < uCount)
    {
        // get a handle from the cache
        OBJECTHANDLE handle = TableAllocSingleHandleFromCache(pTable, uType);

        // if we can't get any more then bail out
        if (!handle)
            break;

        // store the handle in the caller's array
        *pHandleBase = handle;

        // on to the next one
        uSatisfied++;
        pHandleBase++;
    }

    // return the number of handles we allocated
    return uSatisfied;
}


/*
 * TableFreeHandlesToCache
 *
 * Frees multiple handles of the specified type by repeatedly
 * calling TableFreeSingleHandleToCache.
 *
 */
void TableFreeHandlesToCache(HandleTable *pTable, UINT uType, const OBJECTHANDLE *pHandleBase, UINT uCount)
{
    // loop until we have freed all the handles
    while (uCount)
    {
        // get the next handle to free
        OBJECTHANDLE handle = *pHandleBase;

        // advance our state
        uCount--;
        pHandleBase++;

        // sanity
        _ASSERTE(handle);

        // return the handle to the cache
        TableFreeSingleHandleToCache(pTable, uType, handle);
    }
}

/*--------------------------------------------------------------------------*/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\hash.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*++

Module Name:

    synchash.cpp

--*/

#include "common.h"

#include "hash.h"

#include "excep.h"

#include "SyncClean.hpp"

#ifdef PROFILE
#include <iostream.h>
#endif

#if defined (_ALPHA_)
extern "C" void __MB(void);
#endif

static int memoryBarrier = 0;
// Memory Barrier
inline void MemoryBarrier()
{
    //@todo fix this
    // used by hash table lookup and insert methods
    // to make sure the key was fetched before the value
    // key and value fetch operations should not be reordered
    // also, load key should preceed, load value
    // use memory barrier for alpha 
    // and access volatile memory for X86
        #if defined (_X86_)
        
            *(volatile int *)&memoryBarrier = 1;

        #elif defined (_ALPHA_)
            // Memory barrier for Alpha.
            // __MB is an AXP specific compiler intrinsic.
            //
        __MB (); 
        #elif defined (_SH3_)
        #pragma message("SH3 BUGBUG -- define MemoryBarrier")
        #endif

}


void *PtrHashMap::operator new(size_t size, LoaderHeap *pHeap)
{
    return pHeap->AllocMem(size);
}

void PtrHashMap::operator delete(void *p)
{
}


//-----------------------------------------------------------------
// Bucket methods

BOOL Bucket::InsertValue(const UPTR key, const UPTR value)
{
    _ASSERTE(key != EMPTY);
    _ASSERTE(key != DELETED);

    if (!HasFreeSlots())
        return false; //no free slots

    // might have a free slot
    for (UPTR i = 0; i < 4; i++)
    {
        //@NOTE we can't reuse DELETED slots 
        if (m_rgKeys[i] == EMPTY) 
        {   
            SetValue (value, i);
            
            // On multiprocessors we should make sure that
            // the value is propagated before we proceed.  
            // inline memory barrier call, refer to 
            // function description at the beginning of this
            MemoryBarrier();

            m_rgKeys[i] = key;
            return true;
        }
    }       // for i= 0; i < 4; loop

    SetCollision(); // otherwise set the collision bit
    return false;
}

//---------------------------------------------------------------------
//  Array of primes, used by hash table to choose the number of buckets
//

const DWORD g_rgPrimes[70]={
11,17,23,29,37,47,59,71,89,107,131,163,197,239,293,353,431,521,631,761,919,
1103,1327,1597,1931,2333,2801,3371,4049,4861,5839,7013,8419,10103,12143,14591,
17519,21023,25229,30293,36353,43627,52361,62851,75431,90523, 108631, 130363, 
156437, 187751, 225307, 270371, 324449, 389357, 467237, 560689, 672827, 807403,
968897, 1162687, 1395263, 1674319, 2009191, 2411033, 2893249, 3471899, 4166287, 
4999559, 5999471, 7199369 
};

//---------------------------------------------------------------------
//  inline size_t& HashMap::Size(Bucket* rgBuckets)
//  get the number of buckets
inline
size_t& HashMap::Size(Bucket* rgBuckets)
{
    return ((size_t*)rgBuckets)[0];
}

//---------------------------------------------------------------------
//  inline Bucket* HashMap::Buckets()
//  get the pointer to the bucket array
inline 
Bucket* HashMap::Buckets()
{
    _ASSERTE (!g_fEEStarted || !m_fAsyncMode || GetThread() == NULL || GetThread()->PreemptiveGCDisabled());
    return m_rgBuckets + 1;
}

//---------------------------------------------------------------------
//  HashMap::HashMap()
//  constructor, initialize all values
//
HashMap::HashMap()
{
    m_rgBuckets = NULL;
    m_pCompare = NULL;  // comparsion object
    m_cbInserts = 0;        // track inserts
    m_cbDeletes = 0;        // track deletes
    m_cbPrevSlotsInUse = 0; // track valid slots present during previous rehash

    //Debug data member
#ifdef _DEBUG
    m_fInSyncCode = false;
#endif
    // profile data members
#ifdef PROFILE
    m_cbRehash = 0;
    m_cbRehashSlots = 0;
    m_cbObsoleteTables = 0;
    m_cbTotalBuckets =0;
    m_cbInsertProbesGt8 = 0; // inserts that needed more than 8 probes
    maxFailureProbe =0;
    memset(m_rgLookupProbes,0,20*sizeof(LONG));
#endif
#ifdef _DEBUG
    m_lockData = NULL;
    m_pfnLockOwner = NULL;
#endif
}

//---------------------------------------------------------------------
//  void HashMap::Init(unsigned cbInitialIndex, CompareFnPtr ptr, bool fAsyncMode)
//  set the initial size of the hash table and provide the comparison
//  function pointer
//
void HashMap::Init(unsigned cbInitialIndex, CompareFnPtr ptr, BOOL fAsyncMode, LockOwner *pLock)
{
    Compare* pCompare = NULL;
    if (ptr != NULL)
    {
        pCompare = new Compare(ptr);
        _ASSERTE(pCompare != NULL);
    }
    Init(cbInitialIndex, pCompare, fAsyncMode, pLock);
}

//---------------------------------------------------------------------
//  void HashMap::Init(unsigned cbInitialIndex, Compare* pCompare, bool fAsyncMode)
//  set the initial size of the hash table and provide the comparison
//  function pointer
//
void HashMap::Init(unsigned cbInitialIndex, Compare* pCompare, BOOL fAsyncMode, LockOwner *pLock)
{
    DWORD size = g_rgPrimes[m_iPrimeIndex = cbInitialIndex];
    m_rgBuckets = (Bucket*) new BYTE[ ((size+1)*sizeof(Bucket))];
    _ASSERTE(m_rgBuckets != NULL);
    memset (m_rgBuckets, 0, (size+1)*sizeof(Bucket));
    Size(m_rgBuckets) = size;

    m_pCompare = pCompare;
    
    m_fAsyncMode = fAsyncMode;

    // assert null comparison returns true
    //ASSERT(
    //      m_pCompare == NULL || 
    //      (m_pCompare->CompareHelper(0,0) != 0) 
    //    );
    
#ifdef PROFILE
    m_cbTotalBuckets = size+1;
#endif

#ifdef _DEBUG
    if (pLock == NULL) {
        m_lockData = NULL;
        m_pfnLockOwner = NULL;
    }
    else
    {
        m_lockData = pLock->lock;
        m_pfnLockOwner = pLock->lockOwnerFunc;
    }
    if (m_pfnLockOwner == NULL) {
        m_writerThreadId = GetCurrentThreadId();
    }
#endif
}

//---------------------------------------------------------------------
//  void PtrHashMap::Init(unsigned cbInitialIndex, CompareFnPtr ptr, bool fAsyncMode)
//  set the initial size of the hash table and provide the comparison
//  function pointer
//
void PtrHashMap::Init(unsigned cbInitialIndex, CompareFnPtr ptr, BOOL fAsyncMode, LockOwner *pLock)
{
    m_HashMap.Init(cbInitialIndex, (ptr != NULL) ? new ComparePtr(ptr) : NULL, fAsyncMode, pLock);
}

//---------------------------------------------------------------------
//  HashMap::~HashMap()
//  destructor, free the current array of buckets
//
HashMap::~HashMap()
{
    // free the current table
    delete [] m_rgBuckets;
    // compare object
    if (NULL != m_pCompare)
        delete m_pCompare;
}


//---------------------------------------------------------------------
//  UPTR   HashMap::CompareValues(const UPTR value1, const UPTR value2)
//  compare values with the function pointer provided
//
inline 
UPTR   HashMap::CompareValues(const UPTR value1, const UPTR value2)
{
    /// NOTE:: the ordering of arguments are random
    return (m_pCompare == NULL || m_pCompare->CompareHelper(value1,value2));
}

//---------------------------------------------------------------------
//  bool HashMap::Enter()
//  bool HashMap::Leave()
//  check  valid use of the hash table in synchronus mode

inline
void HashMap::Enter()
{
    #ifdef _DEBUG
    // check proper concurrent use of the hash table
    if (m_fInSyncCode)
        ASSERT(0); // oops multiple access to sync.-critical code
    m_fInSyncCode = true;
    #endif
}

inline
void HashMap::Leave()
{
    #ifdef _DEBUG
    // check proper concurrent use of the hash table
    if (m_fInSyncCode == false)
        ASSERT(0); // oops multiple access to sync.-critical code
    m_fInSyncCode = false;
    #endif
}


//---------------------------------------------------------------------
//  void HashMap::ProfileLookup(unsigned ntry)
//  profile helper code
void HashMap::ProfileLookup(UPTR ntry, UPTR retValue)
{
    #ifdef PROFILE
        if (ntry < 18)
            FastInterlockIncrement(&m_rgLookupProbes[ntry]);
        else
            FastInterlockIncrement(&m_rgLookupProbes[18]);

        if (retValue == NULL)
        {   // failure probes
            FastInterlockIncrement(&m_rgLookupProbes[19]);
            // the following code is usually executed
            // only for special case of lookup done before insert
            // check hash.h SyncHash::InsertValue
            if (maxFailureProbe < ntry)
            {
                maxFailureProbe = ntry;
            }
        }
    #endif
}


//---------------------------------------------------------------------
//  void HashMap::InsertValue (UPTR key, UPTR value)
//  Insert into hash table, if the number of retries
//  becomes greater than threshold, expand hash table
//
void HashMap::InsertValue (UPTR key, UPTR value)
{
    _ASSERTE (OwnLock());
    
    MAYBE_AUTO_COOPERATIVE_GC(m_fAsyncMode);

    ASSERT(m_rgBuckets != NULL);

    // check proper use in synchronous mode
    Enter();    // no-op in NON debug code  

    UPTR seed = key;

    ASSERT(value <= VALUE_MASK);
    ASSERT (key > DELETED);

    UPTR cbSize = (UINT)Size(m_rgBuckets);
    Bucket* rgBuckets = Buckets();

    for (UPTR ntry =0; ntry < 8; ntry++) 
    {
        Bucket* pBucket = &rgBuckets[seed % cbSize];
        if(pBucket->InsertValue(key,value))
        {
            goto LReturn;
        }
        
        seed += ((seed >> 5) + 1);
    } // for ntry loop

    // We need to expand to keep lookup short
    Rehash(); 

    // Try again
    PutEntry (Buckets(), key,value);
    
LReturn: // label for return
    
    m_cbInserts++;

    Leave(); // no-op in NON debug code

    #ifdef _DEBUG
        ASSERT (m_pCompare != NULL || value == LookupValue (key,value));
        // check proper concurrent use of the hash table in synchronous mode
    #endif

    return;
}

//---------------------------------------------------------------------
//  UPTR HashMap::LookupValue(UPTR key, UPTR value)
//  Lookup value in the hash table, use the comparison function
//  to verify the values match
//
UPTR HashMap::LookupValue(UPTR key, UPTR value)
{
    _ASSERTE (m_fAsyncMode || OwnLock());
    
    MAYBE_AUTO_COOPERATIVE_GC(m_fAsyncMode);

    ASSERT(m_rgBuckets != NULL);
    // This is necessary in case some other thread
    // replaces m_rgBuckets
    ASSERT (key > DELETED);
    Bucket* rgBuckets = Buckets(); //atomic fetch
    UPTR  cbSize = (UINT)Size(rgBuckets-1);

    UPTR seed = key;

    for(UPTR ntry =0; ntry < cbSize; ntry++)
    {
        Bucket* pBucket = &rgBuckets[seed % cbSize];
        for (int i = 0; i < 4; i++)
        {
            if (pBucket->m_rgKeys[i] == key) // keys match
            {

                // inline memory barrier call, refer to 
                // function description at the beginning of this
                MemoryBarrier();

                UPTR storedVal = pBucket->GetValue(i);
                // if compare function is provided
                // dupe keys are possible, check if the value matches, 
                if (CompareValues(value,storedVal))
                { 
                    ProfileLookup(ntry,storedVal); //no-op in non PROFILE code

                    // return the stored value
                    return storedVal;
                }
            }
        }

        seed += ((seed >> 5) + 1);
        if(!pBucket->IsCollision()) 
            break;
    }   // for ntry loop

    // not found
    ProfileLookup(ntry,INVALIDENTRY); //no-op in non PROFILE code

    return INVALIDENTRY;
}

//---------------------------------------------------------------------
//  UPTR HashMap::ReplaceValue(UPTR key, UPTR value)
//  Replace existing value in the hash table, use the comparison function
//  to verify the values match
//
UPTR HashMap::ReplaceValue(UPTR key, UPTR value)
{
    _ASSERTE(OwnLock());

    MAYBE_AUTO_COOPERATIVE_GC(m_fAsyncMode);

    ASSERT(m_rgBuckets != NULL);
    // This is necessary in case some other thread
    // replaces m_rgBuckets
    ASSERT (key > DELETED);
    Bucket* rgBuckets = Buckets(); //atomic fetch
    UPTR  cbSize = (UINT)Size(rgBuckets-1);

    UPTR seed = key;

    for(UPTR ntry =0; ntry < cbSize; ntry++)
    {
        Bucket* pBucket = &rgBuckets[seed % cbSize];
        for (int i = 0; i < 4; i++)
        {
            if (pBucket->m_rgKeys[i] == key) // keys match
            {

                // inline memory barrier call, refer to 
                // function description at the beginning of this
                MemoryBarrier();

                UPTR storedVal = pBucket->GetValue(i);
                // if compare function is provided
                // dupe keys are possible, check if the value matches, 
                if (CompareValues(value,storedVal))
                { 
                    ProfileLookup(ntry,storedVal); //no-op in non PROFILE code

					pBucket->SetValue(value, i);

					// On multiprocessors we should make sure that
					// the value is propagated before we proceed.  
					// inline memory barrier call, refer to 
					// function description at the beginning of this
					MemoryBarrier();

                    // return the previous stored value
                    return storedVal;
                }
            }
        }

        seed += ((seed >> 5) + 1);
        if(!pBucket->IsCollision()) 
            break;
    }   // for ntry loop

    // not found
    ProfileLookup(ntry,INVALIDENTRY); //no-op in non PROFILE code

    return INVALIDENTRY;
}

//---------------------------------------------------------------------
//  UPTR HashMap::DeleteValue (UPTR key, UPTR value)
//  if found mark the entry deleted and return the stored value
//
UPTR HashMap::DeleteValue (UPTR key, UPTR value)
{
    _ASSERTE (OwnLock());

    MAYBE_AUTO_COOPERATIVE_GC(m_fAsyncMode);

    // check proper use in synchronous mode
    Enter();  //no-op in non DEBUG code

    ASSERT(m_rgBuckets != NULL);
    // This is necessary in case some other thread
    // replaces m_rgBuckets
    ASSERT (key > DELETED);
    Bucket* rgBuckets = Buckets();
    UPTR  cbSize = (UINT)Size(rgBuckets-1);
    
    UPTR seed = key;

    for(UPTR ntry =0; ntry < cbSize; ntry++)
    {
        Bucket* pBucket = &rgBuckets[seed % cbSize];
        for (int i = 0; i < 4; i++)
        {
            if (pBucket->m_rgKeys[i] == key) // keys match
            {
                // inline memory barrier call, refer to 
                // function description at the beginning of this
                MemoryBarrier();

                UPTR storedVal = pBucket->GetValue(i);
                // if compare function is provided
                // dupe keys are possible, check if the value matches, 
                if (CompareValues(value,storedVal))
                { 
                    if(m_fAsyncMode)
                    {
                        pBucket->m_rgKeys[i] = DELETED; // mark the key as DELETED
                    }
                    else
                    {
                        pBucket->m_rgKeys[i] = EMPTY;// otherwise mark the entry as empty
                        pBucket->SetFreeSlots();
                    }
                    m_cbDeletes++;  // track the deletes

                    ProfileLookup(ntry,storedVal); //no-op in non PROFILE code
                    Leave(); //no-op in non DEBUG code

                    // return the stored value
                    return storedVal;
                }
            }
        }

        seed += ((seed >> 5) + 1);
        if(!pBucket->IsCollision()) 
            break;
    }   // for ntry loop

    // not found
    ProfileLookup(ntry,INVALIDENTRY); //no-op in non PROFILE code

    Leave(); //no-op in non DEBUG code

    #ifdef _DEBUG
        ASSERT (m_pCompare != NULL || INVALIDENTRY == LookupValue (key,value));
        // check proper concurrent use of the hash table in synchronous mode
    #endif

    return INVALIDENTRY;
}


//---------------------------------------------------------------------
//  UPTR HashMap::Gethash (UPTR key)
//  use this for lookups with unique keys
// don't need to pass an input value to perform the lookup
//
UPTR HashMap::Gethash (UPTR key)
{
    return LookupValue(key,NULL);
}


//---------------------------------------------------------------------
//  UPTR PutEntry (Bucket* rgBuckets, UPTR key, UPTR value)
//  helper used by expand method below

UPTR HashMap::PutEntry (Bucket* rgBuckets, UPTR key, UPTR value)
{
    UPTR seed = key;
    ASSERT (value > 0);
    ASSERT (key > DELETED);

    UPTR size = (UINT)Size(rgBuckets-1);
    for (UPTR ntry =0; true; ntry++) 
    {
        Bucket* pBucket = &rgBuckets[seed % size];
        if(pBucket->InsertValue(key,value))
        {
            return ntry;
        }
        
        seed += ((seed >> 5) + 1);
        ASSERT(ntry <size);
    } // for ntry loop
    return ntry;
}

//---------------------------------------------------------------------
//  
//  UPTR HashMap::NewSize()
//  compute the new size based on the number of free slots
//
inline
UPTR HashMap::NewSize()
{
    UPTR cbValidSlots = m_cbInserts-m_cbDeletes;
    UPTR cbNewSlots = m_cbInserts - m_cbPrevSlotsInUse;

    ASSERT(cbValidSlots >=0 );
    if (cbValidSlots == 0)
        return 9; // arbid value

    UPTR cbTotalSlots = (m_fAsyncMode) ? (UPTR)(cbValidSlots*3/2+cbNewSlots*.6) : cbValidSlots*3/2;

    //UPTR cbTotalSlots = cbSlotsInUse*3/2+m_cbDeletes;

    for (UPTR iPrimeIndex = 0; iPrimeIndex < 69; iPrimeIndex++)
    {
        if (g_rgPrimes[iPrimeIndex] > cbTotalSlots)
        {
            return iPrimeIndex;
        }
    }
    ASSERT(iPrimeIndex == 69);
    ASSERT(0);
    return iPrimeIndex; 
}

//---------------------------------------------------------------------
//  void HashMap::Rehash()
//  Rehash the hash table, create a new array of buckets and rehash
// all non deleted values from the previous array
//
void HashMap::Rehash()
{
    MAYBE_AUTO_COOPERATIVE_GC(m_fAsyncMode);

    _ASSERTE (!g_fEEStarted || !m_fAsyncMode || GetThread() == NULL || GetThread()->PreemptiveGCDisabled());
    _ASSERTE (OwnLock());

    DWORD cbNewSize = g_rgPrimes[m_iPrimeIndex = NewSize()];
    
    ASSERT(m_iPrimeIndex < 70);

    Bucket* rgBuckets = Buckets();
    UPTR cbCurrSize =   (UINT)Size(m_rgBuckets);

    Bucket* rgNewBuckets = (Bucket*) new BYTE[((cbNewSize + 1)*sizeof (Bucket))];
    if(rgNewBuckets == NULL)
	{
		THROWSCOMPLUSEXCEPTION();
		COMPlusThrowOM();
	}
    memset (rgNewBuckets, 0, (cbNewSize + 1)*sizeof (Bucket));
    Size(rgNewBuckets) = cbNewSize;

    // current valid slots
    UPTR cbValidSlots = m_cbInserts-m_cbDeletes;
    m_cbInserts = cbValidSlots; // reset insert count to the new valid count
    m_cbPrevSlotsInUse = cbValidSlots; // track the previous delete count
    m_cbDeletes = 0;            // reset delete count
    // rehash table into it
    
    if (cbValidSlots) // if there are valid slots to be rehashed
    {
        for (unsigned long nb = 0; nb < cbCurrSize; nb++)
        {
            for (int i = 0; i < 4; i++)
            {
                UPTR key =rgBuckets[nb].m_rgKeys[i];
                if (key > DELETED)
                {
                    UPTR ntry = PutEntry (rgNewBuckets+1, key, rgBuckets[nb].GetValue (i));
                    #ifdef PROFILE
                        if(ntry >=8)
                            m_cbInsertProbesGt8++;
                    #endif

                        // check if we can bail out
                    if (--cbValidSlots == 0) 
                        goto LDone; // break out of both the loops
                }
            } // for i =0 thru 4
        } //for all buckets
    }

    
LDone:
    
    Bucket* pObsoleteTables = m_rgBuckets;

    // memory barrier, to replace the pointer to array of bucket
    MemoryBarrier();

    // replace the old array with the new one. 
    m_rgBuckets = rgNewBuckets;

    #ifdef PROFILE
        m_cbRehash++;
        m_cbRehashSlots+=m_cbInserts;
        m_cbObsoleteTables++; // track statistics
        m_cbTotalBuckets += (cbNewSize+1);
    #endif

#ifdef _DEBUG

    unsigned nb;
    if (m_fAsyncMode)
    {
        // for all non deleted keys in the old table, make sure the corresponding values
        // are in the new lookup table

        for (nb = 1; nb <= ((size_t*)pObsoleteTables)[0]; nb++)
        {
            for (int i =0; i < 4; i++)
            {
                if (pObsoleteTables[nb].m_rgKeys[i] > DELETED)
                {
                    UPTR value = pObsoleteTables[nb].GetValue (i);
                    // make sure the value is present in the new table
                    ASSERT (m_pCompare != NULL || value == LookupValue (pObsoleteTables[nb].m_rgKeys[i], value));
                }
            }
        }
    }
    
    // make sure there are no deleted entries in the new lookup table
    // if the compare function provided is null, then keys must be unique
    for (nb = 0; nb < cbNewSize; nb++)
    {
        for (int i = 0; i < 4; i++)
        {
            UPTR keyv = Buckets()[nb].m_rgKeys[i];
            ASSERT (keyv != DELETED);
            if (m_pCompare == NULL && keyv != EMPTY)
            {
                ASSERT ((Buckets()[nb].GetValue (i)) == Gethash (keyv));
            }
        }
    }
#endif

    if (m_fAsyncMode)
    {
        // If we are allowing asynchronous reads, we must delay bucket cleanup until GC time.
        SyncClean::AddHashMap (pObsoleteTables);
    }
    else
    {
        Bucket* pBucket = pObsoleteTables;
        while (pBucket) {
            Bucket* pNextBucket = NextObsolete(pBucket);
            delete [] pBucket;
            pBucket = pNextBucket;
        }
    }

}

//---------------------------------------------------------------------
//  void HashMap::Compact()
//  delete obsolete tables, try to compact deleted slots by sliding entries
//  in the bucket, note we can slide only if the bucket's collison bit is reset
//  otherwise the lookups will break
//  @perf, use the m_cbDeletes to m_cbInserts ratio to reduce the size of the hash 
//   table
//
void HashMap::Compact()
{
    _ASSERTE (OwnLock());
    
    MAYBE_AUTO_COOPERATIVE_GC(m_fAsyncMode);
    ASSERT(m_rgBuckets != NULL);
    
    UPTR iNewIndex = NewSize();

    if (iNewIndex != m_iPrimeIndex)
    {
        Rehash(); 
    }

    //compact deleted slots, mark them as EMPTY
    
    if (m_cbDeletes)
    {   
        UPTR cbCurrSize = (UINT)Size(m_rgBuckets);
        Bucket *pBucket = Buckets();
        Bucket *pSentinel;
        
        for (pSentinel = pBucket+cbCurrSize; pBucket < pSentinel; pBucket++)
        {   //loop thru all buckets
            for (int i = 0; i < 4; i++)
            {   //loop through all slots
                if (pBucket->m_rgKeys[i] == DELETED)
                {
                    pBucket->m_rgKeys[i] = EMPTY;
                    pBucket->SetFreeSlots(); // mark the bucket as containing free slots
                    if(--m_cbDeletes == 0) // decrement count
                        return; 
                }
            }
        }
    }

}

#ifdef _DEBUG
// A thread must own a lock for a hash if it is a writer.
BOOL HashMap::OwnLock()
{
    if (m_pfnLockOwner == NULL) {
        return m_writerThreadId == GetCurrentThreadId();
    }
    else {
        BOOL ret = m_pfnLockOwner(m_lockData);
        if (!ret) {
            if (g_pGCHeap->IsGCInProgress() && 
                (dbgOnly_IsSpecialEEThread() || GetThread() == g_pGCHeap->GetGCThread())) {
                ret = TRUE;
            }
        }
        return ret;
    }
}
#endif

#ifdef PROFILE
//---------------------------------------------------------------------
//  void HashMap::DumpStatistics()
//  dump statistics collected in profile mode
//
void HashMap::DumpStatistics()
{
    cout << "\n Hash Table statistics "<< endl;
    cout << "--------------------------------------------------" << endl;

    cout << "Current Insert count         " << m_cbInserts << endl;
    cout << "Current Delete count         "<< m_cbDeletes << endl;

    cout << "Current # of tables " << m_cbObsoleteTables << endl;
    cout << "Total # of times Rehashed " << m_cbRehash<< endl;
    cout << "Total # of slots rehashed " << m_cbRehashSlots << endl;

    cout << "Insert : Probes gt. 8 during rehash " << m_cbInsertProbesGt8 << endl;

    cout << " Max # of probes for a failed lookup " << maxFailureProbe << endl;

    cout << "Prime Index " << m_iPrimeIndex << endl;
    cout <<  "Current Buckets " << g_rgPrimes[m_iPrimeIndex]+1 << endl;

    cout << "Total Buckets " << m_cbTotalBuckets << endl;

    cout << " Lookup Probes " << endl;
    for (unsigned i = 0; i < 20; i++)
    {
        cout << "# Probes:" << i << " #entries:" << m_rgLookupProbes[i] << endl;
    }
    cout << "\n--------------------------------------------------" << endl;
}

//---------------------------------------------------------------------
//  void SyncHashMap::DumpStatistics()
//  dump statistics collected in profile mode
//

void SyncHashMap::DumpStatistics()
{
    cout << "\n Hash Table statistics "<< endl;
    cout << "--------------------------------------------------" << endl;

    cout << "Failures during lookup  " << m_lookupFail << endl;

    cout << "Current Insert count         " << m_HashMap.m_cbInserts << endl;
    cout << "Current Delete count         "<< m_HashMap.m_cbDeletes << endl;

    cout << "Current # of tables " << m_HashMap.m_cbObsoleteTables << endl;
    cout << "Total # of Rehash " << m_HashMap.m_cbRehash<< endl;
    cout << "Total # of slots rehashed " << m_HashMap.m_cbRehashSlots << endl;
    
    cout << "Insert : Probes gt. 8 during rehash " << m_HashMap.m_cbInsertProbesGt8 << endl;

    cout << " Max # of probes for a failed lookup " << m_HashMap.maxFailureProbe << endl;

    cout << "Prime Index " << m_HashMap.m_iPrimeIndex << endl;
    cout <<  "Current Buckets " << g_rgPrimes[m_HashMap.m_iPrimeIndex]+1 << endl;

    cout << "Total Buckets " << m_HashMap.m_cbTotalBuckets << endl;

    cout << " Lookup Probes " << endl;
    for (unsigned i = 0; i < 20; i++)
    {
        cout << "# Probes:" << i << " #entries:" << m_HashMap.m_rgLookupProbes[i] << endl;
    }

    cout << "\n--------------------------------------------------" << endl;

}
#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\hash.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*++---------------------------------------------------------------------------------------

Module Name:

    hash.h

Abstract:

    Fast hash table classes, 
--*/

#ifndef _HASH_H_
#define _HASH_H_

#ifndef ASSERT
#define ASSERT _ASSERTE
#endif


#include "crst.h"
#include <member-offset-info.h>

// #define PROFILE

//-------------------------------------------------------
//  enums for special Key values used in hash table
//
enum
{
    EMPTY  = 0,
    DELETED = 1,
    INVALIDENTRY = ~0
};

typedef ULONG_PTR UPTR;

//------------------------------------------------------------------------------
// classes in use
//------------------------------------------------------------------------------
class Bucket;
class HashMap;
class SyncHashMap;

//-------------------------------------------------------
//  class Bucket
//  used by hash table implementation
//
class Bucket
{
public:
    UPTR m_rgKeys[4];
    UPTR m_rgValues[4];
#define VALUE_MASK (sizeof(LPVOID) == 4 ? 0x7FFFFFFF : 0x7FFFFFFFFFFFFFFF)

    void SetValue (UPTR value, UPTR i)
    {
        ASSERT(value <= VALUE_MASK);
        m_rgValues[i] = (UPTR) ((m_rgValues[i] & ~VALUE_MASK) | value);
    }

    UPTR GetValue (UPTR i)
    {
        return (UPTR)(m_rgValues[i] & VALUE_MASK);
    }

    UPTR IsCollision() // useful sentinel for fast fail of lookups
    {
        return (UPTR) (m_rgValues[0] & ~VALUE_MASK);
    }

    void SetCollision()
    {
        m_rgValues[0] |= ~VALUE_MASK; // set collision bit
        m_rgValues[1] &= VALUE_MASK;   // reset has free slots bit
    }

    BOOL HasFreeSlots()
    {
        // check for free slots available in the bucket
        // either there is no collision or a free slot has been during
        // compaction
        return (!IsCollision() || (m_rgValues[1] & ~VALUE_MASK));
    }

    void SetFreeSlots()
    {
        m_rgValues[1] |= ~VALUE_MASK; // set has free slots bit
    }

    BOOL InsertValue(const UPTR key, const UPTR value);
};


//------------------------------------------------------------------------------
// bool (*CompareFnPtr)(UPTR,UPTR); pointer to a function that takes 2 UPTRs 
// and returns a boolean, provide a function with this signature to the HashTable
// to use for comparing Values during lookup
//------------------------------------------------------------------------------
typedef  BOOL (*CompareFnPtr)(UPTR,UPTR);

class Compare
{
protected:
    Compare()
    {
        m_ptr = NULL;
    }
public:
    CompareFnPtr m_ptr;
    
    Compare(CompareFnPtr ptr)
    {
        _ASSERTE(ptr != NULL);
        m_ptr = ptr;
    }

    virtual UPTR CompareHelper(UPTR val1, UPTR storedval)
    {
        return (*m_ptr)(val1,storedval);
    }
};

class ComparePtr : public Compare
{
public:
    ComparePtr (CompareFnPtr ptr)
    {
        _ASSERTE(ptr != NULL);
        m_ptr = ptr;
    }

    virtual UPTR CompareHelper(UPTR val1, UPTR storedval)
    {
        storedval <<=1;
        return (*m_ptr)(val1,storedval);
    }
};

//------------------------------------------------------------------------------
// Class HashMap
// Fast Hash table, for concurrent use, 
// stores a 4 byte Key and a 4 byte Value for each slot.
// Duplicate keys are allowed, (keys are compared as 4 byte UPTRs)
// Duplicate values are allowed,(values are compared using comparison fn. provided)
// but if no comparison function is provided then the values should be unique
// 
// Lookup's don't require to take locks, unless you specify fAsyncMode.
// Insert and Delete operations require locks
// Inserting a duplicate value will assert in DEBUG mode, the PROPER way to perform inserts
// is to take a lock, do a lookup and if the lookup fails then Insert
//
// In async mode, deleted slots are not immediately reclaimed (until a rehash), and 
// accesses to the hash table cause a transition to cooperative GC mode, and reclamation of old
// hash maps (after a rehash) are deferred until GC time.
// In sync mode, none of this is necessary; however calls to LookupValue must be synchronized as well.
//
// Algorithm: 
//   The Hash table is an array of buckets, each bucket can contain 4 key/value pairs
//   Special key values are used to identify EMPTY and DELETED slots        
//   Hash function uses the current size of the hash table and a SEED based on the key 
//   to choose the bucket, seed starts of being the key and gets refined every time 
//   the hash function is re-applied. 
//
//   Inserts choose an empty slot in the current bucket for new entries, if the current bucket
//   is full, then the seed is refined and a new bucket is choosen, if an empty slot is not found
//   after 8 retries, the hash table is expanded, this causes the current array of buckets to  
//   be put in a free list and a new array of buckets is allocated and all non-deleted entries  
//   from the old hash table are rehashed to the new array
//   The old arrays are reclaimed during Compact phase, which should only be called during GC or  
//   any other time it is guaranteed that no Lookups are taking place. 
//   Concurrent Insert and Delete operations need to be serialized
// 
//   Delete operations, mark the Key in the slot as DELETED, the value is not removed and inserts
//   don't reuse these slots, they get reclaimed during expansion and compact phases.
//
//------------------------------------------------------------------------------

class HashMap
{
    friend SyncHashMap;
    friend struct MEMBER_OFFSET_INFO(HashMap);

public:

    //@constructor
    HashMap();
    //destructor
    ~HashMap();

    // Init 
    void Init(BOOL fAsyncMode, LockOwner *pLock)
    {
        Init(0, (Compare *)NULL,fAsyncMode, pLock);
    }
    // Init
    void Init(unsigned cbInitialIndex, BOOL fAsyncMode, LockOwner *pLock)
    {
        Init(cbInitialIndex, (Compare*)NULL, fAsyncMode, pLock);
    }
    // Init
    void Init(CompareFnPtr ptr, BOOL fAsyncMode, LockOwner *pLock)
    {
        Init(0, ptr, fAsyncMode, pLock);
    }

    // Init method
    void Init(unsigned cbInitialIndex, CompareFnPtr ptr, BOOL fAsyncMode, LockOwner *pLock);


    //Init method
    void Init(unsigned cbInitialInde, Compare* pCompare, BOOL fAsyncMode, LockOwner *pLock);

    // check to see if the value is already in the Hash Table
    // key should be > DELETED
    // if provided, uses the comparison function ptr to compare values
    // returns INVALIDENTRY if not found
    UPTR LookupValue(UPTR key, UPTR value);

    // Insert if the value is not already present
    // it is illegal to insert duplicate values in the hash map
    // do a lookup to verify the value is not already present

    void InsertValue(UPTR key, UPTR value);

    // Replace the value if present
    // returns the previous value, or INVALIDENTRY if not present
    // does not insert a new value under any circumstances

    UPTR ReplaceValue(UPTR key, UPTR value);

    // mark the entry as deleted and return the stored value
    // returns INVALIDENTRY, if not found
    UPTR DeleteValue (UPTR key, UPTR value);

    // for unique keys, use this function to get the value that is
    // stored in the hash table, returns INVALIDENTRY if key not found
    UPTR Gethash(UPTR key);
    
    // Called only when all threads are frozed, like during GC
    // for a SINGLE user mode, call compact after every delete 
    // operation on the hash table
    void Compact();
    
    // inline helper, in non PROFILE mode becomes a NO-OP
    void        ProfileLookup(UPTR ntry, UPTR retValue);
    // data members used for profiling
#ifdef PROFILE
    unsigned    m_cbRehash;    // number of times rehashed
    unsigned    m_cbRehashSlots; // number of slots that were rehashed
    unsigned    m_cbObsoleteTables;
    unsigned    m_cbTotalBuckets;
    unsigned    m_cbInsertProbesGt8; // inserts that needed more than 8 probes
    LONG        m_rgLookupProbes[20]; // lookup probes
    UPTR        maxFailureProbe; // cost of failed lookup

    void DumpStatistics();
#endif

protected:
    // static helper function
    static UPTR PutEntry (Bucket* rgBuckets, UPTR key, UPTR value);
private:

    // inline helpers enter/leave becomes NO-OP in non DEBUG mode
    void            Enter();        // check valid to enter
    void            Leave();        // check valid to leave

#ifdef _DEBUG
    BOOL            m_fInSyncCode; // test for non-synchronus access
#endif

    // compute the new size, based on the number of free slots 
    // available, compact or expand
    UPTR            NewSize(); 
    // create a new bucket array and rehash the non-deleted entries
    void            Rehash();
    static size_t&  Size(Bucket* rgBuckets);
    Bucket*         Buckets();      
    UPTR            CompareValues(const UPTR value1, const UPTR value2);


    Compare*        m_pCompare;         // compare object to be used in lookup
    SIZE_T          m_iPrimeIndex;      // current size (index into prime array)
    Bucket*         m_rgBuckets;        // array of buckets

    // track the number of inserts and deletes
    SIZE_T          m_cbPrevSlotsInUse;
    SIZE_T          m_cbInserts;
    SIZE_T          m_cbDeletes;
    // mode of operation, synchronus or single user
    BYTE            m_fAsyncMode;

#ifdef _DEBUG
    LPVOID          m_lockData;
    FnLockOwner     m_pfnLockOwner;
    DWORD           m_writerThreadId;
#endif

#ifdef _DEBUG
    // A thread must own a lock for a hash if it is a writer.
    BOOL OwnLock();
#endif

public:
    ///---------Iterator----------------
        
    // Iterator,
    class Iterator 
    {
        Bucket *m_pBucket;
        Bucket* m_pSentinel;
        int     m_id;
        BOOL    m_fEnd;
       
    public:

        // Constructor 
        Iterator(Bucket* pBucket) :m_id(-1), m_fEnd(false), m_pBucket(pBucket)
        {
            if (!m_pBucket) {
                m_fEnd = true;
                return;
            }
            size_t cbSize = ((size_t*)m_pBucket)[0];
            m_pBucket++;
            m_pSentinel = m_pBucket+cbSize;
            MoveNext(); // start
        }
        
        Iterator(const Iterator& iter) 
        {
            m_pBucket = iter.m_pBucket;
            m_pSentinel = iter.m_pSentinel;
            m_id    = iter.m_id;
            m_fEnd = iter.m_fEnd;

        }

        //destructor
        ~Iterator(){};

        // friend operator==
        friend operator == (const Iterator& lhs, const Iterator& rhs)
        {
            return (lhs.m_pBucket == rhs.m_pBucket && lhs.m_id == rhs.m_id);
        }
        // operator = 
        inline Iterator& operator= (const Iterator& iter)
        {
            m_pBucket = iter.m_pBucket;
            m_pSentinel = iter.m_pSentinel;
            m_id    = iter.m_id;
            m_fEnd = iter.m_fEnd;
            return *this;
        }
        
        // operator ++
        inline void operator++ () 
        { 
            _ASSERTE(!m_fEnd); // check we are not alredy at end
            MoveNext();
        } 
        // operator --
        
        

        //accessors : GetDisc() , returns the discriminator
        inline UPTR GetKey() 
        { 
            _ASSERTE(!m_fEnd); // check we are not alredy at end
            return m_pBucket->m_rgKeys[m_id]; 
        }
        //accessors : SetDisc() , sets the discriminator
    

        //accessors : GetValue(), 
        // returns the pointer that corresponds to the discriminator
        inline UPTR GetValue()
        {
            _ASSERTE(!m_fEnd); // check we are not alredy at end
            return m_pBucket->GetValue(m_id); 
        }
                
        
        // end(), check if the iterator is at the end of the bucket
        inline const BOOL end() 
        {
            return m_fEnd; 
        }

    protected:

        void MoveNext()
        {
            for (m_pBucket = m_pBucket;m_pBucket < m_pSentinel; m_pBucket++)
            {   //loop thru all buckets
                for (m_id = m_id+1; m_id < 4; m_id++)
                {   //loop through all slots
                    if (m_pBucket->m_rgKeys[m_id] > DELETED)
                    {
                        return; 
                    }
                }
                m_id  = -1;
            }
            m_fEnd = true;
        }
            
    };
    // return an iterator, positioned at the beginning of the bucket
    inline Iterator begin() 
    { 
        return Iterator(m_rgBuckets); 
    }

};

//------------------------------------------------------------------------------
// Class SyncHashMap, helper
// this class is a wrapper for the above HashMap class, and shows how the above
// class should be used for concurrent access, 
// some of the rules to follow when using the above hash table
// Insert and delete operations need to take a lock, 
// Lookup operations don't require a lock
// Insert operations, after taking the lock, should verify the value about to be inserted
// is not already in the hash table

class SyncHashMap
{
    HashMap         m_HashMap;
    Crst            m_lock;

    UPTR FindValue(UPTR key, UPTR value)
    {
        return m_HashMap.LookupValue(key,value);;
    }

public:
    SyncHashMap() 
        : m_lock("HashMap", CrstSyncHashLock, TRUE, FALSE)
    {
        #ifdef PROFILE
            m_lookupFail = 0;
        #endif
    }

    void Init(unsigned cbInitialIndex, CompareFnPtr ptr)
    {
        //comparison function, 
        // to be used when duplicate keys are allowed
        LockOwner lock = {&m_lock, IsOwnerOfCrst};
        m_HashMap.Init(cbInitialIndex, ptr,true,&lock);
    }
    
    UPTR DeleteValue (UPTR key, UPTR value)
    {
        m_lock.Enter();
        UPTR retVal = m_HashMap.DeleteValue(key,value);
        m_lock.Leave ();
        return retVal;
    }

    UPTR InsertValue(UPTR key, UPTR value)
    {
        m_lock.Enter(); // lock
        UPTR storedVal = FindValue(key,value); // check to see if someone beat us to it
        //UPTR storedVal = 0;
        if (storedVal == INVALIDENTRY) // value not found
        {       // go ahead and insert
            m_HashMap.InsertValue(key,value);
            storedVal = value;
        }
        m_lock.Leave(); // unlock
        return storedVal; // the value currently in the hash table
    }
    
    // For cases where 'value' we lookup by is not the same 'value' as we store. 
    UPTR InsertValue(UPTR key, UPTR storeValue, UPTR lookupValue)
    {
        m_lock.Enter(); // lock
        UPTR storedVal = FindValue(key,lookupValue); // check to see if someone beat us to it
        //UPTR storedVal = 0;
        if (storedVal == INVALIDENTRY) // value not found
        {       // go ahead and insert
            m_HashMap.InsertValue(key,storeValue);
            storedVal = storeValue;
        }
        m_lock.Leave(); // unlock
        return storedVal; // the value currently in the hash table
    }
    
    UPTR ReplaceValue(UPTR key, UPTR value)
    {
        m_lock.Enter(); // lock
        UPTR storedVal = ReplaceValue(key,value); 
        m_lock.Leave(); // unlock
        return storedVal; // the value currently in the hash table
    }
    
    
    // lookup value in the hash table, uses the compare function to verify the values
    // match, returns the stored value if found, otherwise returns NULL
    UPTR LookupValue(UPTR key, UPTR value)
    {
        UPTR retVal = FindValue(key,value);
        if (retVal == 0)
            return LookupValueSync(key,value);
        return retVal;
    }

    UPTR LookupValueSync(UPTR key, UPTR value)
    {
        m_lock.Enter();

    #ifdef PROFILE
        m_lookupFail++;
    #endif
    
        UPTR retVal  = FindValue(key,value);
        m_lock.Leave();
        return retVal;
    }
        
    // for unique keys, use this function to get the value that is
    // stored in the hash table, returns 0 if key not found
    UPTR GetHash(UPTR key) 
    {
        return m_HashMap.Gethash(key);
    }

    void Compact()
    {
        m_HashMap.Compact();
    }

    // Not protected by a lock ! Right now used only at shutdown, where this is ok
    inline HashMap::Iterator begin() 
    { 
		_ASSERTE(g_fEEShutDown);
        return HashMap::Iterator(m_HashMap.m_rgBuckets); 
    }

#ifdef PROFILE
    unsigned        m_lookupFail;
    void DumpStatistics();
#endif

};


//---------------------------------------------------------------------------------------
// class PtrHashMap
//  Wrapper class for using Hash table to store pointer values
//  HashMap class requires that high bit is always reset
//  The allocator used within the runtime, always allocates objects 8 byte aligned
//  so we can shift right one bit, and store the result in the hash table
class PtrHashMap
{
    friend struct MEMBER_OFFSET_INFO(PtrHashMap);

    HashMap         m_HashMap;

public:
    void *operator new(size_t size, LoaderHeap *pHeap);
    void operator delete(void *p);

    // Init 
    void Init(BOOL fAsyncMode, LockOwner *pLock)
    {
        Init(0,NULL,fAsyncMode,pLock);
    }
    // Init
    void Init(unsigned cbInitialIndex, BOOL fAsyncMode, LockOwner *pLock)
    {
        Init(cbInitialIndex, NULL, fAsyncMode,pLock);
    }
    // Init
    void Init(CompareFnPtr ptr, BOOL fAsyncMode, LockOwner *pLock)
    {
        Init(0, ptr, fAsyncMode,pLock);
    }

    // Init method
    void Init(unsigned cbInitialIndex, CompareFnPtr ptr, BOOL fAsyncMode, LockOwner *pLock);

    // check to see if the value is already in the Hash Table
    LPVOID LookupValue(UPTR key, LPVOID pv)
    {
        _ASSERTE (key > DELETED);

        // gmalloc allocator, always allocates 8 byte aligned
        // so we can shift out the lowest bit
        // ptr right shift by 1
        UPTR value = (UPTR)pv;
        value>>=1; 
        UPTR val =  m_HashMap.LookupValue (key, value);
        if (val != INVALIDENTRY)
        {
            val<<=1;
        }
        return (LPVOID)val;
    }

    // Insert if the value is not already present
    // it is illegal to insert duplicate values in the hash map
    // users should do a lookup to verify the value is not already present

    void InsertValue(UPTR key, LPVOID pv)
    {
        _ASSERTE(key > DELETED);

        // gmalloc allocator, always allocates 8 byte aligned
        // so we can shift out the lowest bit
        // ptr right shift by 1
        UPTR value = (UPTR)pv;
        value>>=1; 
        m_HashMap.InsertValue (key, value);
    }

    // Replace the value if present
    // returns the previous value, or INVALIDENTRY if not present
    // does not insert a new value under any circumstances

    LPVOID ReplaceValue(UPTR key, LPVOID pv)
    {
        _ASSERTE(key > DELETED);

        // gmalloc allocator, always allocates 8 byte aligned
        // so we can shift out the lowest bit
        // ptr right shift by 1
        UPTR value = (UPTR)pv;
        value>>=1; 
        UPTR val = m_HashMap.ReplaceValue (key, value);
        if (val != INVALIDENTRY)
        {
            val<<=1;
        }
        return (LPVOID)val;
    }

    // mark the entry as deleted and return the stored value
    // returns INVALIDENTRY if not found
    LPVOID DeleteValue (UPTR key,LPVOID pv)
    {
        _ASSERTE(key > DELETED);

        UPTR value = (UPTR)pv;
        value >>=1 ;
        UPTR val = m_HashMap.DeleteValue(key, value);
        if (val != INVALIDENTRY)
        {
            val <<= 1;
        }
        return (LPVOID)val;
    }

    // for unique keys, use this function to get the value that is
    // stored in the hash table, returns INVALIDENTRY if key not found
    LPVOID Gethash(UPTR key)
    {
        _ASSERTE(key > DELETED);

        UPTR val = m_HashMap.Gethash(key);
        if (val != INVALIDENTRY)
        {
            val <<= 1;
        }
        return (LPVOID)val;
    }


    class PtrIterator
    {
        HashMap::Iterator iter;

    public:
        PtrIterator(HashMap& hashMap) : iter(hashMap.begin())
        {
        }

        ~PtrIterator()
        {
        }

        BOOL end()
        {
            return iter.end();
        }

        LPVOID GetValue()
        {
            UPTR val = iter.GetValue();
            if (val != INVALIDENTRY)
            {
                val <<= 1;
            }
            return (LPVOID)val;
        }

        void operator++()
        {
            iter.operator++();
        }
    };

    // return an iterator, positioned at the beginning of the bucket
    inline PtrIterator begin() 
    { 
        return PtrIterator(m_HashMap); 
    }
};

//---------------------------------------------------------------------
//  inline Bucket*& NextObsolete (Bucket* rgBuckets)
//  get the next obsolete bucket in the chain
inline
Bucket*& NextObsolete (Bucket* rgBuckets)
{
    return *(Bucket**)&((size_t*)rgBuckets)[1];
}

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\intencode.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/******************************************************************************/
/*                                 intEncode.h                                */
/******************************************************************************/

/* contaings routines for inputing and outputing integers into a byte stream */

/******************************************************************************/


#ifndef intEncode_h
#define intEncode_h

inline int getI1(const unsigned char* ptr) {
	return(*((char*)ptr));
	}

inline int getU1(const unsigned char* ptr) {
	return(*ptr);
	}

inline int getI2(const unsigned char* ptr) {
#if ONLY_ALIGNED_ACCESSES
	return((short)(ptr[0] + ptr[1] << 8))
#else
	return(*((short*)ptr));
#endif
	}

inline int getU2(const unsigned char* ptr) {
#if ONLY_ALIGNED_ACCESSES
	return(ptr[0] + ptr[1] << 8))
#else
	return(*((unsigned short*)ptr));
#endif
	}

inline int getI4(const unsigned char* ptr) {
#if ONLY_ALIGNED_ACCESSES
	return(ptr[0] + (ptr[1] + (ptr[2] + (ptr[3] << 8) << 8) << 8));
#else
	return(*((int*)ptr));
#endif
	}

inline int getU4(const unsigned char* ptr) {
#if ONLY_ALIGNED_ACCESSES
	return(ptr[0] + (ptr[1] + (ptr[2] + (ptr[3] << 8) << 8) << 8));
#else
	return(*((unsigned int*)ptr));
#endif
	}

inline __int64 getI8(const unsigned char* ptr) {
#if ONLY_ALIGNED_ACCESSES
	return(ptr[0] + (ptr[1] + (ptr[2] + (ptr[3] + (ptr[4] + (ptr[5] + (ptr[6] + (ptr[7] << 8) << 8) << 8) << 8) << 8) << 8) << 8));
#else
	return(*((__int64*)ptr));
#endif
	}

inline void putI1(unsigned char* ptr, int val) {
	*ptr = val;
	}

inline void putI2(unsigned char* ptr, int val) {
#if ONLY_ALIGNED_ACCESSES
	*ptr = val & 0xFF;
	*ptr = (val >> 8) & 0xFF;
#else
	*((short*)ptr) = val; 
#endif
	}

inline void putI4(unsigned char* ptr, int val) {
#if ONLY_ALIGNED_ACCESSES
	*ptr = val & 0xFF;
	*ptr = (val >> 8) & 0xFF;
	*ptr = (val >> 16) & 0xFF;
	*ptr = (val >> 24) & 0xFF;
#else
	*((int*)ptr) = val; 
#endif
	}

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\interopconverter.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#include "common.h"
#include "vars.hpp"
#include "excep.h"
#include "stdinterfaces.h"
#include "InteropUtil.h"
#include "ComCallWrapper.h"
#include "ComPlusWrapper.h"
#include "COMInterfaceMarshaler.h"
#include "InteropConverter.h"
#include "remoting.h"
#include "olevariant.h"

#ifdef CUSTOMER_CHECKED_BUILD
    #include "CustomerDebugHelper.h"
#endif

extern BOOL     g_fComStarted;

// if the object we are creating is a proxy to another appdomain, want to create the wrapper for the
// new object in the appdomain of the proxy target
IUnknown* GetIUnknownForMarshalByRefInServerDomain(OBJECTREF* poref)
{
    _ASSERTE((*poref)->GetTrueClass()->IsMarshaledByRef());
    Context *pContext = NULL;

    // so this is an proxy type, 
    // now get it's underlying appdomain which will be null if non-local
    if ((*poref)->GetMethodTable()->IsTransparentProxyType())
    {
        pContext = CRemotingServices::GetServerContextForProxy(*poref);
    }
    if (pContext == NULL)
    {
        pContext = GetCurrentContext();
    }
    
    _ASSERTE(pContext->GetDomain() == GetCurrentContext()->GetDomain());

    ComCallWrapper* pWrap = ComCallWrapper::InlineGetWrapper(poref);      

    IUnknown* pUnk = ComCallWrapper::GetComIPfromWrapper(pWrap, IID_IUnknown, NULL, FALSE);

    ComCallWrapper::Release(pWrap);
    
    return pUnk;
}

//+----------------------------------------------------------------------------
// IUnknown* GetIUnknownForTransparentProxy(OBJECTREF otp)
//+----------------------------------------------------------------------------

IUnknown* GetIUnknownForTransparentProxy(OBJECTREF* poref, BOOL fIsBeingMarshalled)
{    
    THROWSCOMPLUSEXCEPTION();

    // Setup the thread object.
    Thread *pThread = SetupThread();
    _ASSERTE(pThread);
    BOOL fGCDisabled = pThread->PreemptiveGCDisabled();
    if (!fGCDisabled)
    {
        pThread->DisablePreemptiveGC();
    }

    OBJECTREF realProxy = ObjectToOBJECTREF(CRemotingServices::GetRealProxy(OBJECTREFToObject(*poref)));
    _ASSERTE(realProxy != NULL);

    MethodDesc* pMD = CRemotingServices::MDofGetDCOMProxy();
    _ASSERTE(pMD != NULL);

    INT64 args[] = {
        ObjToInt64(realProxy),
        (INT64)fIsBeingMarshalled,
    };

    INT64 ret = pMD->Call(args, METHOD__REAL_PROXY__GETDCOMPROXY);
    
    if (!fGCDisabled)
    {
        pThread->EnablePreemptiveGC();
    }
    IUnknown* pUnk = (IUnknown*)ret;

    return pUnk;
}


//--------------------------------------------------------------------------------
// IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, MethodTable* pMT);
// Convert ObjectRef to a COM IP, based on MethodTable* pMT.
IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, MethodTable* pMT)
{
    THROWSCOMPLUSEXCEPTION();
    _ASSERTE(GetThread()->PreemptiveGCDisabled());

    // COM had better be started up at this point.
    _ASSERTE(g_fComStarted && "COM has not been started up, ensure QuickCOMStartup is called before any COM objects are used!");

    // Validate the arguments.
    _ASSERTE(poref);
    _ASSERTE(pMT);

    BOOL fReleaseWrapper = false;
    HRESULT hr = E_NOINTERFACE;
    IUnknown* pUnk = NULL;
	size_t ul = 0;

    if (*poref == NULL)
        return NULL;

    if ((*poref)->GetMethodTable()->IsTransparentProxyType()) 
       return GetIUnknownForTransparentProxy(poref, FALSE);
    
    SyncBlock* pBlock = (*poref)->GetSyncBlockSpecial();
    if (pBlock == NULL)
        goto LExit;
    ul = (size_t)pBlock->GetComVoidPtr();

    // Com+ to COM Wrappers always have non-null com data in the sync block
    // and the low bit of the comdata is set to 1.
    if(ul == 0 || ((ul & 0x1) == 0))
    {
        // create a COM callable wrapper
        // get iunknown from oref
        ComCallWrapper* pWrap = (ComCallWrapper *)ul;
        if (ul == 0)
        {
            pWrap = ComCallWrapper::InlineGetWrapper(poref);
            fReleaseWrapper = true;
        } 

        pWrap->CheckMakeAgile(*poref);

        pUnk = ComCallWrapper::GetComIPfromWrapper(pWrap, GUID_NULL, pMT, FALSE);

        if (fReleaseWrapper)
            ComCallWrapper::Release(pWrap);
    }
    else
    {
        _ASSERTE(ul != 0);
        _ASSERTE((ul & 0x1) != 0);

        ul^=1;
        ComPlusWrapper* pPlusWrap = ((ComPlusWrapper *)ul);

        // Validate that the OBJECTREF is still attached to its ComPlusWrapper.
        if (!pPlusWrap)
            COMPlusThrow(kInvalidComObjectException, IDS_EE_COM_OBJECT_NO_LONGER_HAS_WRAPPER);

        // The interface will be returned addref'ed.
        pUnk = pPlusWrap->GetComIPFromWrapper(pMT);
    }

LExit:
    // If we failed to retrieve an IP then throw an exception.
    if (pUnk == NULL)
        COMPlusThrowHR(hr);

    return pUnk;
}


//--------------------------------------------------------------------------------
// IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, MethodTable* pMT);
// Convert ObjectRef to a COM IP of the requested type.
IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, ComIpType ReqIpType, ComIpType* pFetchedIpType)
{
    THROWSCOMPLUSEXCEPTION();
    _ASSERTE(GetThread()->PreemptiveGCDisabled());

    // COM had better be started up at this point.
    _ASSERTE(g_fComStarted && "COM has not been started up, ensure QuickCOMStartup is called before any COM objects are used!");

    // Validate the arguments.
    _ASSERTE(poref);
    _ASSERTE(ReqIpType != 0);

    BOOL fReleaseWrapper = false;
    HRESULT hr = E_NOINTERFACE;
    IUnknown* pUnk = NULL;
    size_t ul = 0;
    ComIpType FetchedIpType = ComIpType_None;

    if (*poref == NULL)
        return NULL;

    if ((*poref)->GetMethodTable()->IsTransparentProxyType()) 
       return GetIUnknownForTransparentProxy(poref, FALSE);
    
    SyncBlock* pBlock = (*poref)->GetSyncBlockSpecial();
    if (pBlock == NULL)
        goto LExit;
    ul = (size_t)pBlock->GetComVoidPtr();

    // Com+ to COM Wrappers always have non-null com data in the sync block
    // and the low bit of the comdata is set to 1.
    if(ul == 0 || ((ul & 0x1) == 0))
    {
        // create a COM callable wrapper
        // get iunknown from oref
        ComCallWrapper* pWrap = (ComCallWrapper *)ul;
        if (ul == 0)
        {
            pWrap = ComCallWrapper::InlineGetWrapper(poref);
            fReleaseWrapper = true;
        } 

        pWrap->CheckMakeAgile(*poref);

        // If the user requested IDispatch, then check for IDispatch first.
        if (ReqIpType & ComIpType_Dispatch)
        {
            pUnk = ComCallWrapper::GetComIPfromWrapper(pWrap, IID_IDispatch, NULL, FALSE);
            if (pUnk)
                FetchedIpType = ComIpType_Dispatch;
        }

        // If the ObjectRef doesn't support IDispatch and the caller also accepts
        // an IUnknown pointer, then check for IUnknown.
        if (!pUnk && (ReqIpType & ComIpType_Unknown))
        {
            pUnk = ComCallWrapper::GetComIPfromWrapper(pWrap, IID_IUnknown, NULL, FALSE);
            if (pUnk)
                FetchedIpType = ComIpType_Unknown;
        }

        if (fReleaseWrapper)
            ComCallWrapper::Release(pWrap);
    }
    else
    {
        _ASSERTE(ul != 0);
        _ASSERTE((ul & 0x1) != 0);

        ul^=1;
        ComPlusWrapper* pPlusWrap = ((ComPlusWrapper *)ul);

        // Validate that the OBJECTREF is still attached to its ComPlusWrapper.
        if (!pPlusWrap)
            COMPlusThrow(kInvalidComObjectException, IDS_EE_COM_OBJECT_NO_LONGER_HAS_WRAPPER);

        // If the user requested IDispatch, then check for IDispatch first.
        if (ReqIpType & ComIpType_Dispatch)
        {
            pUnk = pPlusWrap->GetIDispatch();
            if (pUnk)
                FetchedIpType = ComIpType_Dispatch;
        }

        // If the ObjectRef doesn't support IDispatch and the caller also accepts
        // an IUnknown pointer, then check for IUnknown.
        if (!pUnk && (ReqIpType & ComIpType_Unknown))
        {
            pUnk = pPlusWrap->GetIUnknown();
            if (pUnk)
                FetchedIpType = ComIpType_Unknown;
        }
    }

LExit:
    // If we failed to retrieve an IP then throw an exception.
    if (pUnk == NULL)
        COMPlusThrowHR(hr);

    // If the caller wants to know the fetched IP type, then set pFetchedIpType
    // to the type of the IP.
    if (pFetchedIpType)
        *pFetchedIpType = FetchedIpType;

    return pUnk;
}


//+----------------------------------------------------------------------------
// IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, REFIID iid);
// convert ComIP to an ObjectRef, based on riid
//+----------------------------------------------------------------------------
IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, REFIID iid)
{
    THROWSCOMPLUSEXCEPTION();
    _ASSERTE(GetThread()->PreemptiveGCDisabled());
    ASSERT_PROTECTED(poref);

    // COM had better be started up at this point.
    _ASSERTE(g_fComStarted && "COM has not been started up, ensure QuickCOMStartup is called before any COM objects are used!");

    BOOL fReleaseWrapper = false;
    HRESULT hr = E_NOINTERFACE;
    IUnknown* pUnk = NULL;
    size_t ul = 0;
    if (*poref == NULL)
    {
        return NULL;
    }

    if ((*poref)->GetMethodTable()->IsTransparentProxyType()) 
    {
       return GetIUnknownForTransparentProxy(poref, FALSE);
    }

    SyncBlock* pBlock = (*poref)->GetSyncBlockSpecial();
    if (pBlock == NULL)
        goto LExit;

    ul = (size_t)pBlock->GetComVoidPtr();

    ComPlusWrapper* pPlusWrap;

    // Com+ to COM Wrappers always have non-null com data in the sync block
    // and the low bit of the comdata is set to 1.
    if(ul == 0 || ((ul & 0x1) == 0))
    {
        // create a COM callable wrapper
        // get iunknown from oref
        ComCallWrapper* pWrap = (ComCallWrapper *)ul;
        if (ul == 0)
        {
            pWrap = ComCallWrapper::InlineGetWrapper(poref);
            fReleaseWrapper = true;
        }        

        pUnk = ComCallWrapper::GetComIPfromWrapper(pWrap, iid, NULL, FALSE);

        if (fReleaseWrapper)
            ComCallWrapper::Release(pWrap);
    }
    else
    {
        _ASSERTE(ul != 0);
        _ASSERTE((ul & 0x1) != 0);

        ul^=1;
        pPlusWrap = ((ComPlusWrapper *)ul);

        // Validate that the OBJECTREF is still attached to its ComPlusWrapper.
        if (!pPlusWrap)
            COMPlusThrow(kInvalidComObjectException, IDS_EE_COM_OBJECT_NO_LONGER_HAS_WRAPPER);

        // The interface will be returned addref'ed.
        pUnk = pPlusWrap->GetComIPFromWrapper(iid);
    }
LExit:
    if (pUnk == NULL)
    {
        COMPlusThrowHR(hr);
    }
    return pUnk;
}


//+----------------------------------------------------------------------------
// GetObjectRefFromComIP
// pUnk : input IUnknown
// pMTClass : specifies the type of instance to be returned
// NOTE:**  As per COM Rules, the IUnknown passed is shouldn't be AddRef'ed
//+----------------------------------------------------------------------------
OBJECTREF __stdcall GetObjectRefFromComIP(IUnknown* pUnk, MethodTable* pMTClass, BOOL bClassIsHint)
{

#ifdef CUSTOMER_CHECKED_BUILD
    TAutoItf<IUnknown> pCdhTempUnk = NULL;    
    pCdhTempUnk.InitMsg("Release Customer debug helper temp IUnknown");

    BOOL fValid = FALSE;
    CustomerDebugHelper* pCdh = CustomerDebugHelper::GetCustomerDebugHelper();
    if (pCdh->IsProbeEnabled(CustomerCheckedBuildProbe_InvalidIUnknown))
    {
        if (pUnk)
        {
            try
            {
                // Keep reference to pUnk since we don't know if it is valid yet
                pUnk->AddRef();
                pCdhTempUnk = pUnk;

                // Test pUnk
                HRESULT hr = pUnk->QueryInterface(IID_IUnknown, (void**)&pUnk);
                if (hr == S_OK)
                {
                    pUnk->Release();
                    fValid = TRUE;
                }
            }
            catch (...)
            {
            }

            if (!fValid)
                pCdh->ReportError(L"Invalid IUnknown pointer detected.", CustomerCheckedBuildProbe_InvalidIUnknown);
        }
    }

#endif // CUSTOMER_CHECKED_BUILD

    THROWSCOMPLUSEXCEPTION();

    Thread* pThread = GetThread();
    _ASSERTE(pThread && pThread->PreemptiveGCDisabled());

    // COM had better be started up at this point.
    _ASSERTE(g_fComStarted && "COM has not been started up, ensure QuickCOMStartup is called before any COM objects are used!");

    OBJECTREF oref = NULL;
    OBJECTREF oref2 = NULL;
    
    IUnknown* pOuter = pUnk;
    
    GCPROTECT_BEGIN(oref)
    {               
        TAutoItf<IUnknown> pAutoOuterUnk = NULL;    
        pAutoOuterUnk.InitMsg("Release Outer Unknown");

        if (pUnk != NULL)
        {
            // get CCW for IUnknown
            ComCallWrapper* pWrap = GetCCWFromIUnknown(pUnk);
            if (pWrap == NULL)
            {
                
                // could be aggregated scenario
                HRESULT hr = SafeQueryInterface(pUnk, IID_IUnknown, &pOuter);
                LogInteropQI(pUnk, IID_IUnknown, hr, "QI for Outer");
                _ASSERTE(hr == S_OK);               
                // store the outer in the auto pointer
                pAutoOuterUnk = pOuter; 
                pWrap = GetCCWFromIUnknown(pOuter);
            }

            
            if(pWrap != NULL)
            {   // our tear-off
                _ASSERTE(pWrap != NULL);
                AppDomain* pCurrDomain = pThread->GetDomain();
                AppDomain* pObjDomain = pWrap->GetDomainSynchronized();
                if (! pObjDomain)
                {
                    // domain has been unloaded
                    COMPlusThrow(kAppDomainUnloadedException);
                }
                else if (pObjDomain == pCurrDomain)
                {
                    oref = pWrap->GetObjectRef();  
                }
                else
                {
                    // the CCW belongs to a different domain..
                    // unmarshal the object to the current domain
                    UnMarshalObjectForCurrentDomain(pObjDomain, pWrap, &oref);
                }
            }
            else
            {
                // Only pass in the class method table to the interface marshaler if 
                // it is a COM import or COM import derived class. 
                MethodTable *pComClassMT = NULL;
                if (pMTClass && pMTClass->IsComObjectType())
                    pComClassMT = pMTClass;

                // Convert the IP to an OBJECTREF.
                COMInterfaceMarshaler marshaler;
                marshaler.Init(pOuter, pComClassMT);
                oref = marshaler.FindOrCreateObjectRef();             
            }
        }

        // release the interface while we oref is GCPROTECTed
        pAutoOuterUnk.SafeReleaseItf();

#ifdef CUSTOMER_CHECKED_BUILD
        if (fValid)
            pCdhTempUnk.SafeReleaseItf();
#endif

        // make sure we can cast to the specified class
        if(oref != NULL && pMTClass != NULL && !bClassIsHint)
        {
            if(!ClassLoader::CanCastToClassOrInterface(oref, pMTClass->GetClass()))
            {
                CQuickBytes _qb;
				WCHAR* wszObjClsName = (WCHAR *)_qb.Alloc(MAX_CLASSNAME_LENGTH * sizeof(CHAR));

                CQuickBytes _qb2;
				WCHAR* wszDestClsName = (WCHAR *)_qb2.Alloc(MAX_CLASSNAME_LENGTH * sizeof(WCHAR));

                oref->GetTrueClass()->_GetFullyQualifiedNameForClass(wszObjClsName, MAX_CLASSNAME_LENGTH);
                pMTClass->GetClass()->_GetFullyQualifiedNameForClass(wszDestClsName, MAX_CLASSNAME_LENGTH);
                COMPlusThrow(kInvalidCastException, IDS_EE_CANNOTCAST, wszObjClsName, wszDestClsName);
            }
        }
        oref2 = oref;
    }    
    GCPROTECT_END();
    return oref2;
}


//--------------------------------------------------------
// ConvertObjectToBSTR
// serializes object to a BSTR, caller needs to SysFree the Bstr
//--------------------------------------------------------------------------------
HRESULT ConvertObjectToBSTR(OBJECTREF oref, BSTR* pBStr)
{
    _ASSERTE(oref != NULL);
    _ASSERTE(pBStr != NULL);

    HRESULT hr = S_OK;
    Thread * pThread = GetThread();

    COMPLUS_TRY
    {
        if (InitializeRemoting())
        {
            MethodDesc* pMD = CRemotingServices::MDofMarshalToBuffer();
            _ASSERTE(pMD != NULL);


            INT64 args[] = {
                ObjToInt64(oref)
            };

            INT64 ret = pMD->Call(args);

            BASEARRAYREF aref = (BASEARRAYREF)Int64ToObj(ret);

            _ASSERTE(!aref->IsMultiDimArray());
            //@todo ASSERTE that the array is a byte array

            ULONG cbSize = aref->GetNumComponents();
            BYTE* pBuf  = (BYTE *)aref->GetDataPtr();

            BSTR bstr = SysAllocStringByteLen(NULL, cbSize);
            if (bstr != NULL)
                CopyMemory(bstr, pBuf, cbSize);

            *pBStr = bstr;
        }
    }
    COMPLUS_CATCH
    {
        // set up ErrorInfo and the get the hresult to return
        hr = SetupErrorInfo(pThread->GetThrowable());
        _ASSERTE(hr != S_OK);
    }
        COMPLUS_END_CATCH

    return hr;
}

//--------------------------------------------------------------------------------
// ConvertBSTRToObject
// deserializes a BSTR, created using ConvertObjectToBSTR, this api SysFree's the BSTR
//--------------------------------------------------------------------------------
OBJECTREF ConvertBSTRToObject(BSTR bstr)
{
    THROWSCOMPLUSEXCEPTION();

    OBJECTREF oref = NULL;
    EE_TRY_FOR_FINALLY
    {
        if (InitializeRemoting())
        {
            MethodDesc* pMD = CRemotingServices::MDofUnmarshalFromBuffer();
            _ASSERTE(pMD != NULL);

            // convert BSTR to a byte array

            // allocate a byte array
            DWORD elementCount = SysStringByteLen(bstr);
            TypeHandle t = OleVariant::GetArrayForVarType(VT_UI1, TypeHandle((MethodTable *)NULL));
            BASEARRAYREF aref = (BASEARRAYREF) AllocateArrayEx(t, &elementCount, 1);
            // copy the bstr data into the managed byte array
            memcpyNoGCRefs(aref->GetDataPtr(), bstr, elementCount);

            INT64 args[] = {
                ObjToInt64((OBJECTREF)aref)
            };

            INT64 ret = pMD->Call(args);

            oref = (OBJECTREF)Int64ToObj(ret);
        }
    }
    EE_FINALLY
    {
        if (bstr != NULL)
        {
            // free up the BSTR
            SysFreeString(bstr);
            bstr = NULL;
        }
    }
    EE_END_FINALLY

    return oref;
}

//--------------------------------------------------------------------------------
// UnMarshalObjectForCurrentDomain
// unmarshal the managed object for the current domain
//--------------------------------------------------------------------------------
struct ConvertObjectToBSTR_Args
{
    OBJECTREF oref;
    BSTR *pBStr;
    HRESULT hr;
};

void ConvertObjectToBSTR_Wrapper(ConvertObjectToBSTR_Args *args)
{
    args->hr = ConvertObjectToBSTR(args->oref, args->pBStr);
}

void UnMarshalObjectForCurrentDomain(AppDomain* pObjDomain, ComCallWrapper* pWrap, OBJECTREF* pResult)
{
    THROWSCOMPLUSEXCEPTION();
    _ASSERTE(pObjDomain != NULL);
    _ASSERTE(pWrap != NULL);

    Thread* pThread = GetThread();
    _ASSERTE(pThread);

    AppDomain* pCurrDomain = pThread->GetDomain();

    _ASSERTE(pCurrDomain != NULL);
    _ASSERTE(pCurrDomain != pObjDomain);

    BSTR bstr = NULL;
    ConvertObjectToBSTR_Args args;
    args.oref = pWrap->GetObjectRef();
    args.pBStr = &bstr;
    args.hr = S_OK;

    GCPROTECT_BEGIN(args.oref);
    pThread->DoADCallBack(pObjDomain->GetDefaultContext(), ConvertObjectToBSTR_Wrapper, &args);
    GCPROTECT_END();

    // if marshalling succeeded
    if (args.hr != S_OK)
        *pResult = NULL;
    else {
        _ASSERTE(bstr != NULL);
        *pResult = ConvertBSTRToObject(bstr);
    }
}

struct MshlPacket
{
    DWORD size;
};

//--------------------------------------------------------------------------------
// DWORD DCOMGetMarshalSizeMax(IUnknown* pUnk)
//--------------------------------------------------------------------------------
signed DCOMGetMarshalSizeMax(IUnknown* pUnk)
{
    _ASSERTE(pUnk != NULL);
    signed size = -1;


    Thread* pThread = GetThread();
    BOOL fGCDisabled = pThread->PreemptiveGCDisabled();
    if (fGCDisabled)
        pThread->EnablePreemptiveGC();

    IMarshal* pMarsh = 0;
    HRESULT hr = SafeQueryInterface(pUnk, IID_IMarshal, (IUnknown **)&pMarsh);
    LogInteropQI(pUnk, IID_IMarshal, hr, "QI For IMarshal");


    if (hr == S_OK)
    {
        hr = pMarsh->GetMarshalSizeMax(IID_IUnknown, pUnk, MSHCTX_DIFFERENTMACHINE,
                                                NULL, MSHLFLAGS_NORMAL, (unsigned long *)&size);
        size+= sizeof(MshlPacket);

        if (hr != S_OK)
        {
            size = -1;
        }
    }
    if (pMarsh)
    {
        ULONG cbRef = SafeRelease(pMarsh);
        LogInteropRelease(pUnk,cbRef, "Release IMarshal");
    }

    if (fGCDisabled)
        pThread->DisablePreemptiveGC();

    return size;
}

//--------------------------------------------------------------------------------
// IUnknown* __InternalDCOMUnmarshalFromBuffer(BYTE *pMarshBuf)
// unmarshal the passed in buffer and return an IUnknown
// this has to be a buffer created using InternalDCOMMarshalToBuffer
//--------------------------------------------------------------------------------
IUnknown* __InternalDCOMUnmarshalFromBuffer(BYTE *pMarshBuf)
{
    _ASSERTE(pMarshBuf != NULL);

    IUnknown* pUnk  = NULL;
    IStream* pStm = NULL;

    MshlPacket* packet = (MshlPacket*)pMarshBuf;
    BYTE *pBuf = (BYTE *)(packet+1);

    Thread* pThread = GetThread();
    _ASSERTE(pThread != NULL);

    BOOL fGCDisabled = pThread->PreemptiveGCDisabled();

    if (fGCDisabled)
        pThread->EnablePreemptiveGC();

    pStm =  CreateMemStm(packet->size, NULL);
    if (pStm)
    {
        // copy the buffer into the stream
        DWORD cbWritten;
        HRESULT hr = pStm->Write(pBuf, packet->size, &cbWritten);
        _ASSERTE(hr == S_OK);
        _ASSERTE(cbWritten == packet->size);

        // reset the stream
        LARGE_INTEGER li;
        LISet32(li, 0);
        pStm->Seek(li, STREAM_SEEK_SET, NULL);

        // unmarshal the pointer from the stream
        hr = CoUnmarshalInterface(pStm, IID_IUnknown, (void **)&pUnk);

    }

    if (fGCDisabled)
        pThread->DisablePreemptiveGC();

    if (pStm)
    {
        DWORD cbRef = SafeRelease(pStm);
        LogInteropRelease(pStm, cbRef, "Release IStreamOnHGlobal");
    }

    return pUnk;
}

//--------------------------------------------------------------------------------
// VOID DCOMMarshalToBuffer(IUnknown* pUnk)
// use DCOMUnmarshalFromBufffer API to unmarshal this buffer
//--------------------------------------------------------------------------------
HRESULT DCOMMarshalToBuffer(IUnknown* pUnk, DWORD cb, BASEARRAYREF* paref)
{

    Thread* pThread = GetThread();
    BOOL fGCDisabled = pThread->PreemptiveGCDisabled();
    if (fGCDisabled)
        pThread->EnablePreemptiveGC();

    BYTE* pMem;
    IStream* pStm =  CreateMemStm(cb, &pMem);
    HRESULT hr =  S_OK;
    if (pStm != NULL)
    {
        MshlPacket packet;
        packet.size = cb - sizeof(MshlPacket);
        _ASSERTE((cb - sizeof(MshlPacket)) > 0);

        // write marshal packet into the stream

        DWORD cbWritten;
        hr = pStm->Write(&packet, sizeof(MshlPacket), &cbWritten);
        _ASSERTE(hr == S_OK);
        _ASSERTE(cbWritten == sizeof(MshlPacket));

        // marshal the object into the stream
        hr = CoMarshalInterface(pStm,IID_IUnknown, pUnk, MSHCTX_DIFFERENTMACHINE,
                                                NULL, MSHLFLAGS_NORMAL);
        if (hr == S_OK)
        {
            // copy the buffer
            _ASSERTE(pMem != NULL);
            if (hr == S_OK)
            {
                // disable GC as we are going to
                // copy into the managed array
                pThread->DisablePreemptiveGC();

                // get the data portion of the array
                BYTE* pBuf = (*paref)->GetDataPtr();
                memcpyNoGCRefs(pBuf, pMem, cb);
                pThread->EnablePreemptiveGC();
            }
        }
    }
    // release the interfaces

    if (pStm)
    {
        ULONG cbRef = SafeRelease(pStm);
        LogInteropRelease(pStm,cbRef, "Release GlobalIStream");
    }

    if (fGCDisabled)
        pThread->DisablePreemptiveGC();

    return hr;
}

//--------------------------------------------------------------------------------
// IUnknown* DCOMUnmarshalFromBuffer(BASEARRAYREF aref)
// unmarshal the passed in buffer and return an IUnknown
// this has to be a buffer created using InternalDCOMMarshalToBuffer
//--------------------------------------------------------------------------------
IUnknown* DCOMUnmarshalFromBuffer(BASEARRAYREF aref)
{
    IUnknown* pUnk = NULL;
    _ASSERTE(!aref->IsMultiDimArray());
    //@todo ASSERTE that the array is a byte array

    // grab the byte array and copy it to _alloca space
    MshlPacket* packet = (MshlPacket*)aref->GetDataPtr();
    DWORD totSize = packet->size + sizeof(MshlPacket);

    CQuickBytes qb;
    BYTE *pBuf = (BYTE *)qb.Alloc(totSize);

    CopyMemory(pBuf, packet, totSize);

    // use this unmanaged buffer to unmarshal the interface
    pUnk = __InternalDCOMUnmarshalFromBuffer(pBuf);

    return pUnk;
}


//--------------------------------------------------------------------------------
// ComPlusWrapper* GetComPlusWrapperOverDCOMForManaged(OBJECTREF oref)
//--------------------------------------------------------------------------------
ComPlusWrapper* GetComPlusWrapperOverDCOMForManaged(OBJECTREF oref)
{
    THROWSCOMPLUSEXCEPTION();
    ComPlusWrapper* pWrap = NULL;    

    GCPROTECT_BEGIN(oref)
    {
        _ASSERTE(oref != NULL);
        MethodTable* pMT = oref->GetTrueMethodTable();
        _ASSERTE(pMT != NULL);
        
        static MethodDesc* pMDGetDCOMBuffer = NULL;

        if (pMDGetDCOMBuffer == NULL)
        {
            pMDGetDCOMBuffer = pMT->GetClass()->FindMethod("GetDCOMBuffer", &gsig_IM_RetArrByte);
        }
        _ASSERTE(pMDGetDCOMBuffer != NULL);

        
        INT64 args[] = {
            ObjToInt64(oref)
        };

        //@todo FIX THIS
        INT64 ret = pMDGetDCOMBuffer->CallTransparentProxy(args);

        BASEARRAYREF aref = (BASEARRAYREF)Int64ToObj(ret);

        // use this unmanaged buffer to unmarshal the interface
        TAutoItf<IUnknown> pAutoUnk = DCOMUnmarshalFromBuffer(aref);
        pAutoUnk.InitMsg("Release DCOM Unmarshal Unknown");

        if ((IUnknown *)pAutoUnk != NULL)
        {
            // setup the wrapper for this IUnknown and the object
            pWrap = ComPlusWrapperCache::GetComPlusWrapperCache()->SetupComPlusWrapperForRemoteObject(pAutoUnk, oref);        
        }
    }    
    GCPROTECT_END();
    
    return pWrap;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\icecap.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
//*****************************************************************************
// Icecap.cpp
//
//*****************************************************************************
#include "common.h"
#include "Icecap.h"
#include "Winwrap.h"
#include "utsem.h"
#include "ProfilePriv.h"


//********** Types. ***********************************************************

#define ICECAP_NAME L"icecap.dll"

// reserver enough room for 1,000,000 methods to get tracked.
const ULONG MaxRangeSize = (((1000000 - 1) & ~(4096 - 1)) + 4096);

const int DEFAULT_GROWTH_INC = 1000;


// The array of map tables are used to find range in the ID Range used
// for the heap.
struct ICECAP_MAP_TABLE
{
    void        *pHeap;                 // Heap base pointer.
    UINT_PTR    Slots;                  // How many slots for this heap.
#ifdef _DEBUG
    UINT_PTR    cbRange;                // How big is the range.
#endif
};


/*
extern "C" BOOL _declspec(dllexport) _stdcall  
EmitModuleLoadRecord(void *pImageBase, DWORD dwImageSize, LPCSTR szModulePath);

extern "C" BOOL _declspec(dllexport) _stdcall 
EmitModuleUnoadRecord(void *pImageBase, DWORD dwImageSize)
*/

extern "C"
{
typedef BOOL (__stdcall *PFN_EMITMODULELOADRECORD)(void *pImageBaes, DWORD dwImageSize, LPCSTR szModulePath);
typedef BOOL (__stdcall *PFN_EMITMODULEUNLOADRECORD)(void *pImageBaes, DWORD dwImageSize);
}


//********** Locals. **********************************************************
void SetIcecapStubbedHelpers();


//********** Globals. *********************************************************
static HINSTANCE g_hIcecap = 0;         // Loaded instance.
static PFN_EMITMODULELOADRECORD g_pfnEmitLoad = 0;
static PFN_EMITMODULEUNLOADRECORD g_pfnEmitUnload = 0;



#ifdef _DEBUG
#define TBL_ENTRY(name) name, 0, "_CAP_" # name
#else
#define TBL_ENTRY(name) 0, "_CAP_" # name
#endif

ICECAP_FUNCS IcecapFuncs[NUM_ICECAP_PROBES] = 
{
// /fastcap
    TBL_ENTRY(Start_Profiling       ),
    TBL_ENTRY(End_Profiling         ),
// /callcap
    TBL_ENTRY(Enter_Function        ),
    TBL_ENTRY(Exit_Function         ),
// Helper methods
    TBL_ENTRY(Profiling             ),
};





//********** Code. ************************************************************


class CIcecapMapTable : public CDynArray<ICECAP_MAP_TABLE> 
{
public:
    CIcecapMapTable::CIcecapMapTable() :
        CDynArray<ICECAP_MAP_TABLE>(DEFAULT_GROWTH_INC)
    { }
};



//*****************************************************************************
// This class is used to track the allocated range and the map table.
// The ID range is reserved virtual memory, which is never actually
// committed.  This keeps working set size reasonable, while giving us a 
// range that no other apps will get loaded into.
//*****************************************************************************
class IcecapMap
{
public:
    IcecapMap() :
        m_pBase(0), m_cbSize(0), m_SlotMax(0) 
    {}

    ~IcecapMap()
    {
        if (m_pBase)
            VERIFY(VirtualFree(m_pBase, 0, MEM_RELEASE));
        m_pBase = 0;
        m_cbSize = 0;
        m_SlotMax = 0;
        m_rgTable.Clear();
    }

    // This method reserves a range of method IDs, 1 byte for every method.
    HRESULT Init()
    {
        m_pBase = VirtualAlloc(0, MaxRangeSize, MEM_RESERVE, PAGE_NOACCESS);
        if (!m_pBase)
            return (OutOfMemory());
        m_cbSize = MaxRangeSize;
        return (S_OK);
    }

    // When a new heap is added, put it into the map table reserving the next
    // set of bytes (1 per possible method).
    HRESULT AllocateRangeForHeap(void *pHeap, int MaxFunctions, UINT_PTR cbRange)
    {
        ICECAP_MAP_TABLE *pTable = 0;
        int         i;
        HRESULT     hr = S_OK;

        m_Lock.LockWrite();

        // Not perfect, but I've found most allocations go up and therefore
        // they wind up at the end of the table. So take that quick out.
        i = m_rgTable.Count();
        if (i == 0 || pHeap > m_rgTable[i - 1].pHeap)
        {
            pTable = m_rgTable.Append();
        }
        else
        {
            // Loop through the heap table looking for a location.  I don't expect
            // this table to get large enough to justify a b-search for an insert location.
            for (i=0;  i<m_rgTable.Count();  i++)
            {
                if (pHeap < m_rgTable[i].pHeap)
                {
                    pTable = m_rgTable.Insert(i);
                    break;
                }
            }
        }

        if (!pTable)
        {
            hr = OutOfMemory();
            goto ErrExit;
        }

        pTable->pHeap = pHeap;
        pTable->Slots = m_SlotMax;
        DEBUG_STMT(pTable->cbRange = cbRange);
        m_SlotMax += MaxFunctions;
        _ASSERTE(m_SlotMax < MaxRangeSize && "Out of room for icecap profiling range");
        
    ErrExit:
        m_Lock.UnlockWrite();
        return (hr);
    }

    // Remove this particular range from the list.
    void FreeRangeForHeap(void *pHeap)
    {
        m_Lock.LockWrite();
        int i = _GetIndexForPointer(pHeap);
        m_rgTable.Delete(i);
        m_Lock.UnlockWrite();
    }

    // Map an entry to its ID range value.
    UINT_PTR GetProfilingHandle(            // Return a profiling handle.
        MethodDesc  *pMD)                   // The method handle to get ID for.
    {
        m_Lock.LockRead();

        // Get the index in the mapping table for this entry.
        int iMapIndex = _GetIndexForPointer(pMD);
        _ASSERTE(iMapIndex != -1);

        // Get the zero based index of the MethodDesc.
        _ASSERTE((UINT_PTR) pMD >= (UINT_PTR) m_rgTable[iMapIndex].pHeap);
        int iMethodIndex = ((BYTE *) pMD - (BYTE *) m_rgTable[iMapIndex].pHeap) / sizeof(MethodDesc);

        // The ID is the base address for the method range + the slot offset for this
        // heap range + the 0 based index of this item in the slot range.
        UINT_PTR id = (UINT_PTR) m_pBase + m_rgTable[iMapIndex].Slots + iMethodIndex;
        LOG((LF_CORPROF, LL_INFO10000, "**PROF: MethodDesc %08x maps to ID %08x (%s/%s)\n", pMD, id, 
            (pMD->m_pszDebugClassName) ? pMD->m_pszDebugClassName : "<null>", 
            pMD->m_pszDebugMethodName));
        
        m_Lock.UnlockRead();
        return (id);
    }

    // binary search the map table looking for the correct entry.
    int _GetIndexForPointer(void *pItem)
    {
        int iMid, iLow, iHigh;
        iLow = 0;
        iHigh = m_rgTable.Count() - 1;
        while (iLow <= iHigh)
        {
            iMid = (iLow + iHigh) / 2;
            void *p = m_rgTable[iMid].pHeap;

            // If the item is in this range, then we've found it.
            if (pItem >= p)
            {
                if ((iMid < m_rgTable.Count() && pItem < m_rgTable[iMid + 1].pHeap) ||
                    iMid == m_rgTable.Count() - 1)
                {
                    // Sanity check the item is really between the heap start and end.
                    _ASSERTE((UINT_PTR) pItem < (UINT_PTR) m_rgTable[iMid].pHeap + m_rgTable[iMid].cbRange);
                    return (iMid);
                }
            }
            if (pItem < p)
                iHigh = iMid - 1;
            else
                iLow = iMid + 1;
        }
        _ASSERTE(0 && "Didn't find a range for MD, very bad.");
        return (-1);
    }
    
public:
    void        *m_pBase;               // The ID range base.
    UINT_PTR    m_cbSize;               // How big is the range.
    UINT_PTR    m_SlotMax;              // Current slot max.
    CIcecapMapTable m_rgTable;          // The mapping table into this range.
    UTSemReadWrite m_Lock;              // Mutual exclusion on heap map table.
};

static IcecapMap *g_pIcecapRange = 0;


//*****************************************************************************
// Load icecap.dll and get the address of the probes and helpers we will 
// be calling.
//*****************************************************************************
HRESULT IcecapProbes::LoadIcecap()
{
    int         i;
    HRESULT     hr = S_OK;
    
#if defined(_DEBUG)
    {
        for (int i=0;  i<NUM_ICECAP_PROBES;  i++)
            _ASSERTE(IcecapFuncs[i].id == i);
    }
#endif

    // Load the icecap probe library into this process.
    if (g_hIcecap)
        return (S_OK);

    Thread  *thread = GetThread();
    BOOL     toggleGC = (thread && thread->PreemptiveGCDisabled());

    if (toggleGC)
        thread->EnablePreemptiveGC();

    g_hIcecap = WszLoadLibrary(ICECAP_NAME);
    if (!g_hIcecap)
    {
        WCHAR       rcPath[1024];
        WCHAR       rcMsg[1280];

        // Save off the return error.
        hr = HRESULT_FROM_WIN32(GetLastError());

        // Tell the user what happened.
        if (!WszGetEnvironmentVariable(L"path", rcPath, NumItems(rcPath)))
            wcscpy(rcPath, L"<error>");
        swprintf(rcMsg, L"Could not find icecap.dll on path:\n%s", rcPath);
        WszMessageBox(GetDesktopWindow(), rcMsg,
            L"CLR Icecap Integration", MB_OK | MB_ICONEXCLAMATION | MB_SYSTEMMODAL);

        LOG((LF_CORPROF, LL_INFO10, "**PROF: Failed to load icecap.dll: %08x.\n", hr));
        goto ErrExit;
    }
    LOG((LF_CORPROF, LL_INFO10, "**PROF: Loaded icecap.dll.\n", hr));

    // Get the address of each helper method.
    for (i=0;  i<NUM_ICECAP_PROBES;  i++)
    {
        IcecapFuncs[i].pfn = (UINT_PTR) GetProcAddress(g_hIcecap, IcecapFuncs[i].szFunction);
        if (!IcecapFuncs[i].pfn)
        {
            hr = HRESULT_FROM_WIN32(GetLastError());
            LOG((LF_CORPROF, LL_INFO10, "**PROF: Failed get icecap probe %s, %d, %8x\n", IcecapFuncs[i].szFunction, i, hr));
            goto ErrExit;
        }
    }

    // Get the module entry points.
    if ((g_pfnEmitLoad = (PFN_EMITMODULELOADRECORD) GetProcAddress(g_hIcecap, "EmitModuleLoadRecord")) == 0 ||
        (g_pfnEmitUnload = (PFN_EMITMODULEUNLOADRECORD) GetProcAddress(g_hIcecap, "EmitModuleUnloadRecord")) == 0)
    {
        hr = HRESULT_FROM_WIN32(GetLastError());
        LOG((LF_CORPROF, LL_INFO10, "**PROF: Failed GetProcAddress in icecap %8x\n", hr));
        goto ErrExit;
    }

    // Allocate the mapping data structure.
    g_pIcecapRange = new IcecapMap;
    if (!g_pIcecapRange)
    {
        hr = OutOfMemory();
        goto ErrExit;
    }
    hr = g_pIcecapRange->Init();

    // Emit a load record for the ID range.
    {
        WCHAR   rcExeName[_MAX_PATH];

        // Get the output file name and convert it for use in the icecap api.
        GetIcecapProfileOutFile(rcExeName);
        MAKE_UTF8_FROM_WIDE(rcname, rcExeName);
        
        // Tell the Icecap API about our fake module.
        BOOL bRtn = (*g_pfnEmitLoad)(g_pIcecapRange->m_pBase, g_pIcecapRange->m_cbSize, rcname);
        _ASSERTE(bRtn);
        LOG((LF_CORPROF, LL_INFO10, "**PROF: Emitted module load record for base %08x of size %08x with name '%s'\n",
                    g_pIcecapRange->m_pBase, g_pIcecapRange->m_cbSize, rcname));
    }

    // Init the jit helper table to have these probe values.  The JIT will
    // access the data by calling getHelperFtn().
    SetIcecapStubbedHelpers();

ErrExit:
    if (FAILED(hr))
        UnloadIcecap();

    if (toggleGC)
        thread->DisablePreemptiveGC();

    return (hr);
}


//*****************************************************************************
// Unload the icecap dll and zero out entry points.
//*****************************************************************************
void IcecapProbes::UnloadIcecap()
{
    // Free the loaded library.
    FreeLibrary(g_hIcecap);
    g_hIcecap = 0;
    for (int i=0;  i<NUM_ICECAP_PROBES;  i++)
        IcecapFuncs[i].pfn = 0;

    // Free the map data if allocated.
    if (g_pIcecapRange)
        delete g_pIcecapRange;
    g_pIcecapRange = 0;

    LOG((LF_CORPROF, LL_INFO10, "**PROF: icecap.dll unloaded.\n"));
}



//*****************************************************************************
// Call this whenever a new heap is allocated for tracking MethodDesc items.
// This must be tracked in order for the profiling handle map to get updated.
//*****************************************************************************
void IcecapProbes::OnNewMethodDescHeap(
    void        *pbHeap,                // Base address for MD heap.
    int         iMaxEntries,            // How many max items are in the heap.
    UINT_PTR    cbRange)                // For debug, validate ptrs.
{
    _ASSERTE(g_pIcecapRange);
    g_pIcecapRange->AllocateRangeForHeap(pbHeap, iMaxEntries, cbRange);
    LOG((LF_CORPROF, LL_INFO10, "**PROF: New heap range of MethodDescs: heap=%08x, entries=%d\n", pbHeap, iMaxEntries));
}


//*****************************************************************************
// Call this whenever a heap is destroyed.  It will get taken out of the list
// of heap elements.
//*****************************************************************************
void IcecapProbes::OnDestroyMethodDescHeap(
    void        *pbHeap)                // Base address for deleted heap.
{
    _ASSERTE(g_pIcecapRange);
    g_pIcecapRange->FreeRangeForHeap(pbHeap);
}


//*****************************************************************************
// Given a method, return a unique value that can be passed into Icecap probes.
// This value must be unique in a process so that the icecap report tool can
// correlate it back to a symbol name.  The value used is either the native
// IP for native code (N/Direct or ECall), or a value out of the icecap function
// map.
//*****************************************************************************
UINT_PTR IcecapProbes::GetProfilingHandle(  // Return a profiling handle.
    MethodDesc  *pMD)                   // The method handle to get ID for.
{
    _ASSERTE(g_pIcecapRange);
    UINT_PTR ptr = g_pIcecapRange->GetProfilingHandle(pMD);
    return (ptr);
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\interopconverter.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#ifndef _H_INTEROPCONVERTER_
#define _H_INTEROPCONVERTER_

#include "DebugMacros.h"

//
// THE FOLLOWING ARE THE MAIN APIS USED BY EVERY ONE TO CONVERT BETWEEN
// OBJECTREF AND COM IP

//--------------------------------------------------------------------------------
// GetIUnknownForMarshalByRefInServerDomain
// setup a CCW for Transparent proxy/marshalbyref in the server domain
// either the object is in-proc & the domains match, or its out-of proc
// and we don't care about appdomains
IUnknown* GetIUnknownForMarshalByRefInServerDomain(OBJECTREF* poref);

//--------------------------------------------------------------------------------
// GetIUnknownForTransparentProxy
// delegates the call to the managed implementation in the real proxy

IUnknown* GetIUnknownForTransparentProxy(OBJECTREF* poref, BOOL fIsBeingMarshalled);


//--------------------------------------------------------------------------------
// The type of COM IP to convert the OBJECTREF to.
enum ComIpType
{
    ComIpType_None          = 0x0,
    ComIpType_Unknown       = 0x1,
    ComIpType_Dispatch      = 0x2,
    ComIpType_Both          = 0x3
};


//--------------------------------------------------------------------------------
// IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, MethodTable* pMT);
// Convert ObjectRef to a COM IP, based on MethodTable* pMT.
IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, MethodTable* pMT);


//--------------------------------------------------------------------------------
// IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, MethodTable* pMT);
// Convert ObjectRef to a COM IP of the requested type.
IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, ComIpType ReqIpType, ComIpType* pFetchedIpType);


//--------------------------------------------------------------------------------
// IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, REFIID iid);
// Convert ObjectRef to a COM IP, based on riid.
IUnknown* __stdcall GetComIPFromObjectRef(OBJECTREF* poref, REFIID iid);


//--------------------------------------------------------------------------------
// OBJECTREF __stdcall GetObjectRefFromComIP(IUnknown* pUnk, MethodTable* pMT, 
//                                          MethodTable* pMTClass)
// Convert a COM IP to an ObjectRef.
// pUnk : input IUnknown
// pMTClass : used esp. of COM Interfaces that are not tear-offs and is used to wrap the 
//            Com interface with an appropriate class
// bClassIsHint : A flag indicating if the OBJECTREF must be of the specified type
//                or if the pMTClass is just a hint
// NOTE:** the passed in IUnknown shouldn't be AddRef'ed, if we hold a reference
// to it this function will make the extra AddRef
OBJECTREF __stdcall GetObjectRefFromComIP(IUnknown* pUnk, MethodTable* pMTClass = NULL, BOOL bClassIsHint = FALSE);


//--------------------------------------------------------
// managed serialization helpers
//--------------------------------------------------------
// ConvertObjectToBSTR
// serializes object to a BSTR, caller needs to SysFree the Bstr
HRESULT ConvertObjectToBSTR(OBJECTREF oref, BSTR* pBStr);


//--------------------------------------------------------------------------------
// ConvertBSTRToObject
// deserializes a BSTR, created using ConvertObjectToBSTR, this api SysFree's the BSTR
OBJECTREF ConvertBSTRToObject(BSTR bstr);


//--------------------------------------------------------------------------------
// UnMarshalObjectForCurrentDomain
// unmarshal the managed object for the current domain
void UnMarshalObjectForCurrentDomain(AppDomain* pObjDomain, ComCallWrapper* pWrap, OBJECTREF* pResult);


//--------------------------------------------------------
// DCOM marshalling helpers 
// used by ecall methods of MarshalByRefObject class 
//--------------------------------------------------------
signed  DCOMGetMarshalSizeMax(IUnknown* pUnk);
HRESULT DCOMMarshalToBuffer(IUnknown* pUnk, DWORD cb, BASEARRAYREF* paref);
IUnknown* DCOMUnmarshalFromBuffer(BASEARRAYREF aref);

#endif // #ifndef _H_INTEROPCONVERTER_
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\icecap.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
//*****************************************************************************
// Icecap.h
//
// Icecap is a call attributed profiler used internally at Microsoft.  The
// tool requires calls to certain probe methods in icecap.dll.  These probes
// gather the caller and callee ID's and track the time inbetween.  A report
// tool summarizes the data for the user.
//
// In COM+, IL can be compiled to multiple locations with code pitching.  So
// we need to come up with a unique per-process ID for each method.  Additionally,
// Icecap requires the ID's their probes get to be associatd with a loaded
// module in the process space.  For IL, we can call the EmitModuleLoadRecord
// method added just for us with the name of the symbol file.  But we need to
// have a memory range that never changes.  The MethodDesc addresses are
// used to come up with this value in the following way:
//
// MethodDesc Heap				Map Table			ID Range
// +------------------------+   +---------------+   +--------------+
// | f1, f2, f3, ...        |   | heap1 | slots |   |xxxxxxxxxxxxxx|
// +------------------------+   | heap2 | slots |   |xxxxxxxxxxxxxx|
//                              +---------------+   |xxxxxxxxxxxxxx|
// +------------------------+                       |xxxxxxxxxxxxxx|
// | x1, x2, x3, ...        |                       +--------------+
// +------------------------+
//
// The ID Range is reserved memory up front, which gives us a set of addresses
// that will never move.  These can be fed into Icecap, with the
// corresponding values given in the symbol file on shutdown.
//
// To map a MethodDesc:
//	1. b-search the Map Table looking for the heap it lives in, mt_index
//	2. let md_index = the 0 based index in the MethodDesc heap of the pMD
//	3. Add the base address of the ID Range to md_index and rgMapTable[mt_index].slots
//
// This hashes to a unique value for the MD very quickly in the single range
// in the process (another requirement from icecap), but still allows the MethodDesc
// heap to span multiple ranges when it overflows.
//
//*****************************************************************************
#ifndef __Icecap_h__
#define __Icecap_h__

#include "EEProfInterfaces.h"


enum IcecapMethodID
{
// /fastcap probes for wrapping a function call.
	Start_Profiling,
	End_Profiling,
// /callcap probes for function prologue/epilogue hooks.
	Enter_Function,
	Exit_Function,
// Helper methods.
	Profiling,
	NUM_ICECAP_PROBES
};


struct ICECAP_FUNCS
{
#ifdef _DEBUG
	IcecapMethodID id;					// Check enum to array.
#endif
	UINT_PTR	pfn;					// Entry point for this method.
	LPCSTR		szFunction;				// Name of function.
};
extern ICECAP_FUNCS IcecapFuncs[NUM_ICECAP_PROBES];


inline UINT_PTR GetIcecapMethod(IcecapMethodID type)
{
	_ASSERTE(IcecapFuncs[type].pfn);
	return IcecapFuncs[type].pfn;
}


struct IcecapProbes
{
//*****************************************************************************
// Load icecap.dll and get the address of the probes and helpers we will 
// be calling.
//*****************************************************************************
	static HRESULT LoadIcecap();

//*****************************************************************************
// Unload the icecap dll and zero out entry points.
//*****************************************************************************
	static void UnloadIcecap();

//*****************************************************************************
// Call this whenever a new heap is allocated for tracking MethodDesc items.
// This must be tracked in order for the profiling handle map to get updated.
//*****************************************************************************
	static void OnNewMethodDescHeap(
		void		*pbHeap,				// Base address for MD heap.
		int			iMaxEntries,			// How many max items are in the heap.
		UINT_PTR	cbRange);				// For debug, validate ptrs.

//*****************************************************************************
// Call this whenever a heap is destroyed.  It will get taken out of the list
// of heap elements.
//*****************************************************************************
	static void OnDestroyMethodDescHeap(
		void		*pbHeap);				// Base address for deleted heap.

//*****************************************************************************
// Given a method, return a unique value that can be passed into Icecap probes.
// This value must be unique in a process so that the icecap report tool can
// correlate it back to a symbol name.  The value used is either the native
// IP for native code (N/Direct or ECall), or a value out of the icecap function
// map.
//*****************************************************************************
	static UINT_PTR GetProfilingHandle(		// Return a profiling handle.
		MethodDesc	*pMD);					// The method handle to get ID for.
};

#endif // __Icecap_h__
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\interoputil.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#include "common.h"
#include "DispEx.h"
#include "vars.hpp"
#include "excep.h"
#include "stdinterfaces.h"
#include "InteropUtil.h"
#include "ComCallWrapper.h"
#include "ComPlusWrapper.h"
#include "cachelinealloc.h"
#include "orefcache.h"
#include "comcall.h"
#include "compluscall.h"
#include "comutilnative.h"
#include "field.h"
#include "guidfromname.h"
#include "COMVariant.h"
#include "OleVariant.h"
#include "eeconfig.h"
#include "mlinfo.h"
#include "ReflectUtil.h"
#include "ReflectWrap.h"
#include "remoting.h"
#include "appdomain.hpp"
#include "comcache.h"
#include "commember.h"
#include "COMReflectionCache.hpp"
#include "PrettyPrintSig.h"
#include "ComMTMemberInfoMap.h"
#include "interopconverter.h"

#ifdef CUSTOMER_CHECKED_BUILD
    #include "CustomerDebugHelper.h"
#endif // CUSTOMER_CHECKED_BUILD

#define INITGUID
#include "oletls.h"
#undef INITGUID


#define STANDARD_DISPID_PREFIX              L"[DISPID"
#define STANDARD_DISPID_PREFIX_LENGTH       7
#define GET_ENUMERATOR_METHOD_NAME          L"GetEnumerator"

extern HRESULT QuickCOMStartup();


//+----------------------------------------------------------------------------
//
//  Method:     InitOLETEB    public
//
//  Synopsis:   Initialized OLETB info
//
//+----------------------------------------------------------------------------
DWORD g_dwOffsetOfReservedForOLEinTEB = 0;
DWORD g_dwOffsetCtxInOLETLS = 0;


BOOL InitOLETEB()
{
    g_dwOffsetOfReservedForOLEinTEB = offsetof(TEB, ReservedForOle);
    g_dwOffsetCtxInOLETLS = offsetof(SOleTlsData, pCurrentCtx);

    return TRUE;
}



// ULONG GetOffsetOfReservedForOLEinTEB()
// HELPER to determine the offset of OLE struct in TEB
ULONG GetOffsetOfReservedForOLEinTEB()
{   
    return g_dwOffsetOfReservedForOLEinTEB;
}

// ULONG GetOffsetOfContextIDinOLETLS()
// HELPER to determine the offset of Context in OLE TLS struct
ULONG GetOffsetOfContextIDinOLETLS()
{
    return g_dwOffsetCtxInOLETLS;
}


// GUID& GetProcessGUID()
// Global process GUID to identify the process
BSTR GetProcessGUID()
{
    // process unique GUID, every process has a unique GUID
    static GUID processGUID = GUID_NULL;
    static BSTR bstrProcessGUID = NULL;
    static WCHAR guidstr[48];

    if (processGUID == GUID_NULL)
    {
        // setup a process unique GUID
        HRESULT hr = CoCreateGuid(&processGUID);
        _ASSERTE(hr == S_OK);
        if (hr != S_OK)
            return NULL;
    }

    if (bstrProcessGUID == NULL)
    {
        int cbLen = GuidToLPWSTR (processGUID, guidstr, 46);
        _ASSERTE(cbLen <= 46); 
        bstrProcessGUID = guidstr;
    }

    return bstrProcessGUID;
}

//-------------------------------------------------------------------
// void FillExceptionData(ExceptionData* pedata, IErrorInfo* pErrInfo)
// Called from DLLMain, to initialize com specific data structures.
//-------------------------------------------------------------------
void FillExceptionData(ExceptionData* pedata, IErrorInfo* pErrInfo)
{
    if (pErrInfo != NULL)
    {
        Thread* pThread = GetThread();
        if (pThread != NULL)
        {
            pThread->EnablePreemptiveGC();
            pErrInfo->GetSource (&pedata->bstrSource);
            pErrInfo->GetDescription (&pedata->bstrDescription);
            pErrInfo->GetHelpFile (&pedata->bstrHelpFile);
            pErrInfo->GetHelpContext (&pedata->dwHelpContext );
            pErrInfo->GetGUID(&pedata->guid);
            ULONG cbRef = SafeRelease(pErrInfo); // release the IErrorInfo interface pointer
            LogInteropRelease(pErrInfo, cbRef, "IErrorInfo");
            pThread->DisablePreemptiveGC();
        }
    }
}

//------------------------------------------------------------------
//  HRESULT SetupErrorInfo(OBJECTREF pThrownObject)
// setup error info for exception object
//
HRESULT SetupErrorInfo(OBJECTREF pThrownObject)
{
    HRESULT hr = E_FAIL;

    GCPROTECT_BEGIN(pThrownObject)
    {
        // Calls to COM up ahead.
        hr = QuickCOMStartup();

        if (SUCCEEDED(hr) && pThrownObject != NULL)
        {

            static int fExposeExceptionsInCOM = 0;
            static int fReadRegistry = 0;

            Thread * pThread  = GetThread();

            if (!fReadRegistry)
            {
                INSTALL_NESTED_EXCEPTION_HANDLER(pThread->GetFrame()); //who knows..
                fExposeExceptionsInCOM = REGUTIL::GetConfigDWORD(L"ExposeExceptionsInCOM", 0);
                UNINSTALL_NESTED_EXCEPTION_HANDLER();
                fReadRegistry = 1;
            }

            if (fExposeExceptionsInCOM&3)
            {
                INSTALL_NESTED_EXCEPTION_HANDLER(pThread->GetFrame()); //who knows..
                CQuickWSTRNoDtor message;
                GetExceptionMessage(pThrownObject, &message);

                if (fExposeExceptionsInCOM&1)
                {
                    PrintToStdOutW(L".NET exception in COM\n");
                    if (message.Size() > 0) 
                        PrintToStdOutW(message.Ptr());
                    else
                        PrintToStdOutW(L"No exception info available");
                }

                if (fExposeExceptionsInCOM&2)
                {
                    BEGIN_ENSURE_PREEMPTIVE_GC();
                    if (message.Size() > 0) 
                        WszMessageBoxInternal(NULL,message.Ptr(),L".NET exception in COM",MB_ICONSTOP|MB_OK);
                    else
                        WszMessageBoxInternal(NULL,L"No exception information available",L".NET exception in COM",MB_ICONSTOP|MB_OK);
                    END_ENSURE_PREEMPTIVE_GC();
                }


                message.Destroy();
                UNINSTALL_NESTED_EXCEPTION_HANDLER();
            }


            IErrorInfo* pErr = NULL;
            COMPLUS_TRY
            {
                pErr = (IErrorInfo *)GetComIPFromObjectRef(&pThrownObject, IID_IErrorInfo);
                Thread * pThread = GetThread();
                BOOL fToggleGC = pThread && pThread->PreemptiveGCDisabled();
                if (fToggleGC)
                {
                    pThread->EnablePreemptiveGC();
                }

                // set the error info object for the exception that was thrown
                SetErrorInfo(0, pErr);

                if (fToggleGC)
                {
                    pThread->DisablePreemptiveGC();
                }

                if (pErr)
                {
                    hr = GetHRFromComPlusErrorInfo(pErr);
                    ULONG cbRef = SafeRelease(pErr);
                    LogInteropRelease(pErr, cbRef, "IErrorInfo");
                }
            }
            COMPLUS_CATCH
            {
            }
            COMPLUS_END_CATCH
        }
    }
    GCPROTECT_END();
    return hr;
}


//-------------------------------------------------------------------
// HRESULT STDMETHODCALLTYPE EEDllCanUnloadNow(void)
// DllCanUnloadNow delegates the call here
//-------------------------------------------------------------------
HRESULT STDMETHODCALLTYPE EEDllCanUnloadNow(void)
{
    /*if (ComCallWrapperCache::GetComCallWrapperCache()->CanShutDown())
    {
        ComCallWrapperCache::GetComCallWrapperCache()->CorShutdown();
    }*/
    //we should never unload unless the process is dying
    return S_FALSE;

}


//---------------------------------------------------------------------------
//      SYNC BLOCK data helpers
// SyncBlock has a void* to represent COM data
// the following helpers are used to distinguish the different types of
// wrappers stored in the sync block data

BOOL IsComPlusWrapper(void *pComData)
{
    size_t l = (size_t)pComData;
    return (!IsComClassFactory(pComData) && (l & 0x1)) ? TRUE : FALSE;
}

BOOL IsComClassFactory(void *pComData)
{
    size_t l = (size_t)pComData;
    return ((l & 0x3) == 0x3) ? TRUE : FALSE;
}

ComPlusWrapper* GetComPlusWrapper(void *pComData)
{
    size_t l = (size_t)pComData;
    if (l & 0x1)
    {
        l^=0x1;
        return (ComPlusWrapper*)l;
    }
    else
    {
        if (pComData != NULL)
        {
            ComCallWrapper* pComWrap = (ComCallWrapper*)pComData;
            return pComWrap->GetComPlusWrapper();
        }
    }
    return 0;
}

VOID LinkWrappers(ComCallWrapper* pComWrap, ComPlusWrapper* pPlusWrap)
{
    _ASSERTE(pComWrap != NULL);
    _ASSERTE(IsComPlusWrapper(pPlusWrap));
    size_t l = (size_t)pPlusWrap;
    l^=0x1;
    pComWrap->SetComPlusWrapper((ComPlusWrapper*)l);
}
//--------------------------------------------------------------------------------
// Cleanup helpers
//--------------------------------------------------------------------------------
void MinorCleanupSyncBlockComData(LPVOID pv)
{
    _ASSERTE(GCHeap::IsGCInProgress() 
        || ( (g_fEEShutDown & ShutDown_SyncBlock) && g_fProcessDetach ));
        
       //@todo
    size_t l = (size_t)pv;
    if( IsComPlusWrapper(pv))
    {
        //COM+ to COM wrapper
        l^=0x1;
        if (l)
            ((ComPlusWrapper*)l)->MinorCleanup();
    }
    else if (!IsComClassFactory(pv))
    {
        ComCallWrapper* pComCallWrap = (ComCallWrapper*) pv;
        if (pComCallWrap)
        {
            // We have to extract the wrapper directly since ComCallWrapper::GetComPlusWrapper() 
            // tries to go to the synchblock to get the start wrapper which it can't do since
            // the object handle's might have been zeroed out.
            unsigned sindex = ComCallWrapper::IsLinked(pComCallWrap) ? 2 : 1;
            ComPlusWrapper* pPlusWrap = ((SimpleComCallWrapper *)pComCallWrap->m_rgpIPtr[sindex])->GetComPlusWrapper();
            if (pPlusWrap)
                pPlusWrap->MinorCleanup();
        }
    }

    // @TODO(DM): Do we need to do anything to in minor cleanup for ComClassFactories ?
}

void CleanupSyncBlockComData(LPVOID pv)
{
    if ((g_fEEShutDown & ShutDown_SyncBlock) && g_fProcessDetach )
        MinorCleanupSyncBlockComData(pv);

    //@todo
    size_t l = (size_t)pv;

    if (IsComClassFactory(pv))
    {
        l^=0x3;
        if (l)
            ComClassFactory::Cleanup((LPVOID)l);
    }
    else
    if(IsComPlusWrapper(pv))
    {
        //COM+ to COM wrapper
        l^=0x1;
        if (l)
            ((ComPlusWrapper*)l)->Cleanup();
    }
    else
        // COM to COM+ wrapper
        ComCallWrapper::Cleanup((ComCallWrapper*) pv);
}

//--------------------------------------------------------------------------------
// Marshalling Helpers
//--------------------------------------------------------------------------------

//Helpers
// Is the tear-off a com+ created tear-off
UINTPTR IsComPlusTearOff(IUnknown* pUnk)
{
    return (*(size_t **)pUnk)[0] == (size_t)Unknown_QueryInterface;
}

// Convert an IUnknown to CCW, returns NULL if the pUnk is not on
// a managed tear-off (OR) if the pUnk is to a managed tear-off that
// has been aggregated
ComCallWrapper* GetCCWFromIUnknown(IUnknown* pUnk)
{
    ComCallWrapper* pWrap = NULL;
    if(  (*(size_t **)pUnk)[0] == (size_t)Unknown_QueryInterface)
    {
        // check if this wrapper is aggregated
        // find out if this is simple tear-off
        if (IsSimpleTearOff(pUnk))
        {
            SimpleComCallWrapper* pSimpleWrap = SimpleComCallWrapper::GetWrapperFromIP(pUnk);
            if (pSimpleWrap->GetOuter() == NULL)
            {   // check for aggregation
                pWrap = SimpleComCallWrapper::GetMainWrapper(pSimpleWrap);
            }
        }
        else
        {   // it must be one of our main wrappers
            pWrap = ComCallWrapper::GetWrapperFromIP(pUnk);
            if (pWrap->GetOuter() != NULL)
            {   // check for aggregation
                pWrap = NULL;
            }
        }
    }
    return pWrap;
}

// is the tear-off represent one of the standard interfaces such as IProvideClassInfo, IErrorInfo etc.
UINTPTR IsSimpleTearOff(IUnknown* pUnk)
{
    return (*(UINTPTR ***)pUnk)[1] == (UINTPTR*)Unknown_AddRefSpecial;
}

// Is the tear-off represent the inner unknown or the original unknown for the object
UINTPTR IsInnerUnknown(IUnknown* pUnk)
{
    return (*(UINTPTR ***)pUnk)[2] == (UINTPTR*)Unknown_ReleaseInner;
}

// HRESULT for COM PLUS created IErrorInfo pointers are accessible
// from the enclosing simple wrapper
HRESULT GetHRFromComPlusErrorInfo(IErrorInfo* pErr)
{
    _ASSERTE(pErr != NULL);
    _ASSERTE(IsComPlusTearOff(pErr));// check for complus created IErrorInfo pointers
    _ASSERTE(IsSimpleTearOff(pErr));

    SimpleComCallWrapper* pSimpleWrap = SimpleComCallWrapper::GetWrapperFromIP(pErr);
    return pSimpleWrap->IErrorInfo_hr();
}


//--------------------------------------------------------------------------------
// BOOL ExtendsComImport(MethodTable* pMT);
// check if the class is OR extends a COM Imported class
BOOL ExtendsComImport(MethodTable* pMT)
{
    // verify the class extends a COM import class
    EEClass * pClass = pMT->GetClass();
    while (pClass !=NULL && !pClass->IsComImport())
    {
        pClass = pClass->GetParentClass();
    }
    return pClass != NULL;
}

// Function pointer to CoGetObjectContext function in ole32
typedef HRESULT (__stdcall *CLSIDFromProgIdFuncPtr)(LPCOLESTR strProgId, LPCLSID pGuid);

//--------------------------------------------------------------------------------
// HRESULT GetCLSIDFromProgID(WCHAR *strProgId, GUID *pGuid);
// Gets the CLSID from the specified Prog ID.
HRESULT GetCLSIDFromProgID(WCHAR *strProgId, GUID *pGuid)
{
    HRESULT     hr = S_OK;
    static BOOL bInitialized = FALSE;
    static CLSIDFromProgIdFuncPtr g_pCLSIDFromProgId = CLSIDFromProgID;

    if (!bInitialized)
    {
        //  We will load the Ole32.DLL and look for CLSIDFromProgIDEx fn.
        HINSTANCE hiole32 = WszGetModuleHandle(L"OLE32.DLL");
        if (hiole32)
        {
            // We got the handle now let's get the address
            void *pProcAddr = GetProcAddress(hiole32, "CLSIDFromProgIDEx");
            if (pProcAddr)
            {
                // CLSIDFromProgIDEx() is available so use that instead of CLSIDFromProgId().
                g_pCLSIDFromProgId = (CLSIDFromProgIdFuncPtr)pProcAddr;
            }
        }
        else
        {
            hr = HRESULT_FROM_WIN32(GetLastError());
            _ASSERTE(!"OLE32.dll not loaded ");
        }

        // Set the flag indicating initalization has occured.
        bInitialized = TRUE;
    }

    if (SUCCEEDED(hr))
        hr = g_pCLSIDFromProgId(strProgId, pGuid);

    return (hr);
}

//--------------------------------------------------------------------------------
// HRESULT SafeQueryInterface(IUnknown* pUnk, REFIID riid, IUnknown** pResUnk)
// QI helper, enables and disables GC during call-outs
HRESULT SafeQueryInterface(IUnknown* pUnk, REFIID riid, IUnknown** pResUnk)
{
    //---------------------------------------------
    // ** enable preemptive GC. before calling QI **

    Thread* pThread = GetThread();
    int fGC = pThread->PreemptiveGCDisabled();

    if (fGC)
        pThread->EnablePreemptiveGC();
    //----------------------------------------------

    HRESULT hr = pUnk->QueryInterface(riid,(void **)pResUnk);

    LOG((LF_INTEROP, LL_EVERYTHING, hr == S_OK ? "QI Succeeded" : "QI Failed")); 

    //---------------------------------------------
    // ** disable preemptive GC. we are back **
    if (fGC)
        pThread->DisablePreemptiveGC();
    //----------------------------------------------

    return hr;
}

//--------------------------------------------------------------------------------
// ULONG SafeAddRef(IUnknown* pUnk)
// AddRef helper, enables and disables GC during call-outs
ULONG SafeAddRef(IUnknown* pUnk)
{
    //---------------------------------------------
    // ** enable preemptive GC. before calling QI **
    ULONG res = ~0;
    if (pUnk != NULL)
    {
        Thread* pThread = GetThread();
        int fGC = pThread->PreemptiveGCDisabled();

        if (fGC)
            pThread->EnablePreemptiveGC();
        //----------------------------------------------

        res = pUnk->AddRef();

        //LogInteropAddRef(pUnk);

        //---------------------------------------------
        // ** disable preemptive GC. we are back **
        if (fGC)
            pThread->DisablePreemptiveGC();
        //----------------------------------------------
    }
    return res;
}


//--------------------------------------------------------------------------------
// ULONG SafeRelease(IUnknown* pUnk)
// Release helper, enables and disables GC during call-outs
ULONG SafeRelease(IUnknown* pUnk)
{
    if (pUnk == NULL)
        return 0;

    ULONG res = 0;
    Thread* pThread = GetThread();

    int fGC = pThread && pThread->PreemptiveGCDisabled();
    if (fGC)
        pThread->EnablePreemptiveGC();

    try
    {
        res = pUnk->Release();
    }
    catch(...)
    {
        LogInterop(L"An exception occured during release");
        LogInteropLeak(pUnk);
    }

    if (fGC)
        pThread->DisablePreemptiveGC();

    return res;
}


//--------------------------------------------------------------------------------
// Ole RPC seems to return an inconsistent SafeArray for arrays created with
// SafeArrayVector(VT_BSTR). OleAut's SafeArrayGetVartype() doesn't notice
// the inconsistency and returns a valid-seeming (but wrong vartype.)
// Our version is more discriminating. This should only be used for
// marshaling scenarios where we can assume unmanaged code permissions
// (and hence are already in a position of trusting unmanaged data.)

HRESULT ClrSafeArrayGetVartype(SAFEARRAY *psa, VARTYPE *pvt)
{
    if (pvt == NULL || psa == NULL)
        return ::SafeArrayGetVartype(psa, pvt);
    USHORT fFeatures = psa->fFeatures;
    USHORT hardwiredType = (fFeatures & (FADF_BSTR|FADF_UNKNOWN|FADF_DISPATCH|FADF_VARIANT));
    if (hardwiredType == FADF_BSTR && psa->cbElements == sizeof(BSTR))
    {
        *pvt = VT_BSTR;
        return S_OK;
    }
    else if (hardwiredType == FADF_UNKNOWN && psa->cbElements == sizeof(IUnknown*))
    {
        *pvt = VT_UNKNOWN;
        return S_OK;
    }
    else if (hardwiredType == FADF_DISPATCH && psa->cbElements == sizeof(IDispatch*))
    {
        *pvt = VT_DISPATCH;
        return S_OK;
    }
    else if (hardwiredType == FADF_VARIANT && psa->cbElements == sizeof(VARIANT))
    {
        *pvt = VT_VARIANT;
        return S_OK;
    }
    else
    {
        return ::SafeArrayGetVartype(psa, pvt);
    }
}

//--------------------------------------------------------------------------------
//void SafeVariantClear(VARIANT* pVar)
// safe variant helpers
void SafeVariantClear(VARIANT* pVar)
{
    if (pVar)
    {
        Thread* pThread = GetThread();
        int bToggleGC = pThread->PreemptiveGCDisabled();
        if (bToggleGC)
            pThread->EnablePreemptiveGC();

        VariantClear(pVar);

        if (bToggleGC)
            pThread->DisablePreemptiveGC();
    }
}

//--------------------------------------------------------------------------------
//void SafeVariantInit(VARIANT* pVar)
// safe variant helpers
void SafeVariantInit(VARIANT* pVar)
{
    if (pVar)
    {
        Thread* pThread = GetThread();
        int bToggleGC = pThread->PreemptiveGCDisabled();
        if (bToggleGC)
            pThread->EnablePreemptiveGC();

        VariantInit(pVar);

        if (bToggleGC)
            pThread->DisablePreemptiveGC();
    }
}

//--------------------------------------------------------------------------------
// // safe VariantChangeType
// Release helper, enables and disables GC during call-outs
HRESULT SafeVariantChangeType(VARIANT* pVarRes, VARIANT* pVarSrc,
                              unsigned short wFlags, VARTYPE vt)
{
    HRESULT hr = S_OK;

    if (pVarRes)
    {
        Thread* pThread = GetThread();
        int bToggleGC = pThread->PreemptiveGCDisabled();
        if (bToggleGC)
            pThread->EnablePreemptiveGC();

        hr = VariantChangeType(pVarRes, pVarSrc, wFlags, vt);

        if (bToggleGC)
            pThread->DisablePreemptiveGC();
    }

    return hr;
}

//--------------------------------------------------------------------------------
//HRESULT SafeDispGetParam(DISPPARAMS* pdispparams, unsigned argNum,
//                          VARTYPE vt, VARIANT* pVar, unsigned int *puArgErr)
// safe variant helpers
HRESULT SafeDispGetParam(DISPPARAMS* pdispparams, unsigned argNum,
                          VARTYPE vt, VARIANT* pVar, unsigned int *puArgErr)
{
    Thread* pThread = GetThread();
    int bToggleGC = pThread->PreemptiveGCDisabled();
    if (bToggleGC)
        pThread->EnablePreemptiveGC();

    HRESULT hr = DispGetParam (pdispparams, argNum, vt, pVar, puArgErr);

    if (bToggleGC)
        pThread->DisablePreemptiveGC();

    return hr;
}


//--------------------------------------------------------------------------------
//HRESULT SafeVariantChangeTypeEx(VARIANT* pVarRes, VARIANT* pVarSrc,
//                          LCID lcid, unsigned short wFlags, VARTYPE vt)
HRESULT SafeVariantChangeTypeEx(VARIANT* pVarRes, VARIANT* pVarSrc,
                          LCID lcid, unsigned short wFlags, VARTYPE vt)
{
    Thread* pThread = GetThread();
    int bToggleGC = pThread->PreemptiveGCDisabled();
    if (bToggleGC)
        pThread->EnablePreemptiveGC();

    HRESULT hr = VariantChangeTypeEx (pVarRes, pVarSrc,lcid,wFlags,vt);

    if (bToggleGC)
        pThread->DisablePreemptiveGC();

    return hr;
}

//--------------------------------------------------------------------------------
// void SafeReleaseStream(IStream *pStream)
void SafeReleaseStream(IStream *pStream)
{
    _ASSERTE(pStream);

    HRESULT hr = CoReleaseMarshalData(pStream);
#ifdef _DEBUG          
    if (!RunningOnWin95())
    {            
        wchar_t      logStr[200];
        swprintf(logStr, L"Object gone: CoReleaseMarshalData returned %x, file %s, line %d\n", hr, __FILE__, __LINE__);
        LogInterop(logStr);
        if (hr != S_OK)
        {
            // Reset the stream to the begining
            LARGE_INTEGER li;
            LISet32(li, 0);
            ULARGE_INTEGER li2;
            pStream->Seek(li, STREAM_SEEK_SET, &li2);
            hr = CoReleaseMarshalData(pStream);
            swprintf(logStr, L"Object gone: CoReleaseMarshalData returned %x, file %s, line %d\n", hr, __FILE__, __LINE__);
            LogInterop(logStr);
        }
    } 
#endif
    ULONG cbRef = SafeRelease(pStream);
    LogInteropRelease(pStream, cbRef, "Release marshal Stream");
}

//------------------------------------------------------------------------------
// INT64 FieldAccessor(FieldDesc* pFD, OBJECTREF oref, INT64 val, BOOL isGetter, UINT8 cbSize)
// helper to access fields from an object
INT64 FieldAccessor(FieldDesc* pFD, OBJECTREF oref, INT64 val, BOOL isGetter, UINT8 cbSize)
{
    INT64 res = 0;
    _ASSERTE(pFD != NULL);
    _ASSERTE(oref != NULL);

    _ASSERTE(cbSize == 1 || cbSize == 2 || cbSize == 4 || cbSize == 8);

    switch (cbSize)
    {
        case 1: if (isGetter)
                    res = pFD->GetValue8(oref);
                else
                    pFD->SetValue8(oref,(INT8)val);
                    break;

        case 2: if (isGetter)
                    res = pFD->GetValue16(oref);
                else
                    pFD->SetValue16(oref,(INT16)val);
                    break;

        case 4: if (isGetter)
                    res = pFD->GetValue32(oref);
                else
                    pFD->SetValue32(oref,(INT32)val);
                    break;

        case 8: if (isGetter)
                    res = pFD->GetValue64(oref);
                else
                    pFD->SetValue64(oref,val);
                    break;
    };

    return res;
}


//------------------------------------------------------------------------------
// BOOL IsInstanceOf(MethodTable *pMT, MethodTable* pParentMT)
BOOL IsInstanceOf(MethodTable *pMT, MethodTable* pParentMT)
{
    _ASSERTE(pMT != NULL);
    _ASSERTE(pParentMT != NULL);

    while (pMT != NULL)
    {
        if (pMT == pParentMT)
            return TRUE;
        pMT = pMT->GetParentMethodTable();
    }
    return FALSE;
}

//---------------------------------------------------------------------------
// BOOL IsIClassX(MethodTable *pMT, REFIID riid, ComMethodTable **ppComMT);
//  is the iid represent an IClassX for this class
BOOL IsIClassX(MethodTable *pMT, REFIID riid, ComMethodTable **ppComMT)
{
    _ASSERTE(pMT != NULL);
    _ASSERTE(ppComMT);
    EEClass* pClass = pMT->GetClass();
    _ASSERTE(pClass != NULL);

    // Walk up the hierarchy starting at the specified method table and compare
    // the IID's of the IClassX's against the specied IID.
    while (pClass != NULL)
    {
        ComMethodTable *pComMT =
            ComCallWrapperTemplate::SetupComMethodTableForClass(pClass->GetMethodTable(), FALSE);
        _ASSERTE(pComMT);

        if (IsEqualIID(riid, pComMT->m_IID))
        {
            *ppComMT = pComMT;
            return TRUE;
        }

        pClass = pClass->GetParentComPlusClass();
    }

    return FALSE;
}

//---------------------------------------------------------------------------
// void CleanupCCWTemplates(LPVOID pWrap);
//  Cleanup com data stored in EEClass
void CleanupCCWTemplate(LPVOID pWrap)
{
    ComCallWrapperTemplate::CleanupComData(pWrap);
}

//---------------------------------------------------------------------------
// void CleanupComclassfac(LPVOID pWrap);
//  Cleanup com data stored in EEClass
void CleanupComclassfac(LPVOID pWrap)
{
    ComClassFactory::Cleanup(pWrap);
}

//---------------------------------------------------------------------------
//  Unloads any com data associated with a class when class is unloaded
void UnloadCCWTemplate(LPVOID pWrap)
{
    CleanupCCWTemplate(pWrap);
}

//---------------------------------------------------------------------------
//  Unloads any com data associated with a class when class is unloaded
void UnloadComclassfac(LPVOID pWrap)
{
    ComClassFactory::Cleanup(pWrap);    
}

//---------------------------------------------------------------------------
// OBJECTREF AllocateComObject_ForManaged(MethodTable* pMT)
//  Cleanup com data stored in EEClass
OBJECTREF AllocateComObject_ForManaged(MethodTable* pMT)
{
    THROWSCOMPLUSEXCEPTION();
    _ASSERTE(pMT != NULL);

     // Calls to COM up ahead.
    if (FAILED(QuickCOMStartup()))
        return NULL;

    _ASSERTE(pMT->IsComObjectType());

    ComClassFactory* pComClsFac = NULL;
    HRESULT hr = ComClassFactory::GetComClassFactory(pMT, &pComClsFac);
    // we need to pass the pMT of the class
    // as the actual class maybe a subclass of the com class
    if (pComClsFac == NULL)
    {
        _ASSERTE(FAILED(hr));
        COMPlusThrowHR(hr);
    }

    return pComClsFac->CreateInstance(pMT, TRUE);
}

//---------------------------------------------------------------------------
// EEClass* GetEEClassForCLSID(REFCLSID rclsid)
//  get/load EEClass for a given clsid
EEClass* GetEEClassForCLSID(REFCLSID rclsid, BOOL* pfAssemblyInReg)
{
    _ASSERTE(SystemDomain::System());
    BaseDomain* pDomain = SystemDomain::GetCurrentDomain();
    _ASSERTE(pDomain);

    // check to see if we have this class cached
    EEClass *pClass = pDomain->LookupClass(rclsid);
    if (pClass == NULL)
    {
        pClass = SystemDomain::LoadCOMClass(rclsid, NULL, FALSE, pfAssemblyInReg);        
        if (pClass != NULL)
        {
            // if the class didn't have a GUID, 
            // then we won't store it in the GUID hash table
            // so check for null guid and force insert
            CVID cvid;
            pClass->GetGuid(&cvid, FALSE);
            if (IsEqualIID(cvid, GUID_NULL))
            {
                pDomain->InsertClassForCLSID(pClass, TRUE);            
            }        
         }
    }
    return pClass;
}


//---------------------------------------------------------------------------
// EEClass* GetEEValueClassForGUID(REFCLSID rclsid)
//  get/load a value class for a given guid
EEClass* GetEEValueClassForGUID(REFCLSID guid)
{
    THROWSCOMPLUSEXCEPTION();

    _ASSERTE(SystemDomain::System());
    BaseDomain* pDomain = SystemDomain::GetCurrentDomain();
    _ASSERTE(pDomain);

    // Check to see if we have this value class cached
    EEClass *pClass = pDomain->LookupClass(guid);
    if (pClass == NULL)
    {
        pClass = SystemDomain::LoadCOMClass(guid, NULL, TRUE, NULL);        
        if (pClass != NULL)
        {
            // if the class didn't have a GUID, 
            // then we won't store it in the GUID hash table
            // so check for null guid and force insert
            CVID cvid;
            pClass->GetGuid(&cvid, FALSE);
            if (IsEqualIID(cvid, GUID_NULL))
            {
                pDomain->InsertClassForCLSID(pClass, TRUE);            
            }        
         }
    }

    // Make sure the class is a value class.
    if (pClass && !pClass->IsValueClass())
    {
        DefineFullyQualifiedNameForClassW();
        LPWSTR szName = GetFullyQualifiedNameForClassNestedAwareW(pClass);
        COMPlusThrow(kArgumentException, IDS_EE_GUID_REPRESENTS_NON_VC, szName);
    }

    return pClass;
}


//---------------------------------------------------------------------------
// This method determines if a type is visible from COM or not based on
// its visibility. This version of the method works with a type handle.
BOOL IsTypeVisibleFromCom(TypeHandle hndType)
{
    _ASSERTE(hndType.GetClass());

    DWORD                   dwFlags;
    mdTypeDef               tdEnclosingClass;
    HRESULT                 hr;
    const BYTE *            pVal;
    ULONG                   cbVal;
    EEClass *               pClass = hndType.GetClass(); 
    _ASSERTE(pClass);    

    mdTypeDef               mdType = pClass->GetCl();
    IMDInternalImport *     pInternalImport = pClass->GetMDImport();
    Assembly *              pAssembly = pClass->GetAssembly();

    // If the class is a COM imported interface then it is visible from COM.
    if (pClass->IsInterface() && pClass->IsComImport())
        return TRUE;

    // If the class is an array, then it is not visible from COM.
    if (pClass->IsArrayClass())
        return FALSE;

    // Retrieve the flags for the current class.
    tdEnclosingClass = mdType;
    pInternalImport->GetTypeDefProps(tdEnclosingClass, &dwFlags, 0);

    // Handle nested classes.
    while (IsTdNestedPublic(dwFlags))
    {
        hr = pInternalImport->GetNestedClassProps(tdEnclosingClass, &tdEnclosingClass);
        if (FAILED(hr))
        {
            _ASSERTE(!"GetNestedClassProps() failed");
            return FALSE;
        }

        // Retrieve the flags for the enclosing class.
        pInternalImport->GetTypeDefProps(tdEnclosingClass, &dwFlags, 0);
    }

    // If the outermost type is not visible then the specified type is not visible.
    if (!IsTdPublic(dwFlags))
        return FALSE;

    // Check to see if the class has the ComVisible attribute set.
    hr = pInternalImport->GetCustomAttributeByName(mdType, INTEROP_COMVISIBLE_TYPE, (const void**)&pVal, &cbVal);
    if (hr == S_OK)
    {
        _ASSERTE("The ComVisible custom attribute is invalid" && cbVal);
        return (BOOL)*(pVal + 2);
    }

    // Check to see if the assembly has the ComVisible attribute set.
    if (pAssembly->IsAssembly())
    {
        hr = pAssembly->GetManifestImport()->GetCustomAttributeByName(pAssembly->GetManifestToken(), INTEROP_COMVISIBLE_TYPE, (const void**)&pVal, &cbVal);
        if (hr == S_OK)
        {
            _ASSERTE("The ComVisible custom attribute is invalid" && cbVal);
            return (BOOL)*(pVal + 2);
        }
    }

    // The type is visible.
    return TRUE;
}


//---------------------------------------------------------------------------
// This method determines if a member is visible from COM.
BOOL IsMemberVisibleFromCom(IMDInternalImport *pInternalImport, mdToken tk, mdMethodDef mdAssociate)
{
    DWORD                   dwFlags;
    HRESULT                 hr;
    const BYTE *            pVal;
    ULONG                   cbVal;

    // Check to see if the member is public.
    switch (TypeFromToken(tk))
    {
        case mdtFieldDef:
            _ASSERTE(IsNilToken(mdAssociate));
            dwFlags = pInternalImport->GetFieldDefProps(tk);
            if (!IsFdPublic(dwFlags))
                return FALSE;
            break;

        case mdtMethodDef:
            _ASSERTE(IsNilToken(mdAssociate));
            dwFlags = pInternalImport->GetMethodDefProps(tk);
            if (!IsMdPublic(dwFlags))
                return FALSE;
            break;

        case mdtProperty:
            _ASSERTE(!IsNilToken(mdAssociate));
            dwFlags = pInternalImport->GetMethodDefProps(mdAssociate);
            if (!IsMdPublic(dwFlags))
                return FALSE;

            // Check to see if the associate has the ComVisible attribute set.
            hr = pInternalImport->GetCustomAttributeByName(mdAssociate, INTEROP_COMVISIBLE_TYPE, (const void**)&pVal, &cbVal);
            if (hr == S_OK)
            {
                _ASSERTE("The ComVisible custom attribute is invalid" && cbVal);
                return (BOOL)*(pVal + 2);
            }
            break;

        default:
            _ASSERTE(!"The type of the specified member is not handled by IsMemberVisibleFromCom");
            break;
    }

    // Check to see if the member has the ComVisible attribute set.
    hr = pInternalImport->GetCustomAttributeByName(tk, INTEROP_COMVISIBLE_TYPE, (const void**)&pVal, &cbVal);
    if (hr == S_OK)
    {
        _ASSERTE("The ComVisible custom attribute is invalid" && cbVal);
        return (BOOL)*(pVal + 2);
    }

    // The member is visible.
    return TRUE;
}


//---------------------------------------------------------------------------
// Returns the index of the LCID parameter if one exists and -1 otherwise.
int GetLCIDParameterIndex(IMDInternalImport *pInternalImport, mdMethodDef md)
{
    int             iLCIDParam = -1;
    HRESULT         hr;
    const BYTE *    pVal;
    ULONG           cbVal;

    // Check to see if the method has the LCIDConversionAttribute.
    hr = pInternalImport->GetCustomAttributeByName(md, INTEROP_LCIDCONVERSION_TYPE, (const void**)&pVal, &cbVal);
    if (hr == S_OK)
        iLCIDParam = *((int*)(pVal + 2));

    return iLCIDParam;
}

//---------------------------------------------------------------------------
// Transforms an LCID into a CultureInfo.
void GetCultureInfoForLCID(LCID lcid, OBJECTREF *pCultureObj)
{
    THROWSCOMPLUSEXCEPTION();

    static TypeHandle s_CultureInfoType;
    static MethodDesc *s_pCultureInfoConsMD = NULL;

    // Retrieve the CultureInfo type handle.
    if (s_CultureInfoType.IsNull())
        s_CultureInfoType = TypeHandle(g_Mscorlib.GetClass(CLASS__CULTURE_INFO));

    // Retrieve the CultureInfo(int culture) constructor.
    if (!s_pCultureInfoConsMD)
        s_pCultureInfoConsMD = g_Mscorlib.GetMethod(METHOD__CULTURE_INFO__INT_CTOR);

    OBJECTREF CultureObj = NULL;
    GCPROTECT_BEGIN(CultureObj)
    {
        // Allocate a CultureInfo with the specified LCID.
        CultureObj = AllocateObject(s_CultureInfoType.GetMethodTable());

        // Call the CultureInfo(int culture) constructor.
        INT64 pNewArgs[] = {
            ObjToInt64(CultureObj),
            (INT64)lcid
        };
        s_pCultureInfoConsMD->Call(pNewArgs, METHOD__CULTURE_INFO__INT_CTOR);

        // Set the returned culture object.
        *pCultureObj = CultureObj;
    }
    GCPROTECT_END();
}


//---------------------------------------------------------------------------
// This method returns the default interface for the class. A return value of NULL 
// means that the default interface is IDispatch.
DefaultInterfaceType GetDefaultInterfaceForClass(TypeHandle hndClass, TypeHandle *pHndDefClass)
{
    THROWSCOMPLUSEXCEPTION();

    _ASSERTE(!hndClass.IsNull());
    _ASSERTE(pHndDefClass);
    _ASSERTE(!hndClass.GetMethodTable()->IsInterface());

    // Set ppDefComMT to NULL before we start.
    *pHndDefClass = TypeHandle();

    HRESULT hr = S_FALSE;
    HENUMInternal eII;
    mdInterfaceImpl tkImpl;
    mdToken tkItf = 0;
    CorClassIfaceAttr ClassItfType;
    ComMethodTable *pClassComMT = NULL;
    BOOL bComVisible;
    EEClass *pClass = hndClass.GetClass();
    _ASSERTE(pClass);

    if (pClass->IsComImport())
    {
        ClassItfType = clsIfNone;
        bComVisible = TRUE;
    }
    else
    {
        pClassComMT = ComCallWrapperTemplate::SetupComMethodTableForClass(hndClass.GetMethodTable(), FALSE);
        _ASSERTE(pClassComMT);

        ClassItfType = pClassComMT->GetClassInterfaceType();
        bComVisible = pClassComMT->IsComVisible();
    }

    // If the class is not COM visible, then its default interface is IUnknown.
    if (!bComVisible)
        return DefaultInterfaceType_IUnknown;
    
    // If the class's interface type is AutoDispatch or AutoDual then return either the 
    // IClassX for the current class or IDispatch.
    if (ClassItfType != clsIfNone)
    {
        *pHndDefClass = hndClass;
        return ClassItfType == clsIfAutoDisp ? DefaultInterfaceType_AutoDispatch : DefaultInterfaceType_AutoDual;
    }

    // The class interface is set to NONE for this level of the hierarchy. So we need to check
    // to see if this class implements an interface.
    IfFailThrow(pClass->GetMDImport()->EnumInit(mdtInterfaceImpl, pClass->GetCl(), &eII));
    while (pClass->GetMDImport()->EnumNext(&eII, &tkImpl))
    {
        tkItf = pClass->GetMDImport()->GetTypeOfInterfaceImpl(tkImpl);
        _ASSERTE(tkItf);

        // Get the EEClass for the default interface's token.
        NameHandle name(pClass->GetModule(), tkItf);
        MethodTable *pItfMT = pClass->GetModule()->GetClassLoader()->LoadTypeHandle(&name, NULL).GetMethodTable();
        ComCallWrapperTemplate *pTemplate = ComCallWrapperTemplate::GetTemplate(pClass->GetMethodTable());
        ComMethodTable *pDefComMT = pTemplate->GetComMTForItf(pItfMT);
        _ASSERTE(pDefComMT);
    
        // If the interface is visible from COM then use it as the default.
        if (pDefComMT->IsComVisible())
        {
            pClass->GetMDImport()->EnumClose(&eII);
            *pHndDefClass = TypeHandle(pItfMT);
            return DefaultInterfaceType_Explicit;
        }
    }
    pClass->GetMDImport()->EnumClose(&eII);

    // If the class is a COM import with no interfaces, then its default interface will
    // be IUnknown.
    if (pClass->IsComImport())
        return DefaultInterfaceType_IUnknown;

    // If we have a managed parent class then return its default interface.
    EEClass *pParentClass = pClass->GetParentComPlusClass();
    if (pParentClass)
        return GetDefaultInterfaceForClass(TypeHandle(pParentClass->GetMethodTable()), pHndDefClass);

    // Check to see if the class is an extensible RCW.
    if (pClass->GetMethodTable()->IsComObjectType())
        return DefaultInterfaceType_BaseComClass;

    // The class has no interfaces and is marked as ClassInterfaceType.None.
    return DefaultInterfaceType_IUnknown;
}

HRESULT TryGetDefaultInterfaceForClass(TypeHandle hndClass, TypeHandle *pHndDefClass, DefaultInterfaceType *pDefItfType)
{
    HRESULT hr = S_OK;
    COMPLUS_TRY
    {
        *pDefItfType = GetDefaultInterfaceForClass(hndClass, pHndDefClass);
    }
    COMPLUS_CATCH
        {
        BEGIN_ENSURE_COOPERATIVE_GC();
        hr = SetupErrorInfo(GETTHROWABLE());
        END_ENSURE_COOPERATIVE_GC();
    }
    COMPLUS_END_CATCH
    return hr;
        }


//---------------------------------------------------------------------------
// This method retrieves the list of source interfaces for a given class.
void GetComSourceInterfacesForClass(MethodTable *pClassMT, CQuickArray<MethodTable *> &rItfList)
{
    THROWSCOMPLUSEXCEPTION();

    HRESULT hr = S_OK;
    const void  *pvData;
    ULONG cbData;
    EEClass *pClass = pClassMT->GetClass();

    BEGIN_ENSURE_COOPERATIVE_GC();

    // Reset the size of the interface list to 0.
    rItfList.ReSize(0);

    // Starting at the specified class MT retrieve the COM source interfaces 
    // of all the striped of the hierarchy.
    for (; pClass != NULL; pClass = pClass->GetParentClass())
    {
        // See if there is any [source] interface at this level of the hierarchy.
        IfFailThrow(pClass->GetMDImport()->GetCustomAttributeByName(pClass->GetCl(), INTEROP_COMSOURCEINTERFACES_TYPE, &pvData, &cbData));
        if (hr == S_OK && cbData > 2)
        {
            AppDomain *pCurrDomain = SystemDomain::GetCurrentDomain();
            LPCUTF8 pbData = reinterpret_cast<LPCUTF8>(pvData);
            pbData += 2;
            cbData -=2;
            LONG cbStr, cbcb;

            while (cbData > 0)
            {
                // Uncompress the current string of source interfaces.
                cbcb = CorSigUncompressData((PCCOR_SIGNATURE)pbData, (ULONG*)&cbStr);
                pbData += cbcb;
                cbData -= cbcb;
        
                // Allocate a new buffer that will contain the current list of source interfaces.
                LPUTF8 strCurrInterfaces = new (throws) char[cbStr + 1];
                memcpyNoGCRefs(strCurrInterfaces, pbData, cbStr);
                strCurrInterfaces[cbStr] = 0;
                LPUTF8 pCurrInterfaces = strCurrInterfaces;

                // Update the data pointer and count.
                pbData += cbStr;
                cbData -= cbStr;

                EE_TRY_FOR_FINALLY
                {
                    while (cbStr > 0 && *pCurrInterfaces != 0)
                    {
                        // Load the COM source interface specified in the CA.
                        TypeHandle ItfType;
                        OBJECTREF pThrowable = NULL;
                        GCPROTECT_BEGIN(pThrowable);
                        {
                            ItfType = pCurrDomain->FindAssemblyQualifiedTypeHandle(pCurrInterfaces, true, pClass->GetAssembly(), NULL, &pThrowable);
                            if (ItfType.IsNull())
                                COMPlusThrow(pThrowable);
                        }
                        GCPROTECT_END();

                        // Make sure the type handle represents an interface.
                        if (!ItfType.GetClass()->IsInterface())
                        {
                            WCHAR wszClassName[MAX_CLASSNAME_LENGTH];
                            WCHAR wszInvalidItfName[MAX_CLASSNAME_LENGTH];
                            pClass->_GetFullyQualifiedNameForClass(wszClassName, MAX_CLASSNAME_LENGTH);
                            ItfType.GetClass()->_GetFullyQualifiedNameForClass(wszInvalidItfName, MAX_CLASSNAME_LENGTH);
                            COMPlusThrow(kTypeLoadException, IDS_EE_INVALIDCOMSOURCEITF, wszClassName, wszInvalidItfName);
                        }

                        // Retrieve the IID of the COM source interface.
                        IID ItfIID;
                        ItfType.GetClass()->GetGuid(&ItfIID, TRUE);                

                        // Go through the list of source interfaces and check to see if the new one is a duplicate.
                        // It can be a duplicate either if it is the same interface or if it has the same IID.
                        BOOL bItfInList = FALSE;
                        for (UINT i = 0; i < rItfList.Size(); i++)
                        {
                            if (rItfList[i] == ItfType.GetMethodTable())
                            {
                                bItfInList = TRUE;
                                break;
                            }

                            IID ItfIID2;
                            rItfList[i]->GetClass()->GetGuid(&ItfIID2, TRUE);
                            if (IsEqualIID(ItfIID, ItfIID2))
                            {
                                bItfInList = TRUE;
                                break;
                            }
                        }

                        // If the COM source interface is not in the list then add it.
                        if (!bItfInList)
                        {
                            size_t OldSize = rItfList.Size();
                            rItfList.ReSize(OldSize + 1);
                            rItfList[OldSize] = ItfType.GetMethodTable();
                        }

                        // Process the next COM source interfaces in the CA.
                        int StrLen = (int)strlen(pCurrInterfaces) + 1;
                        pCurrInterfaces += StrLen;
                        cbStr -= StrLen;
                    }
                }
                EE_FINALLY
                {
                    delete[] strCurrInterfaces;
                }
                EE_END_FINALLY; 
            }
        }
    }
    END_ENSURE_COOPERATIVE_GC();
}


//--------------------------------------------------------------------------------
// These methods convert a native IEnumVARIANT to a managed IEnumerator.
OBJECTREF ConvertEnumVariantToMngEnum(IEnumVARIANT *pNativeEnum)
{
    OBJECTREF MngEnum = NULL;

    OBJECTREF EnumeratorToEnumVariantMarshaler = NULL;
    GCPROTECT_BEGIN(EnumeratorToEnumVariantMarshaler)
    {
        // Retrieve the custom marshaler and the MD to use to convert the IEnumVARIANT.
        StdMngIEnumerator *pStdMngIEnumInfo = SystemDomain::GetCurrentDomain()->GetMngStdInterfacesInfo()->GetStdMngIEnumerator();
        MethodDesc *pEnumNativeToManagedMD = pStdMngIEnumInfo->GetCustomMarshalerMD(CustomMarshalerMethods_MarshalNativeToManaged);
        EnumeratorToEnumVariantMarshaler = pStdMngIEnumInfo->GetCustomMarshaler();

        // Prepare the arguments that will be passed to MarshalNativeToManaged.
        INT64 MarshalNativeToManagedArgs[] = {
            ObjToInt64(EnumeratorToEnumVariantMarshaler),
            (INT64)pNativeEnum
        };

        // Retrieve the managed view for the current native interface pointer.
        MngEnum = Int64ToObj(pEnumNativeToManagedMD->Call(MarshalNativeToManagedArgs));
    }
    GCPROTECT_END();

    return MngEnum;
}

//--------------------------------------------------------------------------------
// These methods convert a managed IEnumerator to a native IEnumVARIANT. The
// IEnumVARIANT that is returned is already AddRef'd.
IEnumVARIANT *ConvertMngEnumToEnumVariant(OBJECTREF ManagedEnum)
{
    IEnumVARIANT *pEnum;

    OBJECTREF EnumeratorToEnumVariantMarshaler = NULL;
    GCPROTECT_BEGIN(EnumeratorToEnumVariantMarshaler)
    GCPROTECT_BEGIN(ManagedEnum)
    {
        // Retrieve the custom marshaler and the MD to use to convert the IEnumerator.
        StdMngIEnumerator *pStdMngIEnumInfo = SystemDomain::GetCurrentDomain()->GetMngStdInterfacesInfo()->GetStdMngIEnumerator();
        MethodDesc *pEnumManagedToNativeMD = pStdMngIEnumInfo->GetCustomMarshalerMD(CustomMarshalerMethods_MarshalManagedToNative);
        EnumeratorToEnumVariantMarshaler = pStdMngIEnumInfo->GetCustomMarshaler();

        // Prepare the arguments that will be passed to MarshalManagedToNative.
        INT64 MarshalNativeToManagedArgs[] = {
            ObjToInt64(EnumeratorToEnumVariantMarshaler),
            ObjToInt64(ManagedEnum)
        };

        // Retrieve the native view for the current managed object.
        pEnum = (IEnumVARIANT *)pEnumManagedToNativeMD->Call(MarshalNativeToManagedArgs);
    }
    GCPROTECT_END();
    GCPROTECT_END();

    return pEnum;
}

//--------------------------------------------------------------------------------
// Helper method to determine if a type handle represents a System.Drawing.Color.
BOOL IsSystemColor(TypeHandle th)
{
    // Retrieve the System.Drawing.Color type.
    TypeHandle hndOleColor = 
        GetThread()->GetDomain()->GetMarshalingData()->GetOleColorMarshalingInfo()->GetColorType();

    return (th == hndOleColor);
}

//--------------------------------------------------------------------------------
// This method converts an OLE_COLOR to a System.Color.
void ConvertOleColorToSystemColor(OLE_COLOR SrcOleColor, SYSTEMCOLOR *pDestSysColor)
{
    // Retrieve the method desc to use for the current AD.
    MethodDesc *pOleColorToSystemColorMD = 
        GetThread()->GetDomain()->GetMarshalingData()->GetOleColorMarshalingInfo()->GetOleColorToSystemColorMD();

    // Set up the args and call the method.
    INT64 Args[] = {
        (INT64)SrcOleColor,
        (INT64)pDestSysColor
    };   
    pOleColorToSystemColorMD->Call(Args);
}

//--------------------------------------------------------------------------------
// This method converts a System.Color to an OLE_COLOR.
OLE_COLOR ConvertSystemColorToOleColor(SYSTEMCOLOR *pSrcSysColor)
{
    // Retrieve the method desc to use for the current AD.
    MethodDesc *pSystemColorToOleColorMD = 
        GetThread()->GetDomain()->GetMarshalingData()->GetOleColorMarshalingInfo()->GetSystemColorToOleColorMD();

    // Set up the args and call the method.
    MetaSig Sig(MetaSig(pSystemColorToOleColorMD->GetSig(), pSystemColorToOleColorMD->GetModule()));
    return (OLE_COLOR)pSystemColorToOleColorMD->Call((const BYTE *)pSrcSysColor, &Sig);
}

ULONG GetStringizedMethodDef(IMDInternalImport *pMDImport, mdToken tkMb, CQuickArray<BYTE> &rDef, ULONG cbCur)
{
    THROWSCOMPLUSEXCEPTION();

    CQuickBytes rSig;
    HENUMInternal ePm;                  // For enumerating  params.
    mdParamDef  tkPm;                   // A param token.
    DWORD       dwFlags;                // Param flags.
    USHORT      usSeq;                  // Sequence of a parameter.
    ULONG       cPm;                    // Count of params.
    PCCOR_SIGNATURE pSig;
    ULONG       cbSig;
    HRESULT     hr = S_OK;

    // Don't count invisible members.
    if (!IsMemberVisibleFromCom(pMDImport, tkMb, mdMethodDefNil))
        return cbCur;
    
    // accumulate the signatures.
    pSig = pMDImport->GetSigOfMethodDef(tkMb, &cbSig);
    IfFailThrow(::PrettyPrintSigInternal(pSig, cbSig, "", &rSig, pMDImport));
    // Get the parameter flags.
    IfFailThrow(pMDImport->EnumInit(mdtParamDef, tkMb, &ePm));
    cPm = pMDImport->EnumGetCount(&ePm);
    // Resize for sig and params.  Just use 1 byte of param.
    IfFailThrow(rDef.ReSize(cbCur + rSig.Size() + cPm));
    memcpy(rDef.Ptr() + cbCur, rSig.Ptr(), rSig.Size());
    cbCur += rSig.Size()-1;
    // Enumerate through the params and get the flags.
    while (pMDImport->EnumNext(&ePm, &tkPm))
    {
        pMDImport->GetParamDefProps(tkPm, &usSeq, &dwFlags);
        if (usSeq == 0)     // Skip return type flags.
            continue;
        rDef[cbCur++] = (BYTE)dwFlags;
    }
    pMDImport->EnumClose(&ePm);

    // Return the number of bytes.
    return cbCur;
} // void GetStringizedMethodDef()

ULONG GetStringizedFieldDef(IMDInternalImport *pMDImport, mdToken tkMb, CQuickArray<BYTE> &rDef, ULONG cbCur)
{
    THROWSCOMPLUSEXCEPTION();

    CQuickBytes rSig;
    PCCOR_SIGNATURE pSig;
    ULONG       cbSig;
    HRESULT     hr = S_OK;

    // Don't count invisible members.
    if (!IsMemberVisibleFromCom(pMDImport, tkMb, mdMethodDefNil))
        return cbCur;
    
    // accumulate the signatures.
    pSig = pMDImport->GetSigOfFieldDef(tkMb, &cbSig);
    IfFailThrow(::PrettyPrintSigInternal(pSig, cbSig, "", &rSig, pMDImport));
    IfFailThrow(rDef.ReSize(cbCur + rSig.Size()));
    memcpy(rDef.Ptr() + cbCur, rSig.Ptr(), rSig.Size());
    cbCur += rSig.Size()-1;

    // Return the number of bytes.
    return cbCur;
} // void GetStringizedFieldDef()

//--------------------------------------------------------------------------------
// This method generates a stringized version of an interface that contains the
// name of the interface along with the signature of all the methods.
SIZE_T GetStringizedItfDef(TypeHandle InterfaceType, CQuickArray<BYTE> &rDef)
{
    THROWSCOMPLUSEXCEPTION();

    LPWSTR szName;                 
    ULONG cchName;
    HENUMInternal eMb;                  // For enumerating methods and fields.
    mdToken     tkMb;                   // A method or field token.
    HENUMInternal ePm;                  // For enumerating  params.
    SIZE_T       cbCur;
    HRESULT hr = S_OK;
    EEClass *pItfClass = InterfaceType.GetClass();
    _ASSERTE(pItfClass);
    IMDInternalImport *pMDImport = pItfClass->GetMDImport();
    _ASSERTE(pMDImport);

    // Make sure the specified type is an interface with a valid token.
    _ASSERTE(!IsNilToken(pItfClass->GetCl()) && pItfClass->IsInterface());

    // Get the name of the class.
    DefineFullyQualifiedNameForClassW();
    szName = GetFullyQualifiedNameForClassNestedAwareW(pItfClass);
    _ASSERTE(szName);
    cchName = (ULONG)wcslen(szName);

    // Start with the interface name.
    cbCur = cchName * sizeof(WCHAR);
    IfFailThrow(rDef.ReSize(cbCur + sizeof(WCHAR) ));
    wcscpy(reinterpret_cast<LPWSTR>(rDef.Ptr()), szName);

    // Enumerate the methods...
    IfFailThrow(pMDImport->EnumInit(mdtMethodDef, pItfClass->GetCl(), &eMb));
    while(pMDImport->EnumNext(&eMb, &tkMb))
    {   // accumulate the signatures.
        cbCur = GetStringizedMethodDef(pMDImport, tkMb, rDef, cbCur);
    }
    pMDImport->EnumClose(&eMb);

    // Enumerate the fields...
    IfFailThrow(pMDImport->EnumInit(mdtFieldDef, pItfClass->GetCl(), &eMb));
    while(pMDImport->EnumNext(&eMb, &tkMb))
    {   // accumulate the signatures.
        cbCur = GetStringizedFieldDef(pMDImport, tkMb, rDef, cbCur);
    }
    pMDImport->EnumClose(&eMb);

    // Return the number of bytes.
    return cbCur;
} // ULONG GetStringizedItfDef()

//--------------------------------------------------------------------------------
// This method generates a stringized version of a class interface that contains 
// the signatures of all the methods and fields.
ULONG GetStringizedClassItfDef(TypeHandle InterfaceType, CQuickArray<BYTE> &rDef)
{
    THROWSCOMPLUSEXCEPTION();

    LPWSTR      szName;                 
    ULONG       cchName;
    EEClass     *pItfClass = InterfaceType.GetClass();
    IMDInternalImport *pMDImport = 0;
    DWORD       nSlots;                 // Slots on the pseudo interface.
    mdToken     tkMb;                   // A method or field token.
    ULONG       cbCur;
    HRESULT     hr = S_OK;
    ULONG       i;

    // Should be an actual class.
    _ASSERTE(!pItfClass->IsInterface());

    // See what sort of IClassX this class gets.
    DefaultInterfaceType DefItfType;
    TypeHandle hndDefItfClass;
    BOOL bGenerateMethods = FALSE;
    DefItfType = GetDefaultInterfaceForClass(TypeHandle(pItfClass->GetMethodTable()), &hndDefItfClass);
    // The results apply to this class if the hndDefItfClass is this class itself, not a parent class.
    // A side effect is that [ComVisible(false)] types' guids are generated without members.
    if (hndDefItfClass.GetClass() == pItfClass && DefItfType == DefaultInterfaceType_AutoDual)
        bGenerateMethods = TRUE;

    // Get the name of the class.
    DefineFullyQualifiedNameForClassW();
    szName = GetFullyQualifiedNameForClassNestedAwareW(pItfClass);
    _ASSERTE(szName);
    cchName = (ULONG)wcslen(szName);

    // Start with the interface name.
    cbCur = cchName * sizeof(WCHAR);
    IfFailThrow(rDef.ReSize(cbCur + sizeof(WCHAR) ));
    wcscpy(reinterpret_cast<LPWSTR>(rDef.Ptr()), szName);

    if (bGenerateMethods)
    {
        ComMTMemberInfoMap MemberMap(InterfaceType.GetMethodTable()); // The map of members.

        // Retrieve the method properties.
        MemberMap.Init();

        CQuickArray<ComMTMethodProps> &rProps = MemberMap.GetMethods();
        nSlots = (DWORD)rProps.Size();

        // Now add the methods to the TypeInfo.
        for (i=0; i<nSlots; ++i)
        {
            ComMTMethodProps *pProps = &rProps[i];
            if (pProps->bMemberVisible)
            {
                if (pProps->semantic < FieldSemanticOffset)
                {
                    pMDImport = pProps->pMeth->GetMDImport();
                    tkMb = pProps->pMeth->GetMemberDef();
                    cbCur = GetStringizedMethodDef(pMDImport, tkMb, rDef, cbCur);
                }
                else
                {
                    ComCallMethodDesc   *pFieldMeth;    // A MethodDesc for a field call.
                    FieldDesc   *pField;                // A FieldDesc.
                    pFieldMeth = reinterpret_cast<ComCallMethodDesc*>(pProps->pMeth);
                    pField = pFieldMeth->GetFieldDesc();
                    pMDImport = pField->GetMDImport();
                    tkMb = pField->GetMemberDef();
                    cbCur = GetStringizedFieldDef(pMDImport, tkMb, rDef, cbCur);
                }
            }
        }
    }
    
    // Return the number of bytes.
    return cbCur;
} // ULONG GetStringizedClassItfDef()

//--------------------------------------------------------------------------------
// Helper to get the GUID of a class interface.
void GenerateClassItfGuid(TypeHandle InterfaceType, GUID *pGuid)
{
    THROWSCOMPLUSEXCEPTION();
    
    LPWSTR      szName;                 // Name to turn to a guid.
    ULONG       cchName;                // Length of the name (possibly after decoration).
    CQuickArray<BYTE> rName;            // Buffer to accumulate signatures.
    ULONG       cbCur;                  // Current offset.
    HRESULT     hr = S_OK;              // A result.

    cbCur = GetStringizedClassItfDef(InterfaceType, rName);
    
    // Pad up to a whole WCHAR.
    if (cbCur % sizeof(WCHAR))
    {
        int cbDelta = sizeof(WCHAR) - (cbCur % sizeof(WCHAR));
        IfFailThrow(rName.ReSize(cbCur + cbDelta));
        memset(rName.Ptr() + cbCur, 0, cbDelta);
        cbCur += cbDelta;
    }

    // Point to the new buffer.
    cchName = cbCur / sizeof(WCHAR);
    szName = reinterpret_cast<LPWSTR>(rName.Ptr());

    // Generate guid from name.
    CorGuidFromNameW(pGuid, szName, cchName);
} // void GenerateClassItfGuid()

HRESULT TryGenerateClassItfGuid(TypeHandle InterfaceType, GUID *pGuid)
{
    HRESULT hr = S_OK;
    COMPLUS_TRY
    {
        GenerateClassItfGuid(InterfaceType, pGuid);
    }
    COMPLUS_CATCH
    {
        BEGIN_ENSURE_COOPERATIVE_GC();
        hr = SetupErrorInfo(GETTHROWABLE());
        END_ENSURE_COOPERATIVE_GC();
    }
    COMPLUS_END_CATCH
    return hr;
}

//--------------------------------------------------------------------------------
// Helper to get the stringized form of typelib guid.
HRESULT GetStringizedTypeLibGuidForAssembly(Assembly *pAssembly, CQuickArray<BYTE> &rDef, ULONG cbCur, ULONG *pcbFetched)
{
    CANNOTTHROWCOMPLUSEXCEPTION();

    HRESULT     hr = S_OK;              // A result.
    LPCUTF8     pszName = NULL;         // Library name in UTF8.
    ULONG       cbName;                 // Length of name, UTF8 characters.
    LPWSTR      pName;                  // Pointer to library name.
    ULONG       cchName;                // Length of name, wide chars.
    LPWSTR      pch=0;                  // Pointer into lib name.
    const BYTE  *pSN=NULL;              // Pointer to public key.
    DWORD       cbSN=0;                 // Size of public key.
    DWORD       cchData=0;              // Size of the the thing we will turn into a guid.
    USHORT      usMajorVersion;         // The major version number.
    USHORT      usMinorVersion;         // The minor version number.
    USHORT      usBuildNumber;          // The build number.
    USHORT      usRevisionNumber;       // The revision number.
    const BYTE  *pbData;                // Pointer to a custom attribute data.
    ULONG       cbData;                 // Size of custom attribute data.
    static char szTypeLibKeyName[] = {"TypeLib"};
 
    // Get the name, and determine its length.
    pAssembly->GetName(&pszName);
    cbName=(ULONG)strlen(pszName);
    cchName = WszMultiByteToWideChar(CP_ACP,0, pszName,cbName+1, 0,0);
    
    // See if there is a public key.
    if (pAssembly->IsStrongNamed())
        pAssembly->GetPublicKey(&pSN, &cbSN);
    
    // If the ComCompatibleVersionAttribute is set, then use the version
    // number in the attribute when generating the GUID.
    IfFailGo(pAssembly->GetManifestImport()->GetCustomAttributeByName(TokenFromRid(1, mdtAssembly), INTEROP_COMCOMPATIBLEVERSION_TYPE, (const void**)&pbData, &cbData));
    if (hr == S_OK && cbData >= (2 + 4 * sizeof(INT16)))
    {
        // Assert that the metadata blob is valid and of the right format.
        _ASSERTE("TypeLibVersion custom attribute does not have the right format" && (*pbData == 0x01) && (*(pbData + 1) == 0x00));

        // Skip the header describing the type of custom attribute blob.
        pbData += 2;
        cbData -= 2;

        // Retrieve the major and minor version from the attribute.
        usMajorVersion = GET_VERSION_USHORT_FROM_INT(*((INT32*)pbData));
        usMinorVersion = GET_VERSION_USHORT_FROM_INT(*((INT32*)pbData + 1));
        usBuildNumber = GET_VERSION_USHORT_FROM_INT(*((INT32*)pbData + 2));
        usRevisionNumber = GET_VERSION_USHORT_FROM_INT(*((INT32*)pbData + 3));
    }
    else
    {
        usMajorVersion = pAssembly->m_Context->usMajorVersion;
        usMinorVersion =  pAssembly->m_Context->usMinorVersion;
        usBuildNumber =  pAssembly->m_Context->usBuildNumber;
        usRevisionNumber =  pAssembly->m_Context->usRevisionNumber;
    }

    // Get the version information.
    struct  versioninfo
    {
        USHORT      usMajorVersion;         // Major Version.   
        USHORT      usMinorVersion;         // Minor Version.
        USHORT      usBuildNumber;          // Build Number.
        USHORT      usRevisionNumber;       // Revision Number.
    } ver;

    // There is a bug here in that usMajor is used twice and usMinor not at all.
    //  We're not fixing that because everyone has a major version, so all the
    //  generated guids would change, which is breaking.  To compensate, if 
    //  the minor is non-zero, we add it separately, below.
    ver.usMajorVersion = usMajorVersion;
    ver.usMinorVersion =  usMajorVersion;  // Don't fix this line!
    ver.usBuildNumber =  usBuildNumber;
    ver.usRevisionNumber =  usRevisionNumber;
    
    // Resize the output buffer.
    IfFailGo(rDef.ReSize(cbCur + cchName*sizeof(WCHAR) + sizeof(szTypeLibKeyName)-1 + cbSN + sizeof(ver)+sizeof(USHORT)));
                                                                                                          
    // Put it all together.  Name first.
    WszMultiByteToWideChar(CP_ACP,0, pszName,cbName+1, (LPWSTR)(&rDef[cbCur]),cchName);
    pName = (LPWSTR)(&rDef[cbCur]);
    for (pch=pName; *pch; ++pch)
        if (*pch == '.' || *pch == ' ')
            *pch = '_';
    else
        if (iswupper(*pch))
            *pch = towlower(*pch);
    cbCur += (cchName-1)*sizeof(WCHAR);
    memcpy(&rDef[cbCur], szTypeLibKeyName, sizeof(szTypeLibKeyName)-1);
    cbCur += sizeof(szTypeLibKeyName)-1;
        
    // Version.
    memcpy(&rDef[cbCur], &ver, sizeof(ver));
    cbCur += sizeof(ver);

    // If minor version is non-zero, add it to the hash.  It should have been in the ver struct,
    //  but due to a bug, it was omitted there, and fixing it "right" would have been
    //  breaking.  So if it isn't zero, add it; if it is zero, don't add it.  Any
    //  possible value of minor thus generates a different guid, and a value of 0 still generates
    //  the guid that the original, buggy, code generated.
    if (usMinorVersion != 0)
    {
        *reinterpret_cast<USHORT*>(&rDef[cbCur]) = usMinorVersion;
        cbCur += sizeof(USHORT);
    }

    // Public key.
    memcpy(&rDef[cbCur], pSN, cbSN);
    cbCur += cbSN;

    if (pcbFetched)
        *pcbFetched = cbCur;

ErrExit:
    return hr;
}

//--------------------------------------------------------------------------------
// Helper to get the GUID of the typelib that is created from an assembly.
HRESULT GetTypeLibGuidForAssembly(Assembly *pAssembly, GUID *pGuid)
{
    CANNOTTHROWCOMPLUSEXCEPTION();

    HRESULT     hr = S_OK;
    CQuickArray<BYTE> rName;            // String for guid.
    ULONG       cbData;                 // Size of the string in bytes.
 
    // Get GUID from Assembly, else from Manifest Module, else Generate from name.
    hr = pAssembly->GetManifestImport()->GetItemGuid(TokenFromRid(1, mdtAssembly), pGuid);

    if (*pGuid == GUID_NULL)
    {
        // Get the string.
        IfFailGo(GetStringizedTypeLibGuidForAssembly(pAssembly, rName, 0, &cbData));
        
        // Pad to a whole WCHAR.
        if (cbData % sizeof(WCHAR))
        {
            IfFailGo(rName.ReSize(cbData + sizeof(WCHAR)-(cbData%sizeof(WCHAR))));
            while (cbData % sizeof(WCHAR))
                rName[cbData++] = 0;
        }
    
        // Turn into guid
        CorGuidFromNameW(pGuid, (LPWSTR)rName.Ptr(), cbData/sizeof(WCHAR));
}

ErrExit:
    return hr;
} // HRESULT GetTypeLibGuidForAssembly()


//--------------------------------------------------------------------------------
// Validate that the given target is valid for the specified type.
BOOL IsComTargetValidForType(REFLECTCLASSBASEREF* pRefClassObj, OBJECTREF* pTarget)
{
    EEClass* pInvokedClass = ((ReflectClass*)(*pRefClassObj)->GetData())->GetClass();
    EEClass* pTargetClass = (*pTarget)->GetTrueClass();
    _ASSERTE(pTargetClass && pInvokedClass);

    // If the target class and the invoke class are identical then the invoke is valid.
    if (pTargetClass == pInvokedClass)
        return TRUE;

    // We always allow calling InvokeMember on a __ComObject type regardless of the type
    // of the target object.
    if ((*pRefClassObj)->IsComObjectClass())
        return TRUE;

    // If the class that is being invoked on is an interface then check to see if the
    // target class supports that interface.
    if (pInvokedClass->IsInterface())
        return pTargetClass->SupportsInterface(*pTarget, pInvokedClass->GetMethodTable());

    // Check to see if the target class inherits from the invoked class.
    while (pTargetClass)
    {
        pTargetClass = pTargetClass->GetParentClass();
        if (pTargetClass == pInvokedClass)
        {
            // The target class inherits from the invoked class.
            return TRUE;
        }
    }

    // There is no valid relationship between the invoked and the target classes.
    return FALSE;
}

DISPID ExtractStandardDispId(LPWSTR strStdDispIdMemberName)
{
    THROWSCOMPLUSEXCEPTION();

    // Find the first character after the = in the standard DISPID member name.
    LPWSTR strDispId = wcsstr(&strStdDispIdMemberName[STANDARD_DISPID_PREFIX_LENGTH], L"=") + 1;
    if (!strDispId)
        COMPlusThrow(kArgumentException, IDS_EE_INVALID_STD_DISPID_NAME);

    // Validate that the last character of the standard member name is a ].
    LPWSTR strClosingBracket = wcsstr(strDispId, L"]");
    if (!strClosingBracket || (strClosingBracket[1] != 0))
        COMPlusThrow(kArgumentException, IDS_EE_INVALID_STD_DISPID_NAME);

    // Extract the number from the standard DISPID member name.
    return _wtoi(strDispId);
}

struct ByrefArgumentInfo
{
    BOOL        m_bByref;
    VARIANT     m_Val;
};


//--------------------------------------------------------------------------------
// InvokeDispMethod will convert a set of managed objects and call IDispatch.  The
//  result will be returned as a COM+ Variant pointed to by pRetVal.
void IUInvokeDispMethod(OBJECTREF* pReflectClass, OBJECTREF* pTarget, OBJECTREF* pName, DISPID *pMemberID,
                        OBJECTREF* pArgs, OBJECTREF* pByrefModifiers, OBJECTREF* pNamedArgs, OBJECTREF* pRetVal, LCID lcid, int flags, BOOL bIgnoreReturn, BOOL bIgnoreCase)
{
    HRESULT hr;
    UINT i;
    UINT iSrcArg;
    UINT iDestArg;
    UINT cArgs = 0;
    UINT cNamedArgs = 0;
    UINT iArgErr;
    VARIANT VarResult;
    VARIANT *pVarResult = NULL;
    Thread* pThread = GetThread();
    DISPPARAMS DispParams = {0};
    DISPID *aDispID = NULL;
    DISPID MemberID = 0;
    EXCEPINFO ExcepInfo;
    ByrefArgumentInfo *aByrefArgInfos = NULL;
    BOOL bSomeArgsAreByref = FALSE;
    IDispatch *pDisp = NULL;
    IDispatchEx *pDispEx = NULL;

    THROWSCOMPLUSEXCEPTION();


    //
    // Function initialization.
    //

    VariantInit (&VarResult);

    // Validate that we are in cooperative GC mode.
    _ASSERTE(pThread->PreemptiveGCDisabled());

    // InteropUtil.h does not know about anything other than OBJECTREF so
    // convert the OBJECTREF's to their real type.
    REFLECTCLASSBASEREF* pRefClassObj = (REFLECTCLASSBASEREF*) pReflectClass;
    STRINGREF* pStrName = (STRINGREF*) pName;
    PTRARRAYREF* pArrArgs = (PTRARRAYREF*) pArgs;
    PTRARRAYREF* pArrByrefModifiers = (PTRARRAYREF*) pByrefModifiers;
    PTRARRAYREF* pArrNamedArgs = (PTRARRAYREF*) pNamedArgs;
    MethodTable* pInvokedMT = ((ReflectClass*)(*pRefClassObj)->GetData())->GetClass()->GetMethodTable();

    // Retrieve the total count of arguments.
    if (*pArrArgs != NULL)
        cArgs = (*pArrArgs)->GetNumComponents();

    // Retrieve the count of named arguments.
    if (*pArrNamedArgs != NULL)
        cNamedArgs = (*pArrNamedArgs)->GetNumComponents();

    // Validate that the target is valid for the specified type.
    if (!IsComTargetValidForType(pRefClassObj, pTarget))
        COMPlusThrow(kTargetException, L"RFLCT.Targ_ITargMismatch");

    // If the invoked type is an interface, make sure it is IDispatch based.
    if (pInvokedMT->IsInterface() && (pInvokedMT->GetComInterfaceType() == ifVtable))
        COMPlusThrow(kTargetInvocationException, IDS_EE_INTERFACE_NOT_DISPATCH_BASED);

    // Validate that the target is a COM object.
    _ASSERTE((*pTarget)->GetMethodTable()->IsComObjectType());

    EE_TRY_FOR_FINALLY
    {
        //
        // Initialize the DISPPARAMS structure.
        //

        if (cArgs > 0)
        {
            UINT cPositionalArgs = cArgs - cNamedArgs;
            DispParams.cArgs = cArgs;
            DispParams.rgvarg = (VARIANTARG *)_alloca(cArgs * sizeof(VARIANTARG));

            // Initialize all the variants.
            pThread->EnablePreemptiveGC();
            for (i = 0; i < cArgs; i++)
                VariantInit(&DispParams.rgvarg[i]);
            pThread->DisablePreemptiveGC();
        }


        //
        // Retrieve the IDispatch interface that will be invoked on.
        //

        if (pInvokedMT->IsInterface())
        {
            // The invoked type is a dispatch or dual interface so we will make the
            // invocation on it.
            pDisp = (IDispatch*)ComPlusWrapper::GetComIPFromWrapperEx(*pTarget, pInvokedMT);
        }
        else
        {
            // A class was passed in so we will make the invocation on the default
            // IDispatch for the COM component.

            // Validate that the COM object is still attached to its ComPlusWrapper.
            ComPlusWrapper *pWrap = (*pTarget)->GetSyncBlock()->GetComPlusWrapper();
            if (!pWrap)
                COMPlusThrow(kInvalidComObjectException, IDS_EE_COM_OBJECT_NO_LONGER_HAS_WRAPPER);

            // Retrieve the IDispath pointer from the wrapper.
            pDisp = (IDispatch*)pWrap->GetIDispatch();
            if (!pDisp)
                COMPlusThrow(kTargetInvocationException, IDS_EE_NO_IDISPATCH_ON_TARGET);

            // If we aren't ignoring case, then we need to try and QI for IDispatchEx to 
            // be able to use IDispatchEx::GetDispID() which has a flag to control case
            // sentisitivity.
            if (!bIgnoreCase && cNamedArgs == 0)
            {
                hr = SafeQueryInterface(pDisp, IID_IDispatchEx, (IUnknown**)&pDispEx);
                if (FAILED(hr))
                    pDispEx = NULL;
            }
        }
        _ASSERTE(pDisp);


        //
        // Prepare the DISPID's that will be passed to invoke.
        //

        if (pMemberID && (*pMemberID != DISPID_UNKNOWN) && (cNamedArgs == 0))
        {
            // The caller specified a member ID and we don't have any named arguments so
            // we can simply use the member ID the caller specified.
            MemberID = *pMemberID;
        }
        else
        {
            int strNameLength = (*pStrName)->GetStringLength();

            // Check if we are invoking on the default member.
            if (strNameLength == 0)
            {
                // Set the DISPID to 0 (default member).
                MemberID = 0;

                //@todo: Determine if named arguments are allowed with default members.
                // in v1 we throw a not supported exception, should revisit this in v.next (bug #92454)
                _ASSERTE(cNamedArgs == 0);
                if (cNamedArgs != 0)
                    COMPlusThrow(kNotSupportedException,L"NotSupported_IDispInvokeDefaultMemberWithNamedArgs");

            }
            else
            {
                //
                // Create an array of strings that will be passed to GetIDsOfNames().
                //

                UINT cNamesToConvert = cNamedArgs + 1;

                // Allocate the array of strings to convert, the array of pinned handles and the
                // array of converted DISPID's.
                LPWSTR *aNamesToConvert = (LPWSTR *)_alloca(cNamesToConvert * sizeof(LPWSTR));
                LPWSTR strTmpName = NULL;
                aDispID = (DISPID *)_alloca(cNamesToConvert * sizeof(DISPID));

                // The first name to convert is the name of the method itself.
                aNamesToConvert[0] = (*pStrName)->GetBuffer();

                // Check to see if the name is for a standard DISPID.
                if (_wcsnicmp(aNamesToConvert[0], STANDARD_DISPID_PREFIX, STANDARD_DISPID_PREFIX_LENGTH) == 0)
                {
                    // The name is for a standard DISPID so extract it from the name.
                    MemberID = ExtractStandardDispId(aNamesToConvert[0]);

                    // Make sure there are no named arguments to convert.
                    if (cNamedArgs > 0)
                    {
                        STRINGREF *pNamedArgsData = (STRINGREF *)(*pArrNamedArgs)->GetDataPtr();

                        for (i = 0; i < cNamedArgs; i++)
                        {
                            // The first name to convert is the name of the method itself.
                            strTmpName = pNamedArgsData[i]->GetBuffer();

                            // Check to see if the name is for a standard DISPID.
                            if (_wcsnicmp(strTmpName, STANDARD_DISPID_PREFIX, STANDARD_DISPID_PREFIX_LENGTH) != 0)
                                COMPlusThrow(kArgumentException, IDS_EE_NON_STD_NAME_WITH_STD_DISPID);

                            // The name is for a standard DISPID so extract it from the name.
                            aDispID[i + 1] = ExtractStandardDispId(strTmpName);
                        }
                    }
                }
                else
                {
                    BOOL fGotIt = FALSE;
                    BOOL fIsNonGenericComObject = pInvokedMT->IsInterface() || (pInvokedMT != SystemDomain::GetDefaultComObject() && pInvokedMT->IsComObjectType());
                    BOOL fUseCache = fIsNonGenericComObject && !pDispEx && strNameLength <= ReflectionMaxCachedNameLength && cNamedArgs == 0;
                    DispIDCacheElement vDispIDElement;

                    // If the object is not a generic COM object and the member meets the criteria to be
                    // in the cache then look up the DISPID in the cache.
                    if (fUseCache)
                    {
                        vDispIDElement.pMT = pInvokedMT;
                        vDispIDElement.strNameLength = strNameLength;
                        vDispIDElement.lcid = lcid;
                        wcscpy(vDispIDElement.strName, aNamesToConvert[0]);

                        // Only look up if the cache has already been created.
                        DispIDCache* pDispIDCache = GetAppDomain()->GetRefDispIDCache();
                        fGotIt = pDispIDCache->GetFromCache (&vDispIDElement, MemberID);
                    }

                    if (!fGotIt)
                    {
                        OBJECTHANDLE *ahndPinnedObjs = (OBJECTHANDLE*)_alloca(cNamesToConvert * sizeof(OBJECTHANDLE));
                        ahndPinnedObjs[0] = GetAppDomain()->CreatePinningHandle((OBJECTREF)*pStrName);

                        // Copy the named arguments into the array of names to convert.
                        if (cNamedArgs > 0)
                        {
                            STRINGREF *pNamedArgsData = (STRINGREF *)(*pArrNamedArgs)->GetDataPtr();

                            for (i = 0; i < cNamedArgs; i++)
                            {
                                // Pin the string object and retrieve a pointer to its data.
                                ahndPinnedObjs[i + 1] = GetAppDomain()->CreatePinningHandle((OBJECTREF)pNamedArgsData[i]);
                                aNamesToConvert[i + 1] = pNamedArgsData[i]->GetBuffer();
                            }
                        }

                        //
                        // Call GetIDsOfNames to convert the names to DISPID's
                        //

                        // We are about to make call's to COM so switch to preemptive GC.
                        pThread->EnablePreemptiveGC();

                        if (pDispEx)
                        {
                            // We should only get here if we are doing a case sensitive lookup and
                            // we don't have any named arguments.
                            _ASSERTE(cNamedArgs == 0);
                            _ASSERTE(!bIgnoreCase);

                            // We managed to retrieve an IDispatchEx IP so we will use it to
                            // retrieve the DISPID.
                            BSTR bstrTmpName = SysAllocString(aNamesToConvert[0]);
                            if (!bstrTmpName)
                                COMPlusThrowOM();
                            hr = pDispEx->GetDispID(bstrTmpName, fdexNameCaseSensitive, aDispID);
                            SysFreeString(bstrTmpName);
                        }
                        else
                        {
                            // Call GetIdsOfNames() to retrieve the DISPID's of the method and of the arguments.
                            hr = pDisp->GetIDsOfNames(
                                                        IID_NULL,
                                                        aNamesToConvert,
                                                        cNamesToConvert,
                                                        lcid,
                                                        aDispID
                                                    );
                        }

                        // Now that the call's have been completed switch back to cooperative GC.
                        pThread->DisablePreemptiveGC();

                        // Now that we no longer need the method and argument names we can un-pin them.
                        for (i = 0; i < cNamesToConvert; i++)
                            DestroyPinningHandle(ahndPinnedObjs[i]);

                        if (FAILED(hr))
                        {
                            // Check to see if the user wants to invoke the new enum member.
                            if (cNamesToConvert == 1 && _wcsicmp(aNamesToConvert[0], GET_ENUMERATOR_METHOD_NAME) == 0)
                            {
                                // Invoke the new enum member.
                                MemberID = DISPID_NEWENUM;
                            }
                            else
                            {
                                // The name is unknown.
                                COMPlusThrowHR(hr);
                            }
                        }
                        else
                        {
                            // The member ID is the first elements of the array we got back from GetIdsOfNames.
                            MemberID = aDispID[0];
                        }

                        // If the object is not a generic COM object and the member meets the criteria to be
                        // in the cache then insert the member in the cache.
                        if (fUseCache)
                        {
                            DispIDCache *pDispIDCache = GetAppDomain()->GetRefDispIDCache();
                            pDispIDCache->AddToCache (&vDispIDElement, MemberID);
                        }
                    }
                }
            }

            // Store the member ID if the caller passed in a place to store it.
            if (pMemberID)
                *pMemberID = MemberID;
        }


        //
        // Fill in the DISPPARAMS structure.
        //

        if (cArgs > 0)
        {
            // Allocate the byref argument information.
            aByrefArgInfos = (ByrefArgumentInfo*)_alloca(cArgs * sizeof(ByrefArgumentInfo));
            memset(aByrefArgInfos, 0, cArgs * sizeof(ByrefArgumentInfo));

            // Set the byref bit on the arguments that have the byref modifier.
            if (*pArrByrefModifiers != NULL)
            {
                BYTE *aByrefModifiers = (BYTE*)(*pArrByrefModifiers)->GetDataPtr();
                for (i = 0; i < cArgs; i++)
                {
                    if (aByrefModifiers[i])
                    {
                        aByrefArgInfos[i].m_bByref = TRUE;
                        bSomeArgsAreByref = TRUE;
                    }
                }
            }

            // We need to protect the temporary object that will be used to convert from
            // the managed objects to OLE variants.
            OBJECTREF TmpObj = NULL;
            GCPROTECT_BEGIN(TmpObj)
            {
                if (!(flags & (DISPATCH_PROPERTYPUT | DISPATCH_PROPERTYPUTREF)))
                {
                    // For anything other than a put or a putref we just use the specified
                    // named arguments.
                    DispParams.cNamedArgs = cNamedArgs;
                    DispParams.rgdispidNamedArgs = (cNamedArgs == 0) ? NULL : &aDispID[1];

                    // Convert the named arguments from COM+ to OLE. These arguments are in the same order
                    // on both sides.
                    for (i = 0; i < cNamedArgs; i++)
                    {
                        iSrcArg = i;
                        iDestArg = i;
                        if (aByrefArgInfos[iSrcArg].m_bByref)
                        {
                            TmpObj = ((OBJECTREF*)(*pArrArgs)->GetDataPtr())[iSrcArg];
                            if (TmpObj == NULL)
                            {
                                V_VT(&DispParams.rgvarg[iDestArg]) = VT_VARIANT | VT_BYREF;
                                DispParams.rgvarg[iDestArg].pvarVal = &aByrefArgInfos[iSrcArg].m_Val;
                            }
                            else
                            {
                                OleVariant::MarshalOleVariantForObject(&TmpObj, &aByrefArgInfos[iSrcArg].m_Val);
                                OleVariant::CreateByrefVariantForVariant(&aByrefArgInfos[iSrcArg].m_Val, &DispParams.rgvarg[iDestArg]);
                            }
                        }
                        else
                        {
                            TmpObj = ((OBJECTREF*)(*pArrArgs)->GetDataPtr())[iSrcArg];
                            OleVariant::MarshalOleVariantForObject(&TmpObj, &DispParams.rgvarg[iDestArg]);
                        }
                    }

                    // Convert the unnamed arguments. These need to be presented in reverse order to IDispatch::Invoke().
                    for (iSrcArg = cNamedArgs, iDestArg = cArgs - 1; iSrcArg < cArgs; iSrcArg++, iDestArg--)
                    {
                        if (aByrefArgInfos[iSrcArg].m_bByref)
                        {
                            TmpObj = ((OBJECTREF*)(*pArrArgs)->GetDataPtr())[iSrcArg];
                            if (TmpObj == NULL)
                            {
                                V_VT(&DispParams.rgvarg[iDestArg]) = VT_VARIANT | VT_BYREF;
                                DispParams.rgvarg[iDestArg].pvarVal = &aByrefArgInfos[iSrcArg].m_Val;
                            }
                            else
                            {
                                OleVariant::MarshalOleVariantForObject(&TmpObj, &aByrefArgInfos[iSrcArg].m_Val);
                                OleVariant::CreateByrefVariantForVariant(&aByrefArgInfos[iSrcArg].m_Val, &DispParams.rgvarg[iDestArg]);
                            }
                        }
                        else
                        {
                            TmpObj = ((OBJECTREF*)(*pArrArgs)->GetDataPtr())[iSrcArg];
                            OleVariant::MarshalOleVariantForObject(&TmpObj, &DispParams.rgvarg[iDestArg]);
                        }
                    }
                }
                else
                {
                    // If we are doing a property put then we need to set the DISPID of the
                    // argument to DISP_PROPERTYPUT if there is at least one argument.
                    DispParams.cNamedArgs = cNamedArgs + 1;
                    DispParams.rgdispidNamedArgs = (DISPID*)_alloca((cNamedArgs + 1) * sizeof(DISPID));

                    // Fill in the array of named arguments.
                    DispParams.rgdispidNamedArgs[0] = DISPID_PROPERTYPUT;
                    for (i = 1; i < cNamedArgs; i++)
                        DispParams.rgdispidNamedArgs[i] = aDispID[i];

                    // The last argument from reflection becomes the first argument that must be passed to IDispatch.
                    iSrcArg = cArgs - 1;
                    iDestArg = 0;
                    if (aByrefArgInfos[iSrcArg].m_bByref)
                    {
                        TmpObj = ((OBJECTREF*)(*pArrArgs)->GetDataPtr())[iSrcArg];
                        if (TmpObj == NULL)
                        {
                            V_VT(&DispParams.rgvarg[iDestArg]) = VT_VARIANT | VT_BYREF;
                            DispParams.rgvarg[iDestArg].pvarVal = &aByrefArgInfos[iSrcArg].m_Val;
                        }
                        else
                        {
                            OleVariant::MarshalOleVariantForObject(&TmpObj, &aByrefArgInfos[iSrcArg].m_Val);
                            OleVariant::CreateByrefVariantForVariant(&aByrefArgInfos[iSrcArg].m_Val, &DispParams.rgvarg[iDestArg]);
                        }
                    }
                    else
                    {
                        TmpObj = ((OBJECTREF*)(*pArrArgs)->GetDataPtr())[iSrcArg];
                        OleVariant::MarshalOleVariantForObject(&TmpObj, &DispParams.rgvarg[iDestArg]);
                    }

                    // Convert the named arguments from COM+ to OLE. These arguments are in the same order
                    // on both sides.
                    for (i = 0; i < cNamedArgs; i++)
                    {
                        iSrcArg = i;
                        iDestArg = i + 1;
                        if (aByrefArgInfos[iSrcArg].m_bByref)
                        {
                            TmpObj = ((OBJECTREF*)(*pArrArgs)->GetDataPtr())[iSrcArg];
                            if (TmpObj == NULL)
                            {
                                V_VT(&DispParams.rgvarg[iDestArg]) = VT_VARIANT | VT_BYREF;
                                DispParams.rgvarg[iDestArg].pvarVal = &aByrefArgInfos[iSrcArg].m_Val;
                            }
                            else
                            {
                                OleVariant::MarshalOleVariantForObject(&TmpObj, &aByrefArgInfos[iSrcArg].m_Val);
                                OleVariant::CreateByrefVariantForVariant(&aByrefArgInfos[iSrcArg].m_Val, &DispParams.rgvarg[iDestArg]);
                            }
                        }
                        else
                        {
                            TmpObj = ((OBJECTREF*)(*pArrArgs)->GetDataPtr())[iSrcArg];
                            OleVariant::MarshalOleVariantForObject(&TmpObj, &DispParams.rgvarg[iDestArg]);
                        }
                    }

                    // Convert the unnamed arguments. These need to be presented in reverse order to IDispatch::Invoke().
                    for (iSrcArg = cNamedArgs, iDestArg = cArgs - 1; iSrcArg < cArgs - 1; iSrcArg++, iDestArg--)
                    {
                        if (aByrefArgInfos[iSrcArg].m_bByref)
                        {
                            TmpObj = ((OBJECTREF*)(*pArrArgs)->GetDataPtr())[iSrcArg];
                            if (TmpObj == NULL)
                            {
                                V_VT(&DispParams.rgvarg[iDestArg]) = VT_VARIANT | VT_BYREF;
                                DispParams.rgvarg[iDestArg].pvarVal = &aByrefArgInfos[iSrcArg].m_Val;
                            }
                            else
                            {
                                OleVariant::MarshalOleVariantForObject(&TmpObj, &aByrefArgInfos[iSrcArg].m_Val);
                                OleVariant::CreateByrefVariantForVariant(&aByrefArgInfos[iSrcArg].m_Val, &DispParams.rgvarg[iDestArg]);
                            }
                        }
                        else
                        {
                            TmpObj = ((OBJECTREF*)(*pArrArgs)->GetDataPtr())[iSrcArg];
                            OleVariant::MarshalOleVariantForObject(&TmpObj, &DispParams.rgvarg[iDestArg]);
                        }
                    }
                }
            }
            GCPROTECT_END();
        }
        else
        {
            // There are no arguments.
            DispParams.cArgs = cArgs;
            DispParams.cNamedArgs = 0;
            DispParams.rgdispidNamedArgs = NULL;
            DispParams.rgvarg = NULL;
        }


        //
        // Call invoke on the target's IDispatch.
        //

        if (!bIgnoreReturn)
        {
            VariantInit(&VarResult);
            pVarResult = &VarResult;
        }
        memset(&ExcepInfo, 0, sizeof(EXCEPINFO));

#ifdef CUSTOMER_CHECKED_BUILD
        CustomerDebugHelper *pCdh = CustomerDebugHelper::GetCustomerDebugHelper();

        if (pCdh->IsProbeEnabled(CustomerCheckedBuildProbe_ObjNotKeptAlive))
        {
            g_pGCHeap->GarbageCollect();
            g_pGCHeap->FinalizerThreadWait(1000);
        }
#endif // CUSTOMER_CHECKED_BUILD

        // Call the method
        COMPLUS_TRY 
        {
            // We are about to make call's to COM so switch to preemptive GC.
            pThread->EnablePreemptiveGC();

            if (pDispEx)
            {
                hr = pDispEx->InvokeEx(                                    
                                    MemberID,
                                    lcid,
                                    flags,
                                    &DispParams,
                                    pVarResult,
                                    &ExcepInfo,
                                    NULL
                                );
            }
            else
            {
                hr = pDisp->Invoke(
                                    MemberID,
                                    IID_NULL,
                                    lcid,
                                    flags,
                                    &DispParams,
                                    pVarResult,
                                    &ExcepInfo,
                                    &iArgErr
                                );
            }

            // Now that the call's have been completed switch back to cooperative GC.
            pThread->DisablePreemptiveGC();

#ifdef CUSTOMER_CHECKED_BUILD
            if (pCdh->IsProbeEnabled(CustomerCheckedBuildProbe_BufferOverrun))
            {
                g_pGCHeap->GarbageCollect();
                g_pGCHeap->FinalizerThreadWait(1000);
            }
#endif // CUSTOMER_CHECKED_BUILD

            // If the invoke call failed then throw an exception based on the EXCEPINFO.
            if (FAILED(hr))
            {
                if (hr == DISP_E_EXCEPTION)
                {
                    // This method will free the BSTR's in the EXCEPINFO.
                    COMPlusThrowHR(&ExcepInfo);
                }
                else
                {
                    COMPlusThrowHR(hr);
                }
            }
        } 
        COMPLUS_CATCH 
        {
            // If we get here we need to throw an TargetInvocationException
            OBJECTREF pException = GETTHROWABLE();
            _ASSERTE(pException);
            GCPROTECT_BEGIN(pException)
            {
                COMPlusThrow(COMMember::g_pInvokeUtil->CreateTargetExcept(&pException));
            }
            GCPROTECT_END();
        } COMPLUS_END_CATCH


        //
        // Return value handling and cleanup.
        //

        // Back propagate any byref args.
        if (bSomeArgsAreByref)
        {
            OBJECTREF TmpObj = NULL;
            GCPROTECT_BEGIN(TmpObj)
            {
                for (i = 0; i < cArgs; i++)
                {
                    if (aByrefArgInfos[i].m_bByref)
                    {
                        // Convert the variant back to an object.
                        OleVariant::MarshalObjectForOleVariant(&aByrefArgInfos[i].m_Val, &TmpObj);
                        (*pArrArgs)->SetAt(i, TmpObj);
                    }      
                }
            }
            GCPROTECT_END();
        }

        if (!bIgnoreReturn)
        {
            if (MemberID == -4)
            {
                //
                // Use a custom marshaler to convert the IEnumVARIANT to an IEnumerator.
                //

                // Start by making sure that the variant we got back contains an IP.
                if ((VarResult.vt != VT_UNKNOWN) || !VarResult.punkVal)
                    COMPlusThrow(kInvalidCastException, IDS_EE_INVOKE_NEW_ENUM_INVALID_RETURN);

                // Have the custom marshaler do the conversion.
                *pRetVal = ConvertEnumVariantToMngEnum((IEnumVARIANT *)VarResult.punkVal);
            }
            else
            {
                // Convert the return variant to a COR variant.
                OleVariant::MarshalObjectForOleVariant(&VarResult, pRetVal);
            }
        }
    }
    EE_FINALLY
    {
        // Release the IDispatch pointer.
        ULONG cbRef = SafeRelease(pDisp);
        LogInteropRelease(pDisp, cbRef, "IDispatch Release");

        // Release the IDispatchEx pointer if it isn't null.
        if (pDispEx)
        {
            ULONG cbRef = SafeRelease(pDispEx);
            LogInteropRelease(pDispEx, cbRef, "IDispatchEx Release");
        }

        // Clear the contents of the byref variants.
        if (bSomeArgsAreByref && aByrefArgInfos)
        {
            for (i = 0; i < cArgs; i++)
            {
                if (aByrefArgInfos[i].m_bByref)
                    SafeVariantClear(&aByrefArgInfos[i].m_Val);
            }
        }

        // Clear the argument variants.
        for (i = 0; i < cArgs; i++)
            SafeVariantClear(&DispParams.rgvarg[i]);

        // Clear the return variant.
        if (pVarResult)
            SafeVariantClear(pVarResult);
    }
    EE_END_FINALLY; 
}


//-------------------------------------------------------------------
// BOOL InitializeCom()
// Called from DLLMain, to initialize com specific data structures.
//-------------------------------------------------------------------
BOOL InitializeCom()
{
    BOOL fSuccess = ObjectRefCache::Init();
    if (fSuccess)
    {
        fSuccess = ComCall::Init();
    }
    if (fSuccess)
    {
        fSuccess = ComPlusCall::Init();
    }
    if (fSuccess)
    {
        fSuccess = CtxEntryCache::Init();
    }
    if (fSuccess)
    {
        fSuccess = ComCallWrapperTemplate::Init();
    }
    

#ifdef _DEBUG
    VOID IntializeInteropLogging();
    IntializeInteropLogging();
#endif 
    return fSuccess;
}


//-------------------------------------------------------------------
// void TerminateCom()
// Called from DLLMain, to clean up com specific data structures.
//-------------------------------------------------------------------
#ifdef SHOULD_WE_CLEANUP
void TerminateCom()
{
    ComPlusCall::Terminate();
    ComCall::Terminate();
    ObjectRefCache::Terminate();
    CtxEntryCache::Terminate();
    ComCallWrapperTemplate::Terminate();
}
#endif /* SHOULD_WE_CLEANUP */


//--------------------------------------------------------------------------------
// OBJECTREF GCProtectSafeRelease(OBJECTREF oref, IUnknown* pUnk)
// Protect the reference while calling SafeRelease
OBJECTREF GCProtectSafeRelease(OBJECTREF oref, IUnknown* pUnk)
{
    _ASSERTE(oref != NULL);
    _ASSERTE(pUnk != NULL);

    OBJECTREF cref = NULL;
    GCPROTECT_BEGIN(oref)
    {
        // we didn't use the pUnk that was passed in            
        // release the IUnk that was passed in
        ULONG cbRef = SafeRelease(pUnk);
        LogInteropRelease(pUnk, cbRef, "Release :we didn't cache ");
        cref = oref;
    }
    GCPROTECT_END();            
    _ASSERTE(cref != NULL);
    return cref;
}

//--------------------------------------------------------------------------------
// MethodTable* GetClassFromIProvideClassInfo(IUnknown* pUnk)
//  Check if the pUnk implements IProvideClassInfo and try to figure
// out the class from there
MethodTable* GetClassFromIProvideClassInfo(IUnknown* pUnk)
{
    _ASSERTE(pUnk != NULL);

    THROWSCOMPLUSEXCEPTION();

    ULONG cbRef;
    SystemDomain::EnsureComObjectInitialized();
    EEClass* pClass;
    MethodTable* pClassMT = NULL;
    TYPEATTR* ptattr = NULL;
    ITypeInfo* pTypeInfo = NULL;
    IProvideClassInfo* pclsInfo = NULL;
    Thread* pThread = GetThread();

    EE_TRY_FOR_FINALLY
    {
        // Use IProvideClassInfo to detect the appropriate class to use for wrapping
        HRESULT hr = SafeQueryInterface(pUnk, IID_IProvideClassInfo, (IUnknown **)&pclsInfo);
        LogInteropQI(pUnk, IID_IProvideClassInfo, hr, " IProvideClassinfo");
        if (hr == S_OK && pclsInfo)
        {
            hr = E_FAIL;                    

            // Make sure the class info is not our own 
            if (!IsSimpleTearOff(pclsInfo))
            {
                pThread->EnablePreemptiveGC();
                hr = pclsInfo->GetClassInfo(&pTypeInfo);
                pThread->DisablePreemptiveGC();
            }

            // If we succeded in retrieving the type information then keep going.
            if (hr == S_OK && pTypeInfo)
            {
                pThread->EnablePreemptiveGC();
                hr = pTypeInfo->GetTypeAttr(&ptattr);
                pThread->DisablePreemptiveGC();
            
                // If we succeeded in retrieving the attributes and they represent
                // a CoClass, then look up the class from the CLSID.
                if (hr == S_OK && ptattr->typekind == TKIND_COCLASS)
                {           
                    pClass = GetEEClassForCLSID(ptattr->guid);
                    pClassMT = (pClass != NULL) ? pClass->GetMethodTable() : NULL;
                }
            }
        }
    }
    EE_FINALLY
    {
        if (ptattr)
        {
            pThread->EnablePreemptiveGC();
            pTypeInfo->ReleaseTypeAttr(ptattr);
            pThread->DisablePreemptiveGC();
        }
        if (pTypeInfo)
        {
            cbRef = SafeRelease(pTypeInfo);
            LogInteropRelease(pTypeInfo, cbRef, "TypeInfo Release");
        }
        if (pclsInfo)
        {
            cbRef = SafeRelease(pclsInfo);
            LogInteropRelease(pclsInfo, cbRef, "IProvideClassInfo Release");
        }
    }
    EE_END_FINALLY; 

    return pClassMT;
}


//--------------------------------------------------------------------------------
// Determines if a COM object can be cast to the specified type.
BOOL CanCastComObject(OBJECTREF obj, TypeHandle hndType)
{
    if (!obj)
        return TRUE;

    if (hndType.GetMethodTable()->IsInterface())
    {
        return obj->GetClass()->SupportsInterface(obj, hndType.GetMethodTable());
    }
    else
    {
        return TypeHandle(obj->GetMethodTable()).CanCastTo(hndType);
    }
}


VOID
ReadBestFitCustomAttribute(MethodDesc* pMD, BOOL* BestFit, BOOL* ThrowOnUnmappableChar)
{
    ReadBestFitCustomAttribute(pMD->GetMDImport(),
        pMD->GetClass()->GetCl(),
        BestFit, ThrowOnUnmappableChar);
}

VOID
ReadBestFitCustomAttribute(IMDInternalImport* pInternalImport, mdTypeDef cl, BOOL* BestFit, BOOL* ThrowOnUnmappableChar)
{
    HRESULT hr;
    BYTE* pData;
    ULONG cbCount;

    // Set the attributes to their defaults, just to be safe.
    *BestFit = TRUE;
    *ThrowOnUnmappableChar = FALSE;
    
    _ASSERTE(pInternalImport);
    _ASSERTE(cl);

    // A well-formed BestFitMapping attribute will have at least 5 bytes
    // 1,2 for the prolog (should equal 0x1, 0x0)
    // 3 for the BestFitMapping bool
    // 4,5 for the number of named parameters (will be 0 if ThrowOnUnmappableChar doesn't exist)
    // 6 - 29 for the descrpition of ThrowOnUnmappableChar
    // 30 for the ThrowOnUnmappable char bool
    
    // Try the assembly first
    hr = pInternalImport->GetCustomAttributeByName(TokenFromRid(1, mdtAssembly), INTEROP_BESTFITMAPPING_TYPE, (const VOID**)(&pData), &cbCount);
    if ((hr == S_OK) && (pData) && (cbCount > 4) && (pData[0] == 1) && (pData[1] == 0))
    {
        _ASSERTE((cbCount == 30) || (cbCount == 5));
    
        // index to 2 to skip prolog
        *BestFit = pData[2] != 0;

        // if the optional named argument exists
        if (cbCount == 30)
            // index to end of data to skip description of named argument
            *ThrowOnUnmappableChar = pData[29] != 0;
    }

    // Now try the interface/class/struct
    hr = pInternalImport->GetCustomAttributeByName(cl, INTEROP_BESTFITMAPPING_TYPE, (const VOID**)(&pData), &cbCount);
    if ((hr == S_OK) && (pData) && (cbCount > 4) && (pData[0] == 1) && (pData[1] == 0))
    {
        _ASSERTE((cbCount == 30) || (cbCount == 5));
    
        // index to 2 to skip prolog    
        *BestFit = pData[2] != 0;
        
        // if the optional named argument exists
        if (cbCount == 30)
            // index to end of data to skip description of named argument
            *ThrowOnUnmappableChar = pData[29] != 0;
    }
}



//--------------------------------------------------------------------------
// BOOL ReconnectWrapper(switchCCWArgs* pArgs)
// switch objects for this wrapper
// used by JIT&ObjectPooling to ensure a deactivated CCW can point to a new object
// during reactivate
//--------------------------------------------------------------------------
BOOL ReconnectWrapper(switchCCWArgs* pArgs)
{
    OBJECTREF oldref = pArgs->oldtp;
    OBJECTREF neworef = pArgs->newtp;

    _ASSERTE(oldref != NULL);
    EEClass* poldClass = oldref->GetTrueClass();
    
    _ASSERTE(neworef != NULL);
    EEClass* pnewClass = neworef->GetTrueClass();
    
    _ASSERTE(pnewClass == poldClass);

    // grab the sync block for the current object
    SyncBlock* poldSyncBlock = oldref->GetSyncBlockSpecial();
    _ASSERTE(poldSyncBlock);

    // get the wrapper for the old object
    ComCallWrapper* pWrap = poldSyncBlock->GetComCallWrapper();
    _ASSERTE(pWrap != NULL);
    // @TODO verify the appdomains amtch
        
    // remove the _comData from syncBlock
    poldSyncBlock->SetComCallWrapper(NULL);

    // get the syncblock for the new object
    SyncBlock* pnewSyncBlock = pArgs->newtp->GetSyncBlockSpecial();
    _ASSERTE(pnewSyncBlock != NULL);
    _ASSERTE(pnewSyncBlock->GetComCallWrapper() == NULL);
        
    // store the new server object in our handle
    StoreObjectInHandle(pWrap->m_ppThis, pArgs->newtp);
    pnewSyncBlock->SetComCallWrapper(pWrap);

    // store other information about the new server
    SimpleComCallWrapper* pSimpleWrap = ComCallWrapper::GetSimpleWrapper(pWrap);
    _ASSERTE(pSimpleWrap);

    pSimpleWrap->ReInit(pnewSyncBlock);

    return TRUE;
}


#ifdef _DEBUG
//-------------------------------------------------------------------
// LOGGING APIS
//-------------------------------------------------------------------

static int g_TraceCount = 0;
static IUnknown* g_pTraceIUnknown = 0;

VOID IntializeInteropLogging()
{
    g_pTraceIUnknown = g_pConfig->GetTraceIUnknown();
    g_TraceCount = g_pConfig->GetTraceWrapper();
}

VOID LogInterop(LPSTR szMsg)
{
    LOG( (LF_INTEROP, LL_INFO10, "%s\n",szMsg) );
}

VOID LogInterop(LPWSTR wszMsg)
{
    LOG( (LF_INTEROP, LL_INFO10, "%ws\n",wszMsg) );
}

//-------------------------------------------------------------------
// VOID LogComPlusWrapperMinorCleanup(ComPlusWrapper* pWrap, IUnknown* pUnk)
// log wrapper minor cleanup
//-------------------------------------------------------------------
VOID LogComPlusWrapperMinorCleanup(ComPlusWrapper* pWrap, IUnknown* pUnk)
{
    static int dest_count = 0;
    dest_count++;

    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pUnk)
    {
        LPVOID pCurrCtx = GetCurrentCtxCookie();
        LOG( (LF_INTEROP,
            LL_INFO10,
            "Minor Cleanup ComPlusWrapper: Wrapper %p #%d IUnknown %p Context: %p\n",
            pWrap, dest_count,
            pUnk,
            pCurrCtx) );
    }
}

//-------------------------------------------------------------------
// VOID LogComPlusWrapperDestroy(ComPlusWrapper* pWrap, IUnknown* pUnk)
// log wrapper destroy
//-------------------------------------------------------------------
VOID LogComPlusWrapperDestroy(ComPlusWrapper* pWrap, IUnknown* pUnk)
{
    static int dest_count = 0;
    dest_count++;

    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pUnk)
    {
        LPVOID pCurrCtx = GetCurrentCtxCookie();
        LOG( (LF_INTEROP,
            LL_INFO10,
            "Destroy ComPlusWrapper: Wrapper %p #%d IUnknown %p Context: %p\n",
            pWrap, dest_count,
            pUnk,
            pCurrCtx) );
    }
}

//-------------------------------------------------------------------
// VOID LogComPlusWrapperCreate(ComPlusWrapper* pWrap, IUnknown* pUnk)
// log wrapper create
//-------------------------------------------------------------------
VOID LogComPlusWrapperCreate(ComPlusWrapper* pWrap, IUnknown* pUnk)
{
    static int count = 0;
    LPVOID pCurrCtx = GetCurrentCtxCookie();

    // pre-increment the count, so it can never be zero
    count++;

    if (count == g_TraceCount)
    {
        g_pTraceIUnknown = pUnk;
    }

    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pUnk)
    {
        LOG( (LF_INTEROP,
            LL_INFO10,
            "Create ComPlusWrapper: Wrapper %p #%d IUnknown:%p Context %p\n",
            pWrap, count,
            pUnk,
            pCurrCtx) );
    }
}

//-------------------------------------------------------------------
// VOID LogInteropLeak(IUnkEntry * pEntry)
//-------------------------------------------------------------------
VOID LogInteropLeak(IUnkEntry * pEntry)
{
    // log this miss
    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pEntry->m_pUnknown)
    {
        LOG( (LF_INTEROP,
            LL_INFO10,
            "IUnkEntry Leak: %p Context: %p\n",
                    pEntry->m_pUnknown,
                    pEntry->m_pCtxCookie)
                    );
    }
}

//-------------------------------------------------------------------
//  VOID LogInteropRelease(IUnkEntry* pEntry)
//-------------------------------------------------------------------
VOID LogInteropRelease(IUnkEntry* pEntry)
{
    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pEntry->m_pUnknown)
    {
         LOG( (LF_INTEROP,
                LL_EVERYTHING,
                "IUnkEntry Release: %pd Context: %pd\n",
            pEntry->m_pUnknown,
            pEntry->m_pCtxCookie) );
    }
}

//-------------------------------------------------------------------
//      VOID LogInteropLeak(InterfaceEntry * pEntry)
//-------------------------------------------------------------------

VOID LogInteropLeak(InterfaceEntry * pEntry)
{
    // log this miss
    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pEntry->m_pUnknown)
    {
            LOG( (LF_INTEROP,
            LL_INFO10,
            "InterfaceEntry Leak: %pd MethodTable: %s Context: %pd\n",
                pEntry->m_pUnknown,
                (pEntry->m_pMT
                    ? pEntry->m_pMT->GetClass()->m_szDebugClassName
                    : "<no name>"),
                GetCurrentCtxCookie()) );
    }
}

//-------------------------------------------------------------------
//  VOID LogInteropRelease(InterfaceEntry* pEntry)
//-------------------------------------------------------------------

VOID LogInteropRelease(InterfaceEntry* pEntry)
{
    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pEntry->m_pUnknown)
    {
        LOG( (LF_INTEROP,
            LL_EVERYTHING,
            "InterfaceEntry Release: %pd MethodTable: %s Context: %pd\n",
                pEntry->m_pUnknown,
                (pEntry->m_pMT
                    ? pEntry->m_pMT->GetClass()->m_szDebugClassName
                    : "<no name>"),
                GetCurrentCtxCookie()) );
    }
}

//-------------------------------------------------------------------
//  VOID LogInteropLeak(IUnknown* pUnk)
//-------------------------------------------------------------------

VOID LogInteropLeak(IUnknown* pUnk)
{
    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pUnk)
    {
        LPVOID pCurrCtx = GetCurrentCtxCookie();

        LOG( (LF_INTEROP,
        LL_INFO10,
        "Leak: %pd Context %pd\n",
            pUnk,
            pCurrCtx) );
    }
}


//-------------------------------------------------------------------
//  VOID LogInteropRelease(IUnknown* pUnk, ULONG cbRef, LPSTR szMsg)
//-------------------------------------------------------------------

VOID LogInteropRelease(IUnknown* pUnk, ULONG cbRef, LPSTR szMsg)
{
    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pUnk)
    {
        LPVOID pCurrCtx = GetCurrentCtxCookie();

        LOG( (LF_INTEROP,
            LL_EVERYTHING,
            "Release: %pd Context %pd Refcount: %d Msg: %s\n",
            pUnk,
            pCurrCtx, cbRef, szMsg) );
    }
}

//-------------------------------------------------------------------
//  VOID LogInteropAddRef(IUnknown* pUnk, ULONG cbRef, LPSTR szMsg)
//-------------------------------------------------------------------

VOID LogInteropAddRef(IUnknown* pUnk, ULONG cbRef, LPSTR szMsg)
{
    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pUnk)
    {
        LPVOID pCurrCtx = GetCurrentCtxCookie();

        LOG( (LF_INTEROP,
            LL_EVERYTHING,
            "AddRef: %pd Context: %pd Refcount: %d Msg: %s\n",
            pUnk,
            pCurrCtx, cbRef, szMsg) );
    }
}

//-------------------------------------------------------------------
//  VOID LogInteropQI(IUnknown* pUnk, REFIID iid, HRESULT hr, LPSTR szMsg)
//-------------------------------------------------------------------

VOID LogInteropQI(IUnknown* pUnk, REFIID iid, HRESULT hr, LPSTR szMsg)
{
    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pUnk)
    {
        LPVOID pCurrCtx = GetCurrentCtxCookie();

        LOG( (LF_INTEROP,
            LL_EVERYTHING,
        "QI: %pd Context %pd  HR= %pd Msg: %s\n",
            pUnk,
            pCurrCtx, hr, szMsg) );
    }
}

//-------------------------------------------------------------------
// VOID LogInterop(CacheEntry * pEntry, InteropLogType fLogType)
// log interop
//-------------------------------------------------------------------

VOID LogInterop(InterfaceEntry * pEntry, InteropLogType fLogType)
{
    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pEntry->m_pUnknown)
    {
        if (fLogType == LOG_LEAK)
        {
            LogInteropLeak(pEntry);
        }
        else if (fLogType == LOG_RELEASE)
        {
            LogInteropRelease(pEntry);
        }
    }
}

VOID LogInteropScheduleRelease(IUnknown* pUnk, LPSTR szMsg)
{
    if (g_pTraceIUnknown == 0 || g_pTraceIUnknown == pUnk)
    {
        LPVOID pCurrCtx = GetCurrentCtxCookie();

        LOG( (LF_INTEROP,
            LL_EVERYTHING,
            "ScheduleRelease: %pd Context %pd  Msg: %s\n",
            pUnk,
            pCurrCtx, szMsg) );
    }
}

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\intfhelper.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#ifndef _H_INTFHELPER_
#define _H_INTFHELPER_

class MethodTable;
enum InterfaceStubType
{
	IFACE_INVALID = 0,
	IFACE_ONEIMPL = 1,
	IFACE_GENERIC = 2,
	IFACE_FASTGENERIC = 3
};

extern void * g_FailDispatchStub;
#pragma pack(push)
#pragma pack(1)


// specialized method tables that we setup for every interface that is introduced
// in a heirarchy.
// WARNING: Note below that we assume that all pointers to this struct actually
// point immediately after it.  See the this adjustments below.
struct InterfaceInvokeHelper
{
private:
	// don't construct me directly
	InterfaceInvokeHelper()
	{
	}

public:
    MethodTable*    m_pMTIntfClass;		// interface class method table
	WORD			m_wstartSlot;		// the start slot for this interface
	WORD			pad;

	VOID		AddRef()
	{
		Stub::RecoverStub((const BYTE*)(this-1))->IncRef();
	}

	BOOL		Release()
	{
		return Stub::RecoverStub((const BYTE*)(this-1))->DecRef();
	}
	
	// can't get rid of stubs now
	BOOL		ReleaseThreadSafe();

	WORD GetStartSlot()
	{
		// Offset into the vtable where interface begins		
		return (this-1)->m_wstartSlot;
	}

	void SetStartSlot(WORD wstartSlot)
	{
		(this-1)->m_wstartSlot = wstartSlot;
	}

	void *GetEntryPoint() const
    {
        return (void *)this;
    }

    MethodTable* GetInterfaceClassMethodTable()
    {
        return (this-1)->m_pMTIntfClass;
    }

    void SetInterfaceClassMethodTable(MethodTable *pMTIntfClass)
    {
        (this-1)->m_pMTIntfClass = pMTIntfClass;
    }
    
    // Warning!  We do this from ASM, too.  So be sure to keep our inline ASM in
    // sync with any changes here:
	static InterfaceInvokeHelper* RecoverHelper(void *pv)
	{
		_ASSERTE(pv != NULL);
        _ASSERTE(pv != g_FailDispatchStub);
		return (InterfaceInvokeHelper *) pv;
	}

};

#pragma pack(pop)

// get helper for interface for introducing class
InterfaceInvokeHelper* GetInterfaceInvokeHelper(MethodTable* pMTIntfClass, 
												EEClass* pEEObjClass, 
												WORD startSlot);

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\interoputil.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#ifndef _H_INTEROP_UTIL
#define _H_INTEROP_UTIL

#include "DebugMacros.h"
#include "InteropConverter.h"


struct VariantData;
struct ComMethodTable;
class TypeHandle;
interface IStream;


// System.Drawing.Color struct definition.
#pragma pack(push)
#pragma pack(1)

struct SYSTEMCOLOR
{
    INT64 value;
    short knownColor;
    short state;
    STRINGREF name;
};

#pragma pack(pop)


// HR to exception helper.
#ifdef _DEBUG

#define IfFailThrow(EXPR) \
do { hr = (EXPR); if(FAILED(hr)) { DebBreakHr(hr); COMPlusThrowHR(hr); } } while (0)

#else // _DEBUG

#define IfFailThrow(EXPR) \
do { hr = (EXPR); if(FAILED(hr)) { COMPlusThrowHR(hr); } } while (0)

#endif // _DEBUG


// Out of memory helper.
#define IfNullThrow(EXPR) \
do {if ((EXPR) == 0) {IfFailThrow(E_OUTOFMEMORY);} } while (0)


// Helper to determine the version number from an int.
#define GET_VERSION_USHORT_FROM_INT(x) x > (INT)((USHORT)-1) ? 0 : x


// This is the context flags that are passed to CoCreateInstance. This defined
// should be used throught the runtime in all calls to CoCreateInstance.
#define EE_COCREATE_CLSCTX_FLAGS CLSCTX_SERVER

// The format string to use to format unknown members to be passed to
// invoke member
#define DISPID_NAME_FORMAT_STRING                       L"[DISPID=%i]"

//--------------------------------------------------------------------------------
// helper for DllCanUnload now
HRESULT STDMETHODCALLTYPE EEDllCanUnloadNow(void);

struct ExceptionData;
//-------------------------------------------------------------------
// void FillExceptionData(ExceptionData* pedata, IErrorInfo* pErrInfo)
// Called from DLLMain, to initialize com specific data structures.
//-------------------------------------------------------------------
void FillExceptionData(ExceptionData* pedata, IErrorInfo* pErrInfo);

//------------------------------------------------------------------
//  HRESULT SetupErrorInfo(OBJECTREF pThrownObject)
// setup error info for exception object
//
HRESULT SetupErrorInfo(OBJECTREF pThrownObject);

//-------------------------------------------------------------
// Given an IErrorInfo pointer created on a com exception obect
// obtain the hresult stored in the exception object
HRESULT GetHRFromComPlusErrorInfo(IErrorInfo* pErr);

//--------------------------------------------------------------------------------
// Init and Terminate functions, one time use
BOOL InitializeCom();
#ifdef SHOULD_WE_CLEANUP
void TerminateCom();
#endif /* SHOULD_WE_CLEANUP */

//--------------------------------------------------------------------------------
// Clean up Helpers
//--------------------------------------------------------------------------------
// called by syncblock, on the finalizer thread to do major cleanup
void CleanupSyncBlockComData(LPVOID pv);
// called by syncblock, during GC, do only minimal work
void MinorCleanupSyncBlockComData(LPVOID pv);

//--------------------------------------------------------------------------------
// Marshalling Helpers
//--------------------------------------------------------------------------------

// enum to specify the direction of marshalling
enum Dir
{
    in = 0,
    out = 1
};


//--------------------------------------------------------------------------------
// Determines if a COM object can be cast to the specified type.
BOOL CanCastComObject(OBJECTREF obj, TypeHandle hndType);


//---------------------------------------------------------
// Read the BestFit custom attribute info from 
// both assembly level and interface level
//---------------------------------------------------------
VOID ReadBestFitCustomAttribute(MethodDesc* pMD, BOOL* BestFit, BOOL* ThrowOnUnmappableChar);
VOID ReadBestFitCustomAttribute(IMDInternalImport* pInternalImport, mdTypeDef cl, BOOL* BestFit, BOOL* ThrowOnUnmappableChar);


//--------------------------------------------------------------------------------
// GC Safe Helpers
//--------------------------------------------------------------------------------

//--------------------------------------------------------------------------------
// HRESULT SafeQueryInterface(IUnknown* pUnk, REFIID riid, IUnknown** pResUnk)
// QI helper, enables and disables GC during call-outs
HRESULT SafeQueryInterface(IUnknown* pUnk, REFIID riid, IUnknown** pResUnk);

//--------------------------------------------------------------------------------
// ULONG SafeAddRef(IUnknown* pUnk)
// AddRef helper, enables and disables GC during call-outs
ULONG SafeAddRef(IUnknown* pUnk);

//--------------------------------------------------------------------------------
// ULONG SafeRelease(IUnknown* pUnk)
// Release helper, enables and disables GC during call-outs
ULONG SafeRelease(IUnknown* pUnk);

//--------------------------------------------------------------------------------
// void SafeVariantClear(VARIANT* pVar)
// VariantClear helper GC safe.
void SafeVariantClear(VARIANT* pVar);

//--------------------------------------------------------------------------------
// void SafeVariantInit(VARIANT* pVar)
// VariantInit helper GC safe.
void SafeVariantInit(VARIANT* pVar);

//--------------------------------------------------------------------------------
// // safe VariantChangeType
// Release helper, enables and disables GC during call-outs
HRESULT SafeVariantChangeType(VARIANT* pVarRes, VARIANT* pVarSrc,
                              unsigned short wFlags, VARTYPE vt);

//--------------------------------------------------------------------------------
// safe DispGetParam
// Release helper, enables and disables GC during call-outs
HRESULT SafeDispGetParam(DISPPARAMS* pdispparams, unsigned argNum, 
                         VARTYPE vt, VARIANT* pVar, unsigned int *puArgErr);


//--------------------------------------------------------------------------------
// // safe VariantChangeTypeEx
// Release helper, enables and disables GC during call-outs
HRESULT SafeVariantChangeTypeEx (VARIANT* pVarRes, VARIANT* pVarSrc,
                          LCID lcid, unsigned short wFlags, VARTYPE vt);


//--------------------------------------------------------------------------------
// void SafeReleaseStream(IStream *pStream)
// Releases the data in the stream and then releases the stream itself.
void SafeReleaseStream(IStream *pStream);

//--------------------------------------------------------------------------------
// Ole RPC seems to return an inconsistent SafeArray for arrays created with
// SafeArrayVector(VT_BSTR). OleAut's SafeArrayGetVartype() doesn't notice
// the inconsistency and returns a valid-seeming (but wrong vartype.)
// Our version is more discriminating. This should only be used for
// marshaling scenarios where we can assume unmanaged code permissions
// (and hence are already in a position of trusting unmanaged data.)

HRESULT ClrSafeArrayGetVartype(SAFEARRAY *psa, VARTYPE *vt);


//Helpers
// Is the tear-off a com+ created tear-off
UINTPTR IsComPlusTearOff(IUnknown* pUnk);


// Convert an IUnknown to CCW, returns NULL if the pUnk is not on
// a managed tear-off (OR) if the pUnk is to a managed tear-off that
// has been aggregated
ComCallWrapper* GetCCWFromIUnknown(IUnknown* pUnk);
// is the tear-off represent one of the standard interfaces such as IProvideClassInfo, IErrorInfo etc.
UINTPTR IsSimpleTearOff(IUnknown* pUnk);
// Is the tear-off represent the inner unknown or the original unknown for the object
UINTPTR IsInnerUnknown(IUnknown* pUnk);


class FieldDesc;
//------------------------------------------------------------------------------
// INT64 FieldAccessor(FieldDesc* pFD, OBJECTREF oref, INT64 val, BOOL isGetter, UINT8 cbSize)
// helper to access fields from an object
INT64 FieldAccessor(FieldDesc* pFD, OBJECTREF oref, INT64 val, BOOL isGetter, U1 cbSize);

//------------------------------------------------------------------------------
// BOOL IsInstanceOf(MethodTable *pMT, MethodTable* pParentMT)
BOOL IsInstanceOf(MethodTable *pMT, MethodTable* pParentMT);

//---------------------------------------------------------------------------
// BOOL IsIClassX(MethodTable *pMT, REFIID riid, ComMethodTable **ppComMT);
//  is the iid represent an IClassX for this class
BOOL IsIClassX(MethodTable *pMT, REFIID riid, ComMethodTable **ppComMT);

//---------------------------------------------------------------------------
// void CleanupCCWTemplates(LPVOID pWrap);
//  Cleanup com data stored in EEClass
void CleanupCCWTemplate(LPVOID pWrap);

//---------------------------------------------------------------------------
// void CleanupComclassfac(LPVOID pWrap);
//  Cleanup com data stored in EEClass
void CleanupComclassfac(LPVOID pWrap);

//---------------------------------------------------------------------------
//  Unloads any com data associated with a class when class is unloaded
void UnloadCCWTemplate(LPVOID pWrap);

//---------------------------------------------------------------------------
//  Unloads any com data associated with a class when class is unloaded
void UnloadComclassfac(LPVOID pWrap);



//---------------------------------------------------------------------------
// OBJECTREF AllocateComObject_ForManaged(MethodTable* pMT)
//  Cleanup com data stored in EEClass
OBJECTREF AllocateComObject_ForManaged(MethodTable* pMT);


//---------------------------------------------------------------------------
// EEClass* GetEEClassForCLSID(REFCLSID rclsid)
//  get/load EEClass for a given clsid
EEClass* GetEEClassForCLSID(REFCLSID rclsid, BOOL* pfAssemblyInReg = NULL);


//---------------------------------------------------------------------------
// EEClass* GetEEValueClassForGUID(REFCLSID rclsid)
//  get/load a value class for a given guid
EEClass* GetEEValueClassForGUID(REFCLSID guid);


// This method determines if a type is visible from COM or not based on 
// its visibility. This version of the method works with a type handle.
BOOL IsTypeVisibleFromCom(TypeHandle hndType);

//---------------------------------------------------------------------------
// This method determines if a member is visible from COM.
BOOL IsMemberVisibleFromCom(IMDInternalImport *pInternalImport, mdToken tk, mdMethodDef mdAssociate);


//---------------------------------------------------------------------------
// Returns the index of the LCID parameter if one exists and -1 otherwise.
int GetLCIDParameterIndex(IMDInternalImport *pInternalImport, mdMethodDef md);

//---------------------------------------------------------------------------
// Transforms an LCID into a CultureInfo.
void GetCultureInfoForLCID(LCID lcid, OBJECTREF *pCultureObj);

//---------------------------------------------------------------------------
// This method returns the default interface for the class as well as the 
// type of default interface we are dealing with.
enum DefaultInterfaceType
{
    DefaultInterfaceType_Explicit       = 0,
    DefaultInterfaceType_IUnknown       = 1,
    DefaultInterfaceType_AutoDual       = 2,
    DefaultInterfaceType_AutoDispatch   = 3,
    DefaultInterfaceType_BaseComClass   = 4
};
DefaultInterfaceType GetDefaultInterfaceForClass(TypeHandle hndClass, TypeHandle *pHndDefClass);
HRESULT TryGetDefaultInterfaceForClass(TypeHandle hndClass, TypeHandle *pHndDefClass, DefaultInterfaceType *pDefItfType);

//---------------------------------------------------------------------------
// This method retrieves the list of source interfaces for a given class.
void GetComSourceInterfacesForClass(MethodTable *pClassMT, CQuickArray<MethodTable *> &rItfList);

//--------------------------------------------------------------------------------
// These methods convert a managed IEnumerator to an IEnumVARIANT and vice versa.
OBJECTREF ConvertEnumVariantToMngEnum(IEnumVARIANT *pNativeEnum);
IEnumVARIANT *ConvertMngEnumToEnumVariant(OBJECTREF ManagedEnum);

//--------------------------------------------------------------------------------
// Helper method to determine is a type handle represents a System.Drawing.Color.
BOOL IsSystemColor(TypeHandle th);

//--------------------------------------------------------------------------------
// These methods convert an OLE_COLOR to a System.Color and vice versa.
void ConvertOleColorToSystemColor(OLE_COLOR SrcOleColor, SYSTEMCOLOR *pDestSysColor);
OLE_COLOR ConvertSystemColorToOleColor(SYSTEMCOLOR *pSrcSysColor);

//--------------------------------------------------------------------------------
// This method generates a stringized version of an interface that contains the
// name of the interface along with the signature of all the methods.
SIZE_T GetStringizedItfDef(TypeHandle InterfaceType, CQuickArray<BYTE> &rDef);

//--------------------------------------------------------------------------------
// This method generates a stringized version of a class interface that contains 
// the signatures of all the methods and fields.
ULONG GetStringizedClassItfDef(TypeHandle InterfaceType, CQuickArray<BYTE> &rDef);

//--------------------------------------------------------------------------------
// Helper to get the GUID of a class interface.
void GenerateClassItfGuid(TypeHandle InterfaceType, GUID *pGuid);
// Try/Catch wrapped version of the method.
HRESULT TryGenerateClassItfGuid(TypeHandle InterfaceType, GUID *pGuid);

//--------------------------------------------------------------------------------
// Helper to get the stringized form of typelib guid.
HRESULT GetStringizedTypeLibGuidForAssembly(Assembly *pAssembly, CQuickArray<BYTE> &rDef, ULONG cbCur, ULONG *pcbFetched);

//--------------------------------------------------------------------------------
// Helper to get the GUID of the typelib that is created from an assembly.
HRESULT GetTypeLibGuidForAssembly(Assembly *pAssembly, GUID *pGuid);

//--------------------------------------------------------------------------------
// InvokeDispMethod will convert a set of managed objects and call IDispatch.  The
//	result will be returned as a COM+ Variant pointed to by pRetVal.
void IUInvokeDispMethod(OBJECTREF* pReflectClass, OBJECTREF* pTarget,OBJECTREF* pName, DISPID *pMemberID, OBJECTREF* pArgs, OBJECTREF* pModifiers, 
                        OBJECTREF* pNamedArgs, OBJECTREF* pRetVal, LCID lcid, int flags, BOOL bIgnoreReturn, BOOL bIgnoreCase);

//---------------------------------------------------------------------------
//		SYNC BLOCK data helpers
// SyncBlock has a void* to represent COM data
// the following helpers are used to distinguish the different types of
// wrappers stored in the sync block data
class ComCallWrapper;
struct ComPlusWrapper;
BOOL IsComPlusWrapper(void *pComData);
BOOL IsComClassFactory(void*pComData);
ComPlusWrapper* GetComPlusWrapper(void *pComData);
VOID LinkWrappers(ComCallWrapper* pComWrap, ComPlusWrapper* pPlusWrap);

// COM interface pointers are cached in the GIT table
// the following union abstracts the possible variations

union StreamOrCookie    // info needed to locate IP for correct apartment
{
    IStream *m_pMarshalStream;  // marshal/unmarshal via stream
    DWORD    m_dwGITCookie;     // marshal/unmarshal via GIT
    IUnknown*m_pUnk;            // use raw IP, don't marshal
};
	
// loggin APIs
struct ComPlusWrapper;


enum InteropLogType
{
	LOG_RELEASE = 1,
	LOG_LEAK    = 2
};

struct IUnkEntry;

struct InterfaceEntry;

#ifdef _DEBUG

VOID LogComPlusWrapperMinorCleanup(ComPlusWrapper* pWrap, IUnknown* pUnk);
VOID LogComPlusWrapperDestroy(ComPlusWrapper* pWrap, IUnknown* pUnk);
VOID LogComPlusWrapperCreate(ComPlusWrapper* pWrap, IUnknown* pUnk);
VOID LogInteropLeak(IUnkEntry * pEntry);
VOID LogInteropRelease(IUnkEntry * pEntry);
VOID LogInteropLeak(InterfaceEntry * pEntry);
VOID LogInteropRelease(InterfaceEntry * pEntry);
VOID LogInterop(InterfaceEntry * pEntry, InteropLogType fLogType);

VOID LogInteropQI(IUnknown* pUnk, REFIID riid, HRESULT hr, LPSTR szMsg);

VOID LogInteropAddRef(IUnknown* pUnk, ULONG cbRef, LPSTR szMsg);
VOID LogInteropRelease(IUnknown* pUnk, ULONG cbRef, LPSTR szMsg);
VOID LogInteropLeak(IUnknown* pUnk);

VOID LogInterop(LPSTR szMsg);
VOID LogInterop(LPWSTR szMsg);

VOID LogInteropScheduleRelease(IUnknown* pUnk, LPSTR szMsg);

#else
__inline VOID LogComPlusWrapperMinorCleanup(ComPlusWrapper* pWrap, IUnknown* pUnk) {}
__inline VOID LogComPlusWrapperDestroy(ComPlusWrapper* pWrap, IUnknown* pUnk) {}
__inline VOID LogComPlusWrapperCreate(ComPlusWrapper* pWrap, IUnknown* pUnk) {}
__inline VOID LogInteropLeak(IUnkEntry * pEntry) {}
__inline VOID LogInteropRelease(IUnkEntry * pEntry) {}
__inline VOID LogInteropQueue(IUnkEntry * pEntry) {}
__inline VOID LogInteropLeak(InterfaceEntry * pEntry) {}
__inline VOID LogInteropQueue(InterfaceEntry * pEntry) {}
__inline VOID LogInteropRelease(InterfaceEntry * pEntry) {}
__inline VOID LogInterop(InterfaceEntry * pEntry, InteropLogType fLogType) {}
__inline VOID LogInteropQI(IUnknown* pUnk, REFIID riid, HRESULT hr, LPSTR szMsg) {}
__inline VOID LogInteropAddRef(IUnknown* pUnk, ULONG cbRef, LPSTR szMsg) {}
__inline VOID LogInteropRelease(IUnknown* pUnk, ULONG cbRef, LPSTR szMsg) {}
__inline VOID LogInteropLeak(IUnknown* pUnk) {}
__inline VOID LogInterop(LPSTR szMsg) {}
__inline VOID LogInterop(LPWSTR szMsg) {}
__inline VOID LogInteropScheduleRelease(IUnknown* pUnk, LPSTR szMsg) {}
#endif

HRESULT QuickCOMStartup();


//--------------------------------------------------------------------------------
// BOOL ExtendsComImport(MethodTable* pMT);
// check if the class is OR extends a COM Imported class
BOOL ExtendsComImport(MethodTable* pMT);

//--------------------------------------------------------------------------------
// HRESULT GetCLSIDFromProgID(WCHAR *strProgId, GUID *pGuid);
// Gets the CLSID from the specified Prog ID.
HRESULT GetCLSIDFromProgID(WCHAR *strProgId, GUID *pGuid);

//--------------------------------------------------------------------------------
// OBJECTREF GCProtectSafeRelease(OBJECTREF oref, IUnknown* pUnk)
// Protect the reference while calling SafeRelease
OBJECTREF GCProtectSafeRelease(OBJECTREF oref, IUnknown* pUnk);

//--------------------------------------------------------------------------------
// MethodTable* GetClassFromIProvideClassInfo(IUnknown* pUnk)
//	Check if the pUnk implements IProvideClassInfo and try to figure
// out the class from there
MethodTable* GetClassFromIProvideClassInfo(IUnknown* pUnk);


// ULONG GetOffsetOfReservedForOLEinTEB()
// HELPER to determine the offset of OLE struct in TEB
ULONG GetOffsetOfReservedForOLEinTEB();

// ULONG GetOffsetOfContextIDinOLETLS()
// HELPER to determine the offset of Context in OLE TLS struct
ULONG GetOffsetOfContextIDinOLETLS();

// Global process GUID to identify the process
BSTR GetProcessGUID();


// Helper class to Auto Release interfaces in case of exceptions
template <class T>
class TAutoItf
{
	T* m_pUnk;

#ifdef _DEBUG
	LPSTR m_szMsg;
#endif

public:
	
	// constructor
	TAutoItf(T* pUnk)
	{
		m_pUnk = pUnk;
	}

	// assignment operator
	TAutoItf& operator=(IUnknown* pUnk)
	{
		_ASSERTE(m_pUnk == NULL);
		m_pUnk = pUnk;
		return *this;
	}

	operator T*()
	{
		return m_pUnk;
	}

	VOID InitUnknown(T* pUnk)
	{
		_ASSERTE(m_pUnk == NULL);
		m_pUnk = pUnk;
	}

	VOID InitMsg(LPSTR szMsg)
	{
		#ifdef _DEBUG
			m_szMsg = szMsg;
		#endif
	}

	// force safe release of the itf
	VOID SafeReleaseItf()
	{
		if (m_pUnk)
		{
			ULONG cbRef = SafeRelease(m_pUnk);
			#ifdef _DEBUG
				LogInteropRelease(m_pUnk, cbRef, m_szMsg);
			#endif
		}
		m_pUnk = NULL;
	}

	// destructor
	~TAutoItf()
	{		
		if (m_pUnk)
		{
			SafeReleaseItf();
		}
	}



	T* UnHook()
	{
		T* pUnk = m_pUnk;
		m_pUnk = NULL;
		return pUnk;
	}

	T* operator->()
	{
		return m_pUnk;
	}
};

//--------------------------------------------------------------------------
// BOOL ReconnectWrapper(switchCCWArgs* pArgs);
// switch objects for this wrapper
// used by JIT&ObjectPooling to ensure a deactivated CCW can point to a new object
// during reactivate
//--------------------------------------------------------------------------

struct switchCCWArgs
{	
	DECLARE_ECALL_OBJECTREF_ARG( OBJECTREF, newtp );
	DECLARE_ECALL_OBJECTREF_ARG( OBJECTREF, oldtp );
};


BOOL ReconnectWrapper(switchCCWArgs* pArgs);

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\vm\invokeutil.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
////////////////////////////////////////////////////////////////////////////////
// This module defines a Utility Class used by reflection
//
// Author: Daryl Olander
// Date: March/April 1998
////////////////////////////////////////////////////////////////////////////////

#include "common.h"
#include "COMMember.h"
#include "COMString.h"
#include "corpriv.h"
#include "Method.hpp"
#include "threads.h"
#include "excep.h"
#include "gcscan.h"
#include "remoting.h"
#include "COMCodeAccessSecurityEngine.h"
#include "Security.h"
#include "field.h"
#include "COMClass.h"
#include "CustomAttribute.h"
#include "EEConfig.h"

// this file handles string conversion errors for itself
#undef  MAKE_TRANSLATIONFAILED


// Binder.ChangeType()
static LPCUTF8 szChangeType = "ChangeType";

// This is defined in COMSystem...
extern LPVOID GetArrayElementPtr(OBJECTREF a);

// The Attributes Table
//  20 bits for built in types and 12 bits for Properties
//  The properties are followed by the widening mask.  All types widen to them selves.
DWORD InvokeUtil::PrimitiveAttributes[PRIMITIVE_TABLE_SIZE][2] = {
    {0x01,      0x00},                     // ELEMENT_TYPE_END
    {0x02,      0x00},                     // ELEMENT_TYPE_VOID
    {0x04,      PT_Primitive | 0x0004},    // ELEMENT_TYPE_BOOLEAN
    {0x08,      PT_Primitive | 0x3F88},    // ELEMENT_TYPE_CHAR (W = U2, CHAR, I4, U4, I8, U8, R4, R8) (U2 == Char)
    {0x10,      PT_Primitive | 0x3550},    // ELEMENT_TYPE_I1   (W = I1, I2, I4, I8, R4, R8) 
    {0x20,      PT_Primitive | 0x3FE8},    // ELEMENT_TYPE_U1   (W = CHAR, U1, I2, U2, I4, U4, I8, U8, R4, R8)
    {0x40,      PT_Primitive | 0x3540},    // ELEMENT_TYPE_I2   (W = I2, I4, I8, R4, R8)
    {0x80,      PT_Primitive | 0x3F88},    // ELEMENT_TYPE_U2   (W = U2, CHAR, I4, U4, I8, U8, R4, R8)
    {0x0100,    PT_Primitive | 0x3500},    // ELEMENT_TYPE_I4   (W = I4, I8, R4, R8)
    {0x0200,    PT_Primitive | 0x3E00},    // ELEMENT_TYPE_U4   (W = U4, I8, R4, R8)
    {0x0400,    PT_Primitive | 0x3400},    // ELEMENT_TYPE_I8   (W = I8, R4, R8)
    {0x0800,    PT_Primitive | 0x3800},    // ELEMENT_TYPE_U8   (W = U8, R4, R8)
    {0x1000,    PT_Primitive | 0x3000},    // ELEMENT_TYPE_R4   (W = R4, R8)
    {0x2000,    PT_Primitive | 0x2000},    // ELEMENT_TYPE_R8   (W = R8) 
};
DWORD InvokeUtil::Attr_Mask     = 0xFF000000;
DWORD InvokeUtil::Widen_Mask    = 0x00FFFFFF;
MethodTable *InvokeUtil::_pParamArrayAttribute = NULL;

MethodDesc  *RefSecContext::s_pMethPrivateProcessMessage = NULL;
MethodTable *RefSecContext::s_pTypeRuntimeMethodInfo = NULL;
MethodTable *RefSecContext::s_pTypeMethodBase = NULL;
MethodTable *RefSecContext::s_pTypeRuntimeConstructorInfo = NULL;
MethodTable *RefSecContext::s_pTypeConstructorInfo = NULL;
MethodTable *RefSecContext::s_pTypeRuntimeType = NULL;
MethodTable *RefSecContext::s_pTypeType = NULL;
MethodTable *RefSecContext::s_pTypeRuntimeFieldInfo = NULL;
MethodTable *RefSecContext::s_pTypeFieldInfo = NULL;
MethodTable *RefSecContext::s_pTypeRuntimeEventInfo = NULL;
MethodTable *RefSecContext::s_pTypeEventInfo = NULL;
MethodTable *RefSecContext::s_pTypeRuntimePropertyInfo = NULL;
MethodTable *RefSecContext::s_pTypePropertyInfo = NULL;
MethodTable *RefSecContext::s_pTypeActivator = NULL;
MethodTable *RefSecContext::s_pTypeAppDomain = NULL;
MethodTable *RefSecContext::s_pTypeAssembly = NULL;
MethodTable *RefSecContext::s_pTypeTypeDelegator = NULL;
MethodTable *RefSecContext::s_pTypeDelegate = NULL;
MethodTable *RefSecContext::s_pTypeMulticastDelegate = NULL;

// InvokeUtil
//  This routine is the constructor for the InvokeUtil class.  It basically
//  creates the table which drives the various Argument conversion routines.
InvokeUtil::InvokeUtil()
{
    // Initialize the types
    _pVMTargetExcept = 0;
    _pVMClassLoadExcept = 0;
    _pBindSig = 0;
    _cBindSig = 0;
    _pBindModule = 0;
    _pMTCustomAttribute = 0;

    // These are the FieldDesc* that contain the fields for a pointer...
    _ptrValue = NULL;
    _ptrType = NULL;

    _voidPtr = g_Mscorlib.GetType(TYPE__VOID_PTR);

    _IntPtrValue = NULL;

    _UIntPtrValue = NULL;
}

void InvokeUtil:: InitPointers()
{
    // make sure we have already done this...
    if (_ptrType != 0) 
        return;

    _ptr = TypeHandle(g_Mscorlib.GetClass(CLASS__POINTER));
    _ptrValue = g_Mscorlib.GetField(FIELD__POINTER__VALUE);
    _ptrType = g_Mscorlib.GetField(FIELD__POINTER__TYPE);
    MethodTable *pLoadedClass = NULL;
    pLoadedClass = g_Mscorlib.GetClass(CLASS__TYPED_REFERENCE);
    _ASSERTE(pLoadedClass);
    pLoadedClass = g_Mscorlib.GetClass(CLASS__INTPTR);
    _ASSERTE(pLoadedClass);
}

void InvokeUtil::InitIntPtr() {
    if (_IntPtrValue!=NULL) {
        return;
    }
    InitPointers(); // insure the INTPTR class has been loaded
    _IntPtrValue = g_Mscorlib.GetField(FIELD__INTPTR__VALUE);
    _ASSERTE(_IntPtrValue);

    _UIntPtrValue = g_Mscorlib.GetField(FIELD__UINTPTR__VALUE);
    _ASSERTE(_UIntPtrValue);
}

ReflectClass* InvokeUtil::GetPointerType(OBJECTREF* pObj)
{
    InitPointers();
    REFLECTCLASSBASEREF o = (REFLECTCLASSBASEREF) _ptrType->GetRefValue(*pObj);
    ReflectClass* pRC = (ReflectClass*) o->GetData();
    return pRC;
}

void* InvokeUtil::GetPointerValue(OBJECTREF* pObj)
{
    InitPointers();
    void* value = (void*) _ptrValue->GetValuePtr(*pObj);
    return value;
}

void *InvokeUtil::GetIntPtrValue(OBJECTREF* pObj) {
    InitIntPtr();
    return (void *)_IntPtrValue->GetValuePtr(*pObj);
}

void *InvokeUtil::GetUIntPtrValue(OBJECTREF* pObj) {
    InitIntPtr();
    return (void *)_UIntPtrValue->GetValuePtr(*pObj);
}

void InvokeUtil::CheckArg(TypeHandle th, OBJECTREF* obj, RefSecContext *pSCtx)
{
    THROWSCOMPLUSEXCEPTION();

    CorElementType type = th.GetSigCorElementType();
    
    switch (type) {
    case ELEMENT_TYPE_SZARRAY:          // Single Dim
    case ELEMENT_TYPE_ARRAY:            // General Array
    case ELEMENT_TYPE_CLASS:            // Class
    case ELEMENT_TYPE_OBJECT:
    case ELEMENT_TYPE_STRING:           // System.String
    case ELEMENT_TYPE_VAR:
        {
            GCPROTECT_BEGININTERIOR(obj);
            CheckType(th, obj);
            GCPROTECT_END();
            return;
        }


    case ELEMENT_TYPE_BYREF:
        {
            // 
            //     (obj is the parameter passed to MethodInfo.Invoke, by the caller)
            //     if incoming argument, obj, is null
            //        if argument is a primitive
            //             Allocate a boxed object and place ref to it in 'obj'
            //        if argument is a value class
            //             Allocate an object of that valueclass, and place ref to it in 'obj'
            //

            TypeHandle thBaseType = th.AsTypeDesc()->GetTypeParam();
            
            if (*obj == NULL) {
                GCPROTECT_BEGININTERIOR(obj);
                CorElementType dstType = thBaseType.GetSigCorElementType();
                if (IsPrimitiveType(dstType)) {
                    _ASSERTE(!th.IsUnsharedMT());
                    INT64 value = 0;
                    SetObjectReferenceUnchecked(obj, GetBoxedObject(thBaseType, &value));
                }
                else if (dstType == ELEMENT_TYPE_VALUETYPE) {
                    SetObjectReferenceUnchecked(obj, AllocateObject(thBaseType.AsMethodTable()));
                }
                GCPROTECT_END();
            }
            else {
                GCPROTECT_BEGININTERIOR(obj);
                CheckType(thBaseType, obj);
                GCPROTECT_END();
            }
            return;
        }

    case ELEMENT_TYPE_PTR: 
    case ELEMENT_TYPE_FNPTR:
        {
            if (*obj == 0) {
                //if (!Security::CanSkipVerification(pSCtx->GetCallerMethod()->GetModule()))
                    //Security::ThrowSecurityException(g_SecurityPermissionClassName, SPFLAGSSKIPVERIFICATION);
                COMCodeAccessSecurityEngine::SpecialDemand(SECURITY_SKIP_VER);
                return;
            }

            GCPROTECT_BEGININTERIOR(obj);
            InitPointers();
            GCPROTECT_END();
            if ((*obj)->GetTypeHandle() == _ptr && type == ELEMENT_TYPE_PTR) {
                ReflectClass* pRC = GetPointerType(obj);

                TypeHandle srcTH = pRC->GetTypeHandle();
                if (th != _voidPtr) {
                    if (!srcTH.CanCastTo(th))
                        COMPlusThrow(kArgumentException,L"Arg_ObjObj");
                }

                //if (!Security::CanSkipVerification(pSCtx->GetCallerMethod()->GetModule()))
                    //Security::ThrowSecurityException(g_SecurityPermissionClassName, SPFLAGSSKIPVERIFICATION);
                COMCodeAccessSecurityEngine::SpecialDemand(SECURITY_SKIP_VER);
                return;
            }
            else if ((*obj)->GetTypeHandle().AsMethodTable() == g_Mscorlib.GetExistingClass(CLASS__INTPTR)) {
                //if (!Security::CanSkipVerification(pSCtx->GetCallerMethod()->GetModule()))
                    //Security::ThrowSecurityException(g_SecurityPermissionClassName, SPFLAGSSKIPVERIFICATION);
                COMCodeAccessSecurityEngine::SpecialDemand(SECURITY_SKIP_VER);
                return;
            }

            COMPlusThrow(kArgumentException,L"Arg_ObjObj");
        }
    }
}

void InvokeUtil::CopyArg(TypeHandle th, OBJECTREF *obj, void *pDst)
{
    THROWSCOMPLUSEXCEPTION();

    MethodTable* pMT;
    CorElementType oType;
    if (*obj != 0) {
        pMT = (*obj)->GetMethodTable();
        oType = pMT->GetNormCorElementType();
    }
    else {
        pMT = 0;
        oType = ELEMENT_TYPE_OBJECT;
    }
    CorElementType type = th.GetSigCorElementType();

    // This basically maps the Signature type our type and calls the CreatePrimitiveValue
    //  method.  We can nuke this if we get alignment on these types.
    switch (type) {
    case ELEMENT_TYPE_BOOLEAN:
    case ELEMENT_TYPE_I1:
    case ELEMENT_TYPE_U1:
    case ELEMENT_TYPE_I2:
    case ELEMENT_TYPE_U2:
    case ELEMENT_TYPE_CHAR:
    case ELEMENT_TYPE_I4:
    case ELEMENT_TYPE_U4:
    case ELEMENT_TYPE_I8:
    case ELEMENT_TYPE_U8:
    case ELEMENT_TYPE_I:
    case ELEMENT_TYPE_U:
    case ELEMENT_TYPE_R4:
    case ELEMENT_TYPE_R8:
        {
            // If we got the univeral zero...Then assign it and exit.
            if (*obj == 0) 
                InitValueClass(pDst, th.AsMethodTable());
            else
                CreatePrimitiveValue(type, oType, *obj, pDst);
        }
        return;

    case ELEMENT_TYPE_VALUETYPE:
        {
            // If we got the univeral zero...Then assign it and exit.
            if (*obj == 0) {
                EEClass* pBase = GetEEClass(th);
                int size = pBase->GetNumInstanceFieldBytes();
                void* pNewSrc = _alloca(size);
                memset(pNewSrc,0,size);
                CopyValueClassUnchecked(pDst, pNewSrc, pBase->GetMethodTable());
            }
            else {
                TypeHandle srcTH = (*obj)->GetTypeHandle();
                CreateValueTypeValue(th, pDst, oType, srcTH, *obj);
            }
        }
        return;

    case ELEMENT_TYPE_SZARRAY:          // Single Dim
    case ELEMENT_TYPE_ARRAY:            // General Array
    case ELEMENT_TYPE_CLASS:            // Class
    case ELEMENT_TYPE_OBJECT:
    case ELEMENT_TYPE_STRING:           // System.String
    case ELEMENT_TYPE_VAR:
        {
            if (*obj == 0) 
                *((OBJECTREF *)pDst) = NULL;
            else
                *((OBJECTREF *)pDst) = *obj;
        }
        return;

    case ELEMENT_TYPE_BYREF:
        {
           // 
           //     (obj is the parameter passed to MethodInfo.Invoke, by the caller)
           //     if argument is a primitive
           //     {
           //         if incoming argument, obj, is null
           //             Allocate a boxed object and place ref to it in 'obj'
           //         Unbox 'obj' and pass it to callee
           //     }
           //     if argument is a value class
           //     {
           //         if incoming argument, obj, is null
           //             Allocate an object of that valueclass, and place ref to it in 'obj'
           //         Unbox 'obj' and pass it to callee
           //     }
           //     if argument is an objectref
           //     {
           //         pass obj to callee
           //     }
           //

            TypeHandle thBaseType = th.AsTypeDesc()->GetTypeParam();
            TypeHandle srcTH = TypeHandle();
            if (*obj == 0 ) 
                oType = thBaseType.GetSigCorElementType();
            else
                srcTH = (*obj)->GetTypeHandle();
            CreateByRef(thBaseType, pDst, oType, srcTH, *obj, obj);
            return;
        }

    case ELEMENT_TYPE_TYPEDBYREF:
        {
            // If we got the univeral zero...Then assign it and exit.
            if (*obj == 0) {
                TypedByRef* ptr = (TypedByRef*) pDst;
                ptr->data = 0;
                ptr->type = TypeHandle();
            }
            else {
                TypeHandle srcTH = (*obj)->GetTypeHandle();
                CreateByRef(srcTH, pDst, oType, srcTH, *obj, obj);
                void* p = (char*) pDst + sizeof(void*);
                *(void**)p = (*obj)->GetTypeHandle().AsPtr();
            }
            return;
        }

    case ELEMENT_TYPE_PTR: 
    case ELEMENT_TYPE_FNPTR:
        {
            // If we got the univeral zero...Then assign it and exit.
            if (*obj == 0) {
                *((void **)pDst) = NULL;
            }
            else {
                if ((*obj)->GetTypeHandle() == _ptr && type == ELEMENT_TYPE_PTR) 
                    // because we are here only if obj is a System.Reflection.Pointer GetPointerVlaue()
                    // should not cause a gc (no transparent proxy). If otherwise we got a nasty bug here
                    *((void**)pDst) = GetPointerValue(obj);
                else if ((*obj)->GetTypeHandle().AsMethodTable() == g_Mscorlib.GetExistingClass(CLASS__INTPTR)) 
                    CreatePrimitiveValue(oType, oType, *obj, pDst);
                else
                    COMPlusThrow(kArgumentException,L"Arg_ObjObj");
            }
            return;
        }

    case ELEMENT_TYPE_VOID:
    default:
        _ASSERTE(!"Unknown Type");
        COMPlusThrow(kNotSupportedException);
    }
    FATAL_EE_ERROR();
}

// CreateTypedReference
// This routine fills the data that is passed in a typed reference
//  inside the signature.  We through an HRESULT if this fails
//  th -- the Type handle 
//  obj -- the object to put on the stack
//  pDst -- Pointer to the stack location where we copy the value

void InvokeUtil::CreateTypedReference(_ObjectToTypedReferenceArgs *args)
{
    THROWSCOMPLUSEXCEPTION();

    // @TODO: We fixed serious bugs in the code below very late in the endgame
    // for V1 RTM. So it was decided to disable this API (nobody would seem to
    // be using it anyway). When it's been decided that SetTypedReference has
    // had enough testing, the following line can be removed. RudiM.
    COMPlusThrow(kNotSupportedException);

    MethodTable* pMT;
    CorElementType oType;
    _ASSERTE(args->obj != 0);
    pMT = (args->obj)->GetMethodTable();
    oType = pMT->GetNormCorElementType();
    CorElementType type = args->th.GetSigCorElementType();

    // This basically maps the Signature type our type and calls the CreatePrimitiveValue
    //  method.  We can nuke this if we get alignment on these types.
    switch (type) {
    case ELEMENT_TYPE_BOOLEAN:
    case ELEMENT_TYPE_I1:
    case ELEMENT_TYPE_U1:
    case ELEMENT_TYPE_I2:
    case ELEMENT_TYPE_U2:
    case ELEMENT_TYPE_CHAR:
    case ELEMENT_TYPE_I4:
    case ELEMENT_TYPE_U4:
    case ELEMENT_TYPE_I8:
    case ELEMENT_TYPE_U8:
    case ELEMENT_TYPE_I:
    case ELEMENT_TYPE_U:
    case ELEMENT_TYPE_R4:
    case ELEMENT_TYPE_R8:
        {

            EEClass* pBase = GetEEClass(args->th);
            OBJECTREF srcObj = args->obj;
            GCPROTECT_BEGIN(srcObj);
            // Here is very tricky:
            // We protect srcObj.
            // If we have obj->GetTypeHandle in the argument, such as
            // CreateClassValue(th,pDst,vType,srcTH,srcObj,obj->GetData(),pos);
            // srcObj will be pushed to stack.  When obj->GetTypeHandle triggers GC, we protect
            // srcObj, but the one pushed to stack is not protected.
            CreatePrimitiveValue(type,oType,srcObj,args->typedReference.data);
            GCPROTECT_END();
        }
        return;
    case ELEMENT_TYPE_VALUETYPE:
        {

            EEClass* pBase = GetEEClass(args->th);
            OBJECTREF srcObj = args->obj;
            GCPROTECT_BEGIN(srcObj);
            // Here is very tricky:
            // We protect srcObj.
            // If we have obj->GetTypeHandle in the argument, such as
            // CreateClassValue(th,pDst,vType,srcTH,srcObj,obj->GetData(),pos);
            // srcObj will be pushed to stack.  When obj->GetTypeHandle triggers GC, we protect
            // srcObj, but the one pushed to stack is not protected.
            TypeHandle srcTH = srcObj->GetTypeHandle();
            CreateValueTypeValue(args->th, args->typedReference.data, oType, srcTH, srcObj);
            GCPROTECT_END();
        }
        return;

    case ELEMENT_TYPE_SZARRAY:          // Single Dim
    case ELEMENT_TYPE_ARRAY:            // General Array
    case ELEMENT_TYPE_CLASS:            // Class
    case ELEMENT_TYPE_OBJECT:
    case ELEMENT_TYPE_STRING:           // System.String
    case ELEMENT_TYPE_VAR:
        {
            // Copy the object
            SetObjectReferenceUnchecked((OBJECTREF*) args->typedReference.data, args->obj);
        }
        return;

    default:
        _ASSERTE(!"Unknown Type");
        COMPlusThrow(kNotSupportedException);
    }
    FATAL_EE_ERROR();
}

// CreatePrimitiveValue
// This routine will validate the object and then place the value into 
//  the destination
//  dstType -- The type of the destination
//  srcType -- The type of the source
//  srcObj -- The Object containing the primitive value.
//  pDst -- poiner to the destination
void InvokeUtil::CreatePrimitiveValue(CorElementType dstType, 
                                      CorElementType srcType,
                                      OBJECTREF srcObj,
                                      void *pDst)
{
    THROWSCOMPLUSEXCEPTION();

    if (!IsPrimitiveType(srcType) || !CanPrimitiveWiden(dstType,srcType))
        COMPlusThrow(kArgumentException,L"Arg_PrimWiden");

    INT64 data = 0;
    void* p = &data;
    void* pSrc = srcObj->UnBox();
 
    switch (srcType) {
    case ELEMENT_TYPE_I1:
        *(INT64 *)p = *(INT8 *)pSrc;
        break;
    case ELEMENT_TYPE_I2:
        *(INT64 *)p = *(INT16 *)pSrc;
        break;
    IN_WIN32(case ELEMENT_TYPE_I:)
    case ELEMENT_TYPE_I4:
        *(INT64 *)p = *(INT32 *)pSrc;
        break;
    IN_WIN64(case ELEMENT_TYPE_I:)
    case ELEMENT_TYPE_I8:
        *(INT64 *)p = *(INT64 *)pSrc;
        break;
    default:
        switch (srcObj->GetClass()->GetNumInstanceFieldBytes())
        {
        case 1:
            *(UINT8 *)p = *(UINT8 *)pSrc;
            break;
        case 2:
            *(UINT16 *)p = *(UINT16 *)pSrc;
            break;
        case 4:
            *(UINT32 *)p = *(UINT32 *)pSrc;
            break;
        case 8:
            *(UINT64 *)p = *(UINT64 *)pSrc;
            break;
        default:
            memcpy(p,pSrc,srcObj->GetClass()->GetNumInstanceFieldBytes());
            break;
        }
    }

    // Copy the data and return
    switch (dstType) {
    case ELEMENT_TYPE_BOOLEAN:
    case ELEMENT_TYPE_I1:
    case ELEMENT_TYPE_U1:
    case ELEMENT_TYPE_CHAR:
    case ELEMENT_TYPE_I2:
    case ELEMENT_TYPE_U2:
    case ELEMENT_TYPE_I4:
    case ELEMENT_TYPE_U4:
    IN_WIN32(case ELEMENT_TYPE_I:)
    IN_WIN32(case ELEMENT_TYPE_U:)
        *((INT32*)pDst) = *(INT32*)(p);
        break;
    case ELEMENT_TYPE_R4:
        switch (srcType) {
        case ELEMENT_TYPE_BOOLEAN:
        case ELEMENT_TYPE_I1:
        case ELEMENT_TYPE_I2:
        case ELEMENT_TYPE_I4:
        IN_WIN32(case ELEMENT_TYPE_I:)
           *((R4*)pDst) = (R4)(*(INT32*)(p));
            break;
        case ELEMENT_TYPE_U1:
        case ELEMENT_TYPE_CHAR:
        case ELEMENT_TYPE_U2:
        case ELEMENT_TYPE_U4:
        IN_WIN32(case ELEMENT_TYPE_U:)
           *((R4*)pDst) = (R4)(*(UINT32*)(p));
            break;
        case ELEMENT_TYPE_R4:
           *((R4*)pDst) = (*(R4*)p);
            break;
        case ELEMENT_TYPE_U8:
        IN_WIN64(case ELEMENT_TYPE_U:)
            *((R4*)pDst) = (R4)(*(UINT64*)(pSrc));
            break;
        case ELEMENT_TYPE_I8:
        IN_WIN64(case ELEMENT_TYPE_I:)
           *((R4*)pDst) = (R4)(*(INT64*)(p));
            break;
        case ELEMENT_TYPE_R8:
           *((R8*)pDst) = *(R8*)(p);
            break;
        default:
            _ASSERTE(!"Unknown R4 conversion");
            // this is really an impossible condition
            COMPlusThrow(kNotSupportedException);
        }
        break;

    case ELEMENT_TYPE_I8:
    case ELEMENT_TYPE_U8:
    IN_WIN64(case ELEMENT_TYPE_I:)
    IN_WIN64(case ELEMENT_TYPE_U:)
        switch (srcType) {
        case ELEMENT_TYPE_BOOLEAN:
        case ELEMENT_TYPE_I1:
        case ELEMENT_TYPE_U1:
        case ELEMENT_TYPE_CHAR:
        case ELEMENT_TYPE_I2:
        case ELEMENT_TYPE_U2:
        case ELEMENT_TYPE_I4:
        case ELEMENT_TYPE_U4:
        IN_WIN32(case ELEMENT_TYPE_I:)
        IN_WIN32(case ELEMENT_TYPE_U:)
           *((I8*)pDst) = *(INT32*)(p);
            break;
        case ELEMENT_TYPE_R4:
           *((I8*)pDst) = (I8)(*(R4*)(p));
            break;
        IN_WIN64(case ELEMENT_TYPE_I:)
        IN_WIN64(case ELEMENT_TYPE_U:)
        case ELEMENT_TYPE_I8:
        case ELEMENT_TYPE_U8:
           *((I8*)pDst) = *(INT64*)(p);
            break;
        case ELEMENT_TYPE_R8:
           *((I8*)pDst) = (I8)(*(R8*)(p));
            break;
        default:
            _ASSERTE(!"Unknown I8 or U8 conversion");
            // this is really an impossible condition
            COMPlusThrow(kNotSupportedException);
        }
        break;
    case ELEMENT_TYPE_R8:
        switch (srcType) {
        case ELEMENT_TYPE_BOOLEAN:
        case ELEMENT_TYPE_I1:
        case ELEMENT_TYPE_I2:
        case ELEMENT_TYPE_I4:
        IN_WIN32(case ELEMENT_TYPE_I:)
           *((R8*)pDst) = *(INT32*)(p);
            break;
        case ELEMENT_TYPE_U1:
        case ELEMENT_TYPE_CHAR:
        case ELEMENT_TYPE_U2:
        case ELEMENT_TYPE_U4:
        IN_WIN32(case ELEMENT_TYPE_U:)
           *((R8*)pDst) = *(UINT32*)(p);
            break;
        case ELEMENT_TYPE_R4:
           *((R8*)pDst) = *(R4*)(p);
            break;
        case ELEMENT_TYPE_U8:
        IN_WIN64(case ELEMENT_TYPE_U:)
            *((R8*)pDst) = (R8)(*(UINT64*)(pSrc));
            break;
        case ELEMENT_TYPE_I8:
        IN_WIN64(case ELEMENT_TYPE_I:)
           *((R8*)pDst) = (R8)(*(INT64*)(p));
            break;
        case ELEMENT_TYPE_R8:
           *((R8*)pDst) = *(R8*)(p);
            break;
        default:
            _ASSERTE(!"Unknown R8 conversion");
            // this is really an impossible condition
            COMPlusThrow(kNotSupportedException);
        }
        break;
    }
}

void InvokeUtil::CreateByRef(TypeHandle dstTh,void* pDst,CorElementType srcType, TypeHandle srcTH,OBJECTREF srcObj, OBJECTREF *pIncomingObj)
{
    THROWSCOMPLUSEXCEPTION();

    CorElementType dstType = dstTh.GetSigCorElementType();
    if (IsPrimitiveType(srcType) && IsPrimitiveType(dstType)) {
        if (dstType != (CorElementType) srcType)
            COMPlusThrow(kArgumentException,L"Arg_PrimWiden");

        void* pSrc = srcObj->UnBox();

        switch (dstType) {
        case ELEMENT_TYPE_BOOLEAN:
        case ELEMENT_TYPE_I1:
        case ELEMENT_TYPE_U1:
        case ELEMENT_TYPE_CHAR:
        case ELEMENT_TYPE_I2:
        case ELEMENT_TYPE_U2:
        case ELEMENT_TYPE_I4:
        case ELEMENT_TYPE_U4:
        IN_WIN32(case ELEMENT_TYPE_I:)
        IN_WIN32(case ELEMENT_TYPE_U:)
            *(void**)pDst = pSrc;
            break;
        case ELEMENT_TYPE_R4:
            *(void**)pDst = pSrc;
            break;
        case ELEMENT_TYPE_I8:
        case ELEMENT_TYPE_U8:
        IN_WIN64(case ELEMENT_TYPE_I:)
        IN_WIN64(case ELEMENT_TYPE_U:)
            *(void**)pDst = pSrc;
            break;
        case ELEMENT_TYPE_R8:
            *(void**)pDst = pSrc;
            break;
        default:
            _ASSERTE(!"Unknown Primitive");
            COMPlusThrow(kNotSupportedException);
        }
        return;
    }
    if (srcTH.IsNull()) {
        *(void**)pDst = pIncomingObj;
        return;
    }

    _ASSERTE(srcObj != NULL);

    if (dstType == ELEMENT_TYPE_VALUETYPE) {

        *(void**)pDst = srcObj->UnBox();
    }
    else
        *(void**)pDst = pIncomingObj;
}

void InvokeUtil::CheckType(TypeHandle dstTH, OBJECTREF *psrcObj)
{
    THROWSCOMPLUSEXCEPTION();

    if (*psrcObj == NULL) 
        return;

    TypeHandle srcTH = (*psrcObj)->GetTypeHandle();
    if (srcTH.CanCastTo(dstTH))
        return;

    // For transparent proxies we do some extra work to check the cast.
    MethodTable *pSrcMT = srcTH.GetMethodTable();
    if (pSrcMT->IsTransparentProxyType())
    {
        MethodTable *pDstMT = dstTH.GetMethodTable();
        if (!CRemotingServices::CheckCast(*psrcObj, pDstMT->GetClass()))
            COMPlusThrow(kArgumentException,L"Arg_ObjObj");
    }
    else 
    {
        // If the object is a COM object then we need to check to see
        // if it implements the interface.
        EEClass *pSrcClass = srcTH.GetClass();
        MethodTable *pDstMT = dstTH.GetMethodTable();

        if (!pDstMT->IsInterface() || !pSrcMT->IsComObjectType() || !pSrcClass->SupportsInterface(*psrcObj, pDstMT))
            COMPlusThrow(kArgumentException,L"Arg_ObjObj");
    }
}

void InvokeUtil::CreateValueTypeValue(TypeHandle dstTH,void* pDst,CorElementType srcType,TypeHandle srcTH,OBJECTREF srcObj)
{
    THROWSCOMPLUSEXCEPTION();

    EEClass* pBase = GetEEClass(dstTH);

    if (!srcTH.CanCastTo(dstTH))
        COMPlusThrow(kArgumentException,L"Arg_ObjObj");

#ifdef _DEBUG
    // Validate things...
    EEClass* pEEC = GetEEClass(dstTH);
    _ASSERTE(pEEC->IsValueClass());
#endif

    // If pSrc is == to null then we need
    //  to grab the boxed value from an objectref.
    void* p = srcObj->UnBox();
    _ASSERTE(p);

    // Copy the value 
    CopyValueClassUnchecked(pDst, p, pBase->GetMethodTable());
}

// GetBoxedObject
// Given an address of a primitve type, this will box that data...
// @TODO: We need to handle all value classes?
OBJECTREF InvokeUtil::GetBoxedObject(TypeHandle th,void* pData)
{

    MethodTable* pMethTable = th.GetMethodTable();
    _ASSERTE(pMethTable);
    // Save off the data.  We are going to create and object
    //  which may cause GC to occur.
    int size = pMethTable->GetClass()->GetNumInstanceFieldBytes();
    void* p = _alloca(size);
    memcpy(p,pData,size);
    OBJECTREF retO = AllocateObject(pMethTable);
    CopyValueClass(retO->UnBox(), p, pMethTable, retO->GetAppDomain());
    return retO;
}

//ValidField
// This method checks that the object can be widened to the proper type
HRESULT InvokeUtil::ValidField(TypeHandle th, OBJECTREF* value, RefSecContext *pSCtx)
{
    THROWSCOMPLUSEXCEPTION();

    if ((*value) == 0)
        return S_OK;

    MethodTable* pMT;
    CorElementType oType;
    CorElementType type = th.GetSigCorElementType();
    pMT = (*value)->GetMethodTable();
    oType = pMT->GetNormCorElementType();
    if (pMT->GetClass()->IsEnum())
        oType = ELEMENT_TYPE_VALUETYPE;

    // handle pointers
    if (type == ELEMENT_TYPE_PTR || type == ELEMENT_TYPE_FNPTR) {
        InitPointers();
        if ((*value)->GetTypeHandle() == _ptr && type == ELEMENT_TYPE_PTR) {
            ReflectClass* pRC = GetPointerType(value);

            TypeHandle srcTH = pRC->GetTypeHandle();
            if (th != _voidPtr) {
                if (!srcTH.CanCastTo(th))
                    return E_INVALIDARG;
            }
            //if (!Security::CanSkipVerification(pSCtx->GetCallerMethod()->GetModule()))
                //Security::ThrowSecurityException(g_SecurityPermissionClassName, SPFLAGSSKIPVERIFICATION);
            COMCodeAccessSecurityEngine::SpecialDemand(SECURITY_SKIP_VER);
            return S_OK ;
        }
        else if ((*value)->GetTypeHandle().AsMethodTable() == g_Mscorlib.FetchClass(CLASS__INTPTR)) {
            //if (!Security::CanSkipVerification(pSCtx->GetCallerMethod()->GetModule()))
                //Security::ThrowSecurityException(g_SecurityPermissionClassName, SPFLAGSSKIPVERIFICATION);
            COMCodeAccessSecurityEngine::SpecialDemand(SECURITY_SKIP_VER);
            return S_OK;
        }

        return E_INVALIDARG;
    }

    // Need to handle Object special
    if (type == ELEMENT_TYPE_CLASS  || type == ELEMENT_TYPE_VALUETYPE ||
            type == ELEMENT_TYPE_OBJECT || type == ELEMENT_TYPE_STRING ||
            type == ELEMENT_TYPE_ARRAY  || type == ELEMENT_TYPE_SZARRAY) {
        EEClass* EEC = GetEEClass(th);

        if (EEC == g_pObjectClass->GetClass())
            return S_OK;
        if (IsPrimitiveType(oType))
            return E_INVALIDARG;

        //Get the type handle.  For arrays we need to
        //  get it from the object itself.
        TypeHandle h;
        EEClass* pSrcClass = (*value)->GetClass();
        if (pSrcClass->IsArrayClass())
            h = ((BASEARRAYREF)(*value))->GetTypeHandle();
        else
            h = TypeHandle((*value)->GetMethodTable());

        if(h.GetMethodTable()->IsThunking())
        {
            // Extract the true class that the thunking class represents
            h = TypeHandle(h.GetMethodTable()->AdjustForThunking(*value));
        }
        if (!h.CanCastTo(th))
        {
            BOOL fCastOK = FALSE;
            // Give thunking classes a second chance to check the cast
            if ((*value)->GetMethodTable()->IsTransparentProxyType())
            {
                fCastOK = CRemotingServices::CheckCast(*value, th.AsClass());
            }
            else
            {
                // If the object is a COM object then we need to check to see
                // if it implements the interface.
                MethodTable *pSrcMT = pSrcClass->GetMethodTable();
                MethodTable *pDstMT = th.GetMethodTable();
                if (pDstMT->IsInterface() && pSrcMT->IsComObjectType() && pSrcClass->SupportsInterface(*value, pDstMT))
                    fCastOK = TRUE;
            }

            if(!fCastOK)
                return E_INVALIDARG;
        }
        return S_OK;
    }


    if (!IsPrimitiveType(oType))
        return E_INVALIDARG;
    // Now make sure we can widen into the proper type -- CanWiden may run GC...
    return (CanPrimitiveWiden(type,oType)) ? S_OK : E_INVALIDARG;
}

void InvokeUtil::CreateCustomAttributeObject(EEClass *pAttributeClass, 
                                             mdToken tkCtor, 
                                             const void *blobData, 
                                             ULONG blobCnt, 
                                             Module *pModule,
                                             INT32 inheritedLevel,
                                             OBJECTREF *pProtectedCA)
{
    THROWSCOMPLUSEXCEPTION();

    if (_pMTCustomAttribute == NULL) {
        _pMTCustomAttribute = g_Mscorlib.GetClass(CLASS__CUSTOM_ATTRIBUTE);
    }
    CUSTOMATTRIBUTEREF pNewObj = (CUSTOMATTRIBUTEREF)AllocateObject(_pMTCustomAttribute);
    OBJECTREF caType = NULL;
    GCPROTECT_BEGIN(pNewObj);
    caType = pAttributeClass->GetExposedClassObject();
    GCPROTECT_END();
    pNewObj->SetData(*pProtectedCA, caType, tkCtor, blobData, blobCnt, pModule, inheritedLevel);
    *pProtectedCA = pNewObj;
}

// CheckReflectionAccess
// This method will allow callers with the correct reflection permission to fully
//  access an object (including privates, protects, etc.)
void InvokeUtil::CheckReflectionAccess(RuntimeExceptionKind reKind)
{
    THROWSCOMPLUSEXCEPTION();

    if (Security::IsSecurityOff())
        return;

    COMPLUS_TRY {

        // Call the check
        COMCodeAccessSecurityEngine::SpecialDemand(REFLECTION_MEMBER_ACCESS);

    } COMPLUS_CATCH {
        COMPlusThrow(reKind);
    } COMPLUS_END_CATCH
}


// CheckSecurity
// This method will throw a security exception if reflection security is
//  not on.
void InvokeUtil::CheckSecurity()
{
    // Call the check
    COMCodeAccessSecurityEngine::SpecialDemand(REFLECTION_TYPE_INFO);
}

// InternalCreateObject
// This routine will create the specified object from the INT64 value
OBJECTREF InvokeUtil::CreateObject(TypeHandle th,INT64 value)
{
    THROWSCOMPLUSEXCEPTION();

    CorElementType type = th.GetSigCorElementType();

    // Handle the non-table types
    switch (type) {
    case ELEMENT_TYPE_VOID:
        return 0;

    case ELEMENT_TYPE_PTR:
        {
            InitPointers();
            OBJECTREF retO;
            OBJECTREF obj = AllocateObject(_ptr.AsMethodTable());
            GCPROTECT_BEGIN(obj);
            OBJECTREF typeOR = th.CreateClassObj();
            _ptrType->SetRefValue(obj,typeOR);
            _ptrValue->SetValuePtr(obj,(void*) value);
            retO = obj;
            GCPROTECT_END();
            return retO;
        }
        break;

    case ELEMENT_TYPE_FNPTR:
        {
            InitPointers();
            MethodTable *pIntPtrMT = g_Mscorlib.GetExistingClass(CLASS__INTPTR);
            OBJECTREF obj = AllocateObject(pIntPtrMT);
            CopyValueClass(obj->UnBox(), &value, pIntPtrMT, obj->GetAppDomain());
            return obj;
        }

    case ELEMENT_TYPE_VALUETYPE:
    case ELEMENT_TYPE_CLASS:        // Class
    case ELEMENT_TYPE_SZARRAY:      // Single Dim, Zero
    case ELEMENT_TYPE_ARRAY:        // General Array
    case ELEMENT_TYPE_STRING:
    case ELEMENT_TYPE_OBJECT:
    case ELEMENT_TYPE_VAR:
        {
            EEClass* pEEC = GetEEClass(th);
            if (pEEC->IsEnum()) {
                _ASSERTE(th.IsUnsharedMT());
                OBJECTREF obj = AllocateObject(th.AsMethodTable());
                CopyValueClass(obj->UnBox(), &value, th.AsMethodTable(), obj->GetAppDomain());
                return obj;
            }
            else {
                OBJECTREF obj = Int64ToObj(value);
                return obj;
            }
        }
        break;
    case ELEMENT_TYPE_BOOLEAN:      // boolean
    case ELEMENT_TYPE_I1:           // byte
    case ELEMENT_TYPE_U1:
    case ELEMENT_TYPE_I2:           // short
    case ELEMENT_TYPE_U2:           
    case ELEMENT_TYPE_CHAR:         // char
    case ELEMENT_TYPE_I4:           // int
    case ELEMENT_TYPE_U4:
    case ELEMENT_TYPE_I8:           // long
    case ELEMENT_TYPE_U8:       
    case ELEMENT_TYPE_R4:           // float
    case ELEMENT_TYPE_R8:           // double
    case ELEMENT_TYPE_I:
    case ELEMENT_TYPE_U:       
        {
            _ASSERTE(th.IsUnsharedMT());
            OBJECTREF obj = AllocateObject(th.AsMethodTable());
            CopyValueClass(obj->UnBox(), &value, th.AsMethodTable(), obj->GetAppDomain());
            return obj;

        }
        break;
    case ELEMENT_TYPE_END:
    default:
        _ASSERTE(!"Unknown Type");
        COMPlusThrow(kNotSupportedException);
    }
    return 0;
    
}

// This is a special purpose Exception creation function.  It
//  creates the ReflectionTypeLoadException placing the passed
//  classes array and exception array into it.
OBJECTREF InvokeUtil::CreateClassLoadExcept(OBJECTREF* classes,OBJECTREF* except)
{
    THROWSCOMPLUSEXCEPTION();

    OBJECTREF o;
    OBJECTREF oRet = 0;
    STRINGREF str = NULL;

    if (!_pVMClassLoadExcept) {
        _pVMClassLoadExcept = g_Mscorlib.GetException(kReflectionTypeLoadException);
    }
    _ASSERTE(_pVMClassLoadExcept);

    // Create the target object...
    o = AllocateObject(_pVMClassLoadExcept);
    GCPROTECT_BEGIN(o);
    INT64 args[4];

    // Retrieve the resource string.
    ResMgrGetString(L"ReflectionTypeLoad_LoadFailed", &str);

    // Call the constructor
    args[0]  = ObjToInt64(o);
    args[1]  = ObjToInt64((OBJECTREF)str);
    args[2]  = ObjToInt64(*except);
    args[3]  = ObjToInt64(*classes);

    CallConstructor(&gsig_IM_ArrType_ArrException_Str_RetVoid, args);

    oRet = o;

    GCPROTECT_END();
    return oRet;
}


OBJECTREF InvokeUtil::CreateTargetExcept(OBJECTREF* except)
{
    OBJECTREF o;
    OBJECTREF oRet = 0;

    // Only the <cinit> will throw an error here (the one raised by class init)
    THROWSCOMPLUSEXCEPTION();

    if (!_pVMTargetExcept) {
        _pVMTargetExcept = g_Mscorlib.GetException(kTargetInvocationException);
    }
    _ASSERTE(_pVMTargetExcept);


    BOOL fDerivesFromException = TRUE;
    if ( (*except) != NULL ) {
                fDerivesFr