F as GCREF + int=BYREF
            //                             or BYREF+/-int=BYREF
            assert(id->idGCrefGet() == GCT_BYREF &&
                   (ins == INS_add || ins == INS_sub));
            emitGCvarLiveUpd(adr, varNum, id->idGCrefGet(), dst);
            break;

        case IF_SRW_CNS:
        case IF_SRW_RRD:
			// += -= of a byref, no change

        case IF_SRW:
            break;


        default:
#ifdef  DEBUG
            emitDispIns(id, false, false, false);
#endif
            assert(!"unexpected GC ref instruction format");
        }
    }
    else
    {
        switch (id->idInsFmt)
        {
        case IF_RWR_SRD:
        case IF_RRW_SRD:
            emitGCregDeadUpd(id->idRegGet(), dst);
            break;
        }

        if (ins == INS_mulEAX || ins == INS_imulEAX)
        {
            emitGCregDeadUpd(SR_EAX, dst);
            emitGCregDeadUpd(SR_EDX, dst);
        }

        // For the three operand imul instruction the target register
        // is encoded in the opcode

        if (instrIs3opImul(ins))
        {
            emitRegs tgtReg = ((emitRegs) Compiler::inst3opImulReg(ins));
            emitGCregDeadUpd(tgtReg, dst);
        }
    }

    return  dst;
}

/*****************************************************************************
 *
 *  Output an instruction with a static data member (class variable).
 */

BYTE    *  emitter::emitOutputCV  (BYTE *dst, instrDesc *id, unsigned code,
                                                             CnsVal*  addc)
{
    BYTE    *       addr;
    CORINFO_FIELD_HANDLE    fldh;
    int             offs;
    int             doff;

    emitAttr        size = emitDecodeSize(id->idOpSize);
    size_t          opsz = EA_SIZE_IN_BYTES(size);
    instruction     ins  = id->idInsGet();

    /* Get hold of the field handle and offset */

    fldh = id->idAddr.iiaFieldHnd;
    offs = emitGetInsDsp(id);

    /* Special case: mov reg, fs:[ddd] */

    if (fldh == FLD_GLOBAL_FS)
        dst += emitOutputByte(dst, 0x64);

    /* Is there a large constant operand? */

    if  (addc && (size > EA_1BYTE))
    {
        long cval = addc->cnsVal;
        /* Does the constant fit in a byte? */
        if  ((signed char)cval == cval &&
#ifdef RELOC_SUPPORT
             addc->cnsReloc == false   &&
#endif
             ins != INS_mov            &&
             ins != INS_test)
        {
            if  (id->idInsFmt != IF_MRW_SHF)
                code |= 2;

            opsz = 1;
        }
    }
    else
    {
        /* Special case: "mov eax, [addr]" and "mov [addr], eax" */

        if  (ins == INS_mov && id->idReg == SR_EAX && opsz == 4)
        {
            switch (id->idInsFmt)
            {
            case IF_RWR_MRD:

                assert(code == (insCodeRM(ins) | (insEncodeReg345(SR_EAX) << 8) | 0x0500));

                dst += emitOutputByte(dst, 0xA1);
                goto ADDR;

            case IF_MWR_RRD:

                assert(code == (insCodeMR(ins) | (insEncodeReg345(SR_EAX) << 8) | 0x0500));

                dst += emitOutputByte(dst, 0xA3);
                goto ADDR;
            }
        }
    }

    /* Is this a 'big' opcode? */

    if  (code & 0x00FF0000)
    {
        dst += emitOutputByte(dst, code >> 16); code &= 0x0000FFFF;

        if ((ins == INS_movsx || ins == INS_movzx || insIsCMOV(ins)) &&
             size      != EA_1BYTE)
        {
            // movsx and movzx are 'big' opcodes but also have the 'w' bit
            code++;
        }
    }
    else if (Compiler::instIsFP(ins))
    {
        assert(size == EA_4BYTE || size == EA_8BYTE);

        if  (size == EA_8BYTE)
            code += 4;
    }
    else
    {
        /* Is the operand size larger than a byte? */

        switch (size)
        {
        case EA_1BYTE:
            break;

        case EA_2BYTE:

            /* Output a size prefix for a 16-bit operand */

            dst += emitOutputByte(dst, 0x66);

            // Fall through ...

        case EA_4BYTE:

            /* Set the 'w' bit to get the large version */

            code |= 0x1;
            break;

        case EA_8BYTE:

            /* Double operand - set the appropriate bit */

            code |= 0x04;
            break;

        default:
            assert(!"unexpected size");
        }
    }

    if  (id->idInsFmt == IF_MRD_OFF ||
         id->idInsFmt == IF_RWR_MRD_OFF)
        dst += emitOutputByte(dst, code);
    else
        dst += emitOutputWord(dst, code);

ADDR:

    /* Do we have a constant or a static data member? */

    doff = Compiler::eeGetJitDataOffs(fldh);
    if  (doff >= 0)
    {
        /* Is this the constant or data block? */

        if  (doff & 1)
            addr = emitConsBlock + doff - 1;
        else
            addr = emitDataBlock + doff;

        // Check that the offset is properly aligned (i.e. the ddd in [ddd])
        assert((emitChkAlign==false) || (((unsigned)addr & (EA_SIZE_IN_BYTES(size) - 1)) == 0));
    }
    else
    {

        /* Special case: mov reg, fs:[ddd] or mov reg, [ddd] */

        if (fldh == FLD_GLOBAL_DS || fldh == FLD_GLOBAL_FS)
            addr = NULL;
        else
        {
            addr = (BYTE *)emitComp->eeGetFieldAddress(fldh,
                                                       NULL); // @TODO [REVISIT] [04/16/01] []: Support instal-o-jit
            if (addr == NULL)
                NO_WAY("could not obtain address of static field");
        }
    }

    dst += emitOutputLong(dst, (int)(addr + offs));
#ifdef RELOC_SUPPORT
    if (id->idInfo.idDspReloc)
    {
        emitCmpHandle->recordRelocation((void**)dst-1, IMAGE_REL_BASED_HIGHLOW);
    }
#endif

    /* Now generate the constant value, if present */

    if  (addc)
    {
        long cval = addc->cnsVal;
        switch (opsz)
        {
        case 0:
        case 4:
        case 8: dst += emitOutputLong(dst, cval); break;
        case 2: dst += emitOutputWord(dst, cval); break;
        case 1: dst += emitOutputByte(dst, cval); break;

        default:
            assert(!"unexpected operand size");
        }
#ifdef RELOC_SUPPORT
        if (addc->cnsReloc)
        {
            emitCmpHandle->recordRelocation((void**)dst-1, IMAGE_REL_BASED_HIGHLOW);
            assert(opsz == 4);
        }
#endif
    }

    /* Does this instruction operate on a GC ref value? */

    if  (id->idGCref)
    {
        switch (id->idInsFmt)
        {
        case IF_MRD:
        case IF_MRW:
        case IF_MWR:
            break;

        case IF_RRD_MRD:
            break;

        case IF_RWR_MRD:
            emitGCregLiveUpd(id->idGCrefGet(), id->idRegGet(), dst);
            break;

        case IF_MRD_RRD:
        case IF_MWR_RRD:
        case IF_MRW_RRD:
            break;

        case IF_MRD_CNS:
        case IF_MWR_CNS:
        case IF_MRW_CNS:
            break;

        case IF_RRW_MRD:

            assert(id->idGCrefGet() == GCT_BYREF);
            assert(ins == INS_add || ins == INS_sub);

            // Mark it as holding a GCT_BYREF
            emitGCregLiveUpd(GCT_BYREF, id->idRegGet(), dst);
            break;

        default:
#ifdef  DEBUG
            emitDispIns(id, false, false, false);
#endif
            assert(!"unexpected GC ref instruction format");
        }
    }
    else
    {
        switch (id->idInsFmt)
        {
        case IF_RWR_MRD:
            emitGCregDeadUpd(id->idRegGet(), dst);
            break;
        }

        if (ins == INS_mulEAX || ins == INS_imulEAX)
        {
            emitGCregDeadUpd(SR_EAX, dst);
            emitGCregDeadUpd(SR_EDX, dst);
        }

        // For the three operand imul instruction the target register
        // is encoded in the opcode

        if (instrIs3opImul(ins))
        {
            emitRegs tgtReg = ((emitRegs) Compiler::inst3opImulReg(ins));
            emitGCregDeadUpd(tgtReg, dst);
        }
    }

    return  dst;
}

/*****************************************************************************
 *
 *  Output an instruction with one register operand.
 */

BYTE    *           emitter::emitOutputR(BYTE *dst, instrDesc *id)
{
    unsigned        code;

    instruction     ins  = id->idInsGet();
    emitRegs        reg  = id->idRegGet();
    emitAttr        size = emitDecodeSize(id->idOpSize);

    /* Get the 'base' opcode */

    switch(ins)
    {
    case INS_inc:
    case INS_dec:

        if (size == EA_1BYTE)
        {
            assert(INS_inc_l == INS_inc + 1);
            assert(INS_dec_l == INS_dec + 1);

            /* Can't use the compact form, use the long form */

            instruction ins_l = (instruction)(ins + 1);

            dst += emitOutputWord(dst, insCodeRR(ins_l) | insEncodeReg012(reg));
        }
        else
        {
            if (size == EA_2BYTE)
            {
                /* Output a size prefix for a 16-bit operand */
                dst += emitOutputByte(dst, 0x66);
            }
            dst += emitOutputByte(dst, insCodeRR(ins  ) | insEncodeReg012(reg));
        }
        break;

    case INS_pop:
    case INS_pop_hide:
    case INS_push:
    case INS_push_hide:

        assert(size == EA_4BYTE);
        dst += emitOutputByte(dst, insCodeRR(ins) | insEncodeReg012(reg));
        break;

    case INS_seto:
    case INS_setno:
    case INS_setb:
    case INS_setae:
    case INS_sete:
    case INS_setne:
    case INS_setbe:
    case INS_seta:
    case INS_sets:
    case INS_setns:
    case INS_setpe:
    case INS_setpo:
    case INS_setl:
    case INS_setge:
    case INS_setle:
    case INS_setg:

        assert(id->idGCrefGet() == GCT_NONE);

        code = insEncodeMRreg(ins, reg);

        /* We expect this to always be a 'big' opcode */

        assert(code & 0x00FF0000);

        dst += emitOutputByte(dst, code >> 16);
        dst += emitOutputWord(dst, code & 0x0000FFFF);

        break;

    case INS_mulEAX:
    case INS_imulEAX:

        // Kill off any GC refs in EAX or EDX
        emitGCregDeadUpd(SR_EAX, dst);
        emitGCregDeadUpd(SR_EDX, dst);

        // Fall through

    default:

        assert(id->idGCrefGet() == GCT_NONE);

        code = insEncodeMRreg(ins, reg);

        if (size != EA_1BYTE)
        {
            /* Set the 'w' bit to get the large version */
            code |= 0x1;

            if (size == EA_2BYTE)
            {
                /* Output a size prefix for a 16-bit operand */
                dst += emitOutputByte(dst, 0x66);
            }
        }

        dst += emitOutputWord(dst, code);
        break;
    }

    /* Are we writing the register? if so then update the GC information */

    switch (id->idInsFmt)
    {
    case IF_RRD:
        break;
    case IF_RWR:
        if  (id->idGCref)
            emitGCregLiveUpd(id->idGCrefGet(), id->idRegGet(), dst);
        else
            emitGCregDeadUpd(id->idRegGet(), dst);
        break;
    case IF_RRW:
        {
#ifdef DEBUG
            regMaskTP regMask = emitRegMask(reg);
#endif
            if  (id->idGCref)
            {
                // The reg must currently be holding either a gcref or a byref
                // andthe instruction must be inc or dec
                assert((((emitThisGCrefRegs | emitThisByrefRegs) & regMask) && 
                        (ins == INS_inc || ins == INS_dec)));
                assert(id->idGCrefGet() == GCT_BYREF);
                // Mark it as holding a GCT_BYREF
                emitGCregLiveUpd(GCT_BYREF, id->idRegGet(), dst);
            }
            else
            {
                // Can't use RRW to trash a GC ref.  It's OK for unverifiable code
                // to trash Byrefs (see bug 59482).
                assert((emitThisGCrefRegs & regMask) == 0);
            }
        }
        break;
    default:
#ifdef  DEBUG
        emitDispIns(id, false, false, false);
#endif
        assert(!"unexpected instruction format");
        break;
    }

    return  dst;
}

/*****************************************************************************
 *
 *  Output an instruction with two register operands.
 */

BYTE    *           emitter::emitOutputRR(BYTE *dst, instrDesc *id)
{
    unsigned        code;

    instruction     ins  = id->idInsGet();
    emitRegs        reg1 = id->idRegGet();
    emitRegs        reg2 = id->idRg2Get();
    emitAttr        size = emitDecodeSize(id->idOpSize);

    /* Get the 'base' opcode */

    if ((ins == INS_movsx) ||
        (ins == INS_movzx) ||
        (insIsCMOV(ins)))
    {
        code = insEncodeRMreg(ins) | (int)(size == EA_2BYTE);
    }
    else if (ins == INS_test)
    {
        assert(size == EA_4BYTE);
        code = insEncodeMRreg(ins) | 1;
    }
    else
    {
        code = insEncodeMRreg(ins) | 2;

        switch (size)
        {
        case EA_1BYTE:
            assert(SRM_BYTE_REGS & emitRegMask(reg1));
            assert(SRM_BYTE_REGS & emitRegMask(reg2));
            break;

        case EA_2BYTE:

            /* Output a size prefix for a 16-bit operand */

            dst += emitOutputByte(dst, 0x66);

            // Fall through ...

        case EA_4BYTE:

            /* Set the 'w' bit to get the large version */

            code |= 0x1;
            break;

        default:
            assert(!"unexpected size");
        }
    }

    /* Is this a 'big' opcode? */

    if  (code & 0x00FF0000)
    {
        dst += emitOutputByte(dst, code >> 16); code &= 0x0000FFFF;
    }

    dst += emitOutputWord(dst, code | (insEncodeReg345(reg1) |
                                     insEncodeReg012(reg2)) << 8);

    /* Does this instruction operate on a GC ref value? */

    if  (id->idGCref)
    {
        switch (id->idInsFmt)
        {
        case IF_RRD_RRD:
            break;

        case IF_RWR_RRD:

            if  (emitIGisInProlog(emitCurIG) &&
                 (!emitComp->info.compIsStatic) && (reg2 == REG_ARG_0)
                                 && emitComp->lvaTable[0].TypeGet() != TYP_I_IMPL)
            {
                /* We're relocating "this" in the prolog */

                assert(emitComp->lvaIsThisArg(0));
                assert(emitComp->lvaTable[0].lvRegister);
                assert(emitComp->lvaTable[0].lvRegNum == reg1);

                if  (emitFullGCinfo)
                {
                    emitGCregLiveSet(id->idGCrefGet(), emitRegMask(reg1), dst, true);
                    break;
                }
                else
                {
                    /* If emitFullGCinfo==false, the we dont use any
                       regPtrDsc's and so explictly note the location
                       of "this" in GCEncode.cpp
                     */
                }
            }

            emitGCregLiveUpd(id->idGCrefGet(), id->idRegGet(), dst);
            break;

        case IF_RRW_RRD:

            /*
                This must be one of the following cases:

                    xor reg, reg        to assign NULL

                    and r1 , r2         if (ptr1 && ptr2) ...
                    or  r1 , r2         if (ptr1 || ptr2) ...
             */

            switch (id->idIns)
            {
            case INS_xor:
                assert(id->idReg == id->idRg2);
                emitGCregLiveUpd(id->idGCrefGet(), id->idRegGet(), dst);
                break;

            case INS_or:
            case INS_and:
                emitGCregDeadUpd(id->idRegGet(), dst);
                break;

            case INS_add:
            case INS_sub:
                assert(id->idGCrefGet() == GCT_BYREF);

#ifdef DEBUG
                regMaskTP regMask;
                regMask = emitRegMask(reg1) | emitRegMask(reg2);

                // r1/r2 could have been a GCREF as GCREF + int=BYREF
                //                            or BYREF+/-int=BYREF
                assert(((regMask & emitThisGCrefRegs) && (ins == INS_add                  )) ||
                       ((regMask & emitThisByrefRegs) && (ins == INS_add || ins == INS_sub)));
#endif
                // Mark r1 as holding a byref
                emitGCregLiveUpd(GCT_BYREF, id->idRegGet(), dst);
                break;

            default:
#ifdef  DEBUG
                emitDispIns(id, false, false, false);
#endif
                assert(!"unexpected GC reg update instruction");
            }

            break;

        case IF_RRW_RRW:

            /* This must be "xchg reg1, reg2" */
            assert(id->idIns == INS_xchg);

            /* If we got here, one and only ONE of the two registers
             * holds a pointer, so we have to "swap" them in the GC
             * register pointer mask */
#if 0
            GCtype gc1, gc2;

            gc1 = emitRegGCtype(reg1);
            gc2 = emitRegGCtype(reg2);

            if (gc1 != gc2)
            {
                // Kill the GC-info about the GC registers

                if (needsGC(gc1))
                    emitGCregDeadUpd(reg1, dst);

                if (needsGC(gc2))
                    emitGCregDeadUpd(reg2, dst);

                // Now, swap the info

                if (needsGC(gc1))
                    emitGCregLiveUpd(gc1, reg2, dst);

                if (needsGC(gc2))
                    emitGCregLiveUpd(gc2, reg1, dst);
            }
#endif
            break;

        default:
#ifdef  DEBUG
            emitDispIns(id, false, false, false);
#endif
            assert(!"unexpected GC ref instruction format");
        }
    }
    else
    {
        switch (id->idInsFmt)
        {
        case IF_RWR_RRD:
        case IF_RRW_RRD:
            emitGCregDeadUpd(id->idRegGet(), dst);
            break;
        }
    }

    return  dst;
}

/*****************************************************************************
 *
 *  Output an instruction with a register and constant operands.
 */

BYTE    *           emitter::emitOutputRI(BYTE *dst, instrDesc *id)
{
    unsigned     code;
    emitAttr     size      = emitDecodeSize(id->idOpSize);
    instruction  ins       = id->idInsGet();
    emitRegs     reg       = (emitRegs  )id->idReg;
    int          val       = emitGetInsSC(id);
    bool         valInByte = ((signed char)val == val) && (ins != INS_mov) && (ins != INS_test);

#ifdef RELOC_SUPPORT
    if (id->idInfo.idCnsReloc)
    {
        valInByte = false;      // relocs can't be placed in a byte
    }
#endif

    assert(size != EA_1BYTE || (emitRegMask(reg) & SRM_BYTE_REGS));

    /* The 'mov' opcode is special */

    if  (ins == INS_mov)
    {
//        assert(val);

        code = insCodeACC(ins);
        assert(code < 0x100);

        assert(size == EA_4BYTE);       // Only 32-bit mov's are implemented
        code |= 0x08;                   // Set the 'w' bit

        dst += emitOutputByte(dst, code | insEncodeReg012(reg));
        dst += emitOutputLong(dst, val);
#ifdef RELOC_SUPPORT
        if (id->idInfo.idCnsReloc)
        {
            emitCmpHandle->recordRelocation((void**)dst-1, IMAGE_REL_BASED_HIGHLOW);
        }
#endif

        goto DONE;
    }

    /* Decide which encoding is the shortest */

    bool    useSigned, useACC;

    if (reg == SR_EAX && !instrIs3opImul(ins))
    {
         if (size == EA_1BYTE || (ins == INS_test))
         {
             // For al, ACC encoding is always the smallest

             useSigned = false; useACC = true;
         }
         else
         {
             /* For ax/eax, we avoid ACC encoding for small constants as we
              * can emit the small constant and have it sign-extended.
              * For big constants, the ACC encoding is better as we can use
              * the 1 byte opcode
              */

             if (valInByte)
             {
                 // avoid using ACC encoding
                 useSigned = true;  useACC = false;
             }
             else
             {
                 useSigned = false; useACC = true;
             }
         }
    }
    else
    {
        useACC = false;

        if (valInByte)
            useSigned = true;
        else
            useSigned = false;
    }

    /* "test" has no 's' bit */

    if (ins == INS_test) useSigned = false;

    /* Get the 'base' opcode */

    if (useACC)
    {
        assert(!useSigned);

        code    = insCodeACC(ins);
    }
    else
    {
        assert(!useSigned || valInByte);

        code    = insEncodeMIreg(ins, reg);
    }

    switch (size)
    {
    case EA_1BYTE:
        break;

    case EA_2BYTE:

        /* Output a size prefix for a 16-bit operand */

        dst += emitOutputByte(dst, 0x66);

        // Fall through ...

    case EA_4BYTE:

        /* Set the 'w' bit to get the large version */

        code |= 0x1;
        break;

    default:
        assert(!"unexpected size");
    }

    /* Does the value fit in a single byte?
     * We can just set the 's' bit, and issue an immediate byte */

    if  (useSigned)
    {
        dst += emitOutputWord(dst, code | 2);
        dst += emitOutputByte(dst, val);

        goto DONE;
    }

    /* Can we use an accumulator (EAX) encoding? */

    if  (useACC)
        dst += emitOutputByte(dst, code);
    else
        dst += emitOutputWord(dst, code);

    switch(size)
    {
    case EA_1BYTE:   dst += emitOutputByte(dst, val);  break;
    case EA_2BYTE:   dst += emitOutputWord(dst, val);  break;
    case EA_4BYTE:   dst += emitOutputLong(dst, val);  break;
    }

#ifdef RELOC_SUPPORT
    if (id->idInfo.idCnsReloc)
    {
        emitCmpHandle->recordRelocation((void**)dst-1, IMAGE_REL_BASED_HIGHLOW);
        assert(size == EA_4BYTE);
    }
#endif

DONE:

    /* Does this instruction operate on a GC ref value? */


#ifdef DEBUG
    regMaskTP regMask;
#endif
    if  (id->idGCref)
    {
        switch (id->idInsFmt)
        {
        case IF_RRD_CNS:
            break;

        case IF_RWR_CNS:
            emitGCregLiveUpd(id->idGCrefGet(), id->idRegGet(), dst);
            break;

        case IF_RRW_CNS:
            assert(id->idGCrefGet() == GCT_BYREF);

#ifdef DEBUG
            regMask = emitRegMask(reg);
            // FIXNOW review the other places and relax the assert there too

            // The reg must currently be holding either a gcref or a byref
            // GCT_GCREF+int = GCT_BYREF, and GCT_BYREF+/-int = GCT_BYREF
            if (emitThisGCrefRegs & regMask)
                assert(ins == INS_add);
            if (emitThisByrefRegs & regMask)
                assert(ins == INS_add || ins == INS_sub);
#endif
            // Mark it as holding a GCT_BYREF
            emitGCregLiveUpd(GCT_BYREF, id->idRegGet(), dst);
            break;

        default:
#ifdef  DEBUG
            emitDispIns(id, false, false, false);
#endif
            assert(!"unexpected GC ref instruction format");
        }

        // mul can never produce a GC ref

        assert(!instrIs3opImul(ins));
        assert(ins != INS_mulEAX || ins != INS_imulEAX);
    }
    else
    {
        switch (id->idInsFmt)
        {
        case IF_RRD_CNS:
            break;
        case IF_RRW_CNS:
#ifdef DEBUG
            regMask = emitRegMask(reg);
            // The reg must not currently be holding either a gcref
            assert((emitThisGCrefRegs & regMask) == 0);
#endif
            break;
        case IF_RWR_CNS:
            emitGCregDeadUpd(id->idRegGet(), dst);
            break;
        default:
#ifdef  DEBUG
            emitDispIns(id, false, false, false);
#endif
            assert(!"unexpected GC ref instruction format");
        }

        // INS_mulEAX cant be used with any of these formats
        assert(ins != INS_mulEAX || ins != INS_imulEAX);

        // For the three operand imul instruction the target
        // register is encoded in the opcode

        if (instrIs3opImul(ins))
        {
            emitRegs tgtReg = ((emitRegs) Compiler::inst3opImulReg(ins));
            emitGCregDeadUpd(tgtReg, dst);
        }
    }

    return dst;
}

/*****************************************************************************
 *
 *  Output an instruction with a constant operand.
 */

BYTE    *           emitter::emitOutputIV(BYTE *dst, instrDesc *id)
{
    unsigned     code;
    instruction  ins       = id->idInsGet();
    int          val       = emitGetInsSC(id);
    bool         valInByte = ((signed char)val == val);

#ifdef RELOC_SUPPORT
    if (id->idInfo.idCnsReloc)
    {
        valInByte = false;        // relocs can't be placed in a byte

        // Of these instructions only the push instruction can have reloc
        assert(ins == INS_push || ins == INS_push_hide);
    }
#endif

   switch (ins)
    {
    case INS_jge:
        assert((val >= -128) && (val <= 127));
        dst += emitOutputByte(dst, insCode(ins));
        dst += emitOutputByte(dst, val);
        break;

    case INS_loop:
        assert((val >= -128) && (val <= 127));
        dst += emitOutputByte(dst, insCodeMI(ins));
        dst += emitOutputByte(dst, val);
        break;

    case INS_ret:

        assert(val);
        dst += emitOutputByte(dst, insCodeMI(ins));
        dst += emitOutputWord(dst, val);
        break;

    case INS_push_hide:
    case INS_push:

        code = insCodeMI(ins);

        /* Does the operand fit in a byte? */

        if  (valInByte)
        {
            dst += emitOutputByte(dst, code|2);
            dst += emitOutputByte(dst, val);
        }
        else
        {
            dst += emitOutputByte(dst, code);
            dst += emitOutputLong(dst, val);
#ifdef RELOC_SUPPORT
            if (id->idInfo.idCnsReloc)
            {
                emitCmpHandle->recordRelocation((void**)dst-1, IMAGE_REL_BASED_HIGHLOW);
            }
#endif
        }

        /* Did we push a GC ref value? */

        if  (id->idGCref)
        {
#ifdef  DEBUG
            printf("UNDONE: record GCref push [cns]\n");
#endif
        }

        break;

    default:
        assert(!"unexpected instruction");
    }

    return  dst;
}

/*****************************************************************************
 *
 *  Output a local jump instruction.
 */

BYTE    *           emitter::emitOutputLJ(BYTE *dst, instrDesc *i)
{
    unsigned        srcOffs;
    unsigned        dstOffs;
    int             distVal;

    instrDescJmp *  id  = (instrDescJmp*)i;
    instruction     ins = id->idInsGet();
    bool            jmp;
    bool            relAddr = true; // does the instruction use relative-addressing?

    size_t          ssz;
    size_t          lsz;

    switch (ins)
    {
    default:
        ssz = JCC_SIZE_SMALL;
        lsz = JCC_SIZE_LARGE;
        jmp = true;
        break;

    case INS_jmp:
        ssz = JMP_SIZE_SMALL;
        lsz = JMP_SIZE_LARGE;
        jmp = true;
        break;

    case INS_call:
        ssz =
        lsz = CALL_INST_SIZE;
        jmp = false;
        break;

    case INS_push_hide:
    case INS_push:
        ssz =
        lsz = 5;
        jmp = false;
        relAddr = false;
        break;
    }

    /* Figure out the distance to the target */

    srcOffs = emitCurCodeOffs(dst);
    dstOffs = id->idAddr.iiaIGlabel->igOffs;
    if (relAddr)
        distVal = dstOffs - srcOffs;
    else
        distVal = (int)(emitCodeBlock + dstOffs);

    if  (dstOffs <= srcOffs)
    {
        /* This is a backward jump - distance is known at this point */

#ifdef  DEBUG
        if  (id->idNum == INTERESTING_JUMP_NUM || INTERESTING_JUMP_NUM == 0)
        {
            size_t      blkOffs = id->idjIG->igOffs;

            if  (INTERESTING_JUMP_NUM == 0)
            printf("[3] Jump %u:\n", id->idNum);
            printf("[3] Jump  block is at %08X - %02X = %08X\n", blkOffs, emitOffsAdj, blkOffs - emitOffsAdj);
            printf("[3] Jump        is at %08X - %02X = %08X\n", srcOffs, emitOffsAdj, srcOffs - emitOffsAdj);
            printf("[3] Label block is at %08X - %02X = %08X\n", dstOffs, emitOffsAdj, dstOffs - emitOffsAdj);
        }
#endif

        /* Can we use a short jump? */

        if  (jmp && distVal - ssz >= JMP_DIST_SMALL_MAX_NEG)
            id->idjShort = 1;
    }
    else
    {
        /* This is a  forward jump - distance will be an upper limit */

        emitFwdJumps  = true;

        /* The target offset will be closer by at least 'emitOffsAdj' */

        dstOffs -= emitOffsAdj;
        distVal -= emitOffsAdj;

        /* Record the location of the jump for later patching */

        id->idjOffs = dstOffs;

#ifdef  DEBUG
        if  (id->idNum == INTERESTING_JUMP_NUM || INTERESTING_JUMP_NUM == 0)
        {
            size_t      blkOffs = id->idjIG->igOffs;

            if  (INTERESTING_JUMP_NUM == 0)
            printf("[4] Jump %u:\n", id->idNum);
            printf("[4] Jump  block is at %08X\n"              , blkOffs);
            printf("[4] Jump        is at %08X\n"              , srcOffs);
            printf("[4] Label block is at %08X - %02X = %08X\n", dstOffs + emitOffsAdj, emitOffsAdj, dstOffs);
        }
#endif

        /* Can we use a short jump? */

        if  (jmp && distVal - ssz <= JMP_DIST_SMALL_MAX_POS)
            id->idjShort = 1;
    }

    /* Adjust the offset to emit relative to the end of the instruction */

    if (relAddr)
        distVal -= id->idjShort ? ssz : lsz;

#ifdef  DEBUG
    if (0&&verbose)
    {
        size_t  sz          = id->idjShort ?ssz:lsz;
        int     distValSize = id->idjShort ? 4 : 8;
        printf("; %s jump [%08X/%03u] from %0*X to %0*X: dist = %08XH\n",
            (dstOffs <= srcOffs)?"Fwd":"Bwd", id, id->idNum,
            distValSize, srcOffs+sz, distValSize, dstOffs,
            distVal);
    }
#endif

    /* What size jump should we use? */

    if  (id->idjShort)
    {
        /* Short jump */

        assert(JMP_SIZE_SMALL == JCC_SIZE_SMALL);
        assert(JMP_SIZE_SMALL == 2);

        assert(jmp);

        if  (emitInstCodeSz(id) != JMP_SIZE_SMALL)
        {
            emitOffsAdj += emitInstCodeSz(id) - JMP_SIZE_SMALL;

#ifdef  DEBUG
            if (verbose) printf("; NOTE: size of jump [%08X] mis-predicted\n", id);
#endif
        }

        dst += emitOutputByte(dst, insCode(ins));

        /* For forward jumps, record the address of the distance value */

        id->idjTemp.idjAddr = (distVal > 0) ? dst : NULL;

        dst += emitOutputByte(dst, distVal);
    }
    else
    {
        unsigned        code;

        /* Long  jump */

        if  (jmp)
        {
            assert(INS_jmp + (INS_l_jmp - INS_jmp) == INS_l_jmp);
            assert(INS_jo  + (INS_l_jmp - INS_jmp) == INS_l_jo );
            assert(INS_jb  + (INS_l_jmp - INS_jmp) == INS_l_jb );
            assert(INS_jae + (INS_l_jmp - INS_jmp) == INS_l_jae);
            assert(INS_je  + (INS_l_jmp - INS_jmp) == INS_l_je );
            assert(INS_jne + (INS_l_jmp - INS_jmp) == INS_l_jne);
            assert(INS_jbe + (INS_l_jmp - INS_jmp) == INS_l_jbe);
            assert(INS_ja  + (INS_l_jmp - INS_jmp) == INS_l_ja );
            assert(INS_js  + (INS_l_jmp - INS_jmp) == INS_l_js );
            assert(INS_jns + (INS_l_jmp - INS_jmp) == INS_l_jns);
            assert(INS_jpe + (INS_l_jmp - INS_jmp) == INS_l_jpe);
            assert(INS_jpo + (INS_l_jmp - INS_jmp) == INS_l_jpo);
            assert(INS_jl  + (INS_l_jmp - INS_jmp) == INS_l_jl );
            assert(INS_jge + (INS_l_jmp - INS_jmp) == INS_l_jge);
            assert(INS_jle + (INS_l_jmp - INS_jmp) == INS_l_jle);
            assert(INS_jg  + (INS_l_jmp - INS_jmp) == INS_l_jg );

            code = insCode((instruction)(ins + (INS_l_jmp - INS_jmp)));
        }
        else if (ins == INS_push || ins == INS_push_hide)
        {
            assert(insCodeMI(INS_push) == 0x68);
            code = 0x68;
        }
        else
        {
            code = 0xE8;
        }

        dst += emitOutputByte(dst, code);

        if  (code & 0xFF00)
            dst += emitOutputByte(dst, code >> 8);

        /* For forward jumps, record the address of the distance value */

        id->idjTemp.idjAddr = (dstOffs > srcOffs) ? dst : NULL;

        dst += emitOutputLong(dst, distVal);

#ifdef RELOC_SUPPORT
        if (!relAddr)
        {
            emitCmpHandle->recordRelocation((void**)dst-1, IMAGE_REL_BASED_HIGHLOW);
        }
#endif
    }

    // Local calls kill all registers

    if (ins == INS_call && (emitThisGCrefRegs|emitThisByrefRegs))
        emitGCregDeadUpd(emitThisGCrefRegs|emitThisByrefRegs, dst);

    return  dst;
}

/*****************************************************************************
 *
 *  Append the machine code corresponding to the given instruction descriptor
 *  to the code block at '*dp'; the base of the code block is 'bp', and 'ig'
 *  is the instruction group that contains the instruction. Updates '*dp' to
 *  point past the generated code, and returns the size of the instruction
 *  descriptor in bytes.
 */

size_t              emitter::emitOutputInstr(insGroup  *ig,
                                             instrDesc *id, BYTE **dp)
{
    BYTE    *       dst  = *dp;
    size_t          sz   = sizeof(instrDesc);
    instruction     ins  = id->idInsGet();
    emitAttr        size = emitDecodeSize(id->idOpSize);

#ifdef  DEBUG

#if     DUMP_GC_TABLES
    bool            dspOffs = dspGCtbls;
#else
    const
    bool            dspOffs = false;
#endif

    if  (disAsm || dspEmit || verbose)
    {
        if (ig->igData == (BYTE*) id)
            printf("\nG_M%03u_IG%02u:\n", Compiler::s_compMethodsCount, ig->igNum);
        emitDispIns(id, false, dspOffs, true, dst - emitCodeBlock);
    }

#endif

    assert(SR_NA == REG_NA);

    assert(ins != INS_imul                || size == EA_4BYTE); // Has no 'w' bit
    assert(instrIs3opImul(id->idIns) == 0 || size == EA_4BYTE); // Has no 'w' bit

    /* Is there a new set of live GC ref variables for a call-site or
       non-local jump? */

    if ((ins == INS_call) ||
        (ins == INS_i_jmp && (id->idInsFmt == IF_METHOD ||
                              id->idInsFmt == IF_METHPTR)))
    {
        /* We update the GC info before the call as the variables cannot be
           used by the call. Killing variables before the call helps with
           boundary conditions if the call is CORINFO_HELP_THROW - see bug 50029.
           If we ever track aliased variables (which could be used by the
           call), we would have to keep them alive past the call. */

        VARSET_TP       GCvars = 0;

        if (id->idInfo.idLargeCall)
        {
            if (id->idInsFmt == IF_METHOD || id->idInsFmt == IF_METHPTR)
                GCvars = ((instrDescCDGCA*)id)->idcdGCvars;
            else
                GCvars = ((instrDescCIGCA*)id)->idciGCvars;
        }

        emitUpdateLiveGCvars(GCvars, dst);

#ifdef  DEBUG
        if  (verbose&&0)
            printf("[%02u] Gen call GC vars = %016I64X\n", id->idNum, GCvars);
#endif
    }

    /* What instruction format have we got? */

    switch (id->idInsFmt)
    {
        unsigned        code;
        int             args;
        CnsVal          cnsVal;

        BYTE    *       addr;
        CORINFO_METHOD_HANDLE   methHnd;
        bool            recCall;

        unsigned        gcrefRegs;
        unsigned        byrefRegs;
        unsigned        bregs;

        /********************************************************************/
        /*                        No operands                               */
        /********************************************************************/
    case IF_NONE:

        // the loop alignment pseudo instruction
        if (ins == INS_align)
        {
            sz = TINY_IDSC_SIZE;
            break;
        }

#if SCHEDULER
        // an explicit scheduling boundary pseudo instruction
        if (ins == INS_noSched)
        {
            sz = TINY_IDSC_SIZE;
            break;
        }
#endif

        // the cdq instruction kills the EDX register implicitly
        if (ins == INS_cdq)
            emitGCregDeadUpd(SR_EDX, dst);

        // Fall through
    case IF_TRD:
    case IF_TWR:
    case IF_TRW:
        assert(id->idGCrefGet() == GCT_NONE);

        code = insCodeMR(ins);

        if  (code & 0xFF00)
            dst += emitOutputWord(dst, code);
        else
            dst += emitOutputByte(dst, code);

        break;

        /********************************************************************/
        /*                Simple constant, local label, method              */
        /********************************************************************/

    case IF_CNS:

        dst = emitOutputIV(dst, id);
        sz  = emitSizeOfInsDsc(id);
        break;

    case IF_LABEL:

        assert(id->idGCrefGet() == GCT_NONE);
        assert(id->idInfo.idBound);

        dst = emitOutputLJ(dst, id);
        sz  = sizeof(instrDescJmp);
        break;

    case IF_METHOD:
    case IF_METHPTR:

        /* Assume we'll be recording this call */

        recCall  = true;

        /* Get hold of the argument count and field Handle */

        args = emitGetInsCDinfo(id);

        methHnd = id->idAddr.iiaMethHnd;

        /* Is this a "fat" call descriptor? */

        if  (id->idInfo.idLargeCall)
        {
            bregs       = ((instrDescCDGCA*)id)->idcdByrefRegs;
            byrefRegs   = emitDecodeCallGCregs(bregs);

            sz          = sizeof(instrDescCDGCA);
        }
        else
        {
            assert(id->idInfo.idLargeDsp == false);
            assert(id->idInfo.idLargeCns == false);

            byrefRegs   = 0;

            sz          = sizeof(instrDesc);
        }

        /* What kind of a call do we have here? */

        if (id->idInfo.idCallAddr)
        {
            /*  This is call indirect where we know the target, thus we can
                use a direct call; the target to jump to is in iiaAddr.
             */

            assert(id->idInsFmt == IF_METHOD);

            addr = (BYTE *)id->idAddr.iiaAddr;
        }
        else
        {
            /* See if this is a call to a helper function */

            CorInfoHelpFunc helperNum = Compiler::eeGetHelperNum(methHnd);

            if (helperNum != CORINFO_HELP_UNDEF)
            {
                /* This is a helper call */


#ifndef RELOC_SUPPORT
                assert(id->idInsFmt != IF_METHPTR);
#else
                assert(id->idInsFmt != IF_METHPTR || emitComp->opts.compReloc);

                if (id->idInsFmt == IF_METHPTR)
                {
                    assert(id->idInfo.idDspReloc);

                    // Get the indirection handle

                    void * dirAddr = eeGetHelperFtn(emitCmpHandle,
                                                helperNum, (void***)&addr);
                    assert(dirAddr == NULL && addr);

                    goto EMIT_INDIR_CALL;
                }
#endif
                /* Some helpers don't get recorded in GC tables */

                if  (emitNoGChelper(helperNum))
                    recCall = false;

                addr = (BYTE *)eeGetHelperFtn(emitCmpHandle,  helperNum, NULL);
            }
            else
            {
                /* It's a call to a user-defined function/method */

                if  (emitComp->eeIsOurMethod(methHnd))
                {
                    assert(id->idInsFmt != IF_METHPTR);

                    /* Directly recursive call */

                    addr = emitCodeBlock;
                }
                else
                {
                    /* Static method call */

                    InfoAccessType accessType = IAT_VALUE;
                    addr = (BYTE*)emitComp->eeGetMethodEntryPoint(methHnd, &accessType);

                    /* Note that we may get IAT_VALUE even for IF_METHPTR because
                       the method may have been JITcompiled by another thread since
                       the last call to eeGetMethodEntryPoint() */

                    assert(accessType == IAT_VALUE || accessType == IAT_PVALUE);

                    if  (accessType == IAT_PVALUE)
                    {
                        /* This is a call via a global method pointer */
                        assert(id->idInsFmt == IF_METHPTR);

#ifdef RELOC_SUPPORT
                    EMIT_INDIR_CALL:
#endif
                        assert(addr);

                        code = insCodeMR(ins);
                        if (ins == INS_i_jmp)
                            code |= 1;

                        dst += emitOutputWord(dst, code | 0x0500);
                        dst += emitOutputLong(dst, (int)addr);
#ifdef RELOC_SUPPORT
                        if (id->idInfo.idDspReloc)
                        {
                            emitCmpHandle->recordRelocation((void**)dst-1, IMAGE_REL_BASED_HIGHLOW);
                        }
#endif
                        goto DONE_CALL;
                    }

                    /* Direct static method call */

#ifdef RELOC_SUPPORT
                    if (emitComp->opts.compReloc)
                    {
                        /* Output the call opcode followed by the target distance */

                        dst += (ins == INS_l_jmp) ? emitOutputByte(dst, insCode(ins))
                                                  : emitOutputByte(dst, insCodeMI(ins));

                        /* Get true code address for the byte following this call */
                        BYTE* srcAddr = getCurrentCodeAddr(dst + sizeof(void*));

                        if (addr == NULL)   // do we need to defer this?
                        {
                            X86deferredCall * pDC = X86deferredCall::create(emitComp, emitCmpHandle,
                                                                            methHnd,
                                                                            dst, srcAddr);

                            dst += emitOutputLong(dst, 0);

                            emitCmpHandle->deferLocation (methHnd, pDC);
                        }
                        else
                        {
                            /* Calculate PC relative displacement */
                            dst += emitOutputLong(dst, (int)(addr - srcAddr));
                        }

                        emitCmpHandle->recordRelocation((void**)dst-1, IMAGE_REL_BASED_REL32);

                        goto DONE_CALL;
                    }
#endif
                }
            }
        }

#ifdef BIRCH_SP2
        {
            //
            // Should always end up jumping to DONE_CALL for RELOC_SUPPORT
            //
            // Because the code below makes the assumption that we are running
            // in memory and that the PC relative address can be formed by
            // subtracting dst from addr.
            //
            // This assuption is never true for RELOC_SUPPORT
            //

#ifdef DEBUG
            emitDispIns(id, false, false, false);
#endif

            assert(!"Should Not Be Reached");
        }
#endif // BIRCH_SP2

        /* Output the call opcode followed by the target distance */

        dst += (ins == INS_l_jmp) ? emitOutputByte(dst, insCode(ins)) : emitOutputByte(dst, insCodeMI(ins));

        /* Calculate PC relative displacement */
        dst += emitOutputLong(dst, addr - (dst + sizeof(void*)));

#ifdef RELOC_SUPPORT
        if (emitComp->opts.compReloc)
            emitCmpHandle->recordRelocation((void**)dst-1, IMAGE_REL_BASED_REL32);
#endif

    DONE_CALL:

#ifdef BIRCH_SP2
        if (args >= 0)
            emitStackPop(dst, true, args);
        break;
#endif
        /* Get the new set of live GC ref registers */

        gcrefRegs = emitDecodeCallGCregs(id);

        /* If the method returns a GC ref, mark EAX appropriately */

        if       (id->idGCrefGet() == GCT_GCREF)
            gcrefRegs |= SRM_EAX;
        else if  (id->idGCrefGet() == GCT_BYREF)
            byrefRegs |= SRM_EAX;

        /* If the GC register set has changed, report the new set */

        if  (gcrefRegs != emitThisGCrefRegs)
            emitUpdateLiveGCregs(GCT_GCREF, gcrefRegs, dst);

        if  (byrefRegs != emitThisByrefRegs)
            emitUpdateLiveGCregs(GCT_BYREF, byrefRegs, dst);

        if  (recCall || args)
        {
            /* For callee-pop, all arguments will be popped  after the call.
               For caller-pop, any GC arguments will go dead after the call. */

            if (args >= 0)
                emitStackPop(dst, true, args);
            else
                emitStackKillArgs(dst, -args);
        }

        /* Do we need to record a call location for GC purposes? */

        if  (!emitFullGCinfo && recCall)
            emitRecordGCcall(dst);

        break;

        /********************************************************************/
        /*                      One register operand                        */
        /********************************************************************/

    case IF_RRD:
    case IF_RWR:
    case IF_RRW:

        dst = emitOutputR(dst, id);
        sz = TINY_IDSC_SIZE;
        break;

        /********************************************************************/
        /*                 Register and register/constant                   */
        /********************************************************************/

    case IF_RRW_SHF:
        dst += emitOutputWord(dst, insEncodeMRreg(ins, id->idRegGet()) | 1);
        dst += emitOutputByte(dst, emitGetInsSC(id));
        sz   = emitSizeOfInsDsc(id);
        break;

    case IF_RRD_RRD:
    case IF_RWR_RRD:
    case IF_RRW_RRD:
    case IF_RRW_RRW:

        dst = emitOutputRR(dst, id);
        sz  = TINY_IDSC_SIZE;
        break;

    case IF_RWR_METHOD:
        assert(ins == INS_mov);

        /* Move the address of a static method into the target register */

        methHnd = id->idAddr.iiaMethHnd;

        /* Output the mov r32,imm opcode followed by the method's address */
        code = insCodeACC(ins) | 0x08 | insEncodeReg012(id->idRegGet());
        assert(code < 0x100);
        dst += emitOutputByte(dst, code);

        addr = (BYTE*)emitComp->eeGetMethodEntryPoint(methHnd, NULL);

        if (addr != NULL)
        {
            dst += emitOutputLong(dst, (int)addr);
#ifdef RELOC_SUPPORT
            if (id->idInfo.idCnsReloc)
            {
                emitCmpHandle->recordRelocation((void**)dst-1, IMAGE_REL_BASED_HIGHLOW);
            }
#endif
        }
        else
        {
#ifdef RELOC_SUPPORT
            assert(id->idInfo.idCnsReloc);

            // We need to defer this with a fixup

            X86deferredCall * pDC = X86deferredCall::create(emitComp, emitCmpHandle, methHnd, dst, 0);

            assert(id->idInfo.idCnsReloc);

            dst += emitOutputLong(dst, 0);

            emitCmpHandle->recordRelocation((void**)dst-1, IMAGE_REL_BASED_HIGHLOW);

            emitCmpHandle->deferLocation (methHnd, pDC);
#else
            assert(addr != NULL);       // @TODO [REVISIT] [04/16/01] []: fixup address for Std JIT
#endif
        }
        sz = sizeof(instrDescCns);
        break;

    case IF_RRD_CNS:
    case IF_RWR_CNS:
    case IF_RRW_CNS:

        dst = emitOutputRI(dst, id);
        sz  = emitSizeOfInsDsc(id);
        break;

    case IF_RRW_RRW_CNS:

        assert(id->idGCrefGet() == GCT_NONE);

        /* Get the 'base' opcode (it's a big one) */

        code = insEncodeMRreg(ins); assert(code & 0x00FF0000);

        dst += emitOutputByte(dst, code >> 16);

        dst += emitOutputWord(dst, code | (insEncodeReg345(id->idRg2Get()) |
                                           insEncodeReg012(id->idRegGet())) << 8);

        dst += emitOutputByte(dst, emitGetInsSC(id));

        sz   = emitSizeOfInsDsc(id);
        break;

        /********************************************************************/
        /*                      Address mode operand                        */
        /********************************************************************/

    case IF_ARD:
    case IF_AWR:
    case IF_ARW:

    case IF_TRD_ARD:
    case IF_TWR_ARD:
    case IF_TRW_ARD:

//  case IF_ARD_TRD:
    case IF_AWR_TRD:
//  case IF_ARW_TRD:

        dst = emitOutputAM(dst, id, insCodeMR(ins));

        switch (ins)
        {
        case INS_call:

    IND_CALL:
            /* Get hold of the argument count and method handle */

            args = emitGetInsCIargs(id);

            /* Is this a "fat" call descriptor? */

            if  (id->idInfo.idLargeCall)
            {
                bregs       = ((instrDescCIGCA*)id)->idciByrefRegs;
                byrefRegs   = emitDecodeCallGCregs(bregs);

                sz          = sizeof(instrDescCIGCA);
            }
            else
            {
                assert(id->idInfo.idLargeDsp == false);
                assert(id->idInfo.idLargeCns == false);

                byrefRegs   = 0;

                sz          = sizeof(instrDesc);
            }

            recCall = true;

            goto DONE_CALL;

        case INS_i_jmp:
            sz = emitSizeOfInsDsc((instrDescDsp*)id);
            break;

        default:
            sz = emitSizeOfInsDsc((instrDescAmd*)id);
            break;
        }
        break;

    case IF_RRD_ARD:
    case IF_RWR_ARD:
    case IF_RRW_ARD:

        dst = emitOutputAM(dst, id, insCodeRM(ins) | (insEncodeReg345(id->idRegGet()) << 8));
        sz  = emitSizeOfInsDsc((instrDescAmd*)id);
        break;

    case IF_ARD_RRD:
    case IF_AWR_RRD:
    case IF_ARW_RRD:

        dst = emitOutputAM(dst, id, insCodeMR(ins) | (insEncodeReg345(id->idRegGet()) << 8));
        sz  = emitSizeOfInsDsc((instrDescAmd*)id);
        break;

    case IF_ARD_CNS:
    case IF_AWR_CNS:
    case IF_ARW_CNS:

        emitGetInsAmdCns(id, &cnsVal);
        dst = emitOutputAM(dst, id, insCodeMI(ins), &cnsVal);
        sz  = emitSizeOfInsDsc((instrDescAmdCns*)id);
        break;

    case IF_ARW_SHF:

        emitGetInsAmdCns(id, &cnsVal);
        dst = emitOutputAM(dst, id, insCodeMR(ins), &cnsVal);
        sz  = emitSizeOfInsDsc((instrDescAmdCns*)id);
        break;

        /********************************************************************/
        /*                      Stack-based operand                         */
        /********************************************************************/

    case IF_SRD:
    case IF_SWR:
    case IF_SRW:

    case IF_TRD_SRD:
    case IF_TWR_SRD:
    case IF_TRW_SRD:

//  case IF_SRD_TRD:
    case IF_SWR_TRD:
//  case IF_SRW_TRD:

        assert(ins != INS_pop_hide);
        if  (ins == INS_pop)
        {
            /* The offset in "pop [ESP+xxx]" is relative to the new ESP value */

            emitCurStackLvl -= sizeof(int);
            dst = emitOutputSV(dst, id, insCodeMR(ins));
            emitCurStackLvl += sizeof(int);
            break;
        }

        dst = emitOutputSV(dst, id, insCodeMR(ins));

        if (ins == INS_call)
            goto IND_CALL;

        break;

    case IF_SRD_CNS:
    case IF_SWR_CNS:
    case IF_SRW_CNS:

        emitGetInsCns(id, &cnsVal);
        dst = emitOutputSV(dst, id, insCodeMI(ins), &cnsVal);
        sz  = emitSizeOfInsDsc((instrDescCns*)id);
        break;

    case IF_SRW_SHF:

        emitGetInsCns(id, &cnsVal);
        dst = emitOutputSV(dst, id, insCodeMR(ins), &cnsVal);
        sz  = emitSizeOfInsDsc((instrDescCns*)id);
        break;

    case IF_RRD_SRD:
    case IF_RWR_SRD:
    case IF_RRW_SRD:

        dst = emitOutputSV(dst, id, insCodeRM(ins) | (insEncodeReg345(id->idRegGet()) << 8));
        break;

    case IF_SRD_RRD:
    case IF_SWR_RRD:
    case IF_SRW_RRD:

        dst = emitOutputSV(dst, id, insCodeMR(ins) | (insEncodeReg345(id->idRegGet()) << 8));
        break;

        /********************************************************************/
        /*                    Direct memory address                         */
        /********************************************************************/

    case IF_MRD:
    case IF_MRW:
    case IF_MWR:

    case IF_TRD_MRD:
    case IF_TWR_MRD:
    case IF_TRW_MRD:

//  case IF_MRD_TRD:
    case IF_MWR_TRD:
//  case IF_MRW_TRD:

        dst = emitOutputCV(dst, id, insCodeMR(ins) | 0x0500);

        if  (ins == INS_call)
        {
#if 0
            /* All arguments will be popped after the call */

            emitStackPop(dst, true, emitGetInsDspCns(id, &offs));

            /* Figure out the size of the instruction descriptor */

            if  (id->idInfo.idLargeCall)
                sz = sizeof(instrDescDCGC);
            else
                sz = emitSizeOfInsDsc((instrDescDspCns*)id);

            /* Do we need to record a call location for GC purposes? */

            if  (!emitFullGCinfo)
                scRecordGCcall(dst);

#else

            assert(!"what???????");

#endif

        }
        else
            sz = emitSizeOfInsDsc((instrDescDspCns*)id);

        break;

    case IF_MRD_OFF:
        dst = emitOutputCV(dst, id, insCodeMI(ins));
        break;

    case IF_RRD_MRD:
    case IF_RWR_MRD:
    case IF_RRW_MRD:

        dst = emitOutputCV(dst, id, insCodeRM(ins) | (insEncodeReg345(id->idRegGet()) << 8) | 0x0500);
        sz  = emitSizeOfInsDsc((instrDescDspCns*)id);
        break;

    case IF_RWR_MRD_OFF:

        dst = emitOutputCV(dst, id, insCode(ins) | 0x30 | insEncodeReg012(id->idRegGet()));
        sz  = emitSizeOfInsDsc((instrDescDspCns*)id);
        break;

    case IF_MRD_RRD:
    case IF_MWR_RRD:
    case IF_MRW_RRD:

        dst = emitOutputCV(dst, id, insCodeMR(ins) | (insEncodeReg345(id->idRegGet()) << 8) | 0x0500);
        sz  = emitSizeOfInsDsc((instrDescDspCns*)id);
        break;

    case IF_MRD_CNS:
    case IF_MWR_CNS:
    case IF_MRW_CNS:

        emitGetInsDcmCns(id, &cnsVal);
        dst = emitOutputCV(dst, id, insCodeMI(ins) | 0x0500, &cnsVal);
        sz  = sizeof(instrDescDCM);
        break;

    case IF_MRW_SHF:

        emitGetInsDcmCns(id, &cnsVal);
        dst = emitOutputCV(dst, id, insCodeMR(ins) | 0x0500, &cnsVal);
        sz  = sizeof(instrDescDCM);
        break;

        /********************************************************************/
        /*                  FP coprocessor stack operands                   */
        /********************************************************************/

    case IF_TRD_FRD:
    case IF_TWR_FRD:
    case IF_TRW_FRD:

        assert(id->idGCrefGet() == GCT_NONE);

        dst += emitOutputWord(dst, insCodeMR(ins) | 0xC000 | (id->idReg << 8));
        break;

    case IF_FRD_TRD:
    case IF_FWR_TRD:
    case IF_FRW_TRD:

        assert(id->idGCrefGet() == GCT_NONE);

        dst += emitOutputWord(dst, insCodeMR(ins) | 0xC004 | (id->idReg << 8));
        break;

        /********************************************************************/
        /*                           Epilog block                           */
        /********************************************************************/

    case IF_EPILOG:

#if 0

        /* Nothing is live at this point */

        if  (emitThisGCrefRegs)
            emitUpdateLiveGCregs(0, dst); // ISSUE: What if ptr returned in EAX?

        emitUpdateLiveGCvars(0, dst);

#endif

        /* Record the code offset of the epilog */

        ((instrDescCns*)id)->idcCnsVal = emitCurCodeOffs(dst);

        /* Output the epilog code bytes */

        memcpy(dst, emitEpilogCode, emitEpilogSize);
        dst += emitEpilogSize;

        sz = sizeof(instrDescCns);

        break;

        /********************************************************************/
        /*                            oops                                  */
        /********************************************************************/

    default:

#ifdef  DEBUG
        printf("unexpected format %s\n", emitIfName(id->idInsFmt));
        assert(!"don't know how to encode this instruction");
#endif
        break;
    }

    /* Make sure we set the instruction descriptor size correctly */

    assert(sz == emitSizeOfInsDsc(id));

    /* Make sure we keep the current stack level up to date */

    if  (!emitIGisInProlog(ig))
    {
        switch (ins)
        {
        case INS_push:

            /* Please note: {INS_push_hide,IF_LABEL} is used to push the address of the
               finally block for calling it locally for an op_leave. */

            emitStackPush(dst, id->idGCrefGet());
            break;

        case INS_pop:

            emitStackPop(dst, false, 1);
            break;

        case INS_sub:

            /* Check for "sub ESP, icon" */

            if  (ins == INS_sub && id->idInsFmt == IF_RRW_CNS
                                && id->idReg    == SR_ESP)
            {
                emitStackPushN(dst, emitGetInsSC(id) / sizeof(void*));
            }
            break;

        case INS_add:

            /* Check for "add ESP, icon" */

            if  (ins == INS_add && id->idInsFmt == IF_RRW_CNS
                                && id->idReg    == SR_ESP)
            {
                emitStackPop (dst, false, emitGetInsSC(id) / sizeof(void*));
            }
            break;

        }
    }

    assert((int)emitCurStackLvl >= 0);

    /* Only epilog "instructions" and some pseudo-instrs
      are allowed not to generate any code */

    assert(*dp != dst || emitInstHasNoCode(ins) || id->idInsFmt == IF_EPILOG);

#ifdef  TRANSLATE_PDB
    if(*dp != dst)
    {
        // only map instruction groups to instruction groups
        MapCode( id->idilStart, *dp );
    }
#endif

    *dp = dst;

#ifdef DEBUG
    if (ins == INS_mulEAX || ins == INS_imulEAX)
    {
        // INS_mulEAX has implicit target of Edx:Eax. Make sure
        // that we detected this cleared its GC-status.

        assert(((RBM_EAX|RBM_EDX) & (emitThisGCrefRegs|emitThisByrefRegs)) == 0);
    }

    if (instrIs3opImul(ins))
    {
        // The target of the 3-operand imul is implicitly encoded. Make sure
        // that we detected the implicit register and cleared its GC-status.

        regMaskTP regMask = emitRegMask((emitRegs) Compiler::inst3opImulReg(ins));
        assert((regMask & (emitThisGCrefRegs|emitThisByrefRegs)) == 0);
    }
#endif

    return  sz;
}


#ifdef RELOC_SUPPORT

/*****************************************************************************
 *
 *  Fixup a deferred direct call instruction.
 */

#ifdef BIRCH_SP2

#include "corjit.h"
#include "OptJitInfo.h"
#include "PEReader.h"

void emitter::X86deferredCall::applyLocation()
{
    // if m_srcAddr is zero then this is a absolute address, not pcrel
    // but the same code path determines the correct fixup value

    BYTE *   addr  = OptJitInfo::sm_oper->getCallAddrByIndex((unsigned)m_mh);
    unsigned pcrel = (addr - m_srcAddr);

    assert(addr    != 0);
    assert(*m_dest == 0);

    *m_dest = pcrel;
}

#else

void emitter::X86deferredCall::applyLocation()
{
    // !!! is m_cmp still allocated at this point?

    InfoAccessType accessType = IAT_VALUE;
    BYTE * addr = (BYTE*) m_cmp->getFunctionFixedEntryPoint(m_mh, &accessType);
    assert(accessType == IAT_VALUE);

    *m_dest = (unsigned) (addr - (BYTE*)m_srcAddr);
}

#endif // BIRCH_SP2

emitter::X86deferredCall* emitter::X86deferredCall::create(Compiler *   comp, 
                                                           COMP_HANDLE  cmp, 
                                                           CORINFO_METHOD_HANDLE methHnd, 
                                                           BYTE*        dest, 
                                                           BYTE*        srcAddr)
{
    // if srcAddr is zero then this is a absolute address, not pcrel
    return new(comp->compGetMem(sizeof(X86deferredCall))) X86deferredCall(cmp, methHnd, dest, srcAddr);
}

/*****************************************************************************
 *
 *  Return the translated code address for the codeBuffPtr or
 *          the translated code address for the start of the
 *          current method, if codeBuffPtr is NULL
 *
 */

BYTE* emitter::getCurrentCodeAddr(BYTE* codeBuffPtr)
{
#ifdef BIRCH_SP2

    BYTE* srcAddr = OptJitInfo::sm_oper->getCallAddrByIndex(OptJitInfo::sm_oper->m_iCurrentlyCompiling);
    if (codeBuffPtr != NULL)
        srcAddr += emitCurCodeOffs(codeBuffPtr);

#else

    BYTE* srcAddr = codeBuffPtr;

#endif

    return srcAddr;
}

#endif // RELOC_SUPPORT


/*****************************************************************************/
#endif//TGT_x86
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\flowgraph.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                          FlowGraph                                        XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

#include "jitpch.h"
#pragma hdrstop

#include "allocaCheck.h"     // for alloca
#define WELL_FORMED_IL_CHECK
/*****************************************************************************/


void                Compiler::fgInit()
{
    impInit();

    /* Initialization for fgWalkTreePre() and fgWalkTreePost() */

#ifdef DEBUG
    // Reset anti-reentrancy checks
    fgWalkPre .wtprVisitorFn     = NULL;
    fgWalkPre .wtprCallbackData  = NULL;
    fgWalkPost.wtpoVisitorFn     = NULL;
    fgWalkPost.wtpoCallbackData  = NULL;
#endif

    /* We haven't yet computed the dominator sets */
    fgDomsComputed  = false;

    /* We don't know yet which loops will always execute calls */
    fgLoopCallMarked = false;

    /* Initialize the basic block list */

    fgFirstBB       = 0;
    fgLastBB        = 0;
    fgBBcount       = 0;
    fgDomBBcount    = 0;
        
    /* We haven't reached the global morphing phase */
    fgGlobalMorph   = false;
    fgAssertionProp = false;
    fgExpandInline  = false;

    fgModified      = false;

    /* Statement list is not threaded yet */

    fgStmtListThreaded = false;

    // Initialize the logic for adding code. This is used to insert code such
    // as the code that raises an exception when an array range check fails.

    fgAddCodeList    = 0;
    fgAddCodeModf    = false;

    for (int i=0; i<ACK_COUNT; i++)
    {
        fgExcptnTargetCache[i] = NULL;
    }

    /* Keep track of the max count of pointer arguments */

    fgPtrArgCntCur   =
    fgPtrArgCntMax   = 0;

    /* This global flag is set whenever we remove a statement */
    fgStmtRemoved   = false;

    /* This global flag is set whenever we add a throw block for a RngChk */
    fgRngChkThrowAdded = false; /* reset flag for fgIsCodeAdded() */

    fgIncrCount     = 0;

    /* We will record a list of all BBJ_RETURN blocks here */
    fgReturnBlocks  = 0;

    /* These are set by fgComputeDoms */
    fgPerBlock      = 0;
    fgEnterBlks     = NULL;
}

/*****************************************************************************
 *
 *  Create a basic block and append it to the current BB list.
 */

BasicBlock *        Compiler::fgNewBasicBlock(BBjumpKinds jumpKind)
{
    // This method must not be called after the exception table has been
    // constructed, because it doesn't not provide support for patching
    // the exception table.

    assert(!compHndBBtab || (info.compXcptnsCount == 0));

    BasicBlock *    block;

    /* Allocate the block descriptor */

    block = bbNewBasicBlock(jumpKind);
    assert(block->bbJumpKind == jumpKind);

    /* Append the block to the end of the global basic block list */

    if  (fgFirstBB)
        fgLastBB->bbNext = block;
    else
        fgFirstBB        = block;

    fgLastBB = block;

    return block;
}

/*****************************************************************************
 *
 *  A helper to prepend a basic block with the given expression to the current
 *  method.
 */

BasicBlock  *       Compiler::fgPrependBB(GenTreePtr tree)
{
    BasicBlock  *   block;

    assert(tree && tree->gtOper != GT_STMT);

    /* Allocate the block descriptor */

    block = bbNewBasicBlock(BBJ_NONE);

    /* Make sure the block doesn't get thrown away! */

    block->bbFlags |= BBF_IMPORTED | BBF_INTERNAL;

    /* Prepend the block to the global basic block list */

    block->bbNext = fgFirstBB;
                    fgFirstBB = block;

    /* Create a statement expression */

    tree = gtNewStmt(tree);

    /* Set up the linked list */

    tree->gtNext = 0;
    tree->gtPrev = tree;

    /* Store the statement in the block */

    block->bbTreeList = tree;

    return  block;
}

/*****************************************************************************
 *
 *  Insert the given statement at the start of the given basic block.
 */

void                Compiler::fgInsertStmtAtBeg(BasicBlock *block,
                                                GenTreePtr  stmt)
{
    GenTreePtr      list = block->bbTreeList;

    assert(stmt && stmt->gtOper == GT_STMT);

    /* In any case the new block will now be the first one of the block */

    block->bbTreeList = stmt;
    stmt->gtNext      = list;

    /* Are there any statements in the block? */

    if  (list)
    {
        GenTreePtr      last;

        /* There is at least one statement already */

        last = list->gtPrev; assert(last && last->gtNext == 0);

        /* Insert the statement in front of the first one */

        list->gtPrev  = stmt;
        stmt->gtPrev  = last;
    }
    else
    {
        /* The block was completely empty */

        stmt->gtPrev  = stmt;
    }
}

/*****************************************************************************
 *
 *  Insert the given statement at the end of the given basic block.
 */

void                Compiler::fgInsertStmtAtEnd(BasicBlock *block,
                                                GenTreePtr  stmt)
{
    GenTreePtr      list = block->bbTreeList;

    assert(stmt && stmt->gtOper == GT_STMT);

    if  (list)
    {
        GenTreePtr      last;

        /* There is at least one statement already */

        last = list->gtPrev; assert(last && last->gtNext == 0);

        /* Append the statement after the last one */

        last->gtNext = stmt;
        stmt->gtPrev = last;
        list->gtPrev = stmt;
    }
    else
    {
        /* The block is completely empty */

        block->bbTreeList = stmt;
        stmt->gtPrev      = stmt;
    }
}

/*****************************************************************************
 *
 *  Insert the given statement at the end of the given basic block, but before
 *  the GT_JTRUE if present
 */

void        Compiler::fgInsertStmtNearEnd(BasicBlock * block, GenTreePtr stmt)
{
    assert(stmt && stmt->gtOper == GT_STMT);

    if ((block->bbJumpKind == BBJ_COND) ||
        (block->bbJumpKind == BBJ_SWITCH))
    {
        GenTreePtr first = block->bbTreeList; assert(first);
        GenTreePtr last  = first->gtPrev;     assert(last && last->gtNext == NULL);
        GenTreePtr after =  last->gtPrev;

#if DEBUG
        if (block->bbJumpKind == BBJ_COND)
        {
            assert(last->gtStmt.gtStmtExpr->gtOper == GT_JTRUE);
        }
        else
        {
            assert(block->bbJumpKind == BBJ_SWITCH);
            assert(last->gtStmt.gtStmtExpr->gtOper == GT_SWITCH);
        }
#endif
        
        /* Append 'stmt' before 'last' */

        stmt->gtNext = last;
        last->gtPrev = stmt;
        
        if (first == last)
        {
            /* There is only one stmt in the block */
            
            block->bbTreeList = stmt;
            stmt->gtPrev      = last;
        }
        else
        {
            assert(after && (after->gtNext == last));

           /* Append 'stmt' after 'after' */
            
            after->gtNext = stmt;
            stmt->gtPrev  = after;
        }
    }
    else
    {
        fgInsertStmtAtEnd(block, stmt);
    }
}


/*
    Removes a block from the return block list
*/
void                Compiler::fgRemoveReturnBlock(BasicBlock * block)
{
    flowList * fl;

    if (!fgReturnBlocks) return;

    if (fgReturnBlocks->flBlock==block)
    {
        // Its the 1st entry, assign new head of list
        fgReturnBlocks=fgReturnBlocks->flNext;
        return;
    }

    for (fl=fgReturnBlocks ; fl->flNext ; fl=fl->flNext)
    {
        if (fl->flNext->flBlock==block)
        {
            // Found it
            fl->flNext=fl->flNext->flNext;
            return;
        }
    }

}

/*****************************************************************************
 * Checks if a block is in the predecessor list of another
 * This is very helpful in keeping the predecessor list up to date because
 * we avoid expensive operations like memory allocation
 */

//_inline
bool                Compiler::fgIsPredForBlock(BasicBlock * block,
                                               BasicBlock * blockPred)
{
    flowList   *    pred;

    assert(block); assert(blockPred);

    for (pred = block->bbPreds; pred; pred = pred->flNext)
    {
        if (blockPred == pred->flBlock)
            return true;
    }

    return false;
}


/*****************************************************************************
 * Removes a block from the predecessor list
 */

//_inline
void                Compiler::fgRemovePred(BasicBlock * block,
                                           BasicBlock * blockPred)
{
    flowList   *    pred;

    assert(block); assert(blockPred);
    assert(fgIsPredForBlock(block, blockPred));

    /* Any changes to the flow graph invalidate the dominator sets */
    fgModified = true;

    /* Is this the first block in the pred list? */
    if  (blockPred == block->bbPreds->flBlock)
    {
        block->bbPreds = block->bbPreds->flNext;
        return;
    }

    assert(block->bbPreds);
    for (pred = block->bbPreds; pred->flNext; pred = pred->flNext)
    {
        if (blockPred == pred->flNext->flBlock)
        {
            pred->flNext = pred->flNext->flNext;
            return;
        }
    }
}

/*
    Removes all the appearances of block as predecessor of others
*/

void                Compiler::fgRemoveBlockAsPred(BasicBlock * block)
{
    assert(block);

    switch (block->bbJumpKind)
    {
        BasicBlock * bNext;

        case BBJ_CALL:
            if (!(block->bbFlags & BBF_RETLESS_CALL))
            {
                /* The block after the BBJ_CALL block is not reachable */
                bNext = block->bbNext;
            
                /* bNext is the predecessor of the last block in the finally. */

                if (bNext->bbRefs)
                {
                    assert(bNext->bbRefs == 1);
                    bNext->bbRefs = 0;        
                    fgRemovePred(bNext, bNext->bbPreds->flBlock);
                }
            }
 
            /* Fall through */

        case BBJ_COND:
        case BBJ_ALWAYS:

            block->bbJumpDest->bbRefs--;

            /* Update the predecessor list for 'block->bbJumpDest' and 'block->bbNext' */
            fgRemovePred(block->bbJumpDest, block);

            /* If BBJ_COND fall through */
            if (block->bbJumpKind != BBJ_COND)
                break;

        case BBJ_NONE:

            block->bbNext->bbRefs--;

            /* Update the predecessor list for 'block->bbNext' */
            fgRemovePred(block->bbNext, block);
            break;

        case BBJ_RET:

            if (block->bbFlags & BBF_ENDFILTER)
            {
                fgRemovePred(block->bbJumpDest, block);
            }
            else
            {
                /* Remove block as the predecessor of the bbNext of all
                   BBJ_CALL blocks calling this finally */

                unsigned hndIndex = block->getHndIndex();
                EHblkDsc * ehDsc = compHndBBtab + hndIndex;
                BasicBlock * tryBeg = ehDsc->ebdTryBeg;
                BasicBlock * tryEnd = ehDsc->ebdTryEnd;
                BasicBlock * finBeg = ehDsc->ebdHndBeg;

                for(BasicBlock * bcall = tryBeg; bcall != tryEnd; bcall = bcall->bbNext)
                {
                    if  ((bcall->bbFlags & BBF_REMOVED) ||
                          bcall->bbJumpKind != BBJ_CALL || 
                          bcall->bbJumpDest !=  finBeg)
                        continue;

                    assert(!(bcall->bbFlags & BBF_RETLESS_CALL));

                    assert(bcall->bbNext->bbRefs == 1);
                    bcall->bbNext->bbRefs--;
                    fgRemovePred(bcall->bbNext, block);
                }
            }
            break;

        case BBJ_THROW:
        case BBJ_RETURN:
            break;

        case BBJ_SWITCH:
        {
            unsigned        jumpCnt = block->bbJumpSwt->bbsCount;
            BasicBlock * *  jumpTab = block->bbJumpSwt->bbsDstTab;

            do
            {
                (*jumpTab)->bbRefs--;

                /* For all jump targets of BBJ_SWITCH remove predecessor 'block'
                 * It may be that we have jump targets to the same label so
                 * we check that we have indeed a predecessor */

                if  (fgIsPredForBlock(*jumpTab, block))
                    fgRemovePred(*jumpTab, block);
            }
            while (++jumpTab, --jumpCnt);

            break;
        }

        default:
            assert(!"Block doesnt have a valid bbJumpKind!!!!");
            break;
    }
}

/*****************************************************************************
 * Replaces a block in the predecessor list
 */

//_inline
void                Compiler::fgReplacePred(BasicBlock * block,
                                            BasicBlock * oldPred,
                                            BasicBlock * newPred)
{
    flowList   *    pred;

    assert(block); assert(oldPred); assert(newPred);
    assert(fgIsPredForBlock(block, oldPred));

    /* Any changes to the flow graph invalidate the dominator sets */
    fgModified = true; 
    
    for (pred = block->bbPreds; pred; pred = pred->flNext)
    {
        if (oldPred == pred->flBlock)
        {
            pred->flBlock = newPred;
            return;
        }
    }
    assert(!"fgReplacePred failed!!!");
}

/*****************************************************************************
 * Add blockPred to the predecessor list of block.
 * Note: a predecessor appears only once although it can have multiple jumps
 * to the block (e.g. switch, conditional jump to the following block, etc.).
 */

//_inline
void                Compiler::fgAddRefPred(BasicBlock * block,
                                           BasicBlock * blockPred)
{
    assert(!fgIsPredForBlock(block, blockPred)            ||
           ((blockPred->bbJumpKind == BBJ_COND) &&
            (blockPred->bbNext == blockPred->bbJumpDest)) ||
           (blockPred->bbJumpKind == BBJ_SWITCH));
    
    flowList* flow = (flowList *)compGetMem(sizeof(*flow));

#if     MEASURE_BLOCK_SIZE
    genFlowNodeCnt  += 1;
    genFlowNodeSize += sizeof(*flow);
#endif

    /* Any changes to the flow graph invalidate the dominator sets */
    fgModified     = true;


    /* Keep the predecessor list in lowest to highest bbNum order
     * This allows us to discover the loops in optFindNaturalLoops
     *  from innermost to outermost.
     */

    flowList** listp= &block->bbPreds;
    while (*listp && ((*listp)->flBlock->bbNum < blockPred->bbNum))
    {
        listp = & (*listp)->flNext;
    }

    flow->flNext  = *listp;
    flow->flBlock = blockPred;
    *listp        = flow;

    block->bbRefs++;
}

/*****************************************************************************
 *
 *  Returns true if block b1 dominates block b2
 */

bool                Compiler::fgDominate(BasicBlock *b1, BasicBlock *b2)
{
    // If the fgModified flag is false then we made some modifications to
    // the flow graph, like adding a new block or changing a conditional branch
    // into an unconditional branch.
    // We can continue to use the dominator and reachable information to 
    // unmark loops as long as we haven't renumber the blocks or we aren't
    // asking for information about a new block

//    assert(!comp->fgModified || (comp->fgDomsComputed && b2->bbDom));
    assert(fgDomsComputed);

    if (b2->bbNum > fgDomBBcount)
    {
        if (b1 == b2)
            return true;

        for (flowList* pred = b2->bbPreds; pred != NULL; pred = pred->flNext)
            if (!fgDominate(b1, pred->flBlock))
                return false;

        return b2->bbPreds != NULL;
    }

    assert(b2->bbDom);

    if (b1->bbNum > fgDomBBcount)
    {
        // if b1 is a loop preheader and Succ is its only successor, then all predecessors of
        // Succ either are b1 itself or are dominated by Succ. Under these conditions, b1
        // dominates b2 if and only if Succ dominates b2 (or if b2 == b1, but we already tested
        // for this case)
        if (b1->bbFlags & BBF_LOOP_PREHEADER)
        {
            assert(b1->bbFlags & BBF_INTERNAL);
            assert(b1->bbJumpKind == BBJ_NONE);
            return fgDominate(b1->bbNext, b2);
        }

        // unknown dominators; err on the safe side and return false
        return false;
    }

    /* Check if b1 dominates b2 */
    unsigned num = b1->bbNum; assert(num > 0);
    num--;
    if (b2->bbDom[num / USZ] & (1U << (num % USZ)))
        return true;
    return false;
}

/*****************************************************************************
 *
 *  Returns true if block b1 can reach block b2
 */

bool                Compiler::fgReachable(BasicBlock *b1, BasicBlock *b2)
{
    assert(fgDomsComputed);

    if (b2->bbNum > fgDomBBcount)
    {
        if (b1 == b2)
            return true;

        for (flowList* pred = b2->bbPreds; pred != NULL; pred = pred->flNext)
            if (fgReachable(b1, pred->flBlock))
                return true;

        return false;
    }

    assert(b2->bbReach);

    if (b1->bbNum > fgDomBBcount)
    {
        assert(b1->bbJumpKind == BBJ_NONE || b1->bbJumpKind == BBJ_ALWAYS || b1->bbJumpKind == BBJ_COND);

        if (b1->bbFallsThrough() && fgReachable(b1->bbNext, b2))
            return true;

        if(b1->bbJumpKind == BBJ_ALWAYS || b1->bbJumpKind == BBJ_COND)
            return fgReachable(b1->bbJumpDest, b2);

        return false;
    }

    /* Check if b1 can reach b2 */
    unsigned num = b1->bbNum; assert(num > 0);
    num--;
    if (b2->bbReach[num / USZ] & (1U << (num % USZ)))
        return true;
    return false;
}

/*****************************************************************************
 *
 *  Function called to compute the dominator and reachable sets
 *  Returns true if we identify any loops
 */

bool                Compiler::fgComputeDoms()
{
    BasicBlock *  block;
    unsigned      num;
    bool          hasLoops = false; 
    bool          changed  = false; 

#ifdef  DEBUG
    // Make sure that the predecessor lists are accurate
    fgDebugCheckBBlist();

    if (verbose)
        printf("*************** In fgComputeDoms\n");
#endif

    /* First renumber the blocks and setup fgReturnBlocks */
    fgReturnBlocks = NULL;

    /* Walk the flow graph, reassign block numbers to keep them in ascending order */
    for (block = fgFirstBB, num = 1; 
         block != NULL; 
         block = block->bbNext, num++)
    {
        if (block->bbNum != num)
            changed = true;

        block->bbNum = num;

        /* Build list of BBJ_RETURN blocks */
        if (block->bbJumpKind == BBJ_RETURN)
        {
            flowList *  flow = (flowList *)compGetMem(sizeof(*flow));
            
            flow->flNext     = fgReturnBlocks;
            flow->flBlock    = block;
            fgReturnBlocks   = flow;
        }
        
        if (block->bbNext == NULL)
        {
            fgLastBB  = block;
            fgBBcount = num;
        }
    }

#ifdef DEBUG
    if (verbose && changed)
    {
        printf("After renumbering the basic blocks\n");
        fgDispBasicBlocks();
        printf("\n");
    }
#endif

    // numBlocks:: number of blocks that we will allocate for each set
    // perBlock :: how many 'unsigned' elements in each set
    // memSize  :: how many total bytes of memory we need to allocate
    //             we allocate four extra sets: allBlocks, newDom, newReach and enterBlks
    // memory   :: pointer to the allocated memory, it is incremented as we dole it out
    // needMem  :: number of the first block to start allocating at
    // memLast  :: where we expect the memory variable to end up at after all allocations
    
    unsigned      numBlocks = fgBBcount;
    unsigned      perBlock  = roundUp(fgBBcount, USZ) / USZ;
    
    __int64       memSizeL  = ((__int64) (numBlocks * 2 + 4)) *
                              ((__int64) (perBlock * sizeof(unsigned)));
    size_t        memSize   = (size_t) memSizeL;
    
    if (memSizeL != ((__int64) memSize))
    {
        assert(!"Method has too many basic blocks\n");
        NOMEM();
    }

    unsigned *    memory    = (unsigned *) compGetMem(memSize);
#ifdef DEBUG
    unsigned *    memLast   = (unsigned *) ( ((char *) memory) + memSize );
#endif
    
    // Four extra sets for use in the algorithm
    unsigned *    allBlocks = memory; memory += perBlock;
    unsigned *    newDom    = memory; memory += perBlock;
    unsigned *    newReach  = memory; memory += perBlock;
    unsigned *    enterBlks = memory; memory += perBlock;

    for (block = fgFirstBB, num = 0; block; block = block->bbNext, num++)
    {
        assert((block->bbNum == num+1) && (num < fgBBcount));
        
        /* Assign the virtual memory */
        block->bbReach = memory; memory += perBlock;
        block->bbDom   = memory; memory += perBlock;
    }       

    assert(memory == memLast);
    
    /* Initialize allBlocks */
    for (unsigned i=0; i < perBlock; i++)
    {
        if ((i+1 < perBlock) || ((numBlocks % 32) == 0))
            allBlocks[i] = (unsigned) -1;
        else
            allBlocks[i] = (unsigned) ((1 << (numBlocks % 32)) - 1);
    }
    
    /* Initialize enterBlks to the set blocks that we don't have
     *  have an explicit control flow edges for, these are the
     *  entry basic block and each of the handler blocks 
     */
    
    /* First set enterBlks to zero */
    for (i=0; i < perBlock; i++)
        enterBlks[i] = 0;
    
    /* Now or in the entry basic block */
    enterBlks[0] |= 1;


    if (info.compXcptnsCount > 0)
    {
        /* Also or in the handler basic blocks */
        unsigned        XTnum;
        EHblkDsc *      HBtab;
        for (XTnum = 0, HBtab = compHndBBtab;
             XTnum < info.compXcptnsCount;
             XTnum++,   HBtab++)
        {
            if (HBtab->ebdFlags & CORINFO_EH_CLAUSE_FILTER)
            {
                num = HBtab->ebdFilter->bbNum;
                num--;
                enterBlks[num / USZ] |= 1 << (num % USZ);
            }
            num = HBtab->ebdHndBeg->bbNum;
            num--;
            enterBlks[num / USZ] |= 1 << (num % USZ);
        }
        fgEnterBlks = enterBlks;
        fgPerBlock  = perBlock;
    }

    /* initialize dominator bit vectors and reachable bit vectors */
    
    for (block = fgFirstBB, num = 0; block; block = block->bbNext, num++)
    {
        assert((block->bbNum == num+1) && (num < fgBBcount));

        /* Initialize bbReach to zero */
        for (i=0; i < perBlock; i++) {block->bbReach[i] = 0;}
        
        /* Mark bbReach as reaching itself */
        block->bbReach[num / USZ] |= 1 << (num % USZ);
        
        if (enterBlks[num / USZ] & (1 << (num % USZ)))
        {
            /* Initialize bbDom to zero */
            for (i=0; i < perBlock; i++) {block->bbDom[i] = 0;}
            
            /* Mark bbDom as dominating itself */
            block->bbDom[num / USZ] |= 1 << (num % USZ);
        }
        else
        {
            /* Initialize bbDom to allBlocks */
            for (i=0; i < perBlock; i++) {block->bbDom[i] = allBlocks[i];}
        }
    }
    
    /* find the dominators and reachable blocks */
    
    bool change;
    do
    {
        change = false;
        
        for (block = fgFirstBB, num = 0; block; block = block->bbNext, num++)
        {
            /* Initialize newReach to block->bbReach */
            for (i=0; i < perBlock; i++) {newReach[i] = block->bbReach[i];}
            
            /* Initialize newDom to allBlocks        */
            for (i=0; i < perBlock; i++) {newDom[i] = allBlocks[i];}
            
            bool predGcSafe = (block->bbPreds!=NULL); // Do all of our predecessor blocks have a GC safe bit?

            for (flowList* pred = block->bbPreds; 
                 pred != NULL; 
                 pred = pred->flNext)
            {
                BasicBlock * predBlock = pred->flBlock;

                /* Or in the bits from the pred into newReach */
                for (i=0; i < perBlock; i++) {newReach[i] |= predBlock->bbReach[i];}
                    
                /* And in the bits from the pred into newDom  */
                for (i=0; i < perBlock; i++) {newDom[i]   &= predBlock->bbDom[i];}

                if (!(predBlock->bbFlags & BBF_GC_SAFE_POINT))
                    predGcSafe = false;
            }

            if  (predGcSafe)
                block->bbFlags |= BBF_GC_SAFE_POINT;
            
            /* Is newReach different than block->bbReach? */
            for (i=0; i < perBlock; i++)
            {
                if (newReach[i] != block->bbReach[i])
                {
                    block->bbReach[i] = newReach[i];
                    change = true;
                }
            }
            
            if (enterBlks[num / USZ] & (1 << (num % USZ)))
                continue;
            
            /* Mark newDom as dominating itself */
            newDom[num / USZ] |= 1 << (num % USZ);
            
            /* Is newDom different than block->bbDom? */
            for (i=0; i < perBlock; i++)
            {
                if (newDom[i] != block->bbDom[i])
                {
                    block->bbDom[i] = newDom[i];
                    change = true;
                }
            }
        }
    }
    while (change);
 
    bool          hasUnreachableBlocks = false;
    BasicBlock *  bPrev;
    
    /* Record unreachable blocks */
    for (block  = fgFirstBB, bPrev = NULL;
         block != NULL; 
         bPrev  = block, block = block->bbNext)
    {
        //  If none of the blocks in enterBlks[] can reach this block
        //   then the block is unreachable

        /* Internal throw blocks are also reachable */
        if (fgIsThrowHlpBlk(block))
        {
            goto SKIP_BLOCK;
        }
        else
        {
            for (i=0; i < perBlock; i++)
                if (enterBlks[i] & block->bbReach[i])
                    goto SKIP_BLOCK;
        }
        
        //  Remove all the code for the block
        fgUnreachableBlock(block, bPrev);

        // Make sure that the block was marked as removed */
        assert(block->bbFlags & BBF_REMOVED);

        // Some blocks mark the end of trys and catches
        // and can't be removed,. We convert these into
        // empty blocks of type BBJ_THROW
        
        if (block->bbFlags & BBF_DONT_REMOVE)
        {
            /* Unmark the block as removed, */
            /* clear BBF_INTERNAL as well and set BBJ_IMPORTED */

            block->bbFlags    &= ~(BBF_REMOVED | BBF_INTERNAL);
            block->bbFlags    |= BBF_IMPORTED;
            block->bbJumpKind  = BBJ_THROW;
            block->bbSetRunRarely();
        }
        else
        {
            /* We have to call fgRemoveBlock next */
            hasUnreachableBlocks = true;
        }
        continue;

SKIP_BLOCK:;

        if (block->isRunRarely())
            continue;
        if (block->bbJumpKind == BBJ_RETURN)
            continue;

        /* Set BBF_LOOP_HEAD if we have backwards branches to this block */

        for (flowList* pred = block->bbPreds; 
             pred != NULL; 
             pred = pred->flNext)
        {
            BasicBlock* bPred = pred->flBlock;
            unsigned num = block->bbNum - 1;
            if (num < bPred->bbNum)
            {
                if (bPred->isRunRarely())
                    continue;
                if (bPred->bbJumpKind == BBJ_CALL)
                    continue;

                /* If block can reach bPred then we have a loop head */
                if (bPred->bbReach[num / USZ] & (1U << (num % USZ)))
                {
                    hasLoops = true;
                    
                    /* Set the BBF_LOOP_HEAD flag */
                    block->bbFlags |= BBF_LOOP_HEAD;
                    break;
                }
            }
        }
    }

    if (hasUnreachableBlocks)
    {
        /* Now remove the unreachable blocks */
        for (block  = fgFirstBB, bPrev = NULL;
             block != NULL; 
             block = block->bbNext)
        {
            //  If we mark the block with BBF_REMOVED then
            //  we need to call fgRemovedBlock() on it

            if (block->bbFlags & BBF_REMOVED)
            {
                fgRemoveBlock(block, bPrev, true);

                // fgRemoveBlock of a BBJ_CALL block will also remove
                // the ALWAYS block, so we advance 1 place in the block list unless
                // the RETLESS flag is set, whice means that the ALWAYS block was removed
                // before.
                if (block->bbJumpKind==BBJ_CALL && !(block->bbFlags & BBF_RETLESS_CALL))
                {
                    block = block->bbNext;
                }
            }
            else
            {
                bPrev = block;
            }
        }
    }

    fgModified     = false;
    fgDomsComputed = true;
    fgDomBBcount   = fgBBcount;

    return hasLoops;
}

/*****************************************************************************
 *
 *  Function called to compute the bbPred lists
 */

void                Compiler::fgComputePreds()
{
    assert(fgFirstBB);

    BasicBlock *  block;
    unsigned      num;
    
#ifdef  DEBUG
    if  (verbose)
    {
        printf("\n*************** In fgComputePreds()\n");
        fgDispBasicBlocks();
        printf("\n");
    }
#endif

#ifdef DEBUG
    if (verbose)
        printf("Renumbering the basic blocks for fgComputePred\n");
#endif

    /* reset the refs count for each basic block 
     *  and renumber the blocks 
     */
    for (block = fgFirstBB, num = 1; block; block = block->bbNext, num++)
    {
        block->bbNum  = num;
        block->bbRefs = 0;

        if (block->bbNext == NULL)
        {
            fgLastBB  = block;
            fgBBcount = num;
        }
    }

    /* the first block is always reacheable! */
    fgFirstBB->bbRefs = 1;

    /* Treat the initial block as a jump target */
    fgFirstBB->bbFlags |= BBF_JMP_TARGET|BBF_HAS_LABEL;

    for (block = fgFirstBB, num = 0; block; block = block->bbNext)
    {
        /* @TODO [CONSIDER] [04/16/01] []: if we already have predecessors computed, free that memory */
        block->bbPreds = NULL;
    }
    
    for (block = fgFirstBB; block; block = block->bbNext)
    {
        switch (block->bbJumpKind)
        {
        case BBJ_CALL:
            if (!(block->bbFlags & BBF_RETLESS_CALL))
            {
                /* Mark the next block as being a jump target, 
                   since the call target will return there */
                assert(block->bbNext);
                block->bbNext->bbFlags |= BBF_JMP_TARGET|BBF_HAS_LABEL;
            }
            // Fall through

        case BBJ_COND:
        case BBJ_ALWAYS:

            /* Mark the jump dest block as being a jump target */
            block->bbJumpDest->bbFlags |= BBF_JMP_TARGET|BBF_HAS_LABEL;
                
            fgAddRefPred(block->bbJumpDest, block);

            /* Is the next block reachable? */
            
            if  (block->bbJumpKind != BBJ_COND)
                break;
                
            assert(block->bbNext);
                
            /* Fall through, the next block is also reachable */
                
        case BBJ_NONE:

            fgAddRefPred(block->bbNext, block);
            break;
                
        case BBJ_RET:

            if (block->bbFlags & BBF_ENDFILTER)
            {
                /* Connect end of filter to catch handler */
                    
                fgAddRefPred(block->bbJumpDest, block);
            }
            else
            {
                /* Connect the end of the finally to the successor of
                  the call to this finally */

                if (!block->hasHndIndex())
                    NO_WAY("endfinally outside a finally/fault block.");

                unsigned      hndIndex = block->getHndIndex();
                EHblkDsc   *  ehDsc    = compHndBBtab + hndIndex;
                BasicBlock *  tryBeg   = ehDsc->ebdTryBeg;
                BasicBlock *  tryEnd   = ehDsc->ebdTryEnd;
                BasicBlock *  finBeg   = ehDsc->ebdHndBeg;
                
                if ((ehDsc->ebdFlags & (CORINFO_EH_CLAUSE_FINALLY | CORINFO_EH_CLAUSE_FAULT)) == 0)
                    NO_WAY("endfinally outside a finally/fault block.");

                for(BasicBlock * bcall = tryBeg; bcall != tryEnd; bcall = bcall->bbNext)
                {
                    if  (bcall->bbJumpKind != BBJ_CALL || bcall->bbJumpDest !=  finBeg)
                        continue;

                    assert(!(bcall->bbFlags & BBF_RETLESS_CALL));

                    fgAddRefPred(bcall->bbNext, block);
                }
            }
            break;

        case BBJ_THROW:
        case BBJ_RETURN:
            break;
                
        case BBJ_SWITCH:
            unsigned        jumpCnt = block->bbJumpSwt->bbsCount;
            BasicBlock * *  jumpTab = block->bbJumpSwt->bbsDstTab;

            do
            {
                /* Mark the target block as being a jump target */
                (*jumpTab)->bbFlags |= BBF_JMP_TARGET|BBF_HAS_LABEL;

                fgAddRefPred(*jumpTab, block);
            }
            while (++jumpTab, --jumpCnt);

            break;
        }
    }

    for (unsigned EHnum = 0; EHnum < info.compXcptnsCount; EHnum++)
    {
        EHblkDsc *   ehDsc = compHndBBtab + EHnum;

        if (ehDsc->ebdFlags & CORINFO_EH_CLAUSE_FILTER)
            ehDsc->ebdFilter->bbFlags |= BBF_JMP_TARGET|BBF_HAS_LABEL;

        ehDsc->ebdHndBeg->bbFlags |= BBF_JMP_TARGET|BBF_HAS_LABEL;
    }

    fgModified = false;
}

/*****************************************************************************
 *
 *  The following helps find a basic block given its PC offset.
 */

void                Compiler::fgInitBBLookup()
{
    BasicBlock **   dscBBptr;
    BasicBlock  *   tmpBBdesc;

    /* Allocate the basic block table */

    dscBBptr =
    fgBBs = (BasicBlock **)compGetMemArrayA(fgBBcount, sizeof(*fgBBs));

    /* Walk all the basic blocks, filling in the table */

    for (tmpBBdesc = fgFirstBB; tmpBBdesc; tmpBBdesc = tmpBBdesc->bbNext)
    {
        *dscBBptr++ = tmpBBdesc;
    }

    assert(dscBBptr == fgBBs + fgBBcount); 
}


BasicBlock *        Compiler::fgLookupBB(unsigned addr)
{
    unsigned        lo;
    unsigned        hi;

    /* Do a binary search */

    for (lo = 0, hi = fgBBcount - 1;;)
    {
        unsigned    mid = (lo + hi) / 2;
        BasicBlock *dsc = fgBBs[mid];

        // We introduce internal blocks for BBJ_CALL. Skip over these.

        while (dsc->bbFlags & BBF_INTERNAL)
        {
            dsc = dsc->bbNext;
            mid++;

            // We skipped over too many. Do a linear search

            if (mid > hi)
            {
                for (unsigned i = lo; i < hi; i++)
                {
                    dsc = fgBBs[i];

                    if (!(dsc->bbFlags & BBF_INTERNAL) && (dsc->bbCodeOffs == addr))
                        return dsc;
                }
                assert(!"fgLookupBB() failed");
            }
        }

        unsigned    pos = dsc->bbCodeOffs;

#ifdef  DEBUG
        if  (lo > hi)
            printf("ERROR: Couldn't find basic block at offset %04X\n", addr);
        assert(lo <= hi);
#endif

        if  (pos < addr)
        {
            lo = mid+1;
            continue;
        }

        if  (pos > addr)
        {
            hi = mid-1;
            continue;
        }

        return  dsc;
    }
}


/*****************************************************************************
 *
 *  The 'jump target' array uses the following flags to indicate the kind
 *  of a label is present.
 */

#define JT_NONE         0x00        // This IL offset is never used
#define JT_ADDR         0x01        // merely make sure this is an OK address
#define JT_JUMP         0x02        // 'normal' jump target
#define JT_MULTI        0x04        // target of multiple jumps

inline
void                Compiler::fgMarkJumpTarget(BYTE *jumpTarget, unsigned offs)
{
    /* Make sure we set JT_MULTI if target of multiple jumps */

    assert(JT_MULTI == JT_JUMP << 1);

    jumpTarget[offs] |= (jumpTarget[offs] & JT_JUMP) << 1 | JT_JUMP;
}

/*****************************************************************************
 *
 *  Walk the instrs and for any jumps we find set the appropriate entry
 *  in the 'jumpTarget' table.
 *  Also sets lvAddrTaken and lvArgWrite in lvaTable[]
 */

void                Compiler::fgFindJumpTargets(const BYTE * codeAddr,
                                        size_t       codeSize,
                                        BYTE *       jumpTarget)
{

    const   BYTE *  codeBegp = codeAddr;
    const   BYTE *  codeEndp = codeAddr + codeSize;

    while (codeAddr < codeEndp)
    {
        OPCODE      opcode;
        unsigned    sz;

        opcode = (OPCODE) getU1LittleEndian(codeAddr); 
        codeAddr += sizeof(__int8);

DECODE_OPCODE:

        /* Get the size of additional parameters */

        if (opcode >= CEE_COUNT)
            BADCODE3("Illegal opcode", ": %02X", (int) opcode);

        sz = opcodeSizes[opcode];

        switch (opcode)
        {
              signed        jmpDist;
            unsigned        jmpAddr;

            // For CEE_SWITCH
            unsigned        jmpBase;
            unsigned        jmpCnt;

            case CEE_PREFIX1:
                if (codeAddr >= codeEndp)
                    goto TOO_FAR;
                opcode = (OPCODE) (256+getU1LittleEndian(codeAddr));
                codeAddr += sizeof(__int8);
                goto DECODE_OPCODE;

            case CEE_PREFIX2:
            case CEE_PREFIX3:
            case CEE_PREFIX4:
            case CEE_PREFIX5:
            case CEE_PREFIX6:
            case CEE_PREFIX7:
            case CEE_PREFIXREF:
                BADCODE3("Illegal opcode", ": %02X", (int) opcode);

        /* Check for an unconditional jump opcode */

        case CEE_LEAVE:
        case CEE_LEAVE_S:
        case CEE_BR:
        case CEE_BR_S:

        /* Check for a conditional jump opcode */
        
        case CEE_BRFALSE:
        case CEE_BRFALSE_S:
        case CEE_BRTRUE:
        case CEE_BRTRUE_S:
        case CEE_BEQ:
        case CEE_BEQ_S:
        case CEE_BGE:
        case CEE_BGE_S:
        case CEE_BGE_UN:
        case CEE_BGE_UN_S:
        case CEE_BGT:
        case CEE_BGT_S:
        case CEE_BGT_UN:
        case CEE_BGT_UN_S:
        case CEE_BLE:
        case CEE_BLE_S:
        case CEE_BLE_UN:
        case CEE_BLE_UN_S:
        case CEE_BLT:
        case CEE_BLT_S:
        case CEE_BLT_UN:
        case CEE_BLT_UN_S:
        case CEE_BNE_UN:
        case CEE_BNE_UN_S:

            if (codeAddr + sz > codeEndp)
                goto TOO_FAR;

            /* Compute the target address of the jump */

            jmpDist = (sz==1) ? getI1LittleEndian(codeAddr)
                              : getI4LittleEndian(codeAddr);
            jmpAddr = (codeAddr - codeBegp) + sz + jmpDist;

            /* Make sure the target address is reasonable */

            if  (jmpAddr >= codeSize)
            {
                BADCODE3("code jumps to outer space",
                         " at offset %04X", codeAddr - codeBegp);
            }

            /* Finally, set the 'jump target' flag */

            fgMarkJumpTarget(jumpTarget, jmpAddr);
            break;

        case CEE_SWITCH:

            // Make sure we don't go past the end reading the number of cases

            if  (codeAddr + sizeof(DWORD) > codeEndp)
                goto TOO_FAR;

            // Read the number of cases

            jmpCnt = getU4LittleEndian(codeAddr);
            codeAddr += sizeof(DWORD);

            // Find the end of the switch table

            jmpBase = (codeAddr - codeBegp) + jmpCnt*sizeof(DWORD);

            /* Make sure we have room for the switch table */

            if  (jmpBase >= codeSize)
                goto TOO_FAR;

            // jmpBase is also the target of the default case, so mark it

            fgMarkJumpTarget(jumpTarget, jmpBase);

            /* Process all the entries in the jump table */

            while (jmpCnt)
            {
                jmpAddr = jmpBase + getI4LittleEndian(codeAddr);
                codeAddr += 4;

                if  (jmpAddr >= codeSize)
                    BADCODE3("jump target out of range",
                             " at offset %04X", codeAddr - codeBegp);

                fgMarkJumpTarget(jumpTarget, jmpAddr);

                jmpCnt--;
            }

            /* We've now consumed the entire switch opcode */

            continue;

#ifdef DEBUG
        // make certain we did not forget any flow of control instructions
        // by checking the 'ctrl' field in opcode.def.  First filter out all
        // non-ctrl instructions
#       define BREAK(name)          case name: break;
#       define CALL(name)           case name: break;
#       define NEXT(name)           case name: if (opcode == CEE_LDARGA_S || opcode == CEE_LDARGA ||  \
                                                   opcode == CEE_LDLOCA_S || opcode == CEE_LDLOCA)    \
                                                    goto ADDR_TAKEN;                                  \
                                               else if (opcode == CEE_STARG_S || opcode == CEE_STARG) \
                                                    goto ARG_WRITE;                                   \
                                               else                                                   \
                                                    break;
#       define THROW(name)          case name: break;
#       define RETURN(name)         case name: break;
#       define META(name)
#       define BRANCH(name)
#       define COND_BRANCH(name)
#       define PHI(name)

#       define OPDEF(name,string,pop,push,oprType,opcType,l,s1,s2,ctrl) ctrl(name)
#       include "opcode.def"
#       undef OPDEF

#       undef PHI
#       undef BREAK
#       undef CALL
#       undef NEXT
#       undef THROW
#       undef RETURN
#       undef META
#       undef BRANCH
#       undef COND_BRANCH
        // These dont need any handling
        case CEE_VOLATILE:  // CTRL_META
        case CEE_UNALIGNED: // CTRL_META
        case CEE_TAILCALL:  // CTRL_META
            if (codeAddr >= codeEndp)
                goto TOO_FAR;
           break;

        // what's left are forgotten ctrl instructions
        default:
            BADCODE("Unrecognized control Opcode");
            break;
#else
        case CEE_STARG:
        case CEE_STARG_S:  goto ARG_WRITE;

        case CEE_LDARGA:
        case CEE_LDARGA_S:
        case CEE_LDLOCA:
        case CEE_LDLOCA_S: goto ADDR_TAKEN;
#endif

            unsigned varNum;
ADDR_TAKEN:
            assert(sz == sizeof(BYTE) || sz == sizeof(WORD));
            if (codeAddr + sz > codeEndp)
                goto TOO_FAR;
            varNum = (sz == sizeof(BYTE)) ? getU1LittleEndian(codeAddr)
                                          : getU2LittleEndian(codeAddr);
            if (opcode == CEE_LDLOCA || opcode == CEE_LDLOCA_S)
            {
                if (varNum >= info.compMethodInfo->locals.numArgs)
                    BADCODE("bad local number");

                varNum += info.compArgsCount;
            }
            else
            {
                if (varNum >= info.compILargsCount)
                    BADCODE("bad argument number");

                varNum = compMapILargNum(varNum); // account for possible hidden param
            }

            assert(varNum < lvaTableCnt);
            lvaTable[varNum].lvAddrTaken = 1;
            break;

ARG_WRITE:
            assert(sz == sizeof(BYTE) || sz == sizeof(WORD));
            if (codeAddr + sz > codeEndp)
                goto TOO_FAR;
            varNum = (sz == sizeof(BYTE)) ? getU1LittleEndian(codeAddr)
                                          : getU2LittleEndian(codeAddr);
            varNum = compMapILargNum(varNum); // account for possible hidden param
            
            // This check is only intended to prevent an AV.  Bad varNum values will later
            // be handled properly by the verifier.
            if (varNum < lvaTableCnt)
                lvaTable[varNum].lvArgWrite = 1;
            break;
        }

        /* Skip any operands this opcode may have */

        codeAddr += sz;
    }

    if  (codeAddr != codeEndp)
    {
TOO_FAR:
        BADCODE3("Code ends in the middle of an opcode, or there is a branch past the end of the method",
                 " at offset %04X", codeAddr - codeBegp);
    }
}


/*****************************************************************************
 *
 *  Finally link up the bbJumpDest of the blocks together
 */

void            Compiler::fgLinkBasicBlocks()
{
    /* Create the basic block lookup tables */

    fgInitBBLookup();

    /* First block is always reachable */

    fgFirstBB->bbRefs = 1;

    /* Walk all the basic blocks, filling in the target addresses */

    for (BasicBlock * curBBdesc = fgFirstBB;
         curBBdesc;
         curBBdesc = curBBdesc->bbNext)
    {
        switch (curBBdesc->bbJumpKind)
        {
        case BBJ_COND:
        case BBJ_ALWAYS:
        case BBJ_LEAVE:
            curBBdesc->bbJumpDest = fgLookupBB(curBBdesc->bbJumpOffs);
            curBBdesc->bbJumpDest->bbRefs++;

            /* Is the next block reachable? */

            if  (curBBdesc->bbJumpKind == BBJ_ALWAYS ||
                 curBBdesc->bbJumpKind == BBJ_LEAVE)
                break;

            if  (!curBBdesc->bbNext)
                BADCODE("Fall thru the end of a method");

            // Fall through, the next block is also reachable

        case BBJ_NONE:
            curBBdesc->bbNext->bbRefs++;
            break;

        case BBJ_RET:
        case BBJ_THROW:
        case BBJ_RETURN:
            break;

        case BBJ_SWITCH:

            unsigned        jumpCnt = curBBdesc->bbJumpSwt->bbsCount;
            BasicBlock * *  jumpPtr = curBBdesc->bbJumpSwt->bbsDstTab;

            do
            {
                *jumpPtr = fgLookupBB(*(unsigned*)jumpPtr);
                (*jumpPtr)->bbRefs++;
            }
            while (++jumpPtr, --jumpCnt);

            /* Default case of CEE_SWITCH (next block), is at end of jumpTab[] */

            assert(*(jumpPtr-1) == curBBdesc->bbNext);
            break;
        }
    }
}


/*****************************************************************************
 *
 *  Walk the instrs to create the basic blocks.
 */

void                Compiler::fgMakeBasicBlocks(const BYTE * codeAddr,
                                                size_t       codeSize,
                                                BYTE *       jumpTarget)
{
    const   BYTE *  codeBegp = codeAddr;
    const   BYTE *  codeEndp = codeAddr + codeSize;
    bool            tailCall = false;
    unsigned        curBBoffs;
    BasicBlock  *   curBBdesc;

    /* Clear the beginning offset for the first BB */

    curBBoffs = 0;

#ifdef DEBUGGING_SUPPORT
    if (opts.compDbgCode && info.compLocalVarsCount>0)
    {
        compResetScopeLists();

        // Ignore scopes beginning at offset 0
        while (compGetNextEnterScope(0))
            ;
        while(compGetNextExitScope(0))
            ;
    }
#endif


    BBjumpKinds jmpKind;

    do
    {
        OPCODE          opcode;
        unsigned        sz;
        unsigned        jmpAddr;
        unsigned        bbFlags = 0;
        BBswtDesc   *   swtDsc = 0;
        unsigned        nxtBBoffs;

        opcode    = (OPCODE) getU1LittleEndian(codeAddr);
        codeAddr += sizeof(__int8);
        jmpKind    = BBJ_NONE;

DECODE_OPCODE:

        /* Get the size of additional parameters */

        assert(opcode < CEE_COUNT);

        sz = opcodeSizes[opcode];

        switch (opcode)
        {
            signed        jmpDist;


        case CEE_PREFIX1:
            if (jumpTarget[codeAddr - codeBegp] != JT_NONE)
                BADCODE3("jump target between prefix 0xFE and opcode",
                         " at offset %04X", codeAddr - codeBegp);

            opcode = (OPCODE) (256+getU1LittleEndian(codeAddr));
            codeAddr += sizeof(__int8);
            goto DECODE_OPCODE;

        /* Check to see if we have a jump/return opcode */

        case CEE_BRFALSE:
        case CEE_BRFALSE_S:
        case CEE_BRTRUE:
        case CEE_BRTRUE_S:

        case CEE_BEQ:
        case CEE_BEQ_S:
        case CEE_BGE:
        case CEE_BGE_S:
        case CEE_BGE_UN:
        case CEE_BGE_UN_S:
        case CEE_BGT:
        case CEE_BGT_S:
        case CEE_BGT_UN:
        case CEE_BGT_UN_S:
        case CEE_BLE:
        case CEE_BLE_S:
        case CEE_BLE_UN:
        case CEE_BLE_UN_S:
        case CEE_BLT:
        case CEE_BLT_S:
        case CEE_BLT_UN:
        case CEE_BLT_UN_S:
        case CEE_BNE_UN:
        case CEE_BNE_UN_S:

            jmpKind = BBJ_COND;
            goto JMP;


        case CEE_LEAVE:
        case CEE_LEAVE_S:

            // We need to check if we are jumping out of a finally-protected try.
            jmpKind = BBJ_LEAVE;
            goto JMP;


        case CEE_BR:
        case CEE_BR_S:
            jmpKind = BBJ_ALWAYS;
            goto JMP;

        JMP:

            /* Compute the target address of the jump */

            jmpDist = (sz==1) ? getI1LittleEndian(codeAddr)
                              : getI4LittleEndian(codeAddr);
            jmpAddr = (codeAddr - codeBegp) + sz + jmpDist;
            break;

        case CEE_SWITCH:
            {
                unsigned        jmpBase;
                unsigned        jmpCnt; // # of switch cases (excluding defualt)

                BasicBlock * *  jmpTab;
                BasicBlock * *  jmpPtr;

                /* Allocate the switch descriptor */

                swtDsc = (BBswtDesc *)compGetMem(sizeof(*swtDsc));

                /* Read the number of entries in the table */

                jmpCnt = getU4LittleEndian(codeAddr); codeAddr += 4;

                /* Compute  the base offset for the opcode */

                jmpBase = (codeAddr - codeBegp) + jmpCnt*sizeof(DWORD);;

                /* Allocate the jump table */

                jmpPtr =
                jmpTab = (BasicBlock **)compGetMemArray((jmpCnt+1), sizeof(*jmpTab));

                /* Fill in the jump table */

                for (unsigned count = jmpCnt; count; count--)
                {
                    /* Store the target of the jump as a pointer [ISSUE: is this safe?]*/

                    jmpDist   = getI4LittleEndian(codeAddr);
                    codeAddr += 4;

//                  printf("table switch: target = %04X\n", jmpBase + jmpDist);
                    *jmpPtr++ = (BasicBlock*)(jmpBase + jmpDist);
                }

                /* Append the default label to the target table */

                *jmpPtr++ = (BasicBlock*)jmpBase;

                /* Make sure we found the right number of labels */

                assert(jmpPtr == jmpTab + jmpCnt + 1);

                /* Compute the size of the switch opcode operands */

                sz = sizeof(DWORD) + jmpCnt*sizeof(DWORD);

                /* Fill in the remaining fields of the switch descriptor */

                swtDsc->bbsCount  = jmpCnt + 1;
                swtDsc->bbsDstTab = jmpTab;

                /* This is definitely a jump */

                jmpKind = BBJ_SWITCH;
            }
            goto GOT_ENDP;

        case CEE_ENDFILTER:
            bbFlags |= (BBF_ENDFILTER | BBF_DONT_REMOVE);
            jmpKind = BBJ_RET;
            break;
            
            // Fall through
        case CEE_ENDFINALLY:
            // it is possible to infer BBF_ENDFINALLY from BBJ_RET and ~BBF_ENDFILTER 
            bbFlags |= BBF_ENDFINALLY;
            jmpKind = BBJ_RET;
            break;

        case CEE_TAILCALL:
        case CEE_VOLATILE:
        case CEE_UNALIGNED:
            // fgFindJumpTargets should have ruled out this possibility 
            //   (i.e. a prefix opcodes as last intruction in a block)
            assert(codeAddr < codeEndp);

            if (jumpTarget[codeAddr - codeBegp] != JT_NONE)
                BADCODE3("jump target between prefix and an opcode",
                        " at offset %04X", codeAddr - codeBegp);
            break;

        case CEE_CALL:
        case CEE_CALLVIRT:
        case CEE_CALLI:
            if (!tailCall)
            break;

            if (codeAddr + sz >= codeEndp ||
                (OPCODE) getU1LittleEndian(codeAddr + sz) != CEE_RET)
            {
                BADCODE3("tail call not followed by ret",
                        " at offset %04X", codeAddr - codeBegp);
            }

            /* For tail call, we just call CORINFO_HELP_TAILCALL, and it jumps to the
               target. So we dont need an epilog - just like CORINFO_HELP_THROW.
               Make the block BBJ_RETURN, but we will bash it to BBJ_THROW
               if the tailness of the call is satisfied.
               NOTE : The next instruction is guaranteed to be a CEE_RETURN
               and it will create another BasicBlock. But there may be an
               jump directly to that CEE_RETURN. If we want to avoid creating
               an unnecessary block, we need to check if the CEE_RETURN is
               the target of a jump.
             */

            // fall-through

        case CEE_JMP:
            /* These are equivalent to a return from the current method
               But instead of directly returning to the caller we jump and
               execute something else in between */
        case CEE_RET:
            jmpKind = BBJ_RETURN;
            break;

        case CEE_THROW:
        case CEE_RETHROW:
            jmpKind  = BBJ_THROW;
            break;

#ifdef DEBUG
        // make certain we did not forget any flow of control instructions
        // by checking the 'ctrl' field in opcode.def. First filter out all
        // non-ctrl instructions
#       define BREAK(name)          case name: break;
#       define NEXT(name)           case name: break;
#       define CALL(name)
#       define THROW(name)
#       define RETURN(name)
#       define META(name)
#       define BRANCH(name)
#       define COND_BRANCH(name)
#       define PHI(name)

#       define OPDEF(name,string,pop,push,oprType,opcType,l,s1,s2,ctrl) ctrl(name)
#       include "opcode.def"
#       undef OPDEF

#       undef PHI
#       undef BREAK
#       undef CALL
#       undef NEXT
#       undef THROW
#       undef RETURN
#       undef META
#       undef BRANCH
#       undef COND_BRANCH

        // These ctrl-flow opcodes dont need any special handling
        case CEE_NEWOBJ:    // CTRL_CALL
            break;

        // what's left are forgotten instructions
        default:
            BADCODE("Unrecognized control Opcode");
            break;
#endif
        }

        /* Jump over the operand */

        codeAddr += sz;

GOT_ENDP:

        tailCall = (opcode == CEE_TAILCALL);

        /* Make sure a jump target isn't in the middle of our opcode */

        if  (sz)
        {
            unsigned offs = codeAddr - codeBegp - sz; // offset of the operand

            for (unsigned i=0; i<sz; i++, offs++)
            {
                if  (jumpTarget[offs] != JT_NONE)
                    BADCODE3("jump into the middle of an opcode",
                             " at offset %04X", codeAddr - codeBegp);
            }
        }

        /* Compute the offset of the next opcode */

        nxtBBoffs = codeAddr - codeBegp;

#ifdef  DEBUGGING_SUPPORT

        bool foundScope     = false;

        if (opts.compDbgCode && info.compLocalVarsCount>0)
        {
            while(compGetNextEnterScope(nxtBBoffs))  foundScope = true;
            while(compGetNextExitScope(nxtBBoffs))   foundScope = true;
        }
#endif

        /* Do we have a jump? */

        if  (jmpKind == BBJ_NONE)
        {
            /* No jump; make sure we don't fall off the end of the function */

            if  (codeAddr == codeEndp)
                BADCODE3("missing return opcode",
                         " at offset %04X", codeAddr - codeBegp);

            /* If a label follows this opcode, we'll have to make a new BB */

            bool makeBlock = (jumpTarget[nxtBBoffs] != JT_NONE);

#ifdef  DEBUGGING_SUPPORT
            if (!makeBlock && foundScope)
            {
                makeBlock = true;
#ifdef DEBUG
                if (verbose)
                    printf("Splitting at BBoffs = %04u\n", nxtBBoffs);
#endif
            }
#endif
            if (!makeBlock)
                continue;
        }

        /* We need to create a new basic block */

        curBBdesc = fgNewBasicBlock(jmpKind);

        curBBdesc->bbFlags   |= bbFlags;
        curBBdesc->bbRefs     = 0;

        curBBdesc->bbCodeOffs = curBBoffs;
        curBBdesc->bbCodeSize = nxtBBoffs - curBBoffs;

        switch(jmpKind)
        {
        case BBJ_SWITCH:
            curBBdesc->bbJumpSwt  = swtDsc;
            break;

        default:
            curBBdesc->bbJumpOffs = jmpAddr;
            break;
        }

//      printf("BB %08X at PC %u\n", curBBdesc, curBBoffs);

        /* Remember where the next BB will start */

        curBBoffs = nxtBBoffs;
    }
    while (codeAddr <  codeEndp);

    assert(codeAddr == codeEndp);

    /* Do we need to add a dummy block at the end of the method ?? */

    if (jumpTarget[codeSize] != JT_NONE)
    {
        // We must add a block to mark the end of a try or catch clause
        // at the end of the method and it can't be removed.
        // We use an empty BBJ_THROW block for this purpose.
        
        curBBdesc = fgNewBasicBlock(BBJ_THROW);
        
        curBBdesc->bbFlags   |= (BBF_DONT_REMOVE | BBF_IMPORTED);
        curBBdesc->bbRefs     = 0;
        curBBdesc->bbCodeOffs = codeSize;
    }

    /* Finally link up the bbJumpDest of the blocks together */

    fgLinkBasicBlocks();
}


/*****************************************************************************
 *
 *  Main entry point to discover the basic blocks for the current function.
 */

void                Compiler::fgFindBasicBlocks()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In fgFindBasicBlocks() for %s\n",
               info.compFullName);
#endif


    /* Allocate the 'jump target' vector
     *
     *  We need one extra byte as we mark
     *  jumpTarget[info.compCodeSize] with JT_ADDR
     *  when we need to add an dummy block to use
     *  to record the end of a try or handler region.
     */
    BYTE* jumpTarget = (BYTE *)compGetMemA(info.compCodeSize+1);
    memset(jumpTarget, JT_NONE, info.compCodeSize+1);
    assert(JT_NONE == 0);

    /* Walk the instrs to find all jump targets */

    fgFindJumpTargets(info.compCode, info.compCodeSize, jumpTarget);

    unsigned  XTnum;

    /* Are there any exception handlers? */

    if  (info.compXcptnsCount)
    {
        /* Check and mark all the exception handlers */

        for (XTnum = 0; XTnum < info.compXcptnsCount; XTnum++)
        {
            DWORD tmpOffset;
            CORINFO_EH_CLAUSE clause;
            eeGetEHinfo(XTnum, &clause);
            assert(clause.HandlerLength != -1); // @DEPRECATED

            //@TODO [CONSIDER] [04/16/01] []: simply ignore this entry and continue compilation

            if (clause.TryLength <= 0)
                BADCODE("try block length <=0");

            // printf("Mark 'try' block: [%02u..%02u]\n", hndBeg, hndEnd);

            /* Mark the 'try' block extent and the handler itself */

            if (clause.TryOffset > info.compCodeSize)
                BADCODE("try offset is > codesize");
            if  (jumpTarget[clause.TryOffset       ] == JT_NONE)
                 jumpTarget[clause.TryOffset       ] =  JT_ADDR;

            tmpOffset = clause.TryOffset + clause.TryLength;
            if (tmpOffset > info.compCodeSize)
                BADCODE("try end is > codesize");
            if  (jumpTarget[tmpOffset              ] == JT_NONE)
                 jumpTarget[tmpOffset              ] =  JT_ADDR;
 
            if (clause.HandlerOffset > info.compCodeSize)
                BADCODE("handler offset > codesize");
            if  (jumpTarget[clause.HandlerOffset   ] == JT_NONE)
                 jumpTarget[clause.HandlerOffset   ] =  JT_ADDR;

            tmpOffset = clause.HandlerOffset + clause.HandlerLength;
            if (tmpOffset > info.compCodeSize)
                BADCODE("handler end > codesize");
            if  (jumpTarget[tmpOffset              ] == JT_NONE)
                 jumpTarget[tmpOffset              ] =  JT_ADDR;

            if (clause.Flags & CORINFO_EH_CLAUSE_FILTER)
            {
                if (clause.FilterOffset > info.compCodeSize)
                    BADCODE("filter offset + > codesize");
                if (jumpTarget[clause.FilterOffset ] == JT_NONE)
                    jumpTarget[clause.FilterOffset ] =  JT_ADDR;
            }
         }
    }

    /* Now create the basic blocks */

    fgMakeBasicBlocks(info.compCode, info.compCodeSize, jumpTarget);

    /* Mark all blocks within 'try' blocks as such */

    if  (info.compXcptnsCount == 0)
        return;

    /* Allocate the exception handler table */

    compHndBBtab = (EHblkDsc *) compGetMemArray(info.compXcptnsCount,
                                                sizeof(*compHndBBtab));
#ifdef WELL_FORMED_IL_CHECK

    verInitEHTree(info.compXcptnsCount);
    EHNodeDsc* initRoot = ehnNext; // remember the original root since 
                                   // it may get modified during insertion

#endif

    // Annotate BBs with exception handling information required for generating correct eh code 
    // as well as checking for correct IL
    EHblkDsc * handlerTab = compHndBBtab;

    for (XTnum = 0; XTnum < info.compXcptnsCount; XTnum++, handlerTab++)
    {
        CORINFO_EH_CLAUSE clause;
        eeGetEHinfo(XTnum, &clause);
        assert(clause.HandlerLength != -1); // @DEPRECATED

        unsigned      tryBegOff = clause.TryOffset;
        unsigned      tryEndOff = tryBegOff + clause.TryLength;
        unsigned      hndBegOff = clause.HandlerOffset;
        unsigned      hndEndOff = hndBegOff + clause.HandlerLength;

        if  (tryEndOff > info.compCodeSize)
            BADCODE3("end of try block beyond end of method for try",
                     " at offset %04X",tryBegOff);
        if  (hndEndOff > info.compCodeSize)
            BADCODE3("end of hnd block beyond end of method for try",
                     " at offset %04X",tryBegOff);

        /* Convert the various addresses to basic blocks */

        BasicBlock *  tryBegBB  = fgLookupBB(tryBegOff);
        BasicBlock *  tryEndBB  = fgLookupBB(tryEndOff);
        BasicBlock *  hndBegBB  = fgLookupBB(hndBegOff);
        BasicBlock *  hndEndBB  = NULL;
        BasicBlock *  filtBB    = NULL;
        BasicBlock *  blk;

        tryBegBB->bbFlags |= BBF_HAS_LABEL;
        tryEndBB->bbFlags |= BBF_HAS_LABEL;
        hndBegBB->bbFlags |= BBF_HAS_LABEL | BBF_JMP_TARGET;

        if (hndEndOff < info.compCodeSize)
        {
            hndEndBB = fgLookupBB(hndEndOff);
            hndEndBB->bbFlags |= BBF_TRY_HND_END | BBF_HAS_LABEL | BBF_DONT_REMOVE;
        }

        if (clause.Flags & CORINFO_EH_CLAUSE_FILTER)
        {
            filtBB = handlerTab->ebdFilter = fgLookupBB(clause.FilterOffset);

            filtBB->bbCatchTyp  = BBCT_FILTER;
            filtBB->bbFlags    |= BBF_HAS_LABEL | BBF_JMP_TARGET;

            filtBB->bbSetRunRarely();    // filters are rarely executed

            hndBegBB->bbCatchTyp = BBCT_FILTER_HANDLER;

            hndBegBB->bbSetRunRarely();    // filters handlers are rarely executed

#ifdef WELL_FORMED_IL_CHECK

            // Mark all BBs that belong to the filter with the XTnum of the corresponding handler
            for (blk = filtBB; /**/; blk = blk->bbNext)
            {
                if (blk == NULL)
                    BADCODE3("Missing endfilter for filter",
                             " at offset %04X", filtBB->bbCodeOffs);

                // Still inside the filter
                blk->setHndIndex(XTnum);

                if (blk->bbFlags & BBF_ENDFILTER)
                    break;
            }

            if (!blk->bbNext || blk->bbNext != hndBegBB)
                BADCODE3("Filter does not immediately precede handler for filter",
                         " at offset %04X", filtBB->bbCodeOffs);
#endif
        }
        else
        {
            handlerTab->ebdTyp = clause.ClassToken;

            /* Set bbCatchTyp as appropriate */

            if (clause.Flags & CORINFO_EH_CLAUSE_FINALLY)
            {
                hndBegBB->bbCatchTyp   = BBCT_FINALLY;
            }
            else
            {
                if (clause.Flags & CORINFO_EH_CLAUSE_FAULT)
                {
                    hndBegBB->bbCatchTyp  = BBCT_FAULT;
                }
                else
                {
                    hndBegBB->bbCatchTyp  = clause.ClassToken;

                    // These values should be non-zero value that will
                    // not colide with real tokens for bbCatchTyp
                    if (clause.ClassToken == 0)
                        BADCODE("Exception catch type is Null");

                    assert(clause.ClassToken != BBCT_FAULT);
                    assert(clause.ClassToken != BBCT_FINALLY);
                    assert(clause.ClassToken != BBCT_FILTER);
                    assert(clause.ClassToken != BBCT_FILTER_HANDLER);
                }
            }
        }

        /* Mark all blocks in the finally/fault or catch clause */

        for (blk = hndBegBB;
             blk && (blk->bbCodeOffs < hndEndOff);
             blk = blk->bbNext)
        {
            if (!blk->hasHndIndex())
                blk->setHndIndex(XTnum);

            /* All blocks in a catch handler or filter are rarely run */
            if (hndBegBB->bbCatchTyp != BBCT_FINALLY)
                blk->bbSetRunRarely();
        }

        /* Append the info to the table of try block handlers */

        handlerTab->ebdFlags  = clause.Flags;
        handlerTab->ebdTryBeg = tryBegBB;
        handlerTab->ebdTryEnd = tryEndBB;
        handlerTab->ebdHndBeg = hndBegBB;
        handlerTab->ebdHndEnd = hndEndBB;

        /* Mark the initial block and last blocks in the 'try' region */

        tryBegBB->bbFlags   |= BBF_TRY_BEG     | BBF_HAS_LABEL;
        tryEndBB->bbFlags   |= BBF_TRY_HND_END | BBF_HAS_LABEL;

        /*  Prevent future optimizations of removing the first block   */
        /*  of a TRY block and the first block of an exception handler */

        tryBegBB->bbFlags   |= BBF_DONT_REMOVE;
        tryEndBB->bbFlags   |= BBF_DONT_REMOVE;
        hndBegBB->bbFlags   |= BBF_DONT_REMOVE;
        hndBegBB->bbRefs++;

        if (clause.Flags & CORINFO_EH_CLAUSE_FILTER)
        {
            filtBB->bbFlags |= BBF_DONT_REMOVE;
            filtBB->bbRefs++;
        }

        /* Mark all BB's within the covered range of the try*/

        unsigned tryEndOffs = tryEndOff;

        for (blk = tryBegBB;
             blk && (blk->bbCodeOffs < tryEndOffs);
             blk = blk->bbNext)
        {
            /* Mark this BB as belonging to a 'try' block */

            blk->bbFlags   |= BBF_HAS_HANDLER;

            if (!blk->hasTryIndex())
                blk->setTryIndex(XTnum);

#ifdef DEBUG
            /* Note: the BB can't span the 'try' block */

            if (!(blk->bbFlags & BBF_INTERNAL))
            {
                assert(tryBegOff  <= blk->bbCodeOffs);
                assert(tryEndOff >= blk->bbCodeOffs + blk->bbCodeSize ||
                       tryEndOff == tryBegOff );
            }
#endif
        }

        /*  Init ebdNesting of current clause, and bump up value for all
         *  enclosed clauses (which have to be before it in the table)
         *  Innermost try-finally blocks must preceed outermost 
         *  try-finally blocks
         */

        handlerTab->ebdNesting   = 0;
        handlerTab->ebdEnclosing = NO_ENCLOSING_INDEX;

        for (EHblkDsc * xtab = compHndBBtab; xtab < handlerTab; xtab++)
        {
            if (jitIsBetween(xtab->ebdHndBeg->bbCodeOffs, hndBegOff, hndEndOff))
            {
                xtab->ebdNesting++;
            }

            /* If we haven't recorded an enclosing index for xtab then see
             *  if this EH regions should be recorded.  We check if the
             *  first or last offsets in the xtab lies within our region
             */
            if (xtab->ebdEnclosing == NO_ENCLOSING_INDEX)
            {
                bool begBetween = jitIsBetween(xtab->ebdTryBeg->bbCodeOffs,
                                               tryBegOff, tryEndOff);
                bool endBetween = jitIsBetween(xtab->ebdTryEnd->bbCodeOffs - 1, 
                                               tryBegOff, tryEndOff);
                if (begBetween || endBetween)
                {
                    // Record the enclosing scope link
                    xtab->ebdEnclosing = XTnum;

                    assert(XTnum <  info.compXcptnsCount);
                    assert(XTnum == (unsigned)(handlerTab - compHndBBtab));
                }
            }
        }

#ifdef WELL_FORMED_IL_CHECK
        verInsertEhNode(&clause, handlerTab);
#endif

    }

#ifdef DEBUG
    if (verbose)
        fgDispHandlerTab();
#endif

#ifdef WELL_FORMED_IL_CHECK   
#ifndef DEBUG
    if (tiVerificationNeeded)
#endif
    {
        // always run these checks for a debug build
        verCheckNestingLevel(initRoot);
        fgCheckBasicBlockControlFlow();
    }
#endif 
}

/*****************************************************************************
 * The following code checks the following rules for the EH table:
 * 1. Overlapping of try blocks not allowed.
 * 2. Handler blocks cannot be shared between different try blocks.
 * 3. Try blocks with Finally or Fault blocks cannot have other handlers.
 * 4. If block A contains block B, A should also contain B's try/filter/handler.
 * 5. A block cannot contain it's related try/filter/handler.
 * 6. Nested block must appear before containing block
 *
 */

void                Compiler::verInitEHTree(unsigned numEHClauses)
{
    ehnNext = (EHNodeDsc*) compGetMemArray(numEHClauses, sizeof(EHNodeDsc)*3);
    ehnTree = NULL;
}


/* Inserts the try, handler and filter (optional) clause information in a tree structure
 * in order to catch incorrect eh formatting (e.g. illegal overlaps, incorrect order)
 */

void                Compiler::verInsertEhNode(CORINFO_EH_CLAUSE* clause, EHblkDsc* handlerTab)
{
    EHNodeDsc* tryNode = ehnNext++;
    EHNodeDsc* handlerNode = ehnNext++;
    EHNodeDsc* filterNode;              // optional 
    
    tryNode->ehnSetTryNodeType();
    tryNode->ehnStartOffset = clause->TryOffset;
    tryNode->ehnEndOffset   = clause->TryOffset+clause->TryLength - 1;
    tryNode->ehnHandlerNode = handlerNode;

    if (clause->Flags & CORINFO_EH_CLAUSE_FINALLY) 
        handlerNode->ehnSetFinallyNodeType();
    else if (clause->Flags & CORINFO_EH_CLAUSE_FAULT) 
        handlerNode->ehnSetFaultNodeType();
    else 
        handlerNode->ehnSetHandlerNodeType();
    
    handlerNode->ehnStartOffset = clause->HandlerOffset;
    handlerNode->ehnEndOffset = clause->HandlerOffset + clause->HandlerLength - 1;
    handlerNode->ehnTryNode = tryNode;

    if (clause->Flags & CORINFO_EH_CLAUSE_FILTER)
    {
        filterNode = ehnNext++;
        filterNode->ehnStartOffset = clause->FilterOffset;
        // compute end offset of filter by walking the BB chain
        for (BasicBlock * blk = handlerTab->ebdFilter; blk ; blk = blk->bbNext)
        {
            assert(blk);

            if (blk->bbFlags & BBF_ENDFILTER)
            {
                filterNode->ehnEndOffset = blk->bbCodeOffs + blk->bbCodeSize - 1;
                break;
            }
        }

        assert(filterNode->ehnEndOffset != 0);
        filterNode->ehnSetFilterNodeType();
        filterNode->ehnTryNode = tryNode;
        tryNode->ehnFilterNode = filterNode;
    }

    verInsertEhNodeInTree(&ehnTree, tryNode);
    verInsertEhNodeInTree(&ehnTree, handlerNode);
    if (clause->Flags & CORINFO_EH_CLAUSE_FILTER)
        verInsertEhNodeInTree(&ehnTree,filterNode);
}

/*
    The root node could be changed by this method.

    node is inserted to 

        (a) right       of root (root.right       <-- node)
        (b) left        of root (node.right       <-- root; node becomes root)
        (c) child       of root (root.child       <-- node)
        (d) parent      of root (node.child       <-- root; node becomes root)
        (e) equivalent  of root (root.equivalent  <-- node)

    such that siblings are ordered from left to right
    child parent relationship and equivalence relationship are not violated
    

    Here is a list of all possible cases

    Case 1 2 3 4 5 6 7 8 9 10 11 12 13

         | | | | |
         | | | | |
    .......|.|.|.|..................... [ root start ] .....
    |        | | | |             |  |
    |        | | | |             |  |
   r|        | | | |          |  |  |
   o|          | | |          |     |
   o|          | | |          |     |
   t|          | | |          |     |
    |          | | | |     |  |     |
    |          | | | |     |        |
    |..........|.|.|.|.....|........|.. [ root end ] ........
                 | | | |
                 | | | | |
                 | | | | |

        |<-- - - - n o d e - - - -->|


   Case Operation
   --------------
    1    (b)
    2    Error
    3    Error
    4    (d)
    5    (d)
    6    (d)
    7    Error
    8    Error
    9    (a)
    10   (c)
    11   (c)
    12   (c)
    13   (e)


*/

void                Compiler::verInsertEhNodeInTree(EHNodeDsc** ppRoot, 
                                                    EHNodeDsc* node)
{
    unsigned nStart = node->ehnStartOffset;
    unsigned nEnd   = node->ehnEndOffset;

    if (nStart > nEnd)
    {
        BADCODE("start offset greater or equal to end offset");
    }
    node->ehnNext = NULL;
    node->ehnChild = NULL;
    node->ehnEquivalent = NULL;

    while (TRUE)
    {
        if (*ppRoot == NULL)
        {
            *ppRoot = node;
            break;
        }
        unsigned rStart = (*ppRoot)->ehnStartOffset;
        unsigned rEnd   = (*ppRoot)->ehnEndOffset;

        if (nStart < rStart)
        {
            // Case 1
            if (nEnd < rStart)
            {
                // Left sibling
                node->ehnNext     = *ppRoot;
                *ppRoot         = node;
                return;
            }
            // Case 2, 3
            if (nEnd < rEnd)
            {
//[Error]
                BADCODE("Overlapping try regions");
            }
    
            // Case 4, 5
//[Parent]
            verInsertEhNodeParent(ppRoot, node);
            return;
        }
        

        // Cases 6 - 13 (nStart >= rStart)
    
        if (nEnd > rEnd)
        {   // Case 6, 7, 8, 9

            // Case 9
            if (nStart > rEnd)
            {
//[RightSibling]

                // Recurse with Root.Sibling as the new root
                ppRoot = &((*ppRoot)->ehnNext);
                continue;
            }

            // Case 6
            if (nStart == rStart)
            {
//[Parent]
                if (node->ehnIsTryBlock() || (*ppRoot)->ehnIsTryBlock())
                {
                    verInsertEhNodeParent(ppRoot, node);
                    return;
                }

                // non try blocks are not allowed to start at the same offset
                BADCODE("Handlers start at the same offset");
            }

            // Case 7, 8
            BADCODE("Overlapping try regions");
        }

        // Case 10-13 (nStart >= rStart && nEnd <= rEnd)
        if ((nStart != rStart) || (nEnd != rEnd))
        {   // Cases 10,11,12
//[Child]

            if ((*ppRoot)->ehnIsTryBlock())
            {
                BADCODE("Inner try appears after outer try in exception handling table");
            }
            else
            {
                // Case 12 (nStart == rStart)
                // non try blocks are not allowed to start at the same offset
                if ((nStart == rStart) && !node->ehnIsTryBlock()) 
                {
                    BADCODE("Handlers start at the same offset");
                }

                // check this!
                ppRoot = &((*ppRoot)->ehnChild);
                continue;
            }
        }

        // Case 13
//[Equivalent]
        if (!node->ehnIsTryBlock() &&
            !(*ppRoot)->ehnIsTryBlock())
        {
            BADCODE("Handlers cannot be shared");
        }

        node->ehnEquivalent = node->ehnNext = *ppRoot;

        // check that the corresponding handler is either a catch handler
        // or a filter
        if (node->ehnHandlerNode->ehnIsFaultBlock()           ||
            node->ehnHandlerNode->ehnIsFinallyBlock()         ||
            (*ppRoot)->ehnHandlerNode->ehnIsFaultBlock()      ||
            (*ppRoot)->ehnHandlerNode->ehnIsFinallyBlock() )
        {
            BADCODE("Try block with multiple non-filter/non-handler blocks");
        }


        break;
    }    
}

/**********************************************************************
 * Make node the parent of *ppRoot. All siblings of *ppRoot that are
 * fully or partially nested in node remain siblings of *ppRoot
 */

void            Compiler::verInsertEhNodeParent(EHNodeDsc** ppRoot, 
                                                EHNodeDsc*  node)
{
    assert(node->ehnNext == NULL);
    assert(node->ehnChild == NULL);

    // Root is nested in Node
    assert(node->ehnStartOffset <= (*ppRoot)->ehnStartOffset);
    assert(node->ehnEndOffset   >= (*ppRoot)->ehnEndOffset);

    // Root is not the same as Node
    assert(node->ehnStartOffset != (*ppRoot)->ehnStartOffset || 
           node->ehnEndOffset != (*ppRoot)->ehnEndOffset);

    if (node->ehnIsFilterBlock())
    {
        BADCODE("Protected block appearing within filter block");
    }

    EHNodeDsc *lastChild = NULL;
    EHNodeDsc *sibling   = (*ppRoot)->ehnNext;

    while (sibling)
    {
        // siblings are ordered left to right, largest right.
        // nodes have a width of atleast one.
        // Hence sibling start will always be after Node start.

        assert(sibling->ehnStartOffset > node->ehnStartOffset);   // (1)

        // disjoint
        if (sibling->ehnStartOffset > node->ehnEndOffset)
            break;

        // partial containment.
        if (sibling->ehnEndOffset > node->ehnEndOffset)   // (2)
        {
            BADCODE("Overlapping try regions");
        }
        //else full containment (follows from (1) and (2)) 

        lastChild = sibling;
        sibling = sibling->ehnNext;
    }

    // All siblings of Root upto and including lastChild will continue to be 
    // siblings of Root (and children of Node). The node to the right of 
    // lastChild will become the first sibling of Node.
    // 

    if (lastChild)
    {
        // Node has more than one child including Root

        node->ehnNext      = lastChild->ehnNext;
        lastChild->ehnNext = NULL;
    }
    else
    {
        // Root is the only child of Node
        node->ehnNext      = (*ppRoot)->ehnNext;
        (*ppRoot)->ehnNext  = NULL;
    }

    node->ehnChild = *ppRoot;
    *ppRoot     = node;

}

/*****************************************************************************
 * Checks the following two conditions:
 * 1) If block A contains block B, A should also contain B's try/filter/handler.
 * 2) A block cannot contain it's related try/filter/handler.
 * Both these conditions are checked by making sure that all the blocks for an
 * exception clause is at the same level.
 * The algorithm is: for each exception clause, determine the first block and
 * search through the next links for its corresponding try/handler/filter as the
 * case may be. If not found, then fail.
 */

void            Compiler::verCheckNestingLevel(EHNodeDsc* root)
{
    EHNodeDsc* ehnNode = root;

    #define exchange(a,b) { temp = a; a = b; b = temp;}

    for (unsigned XTnum = 0; XTnum < info.compXcptnsCount; XTnum++)
    {
        EHNodeDsc *p1, *p2, *p3, *temp, *search;

        p1 = ehnNode++;
        p2 = ehnNode++;

        // we are relying on the fact that ehn nodes are allocated sequentially.
        assert(p1->ehnHandlerNode == p2);
        assert(p2->ehnTryNode == p1);

        // arrange p1 and p2 in sequential order
        if (p1->ehnStartOffset == p2->ehnStartOffset)
            BADCODE("shared exception handler");

        if (p1->ehnStartOffset > p2->ehnStartOffset)
            exchange(p1,p2);

        temp = p1->ehnNext;
        unsigned numSiblings = 0;

        search = p2;
        if (search->ehnEquivalent)
            search = search->ehnEquivalent;

        do {
            if (temp == search)
            {
                numSiblings++;
                break;
            }
            if (temp)
                temp = temp->ehnNext;
        } while (temp);

        CORINFO_EH_CLAUSE clause;
        eeGetEHinfo(XTnum, &clause);

        if (clause.Flags & CORINFO_EH_CLAUSE_FILTER)
        {
            p3 = ehnNode++;

            assert(p3->ehnTryNode == p1 || p3->ehnTryNode == p2);
            assert(p1->ehnFilterNode == p3 || p2->ehnFilterNode == p3);

            if (p3->ehnStartOffset < p1->ehnStartOffset)
            { 
                temp = p3; search = p1;
            }
            else if (p3->ehnStartOffset < p2->ehnStartOffset)
            {
                temp = p1; search = p3;
            }
            else
            {
                temp = p2; search = p3;
            }
            if (search->ehnEquivalent)
                search = search->ehnEquivalent;
            do {
                if (temp == search)
                {
                    numSiblings++;
                    break;
                }
                temp = temp->ehnNext;
            } while (temp);
        }
        else 
        {
            numSiblings++;
        }

        if (numSiblings != 2)
            BADCODE("Outer block does not contain all code in inner handler");
    }

}

/*****************************************************************************
 * Check control flow constraints for well formed IL. Bail if any of the constraints
 * are violated.
 */

void            Compiler::fgCheckBasicBlockControlFlow()
{
    EHblkDsc *HBtab;

    for (BasicBlock* blk = fgFirstBB; blk; blk = blk->bbNext)
    {
        if (blk->bbFlags & BBF_INTERNAL) 
            continue;

        switch (blk->bbJumpKind)
        {
        case BBJ_NONE:       // block flows into the next one (no jump)
            
            fgControlFlowPermitted(blk,blk->bbNext);
            
            break;

        case BBJ_ALWAYS:    // block does unconditional jump to target
            
            fgControlFlowPermitted(blk,blk->bbJumpDest);
            
            break;

        case BBJ_COND:      // block conditionally jumps to the target

            fgControlFlowPermitted(blk,blk->bbNext);

            fgControlFlowPermitted(blk,blk->bbJumpDest);
            
            break;

        case BBJ_RETURN:    // block ends with 'ret'

            if (blk->hasTryIndex() || blk->hasHndIndex())
            {
                BADCODE3("Return from a protected block",
                         ". Before offset %04X", blk->bbCodeOffs + blk->bbCodeSize);
            }
            break;

        case BBJ_RET:       // block ends with endfinally/endfilter

            if (!blk->hasHndIndex())  // must be part of a handler
            {
                BADCODE3("Missing handler",
                         ". Before offset %04X", blk->bbCodeOffs + blk->bbCodeSize);
            }

            HBtab = compHndBBtab + blk->getHndIndex();

            // Endfilter allowed only in a filter block
            if (blk->bbFlags & BBF_ENDFILTER)
            {    
                if ((HBtab->ebdFlags & CORINFO_EH_CLAUSE_FILTER) == 0)
                {
                    BADCODE("Unexpected endfilter");
                }
            }
            // endfinally allowed only in a finally/fault block
            else if ((HBtab->ebdFlags & (CORINFO_EH_CLAUSE_FINALLY | CORINFO_EH_CLAUSE_FAULT)) == 0)
            {
                BADCODE("Unexpected endfinally");
            }
            
            // The handler block should be the innermost block
            // Exception blocks are listed, innermost first.
            if (blk->hasTryIndex() && (blk->getTryIndex() < blk->getHndIndex()))
            {
                BADCODE("endfinally / endfilter in nested try block");
            }

            break;

        case BBJ_THROW:     // block ends with 'throw'
            /* throw is permitted from every BB, so nothing to check */
            /* importer makes sure that rethrow is done from a catch */
            break;

        case BBJ_LEAVE:      // block always jumps to the target, maybe out of guarded
                             // region. Used temporarily until importing
            fgControlFlowPermitted(blk, blk->bbJumpDest,TRUE);

            break;


        case BBJ_SWITCH:     // block ends with a switch statement*/

            BBswtDesc* swtDesc;
            swtDesc = blk->bbJumpSwt;

            assert (swtDesc);

            unsigned i;
            for (i=0; i<swtDesc->bbsCount; i++)
            {
                fgControlFlowPermitted(blk,swtDesc->bbsDstTab[i]);
            }

            break;

        case BBJ_CALL:       // block always calls the target finallys
        default:
            assert(!"Unexpected BB type");           // BBJ_CALL don't get created until importing
            BADCODE("Internal compiler error");      // can't issue any sensible error message since this
                                                     // reflects a jit bug
            break;
        }
    }
}

/****************************************************************************/

Compiler::EHblkDsc *  Compiler::fgInitHndRange(BasicBlock *  blk,
                                               unsigned   *  hndBeg,
                                               unsigned   *  hndEnd,
                                               bool       *  inFilter)
{
    EHblkDsc * hndTab;

    if (blk->hasHndIndex())
    {
        hndTab  = compHndBBtab + blk->getHndIndex();
        if (bbInFilterBlock(blk))
        {
            *hndBeg   = hndTab->ebdFilter->bbCodeOffs;
            *hndEnd   = hndTab->ebdHndBeg->bbCodeOffs; // filter end is handler begin
            *inFilter = true;
        }
        else
        {
            *hndBeg   = hndTab->ebdHndBeg->bbCodeOffs;
            *hndEnd   = ebdHndEndOffs(hndTab);
            *inFilter = false;
        }
    }
    else
    {
        hndTab    = NULL;
        *hndBeg   = 0;
        *hndEnd   = info.compCodeSize;
        *inFilter = false;
    }
    return hndTab;
}

/****************************************************************************/

Compiler::EHblkDsc *  Compiler::fgInitTryRange(BasicBlock *  blk, 
                                               unsigned   *  tryBeg,
                                               unsigned   *  tryEnd)
{
    EHblkDsc * tryTab;

    if (blk->hasTryIndex())
    {
        tryTab = compHndBBtab + blk->getTryIndex();
        *tryBeg = tryTab->ebdTryBeg->bbCodeOffs;
        *tryEnd = ebdTryEndOffs(tryTab);
    }
    else
    {
        tryTab  = NULL;
        *tryBeg = 0;
        *tryEnd = info.compCodeSize;
    }
    return tryTab;
}

/****************************************************************************
 * Check that the leave from the block is legal. 
 * Consider removing this check here if we  can do it cheaply during importing 
 */

void           Compiler::fgControlFlowPermitted(BasicBlock*  blkSrc, 
                                                BasicBlock*  blkDest,
                                                BOOL         isLeave)
{
    unsigned    srcHndBeg,   destHndBeg;
    unsigned    srcHndEnd,   destHndEnd;
    bool        srcInFilter, destInFilter;
    bool        srcInCatch = false;

    EHblkDsc*   srcHndTab;

    srcHndTab = fgInitHndRange(blkSrc,  &srcHndBeg,  &srcHndEnd,  &srcInFilter);
                fgInitHndRange(blkDest, &destHndBeg, &destHndEnd, &destInFilter);
            
    /* Impose the rules for leaving or jumping from handler blocks */

    if (blkSrc->hasHndIndex())
    {
        srcInCatch = (srcHndTab->ebdFlags == CORINFO_EH_CLAUSE_NONE);

        /* Are we jumping within the same handler index? */
        if (blkSrc->bbHndIndex == blkDest->bbHndIndex)
        {
             /* Do we have a filter clause? */
            if (srcHndTab->ebdFlags & CORINFO_EH_CLAUSE_FILTER)
            {
                /* Update srcInCatch if we have a filter/catch */
                srcInCatch = !srcInFilter;

                /* filters and catch handlers share same eh index  */
                /* we need to check for control flow between them. */
                if (srcInFilter != destInFilter)
                {
                    if (!jitIsBetween(blkDest->bbCodeOffs, srcHndBeg, srcHndEnd))
                        BADCODE3("Illegal control flow between filter and handler",
                                 ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
                }
            }
        }
        else
        {
            /* The handler indexes of blkSrc and blkDest are different */
            if (isLeave)
            {
                /* Any leave instructions must not exit the src handler */
                if (!jitIsBetween(srcHndBeg, destHndBeg, destHndEnd))
                    BADCODE3("Illegal use of leave to exit handler",
                             ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
            }
            else 
            {
                /* We must use a leave to exit a handler */
                BADCODE3("Illegal control flow out of a handler", 
                         ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
            }
            
            /* Do we have a filter clause? */
            if (srcHndTab->ebdFlags & CORINFO_EH_CLAUSE_FILTER)
            {
                /* Update srcInCatch if we have a filter/catch */
                srcInCatch = !srcInFilter;

                /* It is ok to leave from the handler block of a filter, */
                /* but not from the filter block of a filter             */
                if (srcInFilter != destInFilter)
                {
                    BADCODE3("Illegal to leave a filter handler", 
                             ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
                }
            }

            /* We should never leave a finally handler */
            if (srcHndTab->ebdFlags & CORINFO_EH_CLAUSE_FINALLY)
            {
                BADCODE3("Illegal to leave a finally handler",
                         ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
            }

            /* We should never leave a fault handler */
            if (srcHndTab->ebdFlags & CORINFO_EH_CLAUSE_FAULT)
            {
                BADCODE3("Illegal to leave a fault handler",
                         ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
            }
        }
    }
    else if (blkDest->hasHndIndex())
    {
        /* blkSrc was not inside a handler, but blkDst is inside a handler */
        BADCODE3("Illegal control flow into a handler", 
                 ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
    }

    /* branching into the start of a catch / filter handler is illegal */
    if (fgIsStartofCatchOrFilterHandler(blkDest))
    {
        BADCODE3("Illegal control flow to the beginning of a catch / filter handler", 
                 ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
    }

    /* Are we jumping from a catch handler into the corresponding try? */
    /* VB uses this for "on error goto "                               */

    if (isLeave && srcInCatch &&
        jitIsBetween(blkDest->bbCodeOffs,
                     srcHndTab->ebdTryBeg->bbCodeOffs,
                     srcHndTab->ebdTryEnd->bbCodeOffs))
    {
        // Allowed if it is the first instruction of an inner try  
        // (and all trys in between) 
        //
        // try {
        //  ..
        // _tryAgain:
        //  ..
        //      try {
        //      _tryNestedInner:
        //        ..
        //          try {
        //          _tryNestedIllegal:
        //            ..
        //          } catch {
        //            ..
        //          }
        //        ..
        //      } catch {
        //        ..
        //      }
        //  ..
        // } catch {
        //  ..
        //  leave _tryAgain         // Allowed
        //  ..
        //  leave _tryNestedInner   // Allowed
        //  ..
        //  leave _tryNestedIllegal // Not Allowed
        //  ..
        // }

        /* The common case where leave is to the corresponding try */
        if (ebdIsSameTry(blkSrc->getHndIndex(), blkDest->getTryIndex()))
            return;

        /* Also allowed is a leave to the start of a try which starts in the handler's try */
        if (fgFlowToFirstBlockOfInnerTry(srcHndTab->ebdTryBeg, blkDest, false))
            return;
    }

    /* Check all the try block rules */

    unsigned    srcTryBeg;
    unsigned    srcTryEnd;
    unsigned    destTryBeg;
    unsigned    destTryEnd;

    fgInitTryRange(blkSrc,  &srcTryBeg,  &srcTryEnd);
    fgInitTryRange(blkDest, &destTryBeg, &destTryEnd);

    /* Are we jumping between try indexes? */
    if (blkSrc->bbTryIndex != blkDest->bbTryIndex)
    {
        // Are we exiting from an inner to outer try?
        if (jitIsBetween(srcTryBeg,   destTryBeg, destTryEnd) &&
            jitIsBetween(srcTryEnd-1, destTryBeg, destTryEnd)   )
        {
            if (!isLeave)
            {
                BADCODE3("exit from try block without a leave", 
                         ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
            }
        }
        else if (jitIsBetween(destTryBeg, srcTryBeg, srcTryEnd))
        {
            // check that the dest Try is first instruction of an inner try
            if (!fgFlowToFirstBlockOfInnerTry(blkSrc, blkDest, false))
            {
                BADCODE3("control flow into middle of try", 
                         ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
            }
        }
        else // there is no nesting relationship between src and dest
        {
            if (isLeave)
            {
                // check that the dest Try is first instruction of an inner try sibling
                if (!fgFlowToFirstBlockOfInnerTry(blkSrc, blkDest, true))
                {
                    BADCODE3("illegal leave into middle of try", 
                             ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
                }
            }
            else
            {
                BADCODE3("illegal control flow in to/out of try block",
                         ". Before offset %04X", blkSrc->bbCodeOffs + blkSrc->bbCodeSize);
            }
        }
    }
}

/*****************************************************************************
/*  Check if blk is the first block of a catch or filter handler
 */
bool            Compiler::fgIsStartofCatchOrFilterHandler(BasicBlock*  blk)
{
    if (!blk->hasHndIndex())
        return false;

    EHblkDsc* HBtab = compHndBBtab + blk->getHndIndex();

    if ((HBtab->ebdFlags & 
        (CORINFO_EH_CLAUSE_FINALLY | CORINFO_EH_CLAUSE_FAULT)) != 0)
        return false;   // we are looking for Filter and catch handlers

    if (blk->bbCodeOffs == HBtab->ebdHndBeg->bbCodeOffs)
        return true;    // beginning of a catch handler

    if ((HBtab->ebdFlags & CORINFO_EH_CLAUSE_FILTER) &&
            (blk->bbCodeOffs == HBtab->ebdFilter->bbCodeOffs))
        return true;    // beginning of a filter block

    return false;
}

/*****************************************************************************
/*  Check that blkDest is the first block of an inner try or a sibling
 *    with no intervening trys in between
 */

bool            Compiler:: fgFlowToFirstBlockOfInnerTry(BasicBlock*  blkSrc, 
                                                        BasicBlock*  blkDest,
                                                        bool         sibling)
{
    assert(blkDest->hasTryIndex());

    unsigned        XTnum     = blkDest->getTryIndex();
    unsigned        lastXTnum = blkSrc->hasTryIndex() ? blkSrc->getTryIndex()
                                                      : info.compXcptnsCount;
    assert(XTnum     <  info.compXcptnsCount);
    assert(lastXTnum <= info.compXcptnsCount);

    EHblkDsc*       HBtab     = compHndBBtab + XTnum;

    // check that we are not jumping into middle of try
    if (HBtab->ebdTryBeg != blkDest)
    {
        return false;
    }

    if (sibling)
    {
        assert(blkSrc->bbTryIndex != blkDest->bbTryIndex);

        // find the l.u.b of the two try ranges
        // Set lastXTnum to the l.u.b.

        HBtab = compHndBBtab + lastXTnum;

        for (lastXTnum++, HBtab++;
             lastXTnum < info.compXcptnsCount;
             lastXTnum++, HBtab++)
        {
            if (jitIsBetween(blkDest->bbNum,
                             HBtab->ebdTryBeg->bbNum,
                             ebdTryEndBlkNum(HBtab)))
                
                break;
        }
    }

    // now check there are no intervening trys between dest and l.u.b 
    // (it is ok to have intervening trys as long as they all start at 
    //  the same code offset)

    HBtab = compHndBBtab + XTnum;

    for (XTnum++,  HBtab++;
         XTnum < lastXTnum;
         XTnum++,  HBtab++)
    {
        if (jitIsProperlyBetween(blkDest->bbNum,
                                 HBtab->ebdTryBeg->bbNum,
                                 ebdTryEndBlkNum(HBtab)))
        {
            return false;
        }
    }

    return true;
}

/*****************************************************************************
 *  Returns the handler nesting levels of the block.
 *  *pFinallyNesting is set to the nesting level of the inner-most
 *  finally-protected try the block is in.
 *  Assumes the all blocks are sorted by bbNum.
 */

unsigned            Compiler::fgHndNstFromBBnum(unsigned    blkNum,
                                                unsigned  * pFinallyNesting)
{
    unsigned        curNesting = 0; // How many handlers is the block in
    unsigned        tryFin = -1;    // curNesting when we see innermost finally-protected try
    unsigned        XTnum;
    EHblkDsc *      HBtab;

    /* We find the blocks's handler nesting level by walking over the
       complete exception table and find enclosing clauses.
       @TODO [CONSIDER] [04/16/01] []: Store the nesting level with the block */

    for (XTnum = 0, HBtab = compHndBBtab;
         XTnum < info.compXcptnsCount;
         XTnum++,   HBtab++)
    {
        assert(HBtab->ebdTryBeg && HBtab->ebdHndBeg);

        if ((HBtab->ebdFlags & CORINFO_EH_CLAUSE_FINALLY) &&
            jitIsBetween(blkNum,
                         HBtab->ebdTryBeg->bbNum,
                         ebdTryEndBlkNum(HBtab)) &&
            tryFin == -1)
        {
            tryFin = curNesting;
        }
        else
        if (jitIsBetween(blkNum,
                         HBtab->ebdHndBeg->bbNum,
                         ebdHndEndBlkNum(HBtab)))
        {
            curNesting++;
        }
    }

    if  (tryFin == -1)
        tryFin = curNesting;

    if  (pFinallyNesting)
        *pFinallyNesting = curNesting - tryFin;

    return curNesting;
}

/*****************************************************************************
 *
 *  Import the basic blocks of the procedure.
 */

void                    Compiler::fgImport()
{
    fgHasPostfix = false;

#if HOIST_THIS_FLDS
    if (opts.compMinOptim || opts.compDbgCode)
        optThisFldDont = true;
    else
        optHoistTFRinit();
#endif

    if (fgFirstBB->bbFlags & BBF_INTERNAL)
        impImport(fgFirstBB->bbNext);
    else
        impImport(fgFirstBB);

#ifdef DEBUG
    if (verbose && info.compCallUnmanaged)
        printf(">>>>>>%s has unmanaged callee\n", info.compFullName);
#endif
}

/*****************************************************************************
 *
 *  Convert the given node into a call to the specified helper passing
 *  the given argument list.
 */

GenTreePtr          Compiler::fgMorphIntoHelperCall(GenTreePtr tree, 
                                                    int        helper,
                                                    GenTreePtr args)
{
    tree->ChangeOper(GT_CALL);

    tree->gtFlags              |= GTF_CALL;
    tree->gtCall.gtCallType     = CT_HELPER;
    tree->gtCall.gtCallMethHnd  = eeFindHelper(helper);
    tree->gtCall.gtCallArgs     = args;
    tree->gtCall.gtCallObjp     = NULL;
    tree->gtCall.gtCallMoreFlags= 0;
    tree->gtCall.gtCallCookie   = NULL;

    /* NOTE: we assume all helper arguments are enregistered on RISC */

#if TGT_RISC
    genNonLeaf = true;
#endif

    /* Perform the morphing right here if we're using fastcall */

    tree->gtCall.gtCallRegArgs = 0;
    tree = fgMorphArgs(tree);

//
//  ISSUE: Is the following needed? After all, we already have a call ...
//
//  if  (args)
//      tree->gtFlags      |= (args->gtFlags & GTF_GLOB_EFFECT);

    return tree;
}

/*****************************************************************************
 * This node should not be referenced by anyone now. Set its values to garbage
 * to catch extra references
 */

inline
void                DEBUG_DESTROY_NODE(GenTreePtr tree)
{
#ifdef DEBUG
    // Store gtOper into gtRegNum to find out what this node was if needed
    tree->gtRegNum      = (regNumber) tree->gtOper;

    tree->SetOper        (GT_COUNT);
    tree->gtType        = TYP_UNDEF;
    tree->gtOp.gtOp1    =
    tree->gtOp.gtOp2    = NULL;
    tree->gtFlags      |= 0xFFFFFFFF & ~GTF_NODE_MASK;
#endif
}


/*****************************************************************************
 * This function returns true if tree is a node with a call
 * that unconditionally throws an exception
 */

inline 
bool         Compiler::fgIsThrow(GenTreePtr     tree)
{
    if ((tree->gtOper               != GT_CALL  ) ||
        (tree->gtCall.gtCallType    != CT_HELPER)   )
    {
        return false;
    }
    
    if ((tree->gtCall.gtCallMethHnd == eeFindHelper(CORINFO_HELP_OVERFLOW)    ) ||
        (tree->gtCall.gtCallMethHnd == eeFindHelper(CORINFO_HELP_VERIFICATION)) ||
        (tree->gtCall.gtCallMethHnd == eeFindHelper(CORINFO_HELP_RNGCHKFAIL)  ) ||
        (tree->gtCall.gtCallMethHnd == eeFindHelper(CORINFO_HELP_THROW)       ) ||
        (tree->gtCall.gtCallMethHnd == eeFindHelper(CORINFO_HELP_RETHROW)     )   )
    {
        assert(tree->gtFlags & GTF_CALL);
        assert(tree->gtFlags & GTF_EXCEPT);
        return true;
    }

    return false;
}

/*****************************************************************************
 * This function returns true if tree is a GT_COMMA node with a call
 * that unconditionally throws an exception
 */

inline
bool                Compiler::fgIsCommaThrow(GenTreePtr tree,
                                             bool       forFolding /* = false */)
{
    // Instead of always folding comma throws, 
    // with stress enabled we only fold half the time

    if (forFolding && compStressCompile(STRESS_FOLD, 50))
    {
        return false;         /* Don't fold */
    }

    /* Check for cast of a GT_COMMA with a throw overflow */
    if ((tree->gtOper == GT_COMMA)   &&
        (tree->gtFlags & GTF_CALL)   &&
        (tree->gtFlags & GTF_EXCEPT))
    {
        return (fgIsThrow(tree->gtOp.gtOp1));
    }
    return false;
}

/*****************************************************************************
 *
 *  Morph a cast node (we perform some very simple transformations here).
 */

GenTreePtr          Compiler::fgMorphCast(GenTreePtr tree)
{
    assert(tree->gtOper == GT_CAST);
    assert(genTypeSize(TYP_I_IMPL) == sizeof(void*));

    /* The first sub-operand is the thing being cast */

    GenTreePtr      oper    = tree->gtCast.gtCastOp;
    var_types       srcType = genActualType(oper->TypeGet());
    unsigned        srcSize;

    var_types       dstType = tree->gtCast.gtCastType;
    unsigned        dstSize = genTypeSize(dstType);

    int             CPX;

    // See if the cast has to be done in two steps.  R -> I
    if (varTypeIsFloating(srcType) && varTypeIsIntegral(dstType))
    {
        if (srcType == TYP_FLOAT)
            oper = gtNewCastNode(TYP_DOUBLE, oper, TYP_DOUBLE);

        // do we need to do it in two steps R -> I, '-> smallType
        if (dstSize < sizeof(void*))
        {
            oper = gtNewCastNodeL(TYP_I_IMPL, oper, TYP_I_IMPL);
            oper->gtFlags |= (tree->gtFlags & (GTF_OVERFLOW|GTF_EXCEPT));
        }
        else
        {
            /* Note that if we need to use a helper call then we can not morph oper */
            if (!tree->gtOverflow())
            {
                switch (dstType)
                {
                case TYP_INT:
#ifdef DEBUG
                    if (gtDblWasInt(oper)
                        && false) // @TODO: [ENABLE] [07/05/01] After forking off for V1 to improve STRESS_ENREG_FP
                    {
                        /* Inserting a call (to a helper-function will cause all
                           FP variable which are currently live to not be
                           enregistered. Since we know that gtDblWasInt()
                           varaiables will not overflow when cast to TYP_INT,
                           we just use a memory spill and load to do the cast
                           and avoid the call */
                        return tree;
                    }
                    else
#endif
                    if ((oper->gtOper == GT_MATH) &&
                        (oper->gtMath.gtMathFN == CORINFO_INTRINSIC_Round))
                    {
                        /* optimization: conv.i4(round.d(d)) -> round.i(d) */
                        oper->gtType = dstType;
                        return fgMorphTree(oper);
                    }
                    else
                    {
                                CPX = CORINFO_HELP_DBL2INT;    goto CALL; 
                    }
                case TYP_UINT:  CPX = CORINFO_HELP_DBL2UINT;   goto CALL;
                case TYP_LONG:  CPX = CORINFO_HELP_DBL2LNG;    goto CALL;
                case TYP_ULONG: CPX = CORINFO_HELP_DBL2ULNG;   goto CALL;
                }
            }
            else
            {
                switch (dstType)
                {
                case TYP_INT:   CPX = CORINFO_HELP_DBL2INT_OVF;   goto CALL;
                case TYP_UINT:  CPX = CORINFO_HELP_DBL2UINT_OVF;  goto CALL;
                case TYP_LONG:  CPX = CORINFO_HELP_DBL2LNG_OVF;   goto CALL;
                case TYP_ULONG: CPX = CORINFO_HELP_DBL2ULNG_OVF;  goto CALL;
                }
            }
            assert(!"Unexpected dstType");
        }
    }
    // Do we have to do two step I8/U8 -> I -> Small Type?
    else if (varTypeIsLong(srcType) && varTypeIsSmall(dstType))
    {
        oper = gtNewCastNode(TYP_I_IMPL, oper, TYP_I_IMPL);
        oper->gtFlags |= (tree->gtFlags & (GTF_OVERFLOW|GTF_EXCEPT|GTF_UNSIGNED));
        tree->gtFlags &= ~GTF_UNSIGNED;
    }
    // Do we have to do two step U4/8 -> R4/8 ?
    else if ((tree->gtFlags & GTF_UNSIGNED) && varTypeIsFloating(dstType))
    {
        srcType = genUnsignedType(srcType);

        if (srcType == TYP_ULONG)
        {
            CPX = CORINFO_HELP_ULNG2DBL;
            goto CALL;
        }
        else if (srcType == TYP_UINT)
        {
            oper = gtNewCastNode(TYP_LONG, oper, TYP_LONG); 
            oper->gtFlags |= (tree->gtFlags & (GTF_OVERFLOW|GTF_EXCEPT|GTF_UNSIGNED));
            tree->gtFlags &= ~GTF_UNSIGNED;
        }
    }
    else if (varTypeIsGC(srcType) != varTypeIsGC(dstType)) 
    {
        // We are casting away GC information.  we would like to just
        // bash the type to int, however this gives the emitter fits because
        // it believes the variable is a GC variable at the begining of the
        // instruction group, but is not turned non-gc by the code generator
        // we fix this by copying the GC pointer to a non-gc pointer temp.
        if (varTypeIsFloating(srcType) == varTypeIsFloating(dstType))
        {
            assert(!varTypeIsGC(dstType) && "How can we have a cast to a GCRef here?");
            
            // We generate an assignment to an int and then do the cast from an int. With this we avoid
            // the gc problem and we allow casts to bytes, longs,  etc...
            var_types typInter;
            typInter = TYP_INT;            
            
            unsigned lclNum = lvaGrabTemp();
            oper->gtType = typInter;
            GenTreePtr asg  = gtNewTempAssign(lclNum, oper);
            oper->gtType = srcType;

            // do the real cast
            GenTreePtr cast = gtNewCastNode(tree->TypeGet(), gtNewLclvNode(lclNum, typInter), dstType);
                        
            // Generate the comma tree
            oper   = gtNewOperNode(GT_COMMA, tree->TypeGet(), asg, cast);

            return fgMorphTree(oper);                                                    
        }
        else
        {
            tree->gtCast.gtCastOp = fgMorphTree(oper);
            return tree;
        }
    }

    /* Is this a cast of long to an integer type? */
    if  ((oper->gtType == TYP_LONG) && (genActualType(dstType) == TYP_INT))
    {
        /* Special case: (int)(long & small_lcon) */
        if  (oper->gtOper == GT_AND)
        {
            GenTreePtr      and1 = oper->gtOp.gtOp1;
            GenTreePtr      and2 = oper->gtOp.gtOp2;

            if ((and2->gtOper == GT_CAST) &&
                (!and2->gtOverflow())     &&
                (and2->gtCast.gtCastOp->gtOper == GT_CNS_INT))
            {
                /* Change "(int)(long & (long) icon)" into "(int)long & icon" */

                and2 = and2->gtOp.gtOp1;
                goto CHANGE_TO_AND;
            }

            if  (and2->gtOper == GT_CNS_LNG)
            {
                unsigned __int64 lval = and2->gtLngCon.gtLconVal;

                if  (!(lval & ((__int64)0xFFFFFFFF << 32)))
                {
                    /* Change "(int)(long & lcon)" into "(int)long & icon" */

                    and2->ChangeOperConst     (GT_CNS_INT);
                    and2->gtType             = TYP_INT;
                    and2->gtIntCon.gtIconVal = (int)lval;
CHANGE_TO_AND:
                    tree->ChangeOper          (GT_AND);
                    tree->gtOp.gtOp1         = gtNewCastNode(TYP_INT, and1, TYP_INT);
                    tree->gtOp.gtOp2         = and2;

                    return fgMorphSmpOp(tree);
                }
            }
        }
    }

    assert(tree->gtOper == GT_CAST);

    /* Morph the operand */
    tree->gtCast.gtCastOp = oper = fgMorphTree(oper);

    /* Reset the call flag */
    tree->gtFlags &= ~GTF_CALL;

    /* unless we have an overflow cast, reset the except flag */
    if (!tree->gtOverflow())
        tree->gtFlags &= ~GTF_EXCEPT;

    /* Just in case new side effects were introduced */
    tree->gtFlags |= (oper->gtFlags & GTF_GLOB_EFFECT);

    srcType = oper->TypeGet();

    /* if GTF_UNSIGNED is set then force srcType to an unsigned type */
    if (tree->gtFlags & GTF_UNSIGNED)
        srcType = genUnsignedType(srcType);

    srcSize = genTypeSize(srcType);

    /* See if we can discard the cast */
    if (varTypeIsIntegral(srcType) && varTypeIsIntegral(dstType))
    {
        if (srcType == dstType) // Certainly if they are identical it is pointless
            goto REMOVE_CAST;

        bool  unsignedSrc = varTypeIsUnsigned(srcType);
        bool  unsignedDst = varTypeIsUnsigned(dstType);
        bool  signsDiffer = (unsignedSrc != unsignedDst);

        // For same sized casts with 
        //    the same signs or non-overflow cast we discard them as well
        if (srcSize == dstSize)
        {
            /* This should have been handled above */
            assert(varTypeIsGC(srcType) == varTypeIsGC(dstType));

            if (!signsDiffer) 
                goto REMOVE_CAST;

            if (!tree->gtOverflow())
            {
                /* For small type casts, when necessary we force
                   the src operand to the dstType and allow the
                   implied load from memory to perform the casting */
                if (varTypeIsSmall(srcType))
                {
                    switch (oper->gtOper)
                    {
                    case GT_IND:
                    case GT_CLS_VAR:
                    case GT_LCL_FLD:
                    case GT_ARR_ELEM:
                        oper->gtType = dstType;
                        goto REMOVE_CAST;
                    /* @TODO [CONSIDER] [04/16/01] []: GT_COMMA */
                    }
                }
                else
                    goto REMOVE_CAST;
            }
        }

        if (srcSize < dstSize)  // widening cast
        {
            // Keep any long casts
            if (dstSize == sizeof(int))
            {
                // Only keep signed to unsigned widening cast with overflow check
                if (!tree->gtOverflow() || !unsignedDst || unsignedSrc)
                    goto REMOVE_CAST;
            }
            
            // Casts from signed->unsigned can never overflow while widening
            
            if (unsignedSrc || !unsignedDst)
                tree->gtFlags &= ~GTF_OVERFLOW;
        }
        else
        {
            /* Try to narrow the operand of the cast and discard the cast */

            if  (!tree->gtOverflow()                    && 
                 (opts.compFlags & CLFLG_TREETRANS)     &&
                 optNarrowTree(oper, srcType, dstType, false))
            {
                DEBUG_DESTROY_NODE(tree);

                optNarrowTree(oper, srcType, dstType,  true);

                /* If oper is changed into a cast to TYP_INT, or to a GT_NOP, we may need to discard it */
                if ((oper->gtOper == GT_CAST && oper->gtCast.gtCastType == genActualType(oper->gtCast.gtCastOp->gtType)) ||
                    (oper->gtOper == GT_NOP && !(oper->gtFlags & GTF_NOP_RNGCHK)))
                {
                    assert(offsetof(GenTree, gtCast.gtCastOp) == offsetof(GenTree, gtOp.gtOp1));
                    oper = oper->gtCast.gtCastOp;
                }
                goto REMOVE_CAST;
            }
        }
    }

    switch (oper->gtOper)
    {
        /* If the operand is a constant, we'll fold it */
    case GT_CNS_INT:
    case GT_CNS_LNG:
    case GT_CNS_DBL:
    case GT_CNS_STR:
        {
            GenTreePtr oldTree = tree;

            tree = gtFoldExprConst(tree);    // This may not fold the constant (NaN ...)

            // Did we get a comma throw as a result of gtFoldExprConst?
            if ((oldTree != tree) && (oldTree->gtOper != GT_COMMA))
            {
                assert(fgIsCommaThrow(tree));
                tree->gtOp.gtOp1 = fgMorphTree(tree->gtOp.gtOp1);
                fgMorphTreeDone(tree);
                return tree;
            }
            else if (tree->gtOper != GT_CAST)
                return tree;

            assert(tree->gtCast.gtCastOp == oper); // unchanged
        }
        break;

    case GT_CAST:
        /* Check for two consecutive casts into the same dstType */
        if (!tree->gtOverflow())
        {
            var_types dstType2 = oper->gtCast.gtCastType;
            if (dstType == dstType2)
                goto REMOVE_CAST;
        }
        break;

        /* If op1 is a mod node, mark it with the GTF_MOD_INT_RESULT flag
           so that the code generator will know not to convert the result
           of the idiv to a regpair */
    case GT_MOD:
        if (dstType == TYP_INT)
            tree->gtOp.gtOp1->gtFlags |= GTF_MOD_INT_RESULT;
            break;
    case GT_UMOD:
        if (dstType == TYP_UINT)
            tree->gtOp.gtOp1->gtFlags |= GTF_MOD_INT_RESULT;
            break;

    case GT_COMMA:
        /* Check for cast of a GT_COMMA with a throw overflow */
        if (fgIsCommaThrow(oper))
        {
            GenTreePtr commaOp2 = oper->gtOp.gtOp2;

            // need type of oper to be same as tree
            if (tree->gtType == TYP_LONG)
            {
                commaOp2->ChangeOperConst(GT_CNS_LNG);
                commaOp2->gtLngCon.gtLconVal = 0;
                /* Bash the types of oper and commaOp2 to TYP_LONG */
                oper->gtType = commaOp2->gtType = TYP_LONG;
            }
            else if (varTypeIsFloating(tree->gtType))
            {
                commaOp2->ChangeOperConst(GT_CNS_DBL);
                commaOp2->gtDblCon.gtDconVal = 0.0;
                /* Bash the types of oper and commaOp2 to TYP_DOUBLE */
                oper->gtType = commaOp2->gtType = TYP_DOUBLE;
            }
            else
            {
                commaOp2->ChangeOperConst(GT_CNS_INT);
                commaOp2->gtIntCon.gtIconVal = 0;
                /* Bash the types of oper and commaOp2 to TYP_INT */
                oper->gtType = commaOp2->gtType = TYP_INT;
            }

            /* Return the GT_COMMA node as the new tree */
            return oper;
        }
        break;

    } /* end switch (oper->gtOper) */

    if (tree->gtOverflow())
        fgAddCodeRef(compCurBB, compCurBB->bbTryIndex, ACK_OVERFLOW, fgPtrArgCntCur);

    return tree;

CALL:

    /* If the operand is a constant, we'll try to fold it */
    if  (oper->OperIsConst())
    {
        GenTreePtr oldTree = tree;

        tree = gtFoldExprConst(tree);    // This may not fold the constant (NaN ...)

        if (tree != oldTree)
            return fgMorphTree(tree);
        else if (tree->OperKind() & GTK_CONST)
            return fgMorphConst(tree);

        // assert that oper is unchanged and that it is still a GT_CAST node
        assert(tree->gtCast.gtCastOp == oper);
        assert(tree->gtOper == GT_CAST);
    }

    if (tree->gtOverflow())
        fgAddCodeRef(compCurBB, compCurBB->bbTryIndex, ACK_OVERFLOW, fgPtrArgCntCur);

    return fgMorphIntoHelperCall(tree, CPX, gtNewArgList(oper));

REMOVE_CAST:

    /* Here we've eliminated the cast, so just return it's operand */

    DEBUG_DESTROY_NODE(tree);
    return oper;
}

/*****************************************************************************
 *
 *  Perform an unwrap operation on a Proxy object
 */

GenTreePtr          Compiler::fgUnwrapProxy(GenTreePtr objRef)
{
    assert(impIsThis(objRef)      &&
           info.compIsContextful  &&
           info.compUnwrapContextful);

    CORINFO_EE_INFO * pInfo   = eeGetEEInfo();
    GenTreePtr        addTree;
    
    // Perform the unwrap:
    //
    //   This requires two extra indirections.
    //   We mark these indirections as 'invariant' and 
    //   the CSE logic will hoist them when appropriate.
    //
    //  Note that each dereference is a GC pointer and that 
    //  we add 4 since the convention in the VM is to record
    //  the offsets as the number of bytes after the vtable slot 
    
    addTree = gtNewOperNode(GT_ADD, TYP_I_IMPL, 
                            objRef, 
                            gtNewIconNode(pInfo->offsetOfTransparentProxyRP + 4));
    
    objRef           = gtNewOperNode(GT_IND, TYP_REF, addTree);
    objRef->gtFlags |= GTF_IND_INVARIANT;
    
    addTree = gtNewOperNode(GT_ADD, TYP_I_IMPL, 
                            objRef, 
                            gtNewIconNode(pInfo->offsetOfRealProxyServer + 4));
    
    objRef           = gtNewOperNode(GT_IND, TYP_REF, addTree);
    objRef->gtFlags |= GTF_IND_INVARIANT;
    
    // objRef now hold the 'real this' reference (i.e. the unwrapped proxy)
    return objRef;
}

/*****************************************************************************
 *
 *  Morph an argument list; compute the pointer argument count in the process.
 *
 *  NOTE: This function can be called from any place in the JIT to perform re-morphing
 *  due to graph altering modifications such as copy / constant propagation
 */

GenTreePtr          Compiler::fgMorphArgs(GenTreePtr call)
{
    GenTreePtr      args;
    GenTreePtr      argx;

    unsigned        flags = 0;
    unsigned        genPtrArgCntSav = fgPtrArgCntCur;

    unsigned        begTab        = 0;
    unsigned        endTab        = 0;

    unsigned        i;

    unsigned        argRegNum     = 0;
    unsigned        argRegMask    = 0;

    unsigned        maxRealArgs   = MAX_REG_ARG;  // this is for IL where we need
                                                  // to reserve space for the objPtr
    GenTreePtr      tmpRegArgNext = 0;

    struct
    {
        GenTreePtr  node;
        GenTreePtr  parent;
        bool        needTmp;
    }
                    regAuxTab[MAX_REG_ARG],
                    regArgTab[MAX_REG_ARG];

    //memset(regAuxTab, 0, sizeof(regAuxTab));

    assert(call->gtOper == GT_CALL);

#if !NST_FASTCALL
    /* The x86 supports NST_FASTCALL, so we don't use this path on x86 */

    /* Gross - we need to return a different node when hoisting nested calls */

    GenTreePtr      cexp = call;

#define FGMA_RET    cexp

#else

#define FGMA_RET    call

#endif

    /* First we morph any subtrees (arguments, 'this' pointer, etc.)
     * While doing this we also notice how many register arguments we have
     * If this is a second time this function is called then we don't
     * have to recompute the register arguments, just morph them */

    argx = call->gtCall.gtCallObjp;
    if  (argx)
    {
        call->gtCall.gtCallObjp = argx = fgMorphTree(argx);
        flags |= argx->gtFlags;
        argRegNum++;
#if TGT_RISC && !STK_FASTCALL
        fgPtrArgCntCur++;
#endif
    }

    if  (call->gtFlags & GTF_CALL_POP_ARGS)
    {
        assert(argRegNum < maxRealArgs);
        // No more register arguments for varargs (CALL_POP_ARGS)
        maxRealArgs = argRegNum;
        // Except for return arg buff
        if (call->gtCall.gtCallMoreFlags & GTF_CALL_M_RETBUFFARG)
            maxRealArgs++;
    }

#if INLINE_NDIRECT
    if (call->gtFlags & GTF_CALL_UNMANAGED)
    {
        assert(argRegNum == 0);
        maxRealArgs = 0;
    }
#endif

    /* For indirect calls, the function pointer has to be evaluated last.
       It may cause registered args to be spilled. We just dont allow it
       to contain a call. The importer should spill such a pointer */

    assert(call->gtCall.gtCallType != CT_INDIRECT ||
           !(call->gtCall.gtCallAddr->gtFlags & GTF_CALL));

    /* Morph the user arguments */

    for (args = call->gtCall.gtCallArgs; args; args = args->gtOp.gtOp2)
    {
        args->gtOp.gtOp1 = argx = fgMorphTree(args->gtOp.gtOp1);
        flags |= argx->gtFlags;

        /* Bash the node to TYP_I_IMPL so we dont report GC info
         * NOTE: We deffered this from the importer because of the inliner */

        if (argx->IsVarAddr())
            argx->gtType = TYP_I_IMPL;

        if  (argRegNum < maxRealArgs && isRegParamType(genActualType(argx->TypeGet())))
        {
            argRegNum++;
#if TGT_RISC && !STK_FASTCALL
            fgPtrArgCntCur += genTypeStSz(argx->gtType);
#endif
        }
        else
        {
            unsigned size;

            if (argx->gtType != TYP_STRUCT)
            {
                size = genTypeStSz(argx->gtType);
            }
            else
            {
                /* We handle two opcodes: GT_MKREFANY and GT_LDOBJ */
                if (argx->gtOper == GT_MKREFANY)
                {
                    size = 2;
                }
                else /* argx->gtOper == GT_LDOBJ */
                {
                    assert(argx->gtOper == GT_LDOBJ);
                    size = eeGetClassSize(argx->gtLdObj.gtClass);
                    size = roundUp(size, sizeof(void*)) / sizeof(void*);
                }
            }

            assert(size != 0);
            fgPtrArgCntCur += size;
        }
    }

    /* Process the register arguments (which were determined before). Do it
       before resetting fgPtrArgCntCur as gtCallRegArgs may need it if
       gtCallArgs are trivial. */

    if  (call->gtCall.gtCallRegArgs)
    {
        assert(argRegNum && argRegNum <= maxRealArgs);
        call->gtCall.gtCallRegArgs = fgMorphTree(call->gtCall.gtCallRegArgs);
        flags |= call->gtCall.gtCallRegArgs->gtFlags;
    }

    if (call->gtCall.gtCallCookie)
        fgPtrArgCntCur++;

    /* Remember the maximum value we ever see */

    if  (fgPtrArgCntMax < fgPtrArgCntCur)
         fgPtrArgCntMax = fgPtrArgCntCur;

    /* The call will pop all the arguments we pushed */

    fgPtrArgCntCur = genPtrArgCntSav;

    /* Update the 'side effect' flags value for the call */

    call->gtFlags |= (flags & GTF_GLOB_EFFECT);

    /* If the register arguments have already been determined, we are done. */

    if  (call->gtCall.gtCallRegArgs)
        return call;

#if TGT_RISC
    genNonLeaf = true;
#endif

#if  !  NST_FASTCALL
    /* The x86 supports NST_FASTCALL, so we don't use this path on x86 */

    /* Do we have any nested calls? */

    if  (flags & GTF_CALL)
    {
        bool            foundSE;
        bool            hoistSE;

        GenTreePtr      thisx, thisl;
        GenTreePtr      nextx, nextl;
        GenTreePtr      lastx, lastl;

        GenTreePtr      hoistx = NULL;
        GenTreePtr      hoistl = NULL;

        bool            repeat = false;
        unsigned        pass   = 0;

        /*
            We do this in one or two passes: first we find the last argument
            that contains a call, since all preceding arguments with global
            effects need to be hoisted along with the call. We also look for
            any arguments that contains assignments - if those get moved, we
            have to move any other arguments that depend on the old value
            of the assigned variable.

            UNDONE: We actually don't the assignment part - it's kind of a
                    pain, and hopefully we can reuse some of the __fastcall
                    functionality for this later.

            UNDONE: If there are no calls beyond the very first argument,
                    we don't really need to do any hoisting.
         */

#ifdef  DEBUG
        if  (verbose)
        {
            printf("Call contains nested calls which will be hoisted:\n");
            gtDispTree(call);
            printf("\n");
        }
#endif

    HOIST_REP:

        thisx = thisl = call->gtCall.gtCallArgs;
        nextx = nextl = call->gtCall.gtCallObjp;
        lastx = lastl = call->gtCall.gtCallType == CT_INDIRECT ?
                        call->gtCall.gtCallAddr : NULL;

        /*
            Since there is at least one call remaining in the argument list,
            we certainly want to hoist any side effects we find (but we have
            not found any yet).
         */

        hoistSE = true;
        foundSE = false;

        for (;;)
        {
            GenTreePtr      argx;

            unsigned        tmpnum;
            GenTreePtr      tmpexp;

            /* Have we exhausted the current list? */

            if  (!thisx)
            {
                /* Move the remaining list(s) up */

                thisx = nextx;
                nextx = lastx;
                lastx = NULL;

                if  (!thisx)
                {
                    thisx = nextx;
                    nextx = NULL;
                }

                if  (!thisx)
                    break;
            }

            assert(thisx);

            /* Get hold of the argument value */

            argx = thisx;
            if  (argx->gtOper == GT_LIST)
            {
                /* This is a "regular" argument */

                argx = argx->gtOp.gtOp1;
            }
            else
            {
                /* This must be the object or function address argument */

                assert(thisx == call->gtCall.gtCallAddr ||
                       thisx == call->gtCall.gtCallObjp);
            }

            /* Is there a call in this argument? */

            if  (argx->gtFlags & GTF_CALL)
            {
                /* Have we missed any side effects? */

                if  (foundSE && !hoistSE)
                {
                    /* Rats, we'll have to perform a second pass */

                    assert(pass == 0);

                    /* We'll remember the last call we find */

                    hoistl = argx;
                    repeat = true;
                    goto NEXT_HOIST;
                }
            }
            else
            {
                /* Does this argument contain any side effects? */

                if  (!(argx->gtFlags & GTF_SIDE_EFFECT))
                    goto NEXT_HOIST;

                /* Are we currently hoisting side effects? */

                if  (!hoistSE)
                {
                    /* Merely remember that we have side effects and continue */

                    foundSE = true;
                    goto NEXT_HOIST;
                }
            }

            /* We arrive here if the current argument needs to be hoisted */

#ifdef  DEBUG
            if  (verbose)
            {
                printf("Hoisting argument value:\n");
                gtDispTree(argx);
                printf("\n");
            }
#endif

            /* Grab a temp for the argument value */

            tmpnum = lvaGrabTemp();

            /* Create the assignment of the argument value to the temp */

            tmpexp = gtNewTempAssign(tmpnum, argx);

            /* Append the temp to the list of hoisted expressions */

            hoistx = hoistx ? gtNewOperNode(GT_COMMA, TYP_VOID, hoistx, tmpexp)
                            : tmpexp;

            /* Create a copy of the temp to use in the argument list */

            tmpexp = gtNewLclvNode(tmpnum, genActualType(argx->TypeGet()));

            /* Replace the argument with the temp reference */

            if  (thisx->gtOper == GT_LIST)
            {
                /* This is a "regular" argument */

                assert(thisx->gtOp.gtOp1 == argx);
                       thisx->gtOp.gtOp1  = tmpexp;
            }
            else
            {
                /* This must be the object or function address argument */

                if  (call->gtCall.gtCallAddr == thisx)
                {
                     call->gtCall.gtCallAddr  = tmpexp;
                }
                else
                {
                    assert(call->gtCall.gtCallObjp == thisx);
                           call->gtCall.gtCallObjp  = tmpexp;
                }
            }

            /* Which pass are we performing? */

            if  (pass == 0)
            {
                /*
                    First pass - stop hoisting for now., hoping that this
                    is the last call. If we're wrong we'll have to go back
                    and perform a second pass.
                 */

                hoistSE = false;
                foundSE = false;
            }
            else
            {
                /*
                    Second pass - we're done if we just hoisted the last
                    call in the argument list (we figured out which was
                    the last one in the first pass). Otherwise we just
                    keep hoisting.
                 */

                if  (thisx == hoistl)
                    break;
            }

        NEXT_HOIST:

            /* Skip over the argument value we've just processed */

            thisx = (thisx->gtOper == GT_LIST) ? thisx->gtOp.gtOp2
                                               : NULL;
        }

        /* Do we have to perform a second pass? */

        if  (repeat)
        {
            assert(pass == 0); pass++;
            goto HOIST_REP;
        }

        /* Did we hoist any expressions out of the call? */

        if  (hoistx)
        {
            /* Make sure we morph the hoisted expression */

//          hoistx = fgMorphTree(hoistx);  temporarily disabled due to hack above

            /*
                We'll replace the call node with a comma node that
                prefixes the call with the hoisted expression, for
                example:

                    f(a1,a2)    --->    (t1=a1,t2=a2),f(t1,t2)
             */

            cexp = gtNewOperNode(GT_COMMA, call->gtType, hoistx, call);

#ifdef  DEBUG
            if  (verbose)
            {
                printf("Hoisted expression list:\n");
                gtDispTree(hoistx);
                printf("\n");

                printf("Updated call expression:\n");
                gtDispTree(cexp);
                printf("\n");
            }
#endif
        }
    }

#endif // !  NST_FASTCALL

    /* This is the fastcall part - figure out which arguments go to registers */

    if (argRegNum == 0)
    {
        /* No register arguments - don't waste time with this function */

        return FGMA_RET;
    }
    else
    {
        /* First time we morph this function AND it has register arguments
         * Follow into the code below and do the 'defer or eval to temp' analysis */

        argRegNum = 0;
    }

    /* Process the 'this' argument value, if present */

    argx = call->gtCall.gtCallObjp;

    if  (argx)
    {
        assert(call->gtCall.gtCallType == CT_USER_FUNC ||
               call->gtCall.gtCallType == CT_INDIRECT);

        assert(varTypeIsGC(call->gtCall.gtCallObjp->gtType) ||
                           call->gtCall.gtCallObjp->gtType == TYP_I_IMPL);

        assert(argRegNum == 0);

        /* this is a register argument - put it in the table */
        regAuxTab[argRegNum].node    = argx;
        regAuxTab[argRegNum].parent  = 0;

        /* For now we can optimistically assume that we won't need a temp
         * for this argument, unless it has a GTF_ASG */

        //regAuxTab[argRegNum].needTmp = false;
        regAuxTab[argRegNum].needTmp = ((argx->gtFlags & GTF_ASG) && call->gtCall.gtCallArgs != 0) ? true : false;

        /* Increment the argument register count */
        argRegNum++;
    }

    GenTreePtr objRef = argx;

    /* Process the user arguments */

    for (args = call->gtCall.gtCallArgs; args; args = args->gtOp.gtOp2)
    {
        /* If a non-register args calling convention, bail
         * NOTE: The this pointer is still passed in registers
         * UNDONE: If we change our mind about this we will likely have to add
         * the calling convention type to the GT_CALL node */

        argx = args->gtOp.gtOp1;

        if (argRegNum < maxRealArgs && isRegParamType(genActualType(argx->TypeGet())))
        {
            /* This is a register argument - put it in the table */

            regAuxTab[argRegNum].node    = argx;
            regAuxTab[argRegNum].parent  = args;
            regAuxTab[argRegNum].needTmp = false;

            /* If contains an assignment (GTF_ASG) then itself and everything before it
               (except constants) has to evaluate to temp since there may be other argumets
               that follow it and use the value (Can make a little optimization - this is not necessary
               if this is the last argument and everything before is constant)
               EXAMPLE: ArgTab is "a, a=5, a" -> the first two a's have to eval to temp
             */

            if (argx->gtFlags & GTF_ASG)
            {
                    // If there is only one arg you also don't need temp
                if (!(argRegNum == 0 && args->gtOp.gtOp2 == 0))
                    regAuxTab[argRegNum].needTmp = true;

                for(i = 0; i < argRegNum; i++)
                {
                    assert(regAuxTab[i].node);

                    if (regAuxTab[i].node->gtOper != GT_CNS_INT)
                    {
                        regAuxTab[i].needTmp = true;
                    }
                }
            }

            /* If contains a call (GTF_CALL) everything before the call with a GLOB_EFFECT
             * must eval to temp (this is because everything with SIDE_EFFECT has to be kept in the right
             * order since we will move the call to the first position
             */

            if (argx->gtFlags & GTF_CALL)
            {
                for(i = 0; i < argRegNum; i++)
                {
                    assert(regAuxTab[i].node);

                    if (regAuxTab[i].node->gtFlags & GTF_GLOB_EFFECT)
                    {
                        regAuxTab[i].needTmp = true;
                    }
                }
            }

            /* Increment the argument register count */

            argRegNum++;
        }
        else
        {
            /*
                Non-register argument -> all previous register arguments
                with side_efects must be evaluated to temps to maintain
                proper ordering.
             */

            for(i = 0; i < argRegNum; i++)
            {
                assert(regAuxTab[i].node);
                if (regAuxTab[i].node->gtFlags & GTF_SIDE_EFFECT)
                    regAuxTab[i].needTmp = true;
            }

            /* If the argument contains a call (GTF_CALL) it may affect previous
             * global references, so we cannot defer those, as their value might
             * have been changed by the call */

            if (argx->gtFlags & GTF_CALL)
            {
                for(i = 0; i < argRegNum; i++)
                {
                    assert(regAuxTab[i].node);
                    if (regAuxTab[i].node->gtFlags & GTF_GLOB_REF)
                        regAuxTab[i].needTmp = true;
                }
            }

            /* If the argument contains a assignment (GTF_ASG) - for example an x++
             * be conservative and assign everything to temps */

            if (argx->gtFlags & GTF_ASG)
            {
                for(i = 0; i < argRegNum; i++)
                {
                    assert(regAuxTab[i].node);
                    if (regAuxTab[i].node->gtOper != GT_CNS_INT)
                        regAuxTab[i].needTmp = true;
                }
            }
        }
    }

    /* If no register arguments, bail */

    if (!argRegNum)
    {
        return FGMA_RET;
    }

#ifdef  DEBUG
    if  (verbose&&0)
    {
        printf("\nMorphing register arguments:\n");
        gtDispTree(call);
        printf("\n");
    }
#endif

    /* Shuffle the register argument table - The idea is to move all "simple" arguments
     * (like constants and local vars) at the end of the table. This will prevent registers
     * from being spilled by the more complex arguments placed at the beginning of the table.
     */

    /* Set the beginning and end for the new argument table */

    assert(argRegNum <= MAX_REG_ARG);

    begTab = 0;
    endTab = argRegNum - 1;

    /* First take care of eventual constants and calls */

    for(i = 0; i < argRegNum; i++)
    {
        assert(regAuxTab[i].node);

        /* put constants at the end of the table */
        if (regAuxTab[i].node->gtOper == GT_CNS_INT)
        {
            assert(endTab >= 0);
            regArgTab[endTab] = regAuxTab[i];
            regAuxTab[i].node = 0;

            /* Encode the argument register in the register mask */
            argRegMask |= (unsigned short)genRegArgNum(i) << (4 * endTab);
            endTab--;
        }
        else if (regAuxTab[i].node->gtFlags & GTF_CALL)
        {
            /* put calls at the beginning of the table */
            assert(begTab >= 0);
            regArgTab[begTab] = regAuxTab[i];
            regAuxTab[i].node = 0;

            /* Encode the argument register in the register mask */
            argRegMask |= (unsigned short)genRegArgNum(i) << (4 * begTab);
            begTab++;
        }
    }

    /* Second, take care of temps and local vars - Temps should go in registers
     * before any local vars since this will give them a better chance to become
     * enregisterd (in the best case in the same arg register */

    for(i = 0; i < argRegNum; i++)
    {
        if (regAuxTab[i].node == 0) continue;

        if (regAuxTab[i].needTmp)
        {
            /* put temp arguments at the beginning of the table */
            assert(begTab >= 0);
            regArgTab[begTab] = regAuxTab[i];
            regAuxTab[i].node = 0;

            /* Encode the argument register in the register mask */
            argRegMask |= (unsigned short)genRegArgNum(i) << (4 * begTab);
            begTab++;
        }
        else if (regAuxTab[i].node->gtOper == GT_LCL_VAR ||
                 regAuxTab[i].node->gtOper == GT_LCL_FLD)
        {
            /* put non-tmp local vars at the end of the table */
            assert(endTab >= 0);
            assert(regAuxTab[i].needTmp == false);

            regArgTab[endTab] = regAuxTab[i];
            regAuxTab[i].node = 0;

            /* Encode the argument register in the register mask */
            argRegMask |= (unsigned short)genRegArgNum(i) << (4 * endTab);
            endTab--;
        }
    }

    /* Finally take care of any other arguments left */

    for(i = 0; i < argRegNum; i++)
    {
        if (regAuxTab[i].node == 0) continue;

        assert (regAuxTab[i].node->gtOper != GT_LCL_VAR);
        assert (regAuxTab[i].node->gtOper != GT_LCL_FLD);
        assert (regAuxTab[i].node->gtOper != GT_CNS_INT);

        assert (!(regAuxTab[i].node->gtFlags & (GTF_CALL | GTF_ASG)) || argRegNum == 1);

        assert (begTab >= 0); assert (begTab < argRegNum);
        regArgTab[begTab] = regAuxTab[i];
        regAuxTab[i].node = 0;

        /* Encode the argument register in the register mask */
        argRegMask |= (unsigned short)genRegArgNum(i) << (4 * begTab);
        begTab++;
    }

    assert ((unsigned)(begTab - 1) == endTab);

    /* Save the argument register encoding mask in the call node */

    call->gtCall.regArgEncode = argRegMask;

    /* Go through the new register table and perform
     * the necessary changes to the tree */

    GenTreePtr      op1, defArg;

    assert(argRegNum <= MAX_REG_ARG);
    for(i = 0; i < argRegNum; i++)
    {
        assert(regArgTab[i].node);
        if (regArgTab[i].needTmp == true)
        {
            /* Create a temp assignment for the argument
             * Put the temp in the gtCallRegArgs list */

#ifdef  DEBUG
            if (verbose&&0)
            {
                printf("Register argument with 'side effect'...\n");
                gtDispTree(regArgTab[i].node);
            }
#endif
            unsigned        tmp = lvaGrabTemp();

            op1 = gtNewTempAssign(tmp, regArgTab[i].node); assert(op1);

#ifdef  DEBUG
            if (verbose&&0)
            {
                printf("Evaluate to a temp...\n");
                gtDispTree(op1);
            }
#endif
            /* Create a copy of the temp to go to the list of register arguments */

            defArg = gtNewLclvNode(tmp, genActualType(regArgTab[i].node->gtType));
        }
        else
        {
            /* No temp needed - move the whole node to the gtCallRegArgs list
             * In place of the old node put a gtNothing node */

            assert(regArgTab[i].needTmp == false);

#ifdef  DEBUG
            if (verbose&&0)
            {
                printf("Defered register argument ('%s'), replace with NOP node...\n", getRegName((argRegMask >> (4*i)) & 0x000F));
                gtDispTree(regArgTab[i].node);
            }
#endif
            op1 = gtNewNothingNode(); assert(op1);

            /* The argument is defered and put in the register argument list */

            defArg = regArgTab[i].node;
        }

        /* mark this assignment as a register argument that is defered */
        op1->gtFlags |= GTF_REG_ARG;

        if (regArgTab[i].parent)
        {
            /* a normal argument from the list */
            assert(regArgTab[i].parent->gtOper == GT_LIST);
            assert(regArgTab[i].parent->gtOp.gtOp1 == regArgTab[i].node);

            regArgTab[i].parent->gtOp.gtOp1 = op1;
        }
        else
        {
            /* must be the gtCallObjp */
            assert(call->gtCall.gtCallObjp == regArgTab[i].node);

            call->gtCall.gtCallObjp = op1;
        }

        /* defered arg goes into the register argument list */

        if (tmpRegArgNext == NULL)
            call->gtCall.gtCallRegArgs = tmpRegArgNext = gtNewOperNode(GT_LIST, TYP_VOID, defArg, NULL);
        else
        {
            assert(tmpRegArgNext->gtOper == GT_LIST);
            assert(tmpRegArgNext->gtOp.gtOp1);
            tmpRegArgNext->gtOp.gtOp2 = gtNewOperNode(GT_LIST, TYP_VOID, defArg, NULL);
            tmpRegArgNext = tmpRegArgNext->gtOp.gtOp2;
        }
    }

    // An optimization for Contextful classes:
    // we unwrap the proxy when we have a 'this reference' and a virtual call
    if (impIsThis(objRef)                &&
        (call->gtFlags & GTF_CALL_VIRT)  &&   // only unwrap for virtual calls 
        info.compIsContextful            &&
        info.compUnwrapContextful        &&
        info.compUnwrapCallv)
    {
        assert(tmpRegArgNext != NULL);

        defArg = fgUnwrapProxy(gtNewLclvNode(objRef->gtLclVar.gtLclNum, objRef->gtType));

        assert(tmpRegArgNext->gtOper == GT_LIST);
        assert(tmpRegArgNext->gtOp.gtOp1);
        tmpRegArgNext->gtOp.gtOp2 = gtNewOperNode(GT_LIST, TYP_VOID, defArg, 0); 
   }

#ifdef DEBUG
    if (verbose&&0)
    {
        printf("\nShuffled argument register table:\n");
        for(i = 0; i < argRegNum; i++)
        {
            printf("%s ", getRegName((argRegMask >> (4*i)) & 0x000F) );
        }
        printf("\n");
    }
#endif

    return FGMA_RET;
}

/*****************************************************************************
 *
 *  A little helper used to rearrange nested commutative operations. The
 *  effect is that nested commutative operations are transformed into a
 *  'left-deep' tree, i.e. into something like this:
 *
 *      (((a op b) op c) op d) op...
 */

#if REARRANGE_ADDS

void                Compiler::fgMoveOpsLeft(GenTreePtr tree)
{
    GenTreePtr      op1  = tree->gtOp.gtOp1;
    GenTreePtr      op2  = tree->gtOp.gtOp2;
    genTreeOps      oper = tree->OperGet();

    assert(GenTree::OperIsCommutative(oper));
    assert(oper == GT_ADD || oper == GT_XOR || oper == GT_OR ||
           oper == GT_AND || oper == GT_MUL);
    assert(!varTypeIsFloating(tree->TypeGet()) || !genOrder);
    assert(oper == op2->gtOper);

    // Commutativity doesnt hold if overflow checks are needed

    if (tree->gtOverflowEx() || op2->gtOverflowEx())
        return;

    if (oper == GT_MUL && (op2->gtFlags & GTF_MUL_64RSLT))
        return;

    do
    {
        assert(!tree->gtOverflowEx() && !op2->gtOverflowEx());

        GenTreePtr      ad1 = op2->gtOp.gtOp1;
        GenTreePtr      ad2 = op2->gtOp.gtOp2;

        /* Change "(x op (y op z))" to "(x op y) op z" */
        /* ie.    "(op1 op (ad1 op ad2))" to "(op1 op ad1) op ad2" */

        GenTreePtr & new_op1    = op2;
        new_op1->gtOp.gtOp1     = op1;
        new_op1->gtOp.gtOp2     = ad1;

        /* Change the flags. */

        // Make sure we arent throwing away any flags
        assert((new_op1->gtFlags & ~(
            GTF_MAKE_CSE | // HACK: @TODO: fix the GTF_MAKE_CSE issue
            GTF_NODE_MASK|GTF_GLOB_EFFECT|GTF_UNSIGNED)) == 0);
        new_op1->gtFlags = (new_op1->gtFlags & GTF_NODE_MASK) |
                           (op1->gtFlags & GTF_GLOB_EFFECT)  |
                           (ad1->gtFlags & GTF_GLOB_EFFECT);

        /* Retype new_op1 if it has not/become a GC ptr. */

        if      (varTypeIsGC(op1->TypeGet()))
        {
            assert(varTypeIsGC(tree->TypeGet()) && op2->TypeGet() == TYP_I_IMPL);
            new_op1->gtType = tree->gtType;
        }
        else if (varTypeIsGC(ad2->TypeGet()))
        {
            // Neither ad1 nor op1 are GC. So new_op1 isnt either
            assert(op1->gtType == TYP_I_IMPL && ad1->gtType == TYP_I_IMPL);
            new_op1->gtType = TYP_I_IMPL;
        }

        // Old assert - Dont know what it does. Goes off incorrectly sometimes
        // like when you have (int > (bool OR int) )
#if 0
        // Check that new expression new_op1 is typed correctly
        assert((varTypeIsIntegral(op1->gtType) && varTypeIsIntegral(ad1->gtType))
              == varTypeIsIntegral(new_op1->gtType));
#endif

        tree->gtOp.gtOp1 = new_op1;
        tree->gtOp.gtOp2 = ad2;

        /* If 'new_op1' is now the same nested op, process it recursively */

        if  ((ad1->gtOper == oper) && !ad1->gtOverflowEx())
            fgMoveOpsLeft(new_op1);

        /* If   'ad2'   is now the same nested op, process it
         * Instead of recursion, we set up op1 and op2 for the next loop.
         */

        op1 = new_op1;
        op2 = ad2;
    }
    while ((op2->gtOper == oper) && !op2->gtOverflowEx());

    return;
}

#endif

/*****************************************************************************/

void            Compiler::fgSetRngChkTarget(GenTreePtr  tree,
                                            bool        delay)
{
    assert((tree->gtOper == GT_IND && (tree->gtFlags & GTF_IND_RNGCHK)) ||
           (tree->gtOper == GT_ARR_ELEM));

    if  (opts.compMinOptim)
        delay = false;

    if (!opts.compDbgCode)
    {
        if (delay)
        {
            /*  We delay this until after loop-oriented range check
                analysis. For now we merely store the current stack
                level in the tree node.
             */

            assert(!tree->gtInd.gtIndRngFailBB);
            tree->gtInd.gtStkDepth = fgPtrArgCntCur;
        }
        else
        {
            /* Create/find the appropriate "range-fail" label */

            // fgPtrArgCntCur is only valid for global morph or if we walk full stmt.
            assert(tree->gtOper == GT_IND || fgGlobalMorph);
            unsigned stkDepth = (tree->gtOper == GT_IND) ? tree->gtInd.gtStkDepth
                                                         : fgPtrArgCntCur;

            BasicBlock * rngErrBlk = fgRngChkTarget(compCurBB, stkDepth);

            /* Add the label to the indirection node */

            if (tree->gtOper == GT_IND)
                tree->gtInd.gtIndRngFailBB = gtNewCodeRef(rngErrBlk);
        }
    }
}

/*****************************************************************************
 *
 *  Create an array index / range check node.
 *  If tree!=NULL, that node will be reused.
 *  elemSize is the size of the array element, it only needs to be valid for type=TYP_STRUCT
 */

GenTreePtr              Compiler::gtNewRngChkNode(GenTreePtr    tree,
                                                  GenTreePtr    addr,
                                                  GenTreePtr    indx,
                                                  var_types     type,
                                                  unsigned      elemSize,
                                                  bool          isString)
{
    GenTreePtr          temp = tree;
    bool                chkd = true;            // if you set this to false, range checking will be disabled
    bool                nCSE = false;

    /* Did the caller supply a GT_INDEX node that is being morphed? */

    if  (tree)
    {
        assert(tree->gtOper == GT_INDEX);

#if SMALL_TREE_NODES
        assert(tree->gtFlags & GTF_NODE_LARGE);
#endif

        if  ((tree->gtFlags & GTF_DONT_CSE  ) != 0)
            nCSE = true;

        if  ((tree->gtFlags & GTF_INX_RNGCHK) == 0)
            chkd = false;
    }
    else
    {
        tree = gtNewOperNode(GT_IND, type);
    }


    if  (chkd)
    {
        /* Make sure we preserve the index value for range-checking */

        indx = gtNewOperNode(GT_NOP, TYP_INT, indx);
        indx->gtFlags |= GTF_NOP_RNGCHK;
    }

    if (type != TYP_STRUCT)
        elemSize = genTypeSize(type);

    /* Scale the index value if necessary */

    if  (elemSize > 1)
    {
        /* Multiply by the array element size */

        temp = gtNewIconNode(elemSize);
        indx = gtNewOperNode(GT_MUL, TYP_INT, indx, temp);
    }

    /* Add the first element's offset */

    unsigned elemOffs;
    if (isString) 
    {
        elemOffs = offsetof(CORINFO_String, chars);
        tree->gtInd.gtRngChkOffs = offsetof(CORINFO_String, stringLen);
    }
    else
    {
        elemOffs = offsetof(CORINFO_Array, u1Elems);
        if (type == TYP_REF) 
        {
            elemOffs = offsetof(CORINFO_RefArray, refElems);
            tree->gtFlags |= GTF_IND_OBJARRAY;
        }
        tree->gtInd.gtRngChkOffs = offsetof(CORINFO_Array, length);
    }
        /* Morph "tree" into "*(array + elemSize*index + LenOffs)" */

    temp = gtNewIconNode(elemOffs);
    indx = gtNewOperNode(GT_ADD, TYP_INT, indx, temp);

    /* Add the array address and the scaled index value */

    indx = gtNewOperNode(GT_ADD, TYP_BYREF, addr, indx);

    /* Indirect through the result of the "+" */

    tree->SetOper(GT_IND);
    tree->gtInd.gtIndOp1        = indx;
    tree->gtInd.gtIndRngFailBB  = 0;
#if CSELENGTH
    tree->gtInd.gtIndLen        = 0;
#endif

    /* An indirection will cause a GPF if the address is null */

    tree->gtFlags   |= GTF_EXCEPT;

    if  (nCSE)
        tree->gtFlags   |= GTF_DONT_CSE;

    /* Is range-checking enabled? */

    if  (chkd)
    {
        /* Mark the indirection node as needing a range check */

        tree->gtFlags |= GTF_IND_RNGCHK;

#if CSELENGTH
        /*  
         *  Make an explicit length operator on the array as a child of
         *  the GT_IND node, so it can be CSEd.
         */

        GenTreePtr      len = gtNewOperNode(GT_ARR_LENREF, TYP_INT);

        /* Set the array length flags in the node */
        len->gtSetArrLenOffset(tree->gtInd.gtRngChkOffs);

        /* The range check can throw an exception */
        len->gtFlags |= GTF_EXCEPT;

        /*  We point the array length node at the address node. Note
            that this is effectively a cycle in the tree but since
            it's always treated as a special case it doesn't cause
            any problems.
         */

        len->gtArrLen.gtArrLenAdr = addr;
        len->gtArrLen.gtArrLenCse = NULL;

        tree->gtInd.gtIndLen = len;
#endif

        fgSetRngChkTarget(tree);
    }
    else
    {
        /* Mark the indirection node as not needing a range check */

        tree->gtFlags &= ~GTF_IND_RNGCHK;
    }

//  printf("Array expression at %s(%u):\n", __FILE__, __LINE__); gtDispTree(tree); printf("\n\n");

    return  tree;
}


/*****************************************************************************
 *
 *  Transform the given GT_LCL_VAR tree for code generation.
 */

GenTreePtr          Compiler::fgMorphLocalVar(GenTreePtr tree)
{
    assert(tree->gtOper == GT_LCL_VAR);

    unsigned    lclNum  = tree->gtLclVar.gtLclNum;
    var_types   varType = lvaGetRealType(lclNum);
    LclVarDsc * varDsc = &lvaTable[lclNum];

    if (varDsc->lvAddrTaken)
    {
        tree->gtFlags |= GTF_GLOB_REF;
    }

    if (info.compIsVarArgs)
    {

        /* For the fixed stack arguments of a varargs function, we need to go
           through the varargs cookies to access them, except for the
           cookie itself */

        if (varDsc->lvIsParam && !varDsc->lvIsRegArg &&
            lclNum != lvaVarargsHandleArg)
        {
            // We only allow the GTF_VAR_DEF flag operator specific flag to be set
            //    along with any of the non-operator specific flags.
            assert((tree->gtFlags & ~(GTF_VAR_DEF|GTF_COMMON_MASK)) == 0);

            // Create a node representing the local pointing to the base of the args

            GenTreePtr baseOfArgs = tree;
            baseOfArgs->gtFlags &= ~GTF_VAR_DEF;  // Clear the GTF_VAR_DEF flag
            baseOfArgs->gtLclVar.gtLclNum = lvaVarargsBaseOfStkArgs;
            baseOfArgs->gtType = TYP_I_IMPL;

            GenTreePtr ptrArg = gtNewOperNode(GT_SUB, TYP_I_IMPL,
                                              baseOfArgs,
                                              gtNewIconNode(varDsc->lvStkOffs - rsCalleeRegArgNum*sizeof(void*)));

            // Access the argument through the local
            tree = gtNewOperNode(GT_IND, varType, ptrArg);
            return fgMorphTree(tree);
        }
    }

    /* If not during the global morphing phase bail */

    if (!fgGlobalMorph)
        return tree;

    bool varDef  = (tree->gtFlags & GTF_VAR_DEF)  != 0;
    bool varAddr = (tree->gtFlags & GTF_DONT_CSE) != 0;

    assert(!varDef || varAddr);    // varDef should always imply varAddr 

    if (!varAddr                            &&
        varTypeIsSmall(varDsc->TypeGet()) &&
         varDsc->lvNormalizeOnLoad()
#if LOCAL_ASSERTION_PROP
        /* Assertion prop can tell us to omit adding a cast here */
         && (!fgAssertionProp || 
             !optAssertionIsSubrange(lclNum, varType, -1, true DEBUGARG(NULL)))
#endif
        )
    {
        /* Small-typed arguments and aliased locals are normalized on load.
           Other small-typed locals are normalized on store.
           Also, under the debugger as the debugger could write to the variable.
           If this is one of the former, insert a narrowing cast on the load.
                   ie. Convert: var-short --> cast-short(var-int)
           @TODO [CONSIDER] [04/16/01] []: Allow non-aliased arguments to be normalized on store
           as well. Initial normalization would have to be done in the prolog. */

        tree->gtType = TYP_INT;
        fgMorphTreeDone(tree);
        tree = gtNewCastNode(TYP_INT, tree, varType);
        fgMorphTreeDone(tree);
        return tree;
    }

    return tree;
}

/*****************************************************************************
  return the tree that will compute the base of all the statics fields for
  the class 'cls'. It also calls the class constructor if it has not already
  been called.  
*/

GenTreePtr          Compiler::fgGetStaticsBlock(CORINFO_CLASS_HANDLE cls)
{
    GenTreePtr op1;

        // Get the class ID
    unsigned clsID; 
    void* pclsID;
    clsID =  info.compCompHnd->getClassDomainID(cls, &pclsID);

    if (pclsID) {
        op1 = gtNewIconHandleNode((long) pclsID, GTF_ICON_CID_HDL);
        op1 = gtNewOperNode(GT_IND, TYP_I_IMPL, op1);
        op1->gtFlags |= GTF_IND_INVARIANT;
    }
    else 
        op1 = gtNewIconHandleNode(clsID, GTF_ICON_CID_HDL);

    // call the helper to get the base
    op1 = gtNewArgList(op1);
    op1 = gtNewHelperCallNode(CORINFO_HELP_GETSHAREDSTATICBASE, 
                              TYP_I_IMPL, 0, op1);
    return(op1);
}

/*****************************************************************************
 *
 *  Transform the given GT_FIELD tree for code generation.
 */

GenTreePtr          Compiler::fgMorphField(GenTreePtr tree)
{
    assert(tree->gtOper == GT_FIELD);
    assert(tree->gtFlags & GTF_GLOB_REF);

    CORINFO_FIELD_HANDLE  symHnd    = tree->gtField.gtFldHnd; assert(symHnd > 0);
    unsigned              fldOffset = eeGetFieldOffset(symHnd);

    /* Is this an instance data member? */

    if  (tree->gtField.gtFldObj)
    {
        GenTreePtr      addr;

        if (tree->gtFlags & GTF_IND_TLS_REF)
            NO_WAY("instance field can not be a TLS ref.");

#if HOIST_THIS_FLDS
        addr = optHoistTFRupdate(tree);

        if  (addr->gtOper != GT_FIELD)
        {
            DEBUG_DESTROY_NODE(tree);
            assert(addr->gtOper == GT_LCL_VAR);
            return fgMorphLocalVar(addr);
        }
#endif

        /* We'll create the expression "*(objRef + mem_offs)" */

        GenTreePtr      objRef  = tree->gtField.gtFldObj;
        assert(varTypeIsGC(objRef->TypeGet()) || objRef->TypeGet() == TYP_I_IMPL);

        // An optimization for Contextful classes:
        // we unwrap the proxy when we have a 'this reference'
        if (impIsThis(objRef)     &&
            info.compIsContextful &&
            info.compUnwrapContextful)
        {
            objRef = fgUnwrapProxy(objRef);
        }

        /* Is the member at a non-zero offset? */

        if  (fldOffset == 0)
        {
            addr = objRef;
        }
        else
        {
            /* Add the member offset to the object's address */

            addr = gtNewOperNode(GT_ADD, objRef->TypeGet(), objRef,
                                 gtNewIconHandleNode(fldOffset, GTF_ICON_FIELD_HDL));
        }

        /* Now we can create the 'non-static data member' node */

        tree->SetOper(GT_IND);
        tree->gtInd.gtIndOp1        = addr;
        tree->gtInd.gtIndRngFailBB  = 0;

        /* An indirection will cause a GPF if the address is null */

        tree->gtFlags     |= GTF_EXCEPT;
        /* A field access froma TYP_REF, must be null checked if the address is taken */
        if (objRef->TypeGet() == TYP_REF)
            tree->gtFlags |= GTF_IND_FIELD;

        /* OPTIMIZATION - if the object is 'this' and it was not modified
         * in the method don't mark it as GTF_EXCEPT */

        /* check if we have the 'this' pointer */
        if (impIsThis(objRef))
        {
            // the object reference is the 'this' pointer
            // so remove the GTF_EXCEPT flag for this field
            
            tree->gtFlags &= ~(GTF_EXCEPT | GTF_IND_FIELD);
        }
    }
    else     /* This is a static data member */
    {
#if GEN_SHAREABLE_CODE

    GenTreePtr      call;

    /* Create the function call node */

    call = gtNewIconHandleNode(eeGetStaticBlkHnd(symHnd),
                               GTF_ICON_STATIC_HDL);

    call = gtNewHelperCallNode(CORINFO_HELP_GETSTATICDATA,
                               TYP_INT, 0,
                               gtNewArgList(call));

    /* Add the member's offset if non-zero */

    if  (fldOffset)
        call = gtNewOperNode(GT_ADD,
                             TYP_INT, call,
                             gtNewIconNode(fldOffset));

    /* Indirect through the result */

    tree->SetOper(GT_IND);
    tree->gtOp.gtOp1           = call;
    tree->gtOp.gtOp2           = 0;

#else // not GEN_SHAREABLE_CODE

    if (tree->gtFlags & GTF_IND_TLS_REF)
    {
        // Thread Local Storage static field reference
        //
        // Field ref is a TLS 'Thread-Local-Storage' reference
        //
        // Build this tree:  IND(*) #
        //                    |
        //                   ADD(I_IMPL)
        //                   / \
        //                  /  CNS(fldOffset)
        //                 /
        //                /
        //               /
        //             IND(I_IMPL) == [Base of this DLL's TLS]
        //              |
        //             ADD(I_IMPL)
        //             / \
        //            /   CNS(IdValue*4) or MUL
        //           /                      / \
        //          IND(I_IMPL)            /  CNS(4)
        //           |                    /
        //          CNS(TLS_HDL,0x2C)    IND
        //                                |
        //                               CNS(pIdAddr)
        //
        // # Denotes the orginal node
        //
        void **    pIdAddr   = NULL;
        unsigned    IdValue  = eeGetFieldThreadLocalStoreID(symHnd, &pIdAddr);

        //
        // If we can we access the TLS DLL index ID value directly
        // then pIdAddr will be NULL and
        //      IdValue will be the actual TLS DLL index ID
        //
        GenTreePtr dllRef = NULL;
        if (pIdAddr == NULL)
        {
            if (IdValue != 0)
                dllRef = gtNewIconNode(IdValue*4, TYP_INT);
        }
        else
        {
            dllRef = gtNewIconHandleNode((long)pIdAddr, GTF_ICON_STATIC_HDL);
            dllRef = gtNewOperNode(GT_IND, TYP_I_IMPL, dllRef);
            dllRef->gtFlags |= GTF_IND_INVARIANT;

            /* Multiply by 4 */

            dllRef = gtNewOperNode(GT_MUL, TYP_I_IMPL, dllRef, gtNewIconNode(4, TYP_INT));
        }

        #define WIN32_TLS_SLOTS (0x2C) // Offset from fs:[0] where the pointer to the slots resides

        // Mark this ICON as a TLS_HDL, codegen will use FS:[cns]

        GenTreePtr tlsRef = gtNewIconHandleNode(WIN32_TLS_SLOTS, GTF_ICON_TLS_HDL);

        tlsRef = gtNewOperNode(GT_IND, TYP_I_IMPL, tlsRef);

        if (dllRef != NULL)
        {
        /* Add the dllRef */
            tlsRef = gtNewOperNode(GT_ADD, TYP_I_IMPL, tlsRef, dllRef);
        }

        /* indirect to have tlsRef point at the base of the DLLs Thread Local Storage */
        tlsRef = gtNewOperNode(GT_IND, TYP_I_IMPL, tlsRef);

        if (fldOffset != 0)
        {
            GenTreePtr fldOffsetNode = gtNewIconNode(fldOffset, TYP_INT);

            /* Add the TLS static field offset to the address */

            tlsRef = gtNewOperNode(GT_ADD, TYP_I_IMPL, tlsRef, fldOffsetNode);
        }

        // Final indirect to get to actual value of TLS static field

        tree->SetOper(GT_IND);
        tree->gtInd.gtIndOp1        = tlsRef;
        tree->gtInd.gtIndRngFailBB  = NULL;

        assert(tree->gtFlags & GTF_IND_TLS_REF);
    }
    else if (tree->gtFlags & GTF_IND_SHARED)
    {
        GenTreePtr op1;

        // add the field offset
        op1 = gtNewOperNode(GT_ADD, TYP_I_IMPL, 
                            fgGetStaticsBlock(eeGetFieldClass(symHnd)),
                            gtNewIconNode(fldOffset, TYP_INT));

        // Object references point to a handle and need an extra deref
        if (tree->gtType == TYP_REF)
        {
            op1 = gtNewOperNode(GT_IND, TYP_BYREF, op1);
        }

        tree->SetOper(GT_IND);
        tree->gtFlags              |= (op1->gtFlags & GTF_GLOB_EFFECT);
        tree->gtInd.gtIndOp1        = op1;
        tree->gtInd.gtIndRngFailBB  = NULL;
    }
    else
    {
        // Normal static field reference

        //
        // If we can we access the static's address directly
        // then pFldAddr will be NULL and
        //      fldAddr will be the actual address of the static field
        //
        void **  pFldAddr = NULL;
         eeGetFieldAddress(symHnd, &pFldAddr);

        if (pFldAddr == NULL)
        {
            // @TODO [REVISIT] [04/16/01] []: Should really use fldAddr here
            tree->ChangeOper(GT_CLS_VAR);
            tree->gtClsVar.gtClsVarHnd = symHnd;

            return tree;
        }
        else
        {
            GenTreePtr addr  = gtNewIconHandleNode((long)pFldAddr, GTF_ICON_STATIC_HDL);

            // There are two cases here, either the static is RVA based,
            // in which case the type of the FIELD node is not a GC type
            // and the handle to the RVA is a TYP_I_IMPL.  Or the FIELD node is
            // a GC type and the handle to it is a TYP_BYREF in the GC heap
            // because handles to statics now go into the large object heap

            var_types  handleTyp = varTypeIsGC(tree->TypeGet()) ? TYP_BYREF
                                                                : TYP_I_IMPL;
            GenTreePtr op1       = gtNewOperNode(GT_IND, handleTyp, addr);
            op1->gtFlags        |= GTF_IND_INVARIANT;

            tree->SetOper(GT_IND);
            tree->gtInd.gtIndOp1        = op1;
            tree->gtInd.gtIndRngFailBB  = NULL;
        }
    }
#endif // not GEN_SHAREABLE_CODE 
    }

    assert(tree->gtOper == GT_IND);

    return fgMorphSmpOp(tree);

}

/*****************************************************************************
 *
 *  Transform the given GT_CALL tree for code generation.
 */

#ifdef DEBUG
static ConfigDWORD fJitNoInline(L"JitNoInline");
#endif

GenTreePtr          Compiler::fgMorphCall(GenTreePtr call)
{
    assert(call->gtOper == GT_CALL);

    if (!opts.compNeedSecurityCheck &&
        (call->gtCall.gtCallMoreFlags & GTF_CALL_M_CAN_TAILCALL))
    {
        compTailCallUsed = true;

        call->gtCall.gtCallMoreFlags &= ~GTF_CALL_M_CAN_TAILCALL;
        call->gtCall.gtCallMoreFlags |=  GTF_CALL_M_TAILCALL;

        var_types   callType = call->TypeGet();

        /* We have to ensure to pass the incoming retValBuf as the
           outgoing one. Using a temp will not do as this function will
           not regain control to do the copy. */

        if (info.compRetBuffArg >= 0)
        {
            assert(callType == TYP_VOID);
            GenTreePtr retValBuf = call->gtCall.gtCallArgs->gtOp.gtOp1;
            if (retValBuf->gtOper != GT_LCL_VAR ||
                retValBuf->gtLclVar.gtLclNum != unsigned(info.compRetBuffArg))
            {
                goto NO_TAIL_CALL;
            }

            assert(fgMorphStmt->gtNext->gtStmt.gtStmtExpr->gtOper == GT_RETURN);
            fgRemoveStmt(compCurBB, fgMorphStmt->gtNext);
        }

        /* Implementation note : If we optimize tailcall to do a direct jump
           to the target function (after stomping on the return address, etc),
           without using CORINFO_HELP_TAILCALL, we have to make certain that
           we dont starve the hijacking logic (by stomping on the hijacked
           return address etc).  */

        // As we will acutally call CORINFO_HELP_TAILCALL, set the callTyp to TYP_VOID.
        // to avoid doing any extra work for the return value.
        call->gtType = TYP_VOID;

        /* For tail call, we just call CORINFO_HELP_TAILCALL, and it jumps to the
           target. So we dont need an epilog - just like CORINFO_HELP_THROW. */

        assert(compCurBB->bbJumpKind == BBJ_RETURN);
        compCurBB->bbJumpKind = BBJ_THROW;

        /* For void calls, we would have created a GT_CALL in the stmt list.
           For non-void calls, we would have created a GT_RETURN(GT_CAST(GT_CALL)).
           For calls returning structs, we would have a void call, followed by a void return.
           For debuggable code, it would be an assignment of the call to a temp
           We want to get rid of any of this extra trees, and just leave
           the call */

#ifdef DEBUG
        GenTreePtr stmt = fgMorphStmt->gtStmt.gtStmtExpr;
        assert((stmt->gtOper == GT_CALL && stmt == call) ||
               (stmt->gtOper == GT_RETURN && (stmt->gtOp.gtOp1 == call ||
                                              stmt->gtOp.gtOp1->gtOp.gtOp1 == call)) ||
               (stmt->gtOper == GT_ASG && stmt->gtOp.gtOp2 == call));
        assert(fgMorphStmt->gtNext == NULL);
#endif

        call = fgMorphStmt->gtStmt.gtStmtExpr = fgMorphCall(call);

        /* For non-void calls, we return a place holder which will be
           used by the parents of this call */

        if (callType != TYP_VOID)
        {
            call = gtNewZeroConNode(genActualType(callType));
            call = fgMorphTree(call);
        }

        return call;
    }

NO_TAIL_CALL:

#if 0
    // For (final and private) functions which were called with
    // invokevirtual, but which we call directly, we need to dereference
    // the object pointer to check that it is not NULL. But not for "this"

#ifdef HOIST_THIS_FLDS // as optThisPtrModified is used

    if ((call->gtFlags & GTF_CALL_VIRT_RES) && call->gtCall.gtCallVptr)
    {
        GenTreePtr vptr = call->gtCall.gtCallVptr;

        assert((call->gtFlags & GTF_CALL_INTF) == 0);

        /* @TODO [CONSIDER] [04/16/01] []: we should directly check the 'objptr',
         * however this may be complictaed by the regsiter calling
         * convention and the fact that the morpher is re-entrant */

        if (vptr->gtOper == GT_IND)
        {
            if (vptr->gtInd.gtIndOp1                            &&
                !vptr->gtInd.gtIndRngFailBB                     &&
                vptr->gtInd.gtIndOp1->gtOper == GT_LCL_VAR      &&
                vptr->gtInd.gtIndOp1->gtLclVar.gtLclNum == 0    &&
                !info.compIsStatic                              &&
                !optThisPtrModified)
            {
                call->gtFlags &= ~GTF_CALL_VIRT_RES;
                call->gtCall.gtCallVptr = NULL;
            }
        }
    }
#endif
#endif // 0

#if INLINING

    /* See if this function call can be inlined */

    if  (!(call->gtFlags & (GTF_CALL_VIRT | GTF_CALL_INTF)))
    {
        /* The inliner relies the arguments not having been morphed. (addr of
           var being used as a TYP_BYREF/TYP_I_IMPL). Also, may be a problem
           if called later on during optimization becuase we already filled
           in the variable table.
           @TODO [CONSIDER] [04/16/01] []: Use a flag to indicate whether the call has been morphed
         */

        if  (!fgGlobalMorph || call->gtCall.gtCallRegArgs)
            goto NOT_INLINE;

        /* Don't inline if not optimized code */

        if  (opts.compMinOptim || opts.compDbgCode)
            goto NOT_INLINE;

        /* Ignore tail-calls */

        if (call->gtCall.gtCallMoreFlags & (GTF_CALL_M_TAILCALL | GTF_CALL_M_TAILREC))
            goto NOT_INLINE;

        /* Ignore helper calls */

        if  (call->gtCall.gtCallType == CT_HELPER)
            goto NOT_INLINE;

        /* Ignore indirect calls */
        if  (call->gtCall.gtCallType == CT_INDIRECT)
            goto NOT_INLINE;

        /* Cannot inline native or synchronized methods */

        CORINFO_METHOD_HANDLE   fncHandle = call->gtCall.gtCallMethHnd;

        if  (eeGetMethodAttribs(fncHandle) & (CORINFO_FLG_NATIVE | CORINFO_FLG_SYNCH))
            goto NOT_INLINE;

#ifdef DEBUG
        const char *    methodName;
        const char *     className;
        methodName = eeGetMethodName(fncHandle, &className);

        if (fJitNoInline.val()) 
            goto NOT_INLINE;
#endif

#if OPTIMIZE_TAIL_REC
        
        /*
        @TODO [REVISIT] [04/16/01] []
            Unfortunately, we do tail recursion after inlining,
            which means that we might inline a tail recursive
            call, which is almost always a bad idea. For now
            we just use the following hack to check for calls
            that look like they might be tail recursive.
        */
        if  ((compCodeOpt()    != SMALL_CODE) &&
             (fgMorphStmt->gtNext == NULL      ))
        {
            if  (eeIsOurMethod(call->gtCall.gtCallMethHnd))
            {
                /*  The following is just a hack, of course (it doesn't
                    even check for non-void tail recursion - disgusting).
                 */

                if  (compCurBB->bbJumpKind == BBJ_NONE &&
                     compCurBB->bbNext)
                {
                    BasicBlock  *   bnext = compCurBB->bbNext;
                    GenTree     *   retx;

                    if  (bnext->bbJumpKind != BBJ_RETURN)
                        goto NOT_TAIL_REC;

                    assert(bnext->bbTreeList && bnext->bbTreeList->gtOper == GT_STMT);

                    retx = bnext->bbTreeList->gtStmt.gtStmtExpr; assert(retx);

                    if  (retx->gtOper != GT_RETURN)
                        goto NOT_TAIL_REC;
                    if  (retx->gtOp.gtOp1)
                        goto NOT_TAIL_REC;

                    goto NOT_INLINE;
                }
            }
        }

    NOT_TAIL_REC:

#endif

        /* Prevent recursive expansion */

        for (inlExpPtr expLst = fgInlineExpList; expLst; expLst = expLst->ixlNext)
        {
            if  (expLst->ixlMeth == fncHandle)
                goto NOT_INLINE;
        }

        /* Try to inline the call to the method */

        GenTreePtr     inlExpr;
        CorInfoInline  result;

        setErrorTrap()
        {
            assert(fgExpandInline == false);
            fgExpandInline = true;
            result = impExpandInline(call, fncHandle, &inlExpr);
            assert(fgExpandInline == true);
        }
        impErrorTrap(info.compCompHnd)
        {
            inlExpr = NULL;
            result  = INLINE_FAIL;
        }
        endErrorTrap();

        fgExpandInline = false;
        
        if      (result == INLINE_NEVER)
        {
            eeSetMethodAttribs(fncHandle, CORINFO_FLG_DONT_INLINE);
        }
        else if (result == INLINE_PASS)
        {
#ifdef  DEBUG
            if  (verbose)
            {
                const char *    methodName;
                const char *     className;

                methodName = eeGetMethodName(fncHandle, &className);
                printf("Inlined call to '%s.%s':\n", className, methodName);
            }
#endif

            /* Prevent recursive inline expansion */

            inlExpLst   expDsc;

            expDsc.ixlMeth = fncHandle;
            expDsc.ixlNext = fgInlineExpList;
                             fgInlineExpList = &expDsc;

            /* Morph the inlined call */

            DEBUG_DESTROY_NODE(call);

            inlExpr = fgMorphTree(inlExpr);

            fgInlineExpList = expDsc.ixlNext;

            return inlExpr;
        }
    }

NOT_INLINE:

#endif

    /* Couldn't inline - remember that this BB contains method calls */

    /* If this is a 'regular' call, mark the basic block as
       having a call (for computing full interruptibility */

    if ( call->gtCall.gtCallType == CT_INDIRECT     ||
        (call->gtCall.gtCallType == CT_USER_FUNC &&
         !(call->gtCall.gtCallMoreFlags & GTF_CALL_M_NOGCCHECK)))
    {
        compCurBB->bbFlags |= BBF_GC_SAFE_POINT;
    }

#if     RET_64BIT_AS_STRUCTS

    /* Are we returning long/double as a struct? */

    if  (genTypeStSz(call->TypeGet()) > 1 && call->gtCall.gtCallType != CT_HELPER)
    {
        unsigned        tnum;
        GenTreePtr      temp;

//          printf("Call call before:\n"); gtDispTree(call);

        GenTreePtr      origCall    = call;
        var_types       type        = origCall->TypeGet();
        GenTreePtr      args        = origCall->gtCall.gtCallArgs;

        /* Bash the origCall to not return anything */

        origCall->gtType = TYP_VOID;

        /* Allocate a temp for the result */

        tnum = lvaGrabTemp();       // UNDONE: should reuse these temps!!!!

        /* Add "&temp" in front of the argument list */

        temp = gtNewLclvNode(tnum, type);
        temp->gtFlags |= GTF_DONT_CSE;
        temp = gtNewOperNode(GT_ADDR, TYP_INT, temp);

        origCall->gtCall.gtCallArgs = gtNewOperNode(GT_LIST,
                                                    TYP_VOID,
                                                    temp,
                                                    args);

        /* Change the original node into "call(...) , temp" */

        call = gtNewOperNode(GT_COMMA, type, origCall,
                                             gtNewLclvNode(tnum, type));

//          printf("Call call after:\n"); gtDispTree(call);

        return fgMorphSmpOp(call);
    }

#endif

    /* Process the function address, if indirect call */

    if (call->gtCall.gtCallType == CT_INDIRECT)
        call->gtCall.gtCallAddr = fgMorphTree(call->gtCall.gtCallAddr);

    /* Process the "normal" argument list */

    return fgMorphArgs(call);
}

/*****************************************************************************
 *
 *  Transform the given GTK_CONST tree for code generation.
 */

GenTreePtr          Compiler::fgMorphConst(GenTreePtr tree)
{
    assert(tree->OperKind() & GTK_CONST);

    /* Clear any exception flags or other unnecessary flags
     * that may have been set before folding this node to a constant */

    tree->gtFlags &= ~(GTF_SIDE_EFFECT | GTF_REVERSE_OPS);

    if  (tree->OperGet() != GT_CNS_STR)
        return tree;

    assert(tree->gtStrCon.gtScpHnd == info.compScopeHnd    ||
           (unsigned)tree->gtStrCon.gtScpHnd != 0xDDDDDDDD  );

    unsigned strHandle, *pStrHandle;
    strHandle = eeGetStringHandle(tree->gtStrCon.gtSconCPX,
                                  tree->gtStrCon.gtScpHnd,
                                  &pStrHandle);
    assert((!strHandle) != (!pStrHandle));

    // Can we access the string handle directly?

    if (strHandle)
    {
        tree = gtNewIconHandleNode((long)strHandle, GTF_ICON_STR_HDL);
    }
    else
    {
        tree = gtNewIconHandleNode((long)pStrHandle, GTF_ICON_PSTR_HDL);
        tree = gtNewOperNode(GT_IND, TYP_I_IMPL, tree);
        tree->gtFlags |= GTF_IND_INVARIANT;
    }

    /* An indirection of a string handle can't cause an exception so don't set GTF_EXCEPT */

    tree = gtNewOperNode(GT_IND, TYP_REF, tree);
    tree->gtFlags |= GTF_GLOB_REF;

    return fgMorphTree(tree);
}

/*****************************************************************************
 *
 *  Transform the given GTK_LEAF tree for code generation.
 */

GenTreePtr          Compiler::fgMorphLeaf(GenTreePtr tree)
{
    assert(tree->OperKind() & GTK_LEAF);

    if (tree->gtOper == GT_LCL_VAR)
    {
        return fgMorphLocalVar(tree);
    }
    else if (tree->gtOper == GT_FTN_ADDR)
    {
        unsigned              addr;
        InfoAccessType accessType = IAT_VALUE;
        CORINFO_MODULE_HANDLE scope  = (CORINFO_MODULE_HANDLE )tree->gtVal.gtVal2;

        // @TODO [REVISIT] [04/16/01] []: This assert seems pretty worthless
        assert(scope == info.compScopeHnd || tree->gtVal.gtVal2 != 0xDDDDDDDD);

        addr = (unsigned) eeGetMethodPointer( eeFindMethod(tree->gtVal.gtVal1, scope, 0),
                                &accessType);

        tree->SetOper(GT_CNS_INT);
        tree->gtIntCon.gtIconVal = addr;

        switch(accessType)
        {
        case IAT_PPVALUE:
            tree           = gtNewOperNode(GT_IND, TYP_I_IMPL, tree);
            tree->gtFlags |= GTF_IND_INVARIANT;
            // Fall through
        case IAT_PVALUE:
            tree           = gtNewOperNode(GT_IND, TYP_I_IMPL, tree);
            return fgMorphTree(tree);

        case IAT_VALUE:
            return fgMorphConst(tree);
        }

    }

    return tree;
}

// If assigning to a local var, add a cast if the target is 
// marked as NormalizedOnStore. Returns true if any change was made
GenTreePtr Compiler::fgDoNormalizeOnStore(GenTreePtr tree)
{
    assert(tree->OperGet()==GT_ASG);

    GenTreePtr      op1     = tree->gtOp.gtOp1;
    GenTreePtr      op2     = tree->gtOp.gtOp2;

    if (op1->gtOper == GT_LCL_VAR && genActualType(op1->TypeGet()) == TYP_INT)
    {
        // Small-typed arguments and aliased locals are normalized on load.
        // Other small-typed locals are normalized on store.
        // If it is an assignment to one of the latter, insert the cast on RHS
        unsigned    varNum = op1->gtLclVar.gtLclNum;
        LclVarDsc * varDsc = &lvaTable[varNum];

        if (varDsc->lvNormalizeOnStore())
        {
            assert(op1->gtType <= TYP_INT);
            op1->gtType = TYP_INT;
            op2         = gtNewCastNode(TYP_INT, op2, varDsc->TypeGet());
            tree->gtOp.gtOp2 = op2;

            // Propagate GTF_COLON_COND 
            op2->gtFlags|=(tree->gtFlags & GTF_COLON_COND);
        }        
    }

    return tree;
}

/*****************************************************************************
 *
 *  Transform the given GTK_SMPOP tree for code generation.
 */

GenTreePtr          Compiler::fgMorphSmpOp(GenTreePtr tree)
{
    ALLOCA_CHECK();
    assert(tree->OperKind() & GTK_SMPOP);

    /* The steps in this function are :
       o Perform required preorder processing
       o Process the first, then second operand, if any
       o Perform required postorder morphing
       o Perform optional postorder morphing if optimizing
     */

    genTreeOps      oper    = tree->OperGet();
    var_types       typ     = tree->TypeGet();

    GenTreePtr      op1     = tree->gtOp.gtOp1;
    GenTreePtr      op2     = tree->gtGetOp2();

    bool            isQmarkColon     = false;

#if LOCAL_ASSERTION_PROP
    unsigned        origAssertionCount;
    AssertionDsc *  origAssertionTab;

    unsigned        thenAssertionCount;
    AssertionDsc *  thenAssertionTab;
#endif

    /*-------------------------------------------------------------------------
     * First do any PRE-ORDER processing
     */

    switch(oper)
    {
        // Some arithmetic operators need to use a helper call to the EE
        int         helper;

    case GT_ASG:
        tree = fgDoNormalizeOnStore(tree);
        /* fgDoNormalizeOnStore can change op2 */
        assert(op1 == tree->gtOp.gtOp1);
        op2=tree->gtOp.gtOp2;
        // FALL THROUGH


    case GT_ASG_ADD:
    case GT_ASG_SUB:
    case GT_ASG_MUL:
    case GT_ASG_DIV:
    case GT_ASG_MOD:
    case GT_ASG_UDIV:
    case GT_ASG_UMOD:
    case GT_ASG_OR:
    case GT_ASG_XOR:
    case GT_ASG_AND:
    case GT_ASG_LSH:
    case GT_ASG_RSH:
    case GT_ASG_RSZ:
    
    case GT_CHS:
    case GT_ADDR:
        /* We can't CSE something hanging below GT_ADDR or the LHS of an assignment */
        op1->gtFlags |= GTF_DONT_CSE;
        break;

    case GT_JTRUE:
        assert(op1);
        assert(op1->OperKind() & GTK_RELOP);
        /* Mark the comparison node with GTF_RELOP_JMP_USED so it knows that it does
           not need to materialize the result as a 0 or 1. */

        /* We also mark it as DONT_CSE. @TODO [CONSIDER] [02/19/01] [dnotario]: we do this coz we don't want 
           to write much code now. We could write the code to handle JTRUEs with nonRELOP
           nodes underneath them  */
        op1->gtFlags |= (GTF_RELOP_JMP_USED | GTF_DONT_CSE);    
        break;

    case GT_QMARK:
        if (op1->OperKind() & GTK_RELOP)
        {
            assert(op1->gtFlags & GTF_RELOP_QMARK);
            /* Mark the comparison node with GTF_RELOP_JMP_USED so it knows that it does
               not need to materialize the result as a 0 or 1. */

            /* We also mark it as DONT_CSE. @TODO [CONSIDER] [02/19/01] [dnotario]: 
            we do this coz we don't want to write much code now. We could write the code 
            to handle QMARKs with nonRELOP op1s */
            op1->gtFlags |= (GTF_RELOP_JMP_USED | GTF_DONT_CSE);
        }
        else
        {
            assert( (op1->gtOper == GT_CNS_INT) &&
                    ((op1->gtIntCon.gtIconVal == 0) ||
                     (op1->gtIntCon.gtIconVal == 1)    ));
        }
        break;

    case GT_COLON:
#if LOCAL_ASSERTION_PROP
        if (fgAssertionProp)
#endif
            isQmarkColon = true;
        break;

    case GT_INDEX:
        tree = gtNewRngChkNode(tree, op1, op2, typ, tree->gtIndex.gtIndElemSize);
        return fgMorphSmpOp(tree);

    case GT_CAST:
        return fgMorphCast(tree);

    case GT_MUL:

#if !LONG_MATH_REGPARAM
        if  (typ == TYP_LONG)
        {
            /* For (long)int1 * (long)int2, we dont actually do the
               casts, and just multiply the 32 bit values, which will
               give us the 64 bit result in edx:eax */

            assert(op2);
            if  ((op1->gtOper                                 == GT_CAST &&
                  op2->gtOper                                 == GT_CAST &&
                  genActualType(op1->gtCast.gtCastOp->gtType) == TYP_INT &&
                  genActualType(op2->gtCast.gtCastOp->gtType) == TYP_INT)&&
                  !op1->gtOverflow() && !op2->gtOverflow())
            {
                // The casts have to be of the same signedness.

                if ((op1->gtFlags & GTF_UNSIGNED) != (op2->gtFlags & GTF_UNSIGNED))
                    goto NO_MUL_64RSLT;

                // The only combination that can overflow

                if (tree->gtOverflow() && (tree->gtFlags & GTF_UNSIGNED) &&
                                         !( op1->gtFlags & GTF_UNSIGNED))
                    goto NO_MUL_64RSLT;

                /* Remaining combinations can never overflow during long mul. */

                tree->gtFlags &= ~GTF_OVERFLOW;

                /* Do unsigned mul only if the casts were unsigned */

                tree->gtFlags &= ~GTF_UNSIGNED;
                tree->gtFlags |= op1->gtFlags & GTF_UNSIGNED;

                /* Since we are committing to GTF_MUL_64RSLT, we don't want
                   the casts to be folded away. So morph the castees directly */

                op1->gtOp.gtOp1 = fgMorphTree(op1->gtOp.gtOp1);
                op2->gtOp.gtOp1 = fgMorphTree(op2->gtOp.gtOp1);

                // If the GT_MUL can be altogether folded away, we should do that.

                if (op1->gtCast.gtCastOp->OperKind() &
                    op2->gtCast.gtCastOp->OperKind() & GTK_CONST)
                {
                    tree->gtOp.gtOp1 = op1 = gtFoldExprConst(op1);
                    tree->gtOp.gtOp2 = op2 = gtFoldExprConst(op2);
                    assert(op1->OperKind() & op2->OperKind() & GTK_CONST);
                    tree = gtFoldExprConst(tree);
                    assert(tree->OperIsConst());
                    return tree;
                }

                tree->gtFlags |= GTF_MUL_64RSLT;

                // If op1 and op2 are unsigned casts, we need to do an unsigned mult
                tree->gtFlags |= (op1->gtFlags & GTF_UNSIGNED);

                /* Insert a GT_NOP node for op1 */

                op1->gtOp.gtOp1 = gtNewOperNode(GT_NOP, TYP_INT, op1->gtOp.gtOp1);

                /* Propagate the new flags. We don't want to CSE the casts because codegen expects 
                   GTF_MUL_64RSLT muls to have a certain layout.                
                */
                op1->gtFlags  |= (op1->gtOp.gtOp1->gtFlags    & GTF_GLOB_EFFECT);
                op2->gtFlags  |= (op2->gtOp.gtOp1->gtFlags    & GTF_GLOB_EFFECT);
                tree->gtFlags |= ((op1->gtFlags | op2->gtFlags) & GTF_GLOB_EFFECT);
                op1->gtFlags  |= GTF_DONT_CSE;
                op2->gtFlags  |= GTF_DONT_CSE;

                goto DONE_MORPHING_CHILDREN;
            }
            else if ((tree->gtFlags & GTF_MUL_64RSLT) == 0)
            {
            NO_MUL_64RSLT:
                if (tree->gtOverflow())
                    helper = (tree->gtFlags & GTF_UNSIGNED) ? CORINFO_HELP_ULMUL_OVF
                                                            : CORINFO_HELP_LMUL_OVF;
                else
                    helper = CORINFO_HELP_LMUL;

                goto USE_HELPER_FOR_ARITH;
            }
            else
            {
                /* We are seeing this node again. We have decided to use
                   GTF_MUL_64RSLT, so leave it alone. */

                assert(tree->gtIsValid64RsltMul());
            }
        }
#endif // !LONG_MATH_REGPARAM
        break;


    case GT_DIV:

#if !LONG_MATH_REGPARAM
        if  (typ == TYP_LONG)
        {
            helper = CORINFO_HELP_LDIV;
            goto USE_HELPER_FOR_ARITH;
        }
#endif

#ifdef  USE_HELPERS_FOR_INT_DIV
        if  (typ == TYP_INT)
        {
            helper = CPX_I4_DIV;
            goto USE_HELPER_FOR_ARITH;
        }
#endif
        break;


    case GT_UDIV:

#if !LONG_MATH_REGPARAM
        if  (typ == TYP_LONG)
        {
            helper = CORINFO_HELP_ULDIV;
            goto USE_HELPER_FOR_ARITH;
        }
#endif
#ifdef  USE_HELPERS_FOR_INT_DIV

        if  (typ == TYP_INT)
        {
            helper = CPX_U4_DIV;
            goto USE_HELPER_FOR_ARITH;
        }
#endif
        break;


    case GT_MOD:

        if  (varTypeIsFloating(typ))
        {
            helper = CORINFO_HELP_DBLREM;
            assert(op2);
            if (op1->TypeGet() == TYP_FLOAT) 
                if (op2->TypeGet() == TYP_FLOAT)
                    helper = CORINFO_HELP_FLTREM;
                else
                    tree->gtOp.gtOp1 = op1 = gtNewCastNode(TYP_DOUBLE, op1, TYP_DOUBLE);
            else 
                if (op2->TypeGet() == TYP_FLOAT)
                    tree->gtOp.gtOp2 = op2 = gtNewCastNode(TYP_DOUBLE, op2, TYP_DOUBLE);
            goto USE_HELPER_FOR_ARITH;
        }

        // fall-through

    case GT_UMOD:

        /* If this is a long mod with op2 which is a cast to long from a
           constant int, then don't morph to a call to the helper.  This can be done
           faster inline using idiv.
           @TODO [CONSIDER] [04/16/01] []:  Should we avoid this for SMALL_CODE?
        */        
        
        assert(op2);
        if ((typ == TYP_LONG) &&
            (op2->gtOper == GT_CAST) &&
            (op2->gtCast.gtCastOp->gtOper == GT_CNS_INT) &&
            (op2->gtCast.gtCastOp->gtIntCon.gtIconVal >= 2) &&
            (op2->gtCast.gtCastOp->gtIntCon.gtIconVal <= 0x3fffffff) &&
            ((tree->gtFlags & GTF_UNSIGNED) == (op1->gtFlags & GTF_UNSIGNED)) &&
            ((tree->gtFlags & GTF_UNSIGNED) == (op2->gtFlags & GTF_UNSIGNED)) &&
            ((tree->gtFlags & GTF_UNSIGNED) == (op2->gtCast.gtCastOp->gtFlags & GTF_UNSIGNED)))
        {
            tree->gtOp.gtOp1 = op1 = fgMorphTree(op1);

            // Update flags for op1 morph
            tree->gtFlags |= (op1->gtFlags & GTF_GLOB_EFFECT);

            tree->gtOp.gtOp2 = op2 = fgMorphCast(op2);
            return tree;
        }

#if !LONG_MATH_REGPARAM
        if  (typ == TYP_LONG)
        {
            helper = (oper == GT_UMOD) ? CORINFO_HELP_ULMOD : CORINFO_HELP_LMOD;
            goto USE_HELPER_FOR_ARITH;
        }
#endif

#ifdef  USE_HELPERS_FOR_INT_DIV
        if  (typ == TYP_INT)
        {
            helper = (oper == GT_UMOD) ? CPX_U4_MOD : CPX_I4_MOD;
            goto USE_HELPER_FOR_ARITH;
        }
#endif
        break;


    USE_HELPER_FOR_ARITH:

        /* We have to morph these arithmetic operations into helper calls
           before morphing the arguments (preorder), else the arguments
           wont get correct values of fgPtrArgCntCur.
           However, try to fold the tree first in case we end up with a
           simple node which wont need a helper call at all */

        assert(tree->OperIsBinary());

        tree = gtFoldExpr(tree);

        // Were we able to fold it ?
        if (tree->OperIsLeaf())
            return fgMorphLeaf(tree);

        // Did we fold it into a comma node with throw?
        if (tree->gtOper == GT_COMMA)
        {
            assert(fgIsCommaThrow(tree));
            return fgMorphTree(tree);
        }

        return fgMorphIntoHelperCall(tree, helper, gtNewArgList(op1, op2));
    }

#if !CPU_HAS_FP_SUPPORT

    /*
        We have to use helper calls for all FP operations:

            FP operators that operate on FP values
            casts to and from FP
            comparisons of FP values
     */

    if  (varTypeIsFloating(typ) || (op1 && varTypeIsFloating(op1->TypeGet())))
    {
        int         helper;
        GenTreePtr  args;
        size_t      argc = genTypeStSz(typ);

        /* Not all FP operations need helper calls */

        switch (oper)
        {
        case GT_ASG:
        case GT_IND:
        case GT_LIST:
        case GT_ADDR:
        case GT_COMMA:
            goto NOT_FPH;
        }

#ifdef  DEBUG

        /* If the result isn't FP, it better be a compare or cast */

        if  (!(varTypeIsFloating(typ) ||
               tree->OperIsCompare()  || oper == GT_CAST))
            gtDispTree(tree);
        assert(varTypeIsFloating(typ) ||
               tree->OperIsCompare()  || oper == GT_CAST);
#endif

        /* Keep track of how many arguments we're passing */

        fgPtrArgCntCur += argc;

        /* Is this a binary operator? */

        if  (op2)
        {
            /* Add the second operand to the argument count */

            fgPtrArgCntCur += argc; argc *= 2;

            /* What kind of an operator do we have? */

            switch (oper)
            {
            case GT_ADD: helper = CPX_R4_ADD; break;
            case GT_SUB: helper = CPX_R4_SUB; break;
            case GT_MUL: helper = CPX_R4_MUL; break;
            case GT_DIV: helper = CPX_R4_DIV; break;
//              case GT_MOD: helper = CPX_R4_REM; break;

            case GT_EQ : helper = CPX_R4_EQ ; break;
            case GT_NE : helper = CPX_R4_NE ; break;
            case GT_LT : helper = CPX_R4_LT ; break;
            case GT_LE : helper = CPX_R4_LE ; break;
            case GT_GE : helper = CPX_R4_GE ; break;
            case GT_GT : helper = CPX_R4_GT ; break;

            default:
#ifdef  DEBUG
                gtDispTree(tree);
#endif
                assert(!"unexpected FP binary op");
                break;
            }

            args = gtNewArgList(tree->gtOp.gtOp2, tree->gtOp.gtOp1);
        }
        else
        {
            switch (oper)
            {
            case GT_RETURN:
                return tree;

            case GT_CAST:
                assert(!"FP cast");

            case GT_NEG: helper = CPX_R4_NEG; break;

            default:
#ifdef  DEBUG
                gtDispTree(tree);
#endif
                assert(!"unexpected FP unary op");
                break;
            }

            args = gtNewArgList(tree->gtOp.gtOp1);
        }

        /* If we have double result/operands, modify the helper */

        if  (typ == TYP_DOUBLE)
        {
            assert(CPX_R4_NEG+1 == CPX_R8_NEG);
            assert(CPX_R4_ADD+1 == CPX_R8_ADD);
            assert(CPX_R4_SUB+1 == CPX_R8_SUB);
            assert(CPX_R4_MUL+1 == CPX_R8_MUL);
            assert(CPX_R4_DIV+1 == CPX_R8_DIV);

            helper++;
        }
        else
        {
            assert(tree->OperIsCompare());

            assert(CPX_R4_EQ+1 == CPX_R8_EQ);
            assert(CPX_R4_NE+1 == CPX_R8_NE);
            assert(CPX_R4_LT+1 == CPX_R8_LT);
            assert(CPX_R4_LE+1 == CPX_R8_LE);
            assert(CPX_R4_GE+1 == CPX_R8_GE);
            assert(CPX_R4_GT+1 == CPX_R8_GT);
        }

        tree = fgMorphIntoHelperCall(tree, helper, args);

        if  (fgPtrArgCntMax < fgPtrArgCntCur)
            fgPtrArgCntMax = fgPtrArgCntCur;

        fgPtrArgCntCur -= argc;
        return tree;

    case GT_RETURN:

        if  (op1)
        {

#if     RET_64BIT_AS_STRUCTS

            /* Are we returning long/double as a struct? */

#ifdef  DEBUG
            bool        bashed = false;
#endif

            if  (fgRetArgUse)
            {
                var_types       typ = tree->TypeGet();
                var_types       rvt = TYP_REF;

                // ISSUE: The retval arg is not (always) a GC ref!!!!!!!

#ifdef  DEBUG
                bashed = true;
#endif

                /* Convert "op1" to "*retarg = op1 , retarg" */

                GenTreePtr  dst = gtNewOperNode(GT_IND,
                                                typ,
                                                gtNewLclvNode(fgRetArgNum, rvt));

                GenTreePtr  asg = gtNewAssignNode(dst, op1);

                op1 = gtNewOperNode(GT_COMMA,
                                    rvt,
                                    asg,
                                    gtNewLclvNode(fgRetArgNum, rvt));

                /* Update the return value and type */

                tree->gtOp.gtOp1 = op1;
                tree->gtType     = rvt;
            }

//              printf("Return expr:\n"); gtDispTree(op1);

#endif  // RET_64BIT_AS_STRUCTS

#if!TGT_RISC
            if  (compCurBB == genReturnBB)
            {
                /* This is the 'exitCrit' call at the exit label */

                assert(op1->gtType == TYP_VOID);
                assert(op2 == 0);

                tree->gtOp.gtOp1 = op1 = fgMorphTree(op1);

                return tree;
            }
#endif

            /* This is a (real) return value -- check its type */

#ifdef DEBUG
            if (genActualType(op1->TypeGet()) != genActualType(info.compRetType))
            {
                bool allowMismatch = false;

                // Allow TYP_BYREF to be returned as TYP_I_IMPL and vice versa
                if ((info.compRetType == TYP_BYREF &&
                     genActualType(op1->TypeGet()) == TYP_I_IMPL) ||
                    (op1->TypeGet() == TYP_BYREF &&
                     genActualType(info.compRetType) == TYP_I_IMPL))
                    allowMismatch = true;
    
                if (varTypeIsFloating(info.compRetType) && varTypeIsFloating(op1->TypeGet()))
                    allowMismatch = true;

                if (!allowMismatch)
#if     RET_64BIT_AS_STRUCTS
                    if  (!bashed)
#endif
                        NO_WAY("Return type mismatch");
            }
#endif

        }

#if     TGT_RISC

        /* Are we adding a "exitCrit" call to the exit sequence? */

        if  (genMonExitExp)
        {
            /* Is there exactly one return? */

            if  (genReturnCnt == 1)
            {
                /* Can we avoid storing the return value in a temp? */

                if  (!(op1->gtFlags & GTF_GLOB_EFFECT))
                {
                    /* Use "monExit , retval" for the return expression */

                    tree->gtOp.gtOp1 = op1 = gtNewOperNode(GT_COMMA,
                                                           op1->gtType,
                                                           genMonExitExp,
                                                           op1);

                    /* We're done with this exitCrit business */

                    genMonExitExp = NULL;

                    /* Don't forget to morph the entire expression */

                    tree->gtOp.gtOp1 = op1 = fgMorphTree(op1);
                    return tree;
                }
            }

            /* Keep track of how many return statements we've seen */

            genReturnLtm--;
        }

#endif

        break;

    }

NOT_FPH:

#endif

    /* Could this operator throw an exception? */

    if  (tree->OperMayThrow())
    {
        /* Mark the tree node as potentially throwing an exception */
        tree->gtFlags |= GTF_EXCEPT;
    }

    /*-------------------------------------------------------------------------
     * Process the first operand, if any
     */

    if  (op1)
    {

#if LOCAL_ASSERTION_PROP
        // If we are entering the "then" part of a Qmark-Colon we must
        // save the state of the current copy assignment table
        // so that we can restore this state when entering the "else" part
        if (isQmarkColon)
        {
            assert(fgAssertionProp);
            if (optAssertionCount)
            {
                size_t tabSize     = optAssertionCount * sizeof(AssertionDsc);
                origAssertionCount = optAssertionCount;
                origAssertionTab   = (AssertionDsc*) ALLOCA(tabSize);
                memcpy(origAssertionTab, &optAssertionTab, tabSize);
            }
            else
            {
                origAssertionCount = 0;
                origAssertionTab   = NULL;
            }
        }
#endif

        tree->gtOp.gtOp1 = op1 = fgMorphTree(op1);

#if LOCAL_ASSERTION_PROP
        // If we are exiting the "then" part of a Qmark-Colon we must
        // save the state of the current copy assignment table
        // so that we can merge this state with the "else" part exit
        if (isQmarkColon)
        {
            assert(fgAssertionProp);
            if (optAssertionCount)
            {
                size_t tabSize     = optAssertionCount * sizeof(AssertionDsc);
                thenAssertionCount = optAssertionCount;
                thenAssertionTab   = (AssertionDsc*) ALLOCA(tabSize);
                memcpy(thenAssertionTab, &optAssertionTab, tabSize);
            }
            else
            {
                thenAssertionCount = 0;
                thenAssertionTab   = NULL;
            }
        }
#endif

#if     CSELENGTH
        /* For GT_IND, the array length node might have been pointing
           to the array object which was a sub-tree of op1. As op1 might
           have just changed above, find out where to point to now. */

        if ( oper == GT_IND                     &&
            (tree->gtFlags & GTF_IND_RNGCHK)    &&
            tree->gtInd.gtIndLen->gtArrLen.gtArrLenAdr)
        {
            assert(op1->gtType == TYP_BYREF);

            if (op1->gtOper == GT_CNS_INT)
            {
                /* The array pointer arithmetic has been folded to NULL.
                   So just keep a simple dereference (although we want the null check) */
                assert(op1->gtIntCon.gtIconVal == 0);
                tree->gtFlags &= ~GTF_IND_RNGCHK;

                // With GTF_IND_FIELD, we will force the null check. Note that what we're doing 
                // is abusing of this flag. We should have a new flag for this.
                tree->gtFlags |= GTF_IND_FIELD; 
            }
            else
            {
                GenTreePtr  indx;
                GenTreePtr addr = genIsAddrMode(op1, &indx);
                assert(addr && addr->gtType == TYP_REF);

                tree->gtInd.gtIndLen->gtArrLen.gtArrLenAdr = addr;
            }
        }
#endif

        /* Morphing along with folding and inlining may have changed the
         * side effect flags, so we have to reset them
         *
         * NOTE: Don't reset the exception flags on nodes that may throw */

        assert(tree->gtOper != GT_CALL);
        tree->gtFlags &= ~GTF_CALL;

        if  (!tree->OperMayThrow())
            tree->gtFlags &= ~GTF_EXCEPT;

        /* Propagate the new flags */

        tree->gtFlags |= (op1->gtFlags & GTF_GLOB_EFFECT);
    }

    /*-------------------------------------------------------------------------
     * Process the second operand, if any
     */

    if  (op2)
    {

#if LOCAL_ASSERTION_PROP
        // If we are entering the "else" part of a Qmark-Colon we must
        // reset the state of the current copy assignment table
        if (isQmarkColon)
        {
            assert(fgAssertionProp);
            optAssertionReset(0);
            if (origAssertionCount)
            {
                size_t tabSize    = origAssertionCount * sizeof(AssertionDsc);
                memcpy(&optAssertionTab, origAssertionTab, tabSize);
                optAssertionReset(origAssertionCount);
            }
        }
#endif

        tree->gtOp.gtOp2 = op2 = fgMorphTree(op2);

        /* Propagate the new flags */

        tree->gtFlags |= (op2->gtFlags & GTF_GLOB_EFFECT);

#if LOCAL_ASSERTION_PROP
        // If we are exiting the "else" part of a Qmark-Colon we must
        // merge the state of the current copy assignment table with
        // that of the exit of the "then" part.
        if (isQmarkColon)
        {
            assert(fgAssertionProp);
            // If either exit table has zero entries then
            // the merged table also has zero entries
            if (optAssertionCount == 0 || thenAssertionCount == 0)
            {
                optAssertionReset(0);
            }
            else
            {
                size_t tabSize = optAssertionCount * sizeof(AssertionDsc);
                if ( (optAssertionCount != thenAssertionCount) ||
                     (memcmp(thenAssertionTab, &optAssertionTab, tabSize) != 0) )
                {
                    // Yes they are different so we have to find the merged set
                    // Iterate over the copy asgn table removing any entries
                    // that do not have an exact match in the thenAssertionTab
                    unsigned i = 0;
                    while (i < optAssertionCount)
                    {
                        for (unsigned j=0; j < thenAssertionCount; j++)
                        {
                            // Do the left sides match?
                            if ((optAssertionTab[i].op1.lclNum == thenAssertionTab[j].op1.lclNum) &&
                                (optAssertionTab[i].assertion  ==  optAssertionTab[j].assertion))
                            {
                                // Do the right sides match?
                                if ((optAssertionTab[i].op2.type    == thenAssertionTab[j].op2.type) &&
                                    (optAssertionTab[i].op2.lconVal == thenAssertionTab[j].op2.lconVal))
                                {
                                    goto KEEP;
                                }
                                else
                                {
                                    goto REMOVE;
                                }
                            }
                        }
                        //
                        // If we fall out of the loop above then we didn't find
                        // any matching entry in the thenAssertionTab so it must
                        // have been killed on that path so we remove it here
                        //
                    REMOVE:
                        // The data at optAssertionTab[i] is to be removed
#ifdef DEBUG
                        if (verbose)
                        {
                            printf("The QMARK-COLON [%08X] removes assertion candidate #%d\n",
                                   tree, i);
                        }
#endif
                        optAssertionRemove(i);

                        // We will have to redo the i-th iteration
                        continue;
                    KEEP:
                        // The data at optAssertionTab[i] is to be kept
                        i++;
                    }
                }
            }
        }
#endif // LOCAL_ASSERTION_PROP
    }


DONE_MORPHING_CHILDREN:

    /*-------------------------------------------------------------------------
     * Now do POST-ORDER processing
     */

    if (varTypeIsGC(tree->TypeGet()) && (op1 && !varTypeIsGC(op1->TypeGet()))
                                     && (op2 && !varTypeIsGC(op2->TypeGet())))
    {
        // The tree is really not GC but was marked as such. Now that the
        // children have been unmarked, unmark the tree too.

        // Remember that GT_COMMA inherits it's type only from op2
        if (tree->gtOper == GT_COMMA)
            tree->gtType = genActualType(op2->TypeGet());
        else
            tree->gtType = genActualType(op1->TypeGet());
    }

    GenTreePtr oldTree = tree;

    GenTreePtr qmarkOp1 = NULL;    
    GenTreePtr qmarkOp2 = NULL;

    if ((tree->OperGet() == GT_QMARK) &&
        (tree->gtOp.gtOp2->OperGet() == GT_COLON))
    {
        qmarkOp1 = oldTree->gtOp.gtOp2->gtOp.gtOp1;
        qmarkOp2 = oldTree->gtOp.gtOp2->gtOp.gtOp2;
    }

    /* Try to fold it, maybe we get lucky */
    tree = gtFoldExpr(tree);

    if (oldTree != tree) 
    {
        /* if gtFoldExpr returned op1 or op2 then we are done */
        if ((tree == op1) || (tree == op2) || (tree == qmarkOp1) || (tree == qmarkOp2))
            return tree;

        /* If we created a comma-throw tree then we need to morph op1 */
        if (fgIsCommaThrow(tree))
        {
            tree->gtOp.gtOp1 = fgMorphTree(tree->gtOp.gtOp1);
            fgMorphTreeDone(tree);
            return tree;
        }

        return tree;
    }
    else if (tree->OperKind() & GTK_CONST)
    {
        return tree;
    }

    /* gtFoldExpr could have used setOper to change the oper */
    oper    = tree->OperGet();
    typ     = tree->TypeGet();

    /* gtFoldExpr could have changed op1 and op2 */
    op1  = tree->gtOp.gtOp1;
    op2  = tree->gtGetOp2();

    /*-------------------------------------------------------------------------
     * Perform the required oper-specific postorder morphing
     */

    switch (oper)
    {
        GenTreePtr      temp;
        GenTreePtr      addr;
        GenTreePtr      cns1, cns2;
        GenTreePtr      thenNode;
        GenTreePtr      elseNode;
        unsigned        ival1, ival2;

    case GT_ASG:

        // If op1 got folded into a local variable
        if (op1->gtOper == GT_LCL_VAR)
            op1->gtFlags |= GTF_VAR_DEF;

        /* If we are storing a small type, we might be able to omit a cast */
        if ((op1->gtOper == GT_IND) && varTypeIsSmall(op1->TypeGet()))
        {
            if ((op2->gtOper == GT_CAST) &&  !op2->gtOverflow())
            {
                var_types   typ = op2->gtCast.gtCastType;

                // If we are performing a narrowing cast and 
                // typ is larger or the same as op1's type
                // then we can discard the cast.

                if  (varTypeIsSmall(typ) && (typ >= op1->TypeGet()))
                {
                    tree->gtOp.gtOp2 = op2 = op2->gtCast.gtCastOp;
                }
            }
            else if (op2->OperIsCompare() && varTypeIsByte(op1->TypeGet()))
            {
                /* We don't need to zero extend the setcc instruction */
                op2->gtType = TYP_BYTE;
            }
        }        
        // FALL THROUGH

    case GT_ASG_ADD:
    case GT_ASG_SUB:
    case GT_ASG_MUL:
    case GT_ASG_DIV:
    case GT_ASG_MOD:
    case GT_ASG_UDIV:
    case GT_ASG_UMOD:
    case GT_ASG_OR:
    case GT_ASG_XOR:
    case GT_ASG_AND:
    case GT_ASG_LSH:
    case GT_ASG_RSH:
    case GT_ASG_RSZ:

        /* We can't CSE the LHS of an assignment */
        /* We also must set in the pre-morphing phase, otherwise assertionProp doesn't see it */
        op1->gtFlags |= GTF_DONT_CSE;
        break;

    case GT_EQ:
    case GT_NE:

        cns2 = op2;

        /* Check for "(expr +/- icon1) ==/!= (non-zero-icon2)" */

        if  (cns2->gtOper == GT_CNS_INT && cns2->gtIntCon.gtIconVal != 0)
        {
            op1 = tree->gtOp.gtOp1;

            /* Since this can occur repeatedly we use a while loop */

            while ((op1->gtOper == GT_ADD || op1->gtOper == GT_SUB) &&
                   (op1->gtOp.gtOp2->gtOper == GT_CNS_INT)          &&
                   (op1->gtType             == TYP_INT)             &&
                   (op1->gtOverflow()       == false))
            {
                /* Got it; change "x+icon1==icon2" to "x==icon2-icon1" */

                ival1 = op1->gtOp.gtOp2->gtIntCon.gtIconVal;

                if  (op1->gtOper == GT_ADD)
                    ival1 = -ival1;

                cns2->gtIntCon.gtIconVal += ival1;

                op1 = tree->gtOp.gtOp1 = op1->gtOp.gtOp1;
            }
        }

        /* Check for "relOp == 0/1". We can fold alway the "==" or "!=" and directly use relOp */
            
        if ((cns2->gtOper == GT_CNS_INT) &&
            (((unsigned)cns2->gtIntCon.gtIconVal) <= 1U))
        {
            if (op1->gtOper == GT_COMMA)
            {
                // Here we look for the following tree
                //
                //                         EQ
                //                        /  \
                //                     COMMA  CNS_INT 0/1
                //                     /   \
                //                   ASG  LCL_VAR
                //                  /  \
                //           LCL_VAR   RELOP
                //
                // If the LCL_VAR is a temp we can fold the tree away:

                GenTreePtr  asg = op1->gtOp.gtOp1;
                GenTreePtr  lcl = op1->gtOp.gtOp2;
                
                /* Make sure that the left side of the comma is the assignment of the LCL_VAR */
                if (asg->gtOper != GT_ASG)
                    goto SKIP;

                /* The right side of the comma must be a LCL_VAR temp */
                if (lcl->gtOper != GT_LCL_VAR)
                    goto SKIP;

                unsigned lclNum = lcl->gtLclVar.gtLclNum;   assert(lclNum < lvaCount);
                
                /* If the LCL_VAR is not a temp then bail, the temp has a single def */
                if (!lvaTable[lclNum].lvIsTemp)
                    goto SKIP;

                /* We also must be assigning the result of a RELOP */
                if (asg->gtOp.gtOp1->gtOper != GT_LCL_VAR)
                    goto SKIP;
                
                /* Both of the LCL_VAR must match */
                if (asg->gtOp.gtOp1->gtLclVar.gtLclNum != lclNum)
                    goto SKIP;

                /* If right side of asg is not a RELOP then skip */
                if (!asg->gtOp.gtOp2->OperIsCompare())
                    goto SKIP;

                /* Set op1 to the right side of asg, (i.e. the RELOP) */
                op1 = asg->gtOp.gtOp2;

                /* This local variable should never be used again */
                lvaTable[lclNum].lvType = TYP_VOID;

                goto FOLD_RELOP;
            }

            if (op1->OperIsCompare())
            {
                // Here we look for the following tree
                //
                //                         EQ
                //                        /  \
                //                     RELOP  CNS_INT 0/1
                //
FOLD_RELOP:
                assert(op1->OperIsCompare());

                /* Here we reverse the RELOP if necessary */

                if ((cns2->gtIntCon.gtIconVal == 0) == (oper == GT_EQ))
                    op1->SetOper(GenTree::ReverseRelop(op1->OperGet()));

                /* Propagate gtType of tree into op1 in case it is TYP_BYTE for setcc optimization */
                op1->gtType = tree->gtType;

                assert((op1->gtFlags & GTF_RELOP_JMP_USED) == 0);
                op1->gtFlags |= tree->gtFlags & (GTF_RELOP_JMP_USED|GTF_RELOP_QMARK);
                
                DEBUG_DESTROY_NODE(tree);
                return op1;
            }            
        }

SKIP:
        /* Now check for compares with small longs that can be cast to int */

        if  (cns2->gtOper != GT_CNS_LNG)
            goto COMPARE;

        /* Are we comparing against a small const? */

        if  (((long)(cns2->gtLngCon.gtLconVal >> 32) != 0) ||
             (cns2->gtLngCon.gtLconVal & 0x80000000) != 0)
            goto COMPARE;

        /* Is the first comparand mask operation of type long ? */

        if  (op1->gtOper != GT_AND)
        {
            /* Another interesting case: cast from int */

            if  (op1->gtOper                  == GT_CAST &&
                 op1->gtCast.gtCastOp->gtType == TYP_INT &&
                 !op1->gtOverflow())
            {
                /* Simply make this into an integer comparison */

                tree->gtOp.gtOp1 = op1->gtCast.gtCastOp;
                tree->gtOp.gtOp2 = gtNewIconNode((int)cns2->gtLngCon.gtLconVal, TYP_INT);
            }

            goto COMPARE;
        }

        assert(op1->TypeGet() == TYP_LONG && op1->OperGet() == GT_AND);

        /* Is the result of the mask effectively an INT ? */

        addr = op1->gtOp.gtOp2;
        if  (addr->gtOper != GT_CNS_LNG)
            goto COMPARE;
        if  ((long)(addr->gtLngCon.gtLconVal >> 32) != 0)
            goto COMPARE;

        /* Now we know that we can cast gtOp.gtOp1 of AND to int */

        op1->gtOp.gtOp1 = gtNewCastNode(TYP_INT,
                                         op1->gtOp.gtOp1,
                                         TYP_INT);

        /* now replace the mask node (gtOp.gtOp2 of AND node) */

        assert(addr == op1->gtOp.gtOp2);

        ival1 = (long)addr->gtLngCon.gtLconVal;
        addr->SetOper(GT_CNS_INT);
        addr->gtType             = TYP_INT;
        addr->gtIntCon.gtIconVal = ival1;

        /* now bash the type of the AND node */

        op1->gtType = TYP_INT;

        /* finally we replace the comparand */

        ival2 = (long)cns2->gtLngCon.gtLconVal;
        cns2->SetOper(GT_CNS_INT);
        cns2->gtType = TYP_INT;

        assert(cns2 == op2);
        cns2->gtIntCon.gtIconVal = ival2;

        goto COMPARE;

    case GT_LT:
    case GT_LE:
    case GT_GE:
    case GT_GT:

        if ((tree->gtFlags & GTF_UNSIGNED) == 0)
        {
            /* Check for "expr +/- icon1 RELOP non-zero-icon2" */

            if  (op2->gtOper == GT_CNS_INT && op2->gtIntCon.gtIconVal != 0)
            {
                /* Since this can occur repeatedly we use a while loop */

                while ((op1->gtOper == GT_ADD || op1->gtOper == GT_SUB) &&
                       (op1->gtOp.gtOp2->gtOper == GT_CNS_INT)          &&
                       (op1->gtType             == TYP_INT)             &&
                       (op1->gtOverflow()       == false))
                {
                    /* Got it; change "x+icon1 RELOP icon2" to "x RELOP icon2-icon1" */
                    ival1 = op1->gtOp.gtOp2->gtIntCon.gtIconVal;

                    if  (op1->gtOper == GT_ADD)
                        ival1 = -ival1;

                    op2->gtIntCon.gtIconVal += ival1;

                    op1 = tree->gtOp.gtOp1 = op1->gtOp.gtOp1;
                }
            }

            /* Try change an off by one compare to a compare with zero */

            /* Ori points out that this won't work in the following
               unlikely case:
               
               exprB is MAX_INT and we have exprB+1 or
               exprB in MIN_INT and we have exprB-1.

               Its a shame that we can't do this optimazation
            */

            /* Check for "exprA <= exprB + cns" */
            if ( 0 && (op2->gtOper == GT_ADD) && (op2->gtOp.gtOp2->gtOper == GT_CNS_INT))
            {
                cns2 = op2->gtOp.gtOp2;
                goto USE_CNS2;
            }
            /* Check for "expr relop cns" */
            else if (op2->gtOper == GT_CNS_INT)
            {
                cns2 = op2;
USE_CNS2:
                /* Check for "expr relop 1" */
                if (cns2->gtIntCon.gtIconVal == +1)
                {
                    /* Check for "expr >= 1" */
                    if (oper == GT_GE)
                    {
                        /* Change to "expr > 0" */
                        oper = GT_GT;
                        goto SET_OPER;
                    }
                    /* Check for "expr < 1" */
                    else if (oper == GT_LT)
                    {
                        /* Change to "expr <= 0" */
                        oper = GT_LE;
                        goto SET_OPER;
                    }
                }
                /* Check for "expr relop -1" */
                else if ((cns2->gtIntCon.gtIconVal == -1) && ((oper == GT_LE) || (oper == GT_GT)))
                {
                    /* Check for "expr <= -1" */
                    if (oper == GT_LE)
                    {
                        /* Change to "expr < 0" */
                        oper = GT_LT;
                        goto SET_OPER;
                    }
                    /* Check for "expr > -1" */
                    else if (oper == GT_GT)
                    {
                        /* Change to "expr >= 0" */
                        oper = GT_GE;
SET_OPER:
                        tree->SetOper(oper);
                        cns2->gtIntCon.gtIconVal = 0;
                        op2 = tree->gtOp.gtOp2 = gtFoldExpr(op2);
                    }
                }
            }
        }

COMPARE:

        assert(tree->OperKind() & GTK_RELOP);

        /* Check if the result of the comparison is used for a jump
         * If not the only the int (i.e. 32 bit) case is handled in
         * the code generator through the "set" instructions
         * For the rest of the cases we have the simplest way is to
         * "simulate" the comparison with ?:
         *
         * @TODO [CONSIDER] [04/16/01] []: Maybe special code can be added to 
         * genTreeForLong/Float to handle these special cases (e.g. check the FP flags) */

        if ((genActualType(    op1->TypeGet()) == TYP_LONG ||
             varTypeIsFloating(op1->TypeGet()) == true       ) &&
            !(tree->gtFlags & GTF_RELOP_JMP_USED))
        {
            /* We convert it to "(CMP_TRUE) ? (1):(0)" */

            op1             = tree;
            op1->gtFlags |= (GTF_RELOP_JMP_USED | GTF_RELOP_QMARK);

            op2             = gtNewOperNode(GT_COLON, TYP_INT,
                                            gtNewIconNode(0),
                                            gtNewIconNode(1));
            op2  = fgMorphTree(op2);

            tree            = gtNewOperNode(GT_QMARK, TYP_INT, op1, op2);

            fgMorphTreeDone(tree);

            return tree;
        }
        break;

   case GT_QMARK:

        /* If op1 is a comma throw node then we won't be keeping op2 */
        if (fgIsCommaThrow(op1))
            break;

        /* Get hold of the two branches */

        thenNode = op2->gtOp.gtOp1;
        elseNode = op2->gtOp.gtOp2;

        /* If the 'then' branch is empty swap the two branches and reverse the condition */

        if (thenNode->IsNothingNode())
        {
            /* This can only happen for VOID ?: */
            assert(op2->gtType == TYP_VOID);

            /* If the thenNode and elseNode are both nop nodes then optimize away the QMARK */
            if (elseNode->IsNothingNode())
            {
                // We may be able to throw away op1 (unless it has side-effects)

                if ((op1->gtFlags & GTF_SIDE_EFFECT) == 0)
                {
                    /* Just return a a Nop Node */
                    return thenNode;
                }
                else
                {
                    /* Just return the relop, but clear the special flags.  Note
                       that we can't do that for longs and floats (see code under
                       COMPARE label above) */

                    var_types compType = op1->gtOp.gtOp1->TypeGet();
                    if ((genActualType(compType) != TYP_LONG) &&
                        (!varTypeIsFloating(compType)))
                    {
                        op1->gtFlags &= ~(GTF_RELOP_QMARK | GTF_RELOP_JMP_USED);
                        return op1;
                    }
                }
            }
            else
            {
                GenTreePtr tmp = thenNode;

                op2->gtOp.gtOp1 = thenNode = elseNode;
                op2->gtOp.gtOp2 = elseNode = tmp;
                op1->SetOper(GenTree::ReverseRelop(op1->OperGet()));
            }
        }

        // If we have (cond)?0:1, then we just return "cond" for TYP_INTs

        if (genActualType(op1->gtOp.gtOp1->gtType) != TYP_INT ||
            genActualType(typ)                     != TYP_INT)
            break;

        /* Unless both the thenNode and elseNode are constants we bail */
        if (thenNode->gtOper != GT_CNS_INT || elseNode->gtOper != GT_CNS_INT)
            break;

        ival1 = thenNode->gtIntCon.gtIconVal;
        ival2 = elseNode->gtIntCon.gtIconVal;

        // Is one constant 0 and the other 1
        if ((ival1 | ival2) != 1 || (ival1 & ival2) != 0)
            break;

        // If the constants are {1, 0}, reverse the condition
        if (ival1 == 1)
            op1->SetOper(GenTree::ReverseRelop(op1->OperGet()));

        // Unmark GTF_RELOP_JMP_USED on the condition node so it knows that it
        // needs to materialize the result as a 0 or 1.
        assert(op1->gtFlags &   (GTF_RELOP_QMARK | GTF_RELOP_JMP_USED));
               op1->gtFlags &= ~(GTF_RELOP_QMARK | GTF_RELOP_JMP_USED);

        DEBUG_DESTROY_NODE(tree);
        DEBUG_DESTROY_NODE(op2);

        return op1;


    case GT_MUL:

#if!LONG_MATH_REGPARAM
        if (typ == TYP_LONG)
        {
            // This must be GTF_MUL_64RSLT
            assert(tree->gtIsValid64RsltMul());
            return tree;
        }
#endif
        goto CM_OVF_OP;

    case GT_SUB:

        if (tree->gtOverflow())
            goto CM_OVF_OP;

        /* Check for "op1 - cns2" , we change it to "op1 + (-cns2)" */

        assert(op2);
        if  (op2->OperIsConst() && op2->gtType == TYP_INT)
        {
            /* Negate the constant and change the node to be "+" */

            op2->gtIntCon.gtIconVal = -op2->gtIntCon.gtIconVal;
            assert(op2->gtIntCon.gtIconVal != 0);    // This should get folded in gtFoldExprSpecial
            oper = GT_ADD;
            tree->ChangeOper(oper);
            goto CM_ADD_OP;
        }

        /* Check for "cns1 - op2" , we change it to "(cns1 + (-op2))" */

        assert(op1);
        if  (op1->OperIsConst() && op1->gtType == TYP_INT)
        {
            tree->gtOp.gtOp2 = op2 = gtNewOperNode(GT_NEG, genActualType(op2->gtType), op2);
            fgMorphTreeDone(op2);

            oper = GT_ADD;
            tree->ChangeOper(oper);
            goto CM_ADD_OP;
        }

        /* No match - exit */

        break;

    case GT_ADD:

CM_OVF_OP:
        if (tree->gtOverflow())
        {
            // Add the excptn-throwing basic block to jump to on overflow

            fgAddCodeRef(compCurBB, compCurBB->bbTryIndex, ACK_OVERFLOW, fgPtrArgCntCur);

            // We cant do any commutative morphing for overflow instructions

            break;
        }

CM_ADD_OP:

    case GT_OR:
    case GT_XOR:
    case GT_AND:

        /* Commute any non-REF constants to the right */

        assert(op1);
        if  (op1->OperIsConst() && (op1->gtType != TYP_REF))
        {
            /* Any constant cases should have been folded earlier */
            assert(!op2->OperIsConst());

            /* Swap the operands */
            assert((tree->gtFlags & GTF_REVERSE_OPS) == 0);

            tree->gtOp.gtOp1 = op2;
            tree->gtOp.gtOp2 = op1;

            op1 = op2;
            op2 = tree->gtOp.gtOp2;
        }

        /* See if we can fold GT_ADD nodes. */

        if (oper == GT_ADD)
        {
            /* Fold "((x+icon1)+(y+icon2)) to ((x+y)+(icon1+icon2))" */

            if (op1->gtOper             == GT_ADD     && 
                op2->gtOper             == GT_ADD     &&
                op1->gtOp.gtOp2->gtOper == GT_CNS_INT && 
                op2->gtOp.gtOp2->gtOper == GT_CNS_INT &&
                !op1->gtOverflow()                    &&
                !op2->gtOverflow()                       )
            {
                cns1 = op1->gtOp.gtOp2;
                cns2 = op2->gtOp.gtOp2;
                cns1->gtIntCon.gtIconVal += cns2->gtIntCon.gtIconVal;
                tree->gtOp.gtOp2    = cns1;
                DEBUG_DESTROY_NODE(cns2);

                op1->gtOp.gtOp2     = op2->gtOp.gtOp1;
                op1->gtFlags       |= (op1->gtOp.gtOp2->gtFlags & GTF_GLOB_EFFECT);
                DEBUG_DESTROY_NODE(op2);
                op2 = tree->gtOp.gtOp2;
            }
            
            if ((op2->gtOper == GT_CNS_INT) && varTypeIsI(typ))
            {
                /* Fold "((x+icon1)+icon2) to (x+(icon1+icon2))" */

                if (op1->gtOper == GT_ADD                 && 
                    op1->gtOp.gtOp2->gtOper == GT_CNS_INT && 
                    !op1->gtOverflow()                       )
                {
                    cns1                     = op1->gtOp.gtOp2;
                    op2->gtIntCon.gtIconVal += cns1->gtIntCon.gtIconVal;
                    DEBUG_DESTROY_NODE(cns1);

                    tree->gtOp.gtOp1         = op1->gtOp.gtOp1;
                    DEBUG_DESTROY_NODE(op1);
                    op1                      = tree->gtOp.gtOp1;
                }
                
                /* Fold "x+0 to x" */

                if (op2->gtIntCon.gtIconVal == 0)
                {
                    if (varTypeIsGC(op2->TypeGet()))
                    {
                        op2->gtType = tree->gtType;
                        DEBUG_DESTROY_NODE(op1);
                        DEBUG_DESTROY_NODE(tree);
                        return op2;
                    }
                    else
                    {
                        DEBUG_DESTROY_NODE(op2);
                        DEBUG_DESTROY_NODE(tree);
                        return op1;                 // Just return op1
                    }
                }
            }
        }
             /* See if we can fold GT_MUL by const nodes */
        else if (oper == GT_MUL && op2->gtOper == GT_CNS_INT)
        {
            assert(typ <= TYP_UINT);
            assert(!tree->gtOverflow());

            int mult = op2->gtIntCon.gtIconVal;
            if (mult == 0)
            {
                // We may be able to throw away op1 (unless it has side-effects)

                if ((op1->gtFlags & GTF_SIDE_EFFECT) == 0)
                {
                    DEBUG_DESTROY_NODE(op1);
                    DEBUG_DESTROY_NODE(tree);
                    return op2; // Just return the "0" node
                }

                // We need to keep op1 for the side-effects. Hang it off
                // a GT_COMMA node

                tree->ChangeOper(GT_COMMA);
                return tree;
            }

            unsigned abs_mult = (mult >= 0) ? mult : -mult;

                // is it a power of two? (positive or negative)
            if  (abs_mult == genFindLowestBit(abs_mult)) 
            {
                    // if negative negate (min-int does not need negation)
                if (mult < 0 && mult != 0x80000000)
                {
                    tree->gtOp.gtOp1 = op1 = gtNewOperNode(GT_NEG, op1->gtType, op1);
                    fgMorphTreeDone(op1);
                }

                if (abs_mult == 1) 
                {
                    DEBUG_DESTROY_NODE(op2);
                    DEBUG_DESTROY_NODE(tree);
                    return op1;
                }

                /* Change the multiplication into a shift by log2(val) bits */
                 op2->gtIntCon.gtIconVal = genLog2(abs_mult);
                oper = GT_LSH;
                tree->ChangeOper(oper);
                goto DONE_MORPHING_CHILDREN;
            }
        }
        break;

    case GT_CHS:
    case GT_NOT:
    case GT_NEG:

        /* Any constant cases should have been folded earlier */
        assert(!op1->OperIsConst());
        break;

    case GT_CKFINITE:

        assert(varTypeIsFloating(op1->TypeGet()));

        fgAddCodeRef(compCurBB, compCurBB->bbTryIndex, ACK_ARITH_EXCPN, fgPtrArgCntCur);
        break;

    case GT_IND:

        if  (tree->gtFlags & GTF_IND_RNGCHK)
        {
            assert(tree->gtInd.gtIndLen);

            fgSetRngChkTarget(tree);
        }

        /* Fold *(&X) into X */

        if (op1->gtOper == GT_ADDR)
        {
            temp = op1->gtOp.gtOp1;         // X

            if (typ == temp->TypeGet())
            {
                // Keep the DONT_CSE flag in sync 
                // (as the addr always marks it for its op1)
                temp->gtFlags &= ~GTF_DONT_CSE;
                temp->gtFlags |= (tree->gtFlags & GTF_DONT_CSE);

                DEBUG_DESTROY_NODE(tree);   // GT_IND
                DEBUG_DESTROY_NODE(op1);    // GT_ADDR                

                return temp;
            }
            else if (temp->OperGet() == GT_LCL_VAR)
            {
                ival1 = 0;
                goto LCL_FLD;
            }

        }

        /* Change *(&lcl + cns) into lcl[cns] to prevent materialization of &lcl */

        if (op1->OperGet() == GT_ADD &&
            op1->gtOp.gtOp1->OperGet() == GT_ADDR &&
            op1->gtOp.gtOp2->OperGet() == GT_CNS_INT)
        {
            // Cant have range check in local array
            assert((tree->gtFlags & GTF_IND_RNGCHK) == 0);
            // No overflow arithmetic with pointers
            assert(!op1->gtOverflow());

            temp = op1->gtOp.gtOp1->gtOp.gtOp1;
            if (temp->OperGet() != GT_LCL_VAR)
                break;

            ival1 = op1->gtOp.gtOp2->gtIntCon.gtIconVal;

LCL_FLD:
            assert(temp->gtOper == GT_LCL_VAR);

            // The emitter cant handle large offsets
            if (ival1 != (unsigned short)ival1)
                break;

            unsigned lclNum = temp->gtLclVar.gtLclNum;
            assert(lvaTable[lclNum].lvAddrTaken ||
                   genTypeSize(lvaTable[lclNum].TypeGet()) == 0);

            temp->ChangeOper(GT_LCL_FLD);
            temp->gtLclFld.gtLclOffs    = ival1;
            temp->gtType                = tree->gtType;
            temp->gtFlags              |= (tree->gtFlags & GTF_DONT_CSE);

            assert(op1->gtOper == GT_ADD || op1->gtOper == GT_ADDR);
            if (op1->OperGet() == GT_ADD)
            {
                DEBUG_DESTROY_NODE(op1->gtOp.gtOp1);    // GT_ADDR
                DEBUG_DESTROY_NODE(op1->gtOp.gtOp2);    // GT_CNS_INT
            }
            DEBUG_DESTROY_NODE(op1);                    // GT_ADD or GT_ADDR
            DEBUG_DESTROY_NODE(tree);                   // GT_IND

            return temp;
        }

        break;

    case GT_ADDR:

        /* @TODO [CONSIDER] [04/16/01] []:
           For GT_ADDR(GT_IND(ptr)) (typically created by
           ldflda), we perform a null-ptr check on 'ptr'
           during codegen. We could hoist these for
           consecutive ldflda on the same object.
         */
        if (op1->OperGet() == GT_IND && !(op1->gtFlags & (GTF_IND_RNGCHK | GTF_IND_FIELD)))
        {
            GenTreePtr addr = op1->gtInd.gtIndOp1;
            assert(varTypeIsGC(addr->gtType) || addr->gtType == TYP_I_IMPL);

            // obj+offset created for GT_FIELDs are incorrectly marked
            // as TYP_REFs. So we need to bash the type.  Don't do this for
            // nodes that are GT_LCL_VAR, as this is a special case where the
            // negative out-of-range index has cancelled out the length and class
            // pointer fields, and what we have here is a ref pointer to the 
            // array object.
            if ((addr->gtType == TYP_REF) && (addr->gtOper != GT_LCL_VAR))
                addr->gtType = TYP_BYREF;

            DEBUG_DESTROY_NODE(tree);
            return addr;
        }
        else if (op1->gtOper == GT_CAST)
        {
            GenTreePtr casting = op1->gtCast.gtCastOp;
            if (casting->gtOper == GT_LCL_VAR || casting->gtOper == GT_CLS_VAR)
            {
                DEBUG_DESTROY_NODE(op1);
                tree->gtOp.gtOp1 = op1 = casting;
            }
        }

        /* Must set in the pre-morphing phase, otherwise assertionProp doesn't see it */
        assert((op1->gtFlags & GTF_DONT_CSE) != 0);
        break;

    case GT_COLON:
        if (fgGlobalMorph)
        {
            /* Mark the nodes that are conditionally executed */
            fgWalkTreePre(tree, gtMarkColonCond);
        }
        /* Since we're doing this postorder we clear this if it got set by a child */
        fgRemoveRestOfBlock = false;
        break;

    case GT_COMMA:

        /* Special case: trees that don't produce a value */
        if  ((op2->OperKind() & GTK_ASGOP) ||
             (op2->OperGet() == GT_COMMA && op2->TypeGet() == TYP_VOID) ||
             fgIsThrow(op2))
        {
            typ = tree->gtType = TYP_VOID;
        }

        /* If the left operand is worthless, throw it away */
        if  (!(op1->gtFlags & GTF_GLOB_EFFECT))
        {
            DEBUG_DESTROY_NODE(tree);
            DEBUG_DESTROY_NODE(op1);
            return op2;
        }
        /* If the right operand is just a void nop node, throw it away */
        if  (op2->IsNothingNode() && op1->gtType == TYP_VOID)
        {
            DEBUG_DESTROY_NODE(tree);
            DEBUG_DESTROY_NODE(op2);
            return op1;
        }

        /* Try to convert void qmark colon constructs into typed constructs */

        if ((op1->OperGet()             == GT_QMARK) &&
            (op1->TypeGet()             == TYP_VOID) &&
            (op1->gtOp.gtOp2->OperGet() == GT_COLON) &&
            (varTypeIsI(op2->TypeGet())))
        {
            GenTreePtr qmark  = op1;
            GenTreePtr var    = op2;
            GenTreePtr colon  = qmark->gtOp.gtOp2;
            GenTreePtr asg1   = colon->gtOp.gtOp1;
            GenTreePtr asg2   = colon->gtOp.gtOp2;

            if ((asg1->OperGet() == GT_ASG) &&
                (asg2->OperGet() == GT_ASG) &&
                (GenTree::Compare(var, asg1->gtOp.gtOp1)) &&
                (GenTree::Compare(var, asg2->gtOp.gtOp1)))
            {
                qmark->gtType = var->gtType;
                colon->gtType = var->gtType;
                colon->gtOp.gtOp1 = asg1->gtOp.gtOp2;
                colon->gtOp.gtOp2 = asg2->gtOp.gtOp2;
                return qmark;
            }
        }
        break;

    case GT_JTRUE:

        /* Special case if fgRemoveRestOfBlock is set to true */
        if (fgRemoveRestOfBlock)
        {
            if (fgIsCommaThrow(op1, true))
            {
                GenTreePtr throwNode = op1->gtOp.gtOp1;
                assert(throwNode->gtType == TYP_VOID);

                return throwNode;
            }

            assert(op1->OperKind() & GTK_RELOP);
            assert(op1->gtFlags    & GTF_EXCEPT);

            // We need to keep op1 for the side-effects. Hang it off
            // a GT_COMMA node

            tree->ChangeOper(GT_COMMA);
            tree->gtOp.gtOp2 = op2 = gtNewNothingNode();

            // Additionally since we're eliminating the JTRUE 
            // codegen won't like it if op1 is a RELOP of longs, floats or doubles.
            // So we bash it into a GT_COMMA as well.
            op1->ChangeOper(GT_COMMA);
            op1->gtType = op1->gtOp.gtOp1->gtType;

            return tree;
        }
    }

    assert(oper == tree->gtOper);

    if ((oper != GT_ASG) && (oper != GT_COLON) && (oper != GT_LIST))
    {
        /* Check for op1 as a GT_COMMA with a unconditional throw node */
        if (op1 && fgIsCommaThrow(op1, true))
        {
            if ((op1->gtFlags & GTF_COLON_COND) == 0)
            {
                /* We can safely throw out the rest of the statements */
                fgRemoveRestOfBlock = true;
            }

            GenTreePtr throwNode = op1->gtOp.gtOp1;
            assert(throwNode->gtType == TYP_VOID);

            if (oper == GT_COMMA)
            {
                /* Both tree and op1 are GT_COMMA nodes */
                /* Change the tree's op1 to the throw node: op1->gtOp.gtOp1 */
                tree->gtOp.gtOp1 = throwNode;
                return tree;
            }
            else if (oper != GT_NOP)
            {
                if (genActualType(typ) == genActualType(op1->gtType))
                {
                    /* The types match so, return the comma throw node as the new tree */
                    return op1;
                }
                else
                {
                    if (typ == TYP_VOID)
                    {
                        // Return the throw node
                        return throwNode;
                    }
                    else
                    {
                        GenTreePtr commaOp2 = op1->gtOp.gtOp2;

                        // need type of oper to be same as tree
                        if (typ == TYP_LONG)
                        {
                            commaOp2->ChangeOperConst(GT_CNS_LNG);
                            commaOp2->gtLngCon.gtLconVal = 0;
                            /* Bash the types of oper and commaOp2 to TYP_LONG */
                            op1->gtType = commaOp2->gtType = TYP_LONG;
                        }
                        else if (varTypeIsFloating(typ))
                        {
                            commaOp2->ChangeOperConst(GT_CNS_DBL);
                            commaOp2->gtDblCon.gtDconVal = 0.0;
                            /* Bash the types of oper and commaOp2 to TYP_DOUBLE */
                            op1->gtType = commaOp2->gtType = TYP_DOUBLE;
                        }
                        else
                        {
                            commaOp2->ChangeOperConst(GT_CNS_INT);
                            commaOp2->gtIntCon.gtIconVal = 0;
                            /* Bash the types of oper and commaOp2 to TYP_INT */
                            op1->gtType = commaOp2->gtType = TYP_INT;
                        }

                        /* Return the GT_COMMA node as the new tree */
                        return op1;
                    }                    
                }
            }
        }

        /* Check for op2 as a GT_COMMA with a unconditional throw */
        
        if (op2 && fgIsCommaThrow(op2, true))
        {
            if ((op2->gtFlags & GTF_COLON_COND) == 0)
            {
                /* We can safely throw out the rest of the statements */
                fgRemoveRestOfBlock = true;
            }

                // If op1 has no side-effects
                if ((op1->gtFlags & GTF_GLOB_EFFECT) == 0)
                {
                // If tree is an asg node
                if (tree->OperIsAssignment())
                {
                    /* Return the throw node as the new tree */
                    return op2->gtOp.gtOp1;
                }

                /* for the shift nodes the type of op2 can differ from the tree type */
                if ((typ == TYP_LONG) && (genActualType(op2->gtType) == TYP_INT))
                {
                    assert((oper == GT_LSH) || (oper == GT_RSH) || (oper == GT_RSZ));

                    GenTreePtr commaOp2 = op2->gtOp.gtOp2;

                    commaOp2->ChangeOperConst(GT_CNS_LNG);
                    commaOp2->gtLngCon.gtLconVal = 0;

                    /* Bash the types of oper and commaOp2 to TYP_LONG */
                    op2->gtType = commaOp2->gtType = TYP_LONG;
                }

                if ((typ == TYP_INT) && (genActualType(op2->gtType) == TYP_LONG ||
                                         varTypeIsFloating(op2->TypeGet())))
                {
                    assert(tree->OperIsCompare());
                
                    GenTreePtr commaOp2 = op2->gtOp.gtOp2;

                    commaOp2->ChangeOperConst(GT_CNS_INT);
                    commaOp2->gtIntCon.gtIconVal = 0;

                    /* Bash the types of oper and commaOp2 to TYP_INT */
                    op2->gtType = commaOp2->gtType = TYP_INT;
                }

                /* types should now match */
                assert( (genActualType(typ) == genActualType(op2->gtType)));

                /* Return the GT_COMMA node as the new tree */
                return op2;
            }
        }
    }

    /*-------------------------------------------------------------------------
     * Optional morphing is done if tree transformations is permitted
     */

    if  ((opts.compFlags & CLFLG_TREETRANS) == 0)
        return tree;

    if  (GenTree::OperIsCommutative(oper))
    {
        /* Swap the operands so that the more expensive one is 'op1' */

        if  (tree->gtFlags & GTF_REVERSE_OPS)
        {
            tree->gtOp.gtOp1 = op2;
            tree->gtOp.gtOp2 = op1;

            op2 = op1;
            op1 = tree->gtOp.gtOp1;

            tree->gtFlags &= ~GTF_REVERSE_OPS;
        }

        if (oper == op2->gtOper)
        {
            /*  Reorder nested operators at the same precedence level to be
                left-recursive. For example, change "(a+(b+c))" to the
                equivalent expression "((a+b)+c)".
             */

            /* Things are handled differently for floating-point operators */

            if  (varTypeIsFloating(tree->TypeGet()))
            {
                /* Are we supposed to preserve float operand order? */

                if  (!genOrder)
                {
                    // @TODO [CONSIDER] [04/16/01] []:
                    // reorder operands if profitable (floats are
                    // from integers, BTW)
                }
            }
            else
            {
                fgMoveOpsLeft(tree);
                op1 = tree->gtOp.gtOp1;
                op2 = tree->gtOp.gtOp2;
            }
        }

    }

#if REARRANGE_ADDS

    /* Change "((x+icon)+y)" to "((x+y)+icon)" 
       Don't reorder floating-point operations */

    if  ((oper        == GT_ADD) && !tree->gtOverflow() &&
         (op1->gtOper == GT_ADD) && ! op1->gtOverflow() && varTypeIsI(typ))
    {
        GenTreePtr      ad2 = op1->gtOp.gtOp2;

        if  (op2->OperIsConst() == 0 &&
             ad2->OperIsConst() != 0)
        {
            tree->gtOp.gtOp2 = ad2;

            op1 ->gtOp.gtOp2 = op2;
            op1->gtFlags    |= op2->gtFlags & GTF_GLOB_EFFECT;

            op2 = tree->gtOp.gtOp2;
        }
    }

#endif

    /*-------------------------------------------------------------------------
     * Perform optional oper-specific postorder morphing
     */

    switch (oper)
    {
        genTreeOps      cmop;
        bool            dstIsSafeLclVar;

    case GT_ASG:

        /* We'll convert "a = a <op> x" into "a <op>= x"                     */
        /*     and also  "a = x <op> a" into "a <op>= x" for communative ops */

#if !LONG_ASG_OPS
        if  (typ == TYP_LONG)
            break;
#endif

        /* Make sure we're allowed to do this */

        /* Are we assigning to a GT_LCL_VAR ? */

        dstIsSafeLclVar = (op1->gtOper == GT_LCL_VAR);

        /* If we have a GT_LCL_VAR, then is the address taken? */
        if (dstIsSafeLclVar)
        {
            unsigned     lclNum = op1->gtLclVar.gtLclNum;
            LclVarDsc *  varDsc = lvaTable + lclNum;

            assert(lclNum < lvaCount);

            /* Is the address taken? */

            if  (varDsc->lvAddrTaken)
            {
                dstIsSafeLclVar = false;
            }
            else if (op2->gtFlags & GTF_ASG)
            {
#if 0
                /* @TODO [REVISIT] [04/16/01] []: Must walk op2 to see if it has an assignment of op1 */
                fgWalkTreePreReEnter();
                if  (fgWalkTreePre(op2, optIsVarAssgCB, &desc))
                    break;
                fgWalkTreePreRestore();
#else
                break;
#endif
            }
        }

        if (!dstIsSafeLclVar)
        {
            if (op2->gtFlags & GTF_ASG)
                break;

            if  ((op2->gtFlags & GTF_CALL) && (op1->gtFlags & GTF_GLOB_EFFECT))
                break;
        }

        /* Special case: a cast that can be thrown away */

        if  (op1->gtOper == GT_IND  &&
             op2->gtOper == GT_CAST &&
             !op2->gtOverflow()      )
        {
            var_types       srct;
            var_types       cast;
            var_types       dstt;

            srct =             op2->gtCast.gtCastOp->TypeGet();
            cast = (var_types) op2->gtCast.gtCastType;
            dstt =             op1->TypeGet();

            /* Make sure these are all ints and precision is not lost */

            if  (cast >= dstt && dstt <= TYP_INT && srct <= TYP_INT)
                op2 = tree->gtOp.gtOp2 = op2->gtCast.gtCastOp;
        }

        /* Make sure we have the operator range right */

        assert(GT_SUB == GT_ADD + 1);
        assert(GT_MUL == GT_ADD + 2);
        assert(GT_DIV == GT_ADD + 3);
        assert(GT_MOD == GT_ADD + 4);
        assert(GT_UDIV== GT_ADD + 5);
        assert(GT_UMOD== GT_ADD + 6);

        assert(GT_OR  == GT_ADD + 7);
        assert(GT_XOR == GT_ADD + 8);
        assert(GT_AND == GT_ADD + 9);

        assert(GT_LSH == GT_ADD + 10);
        assert(GT_RSH == GT_ADD + 11);
        assert(GT_RSZ == GT_ADD + 12);

        /* Check for a suitable operator on the RHS */

        cmop = op2->OperGet();

        switch (cmop)
        {
        case GT_NEG:
            // @TODO [CONSIDER] [04/16/01] []: supporting GT_CHS for floating point types
            // GT_CHS only supported for integer types
            if  ( varTypeIsFloating(tree->TypeGet()))
                break;

            goto ASG_OP;

        case GT_MUL:
            // @TODO [CONSIDER] [04/16/01] []: supporting GT_ASG_MUL for integer types
            // GT_ASG_MUL only supported for floating point types
            if  (!varTypeIsFloating(tree->TypeGet()))
                break;

            // Fall through

        case GT_ADD:
        case GT_SUB:
            if (op2->gtOverflow())
            {
                /* Disable folding into "<op>=" if the result can be
                   visible to anyone as <op> may throw an exception and
                   the assignment should not proceed
                   We are safe with an assignmnet to a local variables
                 */
                if (compCurBB->hasTryIndex())
                    break;
                if (!dstIsSafeLclVar)
                    break;
            }
#if TGT_x86
            // This is hard for byte-operations as we need to make
            // sure both operands are in RBM_BYTE_REGS.
            if (varTypeIsByte(op2->TypeGet()))
                break;
#endif
            goto ASG_OP;

        case GT_DIV:
        case GT_UDIV:
            // GT_ASG_DIV only supported for floating point types
            if  (!varTypeIsFloating(tree->TypeGet()))
                break;

        case GT_LSH:
        case GT_RSH:
        case GT_RSZ:

#if LONG_ASG_OPS

            if  (typ == TYP_LONG)
                break;
#endif

        case GT_OR:
        case GT_XOR:
        case GT_AND:

#if LONG_ASG_OPS

            /* UNDONE: allow non-const long assignment operators */

            if  (typ == TYP_LONG && op2->gtOp.gtOp2->gtOper != GT_CNS_LNG)
                break;
#endif

        ASG_OP:

            /* Is the destination identical to the first RHS sub-operand? */

            if  (GenTree::Compare(op1, op2->gtOp.gtOp1))
            {
                /* Special case: "x |= -1" and "x &= 0" */

                if  (cmop == GT_AND || cmop == GT_OR)
                {
                    if  (op2->gtOp.gtOp2->gtOper == GT_CNS_INT)
                    {
                        long        icon = op2->gtOp.gtOp2->gtIntCon.gtIconVal;

                        assert(typ <= TYP_UINT);

                        if  ((cmop == GT_AND && icon == 0) ||
                             (cmop == GT_OR  && icon == -1))
                        {
                            /* Simply change to an assignment */

                            tree->gtOp.gtOp2 = op2->gtOp.gtOp2;
                            break;
                        }
                    }
                }

                if  (cmop == GT_NEG)
                {
                    /* This is "x = -x;", use the flipsign operator */

                    tree->ChangeOper  (GT_CHS);

                    if  (op1->gtOper == GT_LCL_VAR)
                        op1->gtFlags |= GTF_VAR_USEASG;

                    tree->gtOp.gtOp2 = gtNewIconNode(0);

                    break;
                }

            ASGOP:

                if (cmop == GT_RSH && varTypeIsSmall(op1->TypeGet()) && varTypeIsUnsigned(op1->TypeGet()))
                {
                    // Changing from x = x op y to x op= y when x is a small integer type
                    // makes the op size smaller (originally the op size was 32 bits, after
                    // sign or zero extension of x, and there is an implicit truncation in the
                    // assignment).
                    // This is ok in most cases because the upper bits were
                    // lost when assigning the op result to a small type var, 
                    // but it may not be ok for the right shift operation where the higher bits
                    // could be shifted into the lower bits and preserved.
                    // Signed right shift of signed x still works (i.e. (sbyte)((int)(sbyte)x >>signed y) == (sbyte)x >>signed y))
                    // as do unsigned right shift ((ubyte)((int)(ubyte)x >>unsigned y) == (ubyte)x >>unsigned y), but
                    // signed right shift of an unigned small type may give the wrong result:
                    // e.g. (ubyte)((int)(ubyte)0xf0 >>signed 4) == 0x0f,
                    // but  (ubyte)0xf0 >>signed 4 == 0xff which is incorrect.
                    // The result becomes correct if we use >>unsigned instead of >>signed.
                    assert(op1->TypeGet() == op2->gtOp.gtOp1->TypeGet());
                    cmop = GT_RSZ;
                }

                /* Replace with an assignment operator */

                assert(GT_ADD - GT_ADD == GT_ASG_ADD - GT_ASG_ADD);
                assert(GT_SUB - GT_ADD == GT_ASG_SUB - GT_ASG_ADD);
                assert(GT_OR  - GT_ADD == GT_ASG_OR  - GT_ASG_ADD);
                assert(GT_XOR - GT_ADD == GT_ASG_XOR - GT_ASG_ADD);
                assert(GT_AND - GT_ADD == GT_ASG_AND - GT_ASG_ADD);
                assert(GT_LSH - GT_ADD == GT_ASG_LSH - GT_ASG_ADD);
                assert(GT_RSH - GT_ADD == GT_ASG_RSH - GT_ASG_ADD);
                assert(GT_RSZ - GT_ADD == GT_ASG_RSZ - GT_ASG_ADD);

                tree->SetOper((genTreeOps)(cmop - GT_ADD + GT_ASG_ADD));

                tree->gtOp.gtOp2 = op2->gtOp.gtOp2;

                /* Propagate GTF_OVERFLOW */

                if (op2->gtOverflowEx())
                {
                    tree->gtType   =  op2->gtType;
                    tree->gtFlags |= (op2->gtFlags &
                                     (GTF_OVERFLOW|GTF_EXCEPT|GTF_UNSIGNED));
                }

                DEBUG_DESTROY_NODE(op2);

                op2 = tree->gtOp.gtOp2;

                /* The target is used as well as being defined */

                if  (op1->gtOper == GT_LCL_VAR)
                    op1->gtFlags |= GTF_VAR_USEASG;

#if CPU_HAS_FP_SUPPORT
                /* Check for the special case "x += y * x;" */

                // @TODO [CONSIDER] [04/16/01] []: supporting GT_ASG_MUL for integer types
                // GT_ASG_MUL only supported for floating point types

                if (cmop != GT_ADD && cmop != GT_SUB)
                    break;

                if  (op2->gtOper == GT_MUL && varTypeIsFloating(tree->TypeGet()))
                {
                    if      (GenTree::Compare(op1, op2->gtOp.gtOp1))
                    {
                        /* Change "x += x * y" into "x *= (y + 1)" */

                        op2 = op2->gtOp.gtOp2;
                    }
                    else if (GenTree::Compare(op1, op2->gtOp.gtOp2))
                    {
                        /* Change "x += y * x" into "x *= (y + 1)" */

                        op2 = op2->gtOp.gtOp1;
                    }
                    else
                        break;

                    op1 = gtNewDconNode(1.0);
                    
                    /* Now make the "*=" node */

                    if (cmop == GT_ADD)
                    {
                        /* Change "x += x * y" into "x *= (y + 1)" */

                        tree->gtOp.gtOp2 = op2 = gtNewOperNode(GT_ADD,
                                                         tree->TypeGet(),
                                                         op2,
                                                         op1);
                    }
                    else
                    {
                        /* Change "x -= x * y" into "x *= (1 - y)" */

                        assert(cmop == GT_SUB);
                        tree->gtOp.gtOp2 = op2 = gtNewOperNode(GT_SUB,
                                                         tree->TypeGet(),
                                                         op1,
                                                         op2);
                    }
                    tree->ChangeOper(GT_ASG_MUL);
                }
#endif // CPU_HAS_FP_SUPPORT
            }
            else
            {
                /* Check for "a = x <op> a" */

                /* Should we be doing this at all? */
                if  ((opts.compFlags & CLFLG_TREETRANS) == 0)
                    break;

                /* For commutative ops only ... */

                if (cmop != GT_ADD && cmop != GT_MUL && 
                    cmop != GT_AND && cmop != GT_OR && cmop != GT_XOR)
                {
                    break;
                }

                /* Can we swap the operands to cmop ... */

                if ((op2->gtOp.gtOp1->gtFlags & GTF_GLOB_EFFECT) &&
                    (op2->gtOp.gtOp2->gtFlags & GTF_GLOB_EFFECT)    )
                {
                    // Both sides must have side effects to prevent swap */
                    break;
                }

                /* Is the destination identical to the second RHS sub-operand? */
                if  (GenTree::Compare(op1, op2->gtOp.gtOp2))
                {
                    // We will transform this from "a = x <op> a" to "a <op>= x"
                    // so we can now destroy the duplicate "a"

                    DEBUG_DESTROY_NODE(op2->gtOp.gtOp2);
                    op2->gtOp.gtOp2 = op2->gtOp.gtOp1; 

                    goto ASGOP;
                }
            }

            break;

        case GT_NOT:

            /* Is the destination identical to the first RHS sub-operand? */

            if  (GenTree::Compare(op1, op2->gtOp.gtOp1))
            {
                /* This is "x = ~x" which is the same as "x ^= -1"
                 * Transform the node into a GT_ASG_XOR */

                assert(genActualType(typ) == TYP_INT ||
                       genActualType(typ) == TYP_LONG);

                op2->gtOp.gtOp2 = (genActualType(typ) == TYP_INT)
                                    ? gtNewIconNode(-1)
                                    : gtNewLconNode(-1);

                cmop = GT_XOR;
                goto ASGOP;
            }

            break;
        }

        break;

    case GT_MUL:

        /* Check for the case "(val + icon) * icon" */

        if  (op2->gtOper == GT_CNS_INT &&
             op1->gtOper == GT_ADD)
        {
            GenTreePtr  add = op1->gtOp.gtOp2;

            if  (add->gtOper == GT_CNS_INT && op2->IsScaleIndexMul())
            {
                if (tree->gtOverflow() || op1->gtOverflow())
                    break;

                long        imul = op2->gtIntCon.gtIconVal;
                long        iadd = add->gtIntCon.gtIconVal;

                /* Change '(val+icon1)*icon2' -> '(val*icon2)+(icon1*icon2)' */

                oper         = GT_ADD;
                tree->ChangeOper(oper);

                op2->gtIntCon.gtIconVal = iadd * imul;

                op1->ChangeOper(GT_MUL);

                add->gtIntCon.gtIconVal = imul;
            }
        }

        break;

    case GT_DIV:

        /* For "val / 1", just return "val" */

        if  (op2->gtOper == GT_CNS_INT &&
             op2->gtIntCon.gtIconVal == 1)
        {
            DEBUG_DESTROY_NODE(tree);
            return op1;
        }

        break;

    case GT_LSH:

        /* Check for the case "(val + icon) << icon" */

        if  (op2->gtOper == GT_CNS_INT &&
             op1->gtOper == GT_ADD && !op1->gtOverflow())
        {
            GenTreePtr  add = op1->gtOp.gtOp2;

            if  (add->gtOper == GT_CNS_INT && op2->IsScaleIndexShf())
            {
                long        ishf = op2->gtIntCon.gtIconVal;
                long        iadd = add->gtIntCon.gtIconVal;

//                  printf("Changing '(val+icon1)<<icon2' into '(val<<icon2+icon1<<icon2)'\n");

                /* Change "(val + iadd) << ishf" into "(val<<ishf + iadd<<ishf)" */

                tree->ChangeOper(GT_ADD);

                op2->gtIntCon.gtIconVal = iadd << ishf;

                op1->ChangeOper(GT_LSH);

                add->gtIntCon.gtIconVal = ishf;
            }
        }

        break;

    case GT_XOR:

        /* "x ^ -1" is "~x" */

        if  (op2->gtOper == GT_CNS_INT && op2->gtIntCon.gtIconVal == -1)
        {
            tree->ChangeOper(GT_NOT);
            tree->gtOp.gtOp2 = NULL;
            DEBUG_DESTROY_NODE(op2);
        }
        else if  (op2->gtOper == GT_CNS_LNG && op2->gtLngCon.gtLconVal == -1)
        {
            tree->ChangeOper(GT_NOT);
            tree->gtOp.gtOp2 = NULL;
            DEBUG_DESTROY_NODE(op2);
        }
        else if (op2->gtOper == GT_CNS_INT && op2->gtIntCon.gtIconVal == 1 &&
                 op1->OperIsCompare())
        {
            /* "binaryVal ^ 1" is "!binaryVal" */

            op1->SetOper(GenTree::ReverseRelop(op1->OperGet()));
            DEBUG_DESTROY_NODE(op2);
            DEBUG_DESTROY_NODE(tree);
            return op1;
        }

        break;

#if ALLOW_MIN_OPT

    // If opts.compMinOptim, then we just allocate lclvars to
    // RBM_MIN_OPT_LCLVAR_REGS (RBM_ESI|RBM_EDI).
    // However, these block instructions absolutely need these registers,
    // so we cant even use those for register allocation.
    //
    // This cannot be done in the raPredictRegUse() as that
    // function is not called if opts.compMinOptim.

#if TGT_x86
    case GT_INITBLK: raMinOptLclVarRegs &= ~(        RBM_EDI); break;
    case GT_COPYBLK: raMinOptLclVarRegs &= ~(RBM_ESI|RBM_EDI); break;
#else
    // ISSUE: Do we need any non-x86 handling here?
#endif
#endif
    }

    return tree;
}

/*****************************************************************************
 *
 *  Transform the given tree for code generation and returns an equivalent tree.
 */

GenTreePtr          Compiler::fgMorphTree(GenTreePtr tree)
{
    assert(tree);
    assert(tree->gtOper != GT_STMT);

    /*-------------------------------------------------------------------------
     * fgMorphTree() can potentially replace a tree with another, and the
     * caller has to store the return value correctly.
     * Turn this on to always make copy of "tree" here to shake out
     * hidden/unupdated references.
     */

#ifdef DEBUG

    if  (compStressCompile(STRESS_GENERIC_CHECK, 0))
    {
        GenTreePtr      copy;

#ifdef SMALL_TREE_NODES
        if  (GenTree::s_gtNodeSizes[tree->gtOper] == TREE_NODE_SZ_SMALL)
            copy = gtNewLargeOperNode(GT_ADD, TYP_INT);
        else
#endif
            copy = gtNewOperNode     (GT_CALL, TYP_INT);

        copy->CopyFrom(tree);

#if defined(JIT_AS_COMPILER) || defined (LATE_DISASM)
        // GT_CNS_INT is considered small, so CopyFrom() wont copy all fields
        if  ((tree->gtOper == GT_CNS_INT) && 
             (tree->gtFlags & GTF_ICON_HDL_MASK))
        {
            copy->gtIntCon.gtIconHdl.gtIconHdl1 = tree->gtIntCon.gtIconHdl.gtIconHdl1;
            copy->gtIntCon.gtIconHdl.gtIconHdl2 = tree->gtIntCon.gtIconHdl.gtIconHdl2;
        }
#endif

        DEBUG_DESTROY_NODE(tree);
        tree = copy;
    }
#endif // DEBUG
        
    if (fgGlobalMorph)
    {
        /* Insure that we haven't morphed this node already */
        assert(((tree->gtFlags & GTF_MORPHED) == 0) && "ERROR: Already morphed this node!");

#if LOCAL_ASSERTION_PROP
        /* Before morphing the tree, we try to propagate any active assertions */
        if (fgAssertionProp)
        {
            /* Do we have any active assertions? */
    
            if (optAssertionCount > 0)
            {
                while (true)
                {
                    /* newTree is non-Null if we propagated an assertion */
                    GenTreePtr newTree = optAssertionProp(-1, tree, true);
                    if ((newTree == NULL) || (newTree == tree))
                        break;
                    else
                        tree = newTree;
                }
            }
        }
#endif
    }

    /* Save the original un-morphed tree for fgMorphTreeDone */

    GenTreePtr oldTree = tree;

    /* Figure out what kind of a node we have */

    unsigned kind = tree->OperKind();

    /* Is this a constant node? */

    if  (kind & GTK_CONST)
    {
        tree = fgMorphConst(tree);
        goto DONE;
    }

    /* Is this a leaf node? */

    if  (kind & GTK_LEAF)
    {
        tree = fgMorphLeaf(tree);
        goto DONE;
    }

    /* Is it a 'simple' unary/binary operator? */

    if  (kind & GTK_SMPOP)
    {
        tree = fgMorphSmpOp(tree);
        goto DONE;
    }

    /* See what kind of a special operator we have here */

    switch  (tree->OperGet())
    {
    case GT_FIELD:
        tree = fgMorphField(tree);
        goto DONE;

    case GT_CALL:
        tree = fgMorphCall(tree);
        goto DONE;

    case GT_ARR_LENREF:
        tree->gtArrLen.gtArrLenAdr      = fgMorphTree(tree->gtArrLen.gtArrLenAdr);
        tree->gtFlags|=tree->gtArrLen.gtArrLenAdr->gtFlags & GTF_SIDE_EFFECT;

        if (tree->gtArrLen.gtArrLenCse)
        {
            tree->gtArrLen.gtArrLenCse  = fgMorphTree(tree->gtArrLen.gtArrLenCse);
            tree->gtFlags|=tree->gtArrLen.gtArrLenCse->gtFlags & GTF_SIDE_EFFECT;
        }
        goto DONE;

    case GT_ARR_ELEM:
        tree->gtArrElem.gtArrObj        = fgMorphTree(tree->gtArrElem.gtArrObj);
        tree->gtFlags|=tree->gtArrElem.gtArrObj->gtFlags & GTF_SIDE_EFFECT;

        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
        {
            tree->gtArrElem.gtArrInds[dim] = fgMorphTree(tree->gtArrElem.gtArrInds[dim]);
            tree->gtFlags|=tree->gtArrElem.gtArrInds[dim]->gtFlags & GTF_SIDE_EFFECT;
        }
        if (fgGlobalMorph)
            fgSetRngChkTarget(tree, false);
        goto DONE;

    default:
#ifdef  DEBUG
        gtDispTree(tree);
#endif
        assert(!"unexpected operator");
    }

    assert(!"Shouldn't get here in fgMorphTree()");

DONE:

    fgMorphTreeDone(tree, oldTree);

    return tree;
}

/*****************************************************************************
 *
 *  This function is call to complete the morphing of a tree node
 *  It should only be called once for each node.
 *  If DEBUG is defined the flag GTF_MORPHED is checked and updated,
 *  to enforce the invariant that each node is only morphed once.
 *  If LOCAL_ASSERTION_PROP is enabled the result tree may be replaced
 *  by an equivalent tree.
 *  
 */

void                Compiler::fgMorphTreeDone(GenTreePtr tree, 
                                              GenTreePtr oldTree /* == NULL */)
{
    if (!fgGlobalMorph)
        return;
    
    if ((oldTree != NULL) && (oldTree != tree))
    {
#ifdef DEBUG
        /* Insure that we have morphed this node */
        assert((tree->gtFlags & GTF_MORPHED) && "ERROR: Did not morph this node!");
#endif
        return;
    }

#ifdef DEBUG
    /* Insure that we haven't morphed this node already */
    assert(((tree->gtFlags & GTF_MORPHED) == 0) && "ERROR: Already morphed this node!");
#endif

    if (tree->OperKind() & GTK_CONST)
        goto DONE;

#if LOCAL_ASSERTION_PROP
    if (!fgAssertionProp)
        goto DONE;

    /* Do we have any active assertions? */
    
    if (optAssertionCount > 0)
    {
        /* Is this an assignment to a local variable */
        
        if ((tree->OperKind() & GTK_ASGOP) &&
            (tree->gtOp.gtOp1->gtOper == GT_LCL_VAR))
        {
            unsigned op1LclNum = tree->gtOp.gtOp1->gtLclVar.gtLclNum; assert(op1LclNum  < lvaCount);
            
            /* All dependent assertions are killed here */
            
            EXPSET_TP killed = lvaTable[op1LclNum].lvAssertionDep;          
            
            if (killed)
            {   
                unsigned   index = optAssertionCount;
                EXPSET_TP  mask  = ((EXPSET_TP)1 << (index-1));            
                
                while (true)
                {
                    index--;
                    
                    if  (killed & mask)
                    {
                        assert((optAssertionTab[index].op1.lclNum  == op1LclNum)     ||
                               ((optAssertionTab[index].op2.type   == GT_LCL_VAR) &&
                                (optAssertionTab[index].op2.lclNum == op1LclNum)    )  );
#ifdef DEBUG
                        if (verbose)
                        {
                            printf("\nThe assignment [%08X] removes local assertion candidate: V%02u",
                                   tree, optAssertionTab[index].op1.lclNum);
                            if (optAssertionTab[index].assertion == OA_EQUAL)
                                printf(" = ");
                            if (optAssertionTab[index].op2.type == GT_LCL_VAR)
                                printf("V%02u", optAssertionTab[index].op2.lclNum);
                            else
                            {
                                switch  (optAssertionTab[index].op2.type)
                                {
                                case GT_CNS_INT:
                                    if ((optAssertionTab[index].op2.iconVal > -1000) && (optAssertionTab[index].op2.iconVal < 1000))
                                        printf(" %ld"   , optAssertionTab[index].op2.iconVal);
                                    else
                                        printf(" 0x%X"  , optAssertionTab[index].op2.iconVal);
                                    break;

                                case GT_CNS_LNG: 
                                    printf(" %I64d" , optAssertionTab[index].op2.lconVal); 
                                    break;

                                case GT_CNS_DBL:
                                    if (*((__int64 *)&optAssertionTab[index].op2.dconVal) == 0x8000000000000000)
                                        printf(" -0.00000");
                                    else
                                        printf(" %#lg"   , optAssertionTab[index].op2.dconVal); 
                                    break;

                                default: assert(!"unexpected constant node");
                                }
                            }
                        }
                            
                        // Remove this bit from the killed mask
                        killed &= ~mask;
#endif
                        optAssertionRemove(index);
                    }

                    if (index == 0)
                        break;
                        
                    mask >>= 1;
                }
                
                // killed mask should now be zero
                assert(killed == 0);
            }
        }
    }
    
    /* If this tree makes a new assertion - make it available */
    optAssertionAdd(tree, true);

#endif // LOCAL_ASSERTION_PROP

DONE:;

#ifdef DEBUG
    /* Mark this node as being morphed */
    tree->gtFlags |= GTF_MORPHED;
#endif
}



/*****************************************************************************/
#if OPTIMIZE_TAIL_REC
/*****************************************************************************
 *
 *  Convert an argument list for a tail-recursive call.
 *
 *  We'll convert a call of the form f(x1, x2, x3) to the following expression:
 *
 *      f(x1 , x2 , (arg3 = x3,arg2 = pop,arg1 = pop))
 *
 *  This is done by recursively walking the argument list, adding those
 *  'arg = pop' assignments for each until we get to the last one. Why this
 *  rigmarole, you ask? Well, it's mostly to make the life-time analysis do
 *  the right thing with the arguments.
 *
 *  UNDONE: Skip arguments whose values are identical in both argument lists!
 */

void                Compiler::fgCnvTailRecArgList(GenTreePtr *argsPtr)
{
    unsigned        anum = 0;
    GenTreePtr      pops = 0;
    GenTreePtr      args = *argsPtr;

    GenTreePtr      argv;
    GenTreePtr      next;
    var_types       type;

    GenTreePtr      temp;

    /* Skip the first argument slot if there is a 'this' argument */

    if  (!info.compIsStatic)
        anum++;

    /* Now walk the argument list, appending the 'pop' expressions */

    for (;;)
    {
        /* Get hold of this and the next argument */

        assert(args);
        assert(args->gtOper == GT_LIST);

        argv = args->gtOp.gtOp1;
        next = args->gtOp.gtOp2;
        type = argv->TypeGet();

        /* Is this the last argument? */

        if  (!next)
            break;

        /* Add 'arg = pop' to the 'pops' list */

        temp = gtNewAssignNode(gtNewLclvNode(anum, type),
                               gtNewNode(GT_POP,   type));

        pops = pops ? gtNewOperNode(GT_COMMA, TYP_VOID, temp, pops)
                    : temp;

        /* Figure out the slot# for the next argument */

        anum += genTypeStSz(type);

        /* Move on to the next argument */

        args = next; assert(args);
    }

    /* Assign the last argument value */

    temp = gtNewAssignNode(gtNewLclvNode(anum, type), argv);

    /* Glue the last argument assignment with the other pops, if any */

    if  (pops)
        temp = gtNewOperNode(GT_COMMA, TYP_VOID, temp, pops);

    /* Set the type of the last argument to 'void' */

    temp->gtType = TYP_VOID;

    /* Replace the last argument with the 'pops' expression */

    assert(args->gtOp.gtOp1 == argv); args->gtOp.gtOp1 = temp;
}

/*****************************************************************************/
#endif//OPTIMIZE_TAIL_REC
/*****************************************************************************/

/*****************************************************************************
 *
 *  Mark whether the edge "srcBB -> dstBB" forms a loop that will always
 *  execute a call or not.
 */

inline
void                Compiler::fgLoopCallTest(BasicBlock *srcBB,
                                             BasicBlock *dstBB)
{
    /* Bail if this is not a backward edge */

    if  (srcBB->bbNum < dstBB->bbNum)
        return;

    /* Unless we already know that there is a loop without a call here ... */

    if  (!(dstBB->bbFlags & BBF_LOOP_CALL0))
    {
        /* Check whether there is a loop path that doesn't call */

        if  (optReachWithoutCall(dstBB, srcBB))
        {
            dstBB->bbFlags |=  BBF_LOOP_CALL0;
            dstBB->bbFlags &= ~BBF_LOOP_CALL1;
        }
        else
            dstBB->bbFlags |=  BBF_LOOP_CALL1;
    }
}

/*****************************************************************************
 *
 *  Mark which loops are guaranteed to execute a call.
 */

void                Compiler::fgLoopCallMark()
{
    BasicBlock  *   block;

    /* If we've already marked all the block, bail */

    if  (fgLoopCallMarked)
        return;

    fgLoopCallMarked = true;

    /* Walk the blocks, looking for backward edges */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        switch (block->bbJumpKind)
        {
        case BBJ_COND:
            fgLoopCallTest(block, block->bbJumpDest);
            break;

        case BBJ_CALL:
        case BBJ_ALWAYS:
            fgLoopCallTest(block, block->bbJumpDest);
            break;

        case BBJ_NONE:
            break;

        case BBJ_RET:
        case BBJ_THROW:
        case BBJ_RETURN:
            break;

        case BBJ_SWITCH:

            unsigned        jumpCnt = block->bbJumpSwt->bbsCount;
            BasicBlock * *  jumpPtr = block->bbJumpSwt->bbsDstTab;

            do
            {
                fgLoopCallTest(block, *jumpPtr);
            }
            while (++jumpPtr, --jumpCnt);

            break;
        }
    }
}

/*****************************************************************************
 *
 *  Note the fact that the given block is a loop header.
 */

inline
void                Compiler::fgMarkLoopHead(BasicBlock *block)
{
    /* Is the loop head block known to execute a method call? */

    if  (block->bbFlags & BBF_GC_SAFE_POINT)
        return;

    /* Have we decided to generate fully interruptible code already? */

    if  (genInterruptible)
        return;

    /* Are dominator sets available? */

    if  (fgDomsComputed)
    {
        /* Make sure that we know which loops will always execute calls */

        if  (!fgLoopCallMarked)
            fgLoopCallMark();

        /* Will every trip through our loop execute a call? */

        if  (block->bbFlags & BBF_LOOP_CALL1)
            return;
    }

    /*
     *  We have to make this method fully interruptible since we can not
     *  insure that this loop will execute a call every time it loops.
     *
     *  We'll also need to generate a full register map for this method.
     */

    assert(genIntrptibleUse == false);

    /*  @TODO [REVISIT] [04/16/01] []: 
     *         GC encoding for fully interruptible methods do not 
     *         support more than 31 pushed arguments
     *         so we can't set genInterruptible here
     */
    if (fgPtrArgCntMax >= 32)
        return;

    genInterruptible = true;
}


/*****************************************************************************
 *
 *  Add any internal blocks/trees we may need
 *  Returns true if we change the basic block list.
 */

bool                Compiler::fgAddInternal()
{
    BasicBlock *    block;
    bool            chgBBlst = false;

    /* Assume we will generate a single shared return sequence */

    bool oneReturn = true;

    /*  We generate an inline copy of the function epilog at each return
        point when compiling for speed, but we have to be careful since
        we won't (in general) know what callee-saved registers we're
        going to save and thus don't know which regs to pop at every
        return except the very last one.
     */

#if!TGT_RISC
    /*
        We generate just one epilog for methods calling into unmanaged code
        or for synchronized methods.
     */

    if (!((opts.compEnterLeaveEventCB) ||
#if INLINE_NDIRECT
          (info.compCallUnmanaged > 0) ||
#endif
          (info.compFlags & CORINFO_FLG_SYNCH)))
    {
        /* Make sure there are not 'too many' exit points */

        unsigned    retCnt = 0;

        for (block = fgFirstBB; block; block = block->bbNext)
        {
            if  (block->bbJumpKind == BBJ_RETURN)
                retCnt++;
        }

        /* We'll only allow an arbitrarily small number of returns */

        if  ((retCnt < 5) ||
             ((compCodeOpt() == SMALL_CODE) && retCnt <= 1))
        {
            /* OK, let's generate multiple individual exits */

            oneReturn = false;
        }
    }
#endif


#if TGT_RISC
    assert(oneReturn);  // this is needed for epilogs to work (for now) !
#endif

    if  (oneReturn)
    {
        genReturnBB  = fgNewBBinRegion(BBJ_RETURN, 0, NULL);
        genReturnBB->bbRefs = 1;

        genReturnBB->bbCodeSize = 0;
        genReturnBB->bbFlags   |= (BBF_INTERNAL|BBF_DONT_REMOVE);
    }
    else
    {
        genReturnBB = NULL;
    }

#if INLINE_NDIRECT
    if (info.compCallUnmanaged != 0)
    {
        info.compLvFrameListRoot = lvaGrabTemp(false);
    }

    /* If we need a locspace region, we will create a dummy variable of
     * type TYP_LCLBLK. Grab a slot and remember it */

    if (lvaScratchMem > 0 || info.compCallUnmanaged != 0)
        lvaScratchMemVar = lvaGrabTemp(false);
#else
    if  (lvaScratchMem > 0)
        lvaScratchMemVar = lvaGrabTemp(false);
#endif

#if TGT_RISC
    genMonExitExp    = NULL;
#endif


    /* Is this a 'synchronized' method? */

    if  (info.compFlags & CORINFO_FLG_SYNCH)
    {
        GenTreePtr      tree;
        void * critSect = 0, **pCrit = 0;

        /* Insert the expression "enterCrit(this)" or "enterCrit(handle)" */

        if  (info.compIsStatic)
        {
            critSect = eeGetMethodSync(info.compMethodHnd, &pCrit);
            assert((!critSect) != (!pCrit));

            tree = gtNewIconEmbHndNode(critSect, pCrit, GTF_ICON_METHOD_HDL);

            tree = gtNewHelperCallNode(CORINFO_HELP_MON_ENTER_STATIC,
                                       TYP_VOID, 0,
                                       gtNewArgList(tree));
        }
        else
        {
            tree = gtNewLclvNode(0, TYP_REF);

            tree = gtNewHelperCallNode(CORINFO_HELP_MON_ENTER,
                                       TYP_VOID, 0,
                                       gtNewArgList(tree));
        }

        /* Create a new basic block and stick the call in it */

        block = bbNewBasicBlock(BBJ_NONE); 
        fgStoreFirstTree(block, tree);

        /* Insert the new BB at the front of the block list */

        block->bbNext = fgFirstBB;

        if  (fgFirstBB == fgLastBB)
            fgLastBB = block;

        fgFirstBB = block;
        block->bbFlags |= BBF_INTERNAL;

#ifdef DEBUG
        if (verbose)
        {
            printf("\nSynchronized method - Add enterCrit statement in new first basic block [%08X]\n", block);
            gtDispTree(tree,0);
            printf("\n");
        }
#endif

        /* Remember that we've changed the basic block list */

        chgBBlst = true;

        /* We must be generating a single exit point for this to work */

        assert(oneReturn);
        assert(genReturnBB);

        /* Create the expression "exitCrit(this)" or "exitCrit(handle)" */

        if  (info.compIsStatic)
        {
            tree = gtNewIconEmbHndNode(critSect, pCrit, GTF_ICON_METHOD_HDL);

            tree = gtNewHelperCallNode(CORINFO_HELP_MON_EXIT_STATIC,
                                       TYP_VOID, 0,
                                       gtNewArgList(tree));
        }
        else
        {
            tree = gtNewLclvNode(0, TYP_REF);

            tree = gtNewHelperCallNode(CORINFO_HELP_MON_EXIT,
                                       TYP_VOID, 0,
                                       gtNewArgList(tree));
        }

#if     TGT_RISC

        /* Is there a non-void return value? */

        if  (info.compRetType != TYP_VOID)
        {
            /* We'll add the exitCrit call later */

            genMonExitExp = tree;
            genReturnLtm  = genReturnCnt;
        }
        else
        {
            /* Add the 'exitCrit' call to the return block */

            fgStoreFirstTree(genReturnBB, tree);
        }

#else

        /* Make the exitCrit tree into a 'return' expression */

        tree = gtNewOperNode(GT_RETURN, TYP_VOID, tree);

        /* Add 'exitCrit' to the return block */

        fgStoreFirstTree(genReturnBB, tree);

#ifdef DEBUG
        if (verbose)
        {
            printf("\nAdded exitCrit to Synchronized method [%08X]\n", genReturnBB);
            gtDispTree(tree,0);
            printf("\n");
        }
#endif

#endif

    }

#if INLINE_NDIRECT || defined(PROFILER_SUPPORT)

    /* prepend a GT_RETURN statement to genReturnBB */
    if  (
#if INLINE_NDIRECT
         info.compCallUnmanaged ||
#endif
        opts.compEnterLeaveEventCB)
    {
        /* Only necessary if it isn't already done */
        if  (!(info.compFlags & CORINFO_FLG_SYNCH))
        {
            GenTreePtr      tree;

            assert(oneReturn);
            assert(genReturnBB);

            tree = gtNewOperNode(GT_RETURN, TYP_VOID, NULL);

            fgStoreFirstTree(genReturnBB, tree);

        }
    }
#endif

    return chgBBlst;
}

/*****************************************************************************
 *
 *  Check and fold blocks of type BBJ_COND and BBJ_SWITCH on constants
 *  Returns true if we modified the flow graph
 */

bool                Compiler::fgFoldConditional(BasicBlock * block)
{
    bool result = false;
    
    // We don't want to make any code unreachable
    if (opts.compDbgCode)
      return false;

    if (block->bbJumpKind == BBJ_COND)
    {
        assert(block->bbTreeList && block->bbTreeList->gtPrev);

        GenTreePtr stmt = block->bbTreeList->gtPrev;

        assert(stmt->gtNext == NULL);

        if (stmt->gtStmt.gtStmtExpr->gtOper == GT_CALL)
        {
            assert(fgRemoveRestOfBlock);

            /* Unconditional throw - transform the basic block into a BBJ_THROW */
            fgConvertBBToThrowBB(block);        
                
            /* Remove 'block' from the predecessor list of 'block->bbNext' */
            if  (block->bbNext->bbPreds)
                fgRemovePred(block->bbNext, block);

            block->bbNext->bbRefs--;

            /* Remove 'block' from the predecessor list of 'block->bbJumpDest' */
            if  (block->bbJumpDest->bbPreds)
                fgRemovePred(block->bbJumpDest, block);

            block->bbJumpDest->bbRefs--;
#ifdef DEBUG
            if  (verbose)
            {
                printf("\nConditional folded at BB%02u\n", block->bbNum);
                printf("BB%02u becomes a BBJ_THROW\n", block->bbNum);

            }
#endif
            goto UPDATE_LOOP_TABLE;
        }

        assert(stmt->gtStmt.gtStmtExpr->gtOper == GT_JTRUE);

        /* Did we fold the conditional */

        assert(stmt->gtStmt.gtStmtExpr->gtOp.gtOp1);
        GenTreePtr  cond = stmt->gtStmt.gtStmtExpr->gtOp.gtOp1;

        if (cond->OperKind() & GTK_CONST)
        {            
            /* Yupee - we folded the conditional!
             * Remove the conditional statement */

            assert(cond->gtOper == GT_CNS_INT);
            assert((block->bbNext->bbRefs     > 0) && 
                   (block->bbJumpDest->bbRefs > 0));

            /* remove the statement from bbTreelist - No need to update
             * the reference counts since there are no lcl vars */
            fgRemoveStmt(block, stmt);
        
            /* modify the flow graph */
        
            if (cond->gtIntCon.gtIconVal != 0)
            {
                /* JTRUE 1 - transform the basic block into a BBJ_ALWAYS */
                block->bbJumpKind = BBJ_ALWAYS;
            
                /* Remove 'block' from the predecessor list of 'block->bbNext' */
                if  (block->bbNext->bbPreds)
                    fgRemovePred(block->bbNext, block);

                block->bbNext->bbRefs--;
            }
            else
            {
                /* Unmark the loop if we are removing a backwards branch */
                /* dest block must also be marked as a loop head and     */
                /* We must be able to reach the backedge block           */
                if ((block->bbJumpDest->isLoopHead())          &&
                    (block->bbJumpDest->bbNum <= block->bbNum) && 
                    fgReachable(block->bbJumpDest, block))
                {
                    optUnmarkLoopBlocks(block->bbJumpDest, block);
                }
            
                /* JTRUE 0 - transform the basic block into a BBJ_NONE   */
                block->bbJumpKind = BBJ_NONE;
            
                /* Remove 'block' from the predecessor list of 'block->bbJumpDest' */
                if  (block->bbJumpDest->bbPreds)
                    fgRemovePred(block->bbJumpDest, block);

                block->bbJumpDest->bbRefs--;
            }
        
#ifdef DEBUG
            if  (verbose)
            {
                printf("\nConditional folded at BB%02u\n", block->bbNum);
                printf("BB%02u becomes a %s", block->bbNum,
                       block->bbJumpKind == BBJ_ALWAYS ? "BBJ_ALWAYS" : "BBJ_NONE");
                if  (block->bbJumpKind == BBJ_ALWAYS)
                    printf(" to BB%02u", block->bbJumpDest->bbNum);
                printf("\n");
            }
#endif

UPDATE_LOOP_TABLE:

            /* if the block was a loop condition we may have to modify
             * the loop table */
        
            for (unsigned loopNum = 0; loopNum < optLoopCount; loopNum++)
            {
                /* Some loops may have been already removed by
                 * loop unrolling or conditional folding */
            
                if (optLoopTable[loopNum].lpFlags & LPFLG_REMOVED)
                    continue;
            
                /* We are only interested in the loop bottom */
            
                if  (optLoopTable[loopNum].lpEnd == block)
                {
                    if  (cond->gtIntCon.gtIconVal == 0)
                    {
                        /* This was a bogus loop (condition always false)
                         * Remove the loop from the table */
                    
                        optLoopTable[loopNum].lpFlags |= LPFLG_REMOVED;
#ifdef DEBUG
                        if  (verbose)
                        {
                            printf("Removing loop L%02u (from BB%02u to BB%02u)\n\n",
                                   loopNum,
                                   optLoopTable[loopNum].lpHead->bbNext->bbNum,
                                   optLoopTable[loopNum].lpEnd         ->bbNum);
                        }
#endif
                    }
                }
            }
            result = true;            
        }
    }
    else if  (block->bbJumpKind == BBJ_SWITCH)
    {
        assert(block->bbTreeList && block->bbTreeList->gtPrev);

        GenTreePtr stmt = block->bbTreeList->gtPrev;

        assert(stmt->gtNext == NULL);

        if (stmt->gtStmt.gtStmtExpr->gtOper == GT_CALL)
        {
            assert(fgRemoveRestOfBlock);

            /* Unconditional throw - transform the basic block into a BBJ_THROW */
            fgConvertBBToThrowBB(block);

            /* update the flow graph */
            
            unsigned        jumpCnt   = block->bbJumpSwt->bbsCount;
            BasicBlock * *  jumpTab   = block->bbJumpSwt->bbsDstTab;
            
            for (unsigned val = 0; val < jumpCnt; val++, jumpTab++)                      
            {
                BasicBlock *  curJump = *jumpTab;
                
                /* Remove 'block' from the predecessor list of 'curJump' */
                if  (curJump->bbPreds)
                    fgRemovePred(curJump, block);

                curJump->bbRefs--;
            }

#ifdef DEBUG
            if  (verbose)
            {
                printf("\nConditional folded at BB%02u\n", block->bbNum);
                printf("BB%02u becomes a BBJ_THROW\n", block->bbNum);

            }
#endif
            goto DONE_SWITCH;
        }

        assert(stmt->gtStmt.gtStmtExpr->gtOper == GT_SWITCH);

        /* Did we fold the conditional */

        assert(stmt->gtStmt.gtStmtExpr->gtOp.gtOp1);
        GenTreePtr  cond = stmt->gtStmt.gtStmtExpr->gtOp.gtOp1;

        if (cond->OperKind() & GTK_CONST)
        {
            /* Yupee - we folded the conditional!
             * Remove the conditional statement */
            
            assert(cond->gtOper == GT_CNS_INT);
            
            /* remove the statement from bbTreelist - No need to update
             * the reference counts since there are no lcl vars */
            fgRemoveStmt(block, stmt);
            
            /* modify the flow graph */
            
            /* Find the actual jump target */
            unsigned        switchVal = cond->gtIntCon.gtIconVal;
            unsigned        jumpCnt   = block->bbJumpSwt->bbsCount;
            BasicBlock * *  jumpTab   = block->bbJumpSwt->bbsDstTab;
            bool            foundVal  = false;
            
            for (unsigned val = 0; val < jumpCnt; val++, jumpTab++)                      
            {
                BasicBlock *  curJump = *jumpTab;


                assert (curJump->bbRefs > 0);
                
                // If val matches switchVal or we are at the last entry and
                // we never found the switch value then set the new jump dest
                
                if ( (val == switchVal) || (!foundVal && (val == jumpCnt-1)))
                {
                    if (curJump != block->bbNext)
                    {
                        /* transform the basic block into a BBJ_ALWAYS */
                        block->bbJumpKind = BBJ_ALWAYS;
                        block->bbJumpDest = curJump;
                    }
                    else
                    {
                        /* transform the basic block into a BBJ_NONE */
                        block->bbJumpKind = BBJ_NONE;
                    }
                    foundVal = true;
                }
                else
                {
                    /* Remove 'block' from the predecessor list of 'curJump' */
                    if  (curJump->bbPreds)
                        fgRemovePred(curJump, block);

                    curJump->bbRefs--;
                }
            }
#ifdef DEBUG
            if  (verbose)
            {
                printf("\nConditional folded at BB%02u\n", block->bbNum);
                printf("BB%02u becomes a %s", block->bbNum,
                       block->bbJumpKind == BBJ_ALWAYS ? "BBJ_ALWAYS" : "BBJ_NONE");
                if  (block->bbJumpKind == BBJ_ALWAYS)
                    printf(" to BB%02u", block->bbJumpDest->bbNum);
                printf("\n");
            }
#endif
DONE_SWITCH:
            result = true;
        }
    }
    return result;
}


/*****************************************************************************
 *
 *  Morph the statements of the given block.
 */

void                Compiler::fgMorphStmts(BasicBlock * block,
                                           bool * mult, bool * lnot, bool * loadw)
{
    fgRemoveRestOfBlock = false;

    assert(fgExpandInline == false);

    /* Make the current basic block address available globally */

    compCurBB = block;

    *mult = *lnot = *loadw = false;

    GenTreePtr stmt, prev;

    for (stmt = block->bbTreeList, prev = NULL;
         stmt;
         prev = stmt->gtStmt.gtStmtExpr,
         stmt = stmt->gtNext)
    {
        assert(stmt->gtOper == GT_STMT);

        if (fgRemoveRestOfBlock)
        {
            fgRemoveStmt(block, stmt);
            continue;
        }

        fgMorphStmt      = stmt;
        GenTreePtr  tree = stmt->gtStmt.gtStmtExpr;

#ifdef  DEBUG
        unsigned oldHash;
        unsigned newHash;

        if (verbose)
            oldHash = gtHashValue(tree);

        if (verbose && 0)
        {
            printf("\nfgMorphTree (before):\n");
            gtDispTree(tree);
        }
#endif

        /* Morph this statement tree */

        GenTreePtr  morph = fgMorphTree(tree);

        // Has fgMorphStmt been sneakily whacked ?

        if (stmt->gtStmt.gtStmtExpr != tree)
        {
            /* This must be tailcall. Ignore 'morph' and carry on with
               the tail-call node */

            morph = stmt->gtStmt.gtStmtExpr;

            assert(compTailCallUsed);
            assert((morph->gtOper == GT_CALL) &&
                   (morph->gtCall.gtCallMoreFlags & GTF_CALL_M_TAILCALL));
            assert(stmt->gtNext == NULL);
            assert(block->bbJumpKind == BBJ_THROW);
        }

#ifdef DEBUG
        if (compStressCompile(STRESS_CLONE_EXPR, 30))
        {
            // Clone all the trees to stress gtCloneExpr()
            morph = gtCloneExpr(morph);
            assert(morph);
        }

        /* If the hash value changes. we modified the tree during morphing */
        if (verbose)
        {
            newHash = gtHashValue(morph);
            if (newHash != oldHash)
            {
                printf("\nfgMorphTree (after):\n");
                gtDispTree(morph);
            }
        }
#endif

        /* Check for morph as a GT_COMMA with an unconditional throw */
        if (fgIsCommaThrow(morph, true))
        {
            /* Use the call as the new stmt */
            morph = morph->gtOp.gtOp1;
            assert(morph->gtOper == GT_CALL);
            assert((morph->gtFlags & GTF_COLON_COND) == 0);

            fgRemoveRestOfBlock = true;
        }
    
        stmt->gtStmt.gtStmtExpr = tree = morph;

        assert(fgPtrArgCntCur == 0);

        if (fgRemoveRestOfBlock)
            continue;

        /* Has the statement been optimized away */

        if (fgCheckRemoveStmt(block, stmt))
            continue;

        /* Check if this block ends with a conditional branch that can be folded */

        if (fgFoldConditional(block))
            continue;

        if  (block->bbFlags & BBF_HAS_HANDLER)
            continue;

#if OPT_MULT_ADDSUB

        /* Note whether we have two or more +=/-= operators in a row */

        if  (tree->gtOper == GT_ASG_ADD ||
             tree->gtOper == GT_ASG_SUB)
        {
            if  (prev && prev->gtOper == tree->gtOper)
                *mult = true;
        }

#endif

#if OPT_BOOL_OPS

        /* Note whether we have two "log0" assignments in a row */

        if  (tree->IsNotAssign() != -1)
        {
            fgMultipleNots |= *lnot; *lnot = true;
        }

#endif

        /* Note "x = a[i] & icon" followed by "x |= a[i] << 8" */

        if  (tree->gtOper == GT_ASG_OR &&
             prev &&
             prev->gtOper == GT_ASG)
        {
            *loadw = true;
        }
    }

    if (fgRemoveRestOfBlock)
    {
        if ((block->bbJumpKind == BBJ_COND) || (block->bbJumpKind == BBJ_SWITCH))
        {
            GenTreePtr first = block->bbTreeList; assert(first);
            GenTreePtr last  = first->gtPrev;     assert(last && last->gtNext == NULL);
            GenTreePtr stmt  = last->gtStmt.gtStmtExpr;

            if (((block->bbJumpKind == BBJ_COND  ) && (stmt->gtOper == GT_JTRUE )) ||
                ((block->bbJumpKind == BBJ_SWITCH) && (stmt->gtOper == GT_SWITCH))   )
            {
                GenTreePtr op1   = stmt->gtOp.gtOp1;

                if (op1->OperKind() & GTK_RELOP)
                {
                    /* Unmark the comparison node with GTF_RELOP_JMP_USED */
                    op1->gtFlags &= ~GTF_RELOP_JMP_USED;
                }

                last->gtStmt.gtStmtExpr = fgMorphTree(op1);
            }
        }

        /* Mark block as a BBJ_THROW block */
        fgConvertBBToThrowBB(block);        
    }

    assert(fgExpandInline == false);

#ifdef DEBUG
    compCurBB = (BasicBlock*)0xDEADBEEF;
#endif
}

/*****************************************************************************
 *
 *  Morph the blocks of the method.
 *  Returns true if the basic block list is modified.
 */

bool                Compiler::fgMorphBlocks()
{
    /* Since fgMorphTree can be called after various optimizations to re-arrange
     * the nodes we need a global flag to signal if we are during the one-pass
     * global morphing */

    fgGlobalMorph   = true;
    fgAssertionProp = (!opts.compDbgCode && !opts.compMinOptim);

#if LOCAL_ASSERTION_PROP
    if (fgAssertionProp)
        optAssertionInit();
#endif

    /*-------------------------------------------------------------------------
     * Process all basic blocks in the function
     */

    bool    chgBBlst = false;

    BasicBlock *    block       = fgFirstBB; assert(block);
    BasicBlock *    bPrev       = NULL;

    do
    {
#if OPT_MULT_ADDSUB
        bool            mult  = false;
#endif

#if OPT_BOOL_OPS
        bool            lnot  = false;
#endif

        bool            loadw = false;

#ifdef DEBUG
        if(verbose&&1)
            printf("\nMorphing BB%02u of '%s'\n", block->bbNum, info.compFullName);
#endif

#if LOCAL_ASSERTION_PROP
        if (fgAssertionProp)
        {
            //
            // Clear out any currently recorded assertion candidates
            // before processing each basic block,
            // also we must  handle QMARK-COLON specially
            //
            optAssertionReset(0);
        }
#endif

        /* Process all statement trees in the basic block */

        GenTreePtr      tree;

        fgMorphStmts(block, &mult, &lnot, &loadw);

#if OPT_MULT_ADDSUB

        if  (mult && (opts.compFlags & CLFLG_TREETRANS) &&
             !opts.compDbgCode && !opts.compMinOptim)
        {
            for (tree = block->bbTreeList; tree; tree = tree->gtNext)
            {
                assert(tree->gtOper == GT_STMT);
                GenTreePtr last = tree->gtStmt.gtStmtExpr;

                if  (last->gtOper == GT_ASG_ADD ||
                     last->gtOper == GT_ASG_SUB)
                {
                    GenTreePtr      temp;
                    GenTreePtr      next;

                    GenTreePtr      dst1 = last->gtOp.gtOp1;
                    GenTreePtr      src1 = last->gtOp.gtOp2;

                    // @TODO [CONSIDER] [04/16/01] []: allow non-int case

                    if  (last->gtType != TYP_INT)
                        goto NOT_CAFFE;

                    // @TODO [CONSIDER] [04/16/01] []:
                    // Allow non-constant case, that is in
                    // general fold "a += x1" followed by
                    // "a += x2" into "a += (x1+x2);".

                    if  (dst1->gtOper != GT_LCL_VAR)
                        goto NOT_CAFFE;
                    if  (src1->gtOper != GT_CNS_INT)
                        goto NOT_CAFFE;

                    for (;;)
                    {
                        GenTreePtr      dst2;
                        GenTreePtr      src2;

                        /* Look at the next statement */

                        temp = tree->gtNext;
                        if  (!temp)
                            goto NOT_CAFFE;

                        assert(temp->gtOper == GT_STMT);
                        next = temp->gtStmt.gtStmtExpr;

                        if  (next->gtOper != last->gtOper)
                            goto NOT_CAFFE;
                        if  (next->gtType != last->gtType)
                            goto NOT_CAFFE;

                        dst2 = next->gtOp.gtOp1;
                        src2 = next->gtOp.gtOp2;

                        if  (dst2->gtOper != GT_LCL_VAR)
                            goto NOT_CAFFE;
                        if  (dst2->gtLclVar.gtLclNum != dst1->gtLclVar.gtLclNum)
                            goto NOT_CAFFE;

                        if  (src2->gtOper != GT_CNS_INT)
                            goto NOT_CAFFE;

                        /* Fold the two increments/decrements into one */

                        src1->gtIntCon.gtIconVal += src2->gtIntCon.gtIconVal;

                        /* Remember that we've changed the basic block list */

                        chgBBlst = true;

                        /* Remove the second statement completely */

                        assert(tree->gtNext == temp);
                        assert(temp->gtPrev == tree);

//                      printf("Caffeine: %08X[%08X] subsumes %08X[%08X]\n", tree, tree->gtStmt.gtStmtExpr,
//                                                                           temp, temp->gtStmt.gtStmtExpr);

                        if  (temp->gtNext)
                        {
                            assert(temp->gtNext->gtPrev == temp);

                            temp->gtNext->gtPrev = tree;
                            tree->gtNext         = temp->gtNext;
                        }
                        else
                        {
                            tree->gtNext = 0;

                            assert(block->bbTreeList->gtPrev == temp);

                            block->bbTreeList->gtPrev = tree;
                        }
                    }
                }

            NOT_CAFFE:;

            }

        }

#endif

#if 0
        /* This optimization is currently broken.
           First of all, we should look for a different pattern
           (a[i+1] in the second statement).  Second, this is not
           so interesting without handling array bound checks (esp.
           when this occurs before bound checks are eliminated).   
        */
        
        if  (loadw && (opts.compFlags & CLFLG_TREETRANS))
        {
            GenTreePtr      last;

            for (tree = block->bbTreeList, last = 0;;)
            {
                GenTreePtr      nxts;

                GenTreePtr      exp1;
                GenTreePtr      exp2;

                GenTreePtr      op11;
                GenTreePtr      op12;
                GenTreePtr      op21;
                GenTreePtr      op22;

                GenTreePtr      indx;
                GenTreePtr      asg1;

                long            bas1;
                long            ind1;
                bool            mva1;
                long            ofs1;
                unsigned        mul1;

                long            bas2;
                long            ind2;
                bool            mva2;
                long            ofs2;
                unsigned        mul2;

                nxts = tree->gtNext;
                if  (!nxts)
                    break;

                assert(tree->gtOper == GT_STMT);
                exp1 = tree->gtStmt.gtStmtExpr;

                assert(nxts->gtOper == GT_STMT);
                exp2 = nxts->gtStmt.gtStmtExpr;

                /*
                    We're looking for the following statements:

                        x  =   a[i] & 0xFF;
                        x |= ((a[i] & 0xFF) << 8);
                 */

                if  (exp2->gtOper != GT_ASG_OR)
                    goto NEXT_WS;
                if  (exp1->gtOper != GT_ASG)
                    goto NEXT_WS;

                asg1 = exp1;

                op11 = exp1->gtOp.gtOp1;
                op21 = exp2->gtOp.gtOp1;

                if  (op11->gtOper != GT_LCL_VAR)
                    goto NEXT_WS;
                if  (op21->gtOper != GT_LCL_VAR)
                    goto NEXT_WS;
                if  (op11->gtLclVar.gtLclNum != op21->gtLclVar.gtLclNum)
                    goto NEXT_WS;

                op12 = exp1->gtOp.gtOp2;
                op22 = exp2->gtOp.gtOp2;

                /* The second operand should have "<< 8" on it */

                if  (op22->gtOper != GT_LSH)
                    goto NEXT_WS;
                op21 = op22->gtOp.gtOp2;
                if  (op21->gtOper != GT_CNS_INT)
                    goto NEXT_WS;
                if  (op21->gtIntCon.gtIconVal != 8)
                    goto NEXT_WS;
                op22 = op22->gtOp.gtOp1;

                /* Both operands should be "& 0xFF" */

                if  (op12->gtOper != GT_AND)
                    goto NEXT_WS;
                if  (op22->gtOper != GT_AND)
                    goto NEXT_WS;

                op11 = op12->gtOp.gtOp2;
                if  (op11->gtOper != GT_CNS_INT)
                    goto NEXT_WS;
                if  (op11->gtIntCon.gtIconVal != 0xFF)
                    goto NEXT_WS;
                op11 = op12->gtOp.gtOp1;

                op21 = op22->gtOp.gtOp2;
                if  (op21->gtOper != GT_CNS_INT)
                    goto NEXT_WS;
                if  (op21->gtIntCon.gtIconVal != 0xFF)
                    goto NEXT_WS;
                op21 = op22->gtOp.gtOp1;

                /* Both operands should be array index expressions */

                if  (op11->gtOper != GT_IND)
                    goto NEXT_WS;
                if  (op21->gtOper != GT_IND)
                    goto NEXT_WS;

                if  (op11->gtFlags & GTF_IND_RNGCHK)
                    goto NEXT_WS;
                if  (op21->gtFlags & GTF_IND_RNGCHK)
                    goto NEXT_WS;

                /* Break apart the index expression */

                if  (!gtCrackIndexExpr(op11, &indx, &ind1, &bas1, &mva1, &ofs1, &mul1))
                    goto NEXT_WS;
                if  (!gtCrackIndexExpr(op21, &indx, &ind2, &bas2, &mva2, &ofs2, &mul2))
                    goto NEXT_WS;

                if  (mva1 || mva2)   goto NEXT_WS;
                if  (ind1 != ind2)   goto NEXT_WS;
                if  (bas1 != bas2)   goto NEXT_WS;
                if  (ofs1 != ofs2-1) goto NEXT_WS;

                /* Got it - update the first expression */

                assert(op11->gtOper == GT_IND);
                assert(op11->gtType == TYP_BYTE);

                op11->gtType = TYP_CHAR;

                assert(asg1->gtOper == GT_ASG);
                asg1->gtOp.gtOp2 = asg1->gtOp.gtOp2->gtOp.gtOp1;

                /* Get rid of the second expression */

                nxts->gtStmt.gtStmtExpr = gtNewNothingNode();

            NEXT_WS:

                last = tree;
                tree = nxts;
            }
        }
#endif

        /*
            Check for certain stupid constructs some compilers might
            generate:

                1.      jump to a jump

                2.      conditional jump around an unconditional one
         */

        switch (block->bbJumpKind)
        {
            BasicBlock   *  nxtBlk;
            BasicBlock   *  jmpBlk;

            BasicBlock * *  jmpTab;
            unsigned        jmpCnt;

#if OPTIMIZE_TAIL_REC
            GenTreePtr      call;
#endif

        case BBJ_CALL:
        case BBJ_RET:
        case BBJ_THROW:
            break;

        case BBJ_COND:
            if (!opts.compDbgCode)
            {

COND_AGAIN:

#ifdef DEBUG
                BasicBlock  *   oldTarget;
                oldTarget = block->bbJumpDest;
#endif
            
                /* Update the GOTO target */

                block->bbJumpDest = block->bbJumpDest->bbJumpTarget();

#ifdef DEBUG
                if  (verbose)
                {
                    if  (block->bbJumpDest != oldTarget)
                        printf("Conditional jump to BB%02u changed to BB%02u\n",
                               oldTarget->bbNum, block->bbJumpDest->bbNum);
                }
#endif

                /*
                    Check for the following:

                        JCC skip           <-- block
                                                 V
                        JMP label          <-- nxtBlk
                                                 V
                    skip:                  <-- jmpBlk
                */

                nxtBlk = block->bbNext;
                jmpBlk = block->bbJumpDest;

                if  (nxtBlk->bbNext == jmpBlk)
                {
                    /* Is the next block just a jump? */
                  
                    if  (nxtBlk->bbJumpKind == BBJ_ALWAYS &&
                         nxtBlk->bbTreeList == 0          &&
                         ((nxtBlk->bbFlags & BBF_DONT_REMOVE) == 0) && // Prevent messing with exception table
                         nxtBlk->bbJumpDest != nxtBlk                  // skip infinite loops
                         )         
                    {
                        GenTreePtr      test;
                        
                        /* Reverse the jump condition */
                        
                        test = block->bbTreeList;
                        assert(test && test->gtOper == GT_STMT);
                        test = test->gtPrev;
                        assert(test && test->gtOper == GT_STMT);
                        
                        test = test->gtStmt.gtStmtExpr;
                        assert(test->gtOper == GT_JTRUE);
                        
                        test->gtOp.gtOp1 = gtReverseCond(test->gtOp.gtOp1);
                        
#ifdef DEBUG
                        if  (verbose)
                        {
                            printf("Reversing conditional jump to BB%02u changed to BB%02u\n",
                                    block->bbJumpDest->bbNum, nxtBlk->bbJumpDest->bbNum);
                            printf("Removing unecessary block BB%02u", nxtBlk->bbNum);
                        }
#endif
                        /*
                          Get rid of the following block; note that we can do
                          this even though other blocks could jump to it - the
                          reason is that elsewhere in this function we always
                          redirect jumps to jumps to jump to the final label,
                          and so even if someone targets the 'jump' block we
                          are about to delete it won't matter once we're done
                          since any such jump will be redirected to the final
                          target by the time we're done here.
                        */

                        block->bbNext     = jmpBlk;
                        block->bbJumpDest = nxtBlk->bbJumpDest;
                        
                        chgBBlst = true;
                        goto COND_AGAIN;
                    }
                }
            }
            break;

        case BBJ_RETURN:

#if OPTIMIZE_TAIL_REC

            if  (compCodeOpt() != SMALL_CODE)
            {
                /* Check for tail recursion */

                if (last && last->gtOper == GT_RETURN)
                {
                    call = last->gtOp.gtOp1;

                    if  (!call)
                        call = prev;

                    if  (!call || call->gtOper != GT_CALL)
                        goto NO_TAIL_REC;

                CHK_TAIL:

                    /* This must not be a virtual/interface call */

                    if  (call->gtFlags & (GTF_CALL_VIRT|GTF_CALL_INTF))
                        goto NO_TAIL_REC;

                    /* Get hold of the constant pool index */

                    gtCallTypes callType  = call->gtCall.gtCallType;

                    /* For now, only allow directly recursive calls */

                    if  (callType == CT_HELPER || !eeIsOurMethod(call->gtCall.gtCallMethHnd))
                        goto NO_TAIL_REC;

                    /* TEMP: only allow static calls */

                    if  (call->gtCall.gtCallObjp)
                        goto NO_TAIL_REC;

                    call->gtFlags |= GTF_CALL_TAILREC;

                    /* Was there a non-void return value? */

                    if  (block->bbJumpKind == BBJ_RETURN)
                    {
                        assert(last->gtOper == GT_RETURN);
                        if  (last->gtType != TYP_VOID)
                        {
                            /* We're not returning a value any more */

                            assert(last->gtOp.gtOp1 == call);

                            last->ChangeOper         (GT_CAST);
                            last->gtType            = TYP_VOID;
                            last->gtCast.gtCastType = TYP_VOID;
                        }
                    }

                    /* Convert the argument list, if non-empty */

                    if  (call->gtCall.gtCallArgs)
                        fgCnvTailRecArgList(&call->gtCall.gtCallArgs);

#ifdef DEBUG
                    if  (verbose)
                    {
                        printf("generate code for tail-recursive call:\n");
                        gtDispTree(call);
                        printf("\n");
                    }
#endif

                    /* Make the basic block jump back to the top */

                    block->bbJumpKind = BBJ_ALWAYS;
                    block->bbJumpDest = entry;

                    /* This makes the whole method into a loop */

                    entry->bbFlags |= BBF_LOOP_HEAD; 
                    fgMarkLoopHead(entry);

                    // @TODO [CONSIDER] [04/16/01] []:If this was the only return, completely
                    // @TODO [CONSIDER] [04/16/01] []:get rid of the return basic block.

                    break;
                }
            }

        NO_TAIL_REC:

            if  (block->bbJumpKind != BBJ_RETURN)
                break;

#endif

            /* Are we using one return code sequence? */

            if  (!genReturnBB || genReturnBB == block)
                break;

            if (block->bbFlags & BBF_HAS_JMP)
                break;

            /* We'll jump to the return label */

            block->bbJumpKind = BBJ_ALWAYS;
            block->bbJumpDest = genReturnBB;


            // Fall into the the 'always' case ...

        case BBJ_ALWAYS:

            if (!opts.compDbgCode)
            {
#ifdef DEBUG
                BasicBlock  *   oldTarget;
                oldTarget = block->bbJumpDest;
#endif
                /* Update the GOTO target */

                block->bbJumpDest = block->bbJumpDest->bbJumpTarget();

#ifdef DEBUG
                if  (verbose)
                {
                    if  (block->bbJumpDest != oldTarget)
                        printf("Unconditional jump to BB%02u changed to BB%02u\n",
                               oldTarget->bbNum, block->bbJumpDest->bbNum);
                }
#endif

                // Check for a jump to the very next block. We eliminate it 
                // unless its the one associated with a BBJ_CALL block

                if  (block->bbJumpDest == block->bbNext &&
                    (!bPrev || bPrev->bbJumpKind != BBJ_CALL))
                {
                    block->bbJumpKind = BBJ_NONE;
#ifdef DEBUG
                    if  (verbose)
                    {
                        printf("Unconditional jump to next basic block (BB%02u -> BB%02u)\n",
                               block->bbNum, block->bbJumpDest->bbNum);
                        printf("BB%02u becomes a BBJ_NONE\n\n", block->bbNum);
                    }
#endif
                }
            }

            break;

        case BBJ_NONE:

#if OPTIMIZE_TAIL_REC

            /* Check for tail recursion */

            if  ((compCodeOpt() != SMALL_CODE) &&
                 (last->gtOper     == GT_CALL   ))
            {
                if  (block->bbNext)
                {
                    BasicBlock  *   bnext = block->bbNext;
                    GenTree     *   retx;

                    if  (bnext->bbJumpKind != BBJ_RETURN)
                        break;

                    assert(bnext->bbTreeList && bnext->bbTreeList->gtOper == GT_STMT);

                    retx = bnext->bbTreeList->gtStmt.gtStmtExpr; assert(retx);

                    if  (retx->gtOper != GT_RETURN)
                        break;
                    if  (retx->gtOp.gtOp1)
                        break;
                }

                call = last;
                goto CHK_TAIL;
            }

#endif

            break;

        case BBJ_SWITCH:

            // @TODO [CONSIDER] [04/16/01] []: Move the default clause so that it's the next block

            jmpCnt = block->bbJumpSwt->bbsCount;
            jmpTab = block->bbJumpSwt->bbsDstTab;

            do
            {
                *jmpTab = (*jmpTab)->bbJumpTarget();
            }
            while (++jmpTab, --jmpCnt);

            // Convert the trivial cases.

            if      (block->bbJumpSwt->bbsCount == 1)
            {
                /* Use BBJ_ALWAYS for a switch with only a default clause */

                GenTreePtr switchStmt = block->bbTreeList->gtPrev;
                GenTreePtr switchTree = switchStmt->gtStmt.gtStmtExpr;
                assert(switchTree->gtOper == GT_SWITCH);
                assert(switchTree->gtOp.gtOp1->gtType <= TYP_INT);

                // Keep the switchVal around for evaluating any side-effects
                switchTree->ChangeOper(GT_NOP);
                assert(gtIsaNothingNode(switchTree));

                block->bbJumpDest = block->bbJumpSwt->bbsDstTab[0];
                block->bbJumpKind = BBJ_ALWAYS;
            }
            else if (block->bbJumpSwt->bbsCount == 2 &&
                     block->bbJumpSwt->bbsDstTab[1] == block->bbNext)
            {
                /* Use BBJ_COND(switchVal==0) for a switch with only one
                   significant clause besides the default clause, if the
                   default clause is bbNext */

                GenTreePtr switchStmt = block->bbTreeList->gtPrev;
                GenTreePtr switchTree = switchStmt->gtStmt.gtStmtExpr;
                assert(switchTree->gtOper == GT_SWITCH && switchTree->gtType == TYP_VOID);
                assert(switchTree->gtOp.gtOp1->gtType <= TYP_INT);

                // Change the GT_SWITCH(switchVal) into GT_JTRUE(GT_EQ(switchVal==0))

                switchTree->ChangeOper(GT_JTRUE);
                switchTree->gtOp.gtOp1 = gtNewOperNode(GT_EQ, TYP_INT,
                                                       switchTree->gtOp.gtOp1,
                                                       gtNewZeroConNode(TYP_INT));

                block->bbJumpDest = block->bbJumpSwt->bbsDstTab[0];
                block->bbJumpKind = BBJ_COND;
            }

            break;
        }

        bPrev       = block;
        block       = block->bbNext;
    }
    while (block);

    /* We are done with the global morphing phase */

    fgGlobalMorph   = false;
    fgAssertionProp = false;

#if TGT_RISC

    /* Do we need to add a exitCrit call at the end? */

    if  (genMonExitExp)
    {
        unsigned        retTmp;
        GenTreePtr      retExp;
        GenTreePtr      retStm;

        var_types       retTyp = genActualType(info.compRetType);

        assert(retTyp != TYP_VOID);
        assert(genReturnLtm == 0);

        /* Grab a temp for the return value */

        retTmp = lvaGrabTemp();

        /* Assign the return value to the temp */

        retExp = gtNewOperNode(GT_RETURN, retTyp);

        /* The value will be present in the return register(s) */

        retExp->gtFlags |= GTF_REG_VAL;
        retExp->gtRegNum = (genTypeStSz(retTyp) == 1) ? (regNumber)REG_INTRET
                                                      : (regNumber)REG_LNGRET;

        retExp = gtNewTempAssign(retTmp, retExp);

        /* Create the expression "tmp = <retreg> , exitCrit" */

        retStm = gtNewOperNode(GT_COMMA, TYP_VOID, retExp, genMonExitExp);

        /* Now append the final return value */

        retExp = gtNewLclvNode(retTmp, retTyp);
        retExp = gtNewOperNode(GT_RETURN, retTyp, retExp);
        retStm = gtNewOperNode(GT_COMMA , retTyp, retStm, retExp);

        /* Make the whole thing into a 'return' expression */

        retExp = gtNewOperNode(GT_RETURN, retTyp, retStm);

        /* Add the return expression to the return block */

        retStm = fgStoreFirstTree(genReturnBB, retExp);

        /* Make sure we don't mess up when we morph the final expression */

        genMonExitExp = NULL;

#ifdef DEBUG
        if (verbose)
        {
            printf("\nAdded exitCrit to Synchronized method [%08X]\n", genReturnBB);
            gtDispTree(retExp, 0);
            printf("\n");
        }
#endif

        /* Make sure the exitCrit call gets morphed */

        fgMorphStmt = retStm;
        assert(retStm->gtStmt.gtStmtExpr == retExp);
        retStm->gtStmt.gtStmtExpr = retExp = fgMorphTree(retExp);
    }

#endif

#ifdef DEBUG
    if  (verboseTrees) fgDispBasicBlocks(true);
#endif

    return chgBBlst;
}


/*****************************************************************************
 *
 *  Make some decisions about the kind of code to generate.
 */

void                Compiler::fgSetOptions()
{

    /* Should we force fully interruptible code ? */

#ifdef DEBUG
    static ConfigDWORD fJitFullyInt(L"JitFullyInt");
    if (fJitFullyInt.val())
    {
        assert(genIntrptibleUse == false);

        genInterruptible = true;
    }
#endif

#ifdef DEBUGGING_SUPPORT
    if (opts.compDbgCode)
    {
        assert(genIntrptibleUse == false);

        genInterruptible = true;        // debugging is easier this way ...
    }
#endif

    /* Assume we won't need an explicit stack frame if this is allowed */

#if TGT_x86

    // CORINFO_HELP_TAILCALL wont work with localloc because of the restoring of
    // the callee-saved registers.
    assert(!compTailCallUsed || !compLocallocUsed);

    if (compLocallocUsed || compTailCallUsed)
    {
        genFPreqd   = true;
    }

    genFPreqd |= !genFPopt || info.compXcptnsCount;

    /*  fpPtrArgCntMax records the maximum number of pushed arguments
     *  Our implementation uses a 32-bit argMask in the GC encodings 
     *  so we force it to have an EBP frame if we push more than 32 items
     */
    if (fgPtrArgCntMax >= 32)
    {
        genFPreqd        = true;
        genInterruptible = false; 
    }


#endif

#if INLINE_NDIRECT
    if (info.compCallUnmanaged)
    {
#if TGT_x86
        genFPreqd = true;
#endif
    }
#endif

    if  (opts.compNeedSecurityCheck)
    {
#if TGT_x86
        genFPreqd = true;
#endif
    }

    /* Record the max. number of arguments */

#if TGT_RISC
    genMaxCallArgs = fgPtrArgCntMax * sizeof(int);
#endif

//  printf("method will %s be fully interruptible\n", genInterruptible ? "   " : "not");
}


/*****************************************************************************
 *
 *  Transform all basic blocks for codegen.
 */

void                Compiler::fgMorph()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In fgMorph()\n");
#endif
    
    // Insert call to class constructor as the first basic block if 
    // we were asked to do so.  
    if (info.compFlags & CORINFO_FLG_RUN_CCTOR) 
        fgPrependBB(fgGetStaticsBlock(info.compClassHnd));

#ifdef PROFILER_SUPPORT
    // If necessary, insert call to profiler
    if (opts.compEnterLeaveEventCB && opts.compInprocDebuggerActiveCB)
    {
#if TGT_x86
        BOOL            bHookFunction = TRUE;
        CORINFO_PROFILING_HANDLE handle, *pHandle;

        // Get the appropriate handles
        handle = eeGetProfilingHandle(info.compMethodHnd, &bHookFunction, &pHandle);

        // Only if the profiler chose to hook this particular function do we actually do it
        if (bHookFunction)
        {
            GenTreePtr op;

            // Create the argument
            op = gtNewIconEmbHndNode((void *)handle, (void *)pHandle, GTF_ICON_METHOD_HDL);
            op = gtNewArgList(op);

            // Create the call with the argument tree
            op = gtNewHelperCallNode(CORINFO_HELP_PROF_FCN_ENTER, TYP_VOID, 0, op);

            // Now emit the code
            fgPrependBB(op);
        }
#endif // TGT_x86
    }
#endif // PROFILER_SUPPORT

#ifdef DEBUG
    if (opts.compGcChecks) {
        for (unsigned i = 0; i < info.compArgsCount; i++) 
        {
            if (lvaTable[i].TypeGet() == TYP_REF) 
            {
                    // confirm that the argument is a GC pointer (for debugging (GC stress))
                GenTreePtr op = gtNewLclvNode(i, TYP_REF);
                op = gtNewArgList(op);
                op = gtNewHelperCallNode(CORINFO_HELP_CHECK_OBJ, TYP_VOID, 0, op);
                fgPrependBB(op);
            }
        }
    }

    if (opts.compStackCheckOnRet) 
    {
        lvaReturnEspCheck = lvaGrabTemp(false);
        lvaTable[lvaReturnEspCheck].lvVolatile = 1;
        lvaTable[lvaReturnEspCheck].lvType = TYP_INT;
        lvaTable[lvaReturnEspCheck].lvRefCnt = 1;
        lvaTable[lvaReturnEspCheck].lvRefCntWtd = 1;
    }

    if (opts.compStackCheckOnCall) 
    {
        lvaCallEspCheck = lvaGrabTemp(false);
        lvaTable[lvaCallEspCheck].lvVolatile = 1;
        lvaTable[lvaCallEspCheck].lvType = TYP_INT;
        lvaTable[lvaCallEspCheck].lvRefCnt = 1;
        lvaTable[lvaCallEspCheck].lvRefCntWtd = 1;
    }


#endif 

    /* Filter out unimported BBs */

    fgRemoveEmptyBlocks();
    
#if HOIST_THIS_FLDS
    if (!opts.compDbgCode && !opts.compMinOptim)
    {
        /* Figure out which field refs should be hoisted */

        optHoistTFRoptimize();
    }
#endif

    bool chgBBlst = false; // has the basic block list been changed

    /* Add any internal blocks/trees we may need */

    chgBBlst |= fgAddInternal();

    /* To prevent recursive expansion in the inliner initialize
     * the list of inlined methods with the current method info
     * @TODO [CONSIDER] [04/16/01] []: Sometimes it may actually 
     * help benchmarks to inline several levels (i.e. Tak or Hanoi) */

    inlExpLst   initExpDsc;

    initExpDsc.ixlMeth = info.compMethodHnd;
    initExpDsc.ixlNext = 0;
    fgInlineExpList = &initExpDsc;

#if OPT_BOOL_OPS
    fgMultipleNots = false;
#endif

    /* Morph the trees in all the blocks of the method */

    chgBBlst |= fgMorphBlocks();

    /* Decide the kind of code we want to generate */

    fgSetOptions();

#ifdef  DEBUG
    compCurBB = 0;
#endif
}


/*****************************************************************************
 *
 *  Helper for Compiler::fgPerBlockDataFlow().
 *  The goal is to compute the USE and DEF sets for a basic block.
 *  However with the new improvement to the DFA analysis,
 *  we do not mark x as used in x = f(x) when there are no side effects in f(x).
 *  'asgdLclVar' is set when 'tree' is part of an expression with no side-effects
 *  which is assigned to asgdLclVar, ie. asgdLclVar = (... tree ...)
 */

inline
void                 Compiler::fgMarkUseDef(GenTreePtr tree, GenTreePtr asgdLclVar)
{
    bool            rhsUSEDEF = false;
    unsigned        lclNum, lhsLclNum;
    LclVarDsc   *   varDsc;

    assert(tree->gtOper == GT_LCL_VAR);
    lclNum = tree->gtLclVar.gtLclNum;

    assert(lclNum < lvaCount);
    varDsc = lvaTable + lclNum;

    if (asgdLclVar)
    {
        /* we have an assignment to a local var : asgdLclVar = ... tree ...
         * check for x = f(x) case */

        assert(asgdLclVar->gtOper == GT_LCL_VAR);
        assert(asgdLclVar->gtFlags & GTF_VAR_DEF);

        lhsLclNum = asgdLclVar->gtLclVar.gtLclNum;

        if ((lhsLclNum == lclNum) &&
            ((tree->gtFlags & GTF_VAR_DEF) == 0) &&
            (tree != asgdLclVar) )
        {
            /* bingo - we have an x = f(x) case */
            asgdLclVar->gtFlags |= GTF_VAR_USEDEF;
            rhsUSEDEF = true;
        }
    }

    /* Is this a tracked variable? */

    if  (varDsc->lvTracked)
    {
        VARSET_TP       bitMask;

        assert(varDsc->lvVarIndex < lvaTrackedCount);

        bitMask = genVarIndexToBit(varDsc->lvVarIndex);

        if  ((tree->gtFlags & GTF_VAR_DEF) != 0 &&
             (tree->gtFlags & (GTF_VAR_USEASG | GTF_VAR_USEDEF)) == 0)
        {
//          if  (!(fgCurUseSet & bitMask)) printf("V%02u,T%02u def at %08X\n", lclNum, varDsc->lvVarIndex, tree);
            if  (!(fgCurUseSet & bitMask))
                fgCurDefSet |= bitMask;
        }
        else
        {
//          if  (!(fgCurDefSet & bitMask)) printf("V%02u,T%02u use at %08X\n", lclNum, varDsc->lvVarIndex, tree);

            /* We have the following scenarios:
             *   1. "x += something" - in this case x is flagged GTF_VAR_USEASG
             *   2. "x = ... x ..." - the LHS x is flagged GTF_VAR_USEDEF,
             *                        the RHS x is has rhsUSEDEF = true
             *                        (both set by the code above)
             *
             * We should not mark an USE of x in the above cases provided the value "x" is not used
             * further up in the tree. For example "while(i++)" is required to mark i as used.
             */

            /* make sure we don't include USEDEF variables in the USE set
             * The first test is for LSH, the second (!rhsUSEDEF) is for any var in the RHS */

            if  ((tree->gtFlags & (GTF_VAR_USEASG | GTF_VAR_USEDEF)) == 0)
            {
                /* Not a special flag - check to see if used to assign to itself */

                if (rhsUSEDEF)
                {
                    /* assign to itself - do not include it in the USE set */
                    if (!opts.compMinOptim && !opts.compDbgCode)
                        return;
                }
            }
            else
            {
                /* Special flag variable - make sure it is not used up the tree */

                GenTreePtr oper = tree->gtNext;
                assert(oper->OperKind() & GTK_ASGOP);

                // If oper is a asgop ovf we want to mark a use as we will need it in order
                // to mantain the side effects
                if (!oper->gtOverflowEx())
                {
                    /* not used if the next node is NULL */
                    if (oper->gtNext == 0)
                        return;

                    /* under a GT_COMMA, if used it will be marked as such later */

                    if (oper->gtNext->gtOper == GT_COMMA)
                        return;
                }
            }

            /* Fall through for the "good" cases above - add the variable to the USE set */

            if  (!(fgCurDefSet & bitMask))
                fgCurUseSet |= bitMask;
        }
    }

    return;
}

/*****************************************************************************/
#if TGT_x86
/*****************************************************************************/

void                Compiler::fgComputeFPlvls(GenTreePtr tree)
{
    genTreeOps      oper;
    unsigned        kind;
    bool            isflt;

    unsigned        savFPstkLevel;

    assert(tree);
    assert(tree->gtOper != GT_STMT);

    /* Figure out what kind of a node we have */

    oper  = tree->OperGet();
    kind  = tree->OperKind();
    isflt = varTypeIsFloating(tree->TypeGet()) ? 1 : 0;

    /* Is this a constant or leaf node? */

    if  (kind & (GTK_CONST|GTK_LEAF))
    {
        genFPstkLevel += isflt;
        goto DONE;
    }

    /* Is it a 'simple' unary/binary operator? */

    if  (kind & GTK_SMPOP)
    {
        GenTreePtr      op1 = tree->gtOp.gtOp1;
        GenTreePtr      op2 = tree->gtGetOp2();

        /* Check for some special cases */

        switch (oper)
        {
        case GT_IND:

            fgComputeFPlvls(op1);

            /* Indirect loads of FP values push a new value on the FP stack */

            genFPstkLevel += isflt;
            goto DONE;

        case GT_CAST:

            fgComputeFPlvls(op1);

            /* Casts between non-FP and FP push on / pop from the FP stack */

            if  (varTypeIsFloating(op1->TypeGet()))
            {
                if  (isflt == false)
                    genFPstkLevel--;
            }
            else
            {
                if  (isflt != false)
                    genFPstkLevel++;
            }

            goto DONE;

        case GT_LIST:   /* GT_LIST presumably part of an argument list */
        case GT_COMMA:  /* Comma tosses the result of the left operand */

            savFPstkLevel = genFPstkLevel;
            fgComputeFPlvls(op1);
            genFPstkLevel = savFPstkLevel;

            if  (op2)
                fgComputeFPlvls(op2);

            goto DONE;
        }

        if  (!op1)
        {
            if  (!op2)
                goto DONE;

            fgComputeFPlvls(op2);
            goto DONE;
        }

        if  (!op2)
        {
            fgComputeFPlvls(op1);
            if (oper == GT_ADDR)
            {
                /* If the operand was floating point pop the value from the stack */
                if (varTypeIsFloating(op1->TypeGet()))
                {
                    assert(genFPstkLevel);
                    genFPstkLevel--;
                }
            }

            // This is a special case to handle the following
            // optimization: conv.i4(round.d(d)) -> round.i(d) 
            // if flowgraph 3186
            // @TODO [CONSIDER] [04/16/01] [dnotario]: using another intrinsic in this optimization
            // or marking with a special flag. This type of special
            // cases is not good. 
            if (oper==GT_MATH && tree->gtMath.gtMathFN==CORINFO_INTRINSIC_Round &&
                tree->TypeGet()==TYP_INT)
            {
                genFPstkLevel--;
            }
            
            goto DONE;
        }

        /* FP assignments need a bit special handling */

        if  (isflt && (kind & GTK_ASGOP))
        {
            /* The target of the assignment won't get pushed */

            if  (tree->gtFlags & GTF_REVERSE_OPS)
            {
                fgComputeFPlvls(op2);
                fgComputeFPlvls(op1);
                 op1->gtFPlvl--;
                genFPstkLevel--;
            }
            else
            {
                fgComputeFPlvls(op1);
                 op1->gtFPlvl--;
                genFPstkLevel--;
                fgComputeFPlvls(op2);
            }

            genFPstkLevel--;
            goto DONE;
        }

        /* Here we have a binary operator; visit operands in proper order */

        if  (tree->gtFlags & GTF_REVERSE_OPS)
        {
            fgComputeFPlvls(op2);
            fgComputeFPlvls(op1);
        }
        else
        {
            fgComputeFPlvls(op1);
            fgComputeFPlvls(op2);
        }

        /*
            Binary FP operators pop 2 operands and produce 1 result;
            assignments consume 1 value and don't produce any.
         */

        if  (isflt)
            genFPstkLevel--;

        /* Float compares remove both operands from the FP stack */

        if  (kind & GTK_RELOP)
        {
            if  (varTypeIsFloating(op1->TypeGet()))
                genFPstkLevel -= 2;
        }

        goto DONE;
    }

    /* See what kind of a special operator we have here */

    switch  (oper)
    {
    case GT_FIELD:
        fgComputeFPlvls(tree->gtField.gtFldObj);
        genFPstkLevel += isflt;
        break;

    case GT_CALL:

        if  (tree->gtCall.gtCallObjp)
            fgComputeFPlvls(tree->gtCall.gtCallObjp);

        if  (tree->gtCall.gtCallArgs)
        {
            savFPstkLevel = genFPstkLevel;
            fgComputeFPlvls(tree->gtCall.gtCallArgs);
            genFPstkLevel = savFPstkLevel;
        }

        if  (tree->gtCall.gtCallRegArgs)
        {
            savFPstkLevel = genFPstkLevel;
            fgComputeFPlvls(tree->gtCall.gtCallRegArgs);
            genFPstkLevel = savFPstkLevel;
        }

        genFPstkLevel += isflt;
        break;

    case GT_ARR_ELEM:

        fgComputeFPlvls(tree->gtArrElem.gtArrObj);

        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
            fgComputeFPlvls(tree->gtArrElem.gtArrInds[dim]);

        /* Loads of FP values push a new value on the FP stack */
        genFPstkLevel += isflt;
        break;

#ifdef DEBUG
    default:
        assert(!"Unhandled special operator in fgComputeFPlvls()");
        break;
#endif
    }

DONE:

    tree->gtFPlvl = genFPstkLevel;

    assert(int(genFPstkLevel) >= 0);
}

/*****************************************************************************/
#endif//TGT_x86
/*****************************************************************************/

void                Compiler::fgFindOperOrder()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In fgFindOperOrder()\n");
#endif
    BasicBlock *    block;
    GenTreePtr      stmt;

    /* Walk the basic blocks and for each statement determine
     * the evaluation order, cost, FP levels, etc... */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            /* Recursively process the statement */

            assert(stmt->gtOper == GT_STMT);

            gtSetStmtInfo(stmt);
        }
    }
}


/*****************************************************************************/

void                Compiler::fgDataFlowInit()
{
    /* If we're not optimizing at all, things are simple */

    if  (opts.compMinOptim)
    {
        /* Simply assume that all variables interfere with each other */

        assert(lvaSortAgain == false);  // Should not be set unless optimizing

        memset(lvaVarIntf, 0xFF, sizeof(lvaVarIntf));
        return;
    }
    else
    {
        memset(lvaVarIntf, 0, sizeof(lvaVarIntf));
    }

    /* Zero out the raFOlvlLife table, as fgComputeLife will fill it in */
    memset(raFPlvlLife,  0, sizeof(raFPlvlLife));
    
    /* This is our last chance to add any extra tracked variables */

    /*   if necessary, re-sort the variable table by ref-count    */

    if (lvaSortAgain)
        lvaSortByRefCount();
}

/*****************************************************************************/

void                Compiler::fgPerBlockDataFlow()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In fgPerBlockDataFlow()\n");
    if  (verboseTrees) fgDispBasicBlocks(true);
#endif
    BasicBlock *    block;

#if CAN_DISABLE_DFA

    /* If we're not optimizing at all, things are simple */

    if  (opts.compMinOptim)
    {
        unsigned        lclNum;
        LclVarDsc   *   varDsc;

        VARSET_TP       liveAll = 0;

        /* We simply make everything live everywhere */

        for (lclNum = 0, varDsc = lvaTable;
             lclNum < lvaCount;
             lclNum++  , varDsc++)
        {
            if  (varDsc->lvTracked)
                liveAll |= genVarIndexToBit(varDsc->lvVarIndex);
        }

        for (block = fgFirstBB; block; block = block->bbNext)
        {
            GenTreePtr      stmt;
            GenTreePtr      tree;

            block->bbLiveIn  =
            block->bbLiveOut = liveAll;

            switch (block->bbJumpKind)
            {
            case BBJ_RET:
                if (block->bbFlags & BBF_ENDFILTER)
                    break;
            case BBJ_THROW:
            case BBJ_RETURN:
                block->bbLiveOut = 0;
                break;
            }

            for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
            {
                assert(stmt->gtOper == GT_STMT);

                for (tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
                {
//                  printf("[%08X]\n", tree);
//                  if  ((int)tree == 0x011d0ab4) debugStop(0);
                    tree->gtLiveSet = liveAll;
                }
            } 
        }

        return;
    }

#endif

    if  (opts.compMinOptim || opts.compDbgCode)
        goto NO_IX_OPT;

    /* Locate all index operations and assign indices to them */
    /* FIX NOW.  This can be put somewhere else - vancem */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        GenTreePtr      stmt;
        GenTreePtr      tree;

        /* Walk the statement trees in this basic block */

        for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

            for (tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
            {
                if (tree->gtOper == GT_ARR_LENGTH)
                {
                    GenTreePtr      con;
                    GenTreePtr      arr;
                    GenTreePtr      add;

                    /* Create the expression "*(array_addr + ArrLenOffs)" */

                    arr = tree->gtOp.gtOp1;
                    assert(arr->gtNext == tree);
    
                    assert(tree->gtArrLenOffset() == offsetof(CORINFO_Array, length) ||
                           tree->gtArrLenOffset() == offsetof(CORINFO_String, stringLen));
                    
                    if ((arr->gtOper == GT_CNS_INT) &&
                        (arr->gtIntCon.gtIconVal == 0))
                    {
                        // If the array is NULL, then we should get a NULL reference
                        // exception when computing its length.  We need to maintain
                        // an invariant where there is no sum of two constants node, so
                        // let's simply return an indirection of NULL.

                        add = arr;
                    }
                    else
                    {

                        con = gtNewIconNode(tree->gtArrLenOffset(), TYP_INT);
                        con->gtRsvdRegs = 0;
#if TGT_x86
                        con->gtFPlvl    = arr->gtFPlvl;
#else
                        con->gtIntfRegs = arr->gtIntfRegs;
#endif
                        add = gtNewOperNode(GT_ADD, TYP_REF, arr, con);
                        add->gtRsvdRegs = arr->gtRsvdRegs;
#if TGT_x86
                        add->gtFPlvl    = arr->gtFPlvl;
#else
                        add->gtIntfRegs = arr->gtIntfRegs;
#endif
                        arr->gtNext = con;
                                      con->gtPrev = arr;

                        con->gtNext = add;
                                      add->gtPrev = con;

                        add->gtNext = tree;
                                      tree->gtPrev = add;
                    }
                    
                    // GT_IND is a large node, but its OK if GTF_IND_RNGCHK is not set
                    assert(!(tree->gtFlags & GTF_IND_RNGCHK));
                    tree->ChangeOperUnchecked(GT_IND);

                    tree->gtOp.gtOp1 = add;
                }
            }
        }
    }

NO_IX_OPT:

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        GenTreePtr      stmt;
        GenTreePtr      tree;
        GenTreePtr      lshNode;

        fgCurUseSet = fgCurDefSet = 0;
        compCurBB   = block;

        for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

            lshNode = 0;
            tree = stmt->gtStmt.gtStmtExpr;
            assert(tree);

            /* the following is to check if we have an assignment expression
             * which may become a GTF_VAR_USEDEF - x=f(x) */

            if (tree->gtOper == GT_ASG)
            {
                assert(tree->gtOp.gtOp1);
                assert(tree->gtOp.gtOp2);

                /* consider if LHS is local var - ignore if RHS contains SIDE_EFFECTS */

                if ((tree->gtOp.gtOp1->gtOper == GT_LCL_VAR) &&
                    ((tree->gtOp.gtOp2->gtFlags & GTF_SIDE_EFFECT) == 0))
                {
                    /* Assignement to local var with no SIDE EFFECTS.
                     * Set lshNode so that genMarkUseDef will flag potential
                     * x=f(x) expressions as GTF_VAR_USEDEF */

                    lshNode = tree->gtOp.gtOp1;
                    assert(lshNode->gtOper == GT_LCL_VAR);
                    assert(lshNode->gtFlags & GTF_VAR_DEF);
                }
            }

            for (tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
            {
                switch (tree->gtOper)
                {
                case GT_LCL_VAR:
                    fgMarkUseDef(tree, lshNode);
                    break;

                case GT_IND:

                    /* We can add in call to error routine now */

                    if  (tree->gtFlags & GTF_IND_RNGCHK)
                    {
                        fgSetRngChkTarget(tree, false);
                    }
                    break;
#if INLINE_NDIRECT
                case GT_CALL:

                    /* Get the TCB local and mark it as used */

                    if  (tree->gtFlags & GTF_CALL_UNMANAGED)
                    {
                        assert(info.compLvFrameListRoot < lvaCount);

                        LclVarDsc * varDsc = &lvaTable[info.compLvFrameListRoot];

                        if (varDsc->lvTracked)
                            fgCurUseSet |= genVarIndexToBit(varDsc->lvVarIndex);
                    }
                    break;
#endif
                }
            }
        }

#if INLINE_NDIRECT

        /* Get the TCB local and mark it as used */

        if  (block->bbJumpKind == BBJ_RETURN && info.compCallUnmanaged)
        {
            assert(info.compLvFrameListRoot < lvaCount);

            LclVarDsc * varDsc = &lvaTable[info.compLvFrameListRoot];

            if (varDsc->lvTracked)
                fgCurUseSet |= genVarIndexToBit(varDsc->lvVarIndex);
        }
#endif

#ifdef  DEBUG
        if  (verbose)
        {
            VARSET_TP allVars = (fgCurUseSet |  fgCurDefSet);
            printf("BB%02u", block->bbNum);
            printf(      " USE="); lvaDispVarSet(fgCurUseSet, allVars);
            printf("\n     DEF="); lvaDispVarSet(fgCurDefSet, allVars);
            printf("\n\n");
        }
#endif

        block->bbVarUse = fgCurUseSet;
        block->bbVarDef = fgCurDefSet;

        /* also initialize the IN set, just in case we will do multiple DFAs */

        block->bbLiveIn = 0;
        //block->bbLiveOut = 0;
    }

#ifdef DEBUG
    if (verbose && fgRngChkThrowAdded)
    {
        printf("\nAfter fgPerBlockDataFlow() added some RngChk throw blocks");
        fgDispBasicBlocks();
        printf("\n");
    }
#endif
}


/*****************************************************************************/

#ifdef DEBUGGING_SUPPORT

// Helper functions to mark variables live over their entire scope

/* static */
void                Compiler::fgBeginScopeLife(LocalVarDsc *var, unsigned clientData)
{
    assert(clientData);
    Compiler * _this = (Compiler *)clientData;
    assert(var);

    LclVarDsc * lclVarDsc1 = & _this->lvaTable[var->lvdVarNum];

    if (lclVarDsc1->lvTracked)
        _this->fgLiveCb |= genVarIndexToBit(lclVarDsc1->lvVarIndex);
}

/* static */
void                Compiler::fgEndScopeLife(LocalVarDsc * var, unsigned clientData)
{
    assert(clientData);
    Compiler * _this = (Compiler *)clientData;
    assert(var);

    LclVarDsc * lclVarDsc1 = &_this->lvaTable[var->lvdVarNum];

    if (lclVarDsc1->lvTracked)
        _this->fgLiveCb &= ~genVarIndexToBit(lclVarDsc1->lvVarIndex);
}

#endif

/*****************************************************************************/
#ifdef DEBUGGING_SUPPORT
/*****************************************************************************/

void                fgMarkInScope(BasicBlock * block, VARSET_TP inScope)
{
    /* Record which vars are artifically kept alive for debugging */

    block->bbScope    = inScope;

    /* Being in scope implies a use of the variable. Add the var to bbVarUse
       so that redoing fgLiveVarAnalisys() will work correctly */

    block->bbVarUse  |= inScope;

    /* Artifically mark all vars in scope as alive */

    block->bbLiveIn  |= inScope;
    block->bbLiveOut |= inScope;
}


void                fgUnmarkInScope(BasicBlock * block, VARSET_TP unmarkScope)
{
    assert((block->bbScope & unmarkScope) == unmarkScope);

    /* Record which vars are artifically kept alive for debugging */

    block->bbScope    &= ~unmarkScope;

    /* Being in scope implies a use of the variable. Add the var to bbVarUse
       so that redoing fgLiveVarAnalisys() will work correctly */

    block->bbVarUse  &= ~unmarkScope;

    /* Artifically mark all vars in scope as alive */

    block->bbLiveIn  &= ~unmarkScope;
    block->bbLiveOut &= ~unmarkScope;
}


/*****************************************************************************
 *
 * For debuggable code, we allow redundant assignments to vars
 * by marking them live over their entire scope
 */

void                Compiler::fgExtendDbgLifetimes()
{
    assert(opts.compDbgCode && info.compLocalVarsCount>0);

#ifdef  DEBUG
    if  (verbose)
        printf("\nMarking vars alive over their enitre scope :\n\n");
#endif // DEBUG

    /*-------------------------------------------------------------------------
     *   Extend the lifetimes over the entire reported scope of the variable.
     */

    compResetScopeLists();

    fgLiveCb = 0;
    compProcessScopesUntil(0, fgBeginScopeLife, fgEndScopeLife, (unsigned)this);
    VARSET_TP   inScope = fgLiveCb;
    unsigned    lastEndOffs = 0;

    // Mark all tracked LocalVars live over their scope - walk the blocks
    // keeping track of the current life, and assign it to the blocks.

    for (BasicBlock * block = fgFirstBB; block; block = block->bbNext)
    {
        // If a block doesnt correspond to any instrs, it can have no
        // scopes defined on its boundaries

        if ((block->bbFlags & BBF_INTERNAL) || block->bbCodeSize==0)
        {
            fgMarkInScope(block, inScope);
            continue;
        }

        // Find scopes becoming alive. If there is a gap in the instr
        // sequence, we need to process any scopes on those missing offsets.

        if (lastEndOffs != block->bbCodeOffs)
        {
            assert(lastEndOffs < block->bbCodeOffs);

            fgLiveCb = inScope;
            compProcessScopesUntil (block->bbCodeOffs,
                                   fgBeginScopeLife, fgEndScopeLife,
                                   (unsigned) this);
            inScope  = fgLiveCb;
        }
        else
        {
            while (LocalVarDsc * LocalVarDsc1 = compGetNextEnterScope(block->bbCodeOffs))
            {
                LclVarDsc * lclVarDsc1 = &lvaTable[LocalVarDsc1->lvdVarNum];

                if (!lclVarDsc1->lvTracked)
                    continue;

                inScope |= genVarIndexToBit(lclVarDsc1->lvVarIndex);
            }
        }

        fgMarkInScope(block, inScope);

        // Find scopes going dead.

        while (LocalVarDsc * LocalVarDsc1 = compGetNextExitScope(block->bbCodeOffs+block->bbCodeSize))
        {
            LclVarDsc * lclVarDsc1 = &lvaTable[LocalVarDsc1->lvdVarNum];

            if (!lclVarDsc1->lvTracked)
                continue;

            inScope &= ~genVarIndexToBit(lclVarDsc1->lvVarIndex);
        }

        lastEndOffs = block->bbCodeOffs + block->bbCodeSize;
    }

    /* Everything should be out of scope by the end of the method. But if the
       last BB got removed, then inScope may not be 0 */

    assert(inScope == 0 || lastEndOffs < info.compCodeSize);

    /*-------------------------------------------------------------------------
     * Partly update liveness info so that we handle any funky BBF_INTERNAL
     * blocks inserted out of sequence.
     */

#ifdef DEBUG
    if (verbose && 0) fgDispBBLiveness();
#endif

    fgLiveVarAnalisys(true);

    /* For compDbgCode, we prepend an empty BB which will hold the
       initializations of variables which are in scope at IL offset 0 (but
       not initialized by the IL code) yet. Since they will currently be
       marked as live on entry to fgFirstBB, unmark the liveness so that
       the following code will know to add the initializations. */

    assert((fgFirstBB->bbFlags & BBF_INTERNAL) &&
           (fgFirstBB->bbJumpKind == BBJ_NONE));

    VARSET_TP   trackedArgs = 0;
    LclVarDsc * argDsc     = &lvaTable[0];
    for (unsigned argNum = 0; argNum < info.compArgsCount; argNum++, argDsc++)
    {
        assert(argDsc->lvIsParam);
        if (argDsc->lvTracked)
        {
            VARSET_TP curArgBit = genVarIndexToBit(argDsc->lvVarIndex);
            assert((trackedArgs & curArgBit) == 0); // Each arg should define a different bit.
            trackedArgs |= curArgBit;
        }
    }

    fgUnmarkInScope(fgFirstBB, fgFirstBB->bbScope & ~trackedArgs);

    /*-------------------------------------------------------------------------
     * As we keep variables artifically alive over their entire scope,
     * we need to also artificially initialize them if the scope does
     * not exactly match the real lifetimes, or they will contain
     * garbage until they are initialized by the IL code
     */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        VARSET_TP initVars = 0; // Vars which are artificially made alive

        switch(block->bbJumpKind)
        {
        case BBJ_NONE:      initVars |= block->bbNext->bbScope;     break;

        case BBJ_ALWAYS:    initVars |= block->bbJumpDest->bbScope; break;

        case BBJ_CALL:      
                            if (!(block->bbFlags & BBF_RETLESS_CALL))
                            {
                                initVars |= block->bbNext->bbScope;
                            }
                            initVars |= block->bbJumpDest->bbScope; break;

        case BBJ_COND:      initVars |= block->bbNext->bbScope;
                            initVars |= block->bbJumpDest->bbScope; break;

        case BBJ_SWITCH:    BasicBlock * *  jmpTab;
                            unsigned        jmpCnt;

                            jmpCnt = block->bbJumpSwt->bbsCount;
                            jmpTab = block->bbJumpSwt->bbsDstTab;

                            do
                            {
                                initVars |= (*jmpTab)->bbScope;
                            }
                            while (++jmpTab, --jmpCnt);
                            break;

        case BBJ_RET:
                            if (block->bbFlags & BBF_ENDFILTER)
                                initVars |= block->bbJumpDest->bbScope;
                            break;
        case BBJ_RETURN:    break;

        case BBJ_THROW:     /* HACK : We dont have to do anything as we mark
                             * all vars live on entry to a catch handler as
                             * volatile anyway
                             */
                            break;

        default:            assert(!"Invalid bbJumpKind");          break;
        }

        /* If the var is already live on entry to the current BB,
           we would have already initialized it. So ignore bbLiveIn */

        initVars &= ~block->bbLiveIn;

        /* Add statements initializing the vars */

        VARSET_TP        varBit = 0x1;

        for(unsigned     varIndex = 0;
            initVars && (varIndex < lvaTrackedCount);
            varBit<<=1,  varIndex++)
        {
            if (!(initVars & varBit))
                continue;

            initVars &= ~varBit;

            /* Create initialization tree */

            unsigned        varNum  = lvaTrackedToVarNum[varIndex];
            LclVarDsc *     varDsc  = &lvaTable[varNum];
            var_types       type    = varDsc->TypeGet();

            // Create a "zero" node

            GenTreePtr      zero    = gtNewZeroConNode(genActualType(type));

            // Create initialization node

            GenTreePtr      varNode = gtNewLclvNode(varNum, type);
            GenTreePtr      initNode= gtNewAssignNode(varNode, zero);
            GenTreePtr      initStmt= gtNewStmt(initNode);

            gtSetStmtInfo (initStmt);

            /* Assign numbers and next/prev links for this tree */

            fgSetStmtSeq(initStmt);

            /* Finally append the statement to the current BB */

            fgInsertStmtNearEnd(block, initStmt);

            varDsc->incRefCnts(block->bbWeight, this);

            /* Update liveness information so that redoing fgLiveVarAnalisys()
               will work correctly if needed */

            if ((block->bbVarUse & varBit) == 0)
                block->bbVarDef |=  varBit;

            block->bbLiveOut    |=  varBit;
            block->bbFlags      |= BBF_CHANGED;  // indicates that the liveness info has changed
        }
    }

    /* raMarkStkVars() reserves stack space for unused variables (which
       needs to be initialized. However, arguments dont need to be initialized.
       So just ensure that they dont have a 0 ref cnt */

       unsigned lclNum = 0;
       for (LclVarDsc * varDsc = lvaTable; lclNum < lvaCount; lclNum++, varDsc++)
       {
           if (varDsc->lvRefCnt == 0 && varDsc->lvArgReg)
               varDsc->lvRefCnt = 1;
       }

#ifdef DEBUG
    if (verbose)
    {
        printf("\n");
        fgDispBBLiveness();
        printf("\n");
    }
#endif
}


/*****************************************************************************/
#endif // DEBUGGING_SUPPORT


VARSET_TP           Compiler::fgGetHandlerLiveVars(BasicBlock *block)
{
    assert(block);
    assert(block->bbFlags & BBF_HAS_HANDLER);
    assert(block->hasTryIndex());

    VARSET_TP   liveVars    = 0;
    unsigned    XTnum       = block->getTryIndex();

    do 
    {
        EHblkDsc *   HBtab = compHndBBtab + XTnum;

        assert(XTnum < info.compXcptnsCount);

        /* Either we enter the filter first or the catch/finally */
        
        if (HBtab->ebdFlags & CORINFO_EH_CLAUSE_FILTER)
            liveVars  |= HBtab->ebdFilter->bbLiveIn;
        else
            liveVars |= HBtab->ebdHndBeg->bbLiveIn;
         
        /* If we have nested try's edbEnclosing will provide them */
        assert((HBtab->ebdEnclosing == NO_ENCLOSING_INDEX) ||
               (HBtab->ebdEnclosing  > XTnum));

        XTnum = HBtab->ebdEnclosing;

    } while (XTnum != NO_ENCLOSING_INDEX);

    return liveVars;
}



/*****************************************************************************
 *
 *  This is the classic algorithm for Live Variable Analisys
 *  If updateInternalOnly==true, only update BBF_INTERNAL blocks.
 */

void                Compiler::fgLiveVarAnalisys(bool updateInternalOnly)
{
    BasicBlock *    block;
    bool            change;
#ifdef DEBUG
    VARSET_TP       extraLiveOutFromFinally = 0;
#endif

    /* Live Variable Analisys - Backward dataflow */

    do
    {
        change = false;

        /* Visit all blocks and compute new data flow values */

        for (block = fgFirstBB; block; block = block->bbNext)
        {
            if (updateInternalOnly)
            {
                /* Only update BBF_INTERNAL blocks as they may be
                   syntactically out of sequence. */

                assert(opts.compDbgCode);

                if (!(block->bbFlags & BBF_INTERNAL))
                    continue;
            }

            VARSET_TP       liveIn;
            VARSET_TP       liveOut;

            /* Compute the 'liveOut' set */

            liveOut    = 0;

            switch (block->bbJumpKind)
            {
                BasicBlock * *  jmpTab;
                unsigned        jmpCnt;

            case BBJ_RET:

                if (block->bbFlags & BBF_ENDFILTER)
                {
                    /* Filter has control flow paths to all of it's catch handlers 
                       and all inner finally/fault handlers */

                    assert(block->hasHndIndex());

                    unsigned    filterNum = block->getHndIndex();
                    EHblkDsc *  filterTab = compHndBBtab + filterNum;

                    for (unsigned XTnum = 0; XTnum < info.compXcptnsCount; XTnum++)
                    {
                        EHblkDsc *   HBtab = compHndBBtab + XTnum;

                        if ((HBtab->ebdFlags & (CORINFO_EH_CLAUSE_FINALLY | CORINFO_EH_CLAUSE_FAULT)) != 0)
                        {
                            /* We can go to inner Finally/Fault handlers */

                            if ((HBtab->ebdTryBeg->bbNum >= filterTab->ebdTryBeg->bbNum) &&
                                (ebdTryEndBlkNum(HBtab) <  ebdTryEndBlkNum(filterTab))   )
                            {
                                liveOut |= HBtab->ebdHndBeg->bbLiveIn;
                            }
                        }
                        else
                        {                    
                            /* We can go to catch handlers with the same tryBeg/tryEnd */

                            if ((HBtab->ebdTryBeg->bbNum == filterTab->ebdTryBeg->bbNum) &&
                                (ebdTryEndBlkNum(HBtab) == ebdTryEndBlkNum(filterTab))   )
                            {
                                liveOut |= HBtab->ebdHndBeg->bbLiveIn;
                            }
                        }
                    }
                }
                else
                {
                    /*  Variables that are live on entry to any block that follows
                        one that 'calls' our block will also be live on exit from
                        this block, since it will return to the block following the
                        call.
                     */

                    unsigned      hndIndex = block->getHndIndex();
                    EHblkDsc   *  ehDsc    = compHndBBtab + hndIndex;
                    BasicBlock *  tryBeg   = ehDsc->ebdTryBeg;
                    BasicBlock *  tryEnd   = ehDsc->ebdTryEnd;
                    BasicBlock *  finBeg   = ehDsc->ebdHndBeg;

                    for (BasicBlock * bcall = tryBeg; bcall != tryEnd; bcall = bcall->bbNext)
                    {
                        if  (bcall->bbJumpKind != BBJ_CALL || bcall->bbJumpDest !=  finBeg)
                            continue;

                        liveOut |= bcall->bbNext->bbLiveIn;
#ifdef DEBUG
                        extraLiveOutFromFinally |= bcall->bbNext->bbLiveIn;
#endif
                    }
                }
                break;

            case BBJ_THROW:

                /* For synchronized methods, "this" has to be kept alive past
                   the throw, as the EE will call CORINFO_HELP_MON_EXIT on it */

                if ((info.compFlags & CORINFO_FLG_SYNCH) &&
                    !info.compIsStatic && lvaTable[0].lvTracked)
                    liveOut |= genVarIndexToBit(lvaTable[0].lvVarIndex);
                break;

            case BBJ_RETURN:
                if (block->bbFlags & BBF_HAS_JMP)
                {
                    // A JMP or JMPI uses all the arguments, so mark them all
                    // as live at the JMP instruction
                    //
                    for(unsigned i =0; i < lvaTrackedCount; i++)
                        if (lvaTable[i].lvTracked)
                            liveOut |= genVarIndexToBit(lvaTable[i].lvVarIndex);
                    break;
                }
                break;

            case BBJ_CALL:
                if (!(block->bbFlags & BBF_RETLESS_CALL))
                {
                    liveOut |= block->bbNext    ->bbLiveIn;
                }                
                liveOut |= block->bbJumpDest->bbLiveIn;
                
                break;

            case BBJ_COND:            
                liveOut |= block->bbNext    ->bbLiveIn;
                liveOut |= block->bbJumpDest->bbLiveIn;

                break;

            case BBJ_ALWAYS:
                liveOut |= block->bbJumpDest->bbLiveIn;

                break;

            case BBJ_NONE:
                liveOut |= block->bbNext    ->bbLiveIn;

                break;

            case BBJ_SWITCH:

                jmpCnt = block->bbJumpSwt->bbsCount;
                jmpTab = block->bbJumpSwt->bbsDstTab;

                do
                {
                    liveOut |= (*jmpTab)->bbLiveIn;
                }
                while (++jmpTab, --jmpCnt);

                break;
            }

            /* Compute the 'liveIn'  set */

            liveIn = block->bbVarUse | (liveOut & ~block->bbVarDef);

            /* Is this block part of a 'try' statement? */

            if  (block->bbFlags & BBF_HAS_HANDLER)
            {
                VARSET_TP liveVars = fgGetHandlerLiveVars(block);

                liveIn  |= liveVars;
                liveOut |= liveVars;
            }

            /* Has there been any change in either live set? */

            if  ((block->bbLiveIn != liveIn) || (block->bbLiveOut != liveOut))
            {
                if (updateInternalOnly)
                {
                    // Only "extend" liveness over BBF_INTERNAL blocks

                    assert(block->bbFlags & BBF_INTERNAL);

                    if ((block->bbLiveIn  & liveIn)  != liveIn ||
                        (block->bbLiveOut & liveOut) != liveOut)
                    {
                        block->bbLiveIn    |= liveIn;
                        block->bbLiveOut   |= liveOut;
                        change = true;
                    }
                }
                else
                {
                    block->bbLiveIn     = liveIn;
                    block->bbLiveOut    = liveOut;
                    change = true;
                }
            }
        }
    }
    while (change);

    //-------------------------------------------------------------------------

#ifdef  DEBUG

    if  (verbose && !updateInternalOnly)
    {
        printf("\n");
        fgDispBBLiveness();
    }

#endif // DEBUG
}

/*****************************************************************************
 *
 *  Mark any variables in varSet1 as interfering with any variables
 *  specified in varSet2.
 *  We insure that the interference graph is reflective:
 *  (if T11 interferes with T16, then T16 interferes with T11)
 *  returns true if an interference was added
 *  If the caller want to know if we added a new interference the he supplies
 *  a non-NULL value for newIntf
 */

void                Compiler::fgMarkIntf(VARSET_TP varSet1,
                                         VARSET_TP varSet2,
                                         bool *    newIntf /* = NULL */)
{
    // We should only call this if we are planning on performing register allocation
    assert(opts.compMinOptim==false);

    // If the newIntf is non-NULL then set it to false
    if (newIntf != 0)
      *newIntf = false;

    /* If either set has no bits set, take an early out */
    if ((varSet2 == 0) || (varSet1 == 0))
        return;

    // We swap varSet1 and varSet2 if varset2 is bigger
    // This allows us to test against varset1 to see if we can early out below
    //
    if (varSet1 < varSet2)
    {
        VARSET_TP varSetTemp = varSet1;
        varSet1 = varSet2;
        varSet2 = varSetTemp;
    }

    // varSet1 should now always be the larger of the two
    assert(varSet1 >= varSet2);

    // Unless lvaTrackedCount is the maximum value, the upper bits of varSet1 should be zeros
    assert((lvaTrackedCount == VARSET_SZ) ||
           (varSet1 < genVarIndexToBit(lvaTrackedCount)));

    unsigned refIndex = 0;
    VARSET_TP varBit  = 1;
    do {

        // if varSet1 has this bit set then it interferes with varSet2
        if (varSet1 & varBit)
        {
            // If the caller want us to tell him if we add a new inteference
            // and we current have not added a new interference
            if (( newIntf != 0)     &&    // need to report new interferences
                (*newIntf == false) &&    // no new interferences yet
                (~lvaVarIntf[refIndex] & varSet2))  // setting any new bits?
            {
                *newIntf = true;
            }
            lvaVarIntf[refIndex] |= varSet2;
        }

        // if varSet2 has this bit set then it interferes with varSet1
        if (varSet2 & varBit)
        {
            // If the caller want us to tell him if we add a new inteference
            // and we current have not added a new interference
            if (( newIntf != 0)     &&    // need to report new interferences
                (*newIntf == false) &&    // no new interferences yet
                (~lvaVarIntf[refIndex] & varSet1))  // setting any new bits?
            {
                *newIntf = true;
            }
            lvaVarIntf[refIndex] |= varSet1;
        }

        varBit <<= 1;

        // Early out when we have no more bits set in varSet1
        if (varBit > varSet1)
            break;

    } while (++refIndex < VARSET_SZ);
}

/*****************************************************************************
 *
 *  Mark any variables in varSet as interfering with each other,
 *  This is a specialized version of the above, when both args are the same
 *  We insure that the interference graph is reflective:
 *  (if T11 interferes with T16, then T16 interferes with T11)
 */


void                Compiler::fgMarkIntf(VARSET_TP varSet)
{
    // We should only call this if we are planning on performing register allocation
    assert(opts.compMinOptim==false);

    /* No bits set, take an early out */
    if (varSet == 0)
        return;

    // Unless lvaTrackedCount is the maximum value, the upper bits of varSet1 should be zeros
    assert((lvaTrackedCount == VARSET_SZ) ||
           (varSet < genVarIndexToBit(lvaTrackedCount)));

    unsigned refIndex = 0;
    VARSET_TP varBit  = 1;
    do {

        // if varSet has this bit set then it interferes with varSet
        if (varSet & varBit)
            lvaVarIntf[refIndex] |= varSet;

        varBit <<= 1;

        // Early out when we have no more bits set in varSet
        if (varBit > varSet)
            break;

    } while (++refIndex < VARSET_SZ);
}

/*****************************************************************************
 */

void                Compiler::fgUpdateRefCntForExtract(GenTreePtr  wholeTree,
                                                       GenTreePtr  keptTree)
{
    /*  Update the refCnts of removed lcl vars - The problem is that
     *  we have to consider back the side effects trees so we first
     *  increment all refCnts for side effects then decrement everything
     *  in the statement
     */
    if (keptTree)
        fgWalkTreePre(keptTree, Compiler::lvaIncRefCntsCB, (void *)this, true);

    fgWalkTreePre(   wholeTree, Compiler::lvaDecRefCntsCB, (void *)this, true);
}


/*****************************************************************************
 *
 * Compute the set of live variables at each node in a given statement
 * or subtree of a statement
 */

VARSET_TP           Compiler::fgComputeLife(VARSET_TP   life,
                                            GenTreePtr  startNode,
                                            GenTreePtr    endNode,
                                            VARSET_TP   volatileVars
                                  DEBUGARG( bool *      treeModf))
{
    GenTreePtr      tree;
    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    GenTreePtr      gtQMark = NULL;     // current GT_QMARK node (walking the trees backwards)
    GenTreePtr      nextColonExit = 0;  // gtQMark->gtOp.gtOp2 while walking the 'else' branch.
                                        // gtQMark->gtOp.gtOp1 while walking the 'then' branch
    VARSET_TP       entryLiveSet;       // liveness when we see gtQMark

    VARSET_TP       keepAliveVars = volatileVars;
#ifdef  DEBUGGING_SUPPORT
    keepAliveVars |= compCurBB->bbScope; /* Dont kill vars in scope */
#endif
    assert((keepAliveVars & life) == keepAliveVars);

    assert(compCurStmt->gtOper == GT_STMT);
    assert(endNode || (startNode == compCurStmt->gtStmt.gtStmtExpr));

    /* NOTE: Live variable analysis will not work if you try
     * to use the result of an assignment node directly */

    for (tree = startNode; tree != endNode; tree = tree->gtPrev)
    {
AGAIN:
        /* Store the current liveset in the node */

        tree->gtLiveSet = life;

#ifdef  DEBUG
        if (verbose && 0)
            gtDispTree(tree, 0, (char *) genVS2str(life), true);
#endif

        /* For ?: nodes if we're done with the second branch
         * then set the correct life as the union of the two branches */

        if (gtQMark && (tree == gtQMark->gtOp.gtOp1))
        {
            assert(tree->gtFlags & GTF_RELOP_QMARK);
            assert(gtQMark->gtOp.gtOp2->gtOper == GT_COLON);

            GenTreePtr  gtColon  = gtQMark->gtOp.gtOp2;
            GenTreePtr  thenNode = gtColon->gtOp.gtOp1;
            GenTreePtr  elseNode = gtColon->gtOp.gtOp2;

            assert(thenNode && elseNode);

            /* Check if we optimized away the ?: */

            if (thenNode->IsNothingNode())
            {
                if (elseNode->IsNothingNode())
                {
                    /* This can only happen for VOID ?: */
                    assert(gtColon->gtType == TYP_VOID);

#ifdef  DEBUG
                    if  (verbose)
                    {
                        printf("BB%02u - Removing dead QMark - Colon ...\n", compCurBB->bbNum);
                        gtDispTree(gtQMark); printf("\n");
                    }
#endif

                    /* Remove the '?:' - keep the side effects in the condition */

                    assert(tree->OperKind() & GTK_RELOP);

                    /* Bash the node to a NOP */

                    gtQMark->gtBashToNOP();
#ifdef DEBUG
                    *treeModf = true;
#endif

                    /* Extract and keep the side effects */

                    if (tree->gtFlags & GTF_SIDE_EFFECT)
                    {
                        GenTreePtr      sideEffList = NULL;

                        gtExtractSideEffList(tree, &sideEffList);

                        if (sideEffList)
                        {
                            assert(sideEffList->gtFlags & GTF_SIDE_EFFECT);
#ifdef  DEBUG
                            if  (verbose)
                            {
                                printf("\nExtracted side effects list from condition...\n");
                                gtDispTree(sideEffList); printf("\n");
                            }
#endif
                            fgUpdateRefCntForExtract(tree, sideEffList);

                            /* The NOP node becomes a GT_COMMA holding the side effect list */

                            gtQMark->ChangeOper(GT_COMMA);
                            gtQMark->gtFlags |= sideEffList->gtFlags & GTF_GLOB_EFFECT;

                            if (sideEffList->gtOper == GT_COMMA)
                            {
                                gtQMark->gtOp.gtOp1 = sideEffList->gtOp.gtOp1;
                                gtQMark->gtOp.gtOp2 = sideEffList->gtOp.gtOp2;
                            }
                            else
                            {
                                gtQMark->gtOp.gtOp1 = sideEffList;
                                gtQMark->gtOp.gtOp2 = gtNewNothingNode();
                            }
                        }
                        else
                        {
#ifdef DEBUG
                          if (verbose)
                          {
                              printf("\nRemoving tree [%08X] in BB%02u as useless\n",
                                     tree, compCurBB->bbNum);
                              gtDispTree(tree, 0);
                              printf("\n");
                          }
#endif
                            fgUpdateRefCntForExtract(tree, NULL);
                        }
                    }

                    /* If top node without side effects remove it */

                    if ((gtQMark == compCurStmt->gtStmt.gtStmtExpr) && gtQMark->IsNothingNode())
                    {
                        fgRemoveStmt(compCurBB, compCurStmt, true);
                        break;
                    }

                    /* Re-link the nodes for this statement */

                    fgSetStmtSeq(compCurStmt);

                    /* Continue analisys from this node */

                    tree = gtQMark;

                    /* As the 'then' and 'else' branches are emtpy, liveness
                       should not have changed */

                    assert(life == entryLiveSet && tree->gtLiveSet == life);
                    goto SKIP_QMARK;
                }
                else
                {
                    // The 'then' branch is empty and the 'else' branch is non-empty
                    // so swap the two branches and reverse the condition

                    GenTreePtr tmp = thenNode;

                    gtColon->gtOp.gtOp1 = thenNode = elseNode;
                    gtColon->gtOp.gtOp2 = elseNode = tmp;
                    assert(tree == gtQMark->gtOp.gtOp1);
                    tree->SetOper(GenTree::ReverseRelop(tree->OperGet()));
                
                    /* Re-link the nodes for this statement */

                    fgSetStmtSeq(compCurStmt);                                
                }
            }

            /* Variables in the two branches that are live at the split
             * must interfere with each other */

            fgMarkIntf(life, gtColon->gtLiveSet);

            /* The live set at the split is the union of the two branches */

            life |= gtColon->gtLiveSet;

            /* Update the current liveset in the node */

            tree->gtLiveSet = gtColon->gtLiveSet = life;

SKIP_QMARK:

            /* We are out of the parallel branches, the rest is sequential */

            gtQMark = NULL;
        }

        if (tree->gtOper == GT_CALL)
        {
#if INLINE_NDIRECT

            /* GC refs cannot be enregistered accross an unmanaged call */

            // UNDONE: we should generate the code for saving to/restoring
            //         from the inlined N/Direct frame instead.

            /* Is this call to unmanaged code? */

            if (tree->gtFlags & GTF_CALL_UNMANAGED)
            {
                /* Get the TCB local and make it live */

                assert(info.compLvFrameListRoot < lvaCount);

                LclVarDsc * varDsc = &lvaTable[info.compLvFrameListRoot];

                if (varDsc->lvTracked)
                {
                    VARSET_TP varBit = genVarIndexToBit(varDsc->lvVarIndex);

// @TODO [NOW] [07/13/01] []: remove the next statement ("tree->gtLiveSet |= varBit;"); see RAID #91277
                    tree->gtLiveSet |= varBit;
                    life |= varBit;

                    /* Record interference with other live variables */

                    fgMarkIntf(life, varBit);
                }

                /* Do we have any live variables? */

                if (life)
                {
                    // For each live variable if it is a GC-ref type, we
                    // mark it volatile to prevent if from being enregistered
                    // across the unmanaged call.

                    for (lclNum = 0, varDsc = lvaTable;
                         lclNum < lvaCount;
                         lclNum++  , varDsc++)
                    {
                        /* Ignore the variable if it's not tracked */

                        if  (!varDsc->lvTracked)
                            continue;

                        unsigned  varNum = varDsc->lvVarIndex;
                        VARSET_TP varBit = genVarIndexToBit(varNum);

                        /* Ignore the variable if it's not live here */

                        if  ((life & varBit) == 0)
                            continue;

                        /* If it is a GC-ref type then mark it volatile */

                        if (varTypeIsGC(varDsc->TypeGet()))
                            varDsc->lvVolatile = true;
                    }
                }
            }
#endif

#if GTF_CALL_FPU_SAVE
            if (!(tree->gtFlags & GTF_CALL_FPU_SAVE))
            {
                /*  The FP stack must be empty at all calls not marked
                 *  with the GTF_CALL_FPU_SAVE flag.
                 *
                 *  Thus any variables that are live accross a call
                 *  are mark as interferring with the deepest FP stk
                 *  and thus cannot be enregistered on the FP stack
                 */
                raFPlvlLife[FP_STK_SIZE-1] |= tree->gtLiveSet;
            }
#endif
        }

        /* Is this a use/def of a local variable? */

        if  (tree->gtOper == GT_LCL_VAR)
        {
            lclNum = tree->gtLclVar.gtLclNum;

            assert(lclNum < lvaCount);
            varDsc = lvaTable + lclNum;

            /* Is this a tracked variable? */

            if  (varDsc->lvTracked)
            {
                unsigned        varIndex;
                VARSET_TP       varBit;

                varIndex = varDsc->lvVarIndex;
                assert(varIndex < lvaTrackedCount);
                varBit   = genVarIndexToBit(varIndex);

                /* Is this a definition or use? */

                if  (tree->gtFlags & GTF_VAR_DEF)
                {
                    /*
                        The variable is being defined here. The variable
                        should be marked dead from here until its closest
                        previous use.

                        IMPORTANT OBSERVATION:

                            For GTF_VAR_USEASG (i.e. x <op>= a) we cannot
                            consider it a "pure" definition because it would
                            kill x (which would be wrong because x is
                            "used" in such a construct) -> see below the case when x is live
                     */

                    if  (life & varBit)
                    {
                        /* The variable is live */

                        if ((tree->gtFlags & GTF_VAR_USEASG) == 0)
                        {
                            /* Mark variable as dead from here to its closest use */

                            if (!(varBit & keepAliveVars))
                                life &= ~varBit;
#ifdef  DEBUG
                            if (verbose&&0) printf("Def V%02u,T%02u at [%08X] life %s -> %s\n",
                                lclNum, varIndex, tree, genVS2str(life|varBit), genVS2str(life));
#endif
                        }
                    }
                    else
                    {
                        /* Dead assignment to the variable */

                        if (opts.compMinOptim)
                            continue;

                        // keepAliveVars always stay alive
                        assert(!(varBit & keepAliveVars));

                        /* This is a dead store unless the variable is marked
                           GTF_VAR_USEASG and we are in an interior statement
                           that will be used (e.g. while(i++) or a GT_COMMA) */

                        GenTreePtr asgNode = tree->gtNext;

                        assert(asgNode->gtFlags & GTF_ASG);
                        assert(asgNode->gtOp.gtOp2);
                        assert(tree->gtFlags & GTF_VAR_DEF);

                        if (asgNode->gtOper != GT_ASG && asgNode->gtOverflowEx())
                        {
                            /// asgNode may be <op_ovf>= (with GTF_OVERFLOW). In that case, we need to keep the <op_ovf> */

                            // Dead <OpOvf>= assignment. We change it to the right operation (taking out the assignment),
                            // update the flags, update order of statement, as we have changed the order of the operation
                            // and we start computing life again from the op_ovf node (we go backwards). Note that we
                            // don't need to update ref counts because we don't change them, we're only changing the
                            // operation.

                            #ifdef DEBUG
                            if  (verbose)
                            {
                                printf("\nChanging dead <asgop> ovf to <op> ovf...\n");                                
                            }
                            #endif
                            
                            switch (asgNode->gtOper)
                            {
                            case GT_ASG_ADD:
                                asgNode->gtOper = GT_ADD;
                                break;
                            case GT_ASG_SUB:
                                asgNode->gtOper = GT_SUB;
                                break;
                            default:
                                // Only add and sub allowed, we don't have ASG_MUL and ASG_DIV for ints, and 
                                // floats don't allow OVF forms.
                                assert(!"Unexpected ASG_OP");
                            }
                                                        
                            asgNode->gtFlags &= ~GTF_REVERSE_OPS;
                            if (!((asgNode->gtOp.gtOp1->gtFlags | asgNode->gtOp.gtOp2->gtFlags)&GTF_ASG))
                                asgNode->gtFlags &= ~GTF_ASG;
                            asgNode->gtOp.gtOp1->gtFlags &= ~(GTF_VAR_DEF | GTF_VAR_USEASG);

                            #ifdef DEBUG
                            *treeModf = true;
                            #endif

                            /* Update ordering, costs, FP levels, etc. */
                            gtSetStmtInfo(compCurStmt);


                            /* Re-link the nodes for this statement */
                            fgSetStmtSeq(compCurStmt);


                            /* Start from the old assign node, as we have changed the order of its operands*/
                            life = asgNode->gtLiveSet;
                            tree = asgNode;


                            goto AGAIN;

                        }

                        /* Do not remove if we need the address of the variable */
                        if(varDsc->lvAddrTaken)
                            continue;

                        /* Test for interior statement */

                        if (asgNode->gtNext == 0)
                        {
                            /* This is a "NORMAL" statement with the
                             * assignment node hanging from the GT_STMT node */

                            assert(compCurStmt->gtStmt.gtStmtExpr == asgNode);

                            /* Check for side effects */

                            if (asgNode->gtOp.gtOp2->gtFlags & (GTF_SIDE_EFFECT & ~GTF_OTHER_SIDEEFF))
                            {
EXTRACT_SIDE_EFFECTS:
                                /* Extract the side effects */

                                GenTreePtr      sideEffList = NULL;
#ifdef  DEBUG
                                if  (verbose)
                                {
                                    printf("\nBB%02u - Dead assignment has side effects...\n", compCurBB->bbNum);
                                    gtDispTree(asgNode); printf("\n");
                                }
#endif
                                gtExtractSideEffList(asgNode->gtOp.gtOp2, &sideEffList);

                                if (sideEffList)
                                {
                                    assert(sideEffList->gtFlags & GTF_SIDE_EFFECT);
#ifdef  DEBUG
                                    if  (verbose)
                                    {
                                        printf("\nExtracted side effects list...\n");
                                        gtDispTree(sideEffList); printf("\n");
                                    }
#endif
                                    fgUpdateRefCntForExtract(asgNode, sideEffList);

                                    /* Replace the assignment statement with the list of side effects */
                                    assert(sideEffList->gtOper != GT_STMT);

                                    tree = compCurStmt->gtStmt.gtStmtExpr = sideEffList;
#ifdef DEBUG
                                    *treeModf = true;
#endif
                                    /* Update ordering, costs, FP levels, etc. */
                                    gtSetStmtInfo(compCurStmt);

                                    /* Re-link the nodes for this statement */
                                    fgSetStmtSeq(compCurStmt);

                                    /* Compute the live set for the new statement */
                                    goto AGAIN;
                                }
                                else
                                {
                                    /* No side effects, most likely we forgot to reset some flags */
                                    fgRemoveStmt(compCurBB, compCurStmt, true);

                                    break;
                                }
                            }
                            else
                            {
                                // @TODO [REVISIT] [04/16/01] []: Not removing the dead assignment of 
                                // GT_CATCH_ARG causes the register allocator to allocate an extra stack frame slot
                                // for the dead variable, since the variable appears to have one use

                                /* If this is GT_CATCH_ARG saved to a local var don't bother */

                                if (asgNode->gtFlags & GTF_OTHER_SIDEEFF)
                                {
                                    if (asgNode->gtOp.gtOp2->gtOper == GT_CATCH_ARG)
                                        goto EXTRACT_SIDE_EFFECTS;
                                }

                                /* No side effects - remove the whole statement from the block->bbTreeList */

                                fgRemoveStmt(compCurBB, compCurStmt, true);

                                /* Since we removed it do not process the rest (i.e. RHS) of the statement
                                 * variables in the RHS will not be marked as live, so we get the benefit of
                                 * propagating dead variables up the chain */

                                break;
                            }
                        }
                        else
                        {
                            /* This is an INTERIOR STATEMENT with a dead assignment - remove it */

                            assert(!(life & varBit));

#ifdef DEBUG
                              *treeModf = true;
#endif

                            if (asgNode->gtOp.gtOp2->gtFlags & GTF_SIDE_EFFECT)
                            {
                                /* Bummer we have side effects */

                                GenTreePtr      sideEffList = NULL;

#ifdef  DEBUG
                                if  (verbose)
                                {
                                    printf("\nBB%02u - INTERIOR dead assignment has side effects...\n", compCurBB->bbNum);
                                    gtDispTree(asgNode); printf("\n");
                                }
#endif

                                gtExtractSideEffList(asgNode->gtOp.gtOp2, &sideEffList);

                                if (!sideEffList)
                                    goto NO_SIDE_EFFECTS;

                                assert(sideEffList->gtFlags & GTF_SIDE_EFFECT);

#ifdef  DEBUG
                                if  (verbose)
                                {
                                    printf("\nExtracted side effects list from condition...\n");
                                    gtDispTree(sideEffList); printf("\n");
                                }
#endif
                                fgUpdateRefCntForExtract(asgNode, sideEffList);

                                /* Bash the node to a GT_COMMA holding the side effect list */

                                asgNode->gtBashToNOP();
#ifdef DEBUG
                                *treeModf = true;
#endif
                                asgNode->ChangeOper(GT_COMMA);
                                asgNode->gtFlags |= sideEffList->gtFlags & GTF_GLOB_EFFECT;

                                if (sideEffList->gtOper == GT_COMMA)
                                {
                                    asgNode->gtOp.gtOp1 = sideEffList->gtOp.gtOp1;
                                    asgNode->gtOp.gtOp2 = sideEffList->gtOp.gtOp2;
                                }
                                else
                                {
                                    asgNode->gtOp.gtOp1 = sideEffList;
                                    asgNode->gtOp.gtOp2 = gtNewNothingNode();
                                }
                            }
                            else
                            {
NO_SIDE_EFFECTS:
                                /* No side effects - Remove the interior statement */

#ifdef DEBUG
                                if (verbose)
                                {
                                    printf("\nRemoving tree [%08X] in BB%02u as useless\n",
                                           asgNode, compCurBB->bbNum);
                                    gtDispTree(asgNode, 0);
                                    printf("\n");
                                }
#endif
                                fgUpdateRefCntForExtract(asgNode, NULL);

                                /* Bash the assignment to a GT_NOP node */

                                asgNode->gtBashToNOP();
#ifdef DEBUG
                                *treeModf = true;
#endif
                            }

                            /* Re-link the nodes for this statement - Do not update ordering! */

                            fgSetStmtSeq(compCurStmt);

                            /* Continue analysis from this node */

                            tree = asgNode;

                            /* Store the current liveset in the node and
                             * continue the for loop with the next node */

                            tree->gtLiveSet = life;

                            continue;
                        }
                    }

                    continue;
                }

                /* Is the variable already known to be alive? */

                if  (life & varBit)
                    continue;
#ifdef  DEBUG
                if (verbose&&0) printf("Ref V%02u,T%02u] at [%08X] life %s -> %s\n",
                    lclNum, varIndex, tree, genVS2str(life), genVS2str(life | varBit));
#endif
                /* The variable is being used, and it is not currently live.
                 * So the variable is just coming to life */

                life |= varBit;

                /* Record interference with other live variables */

                fgMarkIntf(life, varBit);
            }
        }
        else
        {
            if (tree->gtOper == GT_QMARK && tree->gtOp.gtOp1)
            {
                /* Special cases - "? :" operators.

                   The trees are threaded as shown below with nodes 1 to 11 linked
                   by gtNext. Both GT_<cond>->gtLiveSet and GT_COLON->gtLiveSet are
                   the union of the liveness on entry to thenTree and elseTree.

                                  +--------------------+
                                  |      GT_QMARK    11|
                                  +----------+---------+
                                             |
                                             *
                                            / \
                                          /     \
                                        /         \
                   +---------------------+       +--------------------+
                   |      GT_<cond>    3 |       |     GT_COLON     7 |
                   |  w/ GTF_RELOP_QMARK |       |  w/ GTF_COLON_COND |
                   +----------+----------+       +---------+----------+
                              |                            |
                              *                            *
                             / \                          / \
                           /     \                      /     \
                         /         \                  /         \
                        2           1          thenTree 6       elseTree 10
                                   x               |                |
                                  /                *                *
      +----------------+        /                 / \              / \
      |prevExpr->gtNext+------/                 /     \          /     \
      +----------------+                      /         \      /         \
                                             5           4    9           8

                 */

                assert(tree->gtOp.gtOp1->OperKind() & GTK_RELOP);
                assert(tree->gtOp.gtOp1->gtFlags & GTF_RELOP_QMARK);
                assert(tree->gtOp.gtOp2->gtOper == GT_COLON);

                if (gtQMark)
                {
                    /* This is a nested QMARK sequence - we need to use recursivity
                     * Compute the liveness for each node of the COLON branches
                     * The new computation starts from the GT_QMARK node and ends
                     * when the COLON branch of the enclosing QMARK ends */

                    assert(nextColonExit && (nextColonExit == gtQMark->gtOp.gtOp1 ||
                                             nextColonExit == gtQMark->gtOp.gtOp2));

                    life = fgComputeLife(life, tree, nextColonExit, 
                                         volatileVars DEBUGARG(treeModf));

                    /* Continue with exit node (the last node in the enclossing colon branch) */

                    tree = nextColonExit;
                    goto AGAIN;
                    //continue;
                }
                else
                {
                    gtQMark       = tree;
                    entryLiveSet  = life;
                    nextColonExit = gtQMark->gtOp.gtOp2;
                }
            }

            /* If found the GT_COLON, start the new branch with the original life */

            if (gtQMark && tree == gtQMark->gtOp.gtOp2)
            {
                /* The node better be a COLON with a valid 'if' branch
                 * Special case: both branches may be NOP */
                assert(tree->gtOper == GT_COLON);
                assert(!tree->gtOp.gtOp1->IsNothingNode()                                      ||
                       (tree->gtOp.gtOp1->IsNothingNode() && tree->gtOp.gtOp1->IsNothingNode()) );

                life          = entryLiveSet;
                nextColonExit = gtQMark->gtOp.gtOp1;
            }
        }
    }

    /* Return the set of live variables out of this statement */

    return life;
}


/*****************************************************************************
 *
 *  Iterative data flow for live variable info and availability of range
 *  check index expressions.
 */

void                Compiler::fgGlobalDataFlow()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In fgGlobalDataFlow()\n");
#endif

    /* This global flag is set whenever we remove a statement */

    fgStmtRemoved = false;

    /* Compute the IN and OUT sets for tracked variables */

    fgLiveVarAnalisys();

    //-------------------------------------------------------------------------

#ifdef DEBUGGING_SUPPORT

    /* For debuggable code, we mark vars as live over their entire
     * reported scope, so that it will be visible over the entire scope
     */

    if (opts.compDbgCode && info.compLocalVarsCount>0)
    {
        fgExtendDbgLifetimes();
    }

#endif

    /*-------------------------------------------------------------------------
     * Variables involved in exception-handlers and finally blocks need
     * to be specially marked
     */
    BasicBlock *    block;

    VARSET_TP    exceptVars = 0;    // vars live on entry to a handler
    VARSET_TP   finallyVars = 0;    // vars live on exit of a 'finally' block
    VARSET_TP    filterVars = 0;    // vars live on exit from a 'filter'

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        if  (block->bbCatchTyp)
        {
            /* Note the set of variables live on entry to exception handler */

            exceptVars  |= block->bbLiveIn;
        }

        if  (block->bbJumpKind == BBJ_RET)
        {

            if (block->bbFlags & BBF_ENDFILTER)
            {
                /* Get the set of live variables on exit from a 'filter' */
                filterVars |= block->bbLiveOut;
            }
            else
            {
                /* Get the set of live variables on exit from a 'finally' block */

                finallyVars |= block->bbLiveOut;
            }
        }
    }

    LclVarDsc   *   varDsc;
    unsigned        varNum;

    for (varNum = 0, varDsc = lvaTable;
         varNum < lvaCount;
         varNum++  , varDsc++)
    {
        /* Ignore the variable if it's not tracked */

        if  (!varDsc->lvTracked)
            continue;

        VARSET_TP   varBit = genVarIndexToBit(varDsc->lvVarIndex);

        /* Un-init locals may need auto-initialization. Note that the
           liveness of such locals will bubble to the top (fgFirstBB)
           in fgGlobalDataFlow() */

        if (!varDsc->lvIsParam && (varBit & fgFirstBB->bbLiveIn) &&
            (info.compInitMem || varTypeIsGC(varDsc->TypeGet())))
        {
            varDsc->lvMustInit = true;
        }

        /* Mark all variables that live on entry to an exception handler
           or on exit to a filter handler or finally as volatile */

        if  ((varBit & exceptVars) || (varBit & filterVars))
        {
            /* Mark the variable appropriately */

            varDsc->lvVolatile = true;
        }

        /* Mark all pointer variables live on exit from a 'finally'
           block as either volatile for non-GC ref types or as
           'explicitly initialized' (volatile and must-init) for GC-ref types */

        if  (varBit & finallyVars)
        {
            varDsc->lvVolatile = true;

            /* Don't set lvMustInit unless we have a non-arg, GC pointer */

            if  (varDsc->lvIsParam)
                continue;

            if  (!varTypeIsGC(varDsc->TypeGet()))
                continue;

            /* Mark it */

            varDsc->lvMustInit = true;
        }
    }


    /*-------------------------------------------------------------------------
     * Now fill in liveness info within each basic block - Backward DataFlow
     */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        /* Tell everyone what block we're working on */

        compCurBB  = block;

        /* Remember those vars life on entry to exception handlers */
        /* if we are part of a try block */

        VARSET_TP       volatileVars = (VARSET_TP)0;

        if  (block->bbFlags & BBF_HAS_HANDLER)
        {
            volatileVars = fgGetHandlerLiveVars(block);

            // volatileVars is a subset of exceptVars
            assert((volatileVars & exceptVars) == volatileVars);
        }

        /* Start with the variables live on exit from the block */

        VARSET_TP    life = block->bbLiveOut;

        /* Mark any interference we might have at the end of the block */

        fgMarkIntf(life);

        /* Get the first statement in the block */

        GenTreePtr      firstStmt = block->bbTreeList;

        if (!firstStmt) continue;

        /* Walk all the statements of the block backwards - Get the LAST stmt */

        GenTreePtr      nextStmt = firstStmt->gtPrev;

        do
        {
#ifdef DEBUG
            bool treeModf = false;
#endif
            assert(nextStmt);
            assert(nextStmt->gtOper == GT_STMT);

            compCurStmt = nextStmt;
                          nextStmt = nextStmt->gtPrev;
            /* Compute the liveness for each tree node in the statement */

            life = fgComputeLife(life, compCurStmt->gtStmt.gtStmtExpr, NULL, 
                                 volatileVars DEBUGARG(&treeModf));

#ifdef DEBUG
            if (verbose & treeModf)
            {
                printf("\nfgComputeLife modified tree:\n");
                gtDispTree(compCurStmt->gtStmt.gtStmtExpr, 0);
                printf("\n");
            }
#endif
        }
        while (compCurStmt != firstStmt);

        /* Done with the current block - if we removed any statements, some
         * variables may have become dead at the beginning of the block
         * -> have to update bbLiveIn */

        if (life != block->bbLiveIn)
        {
            /* some variables have become dead all across the block
               So life should be a subset of block->bbLiveIn */

            assert((life & block->bbLiveIn) == life);

            /* set the new bbLiveIn */

            block->bbLiveIn = life;

            /* compute the new bbLiveOut for all the predecessors of this block */

            /* @TODO [CONSIDER] [04/16/01] []: Currently we do the DFA only on a 
             * per-block basis because we go through the BBlist forward!
             * We should be able to combine PerBlockDataflow and GlobalDataFlow
             * into one function that goes backward through the whole list and avoid
             * computing the USE and DEF sets.
             */
        }

        assert(compCurBB == block);
#ifdef  DEBUG
        compCurBB = 0;
#endif
    }
}

/*****************************************************************************
 *
 *  Given a basic block that has been removed, return an equivalent basic block
 *  that can be used instead of the removed block.
 */

/* static */ inline
BasicBlock *        Compiler::fgSkipRmvdBlocks(BasicBlock *block)
{
    /* We should always be called with a removed BB */

    assert(block->bbFlags & BBF_REMOVED);

    /* Follow the list until we find a block that will stick around */

    do
    {
        block = block->bbNext;
    }
    while (block && (block->bbFlags & BBF_REMOVED));

    return block;
}

/*****************************************************************************
 *
 *  Find and remove any basic blocks that are useless (e.g. they have not been
 *  imported because they are not reachable, or they have been optimized away).
 */

void                Compiler::fgRemoveEmptyBlocks()
{
    BasicBlock **   lst;
    BasicBlock  *   cur;
    BasicBlock  *   nxt;

    /* If we remove any blocks, we'll have to do additional work */

    unsigned        removedBlks = 0;

    /* 'lst' points to the link to the current block in the list */

    lst = &fgFirstBB;
    cur =  fgFirstBB;

    for (;;)
    {
        /* Make sure our pointers aren't messed up */

        assert(lst && cur && *lst == cur);

        /* Get hold of the next block */

        nxt = cur->bbNext;

        /* Should this block be removed? */

        if  (!(cur->bbFlags & BBF_IMPORTED))
        {
            assert(cur->bbTreeList == 0);

            /* Mark the block as removed */

            cur->bbFlags |= BBF_REMOVED;

            /* Remember that we've removed a block from the list */

            removedBlks++;

#ifdef DEBUG
            if (verbose)
                printf("BB%02u was not imported, marked as removed(%d)\n",
                    cur->bbNum, removedBlks);
#endif

            /* Drop the block from the list */

            *lst = nxt;
        }
        else
        {
            /* It's a useful block; continue with the next one */

            lst = &(cur->bbNext);
        }

        /* stop if we're at the end */

        if  (!nxt)
            break;

        cur = nxt;
    }

    /* If no blocks were removed, we're done */

    if  (!removedBlks)
        return;

    /*  Update all references in the exception handler table
     *  Mark the new blocks as non-removable
     *
     *  We may have the entire try block unreacheable.
     *  Check for this case and remove the entry from the EH table
     */

    unsigned        XTnum;
    EHblkDsc *      HBtab;
#ifdef DEBUG
    unsigned        delCnt = 0;
#endif

    for (XTnum = 0, HBtab = compHndBBtab;
         XTnum < info.compXcptnsCount;
         XTnum++  , HBtab++)
    {
AGAIN:
        /* The beginning of the try block was not imported
         * Need to remove the entry from the EH table */

        if (HBtab->ebdTryBeg->bbFlags & BBF_REMOVED)
        {
            assert(!(HBtab->ebdTryBeg->bbFlags & BBF_IMPORTED));
#ifdef DEBUG
            if (verbose)
            {
                printf("Beginning of try block (BB%02u) not imported "
                       "- remove index #%u from the EH table\n",
                       HBtab->ebdTryBeg->bbNum, 
                        XTnum+delCnt);
            }
            delCnt++;
#endif

            /* Reduce the number of entries in the EH table by one */
            info.compXcptnsCount--;

            if (info.compXcptnsCount == 0)
            {
                // No more entries remaining.
#ifdef DEBUG
                compHndBBtab = (EHblkDsc *)0xBAADF00D;
#endif
                break; /* exit the for loop over XTnum */
            }
            else
            {
                /* If we recorded an enclosing index for xtab then see
                 * if it needs to be updated due to the removal of this entry
                 */

                for (EHblkDsc * xtab = compHndBBtab; 
                     xtab < compHndBBtab + info.compXcptnsCount; 
                     xtab++)
                {
                    if ((xtab != HBtab)                            &&
                        (xtab->ebdEnclosing != NO_ENCLOSING_INDEX) &&
                        (xtab->ebdEnclosing >= XTnum))

                    {
                        // Update the enclosing scope link
                        if (xtab->ebdEnclosing == XTnum)
                            xtab->ebdEnclosing = HBtab->ebdEnclosing;
                        if ((xtab->ebdEnclosing > XTnum) && 
                            (xtab->ebdEnclosing != NO_ENCLOSING_INDEX))
                            xtab->ebdEnclosing--;
                    }
                }
            
                /* We need to update all of the blocks' bbTryIndex */

                for (BasicBlock * blk = fgFirstBB; blk; blk = blk->bbNext)
                {
                    if (blk->hasTryIndex())
                    {
                        if (blk->getTryIndex() == XTnum)
                        {
                            assert(blk->bbFlags & BBF_REMOVED);
#ifdef DEBUG
                            blk->bbTryIndex = -1;
#endif
                        }
                        else if (blk->getTryIndex() > XTnum)
                        {
                            blk->bbTryIndex--;
                        }
                    }
                    
                    if (blk->hasHndIndex())
                    {
                        if (blk->getHndIndex() == XTnum)
                        {
                            assert(blk->bbFlags & BBF_REMOVED);
#ifdef DEBUG
                            blk->bbHndIndex = -1;
#endif
                        }
                        else if (blk->getHndIndex() > XTnum)
                        {
                            blk->bbHndIndex--;
                        }
                    }
                }

                /* Now remove the unused entry from the table */

                if (XTnum < info.compXcptnsCount)
                {
                    /* We copy over the old entry */
                    memcpy(HBtab, HBtab + 1, (info.compXcptnsCount - XTnum) * sizeof(*HBtab));
#ifdef DEBUG
                    if (verbose && 0)
                        fgDispHandlerTab();
#endif
                    goto AGAIN; 
                }
                else
                {
                    /* Last entry. Don't need to do anything */
                    assert(XTnum == info.compXcptnsCount);

                    break;    /* exit the for loop over XTnum */
                }
            }
//              assert(!"Unreachable!");
        }

        /* At this point we know we have a valid try block */

        /* See if the end of the try block and its handler was imported */

#ifdef DEBUG
        assert(HBtab->ebdTryBeg->bbFlags & BBF_IMPORTED);
        assert(HBtab->ebdTryBeg->bbFlags & BBF_DONT_REMOVE);
        assert(HBtab->ebdTryBeg->bbNum <= ebdTryEndBlkNum(HBtab));

        assert(HBtab->ebdHndBeg->bbFlags & BBF_IMPORTED);
        assert(HBtab->ebdHndBeg->bbFlags & BBF_DONT_REMOVE);

        if (HBtab->ebdFlags & CORINFO_EH_CLAUSE_FILTER)
        {
            assert(HBtab->ebdFilter->bbFlags & BBF_IMPORTED);
            assert(HBtab->ebdFilter->bbFlags & BBF_DONT_REMOVE);
        }
#endif

        /* Check if the Try END was reacheable */

        if (HBtab->ebdTryEnd && (HBtab->ebdTryEnd->bbFlags & BBF_REMOVED))
        {
            /* The block has not been imported */
            assert(!(HBtab->ebdTryEnd->bbFlags & BBF_IMPORTED));
#ifdef DEBUG
            if (verbose)
                printf("End of try block (BB%02u) not imported for EH index #%u\n",
                       HBtab->ebdTryEnd->bbNum, XTnum+delCnt);
#endif
            HBtab->ebdTryEnd = fgSkipRmvdBlocks(HBtab->ebdTryEnd);

            if (HBtab->ebdTryEnd)
            {
                HBtab->ebdTryEnd->bbFlags |= BBF_HAS_LABEL | BBF_DONT_REMOVE | BBF_TRY_HND_END;
#ifdef DEBUG
                if (verbose)
                    printf("New end of try block (BB%02u) for EH index #%u\n",
                                                 HBtab->ebdTryEnd->bbNum, XTnum+delCnt);
#endif
            }
#ifdef DEBUG
            else
            {
                if (verbose)
                    printf("End of Try block for EH index #%u is the end of program\n",
                           XTnum+delCnt);
            }
#endif
        }

        /* Check if the end of the handler was reachable */

        if (HBtab->ebdHndEnd && (HBtab->ebdHndEnd->bbFlags & BBF_REMOVED))
        {
            /* The block has not been imported */
            assert(!(HBtab->ebdHndEnd->bbFlags & BBF_IMPORTED));
#ifdef DEBUG
            if (verbose)
                printf("End of catch handler block (BB%02u) not imported for EH index #%u\n",
                       HBtab->ebdHndEnd->bbNum, XTnum+delCnt);
#endif
            HBtab->ebdHndEnd = fgSkipRmvdBlocks(HBtab->ebdHndEnd);

            if (HBtab->ebdHndEnd)
            {
                HBtab->ebdHndEnd->bbFlags |= BBF_HAS_LABEL | BBF_DONT_REMOVE;
#ifdef DEBUG
                if (verbose)
                    printf("New end of catch handler block (BB%02u) for EH index #%u\n",
                           HBtab->ebdHndEnd->bbNum, XTnum+delCnt);
#endif
            }
#ifdef DEBUG
            else
            {
                if (verbose)
                    printf("End of Catch handler block for EH index #%u is the end of program\n", XTnum);
            }
#endif
        }
    } /* end of the for loop over XTnum */

#ifdef DEBUG
    if (verbose)
    {
        fgDispHandlerTab();
        fgDispBasicBlocks();
        printf("\nRenumbering the basic blocks for fgRemoveEmptyBlocks\n");
    }
#endif
    
    /* Update the basic block numbers and jump targets
     * Update fgLastBB if we removed the last block */

    unsigned cnt = 1;
    for (cur = fgFirstBB; cur; cur = cur->bbNext, cnt++)
    {
        cur->bbNum = cnt;

        assert((cur->bbFlags & BBF_REMOVED) == 0);

        if (cur->bbNext == NULL)
        {
            /* this is the last block */
            fgLastBB = cur;
        }
    }

#ifdef DEBUG
    if (verbose)
    {
        fgDispHandlerTab();
        fgDispBasicBlocks();
    }
#endif
}


/*****************************************************************************
 *
 * Remove a useless statement from a basic block
 * If updateRefCnt is true we update the reference counts for
 * all tracked variables in the removed statement
 */

void                Compiler::fgRemoveStmt(BasicBlock *    block,
                                           GenTreePtr      stmt,
                                           bool            updateRefCnt)
{
    GenTreePtr      tree = block->bbTreeList;

    assert(tree);
    assert(stmt->gtOper == GT_STMT);

    if (opts.compDbgCode && stmt->gtPrev != stmt &&
        stmt->gtStmtILoffsx != BAD_IL_OFFSET)
    {
        /* CONDISER: For debuggable code, should we remove significant
           statement boundaries. Or should we leave a GT_NO_OP in its place? */
    }

    /* Is it the first statement in the list? */

    if  (tree == stmt)
    {
        if( !tree->gtNext )
        {
            assert (tree->gtPrev == tree);

            /* this is the only statement - basic block becomes empty */
            block->bbTreeList = 0;
        }
        else
        {
            block->bbTreeList         = tree->gtNext;
            block->bbTreeList->gtPrev = tree->gtPrev;
        }
        goto DONE;
    }

    /* Is it the last statement in the list? */

    if  (tree->gtPrev == stmt)
    {
        assert (stmt->gtNext == 0);

        stmt->gtPrev->gtNext      = 0;
        block->bbTreeList->gtPrev = stmt->gtPrev;
        goto DONE;
    }

    tree = stmt->gtPrev;
    assert(tree);

    tree->gtNext         = stmt->gtNext;
    stmt->gtNext->gtPrev = tree;

    fgStmtRemoved = true;

DONE:

#ifdef DEBUG
    if (verbose)
    {
        printf("\nRemoving statement [%08X] in BB%02u as useless:\n", stmt, block->bbNum);
        gtDispTree(stmt,0);
    }
#endif

    if (updateRefCnt)
        fgWalkTreePre(stmt->gtStmt.gtStmtExpr,
                      Compiler::lvaDecRefCntsCB,
                      (void *) this,
                      true);

#ifdef DEBUG
    if (verbose)
    {
        if  (block->bbTreeList == 0)
        {
            printf("\nBB%02u becomes empty", block->bbNum);
        }
        printf("\n");
    }
#endif

    return;
}

/******************************************************************************
 *  Tries to throw away the stmt.
 *  Returns true if it did.
 */

bool                Compiler::fgCheckRemoveStmt(BasicBlock * block, GenTreePtr stmt)
{
    assert(stmt->gtOper == GT_STMT);

    GenTreePtr  tree = stmt->gtStmt.gtStmtExpr;

    if (tree->OperIsConst())
        goto REMOVE_STMT;

    switch(tree->OperGet())
    {
    case GT_LCL_VAR:
    case GT_LCL_FLD:
    case GT_CLS_VAR:
    REMOVE_STMT:
        fgRemoveStmt(block, stmt);
        return true;
    }

    return false;
}

/****************************************************************************************************/

#define SHOW_REMOVED    0

/*****************************************************************************************************
 *
 *  Function called to compact two given blocks in the flowgraph
 *  Assumes that all necessary checks have been performed
 *  Uses for this function - whenever we change links, insert blocks,...
 *  It will keep the flowgraph data in synch - bbNums, bbRefs, bbPreds
 */

void                Compiler::fgCompactBlocks(BasicBlock * block)
{
    BasicBlock  *   bNext;

    assert(block);
    assert(!(block->bbFlags & BBF_REMOVED));
    assert(block->bbJumpKind == BBJ_NONE);

    bNext = block->bbNext; assert(bNext);
    assert(!(bNext->bbFlags & BBF_REMOVED));
    assert(bNext->bbRefs == 1);
    assert(bNext->bbPreds);
    assert(bNext->bbPreds->flNext == 0);
    assert(bNext->bbPreds->flBlock == block);

    /* Make sure the second block is not a TRY block or an exception handler
     *(those should be marked BBF_DONT_REMOVE)
     * Also, if one has excep handler, then the other one must too */

    assert(!(bNext->bbFlags & BBF_DONT_REMOVE));
    assert(!(bNext->bbFlags & BBF_TRY_HND_END));
    assert(!bNext->bbCatchTyp);
    assert(!(bNext->bbFlags & BBF_TRY_BEG));

    /* both or none must have an exception handler */

    assert((block->bbFlags & BBF_HAS_HANDLER) == (bNext->bbFlags & BBF_HAS_HANDLER));

#ifdef  DEBUG
    if  (verbose || SHOW_REMOVED)
    {
        printf("\nCompacting blocks BB%02u and BB%02u:\n", block->bbNum, bNext->bbNum);
    }
#endif
    
    /* Start compacting - move all the statements in the second block to the first block */

    GenTreePtr stmtList1 = block->bbTreeList;
    GenTreePtr stmtList2 = bNext->bbTreeList;

    /* the block may have an empty list */

    if (stmtList1)
    {
        GenTreePtr stmtLast1 = stmtList1->gtPrev;
        assert(stmtLast1->gtNext == 0);

        /* The second block may be a GOTO statement or something with an empty bbTreeList */
        if (stmtList2)
        {
            GenTreePtr stmtLast2 = stmtList2->gtPrev;
            assert(stmtLast2->gtNext == 0);

            /* append list2 to list 1 */

            stmtLast1->gtNext = stmtList2;
                                stmtList2->gtPrev = stmtLast1;
            stmtList1->gtPrev = stmtLast2;
        }
    }
    else
    {
        /* block was formerly empty and now has bNext's statements */
        block->bbTreeList = stmtList2;
    }

    // Note we could update the local variable weights here by
    // calling lvaMarkLocalVars, with the block and weight adjustment
#if 0
    // @TODO [REVISIT] [04/16/01] []: 
    // Enable this assert if we decide to update the local variable weights
    //
    // If this assert fires we are merging two non-empty blocks,
    // and we have different loop nesting levels for the two blocks.
    // We'll need to decide if the resulting block is in a loop or not
    // and update all of the variable ref-counts based on that decision
    assert((block->bbWeight == 0) || 
           (block->bbWeight == bNext->bbWeight));
#endif

    // If either block has a zero weight we select the zero weight
    // otherwise we just select the highest weight block and don't
    // bother with updating the local variable counts.
    if ((block->bbWeight == 0) || (bNext->bbWeight == 0))
    {
        block->bbWeight = 0;
    }
    else if (block->bbWeight < bNext->bbWeight)
    {
        block->bbWeight = bNext->bbWeight;
    }

    /* set the right links */

    block->bbNext     = bNext->bbNext;
    block->bbJumpKind = bNext->bbJumpKind;
    block->bbLiveOut  = bNext->bbLiveOut;

    /* If both blocks are non-[internal] blocks then update the bbCodeSize */

    if (!((block->bbFlags | bNext->bbFlags) & BBF_INTERNAL))
    {
        if (block->bbCodeOffs < bNext->bbCodeOffs)
        {
            /* This will make the block overlap with other block(s) in some cases */
            block->bbCodeSize = bNext->bbCodeSize + (bNext->bbCodeOffs - block->bbCodeOffs);
        }
        else
        {
            /* This will make the block overlap with other block(s) in most cases */
            block->bbCodeSize = block->bbCodeSize + (block->bbCodeOffs - bNext->bbCodeOffs);
            block->bbCodeOffs = bNext->bbCodeOffs;
        }
    }

    /* Update the flags for block with those found in bNext */

    block->bbFlags |= (bNext->bbFlags & BBF_COMPACT_UPD);

    /* mark bNext as removed */

    bNext->bbFlags |= BBF_REMOVED;

    /* If bNext was the last block update fgLastBB */

    if  (bNext == fgLastBB)
        fgLastBB = block;



    /* If we're collapsing a block created after the dominators are
       computed, rename the block and reuse dominator information from
       the other block */
    if (block->bbNum > fgLastBB->bbNum && fgDomsComputed)
    {
        block->bbReach = bNext->bbReach;
        bNext->bbReach = 0;    
        
        block->bbDom = bNext->bbDom;
        bNext->bbDom = 0;

        block->bbNum = bNext->bbNum;
    }
    

    /* Set the jump targets */

    switch (bNext->bbJumpKind)
    {
    case BBJ_CALL:
        // Propagate RETLESS property
        block->bbFlags |= (bNext->bbFlags & BBF_RETLESS_CALL);

        // fall through
    case BBJ_COND:
    case BBJ_ALWAYS:
        block->bbJumpDest = bNext->bbJumpDest;

        /* Update the predecessor list for 'bNext->bbJumpDest' and 'bNext->bbNext' */
        fgReplacePred(bNext->bbJumpDest, bNext, block);

        if (bNext->bbJumpKind == BBJ_COND)
            fgReplacePred(bNext->bbNext, bNext, block);
        break;

    case BBJ_NONE:
        /* Update the predecessor list for 'bNext->bbNext' */
        fgReplacePred(bNext->bbNext,     bNext, block);
        break;

    case BBJ_RET:
        if (block->bbFlags & BBF_ENDFILTER)
        {
            fgReplacePred(bNext->bbJumpDest, bNext, block);
        }
        else
        {
            unsigned hndIndex = block->getHndIndex();
            EHblkDsc * ehDsc = compHndBBtab + hndIndex;
            BasicBlock * tryBeg = ehDsc->ebdTryBeg;
            BasicBlock * tryEnd = ehDsc->ebdTryEnd;
            BasicBlock * finBeg = ehDsc->ebdHndBeg;

            for(BasicBlock * bcall = tryBeg; bcall != tryEnd; bcall = bcall->bbNext)
            {
                if  (bcall->bbJumpKind != BBJ_CALL || bcall->bbJumpDest !=  finBeg)
                    continue;

                assert(!(block->bbFlags & BBF_RETLESS_CALL));

                fgReplacePred(bcall->bbNext, bNext, block);
            }
        }
        break;

    case BBJ_THROW:
    case BBJ_RETURN:
        /* no jumps or fall through blocks to set here */
        break;

    case BBJ_SWITCH:
        block->bbJumpSwt = bNext->bbJumpSwt;

        /* For all jump targets of BBJ_SWITCH replace predecessor 'bbNext' with 'block' */
        unsigned        jumpCnt = bNext->bbJumpSwt->bbsCount;
        BasicBlock * *  jumpTab = bNext->bbJumpSwt->bbsDstTab;

        do
        {
            fgReplacePred(*jumpTab, bNext, block);
        }
        while (++jumpTab, --jumpCnt);

        break;
    }

    fgUpdateLoopsAfterCompacting(block, bNext);
}

void Compiler::fgUpdateLoopsAfterCompacting(BasicBlock * block, BasicBlock* bNext)
{
    /* Check if the removed block is not part the loop table */ 
    assert(bNext);

    for (unsigned loopNum = 0; loopNum < optLoopCount; loopNum++)
    {
        /* Some loops may have been already removed by
         * loop unrolling or conditional folding */

        if (optLoopTable[loopNum].lpFlags & LPFLG_REMOVED)
            continue;

        /* Check the loop head (i.e. the block preceding the loop) */

        if  (optLoopTable[loopNum].lpHead == bNext)
            optLoopTable[loopNum].lpHead = block;

        /* Check the loop bottom */

        if  (optLoopTable[loopNum].lpEnd == bNext)
            optLoopTable[loopNum].lpEnd = block;

        /* Check the loop exit */

        if  (optLoopTable[loopNum].lpExit == bNext)
        {
            assert(optLoopTable[loopNum].lpExitCnt == 1);
            optLoopTable[loopNum].lpExit = block;
        }

        /* Check the loop entry */

        if  (optLoopTable[loopNum].lpEntry == bNext)
        {
            optLoopTable[loopNum].lpEntry = block;
        }
    }
}

/*****************************************************************************************************
 *
 *  Function called when a block is unreachable
 */

void                Compiler::fgUnreachableBlock(BasicBlock * block, BasicBlock * bPrev)
{
    if (block->bbFlags & BBF_REMOVED)
        return;

    /* Removing an unreachable block */
#ifdef  DEBUG
    if  (verbose || SHOW_REMOVED)
    {
        printf("\nRemoving unreachable BB%02u\n", block->bbNum);
    }
#endif
    assert(bPrev);

    /* First walk the statement trees in this basic block and delete each stmt */

    /* Make the block publicly available */
    compCurBB = block;

    for (GenTreePtr stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
    {
        assert(stmt->gtOper == GT_STMT);
        fgRemoveStmt(block, stmt, fgStmtListThreaded);
    }
    assert(block->bbTreeList == 0);

    /* Next update the loop table and bbWeights */
    optUpdateLoopsBeforeRemoveBlock(block, bPrev);

    /* Mark the block as removed */
    block->bbFlags |= BBF_REMOVED;
    
    /* update bbRefs and bbPreds for the blocks reached by this block */
    fgRemoveBlockAsPred(block);    
}


/*****************************************************************************************************
 *
 *  Function called to remove or morph a GT_JTRUE statement when we jump to the same
 *  block when both the condition is true or false.
 */
void                Compiler::fgRemoveJTrue(BasicBlock *block)
{
    block->bbJumpKind = BBJ_NONE;
#ifdef DEBUG
    block->bbJumpDest = NULL;
    if (verbose)
        printf("Block BB%02u becoming a BBJ_NONE to BB%02u (jump target is the same whether the condition is true or false)\n",
                block->bbNum, block->bbNext->bbNum);
#endif

    /* Remove the block jump condition */

    GenTree *  test = block->bbTreeList;

    assert(test && test->gtOper == GT_STMT);
    test = test->gtPrev;
    assert(test && test->gtOper == GT_STMT);

    GenTree *  tree = test->gtStmt.gtStmtExpr;

    assert(tree->gtOper == GT_JTRUE);
    
    GenTree *  sideEffList = NULL;

    if (tree->gtFlags & GTF_SIDE_EFFECT)
    {
        gtExtractSideEffList(tree, &sideEffList);

        if (sideEffList)
        {
            assert(sideEffList->gtFlags & GTF_SIDE_EFFECT);
#ifdef  DEBUG
            if  (verbose)
            {
                printf("\nExtracted side effects list from condition...\n");
                gtDispTree(sideEffList); printf("\n");
            }
#endif
        }
    }

    // Delete the cond test or replace it with the side effect tree
    if (sideEffList == NULL)
    {
        fgRemoveStmt(block, test);
    }
    else
    {
        test->gtStmt.gtStmtExpr = sideEffList;

        // Relink nodes for statement
        fgSetStmtSeq(test);
    }
}


/*****************************************************************************************************
 *
 *  Function called to remove a basic block
 */

void                Compiler::fgRemoveBlock(BasicBlock *  block, 
                                            BasicBlock *  bPrev,
                                            bool          unreachable)
{
    /* The block has to be either unreacheable or empty */

    assert(block);
    assert((block == fgFirstBB) || (bPrev && (bPrev->bbNext == block)));
    assert(!(block->bbFlags & BBF_DONT_REMOVE));

    if (unreachable)
    {
        fgUnreachableBlock(block, bPrev);

        /* If this is the last basic block update fgLastBB */
        if  (block == fgLastBB)
            fgLastBB = bPrev;

        if (bPrev->bbJumpKind == BBJ_CALL)
        {
            // bPrev CALL becomes RETLESS as the BBJ_ALWAYS block is unreachable
            bPrev->bbFlags |= BBF_RETLESS_CALL;                        
        }

        /* Unlink this block from the bbNext chain */
        bPrev->bbNext = block->bbNext;
    
        /* At this point the bbPreds and bbRefs had better be zero */
        assert((block->bbRefs == 0) && (block->bbPreds == 0));

        /*  A BBJ_CALL is always paired with a BBJ_ALWAYS, unless
         *  the BBJ_CALL is marked with BBF_RETLESS_CALL
         *  If we delete a BBJ_CALL we also delete the BBJ_ALWAYS
         */
        if ((block->bbJumpKind == BBJ_CALL) && !(block->bbFlags & BBF_RETLESS_CALL))
        {
            BasicBlock * leaveBlk = block->bbNext;
            assert(leaveBlk->bbJumpKind == BBJ_ALWAYS);

            leaveBlk->bbFlags &= ~BBF_DONT_REMOVE;
            leaveBlk->bbRefs   = 0;
            leaveBlk->bbPreds  = 0;

            fgRemoveBlock(leaveBlk, bPrev, true);
        }
        else if (block->bbJumpKind == BBJ_RETURN)
        {
            fgRemoveReturnBlock(block);    
        }
    }
    else // block is empty
    {
        assert(block->bbTreeList == 0);
        assert((block == fgFirstBB) || (bPrev && (bPrev->bbNext == block)));

        /* The block cannot follow a BBJ_CALL (because we don't know who may jump to it) */
        assert((block == fgFirstBB) || (bPrev && (bPrev->bbJumpKind != BBJ_CALL)));

        /* This cannot be the last basic block */
        assert(block != fgLastBB);

#ifdef  DEBUG
        if  (verbose || SHOW_REMOVED)
        {
            printf("Removing empty BB%02u\n", block->bbNum);
        }
#endif
        /* Some extra checks for the empty case */

#ifdef DEBUG
        switch (block->bbJumpKind)
        {
        case BBJ_COND:
        case BBJ_SWITCH:
        case BBJ_THROW:
        case BBJ_CALL:
        case BBJ_RET:
        case BBJ_RETURN:
            /* can never happen */
            assert(!"Empty block of this type cannot be removed!");
            break;

        case BBJ_ALWAYS:
            /* Do not remove a block that jumps to itself - used for while(true){} */
            assert(block->bbJumpDest != block);

            /* Empty GOTO can be removed iff bPrev is BBJ_NONE */
            assert(bPrev && bPrev->bbJumpKind == BBJ_NONE);
        }
#endif

        assert(block->bbJumpKind == BBJ_NONE || block->bbJumpKind == BBJ_ALWAYS);

        /* Who is the "real" successor of this block? */

        BasicBlock  *   succBlock;

        if (block->bbJumpKind == BBJ_ALWAYS)
            succBlock = block->bbJumpDest;
        else
            succBlock = block->bbNext;

        bool skipUnmarkLoop = false;

        /* If block is the backedge for a loop and succBlock preceeds block
        /* then the succBlock becomes the new LOOP HEAD  */
        if (block->isLoopHead() && (succBlock->bbNum <= block->bbNum))
        {
            succBlock->bbFlags |= BBF_LOOP_HEAD;
            if (fgReachable(succBlock, block))
            {
                /* Mark all the reachable blocks between'succBlock' and 'block', excluding 'block' */
                optMarkLoopBlocks(succBlock, block, true);
            }
        }
        else if (succBlock->isLoopHead() && (succBlock->bbNum <= bPrev->bbNum))
        {
            skipUnmarkLoop = true;
        }

        assert(succBlock);

        /* First update the loop table and bbWeights */
        optUpdateLoopsBeforeRemoveBlock(block, bPrev, skipUnmarkLoop);

        /* Remove the block */

        if (bPrev == NULL)
        {
            /* special case if this is the first BB */

            assert(block == fgFirstBB);

            /* Must be a fall through to next block */

            assert(block->bbJumpKind == BBJ_NONE);

            /* old block no longer gets the extra ref count for being the first block */
            block->bbRefs--;
            succBlock->bbRefs++;

            /* Set the new firstBB */
            fgFirstBB = succBlock;

            /* Always treat the initial block as a jump target */
            fgFirstBB->bbFlags |= BBF_JMP_TARGET|BBF_HAS_LABEL;
        }
        else
            bPrev->bbNext = block->bbNext;

        /* mark the block as removed and set the change flag */

        block->bbFlags |= BBF_REMOVED;

        /* update bbRefs and bbPreds
         * All blocks jumping to 'block' now jump to 'succBlock' 
           First remove block from the predecessor list of succBlock. */
           
        assert(succBlock->bbRefs > 0);
        fgRemovePred(succBlock, block);
        succBlock->bbRefs--;
        
        for (flowList* pred = block->bbPreds; pred; pred = pred->flNext)
        {
            BasicBlock* predBlock = pred->flBlock;

            /* If predBlock is a new predecessor, then add it to succBlock's
               predecessor's list. */

            if (!fgIsPredForBlock(succBlock, predBlock))
                fgAddRefPred(succBlock, predBlock);

            /* Are we changing a loop backedge into a forward jump? */

            if ( block->isLoopHead()                   && 
                 (predBlock->bbNum >= block->bbNum)    &&
                 (predBlock->bbNum <= succBlock->bbNum)  )
            {
                /* First update the loop table and bbWeights */
                    optUpdateLoopsBeforeRemoveBlock(predBlock, NULL);
            }


            /* change all jumps to the removed block */
            switch(predBlock->bbJumpKind)
            {
            default:
                assert(!"Unexpected bbJumpKind in fgRemoveBlock()");
                break;

            case BBJ_NONE:
                assert(predBlock == bPrev);

                /* In the case of BBJ_ALWAYS we have to change the type of its predecessor */
                if (block->bbJumpKind == BBJ_ALWAYS)
                {
                    /* bPrev now becomes a BBJ_ALWAYS */
                    bPrev->bbJumpKind = BBJ_ALWAYS;
                    bPrev->bbJumpDest = succBlock;
                }
                break;

            case BBJ_COND:
                /* The links for the direct predecessor case have already been updated above */
                if (predBlock->bbJumpDest != block)
                {
                    succBlock->bbFlags |= BBF_HAS_LABEL | BBF_JMP_TARGET;
                    break;
                }

                /* Check if both side of the BBJ_COND now jump to the same block */
                if (predBlock->bbNext == succBlock)
                {
                    fgRemoveJTrue(predBlock);
                    break;
                }
                /* Fall through for the jump case */

            case BBJ_CALL:
            case BBJ_ALWAYS:
                assert(predBlock->bbJumpDest == block);
                predBlock->bbJumpDest = succBlock;
                succBlock->bbFlags |= BBF_HAS_LABEL | BBF_JMP_TARGET;
                break;

            case BBJ_SWITCH:
                unsigned        jumpCnt = predBlock->bbJumpSwt->bbsCount;
                BasicBlock * *  jumpTab = predBlock->bbJumpSwt->bbsDstTab;

                do
                {
                    assert (*jumpTab);
                    if ((*jumpTab) == block)
                        (*jumpTab) = succBlock;
                }
                while (++jumpTab, --jumpCnt);
                succBlock->bbFlags |= BBF_HAS_LABEL | BBF_JMP_TARGET;
            }
        }
    }

    if (bPrev != NULL)
    {
        switch (bPrev->bbJumpKind)
        {
        case BBJ_CALL:
            // If prev is a BBJ_CALL it better be marked as RETLESS 
            assert(bPrev->bbFlags & BBF_RETLESS_CALL);
            break;

        case BBJ_ALWAYS:
            // @TODO [REVISIT] [07/19/01] [dnotario]: See #94013 . We can't remove
            // because bPrev could be part of a BBJ_CALL/BBJ_ALWAYS pair.
            break;

        case BBJ_COND:
            /* Check for branch to next block */
            if (bPrev->bbJumpDest == bPrev->bbNext)
                fgRemoveJTrue(bPrev);
            break;
        }
    }        
}


/*****************************************************************************
 *
 *  Function called to connect to block that previously had a fall through
 */

BasicBlock *        Compiler::fgConnectFallThrough(BasicBlock * bSrc,
                                                   BasicBlock * bDst)
{
    BasicBlock * jmpBlk = NULL;
            
    /* If bSrc falls through, we will to insert a jump to bDst */
  
    if ((bSrc != NULL) && bSrc->bbFallsThrough())
    {
        switch(bSrc->bbJumpKind)
        {
           
        case BBJ_NONE:
            bSrc->bbJumpKind = BBJ_ALWAYS;
            bSrc->bbJumpDest = bDst;
            bSrc->bbJumpDest->bbFlags |= (BBF_JMP_TARGET | BBF_HAS_LABEL);
#ifdef DEBUG
            if  (verbose)
            {
                printf("Block BB%02u ended with a BBJ_NONE, Changed to an unconditional jump to BB%02u\n",
                       bSrc->bbNum, bSrc->bbJumpDest->bbNum);
            }
#endif
            break;
            
          case BBJ_CALL:
          case BBJ_COND:
            // Add a new block which jumps to 'bDst'
            jmpBlk = fgNewBBafter(BBJ_ALWAYS, bSrc);
    
            jmpBlk->bbHndIndex = bSrc->bbHndIndex;
            jmpBlk->bbJumpDest = bDst;
            jmpBlk->bbJumpDest->bbFlags |= (BBF_JMP_TARGET | BBF_HAS_LABEL);

            if (bSrc->bbPreds != NULL)
            {
                fgReplacePred(bDst, bSrc, jmpBlk);
                fgAddRefPred (jmpBlk, bSrc);
            }
            else
            {
                jmpBlk->bbFlags |= BBF_IMPORTED;
            }
#ifdef DEBUG
            if  (verbose)
            {
                printf("Block BB%02u ended with a BBJ_COND, Added an unconditional jump to BB%02u\n",
                       bSrc->bbNum, jmpBlk->bbJumpDest->bbNum);
            }
#endif
            break;
            
        default: assert(!"Bad bbJumpKind");
        }
    }
    return jmpBlk;
}

/*****************************************************************************
 *
 *  Function called to reorder the rarely run blocks 
 */

void                Compiler::fgReorderBlocks()
{
   /* Relocate any rarely run blocks such as throw blocks */

    if (fgFirstBB->bbNext == NULL)
        return;

#ifdef DEBUG
    if  (verbose) 
        printf("*************** In fgReorderBlocks()\n");

    bool newRarelyRun = false;
    bool movedBlocks  = false;
#endif

    assert(opts.compDbgCode == false);
    
    /* First let us expand the set of run rarely blocks      */
    /* We do this by observing that a block that falls into  */
    /* or jumps to a rarely run block, must itself be rarely */
    /* run and a conditional jump in which both branches go  */
    /* to rarely run blocks must itself be rarely run        */

    BasicBlock *  block;
    BasicBlock *  bPrev;

    for( bPrev = fgFirstBB, block = bPrev->bbNext;
                            block != NULL;
         bPrev = block,     block = block->bbNext)
    {

AGAIN:

        if (bPrev->isRunRarely())
            continue;

        /* bPrev is know to be a normal block here */
        switch (bPrev->bbJumpKind)
        {
        case BBJ_ALWAYS:

            /* Is the jump target rarely run? */
            if (bPrev->bbJumpDest->isRunRarely())
            {
#ifdef DEBUG
                if  (verbose)
                {
                    printf("Uncoditional jump to a rarely run block, marking BB%02u as rarely run\n",
                               bPrev->bbNum);
                }
#endif

                goto NEW_RARELY_RUN;
            }
            break;

        case BBJ_NONE:

            /* is fall through target rarely run? */
            if (block->isRunRarely())
            {
#ifdef DEBUG
                if  (verbose)
                {
                    printf("Falling into a rarely run block, marking BB%02u as rarely run\n",
                               bPrev->bbNum);
                }
#endif

                goto NEW_RARELY_RUN;
            }
            break;

        case BBJ_COND:

            if (!block->isRunRarely())
                continue;

            /* If both targets of the BBJ_COND are run rarely then don't reorder */
            if (bPrev->bbJumpDest->isRunRarely())
            {
                /* bPrev should also be marked as run rarely */
                if (!bPrev->isRunRarely())
                {
#ifdef DEBUG
                    if  (verbose)
                    {
                        printf("Both sides of a conditional jump are rarely run, marking BB%02u as rarely run\n",
                                   bPrev->bbNum);
                    }
#endif

NEW_RARELY_RUN:

#ifdef DEBUG
                    newRarelyRun = true;
#endif
                    /* Must not have previously been marked */
                    assert(!bPrev->isRunRarely());
                    
                    /* Mark bPrev as a new rarely run block */
                    bPrev->bbSetRunRarely();

                    /* Now go back to it's earliest predecessor to see */
                    /* if it too should now be marked as rarely run    */
                    flowList* pred = bPrev->bbPreds;

                    if (pred != NULL)
                    {
                        /* bPrev will be set to the earliest predecessor */
                        bPrev = pred->flBlock;
                    
                        for (pred  =  pred->flNext;
                             pred != NULL; 
                             pred  = pred->flNext)
                        {
                            /* Walk the flow graph forward from pred->flBlock */
                            /* if we find block == bPrev then pred->flBlock   */
                            /*  is an earlier predecessor.                    */

                            for (block  = pred->flBlock;
                                 block != NULL;
                                 block  = block->bbNext)
                            {
                                if (block == bPrev)
                                {
                                    bPrev = pred->flBlock;
                                    break;
                                }
                            }
                        }
                    
                        /* Set block to bPrev sucessor and start again */
                         block = bPrev->bbNext;

                        goto AGAIN;
                    }
                }
                break;
            }
        }
    }
    
    /* Now let us look for normal conditional branches that fall */
    /*  into rarely run blocks and normal unconditional branchs  */
    /*  that preceed a rarely run block                          */

    for( bPrev = fgFirstBB, block = bPrev->bbNext;
                            block != NULL;
         bPrev = block,     block = block->bbNext)
    {

        // If block is not run rarely, then check to make sure that it has
        // at least one non rarely run block.

        if (!block->isRunRarely())
        {
            bool rare = true;
            /* Make sure that block has at least one normal predecessor */
            for (flowList* pred  = block->bbPreds;
                           pred != NULL; 
                           pred  = pred->flNext)
            {
                /* Find the fall through predecessor, if any */
                if (!pred->flBlock->isRunRarely())
                {
                    rare = false;
                    break;
                }
            }
            if (rare)
            {
                block->bbSetRunRarely();
#ifdef DEBUG
                newRarelyRun = true;

                if  (verbose)
                {
                    printf("All branches to BB%02u are from rarely run blocks, marking as rarely run\n",
                               block->bbNum);
                }
#endif
            }
            
        }
            
        switch (bPrev->bbJumpKind)
        {
            BasicBlock *  bEnd;
            BasicBlock *  bNext;
            BasicBlock *  bDest;

        case BBJ_COND:

            if (bPrev->isRunRarely())
                break;

            /* bPrev is known to be a normal block here */

            /* For BBJ_COND we want to reorder if we fall into */
            /* a rarely run block from a normal block          */

            if (block->isRunRarely())
            {
                /* If marked with a BBF_DONT_REMOVE flag then we don't move the block */
                if (block->bbFlags & BBF_DONT_REMOVE)
                    break;

                bDest = bPrev->bbJumpDest;
                /* It has to be a forward jump to benifit from a reversal */
                if (bDest->bbNum <= block->bbNum)
                    break;

                /* Ok we found a block that should be relocated      */
                /* We will relocate (block .. bEnd)                  */
                /* Set bEnd to the last block that will be relocated */

                bEnd  = block;
                bNext = bEnd->bbNext;

                while (bNext != NULL)
                {
                    /* All the blocks must have the same try index, */
                    /*   must be run rarely                         */
                    /*   must not have the BBF_DONT_REMOVE flag set */

                    if ( ( block->bbTryIndex != bNext->bbTryIndex)  ||
                         (!bNext->isRunRarely())                    ||
                         ((bNext->bbFlags & BBF_DONT_REMOVE) != 0)     )
                    {
                        /* bEnd is set to the last block that we relocate */
                        break;
                    }

                    /* We can relocate bNext as well */
                    bEnd  = bNext;
                    bNext = bNext->bbNext;
                }
                
                /* Don't move a set of blocks that are at the end of the method */
                if (bNext == NULL)
                    break;

                /* Temporarily unlink (block .. bEnd) from the flow graph */

                bPrev->bbNext = bNext;

                /* Find new location for the rarely executed block    */
                /* Set afterBlk to the block which will preceed block */

                BasicBlock *  afterBlk;

                if (block->hasTryIndex())
                {
                    assert(block->bbTryIndex <= info.compXcptnsCount);

                    /* Setup startBlock and endBlk as the range to search */

                    BasicBlock *  startBlk = compHndBBtab[block->getTryIndex()].ebdTryBeg;
                    BasicBlock *  endBlk   = compHndBBtab[block->getTryIndex()].ebdTryEnd;
                
                    /* Skip until we find a matching try index */

                    while(startBlk->bbTryIndex != block->bbTryIndex)
                        startBlk = startBlk->bbNext;
                
                    /* Set afterBlock to the block which will insert after */

                    afterBlk = fgFindInsertPoint(block->bbTryIndex, 
                                                 startBlk, endBlk); 

                    /* See if afterBlk is the same as where we started */

                    if (afterBlk == bPrev)
                    {
                        /* We couldn't move the block, so put everything back */
                        
                        /* relink (block .. bEnd) into the flow graph */
                        
                        bPrev->bbNext = block;
#ifdef DEBUG
                        if  (verbose)
                        {
                            if (block != bEnd)
                                printf("Could not relocate rarely run blocks (BB%02u .. BB%02u)\n",
                                       block->bbNum, bEnd->bbNum);
                            else
                                printf("Could not relocate rarely run block BB%02u\n",
                                       block->bbNum);
                        }
#endif
                        break;
                    }
                }
                else
                {
                    /* We'll just insert the block at the end of the method */
                    afterBlk = fgLastBB;
                    assert(fgLastBB->bbNext == NULL);
                    assert(afterBlk != bPrev);
                }

#ifdef DEBUG
                movedBlocks = true;

                if  (verbose)
                {
                    if (block != bEnd)
                        printf("Relocated rarely run blocks (BB%02u .. BB%02u)\n",
                               block->bbNum, bEnd->bbNum);
                    else
                        printf("Relocated rarely run block BB%02u\n", 
                               block->bbNum);

                    printf("By reversing conditional jump at BB%02u\n", bPrev->bbNum);
                    printf("Relocated block inserted after BB%02u\n", afterBlk->bbNum);
                }
#endif
                /* Reverse the bPrev jump condition */
                        
                GenTree *  test = bPrev->bbTreeList;

                assert(test && test->gtOper == GT_STMT);
                test = test->gtPrev;
                assert(test && test->gtOper == GT_STMT);
                
                test = test->gtStmt.gtStmtExpr;
                assert(test->gtOper == GT_JTRUE);
                
                test->gtOp.gtOp1 = gtReverseCond(test->gtOp.gtOp1);

                /* Set the new jump dest for bPrev to the rarely run block */
                bPrev->bbJumpDest  = block;
                block->bbFlags    |= (BBF_JMP_TARGET | BBF_HAS_LABEL);

                /* We have decided to insert the block(s) after 'afterBlk' */
                
                assert(afterBlk->bbTryIndex == block->bbTryIndex);

                /* If afterBlk falls through, we are forced to insert */
                /* a jump around the block to insert                  */
                fgConnectFallThrough(afterBlk, afterBlk->bbNext);

                /* relink (block .. bEnd) into the flow graph */
                  
                bEnd    ->bbNext = afterBlk->bbNext;
                afterBlk->bbNext = block;

                /* If afterBlk was fgLastBB then update fgLastBB */
                if (afterBlk == fgLastBB)
                {
                    fgLastBB = bEnd;
                    assert(fgLastBB->bbNext == NULL);
                }

                /* If bEnd falls through, we are forced */
                /* to insert a jump back to bNext       */
                fgConnectFallThrough(bEnd, bNext);

                /* Does the new fall through block match the old jump dest? */
                
                if (bPrev->bbNext != bDest)
                {
                    /* We need to insert an unconditional branch after bPrev to bDest */

                    BasicBlock *  jmpBlk = fgNewBBafter(BBJ_ALWAYS, bPrev);

                    jmpBlk->bbHndIndex   = bPrev->bbHndIndex;
                    jmpBlk->bbJumpDest   = bDest;

                    fgReplacePred(bDest, bPrev, jmpBlk);
                    fgAddRefPred (jmpBlk, bPrev);
#ifdef DEBUG
                    if  (verbose)
                    {
                        printf("Had to add new block BB%02u, with an unconditional jump to BB%02u\n", 
                               jmpBlk->bbNum, jmpBlk->bbJumpDest->bbNum);
                    }
#endif
                }

                /* Set block to be the new bPrev->bbNext */
                /* It will be used as the next bPrev     */
                block = bPrev->bbNext;
            }
            break;

        case BBJ_NONE:

            /* COMPACT blocks if possible */

            if ( (block->bbRefs == 1) &&
                ((block->bbFlags & BBF_DONT_REMOVE) == 0))
            {
                fgCompactBlocks(bPrev);

                block = bPrev;
                break;
            }
        } // end of switch(bPrev->bbJumpKind)

    } // end of for loop(bPrev,block)
#if DEBUG
    bool changed = movedBlocks || newRarelyRun;
    if  (changed && verbose)
    {
        printf("\nAfter fgReorderBlocks");
        fgDispBasicBlocks();
        printf("\n");
    }
#endif
}

/*****************************************************************************
 *
 *  Function called to "comb" the basic block list
 *  Removes any empty blocks, unreacheable blocks and redundant jumps
 *  Most of those appear after dead store removal and folding of conditionals
 *
 *  It also compacts basic blocks 
 *   (consecutive basic blocks that should in fact be one)
 *
 *  @TODO [CONSIDER] [04/16/01] []:
 *    Those are currently introduced by field hoisting, Loop condition 
 *    duplication, etc.
 *    Why not allocate an extra basic block for every loop and prepend
 *    the statements to this basic block?
 *
 *  NOTE:
 *    Debuggable code and Min Optimization JIT also introduces basic blocks 
 *    but we do not optimize those!
 */

void                Compiler::fgUpdateFlowGraph()
{
#ifdef DEBUG
    if  (verbose) 
        printf("\n*************** In fgUpdateFlowGraph()");
#endif

    /* This should never be called for debugable code */

    assert(!opts.compMinOptim && !opts.compDbgCode);

#ifdef  DEBUG
    if  (verbose)
    {
        printf("\nBefore updating the flow graph:\n");
        fgDispBasicBlocks();
        printf("\n");
    }
#endif

    /* Walk all the basic blocks - look for unconditional jumps, empty blocks, blocks to compact, etc...
     *
     * OBSERVATION:
     *      Once a block is removed the predecessors are not accurate (assuming they were at the beginning)
     *      For now we will only use the information in bbRefs because it is easier to be updated
     */

    bool change;
    do
    {
        change = false;

        BasicBlock *  block;              // the current block
        BasicBlock *  bPrev = NULL;       // the previous non-worthless block
        BasicBlock *  bNext;              // the successor of the curent block

        for (block = fgFirstBB; 
             block != NULL; 
             block = block->bbNext)
        {
            /*  Some blocks may be already marked removed by other optimizations
             *  (e.g worthless loop removal), without being explicitely removed 
             *  from the list.
             */

            if (block->bbFlags & BBF_REMOVED)
            {
                if (bPrev)
                {
                    bPrev->bbNext = block->bbNext;
                }
                else
                {
                    /* WEIRD first basic block is removed - should have an assert here */
                    assert(!"First basic block marked as BBF_REMOVED???");

                    fgFirstBB = block->bbNext;
                }
                continue;
            }

            /*  We jump to the REPEAT label if we performed a change involving the current block
             *  This is in case there are other optimizations that can show up 
             *  (e.g. - compact 3 blocks in a row)
             *  If nothing happens, we then finish the iteration and move to the next block
             */

REPEAT:
            bNext = block->bbNext;

            /* Remove JUMPS to the following block */

            if (block->bbJumpKind == BBJ_COND   ||
                block->bbJumpKind == BBJ_ALWAYS  )
            {
                if (block->bbJumpDest == bNext)
                {
                    assert(fgIsPredForBlock(bNext, block));

                    if (block->bbJumpKind == BBJ_ALWAYS)
                    {
                        // We dont remove if the BBJ_ALWAYS is part of 
                        // BBJ_CALL pair                        
                        if (!bPrev || (bPrev->bbJumpKind != BBJ_CALL || (bPrev->bbFlags & BBF_RETLESS_CALL)==BBF_RETLESS_CALL ))
                        {
                            /* the unconditional jump is to the next BB  */
                            block->bbJumpKind = BBJ_NONE;
                            change = true;
                            
#ifdef  DEBUG
                            if  (verbose || SHOW_REMOVED)
                            {
                                printf("\nRemoving unconditional jump to next block (BB%02u -> BB%02u)\n",
                                    block->bbNum, bNext->bbNum);
                            }
#endif
                        }
                        
                    }
                    else
                    {
                        /* remove the conditional statement at the end of block */
                        assert(block->bbJumpKind == BBJ_COND);
                        assert(block->bbTreeList);

                        GenTreePtr      cond = block->bbTreeList->gtPrev;
                        assert(cond->gtOper == GT_STMT);
                        assert(cond->gtStmt.gtStmtExpr->gtOper == GT_JTRUE);

#ifdef  DEBUG
                        if  (verbose || SHOW_REMOVED)
                        {
                            printf("\nRemoving conditional jump to next block (BB%02u -> BB%02u)\n",
                                   block->bbNum, bNext->bbNum);
                        }
#endif
                        /* check for SIDE_EFFECTS */

                        if (cond->gtStmt.gtStmtExpr->gtFlags & GTF_SIDE_EFFECT)
                        {

                            /* Extract the side effects from the conditional */
                            GenTreePtr  sideEffList = NULL;

                            gtExtractSideEffList(cond->gtStmt.gtStmtExpr, &sideEffList);

                            if (sideEffList == NULL)
                                goto NO_SIDE_EFFECT;

                            assert(sideEffList->gtFlags & GTF_SIDE_EFFECT);

#ifdef  DEBUG
                            if  (verbose)
                            {
                                printf("\nConditional has side effects! Extracting side effects...\n");
                                gtDispTree(cond); printf("\n");
                                gtDispTree(sideEffList); printf("\n");
                            }
#endif

                            /* Replace the conditional statement with the list of side effects */
                            assert(sideEffList->gtOper != GT_STMT);
                            assert(sideEffList->gtOper != GT_JTRUE);

                            cond->gtStmt.gtStmtExpr = sideEffList;

                            if (fgStmtListThreaded)
                            {
                                /* Update the lclvar ref counts */
                                compCurBB = block;
                                fgUpdateRefCntForExtract(cond->gtStmt.gtStmtExpr, sideEffList);

                                /* Update ordering, costs, FP levels, etc. */
                                gtSetStmtInfo(cond);

                                /* Re-link the nodes for this statement */
                                fgSetStmtSeq(cond);
                            }

                            //assert(!"Found conditional with side effect!");
                        }
                        else
                        {
NO_SIDE_EFFECT:
                            /* conditional has NO side effect - remove it */
                            fgRemoveStmt(block, cond, fgStmtListThreaded);
                        }

                        /* Conditional is gone - simply fall into the next block */

                        block->bbJumpKind = BBJ_NONE;

                        /* Update bbRefs and bbNums - Conditional predecessors to the same
                         * block are counted twice so we have to remove one of them */

                        assert(bNext->bbRefs > 1);
                        bNext->bbRefs--;
                        fgRemovePred(bNext, block);
                        change = true;
                    }
                }
            }

            assert(!(block->bbFlags & BBF_REMOVED));

            /* COMPACT blocks if possible */

            if ((block->bbJumpKind == BBJ_NONE) && bNext)
            {
                if ( (bNext->bbRefs == 1) &&
                    !(bNext->bbFlags & BBF_DONT_REMOVE))
                {
                    fgCompactBlocks(block);

                    /* we compacted two blocks - goto REPEAT to catch similar cases */
                    change = true;
                    goto REPEAT;
                }
            }

            /* REMOVE UNREACHEABLE or EMPTY blocks - do not consider blocks marked BBF_DONT_REMOVE
             * These include first and last block of a TRY, exception handlers and RANGE_CHECK_FAIL THROW blocks */

            if (block->bbFlags & BBF_DONT_REMOVE)
            {
                bPrev = block;
                continue;
            }


            assert(!block->bbCatchTyp);
            assert(!(block->bbFlags & BBF_TRY_BEG));

            /* Remove UNREACHEABLE blocks
             *
             * We'll look for blocks that have bbRefs = 0 (blocks may become
             * unreacheable due to a BBJ_ALWAYS introduced by conditional folding for example)
             *
             *  @TODO [REVISIT] [04/16/01] []: We don't remove the last and first block of a TRY block (they are marked BBF_DONT_REMOVE)
             *          The reason is we will have to update the exception handler tables and we are lazy
             *
             *  @TODO [CONSIDER] [04/16/01] []: it may be the case that the graph is divided into 
             *          disjunct components and we may not remove the unreacheable ones until we find the 
             *          connected components ourselves
             */

            if (block->bbRefs == 0)
            {
                /* no references -> unreacheable - remove it */
                /* For now do not update the bbNums, do it at the end */

                fgRemoveBlock(block, bPrev, true);

                change     = true;

                /* we removed the current block - the rest of the optimizations won't have a target
                 * continue with the next one */

                continue;
            }
            else if (block->bbRefs == 1)
            {
                switch (block->bbJumpKind)
                {
                case BBJ_COND:
                case BBJ_ALWAYS:
                    if (block->bbJumpDest == block)
                    {
                        fgRemoveBlock(block, bPrev, true);

                        change     = true;

                        /* we removed the current block - the rest of the optimizations 
                         * won't have a target so continue with the next block */

                        continue;
                    }
                    break;

                default:
                    break;
                }
            }

            assert(!(block->bbFlags & BBF_REMOVED));

            /* Remove EMPTY blocks */

            if (block->bbTreeList == 0)
            {
                switch (block->bbJumpKind)
                {
                case BBJ_COND:
                case BBJ_SWITCH:
                case BBJ_THROW:

                    /* can never happen */
                    assert(!"Conditional or throw block with empty body!");
                    break;

                case BBJ_CALL:
                case BBJ_RET:
                case BBJ_RETURN:

                    /* leave them as is */
                    /* OBS - some stupid compilers generate multiple retuns and
                     * puts all of them at the end - to solve that we need the predecessor list */

                    break;

                case BBJ_ALWAYS:

                    /* a GOTO - cannot be to the next block since that must
                     * have been fixed by the other optimization above */
                    assert(block->bbJumpDest != block->bbNext ||
                           (bPrev && bPrev->bbJumpKind == BBJ_CALL));

                    /* Cannot remove the first BB */
                    if (!bPrev) break;

                    /* Do not remove a block that jumps to itself - used for while(true){} */
                    if (block->bbJumpDest == block) break;

                    /* Empty GOTO can be removed iff bPrev is BBJ_NONE */
                    if (bPrev->bbJumpKind != BBJ_NONE) break;

                    /* Can follow through since this is similar with removing
                     * a BBJ_NONE block, only the successor is different */

                case BBJ_NONE:

                    /* special case if this is the first BB */
                    if (!bPrev)
                    {
                        assert (block == fgFirstBB);
                    }
                    else
                    {
                        /* If this block follows a BBJ_CALL do not remove it
                         * (because we don not know who may jump to it) */
                        if (bPrev->bbJumpKind == BBJ_CALL)
                            break;
                    }

                    /* special case if this is the last BB */
                    if (block == fgLastBB)
                    {
                        if (!bPrev)
                            break;
                        fgLastBB = bPrev;
                    }

                    /* Remove the block */
                    fgRemoveBlock(block, bPrev, false);
                    change     = true;
                    break;
                }

                /* have we removed the block? */

                if  (block->bbFlags & BBF_REMOVED)
                {
                    /* block was removed - no change to bPrev */
                    continue;
                }
            }

            /* Set the predecessor of the last reacheable block
             * If we removed the current block, the predecessor remains unchanged
             * otherwise, since the current block is ok, it becomes the predecessor */

            assert(!(block->bbFlags & BBF_REMOVED));

            bPrev = block;
        }
    }
    while (change);

#ifdef  DEBUG
    if  (verbose)
    {
        printf("\nAfter updating the flow graph:\n");
        fgDispBasicBlocks(verboseTrees);
        printf("\n");
    }

    fgDebugCheckUpdate();
#endif

}


/*****************************************************************************
 *  Check that the flow graph is really updated
 */

#ifdef  DEBUG

void            Compiler::fgDebugCheckUpdate()
{
    if (!compStressCompile(STRESS_CHK_FLOW_UPDATE, 30))
        return;

    /* We check for these conditions:
     * no unreacheable blocks -> no blocks have bbRefs = 0
     * no empty blocks        -> no blocks have bbTreeList = 0
     * no un-imported blocks  -> no blocks have BBF_IMPORTED not set (this is
     *                           kind of redundand with the above, but to make sure)
     * no un-compacted blocks -> BBJ_NONE followed by block with no jumps to it (bbRefs = 1)
     */

    BasicBlock * prev, * block;
    for (prev = 0, block = fgFirstBB; block; prev = block, block = block->bbNext)
    {
        /* no unreacheable blocks */

        if  ((block->bbRefs == 0)                &&
             !(block->bbFlags & BBF_DONT_REMOVE)  )
        {
            assert(!"Unreacheable block not removed!");
        }

        /* no empty blocks */

        if  ((block->bbTreeList == 0)            &&
             !(block->bbFlags & BBF_DONT_REMOVE)  )
        {
            switch (block->bbJumpKind)
            {
            case BBJ_CALL:
            case BBJ_RET:
            case BBJ_RETURN:
                /* for BBJ_ALWAYS is probably just a GOTO, but will have to be treated */
            case BBJ_ALWAYS:
                break;

            default:
                /* it may be the case that the block had more than one reference to it
                 * so we couldn't remove it */

                if (block->bbRefs == 0)
                    assert(!"Empty block not removed!");
            }
        }

        /* no un-imported blocks */

        if  (!(block->bbFlags & BBF_IMPORTED))
        {
            /* internal blocks do not count */

            if (!(block->bbFlags & BBF_INTERNAL))
                assert(!"Non IMPORTED block not removed!");
        }

        /* no jumps to the next block
         * Unless we go out of an exception handler (could optimize it but it's not worth it) */

        if (block->bbJumpKind == BBJ_COND   ||
            (block->bbJumpKind == BBJ_ALWAYS && 
            (!prev || prev->bbJumpKind != BBJ_CALL || (prev->bbFlags & BBF_RETLESS_CALL)==BBF_RETLESS_CALL ) ))
        {
            if (block->bbJumpDest == block->bbNext)
                //&& !block->bbCatchTyp)
            {
                assert(!"Jump to the next block!");
            }
        }

        /* For a BBJ_CALL block we make sure that we are followed by */
        /* an BBF_INTERNAL BBJ_ALWAYS block or that its a BBF_RETLESS_CALL */
        if (block->bbJumpKind == BBJ_CALL)
        {
            assert( ( block->bbFlags & BBF_RETLESS_CALL )         ||
                    ((block->bbNext != NULL)                   &&
                     (block->bbNext->bbJumpKind == BBJ_ALWAYS) &&
                     (block->bbNext->bbFlags & BBF_INTERNAL)     )  );
        }

        /* no un-compacted blocks */

        if (block->bbJumpKind == BBJ_NONE)
        {
            if ((block->bbNext->bbRefs == 1) && !(block->bbNext->bbFlags & BBF_DONT_REMOVE))
            {
                assert(!"Found un-compacted blocks!");
            }
        }
    }
}

#endif // DEBUG

/*****************************************************************************
 *
 * Insert a BasicBlock after the given block.
 */

BasicBlock *        Compiler::fgNewBBafter(BBjumpKinds  jumpKind,
                                           BasicBlock * block)
{
    // Create a new BasicBlock and chain it in

    BasicBlock * newBlk = bbNewBasicBlock(jumpKind);
    newBlk->bbFlags    |= BBF_INTERNAL;

    newBlk->bbNext   = block->bbNext;
    block->bbNext    = newBlk;

    /* Update fgLastBB if this is the last block */

    if  (newBlk->bbNext == 0)
    {
        assert(block == fgLastBB);
        fgLastBB = newBlk;
    }

    newBlk->bbRefs = 0;

    if (block->bbFlags & BBF_HAS_HANDLER)
    {
        assert(block->hasTryIndex());
           
        newBlk->bbFlags    |= BBF_HAS_HANDLER;
        newBlk->bbTryIndex  = block->bbTryIndex;
    }

    if (block->bbFallsThrough() && block->isRunRarely())
    {
        newBlk->bbSetRunRarely();
    }

    return newBlk;
}

/*****************************************************************************
 *  Finds the block closest to endBlk in [startBlk...endBlk) after 
 *  which a block can be inserted easily, and which matches tryIndex.
 *  if NearBlk is non-NULL the we return the closest block after nearBlk
 *  that will work best/
 *
 *  Note that nearBlk defaults to NULL, which is the proper value to use
 *  if the new block is to be executed rarely.
 *  
 *  Note that the returned block cannot be in an inner try.
 */

/* static */
BasicBlock *        Compiler::fgFindInsertPoint(unsigned        tryIndex,
                                                BasicBlock *    startBlk,
                                                BasicBlock *    endBlk,
                                                BasicBlock *    nearBlk /* = NULL */)
{
    assert(startBlk && startBlk->bbTryIndex == tryIndex);

    bool           runRarely   = (nearBlk == NULL);
    bool           reachedNear = false;
    BasicBlock *   bestBlk     = NULL;
    BasicBlock *   goodBlk     = NULL;
    BasicBlock *   blk;

    if (nearBlk != NULL)
    {
        /* Does the nearBlk preceed the startBlk? */
        for (blk = nearBlk;  blk != NULL; blk = blk->bbNext)
        {
            if (blk == startBlk)
            {
                reachedNear = true;
                break;
            }
        }
    }

    for (blk = startBlk; blk != endBlk; blk = blk->bbNext)
    {
        if (blk == nearBlk)
            reachedNear = true;

        /* Only block with a matching try index are candidates */
        if (blk->bbTryIndex == tryIndex)
        {
            /* We looking for blocks that don't end with a fall through */
            if (!blk->bbFallsThrough())
            {
                if (bestBlk)
                {
                    /* if bestBlk is run rarely and blk is not then skip */
                    if (runRarely               && 
                        bestBlk->isRunRarely()  &&
                        !blk->isRunRarely()       )
                    {
                        continue;
                    }
                }
                
                /* Prefer blocks closer to endBlk */
                bestBlk     = blk;

                if (reachedNear)
                    goto DONE;
            }

            if ((blk->bbJumpKind != BBJ_CALL) || (blk->bbFlags & BBF_RETLESS_CALL))
            {
                /* Avoid setting goodBlk to a BBJ_COND */
                if ((goodBlk             == NULL)      ||
                    (goodBlk->bbJumpKind == BBJ_COND)  ||
                    (blk->bbJumpKind     != BBJ_COND))
                {
                    if ((goodBlk == NULL) || !reachedNear)
                    {
                        /* Remember this block in case we don't set a bestBlk */
                       goodBlk = blk;
                    }
                }
            }
        }
    }

    /* If we didn't find a non-fall_through block, then insert at the last good block */

    if (bestBlk == NULL)
        bestBlk = goodBlk;

DONE:

    return bestBlk;
}

/*****************************************************************************
 * Inserts a BasicBlock at the given tryIndex, and after afterBlk.
 * Note that it can not be in an inner try.
 * nearBlk defaults to NULL and should always be use if the new block
 * is a rarely run block
 */

BasicBlock *        Compiler::fgNewBBinRegion(BBjumpKinds  jumpKind,
                                              unsigned     tryIndex,
                                              BasicBlock*  nearBlk /* = NULL */)
{
    /* Set afterBlk to the block which will preceed block */
  
    BasicBlock *  afterBlk;

    if ((tryIndex == 0) && (nearBlk == NULL))
    {
        /* We'll just insert the block at the end of the method */
        afterBlk = fgLastBB;
        assert(fgLastBB);
        assert(afterBlk->bbNext == NULL);
    }
    else
    {
        assert(tryIndex <= info.compXcptnsCount);

        // Set the start and end limit for inserting the block

        BasicBlock *  startBlk;
        BasicBlock *  endBlk;

        if (tryIndex == 0)
        {
            startBlk = fgFirstBB;
            endBlk   = fgLastBB;
        }
        else
        {
            startBlk = compHndBBtab[tryIndex-1].ebdTryBeg;
            endBlk   = compHndBBtab[tryIndex-1].ebdTryEnd;
        }

        /* Skip until we find a matching try index */

        while(startBlk->bbTryIndex != tryIndex)
            startBlk = startBlk->bbNext;

        afterBlk = fgFindInsertPoint(tryIndex, 
                                     startBlk, endBlk, 
                                     nearBlk);
    }

    /* We have decided to insert the block after 'afterBlk'. */
    assert(afterBlk);
    assert((afterBlk->bbTryIndex == tryIndex) || (afterBlk == fgLastBB));

    /* If afterBlk falls through, we insert a jump to the fall through */
    BasicBlock * jmpBlk;
    jmpBlk = fgConnectFallThrough(afterBlk, afterBlk->bbNext);

    if (jmpBlk)
        afterBlk = jmpBlk;
   
    /* Insert the new block */  
    BasicBlock * newBlk = fgNewBBafter(jumpKind, afterBlk);

    /* If there are any try regions that extend to the end of the method, and our new block
       needs tryIndex=0, then close those try regions so that they won't include the new block */
    if (tryIndex == 0)
    {
        fgCloseTryRegions(newBlk);
    }

    return newBlk;
}


/*****************************************************************************
 * This method closes any try regions that extend to the end of the method.
 * It is used when adding a new block at the end of the method, so that the
 * try region will not include it when it's not supposed to.
 */

void        Compiler::fgCloseTryRegions(BasicBlock*  newBlk)
{
    if (fgLastBB == newBlk)
    {
        unsigned        XTnum;
        EHblkDsc *      HBtab;

        for (XTnum = 0, HBtab = compHndBBtab;
             XTnum < info.compXcptnsCount;
             XTnum++  , HBtab++)
        {
            if (!HBtab->ebdTryEnd) 
            {
                HBtab->ebdTryEnd = newBlk;
                HBtab->ebdTryEnd->bbFlags |= BBF_HAS_LABEL | BBF_DONT_REMOVE | BBF_TRY_HND_END;
#ifdef DEBUG
                if (verbose) 
                {
                    printf("New fgLastBB (BB%02u) at index #0 is forcing us to close EH index #%u that previously extended to the end of the method\n",
                           fgLastBB->bbNum, XTnum);
                }
#endif            
            }
        }
    }
}


/*****************************************************************************
 */

/* static */
unsigned            Compiler::acdHelper(addCodeKind codeKind)
{
    switch(codeKind)
    {
    case ACK_RNGCHK_FAIL: return CORINFO_HELP_RNGCHKFAIL;
    case ACK_ARITH_EXCPN: return CORINFO_HELP_OVERFLOW;
    default: assert(!"Bad codeKind"); return 0;
    }
}

/*****************************************************************************
 *
 *  Find/create an added code entry associated with the given block and with
 *  the given kind.
 */

BasicBlock *        Compiler::fgAddCodeRef(BasicBlock * srcBlk,
                                           unsigned     refData,
                                           addCodeKind  kind,
                                           unsigned     stkDepth)
{
    /* For debuggable code, genJumpToThrowHlpBlk() will generate the 'throw'
       code inline. It has to be kept consistent with fgAddCodeRef() */
    if (opts.compDbgCode)
        return NULL;

    const static
    BYTE            jumpKinds[] =
    {
        BBJ_NONE,               // ACK_NONE
        BBJ_THROW,              // ACK_RNGCHK_FAIL
        BBJ_ALWAYS,             // ACK_PAUSE_EXEC
        BBJ_THROW,              // ACK_ARITH_EXCP, ACK_OVERFLOW
    };

    assert(sizeof(jumpKinds) == ACK_COUNT); // sanity check

    /* First look for an existing entry that matches what we're looking for */

    AddCodeDsc  *   add = fgFindExcptnTarget(kind, refData);

    if (add) // found it
    {
#if TGT_x86
        // @TODO [PERF] [04/16/01] []: Performance opportunity
        //
        // If different range checks happen at different stack levels,
        // they cant all jump to the same "call @rngChkFailed" AND have
        // frameless methods, as the rngChkFailed may need to unwind the
        // stack, and we have to be able to report the stack level
        //
        // The following check forces most methods that references an
        // array element in a parameter list to have an EBP frame,
        // this restriction can be removed with more careful code
        // generation for BBJ_THROW (i.e. range check failed)
        //
        if  (add->acdStkLvl != stkDepth)
            genFPreqd = true;
#endif
        goto DONE;
    }

    /* We have to allocate a new entry and prepend it to the list */

    add = (AddCodeDsc *)compGetMem(sizeof(*fgAddCodeList));
    add->acdData   = refData;
    add->acdKind   = kind;
#if TGT_x86
    add->acdStkLvl = stkDepth;
#endif
    add->acdNext   = fgAddCodeList;
                     fgAddCodeList = add;

    /* Create the target basic block */

    BasicBlock  *   newBlk;

    newBlk          =
    add->acdDstBlk  = fgNewBBinRegion((BBjumpKinds)jumpKinds[kind],
                                      srcBlk->bbTryIndex);

    add->acdDstBlk->bbFlags |= BBF_JMP_TARGET|BBF_HAS_LABEL;

#ifdef DEBUG
    if (verbose)
    {
        char *  msg = "";
        if  (kind == ACK_RNGCHK_FAIL)
            msg = " for RNGCHK_FAIL";
        else if  (kind == ACK_PAUSE_EXEC)
            msg = " for PAUSE_EXEC";
        else if  (kind == ACK_OVERFLOW)
            msg = " for OVERFLOW";

        printf("\nfgAddCodeRef -"
               " Add BB in Try%s, new block BB%02u [%08X], stkDepth is %d\n",
               msg, add->acdDstBlk->bbNum, add->acdDstBlk, stkDepth);
    }
#endif


#ifdef DEBUG
    newBlk->bbTgtStkDepth = stkDepth;
#endif

    /* Mark the block as added by the compiler and not removable by future flow
       graph optimizations. Note that no bbJumpDest points to these blocks. */

    newBlk->bbFlags |= BBF_DONT_REMOVE;

    /* Remember that we're adding a new basic block */

    fgAddCodeModf      = true;
    fgRngChkThrowAdded = true;

    /* Now figure out what code to insert */

    GenTreePtr  tree;

    switch (kind)
    {
        int     helper;

    case ACK_RNGCHK_FAIL:   helper = CORINFO_HELP_RNGCHKFAIL;
                            goto ADD_HELPER_CALL;

    case ACK_ARITH_EXCPN:   helper = CORINFO_HELP_OVERFLOW;
                            assert(ACK_OVERFLOW == ACK_ARITH_EXCPN);
                            goto ADD_HELPER_CALL;

    ADD_HELPER_CALL:

        /* Add the appropriate helper call */

        tree = gtNewIconNode(refData, TYP_INT);
#if TGT_x86
        tree->gtFPlvl = 0;
#endif
        tree = gtNewArgList(tree);
#if TGT_x86
        tree->gtFPlvl = 0;
#endif
        tree = gtNewHelperCallNode(helper, TYP_VOID, 0, tree);
#if TGT_x86
        tree->gtFPlvl = 0;
#endif

#if TGT_RISC

        /* Make sure that we have room for at least one argument */

        if (fgPtrArgCntMax == 0)
            fgPtrArgCntMax = 1;
#endif
        /* The constant argument must be passed in registers */

        assert(tree->gtOper == GT_CALL);
        assert(tree->gtCall.gtCallArgs->gtOper == GT_LIST);
        assert(tree->gtCall.gtCallArgs->gtOp.gtOp1->gtOper == GT_CNS_INT);
        assert(tree->gtCall.gtCallArgs->gtOp.gtOp2 == 0);

        tree->gtCall.gtCallRegArgs = gtNewOperNode(GT_LIST,
                                                   TYP_VOID,
                                                   tree->gtCall.gtCallArgs->gtOp.gtOp1, 0);
        tree->gtCall.regArgEncode  = (unsigned short)REG_ARG_0;

#if TGT_x86
        tree->gtCall.gtCallRegArgs->gtFPlvl = 0;
#endif

        tree->gtCall.gtCallArgs->gtOp.gtOp1 = gtNewNothingNode();
        tree->gtCall.gtCallArgs->gtOp.gtOp1->gtFlags |= GTF_REG_ARG;

#if TGT_x86
        tree->gtCall.gtCallArgs->gtOp.gtOp1->gtFPlvl = 0;
#endif
        break;

//  case ACK_PAUSE_EXEC:
//      assert(!"add code to pause exec");

    default:
        assert(!"unexpected code addition kind");
    }

    /* Store the tree in the new basic block */

    fgStoreFirstTree(newBlk, tree);

DONE:

    return  add->acdDstBlk;
}

/*****************************************************************************
 * Finds the block to jump to, to throw a given kind of exception
 * We maintain a cache of one AddCodeDsc for each kind, to make searching fast.
 * Note : Each block uses the same (maybe shared) block as the jump target for
 * a given type of exception
 */

Compiler::AddCodeDsc *      Compiler::fgFindExcptnTarget(addCodeKind  kind,
                                                         unsigned     refData)
{
    if (!(fgExcptnTargetCache[kind] &&  // Try the cached value first
          fgExcptnTargetCache[kind]->acdData == refData))
    {
        // Too bad, have to search for the jump target for the exception

        AddCodeDsc * add = NULL;

        for (add = fgAddCodeList; add; add = add->acdNext)
        {
            if  (add->acdData == refData && add->acdKind == kind)
                break;
        }

        fgExcptnTargetCache[kind] = add; // Cache it
    }

    return fgExcptnTargetCache[kind];
}

/*****************************************************************************
 *
 *  The given basic block contains an array range check; return the label this
 *  range check is to jump to upon failure.
 */

inline
BasicBlock *        Compiler::fgRngChkTarget(BasicBlock *block, unsigned stkDepth)
{
    /* We attach the target label to the containing try block (if any) */

    return  fgAddCodeRef(block, block->bbTryIndex, ACK_RNGCHK_FAIL, stkDepth);
}

/*****************************************************************************
 *
 *  Store the given tree in the specified basic block (which must be empty).
 */

GenTreePtr          Compiler::fgStoreFirstTree(BasicBlock * block,
                                               GenTreePtr   tree)
{
    GenTreePtr      stmt;

    assert(block);
    assert(block->bbTreeList == 0);

    assert(tree);
    assert(tree->gtOper != GT_STMT);

    /* Allocate a statement node */

    stmt = gtNewStmt(tree);

    /* Now store the statement in the basic block */

    block->bbTreeList       =
    stmt->gtPrev            = stmt;
    stmt->gtNext            = 0;

    /*
     *  Since we add calls to raise range check error after ordering,
     *  we must set the order here.
     */

    fgSetBlockOrder(block);
    
    /* The block should not already be marked as imported */
    assert((block->bbFlags & BBF_IMPORTED) == 0);

    block->bbFlags         |= BBF_IMPORTED;

    return  stmt;
}

/*****************************************************************************
 *
 *  Assigns sequence numbers to the given tree and its sub-operands, and
 *  threads all the nodes together via the 'gtNext' and 'gtPrev' fields.
 */

void                Compiler::fgSetTreeSeq(GenTreePtr tree)
{
    genTreeOps      oper;
    unsigned        kind;

    assert(tree && (int)tree != 0xDDDDDDDD);
    assert(tree->gtOper != GT_STMT);

    /* Figure out what kind of a node we have */

    oper = tree->OperGet();
    kind = tree->OperKind();

    /* Is this a leaf/constant node? */

    if  (kind & (GTK_CONST|GTK_LEAF))
        goto DONE;

    /* Is it a 'simple' unary/binary operator? */

    if  (kind & GTK_SMPOP)
    {
        GenTreePtr      op1 = tree->gtOp.gtOp1;
        GenTreePtr      op2 = tree->gtGetOp2();

        /* Check for a nilary operator */

        if  (!op1)
        {
            assert(op2 == 0);
            goto DONE;
        }

        /* Is this a unary operator?
         * Although UNARY GT_IND has a special structure */

        if  (oper == GT_IND)
        {
            /* Visit the indirection first - op2 may point to the
             * jump Label for array-index-out-of-range */

            fgSetTreeSeq(op1);

#if CSELENGTH

            /* Special case: GT_IND may have a GT_ARR_LENREF node */

            if  (tree->gtInd.gtIndLen)
            {
                if  (tree->gtFlags & GTF_IND_RNGCHK)
                    fgSetTreeSeq(tree->gtInd.gtIndLen);
            }

#endif
            goto DONE;
        }

        /* Now this is REALLY a unary operator */

        if  (!op2)
        {
            /* Visit the (only) operand and we're done */

            fgSetTreeSeq(op1);
            goto DONE;
        }

#if INLINING

        /*
            For "real" ?: operators, we make sure the order is
            as follows:

                condition
                1st operand
                GT_COLON
                2nd operand
                GT_QMARK
         */

        if  (oper == GT_QMARK)
        {
            assert((tree->gtFlags & GTF_REVERSE_OPS) == 0);

            fgSetTreeSeq(op1);
            fgSetTreeSeq(op2->gtOp.gtOp1);
            fgSetTreeSeq(op2);
            fgSetTreeSeq(op2->gtOp.gtOp2);

            goto DONE;
        }

        if  (oper == GT_COLON)
            goto DONE;

#endif

        /* This is a binary operator */

        if  (tree->gtFlags & GTF_REVERSE_OPS)
        {
            fgSetTreeSeq(op2);
            fgSetTreeSeq(op1);
        }
        else
        {
            fgSetTreeSeq(op1);
            fgSetTreeSeq(op2);
        }

        goto DONE;
    }

    /* See what kind of a special operator we have here */

    switch  (oper)
    {
    case GT_FIELD:
        assert(tree->gtField.gtFldObj == 0);
        break;

    case GT_CALL:

        /* We'll evaluate the 'this' argument value first */
        if  (tree->gtCall.gtCallObjp)
            fgSetTreeSeq(tree->gtCall.gtCallObjp);

        /* We'll evaluate the arguments next, left to right
         * NOTE: setListOrder neds cleanup - eliminate the #ifdef afterwards */

        if  (tree->gtCall.gtCallArgs)
        {
#if 1
            fgSetTreeSeq(tree->gtCall.gtCallArgs);
#else
            GenTreePtr      args = tree->gtCall.gtCallArgs;

            do
            {
                assert(args && args->gtOper == GT_LIST);
                fgSetTreeSeq(args->gtOp.gtOp1);
                args = args->gtOp.gtOp2;
            }
            while (args);
#endif
        }

        /* Evaluate the temp register arguments list
         * This is a "hidden" list and its only purpose is to
         * extend the life of temps until we make the call */

        if  (tree->gtCall.gtCallRegArgs)
        {
#if 1
            fgSetTreeSeq(tree->gtCall.gtCallRegArgs);
#else
            GenTreePtr      tmpArg = tree->gtCall.gtCallRegArgs;

            do
            {
                assert(tmpArg && tmpArg->gtOper == GT_LIST);
                fgSetTreeSeq(tmpArg->gtOp.gtOp1);
                tmpArg = tmpArg->gtOp.gtOp2;
            }
            while (tmpArg);
#endif
        }

        if (tree->gtCall.gtCallCookie)
            fgSetTreeSeq(tree->gtCall.gtCallCookie);

        if (tree->gtCall.gtCallType == CT_INDIRECT)
            fgSetTreeSeq(tree->gtCall.gtCallAddr);

        break;

#if CSELENGTH

    case GT_ARR_LENREF:

        if  (tree->gtFlags & GTF_ALN_CSEVAL)
            fgSetTreeSeq(tree->gtArrLen.gtArrLenAdr);

        if  (tree->gtArrLen.gtArrLenCse)
            fgSetTreeSeq(tree->gtArrLen.gtArrLenCse);

        break;

#endif

    case GT_ARR_ELEM:

        fgSetTreeSeq(tree->gtArrElem.gtArrObj);

        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
            fgSetTreeSeq(tree->gtArrElem.gtArrInds[dim]);

        break;

#ifdef  DEBUG
    default:
        gtDispTree(tree);
        assert(!"unexpected operator");
#endif
    }

DONE:

    /* Append to the node list */

    ++fgTreeSeqNum;

#ifdef  DEBUG
    tree->gtSeqNum = fgTreeSeqNum;

    if  (verbose & 0)
        printf("SetTreeOrder: [%08X] followed by [%08X]\n", fgTreeSeqLst, tree);
#endif

    fgTreeSeqLst->gtNext = tree;
                           tree->gtNext = 0;
                           tree->gtPrev = fgTreeSeqLst;
                                          fgTreeSeqLst = tree;

    /* Remember the very first node */

    if  (!fgTreeSeqBeg) 
    {
        fgTreeSeqBeg = tree;
#ifdef DEBUG
        assert(tree->gtSeqNum == 1);
#endif
    }
}

/*****************************************************************************
 *
 *  Figure out the order in which operators should be evaluated, along with
 *  other information (such as the register sets trashed by each subtree).
 */

void                Compiler::fgSetBlockOrder()
{
#ifdef DEBUG
    if  (verbose) 
    {
        printf("*************** In fgSetBlockOrder()\n");

        fgDispBasicBlocks(verboseTrees);
    }
#endif
    /* Walk the basic blocks to assign sequence numbers */

#ifdef  DEBUG
    BasicBlock::s_nMaxTrees = 0;
#endif

    for (BasicBlock * block = fgFirstBB; block; block = block->bbNext)
    {
        /* If this block is a loop header, mark it appropriately */

        if  (block->isLoopHead())
            fgMarkLoopHead(block);

        fgSetBlockOrder(block);
    }

    /* Remember that now the tree list is threaded */

    fgStmtListThreaded = true;

#ifdef  DEBUG
//  printf("The biggest BB has %4u tree nodes\n", BasicBlock::s_nMaxTrees);
#endif
}


/*****************************************************************************/

void                Compiler::fgSetStmtSeq(GenTreePtr tree)
{
    GenTree         list;            // helper node that we use to start the StmtList
                                     // It's located in front of the first node in the list

    assert(tree->gtOper == GT_STMT);

    /* Assign numbers and next/prev links for this tree */

    fgTreeSeqNum = 0;
    fgTreeSeqLst = &list;
    fgTreeSeqBeg = 0;
    fgSetTreeSeq(tree->gtStmt.gtStmtExpr);

    /* Record the address of the first node */

    tree->gtStmt.gtStmtList = fgTreeSeqBeg;

#ifdef  DEBUG

    GenTreePtr temp;
    GenTreePtr last;

    if  (list.gtNext->gtPrev != &list)
    {
        printf("&list [%08X] != list.next->prev [%08X]\n", &list, list.gtNext->gtPrev);
        goto BAD_LIST;
    }

    for (temp = list.gtNext, last = &list; temp; last = temp, temp = temp->gtNext)
    {
        if (temp->gtPrev != last)
        {
            printf("%08X->gtPrev = %08X, but last = %08X\n", temp, temp->gtPrev, last);

        BAD_LIST:

            printf("\n");
            gtDispTree(tree->gtStmt.gtStmtExpr);
            printf("\n");

            for (GenTreePtr temp = &list; temp; temp = temp->gtNext)
                printf("  entry at %08x [prev=%08X,next=%08X]\n", temp, temp->gtPrev, temp->gtNext);

            printf("\n");
            assert(!"Badly linked tree");
        }
    }
#endif

    /* Fix the first node's 'prev' link */

    assert(list.gtNext->gtPrev == &list);
           list.gtNext->gtPrev = 0;

    /* Keep track of the highest # of tree nodes */

#ifdef  DEBUG
    if  (BasicBlock::s_nMaxTrees < fgTreeSeqNum)
         BasicBlock::s_nMaxTrees = fgTreeSeqNum;
#endif

}

/*****************************************************************************/

void                Compiler::fgSetBlockOrder(BasicBlock * block)
{
    GenTreePtr      tree;

    tree = block->bbTreeList;
    if  (!tree)
        return;

    for (;;)
    {
        fgSetStmtSeq(tree);

        /* Are there any more trees in this basic block? */

        if (!tree->gtNext)
        {
            /* last statement in the tree list */
            assert(block->bbTreeList->gtPrev == tree);
            break;
        }

#ifdef DEBUG
        if (block->bbTreeList == tree)
        {
            /* first statement in the list */
            assert(tree->gtPrev->gtNext == 0);
        }
        else
            assert(tree->gtPrev->gtNext == tree);

        assert(tree->gtNext->gtPrev == tree);
#endif

        tree = tree->gtNext;
    }
}

/*****************************************************************************
 *
 * For GT_INITBLK and GT_COPYBLK, the tree looks like this :
 *                                tree->gtOp
 *                                 /    \
 *                               /        \.
 *                           GT_LIST  [size/clsHnd]
 *                            /    \
 *                           /      \
 *                       [dest]     [val/src]
 *
 * ie. they are ternary operators. However we use nested binary trees so that
 * GTF_REVERSE_OPS will be set just like for other binary operators. As the
 * operands need to end up in specific registers to issue the "rep stos" or
 * the "rep movs" instruction, if we dont allow the order of evaluation of
 * the 3 operands to be mixed, we may generate really bad code
 *
 * eg. For "rep stos", [val] has to be in EAX. Then if [size]
 * has a division, we will have to spill [val] from EAX. It will be better to
 * evaluate [size] and the evaluate [val] into EAX.
 *
 * This function stores the operands in the order to be evaluated
 * into opsPtr[]. The regsPtr[] contains reg0,reg1,reg2 in the correspondingly
 * switched order.
 */


void            Compiler::fgOrderBlockOps( GenTreePtr   tree,
                                           regMaskTP    reg0,
                                           regMaskTP    reg1,
                                           regMaskTP    reg2,
                                           GenTreePtr * opsPtr,   // OUT
                                           regMaskTP  * regsPtr)  // OUT
{
    assert(tree->OperGet() == GT_INITBLK || tree->OperGet() == GT_COPYBLK);

    assert(tree->gtOp.gtOp1 && tree->gtOp.gtOp1->OperGet() == GT_LIST);
    assert(tree->gtOp.gtOp1->gtOp.gtOp1 && tree->gtOp.gtOp1->gtOp.gtOp2);
    assert(tree->gtOp.gtOp2);

    GenTreePtr ops[3] =
    {
        tree->gtOp.gtOp1->gtOp.gtOp1,       // Dest address
        tree->gtOp.gtOp1->gtOp.gtOp2,       // Val / Src address
        tree->gtOp.gtOp2                    // Size of block
    };

    unsigned regs[3] = { reg0, reg1, reg2 };

    static int blockOpsOrder[4][3] =
                        //      tree->gtFlags    |  tree->gtOp.gtOp1->gtFlags
    {                   //  ---------------------+----------------------------
        { 0, 1, 2 },    //           -           |              -
        { 2, 0, 1 },    //     GTF_REVERSE_OPS   |              -
        { 1, 0, 2 },    //           -           |       GTF_REVERSE_OPS
        { 2, 1, 0 }     //     GTF_REVERSE_OPS   |       GTF_REVERSE_OPS
    };

    int orderNum =              ((tree->gtFlags & GTF_REVERSE_OPS) != 0) * 1 +
                    ((tree->gtOp.gtOp1->gtFlags & GTF_REVERSE_OPS) != 0) * 2;

    assert(orderNum < 4);

    int * order = blockOpsOrder[orderNum];

    // Fill in the OUT arrays according to the order we have selected

     opsPtr[0]  =  ops[ order[0] ];
     opsPtr[1]  =  ops[ order[1] ];
     opsPtr[2]  =  ops[ order[2] ];

    regsPtr[0]  = regs[ order[0] ];
    regsPtr[1]  = regs[ order[1] ];
    regsPtr[2]  = regs[ order[2] ];
}


/*****************************************************************************/
#ifdef DEBUG
/*****************************************************************************
 *
 *  A DEBUG-only routine to display the bbPreds basic block list.
 */

void                Compiler::fgDispPreds(BasicBlock * block)
{
    unsigned    cnt   = 0;

    for (flowList* pred = block->bbPreds; pred; pred = pred->flNext)
    {
        printf("%c", (cnt == 0) ? ' ' : ',');
        printf("BB%02u", pred->flBlock->bbNum);
        cnt++;
    }
    printf(" ");

    while (cnt < 4)
    {
        printf("     ");
            cnt++;
    }
}

void                Compiler::fgDispReach()
{
    BasicBlock    *    block;
    unsigned           num;

    printf("------------------------------------------------\n");
    printf("BBnum  Reachable by \n");
    printf("------------------------------------------------\n");

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        printf("BB%02u : ", block->bbNum);
        for (num = 0; num < fgBBcount; num++) 
        {
            if  (block->bbReach[num / USZ] & (1U << (num % USZ)))
            {
                printf("BB%02u ", num+1);
            }
        }
        printf("\n");
    }
}

void                Compiler::fgDispDoms()
{
    BasicBlock    *    block;
    unsigned           num;

    printf("------------------------------------------------\n");
    printf("BBnum  Dominated by \n");
    printf("------------------------------------------------\n");

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        printf("BB%02u : ", block->bbNum);
        for (num = 0; num < fgBBcount; num++) 
        {
            if  (block->bbDom[num / USZ] & (1U << (num % USZ)))
            {
                printf("BB%02u ", num+1);
            }
        }
        printf("\n");
    }
}

#ifdef DEBUG
/*****************************************************************************/

void                Compiler::fgDispHandlerTab()
{
    unsigned    XTnum;
    EHblkDsc *  handlerTab;

    printf("\n***************  Exception Handling table");


    if (info.compXcptnsCount == 0)
    {
        printf(" is empty\n");
        return;
    }

    printf("\nindex  nest, enclosing\n");
    for (XTnum = 0, handlerTab = compHndBBtab;
         XTnum < info.compXcptnsCount;
         XTnum++,   handlerTab++)
    {
        printf(" %2u  ::  %2u,  ", XTnum, handlerTab->ebdNesting);
        
        if (handlerTab->ebdEnclosing == NO_ENCLOSING_INDEX)
            printf("    ");
        else
            printf(" %2u ", handlerTab->ebdEnclosing);
        
        if (handlerTab->ebdHndBeg->bbCatchTyp == BBCT_FAULT)
            printf("- Region BB%02u", handlerTab->ebdTryBeg->bbNum);
        else
            printf("- Try at BB%02u", handlerTab->ebdTryBeg->bbNum);

        if (handlerTab->ebdTryBeg->bbNum != ebdTryEndBlkNum(handlerTab) - 1)
            printf("..BB%02u",ebdTryEndBlkNum(handlerTab) - 1);
        else
            printf("      ");

        printf(" [%03X..%03X], ", handlerTab->ebdTryBeg->bbCodeOffs,
                                  ebdTryEndOffs(handlerTab) - 1);
        
        
        if (handlerTab->ebdHndBeg->bbCatchTyp == BBCT_FINALLY)
            printf("Finally");
        else if (handlerTab->ebdHndBeg->bbCatchTyp == BBCT_FAULT)
            printf("Fault  ");
        else
            printf("Handler");

        printf(" at BB%02u", handlerTab->ebdHndBeg->bbNum);

        if (handlerTab->ebdHndBeg->bbNum != ebdHndEndBlkNum(handlerTab) - 1)
            printf("..BB%02u", ebdHndEndBlkNum(handlerTab) - 1);
        else
            printf("      ");
        
        printf(" [%03X..%03X]", handlerTab->ebdHndBeg->bbCodeOffs,
                                ebdHndEndOffs(handlerTab) - 1);
        
        if (handlerTab->ebdFlags & CORINFO_EH_CLAUSE_FILTER)
        {
            printf(", Filter at BB%02u [%03X]",
                   handlerTab->ebdFilter->bbNum,
                   handlerTab->ebdFilter->bbCodeOffs);
        }

        printf("\n");
    }
}
#endif

/*****************************************************************************/

void                Compiler::fgDispBBLiveness()
{
    for (BasicBlock * block = fgFirstBB; block; block = block->bbNext)
    {
        VARSET_TP allVars = (block->bbLiveIn | block->bbLiveOut);
        printf("BB%02u",       block->bbNum);
        printf(      " IN ="); lvaDispVarSet(block->bbLiveIn , allVars);
        printf("\n     OUT="); lvaDispVarSet(block->bbLiveOut, allVars);
        printf("\n\n");
    }
}

/*****************************************************************************/

void                Compiler::fgDispBasicBlock(BasicBlock * block, bool dumpTrees)
{
    unsigned        flags = block->bbFlags;

    printf("BB%02u [%08X] %2u ", block->bbNum,
                                 block,
                                 block->bbRefs);

    fgDispPreds(block);

    printf("%4d", (block->bbWeight / BB_UNITY_WEIGHT) );

    if (block->bbWeight % BB_UNITY_WEIGHT)
        printf(".5 ");
    else
        printf("   ");

    if  (flags & BBF_INTERNAL)
    {
        printf("[internal] ");
    }
    else
    {
        unsigned offBeg = block->bbCodeOffs;
        unsigned offEnd = block->bbCodeOffs + block->bbCodeSize;
        if (block->bbCodeSize > 1)
            offEnd--;
        printf("[%03X..%03X] ", offBeg, offEnd);
    }

    if  (flags & BBF_REMOVED)
    {
        printf(  "[removed]       ");
    }
    else
    {
        switch (block->bbJumpKind)
        {
        case BBJ_COND:
            printf("-> BB%02u ( cond )", block->bbJumpDest->bbNum);
            break;

        case BBJ_CALL:
            printf("-> BB%02u ( call )", block->bbJumpDest->bbNum);
            break;

        case BBJ_ALWAYS:
            printf("-> BB%02u (always)", block->bbJumpDest->bbNum);
            break;

        case BBJ_LEAVE:
            printf("-> BB%02u (leave )", block->bbJumpDest->bbNum);
            break;

        case BBJ_RET:
            printf(  "        ( hret )");
            break;

        case BBJ_THROW:
            printf(  "        (throw )");
            break;

        case BBJ_RETURN:
            printf(  "        (return)");
            break;

        default:
            printf(  "                ");
            break;

        case BBJ_SWITCH:
            printf("->");

            unsigned        jumpCnt;
                            jumpCnt = block->bbJumpSwt->bbsCount;
            BasicBlock * *  jumpTab;
                            jumpTab = block->bbJumpSwt->bbsDstTab;
            do
            {
                printf("%cBB%02u", 
                       (jumpTab == block->bbJumpSwt->bbsDstTab) ? ' ' : ',',
                       (*jumpTab)->bbNum);
            }
            while (++jumpTab, --jumpCnt);

            printf(" (switch)");
            break;
        }
    }

    if (flags & BBF_HAS_HANDLER)
        printf("%d%c", block->getTryIndex(),
                       (flags & BBF_TRY_BEG)     ? 'b' : ' ');
    else
        printf(" %c",  (flags & BBF_TRY_HND_END) ? 'e' : ' ');

    
    switch(block->bbCatchTyp)
    {
    case BBCT_FAULT          : printf("%c", 'f'); break;
    case BBCT_FINALLY        : printf("%c", 'F'); break;
    case BBCT_FILTER         : printf("%c", 'q'); break;
    case BBCT_FILTER_HANDLER : printf("%c", 'Q'); break;
    default                  : printf("%c", 'X'); break;
    case 0: 
        printf("%c", ((flags & BBF_LOOP_HEAD)  ? 'L' :
                      (flags & BBF_RUN_RARELY) ? 'R' : ' '));
        break;
    }

    printf("%c", (block->bbJumpKind == BBJ_CALL) 
                  && (flags & BBF_RETLESS_CALL ) ? 'r' : ' ');
    
    printf("\n");

    if  (dumpTrees)
    {
        GenTreePtr      tree = block->bbTreeList;

        if  (tree)
        {
            printf("\n");

            do
            {
                assert(tree->gtOper == GT_STMT);

                gtDispTree(tree);
                printf("\n");

                tree = tree->gtNext;
            }
            while (tree);
        }
    }
}


/*****************************************************************************/

void                Compiler::fgDispBasicBlocks(bool dumpTrees)
{
    BasicBlock  *   block;

    printf("\n");
    printf("-----------------------------------------------------------------\n");
    printf("BBnum descAddr refs preds              weight  [IL range]  [jump]\n");
    printf("-----------------------------------------------------------------\n");

    for (block = fgFirstBB;
         block;
         block = block->bbNext)
    {
        fgDispBasicBlock(block, dumpTrees);

        if  (dumpTrees && block->bbNext)
            printf("- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n\n");
    }

    printf("------------------------------------------------------------------\n");
}


/*****************************************************************************
 *
 * A DEBUG routine to check the consistency of the flowgraph,
 * i.e. bbNums, bbRefs, bbPreds have to be up to date
 *
 * @TODO [CONSIDER] [07/03/01] []: should add consistency checks of the exception
 *     table (each block in try region marked with the correct handler, and no
 *     blocks outside region marked with that handler).
 *
 *****************************************************************************/

void                Compiler::fgDebugCheckBBlist()
{
    BasicBlock   *  block;
    BasicBlock   *  blockPred;
    flowList     *  pred;

    unsigned        blockRefs;

    /* Check bbNums, bbRefs and bbPreds */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        blockRefs = 0;

        /* First basic block has bbRefs >= 1 */

        if  (block == fgFirstBB)
        {
            assert(block->bbRefs >= 1);
            blockRefs = 1;
        }

        // If the block is a BBJ_COND or BBJ_SWITCH then
        // make sure it ends with a GT_JTRUE or a GT_SWITCH

        GenTreePtr stmt;
        if (block->bbJumpKind == BBJ_COND)
        {
            stmt = block->bbTreeList->gtPrev;
            assert(stmt->gtNext == NULL && 
                   stmt->gtStmt.gtStmtExpr->gtOper == GT_JTRUE);
        }
        else if (block->bbJumpKind == BBJ_SWITCH)
        {
            stmt = block->bbTreeList->gtPrev;
            assert(stmt->gtNext == NULL && 
                   stmt->gtStmt.gtStmtExpr->gtOper == GT_SWITCH);
        }

        for (pred = block->bbPreds; pred; pred = pred->flNext, blockRefs++)
        {
            /*  make sure this pred is part of the BB list */
            for (blockPred = fgFirstBB; blockPred; blockPred = blockPred->bbNext)
            {
                if (blockPred == pred->flBlock)
                    break;
            }

            assert(blockPred && "Predecessor is not part of BB list!");

            switch (blockPred->bbJumpKind)
            {
            case BBJ_COND:
                assert(blockPred->bbNext == block || blockPred->bbJumpDest == block);
                break;

            case BBJ_NONE:
                assert(blockPred->bbNext == block);
                break;

            case BBJ_CALL:
            case BBJ_ALWAYS:
                assert(blockPred->bbJumpDest == block);
                break;

            case BBJ_RET:

                if (blockPred->bbFlags & BBF_ENDFILTER)
                {
                    assert(blockPred->bbJumpDest == block);
                }
                else
                {
                    unsigned hndIndex = blockPred->getHndIndex();
                    EHblkDsc * ehDsc = compHndBBtab + hndIndex;
                    BasicBlock * tryBeg = ehDsc->ebdTryBeg;
                    BasicBlock * tryEnd = ehDsc->ebdTryEnd;
                    BasicBlock * finBeg = ehDsc->ebdHndBeg;

                    for(BasicBlock * bcall = tryBeg; bcall != tryEnd; bcall = bcall->bbNext)
                    {
                        if  (bcall->bbJumpKind != BBJ_CALL || bcall->bbJumpDest !=  finBeg)
                            continue;

                        if  (block == bcall->bbNext)
                            goto PRED_OK;
                    }
                    assert(!"BBJ_RET predecessor of block that doesn't follow a BBJ_CALL!");
                }
                break;

            case BBJ_THROW:
            case BBJ_RETURN:
                assert(!"THROW and RETURN block cannot be in the predecessor list!");
                break;

            case BBJ_SWITCH:
                unsigned        jumpCnt = blockPred->bbJumpSwt->bbsCount;
                BasicBlock * *  jumpTab = blockPred->bbJumpSwt->bbsDstTab;

                do
                {
                    if  (block == *jumpTab)
                    goto PRED_OK;
                }
                while (++jumpTab, --jumpCnt);

                assert(!"SWITCH in the predecessor list with no jump label to BLOCK!");
                break;
            }
        PRED_OK:;
        }

        /* Check the bbRefs */
        assert(block->bbRefs == blockRefs);

        /* Check if BBF_HAS_HANDLER is set that we have set bbTryIndex */
        if (block->bbFlags & BBF_HAS_HANDLER)
        {
            assert(block->bbTryIndex);
            assert(block->bbTryIndex <= info.compXcptnsCount);
        }
        else
        {
            assert(block->bbTryIndex == 0);
        }

        /* Check if BBF_RUN_RARELY is set that we have bbWeight of zero */
        if (block->isRunRarely())
        {
            assert(block->bbWeight == 0);
        }
        else
        {
            assert(block->bbWeight > 0);
        }
    }
}

/*****************************************************************************
 *
 * A DEBUG routine to check the that the exception flags are correctly set.
 *
 ****************************************************************************/

void                Compiler::fgDebugCheckFlags(GenTreePtr tree)
{
    assert(tree->gtOper != GT_STMT);

    genTreeOps      oper        = tree->OperGet();
    unsigned        kind        = tree->OperKind();
    unsigned        treeFlags   = tree->gtFlags & GTF_GLOB_EFFECT;
    unsigned        chkFlags    = 0;

    /* Is this a leaf node? */

    if  (kind & GTK_LEAF)
    {
        switch(oper)
        {
        case GT_CLS_VAR:
            chkFlags |= GTF_GLOB_REF;
            break;

        case GT_LCL_VAR:
        case GT_LCL_FLD:
            if (lvaTable[tree->gtLclVar.gtLclNum].lvAddrTaken)
                chkFlags |= GTF_GLOB_REF;
            break;

        case GT_CATCH_ARG:
        case GT_BB_QMARK:
            chkFlags |= GTF_OTHER_SIDEEFF;
            break;
        }
    }

    /* Is it a 'simple' unary/binary operator? */

    else if  (kind & GTK_SMPOP)
    {
        GenTreePtr      op1 = tree->gtOp.gtOp1;
        GenTreePtr      op2 = tree->gtGetOp2();

        /* Recursively check the subtrees */

        if (op1) fgDebugCheckFlags(op1);
        if (op2) fgDebugCheckFlags(op2);

        if (op1) chkFlags   |= (op1->gtFlags & GTF_GLOB_EFFECT);
        if (op2) chkFlags   |= (op2->gtFlags & GTF_GLOB_EFFECT);

        if (tree->gtFlags & GTF_REVERSE_OPS)
        {
            /* Must have two operands if GTF_REVERSE is set */
            assert(op1 && op2);

            /* with loose exceptions it is OK for both op1 and op2 to have side effects */
            if (!info.compLooseExceptions)
            {
                /* Make sure that the order of side effects has not been swapped. */

                /* However CSE may introduce an assignment after the reverse flag 
                   was set and thus GTF_ASG cannot be considered here. */

                /* For a GT_ASG(GT_IND(x), y) we are interested in the side effects of x */
                GenTreePtr  op1p; 
                if ((kind & GTK_ASGOP) && (op1->gtOper == GT_IND))
                    op1p = op1->gtOp.gtOp1;
                else
                    op1p = op1;

                /* This isn't true any more with the sticky GTF_REVERSE */
                /*
                // if op1p has side effects, then op2 cannot have side effects
                if (op1p->gtFlags & (GTF_SIDE_EFFECT & ~GTF_ASG))
                {
                    if (op2->gtFlags & (GTF_SIDE_EFFECT & ~GTF_ASG))
                        gtDispTree(tree);
                    assert(!(op2->gtFlags & (GTF_SIDE_EFFECT & ~GTF_ASG)));
                }
                */
            }
        }

        if (kind & GTK_ASGOP)
            chkFlags        |= GTF_ASG;

        /* Note that it is OK for treeFlags to to have a GTF_EXCEPT,
           AssertionProp's non-Null may have cleared it */
        if (tree->OperMayThrow())
            chkFlags        |= (treeFlags & GTF_EXCEPT);

        if ((oper == GT_ADDR) &&
            (op1->gtOper == GT_LCL_VAR || op1->gtOper == GT_CLS_VAR))
        {
            /* &aliasedVar doesnt need GTF_GLOB_REF, though alisasedVar does.
               Similarly for clsVar */
            treeFlags |= GTF_GLOB_REF;
        }
    }

    /* See what kind of a special operator we have here */

    else switch  (tree->OperGet())
    {
    case GT_CALL:

        GenTreePtr      args;
        GenTreePtr      argx;

        chkFlags |= GTF_CALL;

        if ((treeFlags & GTF_EXCEPT) && !(chkFlags & GTF_EXCEPT))
        {
            switch(eeGetHelperNum(tree->gtCall.gtCallMethHnd))
            {
                // Is this a helper call that can throw an exception ?
            case CORINFO_HELP_LDIV:
            case CORINFO_HELP_LMOD:
                chkFlags |= GTF_EXCEPT;
            }
        }

        if (tree->gtCall.gtCallObjp)
        {
            fgDebugCheckFlags(tree->gtCall.gtCallObjp);
            chkFlags |= (tree->gtCall.gtCallObjp->gtFlags & GTF_SIDE_EFFECT);

            /* @TODO [REVISIT] [04/16/01] []: If we spill to a regArg, we may have 
               an assignment. We currently dont propagate this up correctly. So ignore it */

            if (tree->gtCall.gtCallObjp->gtFlags & GTF_ASG)
                treeFlags |= GTF_ASG;
        }

        for (args = tree->gtCall.gtCallArgs; args; args = args->gtOp.gtOp2)
        {
            argx = args->gtOp.gtOp1;
            fgDebugCheckFlags(argx);

            chkFlags |= (argx->gtFlags & GTF_SIDE_EFFECT);

            /* @TODO [REVISIT] [04/16/01] []: If we spill to a regArg, we may have an assignment.
               We currently dont propagate this up correctly. So ignore it */

            if (argx->gtFlags & GTF_ASG)
                treeFlags |= GTF_ASG;
        }

        for (args = tree->gtCall.gtCallRegArgs; args; args = args->gtOp.gtOp2)
        {
            argx = args->gtOp.gtOp1;
            fgDebugCheckFlags(argx);

            chkFlags |= (argx->gtFlags & GTF_SIDE_EFFECT);

            /* @TODO [REVISIT] [04/16/01] []: If we spill to a regArg, we may have an assignment.
               We currently dont propagate this up correctly. So ignore it */

            if (argx->gtFlags & GTF_ASG)
                treeFlags |= GTF_ASG;
        }

        if (tree->gtCall.gtCallCookie)
        {
            fgDebugCheckFlags(tree->gtCall.gtCallCookie);
            chkFlags |= (tree->gtCall.gtCallCookie->gtFlags & GTF_SIDE_EFFECT);
        }

        if (tree->gtCall.gtCallType == CT_INDIRECT)
        {
            fgDebugCheckFlags(tree->gtCall.gtCallAddr);
            chkFlags |= (tree->gtCall.gtCallAddr->gtFlags & GTF_SIDE_EFFECT);
        }
    }

    if (chkFlags & ~treeFlags)
    {
        gtDispTree(tree);

        printf("Missing flags : ");
        GenTree::gtDispFlags(chkFlags & ~treeFlags);
        printf("\n");

        assert(!"Missing flags on tree");
    }
    else if (treeFlags & ~chkFlags)
    {
        /* The tree has extra flags set. However, this will happen if we
           replace a subtree with something, but dont clear the flags up
           the tree. Cant flag this unless we start clearing flags above */
    }
}

/*****************************************************************************/

const SANITY_DEBUG_CHECKS = 0;

/*****************************************************************************
 *
 * A DEBUG routine to check the correctness of the links between GT_STMT nodes
 * and ordinary nodes within a statement.
 *
 ****************************************************************************/

void                Compiler::fgDebugCheckLinks()
{
    // This is quite an expensive operation, so it is not always enabled.
    // Set SANITY_DEBUG_CHECKS to 1 to enable the check

    if (SANITY_DEBUG_CHECKS == 0 &&
        !compStressCompile(STRESS_CHK_FLOW_UPDATE, 30))
        return;

    /* For each basic block check the bbTreeList links */

    for (BasicBlock * block = fgFirstBB; block; block = block->bbNext)
    {
        for (GenTreePtr stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);
            assert(stmt->gtPrev);

            /* Verify that bbTreeList is threaded correctly */

            if  (stmt == block->bbTreeList)
                assert(stmt->gtPrev->gtNext == 0);
            else
                assert(stmt->gtPrev->gtNext == stmt);

            if  (stmt->gtNext)
                assert(stmt->gtNext->gtPrev == stmt);
            else
                assert(block->bbTreeList->gtPrev == stmt);

            /* For each statement check that the exception flags are properly set */

            assert(stmt->gtStmt.gtStmtExpr);

#ifdef  DEBUG
            if (verbose && 0)
                gtDispTree(stmt->gtStmt.gtStmtExpr);
#endif

            fgDebugCheckFlags(stmt->gtStmt.gtStmtExpr);

            /* GTF_OTHER_SIDEEFF have to be specially handled */

            if (stmt->gtFlags & GTF_OTHER_SIDEEFF)
            {
                // GTF_OTHER_SIDEEFF have to be the first thing evaluated
                assert(stmt == block->bbTreeList);
                assert(stmt->gtStmt.gtStmtList->gtOper == GT_CATCH_ARG ||
                       stmt->gtStmt.gtStmtList->gtOper == GT_BB_QMARK);
            }

            /* For each GT_STMT node check that the nodes are threaded correcly - gtStmtList */

            if  (!fgStmtListThreaded)
                continue;

            assert(stmt->gtStmt.gtStmtList);

            for (GenTreePtr tree  = stmt->gtStmt.gtStmtList; 
                            tree != NULL;
                            tree  = tree->gtNext)
            {
                if  (tree->gtPrev)
                    assert(tree->gtPrev->gtNext == tree);
                else
                    assert(tree == stmt->gtStmt.gtStmtList);

                if  (tree->gtNext)
                    assert(tree->gtNext->gtPrev == tree);
                else
                    assert(tree == stmt->gtStmt.gtStmtExpr);

                /* Cross-check gtPrev,gtNext with gtOp for simple trees */

                if (tree->OperIsUnary() && tree->gtOp.gtOp1)
                {
                    if ((tree->gtOper == GT_IND) &&
                        (tree->gtFlags & GTF_IND_RNGCHK) && tree->gtInd.gtIndLen)
                        assert(tree->gtPrev == tree->gtInd.gtIndLen);
                    else
                        assert(tree->gtPrev == tree->gtOp.gtOp1);
                }
                else if (tree->OperIsBinary() && tree->gtOp.gtOp1)
                {
                    switch(tree->gtOper)
                    {
                    case GT_QMARK:
                        assert(tree->gtPrev == tree->gtOp.gtOp2->gtOp.gtOp2); // second operand of the GT_COLON
                        break;

                    case GT_COLON:
                        assert(tree->gtPrev == tree->gtOp.gtOp1); // First conditional result
                        break;

                    default:
                        if (tree->gtOp.gtOp2)
                        {
                            if (tree->gtFlags & GTF_REVERSE_OPS)
                                assert(tree->gtPrev == tree->gtOp.gtOp1);
                            else
                                assert(tree->gtPrev == tree->gtOp.gtOp2);
                        }
                        else
                        {
                            assert(tree->gtPrev == tree->gtOp.gtOp1);
                        }
                        break;
                    }
                }

            }
        }
    }

}

/*****************************************************************************/
#endif // DEBUG
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\gentree.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                               GenTree                                     XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

#include "jitpch.h"
#pragma hdrstop

/*****************************************************************************/

const
unsigned char       GenTree::gtOperKindTable[] =
{
    #define GTNODE(en,sn,cm,ok) ok + GTK_COMMUTE*cm,
    #include "gtlist.h"
    #undef  GTNODE
};

/*****************************************************************************
 *
 *  The types of different GenTree nodes
 */

#ifdef DEBUG

#define INDENT_SIZE         3
//#define NUM_INDENTS       10

static void printIndent(int indent)
{
    char buf[33];

    while (indent > 32)
    {
        printIndent(32);
        indent -= 32;
    }

    memset(buf, ' ', indent);
    buf[indent]  = '\0';
    printf(buf);
}

static
const   char *      nodeNames[] =
{
    #define GTNODE(en,sn,cm,ok) sn,
    #include "gtlist.h"
};

const   char    *   GenTree::NodeName(genTreeOps op)
{
    assert(op < sizeof(nodeNames)/sizeof(nodeNames[0]));

    return  nodeNames[op];
}

#endif

/*****************************************************************************
 *
 *  When 'SMALL_TREE_NODES' is enabled, we allocate tree nodes in 2 different
 *  sizes: 'GTF_NODE_SMALL' for most nodes and 'GTF_NODE_LARGE' for the few
 *  nodes (such as calls and statement list nodes) that have more fields and
 *  take up a lot more space.
 */

#if SMALL_TREE_NODES

/* GT_COUNT'th oper is overloaded as 'undefined oper', so allocate storage for GT_COUNT'th oper also */
/* static */
unsigned char       GenTree::s_gtNodeSizes[GT_COUNT+1];


/* static */
void                GenTree::InitNodeSize()
{
    /* 'GT_LCL_VAR' often gets changed to 'GT_REG_VAR' */

    assert(GenTree::s_gtNodeSizes[GT_LCL_VAR] >= GenTree::s_gtNodeSizes[GT_REG_VAR]);

    /* Set all sizes to 'small' first */

    for (unsigned op = 0; op <= GT_COUNT; op++)
        GenTree::s_gtNodeSizes[op] = TREE_NODE_SZ_SMALL;

    /* Now set all of the appropriate entries to 'large' */

    assert(TREE_NODE_SZ_LARGE == TREE_NODE_SZ_SMALL +
                                 sizeof(((GenTree*)0)->gtLargeOp) -
                                 sizeof(((GenTree*)0)->gtOp));
    assert(sizeof(((GenTree*)0)->gtLargeOp) == sizeof(((GenTree*)0)->gtCall));

    GenTree::s_gtNodeSizes[GT_CALL      ] = TREE_NODE_SZ_LARGE;

    GenTree::s_gtNodeSizes[GT_INDEX     ] = TREE_NODE_SZ_LARGE;

    GenTree::s_gtNodeSizes[GT_IND       ] = TREE_NODE_SZ_LARGE;

    GenTree::s_gtNodeSizes[GT_ARR_ELEM  ] = TREE_NODE_SZ_LARGE;

#if INLINE_MATH
    GenTree::s_gtNodeSizes[GT_MATH      ] = TREE_NODE_SZ_LARGE;
#endif

#if INLINING
    GenTree::s_gtNodeSizes[GT_FIELD     ] = TREE_NODE_SZ_LARGE;
#endif

#ifdef DEBUG
    /* GT_STMT is large in DEBUG */
    GenTree::s_gtNodeSizes[GT_STMT      ] = TREE_NODE_SZ_LARGE;
#endif
}

#ifdef DEBUG
bool                GenTree::IsNodeProperlySized()
{
    size_t          size;

    if      (gtFlags & GTF_NODE_SMALL) 
    {
            // GT_IND are allowed to be small if they don't have a range check
        if (gtOper == GT_IND && !(gtFlags & GTF_IND_RNGCHK))
            return true;

        size = TREE_NODE_SZ_SMALL;
    }
    else  
    {
        assert (gtFlags & GTF_NODE_LARGE);
        size = TREE_NODE_SZ_LARGE;
    }

    return GenTree::s_gtNodeSizes[gtOper] <= size;
}
#endif

#else // SMALL_TREE_NODES

#ifdef DEBUG
bool                GenTree::IsNodeProperlySized()
{
    return  true;
}
#endif

#endif // SMALL_TREE_NODES

/*****************************************************************************/

Compiler::fgWalkResult      Compiler::fgWalkTreePreRec(GenTreePtr tree)
{
    fgWalkResult    result;

    genTreeOps      oper;
    unsigned        kind;

AGAIN:

    assert(tree);
    assert(tree->gtOper != GT_STMT);

    /* Visit this node */

    if  (!fgWalkPre.wtprLclsOnly)
    {
        result = fgWalkPre.wtprVisitorFn(tree, fgWalkPre.wtprCallbackData);
        if  (result != WALK_CONTINUE)
            return result;
    }

    /* Figure out what kind of a node we have */

    oper = tree->OperGet();
    kind = tree->OperKind();

    /* Is this a constant or leaf node? */

    if  (kind & (GTK_CONST|GTK_LEAF))
    {
      if  (fgWalkPre.wtprLclsOnly && (oper == GT_LCL_VAR || oper == GT_LCL_FLD))
            return fgWalkPre.wtprVisitorFn(tree, fgWalkPre.wtprCallbackData);

        return  WALK_CONTINUE;
    }

    /* Is it a 'simple' unary/binary operator? */

    if  (kind & GTK_SMPOP)
    {
        if  (tree->gtGetOp2())
        {
            result = fgWalkTreePreRec(tree->gtOp.gtOp1);
            if  (result == WALK_ABORT)
                return result;

            tree = tree->gtOp.gtOp2;
            goto AGAIN;
        }
        else
        {

#if CSELENGTH

            /* Some GT_IND have "secret" array length subtrees */

            if  ((tree->gtFlags & GTF_IND_RNGCHK) != 0       &&
                 (tree->gtOper                    == GT_IND) &&
                 (tree->gtInd.gtIndLen            != NULL))
            {
                result = fgWalkTreePreRec(tree->gtInd.gtIndLen);
                if  (result == WALK_ABORT)
                    return result;
            }

#endif

            tree = tree->gtOp.gtOp1;
            if  (tree)
                goto AGAIN;

            return WALK_CONTINUE;
        }
    }

    /* See what kind of a special operator we have here */

    switch  (oper)
    {
    case GT_FIELD:
        tree = tree->gtField.gtFldObj;
        break;

    case GT_CALL:

        if  (fgWalkPre.wtprSkipCalls)
            return WALK_CONTINUE;

        assert(tree->gtFlags & GTF_CALL);

#if INLINE_NDIRECT
        /* Is this a call to unmanaged code ? */
        if  (fgWalkPre.wtprLclsOnly && (tree->gtFlags & GTF_CALL_UNMANAGED))
        {
            result = fgWalkPre.wtprVisitorFn(tree, fgWalkPre.wtprCallbackData);
            if  (result == WALK_ABORT)
                return result;
        }
#endif
        if  (tree->gtCall.gtCallObjp)
        {
            result = fgWalkTreePreRec(tree->gtCall.gtCallObjp);
            if  (result == WALK_ABORT)
                return result;
        }

        if  (tree->gtCall.gtCallArgs)
        {
            result = fgWalkTreePreRec(tree->gtCall.gtCallArgs);
            if  (result == WALK_ABORT)
                return result;
        }

        if  (tree->gtCall.gtCallRegArgs)
        {
            result = fgWalkTreePreRec(tree->gtCall.gtCallRegArgs);
            if  (result == WALK_ABORT)
                return result;
        }

        if  (tree->gtCall.gtCallCookie)
        {
            result = fgWalkTreePreRec(tree->gtCall.gtCallCookie);
            if  (result == WALK_ABORT)
                return result;
        }

        if (tree->gtCall.gtCallType == CT_INDIRECT)
            tree = tree->gtCall.gtCallAddr;
        else
            tree = NULL;

        break;

#if CSELENGTH

    case GT_ARR_LENREF:

        if  (tree->gtArrLen.gtArrLenCse)
        {
            result = fgWalkTreePreRec(tree->gtArrLen.gtArrLenCse);
            if  (result == WALK_ABORT)
                return result;
        }

        /* If it is hanging as a gtInd.gtIndLen of a GT_IND, then
           gtArrLen.gtArrLenAdr points the array-address of the GT_IND.
           But we dont want to follow that link as the GT_IND also has
           a reference via gtIndOp1 */

        if  (!(tree->gtFlags & GTF_ALN_CSEVAL))
            return  WALK_CONTINUE;

        /* However, if this is a hoisted copy for CSE, then we should
           process gtArrLen.gtArrLenAdr */

        tree = tree->gtArrLen.gtArrLenAdr; assert(tree);
        break;

#endif

    case GT_ARR_ELEM:

        result = fgWalkTreePreRec(tree->gtArrElem.gtArrObj);
        if  (result == WALK_ABORT)
            return result;

        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
        {
            result = fgWalkTreePreRec(tree->gtArrElem.gtArrInds[dim]);
            if  (result == WALK_ABORT)
                return result;
        }

        return WALK_CONTINUE;

    default:
#ifdef  DEBUG
        gtDispTree(tree);
#endif
        assert(!"unexpected operator");
    }

    if  (tree)
        goto AGAIN;

    return WALK_CONTINUE;
}

/*****************************************************************************
 *
 *  Walk all basic blocks and call the given function pointer for all tree
 *  nodes contained therein.
 */

void                    Compiler::fgWalkAllTreesPre(fgWalkPreFn * visitor,
                                                    void * pCallBackData)
{
    BasicBlock *    block;

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        GenTreePtr      tree;

        for (tree = block->bbTreeList; tree; tree = tree->gtNext)
        {
            assert(tree->gtOper == GT_STMT);

            fgWalkTreePre(tree->gtStmt.gtStmtExpr, visitor, pCallBackData);
        }
    }
}


/*****************************************************************************/

Compiler::fgWalkResult      Compiler::fgWalkTreePostRec(GenTreePtr tree)
{
    fgWalkResult    result;

    genTreeOps      oper;
    unsigned        kind;

    assert(tree);
    assert(tree->gtOper != GT_STMT);

    /* Figure out what kind of a node we have */

    oper = tree->OperGet();
    kind = tree->OperKind();

    /* Is this a prefix node? */

    if  (oper == fgWalkPost.wtpoPrefixNode)
    {
        result = fgWalkPost.wtpoVisitorFn(tree, fgWalkPost.wtpoCallbackData, true);
        if  (result == WALK_ABORT)
            return result;
    }

    /* Is this a constant or leaf node? */

    if  (kind & (GTK_CONST|GTK_LEAF))
        goto DONE;

    /* Is it a 'simple' unary/binary operator? */

    if  (kind & GTK_SMPOP)
    {
        if  (tree->gtOp.gtOp1)
        {
            result = fgWalkTreePostRec(tree->gtOp.gtOp1);
            if  (result == WALK_ABORT)
                return result;
        }

        if  (tree->gtGetOp2())
        {
            result = fgWalkTreePostRec(tree->gtOp.gtOp2);
            if  (result == WALK_ABORT)
                return result;
        }

#if CSELENGTH

        /* Some GT_IND have "secret" array length subtrees */

        if  ((tree->gtFlags & GTF_IND_RNGCHK) != 0       &&
             (tree->gtOper                    == GT_IND) &&
             (tree->gtInd.gtIndLen            != NULL))
        {
            result = fgWalkTreePostRec(tree->gtInd.gtIndLen);
            if  (result == WALK_ABORT)
                return result;
        }

#endif

        goto DONE;
    }

    /* See what kind of a special operator we have here */

    switch  (oper)
    {
    case GT_FIELD:
        if  (tree->gtField.gtFldObj)
        {
            result = fgWalkTreePostRec(tree->gtField.gtFldObj);
            if  (result == WALK_ABORT)
                return result;
        }

        break;

    case GT_CALL:

        assert(tree->gtFlags & GTF_CALL);

        if  (tree->gtCall.gtCallObjp)
        {
            result = fgWalkTreePostRec(tree->gtCall.gtCallObjp);
            if  (result == WALK_ABORT)
                return result;
        }

        if  (tree->gtCall.gtCallArgs)
        {
            result = fgWalkTreePostRec(tree->gtCall.gtCallArgs);
            if  (result == WALK_ABORT)
                return result;
        }

        if  (tree->gtCall.gtCallRegArgs)
        {
            result = fgWalkTreePostRec(tree->gtCall.gtCallRegArgs);
            if  (result == WALK_ABORT)
                return result;
        }

        if  (tree->gtCall.gtCallCookie)
        {
            result = fgWalkTreePostRec(tree->gtCall.gtCallCookie);
            if  (result == WALK_ABORT)
                return result;
        }

        if  (tree->gtCall.gtCallType == CT_INDIRECT)
        {
            result = fgWalkTreePostRec(tree->gtCall.gtCallAddr);
            if  (result == WALK_ABORT)
                return result;
        }

        break;

#if CSELENGTH

    case GT_ARR_LENREF:

        if  (tree->gtArrLen.gtArrLenCse)
        {
            result = fgWalkTreePostRec(tree->gtArrLen.gtArrLenCse);
            if  (result == WALK_ABORT)
                return result;
        }

        /* If it is hanging as a gtInd.gtIndLen of a GT_IND, then
           gtArrLen.gtArrLenAdr points the array-address of the GT_IND.
           But we dont want to follow that link as the GT_IND also has
           a reference via gtIndOp1.
           However, if this is a hoisted copy for CSE, then we should
           process gtArrLen.gtArrLenAdr */

        if  (tree->gtFlags & GTF_ALN_CSEVAL)
        {
            assert(tree->gtArrLen.gtArrLenAdr);

            result = fgWalkTreePostRec(tree->gtArrLen.gtArrLenAdr);
            if  (result == WALK_ABORT)
                return result;
        }

        goto DONE;

#endif

        break;


    case GT_ARR_ELEM:

        result = fgWalkTreePostRec(tree->gtArrElem.gtArrObj);
        if  (result == WALK_ABORT)
            return result;

        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
        {
            result = fgWalkTreePostRec(tree->gtArrElem.gtArrInds[dim]);
            if  (result == WALK_ABORT)
                return result;
        }

        goto DONE;


    default:
#ifdef  DEBUG
        gtDispTree(tree);
#endif
        assert(!"unexpected operator");
    }

DONE:

    /* Finally, visit the current node */

    return  fgWalkPost.wtpoVisitorFn(tree, fgWalkPost.wtpoCallbackData, false);
}



/*****************************************************************************
 *
 *  Returns non-zero if the two trees are identical.
 */

bool                GenTree::Compare(GenTreePtr op1, GenTreePtr op2, bool swapOK)
{
    genTreeOps      oper;
    unsigned        kind;

//  printf("tree1:\n"); gtDispTree(op1);
//  printf("tree2:\n"); gtDispTree(op2);

AGAIN:

#if CSELENGTH
    if  (op1 == NULL) return (op2 == NULL);
    if  (op2 == NULL) return false;
#else
    assert(op1 && op2);
#endif
    if  (op1 == op2)  return true;

    assert(op1->gtOper != GT_STMT);
    assert(op2->gtOper != GT_STMT);

    oper = op1->OperGet();

    /* The operators must be equal */

    if  (oper != op2->gtOper)
        return false;

    /* The types must be equal */

    if  (op1->gtType != op2->gtType)
        return false;

    /* Overflow must be equal */
    if  (op1->gtOverflowEx() != op2->gtOverflowEx())
    {
        return false;
    }
        

    /* Sensible flags must be equal */
    if ( (op1->gtFlags & (GTF_UNSIGNED )) !=
         (op2->gtFlags & (GTF_UNSIGNED )) )
    {
        return false;
    }


    /* Figure out what kind of nodes we're comparing */

    kind = op1->OperKind();

    /* Is this a constant node? */

    if  (kind & GTK_CONST)
    {
        switch (oper)
        {
        case GT_CNS_INT:

            if  (op1->gtIntCon.gtIconVal != op2->gtIntCon.gtIconVal)
                break;

            return true;


            // UNDONE [low pri]: match non-int constant values
        }

        return  false;
    }

    /* Is this a leaf node? */

    if  (kind & GTK_LEAF)
    {
        switch (oper)
        {
        case GT_LCL_VAR:
            if  (op1->gtLclVar.gtLclNum    != op2->gtLclVar.gtLclNum)
                break;

            return true;

        case GT_LCL_FLD:
            if  (op1->gtLclFld.gtLclNum    != op2->gtLclFld.gtLclNum ||
                 op1->gtLclFld.gtLclOffs   != op2->gtLclFld.gtLclOffs)
                break;

            return true;

        case GT_CLS_VAR:
            if  (op1->gtClsVar.gtClsVarHnd != op2->gtClsVar.gtClsVarHnd)
                break;

            return true;

        case GT_LABEL:
            return true;
        }

        return false;
    }

    /* Is it a 'simple' unary/binary operator? */

    if  (kind & GTK_UNOP)
    {
        if (!Compare(op1->gtOp.gtOp1, op2->gtOp.gtOp1) ||
            (op1->gtVal.gtVal2 != op2->gtVal.gtVal2))
        {
            return false;
        }

#if CSELENGTH

        /* Is either operand a GT_IND node with an array length? */

        if  (oper == GT_IND)
        {
            if  ((op1->gtFlags|op2->gtFlags) & GTF_IND_RNGCHK)
            {
                GenTreePtr  tmp1 = op1->gtInd.gtIndLen;
                GenTreePtr  tmp2 = op2->gtInd.gtIndLen;

                if  (!(op1->gtFlags & GTF_IND_RNGCHK)) tmp1 = NULL;
                if  (!(op2->gtFlags & GTF_IND_RNGCHK)) tmp2 = NULL;

                if  (tmp1)
                {
                    if  (!Compare(tmp1, tmp2, swapOK))
                        return  false;
                }
                else
                {
                    if  (tmp2)
                        return  false;
                }
            }
        }

        if (oper == GT_MATH)
        {
            if (op1->gtMath.gtMathFN != op2->gtMath.gtMathFN)
            {
                return false;
            }
        }

        // @TODO [FIXHACK] [04/16/01] [dnotario]: This is a sort of hack to disable CSEs for LOCALLOCS. 
        // setting the GTF_DONT_CSE flag isnt enough, as it doesnt get propagated up the tree, but
        // obviously, we dont want meaningful locallocs to be CSE'd
        if (oper == GT_LCLHEAP)
        {
            return false;
        }

#endif
        return true;
    }

    if  (kind & GTK_BINOP)
    {
        if  (op1->gtOp.gtOp2)
        {
            if  (!Compare(op1->gtOp.gtOp1, op2->gtOp.gtOp1, swapOK))
            {
                if  (swapOK)
                {
                    /* Special case: "lcl1 + lcl2" matches "lcl2 + lcl1" */

                    // @TODO [CONSIDER] [04/16/01] []: This can be enhanced...

                    if  (oper == GT_ADD && op1->gtOp.gtOp1->gtOper == GT_LCL_VAR
                                        && op1->gtOp.gtOp2->gtOper == GT_LCL_VAR)
                    {
                        if  (Compare(op1->gtOp.gtOp1, op2->gtOp.gtOp2, swapOK) &&
                             Compare(op1->gtOp.gtOp2, op2->gtOp.gtOp1, swapOK))
                        {
                            return  true;
                        }
                    }
                }

                return false;
            }

            op1 = op1->gtOp.gtOp2;
            op2 = op2->gtOp.gtOp2;

            goto AGAIN;
        }
        else
        {

            op1 = op1->gtOp.gtOp1;
            op2 = op2->gtOp.gtOp1;

            if  (!op1) return  (op2 == 0);
            if  (!op2) return  false;

            goto AGAIN;
        }
    }

    /* See what kind of a special operator we have here */

    switch  (oper)
    {
    case GT_FIELD:
        if  (op1->gtField.gtFldHnd != op2->gtField.gtFldHnd)
            break;

        op1 = op1->gtField.gtFldObj;
        op2 = op2->gtField.gtFldObj;

        if  (op1 || op2)
        {
            if  (op1 && op2)
                goto AGAIN;
        }

        return true;

    case GT_CALL:

        if (       (op1->gtCall.gtCallType  == op2->gtCall.gtCallType)     &&
            Compare(op1->gtCall.gtCallRegArgs, op2->gtCall.gtCallRegArgs)  &&
            Compare(op1->gtCall.gtCallArgs,    op2->gtCall.gtCallArgs)     &&
            Compare(op1->gtCall.gtCallObjp,    op2->gtCall.gtCallObjp)     &&
            ((op1->gtCall.gtCallType != CT_INDIRECT) ||
             (Compare(op1->gtCall.gtCallAddr,    op2->gtCall.gtCallAddr)))        )
            return true;  
        break;

#if CSELENGTH

    case GT_ARR_LENREF:

        if (op1->gtArrLenOffset() != op2->gtArrLenOffset())
            return false;

        if  (!Compare(op1->gtArrLen.gtArrLenAdr, op2->gtArrLen.gtArrLenAdr, swapOK))
            return  false;

        op1 = op1->gtArrLen.gtArrLenCse;
        op2 = op2->gtArrLen.gtArrLenCse;

        goto AGAIN;

#endif

    case GT_ARR_ELEM:

        if (op1->gtArrElem.gtArrRank != op2->gtArrElem.gtArrRank)
            return false;

        // NOTE: gtArrElemSize may need to be handled

        unsigned dim;
        for(dim = 0; dim < op1->gtArrElem.gtArrRank; dim++)
        {
            if (!Compare(op1->gtArrElem.gtArrInds[dim], op2->gtArrElem.gtArrInds[dim]))
                return false;
        }

        op1 = op1->gtArrElem.gtArrObj;
        op2 = op2->gtArrElem.gtArrObj;
        goto AGAIN;

    default:
        assert(!"unexpected operator");
    }

    return false;
}

/*****************************************************************************
 *
 *  Returns non-zero if the given tree contains a use of a local #lclNum.
 */

 // @TODO [REVISIT] [04/16/01] []: make this work with byrefs.  In particular, calls with byref
 // parameters should be counted as a def.

bool                Compiler::gtHasRef(GenTreePtr tree, int lclNum, bool defOnly)
{
    genTreeOps      oper;
    unsigned        kind;

AGAIN:

    assert(tree);

    oper = tree->OperGet();
    kind = tree->OperKind();

    assert(oper != GT_STMT);

    /* Is this a constant node? */

    if  (kind & GTK_CONST)
        return  false;

    /* Is this a leaf node? */

    if  (kind & GTK_LEAF)
    {
        if  (oper == GT_LCL_VAR)
        {
            if  (tree->gtLclVar.gtLclNum == (unsigned)lclNum)
            {
                if  (!defOnly)
                    return true;
            }
        }

        return false;
    }

    /* Is it a 'simple' unary/binary operator? */

    if  (kind & GTK_SMPOP)
    {
        if  (tree->gtGetOp2())
        {
            if  (gtHasRef(tree->gtOp.gtOp1, lclNum, defOnly))
                return true;

            tree = tree->gtOp.gtOp2;
            goto AGAIN;
        }
        else
        {
            tree = tree->gtOp.gtOp1;

            if  (!tree)
                return  false;

            if  (kind & GTK_ASGOP)
            {
                // 'tree' is the gtOp1 of an assignment node. So we can handle
                // the case where defOnly is either true or false.

                if  (tree->gtOper == GT_LCL_VAR &&
                     tree->gtLclVar.gtLclNum == (unsigned)lclNum)
                {
                    return true;
                }
                else if (tree->gtOper == GT_FIELD &&
                         lclNum == (int)tree->gtField.gtFldHnd)
                {
                     return true;
                }
            }

            goto AGAIN;
        }
    }

    /* See what kind of a special operator we have here */

    switch  (oper)
    {
    case GT_FIELD:
        if  (lclNum == (int)tree->gtField.gtFldHnd)
        {
            if  (!defOnly)
                return true;
        }

        tree = tree->gtField.gtFldObj;
        if  (tree)
            goto AGAIN;
        break;

    case GT_CALL:

        if  (tree->gtCall.gtCallObjp)
            if  (gtHasRef(tree->gtCall.gtCallObjp, lclNum, defOnly))
                return true;

        if  (tree->gtCall.gtCallArgs)
            if  (gtHasRef(tree->gtCall.gtCallArgs, lclNum, defOnly))
                return true;

        if  (tree->gtCall.gtCallRegArgs)
            if  (gtHasRef(tree->gtCall.gtCallRegArgs, lclNum, defOnly))
                return true;

        // pinvoke-calli cookie is a constant, or constant indirection
        assert(tree->gtCall.gtCallCookie == NULL ||
               tree->gtCall.gtCallCookie->gtOper == GT_CNS_INT ||
               tree->gtCall.gtCallCookie->gtOper == GT_IND);

        if  (tree->gtCall.gtCallType == CT_INDIRECT)
            tree = tree->gtCall.gtCallAddr;
        else
            tree = NULL;

        if  (tree)
            goto AGAIN;

        break;

    case GT_ARR_ELEM:
        if (gtHasRef(tree->gtArrElem.gtArrObj, lclNum, defOnly))
            return true;

        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
        {
            if (gtHasRef(tree->gtArrElem.gtArrInds[dim], lclNum, defOnly))
                return true;
        }

        break;

    default:
#ifdef  DEBUG
        gtDispTree(tree);
#endif
        assert(!"unexpected operator");
    }

    return false;
}

/*****************************************************************************
 *
 *  Helper used to compute hash values for trees.
 */

inline
unsigned            genTreeHashAdd(unsigned old, unsigned add)
{
    return  (old + old/2) ^ add;
}

inline
unsigned            genTreeHashAdd(unsigned old, unsigned add1,
                                                 unsigned add2)
{
    return  (old + old/2) ^ add1 ^ add2;
}

/*****************************************************************************
 *
 *  Given an arbitrary expression tree, compute a hash value for it.
 */

unsigned            Compiler::gtHashValue(GenTree * tree)
{
    genTreeOps      oper;
    unsigned        kind;

    unsigned        hash = 0;

#if CSELENGTH
    GenTreePtr      temp;
#endif

AGAIN:

    assert(tree);
    assert(tree->gtOper != GT_STMT);

    /* Figure out what kind of a node we have */

    oper = tree->OperGet();
    kind = tree->OperKind();

#if CSELENGTH

    if  (oper == GT_ARR_LENGTH)
    {
        /* GT_ARR_LENGTH must hash to the same thing as GT_ARR_LENREF so
           that they can be CSEd together */

        hash = genTreeHashAdd(hash, GT_ARR_LENREF);
        temp = tree->gtOp.gtOp1;
        goto ARRLEN;
    }

#endif

    /* Include the operator value in the hash */

    hash = genTreeHashAdd(hash, oper);

    /* Is this a constant or leaf node? */

    if  (kind & (GTK_CONST|GTK_LEAF))
    {
        unsigned        add;

        switch (oper)
        {
        case GT_LCL_VAR: add = tree->gtLclVar.gtLclNum;       break;
        case GT_LCL_FLD: hash = genTreeHashAdd(hash, tree->gtLclFld.gtLclNum);
                         add = tree->gtLclFld.gtLclOffs;      break;

        case GT_CNS_INT: add = (int)tree->gtIntCon.gtIconVal; break;
        case GT_CNS_LNG: add = (int)tree->gtLngCon.gtLconVal; break;
        case GT_CNS_DBL: add = (int)tree->gtDblCon.gtDconVal; break;
        case GT_CNS_STR: add = (int)tree->gtStrCon.gtSconCPX; break;

        case GT_JMP:     add = tree->gtVal.gtVal1;            break;

        default:         add = 0;                             break;
        }

        hash = genTreeHashAdd(hash, add);
        goto DONE;
    }

    /* Is it a 'simple' unary/binary operator? */

    GenTreePtr      op1  = tree->gtOp.gtOp1;

    if (kind & GTK_UNOP)
    {
        hash = genTreeHashAdd(hash, tree->gtVal.gtVal2);

        /* Special case: no sub-operand at all */

        if  (!op1)
            goto DONE;

        tree = op1;
        goto AGAIN;
    }

    if  (kind & GTK_BINOP)
    {
        GenTreePtr      op2  = tree->gtOp.gtOp2;

        /* Is there a second sub-operand? */

        if  (!op2)
        {
            /* Special case: no sub-operands at all */

            if  (!op1)
                goto DONE;

            /* This is a unary operator */

            tree = op1;
            goto AGAIN;
        }

        /* This is a binary operator */

        unsigned        hsh1 = gtHashValue(op1);

        /* Special case: addition of two values */

        if  (oper == GT_ADD)
        {
            unsigned    hsh2 = gtHashValue(op2);

            /* Produce a hash that allows swapping the operands */

            hash = genTreeHashAdd(hash, hsh1, hsh2);
            goto DONE;
        }

        /* Add op1's hash to the running value and continue with op2 */

        hash = genTreeHashAdd(hash, hsh1);

        tree = op2;
        goto AGAIN;
    }

    /* See what kind of a special operator we have here */
    switch  (tree->gtOper)
    {
    case GT_FIELD:
        if (tree->gtField.gtFldObj)
        {
            temp = tree->gtField.gtFldObj; assert(temp);
            hash = genTreeHashAdd(hash, gtHashValue(temp));
        }
        break;

    case GT_STMT:
        temp = tree->gtStmt.gtStmtExpr; assert(temp);
        hash = genTreeHashAdd(hash, gtHashValue(temp));
        break;

#if CSELENGTH

    case GT_ARR_LENREF:
        temp = tree->gtArrLen.gtArrLenAdr;
        assert(temp && temp->gtType == TYP_REF);
ARRLEN:
        hash = genTreeHashAdd(hash, gtHashValue(temp));
        break;

#endif

    case GT_ARR_ELEM:

        hash = genTreeHashAdd(hash, gtHashValue(tree->gtArrElem.gtArrObj));

        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
            hash = genTreeHashAdd(hash, gtHashValue(tree->gtArrElem.gtArrInds[dim]));

        break;

    case GT_CALL:

      if  (tree->gtCall.gtCallObjp && tree->gtCall.gtCallObjp->gtOper != GT_NOP)
      {
          temp = tree->gtCall.gtCallObjp; assert(temp);
          hash = genTreeHashAdd(hash, gtHashValue(temp));
      }
      
      if (tree->gtCall.gtCallArgs)
      {
          temp = tree->gtCall.gtCallArgs; assert(temp);
          hash = genTreeHashAdd(hash, gtHashValue(temp));
      }
      
      if  (tree->gtCall.gtCallType == CT_INDIRECT) 
      {
          temp = tree->gtCall.gtCallAddr; assert(temp);
          hash = genTreeHashAdd(hash, gtHashValue(temp));
      }
      
      if (tree->gtCall.gtCallRegArgs) 
      {
          temp = tree->gtCall.gtCallRegArgs; assert(temp);
          hash = genTreeHashAdd(hash, gtHashValue(temp));
      }
      break;

    default:
#ifdef  DEBUG
        gtDispTree(tree);
#endif
        assert(!"unexpected operator");
        break;
    }

DONE:

    return hash;
}

/*****************************************************************************
 *
 *  Given an arbitrary expression tree, return the set of all local variables
 *  referenced by the tree. If the tree contains any references that are not
 *  local variables or constants, returns 'VARSET_NOT_ACCEPTABLE'. If there
 *  are any indirections or global refs in the expression, the "*refsPtr" argument
 *  will be assigned the appropriate bit set based on the 'varRefKinds' type.
 *  It won't be assigned anything when there are no indirections or global
 *  references, though, so this value should be initialized before the call.
 *  If we encounter an expression that is equal to *findPtr we set *findPtr
 *  to NULL.
 */

VARSET_TP           Compiler::lvaLclVarRefs(GenTreePtr  tree,
                                            GenTreePtr *findPtr,
                                            varRefKinds*refsPtr)
{
    genTreeOps      oper;
    unsigned        kind;
    varRefKinds     refs = VR_NONE;
    VARSET_TP       vars = 0;

AGAIN:

    assert(tree);
    assert(tree->gtOper != GT_STMT);

    /* Remember whether we've come across the expression we're looking for */

    if  (findPtr && *findPtr == tree) *findPtr = NULL;

    /* Figure out what kind of a node we have */

    oper = tree->OperGet();
    kind = tree->OperKind();

    /* Is this a constant or leaf node? */

    if  (kind & (GTK_CONST|GTK_LEAF))
    {
        if  (oper == GT_LCL_VAR)
        {
            unsigned    lclNum = tree->gtLclVar.gtLclNum;

            /* Should we use the variable table? */

            if  (findPtr)
            {
                if (lclNum >= VARSET_SZ)
                    return  VARSET_NOT_ACCEPTABLE;

                vars |= genVarIndexToBit(lclNum);
            }
            else
            {
                assert(lclNum < lvaCount);
                LclVarDsc * varDsc = lvaTable + lclNum;

                if (varDsc->lvTracked == false)
                    return  VARSET_NOT_ACCEPTABLE;

                /* Don't deal with expressions with volatile variables */

                if (varDsc->lvVolatile)
                    return  VARSET_NOT_ACCEPTABLE;

                vars |= genVarIndexToBit(varDsc->lvVarIndex);
            }
        }
        else if (oper == GT_LCL_FLD)
        {
            /* We cant track every field of every var. Moreover, indirections
               may access different parts of the var as different (but
               overlapping) fields. So just treat them as indirect accesses */

            unsigned    lclNum = tree->gtLclFld.gtLclNum;
            assert(lvaTable[lclNum].lvAddrTaken);

            if (varTypeIsGC(tree->TypeGet()))
                refs = VR_IND_PTR;
            else
                refs = VR_IND_SCL;
        }
        else if (oper == GT_CLS_VAR)
        {
            refs = VR_GLB_VAR;
        }

        if (refs != VR_NONE)
        {
            /* Write it back to callers parameter using an 'or' */
            *refsPtr = varRefKinds((*refsPtr) | refs);
        }
        return  vars;
    }

    /* Is it a 'simple' unary/binary operator? */

    if  (kind & GTK_SMPOP)
    {
        if  (oper == GT_IND)
        {
            assert(tree->gtOp.gtOp2 == 0);

            /* Set the proper indirection bit */

            if (tree->gtFlags & GTF_IND_INVARIANT)
                refs = VR_INVARIANT;
            else if (varTypeIsGC(tree->TypeGet()))
                refs = VR_IND_PTR;
            else
                refs = VR_IND_SCL;

            // If the flag GTF_IND_TGTANYWHERE is set this indirection
            // could also point at a global variable

            if (tree->gtFlags & GTF_IND_TGTANYWHERE)
            {
                refs = varRefKinds( ((int) refs) | ((int) VR_GLB_VAR) );
            }

            /* Write it back to callers parameter using an 'or' */
            *refsPtr = varRefKinds((*refsPtr) | refs);

            // For IL volatile memory accesses we mark the GT_IND node
            // with a GTF_DONT_CSE flag.
            //
            // This flag is also set for the left hand side of an assignment.
            //
            // If this flag is set then we return VARSET_NOT_ACCEPTABLE
            //
            if (tree->gtFlags & GTF_DONT_CSE)
            {
                return VARSET_NOT_ACCEPTABLE;
            }

#if CSELENGTH
            /* Some GT_IND have "secret" array length subtrees */

            if  ((tree->gtFlags & GTF_IND_RNGCHK) && tree->gtInd.gtIndLen)
            {
                vars |= lvaLclVarRefs(tree->gtInd.gtIndLen, findPtr, refsPtr);
                if  (vars == VARSET_NOT_ACCEPTABLE)
                    return VARSET_NOT_ACCEPTABLE;
            }
#endif
        }

        if  (tree->gtGetOp2())
        {
            /* It's a binary operator */

            vars |= lvaLclVarRefs(tree->gtOp.gtOp1, findPtr, refsPtr);
            if  (vars == VARSET_NOT_ACCEPTABLE)
                return VARSET_NOT_ACCEPTABLE;

            tree = tree->gtOp.gtOp2; assert(tree);
            goto AGAIN;
        }
        else
        {
            /* It's a unary (or nilary) operator */

            tree = tree->gtOp.gtOp1;
            if  (tree)
                goto AGAIN;

            return vars;
        }
    }

    switch(oper)
    {
#if CSELENGTH

    /* An array length value depends on the array address */

    case GT_ARR_LENREF:
        tree = tree->gtArrLen.gtArrLenAdr;
        goto AGAIN;
#endif

    case GT_ARR_ELEM:
        vars = lvaLclVarRefs(tree->gtArrElem.gtArrObj, findPtr, refsPtr);
        if  (vars == VARSET_NOT_ACCEPTABLE)
            return VARSET_NOT_ACCEPTABLE;

        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
        {
            vars |= lvaLclVarRefs(tree->gtArrElem.gtArrInds[dim], findPtr, refsPtr);
            if  (vars == VARSET_NOT_ACCEPTABLE)
                return VARSET_NOT_ACCEPTABLE;
        }
        return vars;

    case GT_CALL:
        /* Allow calls to the Shared Static helper */
        if ((tree->gtCall.gtCallType == CT_HELPER) &&
            (eeGetHelperNum(tree->gtCall.gtCallMethHnd) == CORINFO_HELP_GETSHAREDSTATICBASE))
        {
            *refsPtr = varRefKinds((*refsPtr) | VR_INVARIANT);
        return vars;
    }
        break;

    } // end switch(oper)

    return  VARSET_NOT_ACCEPTABLE;
}

/*****************************************************************************
 *
 *  Return a relational operator that is the reverse of the given one.
 */

/* static */
genTreeOps          GenTree::ReverseRelop(genTreeOps relop)
{
    static const
    unsigned char   reverseOps[] =
    {
        GT_NE,          // GT_EQ
        GT_EQ,          // GT_NE
        GT_GE,          // GT_LT
        GT_GT,          // GT_LE
        GT_LT,          // GT_GE
        GT_LE,          // GT_GT
    };

    assert(reverseOps[GT_EQ - GT_EQ] == GT_NE);
    assert(reverseOps[GT_NE - GT_EQ] == GT_EQ);

    assert(reverseOps[GT_LT - GT_EQ] == GT_GE);
    assert(reverseOps[GT_LE - GT_EQ] == GT_GT);
    assert(reverseOps[GT_GE - GT_EQ] == GT_LT);
    assert(reverseOps[GT_GT - GT_EQ] == GT_LE);

    assert(OperIsCompare(relop));
    assert(relop >= GT_EQ && relop - GT_EQ < sizeof(reverseOps));

    return (genTreeOps)reverseOps[relop - GT_EQ];
}

/*****************************************************************************
 *
 *  Return a relational operator that will work for swapped operands.
 */

/* static */
genTreeOps          GenTree::SwapRelop(genTreeOps relop)
{
    static const
    unsigned char   swapOps[] =
    {
        GT_EQ,          // GT_EQ
        GT_NE,          // GT_NE
        GT_GT,          // GT_LT
        GT_GE,          // GT_LE
        GT_LE,          // GT_GE
        GT_LT,          // GT_GT
    };

    assert(swapOps[GT_EQ - GT_EQ] == GT_EQ);
    assert(swapOps[GT_NE - GT_EQ] == GT_NE);

    assert(swapOps[GT_LT - GT_EQ] == GT_GT);
    assert(swapOps[GT_LE - GT_EQ] == GT_GE);
    assert(swapOps[GT_GE - GT_EQ] == GT_LE);
    assert(swapOps[GT_GT - GT_EQ] == GT_LT);

    assert(OperIsCompare(relop));
    assert(relop >= GT_EQ && relop - GT_EQ < sizeof(swapOps));

    return (genTreeOps)swapOps[relop - GT_EQ];
}

/*****************************************************************************
 *
 *  Reverse the meaning of the given test condition.
 */

GenTreePtr FASTCALL Compiler::gtReverseCond(GenTree * tree)
{
    if  (tree->OperIsCompare())
    {
        tree->SetOper(GenTree::ReverseRelop(tree->OperGet()));

        /* Flip the GTF_CMP_NAN_UN bit */

        if (varTypeIsFloating(tree->gtOp.gtOp1->TypeGet()))
            tree->gtFlags ^= GTF_RELOP_NAN_UN;
    }
    else
    {
        tree = gtNewOperNode(GT_NOT, TYP_INT, tree);
    }

    return tree;
}

/*****************************************************************************
 *
 *  If the given tree is an assignment of the form "lcl = log0(lcl)",
 *  returns the variable number of the local. Otherwise returns -1.
 */

#if OPT_BOOL_OPS

int                 GenTree::IsNotAssign()
{
    if  (gtOper != GT_ASG)
        return  -1;

    GenTreePtr      dst = gtOp.gtOp1;
    GenTreePtr      src = gtOp.gtOp2;

    if  (dst->gtOper != GT_LCL_VAR)
        return  -1;
    if  (src->gtOper != GT_LOG0)
        return  -1;

    src = src->gtOp.gtOp1;
    if  (src->gtOper != GT_LCL_VAR)
        return  -1;

    if  (dst->gtLclVar.gtLclNum != src->gtLclVar.gtLclNum)
        return  -1;

    return  dst->gtLclVar.gtLclNum;
}

#endif

/*****************************************************************************
 *
 *  Returns non-zero if the given tree is a 'leaf'.
 */

int                 GenTree::IsLeafVal()
{
    unsigned        kind = OperKind();

    if  (kind & (GTK_LEAF|GTK_CONST))
        return 1;

    if  (kind &  GTK_SMPOP)
        return 1;

    if  (gtOper == GT_FIELD && !gtField.gtFldObj)
        return 1;

    return 0;
}


/*****************************************************************************/

#ifdef DEBUG


bool                GenTree::gtIsValid64RsltMul()
{
    if ((gtOper != GT_MUL) || !(gtFlags & GTF_MUL_64RSLT))
        return false;

    GenTreePtr  op1 = gtOp.gtOp1;
    GenTreePtr  op2 = gtOp.gtOp2;

    if (TypeGet() != TYP_LONG ||
        op1->TypeGet() != TYP_LONG || op2->TypeGet() != TYP_LONG)
        return false;

    if (gtOverflow())
        return false;

    // op1 has to be conv.i8(i4Expr)
    if ((op1->gtOper != GT_CAST) ||
        (genActualType(op1->gtCast.gtCastOp->gtType) != TYP_INT))
        return false;

    // op2 has to be conv.i8(i4Expr), or this could be folded to i8const

    if (op2->gtOper == GT_CAST)
    {
        if (genActualType(op2->gtCast.gtCastOp->gtType) != TYP_INT)
            return false;

        // The signedness of both casts must be the same
        if (((op1->gtFlags & GTF_UNSIGNED) != 0) !=
            ((op2->gtFlags & GTF_UNSIGNED) != 0))
            return false;
    }
    else
    {
        if (op2->gtOper != GT_CNS_LNG)
            return false;

        // This must have been conv.i8(i4const) before folding. Ensure this. 
        if ((INT64( INT32(op2->gtLngCon.gtLconVal)) != op2->gtLngCon.gtLconVal) &&
            (INT64(UINT32(op2->gtLngCon.gtLconVal)) != op2->gtLngCon.gtLconVal))
            return false;
    }

    // Do unsigned mul iff both the casts are unsigned
    if (((op1->gtFlags & GTF_UNSIGNED) != 0) != ((gtFlags & GTF_UNSIGNED) != 0))
        return false;

    return true;
}

#endif

/*****************************************************************************
 *
 *  Figure out the evaluation order for a list of values.
 */

unsigned            Compiler::gtSetListOrder(GenTree *list, bool regs)
{
    assert(list && list->gtOper == GT_LIST);

    unsigned        level  = 0;
    unsigned        ftreg  = 0;
    unsigned        costSz = regs ? 1 : 0;  // push is smaller than mov to reg
    unsigned        costEx = regs ? 1 : IND_COST_EX;

#if TGT_x86
    /* Save the current FP stack level since an argument list
     * will implicitly pop the FP stack when pushing the argument */
    unsigned        FPlvlSave = genFPstkLevel;
#endif

    GenTreePtr      next = list->gtOp.gtOp2;

    if  (next)
    {
        unsigned  nxtlvl = gtSetListOrder(next, regs);

        ftreg |= next->gtRsvdRegs;

        if  (level < nxtlvl)
             level = nxtlvl;
        costEx += next->gtCostEx;
        costSz += next->gtCostSz;
    }

    GenTreePtr      op1  = list->gtOp.gtOp1;
    unsigned        lvl  = gtSetEvalOrder(op1);

#if TGT_x86
    /* restore the FP level */
    genFPstkLevel = FPlvlSave;
#endif

    list->gtRsvdRegs = ftreg | op1->gtRsvdRegs;

    if  (level < lvl)
         level = lvl;

    costEx += op1->gtCostEx;
    list->gtCostEx = costEx;

    costSz += op1->gtCostSz;
    list->gtCostSz = costSz;

    return level;
}



/*****************************************************************************
 *
 *  This routine is a helper routine for gtSetEvalOrder() and is used to
 *  mark the interior address computation node with the GTF_DONT_CSE flag
 *  which prevents them from beinf considered for CSE's.
 *
 *  Furthermore this routine is a factoring of the logic use to walk down 
 *  the child nodes of a GT_IND tree.
 *
 *  Previously we had this logic repeated three times inside of gtSetEvalOrder()
 *  Here we combine those three repeats into thsi routine and use the 
 *  vool constOnly to modify the behavior of this routine for the first call.
 *
 *  The object here is to mark all of the interior GT_ADD's and GT_NOP's
 *  with the GTF_DONT_CSE flag and to set op1 and op2 to the terminal nodes
 *  which are later matched against 'adr' and 'idx'
 *
 */

void Compiler::gtWalkOp(GenTree * *  op1WB, 
                        GenTree * *  op2WB, 
                        GenTree *    adr,
                        bool         constOnly)
{
    GenTreePtr op1 = *op1WB;
    GenTreePtr op2 = *op2WB;
    GenTreePtr op1EffectiveVal;

    if (op1->gtOper == GT_COMMA)
    {
        op1EffectiveVal = op1->gtEffectiveVal();
        if ((op1EffectiveVal->gtOper == GT_ADD) &&
            (!op1EffectiveVal->gtOverflow())    && 
            (!constOnly || (op1EffectiveVal->gtOp.gtOp2->gtOper == GT_CNS_INT)))
        {
            op1 = op1EffectiveVal;
        }
    }

    // Now we look for op1's with non-overflow GT_ADDS [of constants]
    while ((op1->gtOper == GT_ADD)  && 
           (!op1->gtOverflow())     && 
           (!constOnly || (op1->gtOp.gtOp2->gtOper == GT_CNS_INT)))
    {
        // mark it with GTF_DONT_CSE
        op1->gtFlags |= GTF_DONT_CSE;
        if (!constOnly)
            op2 = op1->gtOp.gtOp2;
        op1 = op1->gtOp.gtOp1;
        
        // If op1 is a GT_NOP then swap op1 and op2
        if (op1->gtOper == GT_NOP)
        {
            GenTreePtr tmp;

            tmp = op1;
            op1 = op2;
            op2 = tmp;
        }

        // If op2 is a GT_NOP then mark it with GTF_DONT_CSE
        while (op2->gtOper == GT_NOP)
        {
            op2->gtFlags |= GTF_DONT_CSE;
            op2 = op2->gtOp.gtOp1;
        }

        if (op1->gtOper == GT_COMMA)
        {
            op1EffectiveVal = op1->gtEffectiveVal();
            if ((op1EffectiveVal->gtOper == GT_ADD) &&
                (!op1EffectiveVal->gtOverflow())    && 
                (!constOnly || (op1EffectiveVal->gtOp.gtOp2->gtOper == GT_CNS_INT)))
            {
                op1 = op1EffectiveVal;
            }
        }
             
        if (!constOnly && ((op2 == adr) || (op2->gtOper != GT_CNS_INT)))
            break;
    }

    *op1WB = op1;
    *op2WB = op2;
}

/*****************************************************************************
 *
 *  Given a tree, figure out the order in which its sub-operands should be
 *  evaluated.
 *
 *  Returns the Sethi 'complexity' estimate for this tree (the higher
 *  the number, the higher is the tree's resources requirement).
 *
 *  gtCostEx is set to the execution complexity estimate
 *  gtCostSz is set to the code size estimate
 *
 *  #if TGT_x86
 *
 *      We compute the "FPdepth" value for each tree, i.e. the max. number
 *      of operands the tree will push on the x87 (coprocessor) stack.
 *
 *  #else
 *
 *      We compute an estimate of the number of temporary registers each
 *      node will require - this is used later for register allocation.
 *
 *  #endif
 */

unsigned            Compiler::gtSetEvalOrder(GenTree * tree)
{
    assert(tree);
    assert(tree->gtOper != GT_STMT);

    
#ifdef DEBUG
    /* Clear the GTF_MORPHED flag as well */
    tree->gtFlags &= ~GTF_MORPHED;
#endif
    /* Is this a FP value? */

#if TGT_x86
    bool            isflt = varTypeIsFloating(tree->TypeGet());
    unsigned        FPlvlSave;
#endif

    /* Figure out what kind of a node we have */

    genTreeOps      oper = tree->OperGet();
    unsigned        kind = tree->OperKind();

    /* Assume no fixed registers will be trashed */

    unsigned        ftreg = 0;
    unsigned        level;
    unsigned        lvl2;
    int             costEx;
    int             costSz;

#ifdef DEBUG
    costEx = -1;
    costSz = -1;
#endif

#if TGT_RISC
    tree->gtIntfRegs = 0;
#endif


    /* Is this a constant or a leaf node? */

    if (kind & (GTK_LEAF|GTK_CONST))
    {
        switch (oper)
        {
        case GT_CNS_LNG:
            costSz = 8;
            goto COMMON_CNS;

        case GT_CNS_STR:
            costSz = 4;
            goto COMMON_CNS;

        case GT_CNS_INT:
            if (((signed char) tree->gtIntCon.gtIconVal) == tree->gtIntCon.gtIconVal)
                costSz = 1;
            else
                costSz = 4;
            goto COMMON_CNS;

COMMON_CNS:
        /*
            Note that some code below depends on constants always getting
            moved to be the second operand of a binary operator. This is
            easily accomplished by giving constants a level of 0, which
            we do on the next line. If you ever decide to change this, be
               aware that unless you make other arrangements for integer 
               constants to be moved, stuff will break.
         */

            level  = 0;
            costEx = 1;
            break;

        case GT_CNS_DBL:
            level = 0;
            /* We use fldz and fld1 to load 0.0 and 1.0, but all other  */
            /* floating point constants are loaded using an indirection */
            if  ((*((__int64 *)&(tree->gtDblCon.gtDconVal)) == 0) ||
                 (*((__int64 *)&(tree->gtDblCon.gtDconVal)) == 0x3ff0000000000000))
            {
                costEx = 1;
                costSz = 1;
            }
            else
            {
                costEx = IND_COST_EX;
                costSz = 4;
            }
            break;
            
        case GT_LCL_VAR:
            level = 1;
            assert(tree->gtLclVar.gtLclNum < lvaTableCnt);
            if (lvaTable[tree->gtLclVar.gtLclNum].lvVolatile)
            {
                costEx = IND_COST_EX;
                costSz = 2;
                /* Sign-extend and zero-extend are more expensive to load */
                if (varTypeIsSmall(tree->TypeGet()))
                {
                    costEx += 1;
                    costSz += 1;
                }
            }
            else if (isflt || varTypeIsLong(tree->TypeGet()))
            {
                costEx = (IND_COST_EX + 1) / 2;     // Longs and doubles often aren't enregistered 
                costSz = 2;
            }
            else
            {
                costEx = 1;
                costSz = 1;
                /* Sign-extend and zero-extend are more expensive to load */
                if (lvaTable[tree->gtLclVar.gtLclNum].lvNormalizeOnLoad())
                {
                    costEx += 1;
                    costSz += 1;
                }
            }
            break;

        case GT_CLS_VAR:
        case GT_LCL_FLD:
            level = 1;
            costEx = IND_COST_EX;
            costSz = 4;
            if (varTypeIsSmall(tree->TypeGet()))
            {
                costEx += 1;
                costSz += 1;
            }
            break;
            
        default:
            level  = 1;
            costEx = 1;
            costSz = 1;
            break;
        }
#if TGT_x86
        genFPstkLevel += isflt;
#endif
        goto DONE;
    }

    /* Is it a 'simple' unary/binary operator? */

    if  (kind & GTK_SMPOP)
    {
        int             lvlb;

        GenTreePtr      op1 = tree->gtOp.gtOp1;
        GenTreePtr      op2 = tree->gtGetOp2();

        costEx = 0;
        costSz = 0;

        /* Check for a nilary operator */

        if  (!op1)
        {
            assert(op2 == 0);

#if TGT_x86
            if  (oper == GT_BB_QMARK || oper == GT_BB_COLON)
                genFPstkLevel += isflt;
#endif
            level    = 0;

            goto DONE;
        }

        /* Is this a unary operator? */

        if  (!op2)
        {
            /* Process the operand of the operator */

        UNOP:

            /* Most Unary ops have costEx of 1 */
            costEx = 1;
            costSz = 1;

            level  = gtSetEvalOrder(op1);
            ftreg |= op1->gtRsvdRegs;

            /* Special handling for some operators */

            switch (oper)
            {
            case GT_JTRUE:
                costEx = 2;
                costSz = 2;
                break;

            case GT_SWITCH:
                costEx = 10;
                costSz =  5;
                break;

            case GT_CAST:

                if  (isflt)
                {
                    /* Casts to float always go through memory */
                    costEx += IND_COST_EX;
                    costSz += 6;

                    if  (!varTypeIsFloating(op1->TypeGet()))
                    {
                        genFPstkLevel++;
                    }
                }
#ifdef DEBUG
                else if (gtDblWasInt(op1))
                {
                    genFPstkLevel--;
                }
#endif
                
                /* Overflow check are more expensive */
                if (tree->gtOverflow())
                {
                    costEx += 3;
                    costSz += 3;
                }

                break;

            case GT_NOP:

                /* Special case: array range check */

                if  (tree->gtFlags & GTF_NOP_RNGCHK)
                    level++;

                tree->gtFlags |= GTF_DONT_CSE;
                costEx = 0;
                costSz = 0;
                break;

#if     INLINE_MATH
            case GT_MATH:
                if (tree->gtMath.gtMathFN==CORINFO_INTRINSIC_Round &&
                    tree->TypeGet()==TYP_INT)
                {
                    // This is a special case to handle the following
                    // optimization: conv.i4(round.d(d)) -> round.i(d) 
                    // if flowgraph 3186
                    // @TODO [CONSIDER] [04/16/01] [dnotario]: 
                    // using another intrinsic in this optimization
                    // or marking with a special flag. This type of special
                    // cases is not good. dnotario
                        
                    assert(genFPstkLevel);
                    genFPstkLevel--;
                }
                // Fall through

#endif
            case GT_NOT:
            case GT_NEG:
                // We need to ensure that -x is evaluated before x or else
                // we get burned while adjusting genFPstkLevel in x*-x where
                // the rhs x is the last use of the enregsitered x.
                //
                // [briansul] even in the integer case we want to prefer to
                // evaluate the side without the GT_NEG node, all other things
                // being equal.  Also a GT_NOT requires a scratch register

                level++;
                break;

            case GT_ADDR:

#if TGT_x86
                /* If the operand was floating point, pop the value from the stack */

                if (varTypeIsFloating(op1->TypeGet()))
                {
                    assert(genFPstkLevel);
                    genFPstkLevel--;
                }
#endif
                costEx = 0;
                costSz = 1;
                break;

            case GT_MKREFANY:
            case GT_LDOBJ:
                level  = gtSetEvalOrder(tree->gtLdObj.gtOp1);
                ftreg |= tree->gtLdObj.gtOp1->gtRsvdRegs;
                costEx = tree->gtLdObj.gtOp1->gtCostEx + 1;
                costSz = tree->gtLdObj.gtOp1->gtCostSz + 1;
                break;

            case GT_IND:

                /* An indirection should always have a non-zero level *
                 * Only constant leaf nodes have level 0 */

                if (level == 0)
                    level = 1;

                /* Indirections have a costEx of IND_COST_EX */
                costEx = IND_COST_EX;
                costSz = 2;

                /* If we have to sign-extend or zero-extend, bump the cost */
                if (varTypeIsSmall(tree->TypeGet()))
                {
                    costEx += 1;
                    costSz += 1;
                }

#if     TGT_x86
                /* Indirect loads of FP values push a new value on the FP stack */
                genFPstkLevel += isflt;
#endif

#if     CSELENGTH
                if  ((tree->gtFlags & GTF_IND_RNGCHK) && tree->gtInd.gtIndLen)
                {
                    GenTreePtr      len = tree->gtInd.gtIndLen;

                    assert(len->gtOper == GT_ARR_LENREF);

                    lvl2 = gtSetEvalOrder(len);
                    if  (level < lvl2)   level = lvl2;
                }
#endif

                /* Can we form an addressing mode with this indirection? */

                if  (op1->gtOper == GT_ADD)
                {
                    bool            rev;
#if SCALED_ADDR_MODES
                    unsigned        mul;
#endif
                    unsigned        cns;
                    GenTreePtr      adr;
                    GenTreePtr      idx;

                    /* See if we can form a complex addressing mode? */

                    if  (genCreateAddrMode(op1,             // address
                                           0,               // mode
                                           false,           // fold
                                           0,               // reg mask
#if!LEA_AVAILABLE
                                           tree->TypeGet(), // operand type
#endif
                                           &rev,            // reverse ops
                                           &adr,            // base addr
                                           &idx,            // index val
#if SCALED_ADDR_MODES
                                           &mul,            // scaling
#endif
                                           &cns,            // displacement
                                           true))           // don't generate code
                    {
#if TGT_SH3
                        if (adr & idx)
                        {
                            /* The address is "[adr+idx]" */
                            ftreg |= RBM_r00;
                        }
#endif
                        // We can form a complex addressing mode,
                        // so mark each of the interior nodes with GTF_DONT_CSE
                        // and calculate a more accurate cost

                        op1->gtFlags |= GTF_DONT_CSE;

                        if (adr)
                        {
                            costEx += adr->gtCostEx;
                            costSz += adr->gtCostSz;
                        }

                        if (idx)
                        {
                            costEx += idx->gtCostEx;
                            costSz += idx->gtCostSz;
                        }

                        if (cns)
                        {
                            if (((signed char)cns) == ((int)cns))
                                costSz += 1;
                            else
                                costSz += 4;
                        }

                        /* Walk op1 looking for non-overflow GT_ADDs */
                        gtWalkOp(&op1, &op2, adr, false);

                        /* Walk op1 looking for non-overflow GT_ADDs of constants */
                        gtWalkOp(&op1, &op2, NULL, true);

                        /* Walk op2 looking for non-overflow GT_ADDs of constants */
                        gtWalkOp(&op2, &op1, NULL, true);

                        // OK we are done walking the tree
                        // Now assert that op1 and op2 correspond with adr and idx
                        // in one of the several acceptable ways.

                        // Note that sometimes op1/op2 is equal to idx/adr
                        // and other times op1/op2 is a GT_COMMA node with
                        // an effective value that is idx/adr

                        if (mul > 1)
                        {
                            if ((op1 != adr) && (op1->gtOper == GT_LSH))
                            {
                                op1->gtFlags |= GTF_DONT_CSE;
                                assert((adr == NULL) || (op2 == adr) || (op2->gtEffectiveVal() == adr));
                            }
                            else
                            {
                                assert(op2);
                                assert(op2->gtOper == GT_LSH);
                                op2->gtFlags |= GTF_DONT_CSE;
                                assert((op1 == adr) || (op1->gtEffectiveVal() == adr));
                            }
                        }
                        else
                        {
                            assert(mul == 0);

                            if      ((op1 == idx) || (op1->gtEffectiveVal() == idx))
                            {
                                if (idx != NULL)
                                {
                                    if ((op1->gtOper == GT_MUL) || (op1->gtOper == GT_LSH))
                                    {
                                        if (op1->gtOp.gtOp1->gtOper == GT_NOP)
                                            op1->gtFlags |= GTF_DONT_CSE;
                                    }
                                }
                                assert((op2 == adr) || (op2->gtEffectiveVal() == adr));
                            }
                            else if ((op1 == adr) || (op1->gtEffectiveVal() == adr))
                            {
                                if (idx != NULL)
                                {
                                    assert(op2);
                                    if ((op2->gtOper == GT_MUL) || (op2->gtOper == GT_LSH))
                                    {
                                        if (op2->gtOp.gtOp1->gtOper == GT_NOP)
                                            op2->gtFlags |= GTF_DONT_CSE;
                                    }
                                    assert((op2 == idx) || (op2->gtEffectiveVal() == idx));
                                }
                            }
                        }
                        goto DONE;

                    }   // end  if  (genCreateAddrMode(...

                }   // end if  (op1->gtOper == GT_ADD)
                else if (op1->gtOper == GT_CNS_INT)
                {
                    /* Indirection of a CNS_INT, don't add 1 to costEx */
                    goto IND_DONE_EX;
                }
                break;

            default:
                break;
            }
            costEx  += op1->gtCostEx;
IND_DONE_EX:
            costSz  += op1->gtCostSz;

            goto DONE;
        }

        /* Binary operator - check for certain special cases */

        lvlb = 0;

        /* Most Binary ops have a cost of 1 */
        costEx = 1;
        costSz = 1;

        switch (oper)
        {
        case GT_MOD:
        case GT_UMOD:

            /* Modulo by a power of 2 is easy */

            if  (op2->gtOper == GT_CNS_INT)
            {
                unsigned    ival = op2->gtIntCon.gtIconVal;

                if  (ival > 0 && ival == genFindLowestBit(ival))
                    break;
            }

            // Fall through ...

        case GT_DIV:
        case GT_UDIV:

            if  (isflt)
            {
                /* fp division is very expensive to execute */
                costEx = 36;  // TYP_DOUBLE
            }
            else
            {
                /* integer division is also very expensive */
                costEx = 20;

#if     TGT_x86
#if     LONG_MATH_REGPARAM
                if  (tree->gtType == TYP_LONG)
                {
                    /* Encourage the second operand to be evaluated (into EBX/ECX) first*/
                    lvlb += 3;

                    // The second operand must be evaluated (into EBX/ECX) */
                    ftreg |= RBM_EBX|RBM_ECX;
                }
#endif

                // Encourage the first operand to be evaluated (into EAX/EDX) first */
                lvlb -= 3;

                // the idiv and div instruction requires EAX/EDX
                ftreg |= RBM_EAX|RBM_EDX;
#endif
            }
            break;

        case GT_MUL:

            if  (isflt)
            {
                /* FP multiplication instructions are more expensive */
                costEx = 5;
            }
            else
            {
                /* Integer multiplication instructions are more expensive */
                costEx = 4;

                /* Encourage the second operand to be evaluated first (weakly) */
                lvlb++;

#if     TGT_x86
#if     LONG_MATH_REGPARAM

                if  (tree->gtType == TYP_LONG)
                {
                    /* Encourage the second operand to be evaluated (into EBX/ECX) first*/
                    lvlb += 3;
                    
                    // The second operand must be evaluated (into EBX/ECX) */
                    ftreg |= RBM_EBX|RBM_ECX;
                }

#else // not LONG_MATH_REGPARAM

                if  (tree->gtType == TYP_LONG)
                {
                    assert(tree->gtIsValid64RsltMul());
                    goto USE_IMUL_EAX;
                }
                else if (tree->gtOverflow())
                {
                    /* Overflow check are more expensive */
                    costEx += 3;
                    costSz += 3;

                    if  ((tree->gtFlags & GTF_UNSIGNED)  || 
                         varTypeIsSmall(tree->TypeGet())   )
                    {
                        /* We use imulEAX for most overflow multiplications */
USE_IMUL_EAX:
                        // Encourage the first operand to be evaluated (into EAX/EDX) first */
                        lvlb -= 4;

                        // the imulEAX instruction requires EAX/EDX
                        ftreg |= RBM_EAX|RBM_EDX;
                        /* If we have to use EAX:EDX then it costs more */
                        costEx += 1;
                    }
                }
#endif
#endif
            }
            break;


        case GT_ADD:
        case GT_SUB:
        case GT_ASG_ADD:
        case GT_ASG_SUB:

            if  (isflt)
            {
                /* FP multiplication instructions are a bit more expensive */
                costEx = 5;
                break;
            }

            /* Overflow check are more expensive */
            if (tree->gtOverflow())
            {
                costEx += 3;
                costSz += 3;
            }
            break;

        case GT_COMMA:

            /* Comma tosses the result of the left operand */
#if     TGT_x86
            FPlvlSave = genFPstkLevel;
            level = gtSetEvalOrder(op1);
            genFPstkLevel = FPlvlSave;
#else
            level = gtSetEvalOrder(op1);
#endif
            lvl2   = gtSetEvalOrder(op2);

            if  (level < lvl2)
                 level = lvl2;
            else if  (level == lvl2)
                 level += 1;

            ftreg |= op1->gtRsvdRegs|op2->gtRsvdRegs;

            /* GT_COMMA have the same cost as their op2 */
            costEx = op2->gtCostEx;
            costSz = op2->gtCostSz;

            goto DONE;

        case GT_COLON:

#if     TGT_x86
            FPlvlSave = genFPstkLevel;
            level = gtSetEvalOrder(op1);
            genFPstkLevel = FPlvlSave;
#else
            level = gtSetEvalOrder(op1);
#endif
            lvl2  = gtSetEvalOrder(op2);

            if  (level < lvl2)
                 level = lvl2;
            else if  (level == lvl2)
                 level += 1;

            ftreg |= op1->gtRsvdRegs|op2->gtRsvdRegs;
            costEx = op1->gtCostEx + op2->gtCostEx;
            costSz = op1->gtCostSz + op2->gtCostSz;

            goto DONE;

        case GT_IND:

            /* The second operand of an indirection is just a fake */

            goto UNOP;
        }

        /* Assignments need a bit special handling */

        if  (kind & GTK_ASGOP)
        {
            /* Process the target */

            level = gtSetEvalOrder(op1);

#if     TGT_x86

            /* If assigning an FP value, the target won't get pushed */

            if  (isflt)
            {
                op1->gtFPlvl--;
                assert(genFPstkLevel);
                genFPstkLevel--;
            }

#endif

            goto DONE_OP1;
        }

        /* Process the sub-operands */

        level  = gtSetEvalOrder(op1);
        if (lvlb < 0)
        {
            level -= lvlb;      // lvlb is negative, so this increases level
            lvlb   = 0;
        }


    DONE_OP1:

        assert(lvlb >= 0);

        lvl2    = gtSetEvalOrder(op2) + lvlb;

        ftreg  |= op1->gtRsvdRegs|op2->gtRsvdRegs;

        costEx += (op1->gtCostEx + op2->gtCostEx);
        costSz += (op1->gtCostSz + op2->gtCostSz);

#if TGT_x86
        /*
            Binary FP operators pop 2 operands and produce 1 result;
            FP comparisons pop 2 operands and produces 0 results.
            assignments consume 1 value and don't produce anything.
         */
        
        if  (isflt)
        {
            assert(oper != GT_COMMA);
                assert(genFPstkLevel);
                genFPstkLevel--;
        }
#endif

        bool bReverseInAssignment = false;
        if  (kind & GTK_ASGOP)
        {
            /* If this is a local var assignment, evaluate RHS before LHS */

            switch (op1->gtOper)
            {
            case GT_IND:

                // If we have any side effects on the GT_IND child node
                // we have to evaluate op1 first

                if  (op1->gtOp.gtOp1->gtFlags & GTF_GLOB_EFFECT)
                    break;

                // If op2 is simple then evaluate op1 first

                if (op2->OperKind() & GTK_LEAF)
                    break;

                // fall through and set GTF_REVERSE_OPS

            case GT_LCL_VAR:
            case GT_LCL_FLD:

                // We evaluate op2 before op1
                bReverseInAssignment = true;
                tree->gtFlags |= GTF_REVERSE_OPS;
                break;
            }
        }
#if     TGT_x86
        else if (kind & GTK_RELOP)
        {
            /* Float compares remove both operands from the FP stack */
            /* Also FP comparison uses EAX for flags */
            /* @TODO [REVISIT] [04/16/01] []: Handle the FCOMI case here! (don't reserve EAX) */

            if  (varTypeIsFloating(op1->TypeGet()))
            {
                assert(genFPstkLevel >= 2);
                genFPstkLevel -= 2;
                ftreg         |= RBM_EAX;
                level++; lvl2++;
            }

            if ((tree->gtFlags & GTF_RELOP_JMP_USED) == 0)
            {
                /* Using a setcc instruction is more expensive */
                costEx += 3;
            }
        }
#endif

        /* Check for other interesting cases */

        switch (oper)
        {
        case GT_LSH:
        case GT_RSH:
        case GT_RSZ:
        case GT_ASG_LSH:
        case GT_ASG_RSH:
        case GT_ASG_RSZ:

#if     TGT_x86
            /* Shifts by a non-constant amount are expensive and use ECX */

            if  (op2->gtOper != GT_CNS_INT)
            {
                ftreg  |= RBM_ECX;
                costEx += 3;

                if  (tree->gtType == TYP_LONG)
                {
                    ftreg  |= RBM_EAX | RBM_EDX;
                    costEx += 7;
                    costSz += 4;
                }
            }
#endif
            break;

#if     INLINE_MATH
        case GT_MATH:

            // We don't use any binary GT_MATH operators at the moment
#if 0
            switch (tree->gtMath.gtMathFN)
            {
            case CORINFO_INTRINSIC_Exp:
                level += 4;
                break;

            case CORINFO_INTRINSIC_Pow:
                level += 3;
                break;
            default:
                assert(!"Unknown binary GT_MATH operator");
                break;
            }
#else
            assert(!"Unknown binary GT_MATH operator");
#endif

            break;
#endif

        }

        /* We need to evalutate constants later as many places in codegen
           cant handle op1 being a constant. This is normally naturally
           enforced as constants have the least level of 0. However,
           sometimes we end up with a tree like "cns1 < nop(cns2)". In
           such cases, both sides have a level of 0. So encourage constants
           to be evaluated last in such cases */

        if ((level == 0) && (level == lvl2) &&
            (op1->OperKind() & GTK_CONST)   &&
            (tree->OperIsCommutative() || tree->OperIsCompare()))
        {
            lvl2++;
        }

        /* We try to swap operands if the second one is more expensive */
        bool tryToSwap;
        GenTreePtr opA,opB;
        
        if (bReverseInAssignment)
        {
            // Assignments are special, we want the reverseops flags
            // so if posible it was set above. 
            tryToSwap = false;
        }
        else
        {
            if (tree->gtFlags & GTF_REVERSE_OPS)
            {
                tryToSwap = (level > lvl2);
                opA = op2;
                opB = op1;
            }
            else
            {
                tryToSwap = (level < lvl2);
                opA = op1;
                opB = op2;
            }

#ifdef DEBUG
            // We want to stress mostly the reverse flag being set
            if (compStressCompile(STRESS_REVERSEFLAG, 60) &&
                (tree->gtFlags & GTF_REVERSE_OPS) == 0 && 
                (op2->OperKind() & GTK_CONST) == 0 )
            {
                tryToSwap = true;
            }
#endif
        }
        

        /* Force swapping for compLooseExceptions (under some cases) so
           that people dont rely on a certain order (even though a
           different order is permitted) */

        if (info.compLooseExceptions && opts.compDbgCode && ( ((tree->gtFlags & GTF_REVERSE_OPS)?lvl2:level) > 0))
            tryToSwap = true;

        if (tryToSwap)
        {
            /* Relative of order of global / side effects cant be swapped */

            bool    canSwap = true;

            /*  When strict side effect order is disabled we allow
             *  GTF_REVERSE_OPS to be set when one or both sides contains
             *  a GTF_CALL or GTF_EXCEPT.
             *  Currently only the C and C++ languages
             *  allow non strict side effect order
             */
            unsigned strictEffects = GTF_GLOB_EFFECT;
            if (info.compLooseExceptions)
                strictEffects &= ~(GTF_CALL | GTF_EXCEPT);

            if (opA->gtFlags & strictEffects)
            {
                /*  op1 has side efects, that can't be reordered.
                 *  Check for some special cases where we still
                 *  may be able to swap
                 */

                if (opB->gtFlags & strictEffects)
                {
                    /* op2 has also has non reorderable side effects - can't swap */
                    canSwap = false;
                }
                else
                {
                    /* No side effects in op2 - we can swap iff
                     *  op1 has no way of modifying op2,
                     *  i.e. through byref assignments or calls
                     *       unless op2 is a constant
                     */

                    if (opA->gtFlags & strictEffects & (GTF_ASG | GTF_CALL))
                    {
                        /* We have to be conservative - can swap iff op2 is constant */
                        if (!opB->OperIsConst())
                            canSwap = false;
                    }
                }

                /* We cannot swap in the presence of special side effects such as QMARK COLON */

                if (opA->gtFlags & GTF_OTHER_SIDEEFF)
                    canSwap = false;
            }

            if  (canSwap)
            {
                /* Can we swap the order by commuting the operands? */

                switch (oper)
                {
                case GT_ADD:
                case GT_MUL:

                case GT_OR:
                case GT_XOR:
                case GT_AND:

                    /* Swap the operands */

                    tree->gtOp.gtOp1 = op2;
                    tree->gtOp.gtOp2 = op1;
                    
                    /* We may have to recompute FP levels */
#if TGT_x86
                    if  (op1->gtFPlvl || op2->gtFPlvl)
                        fgFPstLvlRedo = true;
#endif
                    break;

#if INLINING
                case GT_QMARK:
                case GT_COLON:
                    break;
#endif

                case GT_LIST:
                    break;

                case GT_SUB:
#if TGT_x86
                    if  (!isflt)
                        break;
#else
                    if  (!varTypeIsFloating(tree->TypeGet()))
                        break;
#endif

                    // Fall through ....

                default:

                    /* Mark the operand's evaluation order to be swapped */
                    if (tree->gtFlags & GTF_REVERSE_OPS)
                    {
                        tree->gtFlags &= ~GTF_REVERSE_OPS;
                    }
                    else
                    {
                        tree->gtFlags |= GTF_REVERSE_OPS;
                    }
                    
                    /* We may have to recompute FP levels */

#if TGT_x86
                    if  (op1->gtFPlvl || op2->gtFPlvl)
                        fgFPstLvlRedo = true;
#endif
                    break;
                }
            }
        }

#if TGT_RISC 

        if  (op1 && op2 && ((op1->gtFlags|op2->gtFlags) & GTF_CALL))
        {
            GenTreePtr  x1 = op1;
            GenTreePtr  x2 = op2;

            if  (tree->gtFlags & GTF_REVERSE_OPS)
            {
                x1 = op2;
                x2 = op1;
            }

            if  (x2->gtFlags & GTF_CALL)
            {
                if  (oper != GT_ASG || op1->gtOper == GT_IND)
                {
#ifdef  DEBUG
                    printf("UNDONE: Needs spill/callee-saved temp\n");
//                  gtDispTree(tree);
#endif
                }
            }
        }

#endif
        /* Swap the level counts */
        if (tree->gtFlags & GTF_REVERSE_OPS)
        {
            unsigned tmpl;

            tmpl = level;
                    level = lvl2;
                            lvl2 = tmpl;
        }

        /* Compute the sethi number for this binary operator */

        if  (level < lvl2)
        {
            level  = lvl2;
        }
        else if  (level == lvl2)
        {
            level += 1;
        }

        goto DONE;
    }

    /* See what kind of a special operator we have here */

    switch  (oper)
    {
    case GT_CALL:

        assert(tree->gtFlags & GTF_CALL);

        level  = 0;
        costEx = 5;
        costSz = 2;

        /* Evaluate the 'this' argument, if present */

        if  (tree->gtCall.gtCallObjp)
        {
            GenTreePtr     thisVal = tree->gtCall.gtCallObjp;

            lvl2   = gtSetEvalOrder(thisVal);
            if  (level < lvl2)   level = lvl2;
            costEx += thisVal->gtCostEx;
            costSz += thisVal->gtCostSz + 1;
            ftreg  |= thisVal->gtRsvdRegs;
        }

        /* Evaluate the arguments, right to left */

        if  (tree->gtCall.gtCallArgs)
        {
#if TGT_x86
            FPlvlSave = genFPstkLevel;
#endif
            lvl2  = gtSetListOrder(tree->gtCall.gtCallArgs, false);
            if  (level < lvl2)   level = lvl2;
            costEx += tree->gtCall.gtCallArgs->gtCostEx;
            costSz += tree->gtCall.gtCallArgs->gtCostSz;
            ftreg  |= tree->gtCall.gtCallArgs->gtRsvdRegs;
#if TGT_x86
            genFPstkLevel = FPlvlSave;
#endif
        }

        /* Evaluate the temp register arguments list
         * This is a "hidden" list and its only purpose is to
         * extend the life of temps until we make the call */

        if  (tree->gtCall.gtCallRegArgs)
        {
#if TGT_x86
            FPlvlSave = genFPstkLevel;
#endif

            lvl2  = gtSetListOrder(tree->gtCall.gtCallRegArgs, true);
            if  (level < lvl2)   level = lvl2;
            costEx += tree->gtCall.gtCallRegArgs->gtCostEx;
            costSz += tree->gtCall.gtCallRegArgs->gtCostSz;
            ftreg  |= tree->gtCall.gtCallRegArgs->gtRsvdRegs;
#if TGT_x86
            genFPstkLevel = FPlvlSave;
#endif
        }

        // pinvoke-calli cookie is a constant, or constant indirection
        assert(tree->gtCall.gtCallCookie == NULL ||
               tree->gtCall.gtCallCookie->gtOper == GT_CNS_INT ||
               tree->gtCall.gtCallCookie->gtOper == GT_IND);

        if  (tree->gtCall.gtCallType == CT_INDIRECT)
        {
            GenTreePtr     indirect = tree->gtCall.gtCallAddr;

            lvl2 += gtSetEvalOrder(indirect);
            if  (level < lvl2)   level = lvl2;
            costEx += indirect->gtCostEx;
            costSz += indirect->gtCostSz;
            ftreg  |= indirect->gtRsvdRegs;
        }
        else
        {
            costSz += 3;
            if (tree->gtCall.gtCallType != CT_HELPER)
                costSz++;
        }

        level += 1;

        /* Function calls that don't save the registers are much more expensive */

        if  (!(tree->gtFlags & GTF_CALL_REG_SAVE))
        {
            level  += 5;
            costEx += 5;
            ftreg  |= RBM_CALLEE_TRASH;
        }

#if TGT_x86
    if (genFPstkLevel > tmpDoubleSpillMax)
        tmpDoubleSpillMax = genFPstkLevel;

        genFPstkLevel += isflt;
#endif

        break;

#if CSELENGTH

    case GT_ARR_LENREF:

        {
            GenTreePtr  addr = tree->gtArrLen.gtArrLenAdr; assert(addr);

            level = 1;

            /* Has the address already been costed? */
            if  (tree->gtFlags & GTF_ALN_CSEVAL)
                level += gtSetEvalOrder(addr);

            ftreg  |= addr->gtRsvdRegs;
            costEx  = addr->gtCostEx + 1;
            costSz  = addr->gtCostSz + 1;

            addr = tree->gtArrLen.gtArrLenCse;
            if (addr)
            {
                lvl2 = gtSetEvalOrder(addr);
                if  (level < lvl2)   level = lvl2;

                ftreg  |= addr->gtRsvdRegs;
                costEx += addr->gtCostEx;
                costSz += addr->gtCostSz;
            }
        }
        break;

#endif

    case GT_ARR_ELEM:

        level  = gtSetEvalOrder(tree->gtArrElem.gtArrObj);
        costEx = tree->gtArrElem.gtArrObj->gtCostEx;
        costSz = tree->gtArrElem.gtArrObj->gtCostSz;

        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
        {
            lvl2 = gtSetEvalOrder(tree->gtArrElem.gtArrInds[dim]);
            if (level < lvl2)  level = lvl2;
            costEx += tree->gtArrElem.gtArrInds[dim]->gtCostEx;
            costSz += tree->gtArrElem.gtArrInds[dim]->gtCostSz;
        }

        genFPstkLevel += isflt;
        level  += tree->gtArrElem.gtArrRank;
        costEx += 2 + 4*tree->gtArrElem.gtArrRank;
        costSz += 2 + 2*tree->gtArrElem.gtArrRank;
        break;


    default:
#ifdef  DEBUG
        gtDispTree(tree);
#endif
        NO_WAY_RET("unexpected operator", unsigned);
    }

DONE:

#if TGT_x86
//  printf("[FPlvl=%2u] ", genFPstkLevel); gtDispTree(tree, 0, true);
    assert(int(genFPstkLevel) >= 0);
    tree->gtFPlvl    = genFPstkLevel;
#endif

    tree->gtRsvdRegs = ftreg;

    assert(costEx != -1);
    tree->gtCostEx = (costEx > MAX_COST) ? MAX_COST : costEx;
    tree->gtCostSz = (costSz > MAX_COST) ? MAX_COST : costSz;

#if     0
#ifdef  DEBUG
    printf("ftregs=%04X ", ftreg);
    gtDispTree(tree, 0, true);
#endif
#endif

    return level;
}


/*****************************************************************************
 *
 *  Returns non-zero if the given tree is an integer constant that can be used
 *  in a scaled index address mode as a multiplier (e.g. "[4*index]").
 */

unsigned            GenTree::IsScaleIndexMul()
{
    if  (gtOper == GT_CNS_INT && jitIsScaleIndexMul(gtIntCon.gtIconVal))
        return gtIntCon.gtIconVal;

    return 0;
}

/*****************************************************************************
 *
 *  Returns non-zero if the given tree is an integer constant that can be used
 *  in a scaled index address mode as a multiplier (e.g. "[4*index]").
 */

unsigned            GenTree::IsScaleIndexShf()
{
    if  (gtOper == GT_CNS_INT)
    {
        if  (gtIntCon.gtIconVal > 0 &&
             gtIntCon.gtIconVal < 4)
        {
            return 1 << gtIntCon.gtIconVal;
        }
    }

    return 0;
}

/*****************************************************************************
 *
 *  If the given tree is a scaled index (i.e. "op * 4" or "op << 2"), returns
 *  the multiplier (and in that case also makes sure the scaling constant is
 *  the second sub-operand of the tree); otherwise returns 0.
 */

unsigned            GenTree::IsScaledIndex()
{
    GenTreePtr      scale;

    switch (gtOper)
    {
    case GT_MUL:

        scale = gtOp.gtOp2;

        if  (scale->IsScaleIndexMul())
            return scale->gtIntCon.gtIconVal;

        break;

    case GT_LSH:

        scale = gtOp.gtOp2;

        if  (scale->IsScaleIndexShf())
            return  1 << scale->gtIntCon.gtIconVal;

        break;
    }

    return 0;
}

/*****************************************************************************
 *
 *  Returns true if the given operator may cause an exception.
 */

bool                GenTree::OperMayThrow()
{
    GenTreePtr  op;

    // ISSUE: Can any other operations cause an exception to be thrown?

    switch (gtOper)
    {
    case GT_MOD:
    case GT_DIV:
    case GT_UMOD:
    case GT_UDIV:

        /* Division with a non-zero, non-minus-one constant does not throw an exception */

        op = gtOp.gtOp2;

        if ((op->gtOper == GT_CNS_INT && op->gtIntCon.gtIconVal != 0 && op->gtIntCon.gtIconVal != -1) ||
            (op->gtOper == GT_CNS_LNG && op->gtLngCon.gtLconVal != 0 && op->gtLngCon.gtLconVal != -1))
            return false;

        return true;

    case GT_IND:
        op = gtOp.gtOp1;

        /* Indirections of handles are known to be safe */
        if (op->gtOper == GT_CNS_INT) 
        {
            unsigned kind = (op->gtFlags & GTF_ICON_HDL_MASK);
            if (kind != 0)
            {
                /* No exception is thrown on this indirection */
                return false;
            }
        }
        return true;

    case GT_ARR_ELEM:
    case GT_CATCH_ARG:
    case GT_ARR_LENGTH:
    case GT_LDOBJ:
    case GT_INITBLK:
    case GT_COPYBLK:
    case GT_LCLHEAP:
    case GT_CKFINITE:

        return  true;
    }

    /* Overflow arithmetic operations also throw exceptions */

    if (gtOverflowEx())
        return true;

    return  false;
}

#ifdef DEBUG

GenTreePtr FASTCALL Compiler::gtNewNode(genTreeOps oper, var_types  type)
{
#if     SMALL_TREE_NODES
    size_t          size = GenTree::s_gtNodeSizes[oper];
#else
    size_t          size = sizeof(*node);
#endif
    GenTreePtr      node = (GenTreePtr)compGetMem(size);

#if     MEASURE_NODE_SIZE
    genNodeSizeStats.genTreeNodeCnt  += 1;
    genNodeSizeStats.genTreeNodeSize += size;
#endif

#ifdef  DEBUG
    memset(node, 0xDD, size);
#endif

    node->gtOper     = oper;
    node->gtType     = type;
    node->gtFlags    = 0;
#if TGT_x86
    node->gtUsedRegs = 0;
#endif
#if CSE
    node->gtCSEnum   = NO_CSE;
#endif
    node->gtNext     = NULL;

#ifdef DEBUG
#if     SMALL_TREE_NODES
    if      (size == TREE_NODE_SZ_SMALL)
    {
        node->gtFlags |= GTF_NODE_SMALL;
    }
    else if (size == TREE_NODE_SZ_LARGE)
    {
        node->gtFlags |= GTF_NODE_LARGE;
    }
    else
        assert(!"bogus node size");
#endif
    node->gtPrev     = NULL;
    node->gtSeqNum   = 0;
#endif

    return node;
}

#endif

GenTreePtr Compiler::gtNewCommaNode  (GenTreePtr     op1,
                                      GenTreePtr     op2)
{
    GenTreePtr      node = gtNewOperNode(GT_COMMA,
                                         op2->gtType, 
                                         op1,
                                         op2);
    
#ifdef DEBUG
    if (compStressCompile(STRESS_REVERSECOMMA, 60))
    {           
        GenTreePtr comma = gtNewOperNode(GT_COMMA,
                                            op2->gtType, 
                                            gtNewNothingNode(),
                                            node);
        comma->gtFlags |= GTF_REVERSE_OPS;

        comma->gtOp.gtOp1->gtFlags |= GTF_SIDE_EFFECT;
        return comma;
    }
    else
#endif
    {
        return node;
    }
}


GenTreePtr FASTCALL Compiler::gtNewOperNode(genTreeOps oper,
                                            var_types  type, GenTreePtr op1,
                                                             GenTreePtr op2)
{
    GenTreePtr      node = gtNewNode(oper, type);
        
    node->gtOp.gtOp1 = op1;
    node->gtOp.gtOp2 = op2;

    if  (op1) node->gtFlags |= op1->gtFlags & GTF_GLOB_EFFECT;
    if  (op2) node->gtFlags |= op2->gtFlags & GTF_GLOB_EFFECT;
    
    return node;
}


GenTreePtr FASTCALL Compiler::gtNewIconNode(long value, var_types type)
{
    GenTreePtr      node = gtNewNode(GT_CNS_INT, type);

    node->gtIntCon.gtIconVal = value;

    return node;
}


/*****************************************************************************
 *
 *  Allocates a integer constant entry that represents a HANDLE to something.
 *  It may not be allowed to embed HANDLEs directly into the JITed code (for eg,
 *  as arguments to JIT helpers). Get a corresponding value that can be embedded.
 *  If the handle needs to be accessed via an indirection, pValue points to it.
 */

GenTreePtr          Compiler::gtNewIconEmbHndNode(void *       value,
                                                  void *       pValue,
                                                  unsigned     flags,
                                                  unsigned     handle1,
                                                  void *       handle2)
{
    GenTreePtr      node;

    assert((!value) != (!pValue));

    if (value)
    {
        node = gtNewIconHandleNode((long)value, flags, handle1, handle2);
    }
    else
    {
        node = gtNewIconHandleNode((long)pValue, flags, handle1, handle2);
        node = gtNewOperNode(GT_IND, TYP_I_IMPL, node);
    }

    return node;
}

/*****************************************************************************/

GenTreePtr FASTCALL Compiler::gtNewLconNode(__int64 value)
{
    GenTreePtr      node = gtNewNode(GT_CNS_LNG, TYP_LONG);

    node->gtLngCon.gtLconVal = value;

    return node;
}


GenTreePtr FASTCALL Compiler::gtNewDconNode(double value)
{
    GenTreePtr      node = gtNewNode(GT_CNS_DBL, TYP_DOUBLE);
    node->gtDblCon.gtDconVal = value;
    return node;
}


GenTreePtr          Compiler::gtNewSconNode(int CPX, CORINFO_MODULE_HANDLE scpHandle)
{

#if SMALL_TREE_NODES

    /* 'GT_CNS_STR' nodes later get transformed into 'GT_CALL' */

    assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_CNS_STR]);

    GenTreePtr      node = gtNewNode(GT_CALL, TYP_REF);
    node->SetOper(GT_CNS_STR);
#else
    GenTreePtr      node = gtNewNode(GT_CNS_STR, TYP_REF);
#endif

    node->gtStrCon.gtSconCPX = CPX;

    /* Because this node can come from an inlined method we need to
     * have the scope handle since it will become a helper call */

    node->gtStrCon.gtScpHnd = scpHandle;

    return node;
}


GenTreePtr          Compiler::gtNewZeroConNode(var_types type)
{
    switch(type)
    {
        GenTreePtr      zero;

    case TYP_INT:       return gtNewIconNode(0);
    case TYP_BYREF:
    case TYP_REF:       zero = gtNewIconNode(0);
                        zero->gtType = type;
                        return zero;
    case TYP_LONG:      return gtNewLconNode(0);
    case TYP_FLOAT:     
    case TYP_DOUBLE:    return gtNewDconNode(0.0);
    default:            assert(!"Bad type");
                        return NULL;
    }
}


GenTreePtr          Compiler::gtNewCallNode(gtCallTypes   callType,
                                            CORINFO_METHOD_HANDLE callHnd,
                                            var_types     type,
                                            GenTreePtr    args)
{
    GenTreePtr      node = gtNewNode(GT_CALL, type);

    node->gtFlags               |= GTF_CALL;
    if (args)
        node->gtFlags           |= (args->gtFlags & GTF_GLOB_EFFECT);
    node->gtCall.gtCallType      = callType;
    node->gtCall.gtCallMethHnd   = callHnd;
    node->gtCall.gtCallArgs      = args;
    node->gtCall.gtCallObjp      = NULL;
    node->gtCall.gtCallMoreFlags = 0;
    node->gtCall.gtCallCookie    = NULL;
    node->gtCall.gtCallRegArgs   = 0;

    return node;
}

GenTreePtr FASTCALL Compiler::gtNewLclvNode(unsigned   lnum,
                                            var_types  type,
                                            IL_OFFSETX ILoffs)
{
    GenTreePtr      node = gtNewNode(GT_LCL_VAR, type);

    /* Cannot have this assert because the inliner uses this function
     * to add temporaries */

    //assert(lnum < lvaCount);

    node->gtLclVar.gtLclNum     = lnum;
    node->gtLclVar.gtLclILoffs  = ILoffs;

    /* If the var is aliased, treat it as a global reference.
       NOTE : This is an overly-conservative approach - functions which
       dont take any byref arguments cannot modify aliased vars. */

    if (lvaTable[lnum].lvAddrTaken)
        node->gtFlags |= GTF_GLOB_REF;

    return node;
}

#if INLINING

GenTreePtr FASTCALL Compiler::gtNewLclLNode(unsigned   lnum,
                                            var_types  type,
                                            IL_OFFSETX ILoffs)
{
    GenTreePtr      node;

#if SMALL_TREE_NODES

    /* This local variable node may later get transformed into a large node */

//    assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_LCL_VAR]);

    node = gtNewNode(GT_CALL   , type);
    node->SetOper(GT_LCL_VAR);
#else
    node = gtNewNode(GT_LCL_VAR, type);
#endif

    node->gtLclVar.gtLclNum     = lnum;
    node->gtLclVar.gtLclILoffs  = ILoffs;

    /* If the var is aliased, treat it as a global reference.
       NOTE : This is an overly-conservative approach - functions which
       dont take any byref arguments cannot modify aliased vars. */

    if (lvaTable[lnum].lvAddrTaken)
        node->gtFlags |= GTF_GLOB_REF;

    return node;
}

#endif

/*****************************************************************************
 *
 *  Create a list out of one value.
 */

GenTreePtr          Compiler::gtNewArgList(GenTreePtr op)
{
    return  gtNewOperNode(GT_LIST, TYP_VOID, op, 0);
}

/*****************************************************************************
 *
 *  Create a list out of the two values.
 */

GenTreePtr          Compiler::gtNewArgList(GenTreePtr op1, GenTreePtr op2)
{
    GenTreePtr      tree;

    tree = gtNewOperNode(GT_LIST, TYP_VOID, op2, 0);
    tree = gtNewOperNode(GT_LIST, TYP_VOID, op1, tree);

    return tree;
}

/*****************************************************************************
 *
 *  Create a node that will assign 'src' to 'dst'.
 */

GenTreePtr FASTCALL Compiler::gtNewAssignNode(GenTreePtr dst, GenTreePtr src)
{
    GenTreePtr      asg;

    /* Mark the target as being assigned */

    if  (dst->gtOper == GT_LCL_VAR) 
        dst->gtFlags |= GTF_VAR_DEF;

    dst->gtFlags |= GTF_DONT_CSE;

    /* Create the assignment node */

    asg = gtNewOperNode(GT_ASG, dst->gtType, dst, src);

    /* Mark the expression as containing an assignment */

    asg->gtFlags |= GTF_ASG;

    return asg;
}

/*****************************************************************************
 *
 *  Clones the given tree value and returns a copy of the given tree.
 *  If 'complexOK' is false, the cloning is only done provided the tree
 *     is not too complex (whatever that may mean);
 *  If 'complexOK' is true, we try slightly harder to clone the tree.
 *  In either case, NULL is returned if the tree cannot be cloned
 *
 *  Note that there is the function gtCloneExpr() which does a more
 *  complete job if you can't handle this function failing.
 */

GenTreePtr          Compiler::gtClone(GenTree * tree, bool complexOK)
{
    GenTreePtr  copy;

    switch (tree->gtOper)
    {
    case GT_CNS_INT:

#if defined(JIT_AS_COMPILER) || defined (LATE_DISASM)
        if (tree->gtFlags & GTF_ICON_HDL_MASK)
            copy = gtNewIconHandleNode(tree->gtIntCon.gtIconVal,
                                       tree->gtFlags,
                                       tree->gtIntCon.gtIconHdl.gtIconHdl1,
                                       tree->gtIntCon.gtIconHdl.gtIconHdl2);
        else
#endif
            copy = gtNewIconNode(tree->gtIntCon.gtIconVal, tree->gtType);
        break;

    case GT_LCL_VAR:
        copy = gtNewLclvNode(tree->gtLclVar.gtLclNum , tree->gtType,
                             tree->gtLclVar.gtLclILoffs);
        break;

    case GT_LCL_FLD:
        copy = gtNewOperNode(GT_LCL_FLD, tree->TypeGet());
        copy->gtOp = tree->gtOp;
        break;

    case GT_CLS_VAR:
        copy = gtNewClsvNode(tree->gtClsVar.gtClsVarHnd, tree->gtType);
        break;

    case GT_REG_VAR:
        assert(!"clone regvar");

    default:
        if  (!complexOK)
            return NULL;

        if  (tree->gtOper == GT_FIELD)
        {
            GenTreePtr  objp;

            // copied from line 9850

            objp = 0;
            if  (tree->gtField.gtFldObj)
            {
                objp = gtClone(tree->gtField.gtFldObj, false);
                if  (!objp)
                    return  objp;
            }

            copy = gtNewFieldRef(tree->TypeGet(),
                                 tree->gtField.gtFldHnd,
                                 objp);

#if HOIST_THIS_FLDS
            copy->gtField.gtFldHTX = tree->gtField.gtFldHTX;
#endif
        }
        else if  (tree->gtOper == GT_ADD)
        {
            GenTreePtr  op1 = tree->gtOp.gtOp1;
            GenTreePtr  op2 = tree->gtOp.gtOp2;

            if  (op1->OperIsLeaf() &&
                 op2->OperIsLeaf())
            {
                op1 = gtClone(op1);
                if (op1 == 0)
                    return 0;
                op2 = gtClone(op2);
                if (op2 == 0)
                    return 0;

                copy =  gtNewOperNode(GT_ADD, tree->TypeGet(), op1, op2);
            }
            else
            {
                return NULL;
            }
        }
        else if (tree->gtOper == GT_ADDR)
        {
            GenTreePtr  op1 = gtClone(tree->gtOp.gtOp1);
            if (op1 == 0)
                return NULL;
            copy = gtNewOperNode(GT_ADDR, tree->TypeGet(), op1);
        }
        else
        {
            return NULL;
        }

        break;
    }

    copy->gtFlags |= tree->gtFlags & ~GTF_NODE_MASK;
    return copy;
}

/*****************************************************************************
 *
 *  Clones the given tree value and returns a copy of the given tree. Any
 *  references to local variable varNum will be replaced with the integer
 *  constant varVal.
 */

GenTreePtr          Compiler::gtCloneExpr(GenTree * tree,
                                          unsigned  addFlags,
                                          unsigned  varNum,
                                          long      varVal)
{
    if (tree == NULL)
        return NULL;

    /* Figure out what kind of a node we have */

    genTreeOps      oper = tree->OperGet();
    unsigned        kind = tree->OperKind();
    GenTree *       copy;

    /* Is this a constant or leaf node? */

    if  (kind & (GTK_CONST|GTK_LEAF))
    {
        switch (oper)
        {
        case GT_CNS_INT:

#if defined(JIT_AS_COMPILER) || defined (LATE_DISASM)
            if  (tree->gtFlags & GTF_ICON_HDL_MASK)
            {
                copy = gtNewIconHandleNode(tree->gtIntCon.gtIconVal,
                                           tree->gtFlags,
                                           tree->gtIntCon.gtIconCPX,
                                           tree->gtIntCon.gtIconCls);

            }
            else
#endif
            {
                copy = gtNewIconNode      (tree->gtIntCon.gtIconVal,
                                           tree->gtType);
            }

            goto DONE;

        case GT_CNS_LNG:
            copy = gtNewLconNode(tree->gtLngCon.gtLconVal);
            goto DONE;

        case GT_CNS_DBL:
            copy = gtNewDconNode(tree->gtDblCon.gtDconVal);
            copy->gtType = tree->gtType;    // keep the same type  
            goto DONE;

        case GT_CNS_STR:
            copy = gtNewSconNode(tree->gtStrCon.gtSconCPX, tree->gtStrCon.gtScpHnd);
            goto DONE;

        case GT_LCL_VAR:

            if  (tree->gtLclVar.gtLclNum == varNum)
                copy = gtNewIconNode(varVal, tree->gtType);
            else
            {
                copy = gtNewLclvNode(tree->gtLclVar.gtLclNum , tree->gtType,
                                     tree->gtLclVar.gtLclILoffs);
            }

            goto DONE;

        case GT_LCL_FLD:
            copy = gtNewOperNode(GT_LCL_FLD, tree->TypeGet());
            copy->gtOp = tree->gtOp;
            goto DONE;

        case GT_CLS_VAR:
            copy = gtNewClsvNode(tree->gtClsVar.gtClsVarHnd, tree->gtType);

            goto DONE;

        case GT_REG_VAR:
            assert(!"regvar should never occur here");

        default:
            assert(!"unexpected leaf/const");

        case GT_NO_OP:
        case GT_BB_QMARK:
        case GT_CATCH_ARG:
        case GT_LABEL:
        case GT_END_LFIN:
        case GT_JMP:
            copy = gtNewOperNode(oper, tree->gtType);
            copy->gtVal = tree->gtVal;

            goto DONE;
        }
    }

    /* Is it a 'simple' unary/binary operator? */

    if  (kind & GTK_SMPOP)
    {
        /* If necessary, make sure we allocate a "fat" tree node */

#if SMALL_TREE_NODES
        switch (oper)
        {
        case GT_MUL:
        case GT_DIV:
        case GT_MOD:
        case GT_CAST:
        
        case GT_UDIV:
        case GT_UMOD:

            /* These nodes sometimes get bashed to "fat" ones */

            copy = gtNewLargeOperNode(oper, tree->TypeGet());
            copy->gtLargeOp = tree->gtLargeOp;
            break;

        default:
            copy = gtNewOperNode(oper, tree->TypeGet());
            if (GenTree::s_gtNodeSizes[oper] == TREE_NODE_SZ_SMALL)
                copy->gtOp = tree->gtOp;
            else
                copy->gtLargeOp = tree->gtLargeOp;
           break;
        }
#else
        copy = gtNewOperNode(oper, tree->TypeGet());
        copy->gtLargeOp = tree->gtLargeOp;
#endif

        /* Some unary/binary nodes have extra GenTreePtr fields */

        switch (oper)
        {
        case GT_IND:

#if CSELENGTH

            if  (tree->gtOper == GT_IND && tree->gtInd.gtIndLen)
            {
                if  (tree->gtFlags & GTF_IND_RNGCHK)
                {
                    copy->gtInd.gtIndRngFailBB = gtCloneExpr(tree->gtInd.gtIndRngFailBB, addFlags, varNum, varVal);

                    GenTreePtr      len = tree->gtInd.gtIndLen;
                    GenTreePtr      tmp;

                    GenTreePtr      gtSaveCopyVal;
                    GenTreePtr      gtSaveCopyNew;

                    /* Make sure the array length value looks reasonable */

                    assert(len->gtOper == GT_ARR_LENREF);

                    /* Clone the array length subtree */

                    copy->gtInd.gtIndLen = tmp = gtCloneExpr(len, addFlags, varNum, varVal);

                    /*
                        When we clone the operand, we take care to find
                        the copied array address.
                     */

                    gtSaveCopyVal = gtCopyAddrVal;
                    gtSaveCopyNew = gtCopyAddrNew;

                    gtCopyAddrVal = len->gtArrLen.gtArrLenAdr;
#ifdef DEBUG
                    gtCopyAddrNew = (GenTreePtr)-1;
#endif
                    copy->gtInd.gtIndOp1 = gtCloneExpr(tree->gtInd.gtIndOp1, addFlags, varNum, varVal);
#ifdef DEBUG
                    assert(gtCopyAddrNew != (GenTreePtr)-1);
#endif
                    tmp->gtArrLen.gtArrLenAdr = gtCopyAddrNew;

                    gtCopyAddrVal = gtSaveCopyVal;
                    gtCopyAddrNew = gtSaveCopyNew;

                    goto DONE;
                }
            }

#endif // CSELENGTH

            break;
        }

        if  (tree->gtOp.gtOp1)
            copy->gtOp.gtOp1 = gtCloneExpr(tree->gtOp.gtOp1, addFlags, varNum, varVal);

        if  (tree->gtGetOp2())
            copy->gtOp.gtOp2 = gtCloneExpr(tree->gtOp.gtOp2, addFlags, varNum, varVal);

        
        /* Flags */
        addFlags |= tree->gtFlags;

        #ifdef  DEBUG
        /* GTF_NODE_MASK should not be propagated from 'tree' to 'copy' */
        addFlags &= ~GTF_NODE_MASK;
        #endif

        copy->gtFlags |= addFlags;

        /* Try to do some folding */
        copy = gtFoldExpr(copy);

        goto DONE;
    }

    /* See what kind of a special operator we have here */

    switch  (oper)
    {
    case GT_STMT:
        copy = gtCloneExpr(tree->gtStmt.gtStmtExpr, addFlags, varNum, varVal);
        copy = gtNewStmt(copy, tree->gtStmtILoffsx);
        goto DONE;

    case GT_CALL:

        copy = gtNewOperNode(oper, tree->TypeGet());

        copy->gtCall.gtCallObjp     = tree->gtCall.gtCallObjp    ? gtCloneExpr(tree->gtCall.gtCallObjp,    addFlags, varNum, varVal) : 0;
        copy->gtCall.gtCallArgs     = tree->gtCall.gtCallArgs    ? gtCloneExpr(tree->gtCall.gtCallArgs,    addFlags, varNum, varVal) : 0;
        copy->gtCall.gtCallMoreFlags= tree->gtCall.gtCallMoreFlags;
        copy->gtCall.gtCallRegArgs  = tree->gtCall.gtCallRegArgs ? gtCloneExpr(tree->gtCall.gtCallRegArgs, addFlags, varNum, varVal) : 0;
        copy->gtCall.regArgEncode   = tree->gtCall.regArgEncode;
        copy->gtCall.gtCallType     = tree->gtCall.gtCallType;
        copy->gtCall.gtCallCookie   = tree->gtCall.gtCallCookie  ? gtCloneExpr(tree->gtCall.gtCallCookie,  addFlags, varNum, varVal) : 0;

        /* Copy the union */

        if (tree->gtCall.gtCallType == CT_INDIRECT)
            copy->gtCall.gtCallAddr = tree->gtCall.gtCallAddr    ? gtCloneExpr(tree->gtCall.gtCallAddr,    addFlags, varNum, varVal) : 0;
        else
            copy->gtCall.gtCallMethHnd = tree->gtCall.gtCallMethHnd;

        goto DONE;

    case GT_FIELD:

        copy = gtNewFieldRef(tree->TypeGet(),
                             tree->gtField.gtFldHnd,
                             0);

        copy->gtField.gtFldObj  = tree->gtField.gtFldObj  ? gtCloneExpr(tree->gtField.gtFldObj , addFlags, varNum, varVal) : 0;

#if HOIST_THIS_FLDS
        copy->gtField.gtFldHTX  = tree->gtField.gtFldHTX;
#endif

        goto DONE;

#if CSELENGTH

    case GT_ARR_LENREF:

        copy = gtNewOperNode(GT_ARR_LENREF, tree->TypeGet());
        copy->gtSetArrLenOffset(tree->gtArrLenOffset());

        /* The range check can throw an exception */

        copy->gtFlags |= GTF_EXCEPT;

        /*  Note: if we're being cloned as part of an GT_IND expression,
            gtArrLenAdr will be filled in when the GT_IND is cloned. If
            we're the root of the tree being copied, though, we need to
            make a copy of the address expression.
         */

        copy->gtArrLen.gtArrLenCse = tree->gtArrLen.gtArrLenCse ? gtCloneExpr(tree->gtArrLen.gtArrLenCse, addFlags, varNum, varVal) : 0;
        copy->gtArrLen.gtArrLenAdr = NULL;

        if  (tree->gtFlags & GTF_ALN_CSEVAL)
        {
            assert(tree->gtArrLen.gtArrLenAdr);
            copy->gtArrLen.gtArrLenAdr = gtCloneExpr(tree->gtArrLen.gtArrLenAdr, addFlags, varNum, varVal);
        }

        goto DONE;

#endif

    case GT_ARR_ELEM:

        copy = gtNewOperNode(oper, tree->TypeGet());

        copy->gtArrElem.gtArrObj        = gtCloneExpr(tree->gtArrElem.gtArrObj, addFlags, varNum, varVal);

        copy->gtArrElem.gtArrRank       = tree->gtArrElem.gtArrRank;
        copy->gtArrElem.gtArrElemSize   = tree->gtArrElem.gtArrElemSize;
        copy->gtArrElem.gtArrElemType   = tree->gtArrElem.gtArrElemType;

        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
            copy->gtArrElem.gtArrInds[dim] = gtCloneExpr(tree->gtArrElem.gtArrInds[dim], addFlags, varNum, varVal);

        break;


    default:
#ifdef  DEBUG
        gtDispTree(tree);
#endif
        NO_WAY_RET("unexpected operator", GenTreePtr);
    }

DONE:

    /* We assume the FP stack level will be identical */

#if TGT_x86
    copy->gtFPlvl = tree->gtFPlvl;
#endif

    /* Compute the flags for the copied node. Note that we can do this only
       if we didnt gtFoldExpr(copy) */

    if (copy->gtOper == oper)
    {
        addFlags |= tree->gtFlags;

#ifdef  DEBUG
        /* GTF_NODE_MASK should not be propagated from 'tree' to 'copy' */
        addFlags &= ~GTF_NODE_MASK;
#endif

        copy->gtFlags |= addFlags;
    }

    /* GTF_COLON_COND should be propagated from 'tree' to 'copy' */
    copy->gtFlags |= (tree->gtFlags & GTF_COLON_COND);                        

#if CSELENGTH

    if  (tree == gtCopyAddrVal)
        gtCopyAddrNew = copy;

#endif

    /* Make sure to copy back fields that may have been initialized */

    copy->gtCostEx   = tree->gtCostEx;
    copy->gtCostSz   = tree->gtCostSz;
    copy->gtRsvdRegs = tree->gtRsvdRegs;

    return  copy;
}


/*****************************************************************************/
#ifdef DEBUG
/*****************************************************************************/

// static
void                GenTree::gtDispFlags(unsigned flags)
{
    printf("%c", (flags & GTF_ASG           ) ? 'A' : '-');
    printf("%c", (flags & GTF_CALL          ) ? 'C' : '-');
    printf("%c", (flags & GTF_EXCEPT        ) ? 'X' : '-');
    printf("%c", (flags & GTF_GLOB_REF      ) ? 'G' : '-');
    printf("%c", (flags & GTF_OTHER_SIDEEFF ) ? 'O' : '-');
    printf("%c", (flags & GTF_COLON_COND    ) ? '?' : '-');
    printf("%c", (flags & GTF_DONT_CSE      ) ? 'N' : '-');
    printf("%c", (flags & GTF_REVERSE_OPS   ) ? 'R' : '-');
    printf("%c", (flags & GTF_UNSIGNED      ) ? 'U' :
                 (flags & GTF_BOOLEAN       ) ? 'B' : '-');
}

/*****************************************************************************/

//  "tree" may be NULL.

void                Compiler::gtDispNode(GenTree    *   tree,
                                         unsigned       indent,
                                         bool           terse,
                                         char       *   msg)
{
    //Sleep(5);
    
    /* Indent the node accordingly */
    unsigned nodeIndent = indent*INDENT_SIZE;

    GenTree *  prev;

    if  (tree->gtSeqNum)
        printf("N%03d (%2d,%2d) ", tree->gtSeqNum, tree->gtCostEx, tree->gtCostSz);
    else if (tree->gtOper == GT_STMT)
    {
        prev = tree->gtOp.gtOp1;
        goto STMT_SET_PREV;
    }
    else
    {
        int dotNum;
        prev = tree;
STMT_SET_PREV:
        dotNum = 0;

        do {
            dotNum++;
            prev = prev->gtPrev;

            if ((prev == NULL) || (prev == tree))
                goto NO_SEQ_NUM;

            assert(prev);
        } while (prev->gtSeqNum == 0);

        if (tree->gtOper != GT_STMT)
            printf("N%03d.%d       ", prev->gtSeqNum, dotNum);
        else
NO_SEQ_NUM:
            printf("             ");
    }

    if  (nodeIndent)
        printIndent(nodeIndent);

    /* Print the node address */

    printf("[%08X] ", tree);

    if  (tree)
    {
        /* Print the flags associated with the node */

        switch (tree->gtOper)
        {
        case GT_IND:
            if      (tree->gtFlags & GTF_IND_INVARIANT) { printf("I"); break; }
            else goto DASH;

        case GT_LCL_FLD:
        case GT_LCL_VAR:
        case GT_REG_VAR:
            if      (tree->gtFlags & GTF_VAR_USEASG)   { printf("U"); break; }
            else if (tree->gtFlags & GTF_VAR_USEDEF)   { printf("B"); break; }
            else if (tree->gtFlags & GTF_VAR_DEF)      { printf("D"); break; }
            // Fall through

        default:
DASH:
            printf("-");
            break;
        }

        if (!terse)
            GenTree::gtDispFlags(tree->gtFlags);

#if TGT_x86
        if  (((BYTE)tree->gtFPlvl == 0xDD) || ((BYTE)tree->gtFPlvl == 0x00))
            printf("-");
        else
            printf("%1u", tree->gtFPlvl);

#else
        if  ((unsigned char)tree->gtTempRegs == 0xDD)
            printf(ch);
        else
            printf("%1u", tree->gtTempRegs);
#endif
    }

    /* print the msg associated with the node */

    if (msg == 0)
        msg = "";

    printf(" %-11s ", msg);

    /* print the node name */

    const char * name;

    assert(tree);
    if (tree->gtOper < GT_COUNT)
        name = GenTree::NodeName(tree->OperGet());
    else
        name = "<ERROR>";

    char    buf[32];
    char *  bufp     = &buf[0];

    if ((tree->gtOper == GT_CNS_INT) && (tree->gtFlags & GTF_ICON_HDL_MASK))
    {
        sprintf(bufp, " %s(h)%c", name, 0);
    }
    else if (tree->gtOper == GT_CALL)
    {
        char *  callType = "call";
        char *  gtfType  = "";
        char *  ctType   = "";

        if (tree->gtCall.gtCallType == CT_USER_FUNC)
        {
            if (tree->gtFlags & (GTF_CALL_VIRT | GTF_CALL_VIRT_RES))
              callType = "callv";
        }
        else if (tree->gtCall.gtCallType == CT_HELPER)
            ctType  = " help";
        else if (tree->gtCall.gtCallType == CT_INDIRECT)
            ctType  = " ind";
        else
            assert(!"Unknown gtCallType");

        if (tree->gtFlags & GTF_CALL_INTF)
            gtfType = " intf";
        else if (tree->gtFlags & GTF_CALL_VIRT_RES)
            gtfType = " dir";
        else if (tree->gtFlags & GTF_CALL_VIRT)
            gtfType = " ind";
        else if (tree->gtFlags & GTF_CALL_UNMANAGED)
            gtfType = " unman";
        else if (tree->gtCall.gtCallMoreFlags & GTF_CALL_M_TAILREC)
            gtfType = " tail";

        sprintf(bufp, "%s%s%s%c", callType, ctType, gtfType, 0);
    }
    else if (tree->gtOper == GT_ARR_ELEM)
    {
        bufp += sprintf(bufp, " %s[", name);
        for(unsigned rank = tree->gtArrElem.gtArrRank-1; rank; rank--)
            bufp += sprintf(bufp, ",");
        sprintf(bufp, "]");
    }
    else if (tree->gtOverflowEx())
    {
        sprintf(bufp, " %s_ovfl%c", name, 0);
    }
    else
    {
        sprintf(bufp, " %s%c", name, 0);
    }

    if (strlen(buf) < 10)
        printf(" %-10s", buf);
    else
        printf(" %s", buf);

    assert(tree == 0 || tree->gtOper < GT_COUNT);

    if  (tree)
    {
        /* print the type of the node */

        if (tree->gtOper == GT_ARR_LENREF)
        {
            if (tree->gtFlags & GTF_ALN_CSEVAL)
                printf("array length CSE def\n");
            else
                printf(" array=[%08X]\n", tree->gtArrLen.gtArrLenAdr);
            return;
        }
        else if (tree->gtOper != GT_CAST) 
        {
            printf(" %-6s", varTypeName(tree->TypeGet()));

            if (tree->gtOper == GT_STMT && opts.compDbgInfo)
            {
                IL_OFFSET startIL = jitGetILoffs(tree->gtStmtILoffsx);
                IL_OFFSET endIL = tree->gtStmt.gtStmtLastILoffs;

                startIL = (startIL == BAD_IL_OFFSET) ? 0xFFF : startIL;
                endIL   = (endIL   == BAD_IL_OFFSET) ? 0xFFF : endIL;

                printf("(IL %03Xh...%03Xh)", startIL, endIL);
            }
        }

        // for tracking down problems in reguse prediction or liveness tracking

        if (verbose&&0)
        {
            printf(" RR="); dspRegMask(tree->gtRsvdRegs);
            printf(",UR="); dspRegMask(tree->gtUsedRegs);
            printf(",LS=%s", genVS2str(tree->gtLiveSet));
        }
    }
}

void                Compiler::gtDispRegVal(GenTree *  tree)
{
    if  (tree->gtFlags & GTF_REG_VAL)
    {
#if TGT_x86
        if (tree->gtType == TYP_LONG)
            printf(" %s", compRegPairName(tree->gtRegPair));
        else
#endif
            printf(" %s", compRegVarName(tree->gtRegNum));
    }
    printf("\n");
}

/*****************************************************************************/
void                Compiler::gtDispLclVar(unsigned lclNum) 
{
    printf("V%02u (", lclNum);

    const char* ilKind;
    unsigned     ilNum  = compMap2ILvarNum(lclNum);
    if (ilNum == RETBUF_ILNUM)
    {
        printf("retb)  ");
        return;
    }
    else if (ilNum == VARG_ILNUM)
    {
        printf("varg)  ");
        return;
    }
    else if (ilNum == UNKNOWN_ILNUM)
    {
        if (lclNum < optCSEstart)
        {
            ilKind = "tmp";
            ilNum  = lclNum - info.compLocalsCount;
        }
        else
        {
            ilKind = "cse";
            ilNum  = lclNum - optCSEstart;
        }
    }
    else if (lvaTable[lclNum].lvIsParam)
    {
        ilKind = "arg";
        if (ilNum == 0 && !info.compIsStatic)
        {
            printf("this)  ");
            return;
        }
    }
    else
    {
        ilKind  = "loc";
        ilNum  -= info.compILargsCount;
    }
    printf("%s%d) ", ilKind, ilNum);
    if (ilNum < 10)
        printf(" ");
}

/*****************************************************************************/

void                Compiler::gtDispTree(GenTree *   tree,
                                         unsigned    indent,   /* = 0     */
                                         char *      msg,      /* = NULL  */
                                         bool        topOnly)  /* = false */
{
    unsigned        kind;

    if  (tree == 0)
    {
        printIndent(indent*INDENT_SIZE);
        printf(" [%08X] <NULL>\n", tree);
        printf("");         // null string means flush
        return;
    }

    assert((int)tree != 0xDDDDDDDD);    /* Value used to initalize nodes */

    if  (tree->gtOper >= GT_COUNT)
    {
        gtDispNode(tree, indent, topOnly, msg); assert(!"bogus operator");
    }

    kind = tree->OperKind();

    /* Is tree a constant node? */

    if  (kind & GTK_CONST)
    {
        gtDispNode(tree, indent, topOnly, msg);

        switch  (tree->gtOper)
        {
        case GT_CNS_INT:
            if ((tree->gtFlags & GTF_ICON_HDL_MASK) == GTF_ICON_STR_HDL)
            {
               printf(" 0x%X \"%S\"", tree->gtIntCon.gtIconVal, eeGetCPString(tree->gtIntCon.gtIconVal));
            }
            else
            {
                if (tree->TypeGet() == TYP_REF)
                {
                    assert(tree->gtIntCon.gtIconVal == 0);
                    printf(" null");
                }
                else if ((tree->gtIntCon.gtIconVal > -1000) && (tree->gtIntCon.gtIconVal < 1000))
                    printf(" %ld",  tree->gtIntCon.gtIconVal);
                else
                    printf(" 0x%X", tree->gtIntCon.gtIconVal);

                unsigned hnd = (tree->gtFlags & GTF_ICON_HDL_MASK) >> 28;

                switch (hnd)
                {
                default:
                    break;
                case 1:
                    printf(" scope");
                    break;
                case 2:
                    printf(" class");
                    break;
                case 3:
                    printf(" method");
                    break;
                case 4:
                    printf(" field");
                    break;
                case 5:
                    printf(" static");
                    break;
                case 7:
                    printf(" pstr");
                    break;
                case 8:
                    printf(" ptr");
                    break;
                case 9:
                    printf(" vararg");
                    break;
                case 10:
                    printf(" pinvoke");
                    break;
                case 11:
                    printf(" token");
                    break;
                case 12:
                    printf(" tls");
                    break;
                case 13:
                    printf(" ftn");
                    break;
                case 14:
                    printf(" cid");
                    break;
                }

            }
            break;
        case GT_CNS_LNG: 
            printf(" %I64d", tree->gtLngCon.gtLconVal); 
            break;
        case GT_CNS_DBL:
            if (*((__int64 *)&tree->gtDblCon.gtDconVal) == 0x8000000000000000)
                printf(" -0.00000");
            else
                printf(" %#lg", tree->gtDblCon.gtDconVal); 
            break;
        case GT_CNS_STR: {
            unsigned strHandle, *pStrHandle;
            strHandle = eeGetStringHandle(tree->gtStrCon.gtSconCPX,
                tree->gtStrCon.gtScpHnd, &pStrHandle);
            if (strHandle != 0)
               printf("'%S'", strHandle, eeGetCPString(strHandle));
            else
               printf(" <UNKNOWN STR> ");
            }
            break;
        default: assert(!"unexpected constant node");
        }
        gtDispRegVal(tree);
        return;
    }

    /* Is tree a leaf node? */

    if  (kind & GTK_LEAF)
    {
        gtDispNode(tree, indent, topOnly, msg);

        switch  (tree->gtOper)
        {
        unsigned        varNum;
        LclVarDsc *     varDsc;
        VARSET_TP       varBit;

        case GT_LCL_VAR:
            printf(" ");
            gtDispLclVar(tree->gtLclVar.gtLclNum);
            goto LCL_COMMON;

        case GT_LCL_FLD:
            printf(" ");
            gtDispLclVar(tree->gtLclVar.gtLclNum);
            printf("[+%u]", tree->gtLclFld.gtLclOffs);

LCL_COMMON:

            varNum = tree->gtLclVar.gtLclNum;
            varDsc = &lvaTable[varNum];
            varBit = genVarIndexToBit(varDsc->lvVarIndex);

            if (varDsc->lvRegister)
            {
                if (isRegPairType(varDsc->TypeGet()))
                    printf(" %s:%s", getRegName(varDsc->lvOtherReg),  // hi32
                                     getRegName(varDsc->lvRegNum));   // lo32
                else
                    printf(" %s", getRegName(varDsc->lvRegNum));
            }
            if ((varDsc->lvTracked) &&
                ( varBit & lvaVarIntf[varDsc->lvVarIndex]) && // has liveness been computed?
                ((varBit & tree->gtLiveSet) == 0))
            {
                printf(" (last use)");
            }

#if 0
            /* BROKEN: This always just prints (null) */

            if  (info.compLocalVarsCount>0 && compCurBB)
            {
                unsigned        blkBeg = compCurBB->bbCodeOffs;
                unsigned        blkEnd = compCurBB->bbCodeSize + blkBeg;

                unsigned        i;
                LocalVarDsc *   t;

                for (i = 0, t = info.compLocalVars;
                     i < info.compLocalVarsCount;
                     i++  , t++)
                {
                    if  (t->lvdVarNum  != varNum)
                        continue;
                    if  (t->lvdLifeBeg >= blkEnd)
                        continue;
                    if  (t->lvdLifeEnd <= blkBeg)
                        continue;

                    printf(" '%s'", lvdNAMEstr(t->lvdName));
                    break;
                }
            }
#endif
            break;

        case GT_REG_VAR:
            printf(" ");
            gtDispLclVar(tree->gtRegVar.gtRegVar);
            if  (isFloatRegType(tree->gtType))
                printf(" ST(%u)",            tree->gtRegVar.gtRegNum);
            else
                printf(" %s", compRegVarName(tree->gtRegVar.gtRegNum));

            varNum = tree->gtRegVar.gtRegVar;
            varDsc = &lvaTable[varNum];
            varBit = genVarIndexToBit(varDsc->lvVarIndex);

            if ((varDsc->lvTracked) &&
                ( varBit & lvaVarIntf[varDsc->lvVarIndex]) && // has liveness been computed?
                ((varBit & tree->gtLiveSet) == 0))
            {
                printf(" (last use)");
            }
#if 0
            /* BROKEN: This always just prints (null) */

            if  (info.compLocalVarsCount>0 && compCurBB)
            {
                unsigned        blkBeg = compCurBB->bbCodeOffs;
                unsigned        blkEnd = compCurBB->bbCodeSize + blkBeg;

                unsigned        i;
                LocalVarDsc *   t;

                for (i = 0, t = info.compLocalVars;
                     i < info.compLocalVarsCount;
                     i++  , t++)
                {
                    if  (t->lvdVarNum  != tree->gtRegVar.gtRegVar)
                        continue;
                    if  (t->lvdLifeBeg >  blkEnd)
                        continue;
                    if  (t->lvdLifeEnd <= blkBeg)
                        continue;

                    printf(" '%s'", lvdNAMEstr(t->lvdName));
                    break;
                }
            }
#endif
            break;

        case GT_JMP:
            const char *    methodName;
            const char *     className;

            methodName = eeGetMethodName((CORINFO_METHOD_HANDLE)tree->gtVal.gtVal1, &className);
            printf(" %s.%s\n", className, methodName);
            break;

        case GT_CLS_VAR:
            printf(" Hnd=%#x"     , tree->gtClsVar.gtClsVarHnd);
            break;

        case GT_LABEL:
            printf(" dst=BB%02u"  , tree->gtLabel.gtLabBB->bbNum);
            break;

        case GT_FTN_ADDR:
            printf(" fntAddr=%d" , tree->gtVal.gtVal1);
            break;

        case GT_END_LFIN:
            printf(" endNstLvl=%d", tree->gtVal.gtVal1);
            break;

        // Vanilla leaves. No qualifying information available. So do nothing

        case GT_NO_OP:
        case GT_RET_ADDR:
        case GT_CATCH_ARG:
        case GT_POP:
        case GT_BB_QMARK:
            break;

        default:
            assert(!"don't know how to display tree leaf node");
        }
        gtDispRegVal(tree);
        return;
    }

    /* Is it a 'simple' unary/binary operator? */

    char * childMsg = NULL;

    if  (kind & GTK_SMPOP)
    {
        if (!topOnly)
        {

#if CSELENGTH
            if  (tree->gtOper == GT_IND)
            {
                if  (tree->gtInd.gtIndLen && tree->gtFlags & GTF_IND_RNGCHK)
                {
                    gtDispTree(tree->gtInd.gtIndLen, indent + 1);
                }
            }
            else
#endif
            if  (tree->gtGetOp2())
            {
                // Label the childMsgs of the GT_COLON operator
                // op2 is the then part

                if (tree->gtOper == GT_COLON)
                    childMsg = "then";

                gtDispTree(tree->gtOp.gtOp2, indent + 1, childMsg);
            }
        }

        gtDispNode(tree, indent, topOnly, msg);

        if (tree->gtOper == GT_CAST)
        {
            /* Format a message that explains the effect of this GT_CAST */

            var_types fromType  = genActualType(tree->gtCast.gtCastOp->TypeGet());
            var_types toType    = tree->gtCast.gtCastType;
            var_types finalType = tree->TypeGet();

            /* if GTF_UNSIGNED is set then force fromType to an unsigned type */
            if (tree->gtFlags & GTF_UNSIGNED)
                fromType = genUnsignedType(fromType);

            if (finalType != toType)
                printf(" %s <-", varTypeName(finalType));

            printf(" %s <- %s", varTypeName(toType), varTypeName(fromType));
        }

        if  (tree->gtOper == GT_IND)
        {
            int         temp;

            temp = tree->gtInd.gtRngChkIndex;
            if  (temp != 0xDDDDDDDD) printf(" index=%u", temp);

            temp = tree->gtInd.gtStkDepth;
            if  (temp != 0xDDDDDDDD) printf(" stkDepth=%u", temp);
        }

        gtDispRegVal(tree);

        if  (!topOnly && tree->gtOp.gtOp1)
        {

            // Label the child of the GT_COLON operator
            // op1 is the else part

            if (tree->gtOper == GT_COLON)
                childMsg = "else";
            else if (tree->gtOper == GT_QMARK)
                childMsg = "   if"; // Note the "if" should be indented by INDENT_SIZE

            gtDispTree(tree->gtOp.gtOp1, indent + 1, childMsg);
        }

        return;
    }

    /* See what kind of a special operator we have here */

    switch  (tree->gtOper)
    {
    case GT_FIELD:
        gtDispNode(tree, indent, topOnly, msg);
        printf(" %s", eeGetFieldName(tree->gtField.gtFldHnd), 0);

        if  (tree->gtField.gtFldObj && !topOnly)
        {
            printf("\n");
            gtDispTree(tree->gtField.gtFldObj, indent + 1);
        }
        else
        {
            gtDispRegVal(tree);
        }
        break;

    case GT_CALL:
        assert(tree->gtFlags & GTF_CALL);

        gtDispNode(tree, indent, topOnly, msg);

        if (tree->gtCall.gtCallType != CT_INDIRECT)
        {
            const char *    methodName;
            const char *     className;

            methodName = eeGetMethodName(tree->gtCall.gtCallMethHnd, &className);

            printf(" %s.%s", className, methodName);
        }
        printf("\n");

        if (!topOnly)
        {
            char   buf[64];
            char * bufp;

            bufp = &buf[0];

            if  (tree->gtCall.gtCallObjp && tree->gtCall.gtCallObjp->gtOper != GT_NOP)
            {
                if (tree->gtCall.gtCallObjp->gtOper == GT_ASG)
                    sprintf(bufp, "this SETUP%c", 0);
                else
                    sprintf(bufp, "this in %s%c", compRegVarName(REG_ARG_0), 0);
                gtDispTree(tree->gtCall.gtCallObjp, indent+1, bufp);
            }

            if (tree->gtCall.gtCallArgs)
                gtDispArgList(tree, indent);

            if  (tree->gtCall.gtCallType == CT_INDIRECT)
                gtDispTree(tree->gtCall.gtCallAddr, indent+1, "calli tgt");

            if (tree->gtCall.gtCallRegArgs)
            {
                GenTreePtr regArgs = tree->gtCall.gtCallRegArgs;
                unsigned mask = tree->gtCall.regArgEncode;
                while(regArgs != 0)
                {
                    assert(regArgs->gtOper == GT_LIST);

                    regNumber argreg = regNumber(mask & 0xF);
                    unsigned   argnum;
                    if (argreg == REG_ARG_0)
                        argnum = 0;
                    else if (argreg == REG_ARG_1)
                        argnum = 1;

                    if  (tree->gtCall.gtCallObjp && (argreg == REG_ARG_0))
                        sprintf(bufp, "this in %s%c", compRegVarName(REG_ARG_0), 0);
                    else if (tree->gtCall.gtCallObjp && (argreg == REG_EAX))
                        sprintf(bufp, "unwrap in %s%c", compRegVarName(REG_EAX), 0);
                    else
                        sprintf(bufp, "arg%d in %s%c", argnum, compRegVarName(argreg), 0);
                    gtDispTree(regArgs->gtOp.gtOp1, indent+1, bufp);

                    regArgs = regArgs->gtOp.gtOp2;
                    mask >>= 4;
                }
            }
        }
        break;

    case GT_STMT:
        gtDispNode(tree, indent, topOnly, msg);
        printf("\n");

        if  (!topOnly)
            gtDispTree(tree->gtStmt.gtStmtExpr, indent + 1);
        break;

#if CSELENGTH

    case GT_ARR_LENREF:
        if (!topOnly && tree->gtArrLen.gtArrLenCse)
            gtDispTree(tree->gtArrLen.gtArrLenCse, indent + 1);

        gtDispNode(tree, indent, topOnly, msg);

        if (!topOnly && (tree->gtFlags  & GTF_ALN_CSEVAL))
            gtDispTree(tree->gtArrLen.gtArrLenAdr, indent + 1);
        break;

#endif

    case GT_ARR_ELEM:

        gtDispNode(tree, indent, topOnly, msg);
        printf("\n");

        gtDispTree(tree->gtArrElem.gtArrObj, indent + 1);

        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
            gtDispTree(tree->gtArrElem.gtArrInds[dim], indent + 1);

        break;

    default:
        printf("<DON'T KNOW HOW TO DISPLAY THIS NODE> :");
        gtDispNode(tree, indent, topOnly, msg);
        printf("");         // null string means flush
        break;
    }
}

/*****************************************************************************/
void                Compiler::gtDispArgList(GenTree * tree, unsigned indent)
{
    GenTree *  args     = tree->gtCall.gtCallArgs;
    unsigned   argnum   = 0;
    char       buf[16];
    char *     bufp     = &buf[0];

    if (tree->gtCall.gtCallObjp != NULL)
        argnum++;

    while(args != 0)
    {
        assert(args->gtOper == GT_LIST);
        if (args->gtOp.gtOp1->gtOper != GT_NOP)
        {
            if (args->gtOp.gtOp1->gtOper == GT_ASG)
                sprintf(bufp, "arg%d SETUP%c", argnum, 0);
            else
                sprintf(bufp, "arg%d on STK%c", argnum, 0);
            gtDispTree(args->gtOp.gtOp1, indent + 1, bufp);
        }
        args = args->gtOp.gtOp2;
        argnum++;
    }
}

void                Compiler::gtDispTreeList(GenTree * tree, unsigned indent)
{
    for (/*--*/; tree != NULL; tree = tree->gtNext)
    {
        gtDispTree(tree, indent);
        printf("\n");
    }
}

/*****************************************************************************/
#endif // DEBUG

/*****************************************************************************
 *
 *  Check if the given node can be folded,
 *  and call the methods to perform the folding
 */

GenTreePtr             Compiler::gtFoldExpr(GenTreePtr tree)
{
    unsigned        kind = tree->OperKind();

    /* We must have a simple operation to fold */

    if (!(kind & GTK_SMPOP))
        return tree;

    /* Filter out non-foldable trees that can have constant children */

    assert (kind & (GTK_UNOP | GTK_BINOP));
    switch (tree->gtOper)
    {
    case GT_RETFILT:
    case GT_RETURN:
    case GT_IND:
    case GT_NOP:
        return tree;
    }

    /* Prune any unnecessary GT_COMMA child trees */
    
    GenTreePtr  op1  = tree->gtOp.gtOp1;
    tree->gtOp.gtOp1 = op1;

    /* try to fold the current node */

    if  ((kind & GTK_UNOP) && op1)
    {
        if  (op1->OperKind() & GTK_CONST)
            return gtFoldExprConst(tree);
    }
    else if ((kind & GTK_BINOP) && op1 && tree->gtOp.gtOp2 &&
             // Don't take out conditionals for debugging
             // @TODO [CONSIDER] [04/16/01] []: find a more elegant way of doing this, because
             // we will generate some stupid code. 
             !(opts.compDbgCode && 
               tree->OperIsCompare())) 
    {
        /* Prune any unnecessary GT_COMMA child trees */
    
        GenTreePtr  op2  = tree->gtOp.gtOp2;
        tree->gtOp.gtOp2 = op2;

        if  ((op1->OperKind() & op2->OperKind()) & GTK_CONST)
        {
            /* both nodes are constants - fold the expression */
            return gtFoldExprConst(tree);
        }
        else if ((op1->OperKind() | op2->OperKind()) & GTK_CONST)
        {
            /* at least one is a constant - see if we have a
             * special operator that can use only one constant
             * to fold - e.g. booleans */

            return gtFoldExprSpecial(tree);
        }
        else if (tree->OperIsCompare())
        {
            /* comparisions of two local variables can
             * sometimes be folded */

            return gtFoldExprCompare(tree);
        }
    }

    /* Return the original node (folded/bashed or not) */

    return tree;
}

/*****************************************************************************
 *
 *  Some comparisons can be folded:
 *
 *    locA        == locA
 *    classVarA   == classVarA
 *    locA + locB == locB + locA
 *
 */

GenTreePtr          Compiler::gtFoldExprCompare(GenTreePtr tree)
{
    GenTreePtr      op1 = tree->gtOp.gtOp1;
    GenTreePtr      op2 = tree->gtOp.gtOp2;

    assert(tree->OperIsCompare());

    /* Filter out cases that cannot be folded here */

    /* Do not fold floats or doubles (e.g. NaN != Nan) */

    if  (varTypeIsFloating(op1->TypeGet()))
        return tree;

    /* Currently we can only fold when the two subtrees exactly match */

    if ((tree->gtFlags & GTF_SIDE_EFFECT) || GenTree::Compare(op1, op2, true) == false)
        return tree;                   /* return unfolded tree */

    GenTreePtr cons;

    switch (tree->gtOper)
    {
      case GT_EQ:
      case GT_LE:
      case GT_GE:
          cons = gtNewIconNode(true);   /* Folds to GT_CNS_INT(true) */
          break;

      case GT_NE:
      case GT_LT:
      case GT_GT:
          cons = gtNewIconNode(false);  /* Folds to GT_CNS_INT(false) */
          break;

      default:
          assert(!"Unexpected relOp");
          return tree;
    }

    /* The node has beeen folded into 'cons' */

    if (fgGlobalMorph)
    {
        if (!fgIsInlining())
            fgMorphTreeDone(cons);
    }
    else
    {
        cons->gtNext = tree->gtNext;
        cons->gtPrev = tree->gtPrev;
    }

    return cons;
}


/*****************************************************************************
 *
 *  Some binary operators can be folded even if they have only one
 *  operand constant - e.g. boolean operators, add with 0
 *  multiply with 1, etc
 */

GenTreePtr              Compiler::gtFoldExprSpecial(GenTreePtr tree)
{
    GenTreePtr      op1     = tree->gtOp.gtOp1;
    GenTreePtr      op2     = tree->gtOp.gtOp2;
    genTreeOps      oper    = tree->OperGet();

    GenTreePtr      op, cons;
    unsigned        val;

    assert(tree->OperKind() & GTK_BINOP);

    /* Filter out operators that cannot be folded here */
    if  ((tree->OperKind() & GTK_RELOP) || (oper == GT_CAST))
         return tree;

    /* We only consider TYP_INT for folding
     * Do not fold pointer arithmetic (e.g. addressing modes!) */

    if (oper != GT_QMARK && tree->gtType != TYP_INT)
        return tree;

    /* Find out which is the constant node
     * @TODO [CONSIDER] [04/16/01] []: allow constant other than INT */

    if (op1->gtOper == GT_CNS_INT)
    {
        op    = op2;
        cons  = op1;
    }
    else if (op2->gtOper == GT_CNS_INT)
    {
        op    = op1;
        cons  = op2;
    }
    else
        return tree;

    /* Get the constant value */

    val = cons->gtIntCon.gtIconVal;

    /* Here op is the non-constant operand, val is the constant,
       first is true if the constant is op1 */

    switch  (oper)
    {

    case GT_ADD:
    case GT_ASG_ADD:
        if  (val == 0) goto DONE_FOLD;
        break;

    case GT_MUL:
    case GT_ASG_MUL:
        if  (val == 1)
            goto DONE_FOLD;
        else if (val == 0)
        {
            /* Multiply by zero - return the 'zero' node, but not if side effects */
            if (!(op->gtFlags & (GTF_SIDE_EFFECT & ~GTF_OTHER_SIDEEFF)))
            {
                op = cons;
                goto DONE_FOLD;
            }
        }
        break;

    case GT_DIV:
    case GT_UDIV:
    case GT_ASG_DIV:
        if ((op2 == cons) && (val == 1) && !(op1->OperKind() & GTK_CONST))
        {
            goto DONE_FOLD;
        }
        break;

    case GT_SUB:
    case GT_ASG_SUB:
        if ((op2 == cons) && (val == 0) && !(op1->OperKind() & GTK_CONST))
        {
            goto DONE_FOLD;
        }
        break;

    case GT_AND:
        if  (val == 0)
        {
            /* AND with zero - return the 'zero' node, but not if side effects */

            if (!(op->gtFlags & (GTF_SIDE_EFFECT & ~GTF_OTHER_SIDEEFF)))
            {
                op = cons;
                goto DONE_FOLD;
            }
        }
        else
        {
            /* The GTF_BOOLEAN flag is set for nodes that are part
             * of a boolean expression, thus all their children
             * are known to evaluate to only 0 or 1 */

            if (tree->gtFlags & GTF_BOOLEAN)
            {

                /* The constant value must be 1
                 * AND with 1 stays the same */
                assert(val == 1);
                goto DONE_FOLD;
            }
        }
        break;

    case GT_OR:
        if  (val == 0)
            goto DONE_FOLD;
        else if (tree->gtFlags & GTF_BOOLEAN)
        {
            /* The constant value must be 1 - OR with 1 is 1 */

            assert(val == 1);

            /* OR with one - return the 'one' node, but not if side effects */

            if (!(op->gtFlags & (GTF_SIDE_EFFECT & ~GTF_OTHER_SIDEEFF)))
            {
                op = cons;
                goto DONE_FOLD;
            }
        }
        break;

    case GT_LSH:
    case GT_RSH:
    case GT_RSZ:
    case GT_ASG_LSH:
    case GT_ASG_RSH:
    case GT_ASG_RSZ:
        if (val == 0)
        {
            if (op2 == cons)
                goto DONE_FOLD;
            else if (!(op->gtFlags & (GTF_SIDE_EFFECT & ~GTF_OTHER_SIDEEFF)))
            {
                op = cons;
                goto DONE_FOLD;
            }
        }
        break;

    case GT_QMARK:
        assert(op1 == cons && op2 == op && op2->gtOper == GT_COLON);
        assert(op2->gtOp.gtOp1 && op2->gtOp.gtOp2);

        assert(val == 0 || val == 1);

        if (val)
            op = op2->gtOp.gtOp2;
        else
            op = op2->gtOp.gtOp1;
        
        // Clear colon flags only if the qmark itself is not conditionaly executed
        if ( (tree->gtFlags & GTF_COLON_COND)==0 )
        {
            fgWalkTreePre(op, gtClearColonCond);
        }

        goto DONE_FOLD;

    default:
        break;
    }

    /* The node is not foldable */

    return tree;

DONE_FOLD:

    /* The node has beeen folded into 'op' */
    
    // If there was an assigment update, we just morphed it into
    // a use, update the flags appropriately
    if (op->gtOper == GT_LCL_VAR)
    {
        assert ((tree->OperKind() & GTK_ASGOP) ||
                (op->gtFlags & (GTF_VAR_USEASG | GTF_VAR_USEDEF | GTF_VAR_DEF)) == 0);

        op->gtFlags &= ~(GTF_VAR_USEASG | GTF_VAR_USEDEF | GTF_VAR_DEF);
    }

    op->gtNext = tree->gtNext;
    op->gtPrev = tree->gtPrev;

    return op;
}

/*****************************************************************************
 *
 *  Fold the given constant tree.
 */

GenTreePtr                  Compiler::gtFoldExprConst(GenTreePtr tree)
{
    unsigned        kind = tree->OperKind();

    INT32           i1, i2, itemp;
    INT64           lval1, lval2, ltemp;
    float           f1, f2;
    double          d1, d2;

    assert (kind & (GTK_UNOP | GTK_BINOP));

    GenTreePtr      op1 = tree->gtOp.gtOp1;
    GenTreePtr      op2 = tree->gtGetOp2();

    if      (kind & GTK_UNOP)
    {
        assert(op1->OperKind() & GTK_CONST);

#ifdef  DEBUG
        if  (verbose&&1)
        {
            if (tree->gtOper == GT_NOT ||
                tree->gtOper == GT_NEG ||
                tree->gtOper == GT_CHS ||
                tree->gtOper == GT_CAST)
            {
                printf("\nFolding unary operator with constant node:\n");
                gtDispTree(tree);
            }
        }
#endif
        switch(op1->gtType)
        {
        case TYP_INT:

            /* Fold constant INT unary operator */
            i1 = op1->gtIntCon.gtIconVal;

            switch (tree->gtOper)
            {
            case GT_NOT: i1 = ~i1; break;

            case GT_NEG:
            case GT_CHS: i1 = -i1; break;

            case GT_CAST:
                // assert (genActualType(tree->gtCast.gtCastType) == tree->gtType);
                switch (tree->gtCast.gtCastType)
                {
                case TYP_BYTE:
                    itemp = INT32(INT8(i1));
                    goto CHK_OVF;

                case TYP_SHORT:
                    itemp = INT32(INT16(i1));
CHK_OVF:
                    if (tree->gtOverflow() &&
                        ((itemp != i1) ||
                         ((tree->gtFlags & GTF_UNSIGNED) && i1 < 0)))
                    {
                         goto INT_OVF;
                    }
                    i1 = itemp; goto CNS_INT;

                case TYP_CHAR:
                    itemp = INT32(UINT16(i1));
                    if (tree->gtOverflow())
                        if (itemp != i1) goto INT_OVF;
                    i1 = itemp;
                    goto CNS_INT;

                case TYP_BOOL:
                case TYP_UBYTE:
                    itemp = INT32(UINT8(i1));
                    if (tree->gtOverflow()) if (itemp != i1) goto INT_OVF;
                    i1 = itemp; goto CNS_INT;

                case TYP_UINT:
                    if (!(tree->gtFlags & GTF_UNSIGNED) && tree->gtOverflow() && i1 < 0)
                        goto INT_OVF;
                    goto CNS_INT;

                case TYP_INT:
                    if ((tree->gtFlags & GTF_UNSIGNED) && tree->gtOverflow() && i1 < 0)
                        goto INT_OVF;
                    goto CNS_INT;

                case TYP_ULONG:
                    if (!(tree->gtFlags & GTF_UNSIGNED) && tree->gtOverflow() && i1 < 0)
                    {
                        op1->ChangeOperConst(GT_CNS_LNG); // need type of oper to be same as tree
                        op1->gtType = TYP_LONG;
                        // We don't care about the value as we are throwing an exception
                        goto LNG_OVF;
                    }
                    lval1 = UINT64(UINT32(i1));
                    goto CNS_LONG;

                case TYP_LONG:
                    if (tree->gtFlags & GTF_UNSIGNED)
                    {
                        lval1 = INT64(UINT32(i1));                    
                    }
                    else
                    {
                    lval1 = INT64(i1);
                    }
                    goto CNS_LONG;

                case TYP_FLOAT:
                    if (tree->gtFlags & GTF_UNSIGNED)
                        f1 = forceFloatSpill((float) UINT32(i1));
                    else
                        f1 = forceFloatSpill((float) i1);
                    d1 = f1;
                    goto CNS_DOUBLE;
                
                case TYP_DOUBLE:
                    if (tree->gtFlags & GTF_UNSIGNED)
                        d1 = (double) UINT32(i1);
                    else
                        d1 = (double) i1;
                    goto CNS_DOUBLE;

#ifdef  DEBUG
                default:
                    assert(!"BAD_TYP");
#endif
                }
                return tree;

            default:
                return tree;
            }

            goto CNS_INT;

        case TYP_LONG:

            /* Fold constant LONG unary operator */

            lval1 = op1->gtLngCon.gtLconVal;

            switch (tree->gtOper)
            {
            case GT_NOT: lval1 = ~lval1; break;

            case GT_NEG:
            case GT_CHS: lval1 = -lval1; break;

            case GT_CAST:
                assert (genActualType(tree->gtCast.gtCastType) == tree->gtType);
                switch (tree->gtCast.gtCastType)
                {
                case TYP_BYTE:
                    i1 = INT32(INT8(lval1));
                    goto CHECK_INT_OVERFLOW;

                case TYP_SHORT:
                    i1 = INT32(INT16(lval1));
                    goto CHECK_INT_OVERFLOW;

                case TYP_CHAR:
                    i1 = INT32(UINT16(lval1));
                    goto CHECK_UINT_OVERFLOW;

                case TYP_UBYTE:
                    i1 = INT32(UINT8(lval1));
                    goto CHECK_UINT_OVERFLOW;

                case TYP_INT:
                    i1 = INT32(lval1);

    CHECK_INT_OVERFLOW:
                    if (tree->gtOverflow())
                    {
                        if (i1 != lval1)
                            goto INT_OVF;
                        if ((tree->gtFlags & GTF_UNSIGNED) && i1 < 0)
                            goto INT_OVF;
                    }
                    goto CNS_INT;

                case TYP_UINT:
                    i1 = UINT32(lval1);

    CHECK_UINT_OVERFLOW:
                    if (tree->gtOverflow() && UINT32(i1) != lval1)
                        goto INT_OVF;
                    goto CNS_INT;

                case TYP_ULONG:
                    if (!(tree->gtFlags & GTF_UNSIGNED) && tree->gtOverflow() && lval1 < 0)
                        goto LNG_OVF;
                    goto CNS_LONG;

                case TYP_LONG:
                    if ( (tree->gtFlags & GTF_UNSIGNED) && tree->gtOverflow() && lval1 < 0)
                        goto LNG_OVF;
                    goto CNS_LONG;

                case TYP_FLOAT:
                case TYP_DOUBLE:
                    // VC does not have unsigned convert to double, so we
                    // implement it by adding 2^64 if the number is negative
                    d1 = (double) lval1;
                    if ((tree->gtFlags & GTF_UNSIGNED) && lval1 < 0)
                        d1 +=  4294967296.0 * 4294967296.0;

                    if (tree->gtCast.gtCastType == TYP_FLOAT)
                    {
                        f1 = forceFloatSpill((float) d1);    // truncate precision
                        d1 = f1;
                    }
                    goto CNS_DOUBLE;
#ifdef  DEBUG
                default:
                    assert(!"BAD_TYP");
#endif
                }
                return tree;

            default:
                return tree;
            }

            goto CNS_LONG;

        case TYP_FLOAT:
        case TYP_DOUBLE:
            assert(op1->gtOper == GT_CNS_DBL);

            /* Fold constant DOUBLE unary operator */
            
            d1 = op1->gtDblCon.gtDconVal;
            
            switch (tree->gtOper)
            {
            case GT_NEG:
            case GT_CHS:
                d1 = -d1;
                break;

            case GT_CAST:

                // @TODO [CONSIDER] [04/16/01] []: Add these cases
                if (tree->gtOverflowEx())
                    return tree;

                /* If not finite don't bother */
                if ((op1->gtType == TYP_FLOAT  && !_finite(float(d1))) ||
                    (op1->gtType == TYP_DOUBLE && !_finite(d1)))
                    return tree;

                assert (genActualType(tree->gtCast.gtCastType) == tree->gtType);
                switch (tree->gtCast.gtCastType)
                {
                case TYP_BYTE:
                    i1 = INT32(INT8(d1));   goto CNS_INT;

                case TYP_SHORT:
                    i1 = INT32(INT16(d1));  goto CNS_INT;

                case TYP_CHAR:
                    i1 = INT32(UINT16(d1)); goto CNS_INT;

                case TYP_UBYTE:
                    i1 = INT32(UINT8(d1));  goto CNS_INT;

                case TYP_INT:
                    i1 = INT32(d1);         goto CNS_INT;

                case TYP_UINT:
                    i1 = UINT32(d1);        goto CNS_INT;

                case TYP_LONG:
                    lval1 = INT64(d1);      goto CNS_LONG;

                case TYP_ULONG:
                    lval1 = UINT64(d1);     goto CNS_LONG;

                case TYP_FLOAT:
                    d1 = forceFloatSpill((float)d1);  
                    goto CNS_DOUBLE;

                case TYP_DOUBLE:
                    if (op1->gtType == TYP_FLOAT)
                        d1 = forceFloatSpill((float)d1); // truncate precision
                    goto CNS_DOUBLE; // redundant cast

#ifdef  DEBUG
                default:
                    assert(!"BAD_TYP");
#endif
                }
                return tree;

            default:
                return tree;
            }
            goto CNS_DOUBLE_NO_MSG;

        default:
            /* not a foldable typ - e.g. RET const */
            return tree;
        }
    }

    /* We have a binary operator */

    assert(kind & GTK_BINOP);
    assert(op2);
    assert(op1->OperKind() & GTK_CONST);
    assert(op2->OperKind() & GTK_CONST);

    if (tree->gtOper == GT_COMMA)
        return op2;

    typedef   signed char   INT8;
    typedef unsigned char   UINT8;
    typedef   signed short  INT16;
    typedef unsigned short  UINT16;
    #define LNG_MIN        (INT64(INT_MIN) << 32)

    switch(op1->gtType)
    {

    /*-------------------------------------------------------------------------
     * Fold constant INT binary operator
     */

    case TYP_INT:

        if (tree->OperIsCompare() && (tree->gtType == TYP_BYTE))
            tree->gtType = TYP_INT;

        assert (tree->gtType == TYP_INT || varTypeIsGC(tree->TypeGet()) ||
                tree->gtOper == GT_LIST);

        i1 = op1->gtIntCon.gtIconVal;
        i2 = op2->gtIntCon.gtIconVal;

        switch (tree->gtOper)
        {
        case GT_EQ : i1 = (i1 == i2); break;
        case GT_NE : i1 = (i1 != i2); break;

        case GT_LT :
            if (tree->gtFlags & GTF_UNSIGNED)
                i1 = (UINT32(i1) <  UINT32(i2));
            else
                i1 = (i1 < i2);
            break;

        case GT_LE :
            if (tree->gtFlags & GTF_UNSIGNED)
                i1 = (UINT32(i1) <= UINT32(i2));
            else
                i1 = (i1 <= i2);
            break;

        case GT_GE :
            if (tree->gtFlags & GTF_UNSIGNED)
                i1 = (UINT32(i1) >= UINT32(i2));
            else
                i1 = (i1 >= i2);
            break;

        case GT_GT :
            if (tree->gtFlags & GTF_UNSIGNED)
                i1 = (UINT32(i1) >  UINT32(i2));
            else
                i1 = (i1 >  i2);
            break;

        case GT_ADD:
            itemp = i1 + i2;
            if (tree->gtOverflow())
            {
                if (tree->gtFlags & GTF_UNSIGNED)
                {
                    if (INT64(UINT32(itemp)) != INT64(UINT32(i1)) + INT64(UINT32(i2)))
                        goto INT_OVF;
                }
                else
                {
                    if (INT64(itemp)         != INT64(i1)+INT64(i2))
                        goto INT_OVF;
                }
            }
            i1 = itemp; break;

        case GT_SUB:
            itemp = i1 - i2;
            if (tree->gtOverflow())
            {
                if (tree->gtFlags & GTF_UNSIGNED)
                {
                    if (INT64(UINT32(itemp)) != (INT64(UINT32(i1)) - INT64(UINT32(i2))))
                        goto INT_OVF;
                }
                else
                {
                    if (INT64(itemp)         != INT64(i1) - INT64(i2))
                        goto INT_OVF;
                }
            }
            i1 = itemp; break;

        case GT_MUL:
            itemp = i1 * i2;
            if (tree->gtOverflow())
            {
                if (tree->gtFlags & GTF_UNSIGNED)
                {
                    if (INT64(UINT32(itemp)) != (INT64(UINT32(i1)) * INT64(UINT32(i2))))
                        goto INT_OVF;
                }
                else
                {
                    if (INT64(itemp)         != INT64(i1) * INT64(i2))
                        goto INT_OVF;
                }
            }
            i1 = itemp; break;

        case GT_OR : i1 |= i2; break;
        case GT_XOR: i1 ^= i2; break;
        case GT_AND: i1 &= i2; break;

        case GT_LSH: i1 <<= (i2 & 0x1f); break;
        case GT_RSH: i1 >>= (i2 & 0x1f); break;
        case GT_RSZ:
                /* logical shift -> make it unsigned to propagate the sign bit */
                i1 = UINT32(i1) >> (i2 & 0x1f);
            break;

        /* DIV and MOD can generate an INT 0 - if division by 0
         * or overflow - when dividing MIN by -1 */

        //@TODO [CONSIDER] [04/16/01] []: Convert into std exception throw
        case GT_DIV:
            if (!i2) return tree;
            if (UINT32(i1) == 0x80000000 && i2 == -1)
            {
                /* In IL we have to throw an exception */
                return tree;
            }
            i1 /= i2; break;

        case GT_MOD:
            if (!i2) return tree;
            if (UINT32(i1) == 0x80000000 && i2 == -1)
            {
                /* In IL we have to throw an exception */
                return tree;
            }
            i1 %= i2; break;

        case GT_UDIV:
            if (!i2) return tree;
            if (UINT32(i1) == 0x80000000 && i2 == -1) return tree;
            i1 = UINT32(i1) / UINT32(i2); break;

        case GT_UMOD:
            if (!i2) return tree;
            if (UINT32(i1) == 0x80000000 && i2 == -1) return tree;
            i1 = UINT32(i1) % UINT32(i2); break;
        default:
            return tree;
        }

        /* We get here after folding to a GT_CNS_INT type
         * bash the node to the new type / value and make sure the node sizes are OK */
CNS_INT:
FOLD_COND:

#ifdef  DEBUG
        if  (verbose)
        {
            printf("\nFolding binary operator with constant nodes into a constant:\n");
            gtDispTree(tree);
        }
#endif
        /* Also all conditional folding jumps here since the node hanging from
         * GT_JTRUE has to be a GT_CNS_INT - value 0 or 1 */

        tree->ChangeOperConst      (GT_CNS_INT);
        tree->gtType             = TYP_INT;
        tree->gtIntCon.gtIconVal = i1;
        goto DONE;

        /* This operation is going to cause an overflow exception. Morph into
           an overflow helper. Put a dummy constant value for code generation.

           We could remove all subsequent trees in the current basic block,
           unless this node is a child of GT_COLON

           NOTE: Since the folded value is not constant we should not bash the
                 "tree" node - otherwise we confuse the logic that checks if the folding
                 was successful - instead use one of the operands, e.g. op1
         */

LNG_OVF:
        op1 = gtNewLconNode(0);
        goto OVF;

INT_OVF:
        op1 = gtNewIconNode(0);
        goto OVF;

OVF:

#ifdef  DEBUG
        if  (verbose)
        {
            printf("\nFolding binary operator with constant nodes into a comma throw:\n");
            gtDispTree(tree);
        }
#endif
        /* We will bashed cast to a GT_COMMA and attach the exception helper as gtOp.gtOp1
         * the constant expression zero becomes op2 */

        assert(tree->gtOverflow());
        assert(tree->gtOper == GT_ADD  || tree->gtOper == GT_SUB ||
               tree->gtOper == GT_CAST || tree->gtOper == GT_MUL);
        assert(op1);

        op2 = op1;
        op1 = gtNewHelperCallNode(CORINFO_HELP_OVERFLOW, 
                                  TYP_VOID, 
                                  GTF_EXCEPT,
                                  gtNewArgList(gtNewIconNode(compCurBB->bbTryIndex)));

        tree = gtNewOperNode(GT_COMMA, tree->gtType, op1, op2);

        return tree;

    /*-------------------------------------------------------------------------
     * Fold constant REF of BYREF binary operator
     * These can only be comparisons or null pointers
     * Currently cannot have constant byrefs
     */

    case TYP_REF:

        /* String nodes are an RVA at this point */

        if (op1->gtOper == GT_CNS_STR || op2->gtOper == GT_CNS_STR)
            return tree;

        i1 = op1->gtIntCon.gtIconVal;
        i2 = op2->gtIntCon.gtIconVal;

        assert(i1 == 0);

        switch (tree->gtOper)
        {
        case GT_EQ : i1 = 1; goto FOLD_COND;
        case GT_NE : i1 = 0; goto FOLD_COND;

        /* For the null pointer case simply keep the null pointer */

        case GT_ADD:
#ifdef  DEBUG
            if  (verbose)
            {
                printf("\nFolding a null+cns dereference:\n");
                gtDispTree(tree);
            }
#endif
            tree->ChangeOperConst(GT_CNS_INT);
            tree->gtType = TYP_BYREF;
            tree->gtIntCon.gtIconVal = i1;
            goto DONE;

        default:
            //assert(!"Illegal operation on TYP_REF");
            return tree;
        }

    case TYP_BYREF:
        i1 = op1->gtIntCon.gtIconVal;
        i2 = op2->gtIntCon.gtIconVal;

        assert(i1 == 0);

        switch (tree->gtOper)
        {
        case GT_EQ : i1 = 1; goto FOLD_COND;
        case GT_NE : i1 = 0; goto FOLD_COND;

        /* For the null pointer case simply keep the null pointer */

        case GT_ADD:
#ifdef  DEBUG
            if  (verbose)
            {
                printf("\nFolding a null+cns dereference:\n");
                gtDispTree(tree);
            }
#endif
            tree = op1;
            goto DONE;

        default:
            //assert(!"Illegal operation on TYP_REF");
            return tree;
        }

    /*-------------------------------------------------------------------------
     * Fold constant LONG binary operator
     */

    case TYP_LONG:

        lval1 = op1->gtLngCon.gtLconVal;
        lval2 = op2->gtLngCon.gtLconVal;

        assert((tree->gtOper == GT_LSH || tree->gtOper == GT_RSH || tree->gtOper == GT_RSZ) && op2->gtType == TYP_INT 
               || op2->gtType == TYP_LONG);

        switch (tree->gtOper)
        {
        case GT_EQ : i1 = (lval1 == lval2); goto FOLD_COND;
        case GT_NE : i1 = (lval1 != lval2); goto FOLD_COND;

        case GT_LT :
            if (tree->gtFlags & GTF_UNSIGNED)
                i1 = (UINT64(lval1) <  UINT64(lval2));
            else
                i1 = (lval1 <  lval2);
            goto FOLD_COND;

        case GT_LE :
            if (tree->gtFlags & GTF_UNSIGNED)
                i1 = (UINT64(lval1) <= UINT64(lval2));
            else
                i1 = (lval1 <=  lval2);
            goto FOLD_COND;

        case GT_GE :
            if (tree->gtFlags & GTF_UNSIGNED)
                i1 = (UINT64(lval1) >= UINT64(lval2));
            else
                i1 = (lval1 >=  lval2);
            goto FOLD_COND;

        case GT_GT :
            if (tree->gtFlags & GTF_UNSIGNED)
                i1 = (UINT64(lval1) >  UINT64(lval2));
            else
                i1 = (lval1  >  lval2);
            goto FOLD_COND;

        case GT_ADD:
            ltemp = lval1 + lval2;

LNG_ADD_CHKOVF:
            /* For the SIGNED case - If there is one positive and one negative operand, there can be no overflow
             * If both are positive, the result has to be positive, and similary for negatives.
             *
             * For the UNSIGNED case - If a UINT32 operand is bigger than the result then OVF */

            if (tree->gtOverflow())
            {
                if (tree->gtFlags & GTF_UNSIGNED)
                {
                    if ( (UINT64(lval1) > UINT64(ltemp)) ||
                         (UINT64(lval2) > UINT64(ltemp))  )
                        goto LNG_OVF;
                }
                else
                    if ( ((lval1<0) == (lval2<0)) && ((lval1<0) != (ltemp<0)) )
                        goto LNG_OVF;
            }
            lval1 = ltemp; break;

        case GT_SUB:
            ltemp = lval1 - lval2;
            if (tree->gtOverflow())
            {
                if (tree->gtFlags & GTF_UNSIGNED)
                {
                    if (UINT64(lval2) > UINT64(lval1))
                        goto LNG_OVF;
                }
                else
                {
                    /* If both operands are +ve or both are -ve, there can be no
                       overflow. Else use the logic for : lval1 + (-lval2) */

                    if ((lval1<0) != (lval2<0))
                    {
                        if (lval2 == LNG_MIN) goto LNG_OVF;
                        lval2 = -lval2; goto LNG_ADD_CHKOVF;
                    }
                }
            }
            lval1 = ltemp; break;

        case GT_MUL:
            ltemp = lval1 * lval2;
            if (tree->gtOverflow() && lval2 != 0)
            {
                if (tree->gtFlags & GTF_UNSIGNED)
                {
                    UINT64 ultemp = ltemp;
                    UINT64 ulval1 = lval1;
                    UINT64 ulval2 = lval2;
                    if ((ultemp/ulval2) != ulval1) goto LNG_OVF;
                }
                else
                {
                    if ((ltemp/lval2) != lval1) goto LNG_OVF;
                }
            }
            lval1 = ltemp; break;

        case GT_OR : lval1 |= lval2; break;
        case GT_XOR: lval1 ^= lval2; break;
        case GT_AND: lval1 &= lval2; break;

        case GT_LSH: lval1 <<= (op2->gtIntCon.gtIconVal & 0x3f); break;
        case GT_RSH: lval1 >>= (op2->gtIntCon.gtIconVal & 0x3f); break;
        case GT_RSZ:
                /* logical shift -> make it unsigned to propagate the sign bit */
                lval1 = UINT64(lval1) >> (op2->gtIntCon.gtIconVal & 0x3f);
            break;

        case GT_DIV:
            if (!lval2) return tree;
            if (UINT64(lval1) == 0x8000000000000000 && lval2 == INT64(-1))
            {
                /* In IL we have to throw an exception */
                return tree;
            }
            lval1 /= lval2; break;

        case GT_MOD:
            if (!lval2) return tree;
            if (UINT64(lval1) == 0x8000000000000000 && lval2 == INT64(-1))
            {
                /* In IL we have to throw an exception */
                return tree;
            }
            lval1 %= lval2; break;

        case GT_UDIV:
            if (!lval2) return tree;
            if (UINT64(lval1) == 0x8000000000000000 && lval2 == INT64(-1)) return tree;
            lval1 = UINT64(lval1) / UINT64(lval2); break;

        case GT_UMOD:
            if (!lval2) return tree;
            if (UINT64(lval1) == 0x8000000000000000 && lval2 == INT64(-1)) return tree;
            lval1 = UINT64(lval1) % UINT64(lval2); break;
        default:
            return tree;
        }

CNS_LONG:

#ifdef  DEBUG
        if  (verbose)
        {
            printf("\nFolding binary operator with constant nodes into a constant:\n");
            gtDispTree(tree);
        }
#endif
        assert ((GenTree::s_gtNodeSizes[GT_CNS_LNG] == TREE_NODE_SZ_SMALL) ||
                (tree->gtFlags & GTF_NODE_LARGE)                            );

        tree->ChangeOperConst(GT_CNS_LNG);
        tree->gtLngCon.gtLconVal = lval1;
        goto DONE;

    /*-------------------------------------------------------------------------
     * Fold constant FLOAT binary operator
     */

    case TYP_FLOAT:

        if (tree->gtOper != GT_CAST)
            goto DO_DOUBLE;

        // @TODO [CONSIDER] [04/16/01] []: Add these cases
        if (tree->gtOverflowEx())
            return tree;

        assert(op1->gtOper == GT_CNS_DBL);
        f1 = op1->gtDblCon.gtDconVal;
        f2 = 0.0;

        /* If not finite don't bother */
        if (!_finite(f1))
            return tree;

        /* second operand is an INT */
        assert (op2->gtType == TYP_INT);
        i2 = op2->gtIntCon.gtIconVal;

        switch (tree->gtOper)
        {
        case GT_EQ : i1 = (f1 == f2); goto FOLD_COND;
        case GT_NE : i1 = (f1 != f2); goto FOLD_COND;

        case GT_LT : i1 = (f1 <  f2); goto FOLD_COND;
        case GT_LE : i1 = (f1 <= f2); goto FOLD_COND;
        case GT_GE : i1 = (f1 >= f2); goto FOLD_COND;
        case GT_GT : i1 = (f1 >  f2); goto FOLD_COND;

        case GT_ADD: f1 += f2; break;
        case GT_SUB: f1 -= f2; break;
        case GT_MUL: f1 *= f2; break;

        case GT_DIV: if (!f2) return tree;
                     f1 /= f2; break;

        default:
            return tree;
        }

        assert(!"Should be unreachable!");
        goto DONE;

    /*-------------------------------------------------------------------------
     * Fold constant DOUBLE binary operator
     */

    case TYP_DOUBLE:

DO_DOUBLE:

        // @TODO [CONSIDER] [04/16/01] []: Add these cases
        if (tree->gtOverflowEx())
            return tree;

        assert(op1->gtOper == GT_CNS_DBL);
        d1 = op1->gtDblCon.gtDconVal;

        assert (varTypeIsFloating(op2->gtType));
        assert(op1->gtOper == GT_CNS_DBL);
        d2 = op2->gtDblCon.gtDconVal;

        /* Special case - check if we have NaN operands
         * For comparissons if not an unordered operation always return 0
         * For unordered operations (i.e the GTF_RELOP_NAN_UN flag is set)
         * the result is always true - return 1 */

        if (_isnan(d1) || _isnan(d2))
        {
#ifdef  DEBUG
            if  (verbose)
                printf("Double operator(s) is NaN\n");
#endif
            if (tree->OperKind() & GTK_RELOP)
                if (tree->gtFlags & GTF_RELOP_NAN_UN)
                {
                    /* Unordered comparisson with NaN always succeeds */
                    i1 = 1; goto FOLD_COND;
                }
                else
                {
                    /* Normal comparisson with NaN always fails */
                    i1 = 0; goto FOLD_COND;
                }
        }

        switch (tree->gtOper)
        {
        case GT_EQ : i1 = (d1 == d2); goto FOLD_COND;
        case GT_NE : i1 = (d1 != d2); goto FOLD_COND;

        case GT_LT : i1 = (d1 <  d2); goto FOLD_COND;
        case GT_LE : i1 = (d1 <= d2); goto FOLD_COND;
        case GT_GE : i1 = (d1 >= d2); goto FOLD_COND;
        case GT_GT : i1 = (d1 >  d2); goto FOLD_COND;

        case GT_ADD: d1 += d2; break;
        case GT_SUB: d1 -= d2; break;
        case GT_MUL: d1 *= d2; break;

        case GT_DIV: if (!d2) return tree;
                     d1 /= d2; break;
        default:
            return tree;
        }

CNS_DOUBLE:

#ifdef  DEBUG
        if  (verbose)
        {
            printf("\nFolding binary operator with constant nodes into a constant:\n");
            gtDispTree(tree);
        }
#endif

CNS_DOUBLE_NO_MSG:

        assert ((GenTree::s_gtNodeSizes[GT_CNS_DBL] == TREE_NODE_SZ_SMALL) ||
                (tree->gtFlags & GTF_NODE_LARGE)                            );

        tree->ChangeOperConst(GT_CNS_DBL);
        tree->gtDblCon.gtDconVal = d1;

        goto DONE;

    default:
        /* not a foldable typ */
        return tree;
    }

    //-------------------------------------------------------------------------

DONE:

    /* Make sure no side effect flags are set on this constant node */

    tree->gtFlags &= ~GTF_SIDE_EFFECT;

    return tree;
}

/*****************************************************************************
 *
 *  Create an assignment of the given value to a temp.
 */

GenTreePtr          Compiler::gtNewTempAssign(unsigned tmp, GenTreePtr val)
{
    LclVarDsc  *    varDsc = lvaTable + tmp;

    /* @TODO [REVISIT] [06/25/01] [dnotario]: Since ldloca's result can be interpreted 
       either as TYP_I_IMPL or TYP_BYREF, we have a problem. Also complicated
       by ldnull. Got to check all cases. RAID 90160*/

    if (varDsc->TypeGet() == TYP_I_IMPL && val->TypeGet() == TYP_BYREF)
        impBashVarAddrsToI(val);

    /* @TODO [CONSIDER] [06/25/01] []: should we upgrade FLOAT temps to DOUBLE?   
       right now we don't, which is still OK by the spec */

    var_types   valTyp =    val->TypeGet();
    var_types   dstTyp = varDsc->TypeGet();
    
    /* If the variable's lvType is not yet set then set it here */
    if (dstTyp == TYP_UNDEF) 
    {
        varDsc->lvType = dstTyp = genActualType(valTyp);
        if (varTypeIsGC(dstTyp))
            varDsc->lvStructGcCount = 1;
    }

    /* Make sure the actual types match               */
    /*   or valTyp is TYP_INT and dstTyp is TYP_BYREF */
    /*   or both types are floating point types       */
    
    assert( genActualType(valTyp) == genActualType(dstTyp)  ||
            (valTyp == TYP_INT    && dstTyp == TYP_BYREF)   ||
            (varTypeIsFloating(dstTyp) && varTypeIsFloating(valTyp)));

    /* Create the assignment node */

    return gtNewAssignNode(gtNewLclvNode(tmp, dstTyp), val);
}

/*****************************************************************************
 *
 *  If the field is a NStruct field of a simple type, then we can directly
 *  access it without using a helper call.
 *  This function returns NULL if the field is not a simple NStruct field
 *  Else it will create a tree to do the field access and return it.
 *  "assg" is 0 for ldfld, and the value to assign for stfld.
 */

GenTreePtr          Compiler::gtNewDirectNStructField (GenTreePtr   objPtr,
                                                       unsigned     fldIndex,
                                                       var_types    lclTyp,
                                                       GenTreePtr   assg)
{
    CORINFO_FIELD_HANDLE        fldHnd = eeFindField(fldIndex, info.compScopeHnd, 0);
    CorInfoFieldCategory   fldNdc;

    fldNdc = info.compCompHnd->getFieldCategory(fldHnd);

    /* Check if it is a simple type. If so, map it to "var_types" */

    var_types           type;

    switch(fldNdc)
    {
    // Most simple types - exact match

    case CORINFO_FIELDCATEGORY_I1_I1        : type = TYP_BYTE;      break;
    case CORINFO_FIELDCATEGORY_I2_I2        : type = TYP_SHORT;     break;
    case CORINFO_FIELDCATEGORY_I4_I4        : type = TYP_INT;       break;
    case CORINFO_FIELDCATEGORY_I8_I8        : type = TYP_LONG;      break;

    // These will need some extra work

    case CORINFO_FIELDCATEGORY_BOOLEAN_BOOL : type = TYP_BYTE;      break;
    case CORINFO_FIELDCATEGORY_CHAR_CHAR    : type = TYP_UBYTE;     break;

    // Others

    default     : //assert(fldNdc == CORINFO_FIELDCATEGORY_NORMAL ||
                  //       fldNdc == CORINFO_FIELDCATEGORY_UNKNOWN);

                                          type = TYP_UNDEF;     break;
    }

    if (type == TYP_UNDEF)
    {
        /* If the field is not NStruct (must be a COM object), or if it is
         * not of a simple type, we will simply use a helper call to
         * access it. So just return NULL
         */

        return NULL;
    }

    NO_WAY_RET("I thought NStruct is now defunct?", GenTreePtr);
#if 0


    /* Create the following tree :
     * GT_IND( GT_IND(obj + INDIR_OFFSET) + nativeFldOffs )
     */

    GenTreePtr      tree;

    /* Get the offset of the field in the real native struct */

    unsigned        fldOffs = eeGetFieldOffset(fldHnd);

    /* Get the real ptr from the proxy object */

    tree = gtNewOperNode(GT_ADD, TYP_REF,
                        objPtr,
                        gtNewIconNode(Info::compNStructIndirOffset));

    tree = gtNewOperNode(GT_IND, TYP_I_IMPL, tree);
    tree->gtFlags |= GTF_EXCEPT;

    /* Access the field using the real ptr */

    tree = gtNewOperNode(GT_ADD, TYP_I_IMPL,
                        tree,
                        gtNewIconNode(fldOffs));

    /* Check that we used the right suffix (ie, the XX in ldfld.XX)
       to access the field */

    assert(genActualType(lclTyp) == genActualType(type));

    tree = gtNewOperNode(GT_IND, type, tree);

    /* Morph tree for some of the categories, and
       create the assignment node if needed */

    if (assg)
    {
        if (fldNdc == CORINFO_FIELDCATEGORY_BOOLEAN_BOOL)
        {
            // Need to noramlize the "bool"

            assg = gtNewOperNode(GT_NE, TYP_INT, assg, gtNewIconNode(0));
        }

        tree = gtNewAssignNode(tree, assg);
    }
    else
    {
        if (fldNdc == CORINFO_FIELDCATEGORY_BOOLEAN_BOOL)
        {
            // Need to noramlize the "bool"

            tree = gtNewOperNode(GT_NE, TYP_INT, tree, gtNewIconNode(0));
        }

        /* Dont need to do anything for CORINFO_FIELDCATEGORY_CHAR_CHAR, as
           we set the type to TYP_UBYTE, so it will be automatically
           expanded to 16/32 bits as needed.
         */
    }

    return tree;
#endif
}

/*****************************************************************************
 *
 *  Create a helper call to access a COM field (iff 'assg' is non-zero this is
 *  an assignment and 'assg' is the new value).
 */

GenTreePtr          Compiler::gtNewRefCOMfield(GenTreePtr   objPtr,
                                               CorInfoFieldAccess access,
                                               unsigned     fldIndex,
                                               var_types    lclTyp,
                                               CORINFO_CLASS_HANDLE structType,
                                               GenTreePtr   assg)
{
    /* See if we can directly access the NStruct field */

    if (objPtr != 0)
    {
        GenTreePtr ntree = gtNewDirectNStructField(objPtr, fldIndex, lclTyp, assg);
        if (ntree)
            return ntree;
    }

    /* If we cant access it directly, we need to call a helper function */
    GenTreePtr      args = 0;
    CorInfoFieldAccess helperAccess = access;
    var_types helperType = lclTyp;

    CORINFO_FIELD_HANDLE memberHandle = eeFindField(fldIndex, info.compScopeHnd, 0);
    unsigned mflags = eeGetFieldAttribs(memberHandle);

    if (mflags & CORINFO_FLG_STATIC)    // Statics implemented as addr followed by indirection
    {
        helperAccess = CORINFO_ADDRESS;
        helperType = TYP_BYREF;
    }

    if  (helperAccess == CORINFO_SET)
    {
        assert(assg != 0);
        // helper needs pointer to struct, not struct itself
        if (assg->TypeGet() == TYP_STRUCT)
        {
            assert(structType != 0);
            assg = impGetStructAddr(assg, structType, CHECK_SPILL_ALL, true);
        }
        else if (lclTyp == TYP_DOUBLE && assg->TypeGet() == TYP_FLOAT)
            assg = gtNewCastNode(TYP_DOUBLE, assg, TYP_DOUBLE);
        else if (lclTyp == TYP_FLOAT && assg->TypeGet() == TYP_DOUBLE)
            assg = gtNewCastNode(TYP_FLOAT, assg, TYP_FLOAT);  
        
        args = gtNewOperNode(GT_LIST, TYP_VOID, assg, 0);
        helperType = TYP_VOID;
    }

    int CPX = (int) info.compCompHnd->getFieldHelper(memberHandle, helperAccess);
    args = gtNewOperNode(GT_LIST, TYP_VOID,
                         gtNewIconEmbFldHndNode(memberHandle, fldIndex, info.compScopeHnd),
                         args);

    if (objPtr != 0)
        args = gtNewOperNode(GT_LIST, TYP_VOID, objPtr, args);

    GenTreePtr tree = gtNewHelperCallNode(CPX, genActualType(helperType), 0, args);

    // OK, now do the indirection
    if (mflags & CORINFO_FLG_STATIC)
    {
        if (access == CORINFO_GET)
        {
            tree = gtNewOperNode(GT_IND, lclTyp, tree);
            tree->gtFlags |= (GTF_EXCEPT | GTF_GLOB_REF);
            if (lclTyp == TYP_STRUCT)
            {
                tree->ChangeOper(GT_LDOBJ);
                tree->gtLdObj.gtClass = structType;
            }
        }
        else if (access == CORINFO_SET)
        {
            if (lclTyp == TYP_STRUCT)
                tree = impAssignStructPtr(tree, assg, structType, CHECK_SPILL_ALL);
            else
            {
                tree = gtNewOperNode(GT_IND, lclTyp, tree);
                tree->gtFlags |= (GTF_EXCEPT | GTF_GLOB_REF | GTF_IND_TGTANYWHERE);
                tree = gtNewAssignNode(tree, assg);
            }
        }
    }
    return(tree);
}

/*****************************************************************************
 *
 *  Return true if the given node (excluding children trees) contains side effects.
 *  Note that it does not recurse, and children need to be handled separately.
 *
 *  Similar to OperMayThrow() (but handles GT_CALLs specially), but considers
 *  assignments too.
 */

bool                Compiler::gtHasSideEffects(GenTreePtr tree)
{
    if  (tree->OperKind() & GTK_ASGOP)
        return  true;

    if  (tree->gtFlags & GTF_OTHER_SIDEEFF)
        return  true;

    if (tree->gtOper != GT_CALL)
        return tree->OperMayThrow();

    if (tree->gtCall.gtCallType != CT_HELPER)
        return true;

    /* Some helper calls are not side effects */

    assert(tree->gtOper == GT_CALL && tree->gtCall.gtCallType == CT_HELPER);

    CorInfoHelpFunc helperNum = eeGetHelperNum(tree->gtCall.gtCallMethHnd);
    assert(helperNum != CORINFO_HELP_UNDEF);

    if (helperNum == CORINFO_HELP_NEWSFAST)
    {
        return false;
    }

    if (helperNum == CORINFO_HELP_LDIV || helperNum == CORINFO_HELP_LMOD)
    {
        /* This is not a side effect if RHS is always non-zero */

        tree = tree->gtCall.gtCallArgs;
        assert(tree->gtOper == GT_LIST);
        tree = tree->gtOp.gtOp1;

        if  (tree->gtOper == GT_CNS_LNG && tree->gtLngCon.gtLconVal != 0)
            return  false;
    }

    return  true;
}


/*****************************************************************************
 *
 *  Extracts side effects from the given expression
 *  and appends them to a given list (actually a GT_COMMA list)
 */

void                Compiler::gtExtractSideEffList(GenTreePtr expr, GenTreePtr * list)
{
    assert(expr); assert(expr->gtOper != GT_STMT);

    /* If no side effect in the expression return */

    if (!(expr->gtFlags & GTF_SIDE_EFFECT))
        return;

    genTreeOps      oper = expr->OperGet();
    unsigned        kind = expr->OperKind();

    /* NOTE - It may be that an indirection has the GTF_EXCEPT flag cleared so no side effect
     *      - What about range checks - are they marked as GTF_EXCEPT?
     * UNDONE: For removed range checks do not extract them
     */

    /* Look for side effects
     *  - Any assignment, GT_CALL, or operator that may throw
     *    (GT_IND, GT_DIV, GTF_OVERFLOW, etc)
     *  - Special case GT_ADDR which is a side effect 
     *
     * @TODO [CONSIDER] [04/16/01] []: Use gtHasSideEffects() for this check
     */

    if ((kind & GTK_ASGOP) ||
        oper == GT_CALL    || oper == GT_BB_QMARK || oper == GT_BB_COLON ||
        oper == GT_QMARK   || oper == GT_COLON    ||
        expr->OperMayThrow())
    {
        /* Add the side effect to the list and return */

        *list = (*list == 0) ? expr : gtNewOperNode(GT_COMMA, TYP_VOID, expr, *list);

#ifdef  DEBUG
        if  (verbose && 0)
        {
            printf("Adding extracted side effects to the list:\n");
            gtDispTree(expr);
            printf("\n");
        }
#endif
        return;
    }

    if (kind & GTK_LEAF)
        return;

    assert(kind & GTK_SMPOP);

    GenTreePtr      op1  = expr->gtOp.gtOp1;
    GenTreePtr      op2  = expr->gtGetOp2();

    /* @MIHAII - Special case - GT_ADDR of GT_IND nodes of TYP_STRUCT
     * have to be kept together
     * @TODO [CONSIDER] [04/16/01] []: - This is a hack, 
     * remove after we fold this special construct */

    if (oper == GT_ADDR && op1->gtOper == GT_IND && op1->gtType == TYP_STRUCT)
    {
        *list = (*list == 0) ? expr : gtNewOperNode(GT_COMMA, TYP_VOID, expr, *list);

#ifdef  DEBUG
        if  (verbose)
            printf("Keep the GT_ADDR and GT_IND together:\n");
#endif
        return;
    }

    /* Continue searching for side effects in the subtrees of the expression
     * NOTE: Be careful to preserve the right ordering - side effects are prepended
     * to the list */

    /* Continue searching for side effects in the subtrees of the expression
     * NOTE: Be careful to preserve the right ordering 
     * as side effects are prepended to the list */

    if (expr->gtFlags & GTF_REVERSE_OPS)
    {
        if (op1) gtExtractSideEffList(op1, list);
        if (op2) gtExtractSideEffList(op2, list);
    }
    else
    {
        if (op2) gtExtractSideEffList(op2, list);
        if (op1) gtExtractSideEffList(op1, list);
    }
}


/*****************************************************************************
 *
 *  For debugging only - displays a tree node list and makes sure all the
 *  links are correctly set.
 */

#ifdef  DEBUG

void                dispNodeList(GenTreePtr list, bool verbose)
{
    GenTreePtr      last = 0;
    GenTreePtr      next;

    if  (!list)
        return;

    for (;;)
    {
        next = list->gtNext;

        if  (verbose)
            printf("%08X -> %08X -> %08X\n", last, list, next);

        assert(!last || last->gtNext == list);

        assert(next == 0 || next->gtPrev == list);

        if  (!next)
            break;

        last = list;
        list = next;
    }
    printf("");         // null string means flush
}

/*****************************************************************************
 * Callback to assert that the nodes of a qmark-colon subtree are marked
 */

/* static */
Compiler::fgWalkResult      Compiler::gtAssertColonCond(GenTreePtr  tree,
                                                        void    *   pCallBackData)
{
    assert(pCallBackData == NULL);

    assert(tree->gtFlags & GTF_COLON_COND);

    return WALK_CONTINUE;
}
#endif // DEBUG

/*****************************************************************************
 * Callback to mark the nodes of a qmark-colon subtree that are conditionally 
 * executed.
 */

/* static */
Compiler::fgWalkResult      Compiler::gtMarkColonCond(GenTreePtr  tree,
                                                      void    *   pCallBackData)
{
    assert(pCallBackData == NULL);

    tree->gtFlags |= GTF_COLON_COND;

    return WALK_CONTINUE;
}

/*****************************************************************************
 * Callback to clear the conditionally executed flags of nodes that no longer
   will be conditionally executed. Note that when we find another colon we must 
   stop, as the nodes below this one WILL be conditionally executed. This callback
   is called when folding a qmark condition (ie the condition is constant).
 */

/* static */
Compiler::fgWalkResult      Compiler::gtClearColonCond(GenTreePtr  tree,
                                                      void    *   pCallBackData)
{
    assert(pCallBackData == NULL);

    if (tree->OperGet()==GT_COLON)
    {
        // Nodes below this will be conditionally executed.
        return WALK_SKIP_SUBTREES;
    }

    tree->gtFlags &= ~GTF_COLON_COND;
    return WALK_CONTINUE;
}




/*****************************************************************************/

#if 0
#if CSELENGTH

struct  treeRmvDsc
{
    GenTreePtr          trFirst;
    unsigned            trPhase;
#ifdef DEBUG
    void    *           trSelf;
    Compiler*           trComp;
#endif
};

Compiler::fgWalkResult  Compiler::gtRemoveExprCB(GenTreePtr     tree,
                                                 void         * p)
{
    treeRmvDsc  *   desc = (treeRmvDsc*)p;

    assert(desc && desc->trSelf == desc);

    if  (desc->trPhase == 1)
    {
        /* In the first  phase we mark all the nodes as dead */

        assert((tree->gtFlags &  GTF_DEAD) == 0);
                tree->gtFlags |= GTF_DEAD;
    }
    else
    {
        /* In the second phase we notice the first node */

        if  (!tree->gtPrev || !(tree->gtPrev->gtFlags & GTF_DEAD))
        {
            /* We've found the first node in the subtree */

            desc->trFirst = tree;

            return  WALK_ABORT;
        }
    }

    return  WALK_CONTINUE;
}

/*****************************************************************************
 *
 *  Given a subtree and the head of the tree node list that contains it,
 *  remove all the nodes in the subtree from the list.
 *
 *  When 'dead' is non-zero on entry, all the nodes in the subtree have
 *  already been marked with GTF_DEAD.
 */

void                Compiler::gtRemoveSubTree(GenTreePtr    tree,
                                              GenTreePtr    list,
                                              bool          dead)
{
    GenTreePtr      opr1;
    GenTreePtr      next;
    GenTreePtr      prev;

    assert(list && list->gtOper == GT_STMT);

#if 0
    printf("Remove subtree %08X from:\n", tree);
    gtDispTree(list);
    printf("\n");
    dispNodeList(list->gtStmt.gtStmtList, true);
    printf("\n\n");
#endif

    /* Are we just removing a leaf node? */

    if  (tree->OperIsLeaf())
    {
        opr1 = tree;
        goto RMV;
    }

    /* Special easy case: unary operator with a leaf sub-operand */

    if  (tree->OperKind() & GTK_SMPOP)
    {
        opr1 = tree->gtOp.gtOp1;

        if  (!tree->gtGetOp2() && opr1->OperIsLeaf())
        {
            /* This is easy: the order is simply "prev -> opr1 -> tree -> next */

            assert(opr1->gtNext == tree);
            assert(tree->gtPrev == opr1);

            goto RMV;
        }
    }

    treeRmvDsc      desc;

    /* This is a non-trivial subtree, we'll remove it "the hard way" */

#ifdef DEBUG
    desc.trFirst = 0;
    desc.trSelf  = &desc;
    desc.trComp  = this;
#endif

    /* First  phase: mark the nodes in the subtree as dead */

    if  (!dead)
    {
        desc.trPhase = 1;
        fgWalkTreePre(tree, gtRemoveExprCB, &desc);
    }

    /* Second phase: find the first node of the subtree within the global list */

    desc.trPhase = 2;
    fgWalkTreePre(tree, fgRemoveExprCB, &desc);

    /* The second phase should have located the first node */

    opr1 = desc.trFirst; assert(opr1);

RMV:

    /* At this point, our subtree starts at "opr1" and ends at "tree" */

    next = tree->gtNext;
    prev = opr1->gtPrev;

    /* Set the next node's prev field */

    next->gtPrev = prev;

    /* Is "opr1" the very first node in the tree list? */

    if  (prev == NULL)
    {
        /* Make sure the list indeed starts at opr1 */

        assert(list->gtStmt.gtStmtList == opr1);

        /* We have a new start for the list */

        list->gtStmt.gtStmtList = next;
    }
    else
    {
        /* Not the first node, update the previous node's next field */

        opr1->gtPrev->gtNext = next;
    }

#if 0
    printf("Subtree is gone:\n");
    dispNodeList(list%08X->gtStmt.gtStmtList, true);
    printf("\n\n\n");
#endif

}

#endif // CSELENGTH
#endif // 0
/*
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                          BasicBlock                                       XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/


#if     MEASURE_BLOCK_SIZE
/* static  */
size_t              BasicBlock::s_Size;
/* static */
size_t              BasicBlock::s_Count;
#endif

#ifdef DEBUG
 // The max # of tree nodes in any BB
/* static */
unsigned            BasicBlock::s_nMaxTrees;
#endif


/*****************************************************************************
 *
 *  Allocate a basic block but don't append it to the current BB list.
 */

BasicBlock *        Compiler::bbNewBasicBlock(BBjumpKinds jumpKind)
{
    BasicBlock *    block;

    /* Allocate the block descriptor and zero it out */

    block = (BasicBlock *) compGetMem(sizeof(*block));

#if     MEASURE_BLOCK_SIZE
    BasicBlock::s_Count += 1;
    BasicBlock::s_Size  += sizeof(*block);
#endif

#ifdef DEBUG
    // fgLookupBB() is invalid until fgInitBBLookup() is called again.
    fgBBs = (BasicBlock**)0xCDCD;
#endif

    // ISSUE: The following memset is pretty expensive - do something else?

    memset(block, 0, sizeof(*block));

    // scopeInfo needs to be able to differentiate between blocks which
    // correspond to some instrs (and so may have some LocalVarInfo
    // boundaries), or have been inserted by the JIT
    // block->bbCodeSize = 0; // The above memset() does this

    /* Give the block a number, set the ancestor count and weight */

    block->bbNum      = ++fgBBcount;
    block->bbRefs     = 1;
    block->bbWeight   = BB_UNITY_WEIGHT;

    block->bbStkTemps = NO_BASE_TMP;

    /* Record the jump kind in the block */

    block->bbJumpKind = jumpKind;

    if (jumpKind == BBJ_THROW)
        block->bbSetRunRarely();

    return block;
}

/*****************************************************************************
 *
 *  Mark a block as rarely run, we also don't want to have a loop in a
 *   rarely run block, and we set it's weight to zero.
 */

void                BasicBlock::bbSetRunRarely()
{
    bbFlags  |= BBF_RUN_RARELY;  // This block is never/rarely run
    bbFlags  &= ~BBF_LOOP_HEAD;  // Don't bother with any loops
    bbWeight  = 0;               // Don't count LclVars uses
}

/*****************************************************************************
 *
 *  Can a BasicBlock be inserted after this without altering the flowgraph
 */

bool                BasicBlock::bbFallsThrough()
{
    switch (bbJumpKind)
    {

    case BBJ_THROW:
    case BBJ_RET:
    case BBJ_RETURN:
    case BBJ_ALWAYS:
    case BBJ_LEAVE:
    case BBJ_SWITCH:
        return false;

    case BBJ_NONE:
    case BBJ_COND:
        return true;

    case BBJ_CALL:
        return ((bbFlags & BBF_RETLESS_CALL) == 0);
    
    default:
        assert(!"Unknown bbJumpKind in bbFallsThrough()");
        return true;
    }
}


/*****************************************************************************
 *
 *  If the given block is an unconditional jump, return the (final) jump
 *  target (otherwise simply return the same block).
 */

BasicBlock *        BasicBlock::bbJumpTarget()
{
    BasicBlock *  block = this;
    int i = 0;

    while (block->bbJumpKind == BBJ_ALWAYS &&
           block->bbTreeList == 0)
    {
        if (i > 64)      // break infinite loops
            break;
        block = block->bbJumpDest;
        i++;
    }

    return block;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\host.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*****************************************************************************/

#ifdef DEBUG
#define printf logf

class Compiler;
class LogEnv {
public:
	LogEnv(ICorJitInfo* aCompHnd);
	~LogEnv();
	static LogEnv* cur();			// get current logging environement
	static void cleanup();			// clean up cached information (TLS ID)
	void setCompiler(Compiler* val) { const_cast<Compiler*&>(compiler) = val; }

	ICorJitInfo* const compHnd;
	Compiler* const compiler;
private:
	static int tlsID;
	LogEnv* next;
};

void logf(const char* ...);
void logf(unsigned level, const char* fmt, ...);

extern  "C" 
void    __cdecl     assertAbort(const char *why, const char *file, unsigned line);

#undef  assert
#define assert(p)   (void)((p) || (assertAbort(#p, __FILE__, __LINE__),0))
#else

#undef  assert
#define assert(p)		(void) 0
#endif

/*****************************************************************************/
#ifndef _HOST_H_
#define _HOST_H_
/*****************************************************************************/

#pragma warning(disable:4237)

/*****************************************************************************/

#if _MSC_VER < 1100

enum bool
{
    false = 0,
    true  = 1
};

#endif

/*****************************************************************************/

const   size_t      OS_page_size = (4*1024);

/*****************************************************************************/

#ifdef  __ONE_BYTE_STRINGS__
#define _char_type  t_sgnInt08
#else
#define _char_type  t_sgnInt16
#endif

/*****************************************************************************/

#define size2mem(s,m)   (offsetof(s,m) + sizeof(((s *)0)->m))

/*****************************************************************************/

enum yesNo
{
    YN_ERR,
    YN_NO,
    YN_YES
};

/*****************************************************************************/
#endif
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\gentree.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                          GenTree                                          XX
XX                                                                           XX
XX  This is the node in the semantic tree graph. It represents the operation XX
XX  corresponing to the node, and other information during code-gen          XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

/*****************************************************************************/
#ifndef _GENTREE_H_
#define _GENTREE_H_
/*****************************************************************************/

#include "vartype.h"    // For "var_types"
#include "target.h"     // For "regNumber"

/*****************************************************************************/

enum _genTreeOps_enum
{
    #define GTNODE(en,sn,cm,ok) en,
    #include "gtlist.h"
    #undef  GTNODE

    GT_COUNT
};

#ifdef DEBUG
typedef _genTreeOps_enum genTreeOps;
#else
typedef BYTE genTreeOps;
#endif

/*****************************************************************************
 *
 *  The following enum defines a set of bit flags that can be used
 *  to classify expression tree nodes. Note that some operators will
 *  have more than one bit set, as follows:
 *
 *          GTK_CONST    implies    GTK_LEAF
 *          GTK_RELOP    implies    GTK_BINOP
 *          GTK_LOGOP    implies    GTK_BINOP
 */

enum genTreeKinds
{
    GTK_SPECIAL = 0x0000,       // unclassified operator (special handling reqd)

    GTK_CONST   = 0x0001,       // constant     operator
    GTK_LEAF    = 0x0002,       // leaf         operator
    GTK_UNOP    = 0x0004,       // unary        operator
    GTK_BINOP   = 0x0008,       // binary       operator
    GTK_RELOP   = 0x0010,       // comparison   operator
    GTK_LOGOP   = 0x0020,       // logical      operator
    GTK_ASGOP   = 0x0040,       // assignment   operator

    GTK_COMMUTE = 0x0080,       // commutative  operator

    /* Define composite value(s) */

    GTK_SMPOP   = (GTK_UNOP|GTK_BINOP|GTK_RELOP|GTK_LOGOP)
};

/*****************************************************************************/

#define SMALL_TREE_NODES    1

/*****************************************************************************/

enum _gtCallTypes_enum
{
    CT_USER_FUNC,       // User function
    CT_HELPER,          // Jit-helper
    CT_DESCR,           // @TODO: Obsolete but the name is used by the RISC emitter
    CT_INDIRECT,        // Indirect call

    CT_COUNT            // fake entry (must be last)
};

#ifdef DEBUG
typedef _gtCallTypes_enum gtCallTypes;
#else
typedef BYTE gtCallTypes;
#endif



/*****************************************************************************/

struct                  BasicBlock;

/*****************************************************************************/

typedef struct GenTree *  GenTreePtr;

/*****************************************************************************/
#pragma pack(push, 4)
/*****************************************************************************/

struct GenTree
{
    genTreeOps          gtOper;
    var_types           gtType;
    genTreeOps          OperGet() { return (genTreeOps)gtOper; };
    var_types           TypeGet() { return (var_types )gtType; };

    unsigned char       gtCostEx;     // estimate of expression execution cost
    unsigned char       gtCostSz;     // estimate of expression code size cost

#define MAX_COST        UCHAR_MAX
#define IND_COST_EX     3	      // execution cost for an indirection

#if CSE

#define NO_CSE           (0)

#define IS_CSE_INDEX(x)  (x != 0)
#define IS_CSE_USE(x)    (x > 0)
#define IS_CSE_DEF(x)    (x < 0)
#define GET_CSE_INDEX(x) ((x > 0) ? x : -x)
#define TO_CSE_DEF(x)    (-x)

#endif // end CSE

    signed char       gtCSEnum;       // 0 or the CSE index (negated if def)
                                      // valid only for CSE expressions
    union {
#if ASSERTION_PROP
      unsigned char     gtAssertionNum; // 0 or Assertion table index
                                        // valid only for non-GT_STMT nodes
#endif
#if TGT_x86
      unsigned char     gtStmtFPrvcOut; // FP regvar count on exit
                                        // valid only for GT_STMT nodes
#endif
    };

#if TGT_x86

    regMaskSmall        gtRsvdRegs;     // set of fixed trashed  registers
    regMaskSmall        gtUsedRegs;     // set of used (trashed) registers
    unsigned char       gtFPlvl;        // x87 stack depth at this node

#else // not TGT_x86

    unsigned char       gtTempRegs;     // temporary registers used by op
    regMaskSmall        gtIntfRegs;     // registers used at this node

#endif

    union
    {
       regNumberSmall   gtRegNum;       // which register      the value is in
       regPairNoSmall   gtRegPair;      // which register pair the value is in
    };

    unsigned            gtFlags;        // see GTF_xxxx below

    union
    {
        VARSET_TP       gtLiveSet;      // set of variables live after op - not used for GT_STMT
#if defined(DEBUGGING_SUPPORT) || defined(DEBUG)
        IL_OFFSETX      gtStmtILoffsx;  // instr offset (if available) - only for GT_STMT nodes
#endif
    };

    //---------------------------------------------------------------------
    //  The first set of flags can be used with a large set of nodes, and
    //  thus they must all have distinct values. That is, one can test any
    //  expression node for one of these flags.
    //---------------------------------------------------------------------

    #define GTF_ASG             0x00000001  // sub-expression contains an assignment
    #define GTF_CALL            0x00000002  // sub-expression contains a  func. call
    #define GTF_EXCEPT          0x00000004  // sub-expression might throw an exception
    #define GTF_GLOB_REF        0x00000008  // sub-expression uses global variable(s)
    #define GTF_OTHER_SIDEEFF   0x00000010  // sub-expression has other side effects

    #define GTF_SIDE_EFFECT     (GTF_ASG|GTF_CALL|GTF_EXCEPT|GTF_OTHER_SIDEEFF)
    #define GTF_GLOB_EFFECT     (GTF_SIDE_EFFECT|GTF_GLOB_REF)

    #define GTF_REVERSE_OPS     0x00000020  // second operand should be eval'd first
    #define GTF_REG_VAL         0x00000040  // operand is sitting in a register (or part of a TYP_LONG operand)

#ifdef DEBUG
    #define GTF_MORPHED         0x00000080  // the node has been morphed (in the global morphing phase)
#endif
    #define GTF_SPILLED         0x00000080  // the value   has been spilled
    #define GTF_SPILLED_OPER    0x00000100  // sub-operand has been spilled
    #define GTF_SPILLED_OP2     0x00000200  // both sub-operands have been spilled

    #define GTF_REDINDEX_CHECK  0x00000100  // Used for redundant range checks. Disjoint from GTF_SPILLED_OPER

    #define GTF_ZF_SET          0x00000400  // the zero/sign flag  set to the operand
    #define GTF_CC_SET          0x00000800  // all condition flags set to the operand

#if CSE
    #define GTF_DEAD            0x00001000  // this node won't be used any more
    #define GTF_MAKE_CSE        0x00002000  // try hard to make this into CSE
#endif
    #define GTF_DONT_CSE        0x00004000  // don't bother CSE'ing this expr
    #define GTF_COLON_COND      0x00008000  // this node is conditionally executed (part of ? :)

#if defined(DEBUG) && defined(SMALL_TREE_NODES)
    #define GTF_NODE_LARGE      0x00010000
    #define GTF_NODE_SMALL      0x00020000

    // Property of the node itself, not the gtOper
    #define GTF_NODE_MASK       (GTF_COLON_COND | GTF_MORPHED   | \
                                 GTF_NODE_SMALL | GTF_NODE_LARGE)
#else
    #define GTF_NODE_MASK       (GTF_COLON_COND)
#endif

    #define GTF_BOOLEAN         0x00040000  // value is known to be 0/1

    #define GTF_SMALL_OK        0x00080000  // actual small int sufficient

    #define GTF_UNSIGNED        0x00100000  // with GT_CAST:   the source operand is an unsigned type
                                            // with operators: the specified node is an unsigned operator

    #define GTF_REG_ARG         0x00200000  // the specified node is a register argument

    #define GTF_CONTEXTFUL      0x00400000  // TYP_REF node with contextful class

#if TGT_RISC
    #define GTF_DEF_ADDRMODE    0x00800000  // address mode which may not be ready yet
#endif

    #define GTF_COMMON_MASK     0x00FFFFFF  // mask of all the flags above
    //---------------------------------------------------------------------
    //  The following flags can be used only with a small set of nodes, and
    //  thus their values need not be distinct (other than within the set
    //  that goes with a particular node/nodes, of course). That is, one can
    //  only test for one of these flags if the 'gtOper' value is tested as
    //  well to make sure it's the right opetrator for the particular flag.
    //---------------------------------------------------------------------

    #define GTF_VAR_DEF         0x80000000  // GT_LCL_VAR -- this is a definition
    #define GTF_VAR_USEASG      0x40000000  // GT_LCL_VAR -- this is a use/def for a x<op>=y
    #define GTF_VAR_USEDEF      0x20000000  // GT_LCL_VAR -- this is a use/def as in x=x+y (only the lhs x is tagged)
    #define GTF_VAR_CAST        0x10000000  // GT_LCL_VAR -- has been explictly cast (variable node may not be type of local)

    #define GTF_REG_BIRTH       0x08000000  // GT_REG_VAR -- enregistered variable born here
    #define GTF_REG_DEATH       0x04000000  // GT_REG_VAR -- enregistered variable dies here

    #define GTF_CALL_UNMANAGED  0x80000000  // GT_CALL    -- direct call to unmanaged code
    #define GTF_CALL_INTF       0x40000000  // GT_CALL    -- interface call?
    #define GTF_CALL_VIRT       0x20000000  // GT_CALL    -- virtual   call?
    #define GTF_CALL_VIRT_RES   0x10000000  // GT_CALL    -- resolvable virtual call. Can call direct
    #define GTF_CALL_POP_ARGS   0x08000000  // GT_CALL    -- caller pop arguments?
//  #define GTF_CALL_REG_SAVE   0x02000000  // GT_CALL    -- call preserves all integer regs
    #define GTF_CALL_REG_SAVE   0x00000000  // GT_CALL    -- (disabled) call preserves all integer regs
    #define GTF_CALL_FPU_SAVE   0x01000000  // GT_CALL    -- call preserves all fpu regs

#ifdef DEBUG
    #define GTFD_NOP_BASH       0x02000000  // GT_NOP     -- Node was bashed to a NOP in fgComputeLife
    #define GTFD_VAR_CSE_REF    0x02000000  // GT_LCL_VAR -- This is a CSE LCL_VAR node
#endif

    #define GTF_NOP_DEATH       0x40000000  // GT_NOP     -- operand dies here
    #define GTF_NOP_RNGCHK      0x80000000  // GT_NOP     -- checked array index

    #define GTF_INX_RNGCHK      0x80000000  // GT_INDEX   -- checked array index

    #define GTF_IND_RNGCHK      0x80000000  // GT_IND     -- checked array index

    #define GTF_IND_OBJARRAY    0x20000000  // GT_IND     -- the array holds objects (effects layout of Arrays)
    #define GTF_IND_TGTANYWHERE 0x10000000  // GT_IND     -- the target could be anywhere
    #define GTF_IND_TLS_REF     0x08000000  // GT_IND     -- the target is accessed via TLS
    #define GTF_IND_FIELD       0x04000000  // GT_IND     -- the target is a field of an object
    #define GTF_IND_SHARED      0x02000000  // GT_IND     -- the target is a shared field access
    #define GTF_IND_INVARIANT   0x01000000  // GT_IND     -- the target is invariant (a prejit indirection)

    #define GTF_ADDR_ONSTACK    0x80000000  // GT_ADDR    -- this expression is guarenteed to be on the stack

    #define GTF_ALN_CSEVAL      0x80000000  // GT_ARR_LENREF -- copied for CSE
    #define GTF_ALN_OFFS_MASK   0x0F000000  // holds the offset (in dwords) (usually 2)
    #define GTF_ALN_OFFS_SHIFT  24

    #define GTF_MUL_64RSLT      0x80000000  // GT_MUL     -- produce 64-bit result

    #define GTF_MOD_INT_RESULT  0x80000000  // GT_MOD,    -- the real tree represented by this
                                            // GT_UMOD       node evaluates to an int even though
                                            //               its type is long.  The result is
                                            //               placed in the low member of the
                                            //               reg pair

    #define GTF_RELOP_NAN_UN    0x80000000  // GT_<relop> -- Is branch taken if ops are NaN?
    #define GTF_RELOP_JMP_USED  0x40000000  // GT_<relop> -- result of compare used for jump or ?:
    #define GTF_RELOP_QMARK     0x20000000  // GT_<relop> -- the node is the condition for ?:

    #define GTF_ICON_HDL_MASK   0xF0000000  // Bits used by handle types below

    #define GTF_ICON_SCOPE_HDL  0x10000000  // GT_CNS_INT -- constant is a scope handle
    #define GTF_ICON_CLASS_HDL  0x20000000  // GT_CNS_INT -- constant is a class handle
    #define GTF_ICON_METHOD_HDL 0x30000000  // GT_CNS_INT -- constant is a method handle
    #define GTF_ICON_FIELD_HDL  0x40000000  // GT_CNS_INT -- constant is a field handle
    #define GTF_ICON_STATIC_HDL 0x50000000  // GT_CNS_INT -- constant is a handle to static data
    #define GTF_ICON_STR_HDL    0x60000000  // GT_CNS_INT -- constant is a string handle
    #define GTF_ICON_PSTR_HDL   0x70000000  // GT_CNS_INT -- constant is a ptr to a string handle
    #define GTF_ICON_PTR_HDL    0x80000000  // GT_CNS_INT -- constant is a ldptr handle
    #define GTF_ICON_VARG_HDL   0x90000000  // GT_CNS_INT -- constant is a var arg cookie handle
    #define GTF_ICON_PINVKI_HDL 0xA0000000  // GT_CNS_INT -- constant is a pinvoke calli handle
    #define GTF_ICON_TOKEN_HDL  0xB0000000  // GT_CNS_INT -- constant is a token handle
    #define GTF_ICON_TLS_HDL    0xC0000000  // GT_CNS_INT -- constant is a TLS ref with offset
    #define GTF_ICON_FTN_ADDR   0xD0000000  // GT_CNS_INT -- constant is a function address
    #define GTF_ICON_CID_HDL    0xE0000000  // GT_CNS_INT -- constant is a class ID handle


#if     TGT_SH3
    #define GTF_SHF_NEGCNT      0x80000000  // GT_RSx     -- shift count negated?
#endif

    #define GTF_OVERFLOW        0x10000000  // GT_ADD, GT_SUB, GT_MUL, - Need overflow check
                                            // GT_ASG_ADD, GT_ASG_SUB,
                                            // GT_CAST
                                            // Use gtOverflow(Ex)() to check this flag

    //----------------------------------------------------------------

    #define GTF_STMT_CMPADD     0x80000000  // GT_STMT    -- added by compiler
    #define GTF_STMT_HAS_CSE    0x40000000  // GT_STMT    -- CSE def or use was subsituted

    //----------------------------------------------------------------

    GenTreePtr          gtNext;
    GenTreePtr          gtPrev;

#ifdef DEBUG
    unsigned            gtSeqNum;           // liveness traversal order 
#endif

    union
    {
        /*  NOTE: Any tree nodes that are larger than 8 bytes (two ints or
            pointers) must be flagged as 'large' in GenTree::InitNodeSize().
         */

        struct
        {
            GenTreePtr      gtOp1;
            GenTreePtr      gtOp2;
        }
                        gtOp;

        struct
        {
            unsigned        gtVal1;
            unsigned        gtVal2;
        }
                        gtVal;

        /* gtIntCon -- integer constant (GT_CNS_INT) */

        struct
        {
            long            gtIconVal;

#if defined(JIT_AS_COMPILER) || defined (LATE_DISASM)

            /*  If the constant was morphed from some other node,
                these fields enable us to get back to what the node
                originally represented. See use of gtNewIconHandleNode()
             */

            union
            {
                /* Template struct - The significant field of the other
                 * structs should overlap exactly with this struct
                 */

                struct
                {
                    unsigned        gtIconHdl1;
                    void *          gtIconHdl2;
                }
                                    gtIconHdl;

                /* GT_FIELD, etc */

                struct
                {
                    unsigned        gtIconCPX;
                    CORINFO_CLASS_HANDLE    gtIconCls;
                };
            };
#endif
        }
                        gtIntCon;

        /* gtLngCon -- long    constant (GT_CNS_LNG) */

        struct
        {
            __int64         gtLconVal;
        }
                        gtLngCon;

        /* gtDblCon -- double  constant (GT_CNS_DBL) */

        struct
        {
            double          gtDconVal;
        }
                        gtDblCon;

        /* gtStrCon -- string  constant (GT_CNS_STR) */

        struct
        {
            unsigned        gtSconCPX;
            CORINFO_MODULE_HANDLE    gtScpHnd;
        }
                        gtStrCon;

        /* gtLclVar -- local variable   (GT_LCL_VAR) */

        struct
        {
            unsigned        gtLclNum;
            IL_OFFSET       gtLclILoffs;    // instr offset of ref (only for debug info)
        }
                        gtLclVar;

        /* gtLclFld -- local variable field  (GT_LCL_FLD) */

        struct
        {
            unsigned        gtLclNum;
            unsigned        gtLclOffs;      // offset into the variable to access
        }
                        gtLclFld;

        /* gtCast -- conversion to a different type  (GT_CAST) */

        struct
        {
            GenTreePtr      gtCastOp;
            var_types       gtCastType;
        }
                        gtCast;

        /* gtField  -- data member ref  (GT_FIELD) */

        struct
        {
            GenTreePtr      gtFldObj;
            CORINFO_FIELD_HANDLE    gtFldHnd;
#if HOIST_THIS_FLDS
            unsigned short  gtFldHTX;       // hoist index
#endif
        }
                        gtField;

        /* gtCall   -- method call      (GT_CALL) */

        struct
        {
            GenTreePtr      gtCallArgs;             // list of the arguments
            GenTreePtr      gtCallObjp;
            GenTreePtr      gtCallRegArgs;
            unsigned short  regArgEncode;           // argument register mask

#define     GTF_CALL_M_CAN_TAILCALL   0x0001        // GT_CALL -- the call can be converted to a tailcall
#define     GTF_CALL_M_TAILCALL       0x0002        // GT_CALL -- the call is a tailcall
#define     GTF_CALL_M_TAILREC        0x0004        // GT_CALL -- this is a tail-recursive call
#define     GTF_CALL_M_RETBUFFARG     0x0008        // GT_CALL -- first parameter is the return buffer argument
#define     GTF_CALL_M_DELEGATE_INV   0x0010        // GT_CALL -- call to Delegate.Invoke
#define     GTF_CALL_M_NOGCCHECK      0x0020        // GT_CALL -- not a call for computing full interruptability

            unsigned char   gtCallMoreFlags;        // in addition to gtFlags
            gtCallTypes     gtCallType;
            GenTreePtr      gtCallCookie;           // only used for CALLI unmanaged calls

            union
            {
      CORINFO_METHOD_HANDLE gtCallMethHnd;          // CT_USER_FUNC
                GenTreePtr  gtCallAddr;             // CT_INDIRECT
            };
        }
                        gtCall;

#if INLINE_MATH

        /* gtMath   -- math intrinsic   (binary op with an additional field) */

        struct
        {
            GenTreePtr      gtOp1;
            GenTreePtr      gtOp2;
          CorInfoIntrinsics gtMathFN;
        }
                        gtMath;

#endif

        /* gtIndex -- array access */

        struct
        {
            GenTreePtr      gtIndOp1;       // The pointer to indirect
            GenTreePtr      gtIndRngFailBB; // Label to jump to for array-index-out-of-range
            unsigned        gtIndElemSize;  // size of elements in the array
        }
                        gtIndex;

        /* gtInd -- indirect mem access (fields, arrays, C ptrs, etc) (GT_IND)
          genIsAddrMode(), gtCrackIndexExpr(), optParseArrayRef(),
          genCreateAddrMode() can be used to parse GT_IND trees */

        struct
        {
            GenTreePtr      gtIndOp1;       // The pointer to indirect
            GenTreePtr      gtIndRngFailBB; // Label to jump to for array-index-out-of-range

            // Note that the following fields are only present if GTF_IND_RNGCHK is on
            // This is important as our node size logic uses this

#if     CSELENGTH
            /* If  (gtInd != NULL) && (gtInd->gtFlags & GTF_IND_RNGCHK),
               gtInd.gtIndLen (GT_ARR_LENREF) has the array-length for the range check */

            GenTreePtr      gtIndLen;       // array length node (optional)
#endif

            unsigned        gtRngChkIndex;  // Hash index for the index for range check tree for optOptimizeCSEs()

            /* Only out-of-ranges at same stack depth can jump to the same label (finding return address is easier)
               For delayed calling of fgSetRngChkTarget() so that the
               optimizer has a chance of eliminating some of the rng checks */
            unsigned        gtStkDepth;
            unsigned char   gtRngChkOffs;   // offset of the length which you do the range check on
        }
                        gtInd;

#if     CSELENGTH

        /* gtArrLen -- array length (GT_ARR_LENGTH or GT_ARR_LENREF)
           GT_ARR_LENREF hangs off gtInd.gtIndLen
           GT_ARR_LENGTH is used for "arr.length" */

        struct
        {
            GenTreePtr      gtArrLenAdr;    // the array address node
            GenTreePtr      gtArrLenCse;    // optional CSE def/use expr
        }
                        gtArrLen;
#endif

        /* gtArrElem -- array element (GT_ARR_ELEM) */

        struct
        {
            GenTreePtr      gtArrObj;

            #define         GT_ARR_MAX_RANK 3
            unsigned char   gtArrRank;                  // Rank of the array
            GenTreePtr      gtArrInds[GT_ARR_MAX_RANK]; // Indices

            unsigned char   gtArrElemSize;              // Size of the array elements
            var_types       gtArrElemType;              // Related to GTF_IND_OBJARRAY
        }
                        gtArrElem;

        /* gtStmt   -- 'statement expr' (GT_STMT)
         * NOTE: GT_STMT is a SMALL NODE in retail */

        struct
        {
            GenTreePtr      gtStmtExpr;     // root of the expression tree
            GenTreePtr      gtStmtList;     // first node (for  forward walks)
#ifdef DEBUG
            IL_OFFSET       gtStmtLastILoffs;// instr offset at end of stmt
#endif
        }
                        gtStmt;

        /* gtLdObj  -- 'push object' (GT_LDOBJ) */

        struct
        {
            GenTreePtr      gtOp1;          // pointer to object
       CORINFO_CLASS_HANDLE gtClass;        // object being loaded
        }
                        gtLdObj;


        //--------------------------------------------------------------------
        //  The following nodes used only within the code generator:
        //--------------------------------------------------------------------

        /* gtRegVar -- 'register variable'  (GT_REG_VAR) */

        struct
        {
            unsigned        gtRegVar;       // variable #
            regNumberSmall  gtRegNum;       // register #
        }
                        gtRegVar;

        /* gtClsVar -- 'static data member' (GT_CLS_VAR) */

        struct
        {
            CORINFO_FIELD_HANDLE    gtClsVarHnd;    //
        }
                        gtClsVar;

        /* gtLabel  -- code label target    (GT_LABEL) */

        struct
        {
            BasicBlock  *   gtLabBB;
        }
                        gtLabel;

        /* gtLargeOp represents the largest node type. Enforced in InitNodeSize() */

        struct
        {
            int         gtLargeOps[7];
        }
                        gtLargeOp;
    };


    static
    const   BYTE    gtOperKindTable[];

    static
    unsigned        OperKind(unsigned gtOper)
    {
        assert(gtOper < GT_COUNT);

        return  gtOperKindTable[gtOper];
    }

    unsigned        OperKind()
    {
        assert(gtOper < GT_COUNT);

        return  gtOperKindTable[gtOper];
    }

    static
    int             OperIsConst(genTreeOps gtOper)
    {
        return  (OperKind(gtOper) & GTK_CONST  ) != 0;
    }

    int             OperIsConst()
    {
        return  (OperKind(gtOper) & GTK_CONST  ) != 0;
    }

    static
    int             OperIsLeaf(genTreeOps gtOper)
    {
        return  (OperKind(gtOper) & GTK_LEAF   ) != 0;
    }

    int             OperIsLeaf()
    {
        return  (OperKind(gtOper) & GTK_LEAF   ) != 0;
    }

    static
    int             OperIsCompare(genTreeOps gtOper)
    {
        return  (OperKind(gtOper) & GTK_RELOP  ) != 0;
    }

    int             OperIsCompare()
    {
        return  (OperKind(gtOper) & GTK_RELOP  ) != 0;
    }

    static
    int             OperIsLogical(genTreeOps gtOper)
    {
        return  (OperKind(gtOper) & GTK_LOGOP  ) != 0;
    }

    int             OperIsLogical()
    {
        return  (OperKind(gtOper) & GTK_LOGOP  ) != 0;
    }

    #ifdef DEBUG
    static
    int             OperIsArithmetic(genTreeOps gtOper)
    {
        // @TODO [CONSIDER] [04/16/01] having a flag for this (GTK_ARTHMOP) as its a debug function, 
        // no need to have it for the moment
        return     gtOper==GT_ADD 
                || gtOper==GT_SUB        
                || gtOper==GT_MUL 
                || gtOper==GT_DIV
                || gtOper==GT_MOD
        
                || gtOper==GT_UDIV
                || gtOper==GT_UMOD

                || gtOper==GT_OR 
                || gtOper==GT_XOR
                || gtOper==GT_AND

                || gtOper==GT_LSH
                || gtOper==GT_RSH
                || gtOper==GT_RSZ;        
    }
    #endif
    

    static
    int             OperIsUnary(genTreeOps gtOper)
    {
        return  (OperKind(gtOper) & GTK_UNOP   ) != 0;
    }

    int             OperIsUnary()
    {
        return  (OperKind(gtOper) & GTK_UNOP   ) != 0;
    }

    static
    int             OperIsBinary(genTreeOps gtOper)
    {
        return  (OperKind(gtOper) & GTK_BINOP  ) != 0;
    }

    int             OperIsBinary()
    {
        return  (OperKind(gtOper) & GTK_BINOP  ) != 0;
    }

    static
    int             OperIsSimple(genTreeOps gtOper)
    {
        return  (OperKind(gtOper) & GTK_SMPOP  ) != 0;
    }

    int             OperIsSimple()
    {
        return  (OperKind(gtOper) & GTK_SMPOP  ) != 0;
    }

    static
    int             OperIsCommutative(genTreeOps gtOper)
    {
        return  (OperKind(gtOper) & GTK_COMMUTE) != 0;
    }

    int             OperIsCommutative()
    {
        return  (OperKind(gtOper) & GTK_COMMUTE) != 0;
    }

    int             OperIsAssignment()
    {
        return  (OperKind(gtOper) & GTK_ASGOP) != 0;
    }

    GenTreePtr      gtGetOp2()
    {
        /* gtOp.gtOp2 is only valid for GTK_BINARY nodes.
           GTK_UNARY nodes should use gtVal.gtVal2 if they need */

        return OperIsBinary() ? gtOp.gtOp2 : NULL;
    }

    GenTreePtr      gtEffectiveVal()
    {
        return (gtOper == GT_COMMA) ? gtOp.gtOp2->gtEffectiveVal()
                                    : this;
    }

#if     CSELENGTH
	unsigned gtArrLenOffset() { 
		assert(gtOper == GT_ARR_LENGTH || gtOper == GT_ARR_LENREF);
		return (gtFlags & GTF_ALN_OFFS_MASK) >> (GTF_ALN_OFFS_SHIFT-2); 
	}

	void gtSetArrLenOffset(unsigned val) { 
		assert(gtOper == GT_ARR_LENGTH || gtOper == GT_ARR_LENREF);
		assert(((val & 3) == 0) && (val >> 2) < GTF_ALN_OFFS_MASK);
		assert((gtFlags & GTF_ALN_OFFS_MASK) == 0);		// we only need to set once
		gtFlags |= val << (GTF_ALN_OFFS_SHIFT-2); 
	}
#endif

#if OPT_BOOL_OPS
    int             IsNotAssign();
#endif
    int             IsLeafVal();
    bool            OperMayThrow();

    unsigned        IsScaleIndexMul();
    unsigned        IsScaleIndexShf();
    unsigned        IsScaledIndex();


public:

#if SMALL_TREE_NODES
    static
    unsigned char   s_gtNodeSizes[];
#endif

    static
    void            InitNodeSize();

    bool            IsNodeProperlySized();

    void            CopyFrom(GenTreePtr src);

    static
    genTreeOps      ReverseRelop(genTreeOps relop);

    static
    genTreeOps      SwapRelop(genTreeOps relop);

    //---------------------------------------------------------------------

    static
    bool            Compare(GenTreePtr op1, GenTreePtr op2, bool swapOK = false);

    //---------------------------------------------------------------------
    #ifdef DEBUG
    //---------------------------------------------------------------------

    static
    const   char *  NodeName(genTreeOps op);

    //---------------------------------------------------------------------
    #endif
    //---------------------------------------------------------------------

    bool                        IsNothingNode       ();
                                                    
    void                        gtBashToNOP         ();

    void                        SetOper             (genTreeOps oper);  // set gtOper
    void                        SetOperResetFlags   (genTreeOps oper);  // set gtOper and reset flags

    void                        ChangeOperConst     (genTreeOps oper);  // ChangeOper(constOper)
    void                        ChangeOper          (genTreeOps oper);  // set gtOper and only keep GTF_COMMON_MASK flags
    void                        ChangeOperUnchecked (genTreeOps oper);

    bool                        IsVarAddr           ();
    bool                        gtOverflow          ();
    bool                        gtOverflowEx        ();
#ifdef DEBUG
    bool                        gtIsValid64RsltMul  ();
    static void                 gtDispFlags         (unsigned   flags);
#endif
};

/*****************************************************************************/
#pragma pack(pop)
/*****************************************************************************/

#if     SMALL_TREE_NODES

const
size_t              TREE_NODE_SZ_SMALL = offsetof(GenTree, gtOp) + sizeof(((GenTree*)0)->gtOp);

const
size_t              TREE_NODE_SZ_LARGE = sizeof(GenTree);

#endif

/*****************************************************************************
 * Types returned by GenTree::lvaLclVarRefs()
 */

enum varRefKinds
{
    VR_NONE       = 0x00,
    VR_IND_PTR    = 0x01,      // a pointer object-field
    VR_IND_SCL    = 0x02,      // a scalar  object-field
    VR_GLB_VAR    = 0x04,      // a global (clsVar)
    VR_INVARIANT  = 0x08,      // an invariant value
};

/*****************************************************************************/
#endif  // !GENTREE_H
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\instr.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*****************************************************************************/
#ifndef _INSTR_H_
#define _INSTR_H_
/*****************************************************************************/

#define BAD_CODE    0xFFFFFF        // better not match a real encoding!

/*****************************************************************************/

enum _instruction_enum
{

#if     TGT_x86

    #define INST0(id, nm, fp, um, rf, wf, ss, mr                ) INS_##id,
    #define INST1(id, nm, fp, um, rf, wf, ss, mr                ) INS_##id,
    #define INST2(id, nm, fp, um, rf, wf, ss, mr, mi            ) INS_##id,
    #define INST3(id, nm, fp, um, rf, wf, ss, mr, mi, rm        ) INS_##id,
    #define INST4(id, nm, fp, um, rf, wf, ss, mr, mi, rm, a4    ) INS_##id,
    #define INST5(id, nm, fp, um, rf, wf, ss, mr, mi, rm, a4, rr) INS_##id,
    #include "instrs.h"
    #undef  INST0
    #undef  INST1
    #undef  INST2
    #undef  INST3
    #undef  INST4
    #undef  INST5

#elif   TGT_SH3

    #define INST1(id, nm, bd, um, rf, wf, rx, wx, br, i1        ) INS_##id,
    #define INST2(id, nm, bd, um, rf, wf, rx, wx, br, i1, i2    ) INS_##id,
    #define INST3(id, nm, bd, um, rf, wf, rx, wx, br, i1, i2, i3) INS_##id,
    #include "instrSH3.h"
    #undef  INST1
    #undef  INST2
    #undef  INST3

#elif   TGT_MIPS32

    #define INST1(id, nm, bd, um, rf, wf, rx, wx, br, i1        ) INS_##id,
    #include "instrMIPS.h"
    #undef  INST1
    #undef  INST2
    #undef  INST3

#elif   TGT_ARM

    #define INST1(id, nm, bd, um, rf, wf, rx, wx, br, i1        ) INS_##id,
    #define INST2(id, nm, bd, um, rf, wf, rx, wx, br, i1, i2    ) INS_##id,
    #define INST3(id, nm, bd, um, rf, wf, rx, wx, br, i1, i2, i3) INS_##id,
    #include "instrARM.h"
    #undef  INST1
    #undef  INST2
    #undef  INST3

#elif   TGT_PPC

    #define INST1(id, nm, bd, um, rf, wf, rx, wx, br, i1        ) INS_##id,
    #include "instrPPC.h"
    #undef  INST1
    #undef  INST2
    #undef  INST3

#else

    #error  Unknown target

#endif

    INS_none,
    INS_count = INS_none
};

#ifdef DEBUG
typedef _instruction_enum instruction;
#else
typedef BYTE instruction;
#endif


/*****************************************************************************/

enum insUpdateModes
{
    IUM_RD,
    IUM_WR,
    IUM_RW,
};

/*****************************************************************************/

enum emitJumpKind
{
    EJ_NONE,

    #define JMP_SMALL(en, nm, rev, op) EJ_##en,
    #define JMP_LARGE(en, nm, rev, op)
    #include "emitjmps.h"
    #undef  JMP_SMALL
    #undef  JMP_LARGE

    EJ_call,
    EJ_COUNT
};

/*****************************************************************************/

extern const
emitJumpKind    emitReverseJumpKinds[EJ_COUNT];

inline
emitJumpKind    emitReverseJumpKind(emitJumpKind jumpKind)
{
    assert(jumpKind < EJ_COUNT);
    return emitReverseJumpKinds[jumpKind];
}

/*****************************************************************************/

#if TGT_MIPSFP
enum opformat
{
    OPF_S = 16, OPF_D, OPF_W = 20
};
#endif

/*****************************************************************************/
#endif//_INSTR_H_
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\instr.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                           Instruction                                     XX
XX                                                                           XX
XX          The interface to generate a machine-instruction.                 XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

#include "jitpch.h"
#pragma hdrstop

#include "instr.h"
#include "emit.h"

/*****************************************************************************
 *
 *  The following table is used by the instIsFP()/instUse/DefFlags() helpers.
 */

BYTE                Compiler::instInfo[] =
{

    #if     TGT_x86
    #define INST0(id, nm, fp, um, rf, wf, ss, mr                 ) (INST_USE_FL*rf|INST_DEF_FL*wf|INST_FP*fp|INST_SPSCHD*ss),
    #define INST1(id, nm, fp, um, rf, wf, ss, mr                 ) (INST_USE_FL*rf|INST_DEF_FL*wf|INST_FP*fp|INST_SPSCHD*ss),
    #define INST2(id, nm, fp, um, rf, wf, ss, mr, mi             ) (INST_USE_FL*rf|INST_DEF_FL*wf|INST_FP*fp|INST_SPSCHD*ss),
    #define INST3(id, nm, fp, um, rf, wf, ss, mr, mi, rm         ) (INST_USE_FL*rf|INST_DEF_FL*wf|INST_FP*fp|INST_SPSCHD*ss),
    #define INST4(id, nm, fp, um, rf, wf, ss, mr, mi, rm, a4     ) (INST_USE_FL*rf|INST_DEF_FL*wf|INST_FP*fp|INST_SPSCHD*ss),
    #define INST5(id, nm, fp, um, rf, wf, ss, mr, mi, rm, a4, rr ) (INST_USE_FL*rf|INST_DEF_FL*wf|INST_FP*fp|INST_SPSCHD*ss),
    #include "instrs.h"
    #undef  INST0
    #undef  INST1
    #undef  INST2
    #undef  INST3
    #undef  INST4
    #undef  INST5
    #endif

    #if     TGT_SH3
    #define INST1(id, nm, bd, um, rf, wf, rx, wx, br, i1         ) (INST_USE_FL*rf|INST_DEF_FL*wf|INST_BD*(bd&1)|INST_BD_C*((bd&2)!=0)|INST_BR*br|INST_SPSCHD*(rx||wx)),
    #define INST2(id, nm, bd, um, rf, wf, rx, wx, br, i1, i2     ) (INST_USE_FL*rf|INST_DEF_FL*wf|INST_BD*(bd&1)|INST_BD_C*((bd&2)!=0)|INST_BR*br|INST_SPSCHD*(rx||wx)),
    #define INST3(id, nm, bd, um, rf, wf, rx, wx, br, i1, i2, i3 ) (INST_USE_FL*rf|INST_DEF_FL*wf|INST_BD*(bd&1)|INST_BD_C*((bd&2)!=0)|INST_BR*br|INST_SPSCHD*(rx||wx)),
    #include "instrSH3.h"
    #undef  INST1
    #undef  INST2
    #undef  INST3
    #endif

};

/*****************************************************************************/
#ifdef  DEBUG
/*****************************************************************************
 *
 *  Returns the string representation of the given CPU instruction.
 */

const   char *      Compiler::genInsName(instruction ins)
{
    static
    const char * const insNames[] =
    {

        #if TGT_x86
        #define INST0(id, nm, fp, um, rf, wf, mr, ss                 ) nm,
        #define INST1(id, nm, fp, um, rf, wf, mr, ss                 ) nm,
        #define INST2(id, nm, fp, um, rf, wf, mr, ss, mi             ) nm,
        #define INST3(id, nm, fp, um, rf, wf, mr, ss, mi, rm         ) nm,
        #define INST4(id, nm, fp, um, rf, wf, mr, ss, mi, rm, a4     ) nm,
        #define INST5(id, nm, fp, um, rf, wf, mr, ss, mi, rm, a4, rr ) nm,
        #include "instrs.h"
        #undef  INST0
        #undef  INST1
        #undef  INST2
        #undef  INST3
        #undef  INST4
        #undef  INST5
        #endif

        #if TGT_SH3
        #define INST1(id, nm, bd, um, rf, wf, rx, wx, br, i1        ) nm,
        #define INST2(id, nm, bd, um, rf, wf, rx, wx, br, i1, i2    ) nm,
        #define INST3(id, nm, bd, um, rf, wf, rx, wx, br, i1, i2, i3) nm,
        #include "instrSH3.h"
        #undef  INST1
        #undef  INST2
        #undef  INST3
        #endif

    };

    assert(ins < sizeof(insNames)/sizeof(insNames[0]));

    assert(insNames[ins]);

    return insNames[ins];
}

void    __cdecl     Compiler::instDisp(instruction ins, bool noNL, const char *fmt, ...)
{
    if  (dspCode)
    {
        /* Display the instruction offset within the emit block */

//      printf("[%08X:%04X]", genEmitter.emitCodeCurBlock(), genEmitter.emitCodeOffsInBlock());

        /* Display the FP stack depth (before the instruction is executed) */

#if TGT_x86
//      printf("[FP=%02u] ", genFPstkLevel);
#endif

        /* Display the instruction mnemonic */
        printf("        ");

        printf("            %-8s", genInsName(ins));

        if  (fmt)
        {
            va_list  args;
            va_start(args, fmt);
            logf(4, fmt,  args);
            va_end  (args);
        }

        if  (!noNL)
            printf("\n");
    }
}

/*****************************************************************************/
#endif//DEBUG
/*****************************************************************************/

void                Compiler::instInit()
{
}

/*****************************************************************************/
#if TGT_x86
/*****************************************************************************/

#if!INLINING

#undef inst_IV_handle
#undef instEmitDataFixup
#undef inst_CV

#if INDIRECT_CALLS
#undef inst_SM
#endif

#undef inst_CV_RV
#undef inst_CV_IV
#undef inst_RV_CV
#undef instEmit_vfnCall

#endif

/*****************************************************************************
 *
 *  @TODO [FIXHACK] [04/16/01] []: What a collossal hack, one of these we'll have to clean this up ...
 */

#if     DOUBLE_ALIGN
#define DOUBLE_ALIGN_BPREL_ARG  , genFPused || genDoubleAlign && varDsc->lvIsParam
#else
#define DOUBLE_ALIGN_BPREL_ARG
#endif

/*****************************************************************************
 *
 *  Return the size string (e.g. "word ptr") appropriate for the given size.
 */

#ifdef  DEBUG

const   char *      Compiler::genSizeStr(emitAttr attr)
{
    static
    const char * const sizes[] =
    {
        "",
        "byte  ptr ",
        "word  ptr ",
        0,
        "dword ptr ",
        0,
        0,
        0,
        "qword ptr ",
    };

    unsigned size = EA_SIZE(attr);

    assert(size == 0 || size == 1 || size == 2 || size == 4 || size == 8);

    if (EA_ATTR(size) == attr)
        return sizes[size];
    else if (attr == EA_GCREF)
        return "gword ptr ";
    else if (attr == EA_BYREF)
        return "bword ptr ";
    else if (EA_IS_DSP_RELOC(attr))
        return "rword ptr ";
    else
    {
        assert(!"Unexpected");
        return "unknw ptr ";
    }
}

#endif

/*****************************************************************************
 *
 *  Generate an instruction.
 */

void                Compiler::instGen(instruction ins)
{
#ifdef  DEBUG

#if     INLINE_MATH
    if    (ins != INS_fabs    &&
           ins != INS_fsqrt   &&
           ins != INS_fsin    &&
           ins != INS_fcos)
#endif
    assert(ins == INS_cdq     ||
           ins == INS_f2xm1   ||
           ins == INS_fchs    ||
           ins == INS_fld1    ||
           ins == INS_fld1    ||
           ins == INS_fldl2e  ||
           ins == INS_fldz    ||
           ins == INS_fprem   ||
           ins == INS_frndint ||
           ins == INS_fscale  ||
           ins == INS_int3    ||
           ins == INS_leave   ||
           ins == INS_movsb   ||
           ins == INS_movsd   ||
           ins == INS_nop     ||
           ins == INS_r_movsb ||
           ins == INS_r_movsd ||
           ins == INS_r_stosb ||
           ins == INS_r_stosd ||
           ins == INS_ret     ||
           ins == INS_sahf    ||
           ins == INS_stosb   ||
           ins == INS_stosd      );

#endif

    genEmitter->emitIns(ins);
}

/*****************************************************************************
 *
 *  Generate a jump instruction.
 */

void        Compiler::inst_JMP(emitJumpKind     jmp,
                               BasicBlock *     tgtBlock,
#if SCHEDULER
                               bool             except,
                               bool             moveable,
#endif
                               bool             newBlock)
{
    assert(tgtBlock->bbTgtStkDepth*sizeof(int) == genStackLevel || genFPused);

    const static
    instruction     EJtoINS[] =
    {
        INS_nop,

        #define JMP_SMALL(en, nm, rev, op) INS_##en,
        #define JMP_LARGE(en, nm, rev, op)
        #include "emitjmps.h"
        #undef  JMP_SMALL
        #undef  JMP_LARGE

        INS_call,
    };

    assert(jmp < sizeof(EJtoINS)/sizeof(EJtoINS[0]));

    genEmitter->emitIns_J(EJtoINS[jmp], except, moveable, tgtBlock);
}

/*****************************************************************************
 *
 *  Generate a set instruction.
 */

void                Compiler::inst_SET(emitJumpKind   condition,
                                       regNumber      reg)
{
    instruction     ins;

    /* Convert the condition to a string */

    switch (condition)
    {
    case EJ_js  : ins = INS_sets  ; break;
    case EJ_jns : ins = INS_setns ; break;
    case EJ_je  : ins = INS_sete  ; break;
    case EJ_jne : ins = INS_setne ; break;

    case EJ_jl  : ins = INS_setl  ; break;
    case EJ_jle : ins = INS_setle ; break;
    case EJ_jge : ins = INS_setge ; break;
    case EJ_jg  : ins = INS_setg  ; break;

    case EJ_jb  : ins = INS_setb  ; break;
    case EJ_jbe : ins = INS_setbe ; break;
    case EJ_jae : ins = INS_setae ; break;
    case EJ_ja  : ins = INS_seta  ; break;

    case EJ_jpe : ins = INS_setpe ; break;
    case EJ_jpo : ins = INS_setpo ; break;

    default:      NO_WAY("unexpected condition type");
    }

    assert(genRegMask(reg) & RBM_BYTE_REGS);

    genEmitter->emitIns_R(ins, EA_4BYTE, (emitRegs)reg);
}

/*****************************************************************************
 *
 *  Generate a "op reg" instruction.
 */

void        Compiler::inst_RV(instruction ins, regNumber reg, var_types type, emitAttr size)
{
    if (size == EA_UNKNOWN)
        size = emitActualTypeSize(type);

    genEmitter->emitIns_R(ins, size, (emitRegs)reg);
}

/*****************************************************************************
 *
 *  Generate a "op reg1, reg2" instruction.
 */

void                Compiler::inst_RV_RV(instruction ins, regNumber reg1,
                                                          regNumber reg2,
                                                          var_types type,
                                                          emitAttr  size)
{
    assert(ins == INS_test ||
           ins == INS_add  ||
           ins == INS_adc  ||
           ins == INS_sub  ||
           ins == INS_sbb  ||
           ins == INS_imul ||
           ins == INS_idiv ||
           ins == INS_cmp  ||
           ins == INS_mov  ||
           ins == INS_and  ||
           ins == INS_or   ||
           ins == INS_xor  ||
           ins == INS_xchg ||
           ins == INS_movsx||
           ins == INS_movzx||
           insIsCMOV(ins));

    if (size == EA_UNKNOWN)
        size = emitActualTypeSize(type);

    genEmitter->emitIns_R_R(ins, size, (emitRegs)reg1, (emitRegs)reg2);
}

/*****************************************************************************
 *
 *  Generate a "op icon" instruction.
 */

void                Compiler::inst_IV(instruction ins, long val)
{
    genEmitter->emitIns_I(ins,
                          EA_4BYTE,
                          val);
}

/*****************************************************************************
 *
 *  Generate a "op icon" instruction where icon is a handle of type specified
 *  by 'flags' and representing the CP index CPnum
 */

void                Compiler::inst_IV_handle(instruction    ins,
                                             long           val,
                                             unsigned       flags,
                                             unsigned       CPnum,
                                             CORINFO_CLASS_HANDLE   CLS)
{
#if!INLINING
    CLS = info.compScopeHnd;
#endif

#ifdef  JIT_AS_COMPILER

    execFixTgts     fixupKind;

    assert(ins == INS_push);

#ifdef  DEBUG
    instDisp(ins, false, "%d", val);
#endif

    fixupKind = (execFixTgts)((flags >> GTF_ICON_HDL_SHIFT)-1+FIX_TGT_CLASS_HDL);

    assert((GTF_ICON_CLASS_HDL  >> GTF_ICON_HDL_SHIFT)-1+FIX_TGT_CLASS_HDL == FIX_TGT_CLASS_HDL);
    assert((GTF_ICON_METHOD_HDL >> GTF_ICON_HDL_SHIFT)-1+FIX_TGT_CLASS_HDL == FIX_TGT_METHOD_HDL);
    assert((GTF_ICON_FIELD_HDL  >> GTF_ICON_HDL_SHIFT)-1+FIX_TGT_CLASS_HDL == FIX_TGT_FIELD_HDL);
    assert((GTF_ICON_STATIC_HDL >> GTF_ICON_HDL_SHIFT)-1+FIX_TGT_CLASS_HDL == FIX_TGT_STATIC_HDL);
    assert((GTF_ICON_STRCNS_HDL >> GTF_ICON_HDL_SHIFT)-1+FIX_TGT_CLASS_HDL == FIX_TGT_STRCNS_HDL);

    assert(!"NYI for pre-compiled code");

#else   // not JIT_AS_COMPILER

    genEmitter->emitIns_I(ins, EA_4BYTE_CNS_RELOC, val);

#endif  // JIT_AS_COMPILER

}

#if!INLINING
#define inst_IV_handle(ins,val,flags,cpx,cls) inst_IV_handle(ins,val,flags,cpx,0)
#endif

/*****************************************************************************
 *
 *  Generate a "op ST(n), ST(0)" instruction.
 */

void                Compiler::inst_FS(instruction ins, unsigned stk)
{
    assert(stk < 8);

#ifdef  DEBUG

    switch (ins)
    {
    case INS_fcompp:
        assert(stk == 1); break; // Implicit operand of compp is ST(1)
    case INS_fld:
    case INS_fxch:
        assert(!"don't do this. Do you want to use inst_FN() instead?");
    }

#endif

    genEmitter->emitIns_F_F0(ins, stk);
}

/*****************************************************************************
 *
 *  Generate a "op ST(0), ST(n)" instruction
 */

void                Compiler::inst_FN(instruction ins, unsigned stk)
{
    assert(stk < 8);

#ifdef  DEBUG

    switch (ins)
    {
    case INS_fst:
    case INS_fstp:
    case INS_faddp:
    case INS_fsubp:
    case INS_fsubrp:
    case INS_fmulp:
    case INS_fdivp:
    case INS_fdivrp:
    case INS_fcompp:
        assert(!"don't do this. Do you want to use inst_FS() instead?");
    }

#endif

    genEmitter->emitIns_F0_F(ins, stk);
}

/*****************************************************************************
 *
 *  Display a stack frame reference.
 */

inline
void                Compiler::inst_set_SV_var(GenTreePtr tree)
{
#ifdef  DEBUG

    assert(tree && tree->gtOper == GT_LCL_VAR);
    assert(tree->gtLclVar.gtLclNum < lvaCount);

    genEmitter->emitVarRefOffs = tree->gtLclVar.gtLclILoffs;

#endif//DEBUG
}

/*****************************************************************************
 *
 *  Generate a "op reg, icon" instruction.
 */

void                Compiler::inst_RV_IV(instruction    ins,
                                         regNumber      reg,
                                         long           val,
                                         var_types      type)
{
    emitAttr  size = emitTypeSize(type);

    assert(size != EA_8BYTE);

    genEmitter->emitIns_R_I(ins, size, (emitRegs)reg, val);
}

/*****************************************************************************
 *
 *  Generate a "op    offset <class variable address>" instruction.
 */

void                Compiler::inst_AV(instruction  ins,
                                      GenTreePtr   tree, unsigned offs)
{
    assert(ins == INS_push);

    assert(tree->gtOper == GT_CLS_VAR);
        // This is a jit data offset, not a normal class variable.
    assert(eeGetJitDataOffs(tree->gtClsVar.gtClsVarHnd) >= 0);
    assert(tree->TypeGet() == TYP_INT);

    /* Size equal to EA_OFFSET indicates "push offset clsvar" */

    genEmitter->emitIns_C(ins, EA_OFFSET, tree->gtClsVar.gtClsVarHnd, offs);
}

/*****************************************************************************
 *  Figure out the operands to address the tree.
 *  addr can be - a pointer to be indirected
 *                a caluclation to be done with LEA_AVAILABLE
 *                GT_ARR_ELEM
 */

void        Compiler::instGetAddrMode(GenTreePtr    addr,
                                      regNumber *   baseReg,
                                      unsigned *    indScale,
                                      regNumber *   indReg,
                                      unsigned *    cns)
{
    if (addr->gtOper == GT_ARR_ELEM)
    {
        /* For GT_ARR_ELEM, the addressibility registers are marked on
           gtArrObj and gtArrInds[0] */

        assert(addr->gtArrElem.gtArrObj->gtFlags & GTF_REG_VAL);
        *baseReg = addr->gtArrElem.gtArrObj->gtRegNum;

        assert(addr->gtArrElem.gtArrInds[0]->gtFlags & GTF_REG_VAL);
        *indReg = addr->gtArrElem.gtArrInds[0]->gtRegNum;

        if (jitIsScaleIndexMul(addr->gtArrElem.gtArrElemSize))
            *indScale = addr->gtArrElem.gtArrElemSize;
        else
            *indScale = 0;

        *cns = ARR_DIMCNT_OFFS(addr->gtArrElem.gtArrElemType)
            + 2 * sizeof(int) * addr->gtArrElem.gtArrRank;

        return;
    }

    /* Figure out what complex address mode to use */

    GenTreePtr  rv1, rv2;
    bool        rev;

    bool yes = genCreateAddrMode(addr,
                                 -1,
                                 true,
                                 0,
#if!LEA_AVAILABLE
                                 type,
#endif
                                 &rev,
                                 &rv1,
                                 &rv2,
#if SCALED_ADDR_MODES
                                 indScale,
#endif
                                 cns);

    assert(yes);
    assert(!rv1 || (rv1->gtFlags & GTF_REG_VAL));
    assert(!rv2 || (rv2->gtFlags & GTF_REG_VAL));

    *baseReg = rv1 ? rv1->gtRegNum : REG_NA;
    * indReg = rv2 ? rv2->gtRegNum : REG_NA;
}


/*****************************************************************************
 *
 *  Schedule an "ins reg, [r/m]" (rdst=true), or "ins [r/m], reg" (rdst=false)
 *  instruction (the r/m operand given by a tree). We also allow instructions
 *  of the form "ins [r/m], icon", these are signalled by setting 'cons' to
 *  true.
 */

void                Compiler::sched_AM(instruction  ins,
                                       emitAttr     size,
                                       regNumber    ireg,
                                       bool         rdst,
                                       GenTreePtr   addr,
                                       unsigned     offs,
                                       bool         cons,
                                       int          val)
{
    emitRegs       reg;
    emitRegs       rg2;
    emitRegs       irg = (emitRegs)ireg;

    /* Don't use this method for issuing calls. Use instEmit_xxxCall() */

    assert(ins != INS_call);

    assert(addr);
    assert(size != EA_UNKNOWN);

    assert(REG_EAX == SR_EAX);
    assert(REG_ECX == SR_ECX);
    assert(REG_EDX == SR_EDX);
    assert(REG_EBX == SR_EBX);
    assert(REG_ESP == SR_ESP);
    assert(REG_EBP == SR_EBP);
    assert(REG_ESI == SR_ESI);
    assert(REG_EDI == SR_EDI);

    /* Has the address been conveniently loaded into a register,
       or is it an absolute value ? */

    if  ((addr->gtFlags & GTF_REG_VAL) ||
         (addr->gtOper == GT_CNS_INT))
    {
        if (addr->gtFlags & GTF_REG_VAL)
        {
            /* The address is "[reg+offs]" */

            reg = (emitRegs)addr->gtRegNum;
        }
        else
        {
            /* The address is an absolute value */

            assert(addr->gtOper == GT_CNS_INT);

#ifdef RELOC_SUPPORT
            // Do we need relocations?
            if (opts.compReloc && (addr->gtFlags & GTF_ICON_HDL_MASK))
            {
                size = EA_SET_FLG(size, EA_DSP_RELOC_FLG);
            }
#endif
            reg = SR_NA;
            offs += addr->gtIntCon.gtIconVal;
        }

        if      (cons)
            genEmitter->emitIns_I_AR  (ins, size,      val, reg, offs);
        else if (rdst)
            genEmitter->emitIns_R_AR  (ins, size, irg,      reg, offs);
        else
            genEmitter->emitIns_AR_R  (ins, size, irg,      reg, offs);

        return;
    }

    /* Figure out what complex address mode to use */

    regNumber   baseReg, indReg;
    unsigned    indScale, cns;

    instGetAddrMode(addr, &baseReg, &indScale, &indReg, &cns);

    /* Add the constant offset value, if present */

    offs += cns;

    /* Is there an additional operand? */

    if  (indReg != REG_NA)
    {
        /* The additional operand must be sitting in a register */

        rg2 = emitRegs(indReg);

        /* Is the additional operand scaled? */

        if  (indScale)
        {
            /* Is there a base address operand? */

            if  (baseReg != REG_NA)
            {
                reg = emitRegs(baseReg);

                /* The address is "[reg1 + {2/4/8} * reg2 + offs]" */

                if      (cons)
                    genEmitter->emitIns_I_ARX(ins, size, val, reg, rg2, indScale, offs);
                else if (rdst)
                    genEmitter->emitIns_R_ARX(ins, size, irg, reg, rg2, indScale, offs);
                else
                    genEmitter->emitIns_ARX_R(ins, size, irg, reg, rg2, indScale, offs);
            }
            else
            {
                /* The address is "[{2/4/8} * reg2 + offs]" */

                if      (cons)
                    genEmitter->emitIns_I_AX (ins, size, val,      rg2, indScale, offs);
                else if (rdst)
                    genEmitter->emitIns_R_AX (ins, size, irg,      rg2, indScale, offs);
                else
                    genEmitter->emitIns_AX_R (ins, size, irg,      rg2, indScale, offs);
            }
        }
        else
        {
            assert(baseReg != REG_NA);
            reg = emitRegs(baseReg);

            /* The address is "[reg1 + reg2 + offs]" */

            if      (cons)
                genEmitter->emitIns_I_ARR(ins, size, val, reg, rg2, offs);
            else if (rdst)
                genEmitter->emitIns_R_ARR(ins, size, irg, reg, rg2, offs);
            else
                genEmitter->emitIns_ARR_R(ins, size, irg, reg, rg2, offs);
        }
    }
    else
    {
        unsigned        cpx = 0;
        CORINFO_CLASS_HANDLE    cls = 0;

        /* No second operand: the address is "[reg  + icon]" */

        assert(baseReg != REG_NA); reg = emitRegs(baseReg);

#ifdef  LATE_DISASM

        /*
            Keep in mind that non-static data members (GT_FIELD nodes) were
            transformed into GT_IND nodes - we keep the CLS/CPX information
            in the GT_CNS_INT node representing the field offset of the
            class member
         */

        if  ((addr->gtOp.gtOp2->gtOper == GT_CNS_INT) &&
             ((addr->gtOp.gtOp2->gtFlags & GTF_ICON_HDL_MASK) == GTF_ICON_FIELD_HDL))
        {
            /* This is a field offset - set the CPX/CLS values to emit a fixup */

            cpx = addr->gtOp.gtOp2->gtIntCon.gtIconCPX;
            cls = addr->gtOp.gtOp2->gtIntCon.gtIconCls;
        }

#endif

        if      (cons)
            genEmitter->emitIns_I_AR(ins, size, val, reg, offs, cpx, cls);
        else if (rdst)
            genEmitter->emitIns_R_AR(ins, size, irg, reg, offs, cpx, cls);
        else
            genEmitter->emitIns_AR_R(ins, size, irg, reg, offs, cpx, cls);
    }
}

/*****************************************************************************
 *
 *  Emit a "call [r/m]" instruction (the r/m operand given by a tree).
 */

void                Compiler::instEmit_indCall(GenTreePtr   call,
                                               size_t       argSize,
                                               size_t       retSize)
{
    GenTreePtr              addr;

    emitter::EmitCallType   emitCallType;

    emitRegs                brg = SR_NA;
    emitRegs                xrg = SR_NA;
    unsigned                mul = 0;
    unsigned                cns = 0;

    assert(call->gtOper == GT_CALL);

    assert(REG_EAX == SR_EAX);
    assert(REG_ECX == SR_ECX);
    assert(REG_EDX == SR_EDX);
    assert(REG_EBX == SR_EBX);
    assert(REG_ESP == SR_ESP);
    assert(REG_EBP == SR_EBP);
    assert(REG_ESI == SR_ESI);
    assert(REG_EDI == SR_EDI);

    /* Get hold of the function address */

    assert(call->gtCall.gtCallType == CT_INDIRECT);
    addr = call->gtCall.gtCallAddr;
    assert(addr);

    /* Is there an indirection? */

    if  (addr->gtOper != GT_IND)
    {
        if (addr->gtFlags & GTF_REG_VAL)
        {
            emitCallType = emitter::EC_INDIR_R;
            brg = (emitRegs)addr->gtRegNum;
        }
        else
        {
            if (addr->OperGet() != GT_CNS_INT)
            {
                assert(addr->OperGet() == GT_LCL_VAR);

                emitCallType = emitter::EC_INDIR_SR;
                cns = addr->gtLclVar.gtLclNum;
            }
            else
            {
#ifdef _WIN64
                __int64     funcPtr = addr->gtLngCon.gtLconVal;
#else
                unsigned    funcPtr = addr->gtIntCon.gtIconVal;
#endif

                genEmitter->emitIns_Call( emitter::EC_FUNC_ADDR,
                                          (void*) funcPtr,
                                          argSize,
                                          retSize,
                                          gcVarPtrSetCur,
                                          gcRegGCrefSetCur,
                                          gcRegByrefSetCur);
                return;
            }
        }
    }
    else
    {
        /* This is an indirect call */

        emitCallType = emitter::EC_INDIR_ARD;

        /* Get hold of the address of the function pointer */

        addr = addr->gtOp.gtOp1;

        /* Has the address been conveniently loaded into a register? */

        if  (addr->gtFlags & GTF_REG_VAL)
        {
            /* The address is "reg" */

            brg = (emitRegs)addr->gtRegNum;
        }
        else
        {
            bool            rev;

            GenTreePtr      rv1;
            GenTreePtr      rv2;

            /* Figure out what complex address mode to use */

            {
             bool yes = genCreateAddrMode(addr, -1, true, 0, &rev, &rv1, &rv2, &mul, &cns); assert(yes);
            }

            /* Get the additional operands if any */

            if  (rv1)
            {
                assert(rv1->gtFlags & GTF_REG_VAL); brg = (emitRegs)rv1->gtRegNum;
            }

            if  (rv2)
            {
                assert(rv2->gtFlags & GTF_REG_VAL); xrg = (emitRegs)rv2->gtRegNum;
            }
        }
    }

    assert(emitCallType == emitter::EC_INDIR_R || emitCallType == emitter::EC_INDIR_SR ||
           emitCallType == emitter::EC_INDIR_C || emitCallType == emitter::EC_INDIR_ARD);

    genEmitter->emitIns_Call( emitCallType,
                              NULL,                 // will be ignored
                              argSize,
                              retSize,
                              gcVarPtrSetCur,
                              gcRegGCrefSetCur,
                              gcRegByrefSetCur,
                              brg, xrg, mul, cns);  // addressing mode values
}

/*****************************************************************************
 *
 *  Emit an "op [r/m]" instruction (the r/m operand given by a tree).
 */

void                Compiler::instEmit_RM(instruction  ins,
                                          GenTreePtr   tree,
                                          GenTreePtr   addr,
                                          unsigned     offs)
{
    emitAttr   size;

    if (!instIsFP(ins))
        size = emitTypeSize(tree->TypeGet());
    else
        size = EA_ATTR(genTypeSize(tree->TypeGet()));

    sched_AM(ins, size, REG_NA, false, addr, offs);
}

/*****************************************************************************
 *
 *  Emit an "op [r/m], reg" instruction (the r/m operand given by a tree).
 */

void                Compiler::instEmit_RM_RV(instruction  ins,
                                             emitAttr     size,
                                             GenTreePtr   tree,
                                             regNumber    reg,
                                             unsigned     offs)
{
    assert(instIsFP(ins) == 0);

    sched_AM(ins, size, reg, false, tree, offs);
}

/*****************************************************************************
 *
 *  Generate an instruction that has one operand given by a tree (which has
 *  been made addressable).
 */

void                Compiler::inst_TT(instruction   ins,
                                      GenTreePtr    tree,
                                      unsigned      offs,
                                      int           shfv,
                                      emitAttr      size)
{
    if (size == EA_UNKNOWN)
    {
        if (instIsFP(ins))
            size = EA_ATTR(genTypeSize(tree->TypeGet()));
        else
            size = emitTypeSize(tree->TypeGet());
    }

AGAIN:

    /* Is the value sitting in a register? */

    if  (tree->gtFlags & GTF_REG_VAL)
    {
        regNumber       reg;

    LONGREG_TT:

        reg = tree->gtRegNum;

        /* Is this a floating-point instruction? */

        if  (isFloatRegType(tree->gtType))
        {
            assert(instIsFP(ins) && ins != INS_fst && ins != INS_fstp);
            assert(shfv == 0);

            inst_FS(ins, reg + genFPstkLevel);
            return;
        }

        assert(instIsFP(ins) == 0);

        if  (tree->gtType == TYP_LONG)
        {
            if  (offs)
            {
                assert(offs == sizeof(int));

                reg = genRegPairHi((regPairNo)reg);
            }
            else
                reg = genRegPairLo((regPairNo)reg);
        }

        /* Make sure it is not the "stack-half" of an enregistered long */

        if  (reg != REG_STK)
        {
                // For short types, indicate that the value is promoted to 4 bytes.
                // For longs, we are only emitting half of it so again set it to 4 bytes.
				// but leave the GC tracking information alone
			size = EA_SET_SIZE(size, 4);

            if  (shfv)
                genEmitter->emitIns_R_I(ins, size, (emitRegs)reg, shfv);
            else
                inst_RV(ins, reg, tree->TypeGet(), size);

            return;
        }
    }

    /* Is this a spilled value? */

    if  (tree->gtFlags & GTF_SPILLED)
    {
        assert(!"ISSUE: If this can happen, we need to generate 'ins [ebp+spill]'");
    }

    switch (tree->gtOper)
    {
        unsigned        varNum;

    case GT_LCL_VAR:

        assert(genTypeSize(tree->gtType) >= sizeof(int));

        /* Is this an enregistered long ? */

        if  (tree->gtType == TYP_LONG && !(tree->gtFlags & GTF_REG_VAL))
        {
            /* Avoid infinite loop */

            if  (genMarkLclVar(tree))
                goto LONGREG_TT;
        }

        inst_set_SV_var(tree);
        goto LCL;

    case GT_LCL_FLD:

        offs += tree->gtLclFld.gtLclOffs;
        goto LCL;

    LCL:
        varNum = tree->gtLclVar.gtLclNum; assert(varNum < lvaCount);

        if  (shfv)
            genEmitter->emitIns_S_I(ins, size, varNum, offs, shfv);
        else
            genEmitter->emitIns_S  (ins, size, varNum, offs);

        return;

    case GT_CLS_VAR:

        if  (shfv)
            genEmitter->emitIns_C_I(ins, size, tree->gtClsVar.gtClsVarHnd,
                                                offs,
                                                shfv);
        else
            genEmitter->emitIns_C  (ins, size, tree->gtClsVar.gtClsVarHnd,
                                                offs);
        return;

    case GT_IND:
    case GT_ARR_ELEM:

        GenTreePtr addr; addr = (tree->gtOper == GT_IND) ? tree->gtOp.gtOp1
                                                         : tree;
        if  (shfv)
            sched_AM(ins, size, REG_NA, false, addr, offs, true, shfv);
        else
            instEmit_RM(ins, tree,             addr, offs);

        break;

    case GT_COMMA:
        //     tree->gtOp.gtOp1 - already processed by genCreateAddrMode()
        tree = tree->gtOp.gtOp2;
        goto AGAIN;

    default:
        assert(!"invalid address");
    }

}

/*****************************************************************************
 *
 *  Generate an instruction that has one operand given by a tree (which has
 *  been made addressable) and another that is a register.
 */

void                Compiler::inst_TT_RV(instruction   ins,
                                         GenTreePtr    tree,
                                         regNumber     reg, unsigned offs)
{
    assert(reg != REG_STK);

AGAIN:

    /* Is the value sitting in a register? */

    if  (tree->gtFlags & GTF_REG_VAL)
    {
        regNumber       rg2;

    LONGREG_TT_RV:

        assert(instIsFP(ins) == 0);

        rg2 = tree->gtRegNum;

        if  (tree->gtType == TYP_LONG)
        {
            if  (offs)
            {
                assert(offs == sizeof(int));

                rg2 = genRegPairHi((regPairNo)rg2);
            }
            else
                rg2 = genRegPairLo((regPairNo)rg2);
        }

        if  (rg2 != REG_STK)
        {
            if (ins != INS_mov || rg2 != reg)
                inst_RV_RV(ins, rg2, reg, tree->TypeGet());
            return;
        }
    }

    /* Is this a spilled value? */

    if  (tree->gtFlags & GTF_SPILLED)
    {
        assert(!"ISSUE: If this can happen, we need to generate 'ins [ebp+spill]'");
    }

    emitAttr   size;

    if (!instIsFP(ins))
        size = emitTypeSize(tree->TypeGet());
    else
        size = EA_ATTR(genTypeSize(tree->TypeGet()));

    switch (tree->gtOper)
    {
        unsigned        varNum;

    case GT_LCL_VAR:

        if  (tree->gtType == TYP_LONG && !(tree->gtFlags & GTF_REG_VAL))
        {
            /* Avoid infinite loop */

            if  (genMarkLclVar(tree))
                goto LONGREG_TT_RV;
        }

        inst_set_SV_var(tree);
        goto LCL;

    case GT_LCL_FLD:

        offs += tree->gtLclFld.gtLclOffs;
        goto LCL;

    LCL:

        varNum = tree->gtLclVar.gtLclNum; assert(varNum < lvaCount);

        genEmitter->emitIns_S_R(ins, size, (emitRegs)reg, varNum, offs);
        return;

    case GT_CLS_VAR:

        genEmitter->emitIns_C_R(ins, size, tree->gtClsVar.gtClsVarHnd,
                                            (emitRegs)reg,
                                            offs);
        return;

    case GT_IND:
    case GT_ARR_ELEM:

        GenTreePtr addr; addr = (tree->gtOper == GT_IND) ? tree->gtOp.gtOp1
                                                         : tree;
        instEmit_RM_RV(ins, size, addr, reg, offs);
        break;

    case GT_COMMA:
        //     tree->gtOp.gtOp1 - already processed by genCreateAddrMode()
        tree = tree->gtOp.gtOp2;
        goto AGAIN;

    default:
        assert(!"invalid address");
    }
}

/*****************************************************************************
 *
 *  Generate an instruction that has one operand given by a tree (which has
 *  been made addressable) and another that is an integer constant.
 */

void                Compiler::inst_TT_IV(instruction   ins,
                                         GenTreePtr    tree,
                                         long          val, unsigned offs)
{
AGAIN:

    /* Is the value sitting in a register? */

    if  (tree->gtFlags & GTF_REG_VAL)
    {
LONGREG_TT_IV:
        regNumber       reg = tree->gtRegNum;

        assert(instIsFP(ins) == 0);

        if  (tree->gtType == TYP_LONG)
        {
            if  (offs)
            {
                assert(offs == sizeof(int));

                reg = genRegPairHi((regPairNo)reg);
            }
            else
                reg = genRegPairLo((regPairNo)reg);
        }

        if  (reg != REG_STK)
        {
            if  (ins == INS_mov)
            {
                genSetRegToIcon(reg, val, tree->TypeGet());
            }
            else
                inst_RV_IV(ins, reg, val);

            return;
        }
    }

    /* Are we storing a zero? */

    if (false && // All callers dont think any of rsRegMaskFree() can be modified
        (ins == INS_mov) && (val == 0) &&
        (genTypeSize(tree->gtType) == sizeof(int)))
    {
        regNumber zeroReg;

#if REDUNDANT_LOAD

        /* Is the constant already in some register? */

        zeroReg = rsIconIsInReg(0);

        if  (zeroReg == REG_NA)
#endif
        {
            regMaskTP freeMask = rsRegMaskFree();

            if ((freeMask != 0) && (compCodeOpt() != FAST_CODE))
            {
                // For SMALL_CODE and BLENDED_CODE,
                // we try to generate:
                //
                //  xor   reg,  reg
                //  mov   dest, reg
                //
                // When selecting a register to xor we avoid EAX
                // if we have EDX or ECX available.
                // This will often let us reuse the zeroed register in
                // several back-to-back assignments
                //
                if ((freeMask & (RBM_EDX | RBM_ECX)) != 0)
                    freeMask &= ~RBM_EAX;
                zeroReg   = rsPickReg(freeMask);
                genSetRegToIcon(zeroReg, 0, TYP_INT);
            }
        }

        if  (zeroReg != REG_NA)
        {
            inst_TT_RV(INS_mov, tree, zeroReg, offs);
            return;
        }
    }

    /* Is this a spilled value? */

    if  (tree->gtFlags & GTF_SPILLED)
    {
        assert(!"ISSUE: If this can happen, we need to generate 'ins [ebp+spill], icon'");
    }

    emitAttr   size;

    if (!instIsFP(ins))
        size = emitTypeSize(tree->TypeGet());
    else
        size = EA_ATTR(genTypeSize(tree->TypeGet()));

    switch (tree->gtOper)
    {
        unsigned        varNum;

    case GT_LCL_VAR:

        /* Is this an enregistered long ? */

        if  (tree->gtType == TYP_LONG && !(tree->gtFlags & GTF_REG_VAL))
        {
            /* Avoid infinite loop */

            if  (genMarkLclVar(tree))
                goto LONGREG_TT_IV;
        }

        inst_set_SV_var(tree);

        varNum = tree->gtLclVar.gtLclNum; assert(varNum < lvaCount);

        /* Integer instructions never operate on more than EA_4BYTE */

        assert(instIsFP(ins) == false);

        if  (size == EA_8BYTE)
            size = EA_4BYTE;

        if (size < EA_4BYTE && !varTypeIsUnsigned(lvaTable[varNum].TypeGet()))
        {
            if (size == EA_1BYTE)
            {
                if ((val & 0x7f) != val)
                    val = val | 0xffffff00;
            }
            else
            {
                assert(size == EA_2BYTE);
                if ((val & 0x7fff) != val)
                    val = val | 0xffff0000;
            }
        }
        size = EA_4BYTE;

        genEmitter->emitIns_S_I(ins, size, varNum, offs, val);
        return;

    case GT_LCL_FLD:

        varNum = tree->gtLclFld.gtLclNum; assert(varNum < lvaCount);
        offs  += tree->gtLclFld.gtLclOffs;

        /* Integer instructions never operate on more than EA_4BYTE */

        assert(instIsFP(ins) == false);

        if  (size == EA_8BYTE)
            size = EA_4BYTE;

        genEmitter->emitIns_S_I(ins, size, varNum, offs, val);
        return;

    case GT_CLS_VAR:

        genEmitter->emitIns_C_I(ins, size, tree->gtClsVar.gtClsVarHnd, offs, val);
        return;

    case GT_IND:
    case GT_ARR_ELEM:

        GenTreePtr addr; addr = (tree->gtOper == GT_IND) ? tree->gtOp.gtOp1
                                                         : tree;
        sched_AM(ins, size, REG_NA, false, addr, offs, true, val);
        return;

    case GT_COMMA:
        //     tree->gtOp.gtOp1 - already processed by genCreateAddrMode()
        tree = tree->gtOp.gtOp2;
        goto AGAIN;

    default:
        assert(!"invalid address");
    }
}

/*****************************************************************************
 *
 *  Generate an instruction that has one operand given by a register and the
 *  other one by an indirection tree (which has been made addressable).
 */

void                Compiler::inst_RV_AT(instruction   ins,
                                         emitAttr      size,
                                         var_types     type, regNumber     reg,
                                                             GenTreePtr    tree,
                                                             unsigned      offs)
{
    assert(instIsFP(ins) == 0);

#if TRACK_GC_REFS

    /* Set "size" to EA_GCREF or EA_BYREF if the operand is a pointer */

    if  (type == TYP_REF)
    {
        if      (size == EA_4BYTE)
        {
            size = EA_GCREF;
        }
        else if (size == EA_GCREF)
        {
            /* Already marked as a pointer value */
        }
        else
        {
            /* Must be a derived pointer */

            assert(ins == INS_lea);
        }
    }
    else if (type == TYP_BYREF)
    {
        if      (size == EA_4BYTE)
        {
            size = EA_BYREF;
        }
        else if (size == EA_BYREF)
        {
            /* Already marked as a pointer value */
        }
        else
        {
            /* Must be a derived pointer */

            assert(ins == INS_lea);
        }
    }
    else
#endif
    {
        /* Integer instructions never operate on more than EA_4BYTE */

        if  (size == EA_8BYTE && !instIsFP(ins))
            size = EA_4BYTE;
    }

    sched_AM(ins, size, reg, true, tree, offs);
}

/*****************************************************************************
 *
 *  Generate an instruction that has one operand given by an indirection tree
 *  (which has been made addressable) and an integer constant.
 */

void        Compiler::inst_AT_IV(instruction   ins,
                                 emitAttr      size, GenTreePtr    tree,
                                                     long          icon,
                                                     unsigned      offs)
{
    sched_AM(ins, size, REG_NA, false, tree, offs, true, icon);
}

/*****************************************************************************
 *
 *  Generate an instruction that has one operand given by a register and the
 *  other one by a tree (which has been made addressable).
 */

void                Compiler::inst_RV_TT(instruction   ins,
                                         regNumber     reg,
                                         GenTreePtr    tree, unsigned offs,
                                                             emitAttr size)
{
    assert(reg != REG_STK);

    if (size == EA_UNKNOWN)
    {
        if (!instIsFP(ins))
            size = emitTypeSize(tree->TypeGet());
        else
            size = EA_ATTR(genTypeSize(tree->TypeGet()));
    }

#ifdef DEBUG
    // If it is a GC type and the result is not, then either
    // 1) it is an LEA
    // 2) we optimized if(ref != 0 && ref != 0) to if (ref & ref)
    // 3) we optimized if(ref == 0 || ref == 0) to if (ref | ref)
    // 4) byref - byref = int
    if  (tree->gtType == TYP_REF   && !EA_IS_GCREF(size))
        assert((EA_IS_BYREF(size) && ins == INS_add) || 
               (ins == INS_lea || ins == INS_and || ins == INS_or));
    if  (tree->gtType == TYP_BYREF && !EA_IS_BYREF(size))
        assert(ins == INS_lea || ins == INS_and || ins == INS_or || ins == INS_sub);
#endif

AGAIN:

    /* Is the value sitting in a register? */

    if  (tree->gtFlags & GTF_REG_VAL)
    {
LONGREG_RVTT:

        regNumber       rg2 = tree->gtRegNum;

        assert(instIsFP(ins) == 0);

        if  (tree->gtType == TYP_LONG)
        {
            if  (offs)
            {
                assert(offs == sizeof(int));

                rg2 = genRegPairHi((regPairNo)rg2);
            }
            else
                rg2 = genRegPairLo((regPairNo)rg2);
        }

        if  (rg2 != REG_STK)
        {
            if (ins != INS_mov || rg2 != reg)
                inst_RV_RV(ins, reg, rg2, tree->TypeGet(), size);
            return;
        }
    }

    /* Is this a spilled value? */

    if  (tree->gtFlags & GTF_SPILLED)
    {
        assert(!"ISSUE: If this can happen, we need to generate 'ins [ebp+spill]'");
    }

    switch (tree->gtOper)
    {
        unsigned        varNum;

    case GT_LCL_VAR:

        /* Is this an enregistered long ? */

        if  (tree->gtType == TYP_LONG && !(tree->gtFlags & GTF_REG_VAL))
        {

            /* Avoid infinite loop */

            if  (genMarkLclVar(tree))
                goto LONGREG_RVTT;
        }

        inst_set_SV_var(tree);
        goto LCL;

    case GT_LCL_FLD:
        offs += tree->gtLclFld.gtLclOffs;
        goto LCL;

    LCL:
        varNum = tree->gtLclVar.gtLclNum; assert(varNum < lvaCount);

        genEmitter->emitIns_R_S(ins, size, (emitRegs)reg,
                                varNum, offs);
        return;

    case GT_CLS_VAR:

        genEmitter->emitIns_R_C(ins, size, (emitRegs)reg,
                                            tree->gtClsVar.gtClsVarHnd,
                                            offs);

        return;

    case GT_IND:
    case GT_ARR_ELEM:
        GenTreePtr addr; addr = (tree->gtOper == GT_IND) ? tree->gtOp.gtOp1
                                                         : tree;
        inst_RV_AT(ins, size, tree->TypeGet(), reg, addr, offs);
        break;

    case GT_CNS_INT:

        assert(offs == 0);
        assert(size == EA_UNKNOWN || size == EA_4BYTE);

        inst_RV_IV(ins, reg, tree->gtIntCon.gtIconVal);
        break;

    case GT_CNS_LNG:

        assert(size == EA_4BYTE || size == EA_8BYTE);

        if  (offs == 0)
            inst_RV_IV(ins, reg, (long)(tree->gtLngCon.gtLconVal      ));
        else
            inst_RV_IV(ins, reg, (long)(tree->gtLngCon.gtLconVal >> 32));

        break;

    case GT_COMMA:
        tree = tree->gtOp.gtOp2;
        goto AGAIN;

    default:
        assert(!"invalid address");
    }

}

/*****************************************************************************
 *
 *  Generate the 3-operand imul instruction "imul reg, [tree], icon"
 *  which is reg=[tree]*icon
 */

void                Compiler::inst_RV_TT_IV(instruction    ins,
                                            regNumber      reg,
                                            GenTreePtr     tree,
                                            long           val)
{
    genUpdateLife(tree);

    /* Only 'imul' uses this instruction format. Since we cant represent
       3 operands for an instruction, we encode the target register as
       an implicit operand */

    assert(ins == INS_imul);
    assert(tree->gtType <= TYP_INT);

    instruction     ins2 = inst3opImulForReg(reg);

    inst_TT_IV(ins2, tree, val);
}

/*****************************************************************************
 *
 *  Generate a "shift reg, icon" instruction.
 */

void        Compiler::inst_RV_SH(instruction ins, regNumber reg, unsigned val)
{
    assert(ins == INS_rcl  ||
           ins == INS_rcr  ||
           ins == INS_shl  ||
           ins == INS_shr  ||
           ins == INS_sar);

    /* Which format should we use? */

    if  (val == 1)
    {
        /* Use the shift-by-one format */

        assert(INS_rcl + 1 == INS_rcl_1);
        assert(INS_rcr + 1 == INS_rcr_1);
        assert(INS_shl + 1 == INS_shl_1);
        assert(INS_shr + 1 == INS_shr_1);
        assert(INS_sar + 1 == INS_sar_1);

        inst_RV((instruction)(ins+1), reg, TYP_INT);
    }
    else
    {
        /* Use the shift-by-NNN format */

        assert(INS_rcl + 2 == INS_rcl_N);
        assert(INS_rcr + 2 == INS_rcr_N);
        assert(INS_shl + 2 == INS_shl_N);
        assert(INS_shr + 2 == INS_shr_N);
        assert(INS_sar + 2 == INS_sar_N);

        genEmitter->emitIns_R_I((instruction)(ins+2),
                                 EA_4BYTE,
                                 (emitRegs)reg,
                                 val);
    }
}

/*****************************************************************************
 *
 *  Generate a "shift [r/m], icon" instruction.
 */

void                Compiler::inst_TT_SH(instruction   ins,
                                         GenTreePtr    tree,
                                         unsigned      val, unsigned offs)
{
    /* Which format should we use? */

    switch (val)
    {
    case 1:

        /* Use the shift-by-one format */

        assert(INS_rcl + 1 == INS_rcl_1);
        assert(INS_rcr + 1 == INS_rcr_1);
        assert(INS_shl + 1 == INS_shl_1);
        assert(INS_shr + 1 == INS_shr_1);
        assert(INS_sar + 1 == INS_sar_1);

        inst_TT((instruction)(ins+1), tree, offs, 0, emitTypeSize(tree->TypeGet()));

        break;

    case 0:

        // Shift by 0 - why are you wasting our precious time????

        return;

    default:

        /* Use the shift-by-NNN format */

        assert(INS_rcl + 2 == INS_rcl_N);
        assert(INS_rcr + 2 == INS_rcr_N);
        assert(INS_shl + 2 == INS_shl_N);
        assert(INS_shr + 2 == INS_shr_N);
        assert(INS_sar + 2 == INS_sar_N);

        inst_TT((instruction)(ins+2), tree, offs, val, emitTypeSize(tree->TypeGet()));

        break;
    }
}

/*****************************************************************************
 *
 *  Generate a "shift [addr], CL" instruction.
 */

void                Compiler::inst_TT_CL(instruction   ins,
                                         GenTreePtr    tree, unsigned offs)
{
    inst_TT(ins, tree, offs, 0, emitTypeSize(tree->TypeGet()));
}

/*****************************************************************************
 *
 *  Generate an instruction of the form "op reg1, reg2, icon".
 */

void                Compiler::inst_RV_RV_IV(instruction    ins,
                                            regNumber      reg1,
                                            regNumber      reg2,
                                            unsigned       ival)
{
    assert(ins == INS_shld || ins == INS_shrd);

    genEmitter->emitIns_R_R_I(ins, (emitRegs)reg1, (emitRegs)reg2, ival);
}

/*****************************************************************************
 *
 *  Generate an instruction with two registers, the second one being a byte
 *  or word register (i.e. this is something like "movzx eax, cl").
 */

void                Compiler::inst_RV_RR(instruction  ins,
                                         emitAttr     size,
                                         regNumber    reg1,
                                         regNumber    reg2)
{
    assert(size == EA_1BYTE || size == EA_2BYTE);
    assert(ins == INS_movsx || ins == INS_movzx);

    genEmitter->emitIns_R_R(ins, size, (emitRegs)reg1, (emitRegs)reg2);
}

/*****************************************************************************
 *
 *  The following should all end up inline in compiler.hpp at some point.
 */

void                Compiler::inst_ST_RV(instruction    ins,
                                         TempDsc    *   tmp,
                                         unsigned       ofs,
                                         regNumber      reg,
                                         var_types      type)
{
    genEmitter->emitIns_S_R(ins,
                            emitActualTypeSize(type),
                            (emitRegs)reg,
                            tmp->tdTempNum(),
                            ofs);
}

void                Compiler::inst_ST_IV(instruction    ins,
                                         TempDsc    *   tmp,
                                         unsigned       ofs,
                                         long           val,
                                         var_types      type)
{
    genEmitter->emitIns_S_I(ins,
                            emitActualTypeSize(type),
                            tmp->tdTempNum(),
                            ofs,
                            val);
}

/*****************************************************************************
 *
 *  Generate an instruction with one register and one operand that is byte
 *  or short (e.g. something like "movzx eax, byte ptr [edx]").
 */

void                Compiler::inst_RV_ST(instruction   ins,
                                         emitAttr      size,
                                         regNumber     reg,
                                         GenTreePtr    tree)
{
    assert(size == EA_1BYTE || size == EA_2BYTE);

    /* "movsx erx, rl" must be handled as a special case */

    if  (tree->gtFlags & GTF_REG_VAL)
        inst_RV_RR(ins, size, reg, tree->gtRegNum);
    else
        inst_RV_TT(ins, reg, tree, 0, size);
}

void                Compiler::inst_RV_ST(instruction    ins,
                                         regNumber      reg,
                                         TempDsc    *   tmp,
                                         unsigned       ofs,
                                         var_types      type,
                                         emitAttr       size)
{
    if (size == EA_UNKNOWN)
        size = emitActualTypeSize(type);

    genEmitter->emitIns_R_S(ins,
                            size,
                            (emitRegs)reg,
                            tmp->tdTempNum(),
                            ofs);
}

void                Compiler::inst_mov_RV_ST(regNumber      reg,
                                             GenTreePtr     tree)
{
    /* Figure out the size of the value being loaded */

    emitAttr    size = EA_ATTR(genTypeSize(tree->gtType));

    if  (size < EA_4BYTE)
    {
        if  ((tree->gtFlags & GTF_SMALL_OK)    &&
             (genRegMask(reg) & RBM_BYTE_REGS) && size == EA_1BYTE)
        {
            /* We only need to load the actual size */

            inst_RV_TT(INS_mov, reg, tree, 0, EA_1BYTE);
        }
        else
        {
            bool uns = varTypeIsUnsigned(tree->TypeGet());

            /* Generate the "movsx/movzx" opcode */

            inst_RV_ST(uns ? INS_movzx : INS_movsx, size, reg, tree);
        }
    }
    else
    {
        /* Compute op1 into the target register */

        inst_RV_TT(INS_mov, reg, tree);
    }
}

void                Compiler::inst_FS_ST(instruction    ins,
                                         emitAttr       size,
                                         TempDsc    *   tmp,
                                         unsigned       ofs)
{
    genEmitter->emitIns_S(ins,
                          size,
                          tmp->tdTempNum(),
                          ofs);
}

/*****************************************************************************/
#endif//TGT_x86
/*****************************************************************************/
#if     TGT_RISC
/*****************************************************************************
 *
 *  Output an "ins reg, [r/m]" (rdst=true) or "ins [r/m], reg" (rdst=false)
 *  instruction (the r/m operand given by a tree).
 */

void                Compiler::sched_AM(instruction  ins,
                                       var_types    type,
                                       regNumber    ireg,
                                       bool         rdst,
                                       GenTreePtr   addr,
                                       unsigned     offs)
{
    emitRegs        reg;
    emitRegs        rg2;
    emitRegs        irg = (emitRegs)ireg;

    emitAttr        size;

    assert(addr);

#if TGT_SH3
    assert(ins == INS_mov);
#endif

    /* Figure out the correct "size" value */

    size = emitTypeSize(type);

    /* Has the address been conveniently loaded into a register? */

    if  (addr->gtFlags & GTF_REG_VAL)
    {
        /* The address is "[reg]" or "[reg+offs]" */

        reg = (emitRegs)addr->gtRegNum;

        if  (offs)
        {
            if  (rdst)
                genEmitter->emitIns_R_RD(irg, reg,  offs, size);
            else
                genEmitter->emitIns_RD_R(reg, irg,  offs, size);
        }
        else
        {
            if  (rdst)
                genEmitter->emitIns_R_IR(irg, reg, false, size);
            else
                genEmitter->emitIns_IR_R(reg, irg, false, size);
        }

        return;
    }

    regNumber       baseReg, indReg;
    unsigned        cns;
#if SCALED_ADDR_MODES
    unsigned        indScale;
#endif

    instGetAddrMode(addr, &baseReg, &indScale, &indReg, &cns);

    /* Add the constant offset value, if present */

    offs += cns;

    /* Is there an additional operand? */

    if  (indReg != REG_NA)
    {
        /* The additional operand must be sitting in a register */

        rg2 = emitRegs(indReg);

        /* Is the additional operand scaled? */

#if SCALED_ADDR_MODES
        if  (indScale)
        {
            /* Is there a base address operand? */

            if  (baseReg != REG_NA)
            {
                reg = emitRegs(baseReg);

                /* The address is "[reg1 + {2/4/8} * reg2 + offs]" */

                if  (offs)
                    assert(!"indirection [rg1+indScale*rg2+disp]");
                else
                    assert(!"indirection [rg1+indScale*rg2]");
            }
            else
            {
                /* The address is "[{2/4/8} * reg2 + offs]" */

                if  (offs)
                    assert(!"indirection [indScale*rg2+disp]");
                else
                    assert(!"indirection [indScale*rg2]");
            }
        }
        else
#endif
        {
            assert(baseReg != REG_NA)
            reg = emitRegs(baseReg);

#if TGT_SH3
            assert(offs == 0);

            /* One of the operands must be in r0 */

            if  (reg == REG_r00)
                reg = rg2;
            else
                assert(rg2 == REG_r00);

            if  (rdst)
                genEmitter->emitIns_R_XR0(irg, reg, size);
            else
                genEmitter->emitIns_XR0_R(reg, irg, size);

#else
#error  Unexpected target
#endif

        }
    }
    else
    {
        /* No second operand: the address is "[reg  + icon]" */

        assert(baseReg != REG_NA)
        reg = emitRegs(baseReg);

        // UNDONE: Pass handle for instance member name display

        if  (offs)
        {
            if  (rdst)
                genEmitter->emitIns_R_RD(irg, reg,  offs, size);
            else
                genEmitter->emitIns_RD_R(reg, irg,  offs, size);
        }
        else
        {
            if  (rdst)
                genEmitter->emitIns_R_IR(irg, reg, false, size);
            else
                genEmitter->emitIns_IR_R(reg, irg, false, size);
        }
    }
}

/*****************************************************************************
 *
 *  Generate an instruction that has one operand given by a tree (which has
 *  been made addressable) and another that is a register; the tree is the
 *  target of the operation, the register is the source.
 */

void                Compiler::inst_TT_RV(instruction   ins,
                                         GenTreePtr    tree,
                                         regNumber     reg, unsigned offs)
{
    emitAttr        size;

    assert(reg != REG_STK);

    /* Get hold of the correct size value (for GC refs, etc) */

    size = emitTypeSize(tree->TypeGet());

    /* Is the value sitting in a register? */

    if  (tree->gtFlags & GTF_REG_VAL)
    {
        regNumber       rg2;

    LONGREG_TT:

        rg2 = tree->gtRegNum;

        if  (tree->gtType == TYP_LONG)
        {
            if  (offs)
            {
                assert(offs == sizeof(int));

                rg2 = genRegPairHi((regPairNo)rg2);
            }
            else
                rg2 = genRegPairLo((regPairNo)rg2);
        }

        /* Make sure it is not the "stack-half" of an enregistered long/double */

        if  (rg2 != REG_STK)
        {
            genEmitter->emitIns_R_R(ins, EA_4BYTE, (emitRegs)rg2,
                                                   (emitRegs)reg);
            return;
        }
    }

    switch (tree->gtOper)
    {
        unsigned        varNum;
        LclVarDsc   *   varDsc;
        unsigned        varOfs;

        regNumber       rga;

    case GT_LCL_VAR:

        assert(genTypeSize(tree->gtType) >= sizeof(int));

        /* Is this an enregistered long ? */

        if  (tree->gtType == TYP_LONG && !(tree->gtFlags & GTF_REG_VAL))
        {
            /* Avoid infinite loop */

            if  (genMarkLclVar(tree))
                goto LONGREG_TT;
        }

        goto LCL;

    case GT_LCL_FLD:

        assert(genTypeSize(tree->gtType) >= sizeof(int));

        offs += tree->gtLclFld.gtLclOffs;
        goto LCL;

    LCL:
        /* Figure out the variable's frame offset */

        varNum = tree->gtLclVar.gtLclNum; assert(varNum < lvaCount);
        varDsc = lvaTable + varNum;
        varOfs = varDsc->lvStkOffs + offs;

        assert(MAX_FPBASE_OFFS == MAX_SPBASE_OFFS);

        if  (varOfs > MAX_SPBASE_OFFS)
        {
            assert(!"local variable too far, need access code");
        }
        else
        {
            assert(ins == INS_mov);

            genEmitter->emitIns_S_R(INS_mov_dsp,
                                    size,
                                    (emitRegs)reg,
                                    varNum,
                                    offs);
        }
        return;

    case GT_IND:
    case GT_ARR_ELEM:
        GenTreePtr addr; addr = (tree->gtOper == GT_IND) ? tree->gtOp.gtOp1
                                                         : tree;
        sched_AM(ins, tree->TypeGet(), reg, false, addr, offs);
        break;

    case GT_CLS_VAR:

        assert(ins == INS_mov);
        assert(eeGetJitDataOffs(tree->gtClsVar.gtClsVarHnd) < 0);

        /* Get a temp register for the variable address */

        // @TODO [CONSIDER] [04/16/01] []: This should be moved to codegen.cpp, right?

        rga = rsGrabReg(RBM_ALL);

        /* @TODO [REVISIT] [04/16/01] []: Reuse addresses of globals via load suppression */

        genEmitter->emitIns_R_LP_V((emitRegs)rga,
                                   tree->gtClsVar.gtClsVarHnd);

        /* Store the value by indirecting via the address register */

        genEmitter->emitIns_IR_R((emitRegs)reg,
                                 (emitRegs)rga,
                                 false,
                                 emitTypeSize(tree->TypeGet()));
        return;

    default:

#ifdef  DEBUG
        gtDispTree(tree);
#endif
        assert(!"unexpected tree in inst_TT_RV()");
    }
}

/*****************************************************************************
 *
 *  Generate an instruction that has one operand given by a register and the
 *  other one by a tree (which has been made addressable); the tree is the
 *  source of the operation, the register is the target.
 */

void                Compiler::inst_RV_TT(instruction   ins,
                                         regNumber     reg,
                                         GenTreePtr    tree, unsigned offs,
                                                             emitAttr size)
{
    assert(reg != REG_STK);

    /* Set "size" to EA_GCREF or EA_BYREF if the operand is a pointer */

    if (size == EA_UNKNOWN)
        size = emitTypeSize(tree->TypeGet());

    assert(size != EA_8BYTE);

    /* Is the value sitting in a register? */

    if  (tree->gtFlags & GTF_REG_VAL)
    {
        regNumber       rg2;

    LONGREG_TT:

        rg2 = tree->gtRegNum;

        if  (tree->gtType == TYP_LONG)
        {
            if  (offs)
            {
                assert(offs == sizeof(int));

                rg2 = genRegPairHi((regPairNo)rg2);
            }
            else
                rg2 = genRegPairLo((regPairNo)rg2);
        }

        /* Make sure it is not the "stack-half" of an enregistered long/double */

        if  (rg2 != REG_STK)
        {
            genEmitter->emitIns_R_R(ins, EA_4BYTE, (emitRegs)reg,
                                                   (emitRegs)rg2);
            return;
        }
    }

    switch (tree->gtOper)
    {
        unsigned        varNum;
        LclVarDsc   *   varDsc;
        unsigned        varOfs;

    case GT_LCL_VAR:

        assert(genTypeSize(tree->gtType) >= sizeof(int));

        /* Is this an enregistered long ? */

        if  (tree->gtType == TYP_LONG && !(tree->gtFlags & GTF_REG_VAL))
        {
            /* Avoid infinite loop */

            if  (genMarkLclVar(tree))
                goto LONGREG_TT;
        }

        goto LCL;

    case GT_LCL_FLD:

        assert(genTypeSize(tree->gtType) >= sizeof(int));

        offs += tree->gtLclFld.gtLclOffs;
        goto LCL;

    LCL:
        /* Figure out the variable's frame offset */

        varNum = tree->gtLclVar.gtLclNum; assert(varNum < lvaCount);
        varDsc = lvaTable + varNum;
        varOfs = varDsc->lvStkOffs + offs;

        assert(MAX_FPBASE_OFFS == MAX_SPBASE_OFFS);

        if  (varOfs > MAX_SPBASE_OFFS)
        {
            assert(!"local variable too far, need access code");
        }
        else
        {
            assert(ins == INS_mov);

            genEmitter->emitIns_R_S(INS_mov_dsp,
                                    size,
                                    (emitRegs)reg,
                                    varNum,
                                    offs);
        }
        return;

    case GT_IND:
    case GT_ARR_ELEM:
        GenTreePtr addr; addr = (tree->gtOper == GT_IND) ? tree->gtOp.gtOp1
                                                         : tree;
        sched_AM(ins, tree->TypeGet(), reg,  true, addr, offs);
        break;

    case GT_CLS_VAR:

        // @TODO [CONSIDER] [04/16/01] []: Sometimes it's better to use another reg for the addr!

        assert(ins == INS_mov);

        /* Load the variable address into the register */

        genEmitter->emitIns_R_LP_V((emitRegs)reg,
                                    tree->gtClsVar.gtClsVarHnd);

        // HACK: We know we always want the address of a data area

        if  (eeGetJitDataOffs(tree->gtClsVar.gtClsVarHnd) < 0)
        {
            /* Load the value by indirecting via the address */

            genEmitter->emitIns_R_IR((emitRegs)reg,
                                     (emitRegs)reg,
                                     false,
                                     emitTypeSize(tree->TypeGet()));
        }

        return;

    default:

#ifdef  DEBUG
        gtDispTree(tree);
#endif
        assert(!"unexpected tree in inst_RV_TT()");
    }
}

/*****************************************************************************/
#endif//TGT_RISC
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\importer.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                           Importer                                        XX
XX                                                                           XX
XX   Imports the given method and converts it to semantic trees              XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

#include "jitpch.h"
#pragma hdrstop

    // This is only used locally in the JIT to indicate that 
    // a verification block should be inserted 
#define SEH_VERIFICATION_EXCEPTION 0xe0564552   // VER

#define Verify(cond, msg)                              \
    do {                                                    \
        if (!(cond)) {                                  \
            verRaiseVerifyExceptionIfNeeded(INDEBUG(msg) DEBUGARG(__FILE__) DEBUGARG(__LINE__));  \
        }                                                   \
    } while(0)

#define VerifyOrReturn(cond, msg)                           \
    do {                                                    \
        if (!(cond)) {                                  \
            verRaiseVerifyExceptionIfNeeded(INDEBUG(msg) DEBUGARG(__FILE__) DEBUGARG(__LINE__));  \
        return;                                         \
        }                                                   \
    } while(0)

/*****************************************************************************/

void                Compiler::impInit()
{
#ifdef DEBUG
    impTreeList = impTreeLast = NULL;
#endif
}

/*****************************************************************************
 *
 *  Pushes the given tree on the stack.
 */

inline
void                Compiler::impPushOnStack(GenTreePtr tree, typeInfo ti)
{
    /* Check for overflow. If inling, we may be using a bigger stack */
    
        // FIX NOW; review this.  Seems like it can be simplified - vancem
    if ((verCurrentState.esStackDepth >= info.compMaxStack) &&
        (verCurrentState.esStackDepth >= impStkSize || ((compCurBB->bbFlags & BBF_IMPORTED) == 0)))
    {
        BADCODE("stack overflow");
    }

        // If we are pushing a struct, make certain we not know the precise type!
#ifdef DEBUG
    if (tree->TypeGet() == TYP_STRUCT)
    {
        assert(ti.IsType(TI_STRUCT));
        CORINFO_CLASS_HANDLE clsHnd = ti.GetClassHandle();
        assert(clsHnd != 0 && clsHnd != BAD_CLASS_HANDLE);
    }
    if (tiVerificationNeeded && !ti.IsDead()) 
    {
        assert(NormaliseForStack(ti) == ti);                                       // types are normalized
        assert(ti.IsDead() ||
               ti.IsByRef() && (tree->TypeGet() == TYP_INT || tree->TypeGet() == TYP_BYREF) ||
               ti.IsObjRef() && tree->TypeGet() == TYP_REF ||
               ti.IsMethod() && tree->TypeGet() == TYP_INT ||
               ti.IsType(TI_STRUCT) && tree->TypeGet() != TYP_REF ||
               NormaliseForStack(ti) == NormaliseForStack(typeInfo(tree->TypeGet())));
    }
#endif

    if (tree->gtType == TYP_LONG)
        compLongUsed = true;

    verCurrentState.esStack[verCurrentState.esStackDepth].seTypeInfo = ti; 
    verCurrentState.esStack[verCurrentState.esStackDepth++].val      = tree;
}

/******************************************************************************/
// used in the inliner, where we can assume typesafe code. please dont use in the importer!!
inline
void                Compiler::impPushOnStackNoType(GenTreePtr tree)
{
    assert(verCurrentState.esStackDepth < impStkSize);
    INDEBUG(verCurrentState.esStack[verCurrentState.esStackDepth].seTypeInfo = typeInfo());
    verCurrentState.esStack[verCurrentState.esStackDepth++].val      = tree;

    if (tree->gtType == TYP_LONG)
        compLongUsed = true;
}

inline
void                Compiler::impPushNullObjRefOnStack()
{
    impPushOnStack(gtNewIconNode(0, TYP_REF), typeInfo(TI_NULL));
}

inline void Compiler::verRaiseVerifyExceptionIfNeeded(INDEBUG(const char* msg) DEBUGARG(const char* file) DEBUGARG(unsigned line))
{
#ifdef DEBUG
    const char* tail = strrchr(file, '\\');
    if (tail) file = tail+1;

    static ConfigDWORD fJitBreakOnUnsafeCode(L"JitBreakOnUnsafeCode");
    if (fJitBreakOnUnsafeCode.val())
        assert(!"Unsafe code detected");
#endif

    JITLOG((LL_INFO100, "Detected unsafe code: %s:%d : %s, while compiling %s opcode %s, IL offset %x\n", 
        file, line, msg, info.compFullName, impCurOpcName, impCurOpcOffs));
    
    if (verNeedsVerification())  {
        JITLOG((LL_ERROR, "Verification failure:  %s:%d : %s, while compiling %s opcode %s, IL offset %x\n", 
            file, line, msg, info.compFullName, impCurOpcName, impCurOpcOffs));
#ifdef DEBUG
        if (verbose)
            printf("\n\nVerification failure: %s:%d : %s\n", file, line, msg);
#endif
        verRaiseVerifyException();
    }
}


inline void Compiler::verRaiseVerifyException()
{

#ifdef DEBUG
    BreakIfDebuggerPresent();
    extern ConfigDWORD fBreakOnBadCode;
    if (fBreakOnBadCode.val())
        assert(!"Typechecking error");
#endif

    eeSetMethodAttribs(info.compMethodHnd, CORINFO_FLG_DONT_INLINE);
    RaiseException(SEH_VERIFICATION_EXCEPTION, EXCEPTION_NONCONTINUABLE, 0, NULL);
}

unsigned __int32 Compiler::impGetToken(const BYTE* addr, 
                                       CORINFO_MODULE_HANDLE scpHandle,
                                       BOOL verificationNeeded)
{
    unsigned token = getU4LittleEndian(addr);
    if (verificationNeeded)
        Verify(info.compCompHnd->isValidToken(scpHandle, token), "bad token");
    return token;
}


/*****************************************************************************
 *
 *  Pop one tree from the stack.
 */

inline
StackEntry          Compiler::impPopStack()
{
        // @TODO [CONSIDER] [04/16/01] []: except for the LDFLD, and STFLD
        // instructions we always check before we pop for verified code
        // Thus we don't need to do this check if we don't want to.
    if (! verCurrentState.esStackDepth)
    {
        verRaiseVerifyExceptionIfNeeded(INDEBUG("stack underflow") DEBUGARG(__FILE__) DEBUGARG(__LINE__));
        BADCODE("stack underflow");
    }

    return verCurrentState.esStack[--verCurrentState.esStackDepth];
}

/* @TODO [CONSIDER] [04/16/01] []: remove this function */
inline
StackEntry          Compiler::impPopStack(CORINFO_CLASS_HANDLE& structType)
{
    StackEntry ret = impPopStack();
    structType = verCurrentState.esStack[verCurrentState.esStackDepth].seTypeInfo.GetClassHandle();
    return(ret);
}

    /* @TODO [CONSIDER] [04/16/01] []: remove this? */
inline
GenTreePtr          Compiler::impPopStack(typeInfo& ti)
{
    StackEntry ret = impPopStack();
    ti = ret.seTypeInfo;
    return(ret.val);
}



/*****************************************************************************
 *
 *  Peep at n'th (0-based) tree on the top of the stack.
 */

inline
StackEntry&         Compiler::impStackTop(unsigned n)
{
    if (verCurrentState.esStackDepth <= n)
    {
        verRaiseVerifyExceptionIfNeeded(INDEBUG("stack underflow") DEBUGARG(__FILE__) DEBUGARG(__LINE__));
        BADCODE("stack underflow");
    }

    return verCurrentState.esStack[verCurrentState.esStackDepth-n-1];
}
/*****************************************************************************
 *  Some of the trees are spilled specially. While unspilling them, or
 *  making a copy, these need to be handled specially. The function
 *  enumerates the operators possible after spilling.
 */

static
bool                impValidSpilledStackEntry(GenTreePtr tree)
{
    if (tree->gtOper == GT_LCL_VAR)
        return true;

    if (tree->OperIsConst())
        return true;

    return false;
}

/*****************************************************************************
 *
 *  The following logic is used to save/restore stack contents.
 *  If 'copy' is true, then we make a copy of the trees on the stack. These
 *  have to all be cloneable/spilled values.
 */

void                Compiler::impSaveStackState(SavedStack *savePtr,
                                                bool        copy)
{
    savePtr->ssDepth = verCurrentState.esStackDepth;

    if  (verCurrentState.esStackDepth)
    {
        savePtr->ssTrees = (StackEntry *) compGetMemArray(verCurrentState.esStackDepth,
                                                          sizeof(*savePtr->ssTrees));
        size_t  saveSize = verCurrentState.esStackDepth*sizeof(*savePtr->ssTrees);

        if  (copy)
        {
            StackEntry *table = savePtr->ssTrees;

            /* Make a fresh copy of all the stack entries */

            for (unsigned level = 0; level < verCurrentState.esStackDepth; level++, table++)
            {
                table->seTypeInfo = verCurrentState.esStack[level].seTypeInfo;
                GenTreePtr  tree = verCurrentState.esStack[level].val;

                assert(impValidSpilledStackEntry(tree));

                switch(tree->gtOper)
                {
                case GT_CNS_INT:
                case GT_CNS_LNG:
                case GT_CNS_DBL:
                case GT_CNS_STR:
                case GT_LCL_VAR:
                    table->val = gtCloneExpr(tree);
                    break;

                default:
                    assert(!"Bad oper - Not covered by impValidSpilledStackEntry()");
                    break;
                }
            }
        }
        else
        {
            memcpy(savePtr->ssTrees, verCurrentState.esStack, saveSize);
        }
    }
}

void                Compiler::impRestoreStackState(SavedStack *savePtr)
{
    verCurrentState.esStackDepth = savePtr->ssDepth;

    if (verCurrentState.esStackDepth)
        memcpy(verCurrentState.esStack, savePtr->ssTrees, verCurrentState.esStackDepth*sizeof(*verCurrentState.esStack));
}


/*****************************************************************************
 *
 *  Get the tree list started for a new basic block.
 */
inline
void       FASTCALL Compiler::impBeginTreeList()
{
    assert(impTreeList == NULL && impTreeLast == NULL);

    impTreeList =
    impTreeLast = gtNewOperNode(GT_BEG_STMTS, TYP_VOID);
}


/*****************************************************************************
 *
 *  Store the given start and end stmt in the given basic block. This is
 *  mostly called by impEndTreeList(BasicBlock *block). It is called
 *  directly only for handling CEE_LEAVEs out of finally-protected try's.
 */

inline
void            Compiler::impEndTreeList(BasicBlock *   block,
                                         GenTreePtr     firstStmt,
                                         GenTreePtr     lastStmt)
{
    assert(firstStmt->gtOper == GT_STMT);
    assert( lastStmt->gtOper == GT_STMT);

    /* Make the list circular, so that we can easily walk it backwards */

    firstStmt->gtPrev =  lastStmt;

    /* Store the tree list in the basic block */

    block->bbTreeList = firstStmt;

    /* The block should not already be marked as imported */
    assert((block->bbFlags & BBF_IMPORTED) == 0);

    block->bbFlags |= BBF_IMPORTED;

#ifdef DEBUG
    if (verbose)
    {
        printf("\n");
        gtDispTreeList(block->bbTreeList);
    }
#endif
}

/*****************************************************************************
 *
 *  Store the current tree list in the given basic block.
 */

inline
void       FASTCALL Compiler::impEndTreeList(BasicBlock *block)
{
    assert(impTreeList->gtOper == GT_BEG_STMTS);

    GenTreePtr      firstTree = impTreeList->gtNext;

    if  (!firstTree)
    {
        /* The block should not already be marked as imported */
        assert((block->bbFlags & BBF_IMPORTED) == 0);

        // Empty block. Just mark it as imported
        block->bbFlags |= BBF_IMPORTED;
    }
    else
    {
        // Ignore the GT_BEG_STMTS
        assert(firstTree->gtPrev == impTreeList);

        impEndTreeList(block, firstTree, impTreeLast);
    }

#ifdef DEBUG
    if (impLastILoffsStmt != NULL)
    {
        impLastILoffsStmt->gtStmt.gtStmtLastILoffs = impCurOpcOffs;
        impLastILoffsStmt = NULL;
    }

    impTreeList = impTreeLast = NULL;
#endif
}

/*****************************************************************************
 *
 *  Check that storing the given tree doesnt mess up the semantic order. Note
 *  that this has only limited value as we can only check [0..chkLevel).
 */

inline
void       FASTCALL Compiler::impAppendStmtCheck(GenTreePtr stmt, unsigned chkLevel)
{
#ifndef DEBUG
    return;
#else
    assert(stmt->gtOper == GT_STMT);

    if (chkLevel == CHECK_SPILL_ALL)
        chkLevel = verCurrentState.esStackDepth;

    if (verCurrentState.esStackDepth == 0 || chkLevel == 0 || chkLevel == CHECK_SPILL_NONE)
        return;

    // GT_BB_QMARK, GT_CATCH_ARG etc must be spilled first
    for (unsigned level = 0; level < chkLevel; level++)
        assert((verCurrentState.esStack[level].val->gtFlags & GTF_OTHER_SIDEEFF) == 0);

    GenTreePtr tree = stmt->gtStmt.gtStmtExpr;

    // Calls can only be appended if there are no GTF_GLOB_EFFECT on the stack

    if (tree->gtFlags & GTF_CALL)
    {
        for (unsigned level = 0; level < chkLevel; level++)
            assert((verCurrentState.esStack[level].val->gtFlags & GTF_GLOB_EFFECT) == 0);
    }

    if (tree->gtOper == GT_ASG)
    {
        // For an assignment to a local variable, all references of that
        // variable have to be spilled. If it is aliased, all calls and
        // indirect accesses have to be spilled

        if (tree->gtOp.gtOp1->gtOper == GT_LCL_VAR)
        {
            unsigned lclNum = tree->gtOp.gtOp1->gtLclVar.gtLclNum;
            for (unsigned level = 0; level < chkLevel; level++)
            {
                assert(!gtHasRef(verCurrentState.esStack[level].val, lclNum, false));
                assert(!lvaTable[lclNum].lvAddrTaken ||
                       (verCurrentState.esStack[level].val->gtFlags & GTF_SIDE_EFFECT) == 0);
            }
        }

        // If the access is to glob-memory, all side effects have to be spilled

        else if (tree->gtOp.gtOp1->gtFlags & GTF_GLOB_REF)
        {
        SPILL_GLOB_EFFECT:
            for (unsigned level = 0; level < chkLevel; level++)
            {
                assert((verCurrentState.esStack[level].val->gtFlags & GTF_GLOB_REF) == 0);
            }
        }
    }
    else if (tree->gtOper == GT_COPYBLK || tree->gtOper == GT_INITBLK)
    {
        // If the access is to glob-memory, all side effects have to be spilled

        // @TODO [REVISIT] [04/16/01] []: add the appropriate code here ....

        if (false)
            goto SPILL_GLOB_EFFECT;
    }
#endif
}


/*****************************************************************************
 *
 *  Append the given GT_STMT node to the current block's tree list.
 *  [0..chkLevel) is the portion of the stack which we will check for
 *    interference with stmt and spill if needed.
 */

inline
void       FASTCALL Compiler::impAppendStmt(GenTreePtr stmt, unsigned chkLevel)
{
    assert(stmt->gtOper == GT_STMT);

    /* If the statement being appended has any side-effects, check the stack
       to see if anything needs to be spilled to preserve correct ordering. */

    GenTreePtr  expr    = stmt->gtStmt.gtStmtExpr;
    unsigned    flags   = expr->gtFlags & GTF_GLOB_EFFECT;

    /* Assignment to (unaliased) locals dont count as a side-effect as
       we handle them specially using impSpillLclRefs(). Temp locals should
       be fine too */

    if ((expr->gtOper == GT_ASG) && (expr->gtOp.gtOp1->gtOper == GT_LCL_VAR) &&
        !(expr->gtOp.gtOp1->gtFlags & GTF_GLOB_REF))
    {
        unsigned op2Flags = expr->gtOp.gtOp2->gtFlags & GTF_GLOB_EFFECT;
        assert(flags == (op2Flags | GTF_ASG));
        flags = op2Flags;
    }

    if (chkLevel == CHECK_SPILL_ALL)
        chkLevel = verCurrentState.esStackDepth;

    if  (chkLevel && chkLevel != CHECK_SPILL_NONE)
    {
        assert(chkLevel <= verCurrentState.esStackDepth);

        if (flags)
        {
            // If there is a call, we have to spill global refs
            bool spillGlobEffects = (flags & GTF_CALL) ? true : false;

            if (expr->gtOper == GT_ASG)
            {
                // If we are assigning to a global ref, we have to spill global refs on stack
                if (expr->gtOp.gtOp1->gtFlags & GTF_GLOB_REF)
                    spillGlobEffects = true;
            }
            else if ((expr->gtOper == GT_INITBLK) || (expr->gtOper == GT_COPYBLK)) 
            {
                // INITBLK and COPYBLK are other ways of performing an assignment
                spillGlobEffects = true;
            }

            impSpillSideEffects(spillGlobEffects, chkLevel);
        }
        else
        {
            impSpillSpecialSideEff();
        }
    }

    impAppendStmtCheck(stmt, chkLevel);

    /* Point 'prev' at the previous node, so that we can walk backwards */

    stmt->gtPrev = impTreeLast;

    /* Append the expression statement to the list */

    impTreeLast->gtNext = stmt;
    impTreeLast         = stmt;

#ifdef DEBUGGING_SUPPORT

    /* Once we set impCurStmtOffs in an appended tree, we are ready to
       report the following offsets. So reset impCurStmtOffs */

    if (impTreeLast->gtStmtILoffsx == impCurStmtOffs)
    {
        impCurStmtOffsSet(BAD_IL_OFFSET);
    }

#endif

#ifdef DEBUG
    if (impLastILoffsStmt == NULL)
        impLastILoffsStmt = stmt;
#endif
}

/*****************************************************************************
 *
 *  Insert the given GT_STMT node to the start of the current block's tree list.
 */

inline
void       FASTCALL Compiler::impInsertStmt(GenTreePtr stmt)
{
    assert(stmt->gtOper == GT_STMT);
    assert(impTreeList->gtOper == GT_BEG_STMTS);

    /* Point 'prev' at the previous node, so that we can walk backwards */

    stmt->gtPrev = impTreeList;
    stmt->gtNext = impTreeList->gtNext;

    /* Insert the expression statement to the list (just behind GT_BEG_STMTS) */

    impTreeList->gtNext  = stmt;
    stmt->gtNext->gtPrev = stmt;

    /* if the list was empty (i.e. just the GT_BEG_STMTS) we have to advance treeLast */
    if (impTreeLast == impTreeList)
        impTreeLast = stmt;
}


/*****************************************************************************
 *
 *  Append the given expression tree to the current block's tree list.
 */

void       FASTCALL Compiler::impAppendTree(GenTreePtr  tree,
                                            unsigned    chkLevel,
                                            IL_OFFSETX  offset)
{
    assert(tree);

    /* Allocate an 'expression statement' node */

    GenTreePtr      expr = gtNewStmt(tree, offset);

    /* Append the statement to the current block's stmt list */

    impAppendStmt(expr, chkLevel);
}


/*****************************************************************************
 *
 *  Insert the given exression tree at the start of the current block's tree list.
 */

void       FASTCALL Compiler::impInsertTree(GenTreePtr tree, IL_OFFSETX offset)
{
    /* Allocate an 'expression statement' node */

    GenTreePtr      expr = gtNewStmt(tree, offset);

    /* Append the statement to the current block's stmt list */

    impInsertStmt(expr);
}

/*****************************************************************************
 *
 *  Append an assignment of the given value to a temp to the current tree list.
 *  curLevel is the stack level for which the spill to the temp is being done.
 */

inline
GenTreePtr          Compiler::impAssignTempGen(unsigned     tmp,
                                               GenTreePtr   val,
                                               unsigned     curLevel)
{
    GenTreePtr      asg = gtNewTempAssign(tmp, val);

    impAppendTree(asg, curLevel, impCurStmtOffs);

    return  asg;
}

/*****************************************************************************
 * same as above, but handle the valueclass case too
 */

GenTreePtr          Compiler::impAssignTempGen(unsigned     tmpNum,
                                               GenTreePtr   val,
                                               CORINFO_CLASS_HANDLE structType,
                                               unsigned     curLevel)
{
    GenTreePtr asg;

    if (val->TypeGet() == TYP_STRUCT)
    {
        GenTreePtr  dst = gtNewLclvNode(tmpNum, TYP_STRUCT);

        assert(tmpNum < lvaCount);
        assert(structType != BAD_CLASS_HANDLE);

        // if the method is non-verifiable the assert is not true
        // so at least ignore it in the case when verification is turned on
        // since any block that tries to use the temp would have failed verification
        assert(tiVerificationNeeded ||              
               lvaTable[tmpNum].lvType == TYP_UNDEF ||
               lvaTable[tmpNum].lvType == TYP_STRUCT);
        lvaSetStruct(tmpNum, structType);
          
        asg = impAssignStruct(dst, val, structType, curLevel);
    }
    else
    {
        asg = gtNewTempAssign(tmpNum, val);
    }

    impAppendTree(asg, curLevel, impCurStmtOffs);
    return  asg;
}

/*****************************************************************************
 *
 *  Insert an assignment of the given value to a temp to the start of the
 *  current tree list.
 */

inline
void                Compiler::impAssignTempGenTop(unsigned      tmp,
                                                  GenTreePtr    val)
{
    impInsertTree(gtNewTempAssign(tmp, val), impCurStmtOffs);
}

/*****************************************************************************
 *
 *  Pop the given number of values from the stack and return a list node with
 *  their values. The 'treeList' argument may optionally contain an argument
 *  list that is prepended to the list returned from this function.
 */

GenTreePtr          Compiler::impPopList(unsigned   count,
                                         unsigned * flagsPtr,
                                         CORINFO_SIG_INFO*  sig, 
                                         GenTreePtr treeList)
{
    assert(sig == 0 || count == sig->numArgs);

    unsigned        flags = 0;
    CORINFO_CLASS_HANDLE    structType;

    while(count--)
    {
        StackEntry      se   = impPopStack();
        typeInfo        ti   = se.seTypeInfo;
        GenTreePtr      temp = se.val;
        if (ti.IsType(TI_STRUCT))
            structType = ti.GetClassHandleForValueClass();

            // Morph that aren't already LDOBJs or MKREFANY to be LDOBJs
        if (temp->TypeGet() == TYP_STRUCT)
            temp = impNormStructVal(temp, structType, CHECK_SPILL_ALL);

            /* NOTE: we defer bashing the type for I_IMPL to fgMorphArgs */
        flags |= temp->gtFlags;
        treeList = gtNewOperNode(GT_LIST, TYP_VOID, temp, treeList);
    }

    *flagsPtr = flags;

    if (sig == 0)
        return treeList;

    if (sig->retTypeSigClass != 0 && 
        sig->retType != CORINFO_TYPE_CLASS && 
        sig->retType != CORINFO_TYPE_BYREF && 
        sig->retType != CORINFO_TYPE_PTR) 
    {
            // We need to ensure that we have loaded all the classes of any value
            // type arguments that we push (including enums)
        info.compCompHnd->loadClass(sig->retTypeSigClass, info.compMethodHnd, FALSE);
    }

    if (sig->numArgs == 0)
        return treeList;

        // insert implied casts (from float to double or double to float)
    CORINFO_ARG_LIST_HANDLE     argLst  = sig->args;
    CORINFO_CLASS_HANDLE        argClass;
    CORINFO_CLASS_HANDLE        argRealClass;
    GenTreePtr                  args;

    for(args = treeList, count = sig->numArgs;
        count > 0;
        args = args->gtOp.gtOp2, count--)
    {
        assert(args->gtOper == GT_LIST);
        CorInfoType type = strip(info.compCompHnd->getArgType(sig, argLst, &argClass));
        if (type == CORINFO_TYPE_DOUBLE && args->gtOp.gtOp1->TypeGet() == TYP_FLOAT)
            args->gtOp.gtOp1 = gtNewCastNode(TYP_DOUBLE, args->gtOp.gtOp1, TYP_DOUBLE);
        else if (type == CORINFO_TYPE_FLOAT && args->gtOp.gtOp1->TypeGet() == TYP_DOUBLE)
            args->gtOp.gtOp1 = gtNewCastNode(TYP_FLOAT, args->gtOp.gtOp1, TYP_FLOAT);  

        if (type != CORINFO_TYPE_CLASS
            && type != CORINFO_TYPE_BYREF
            && type != CORINFO_TYPE_PTR
            && (argRealClass = info.compCompHnd->getArgClass(sig, argLst)) != NULL)
        {
            // We need to ensure that we have loaded all the classes of any value
            // type arguments that we push (including enums)
            info.compCompHnd->loadClass(argRealClass, info.compMethodHnd, FALSE);
        }

#ifdef DEBUG
        // Ensure that IL is half-way sane (what was pushed is what the function expected)
        var_types argType = genActualType(args->gtOp.gtOp1->TypeGet());
        unsigned argSize = genTypeSize(argType);
        if (argType == TYP_STRUCT) {
            if (args->gtOp.gtOp1->gtOper == GT_LDOBJ)
                argSize = info.compCompHnd->getClassSize(args->gtOp.gtOp1->gtLdObj.gtClass);
            else if (args->gtOp.gtOp1->gtOper == GT_MKREFANY)
                argSize = 2 * sizeof(void*);
            else
                assert(!"Unexpeced tree node with type TYP_STRUCT");
        }
        var_types sigType = genActualType(JITtype2varType(type));
        unsigned sigSize = genTypeSize(sigType);
        if (type == CORINFO_TYPE_VALUECLASS)
            sigSize = info.compCompHnd->getClassSize(argClass);
        else if (type == CORINFO_TYPE_REFANY)
            sigSize = 2 * sizeof(void*);
        
        if (argSize != sigSize)
        {
            char buff[400];
            _snprintf(buff, 400, "Arg type mismatch: Wanted %s (size %d), Got %s (size %d) for call at IL offset 0x%x\n", 
                varTypeName(sigType), sigSize, varTypeName(argType), argSize, impCurOpcOffs);
            buff[400-1] = 0;
            assertAbort(buff, __FILE__, __LINE__);
        }
#endif

        argLst = info.compCompHnd->getArgNext(argLst);
    }

    return treeList;
}

/*****************************************************************************
   Assign (copy) the structure from 'src' to 'dest'.  The structure is a value
   class of type 'clsHnd'.  It returns the tree that should be appended to the
   statement list that represents the assignment.
   Temp assignments may be appended to impTreeList if spilling is necessary.
   curLevel is the stack level for which a spill may be being done.
 */

GenTreePtr Compiler::impAssignStruct(GenTreePtr     dest,
                                     GenTreePtr     src,
                                     CORINFO_CLASS_HANDLE   structHnd,
                                     unsigned       curLevel)
{
    assert(dest->TypeGet() == TYP_STRUCT);
    assert(dest->gtOper == GT_LCL_VAR || dest->gtOper == GT_RETURN ||
           dest->gtOper == GT_FIELD   || dest->gtOper == GT_IND    ||
           dest->gtOper == GT_LDOBJ);

    GenTreePtr destAddr;

    if (dest->gtOper == GT_IND || dest->gtOper == GT_LDOBJ)
    {
        destAddr = dest->gtOp.gtOp1;
    }
    else
    {
        if (dest->gtOper == GT_LCL_VAR)
        {
            destAddr = gtNewOperNode(GT_ADDR, TYP_I_IMPL, dest);
            destAddr->gtFlags |= GTF_ADDR_ONSTACK;
        }
        else
        {
            destAddr = gtNewOperNode(GT_ADDR, TYP_BYREF,  dest);
        }
    }

    return(impAssignStructPtr(destAddr, src, structHnd, curLevel));
}

/*****************************************************************************
 * handles the GT_COPYBLK and GT_INITBLK opcodes.
 * If the blkShape is small we transform the block operation into
 * into one or more GT_ASG trees.
 * otherwise we create a GT_COPYBLK which copies a block from 'src' to 'dest'
 * or create a GT_INITBLK which copies the const value in 'src' to 'dest'
 *
 * 'blkShape' is either a size or a class handle
 * the GTF_ICON_CLASS_HDL flag is set for a class handle blkShape
 */

GenTreePtr Compiler::impHandleBlockOp(genTreeOps   oper,
                                      GenTreePtr   dest,
                                      GenTreePtr   src,
                                      GenTreePtr   blkShape,
                                      bool         volatil)
{
    GenTreePtr result;

    // Only these two opcodes are possible
    assert(oper == GT_COPYBLK || oper == GT_INITBLK);

    // The dest must be an address
    assert(genActualType(dest->gtType) == TYP_I_IMPL  ||
           dest->gtType  == TYP_BYREF);

    // For COPYBLK the src must be an address
    assert(oper != GT_COPYBLK ||
           (genActualType( src->gtType) == TYP_I_IMPL ||
            src->gtType  == TYP_BYREF));

    // For INITBLK the src must be a TYP_INT
    assert(oper != GT_INITBLK ||
           (genActualType( src->gtType) == TYP_INT));

    // The size must be an integer type
    assert(varTypeIsIntegral(blkShape->gtType));

    CORINFO_CLASS_HANDLE  clsHnd;
    unsigned              size;
    var_types             type    = TYP_UNDEF;

    if (blkShape->gtOper != GT_CNS_INT)
        goto GENERAL_BLKOP;

    if ((blkShape->gtFlags & GTF_ICON_HDL_MASK) == 0)
    {
        clsHnd = 0;
        size   = blkShape->gtIntCon.gtIconVal;

        /* A four byte BLK_COPY can be treated as an integer asignment */
        if (size == 4)
            type = TYP_INT;
    }
    else
    {
        clsHnd = (CORINFO_CLASS_HANDLE) blkShape->gtIntCon.gtIconVal;
        size   = roundUp(eeGetClassSize(clsHnd), sizeof(void*));

            // TODO since we round up, we are not handling the case where we have a non-dword sized struct with GC pointers.
            // The EE currently does not allow this, but we may chnage.  Lets assert it just to be safe
            // going forward we should simply  handle this case
        assert(eeGetClassSize(clsHnd) == size);

        if (size == 4)
        {
            BYTE gcPtr;

            eeGetClassGClayout(clsHnd, &gcPtr);

            if       (gcPtr == TYPE_GC_NONE)
                type = TYP_INT;
            else if  (gcPtr == TYPE_GC_REF)
                type = TYP_REF;
            else if  (gcPtr == TYPE_GC_BYREF)
                type = TYP_BYREF;
        }
    }

    //
    //  See if we can do a simple transformation:
    //
    //          GT_ASG <TYP_size>
    //          /   \
    //      GT_IND GT_IND or CNS_INT
    //         |      |
    //       [dest] [src]
    //

    switch (size)
    {
    case 1:
        type = TYP_BYTE;
        goto ONE_SIMPLE_ASG;
    case 2:
        type = TYP_SHORT;
        goto ONE_SIMPLE_ASG;
    case 4:
        assert(type != TYP_UNDEF);

ONE_SIMPLE_ASG:

        // For INITBLK, a non constant source is not going to allow us to fiddle
        // with the bits to create a single assigment.

        if ((oper == GT_INITBLK) && (src->gtOper != GT_CNS_INT))
        {
            goto GENERAL_BLKOP;
        }

        /* Indirect the dest node */

        dest = gtNewOperNode(GT_IND, type, dest);

        /* As long as we don't have more information about the destination we
           have to assume it could live anywhere (not just in the GC heap). Mark
           the GT_IND node so that we use the correct write barrier helper in case
           the field is a GC ref.
        */

        dest->gtFlags |= (GTF_EXCEPT | GTF_GLOB_REF | GTF_IND_TGTANYWHERE);

        if (volatil)
            dest->gtFlags |= GTF_DONT_CSE;

        if (oper == GT_COPYBLK)
        {
            /* Indirect the src node */
            src  = gtNewOperNode(GT_IND, type, src);
            src->gtFlags     |= (GTF_EXCEPT | GTF_GLOB_REF | GTF_IND_TGTANYWHERE);
            if (volatil)
                src->gtFlags |= GTF_DONT_CSE;
        }
        else
        {
            if (size > 1)
            {
                unsigned cns = src->gtIntCon.gtIconVal;
                cns  = cns & 0xFF;
                cns |= cns << 8;
                if (size == 4)
                    cns |= cns << 16;
                src->gtIntCon.gtIconVal = cns;
            }
        }

        /* Create the assignment node */

        result = gtNewAssignNode(dest, src);
        result->gtType = type;

        return result;
    }

GENERAL_BLKOP:

    result = gtNewOperNode(oper, TYP_VOID,        /*        GT_<oper>          */
                                                  /*        /    \             */
                 gtNewOperNode(GT_LIST, TYP_VOID, /*   GT_LIST    \            */
                                                  /*    /    \     \           */
                               dest, src),        /* [dest] [src]   \          */
                           blkShape);             /*              [size/clsHnd]*/

    result->gtFlags |=  (GTF_EXCEPT | GTF_GLOB_REF);

    compBlkOpUsed = true;

    return result;
}

/*****************************************************************************/

GenTreePtr Compiler::impAssignStructPtr(GenTreePtr      dest,
                                        GenTreePtr      src,
                                        CORINFO_CLASS_HANDLE    structHnd,
                                        unsigned        curLevel)
{

    assert(src->TypeGet() == TYP_STRUCT);
    assert(src->gtOper == GT_LCL_VAR || src->gtOper == GT_FIELD    ||
           src->gtOper == GT_IND     || src->gtOper == GT_LDOBJ    ||
           src->gtOper == GT_CALL    || src->gtOper == GT_MKREFANY ||
           src->gtOper == GT_COMMA );

    if (src->gtOper == GT_CALL)
    {
        // insert the return value buffer into the argument list as first byref parameter
        src->gtCall.gtCallArgs = gtNewOperNode(GT_LIST, TYP_VOID,
                                               dest, src->gtCall.gtCallArgs);
        // now returns void, not a struct
        src->gtType = TYP_VOID;

        // remember that the first arg is return buffer
        src->gtCall.gtCallMoreFlags |= GTF_CALL_M_RETBUFFARG; 

        // return the morphed call node
        return src;
    }

    if (src->gtOper == GT_LDOBJ)
    {
        assert(src->gtLdObj.gtClass == structHnd);
        src = src->gtOp.gtOp1;
    }
    else if (src->gtOper == GT_MKREFANY)
    {
        GenTreePtr destClone;
        dest = impCloneExpr(dest, &destClone, structHnd, curLevel);

        assert(offsetof(CORINFO_RefAny, dataPtr) == 0);
        assert(dest->gtType == TYP_I_IMPL);
        GenTreePtr ptrSlot  = gtNewOperNode(GT_IND, TYP_I_IMPL, dest);
        GenTreePtr typeSlot = gtNewOperNode(GT_IND, TYP_I_IMPL,
                                  gtNewOperNode(GT_ADD, TYP_I_IMPL, destClone,
                                      gtNewIconNode(offsetof(CORINFO_RefAny, type))));

        // append the assign of the pointer value
        GenTreePtr asg = gtNewAssignNode(ptrSlot, src->gtLdObj.gtOp1);
        impAppendTree(asg, curLevel, impCurStmtOffs);

        // return the assign of the type value, to be appended
        return gtNewAssignNode(typeSlot, src->gtOp.gtOp2);
    }
    else if (src->gtOper == GT_COMMA)
    {
        assert(src->gtOp.gtOp2->gtType == TYP_STRUCT);  // Second thing is the struct
        impAppendTree(src->gtOp.gtOp1, curLevel, impCurStmtOffs);  // do the side effect

        // evaluate the second thing using recursion
        return impAssignStructPtr(dest, src->gtOp.gtOp2, structHnd, curLevel);
    }
    else
    {
        src = gtNewOperNode(GT_ADDR, TYP_BYREF, src);
    }

    // return a GT_COPYBLK node, to be appended
    return impHandleBlockOp(GT_COPYBLK,
                            dest, src,
                            impGetCpobjHandle(structHnd),
                            false);
}

/*****************************************************************************
   Given TYP_STRUCT value, and the class handle for that structure, return
   the expression for the address for that structure value.

   willDeref - does the caller guarantee to dereference the pointer.
*/

GenTreePtr Compiler::impGetStructAddr(GenTreePtr    structVal,
                                      CORINFO_CLASS_HANDLE  structHnd,
                                      unsigned      curLevel,
                                      bool          willDeref)
{
    assert(structVal->TypeGet() == TYP_STRUCT);

    genTreeOps      oper = structVal->gtOper;

    if (oper == GT_LDOBJ && willDeref)
    {
        assert(structVal->gtLdObj.gtClass == structHnd);
        return(structVal->gtLdObj.gtOp1);
    }
    else if (oper == GT_CALL || oper == GT_LDOBJ || oper == GT_MKREFANY)
    {
        unsigned    tmpNum = lvaGrabTemp();

        impAssignTempGen(tmpNum, structVal, structHnd, curLevel);

        // The 'return value' is now the temp itself

        GenTreePtr temp = gtNewLclvNode(tmpNum, TYP_STRUCT);
        temp = gtNewOperNode(GT_ADDR, TYP_I_IMPL, temp);
        temp->gtFlags |= GTF_ADDR_ONSTACK;

        return temp;
    }
    else if (oper == GT_COMMA)
    {
        assert(structVal->gtOp.gtOp2->gtType == TYP_STRUCT); // Second thing is the struct
        structVal->gtOp.gtOp2 = impGetStructAddr(structVal->gtOp.gtOp2, structHnd, curLevel, willDeref);
        structVal->gtType = TYP_BYREF;
        return(structVal);
    }
    else if (oper == GT_LCL_VAR)
    {
        /* Only IL can do explicit aliasing. Here, we must be taking the
           address for doing a GT_COPYBLK or a similar operation */
        if (false) lvaTable[structVal->gtLclVar.gtLclNum].lvAddrTaken = 1;
    }

    return(gtNewOperNode(GT_ADDR, TYP_BYREF, structVal));
}

/*****************************************************************************
/* Given TYP_STRUCT value 'structVal', make certain it is 'canonical', that is
   it is either a LDOBJ or a MKREFANY node */

GenTreePtr      Compiler::impNormStructVal(GenTreePtr    structVal,
                                           CORINFO_CLASS_HANDLE  structType,
                                           unsigned      curLevel)
{
    assert(structVal->TypeGet() == TYP_STRUCT);
    assert(structType != BAD_CLASS_HANDLE);

    // Is it already normalized?
    if (structVal->gtOper == GT_MKREFANY || structVal->gtOper == GT_LDOBJ)
        return(structVal);

    // Normalize it by wraping it in a LDOBJ

    structVal = impGetStructAddr(structVal, structType, curLevel, true); // get the addr of struct
    structVal = gtNewOperNode(GT_LDOBJ, TYP_STRUCT, structVal);
    structVal->gtFlags |= GTF_EXCEPT | GTF_GLOB_REF;
    structVal->gtLdObj.gtClass = structType;
    return(structVal);
}

/*****************************************************************************
 *
 *  Pop the given number of values from the stack in reverse order (STDCALL)
 */

GenTreePtr          Compiler::impPopRevList(unsigned   count,
                                            unsigned * flagsPtr,
                                            CORINFO_SIG_INFO*  sig)

{
    GenTreePtr ptr = impPopList(count, flagsPtr, sig);

        // reverse the list
    if (ptr == 0)
        return ptr;

    GenTreePtr prev = 0;
    do {
        assert(ptr->gtOper == GT_LIST);
        GenTreePtr tmp = ptr->gtOp.gtOp2;
        ptr->gtOp.gtOp2 = prev;
        prev = ptr;
        ptr = tmp;
    }
    while (ptr != 0);
    return prev;
}

/*****************************************************************************
 *
 *  We have a jump to 'block' with a non-empty stack, and the block expects
 *  its input to come in a different set of temps than we have it in at the
 *  end of the previous block. Therefore, we'll have to insert a new block
 *  along the jump edge to transfer the temps to the expected place.
 */

BasicBlock *        Compiler::impMoveTemps(BasicBlock * srcBlk, BasicBlock *destBlk, unsigned baseTmp)
{
    unsigned        destTmp = destBlk->bbStkTemps;

    BasicBlock *    mvBlk;
    unsigned        tmpNo;

    assert(verCurrentState.esStackDepth);
    assert(destTmp != NO_BASE_TMP);
    assert(destTmp != baseTmp);

    mvBlk               = fgNewBBinRegion(BBJ_ALWAYS, srcBlk->bbTryIndex, NULL);
    mvBlk->bbRefs       = 1;
    mvBlk->bbStkDepth   = verCurrentState.esStackDepth;
    mvBlk->bbJumpDest   = destBlk;
    mvBlk->bbFlags     |= BBF_INTERNAL | ((srcBlk->bbFlags|destBlk->bbFlags) & BBF_RUN_RARELY);

#ifdef DEBUG
    if  (verbose) printf("Transfer %u temps from #%u (BB%02u) to #%u (BB%02u) via new BB%02u (%08X) \n",
        verCurrentState.esStackDepth, baseTmp, srcBlk->bbNum, destTmp, destBlk->bbNum, mvBlk->bbNum, mvBlk);
#endif

    /* Create the transfer list of trees */

    impBeginTreeList();

    tmpNo = verCurrentState.esStackDepth;
    do
    {
        /* One less temp to deal with */

        assert(tmpNo); tmpNo--;

        GenTreePtr  tree = verCurrentState.esStack[tmpNo].val;
        assert(impValidSpilledStackEntry(tree));
        assert(tree->gtOper == GT_LCL_VAR);
        
        /* Get hold of the type we're transferring */

        var_types  lclTyp = tree->TypeGet(); 

        /* Create the assignment node */
        GenTreePtr  asg = gtNewAssignNode(gtNewLclvNode(destTmp + tmpNo, lclTyp),
                                          gtNewLclvNode(baseTmp + tmpNo, lclTyp));

        /* Append the expression statement to the list */

        impAppendTree(asg, CHECK_SPILL_NONE, impCurStmtOffs);
    }
    while (tmpNo);

    impEndTreeList(mvBlk);

    return mvBlk;
}

/******************************************************************************
 *  Spills the stack at verCurrentState.esStack[level] and replaces it with a temp.
 *  If tnum!=BAD_VAR_NUM, the temp var used to replace the tree is tnum,
 *     else, grab a new temp.
 *  For structs (which can be pushed on the stack using ldobj, etc),
 *  special handling is needed
 */

bool                Compiler::impSpillStackEntry(unsigned   level,
                                                 unsigned   tnum)
{
    GenTreePtr      tree = verCurrentState.esStack[level].val;

    /* Allocate a temp if we havent been asked to use a particular one */


    if (tiVerificationNeeded)
    {
        // Ignore bad temp requests (they will happen with bad code and will be
        // catched when importing the destblock)
        if ((tnum != BAD_VAR_NUM && tnum >= lvaCount) && verNeedsVerification())
            return false;
    }        
    else
    {
        assert(tnum == BAD_VAR_NUM || tnum < lvaCount);
    }

    if (tnum == BAD_VAR_NUM)
    {
        tnum = lvaGrabTemp();
    }
    else if (tiVerificationNeeded && lvaTable[tnum].TypeGet() != TYP_UNDEF)
    {
        // if verification is needed and tnum's type is incompatible with
        // type on that stack, we grab a new temp. This is safe since
        // we will throw a verification exception in the dest block.
        
        var_types valTyp = tree->TypeGet();
        var_types dstTyp = lvaTable[tnum].TypeGet();

        // if the two types are different, we return. This will only happen with bad code and will
        // be catched when importing the destblock. We still allow int/byrefs and float/double differences.
        if ((genActualType(valTyp) != genActualType(dstTyp)) &&
            !((valTyp == TYP_INT    && dstTyp == TYP_BYREF)   ||
              (valTyp == TYP_BYREF  &&  dstTyp == TYP_INT)    ||
              (varTypeIsFloating(dstTyp) && varTypeIsFloating(valTyp))))
        {
            //tnum = lvaGrabTemp();

            if (verNeedsVerification())
                return false;
        }
    }

    /* get the original type of the tree (it may be wacked by impAssignTempGen) */
    var_types type = genActualType(tree->gtType);

    /* Assign the spilled entry to the temp */
    impAssignTempGen(tnum, tree, verCurrentState.esStack[level].seTypeInfo.GetClassHandle(), level);

    GenTreePtr temp = gtNewLclvNode(tnum, type);
    verCurrentState.esStack[level].val = temp;

    return true;
}

/*****************************************************************************
 *
 *  Ensure that the stack has only spilled values
 */

void                Compiler::impSpillStackEnsure(bool spillLeaves)
{
    assert(!spillLeaves || opts.compDbgCode);

    for (unsigned level = 0; level < verCurrentState.esStackDepth; level++)
    {
        GenTreePtr      tree = verCurrentState.esStack[level].val;

        if (!spillLeaves && tree->OperIsLeaf())
            continue;

        // Temps introduced by the importer itself dont need to be spilled

        bool isTempLcl = (tree->OperGet() == GT_LCL_VAR) &&
                         (tree->gtLclVar.gtLclNum >= info.compLocalsCount);

        if  (isTempLcl)
            continue;

        // @TODO [CONSIDER] [04/16/01] []: skip all trees which are impValidSpilledStackEntry(tree)

        impSpillStackEntry(level);
    }
}

/*****************************************************************************
 *
 *  If the stack contains any trees with side effects in them, assign those
 *  trees to temps and append the assignments to the statement list.
 *  On return the stack is guaranteed to be empty.
 */

inline
void                Compiler::impEvalSideEffects()
{
    impSpillSideEffects(false, CHECK_SPILL_ALL);
    verCurrentState.esStackDepth = 0;
}

/*****************************************************************************
 *
 *  If the stack contains any trees with side effects in them, assign those
 *  trees to temps and replace them on the stack with refs to their temps.
 *  [0..chkLevel) is the portion of the stack which will be checked and spilled.
 */

inline
void                Compiler::impSpillSideEffects(bool      spillGlobEffects,
                                                  unsigned  chkLevel)
{
    assert(chkLevel != CHECK_SPILL_NONE);

    /* Before we make any appends to the tree list we must spill the
     * "special" side effects (GTF_OTHER_SIDEEFF) - GT_BB_QMARK, GT_CATCH_ARG */

    impSpillSpecialSideEff();

    if (chkLevel == CHECK_SPILL_ALL)
        chkLevel = verCurrentState.esStackDepth;

    assert(chkLevel <= verCurrentState.esStackDepth);

    unsigned spillFlags = spillGlobEffects ? GTF_GLOB_EFFECT : GTF_SIDE_EFFECT;

    for (unsigned i = 0; i < chkLevel; i++)
    {
        if  (verCurrentState.esStack[i].val->gtFlags & spillFlags)
            impSpillStackEntry(i);
    }
}

/*****************************************************************************
 *
 *  If the stack contains any trees with special side effects in them, assign
 *  those trees to temps and replace them on the stack with refs to their temps.
 */

inline
void                Compiler::impSpillSpecialSideEff()
{
    // Only exception objects and _?: need to be carefully handled

    if  (!compCurBB->bbCatchTyp &&
         !(isBBF_BB_QMARK(compCurBB->bbFlags) && compCurBB->bbStkDepth == 1))
         return;

    for (unsigned level = 0; level < verCurrentState.esStackDepth; level++)
    {
        if  (verCurrentState.esStack[level].val->gtFlags & GTF_OTHER_SIDEEFF)
            impSpillStackEntry(level);
    }
}




/*****************************************************************************
 *
 *  Spill all stack references to value classes (TYP_STRUCT nodes)
 */

void                Compiler::impSpillValueClasses()
{
    for (unsigned level = 0; level < verCurrentState.esStackDepth; level++)
    {
        GenTreePtr      tree = verCurrentState.esStack[level].val;

        if (fgWalkTreePre(tree, impFindValueClasses) == WALK_ABORT)
        {
            // Tree walk was aborted, which means that we found a
            // value class on the stack.  Need to spill that
            // stack entry.

            impSpillStackEntry(level);
        }

    }
}

/*****************************************************************************
 *
 *  Callback that checks if a tree node is TYP_STRUCT
 */

Compiler::fgWalkResult Compiler::impFindValueClasses(GenTreePtr     tree,
                                                     void       *   pCallBackData)
{
    fgWalkResult walkResult = WALK_CONTINUE;
    
    if (tree->gtType == TYP_STRUCT)
    {
        // Abort the walk and indicate that we found a value class

        walkResult = WALK_ABORT;
    }

    return walkResult;
}

/*****************************************************************************
 *
 *  If the stack contains any trees with references to local #lclNum, assign
 *  those trees to temps and replace their place on the stack with refs to
 *  their temps.
 */

void                Compiler::impSpillLclRefs(int lclNum)
{
    /* Before we make any appends to the tree list we must spill the
     * "special" side effects (GTF_OTHER_SIDEEFF) - GT_BB_QMARK, GT_CATCH_ARG */

    impSpillSpecialSideEff();

    for (unsigned level = 0; level < verCurrentState.esStackDepth; level++)
    {
        GenTreePtr      tree = verCurrentState.esStack[level].val;

        /* If the tree may throw an exception, and the block has a handler,
           then we need to spill assignments to the local if the local is
           live on entry to the handler.
           Just spill 'em all without considering the liveness */

        bool xcptnCaught = (compCurBB->bbFlags & BBF_HAS_HANDLER) &&
                           (tree->gtFlags & (GTF_CALL | GTF_EXCEPT));

        /* Skip the tree if it doesn't have an affected reference,
           unless xcptnCaught */

        if  (xcptnCaught || gtHasRef(tree, lclNum, false))
        {
            impSpillStackEntry(level);
        }
    }
}

/*****************************************************************************
 *
 *  If the given block pushes a value on the stack and doesn't contain any
 *  assignments that would interfere with the current stack contents, return
 *  the type of the pushed value; otherwise, return 'TYP_UNDEF'.
 *  If the block pushes a floating type on the stack, *pHasFloat is set to true
 *    @TODO [REVISIT] [04/16/01] []: Remove pHasFloat after ?: works with floating point values.
 *    Currently, raEnregisterFPvar() doesnt correctly handle the flow of control
 *    implicit in a ?:
 */

var_types           Compiler::impBBisPush(BasicBlock *  block,
                                          bool       *  pHasFloat)
{
    const   BYTE *  codeAddr;
    const   BYTE *  codeEndp;

    unsigned char   stackCont[64];      // arbitrary stack depth restriction

    unsigned char * stackNext = stackCont;
    unsigned char * stackBeg  = stackCont;
    unsigned char * stackEnd  = stackCont + sizeof(stackCont);

    /* Walk the opcodes that comprise the basic block */

    codeAddr = info.compCode + block->bbCodeOffs;
    codeEndp =      codeAddr + block->bbCodeSize;
    unsigned        numArgs = info.compArgsCount;

    while (codeAddr < codeEndp)
    {
        signed  int     sz;
        OPCODE          opcode;
        CORINFO_CLASS_HANDLE    clsHnd;
        /* Get the next opcode and the size of its parameters */

        opcode = (OPCODE) getU1LittleEndian(codeAddr);
        codeAddr += sizeof(__int8);

    DECODE_OPCODE:

        /* Get the size of additional parameters */

        sz = opcodeSizes[opcode];

        /* See what kind of an opcode we have, then */

        switch (opcode)
        {
            var_types       lclTyp;
            unsigned        lclNum;
            int             memberRef, descr;
            CORINFO_SIG_INFO    sig;
            CORINFO_METHOD_HANDLE   methHnd;

        case CEE_PREFIX1:
            opcode = OPCODE(getU1LittleEndian(codeAddr) + 256);
            codeAddr += sizeof(__int8);
            goto DECODE_OPCODE;
        case CEE_LDARG_0:
        case CEE_LDARG_1:
        case CEE_LDARG_2:
        case CEE_LDARG_3:
            lclNum = (opcode - CEE_LDARG_0);
            assert(lclNum >= 0 && lclNum < 4);
            goto LDARG;

        case CEE_LDARG_S:
            lclNum = getU1LittleEndian(codeAddr);
            goto LDARG;

        case CEE_LDARG:
            lclNum = getU2LittleEndian(codeAddr);
        LDARG:
            lclNum = compMapILargNum(lclNum);     // account for possible hidden param
            goto LDLOC;

        case CEE_LDLOC_0:
        case CEE_LDLOC_1:
        case CEE_LDLOC_2:
        case CEE_LDLOC_3:
            lclNum = (opcode - CEE_LDLOC_0);
            assert(lclNum >= 0 && lclNum < 4);
            lclNum += numArgs;
            goto LDLOC;

        case CEE_LDLOC_S:
            lclNum = getU1LittleEndian(codeAddr) + numArgs;
            goto LDLOC;

        case CEE_LDLOC:
            lclNum = getU2LittleEndian(codeAddr) + numArgs;
        LDLOC:
            lclTyp = lvaGetActualType(lclNum);
            goto PUSH;

        case CEE_LDC_I4_M1 :
        case CEE_LDC_I4_0 :
        case CEE_LDC_I4_1 :
        case CEE_LDC_I4_2 :
        case CEE_LDC_I4_3 :
        case CEE_LDC_I4_4 :
        case CEE_LDC_I4_5 :
        case CEE_LDC_I4_6 :
        case CEE_LDC_I4_7 :
        case CEE_LDC_I4_8 :     lclTyp = TYP_I_IMPL;    goto PUSH;

        case CEE_LDC_I4_S :
        case CEE_LDC_I4 :       lclTyp = TYP_INT;       goto PUSH;

        case CEE_LDFTN :

        case CEE_LDSTR :        lclTyp = TYP_REF;       goto PUSH;
        case CEE_LDNULL :       lclTyp = TYP_REF;       goto PUSH;
        case CEE_LDC_I8 :       lclTyp = TYP_LONG;      goto PUSH;
        case CEE_LDC_R4 :       lclTyp = TYP_FLOAT;     goto PUSH;
        case CEE_LDC_R8 :       lclTyp = TYP_DOUBLE;    goto PUSH;

    PUSH:
            /* Make sure there is room on our little stack */

            if  (stackNext == stackEnd)
                return  TYP_UNDEF;

            *stackNext++ = lclTyp;
            break;


        case CEE_LDIND_I1 :
        case CEE_LDIND_I2 :
        case CEE_LDIND_I4 :
        case CEE_LDIND_U1 :
        case CEE_LDIND_U2 :
        case CEE_LDIND_U4 :     lclTyp = TYP_INT;   goto LD_IND;

        case CEE_LDIND_I8 :     lclTyp = TYP_LONG;  goto LD_IND;
        case CEE_LDIND_R4 :     lclTyp = TYP_FLOAT; goto LD_IND;
        case CEE_LDIND_R8 :     lclTyp = TYP_DOUBLE;goto LD_IND;
        case CEE_LDIND_REF :    lclTyp = TYP_REF;   goto LD_IND;
        case CEE_LDIND_I :      lclTyp = TYP_I_IMPL;goto LD_IND;

    LD_IND:

            stackNext--;        // Pop the pointer

            if  (stackNext < stackBeg)
                return  TYP_UNDEF;

            assert((TYP_I_IMPL == (var_types)stackNext[0]) ||
                   (TYP_BYREF  == (var_types)stackNext[0]));

            goto PUSH;


        case CEE_UNALIGNED:
            break;

        case CEE_VOLATILE:
            break;

        case CEE_LDELEM_I1 :
        case CEE_LDELEM_I2 :
        case CEE_LDELEM_U1 :
        case CEE_LDELEM_U2 :
        case CEE_LDELEM_I  :
        case CEE_LDELEM_U4 :
        case CEE_LDELEM_I4 :    lclTyp = TYP_INT   ; goto ARR_LD;

        case CEE_LDELEM_I8 :    lclTyp = TYP_LONG  ; goto ARR_LD;
        case CEE_LDELEM_R4 :    lclTyp = TYP_FLOAT ; goto ARR_LD;
        case CEE_LDELEM_R8 :    lclTyp = TYP_DOUBLE; goto ARR_LD;
        case CEE_LDELEM_REF:    lclTyp = TYP_REF   ; goto ARR_LD;

        case CEE_LDELEMA   :    lclTyp = TYP_BYREF ; goto ARR_LD;

        ARR_LD:

            /* Pop the index value and array address */

            stackNext -= 2;

            if  (stackNext < stackBeg)
                return  TYP_UNDEF;

            assert(TYP_REF == (var_types)stackNext[0]);     // Array object
            assert(TYP_INT == (var_types)stackNext[1]);     // Index

            /* Push the result of the indexing load */

            goto PUSH;

        case CEE_LDLEN :

            /* Pop the array object from the stack */

            stackNext--;

            if  (stackNext < stackBeg)
                return  TYP_UNDEF;

            assert(TYP_REF == (var_types)stackNext[0]);     // Array object

            lclTyp = TYP_INT;
            goto PUSH;

        case CEE_LDFLD :

            /* Pop the address from the stack */

            stackNext--;

            if  (stackNext < stackBeg)
                return  TYP_UNDEF;

            assert(varTypeIsGC(var_types(stackNext[0])) ||
                   varTypeIsI (var_types(stackNext[0])) ||
                (TYP_STRUCT == var_types(stackNext[0])));

            // FALL Through

        case CEE_LDSFLD :

            memberRef = getU4LittleEndian(codeAddr);
            lclTyp = genActualType(eeGetFieldType(eeFindField(memberRef, info.compScopeHnd, 0), &clsHnd));
            goto PUSH;


        case CEE_STLOC_0:
        case CEE_STLOC_1:
        case CEE_STLOC_2:
        case CEE_STLOC_3:
        case CEE_STLOC_S:
        case CEE_STLOC:

        case CEE_STARG_S:
        case CEE_STARG:

            /* For now, don't bother with assignmnents */

            return  TYP_UNDEF;

        case CEE_LDLOCA :
        case CEE_LDLOCA_S :
        case CEE_LDARGA :
        case CEE_LDARGA_S :     lclTyp = TYP_BYREF;   goto PUSH;

        case CEE_ARGLIST :      lclTyp = TYP_I_IMPL;  goto PUSH;

        case CEE_ADD :
        case CEE_DIV :
        case CEE_DIV_UN :

        case CEE_REM :
        case CEE_REM_UN :

        case CEE_MUL :
        case CEE_SUB :
        case CEE_AND :

        case CEE_OR :

        case CEE_XOR :

            /* Make sure we're not reaching below our stack start */

            if  (stackNext <= stackBeg + 1)
                return  TYP_UNDEF;

            if  (stackNext[-1] != stackNext[-2])
                return TYP_UNDEF;

            /* Pop 2 operands, push one result -> pop one stack slot */

            stackNext--;
            break;


        case CEE_CEQ :
        case CEE_CGT :
        case CEE_CGT_UN :
        case CEE_CLT :
        case CEE_CLT_UN :

            /* Make sure we're not reaching below our stack start */

            if  (stackNext < stackBeg + 2)
                return  TYP_UNDEF;

            /* Pop one value off the stack, change the other one to TYP_INT */

            if  (stackNext[-1] != stackNext[-2])
                return TYP_UNDEF;

            stackNext--;
            stackNext[-1] = TYP_INT;
            break;

        case CEE_SHL :
        case CEE_SHR :
        case CEE_SHR_UN :

            /* Pop one of the shiftAmount, and leave the other operand */

            stackNext--;

            if  (stackNext < stackBeg + 1)
                return  TYP_UNDEF;

            assert((TYP_INT   == (var_types)stackNext[0]) ||
                   (TYP_BYREF == (var_types)stackNext[0]));

            break;

        case CEE_NEG :

        case CEE_NOT :

        case CEE_CASTCLASS :
        case CEE_ISINST :

            /* Merely make sure the stack is non-empty */

            if  (stackNext == stackBeg)
                return  TYP_UNDEF;

            break;

        case CEE_CONV_I1 :
        case CEE_CONV_I2 :
        case CEE_CONV_I4 :
        case CEE_CONV_U1 :
        case CEE_CONV_U2 :
        case CEE_CONV_U4 :
        case CEE_CONV_U  :
        case CEE_CONV_I  :
        case CEE_CONV_OVF_I :
        case CEE_CONV_OVF_I_UN:
        case CEE_CONV_OVF_I1 :
        case CEE_CONV_OVF_I1_UN :
        case CEE_CONV_OVF_U1 :
        case CEE_CONV_OVF_U1_UN :
        case CEE_CONV_OVF_I2 :
        case CEE_CONV_OVF_I2_UN :
        case CEE_CONV_OVF_U2 :
        case CEE_CONV_OVF_U2_UN :
        case CEE_CONV_OVF_I4 :
        case CEE_CONV_OVF_I4_UN :
        case CEE_CONV_OVF_U :
        case CEE_CONV_OVF_U_UN:
        case CEE_CONV_OVF_U4 :
        case CEE_CONV_OVF_U4_UN :    lclTyp = TYP_INT;     goto CONV;
        case CEE_CONV_OVF_I8 :
        case CEE_CONV_OVF_I8_UN:
        case CEE_CONV_OVF_U8 :
        case CEE_CONV_OVF_U8_UN:
        case CEE_CONV_U8 :
        case CEE_CONV_I8 :        lclTyp = TYP_LONG;    goto CONV;

        case CEE_CONV_R4 :        lclTyp = TYP_FLOAT;   goto CONV;

        case CEE_CONV_R_UN :
        case CEE_CONV_R8 :        lclTyp = TYP_DOUBLE;  goto CONV;

    CONV:
            /* Make sure the stack is non-empty and bash the top type */

            if  (stackNext == stackBeg)
                return  TYP_UNDEF;

            stackNext[-1] = lclTyp;
            break;

        case CEE_POP :

            stackNext--;

            if (stackNext < stackBeg)
                return TYP_UNDEF;

            break;

        case CEE_DUP :

            /* Make sure the stack is non-empty */

            if  (stackNext == stackBeg)
                return  TYP_UNDEF;

            /* Repush what's at the top */

            lclTyp = (var_types)stackNext[-1];
            goto PUSH;

         case CEE_NEWARR :

            /* Make sure the stack is non-empty */

            if  (stackNext == stackBeg)
                return  TYP_UNDEF;

            // Replace the numElems with the array object

            assert(TYP_INT == (var_types)stackNext[-1]);

            stackNext[-1] = TYP_REF;
            break;

        case CEE_CALLI :
            descr  = getU4LittleEndian(codeAddr);
            eeGetSig(descr, info.compScopeHnd, &sig);
            --stackNext;        // pop the pointer 
            goto CALL;

        case CEE_NEWOBJ :
        case CEE_CALL :
        case CEE_CALLVIRT :
            memberRef  = getU4LittleEndian(codeAddr);
            methHnd = eeFindMethod(memberRef, info.compScopeHnd, 0);
            eeGetMethodSig(methHnd, &sig);

            if  ((sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_VARARG)
            {
                /* Get the total number of arguments for this call site */
                unsigned    numArgsDef = sig.numArgs;
                eeGetCallSiteSig(memberRef, info.compScopeHnd, &sig);
                assert(numArgsDef <= sig.numArgs);
            }

        CALL:

            /* Pop the arguments and make sure we pushed them */

            stackNext -= (opcode == CEE_NEWOBJ) ? sig.numArgs
                                                : sig.totalILArgs();
            if  (stackNext < stackBeg)
                return  TYP_UNDEF;

            /* Push the result of the call if non-void */

            if (opcode == CEE_NEWOBJ)
            {
                if (eeGetClassAttribs(eeGetMethodClass(methHnd)) & CORINFO_FLG_VALUECLASS)
                    return TYP_UNDEF;
                lclTyp = TYP_REF;
            }
            else
                lclTyp = genActualType(JITtype2varType(sig.retType));

            if  (lclTyp != TYP_VOID)
                goto PUSH;

            break;

        case CEE_BR :
        case CEE_BR_S :
        case CEE_LEAVE :
        case CEE_LEAVE_S :
            assert(codeAddr + sz == codeEndp);
            break;

#ifdef DEBUG
        case CEE_LDVIRTFTN:
        case CEE_LDFLDA :
        case CEE_LDSFLDA :
        case CEE_STIND_I1 :
        case CEE_STIND_I2 :
        case CEE_STIND_I4 :
        case CEE_STIND_I8 :
        case CEE_STIND_I :
        case CEE_STIND_REF :
        case CEE_STIND_R4 :
        case CEE_STIND_R8 :
        case CEE_STELEM_I1 :
        case CEE_STELEM_I2 :
        case CEE_STELEM_I4 :
        case CEE_STELEM_I :
        case CEE_STELEM_I8 :
        case CEE_STELEM_REF :
        case CEE_STELEM_R4 :
        case CEE_STELEM_R8 :
        case CEE_STFLD :
        case CEE_STSFLD :

        case CEE_BEQ:
        case CEE_BEQ_S:
        case CEE_BGE:
        case CEE_BGE_S:
        case CEE_BGE_UN:
        case CEE_BGE_UN_S:
        case CEE_BGT:
        case CEE_BGT_S:
        case CEE_BGT_UN:
        case CEE_BGT_UN_S:
        case CEE_BLE:
        case CEE_BLE_S:
        case CEE_BLE_UN:
        case CEE_BLE_UN_S:
        case CEE_BLT:
        case CEE_BLT_S:
        case CEE_BLT_UN:
        case CEE_BLT_UN_S:
        case CEE_BNE_UN:
        case CEE_BNE_UN_S:
        case CEE_BRFALSE_S :
        case CEE_BRTRUE_S :
        case CEE_BRFALSE :
        case CEE_BRTRUE :

        case CEE_SWITCH :
        case CEE_BREAK :
        case CEE_TAILCALL :
        case CEE_RET :
        case CEE_JMP:


        case CEE_THROW :
        case CEE_RETHROW :
        case CEE_ENDFILTER:
        case CEE_ENDFINALLY:

        case CEE_INITOBJ :
        case CEE_LDOBJ :
        case CEE_CPOBJ :
        case CEE_STOBJ :
        case CEE_CPBLK :
        case CEE_INITBLK :
        case CEE_BOX:
        case CEE_UNBOX:

        case CEE_MACRO_END :
        case CEE_LOCALLOC :
        case CEE_SIZEOF :
        case CEE_CKFINITE :
        case CEE_LDTOKEN :
        case CEE_MKREFANY :
        case CEE_REFANYVAL :
        case CEE_REFANYTYPE :

        case CEE_NOP:

        //@TODO [CONSIDER] [04/16/01] []: these should be handled like regular arithmetic operations

        case CEE_SUB_OVF:
        case CEE_SUB_OVF_UN:
        case CEE_ADD_OVF:
        case CEE_ADD_OVF_UN:
        case CEE_MUL_OVF:
        case CEE_MUL_OVF_UN:

            return TYP_UNDEF;

        default :
            printf("ASSERT: Handle 'OP_%s', or add to the list above\n",
                   opcodeNames[opcode]);
            assert(!"Invalid opcode in impBBisPush()");
            return TYP_UNDEF;
#else
        default:
            return TYP_UNDEF;
#endif
        }

        codeAddr += sz;

        // Have we pushed a floating point value on the stack.
        // ?: doesnt work with floating points.

        if ( (stackNext > stackBeg) &&
             (varTypeIsFloating((var_types)stackNext[-1])) )
        {
            *pHasFloat = true;
        }
    }

    /* Did we end up with precisely one item on the stack? */

    if  (stackNext == stackCont+1)
        return  (var_types)stackCont[0];

    return  TYP_UNDEF;
}

/*****************************************************************************
 *
 *  If the given block (which is known to end with a conditional jump) forms
 *  a ?: expression, return the false/true/result blocks and the type of the
 *  result.
 */

bool                Compiler::impCheckForQmarkColon( BasicBlock *   block,
                                                     BasicBlock * * falseBlkPtr,
                                                     BasicBlock * * trueBlkPtr,
                                                     BasicBlock * * rsltBlkPtr,
                                                     var_types    * rsltTypPtr,
                                                     bool         * pHasFloat)
{
    assert( (opts.compFlags & CLFLG_QMARK)     && 
           !(block->bbFlags & BBF_HAS_HANDLER) &&
           !tiVerificationNeeded);
    assert(block->bbJumpKind == BBJ_COND);

    var_types     falseType;
    BasicBlock *  falseBlk;
    var_types     trueType;
    BasicBlock *  trueBlk;
    var_types     rsltType;
    BasicBlock *  rsltBlk;

    /*
        We'll look for the following flow-graph pattern:

            ---------------------
                   #refs   [jump]
            ---------------------
            block         -> trueBlk
            falseBlk  1   -> rsltBlk
            trueBlk   1
            rsltBlk   2
            ---------------------

        If both 'falseBlk' and 'trueBlk' push a value
        of the same type on the stack and don't contain
        any harmful side effects, we'll avoid spilling
        the entire stack.
     */

    falseBlk = block->bbNext;
    trueBlk  = block->bbJumpDest;

    if  (falseBlk->bbNext     != trueBlk)
        return  false;

    if  (falseBlk->bbJumpKind != BBJ_ALWAYS)
        return  false;

    if  ( trueBlk->bbJumpKind != BBJ_NONE)
        return  false;

    rsltBlk = trueBlk->bbNext;

    if  (falseBlk->bbJumpDest != rsltBlk)
        return  false;
 
    if  (falseBlk->bbRefs     != 1)
        return  false;

    if  ( trueBlk->bbRefs     != 1)
        return  false;

    if  (rsltBlk->bbRefs      != 2)
        return false;

    // The blocks couldn't have been processed already as we are
    // processing their only predecessor

    assert((falseBlk->bbFlags & BBF_IMPORTED) == 0);
    assert(( trueBlk->bbFlags & BBF_IMPORTED) == 0);
    assert(( rsltBlk->bbFlags & BBF_IMPORTED) == 0);

    // Now see if both falseBlk and trueBlk push a value

    *pHasFloat = false;

    falseType = genActualType(impBBisPush(falseBlk, pHasFloat));
    trueType  = genActualType(impBBisPush( trueBlk, pHasFloat));
    rsltType  = trueType;

    if ((trueType == TYP_UNDEF) || (falseType == TYP_UNDEF))
        return false;

    if  (falseType != trueType)
    {
        // coarse some TYP_BYREF into TYP_INT, to make both sides TYP_INT

        if      ((falseType == TYP_BYREF)  && (trueType == TYP_INT))
        {
            falseType = TYP_INT;
        }
        else if ((falseType == TYP_INT)    && (trueType == TYP_BYREF))
        {
            trueType  = TYP_INT;
            rsltType  = TYP_INT;
        }

        // coarse TYP_FLOAT into TYP_DOUBLE, to make both sides TYP_DOUBLE
        
        if      ((falseType == TYP_FLOAT)  && (trueType == TYP_DOUBLE))
        {
            falseType = TYP_DOUBLE;
        }
        else if ((falseType == TYP_DOUBLE) && (trueType == TYP_FLOAT))
        {
            trueType  = TYP_DOUBLE;
            rsltType  = TYP_DOUBLE;
        }

        if (falseType != trueType)
        {
            assert(!"Investigate why we don't always have matching types");
            return false;
        }
    }
    //  Make sure that all three types match
    assert(rsltType ==  falseType);
    assert(rsltType == trueType);

    /* @TODO [CONSIDER] [04/16/01] []: we might want to make ?: optimization work for structs */
    if (rsltType == TYP_STRUCT)
        return false;

    /* This is indeed a "?:" expression */

    *falseBlkPtr = falseBlk;
    * trueBlkPtr =  trueBlk;
    * rsltBlkPtr =  rsltBlk;
    * rsltTypPtr =  rsltType;

    return  true;
}


/*****************************************************************************
 *
 *  If the given block (which is known to end with a conditional jump) forms
 *  a ?: expression, mark it appropriately.
 *
 *  Using GT_QMARK is generally beneficial as having fewer control flow
 *  make things more amenable to optimizations. Also, we save on using
 *  a spill-temp.
 *  GT_BB_QMARK also saves a spill temp, and guaratees that the computation
 *  will be done via a register
 *
 *  Returns true if the successors of the block have been processed.
 */

bool                Compiler::impCheckForQmarkColon(BasicBlock *  block)
{
    GenTreePtr result;

    assert((opts.compFlags & CLFLG_QMARK) && !(block->bbFlags & BBF_HAS_HANDLER)
        && !tiVerificationNeeded);
    assert(block->bbJumpKind == BBJ_COND);

    if (opts.compMinOptim || opts.compDbgCode)
        return false;

    BasicBlock *  falseBlk;
    BasicBlock *  trueBlk;
    BasicBlock *  rsltBlk;
    var_types     rsltType;
    bool          hasFloat;

    if  (!impCheckForQmarkColon(block, & falseBlk,
                                       & trueBlk,
                                       & rsltBlk,
                                       & rsltType, 
                                       & hasFloat))
        return false;

    assert(rsltType != TYP_VOID);

    if (hasFloat || rsltType == TYP_LONG)
    {
    BB_QMARK_COLON:

        // Currently FP enregistering doesn't know about GT_QMARK - GT_COLON
        if (verCurrentState.esStackDepth)
            return false;

        // If verCurrentState.esStackDepth is 0, we can use the simpler GT_BB_QMARK - GT_BB_COLON

        trueBlk->bbFlags  |= BBF_BB_COLON;
        falseBlk->bbFlags |= BBF_BB_COLON;
        rsltBlk->bbFlags  |= BBF_BB_QMARK;

        return false;
    }

    /* We've detected a "?:" expression */

#ifdef DEBUG
    if (verbose)
    {
        printf("\nConvert [type=%s] BB%02u ? BB%02u : BB%02u -> BB%02u:\n", 
               varTypeName(rsltType), 
               block->bbNum, falseBlk->bbNum, trueBlk->bbNum, rsltBlk->bbNum);
    }
#endif

    /* Remember the conditional expression. This will be used as the
       condition of the GT_QMARK. */

    GenTreePtr condStmt = impTreeLast;
    GenTreePtr condExpr = condStmt->gtStmt.gtStmtExpr;
    assert(condExpr->gtOper == GT_JTRUE);

    if ((block->bbCatchTyp && handlerGetsXcptnObj(block->bbCatchTyp)) ||
        isBBF_BB_QMARK(block->bbFlags))
    {
        // condStmt will be moved to rsltBlk as a child of the GT_QMARK.
        // This is a problem if it has a reference to GT_BB_QMARK/GT_CATCH_ARG.
        // So use old style _?:

        if (verCurrentState.esStackDepth || (condExpr->gtFlags & GTF_OTHER_SIDEEFF))
            goto BB_QMARK_COLON;
    }

    // Skip the GT_JTRUE and set condExpr to the relop child 
    condExpr = condExpr->gtOp.gtOp1;
    assert(condExpr->OperIsCompare());

    /* Finish the current BBJ_COND basic block */
    impEndTreeList(block);

    /* Remeber the current stack state for the start of the rsltBlk */

    SavedStack blockState;

    impSaveStackState(&blockState, false);

    //-------------------------------------------------------------------------
    //  Process the true and false blocks to get the expressions they evaluate
    //-------------------------------------------------------------------------

    /* Note that we dont need to make copies of the stack state as these
       blocks wont import the trees on the stack at all.
       To ensure that these blocks dont try to spill the current stack,
       overwrite it with non-spillable items */

    for (unsigned level = 0; level < verCurrentState.esStackDepth; level++)
    {
        static GenTree nonSpill = { GT_CNS_INT, TYP_VOID };
#ifdef DEBUG
        nonSpill.gtFlags |= GTF_MORPHED;
#endif
        verCurrentState.esStack[level].val = &nonSpill;
    }

    // Recursively import falseBlk and trueBlk. These are guaranteed not to
    // cause further recursion as they are both marked with BBF_COLON

    falseBlk->bbFlags |= BBF_COLON;
    impImportBlock(falseBlk);

    trueBlk->bbFlags  |= BBF_COLON;
    impImportBlock(trueBlk);

    // Reset state for rsltBlk. Make sure that it didnt get recursively imported.
    assert((rsltBlk->bbFlags & BBF_IMPORTED) == 0);
    impRestoreStackState(&blockState);

    //-------------------------------------------------------------------------
    // Grab the expressions evaluated by the falseBlk 
    //-------------------------------------------------------------------------

    GenTreePtr  falseExpr = NULL;

    for (GenTreePtr falseStmt = falseBlk->bbTreeList; falseStmt; falseStmt = falseStmt->gtNext)
    {
        assert(falseStmt->gtOper == GT_STMT);
        GenTreePtr expr = falseStmt->gtStmt.gtStmtExpr;
        falseExpr = falseExpr ? gtNewOperNode(GT_COMMA, TYP_VOID, falseExpr, expr)
                              : expr;
    }

    if (falseExpr->gtOper == GT_COMMA)
        falseExpr->gtType = rsltType;

    if (falseExpr->gtType != rsltType)
    {
        assert((rsltType == TYP_INT) &&
               ((falseExpr->gtType == TYP_BYREF) ||
                (genActualType(falseExpr->gtType) == TYP_INT)));
        if (falseExpr->gtType == TYP_BYREF)
            falseExpr->gtType = TYP_INT;
        else
            falseExpr = gtNewCastNode(TYP_INT, falseExpr, rsltType);
    }

    //-------------------------------------------------------------------------
    // Grab the expressions evaluated by the trueBlk 
    //-------------------------------------------------------------------------

    GenTreePtr  trueExpr = NULL;

    for (GenTreePtr trueStmt = trueBlk->bbTreeList; trueStmt; trueStmt = trueStmt->gtNext)
    {
        assert(trueStmt->gtOper == GT_STMT);
        GenTreePtr expr = trueStmt->gtStmt.gtStmtExpr;
        trueExpr = trueExpr ? gtNewOperNode(GT_COMMA, TYP_VOID, trueExpr, expr)
                            : expr;
    }

    if (trueExpr->gtOper == GT_COMMA)
        trueExpr->gtType = rsltType;

    if (trueExpr->gtType != rsltType)
    {
        assert((rsltType == TYP_INT) &&
               ((trueExpr->gtType == TYP_BYREF) ||
                (genActualType(trueExpr->gtType) == TYP_INT)));

        if (trueExpr->gtType == TYP_BYREF)
            trueExpr->gtType = TYP_INT;
        else
            trueExpr = gtNewCastNode(TYP_INT, trueExpr, rsltType);
    }

    //-------------------------------------------------------------------------
    // If falseExpr and trueExpr are both GT_CNS_INT then use the condExpr
    //-------------------------------------------------------------------------

    if ((falseExpr->gtOper == GT_CNS_INT) && 
        ( trueExpr->gtOper == GT_CNS_INT)   )
    {
        bool condRev = false;
        int  diff    = trueExpr->gtIntCon.gtIconVal - falseExpr->gtIntCon.gtIconVal;
        int  loVal   = falseExpr->gtIntCon.gtIconVal;
        
        if ((diff < 0) && (diff != -0x80000000))
        {
            // Reverse the condition
            condExpr->SetOper(GenTree::ReverseRelop(condExpr->gtOper));
            diff  = -diff;
            loVal = trueExpr->gtIntCon.gtIconVal;
            condRev = !condRev;
        }

        unsigned udiff = diff;

        if (udiff == 0)
        {
            result = falseExpr;
            loVal  = 0;
        }
        else if (udiff == 1)
        {
            result = condExpr;
        }
        else if (udiff == genFindLowestBit(udiff))
        {
            int cnt = 0;
            while (udiff > 1)
            {
                cnt++;
                udiff >>= 1;
            }
            result = gtNewOperNode(GT_LSH, rsltType, condExpr, gtNewIconNode(cnt));

            if (cnt >= 24)
            {
                /* We don't need to zero extend the setcc instruction */
                condExpr->gtType = TYP_BYTE;
            }                
        }
        else
        {
            // Reverse the condition
            condExpr->SetOper(GenTree::ReverseRelop(condExpr->gtOper));
            condRev = !condRev;

            result = gtNewOperNode(GT_SUB, rsltType, condExpr, gtNewIconNode(1));
            result = gtNewOperNode(GT_AND, rsltType, result,   gtNewIconNode(udiff));
        }

        /* Now add in the loVal if it not zero */
        if (loVal != 0)
        {
            result = gtNewOperNode(GT_ADD, rsltType, result, gtNewIconNode(loVal));
        }

#if DEBUG
        if (verbose)
        {
            printf("\nFolded into a RELOP expression");
            if (condRev)
                printf("\nReversed the RELOP");
        }
#endif

        goto DONE;
    }

    //-------------------------------------------------------------------------
    // If falseExpr or trueExpr is a simple leaf node 
    // then transform this into a void qmark colon tree
    //-------------------------------------------------------------------------

    unsigned    initLclNum = 0;
    GenTreePtr  initExpr   = NULL;

    if (((falseExpr->OperKind() & GTK_LEAF) ||
         ( trueExpr->OperKind() & GTK_LEAF)   ) &&
        (!(condExpr->gtFlags & GTF_SIDE_EFFECT))  )
    {
        GenTreePtr  leafExpr;
        GenTreePtr  otherExpr;

        if (trueExpr->OperKind() & GTK_LEAF)
        {
            leafExpr  = trueExpr;
            otherExpr = falseExpr;
        }
        else
        {
            assert(falseExpr->OperKind() & GTK_LEAF);

            leafExpr  = falseExpr;
            otherExpr = trueExpr;

            // Reverse the condition
            condExpr->SetOper(GenTree::ReverseRelop(condExpr->gtOper));
#if DEBUG
            if (verbose)
                printf("\nCondition reversed");
#endif
        }

        initLclNum = lvaGrabTemp();
            
        // Set lvType on the new Temp Lcl Var
        lvaTable[initLclNum].lvType = rsltType;

        initExpr  = gtNewTempAssign(initLclNum, leafExpr);

        falseExpr = gtNewTempAssign(initLclNum, otherExpr);
        trueExpr  = gtNewNothingNode();

        rsltType  = TYP_VOID;

#if DEBUG
        if (verbose)
        {
            printf("\nConverted to void QMARK COLON tree");
        }
#endif
    }
    
    //-------------------------------------------------------------------------
    // Make the GT_QMARK node for the rsltBlk
    //-------------------------------------------------------------------------

    // Create the GT_COLON and mark the cond relop with the GTF_RELOP_QMARK

    GenTreePtr  colon  = gtNewOperNode(GT_COLON, rsltType, falseExpr, trueExpr);
    condExpr->gtFlags |= GTF_RELOP_QMARK;

    // Create the GT_QMARK, and push on the stack for rsltBlk

    result = gtNewOperNode(GT_QMARK, rsltType, condExpr, colon);

    if (initExpr != NULL)
    {
        var_types   initType  = lvaTable[initLclNum].TypeGet();
        GenTreePtr  initComma;

        initComma = gtNewOperNode(GT_COMMA,  TYP_VOID, 
                                  initExpr,  result);

        result    = gtNewOperNode(GT_COMMA,  initType, 
                                  initComma, gtNewLclvNode(initLclNum, initType));
    }

DONE:

    // Replace the condition in the original BBJ_COND block with a nop
    // and bash the block to unconditionally jump to rsltBlk.

    condStmt->gtStmt.gtStmtExpr = gtNewNothingNode();
    block->bbJumpKind           = BBJ_ALWAYS;
    block->bbJumpDest           = rsltBlk;

    // Discard the falseBlk and the trueBlk

    falseBlk->bbTreeList = NULL;
    trueBlk->bbTreeList  = NULL;

    impPushOnStack(result, typeInfo());

    impImportBlockPending(rsltBlk, false);

    /* We're done */

    return true;
}


/*****************************************************************************
 *
 *  Given a tree, clone it. *pClone is set to the cloned tree.
 *  Returns the orignial tree if the cloning was easy,
 *   else returns the temp to which the tree had to be spilled to.
 */

GenTreePtr          Compiler::impCloneExpr(GenTreePtr       tree,
                                           GenTreePtr *     pClone,
                                           CORINFO_CLASS_HANDLE     structHnd,
                                           unsigned         curLevel)
{
    GenTreePtr      clone = gtClone(tree, true);

    if (clone)
    {
        *pClone = clone;
        return tree;
    }

    /* Store the operand in a temp and return the temp */

    unsigned        temp = lvaGrabTemp();

    // impAssignTempGen() bashes tree->gtType to TYP_VOID for calls which
    // return TYP_STRUCT. So cache it
    var_types       type = genActualType(tree->TypeGet());

    impAssignTempGen(temp, tree, structHnd, curLevel);

    *pClone = gtNewLclvNode(temp, type);
    return    gtNewLclvNode(temp, type);
}

/*****************************************************************************
 * Remember the IL offset (including stack-empty info) for the trees we will
 * generate now.
 */

inline
void                Compiler::impCurStmtOffsSet(IL_OFFSET offs)
{
    assert(offs == BAD_IL_OFFSET || (offs & IL_OFFSETX_STKBIT) == 0);
    IL_OFFSETX stkBit = (verCurrentState.esStackDepth > 0) ? IL_OFFSETX_STKBIT : 0;
    impCurStmtOffs = offs | stkBit;
}

/*****************************************************************************
 *
 *  Remember the instr offset for the statements
 *
 *  When we do impAppendTree(tree), we cant set tree->gtStmtLastILoffs to
 *  impCurOpcOffs, if the append was done because of a partial stack spill,
 *  as some of the trees corresponding to code upto impCurOpcOffs might
 *  still be sitting on the stack.
 *  So we delay marking of gtStmtLastILoffs until impNoteLastILoffs().
 *  This should be called when an opcode finally/expilicitly causes
 *  impAppendTree(tree) to be called (as opposed to being called because of
 *  a spill caused by the opcode)
 */

#ifdef DEBUG

void                Compiler::impNoteLastILoffs()
{
    if (impLastILoffsStmt == NULL)
    {
        // We should have added a statement for the current basic block
        // Is this assert correct ?

        assert(impTreeLast);
        assert(impTreeLast->gtOper == GT_STMT);

        impTreeLast->gtStmt.gtStmtLastILoffs = impCurOpcOffs;
    }
    else
    {
        impLastILoffsStmt->gtStmt.gtStmtLastILoffs = impCurOpcOffs;
        impLastILoffsStmt = NULL;
    }
}

#endif


/*****************************************************************************
We dont create any GenTree (excluding spills) for a branch.
For debugging info, we need a placeholder so that we can note
the IL offset in gtStmt.gtStmtOffs. So append an empty statement.
*/

void                Compiler::impNoteBranchOffs()
{
    if (opts.compDbgCode)
        impAppendTree(gtNewNothingNode(), CHECK_SPILL_NONE, impCurStmtOffs);
}

/*****************************************************************************
 * Locate the next stmt boundary for which we need to record info.
 * We will have to spill the stack at such boundaries if it is not
 * already empty.
 * Returns the next stmt boundary (after the start of the block)
 */

unsigned            Compiler::impInitBlockLineInfo()
{
    /* Assume the block does not correspond with any IL offset. This prevents
       us from reporting extra offsets. Extra mappings can cause confusing
       stepping, especially if the extra mapping is a jump-target, and the 
       debugger does not ignore extra mappings, but instead rewinds to the
       nearest known offset */

    impCurStmtOffsSet(BAD_IL_OFFSET);

    IL_OFFSET       blockOffs = compCurBB->bbCodeOffs;

    if ((verCurrentState.esStackDepth == 0) &&
        (info.compStmtOffsetsImplicit & STACK_EMPTY_BOUNDARIES))
    {
        impCurStmtOffsSet(blockOffs);
    }

    /* @TODO [REVISIT] [04/16/01] []: If the last opcode of the previous block was a call, 
       we need to report the start of the current block. However we dont know
       what the previous opcode was.
       Note: This is not too bad as it is unlikely that such a construct
       occurs often.
    */
    if (false && (info.compStmtOffsetsImplicit & CALL_SITE_BOUNDARIES))
    {
        impCurStmtOffsSet(blockOffs);
    }

    /* Always report IL offset 0 or some tests get confused.
       Probably a good idea anyways */

    if (blockOffs == 0)
    {
        impCurStmtOffsSet(blockOffs);
    }

    if  (!info.compStmtOffsetsCount)
        return ~0;

    /* Find the lowest explicit stmt boundary within the block */

    /* Start looking at an entry that is based on our instr offset */

    unsigned    index = (info.compStmtOffsetsCount * blockOffs) / info.compCodeSize;

    if  (index >= info.compStmtOffsetsCount)
         index  = info.compStmtOffsetsCount - 1;

    /* If we've guessed too far, back up */

    while (index > 0 &&
           info.compStmtOffsets[index - 1] >= blockOffs)
    {
        index--;
    }

    /* If we guessed short, advance ahead */

    while (info.compStmtOffsets[index] < blockOffs)
    {
        index++;

        if (index == info.compStmtOffsetsCount)
            return info.compStmtOffsetsCount;
    }

    assert(index < info.compStmtOffsetsCount);

    if (info.compStmtOffsets[index] == blockOffs)
    {
        /* There is an explicit boundary for the start of this basic block.
           So we will start with bbCodeOffs. Else we will wait until we
           get to the next explicit boundary */

        impCurStmtOffsSet(blockOffs);

        index++;
    }

    return index;
}

/*****************************************************************************
 *
 *  Check for the special case where the object is the constant 0.
 *  As we can't even fold the tree (null+fldOffs), we are left with
 *  op1 and op2 both being a constant. This causes lots of problems.
 *  We simply grab a temp and assign 0 to it and use it in place of the NULL.
 */

inline
GenTreePtr          Compiler::impCheckForNullPointer(GenTreePtr obj)
{
    /* If it is not a GC type, we will be able to fold it.
       So dont need to do anything */

    if (!varTypeIsGC(obj->TypeGet()))
        return obj;

    if (obj->gtOper == GT_CNS_INT)
    {
        assert(obj->gtType == TYP_REF);
        assert (obj->gtIntCon.gtIconVal == 0);

        unsigned tmp = lvaGrabTemp();

        // We dont need to spill while appending as we are only assigning
        // NULL to a freshly-grabbed temp.

        impAssignTempGen (tmp, obj, CHECK_SPILL_NONE);

        obj = gtNewLclvNode (tmp, obj->gtType);
    }

    return obj;
}

/*****************************************************************************
 *
 *  Check for the special case where the object is the methods 'this' pointer
 */

inline
bool                Compiler::impIsThis(GenTreePtr obj)
{
    return ((obj                     != NULL)       &&
            (obj->gtOper             == GT_LCL_VAR) &&
            (obj->gtLclVar.gtLclNum  == 0)          && 
            (info.compIsStatic       == false)      &&
            (lvaTable[0].lvArgWrite  == false)      &&
            (lvaTable[0].lvAddrTaken == false));
}

/*****************************************************************************/

static
bool        impOpcodeIsCall(OPCODE opcode)
{
    switch(opcode)
    {
        case CEE_CALLI:
        case CEE_CALLVIRT:
        case CEE_CALL:
        case CEE_JMP:
            return true;

        default:
            return false;
    }
}

// Review: Is it worth caching these values?
CORINFO_CLASS_HANDLE        Compiler::impGetRefAnyClass()
{
    static CORINFO_CLASS_HANDLE s_RefAnyClass;

    if  (s_RefAnyClass == (CORINFO_CLASS_HANDLE) 0)
    {
        s_RefAnyClass = eeGetBuiltinClass(CLASSID_TYPED_BYREF);
        assert(s_RefAnyClass != (CORINFO_CLASS_HANDLE) 0);
    }

    return s_RefAnyClass;
}
CORINFO_CLASS_HANDLE        Compiler::impGetTypeHandleClass()
{
    static CORINFO_CLASS_HANDLE s_TypeHandleClass;

    if  (s_TypeHandleClass == (CORINFO_CLASS_HANDLE) 0)
    {
        s_TypeHandleClass = eeGetBuiltinClass(CLASSID_TYPE_HANDLE);
        assert(s_TypeHandleClass != (CORINFO_CLASS_HANDLE) 0);
    }

    return s_TypeHandleClass;

}

CORINFO_CLASS_HANDLE        Compiler::impGetRuntimeArgumentHandle()
{
    static CORINFO_CLASS_HANDLE s_ArgIteratorClass;

    if  (s_ArgIteratorClass == (CORINFO_CLASS_HANDLE) 0)
    {
        s_ArgIteratorClass = eeGetBuiltinClass(CLASSID_ARGUMENT_HANDLE);
        assert(s_ArgIteratorClass != (CORINFO_CLASS_HANDLE) 0);
    }

    return s_ArgIteratorClass;
}

CORINFO_CLASS_HANDLE        Compiler::impGetStringClass()
{
    static CORINFO_CLASS_HANDLE s_StringClass;

    if  (s_StringClass == (CORINFO_CLASS_HANDLE) 0)
    {
        s_StringClass = eeGetBuiltinClass(CLASSID_STRING);
        assert(s_StringClass != (CORINFO_CLASS_HANDLE) 0);
    }

    return s_StringClass;

}

CORINFO_CLASS_HANDLE        Compiler::impGetObjectClass()
{
    static CORINFO_CLASS_HANDLE s_ObjectClass;

    if  (s_ObjectClass == (CORINFO_CLASS_HANDLE) 0)
    {
        s_ObjectClass = eeGetBuiltinClass(CLASSID_SYSTEM_OBJECT);
        assert(s_ObjectClass != (CORINFO_CLASS_HANDLE) 0);
    }

    return s_ObjectClass;
}

/*****************************************************************************
 * CEE_CPOBJ can be treated either as a cpblk or a cpobj depending on
 * whether the ValueClass has any GC pointers and if the target is on the
 * GC heap. If there are no GC fields it will be treated like a CEE_CPBLK. 
 * Else we need to use a jit-helper for the GC info.
 * Both cases are represented by GT_COPYBLK, and op2 stores
 * either the size (cpblk) or the class-handle (cpobj)
 */

GenTreePtr              Compiler::impGetCpobjHandle(CORINFO_CLASS_HANDLE    structHnd)
{
    unsigned    size;
    unsigned    slots, i;
    BYTE *      gcPtrs;
    bool        hasGCfield = false;

    /* Get the GC fields info */

    size   = eeGetClassSize(structHnd);

    if (size < sizeof(void*))
        goto CPBLK;

    slots  = roundUp(size, sizeof(void*)) / sizeof(void*);
    gcPtrs = (BYTE*) compGetMemArrayA(slots, sizeof(BYTE));

    // TODO NOW 7/24/01  replace for loop below with 
    // if (eeGetClassGClayout(structHnd, gcPtrs) > 0)
    //        hasGCfield = true;

    eeGetClassGClayout(structHnd, gcPtrs);
    for (i = 0; i < slots; i++)
    {
        if (gcPtrs[i] != TYPE_GC_NONE)
        {
            hasGCfield = true;
            break;
        }
    }

    GenTreePtr handle;
    if (hasGCfield)
    {
        /* This will treated as a cpobj as we need to note GC info.
           Store the class handle and mark the node */

        handle = gtNewIconHandleNode((long)structHnd, GTF_ICON_CLASS_HDL);
    }
    else
    {
CPBLK:

        /* Doesnt need GC info. Treat operation as a cpblk */

        handle = gtNewIconNode(size);
    }

    return handle;
}

/*****************************************************************************
 *  "&var" can be used either as TYP_BYREF or TYP_I_IMPL, but we
 *  set its type to TYP_BYREF when we create it. We know if it can be
 *  whacked to TYP_I_IMPL only at the point where we use it
 */

/* static */
void        Compiler::impBashVarAddrsToI(GenTreePtr  tree1,
                                         GenTreePtr  tree2)
{
    if (         tree1->IsVarAddr())
        tree1->gtType = TYP_I_IMPL;

    if (tree2 && tree2->IsVarAddr())
        tree2->gtType = TYP_I_IMPL;
}


/*****************************************************************************/
static
Compiler::fgWalkResult  impChkNoLocAlloc(GenTreePtr tree,  void * data)
{
    if (tree->gtOper == GT_LCLHEAP)
        return Compiler::WALK_ABORT;

    return Compiler::WALK_CONTINUE;
}

/*****************************************************************************/
BOOL            Compiler::impLocAllocOnStack()
{
    if (!compLocallocUsed)
        return(FALSE);

    // Since PINVOKE pushes stuff on the stack, we can't do it if it is
    // in any of the args on the stack
    for (unsigned i=0; i < verCurrentState.esStackDepth; i++)
    {
        if (fgWalkTreePre(verCurrentState.esStack[i].val, impChkNoLocAlloc, 0, false))
            return(TRUE);
    }
    return(FALSE);
}

/*****************************************************************************/

GenTreePtr      Compiler::impIntrinsic(CORINFO_CLASS_HANDLE     clsHnd,
                                       CORINFO_METHOD_HANDLE    method,
                                       CORINFO_SIG_INFO *       sig,
                                       int                      memberRef)
{
    CorInfoIntrinsics intrinsicID = info.compCompHnd->getIntrinsicID(method);

    if (intrinsicID >= CORINFO_INTRINSIC_Count)
        return NULL;

    // Currently we don't have CORINFO_INTRINSIC_Exp because it does not
    // seem to work properly for Infinity values, we don't do
    // CORINFO_INTRINSIC_Pow because it needs a Helper which we currently don't have

    var_types   callType = JITtype2varType(sig->retType);

    /* First do the intrinsics which are always smaller than a call */

    switch(intrinsicID)
    {
        GenTreePtr  op1, op2;

    case CORINFO_INTRINSIC_Array_GetDimLength:

        /* For generic System.Array::GetLength(dim), its more work to get the
           dimension size, as its locations depends on whether the array is
           CORINFO_Array or CORINFO_RefArray */
   CORINFO_CLASS_HANDLE elemClsHnd;
        CorInfoType     corType; corType = info.compCompHnd->getChildType(clsHnd, &elemClsHnd);
        if (corType == CORINFO_TYPE_UNDEF)
            return NULL;

        var_types       elemType; elemType = JITtype2varType(corType);

        op2 = impPopStack().val; assert(op2->gtType == TYP_I_IMPL);
        op1 = impPopStack().val; assert(op1->gtType == TYP_REF);

        op2 = gtNewOperNode(GT_MUL, TYP_I_IMPL, op2, gtNewIconNode(sizeof(int)));
        op2 = gtNewOperNode(GT_ADD, TYP_I_IMPL, op2, gtNewIconNode(ARR_DIMCNT_OFFS(elemType)));

        op1 = gtNewOperNode(GT_ADD, TYP_BYREF, op1, op2);
        return gtNewOperNode(GT_IND, TYP_I_IMPL, op1);


    case CORINFO_INTRINSIC_Sin:
    case CORINFO_INTRINSIC_Cos:
    case CORINFO_INTRINSIC_Sqrt:
    case CORINFO_INTRINSIC_Abs:
    case CORINFO_INTRINSIC_Round:

        assert(callType != TYP_STRUCT);
        assert(sig->numArgs <= 2);

        op2 = (sig->numArgs > 1) ? impPopStack().val : NULL;
        op1 = (sig->numArgs > 0) ? impPopStack().val : NULL;

        op1 = gtNewOperNode(GT_MATH, genActualType(callType), op1, op2);
        op1->gtMath.gtMathFN = intrinsicID;
        return op1;

    case CORINFO_INTRINSIC_StringLength:
        op1 = impPopStack().val;
        if  (!opts.compMinOptim && !opts.compDbgCode)
        {
            op1 = gtNewOperNode(GT_ARR_LENGTH, TYP_INT, op1);
            op1->gtSetArrLenOffset(offsetof(CORINFO_String, stringLen));
        }
        else
        {
            /* Create the expression "*(str_addr + stringLengthOffset)" */
            op1 = gtNewOperNode(GT_ADD, TYP_BYREF, op1, gtNewIconNode(offsetof(CORINFO_String, stringLen), TYP_INT));
            op1 = gtNewOperNode(GT_IND, TYP_INT, op1);
        }
        return op1;
        
    case CORINFO_INTRINSIC_StringGetChar:
        op2 = impPopStack().val;
        op1 = impPopStack().val;
        op1 = gtNewRngChkNode(NULL, op1, op2, TYP_CHAR, sizeof(wchar_t), true);
        return op1;
    }

    /* If we are generating SMALL_CODE, we dont want to use intrinsics for
       the following, as it generates fatter code.
       @TODO [CONSIDER] [04/16/01] []: Use the weight of the BasicBlock and compCodeOpt to
       determine whether to use an instrinsic or not.
    */

    if (compCodeOpt() == SMALL_CODE)
        return NULL;

    /* These intrinsics generate fatter (but faster) code and are only
       done if we dont need SMALL_CODE */

    switch(intrinsicID)
    {
    default:
        /* Unknown intrinsic */
        return NULL;

        bool        loadElem;

    case CORINFO_INTRINSIC_Array_Get:
        loadElem = true;
        goto ARRAY_ACCESS;

    case CORINFO_INTRINSIC_Array_Set:
        loadElem = false;
        goto ARRAY_ACCESS;

    ARRAY_ACCESS:

        var_types           elemType;
        unsigned            rank;
        GenTreePtr          val;
        CORINFO_CLASS_HANDLE argClass;

        if (loadElem)
        {
            rank = sig->numArgs;
                // The rank 1 case is special because it has to handle to array formats
                // we will simply not do that case
            if (rank > GT_ARR_MAX_RANK || rank <= 1)
                return NULL;

            elemType    = callType;
        }
        else
        {
            rank = sig->numArgs - 1;
                // The rank 1 case is special because it has to handle to array formats
                // we will simply not do that case
            if (rank > GT_ARR_MAX_RANK || rank <= 1)
                return NULL;

            /* val->gtType may not be accurate, so look up the sig */

            CORINFO_ARG_LIST_HANDLE argLst = sig->args;
            for(unsigned r = 0; r < rank; r++)
                argLst = info.compCompHnd->getArgNext(argLst);
            
            elemType = JITtype2varType(strip(info.compCompHnd->getArgType(sig, argLst, &argClass)));

            // Assignment of a struct is more work, and there are more gets than sets.
            // disallow object pointers because we need to do a cast check which we dont do at present
            if (elemType == TYP_STRUCT)
                return NULL;

            // For the ref case, we will only be able to inline if the types match  
            // (verifier checks for this, we don't care for the nonverified case and the
            // type is final (so we don't need to do the cast)
            if (varTypeIsGC(elemType))
            {
                // Get the call site signature
                CORINFO_SIG_INFO LocalSig;                
                eeGetCallSiteSig(memberRef, info.compScopeHnd, &LocalSig);

                assert(LocalSig.hasThis());

                // Fetch the last argument, the one that indicates the type we are setting.
                CORINFO_ARG_LIST_HANDLE argType = LocalSig.args;                                
                for(unsigned r = 0; r < rank; r++)
                    argType = info.compCompHnd->getArgNext(argType);
                typeInfo argInfo = verParseArgSigToTypeInfo(&LocalSig, argType);

                // if it's not final, we can't do the optimization
                if (!(eeGetClassAttribs(argInfo.GetClassHandle()) & CORINFO_FLG_FINAL))
                {
                    return NULL;
                }                
            }

            val = impPopStack().val;
            assert(genActualType(elemType) == genActualType(val->gtType) ||
                   elemType == TYP_FLOAT && val->gtType == TYP_DOUBLE);
        }

        GenTreePtr  arrElem;
        arrElem = gtNewOperNode(GT_ARR_ELEM, TYP_BYREF);
        arrElem->gtFlags |= GTF_EXCEPT;

        arrElem->gtArrElem.gtArrRank    = rank;

        for(/**/; rank; rank--)
        {
            arrElem->gtArrElem.gtArrInds[rank - 1] = impPopStack().val;
        }

        arrElem->gtArrElem.gtArrObj         = impPopStack().val;
        assert(arrElem->gtArrElem.gtArrObj->gtType == TYP_REF);
        
        if (elemType == TYP_STRUCT)
        {
            assert(loadElem || !"sig->retTypeClass will be void for a Set");
            arrElem->gtArrElem.gtArrElemSize = eeGetClassSize(sig->retTypeClass);
        }
        else
            arrElem->gtArrElem.gtArrElemSize = genTypeSize(elemType);

        arrElem->gtArrElem.gtArrElemType = elemType;

        arrElem = gtNewOperNode(GT_IND, elemType, arrElem);

        if (loadElem)
            return arrElem;
        else
            return gtNewAssignNode(arrElem, val);
    }
}

/*****************************************************************************
 * Check that the current state is compatible with the initial state of the basic block pBB
 * to determine if we need to rescan the BB for verification
 * Input is the initial state of a BB. Returns FALSE if
 * 1) a locvar is live in the initial state, but not in the current state
 * 2) the type of an element on the stack is narrowed in the current state
 */
BOOL    Compiler::verEntryStateMatches(BasicBlock* block)
{

    // Check if all live vars in the previous state are live in the current state.
    if (verNumBytesLocVarLiveness > 0)
    {
        BYTE* locVarLivenessBitmap = block->bbLocVarLivenessBitmapOnEntry();

        for (unsigned i = 0; i < verNumBytesLocVarLiveness; i++)
        {
            // It is OK if more are live in the current state.
            if ((verCurrentState.esLocVarLiveness[i] & locVarLivenessBitmap[i]) 
                       != locVarLivenessBitmap[i])
                return FALSE;
        }
    }

    // if valuetype constructor, check init state of valuetype fields

    if (verNumBytesValuetypeFieldInitialized > 0)
    {
        BYTE* valuetypeFieldBitmap = block->bbValuetypeFieldBitmapOnEntry();

        for (unsigned i = 0; i < verNumBytesValuetypeFieldInitialized; i++)
        {
            if ((verCurrentState.esValuetypeFieldInitialized[i] & valuetypeFieldBitmap[i]) 
                != valuetypeFieldBitmap[i])
                return FALSE;
        }
    }

    // Check that the stacks match
    if (block->bbStackDepthOnEntry() != verCurrentState.esStackDepth)
        return FALSE;

    if (verCurrentState.esStackDepth > 0)
    {
        StackEntry* parentStack =  block->bbStackOnEntry();
        StackEntry* childStack  =  verCurrentState.esStack;

        for (unsigned i = 0; 
             i < verCurrentState.esStackDepth ; 
             i++, parentStack++, childStack++)
        {
            // Check that the state's stack element is the same as or a subclass of the basic block's
            // entrypoint state
            if (tiCompatibleWith(childStack->seTypeInfo, parentStack->seTypeInfo) == FALSE)
                return FALSE;

        }
    }

    // Verify that the initialisation status of the 'this' pointer is compatible
    if (block->bbThisOnEntry() && !(verCurrentState.thisInitialized))
        return FALSE;


    return TRUE;
}

/*****************************************************************************
 * Merge the current state onto the EntryState of the provided basic block (which must
 * already exist).
 * Return FALSE if the states cannot be merged
 * ignore stack if this is the start of an exception handler block
 */
BOOL    Compiler::verMergeEntryStates(BasicBlock* block)
{
    unsigned i;

    // do some basic checks first
    if (block->bbStackDepthOnEntry() != verCurrentState.esStackDepth)
        return FALSE;

    // The following may not be needed since we do autoinit. Uncomment if 
    // we decide to track these
/*    
// merge loc var liveness info

    BYTE* locVarLivenessBitmap = block->bbLocVarLivenessBitmapOnEntry();

    for (i = 0; i < verNumBytesLocVarLiveness; i++)
    {
        locVarLivenessBitmap[i] &= verCurrentState.esLocVarLiveness[i];
    }

    // if valuetype constructor merge init state of valuetype fields
    BYTE* valuetypeFieldBitmap = block->bbValuetypeFieldBitmapOnEntry();
  
    for (i = 0; i < verNumBytesValuetypeFieldInitialized; i++)
    {
        valuetypeFieldBitmap[i] &= verCurrentState.esValuetypeFieldInitialized[i]; 
    }

*/
    if (verCurrentState.esStackDepth > 0)
    {
        // merge stack types
        StackEntry* parentStack = block->bbStackOnEntry();
        StackEntry* childStack  = verCurrentState.esStack;
 
        for (i = 0; 
             i < verCurrentState.esStackDepth ; 
             i++, parentStack++, childStack++)
        {
            if (tiMergeToCommonParent(&parentStack->seTypeInfo,
                                      &childStack->seTypeInfo) 
                == FALSE)
            {
                return FALSE;
            }

        }
    }

    // merge initialization status of this ptr
    if (!verCurrentState.thisInitialized)
        verSetThisInit(block, FALSE);

    return TRUE;
}

/*****************************************************************************
 * 'logMsg' is true if a log message needs to be logged. false if the caller has
 *   already logged it (presumably in a more detailed fashion than done here)
 */

void    Compiler::verConvertBBToThrowVerificationException(BasicBlock* block DEBUGARG(bool logMsg))
{
    block->bbJumpKind = BBJ_THROW;
    block->bbFlags |= BBF_FAILED_VERIFICATION;

    impCurStmtOffsSet(block->bbCodeOffs);

#ifdef DEBUG
    // we need this since BeginTreeList asserts otherwise
    impTreeList = impTreeLast = NULL;
    block->bbFlags &= ~BBF_IMPORTED;

    if (logMsg)
    {
        JITLOG((LL_ERROR, "Verification failure: while compiling %s near IL offset %x..%xh \n", 
                        info.compFullName, block->bbCodeOffs, block->bbCodeOffs+block->bbCodeSize));
        if (verbose)
            printf("\n\nVerification failure: %s near IL %xh \n", info.compFullName, block->bbCodeOffs);
    }
#endif

    impBeginTreeList();

    // if the stack is non-empty evaluate all the side-effects
    if (verCurrentState.esStackDepth > 0)
        impEvalSideEffects();
    assert(verCurrentState.esStackDepth == 0);

    GenTreePtr op1 = gtNewHelperCallNode(CORINFO_HELP_VERIFICATION,
                                          TYP_VOID,
                                          0,
                                          gtNewArgList(gtNewIconNode(block->bbCodeOffs)));
    //verCurrentState.esStackDepth = 0;
    impAppendTree(op1, CHECK_SPILL_NONE, impCurStmtOffs);

    // The inliner is not able to handle methods that require throw block, so
    // make sure this methods never gets inlined.
    eeSetMethodAttribs(info.compMethodHnd, CORINFO_FLG_DONT_INLINE);
}


/*****************************************************************************
 * 
 */
void    Compiler::verHandleVerificationFailure(BasicBlock* block DEBUGARG(bool logMsg))

{
    verResetCurrentState(block,&verCurrentState);

    verConvertBBToThrowVerificationException(block DEBUGARG(logMsg));
            
#ifdef DEBUG
    impNoteLastILoffs();    // Remember at which BC offset the tree was finished
#endif
}

/******************************************************************************/
typeInfo        Compiler::verMakeTypeInfo(CorInfoType ciType, CORINFO_CLASS_HANDLE clsHnd)
{
    assert(ciType < CORINFO_TYPE_COUNT);

    typeInfo tiResult;
    switch(ciType)
    {
    case CORINFO_TYPE_STRING:
    case CORINFO_TYPE_CLASS:
        tiResult = verMakeTypeInfo(clsHnd);
        if (!tiResult.IsType(TI_REF))   // type must be constant with element type
            return typeInfo();
        break;

    case CORINFO_TYPE_VALUECLASS:
    case CORINFO_TYPE_REFANY:
        tiResult = verMakeTypeInfo(clsHnd);
            // type must be constant with element type;
        if (!tiResult.IsValueClass())
            return typeInfo();
        break;

    case CORINFO_TYPE_PTR:              // for now, pointers are treated as and error
    case CORINFO_TYPE_VOID:
        return typeInfo(); 
        break;

    case CORINFO_TYPE_BYREF: {
        CORINFO_CLASS_HANDLE childClassHandle;
        CorInfoType childType = info.compCompHnd->getChildType(clsHnd, &childClassHandle);
        return ByRef(verMakeTypeInfo(childType, childClassHandle));
        } 
        break;
    default:
        assert(clsHnd != BAD_CLASS_HANDLE);
        if (clsHnd)       // If we have more precise information, use it
            return typeInfo(TI_STRUCT, clsHnd);
        else 
            return typeInfo(JITtype2tiType(ciType));
    }
    return tiResult;
}

/******************************************************************************/
typeInfo        Compiler::verMakeTypeInfo(CORINFO_CLASS_HANDLE clsHnd)
{
    if (clsHnd == NULL)
        return typeInfo();

    if (eeGetClassAttribs(clsHnd) & CORINFO_FLG_VALUECLASS) {
        CorInfoType t = info.compCompHnd->getTypeForPrimitiveValueClass(clsHnd);

            // Meta-data validation should insure that CORINF_TYPE_BYREF should
            // not occur here, so we may want to change this to an assert instead.
        if (t == CORINFO_TYPE_VOID || t == CORINFO_TYPE_BYREF)  
            return typeInfo();

        if (t != CORINFO_TYPE_UNDEF)
            return(typeInfo(JITtype2tiType(t)));
        else
            return(typeInfo(TI_STRUCT, clsHnd));
    }
    else
        return(typeInfo(TI_REF, clsHnd));
    }

/******************************************************************************/
BOOL Compiler::verIsSDArray(typeInfo ti)
    {
    if (ti.IsNullObjRef())      // nulls are SD arrays
        return TRUE;

    if (!ti.IsType(TI_REF))
        return FALSE;

    if (!info.compCompHnd->isSDArray(ti.GetClassHandleForObjRef()))
        return FALSE;
    return TRUE;
}

/******************************************************************************/
/* given 'ti' which is an array type, fetch the element type.  Returns
   an error type if anything goes wrong */

typeInfo Compiler::verGetArrayElemType(typeInfo ti)
{
    assert(!ti.IsNullObjRef());     // you need to check for null explictly since that is a success case

    if (!verIsSDArray(ti))
        return typeInfo();

    CORINFO_CLASS_HANDLE childClassHandle = NULL;
    CorInfoType ciType = info.compCompHnd->getChildType(ti.GetClassHandleForObjRef(), &childClassHandle);

    return verMakeTypeInfo(ciType, childClassHandle);
}

/*****************************************************************************
 */
typeInfo Compiler::verParseArgSigToTypeInfo(CORINFO_SIG_INFO* sig,
                                            CORINFO_ARG_LIST_HANDLE    args)

{
    CORINFO_CLASS_HANDLE classHandle;
    CorInfoType ciType = strip(info.compCompHnd->getArgType(sig, args, &classHandle));

    var_types type =  JITtype2varType(ciType);
    if (varTypeIsGC(type))
    {
            // For efficiency, getArgType only returns something in classHandle for
            // value types.  For other types that have addition type info, you 
            // have to call back explicitly
        classHandle = eeGetArgClass(args, sig);
        assert(classHandle);
    }
    
    return verMakeTypeInfo(ciType, classHandle);
}   

/*****************************************************************************/

// This does the expensive check to figure out whether the method 
// needs to be verified. It is called only when we fail verification, 
// just before throwing the verification exception.

BOOL Compiler::verNeedsVerification()
{
    tiVerificationNeeded = !info.compCompHnd->canSkipVerification(info.compScopeHnd, FALSE);
    return tiVerificationNeeded;
}

BOOL Compiler::verIsByRefLike(const typeInfo& ti)
{                         
    if (ti.IsByRef())
        return TRUE;
    if (!ti.IsType(TI_STRUCT))
        return FALSE;
    return eeGetClassAttribs(ti.GetClassHandleForValueClass()) & CORINFO_FLG_CONTAINS_STACK_PTR;
}

/*****************************************************************************
 *
 *  Checks the IL verification rules for the call
 */

void           Compiler::verVerifyCall (OPCODE                  opcode,
                                        int                     memberRef,
                                        bool                    tailCall,
                                        const BYTE*             delegateCreateStart,
                                        const BYTE*             codeAddr
                                        DEBUGARG(const char *   methodName))
{
    CORINFO_METHOD_HANDLE   methodHnd;
    DWORD                   mflags;
    CORINFO_SIG_INFO        sig;
    unsigned int            popCount = 0;       // we can't pop the stack since impImportCall needs it, so 
                                                // this counter is used to keep track of how many items have been
                                                // virtually popped


    // for calli, VerifyOrReturn that this is not a virtual method
    if (opcode == CEE_CALLI)
    {
        Verify(false, "Calli not verifiable right now");
        return;

            // @TODO [REVISIT] [04/16/01] [vancem]: review and enable 
            // Please fix the problem with void return type comparision before turning this on -vancem 
#if 0
        typeInfo tiMethPtr = impStackTop(0).seTypeInfo;
        popCount++;

        VerifyOrReturn(tiMethPtr.IsMethod(), "expected method");

        methodHnd = tiMethPtr.GetMethod();
        mflags = eeGetMethodAttribs(methodHnd);

        VerifyOrReturn((mflags & CORINFO_FLG_VIRTUAL) == 0, "calli on virtual method");

        eeGetSig(memberRef, info.compScopeHnd, &sig, false);

        unsigned int argCount  = sig.numArgs;
        CORINFO_SIG_INFO actualSig;

        eeGetMethodSig(methodHnd, &actualSig, !fgIsInlining());

        // Check calling convention
        VerifyOrReturn(sig.hasThis()==actualSig.hasThis(), "conv mismatch");

        // Check need of THIS pointer
        VerifyOrReturn(sig.getCallConv()==actualSig.getCallConv(), "this mismatch");

        VerifyOrReturn(argCount == actualSig.numArgs, "arg count");

        // @TODO [REVISIT] [04/16/01] []: we can actually relax this a bit, we don't need perfect matches.  
        typeInfo tiDeclaredRet = verMakeTypeInfo(sig.retType, sig.retTypeClass);
        typeInfo tiActualRet   = verMakeTypeInfo(actualSig.retType, actualSig.retTypeClass);
        VerifyOrReturn(tiDeclaredRet == tiActualRet, "ret mismatch");
        
        CORINFO_ARG_LIST_HANDLE declaredArgs = sig.args; 
        CORINFO_ARG_LIST_HANDLE actualArgs = actualSig.args; 
        for (unsigned i=0; i< argCount; i++)
        {
            typeInfo tiDeclared = verParseArgSigToTypeInfo(&sig, declaredArgs);
            typeInfo tiActual = verParseArgSigToTypeInfo(&actualSig, actualArgs);

            VerifyOrReturn(tiDeclared == tiActual, "arg mismatch");
        }
#endif
    }
    else
    {
        methodHnd = eeFindMethod(memberRef, info.compScopeHnd, info.compMethodHnd);
        mflags = eeGetMethodAttribs(methodHnd);
        eeGetMethodSig(methodHnd, &sig, false);
        if ((sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_VARARG)
            eeGetCallSiteSig(memberRef, info.compScopeHnd, &sig, !fgIsInlining());
    }

    assert (methodHnd);
    CORINFO_CLASS_HANDLE methodClassHnd = eeGetMethodClass(methodHnd);
    assert(methodClassHnd);

    // opcode specific check
    unsigned methodClassFlgs = eeGetClassAttribs(methodClassHnd);
    switch(opcode)
    {
    case CEE_CALLVIRT:
        // cannot do callvirt on valuetypes
        VerifyOrReturn(!(eeGetClassAttribs(methodClassHnd) & CORINFO_FLG_VALUECLASS), "callVirt on value class");
        VerifyOrReturn(sig.hasThis(), "CallVirt on static method");
        break;

    case CEE_NEWOBJ: {
        assert(!tailCall);      // Importer should not allow this
        VerifyOrReturn((mflags & CORINFO_FLG_CONSTRUCTOR) && !(mflags & CORINFO_FLG_STATIC), "newobj must be on instance");

        if (methodClassFlgs & CORINFO_FLG_DELEGATE) 
        {
            VerifyOrReturn(sig.numArgs == 2, "wrong number args to delegate ctor");
            typeInfo tiDeclaredObj = verParseArgSigToTypeInfo(&sig, sig.args).NormaliseForStack();
            typeInfo tiDeclaredFtn = verParseArgSigToTypeInfo(&sig, info.compCompHnd->getArgNext(sig.args)).NormaliseForStack();
        
            VerifyOrReturn(tiDeclaredFtn.IsType(TI_INT), "ftn arg needs to be a type IntPtr");
        
            assert(popCount == 0);
            typeInfo tiActualObj = impStackTop(1).seTypeInfo;
            typeInfo tiActualFtn = impStackTop(0).seTypeInfo;

            VerifyOrReturn(tiActualFtn.IsMethod(), "delete needs method as first arg");
            VerifyOrReturn(tiCompatibleWith(tiActualObj, tiDeclaredObj), "delegate object type mismatch");

            // the method signature must be compatible with the delegate's invoke method
            VerifyOrReturn((tiActualObj.IsNullObjRef() || tiActualObj.IsType(TI_REF)) && 
                info.compCompHnd->isCompatibleDelegate(
                  tiActualObj.IsNullObjRef() ? NULL : tiActualObj.GetClassHandleForObjRef(),
                  tiActualFtn.GetMethod(), 
                  methodHnd),
                "function incompatible with delegate");

            // in the case of protected methods, it is a requirement that the 'this' pointer
            // be a subclass of the current context.  Perform this check
            CORINFO_CLASS_HANDLE instanceClassHnd = info.compClassHnd;
            if (!(tiActualObj.IsNullObjRef() || (eeGetMethodAttribs(tiActualFtn.GetMethod()) & CORINFO_FLG_STATIC)))
                instanceClassHnd = tiActualObj.GetClassHandleForObjRef();
            VerifyOrReturn(info.compCompHnd->canAccessMethod(info.compMethodHnd,
                                                     tiActualFtn.GetMethod(),
                                                     instanceClassHnd),
                           "can't access method");

            // check that for virtual functions, the type of the object used to get the
            // ftn ptr is the same as the type of the object passed to the delegate ctor.
            // since this is a bit of work to determine in general, we pattern match stylized
            // code sequences
            VerifyOrReturn(verCheckDelegateCreation(delegateCreateStart, codeAddr), "must create delegates with certain IL");
            goto DONE_ARGS;
        }
    }
        // fall thru to default checks
    default:
        VerifyOrReturn(!(mflags & CORINFO_FLG_ABSTRACT),  "method abstract");
    }
    Verify(!((mflags & CORINFO_FLG_CONSTRUCTOR) && (methodClassFlgs & CORINFO_FLG_DELEGATE)), "can only newobj a delegate constructor");

        // check compatibility of the arguments
    unsigned int argCount  = sig.numArgs;
    CORINFO_ARG_LIST_HANDLE args = sig.args;
    while (argCount--)
        {
        typeInfo tiActual = impStackTop(popCount+argCount).seTypeInfo;
        typeInfo tiDeclared = verParseArgSigToTypeInfo(&sig, args).NormaliseForStack();
            VerifyOrReturn(tiCompatibleWith(tiActual, tiDeclared), "type mismatch");

        // check that the argument is not a byref for tailcalls
        if (tailCall) 
            VerifyOrReturn(!verIsByRefLike(tiDeclared), "tailcall on byrefs");  
        
        args = info.compCompHnd->getArgNext(args);
    }

   
DONE_ARGS:

    // update popCount
    popCount += sig.numArgs;

    // check for 'this' which are on non-static methods, not called via NEWOBJ
    CORINFO_CLASS_HANDLE instanceClassHnd = info.compClassHnd; 
    if (!(mflags & CORINFO_FLG_STATIC) && (opcode != CEE_NEWOBJ))
    {
        typeInfo tiThis = impStackTop(popCount).seTypeInfo;
        popCount++;
        
        // This is a bit ugly.  Method on arrays, don't know their precise type
        // (like Foo[]), but only some approximation (like Object[]).  We have to
        // go back to the meta data token to look up the fully precise type
        if (eeGetClassAttribs(methodClassHnd) & CORINFO_FLG_ARRAY)
            methodClassHnd = info.compCompHnd->findMethodClass(info.compScopeHnd, memberRef);
        
        // If it is null, we assume we can access it (since it will AV shortly)
        // If it is anything but a refernce class, there is no hierarchy, so
        // again, we don't need the precise instance class to compute 'protected' access
        if (tiThis.IsType(TI_REF))
            instanceClassHnd = tiThis.GetClassHandleForObjRef();
        
        // Check type compatability of the this argument
        typeInfo tiDeclaredThis = verMakeTypeInfo(methodClassHnd);
        if (tiDeclaredThis.IsValueClass())
            tiDeclaredThis.MakeByRef();

        // If this is a call to the base class .ctor, set thisPtr Init for 
        // this block.
        if (mflags & CORINFO_FLG_CONSTRUCTOR)
        {
            if (verTrackObjCtorInitState && tiThis.IsThisPtr() && verIsCallToInitThisPtr(info.compClassHnd, methodClassHnd))
            {
                verCurrentState.thisInitialized = TRUE;
                tiThis.SetInitialisedObjRef();
            }
            else
            {
                // We allow direct calls to value type constructors
                VerifyOrReturn(tiThis.IsByRef(), "Bad call to a constructor");
            }
        }

        VerifyOrReturn(tiCompatibleWith(tiThis, tiDeclaredThis), "this type mismatch");
        
        // also check the specil tailcall rule
        VerifyOrReturn(!(tailCall && verIsByRefLike(tiDeclaredThis)), "byref in tailcall");

    }

    // check access permission
    VerifyOrReturn(info.compCompHnd->canAccessMethod(info.compMethodHnd,
                                                     methodHnd,
                                                     instanceClassHnd), "can't access method");
                                                       
    // special checks for tailcalls
    if (tailCall)
    {
        typeInfo tiCalleeRetType = verMakeTypeInfo(sig.retType, sig.retTypeClass);
        typeInfo tiCallerRetType = verMakeTypeInfo(info.compMethodInfo->args.retType, info.compMethodInfo->args.retTypeClass);
        
            // void return type gets morphed into the error type, so we have to treat them specially here 
        if (sig.retType == CORINFO_TYPE_VOID)
            Verify(info.compMethodInfo->args.retType == CORINFO_TYPE_VOID, "tailcall return mismatch");
        else 
            Verify(tiCompatibleWith(NormaliseForStack(tiCalleeRetType), NormaliseForStack(tiCallerRetType)),  "tailcall return mismatch");

        // for tailcall, stack must be empty
        VerifyOrReturn(verCurrentState.esStackDepth == popCount, "stack non-empty on tailcall");
    }
}


/*****************************************************************************
 *  Checks that a delegate creation is done using the following pattern:
 *     dup
 *     ldvirtftn
 *
 *  OR
 *     ldftn
 *
 * 'delegateCreateStart' points at the last dup or ldftn in this basic block (null if
 *  not in this basic block) 
 */

BOOL Compiler::verCheckDelegateCreation(const BYTE* delegateCreateStart, const BYTE* codeAddr)
{

    if (codeAddr - delegateCreateStart == 6)        // LDFTN <TOK> takes 6 bytes
    {
        if (delegateCreateStart[0] == CEE_PREFIX1 && delegateCreateStart[1] == (CEE_LDFTN & 0xFF))
            return TRUE;
    }
    else if (codeAddr - delegateCreateStart == 7)       // DUP LDVIRTFTN <TOK> takes 7 bytes
    {
        if (delegateCreateStart[0] == CEE_DUP  && 
            delegateCreateStart[1] == CEE_PREFIX1 &&
            delegateCreateStart[2] == (CEE_LDVIRTFTN & 0xFF))
            return TRUE;
    }

    return FALSE;
}

typeInfo Compiler::verVerifySTIND(const typeInfo& ptr, const typeInfo& value, var_types instrType)
{
    typeInfo ptrVal = verVerifyLDIND(ptr, instrType);
    Verify(tiCompatibleWith(value, ptrVal.NormaliseForStack()), "type mismatch");
    return ptrVal;
}

typeInfo Compiler::verVerifyLDIND(const typeInfo& ptr, var_types instrType)
    {

        // @TODO [CONSIDER] [4/25/01] 
        // 64BIT, this is not correct for LDIND.I on a 64 bit machine.
        // it will allow a I4* to be derererenced by LDIND.I
    typeInfo ptrVal;
    if (ptr.IsByRef())  
    {
        ptrVal = DereferenceByRef(ptr);
        if (instrType == TYP_REF)
            Verify(ptrVal.IsObjRef(), "bad pointer"); 
        else if (instrType == TYP_STRUCT)
            Verify(ptrVal.IsValueClass(), "presently only allowed on value classes");
        else 
            Verify(typeInfo(instrType) == ptrVal, "pointer not consistant with instr"); 
    }
    else
        Verify(false, "pointer not byref");
    
    return ptrVal;
    }


/* verify that the field is used properly.  'tiThis' is NULL for statics, 
   fieldflags is the fields attributes, and mutator is TRUE if it is a ld*flda or a st*fld */

void Compiler::verVerifyField(CORINFO_FIELD_HANDLE fldHnd, const typeInfo* tiThis, unsigned fieldFlags, BOOL mutator)
{
    CORINFO_CLASS_HANDLE enclosingClass = info.compCompHnd->getEnclosingClass(fldHnd);
    CORINFO_CLASS_HANDLE instanceClass = info.compClassHnd; // for statics, we imagine the instance is the same as the current

    bool isStaticField = ((fieldFlags & CORINFO_FLG_STATIC) != 0);
    if (mutator)  
    {
        Verify(!(fieldFlags & CORINFO_FLG_UNMANAGED), "mutating an RVA bases static");

        if ((fieldFlags & CORINFO_FLG_FINAL))
        {
            Verify((info.compFlags & CORINFO_FLG_CONSTRUCTOR) &&
                   enclosingClass == info.compClassHnd && info.compIsStatic == isStaticField,
                    "bad use of initonly field (set or address taken)");
        }
    }

    if (tiThis == 0)
    {
        Verify(isStaticField, "used static opcode with non-static field");
    }
    else
    {
        typeInfo tThis = *tiThis;
            // If it is null, we assume we can access it (since it will AV shortly)
            // If it is anything but a refernce class, there is no hierarchy, so
            // again, we don't need the precise instance class to compute 'protected' access
        if (tiThis->IsType(TI_REF))
            instanceClass = tiThis->GetClassHandleForObjRef();

            // Note that even if the field is static, we require that the this pointer
            // satisfy the same constraints as a non-static field  This happens to
            // be simpler and seems reasonable
        typeInfo tiDeclaredThis = verMakeTypeInfo(enclosingClass);
        if (tiDeclaredThis.IsValueClass())
            tiDeclaredThis.MakeByRef();
        else if (verTrackObjCtorInitState && tThis.IsThisPtr())
            tThis.SetInitialisedObjRef();

        Verify(tiCompatibleWith(tThis, tiDeclaredThis), "this type mismatch");
    }

    // Presently the JIT doe not check that we dont store or take the address of init-only fields
    // since we can not guarentee their immutability and it is not a security issue

    Verify(instanceClass && info.compCompHnd->canAccessField(info.compMethodHnd, fldHnd, instanceClass), "field access denied");
}

void Compiler::verVerifyCond(const typeInfo& tiOp1, const typeInfo& tiOp2, unsigned opcode)
{
    if (tiOp1.IsNumberType())
        Verify(tiOp1 == tiOp2, "Cond type mismatch");
    else if (tiOp1.IsObjRef()) 
    {
        switch(opcode) 
        {
            case CEE_BEQ_S:
            case CEE_BEQ:
            case CEE_BNE_UN_S:
            case CEE_BNE_UN:
            case CEE_CEQ:
            case CEE_CGT_UN:
                break;
            default:
                Verify(FALSE, "Cond not allowed on object types");
        }
        Verify(tiOp2.IsObjRef(), "Cond type mismatch");
    }
    else if (tiOp1.IsByRef())
        Verify(tiOp2.IsByRef(), "Cond type mismatch");
    else 
        Verify(tiOp1.IsMethod() && tiOp2.IsMethod(), "Cond type mismatch");
}

void Compiler::verVerifyThisPtrInitialised()
{
    if (verTrackObjCtorInitState)
        Verify(verCurrentState.thisInitialized, "this ptr is not initialized");
}

BOOL Compiler::verIsCallToInitThisPtr(CORINFO_CLASS_HANDLE context, 
                                           CORINFO_CLASS_HANDLE target)
{
    // Either target == context, in this case calling an alternate .ctor
    // Or target is the immediate parent of context

    return ((target == context) || 
            (target == info.compCompHnd->getParentType(context)));
}

/*****************************************************************************
 *
 *  Import the call instructions.
 *  For CEE_NEWOBJ, newobjThis should be the temp grabbed for the allocated
 *     uninitalized object.
 */

var_types           Compiler::impImportCall (OPCODE         opcode,
                                             int            memberRef,
                                             GenTreePtr     newobjThis,
                                             bool           tailCall)
{
    assert(opcode == CEE_CALL   || opcode == CEE_CALLVIRT ||
           opcode == CEE_NEWOBJ || opcode == CEE_CALLI);

    CORINFO_SIG_INFO        sig;
    var_types               callTyp     = TYP_COUNT;
    CORINFO_METHOD_HANDLE   methHnd     = NULL;
    CORINFO_CLASS_HANDLE    clsHnd      = NULL;
    unsigned                mflags      = 0;
    unsigned                clsFlags    = 0;
    unsigned                argFlags    = 0;
    GenTreePtr              call        = NULL;
    GenTreePtr              args        = NULL;

    // Synchronized methods need to call CORINFO_HELP_MON_EXIT at the end. We could
    // do that before tailcalls, but that is probably not the intended
    // semantic. So just disallow tailcalls from synchronized methods.
    // Also, popping arguments in a varargs function is more work and NYI
    bool            canTailCall = !(info.compFlags & CORINFO_FLG_SYNCH) &&
                                  !info.compIsVarArgs &&
                                  !compLocallocUsed;

#if HOIST_THIS_FLDS
    // Calls may modify any field of the "this" object
    optHoistTFRhasCall();
#endif

    /*-------------------------------------------------------------------------
     * First create the call node
     */

    if (opcode == CEE_CALLI)
    {
        /* Get the call sig */

        eeGetSig(memberRef, info.compScopeHnd, &sig);
        callTyp = JITtype2varType(sig.retType);

        /* The function pointer is on top of the stack - It may be a
         * complex expression. As it is evaluated after the args,
         * it may cause registered args to be spilled. Simply spill it.
         * @TODO [CONSIDER] [04/16/01] []:: Lock the register args, and then generate code for
         *                                     the function pointer.
         */

        if  (impStackTop().val->gtOper != GT_LCL_VAR) // ignore this trivial case. @TODO [REVISIT] [04/16/01] []: lvAddrTaken
            impSpillStackEntry(verCurrentState.esStackDepth - 1);

        /* Create the call node */

        call = gtNewCallNode(CT_INDIRECT, NULL, genActualType(callTyp), NULL);

        /* Get the function pointer */

        GenTreePtr fptr = impPopStack().val;
        assert(genActualType(fptr->gtType) == TYP_I_IMPL);

#ifdef DEBUG
        // This temporary must never be converted to a double in stress mode,
        // because that can introduce a call to the cast helper after the
        // arguments have already been evaluated.
        
        if (fptr->OperGet() == GT_LCL_VAR)
            lvaTable[fptr->gtLclVar.gtLclNum].lvKeepType = 1;
#endif

        call->gtCall.gtCallAddr = fptr;
        call->gtFlags |= GTF_EXCEPT | (fptr->gtFlags & GTF_GLOB_EFFECT);

        /* @TODO [FIXHACK] [04/16/01] []: The EE wants us to believe that these are calls to "unmanaged" */
        /* functions. Right now we are calling just a managed stub         */

#if INLINE_NDIRECT
        // ISSUE:
        // We have to disable pinvoke inlining inside of filters
        // because in case the main execution (i.e. in the try block) is inside
        // unmanaged code, we cannot reuse the inlined stub (we still need the
        // original state until we are in the catch handler)

        if  (!bbInFilterBlock(compCurBB) && // @TODO [REVISIT] [04/16/01] []: don't use pinvoke inlining in filters
             getInlinePInvokeEnabled() &&
            !opts.compDbgCode &&
             compCodeOpt() != SMALL_CODE &&
            ((sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_STDCALL ||
             (sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_C) &&
            !eeGetEEInfo()->noDirectTLS && !impLocAllocOnStack() &&
            callTyp != TYP_STRUCT &&
            !opts.compNoPInvokeInlineCB &&  // profiler is preventing inline pinvoke
            !eeNDMarshalingRequired(0, &sig))
        {
            JITLOG((LL_INFO1000000, "Inline a CALLI PINVOKE call from method %s\n",
                    info.compFullName));

            call->gtFlags |= GTF_CALL_UNMANAGED;
            info.compCallUnmanaged++;

            if ((sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_C)
                call->gtFlags |= GTF_CALL_POP_ARGS;

            canTailCall = false;
        }
        else
#endif  //INLINE_NDIRECT
        if  ((sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_STDCALL  ||
             (sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_C        ||
             (sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_THISCALL ||
             (sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_FASTCALL)

        {
            call->gtCall.gtCallCookie = eeGetPInvokeCookie(&sig);

            // @TODO [REVISIT] [04/16/01] []: We go through the PInvoke stub. Can it work with CORINFO_HELP_TAILCALL?
            canTailCall = false;
        }

        // We dont know the target method, so we have to infer the flags, or
        // assume the worst-case.
        mflags  = (sig.callConv & CORINFO_CALLCONV_HASTHIS) ? 0 : CORINFO_FLG_STATIC;
    }
    else // (opcode != CEE_CALLI)
    {
        methHnd = eeFindMethod(memberRef, info.compScopeHnd, info.compMethodHnd);
        eeGetMethodSig(methHnd, &sig);
        callTyp = JITtype2varType(sig.retType);

        mflags   = eeGetMethodAttribs(methHnd);
        clsHnd   = eeGetMethodClass(methHnd);
        clsFlags = clsHnd ? eeGetClassAttribs(clsHnd):0;

        if ((sig.callConv & CORINFO_CALLCONV_MASK) != CORINFO_CALLCONV_DEFAULT &&
            (sig.callConv & CORINFO_CALLCONV_MASK) != CORINFO_CALLCONV_VARARG)
            BADCODE("Bad calling convention");

        if (mflags & CORINFO_FLG_INTRINSIC)
        {
            call = impIntrinsic(clsHnd, methHnd, &sig, memberRef);

            if (call != NULL)
            {
                assert(!(mflags & CORINFO_FLG_VIRTUAL) ||
                       (mflags & CORINFO_FLG_FINAL) ||
                       (clsFlags & CORINFO_FLG_FINAL));
                goto DONE_CALL;
            }
        }

        /* For virtual methods added by EnC, they wont exist in the
           original vtable. So we call a helper funciton to do the
           lookup and the dispatch */

        if ((mflags & CORINFO_FLG_VIRTUAL) && (mflags & CORINFO_FLG_EnC) &&
            (opcode == CEE_CALLVIRT))
        {
            args = impPopList(sig.numArgs, &argFlags, &sig);

            /* Get the address of the target function by calling helper */

            GenTreePtr thisPtr = impPopStack().val; assert(thisPtr->gtType == TYP_REF);
            GenTreePtr thisPtrCopy;
            thisPtr = impCloneExpr(thisPtr, &thisPtrCopy, BAD_CLASS_HANDLE, CHECK_SPILL_ALL);

            GenTreePtr helpArgs = gtNewOperNode(GT_LIST, TYP_VOID,
                                                gtNewIconEmbMethHndNode(methHnd));

            helpArgs = gtNewOperNode(GT_LIST, TYP_VOID, thisPtr, helpArgs);
            thisPtr  = 0; // cant reuse it

            // Call helper function

            GenTreePtr fptr = gtNewHelperCallNode( CORINFO_HELP_EnC_RESOLVEVIRTUAL,
                                                   TYP_I_IMPL, GTF_EXCEPT, helpArgs);

            /* Now make an indirect call through the function pointer */

            unsigned    lclNum = lvaGrabTemp();
            impAssignTempGen(lclNum, fptr, CHECK_SPILL_ALL);
            fptr = gtNewLclvNode(lclNum, TYP_I_IMPL);

            // Create the acutal call node

            // @TODO [REVISIT] [04/16/01] []: Need to reverse args and all that. "this" needs
            // to be in reg but we dont set gtCallObjp
            assert((sig.callConv & CORINFO_CALLCONV_MASK) != CORINFO_CALLCONV_VARARG);

            args = gtNewOperNode(GT_LIST, TYP_VOID, thisPtrCopy, args);

            call = gtNewCallNode(CT_INDIRECT, 
                                 (CORINFO_METHOD_HANDLE)fptr,
                                 genActualType(callTyp), 
                                 args);

            goto DONE_CALL;
        }

        call = gtNewCallNode(CT_USER_FUNC, methHnd, genActualType(callTyp), NULL);

        if (mflags & CORINFO_FLG_NOGCCHECK)
            call->gtCall.gtCallMoreFlags |= GTF_CALL_M_NOGCCHECK;
    }

    /* Some sanity checks */

    // CALL_VIRT and NEWOBJ must have a THIS pointer
    assert(!(opcode == CEE_CALLVIRT && opcode == CEE_NEWOBJ) ||
           (sig.callConv & CORINFO_CALLCONV_HASTHIS));
    // static bit and hasThis are negations of one another
    assert(((mflags & CORINFO_FLG_STATIC)             != 0) ==
           ((sig.callConv & CORINFO_CALLCONV_HASTHIS) == 0));

    /*-------------------------------------------------------------------------
     * Set flags, check special-cases etc
     */

    /* Set the correct GTF_CALL_VIRT etc flags */

    if (opcode == CEE_CALLVIRT)
    {
        assert(!(mflags & CORINFO_FLG_STATIC));     // can't call a static method

        /* Cannot call virtual on a value class method */

        assert(!(clsFlags & CORINFO_FLG_VALUECLASS));

        /* Set the correct flags - virtual, interface, etc...
         * If the method is final or private mark it VIRT_RES
         * which indicates that we should check for a null "this" pointer */

        if (clsFlags & CORINFO_FLG_INTERFACE)
            call->gtFlags |= GTF_CALL_INTF | GTF_CALL_VIRT;
        else if (!(mflags & CORINFO_FLG_VIRTUAL) || (mflags & CORINFO_FLG_FINAL))
            call->gtFlags |= GTF_CALL_VIRT_RES;
        else
            call->gtFlags |= GTF_CALL_VIRT;
    }

    /* Special case - Check if it is a call to Delegate.Invoke(). */

    if (mflags & CORINFO_FLG_DELEGATE_INVOKE)
    {
        assert(!(mflags & CORINFO_FLG_STATIC));     // can't call a static method
        assert(mflags & CORINFO_FLG_FINAL);

        /* Set the delegate flag */
        call->gtCall.gtCallMoreFlags |= GTF_CALL_M_DELEGATE_INV;

        if (opcode == CEE_CALLVIRT)
        {
            assert(mflags & CORINFO_FLG_FINAL);

            /* It should have the GTF_CALL_VIRT_RES flag set. Reset it */
            assert(call->gtFlags & GTF_CALL_VIRT_RES);
            call->gtFlags &= ~GTF_CALL_VIRT_RES;
        }
    }

    /* Check for varargs */

    if  ((sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_VARARG)
    {
        /* Set the right flags */

        call->gtFlags |= GTF_CALL_POP_ARGS;

        /* Cant allow tailcall for varargs as it is caller-pop. The caller
           will be expecting to pop a certain number of arguments, but if we
           tailcall to a function with a different number of arguments, we
           are hosed. There are ways around this (caller remembers esp value,
           varargs is not caller-pop, etc), but not worth it.

           @TODO [CONSIDER] [04/16/01] []: Allow it if the caller and callee have the 
           exact same signature including CORINFO_CALLCONV_VARARG */

        canTailCall = false;

        /* Get the total number of arguments - this is already correct
         * for CALLI - for methods we have to get it from the call site */

        if  (opcode != CEE_CALLI)
        {
            unsigned    numArgsDef = sig.numArgs;
            eeGetCallSiteSig(memberRef, info.compScopeHnd, &sig);
            assert(numArgsDef <= sig.numArgs);
        }

        /* We will have "cookie" as the last argument but we cannot push
         * it on the operand stack because we may overflow, so we append it
         * to the arg list next after we pop them */
    }

    // If the current method calls a method which needs a security
    // check, we need to reserve a slot for the security object in
    // the current method's stack frame

    if (mflags & CORINFO_FLG_SECURITYCHECK)
       opts.compNeedSecurityCheck = true;

    //--------------------------- Inline NDirect ------------------------------

#if INLINE_NDIRECT
    // ISSUE:
    // We have to disable pinvoke inlining inside of filters
    // because in case the main execution (i.e. in the try block) is inside
    // unmanaged code, we cannot reuse the inlined stub (we still need the
    // original state until we are in the catch handler)

    if (!bbInFilterBlock(compCurBB) && // @TODO [REVISIT] [04/16/01] []: don't use pinvoke inlining in filters
        !opts.compDbgCode &&
        getInlinePInvokeEnabled() &&
        compCodeOpt() != SMALL_CODE &&
        (mflags & CORINFO_FLG_UNCHECKEDPINVOKE) && !eeGetEEInfo()->noDirectTLS
         && !impLocAllocOnStack()
         && !opts.compNoPInvokeInlineCB) // profiler is preventing inline pinvoke
    {
        JITLOG((LL_INFO1000000, "Inline a PINVOKE call to method %s from method %s\n",
                eeGetMethodFullName(methHnd), info.compFullName));

        CorInfoUnmanagedCallConv cType = eeGetUnmanagedCallConv(methHnd);

        if ((cType == CORINFO_UNMANAGED_CALLCONV_STDCALL || cType == CORINFO_UNMANAGED_CALLCONV_C) &&
            !eeNDMarshalingRequired(methHnd, &sig))
        {
            call->gtFlags |= GTF_CALL_UNMANAGED;
            info.compCallUnmanaged++;

            if (cType == CORINFO_UNMANAGED_CALLCONV_C)
                call->gtFlags |= GTF_CALL_POP_ARGS;

            // We set up the unmanaged call by linking the frame, disabling GC, etc
            // This needs to be cleaned up on return
            canTailCall = false;

#ifdef DEBUG
            if (verbose)
                printf(">>>>>>%s has unmanaged callee\n", info.compFullName);
#endif
        }
    }

    if (sig.numArgs && (call->gtFlags & GTF_CALL_UNMANAGED))
    {
        /* Since we push the arguments in reverse order (i.e. right -> left)
         * spill any side effects from the stack
         *
         * OBS: If there is only one side effect we do not need to spill it
         *      thus we have to spill all side-effects except last one
         */

        unsigned    lastLevel;
        bool        moreSideEff = false;

        for (unsigned level = verCurrentState.esStackDepth - sig.numArgs; level < verCurrentState.esStackDepth; level++)
        {
            if      (verCurrentState.esStack[level].val->gtFlags & GTF_OTHER_SIDEEFF)
            {
                assert(moreSideEff == false);

                impSpillStackEntry(level);
            }
            else if (verCurrentState.esStack[level].val->gtFlags & GTF_SIDE_EFFECT)
            {
                if  (moreSideEff)
                {
                    /* We had a previous side effect - must spill it */
                    impSpillStackEntry(lastLevel);

                    /* Record the level for the current side effect in case we will spill it */
                    lastLevel   = level;
                }
                else
                {
                    /* This is the first side effect encountered - record its level */

                    moreSideEff = true;
                    lastLevel   = level;
                }
            }
        }

        /* The argument list is now "clean" - no out-of-order side effects
         * Pop the argument list in reverse order */

        args = call->gtCall.gtCallArgs = impPopRevList(sig.numArgs, &argFlags, &sig);
        call->gtFlags |= args->gtFlags & GTF_GLOB_EFFECT;

        goto DONE;
    }

#endif // INLINE_NDIRECT

    /*-------------------------------------------------------------------------
     * Create the argument list
     */

    /* Special case - for varargs we have an implicit last argument */

    GenTreePtr      extraArg;

    extraArg = 0;

    if  ((sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_VARARG)
    {
        void *          varCookie, *pVarCookie;
        varCookie = info.compCompHnd->getVarArgsHandle(&sig, &pVarCookie);
        assert((!varCookie) != (!pVarCookie));
        GenTreePtr  cookie = gtNewIconEmbHndNode(varCookie, pVarCookie, GTF_ICON_VARG_HDL);

        extraArg = gtNewOperNode(GT_LIST, TYP_I_IMPL, cookie);
    }

    if (sig.callConv & CORINFO_CALLCONV_PARAMTYPE)
    {
        if (clsHnd == 0)
            NO_WAY("CALLI on parameterized type");

        // Parameterized type, add an extra argument which indicates the type parameter
        // This is pushed as the last argument
        void* pTypeParam;
        void* typeParam = info.compCompHnd->getInstantiationParam(
                                  info.compScopeHnd, memberRef, &pTypeParam);

        // @TODO [REVISIT] [04/16/01] []: post V1 the type parameter will not be a CLASS_HDL (it is pre-V1)
        extraArg = gtNewOperNode(GT_LIST, TYP_I_IMPL, 
                                 gtNewIconEmbHndNode(typeParam, 
                                                     pTypeParam, 
                                                     GTF_ICON_CLASS_HDL));
    }

    /* Now pop the arguments */

    args = call->gtCall.gtCallArgs = impPopList(sig.numArgs, &argFlags, &sig, extraArg);

    if (args)
        call->gtFlags |= args->gtFlags & GTF_GLOB_EFFECT;

    /* Are we supposed to have a 'this' pointer? */

    if (!(mflags & CORINFO_FLG_STATIC) || opcode == CEE_NEWOBJ)
    {
        GenTreePtr obj;

        if (opcode == CEE_NEWOBJ)
            obj = newobjThis;
        else
            obj = impPopStack().val;

        assert(varTypeIsGC(obj->gtType) ||      // "this" is a managed object
               (obj->TypeGet() == TYP_I_IMPL && // "this" is unmgd but the method's class doesnt care
                (clsFlags & CORINFO_FLG_VALUECLASS)));

        /* Is this a virtual or interface call? */

        if  (call->gtFlags & (GTF_CALL_VIRT | GTF_CALL_INTF | GTF_CALL_VIRT_RES))
        {
            /* only true object pointers can be virtual */

            assert(obj->gtType == TYP_REF);
        }

        /* Store the "this" value in the call */

        call->gtFlags          |= obj->gtFlags & GTF_GLOB_EFFECT;
        call->gtCall.gtCallObjp = obj;
    }

    if (opcode == CEE_NEWOBJ)
    {
        if (clsFlags & CORINFO_FLG_VAROBJSIZE)
        {
            assert(!(clsFlags & CORINFO_FLG_ARRAY));    // arrays handled separately
            // This is a 'new' of a variable sized object, wher
            // the constructor is to return the object.  In this case
            // the constructor claims to return VOID but we know it
            // actually returns the new object
            assert(callTyp == TYP_VOID);
            callTyp = TYP_REF;
            call->gtType = TYP_REF;
            impSpillSpecialSideEff();

            impPushOnStack(call, typeInfo(TI_REF, clsHnd));
        }
        else
        {
            // append the call node.
            impAppendTree(call, CHECK_SPILL_ALL, impCurStmtOffs);

            // Now push the value of the 'new onto the stack

            // This is a 'new' of a non-variable sized object.
            // append the new node (op1) to the statement list,
            // and then push the local holding the value of this
            // new instruction on the stack.

            if (clsFlags & CORINFO_FLG_VALUECLASS)
            {
                assert(newobjThis->gtOper == GT_ADDR &&
                       newobjThis->gtOp.gtOp1->gtOper == GT_LCL_VAR);

                unsigned tmp = newobjThis->gtOp.gtOp1->gtLclVar.gtLclNum;
                impPushOnStack(gtNewLclvNode(tmp, lvaGetRealType(tmp)), verMakeTypeInfo(clsHnd).NormaliseForStack());
            }
            else
            {
                assert(newobjThis->gtOper == GT_LCL_VAR);
                impPushOnStack(gtNewLclvNode(newobjThis->gtLclVar.gtLclNum, TYP_REF), typeInfo(TI_REF, clsHnd));
            }
        }
        return callTyp;
    }

DONE:

    if (tailCall)
    {
        if (verCurrentState.esStackDepth)
            BADCODE("Stack should be empty after tailcall");

        /* Note that we can not relax this condition with genActualType() as
           the calling convention dictates that the caller of a function with
           a small-typed return value is responsible for normalizing the return val */

        if (callTyp != info.compRetType)
            canTailCall = false;

//      assert(compCurBB is not a catch, finally or filter block);
//      assert(compCurBB is not a try block protected by a finally block);
        assert(compCurBB->bbJumpKind == BBJ_RETURN);

        /* Check for permission to tailcall */

        if (canTailCall)
        {
            CORINFO_METHOD_HANDLE calleeHnd = (opcode==CEE_CALL) ? methHnd : NULL;
            CORINFO_ACCESS_FLAGS  aflags    = CORINFO_ACCESS_ANY;
            GenTreePtr            thisArg   = call->gtCall.gtCallObjp;

            if (impIsThis(thisArg))
                aflags = CORINFO_ACCESS_THIS;

            if (info.compCompHnd->canTailCall(info.compMethodHnd, calleeHnd, aflags))
            {
                call->gtCall.gtCallMoreFlags |= GTF_CALL_M_CAN_TAILCALL;
            }
        }
    }

    /* If the call is of a small type, then we need to normalize its
       return value */

    if (varTypeIsIntegral(callTyp) && 
        /* callTyp != TYP_BOOL && @TODO [REVISIT] [04/16/01] [vancem]*/
        genTypeSize(callTyp) < genTypeSize(TYP_I_IMPL))
    {
        call = gtNewCastNode(genActualType(callTyp), call, callTyp);
    }

DONE_CALL:

    /* Push or append the result of the call */

    if  (callTyp == TYP_VOID)
    {
        impAppendTree(call, CHECK_SPILL_ALL, impCurStmtOffs);
    }
    else
    {
        impSpillSpecialSideEff();
        if (clsFlags & CORINFO_FLG_ARRAY)
        {
            eeGetCallSiteSig(memberRef, info.compScopeHnd, &sig);
        }
        
        typeInfo tiRetVal = verMakeTypeInfo(sig.retType, sig.retTypeClass);
        tiRetVal.NormaliseForStack();
        impPushOnStack(call, tiRetVal);
    }

    return callTyp;
}

/*****************************************************************************
   CEE_LEAVE may be jumping out of a protected block, viz, a catch or a
   finally-protected try. We find the finally's protecting the current
   offset (in order) by walking over the complete exception table and
   finding enclosing clauses. This assumes that the table is sorted.
   This will create a series of BBJ_CALL -> BBJ_CALL ... -> BBJ_ALWAYS.

   If we are leaving a catch handler, we need to attach the
   CPX_ENDCATCHes to the correct BBJ_CALL blocks.
 */

void                Compiler::impImportLeave(BasicBlock * block)
{
    unsigned        blkAddr     = block->bbCodeOffs;
    BasicBlock *    leaveTarget = block->bbJumpDest;
    unsigned        jmpAddr     = leaveTarget->bbCodeOffs;

    // LEAVE clears the stack, spill side effects, and set stack to 0

    impSpillSideEffects(true, CHECK_SPILL_ALL);
    verCurrentState.esStackDepth = 0;       

    assert(block->bbJumpKind == BBJ_LEAVE);
    assert(fgBBs == (BasicBlock**)0xCDCD ||
           fgLookupBB(jmpAddr) != NULL); // should be a BB boundary

    BasicBlock *    step;
    unsigned        encFinallies    = 0;
    GenTreePtr      endCatches      = NULL;
    GenTreePtr      endLFin         = NULL;

    unsigned        XTnum;
    EHblkDsc *      HBtab;

    for (XTnum = 0, HBtab = compHndBBtab;
         XTnum < info.compXcptnsCount;
         XTnum++  , HBtab++)
    {
        // Grab the handler offsets

        unsigned tryBeg = HBtab->ebdTryBeg->bbCodeOffs;
        unsigned tryEnd = ebdTryEndOffs(HBtab);
        unsigned hndBeg = HBtab->ebdHndBeg->bbCodeOffs;
        unsigned hndEnd = ebdHndEndOffs(HBtab);

        /* Is this a catch-handler we are CEE_LEAVEing out of?
         * If so, we need to call CORINFO_HELP_ENDCATCH.
         */

        if      ( jitIsBetween(blkAddr, hndBeg, hndEnd) &&
                 !jitIsBetween(jmpAddr, hndBeg, hndEnd))
        {
            // Cant CEE_LEAVE out of a finally/fault handler
            if ((HBtab->ebdFlags & (CORINFO_EH_CLAUSE_FINALLY | CORINFO_EH_CLAUSE_FAULT)) != 0)
                BADCODE("leave out of fault/finally block");

            // Create the call to CORINFO_HELP_ENDCATCH
            GenTreePtr endCatch = gtNewHelperCallNode(CORINFO_HELP_ENDCATCH, TYP_VOID);

            // Make a list of all the currently pending endCatches
            if (endCatches)
                endCatches = gtNewOperNode(GT_COMMA, TYP_VOID,
                                           endCatches, endCatch);
            else
                endCatches = endCatch;
        }
        else if ((HBtab->ebdFlags & CORINFO_EH_CLAUSE_FINALLY) &&
                  jitIsBetween(blkAddr, tryBeg, tryEnd)    &&
                 !jitIsBetween(jmpAddr, tryBeg, tryEnd))
        {
            /* This is a finally-protected try we are jumping out of */

            /* If there are any pending endCatches, and we have already
               jumped out of a finally-protected try, then the endCatches
               have to be put in a block in an outer try for async
               exceptions to work correctly.
               Else, just use append to the original block */

            BasicBlock * callBlock;

            assert(!encFinallies == !endLFin);

            if (encFinallies == 0)
            {
                callBlock               = block;
                callBlock->bbJumpKind   = BBJ_CALL;
                if (endCatches)
                    impAppendTree(endCatches, CHECK_SPILL_NONE, impCurStmtOffs);
            }
            else
            {
                /* Calling the finally block */
                callBlock           = fgNewBBinRegion(BBJ_CALL, 
                                                      XTnum+1, 
                                                      step);
                assert(step->bbJumpKind == BBJ_ALWAYS);
                step->bbJumpDest    = callBlock;

#ifdef DEBUG
                if (verbose)
                {
                    printf("\nimpImportLeave - jumping out of a finally-protected try, step block [%08X]\n", callBlock);
                }
#endif
                GenTreePtr lastStmt;

                if (endCatches)
                {
                    lastStmt        = gtNewStmt(endCatches);
                    endLFin->gtNext = lastStmt;
                    lastStmt->gtPrev= endLFin;
                }
                else
                {
                    lastStmt        = endLFin;
                }

                impEndTreeList(callBlock, endLFin, lastStmt);
            }

            step           = fgNewBBafter(BBJ_ALWAYS, callBlock);
            step->bbFlags |= BBF_IMPORTED;

#ifdef DEBUG
            if (verbose)
            {
                printf("\nimpImportLeave - jumping out of a finally-protected try, step block [%08X]\n", step);
            }
#endif
            unsigned finallyNesting = compHndBBtab[XTnum].ebdNesting;
            assert(finallyNesting <= info.compXcptnsCount);

            callBlock->bbJumpDest   = HBtab->ebdHndBeg;

            endLFin  = gtNewOperNode(GT_END_LFIN, TYP_VOID);
            endLFin->gtVal.gtVal1   = finallyNesting;
            endLFin                 = gtNewStmt(endLFin);

            endCatches              = NULL;
            encFinallies++;
        }
    }

    /* Append any remaining endCatches, if any */

    assert(!encFinallies == !endLFin);

    if (encFinallies == 0)
    {
        block->bbJumpKind = BBJ_ALWAYS;
        if (endCatches)
            impAppendTree(endCatches, CHECK_SPILL_NONE, impCurStmtOffs);
    }
    else
    {
        /* Jumping out of a try block */
        BasicBlock * step2  = fgNewBBinRegion(BBJ_ALWAYS,
                                              leaveTarget->bbTryIndex,
                                              step);
        step->bbJumpDest    = step2;

#ifdef DEBUG
        if (verbose)
        {
            printf("\nimpImportLeave - step2 block required (encFinallies > 0), new block [%08X]\n", step2);
        }
#endif
            
        GenTreePtr lastStmt;

        if (endCatches)
        {
            lastStmt            = gtNewStmt(endCatches);
            endLFin->gtNext     = lastStmt;
            lastStmt->gtPrev    = endLFin;
        }
        else
        {
            lastStmt = endLFin;
        }

        impEndTreeList(step2, endLFin, lastStmt);

        step2->bbJumpDest   = leaveTarget;

        // Queue up the jump target for importing

        impImportBlockPending(leaveTarget, false);
    }
}

/*****************************************************************************/
// This is called when reimporting a leave block. It resets the JumpKind,
// JumpDest, and bbNext to the original values

void                Compiler::impResetLeaveBlock(BasicBlock* block, unsigned jmpAddr)
{
    block->bbJumpKind = BBJ_LEAVE;
    fgInitBBLookup();
    block->bbJumpDest = fgLookupBB(jmpAddr);

    // We will leave the BBJ_ALWAYS block we introduced. When it's reimported
    // the BBJ_ALWAYS block will be unreachable, and will be removed after. The
    // reason we don't want to remove the block at this point is that if we call
    // fgInitBBLookup() again we will do it wrong as the BBJ_ALWAYS block won't be
    // added and the linked list length will be different than fgBBcount.

    /*
    for (unsigned i = 0; i< fgBBcount; i++)
    {
        if (fgBBs[i]->bbNum == (block->bbNum + 1))
        {
            //block->bbNext = fgBBs[i];
            return;
        }
    }
    
    assert(!"unable to find block");
    */

}

/*****************************************************************************/

#ifdef DEBUG

enum            controlFlow_t
{
    NEXT,
    CALL,
    RETURN,
    THROW,
    BRANCH,
    COND_BRANCH,
    BREAK,
    PHI,
    META,
};

const static
controlFlow_t   controlFlow[] =
{
    #define OPDEF(c,s,pop,push,args,type,l,s1,s2,flow) flow,
    #include "opcode.def"
    #undef OPDEF
};

#endif

/*****************************************************************************
 *  Import the instr for the given basic block
 */
void                Compiler::impImportBlockCode(BasicBlock * block)
{

#define _impGetToken(addr, scope, verify) impGetToken(addr, scope, verify)

#ifdef  DEBUG
    if (verbose)
        printf("\nImporting BB%02u (PC=%03u) of '%s'",
                block->bbNum, block->bbCodeOffs, info.compFullName);

    // Stress impBBisPush() by calling it often
    if (compStressCompile(STRESS_GENERIC_CHECK, 30))
    {
        bool    hasFloat;
        impBBisPush(block, &hasFloat);
    }
#endif

    unsigned        nxtStmtIndex = impInitBlockLineInfo();
    IL_OFFSET       nxtStmtOffs  = (nxtStmtIndex < info.compStmtOffsetsCount)
                                    ? info.compStmtOffsets[nxtStmtIndex]
                                    : BAD_IL_OFFSET;

    /* Get the tree list started */

    impBeginTreeList();

    /* Walk the opcodes that comprise the basic block */

    const BYTE *codeAddr      = info.compCode + block->bbCodeOffs;
    const BYTE *codeEndp      = codeAddr + block->bbCodeSize;

    IL_OFFSET   opcodeOffs    = block->bbCodeOffs;
    IL_OFFSET   lastSpillOffs = opcodeOffs;

    /* remember the start of the delegate creation sequence (used for verification) */
    const BYTE* delegateCreateStart = 0;


    int prefixFlags = 0;
    enum { PREFIX_TAILCALL = 1, PREFIX_VOLATILE = 2, PREFIX_UNALIGNED = 4 };
    bool tailCall;

    bool        insertLdloc   = false;  // set by CEE_DUP and cleared by following store
    typeInfo    tiRetVal;

    unsigned    numArgs       = info.compArgsCount;

    /* Now process all the opcodes in the block */

    var_types   callTyp     = TYP_COUNT;
    OPCODE      opcode      = CEE_ILLEGAL;

    while (codeAddr < codeEndp)
    {
#ifdef DEBUG
        tiRetVal = typeInfo();  // Force to error on each instruction (should we leave in retail?)
#endif

        //---------------------------------------------------------------------

        /* We need to restrict the max tree depth as many of the Compiler
           functions are recursive. We do this by spilling the stack */

        if (verCurrentState.esStackDepth)
        {
            /* Has it been a while since we last saw a non-empty stack (which
               guarantees that the tree depth isnt accumulating. */

            if ((opcodeOffs - lastSpillOffs) > 200)
            {
                impSpillStackEnsure();
                lastSpillOffs = opcodeOffs;
            }
        }
        else
        {
            lastSpillOffs = opcodeOffs;
            impBoxTempInUse = false;        // nothing on the stack, box temp OK to use again
        }

        /* Compute the current instr offset */

        opcodeOffs = codeAddr - info.compCode;

#if defined(DEBUGGING_SUPPORT) || defined(DEBUG)

#ifndef DEBUG
        if (opts.compDbgInfo)
#endif
        {
            /* Have we reached the next stmt boundary ? */

            if  (nxtStmtOffs != BAD_IL_OFFSET && opcodeOffs >= nxtStmtOffs)
            {
                assert(nxtStmtOffs == info.compStmtOffsets[nxtStmtIndex]);

                if  (verCurrentState.esStackDepth != 0 && opts.compDbgCode)
                {
                    /* We need to provide accurate IP-mapping at this point.
                       So spill anything on the stack so that it will form
                       gtStmts with the correct stmt offset noted */

                    impSpillStackEnsure(true);
                }

                // Has impCurStmtOffs been reported in any tree?

                if (impCurStmtOffs != BAD_IL_OFFSET && opts.compDbgCode)
                {
                    GenTreePtr placeHolder = gtNewOperNode(GT_NO_OP, TYP_VOID);
                    impAppendTree(placeHolder, CHECK_SPILL_NONE, impCurStmtOffs);

                    assert(impCurStmtOffs == BAD_IL_OFFSET);
                }

                if (impCurStmtOffs == BAD_IL_OFFSET)
                {
                    /* Make sure that nxtStmtIndex is in sync with opcodeOffs.
                       If opcodeOffs has gone past nxtStmtIndex, catch up */

                    while ((nxtStmtIndex+1) < info.compStmtOffsetsCount &&
                           info.compStmtOffsets[nxtStmtIndex+1] <= opcodeOffs)
                    {
                        nxtStmtIndex++;
                    }

                    /* Switch to the new stmt */

                    impCurStmtOffsSet(info.compStmtOffsets[nxtStmtIndex]);

                    /* Update the stmt boundary index */

                    nxtStmtIndex++;
                    assert(nxtStmtIndex <= info.compStmtOffsetsCount);

                    /* Are there any more line# entries after this one? */

                    if  (nxtStmtIndex < info.compStmtOffsetsCount)
                    {
                        /* Remember where the next line# starts */

                        nxtStmtOffs = info.compStmtOffsets[nxtStmtIndex];
                    }
                    else
                    {
                        /* No more line# entries */

                        nxtStmtOffs = BAD_IL_OFFSET;
                    }
                }
            }
            else if  ((info.compStmtOffsetsImplicit & STACK_EMPTY_BOUNDARIES) &&
                      (verCurrentState.esStackDepth == 0))
            {
                /* At stack-empty locations, we have already added the tree to
                   the stmt list with the last offset. We just need to update
                   impCurStmtOffs
                 */

                impCurStmtOffsSet(opcodeOffs);
            }
            else if  ((info.compStmtOffsetsImplicit & CALL_SITE_BOUNDARIES) &&
                      (impOpcodeIsCall(opcode)) && (opcode != CEE_JMP))
            {
                // @TODO [REVISIT] [04/16/01] []: Do we need to spill locals (or only lvAddrTaken ones?)
                if (callTyp == TYP_VOID)
                {
                    impCurStmtOffsSet(opcodeOffs);
                }
                else if (opts.compDbgCode)
                {
                    impSpillStackEnsure(true);
                    impCurStmtOffsSet(opcodeOffs);
                }
            }

            assert(impCurStmtOffs == BAD_IL_OFFSET || nxtStmtOffs == BAD_IL_OFFSET ||
                   jitGetILoffs(impCurStmtOffs) <= nxtStmtOffs);
        }

#endif

        CORINFO_CLASS_HANDLE    clsHnd;
        var_types       lclTyp;

        /* Get the next opcode and the size of its parameters */

        opcode = (OPCODE) getU1LittleEndian(codeAddr);
        codeAddr += sizeof(__int8);

#ifdef  DEBUG
        impCurOpcOffs   = codeAddr - info.compCode - 1;

        if  (verbose)
            printf("\n[%2u] %3u (0x%03x)",
                   verCurrentState.esStackDepth, impCurOpcOffs, impCurOpcOffs);
#endif

DECODE_OPCODE:

        /* Get the size of additional parameters */

        signed  int     sz = opcodeSizes[opcode];

#ifdef  DEBUG

        clsHnd          = BAD_CLASS_HANDLE;
        lclTyp          = TYP_COUNT;
        callTyp         = TYP_COUNT;

        impCurOpcOffs   = codeAddr - info.compCode - 1;
        impCurOpcName   = opcodeNames[opcode];

        if (verbose && (controlFlow[opcode] != META))
            printf(" %s", impCurOpcName);
#endif

#if COUNT_OPCODES
        assert(opcode < OP_Count); genOpcodeCnt[opcode].ocCnt++;
#endif

        GenTreePtr      op1;
        GenTreePtr      op2;

        /* Use assertImp() to display the opcode */

#ifndef DEBUG
#define assertImp(cond)     ((void)0)
#else
        op1 = NULL;
        op2 = NULL;
        char assertImpBuf[400];
#define assertImp(cond)                                                        \
            do { if (!(cond)) {                                                \
                _snprintf(assertImpBuf, 400, "%s : Possibly bad IL with CEE_%s"\
                               " at offset %04Xh (op1=%s op2=%s stkDepth=%d)", \
                        #cond, impCurOpcName, impCurOpcOffs,                   \
                        op1?varTypeName(op1->TypeGet()):"NULL",                \
                        op2?varTypeName(op2->TypeGet()):"NULL", verCurrentState.esStackDepth);   \
                assertImpBuf[400-1] = 0;                                       \
                assertAbort(assertImpBuf, __FILE__, __LINE__);   \
            } } while(0)
#endif

        /* See what kind of an opcode we have, then */

        switch (opcode)
        {
            unsigned        lclNum;
            var_types       type;

            GenTreePtr      op3;
            GenTreePtr      thisPtr;

            genTreeOps      oper;

            int             memberRef, typeRef, val;

            CORINFO_METHOD_HANDLE   methHnd;
            CORINFO_FIELD_HANDLE    fldHnd;

            CORINFO_SIG_INFO    sig;
            unsigned        mflags, clsFlags;
            unsigned        flags, jmpAddr;
            bool            ovfl, uns, unordered, callNode;

            union
            {
                long            intVal;
                float           fltVal;
                __int64         lngVal;
                double          dblVal;
            }
                            cval;

            case CEE_PREFIX1:
                opcode = (OPCODE) (getU1LittleEndian(codeAddr) + 256);
                codeAddr += sizeof(__int8);
                goto DECODE_OPCODE;


        SPILL_APPEND:

            /* Append 'op1' to the list of statements */

            impAppendTree(op1, CHECK_SPILL_ALL, impCurStmtOffs);
            goto DONE_APPEND;

        APPEND:

            /* Append 'op1' to the list of statements */

            impAppendTree(op1, CHECK_SPILL_NONE, impCurStmtOffs);
            goto DONE_APPEND;

        DONE_APPEND:

            // Remember at which BC offset the tree was finished
#ifdef DEBUG
            impNoteLastILoffs();
#endif
            break;

        case CEE_LDNULL:
            impPushNullObjRefOnStack();

            break;

        case CEE_LDC_I4_M1 :
        case CEE_LDC_I4_0 :
        case CEE_LDC_I4_1 :
        case CEE_LDC_I4_2 :
        case CEE_LDC_I4_3 :
        case CEE_LDC_I4_4 :
        case CEE_LDC_I4_5 :
        case CEE_LDC_I4_6 :
        case CEE_LDC_I4_7 :
        case CEE_LDC_I4_8 :
            cval.intVal = (opcode - CEE_LDC_I4_0);
            assert(-1 <= cval.intVal && cval.intVal <= 8);
            goto PUSH_I4CON;

        case CEE_LDC_I4_S: cval.intVal = getI1LittleEndian(codeAddr); goto PUSH_I4CON;
        case CEE_LDC_I4:   cval.intVal = getI4LittleEndian(codeAddr); goto PUSH_I4CON;
        PUSH_I4CON:
#ifdef DEBUG
            if (verbose) printf(" %d", cval.intVal);
#endif
            impPushOnStack(gtNewIconNode(cval.intVal), typeInfo(TI_INT));
            break;

        case CEE_LDC_I8:  cval.lngVal = getI8LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %I64d", cval.lngVal);
#endif
            impPushOnStack(gtNewLconNode(cval.lngVal), typeInfo(TI_LONG));
            break;

        case CEE_LDC_R8:  cval.dblVal = getR8LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %g", cval.dblVal);
#endif
            impPushOnStack(gtNewDconNode(cval.dblVal), typeInfo(TI_DOUBLE));
            break;

        case CEE_LDC_R4: 
            cval.dblVal = getR4LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %g", cval.dblVal);
#endif
            impPushOnStack(gtNewDconNode(cval.dblVal), typeInfo(TI_DOUBLE));
            break;

        case CEE_LDSTR:
            val = getU4LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %08X", val);
#endif
            if (tiVerificationNeeded) 
            {          
                Verify(info.compCompHnd->isValidStringRef(info.compScopeHnd, val), "bad string");
                tiRetVal = typeInfo(TI_REF, impGetStringClass());
            }
            impPushOnStack(gtNewSconNode(val, info.compScopeHnd), tiRetVal);

            break;

        case CEE_LDARG:
            lclNum = getU2LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %u", lclNum);
#endif
            goto LD_ARGVAR;

        case CEE_LDARG_S:
            lclNum = getU1LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %u", lclNum);
#endif
            goto LD_ARGVAR;

        case CEE_LDARG_0:
        case CEE_LDARG_1:
        case CEE_LDARG_2:
        case CEE_LDARG_3:
            lclNum = (opcode - CEE_LDARG_0);
            assert(lclNum >= 0 && lclNum < 4);

        LD_ARGVAR:
            Verify(lclNum < info.compILargsCount, "bad arg num");
            lclNum = compMapILargNum(lclNum);   // account for possible hidden param 
            assertImp(lclNum < numArgs);
            goto LDVAR;


        case CEE_LDLOC:
            lclNum = getU2LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %u", lclNum);
#endif
            goto LD_LCLVAR;

        case CEE_LDLOC_S:
            lclNum = getU1LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %u", lclNum);
#endif
            goto LD_LCLVAR;

        case CEE_LDLOC_0:
        case CEE_LDLOC_1:
        case CEE_LDLOC_2:
        case CEE_LDLOC_3:
            lclNum = (opcode - CEE_LDLOC_0);
            assert(lclNum >= 0 && lclNum < 4);

        LD_LCLVAR:
            if (tiVerificationNeeded) 
            {
                Verify(lclNum < info.compMethodInfo->locals.numArgs, "bad loc num");
                Verify(info.compInitMem, "initLocals not set");
            }
            lclNum += numArgs;
            
            // info.compLocalsCount would be zero for 65536 variables because it's
            // stored in a 16-bit counter.
            
            if (lclNum >= info.compLocalsCount)
            {
                IMPL_LIMITATION("Bad IL or unsupported 65536 local variables");
            }
            
        LDVAR:
            if (lvaTable[lclNum].lvNormalizeOnLoad())
                lclTyp = lvaGetRealType  (lclNum);
            else
                lclTyp = lvaGetActualType(lclNum);

            op1 = gtNewLclvNode(lclNum, lclTyp, opcodeOffs + sz + 1);

            tiRetVal = lvaTable[lclNum].lvVerTypeInfo;
            tiRetVal.NormaliseForStack();


            if (verTrackObjCtorInitState && 
                !verCurrentState.thisInitialized && 
                tiRetVal.IsThisPtr())
                tiRetVal.SetUninitialisedObjRef();

            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_STARG:
            lclNum = getU2LittleEndian(codeAddr);
            goto STARG;

        case CEE_STARG_S:
            lclNum = getU1LittleEndian(codeAddr);
        STARG:
#ifdef DEBUG
            if (verbose) printf(" %u", lclNum);
#endif

            if (tiVerificationNeeded)
            {
                Verify(lclNum < info.compILargsCount, "bad arg num");
                typeInfo& tiLclVar = lvaTable[compMapILargNum(lclNum)].lvVerTypeInfo;
                Verify(tiCompatibleWith(impStackTop().seTypeInfo, NormaliseForStack(tiLclVar)), "type mismatch");

                if (verTrackObjCtorInitState && !verCurrentState.thisInitialized)
                    Verify(!tiLclVar.IsThisPtr(), "storing to uninit this ptr");
            }

            lclNum = compMapILargNum(lclNum);     // account for possible hidden param

            assertImp(lclNum < numArgs);
            goto VAR_ST;

        case CEE_STLOC:
            lclNum = getU2LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %u", lclNum);
#endif
            goto LOC_ST;

        case CEE_STLOC_S:
            lclNum = getU1LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %u", lclNum);
#endif
            goto LOC_ST;

        case CEE_STLOC_0:
        case CEE_STLOC_1:
        case CEE_STLOC_2:
        case CEE_STLOC_3:
            lclNum = (opcode - CEE_STLOC_0);
            assert(lclNum >= 0 && lclNum < 4);

        LOC_ST:
            if (tiVerificationNeeded)
            {
                Verify(lclNum < info.compMethodInfo->locals.numArgs, "bad local num");
                Verify(tiCompatibleWith(impStackTop().seTypeInfo, NormaliseForStack(lvaTable[lclNum + numArgs].lvVerTypeInfo)), "type mismatch");
            }
            lclNum += numArgs;

        VAR_ST:

            // info.compLocalsCount would be zero for 65536 variables because it's
            // stored in a 16-bit counter.
            
            if (lclNum >= info.compLocalsCount)
            {
                IMPL_LIMITATION("Bad IL or unsupported 65536 local variables");
            }

            /* Pop the value being assigned */

            op1 = impPopStack(clsHnd).val;

            /* if it is a struct assignment, make certain we don't overflow the buffer */ 
            assert(lclTyp != TYP_STRUCT || lvaLclSize(lclNum) >= info.compCompHnd->getClassSize(clsHnd));

            if (lvaTable[lclNum].lvNormalizeOnLoad())
                lclTyp = lvaGetRealType  (lclNum);
            else
                lclTyp = lvaGetActualType(lclNum);

#if HOIST_THIS_FLDS
            if (varTypeIsGC(lclTyp))
                optHoistTFRasgThis();
#endif
            // We had better assign it a value of the correct type

            assertImp(genActualType(lclTyp) == genActualType(op1->gtType) ||
                      genActualType(lclTyp) == TYP_I_IMPL && op1->IsVarAddr() ||
                      (genActualType(lclTyp) == TYP_INT && op1->gtType == TYP_BYREF) ||
                      (genActualType(op1->gtType) == TYP_INT && lclTyp == TYP_BYREF) ||
                      (varTypeIsFloating(lclTyp) && varTypeIsFloating(op1->TypeGet())) ||
                      ((genActualType(lclTyp) == TYP_BYREF) && genActualType(op1->TypeGet()) == TYP_REF));

            /* If op1 is "&var" then its type is the transient "*" and it can
               be used either as TYP_BYREF or TYP_I_IMPL */

            if (op1->IsVarAddr())
            {
                assertImp(genActualType(lclTyp) == TYP_I_IMPL || lclTyp == TYP_BYREF);

                /* When "&var" is created, we assume it is a byref. If it is
                   being assigned to a TYP_I_IMPL var, bash the type to
                   prevent unnecessary GC info */

                if (genActualType(lclTyp) == TYP_I_IMPL)
                    op1->gtType = TYP_I_IMPL;
            }

            /* Filter out simple assignments to itself */

            if  (op1->gtOper == GT_LCL_VAR && lclNum == op1->gtLclVar.gtLclNum)
            {
                if (insertLdloc)
                {
                    // This is a sequence of (ldloc, dup, stloc).  Can simplify
                    // to (ldloc).  Goto LDVAR to reconstruct the ldloc node.
                    
                    op1 = NULL;
                    insertLdloc = false;

                    goto LDVAR;
                }
                else
                {
                    break;
                }
            }

            /* Create the assignment node */

            op2 = gtNewLclvNode(lclNum, lclTyp, opcodeOffs + sz + 1);

            /* If the local is aliased, we need to spill calls and
               indirections from the stack. */

            if (lvaTable[lclNum].lvAddrTaken && verCurrentState.esStackDepth > 0)
                impSpillSideEffects(false, CHECK_SPILL_ALL);

            /* Spill any refs to the local from the stack */

            impSpillLclRefs(lclNum);

            if (lclTyp == TYP_STRUCT)
            {
                op1 = impAssignStruct(op2, op1, clsHnd, CHECK_SPILL_ALL);
            }
            else
            {
                // ISSUE: the code generator generates GC tracking information
                // based on the RHS of the assignment.  Later the LHS (which is
                // is a BYREF) gets used and the emitter checks that that variable 
                // is being tracked.  It is not (since the RHS was an int and did
                // not need tracking).  To keep this assert happy, we bash the RHS
                if (lclTyp == TYP_BYREF && !varTypeIsGC(op1->gtType))
                    op1->gtType = TYP_BYREF;
                op1 = gtNewAssignNode(op2, op1);
            }

            /* If insertLdloc is true, then we need to insert a ldloc following the
               stloc.  This is done when converting a (dup, stloc) sequence into
               a (stloc, ldloc) sequence. */

            if (insertLdloc)
            {
                // From SPILL_APPEND
                impAppendTree(op1, CHECK_SPILL_ALL, impCurStmtOffs);
                
                // From DONE_APPEND
#ifdef DEBUG
                impNoteLastILoffs();
#endif
                op1 = NULL;
                insertLdloc = false;

                goto LDVAR;
            }

            goto SPILL_APPEND;


        case CEE_LDLOCA:
            lclNum = getU2LittleEndian(codeAddr);
            goto LDLOCA;

        case CEE_LDLOCA_S:
            lclNum = getU1LittleEndian(codeAddr);
        LDLOCA:
#ifdef DEBUG
            if (verbose) printf(" %u", lclNum);
#endif
            if (tiVerificationNeeded)
            {
                Verify(lclNum < info.compMethodInfo->locals.numArgs, "bad local num");
                Verify(info.compInitMem, "initLocals not set");
            }
            lclNum += numArgs;
            assertImp(lclNum < info.compLocalsCount);
            goto ADRVAR;


        case CEE_LDARGA:
            lclNum = getU2LittleEndian(codeAddr);
            goto LDARGA;

        case CEE_LDARGA_S:
            lclNum = getU1LittleEndian(codeAddr);
        LDARGA:
#ifdef DEBUG
            if (verbose) printf(" %u", lclNum);
#endif
            Verify(lclNum < info.compILargsCount, "bad arg num");
            lclNum = compMapILargNum(lclNum);     // account for possible hidden param
            assertImp(lclNum < numArgs);
            goto ADRVAR;

        ADRVAR:

            assert(lvaTable[lclNum].lvAddrTaken);

            op1 = gtNewLclvNode(lclNum, lvaGetActualType(lclNum), opcodeOffs + sz + 1);

            /* Note that this is supposed to create the transient type "*"
               which may be used as a TYP_I_IMPL. However we catch places
               where it is used as a TYP_I_IMPL and bash the node if needed.
               Thus we are pessimistic and may report byrefs in the GC info
               where it was not absolutely needed, but it is safer this way.
             */
            op1 = gtNewOperNode(GT_ADDR, TYP_BYREF, op1);

            // &aliasedVar doesnt need GTF_GLOB_REF, though alisasedVar does
            op1->gtFlags &= ~GTF_GLOB_REF;

            op1->gtFlags |= GTF_ADDR_ONSTACK;
            if (tiVerificationNeeded)
            {

                tiRetVal = lvaTable[lclNum].lvVerTypeInfo;

                // Don't allow taking address of uninit this ptr.
                if (verTrackObjCtorInitState && 
                    !verCurrentState.thisInitialized)
                    Verify(!tiRetVal.IsThisPtr(), "address of uninit this ptr");

                if (!tiRetVal.IsByRef())
                    tiRetVal.MakeByRef();
                else
                    Verify(false, "byref to byref");
            }
            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_ARGLIST:

            if (tiVerificationNeeded) 
            {
                Verify(info.compIsVarArgs, "arglist in non-vararg method");
                tiRetVal = typeInfo(TI_STRUCT, impGetRuntimeArgumentHandle());
            }
            assertImp((info.compMethodInfo->args.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_VARARG);

            /* The ARGLIST cookie is a hidden 'last' parameter, we have already
               adjusted the arg count cos this is like fetching the last param */
            assertImp(0 < numArgs);
            assert(lvaTable[lvaVarargsHandleArg].lvAddrTaken);
            lclNum = lvaVarargsHandleArg;
            op1 = gtNewLclvNode(lclNum, TYP_I_IMPL, opcodeOffs + sz + 1);
            op1 = gtNewOperNode(GT_ADDR, TYP_BYREF, op1);

            op1->gtFlags |= GTF_ADDR_ONSTACK;
            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_ENDFINALLY:
               // @TODO [REVISIT] 4/25/01 enable this post V1 -vancem 
#if 0       
            if (verCurrentState.esStackDepth != 0) 
            {
                verRaiseVerifyExceptionIfNeeded(INDEBUG("stack must be 0 on end of finally") DEBUGARG(__FILE__) DEBUGARG(__LINE__));
                BADCODE("Stack must be 1 on end of finally");
            }

#else
            if (verCurrentState.esStackDepth > 0)
                impEvalSideEffects();
#endif

            if (info.compXcptnsCount == 0)
                BADCODE("endfinally outside finally");

            assert(verCurrentState.esStackDepth == 0);

            op1 = gtNewOperNode(GT_RETFILT, TYP_VOID);
            goto APPEND;

        case CEE_ENDFILTER:
            block->bbSetRunRarely();     // filters are rare

            if (info.compXcptnsCount == 0)
                BADCODE("endfilter outside filter");

            if (tiVerificationNeeded)
                Verify(impStackTop().seTypeInfo.IsType(TI_INT), "bad endfilt arg");

            op1 = impPopStack().val;    
            assertImp(op1->gtType == TYP_INT);
            if (!bbInFilterBlock(block))
                BADCODE("EndFilter outside a filter handler");

            /* Mark current bb as end of filter */

            assert((compCurBB->bbFlags & (BBF_ENDFILTER|BBF_DONT_REMOVE)) ==
                                         (BBF_ENDFILTER|BBF_DONT_REMOVE));
            assert(compCurBB->bbJumpKind == BBJ_RET);

            /* Mark catch handler as successor */

            compCurBB->bbJumpDest = compHndBBtab[compCurBB->getHndIndex()].ebdHndBeg;
            assert(compCurBB->bbJumpDest->bbCatchTyp == BBCT_FILTER_HANDLER);

            op1 = gtNewOperNode(GT_RETFILT, op1->TypeGet(), op1);
            if (verCurrentState.esStackDepth != 0) 
            {
                verRaiseVerifyExceptionIfNeeded(INDEBUG("stack must be 1 on end of filter") DEBUGARG(__FILE__) DEBUGARG(__LINE__));
                BADCODE("Stack must be 1 on end of filter");
            }
            goto APPEND;

        case CEE_RET:
            prefixFlags &= ~PREFIX_TAILCALL;    // ret without call before it
        RET:
            if (tiVerificationNeeded)
            {
                verVerifyThisPtrInitialised();

                unsigned expectedStack = 0;
                if (info.compRetType != TYP_VOID) 
                {
                    typeInfo tiVal = impStackTop().seTypeInfo;
                    typeInfo tiDeclared = verMakeTypeInfo(info.compMethodInfo->args.retType, info.compMethodInfo->args.retTypeClass);
                    
                    Verify(tiCompatibleWith(tiVal, tiDeclared.NormaliseForStack()), "type mismatch");
                    Verify(!verIsByRefLike(tiDeclared), "byref return");
                    expectedStack=1;
                }
                Verify(verCurrentState.esStackDepth == expectedStack, "stack non-empty on return");
            }
            
            op2 = 0;
            if (info.compRetType != TYP_VOID)
                {
                    op2 = impPopStack(clsHnd).val;
                impBashVarAddrsToI(op2);
                assertImp((genActualType(op2->TypeGet()) == genActualType(info.compRetType)) ||
                          ((op2->TypeGet() == TYP_INT) && (info.compRetType == TYP_BYREF)) ||
                          ((op2->TypeGet() == TYP_BYREF) && (info.compRetType == TYP_INT)) ||
                          varTypeIsFloating(op2->gtType) && varTypeIsFloating(info.compRetType));
            }

            if (info.compRetType == TYP_STRUCT)
            {
                // Assign value to return buff (first param)
                GenTreePtr retBuffAddr = gtNewLclvNode(info.compRetBuffArg, TYP_BYREF, impCurStmtOffs);

                op2 = impAssignStructPtr(retBuffAddr, op2, clsHnd, CHECK_SPILL_ALL);
                impAppendTree(op2, CHECK_SPILL_NONE, impCurStmtOffs);

                // and return void
                op1 = gtNewOperNode(GT_RETURN);
            }
            else
                op1 = gtNewOperNode(GT_RETURN, genActualType(info.compRetType), op2);

#if TGT_RISC
            genReturnCnt++;
#endif
            // We must have imported a tailcall and jumped to RET
            if (prefixFlags & PREFIX_TAILCALL)
            {
                assert(verCurrentState.esStackDepth == 0 && impOpcodeIsCall(opcode));
                opcode = CEE_RET; // To prevent trying to spill if CALL_SITE_BOUNDARIES

                // impImportCall() would have already appended TYP_VOID calls
                if (info.compRetType == TYP_VOID)
                    break;
            }

            goto APPEND;

            /* These are similar to RETURN */

        case CEE_JMP:
           if (tiVerificationNeeded)
               Verify(false, "Invalid opcode");

           if ((info.compFlags & CORINFO_FLG_SYNCH) ||
               block->hasTryIndex() || block->hasHndIndex())
           {
                /* @TODO [FIXHACK] [06/13/01] [] : The same restrictions as
                   canTailCall() should be applicable to CEE_JMP. In those
                   cases, CEE_JMP should behave like CEE_CALL+CEE_RET */
           }

           memberRef = getU4LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %08X", memberRef);
#endif

            methHnd   = eeFindMethod(memberRef, info.compScopeHnd, info.compMethodHnd);

            /* The signature of the target has to be identical to ours.
               At least check that argCnt and returnType match */

            eeGetMethodSig(methHnd, &sig);
            if  (sig.numArgs != info.compMethodInfo->args.numArgs ||
                 sig.retType != info.compMethodInfo->args.retType ||
                 sig.callConv != info.compMethodInfo->args.callConv)
                BADCODE("Incompatible target for CEE_JMPs");

            op1 = gtNewOperNode(GT_JMP);
            op1->gtVal.gtVal1 = (unsigned) methHnd;

            if (verCurrentState.esStackDepth != 0)   BADCODE("Stack must be empty after CEE_JMPs");

            /* Mark the basic block as being a JUMP instead of RETURN */

            block->bbFlags |= BBF_HAS_JMP;

            /* Set this flag to make sure register arguments have a location assigned
             * even if we don't use them inside the method */

            compJmpOpUsed = true;

#if TGT_RISC
            genReturnCnt++;
#endif
            goto APPEND;

        case CEE_LDELEMA :
            assertImp(sz == sizeof(unsigned));
            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf(" %08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);
            clsFlags = eeGetClassAttribs(clsHnd);

            if (tiVerificationNeeded) 
            {

                typeInfo tiArray = impStackTop(1).seTypeInfo;
                typeInfo tiIndex = impStackTop().seTypeInfo;
                
                Verify(tiIndex.IsType(TI_INT), "bad index");
                typeInfo arrayElemType = verMakeTypeInfo(clsHnd);
                Verify(verGetArrayElemType(tiArray) == arrayElemType, "bad array");
                
                tiRetVal = arrayElemType;
                tiRetVal.MakeByRef();
            }
            
            if (clsFlags & CORINFO_FLG_VALUECLASS) {
                lclTyp = TYP_STRUCT;
                goto ARR_LD;
            }
            
                op1 = gtNewIconEmbClsHndNode(clsHnd, typeRef, info.compScopeHnd);
                op1 = gtNewOperNode(GT_LIST, TYP_VOID, op1);                // Type
                op1 = gtNewOperNode(GT_LIST, TYP_VOID, impPopStack().val, op1); // index

            op1 = gtNewOperNode(GT_LIST, TYP_VOID, impPopStack().val, op1);      // array 
                op1 = gtNewHelperCallNode(CORINFO_HELP_LDELEMA_REF, TYP_BYREF, GTF_EXCEPT, op1);
                
            impPushOnStack(op1, tiRetVal);
                break;

        case CEE_LDELEM_I1 : lclTyp = TYP_BYTE  ; goto ARR_LD;
        case CEE_LDELEM_I2 : lclTyp = TYP_SHORT ; goto ARR_LD;
        case CEE_LDELEM_I  :
        case CEE_LDELEM_U4 :
        case CEE_LDELEM_I4 : lclTyp = TYP_INT   ; goto ARR_LD;
        case CEE_LDELEM_I8 : lclTyp = TYP_LONG  ; goto ARR_LD;
        case CEE_LDELEM_REF: lclTyp = TYP_REF   ; goto ARR_LD;
        case CEE_LDELEM_R4 : lclTyp = TYP_FLOAT ; goto ARR_LD;
        case CEE_LDELEM_R8 : lclTyp = TYP_DOUBLE; goto ARR_LD;
        case CEE_LDELEM_U1 : lclTyp = TYP_UBYTE ; goto ARR_LD;
        case CEE_LDELEM_U2 : lclTyp = TYP_CHAR  ; goto ARR_LD;

        ARR_LD:

#if CSELENGTH
            fgHasRangeChks = true;
#endif
            if (tiVerificationNeeded && opcode != CEE_LDELEMA)
            {
                typeInfo tiArray = impStackTop(1).seTypeInfo;
                typeInfo tiIndex = impStackTop().seTypeInfo;
    
                Verify(tiIndex.IsType(TI_INT), "bad index");
                if (tiArray.IsNullObjRef()) 
                    {
                    if (lclTyp == TYP_REF)             // we will say a deref of a null array yields a null ref
                        tiRetVal = typeInfo(TI_NULL);
                    else
                        tiRetVal = typeInfo(lclTyp);
                    }
                    else
                    {
                    tiRetVal = verGetArrayElemType(tiArray);
                    Verify(tiRetVal.IsType(varType2tiType(lclTyp)), "bad array");                       
                }
                tiRetVal.NormaliseForStack();
            }
            
            /* Pull the index value and array address */
                op2 = impPopStack().val;
                op1 = impPopStack().val;   assertImp(op1->gtType == TYP_REF);

            op1 = impCheckForNullPointer(op1);

            /* Mark the block as containing an index expression */

            if  (op1->gtOper == GT_LCL_VAR)
            {
                if  (op2->gtOper == GT_LCL_VAR ||
                     op2->gtOper == GT_ADD)
                {
                    block->bbFlags |= BBF_HAS_INDX;
                }
            }

            /* Create the index node and push it on the stack */
            op1 = gtNewIndexRef(lclTyp, op1, op2);
            if (opcode == CEE_LDELEMA)
            {
                    // rememer the element size
                if (lclTyp == TYP_REF)
                    op1->gtIndex.gtIndElemSize = sizeof(void*);
                else
                    op1->gtIndex.gtIndElemSize = eeGetClassSize(clsHnd);

                // wrap it in a &                
                lclTyp = TYP_BYREF;

                op1 = gtNewOperNode(GT_ADDR, lclTyp, op1);
            }
            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_STELEM_REF:

            // @TODO [REVISIT] [04/16/01] []: Check for assignment of null and generate inline code

            /* Call a helper function to do the assignment */

            if (tiVerificationNeeded)
            {
                typeInfo tiArray = impStackTop(2).seTypeInfo;
                typeInfo tiIndex = impStackTop(1).seTypeInfo;
                typeInfo tiValue = impStackTop().seTypeInfo;

                Verify(tiIndex.IsType(TI_INT), "bad index");
                Verify(tiValue.IsObjRef(), "bad value");

                    // we only check that it is an object referece, The helper does additional checks
                Verify(tiArray.IsNullObjRef() || 
                            verGetArrayElemType(tiArray).IsType(TI_REF), "bad array");
            }

            op1 = gtNewHelperCallNode(CORINFO_HELP_ARRADDR_ST,
                                      TYP_VOID, 0,
                                      impPopList(3, &flags, 0));

            goto SPILL_APPEND;

        case CEE_STELEM_I1: lclTyp = TYP_BYTE  ; goto ARR_ST;
        case CEE_STELEM_I2: lclTyp = TYP_SHORT ; goto ARR_ST;
        case CEE_STELEM_I:
        case CEE_STELEM_I4: lclTyp = TYP_INT   ; goto ARR_ST;
        case CEE_STELEM_I8: lclTyp = TYP_LONG  ; goto ARR_ST;
        case CEE_STELEM_R4: lclTyp = TYP_FLOAT ; goto ARR_ST;
        case CEE_STELEM_R8: lclTyp = TYP_DOUBLE; goto ARR_ST;

        ARR_ST:

            /* The strict order of evaluation is LHS-operands, RHS-operands,
               range-check, and then assignment. However, codegen currently
               does the range-check before evaluation the RHS-operands. So to
               maintain strict ordering, we spill the stack.
               @TODO [REVISIT] [04/16/01] []: @PERF: This also works against the CSE logic */
            
            if (!info.compLooseExceptions &&
                (impStackTop().val->gtFlags & GTF_SIDE_EFFECT) )
            {
                impSpillSideEffects(false, CHECK_SPILL_ALL);
            }

#if CSELENGTH
            fgHasRangeChks = true;
#endif

            if (tiVerificationNeeded)
            {
                typeInfo tiArray = impStackTop(2).seTypeInfo;
                typeInfo tiIndex = impStackTop(1).seTypeInfo;
                typeInfo tiValue = impStackTop().seTypeInfo;

                Verify(tiIndex.IsType(TI_INT), "bad index");

                typeInfo arrayElem(lclTyp);
                Verify(tiArray.IsNullObjRef() || verGetArrayElemType(tiArray) == arrayElem, "bad array");

                Verify(tiValue == arrayElem.NormaliseForStack(), "bad value");
                }

                /* Pull the new value from the stack */
                op2 = impPopStack().val;

                /* Pull the index value */
                op1 = impPopStack().val;

                /* Pull the array address */
                op3 = impPopStack().val;   
            
            assertImp(op3->gtType == TYP_REF);
            if (op2->IsVarAddr())
                op2->gtType = TYP_I_IMPL;

            op3 = impCheckForNullPointer(op3);

            /* Create the index node */

            op1 = gtNewIndexRef(lclTyp, op3, op1);

            /* Create the assignment node and append it */

            op1 = gtNewAssignNode(op1, op2);

            /* Mark the expression as containing an assignment */

            op1->gtFlags |= GTF_ASG;

            // @TODO [CONSIDER] [04/16/01] []: Do we need to spill 
            // assignments to array elements with the same type?

            goto SPILL_APPEND;

        case CEE_ADD:           oper = GT_ADD;                      goto MATH_OP2;

        case CEE_ADD_OVF:       lclTyp = TYP_UNKNOWN; uns = false;  goto ADD_OVF;
        case CEE_ADD_OVF_UN:    lclTyp = TYP_UNKNOWN; uns = true;   goto ADD_OVF;

ADD_OVF:                        ovfl = true;     callNode = false;
                                oper = GT_ADD;                      goto MATH_OP2_FLAGS;

        case CEE_SUB:           oper = GT_SUB;                      goto MATH_OP2;

        case CEE_SUB_OVF:       lclTyp = TYP_UNKNOWN; uns = false;  goto SUB_OVF;
        case CEE_SUB_OVF_UN:    lclTyp = TYP_UNKNOWN; uns = true;   goto SUB_OVF;

SUB_OVF:                        ovfl = true;     callNode = false;
                                oper = GT_SUB;                      goto MATH_OP2_FLAGS;

        case CEE_MUL:           oper = GT_MUL;                      goto MATH_CALL_ON_LNG;

        case CEE_MUL_OVF:       lclTyp = TYP_UNKNOWN; uns = false;  goto MUL_OVF;
        case CEE_MUL_OVF_UN:    lclTyp = TYP_UNKNOWN; uns = true;   goto MUL_OVF;

MUL_OVF:                        ovfl = true;     callNode = false;
                                oper = GT_MUL;                      goto MATH_CALL_ON_LNG_OVF;

        // Other binary math operations

        case CEE_DIV:           oper = GT_DIV;                      goto MATH_CALL_ON_LNG;

        case CEE_DIV_UN:        oper = GT_UDIV;                     goto MATH_CALL_ON_LNG;

        case CEE_REM:           ovfl = false;
                                // can use small node for INT case
                                callNode = (impStackTop().val->gtType != TYP_INT);
                                oper = GT_MOD;                      goto MATH_OP2_FLAGS;

        case CEE_REM_UN:        oper = GT_UMOD;                     goto MATH_CALL_ON_LNG;

MATH_CALL_ON_LNG:               ovfl = false;
MATH_CALL_ON_LNG_OVF:           callNode = (impStackTop().val->gtType == TYP_LONG);
                                                                    goto MATH_OP2_FLAGS;

        case CEE_AND:        oper = GT_AND;  goto MATH_OP2;
        case CEE_OR:         oper = GT_OR ;  goto MATH_OP2;
        case CEE_XOR:        oper = GT_XOR;  goto MATH_OP2;

MATH_OP2:       // For default values of 'ovfl' and 'callNode'

            ovfl        = false;
            callNode    = false;

MATH_OP2_FLAGS: // If 'ovfl' and 'callNode' have already been set

            /* Pull two values and push back the result */

            if (tiVerificationNeeded)
            {
                const typeInfo& tiOp1 = impStackTop(1).seTypeInfo;
                const typeInfo& tiOp2 = impStackTop().seTypeInfo;
                
                Verify(tiOp1 == tiOp2, "different arg type");
                if (oper == GT_ADD || oper == GT_DIV || oper == GT_SUB || oper == GT_MUL || oper == GT_MOD)
                    Verify(tiOp1.IsNumberType(), "not number");
                else 
                    Verify(tiOp1.IsIntegerType(), "not integer");
                
                Verify(!ovfl || tiOp1.IsIntegerType(), "not integer"); 
                tiRetVal = tiOp1;
            }

                op2 = impPopStack().val;
                op1 = impPopStack().val;

#if !CPU_HAS_FP_SUPPORT
            if (varTypeIsFloating(op1->gtType))
            {
                callNode = true;
            }
#endif
            /* Cant do arithmetic with references */
            assertImp(genActualType(op1->TypeGet()) != TYP_REF &&
                      genActualType(op2->TypeGet()) != TYP_REF);

            // Bash both to TYP_I_IMPL (impBashVarAddrsToI wont change if its a true byref, only
            // if it is in the stack)
            impBashVarAddrsToI(op1, op2);

            // Arithemetic operations are generally only allowed with
            // primitive types, but certain operations are allowed
            // with byrefs

            if ((oper == GT_SUB) &&
                (genActualType(op1->TypeGet()) == TYP_BYREF ||
                 genActualType(op2->TypeGet()) == TYP_BYREF))
            {
                // byref1-byref2 => gives an int
                // byref - int   => gives a byref

                if ((genActualType(op1->TypeGet()) == TYP_BYREF) &&
                    (genActualType(op2->TypeGet()) == TYP_BYREF))
                {
                    // byref1-byref2 => gives an int
                    type = TYP_I_IMPL;
                }
                else
                {
                    // byref - int => gives a byref
                    type = TYP_BYREF;
                }
            }
            else if ( (oper== GT_ADD) &&
                      (genActualType(op1->TypeGet()) == TYP_BYREF ||
                       genActualType(op2->TypeGet()) == TYP_BYREF))
            {
                // only one can be a byref : byref op byref not allowed
                assertImp(genActualType(op1->TypeGet()) != TYP_BYREF ||
                          genActualType(op2->TypeGet()) != TYP_BYREF);
                assertImp(genActualType(op1->TypeGet()) == TYP_I_IMPL ||
                          genActualType(op2->TypeGet()) == TYP_I_IMPL);

                // byref + int => gives a byref
                // (but if &var, then dont need to report to GC)
                type = TYP_BYREF;
            }
            else
            {
                assertImp(genActualType(op1->TypeGet()) != TYP_BYREF &&
                          genActualType(op2->TypeGet()) != TYP_BYREF);

                assertImp(genActualType(op1->TypeGet()) == genActualType(op2->TypeGet()) ||
                    varTypeIsFloating(op1->gtType) && varTypeIsFloating(op2->gtType));

                type = genActualType(op1->gtType);

                if (type == TYP_FLOAT)          // spill intermediate expressions as double
                    type = TYP_DOUBLE;
            }

            assert(!ovfl || !varTypeIsFloating(op1->gtType)); 

            /* Special case: "int+0", "int-0", "int*1", "int/1" */

            if  (op2->gtOper == GT_CNS_INT)
            {
                if  (((op2->gtIntCon.gtIconVal == 0) && (oper == GT_ADD || oper == GT_SUB)) ||
                     ((op2->gtIntCon.gtIconVal == 1) && (oper == GT_MUL || oper == GT_DIV)))

                {
                    impPushOnStack(op1, tiRetVal);
                    break;
                }
            }

#if SMALL_TREE_NODES
            if (callNode)
            {
                /* These operators later get transformed into 'GT_CALL' */

                assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_MUL]);
                assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_DIV]);
                assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_UDIV]);
                assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_MOD]);
                assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_UMOD]);

                op1 = gtNewOperNode(GT_CALL, type, op1, op2);
                op1->SetOper(oper);
            }
            else
#endif
            {
                op1 = gtNewOperNode(oper,    type, op1, op2);
            }

            /* Special case: integer/long division may throw an exception */

            if  (varTypeIsIntegral(op1->TypeGet()) && op1->OperMayThrow())
            {
                op1->gtFlags |=  GTF_EXCEPT;
            }

            if  (ovfl)
            {
                assert(oper==GT_ADD || oper==GT_SUB || oper==GT_MUL);
                if (lclTyp != TYP_UNKNOWN)
                    op1->gtType   = lclTyp;
                op1->gtFlags |= (GTF_EXCEPT | GTF_OVERFLOW);
                if (uns)
                    op1->gtFlags |= GTF_UNSIGNED;
            }            

            impPushOnStack(op1, tiRetVal);
            break;


        case CEE_SHL:        oper = GT_LSH;  goto CEE_SH_OP2;

        case CEE_SHR:        oper = GT_RSH;  goto CEE_SH_OP2;
        case CEE_SHR_UN:     oper = GT_RSZ;  goto CEE_SH_OP2;

        CEE_SH_OP2:
            if (tiVerificationNeeded)
            {
                const typeInfo& tiVal = impStackTop(1).seTypeInfo;
                const typeInfo& tiShift = impStackTop(0).seTypeInfo;
                Verify(tiVal.IsIntegerType() && tiShift.IsType(TI_INT), "Bad shift args");
                tiRetVal = tiVal;
            }

            op2     = impPopStack().val;
            op1     = impPopStack().val;    // operand to be shifted
            impBashVarAddrsToI(op1, op2);

            type    = genActualType(op1->TypeGet());
            op1     = gtNewOperNode(oper, type, op1, op2);

            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_NOT:
            if (tiVerificationNeeded) 
            {
                tiRetVal = impStackTop().seTypeInfo;
                Verify(tiRetVal.IsIntegerType(), "bad int value");
            }

            op1 = impPopStack().val;
            impBashVarAddrsToI(op1, NULL);
            type = genActualType(op1->TypeGet());
            impPushOnStack(gtNewOperNode(GT_NOT, type, op1), tiRetVal);
            break;

        case CEE_CKFINITE:
            if (tiVerificationNeeded) 
            {
                tiRetVal = impStackTop().seTypeInfo;
                Verify(tiRetVal.IsType(TI_DOUBLE), "bad R value");
            }
            op1 = impPopStack().val;
            type = op1->TypeGet();
            op1 = gtNewOperNode(GT_CKFINITE, type, op1);
            op1->gtFlags |= GTF_EXCEPT;

            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_LEAVE:

            val     = getI4LittleEndian(codeAddr); // jump distance
            jmpAddr = (codeAddr - info.compCode + sizeof(__int32)) + val;
            goto LEAVE;

        case CEE_LEAVE_S:
            val     = getI1LittleEndian(codeAddr); // jump distance
            jmpAddr = (codeAddr - info.compCode + sizeof(__int8 )) + val;
            goto LEAVE;

        LEAVE:
#ifdef DEBUG
            if (verbose) printf(" %04X", jmpAddr);
#endif
            if (block->bbJumpKind != BBJ_LEAVE)
            {
                impResetLeaveBlock(block, jmpAddr);
            }

            assert(jmpAddr == block->bbJumpDest->bbCodeOffs);
            impImportLeave(block);
            impNoteBranchOffs();

            break;


        case CEE_BR:
        case CEE_BR_S:

#if HOIST_THIS_FLDS
            if  (block->bbNum >= block->bbJumpDest->bbNum)
                optHoistTFRhasLoop();
#endif
            impNoteBranchOffs();
            break;


        case CEE_BRTRUE:
        case CEE_BRTRUE_S:
        case CEE_BRFALSE:
        case CEE_BRFALSE_S:

            /* Pop the comparand (now there's a neat term) from the stack */
            if (tiVerificationNeeded) 
            {
                typeInfo& tiVal = impStackTop().seTypeInfo;
                Verify(tiVal.IsObjRef() || tiVal.IsByRef() || tiVal.IsIntegerType() || tiVal.IsMethod(), "bad value");
            }

            op1  = impPopStack().val;
            type = op1->TypeGet();

            // brfalse and brtrue is only allowed on I4, refs, and byrefs.
            if (!opts.compMinOptim && !opts.compDbgCode &&
                block->bbJumpDest == block->bbNext)
            {
                block->bbJumpKind = BBJ_NONE;

                if (op1->gtFlags & GTF_GLOB_EFFECT)
                {
                    op1 = gtUnusedValNode(op1);
                    goto SPILL_APPEND;
                }
                else break;
            }

            if (op1->OperIsCompare())
            {
                if (opcode == CEE_BRFALSE || opcode == CEE_BRFALSE_S)
                {
                    // Flip the sense of the compare

                    op1 = gtReverseCond(op1);
                }
            }
            else
            {
                /* We'll compare against an equally-sized integer 0 */
                /* For small types, we always compare against int   */
                op2 = gtNewZeroConNode(genActualType(op1->gtType));

                /* Create the comparison operator and try to fold it */

                oper = (opcode==CEE_BRTRUE || opcode==CEE_BRTRUE_S) ? GT_NE
                                                                    : GT_EQ;
                op1 = gtNewOperNode(oper, TYP_INT , op1, op2);
            }

            // fall through

        COND_JUMP:

            /* Fold comparison if we can */

            op1 = gtFoldExpr(op1);

            /* Try to fold the really dumb cases like 'iconst *, ifne/ifeq'*/
            /* Don't make any blocks unreachable in import only mode */

            if ((op1->gtOper == GT_CNS_INT) &&
                ((opts.eeFlags & CORJIT_FLG_IMPORT_ONLY) == 0))
            {
                /* gtFoldExpr() should prevent this as we dont want to make any blocks
                   unreachable under compDbgCode */
                assert(!opts.compDbgCode);

                BBjumpKinds foldedJumpKind = op1->gtIntCon.gtIconVal ? BBJ_ALWAYS
                                                                     : BBJ_NONE;
                assertImp((block->bbJumpKind == BBJ_COND) // normal case
                       || (block->bbJumpKind == foldedJumpKind && impCanReimport)); // this can happen if we are reimporting the block for the second time

                block->bbJumpKind = foldedJumpKind;
#ifdef DEBUG
                if (verbose)
                {
                    if (op1->gtIntCon.gtIconVal)
                        printf("\nThe conditional jump becomes an unconditional jump to BB%02u\n",
                                                                         block->bbJumpDest->bbNum);
                    else
                        printf("\nThe block falls through into the next BB%02u\n",
                                                                         block->bbNext    ->bbNum);
                }
#endif
                break;
            }

#if HOIST_THIS_FLDS
            if  (block->bbNum >= block->bbJumpDest->bbNum)
                optHoistTFRhasLoop();
#endif

            op1 = gtNewOperNode(GT_JTRUE, TYP_VOID, op1, 0);

            /* GT_JTRUE is handled specially for non-empty stacks. See 'addStmt'
               in impImportBlock(block). For correct line numbers, spill stack. */

            if (opts.compDbgCode && impCurStmtOffs != BAD_IL_OFFSET)
                impSpillStackEnsure(true);

            goto SPILL_APPEND;


        case CEE_CEQ:    oper = GT_EQ; goto CMP_2_OPs;

        case CEE_CGT_UN:
        case CEE_CGT: oper = GT_GT; goto CMP_2_OPs;

        case CEE_CLT_UN:
        case CEE_CLT: oper = GT_LT; goto CMP_2_OPs;

CMP_2_OPs:
            if (tiVerificationNeeded) 
            {
                verVerifyCond(impStackTop(1).seTypeInfo, impStackTop().seTypeInfo, opcode);
                tiRetVal = typeInfo(TI_INT);
            }

            op2 = impPopStack().val;
            op1 = impPopStack().val;

                assertImp(genActualType(op1->TypeGet()) == genActualType(op2->TypeGet()) ||
                          varTypeIsI(op1->TypeGet()) && varTypeIsI(op2->TypeGet()) ||
                          varTypeIsFloating(op1->gtType) && varTypeIsFloating(op2->gtType));

            /* Create the comparison node */

            op1 = gtNewOperNode(oper, TYP_INT, op1, op2);

                /* REVIEW: I am settng both flags when only one is approprate */
            if (opcode==CEE_CGT_UN || opcode==CEE_CLT_UN)
                op1->gtFlags |= GTF_RELOP_NAN_UN | GTF_UNSIGNED;

            // @ISSUE :  The next opcode will almost always be a conditional
            // branch. Should we try to look ahead for it here ?

            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_BEQ_S:
        case CEE_BEQ:           oper = GT_EQ; goto CMP_2_OPs_AND_BR;

        case CEE_BGE_S:
        case CEE_BGE:           oper = GT_GE; goto CMP_2_OPs_AND_BR;

        case CEE_BGE_UN_S:
        case CEE_BGE_UN:        oper = GT_GE; goto CMP_2_OPs_AND_BR_UN;

        case CEE_BGT_S:
        case CEE_BGT:           oper = GT_GT; goto CMP_2_OPs_AND_BR;

        case CEE_BGT_UN_S:
        case CEE_BGT_UN:        oper = GT_GT; goto CMP_2_OPs_AND_BR_UN;

        case CEE_BLE_S:
        case CEE_BLE:           oper = GT_LE; goto CMP_2_OPs_AND_BR;

        case CEE_BLE_UN_S:
        case CEE_BLE_UN:        oper = GT_LE; goto CMP_2_OPs_AND_BR_UN;

        case CEE_BLT_S:
        case CEE_BLT:           oper = GT_LT; goto CMP_2_OPs_AND_BR;

        case CEE_BLT_UN_S:
        case CEE_BLT_UN:        oper = GT_LT; goto CMP_2_OPs_AND_BR_UN;

        case CEE_BNE_UN_S:
        case CEE_BNE_UN:        oper = GT_NE; goto CMP_2_OPs_AND_BR_UN;

        CMP_2_OPs_AND_BR_UN:    uns = true;  unordered = true;  goto CMP_2_OPs_AND_BR_ALL;
        CMP_2_OPs_AND_BR:       uns = false; unordered = false; goto CMP_2_OPs_AND_BR_ALL;
        CMP_2_OPs_AND_BR_ALL:

            if (tiVerificationNeeded) 
                verVerifyCond(impStackTop(1).seTypeInfo, impStackTop().seTypeInfo, opcode);

            /* Pull two values */
            op2 = impPopStack().val;
            op1 = impPopStack().val;

                            assertImp(genActualType(op1->TypeGet()) == genActualType(op2->TypeGet()) ||
                                      varTypeIsI(op1->TypeGet()) && varTypeIsI(op2->TypeGet()) ||
                                      varTypeIsFloating(op1->gtType) && varTypeIsFloating(op2->gtType));

            if (!opts.compMinOptim && !opts.compDbgCode &&
                block->bbJumpDest == block->bbNext)
            {
                block->bbJumpKind = BBJ_NONE;

                if (op1->gtFlags & GTF_GLOB_EFFECT)
                {
                    impSpillSideEffects(false, CHECK_SPILL_ALL);
                    impAppendTree(gtUnusedValNode(op1), CHECK_SPILL_NONE, impCurStmtOffs);
                }
                if (op2->gtFlags & GTF_GLOB_EFFECT)
                {
                    impSpillSideEffects(false, CHECK_SPILL_ALL);
                    impAppendTree(gtUnusedValNode(op2), CHECK_SPILL_NONE, impCurStmtOffs);
                }

#ifdef DEBUG
                if ((op1->gtFlags | op2->gtFlags) & GTF_GLOB_EFFECT)
                    impNoteLastILoffs();
#endif
                break;
            }

            /* Create and append the operator */

            op1 = gtNewOperNode(oper, TYP_INT , op1, op2);

            if (uns)
                op1->gtFlags |= GTF_UNSIGNED;

            if (unordered)
                op1->gtFlags |= GTF_RELOP_NAN_UN;

            goto COND_JUMP;


        case CEE_SWITCH:

            if (tiVerificationNeeded)
                Verify(impStackTop().seTypeInfo.IsType(TI_INT), "Bad switch val");

            /* Pop the switch value off the stack */
            op1 = impPopStack().val;

            
                assertImp(genActualType(op1->TypeGet()) == TYP_INT);
            /* We can create a switch node */

            op1 = gtNewOperNode(GT_SWITCH, TYP_VOID, op1, 0);

            val = (int)getU4LittleEndian(codeAddr);
            codeAddr += 4 + val*4; // skip over the switch-table

            goto SPILL_APPEND;

        /************************** Casting OPCODES ***************************/

        case CEE_CONV_OVF_I1:      lclTyp = TYP_BYTE ;    goto CONV_OVF;
        case CEE_CONV_OVF_I2:      lclTyp = TYP_SHORT;    goto CONV_OVF;
        case CEE_CONV_OVF_I :
        case CEE_CONV_OVF_I4:      lclTyp = TYP_INT  ;    goto CONV_OVF;
        case CEE_CONV_OVF_I8:      lclTyp = TYP_LONG ;    goto CONV_OVF;

        case CEE_CONV_OVF_U1:      lclTyp = TYP_UBYTE;    goto CONV_OVF;
        case CEE_CONV_OVF_U2:      lclTyp = TYP_CHAR ;    goto CONV_OVF;
        case CEE_CONV_OVF_U :
        case CEE_CONV_OVF_U4:      lclTyp = TYP_UINT ;    goto CONV_OVF;
        case CEE_CONV_OVF_U8:      lclTyp = TYP_ULONG;    goto CONV_OVF;

        case CEE_CONV_OVF_I1_UN:   lclTyp = TYP_BYTE ;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_I2_UN:   lclTyp = TYP_SHORT;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_I_UN :
        case CEE_CONV_OVF_I4_UN:   lclTyp = TYP_INT  ;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_I8_UN:   lclTyp = TYP_LONG ;    goto CONV_OVF_UN;

        case CEE_CONV_OVF_U1_UN:   lclTyp = TYP_UBYTE;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_U2_UN:   lclTyp = TYP_CHAR ;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_U_UN :
        case CEE_CONV_OVF_U4_UN:   lclTyp = TYP_UINT ;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_U8_UN:   lclTyp = TYP_ULONG;    goto CONV_OVF_UN;

CONV_OVF_UN:
            uns      = true;    goto CONV_OVF_COMMON;
CONV_OVF:
            uns      = false;   goto CONV_OVF_COMMON;

CONV_OVF_COMMON:
            ovfl     = true;
            goto _CONV;

        case CEE_CONV_I1:       lclTyp = TYP_BYTE  ;    goto CONV;
        case CEE_CONV_I2:       lclTyp = TYP_SHORT ;    goto CONV;
        case CEE_CONV_I:
        case CEE_CONV_I4:       lclTyp = TYP_INT   ;    goto CONV;
        case CEE_CONV_I8:       lclTyp = TYP_LONG  ;    goto CONV;

        case CEE_CONV_U1:       lclTyp = TYP_UBYTE ;    goto CONV;
        case CEE_CONV_U2:       lclTyp = TYP_CHAR  ;    goto CONV;
        case CEE_CONV_U:
        case CEE_CONV_U4:       lclTyp = TYP_UINT  ;    goto CONV;
        case CEE_CONV_U8:       lclTyp = TYP_ULONG ;    goto CONV_UN;

        case CEE_CONV_R4:       lclTyp = TYP_FLOAT;     goto CONV;
        case CEE_CONV_R8:       lclTyp = TYP_DOUBLE;    goto CONV;

        case CEE_CONV_R_UN :    lclTyp = TYP_DOUBLE;    goto CONV_UN;
    
CONV_UN:
            uns    = true; 
            ovfl   = false;
            goto _CONV;

CONV:
            uns      = false;
            ovfl     = false;
            goto _CONV;

_CONV:
                // just check that we have a number on the stack
            if (tiVerificationNeeded) {
                const typeInfo& tiVal = impStackTop().seTypeInfo;
                Verify(tiVal.IsNumberType(), "bad arg");
                tiRetVal = typeInfo(lclTyp).NormaliseForStack();
            }
            
            // only converts from FLOAT or DOUBLE to an integer type 
            //  and converts from  ULONG          to DOUBLE are morphed to calls

            if (varTypeIsFloating(lclTyp))
            {
                callNode = uns && varTypeIsLong(impStackTop().val->TypeGet());
            }
            else
            {
                callNode = varTypeIsFloating(impStackTop().val->TypeGet());
            }

            // At this point uns, ovf, callNode all set

            op1 = impPopStack().val;
            impBashVarAddrsToI(op1);

            /* Check for a worthless cast, such as "(byte)(int & 32)" */
            /* @TODO [CONSIDER] [04/16/01] []: this should be in the morpher */

            if  (varTypeIsSmall(lclTyp) && !ovfl &&
                 op1->gtType == TYP_INT && op1->gtOper == GT_AND)
            {
                op2 = op1->gtOp.gtOp2;

                if  (op2->gtOper == GT_CNS_INT)
                {
                    int         ival = op2->gtIntCon.gtIconVal;
                    int         mask, umask;

                    switch (lclTyp)
                    {
                    case TYP_BYTE :
                    case TYP_UBYTE: mask = 0x00FF; umask = 0x007F; break;
                    case TYP_CHAR :
                    case TYP_SHORT: mask = 0xFFFF; umask = 0x7FFF; break;

                    default:
                        assert(!"unexpected type");
                    }

                    if  (((ival & umask) == ival) ||
                         ((ival &  mask) == ival && uns))
                    {
                        /* Toss the cast, it's a waste of time */

                        impPushOnStack(op1, tiRetVal);
                        break;
                    }
                    else if (ival == mask)
                    {
                        /* Toss the masking, it's a waste of time, since
                           we sign-extend from the small value anyways */

                        op1 = op1->gtOp.gtOp1;

                    }
                }
            }

            /*  The 'op2' sub-operand of a cast is the 'real' type number,
                since the result of a cast to one of the 'small' integer
                types is an integer.
             */

            type = genActualType(lclTyp);

#if SMALL_TREE_NODES
            if (callNode)
                op1 = gtNewCastNodeL(type, op1, lclTyp);
            else
#endif
                op1 = gtNewCastNode (type, op1, lclTyp);

            if (ovfl)
                op1->gtFlags |= (GTF_OVERFLOW | GTF_EXCEPT);
            if (uns)
                op1->gtFlags |= GTF_UNSIGNED;
            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_NEG:
            if (tiVerificationNeeded) 
            {
                tiRetVal = impStackTop().seTypeInfo;
                Verify(tiRetVal.IsNumberType(), "Bad arg");
            }

            op1 = impPopStack().val;
                impBashVarAddrsToI(op1, NULL);
            impPushOnStack(gtNewOperNode(GT_NEG, genActualType(op1->gtType), op1), tiRetVal);
            break;

        case CEE_POP:
            if (tiVerificationNeeded)
                impStackTop(0);

            /* Pull the top value from the stack */

            op1 = impPopStack(clsHnd).val;

            /* Get hold of the type of the value being duplicated */

            lclTyp = genActualType(op1->gtType);

            /* Does the value have any side effects? */

            if  (op1->gtFlags & GTF_SIDE_EFFECT)
            {
                // Since we are throwing away the value, just normalize
                // it to its address.  This is more efficient.

                if (op1->TypeGet() == TYP_STRUCT)
                    op1 = impGetStructAddr(op1, clsHnd, CHECK_SPILL_ALL, false);

                // If 'op1' is an expression, create an assignment node.
                // Helps analyses (like CSE) to work fine.

                if (op1->gtOper != GT_CALL)
                    op1 = gtUnusedValNode(op1);

                /* Append the value to the tree list */
                goto SPILL_APPEND;
            }

            /* No side effects - just throw the <BEEP> thing away */
            break;


        case CEE_DUP:
            if (tiVerificationNeeded) {
                    // Dup could start the begining of delegate creation sequence, remember that
                delegateCreateStart = codeAddr - 1; 
                impStackTop(0);
                }

            /* Convert a (dup, stloc) sequence into a (stloc, ldloc)
               sequence so that CSE will recognize the two as
               equal (this helps eliminate a redundant bounds check
               in cases such as:  ariba[i+3] += some value; */

            if (codeAddr < codeEndp)
            {
                OPCODE nextOpcode = (OPCODE) getU1LittleEndian(codeAddr);
                if ((nextOpcode == CEE_STLOC)   ||
                    (nextOpcode == CEE_STLOC_S) ||
                    ((nextOpcode >= CEE_STLOC_0) && (nextOpcode <= CEE_STLOC_3)))
                {
                    insertLdloc = true;
                    break;
                }
            }

            /* Pull the top value from the stack */
            op1 = impPopStack(tiRetVal);
            
            /* Clone the value? */
            op1 = impCloneExpr(op1, &op2, tiRetVal.GetClassHandle(), CHECK_SPILL_ALL);

            /* Push the tree/temp back on the stack*/
            impPushOnStack(op1, tiRetVal);

            /* Push the copy on the stack */
            impPushOnStack(op2, tiRetVal);
            
            break;

        case CEE_STIND_I1:      lclTyp  = TYP_BYTE;     goto STIND;
        case CEE_STIND_I2:      lclTyp  = TYP_SHORT;    goto STIND;
        case CEE_STIND_I4:      lclTyp  = TYP_INT;      goto STIND;
        case CEE_STIND_I8:      lclTyp  = TYP_LONG;     goto STIND;
        case CEE_STIND_I:       lclTyp  = TYP_I_IMPL;   goto STIND;
        case CEE_STIND_REF:     lclTyp  = TYP_REF;      goto STIND;
        case CEE_STIND_R4:      lclTyp  = TYP_FLOAT;    goto STIND;
        case CEE_STIND_R8:      lclTyp  = TYP_DOUBLE;   goto STIND;
STIND:

            if (tiVerificationNeeded)
                verVerifySTIND(impStackTop(1).seTypeInfo, impStackTop(0).seTypeInfo, lclTyp);

STIND_POST_VERIFY:

                op2 = impPopStack().val;    // value to store
                op1 = impPopStack().val;    // address to store to
            
            // you can indirect off of a TYP_I_IMPL (if we are in C) or a BYREF
            assertImp(genActualType(op1->gtType) == TYP_I_IMPL ||
                                    op1->gtType  == TYP_BYREF);

            impBashVarAddrsToI(op1, op2);

            if (opcode == CEE_STIND_REF)
            {
                // STIND_REF can be used to store TYP_I_IMPL, TYP_REF, or TYP_BYREF
                assertImp(op2->gtType == TYP_I_IMPL || varTypeIsGC(op2->gtType));
                lclTyp = genActualType(op2->TypeGet());
            }

                // Check target type.
#ifdef DEBUG
            if (op2->gtType == TYP_BYREF || lclTyp == TYP_BYREF)
            {
                if (op2->gtType == TYP_BYREF)
                    assertImp(lclTyp == TYP_BYREF || lclTyp == TYP_I_IMPL);
                else if (lclTyp == TYP_BYREF)
                    assertImp(op2->gtType == TYP_BYREF ||op2->gtType == TYP_I_IMPL);
            }
            else
                assertImp(genActualType(op2->gtType) == genActualType(lclTyp) || 
                      varTypeIsFloating(op2->gtType) && varTypeIsFloating(lclTyp));
#endif

            op1 = gtNewOperNode(GT_IND, lclTyp, op1);

            // stind could point anywhere, example a boxed class static int
            op1->gtFlags |= GTF_IND_TGTANYWHERE;

            if (prefixFlags & PREFIX_VOLATILE)
            {
                op1->gtFlags |= GTF_DONT_CSE;
            }

            op1 = gtNewAssignNode(op1, op2);
            op1->gtFlags |= GTF_EXCEPT | GTF_GLOB_REF;

            // Spill side-effects AND global-data-accesses
            if (verCurrentState.esStackDepth > 0)
                impSpillSideEffects(true, CHECK_SPILL_ALL);

            goto APPEND;


        case CEE_LDIND_I1:      lclTyp  = TYP_BYTE;     goto LDIND;
        case CEE_LDIND_I2:      lclTyp  = TYP_SHORT;    goto LDIND;
        case CEE_LDIND_U4:
        case CEE_LDIND_I4:      lclTyp  = TYP_INT;      goto LDIND;
        case CEE_LDIND_I8:      lclTyp  = TYP_LONG;     goto LDIND;
        case CEE_LDIND_REF:     lclTyp  = TYP_REF;      goto LDIND;
        case CEE_LDIND_I:       lclTyp  = TYP_I_IMPL;   goto LDIND;
        case CEE_LDIND_R4:      lclTyp  = TYP_FLOAT;    goto LDIND;
        case CEE_LDIND_R8:      lclTyp  = TYP_DOUBLE;   goto LDIND;
        case CEE_LDIND_U1:      lclTyp  = TYP_UBYTE;    goto LDIND;
        case CEE_LDIND_U2:      lclTyp  = TYP_CHAR;     goto LDIND;
LDIND:

            if (tiVerificationNeeded)
            {
                tiRetVal = verVerifyLDIND(impStackTop().seTypeInfo, lclTyp);
                tiRetVal.NormaliseForStack();
                }
                
                op1 = impPopStack().val;    // address to load from
            impBashVarAddrsToI(op1);

            assertImp(genActualType(op1->gtType) == TYP_I_IMPL ||
                                    op1->gtType  == TYP_BYREF);

            op1 = gtNewOperNode(GT_IND, lclTyp, op1);

            // ldind could point anywhere, example a boxed class static int
            op1->gtFlags |= (GTF_EXCEPT | GTF_GLOB_REF | GTF_IND_TGTANYWHERE);

            if (prefixFlags & PREFIX_VOLATILE)
            {
                op1->gtFlags |= GTF_DONT_CSE;
            }

            impPushOnStack(op1, tiRetVal);

            break;


        case CEE_UNALIGNED:
            val = getU1LittleEndian(codeAddr);
#ifdef DEBUG
            if (verbose) printf(" %u", val);
#endif
#if !TGT_x86
            assert(!"CEE_UNALIGNED NYI for risc");
#endif
            prefixFlags |= PREFIX_UNALIGNED;    // currently unused
            assert(sz == 1);
            ++codeAddr;
            continue;


        case CEE_VOLATILE:
            prefixFlags |= PREFIX_VOLATILE;
            assert(sz == 0);
            continue;

        case CEE_LDFTN:

            // Need to do a lookup here so that we perform an access check
            // and do a NOWAY if protections are violated
            memberRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);

#ifdef DEBUG
            if (verbose) printf(" %08X", memberRef);
#endif

            methHnd   = eeFindMethod(memberRef, info.compScopeHnd, info.compMethodHnd);
            info.compCompHnd->getMethodSig(methHnd, &sig);
            if  (sig.callConv & CORINFO_CALLCONV_PARAMTYPE)
                NO_WAY("Currently do not support LDFTN of Parameterized functions");

            if (tiVerificationNeeded) {
                    // LDFTN could start the begining of delegate creation sequence, remember that
                delegateCreateStart = codeAddr - 2; 
                Verify(info.compCompHnd->canAccessMethod(info.compMethodHnd, //from
                                                                      methHnd, // what 
                                                                      info.compClassHnd), "Can't access method");
                Verify(!(eeGetMethodAttribs(methHnd) & CORINFO_FLG_CONSTRUCTOR), "LDFTN on a constructor");
            }

        DO_LDFTN:
            // @TODO [REVISIT] [04/16/01] []: use the handle instead of the token.
            op1 = gtNewIconHandleNode(memberRef, GTF_ICON_FTN_ADDR, (unsigned)info.compScopeHnd);
            op1->gtVal.gtVal2 = (unsigned)info.compScopeHnd;
            op1->SetOper(GT_FTN_ADDR);
            op1->gtType = TYP_I_IMPL;

            impPushOnStack(op1, typeInfo(methHnd));
            break;

        case CEE_LDVIRTFTN:

            /* Get the method token */

            memberRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);

#ifdef DEBUG
            if (verbose) printf(" %08X", memberRef);
#endif
            methHnd   = eeFindMethod(memberRef, info.compScopeHnd, info.compMethodHnd);
            info.compCompHnd->getMethodSig(methHnd, &sig);
            if  (sig.callConv & CORINFO_CALLCONV_PARAMTYPE)
                NO_WAY("Currently do not support LDFTN of Parameterized functions");

            mflags = eeGetMethodAttribs(methHnd);

            /* Get the object-ref */
            if (tiVerificationNeeded)
            {
                CORINFO_SIG_INFO        sig;
                eeGetMethodSig(methHnd, &sig, false);
                Verify(sig.hasThis(), "ldvirtftn on a static method");
                Verify(!(mflags & CORINFO_FLG_CONSTRUCTOR), "LDVIRTFTN on a constructor");

                typeInfo declType =  verMakeTypeInfo(eeGetMethodClass(methHnd));
                typeInfo arg = impStackTop().seTypeInfo;
                Verify(arg.IsType(TI_REF) && tiCompatibleWith(arg, declType), "bad ldvirtftn");
                Verify(info.compCompHnd->canAccessMethod(info.compMethodHnd, //from
                                                         methHnd,
                                                         arg.GetClassHandleForObjRef()), "bad ldvirtftn");
            }

                op1 = impPopStack().val;
            assertImp(op1->gtType == TYP_REF);

            if (mflags & (CORINFO_FLG_FINAL|CORINFO_FLG_STATIC) || !(mflags & CORINFO_FLG_VIRTUAL))
            {
                if  (op1->gtFlags & GTF_SIDE_EFFECT)
                {
                    op1 = gtUnusedValNode(op1);
                    impAppendTree(op1, CHECK_SPILL_ALL, impCurStmtOffs);
                }
                goto DO_LDFTN;
            }

            clsFlags = eeGetClassAttribs(eeGetMethodClass(methHnd));

            // If the method has been added via EnC, then it wont exit
            // in the original vtable. So use a helper which will resolve it.

            if ((mflags & CORINFO_FLG_EnC) && !(clsFlags & CORINFO_FLG_INTERFACE))
            {
                op1 = gtNewHelperCallNode(CORINFO_HELP_EnC_RESOLVEVIRTUAL, TYP_I_IMPL, GTF_EXCEPT);

                impPushOnStack(op1, typeInfo(methHnd));

                break;
            }

            /* get the vtable-ptr from the object */

            op1 = gtNewOperNode(GT_IND, TYP_I_IMPL, op1);

            op1->gtFlags |= GTF_EXCEPT; // Null-pointer exception

            op1 = gtNewOperNode(GT_VIRT_FTN, TYP_I_IMPL, op1);
            op1->gtVal.gtVal2 = unsigned(methHnd);

            /* @TODO [REVISIT] [04/16/01] []: this shouldn't be marked as a call anymore */

            if (clsFlags & CORINFO_FLG_INTERFACE)
                op1->gtFlags |= GTF_CALL_INTF | GTF_CALL;

            impPushOnStack(op1, typeInfo(methHnd));
            break;

        case CEE_TAILCALL:
#ifdef DEBUG
            if (verbose) printf(" tail.");
#endif
            prefixFlags |= PREFIX_TAILCALL;
            assert(sz == 0);
            continue;

        case CEE_NEWOBJ:
            /* Since we will implicitly insert thisPtr at the start of the
               argument list, spill any GTF_OTHER_SIDEEFF */
            impSpillSpecialSideEff();

            /* NEWOBJ does not respond to TAIL */
            prefixFlags &= ~PREFIX_TAILCALL;

            memberRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);

            methHnd = eeFindMethod(memberRef, info.compScopeHnd, info.compMethodHnd);
            if (!methHnd) BADCODE("no constructor for newobj found?");

            mflags = eeGetMethodAttribs(methHnd);

            if ((mflags & (CORINFO_FLG_STATIC|CORINFO_FLG_ABSTRACT)) != 0)
                BADCODE("newobj on static or abstract method");

            clsHnd = eeGetMethodClass(methHnd);

            // There are three different cases for new
            // Object size is variable (depends on arguments)
            //      1) Object is an array (arrays treated specially by the EE)
            //      2) Object is some other variable sized object (e.g. String)
            // 3) Class Size can be determined beforehand (normal case
            // In the first case, we need to call a NEWOBJ helper (multinewarray)
            // in the second case we call the constructor with a '0' this pointer
            // In the third case we alloc the memory, then call the constuctor

            clsFlags = eeGetClassAttribs(clsHnd);
            if (clsFlags & CORINFO_FLG_ARRAY)
            {
                if (tiVerificationNeeded)
                {
                    CORINFO_CLASS_HANDLE elemTypeHnd;
                    CorInfoType corType = info.compCompHnd->getChildType(clsHnd, &elemTypeHnd);
                    assert(!(elemTypeHnd == 0 && corType == CORINFO_TYPE_VALUECLASS));
                    Verify(elemTypeHnd == 0 || !(eeGetClassAttribs(elemTypeHnd) & CORINFO_FLG_CONTAINS_STACK_PTR),
                        "newarr of byref-like objects");
                    verVerifyCall(opcode,
                                  memberRef,
                                  ((prefixFlags & PREFIX_TAILCALL) != 0), 
                                  delegateCreateStart,
                                  codeAddr - 1
                                  DEBUGARG(info.compFullName));
                }

                // Arrays need to call the NEWOBJ helper.
                assertImp(clsFlags & CORINFO_FLG_VAROBJSIZE);

                /* The varargs helper needs the scope and method token as last
                   and  last-1 param (this is a cdecl call, so args will be
                   pushed in reverse order on the CPU stack) */

                op1 = gtNewIconEmbScpHndNode(info.compScopeHnd);
                op1 = gtNewOperNode(GT_LIST, TYP_VOID, op1);

                op2 = gtNewIconNode(memberRef, TYP_INT);
                op2 = gtNewOperNode(GT_LIST, TYP_VOID, op2, op1);

                eeGetMethodSig(methHnd, &sig);
                assertImp(sig.numArgs);

                flags = 0;
                op2 = impPopList(sig.numArgs, &flags, &sig, op2);

                op1 = gtNewHelperCallNode(  CORINFO_HELP_NEWOBJ,
                                            TYP_REF, 0,
                                            op2 );

                // varargs, so we pop the arguments
                op1->gtFlags |= GTF_CALL_POP_ARGS;

#ifdef DEBUG
                // At the present time we don't track Caller pop arguments
                // that have GC references in them
                GenTreePtr temp = op2;
                while(temp != 0)
                {
                    assertImp(temp->gtOp.gtOp1->gtType != TYP_REF);
                    temp = temp->gtOp.gtOp2;
                }
#endif
                op1->gtFlags |= op2->gtFlags & GTF_GLOB_EFFECT;

                clsHnd = info.compCompHnd->findMethodClass(info.compScopeHnd, memberRef);
                impPushOnStack(op1, typeInfo(TI_REF, clsHnd));
                break;
            }
            else if (clsFlags & CORINFO_FLG_VAROBJSIZE)
            {
                // This is the case for varible sized objects that are not
                // arrays.  In this case, call the constructor with a null 'this'
                // pointer
                thisPtr = gtNewIconNode(0, TYP_REF);
            }
            else
            {
                // This is the normal case where the size of the object is
                // fixed.  Allocate the memory and call the constructor.

                /* get a temporary for the new object */
                lclNum = lvaGrabTemp();

                if (clsFlags & CORINFO_FLG_VALUECLASS)
                {
                    CorInfoType jitTyp = info.compCompHnd->asCorInfoType(clsHnd);
                    if (CORINFO_TYPE_BOOL <= jitTyp && jitTyp <= CORINFO_TYPE_DOUBLE)
                        lvaTable[lclNum].lvType  = JITtype2varType(jitTyp);
                    else
                    {
                        // The local variable itself is the allocated space
                        lvaSetStruct(lclNum, clsHnd);
                    }

                    // @TODO [REVISIT] [04/16/01] []: why do we care if the class is unmanged, 
                    // the local always points to the stack
                    lclTyp = TYP_BYREF;

                    lvaTable[lclNum].lvAddrTaken = 1;
                    thisPtr = gtNewOperNode(GT_ADDR, lclTyp, gtNewLclvNode(lclNum, lvaTable[lclNum].TypeGet())); 
                    thisPtr->gtFlags |= GTF_ADDR_ONSTACK;
                }
                else
                {
                    // Can we directly access the class handle ?

                    op1 = gtNewIconEmbClsHndNode(clsHnd);

                    op1 = gtNewHelperCallNode(  eeGetNewHelper(clsHnd, info.compMethodHnd),
                                                TYP_REF, 0,
                                                gtNewArgList(op1));

                    /* Append the assignment to the temp/local. Dont need to spill
                       at all as we are just calling an EE-Jit helper which can only
                       cause an (async) OutOfMemoryException */

                    impAssignTempGen(lclNum, op1, CHECK_SPILL_NONE);

                    thisPtr = gtNewLclvNode(lclNum, TYP_REF);
                }

            }

            goto CALL;

        case CEE_CALLI:

            /* Get the call sig */

            memberRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
            goto CALL;

        case CEE_CALLVIRT:
        case CEE_CALL:

            /* Get the method token */

            memberRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);

    CALL:   // memberRef should be set.
            // thisPtr should be set for CEE_NEWOBJ

#ifdef DEBUG
            if (verbose) printf(" %08X", memberRef);
#endif
            tailCall = (prefixFlags & PREFIX_TAILCALL) != 0;
            if (tiVerificationNeeded) 
                verVerifyCall(opcode,
                              memberRef,
                              tailCall, 
                              delegateCreateStart,
                              codeAddr - 1
                              DEBUGARG(info.compFullName));
                
            callTyp = impImportCall(opcode, memberRef, thisPtr, tailCall);

            if (tailCall)
                goto RET;

            break;


        case CEE_LDFLD:
        case CEE_LDSFLD:
        case CEE_LDFLDA:
        case CEE_LDSFLDA: {

            BOOL isStaticField;
            BOOL isStaticOpcode = (opcode == CEE_LDSFLD || opcode == CEE_LDSFLDA);
            BOOL isLoadAddress  = (opcode == CEE_LDFLDA || opcode == CEE_LDSFLDA);

            /* Get the CP_Fieldref index */
            assertImp(sz == sizeof(unsigned));
            memberRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf(" %08X", memberRef);
#endif

            fldHnd = eeFindField(memberRef, info.compScopeHnd, 
                                            info.compMethodHnd);

            /* Figure out the type of the member */
            CorInfoType ciType = info.compCompHnd->getFieldType(fldHnd, &clsHnd);
            lclTyp = JITtype2varType(ciType);

            /* Preserve 'small' int types */
            if  (lclTyp > TYP_INT)
                lclTyp = genActualType(lclTyp);

            CORINFO_ACCESS_FLAGS  aflags          = CORINFO_ACCESS_ANY;
            GenTreePtr            obj             = NULL;
            typeInfo*             tiObj           = NULL;
            CORINFO_CLASS_HANDLE  objType;                 // used for fields

            if  (!isStaticOpcode)   // LDFLD or LDFLDA
            {
                StackEntry se = impPopStack();
                    obj = se.val;
                tiObj = &se.seTypeInfo;
                objType = tiObj->GetClassHandle();

                if (impIsThis(obj))
                {
                    aflags = CORINFO_ACCESS_THIS;

                    // An optimization for Contextful classes:
                    // we unwrap the proxy when we have a 'this reference'

                    if (info.compUnwrapContextful && info.compIsContextful)
                        aflags = (CORINFO_ACCESS_FLAGS) (CORINFO_ACCESS_THIS | CORINFO_ACCESS_UNWRAP);
                }
            }

            /* Get hold of the flags for the member */
            mflags = eeGetFieldAttribs(fldHnd, aflags);
            isStaticField = mflags & CORINFO_FLG_STATIC;
            if (tiVerificationNeeded)
            {
                    // You can also pass the unboxed struct to  LDFLD
                if (opcode == CEE_LDFLD && tiObj->IsValueClass()) 
                    tiObj->MakeByRef();
                verVerifyField(fldHnd, tiObj, mflags, isLoadAddress);
                tiRetVal = verMakeTypeInfo(ciType, clsHnd);
                if (isLoadAddress) 
                    tiRetVal.MakeByRef();
                else
                    tiRetVal.NormaliseForStack();
            }
            else if (lclTyp == TYP_STRUCT)
                tiRetVal = typeInfo(TI_STRUCT, clsHnd);

            /* Morph an RVA into a constant address if possible */

            if (opcode == CEE_LDSFLDA &&
                (mflags & (CORINFO_FLG_UNMANAGED|CORINFO_FLG_TLS)) == CORINFO_FLG_UNMANAGED)
            {
                //
                // If we can we access the static's address directly
                // then pFldAddr will be NULL and
                //      fldAddr will be the actual address of the static field
                //
                void **  pFldAddr = NULL;
                void *    fldAddr = eeGetFieldAddress(fldHnd, &pFldAddr);

                if (pFldAddr == NULL)
                {
                    op1 = gtNewIconHandleNode((long)fldAddr, GTF_ICON_PTR_HDL);
                    impPushOnStack(op1, tiRetVal);
                    break;
                }
            }

            if  (!isStaticOpcode)   // LDFLD or LDFLDA
            {
                if (obj->TypeGet() != TYP_STRUCT)
                    obj = impCheckForNullPointer(obj);

                if (isStaticField)
                {
                    if (obj->gtFlags & GTF_SIDE_EFFECT)
                    {
                        /* We are using ldfld/a on a static field. We allow
                           it, but need to get side-effect from obj */
                        obj = gtUnusedValNode(obj);
                        impAppendTree(obj, CHECK_SPILL_ALL, impCurStmtOffs);
                    }
                    obj = 0;
                }
                else
                {
                    // If the object is a struct, what we really want is
                    // for the field to operate on the address of the struct.
                    if (obj->TypeGet() == TYP_STRUCT)
                    {
                        assert(opcode == CEE_LDFLD);
                        obj = impGetStructAddr(obj, objType, CHECK_SPILL_ALL, true);
                    }
                }
            }

            /* Does this field need a helper call? */

            if  (mflags & CORINFO_FLG_HELPER)
            {
                CorInfoFieldAccess access = CORINFO_GET;
                if (isLoadAddress)
                {
                    access = CORINFO_ADDRESS;
                    lclTyp = TYP_BYREF;
                }
                op1 = gtNewRefCOMfield(obj, access, memberRef, lclTyp, clsHnd, 0);
            }
            else if ((mflags & CORINFO_FLG_EnC) && !isStaticField)
            {
                /* We call a helper function to get the address of the
                   EnC-added non-static field. */

                op1 = gtNewOperNode(GT_LIST,
                                     TYP_VOID,
                                     gtNewIconEmbFldHndNode(fldHnd,
                                                            memberRef,
                                                            info.compScopeHnd));

                op1 = gtNewOperNode(GT_LIST, TYP_VOID, obj, op1);

                op1 = gtNewHelperCallNode(  CORINFO_HELP_GETFIELDADDR,
                                            TYP_BYREF,
                                            (obj->gtFlags & GTF_GLOB_EFFECT) | GTF_EXCEPT,
                                            op1);

                assertImp(!isStaticOpcode);
                if (opcode == CEE_LDFLD)
                    op1 = gtNewOperNode(GT_IND, lclTyp, op1);
            }
            else
            {
                CORINFO_CLASS_HANDLE fldClass;
                DWORD                fldClsAttr;

                if ((isStaticField && !isLoadAddress && (mflags & CORINFO_FLG_FINAL)) &&
                    (varTypeIsIntegral(lclTyp) || varTypeIsFloating(lclTyp)) && !(mflags & CORINFO_FLG_STATIC_IN_HEAP))
                {
                    fldClass   = eeGetFieldClass(fldHnd);
                    fldClsAttr = eeGetClassAttribs(fldClass);
                    
                    // Since the class is marked with the INITIALIZED flag
                    // prior to running the class constructor we have to
                    // check if we are currently jitting the class constructor

                    bool inFldClassConstructor = ((info.compClassHnd == fldClass)            &&
                                                  (info.compFlags & CORINFO_FLG_CONSTRUCTOR) &&
                                                  (info.compFlags & CORINFO_FLG_STATIC));

                    // for the standalone jit CORINFO_FLG_INITIALIZED is always true,
                    // but we can't really perform the readonly optimization, 
                    // so we must also check for the new flag CORJIT_FLG_PREJIT

                    if (!inFldClassConstructor &&
                        !(opts.eeFlags & CORJIT_FLG_PREJIT) &&
                        ((fldClsAttr & CORINFO_FLG_INITIALIZED) ||
                         info.compCompHnd->initClass(fldClass, info.compMethodHnd)))
                    {
                        // We should always be able to access this static's address directly
                        void **  pFldAddr = NULL;
                        void *   fldAddr  = NULL;
                        fldAddr = eeGetFieldAddress(fldHnd, &pFldAddr);
                        assert(pFldAddr == NULL);

                        switch (lclTyp) {
                            int      ival;
                            __int64  lval;
                            double   dval;

                        case TYP_BOOL:
                            ival = *((bool *) fldAddr);
                            assert((ival == 0) || (ival == 1));
                            goto IVAL_COMMON;

                        case TYP_BYTE:
                            ival = *((signed char *) fldAddr);
                            goto IVAL_COMMON;

                        case TYP_UBYTE:
                            ival = *((unsigned char *) fldAddr);
                            goto IVAL_COMMON;

                        case TYP_SHORT:
                            ival = *((short *) fldAddr);
                            goto IVAL_COMMON;

                        case TYP_CHAR:
                        case TYP_USHORT:
                            ival = *((unsigned short *) fldAddr);
                            goto IVAL_COMMON;

                        case TYP_UINT:
                        case TYP_INT:
                            ival = *((int *) fldAddr);
IVAL_COMMON:
                            op1 = gtNewIconNode(ival);
                            break;

                        case TYP_LONG:
                        case TYP_ULONG:
                            lval = *((__int64 *) fldAddr);
                            op1 = gtNewLconNode(lval);
                            break;

                        case TYP_FLOAT:
                            dval = *((float *) fldAddr);
                            goto DVAL_COMMON;

                        case TYP_DOUBLE:
                            dval = *((double *) fldAddr);
DVAL_COMMON:
                            op1 = gtNewDconNode(dval);
                            break;

                        default:
                            assert(!"Unexpected lclTyp");
                            break;
                        }
                        goto FIELD_DONE;
                    }
                }
                
                assert((lclTyp != TYP_BYREF) && "Illegal to have an field of TYP_BYREF");

                /* Create the data member node */
                op1 = gtNewFieldRef(lclTyp, fldHnd);

                if (mflags & CORINFO_FLG_TLS)   // fpMorphField will handle the transformation
                    op1->gtFlags |= GTF_IND_TLS_REF;

                /* Is this an instance field */
                if  (!isStaticField)
                {
                    obj = impCheckForNullPointer(obj);

                    if (obj == 0)                  BADCODE("LDSFLD done on an instance field.");
                    if (mflags & CORINFO_FLG_TLS)  BADCODE("instance field can not be a TLS ref.");

                    op1->gtField.gtFldObj = obj;

#if HOIST_THIS_FLDS
                    if  (obj->gtOper == GT_LCL_VAR && obj->gtLclVar.gtLclNum == 0)
                        optHoistTFRrecRef(fldHnd, op1);
#endif

                    op1->gtFlags |= (obj->gtFlags & GTF_GLOB_EFFECT) | GTF_EXCEPT;

                    // If gtFldObj is a BYREF then our target is a value class and
                    // it could point anywhere, example a boxed class static int
                    if (obj->gtType == TYP_BYREF)
                        op1->gtFlags |= GTF_IND_TGTANYWHERE;

                    // wrap it in a address of operator if necessary
                    if (opcode == CEE_LDFLDA)
                    {
                        INDEBUG(clsHnd = BAD_CLASS_HANDLE;)
                        op1 = gtNewOperNode(GT_ADDR, varTypeIsGC(obj->TypeGet()) ?
                                                     TYP_BYREF : TYP_I_IMPL, op1);
                    }
                }
                else
                {
                    /* This is a static field */
                    assert(isStaticField);

                    fldClass   = eeGetFieldClass(fldHnd);
                    fldClsAttr = eeGetClassAttribs(fldClass);
                    
                    bool mgdField = (mflags     & CORINFO_FLG_UNMANAGED) == 0;

                    if (mflags & CORINFO_FLG_SHARED_HELPER)   
                        op1->gtFlags |= GTF_IND_SHARED;

                        // The EE gives us the 
                        // handle to the boxed object. We then access the unboxed object from it.
                        // Remove when our story for static value classes changes.

                    if (mflags & CORINFO_FLG_STATIC_IN_HEAP)
                    {
                        assert(mgdField);
                        op1->gtType = TYP_REF;          // points at boxed object
                        op2 = gtNewIconNode(sizeof(void*), TYP_I_IMPL);
                        op1 = gtNewOperNode(GT_ADD, TYP_BYREF, op1, op2);
                        op1 = gtNewOperNode(GT_IND, lclTyp, op1);
                    }

                    if (prefixFlags & PREFIX_VOLATILE)
                        op1->gtFlags |= GTF_DONT_CSE;

                    // wrap it in a address of operator if necessary
                    if (isLoadAddress)
                    {
                        INDEBUG(clsHnd = BAD_CLASS_HANDLE;)
                        op1 = gtNewOperNode(GT_ADDR,
                                            mgdField ? TYP_BYREF : TYP_I_IMPL,
                                            op1);

                        // &clsVar doesnt need GTF_GLOB_REF, though clsVar does
                        if (isStaticField)
                            op1->gtFlags &= ~GTF_GLOB_REF;
                    }

                    /* For static fields check if the class is initialized or 
                     * is our current class, or uses the shared helper
                     * otherwise create the helper call node */

                    if ((info.compClassHnd != fldClass)         &&
                        !(mflags & CORINFO_FLG_SHARED_HELPER)   &&
                        !(fldClsAttr & CORINFO_FLG_INITIALIZED) &&
                        (fldClsAttr & CORINFO_FLG_NEEDS_INIT)   &&
                        !info.compCompHnd->initClass(fldClass, info.compMethodHnd))
                    {
                        GenTreePtr  helperNode;
                        helperNode = gtNewIconEmbClsHndNode(fldClass,
                                                            memberRef,
                                                            info.compScopeHnd);

                        helperNode = gtNewHelperCallNode(CORINFO_HELP_INITCLASS,
                                                         TYP_VOID, 0,
                                                         gtNewArgList(helperNode));

                        if (prefixFlags & PREFIX_VOLATILE)
                            op1->gtFlags |= GTF_DONT_CSE;
                        op1 = gtNewOperNode(GT_COMMA, op1->TypeGet(), helperNode, op1);
                    }
                }
            }

            // Set GTF_GLOB_REF for all static field accesses
            if (!isLoadAddress && isStaticField)
                op1->gtFlags |= GTF_GLOB_REF;

            // If this assert goes of we need to make certain we are putting
            // the GTF_DONT_CSE on the correct node.
            assert(op1->gtOper == GT_IND   || op1->gtOper == GT_CALL  ||
                   op1->gtOper == GT_FIELD || op1->gtOper == GT_COMMA ||
                   op1->gtOper == GT_ADDR  || op1->gtOper == GT_LDOBJ);

            if (prefixFlags & PREFIX_VOLATILE)
                op1->gtFlags |= GTF_DONT_CSE;

FIELD_DONE:
            impPushOnStack(op1, tiRetVal);
            }
            break;

        case CEE_STFLD:
        case CEE_STSFLD: {

            BOOL        isStaticField;
            BOOL        isStaticOpcode = opcode == CEE_STSFLD;
            StackEntry  se1;
            StackEntry  se2;
            CORINFO_CLASS_HANDLE fieldClsHnd; // class of the field (if it's a ref type)

            /* Get the CP_Fieldref index */

            assertImp(sz == sizeof(unsigned));
            memberRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf(" %08X", memberRef);
#endif
            
            fldHnd = eeFindField(memberRef, info.compScopeHnd, 
                                            info.compMethodHnd);

            /* Figure out the type of the member */
            CorInfoType ciType = info.compCompHnd->getFieldType(fldHnd, &fieldClsHnd);
            lclTyp = JITtype2varType(ciType);

            CORINFO_ACCESS_FLAGS  aflags   = CORINFO_ACCESS_ANY;
            GenTreePtr            obj      = NULL;
            typeInfo*           tiObj      = NULL;
            typeInfo            tiVal;

                /* Pull the value from the stack */
            op2 = impPopStack(tiVal);
            clsHnd = tiVal.GetClassHandle();

            if (!isStaticOpcode) 
            {
                StackEntry se = impPopStack();
                obj = se.val;
                tiObj = &se.seTypeInfo;
            }

            if  (!isStaticOpcode && impIsThis(obj))
            {
                aflags = CORINFO_ACCESS_THIS;

                // An optimization for Contextful classes:
                // we unwrap the proxy when we have a 'this reference'

                if (info.compUnwrapContextful && info.compIsContextful)
                {
                    aflags = (CORINFO_ACCESS_FLAGS) (CORINFO_ACCESS_THIS | CORINFO_ACCESS_UNWRAP);
                }
            }

            mflags  = eeGetFieldAttribs(fldHnd, aflags);

            isStaticField = mflags & CORINFO_FLG_STATIC;

            if (tiVerificationNeeded)
            {
                verVerifyField(fldHnd, tiObj, mflags, TRUE);
                typeInfo fieldType = verMakeTypeInfo(ciType, fieldClsHnd);
                Verify(tiCompatibleWith(tiVal, fieldType.NormaliseForStack()), "type mismatch");
            }

            /* Preserve 'small' int types */
            if  (lclTyp > TYP_INT)
                lclTyp = genActualType(lclTyp);

            /* Is this a 'special' (COM) field? */

            if  (mflags & CORINFO_FLG_HELPER)
            {
                op1 = gtNewRefCOMfield(obj, CORINFO_SET, memberRef, lclTyp, clsHnd, op2);
                goto SPILL_APPEND;
            }

            assert((lclTyp != TYP_BYREF) && "Illegal to have an field of TYP_BYREF");

            if ((mflags & CORINFO_FLG_EnC) && !isStaticField)
            {
                /* We call a helper function to get the address of the
                   EnC-added non-static field. */
                op1 = gtNewOperNode(GT_LIST,
                                     TYP_VOID,
                                     gtNewIconEmbFldHndNode(fldHnd,
                                                            memberRef,
                                                            info.compScopeHnd));

                op1 = gtNewOperNode(GT_LIST, TYP_VOID, obj, op1);

                op1 = gtNewHelperCallNode(  CORINFO_HELP_GETFIELDADDR,
                                            TYP_BYREF,
                                            (obj->gtFlags & GTF_GLOB_EFFECT) | GTF_EXCEPT,
                                            op1);

                op1 = gtNewOperNode(GT_IND, lclTyp, op1);
            }
            else
            {
                /* Create the data member node */

                op1 = gtNewFieldRef(lclTyp, fldHnd);

                if (mflags & CORINFO_FLG_TLS)   // fpMorphField will handle the transformation
                    op1->gtFlags |= GTF_IND_TLS_REF;

                /* Is this a static field */
                if  (isStaticField)
                {
                    if (!isStaticOpcode)
                    {
                        assert(obj);

                        // We are using stfld on a static field.
                        // We allow it, but need to eval any side-effects for obj

                        if (obj->gtFlags & GTF_SIDE_EFFECT)
                        {
                            obj = gtUnusedValNode(obj);
                            impAppendTree(obj, CHECK_SPILL_ALL, impCurStmtOffs);
                        }
                    }


                    bool mgdField = (mflags     & CORINFO_FLG_UNMANAGED) == 0;

                    if (mflags & CORINFO_FLG_SHARED_HELPER)   
                        op1->gtFlags |= GTF_IND_SHARED;

                        // The EE gives us the 
                        // handle to the boxed object. We then access the unboxed object from it.
                        // Remove when our story for static value classes changes.
                    if (mflags & CORINFO_FLG_STATIC_IN_HEAP)
                    {
                        assert(mgdField);
                        op1->gtType = TYP_REF; // points at boxed object
                        op1 = gtNewOperNode(GT_ADD, TYP_BYREF, 
                                            op1, gtNewIconNode(sizeof(void*), TYP_I_IMPL));
                        op1 = gtNewOperNode(GT_IND, lclTyp, op1);
                    }

                    if (mflags & CORINFO_FLG_SHARED_HELPER)   
                        op1->gtFlags |= GTF_IND_SHARED;
                }
                else
                {
                    if (obj == 0)                  BADCODE("STSFLD done on an instance field.");
                    if (mflags & CORINFO_FLG_TLS)  BADCODE("instance field can not be a TLS ref.");

                    /* This an instance field */
                    obj = impCheckForNullPointer(obj);

                    op1->gtField.gtFldObj = obj;
#if HOIST_THIS_FLDS
                    if  (obj->gtOper == GT_LCL_VAR && !obj->gtLclVar.gtLclNum)
                        optHoistTFRrecDef(fldHnd, op1);
#endif
                    op1->gtFlags |= (obj->gtFlags & GTF_GLOB_EFFECT) | GTF_EXCEPT;

                    // If gtFldObj is a BYREF then our target is a value class and
                    // it could point anywhere, example a boxed class static int
                    if (obj->gtType == TYP_BYREF)
                        op1->gtFlags |= GTF_IND_TGTANYWHERE;
                }
            }

            /* Create the member assignment */
            if (lclTyp == TYP_STRUCT)
                op1 = impAssignStruct(op1, op2, clsHnd, CHECK_SPILL_ALL);
            else
                op1 = gtNewAssignNode(op1, op2);

            /* Mark the expression as containing an assignment */

            op1->gtFlags |= GTF_ASG;

            if  (isStaticField)
            {
                /* For static fields check if the class is initialized or is our current class
                 * otherwise create the helper call node */
                CORINFO_CLASS_HANDLE fldClass   = eeGetFieldClass(fldHnd);
                DWORD                fldClsAttr = eeGetClassAttribs(fldClass);

                if ((info.compClassHnd != fldClass) && 
                    !(mflags & CORINFO_FLG_SHARED_HELPER)     && 
                    !(fldClsAttr & CORINFO_FLG_INITIALIZED)   &&
                    (fldClsAttr & CORINFO_FLG_NEEDS_INIT)   &&
                    !info.compCompHnd->initClass(fldClass, info.compMethodHnd))
                {
                    GenTreePtr  helperNode;

                    helperNode = gtNewIconEmbClsHndNode(fldClass,
                                                        memberRef,
                                                        info.compScopeHnd);

                    helperNode = gtNewHelperCallNode(CORINFO_HELP_INITCLASS,
                                                     TYP_VOID, 0,
                                                     gtNewArgList(helperNode));

                    op1 = gtNewOperNode(GT_COMMA, op1->TypeGet(), helperNode, op1);
                }
            }

            /* stsfld can interfere with value classes (consider the sequence
               ldloc, ldloca, ..., stfld, stloc).  We will be conservative and
               spill all value class references from the stack. */

            if (obj && ((obj->gtType == TYP_BYREF) || (obj->gtType == TYP_I_IMPL)))
            {
                impSpillValueClasses();
            }

            /* Spill any refs to the same member from the stack */

            impSpillLclRefs((int)fldHnd);

            /* stsfld also interferes with indirect accesses (for aliased
               statics) and calls. But dont need to spill other statics
               as we have explicitly spilled this particular static field. */

            impSpillSideEffects(false, CHECK_SPILL_ALL);

            }
            goto APPEND;


        case CEE_NEWARR: {

            /* Get the class type index operand */

            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf(" %08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);

            CORINFO_CLASS_HANDLE arrayClsHnd = info.compCompHnd->getSDArrayForClass(clsHnd);
            if (arrayClsHnd == 0)
                NO_WAY("Can't get array class");

            if (tiVerificationNeeded)
            {
                Verify(impStackTop().seTypeInfo.IsType(TI_INT), "bad bound");
                Verify(!(eeGetClassAttribs(clsHnd) & CORINFO_FLG_CONTAINS_STACK_PTR), "array of byref-like type");
                tiRetVal = verMakeTypeInfo(arrayClsHnd);
            }

            /* Form the arglist: array class handle, size */
            op2 = impPopStack().val;

            op2 = gtNewOperNode(GT_LIST, TYP_VOID,           op2, 0);
            op1 = gtNewIconEmbClsHndNode(arrayClsHnd,
                                         typeRef,
                                         info.compScopeHnd);
            op2 = gtNewOperNode(GT_LIST, TYP_VOID, op1, op2);

            /* Create a call to 'new' */

            op1 = gtNewHelperCallNode(info.compCompHnd->getNewArrHelper(arrayClsHnd, info.compMethodHnd),
                                      TYP_REF, 0, op2);

            /* Remember that this basic block contains 'new' of an array */

            block->bbFlags |= BBF_NEW_ARRAY;

            /* Push the result of the call on the stack */

            impPushOnStack(op1, tiRetVal);

            } break;

        case CEE_LOCALLOC:
            if (tiVerificationNeeded)
                Verify(false, "bad opcode");

            // We don't allow locallocs inside handlers
            if (block->bbHndIndex != 0)
            {
                BADCODE("Localloc can't be inside handler");                
            }
                
              

            /* The FP register may not be back to the original value at the end
               of the method, even if the frame size is 0, as localloc may
               have modified it. So we will HAVE to reset it */

            compLocallocUsed = true;

            // Get the size to allocate

            op2 = impPopStack().val;
            assertImp(genActualType(op2->gtType) == TYP_INT);

            if (verCurrentState.esStackDepth != 0)
            {
                BADCODE("Localloc can only be used when the stack is empty");
            }
                

            op1 = gtNewOperNode(GT_LCLHEAP, TYP_I_IMPL, op2);

            // May throw a stack overflow excptn

            op1->gtFlags |= GTF_EXCEPT;

            impPushOnStack(op1, tiRetVal);
            break;


        case CEE_ISINST:

            /* Get the type token */

            assertImp(sz == sizeof(unsigned));
            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);

#ifdef DEBUG
            if (verbose) printf(" %08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);

            if (tiVerificationNeeded)
            {
                Verify(impStackTop().seTypeInfo.IsObjRef(), "obj reference needed");
                    // Even if this is a value class, we know it is boxed.  
                tiRetVal = typeInfo(TI_REF, clsHnd);
            }
            
            /* Pop the address and create the 'instanceof' helper call */
            op1 = impPopStack().val;

            op2 = gtNewIconEmbClsHndNode(clsHnd,
                                         typeRef,
                                         info.compScopeHnd);
            op2 = gtNewArgList(op2, op1);

            op1 = gtNewHelperCallNode(eeGetIsTypeHelper(clsHnd), TYP_REF, 0, op2);

            /* Push the result back on the stack */

            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_REFANYVAL:

            // get the class handle and make a ICON node out of it
            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf(" %08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);

            if (tiVerificationNeeded)
            {
                Verify(impStackTop().seTypeInfo == verMakeTypeInfo(impGetRefAnyClass()), "need refany");
                tiRetVal = verMakeTypeInfo(clsHnd).MakeByRef();
            }
    
                op1 = impPopStack().val;
            // make certain it is normalized;
            op1 = impNormStructVal(op1, impGetRefAnyClass(), CHECK_SPILL_ALL);



            op2 = gtNewIconEmbClsHndNode(clsHnd,
                                         typeRef,
                                         info.compScopeHnd);

            // Call helper GETREFANY(classHandle, op1);
            op2 = gtNewArgList(op2, op1);
            op1 = gtNewHelperCallNode(CORINFO_HELP_GETREFANY, TYP_BYREF, 0, op2);

            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_REFANYTYPE:

            if (tiVerificationNeeded)
            {
                Verify(impStackTop().seTypeInfo == verMakeTypeInfo(impGetRefAnyClass()), "need refany");
                tiRetVal = typeInfo(TI_STRUCT, impGetTypeHandleClass());
            }
    
                op1 = impPopStack().val;

            // make certain it is normalized;
            op1 = impNormStructVal(op1, impGetRefAnyClass(), CHECK_SPILL_ALL);

            if (op1->gtOper == GT_LDOBJ)
            {
                // Get the address of the refany
                op1 = op1->gtOp.gtOp1;

                // Fetch the type from the correct slot
                op1 = gtNewOperNode(GT_ADD, TYP_BYREF, op1, gtNewIconNode(offsetof(CORINFO_RefAny, type)));
                op1 = gtNewOperNode(GT_IND, TYP_BYREF, op1);
            }
            else
            {
                assertImp(op1->gtOper == GT_MKREFANY);

                // The pointer may have side-effects
                if (op1->gtLdObj.gtOp1->gtFlags & GTF_SIDE_EFFECT)
                {
                    impAppendTree(op1->gtLdObj.gtOp1, CHECK_SPILL_ALL, impCurStmtOffs);
#ifdef DEBUG
                    impNoteLastILoffs();
#endif
                }

                // We already have the class handle
                op1 = op1->gtOp.gtOp2;
            }

           
            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_LDTOKEN:
                /* Get the Class index */
            assertImp(sz == sizeof(unsigned));
            val = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);

            void * embedGenHnd, * pEmbedGenHnd;
            CORINFO_CLASS_HANDLE tokenType;
            embedGenHnd = embedGenericHandle(val, info.compScopeHnd, info.compMethodHnd, &pEmbedGenHnd, tokenType);

            if (tiVerificationNeeded) 
                tiRetVal = verMakeTypeInfo(tokenType);

            op1 = gtNewIconEmbHndNode(embedGenHnd, pEmbedGenHnd, GTF_ICON_TOKEN_HDL);
            op1->gtFlags |= GTF_DONT_CSE;
            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_UNBOX:
            /* Get the Class index */
            assertImp(sz == sizeof(unsigned));

            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf(" %08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);

            /* Pop the object and create the unbox helper call */
            if (tiVerificationNeeded)
            { 
                typeInfo tiUnbox = impStackTop().seTypeInfo;
                Verify(tiUnbox.IsObjRef(), "Bad unbox arg");

                tiRetVal = verMakeTypeInfo(clsHnd);
                Verify(tiRetVal.IsValueClass(), "not value class");
                tiRetVal.MakeByRef();
            }

            op1 = impPopStack().val;
            op2 = gtNewIconEmbClsHndNode(clsHnd, typeRef, info.compScopeHnd);
            assertImp(op1->gtType == TYP_REF);
            if (1)
            {
                // try to inline the common case of the unbox helper
                // UNBOX(exp) morphs into
                // clone = pop(exp);
                // ((*clone != typeToken) ? UnboxHelper(clone, typeToken) : nop);
                // push(clone + 4)
                //
                // I am worried about the mispredicted jump here (common case
                // does not fall through).
                GenTreePtr clone;
                op1 = impCloneExpr(op1, &clone, BAD_CLASS_HANDLE, CHECK_SPILL_ALL);
                op1 = gtNewOperNode(GT_IND, TYP_I_IMPL, op1);
                GenTreePtr cond = gtNewOperNode(GT_EQ, TYP_I_IMPL, op1, op2);

                op1 = impCloneExpr(clone, &clone, BAD_CLASS_HANDLE, CHECK_SPILL_ALL);
                op2 = gtNewIconEmbClsHndNode(clsHnd, typeRef, info.compScopeHnd);
                op1 = gtNewArgList(op2, op1);
                op1 = gtNewHelperCallNode(CORINFO_HELP_UNBOX, TYP_VOID, 0, op1);

                op1 = gtNewOperNode(GT_COLON, TYP_VOID, op1, gtNewNothingNode());
                op1 = gtNewOperNode(GT_QMARK, TYP_VOID, cond, op1);
                cond->gtFlags |= GTF_RELOP_QMARK;

                GenTreePtr op3;

                op2 = gtNewIconNode(sizeof(void*), TYP_I_IMPL);
                op3 = gtNewOperNode(GT_ADD, TYP_BYREF, clone, op2);

                op1 = gtNewOperNode(GT_COMMA, op3->TypeGet(), op1, op3);
            }
            else
            {
                // If we care about code size instead of speed, we should just make the call
                op2 = gtNewArgList(op2, op1);
                op1 = gtNewHelperCallNode(CORINFO_HELP_UNBOX, TYP_BYREF, 0, op2);
            }

            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_BOX: {
            /* Get the Class index */
            assertImp(sz == sizeof(unsigned));
            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf(" %08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);
            if (tiVerificationNeeded)
            {
                typeInfo tiActual = impStackTop().seTypeInfo;
                typeInfo tiBox = verMakeTypeInfo(clsHnd);

                Verify(tiBox.IsValueClass(), "value class expected");
                Verify(!verIsByRefLike(tiBox), "boxing byreflike");
                Verify(tiActual == tiBox.NormaliseForStack(), "type mismatch");

                /* Push the result back on the stack, */
                /* even if clsHnd is a value class we want the TI_REF */
                tiRetVal = typeInfo(TI_REF, clsHnd);
            }

            // Here is the new way to box. can only do it if the class construtor has been called
            // can always do it on primitive types

            op2 = gtNewIconEmbClsHndNode(clsHnd, typeRef, info.compScopeHnd);
            
            impSpillSpecialSideEff();

            // insure that the value class is restored
            if (info.compCompHnd->loadClass(clsHnd, info.compMethodHnd, FALSE))
            {
                // Box(expr) gets morphed into
                // temp = new(clsHnd)
                // cpobj(temp+4, expr, clsHnd)
                // push temp            
                
                if (impBoxTempInUse || impBoxTemp == BAD_VAR_NUM) 
                    impBoxTemp = lvaGrabTemp();
    
                // needs to stay in use until this box expression is appended 
                // some other node.  We approximate this by keeping it alive until
                // the opcode stack becomes empty
                impBoxTempInUse = true;
                
                op1 = gtNewHelperCallNode(  eeGetNewHelper(clsHnd, info.compMethodHnd),
                                            TYP_REF, 0,
                                            gtNewArgList(op2));
                
                GenTreePtr asg = gtNewTempAssign(impBoxTemp, op1);
                
                op1 = gtNewLclvNode(impBoxTemp, TYP_REF);
                op2 = gtNewIconNode(sizeof(void*), TYP_I_IMPL);
                op1 = gtNewOperNode(GT_ADD, TYP_BYREF, op1, op2);
                
                CORINFO_CLASS_HANDLE operCls;
                op2 = impPopStack(operCls).val;
                if (op2->TypeGet() == TYP_STRUCT)
                {
                    assert(eeGetClassSize(clsHnd) == eeGetClassSize(operCls));
                    op1 = impAssignStructPtr(op1, op2, operCls, CHECK_SPILL_ALL);
                }
                else
                {
                    lclTyp = op2->TypeGet();
                    if (lclTyp == TYP_BYREF)
                        lclTyp = TYP_I_IMPL;
                    CorInfoType jitType = info.compCompHnd->asCorInfoType(clsHnd);
                    if (CORINFO_TYPE_BOOL <= jitType && jitType <= CORINFO_TYPE_DOUBLE)

                        lclTyp = JITtype2varType(jitType);
                    assert(genActualType(op2->TypeGet()) == genActualType(lclTyp) ||
                        varTypeIsFloating(lclTyp) == varTypeIsFloating(op2->TypeGet()));
                    op1 = gtNewAssignNode(gtNewOperNode(GT_IND, lclTyp, op1), op2);
                }
                op2 = gtNewLclvNode(impBoxTemp, TYP_REF);
                op1 = gtNewOperNode(GT_COMMA, TYP_REF, op1, op2);
                op1 = gtNewOperNode(GT_COMMA, TYP_REF, asg, op1);               
            }
            else 
            {
                // Get address of value and pass it to the box helper
                CORINFO_CLASS_HANDLE operCls;
                op1 = impPopStack(operCls).val;
                if (op1->TypeGet() == TYP_STRUCT)
                {
                    op1 = impGetStructAddr(op1, operCls, CHECK_SPILL_ALL, true);
                }
                else 
                {
                    // This is likely to be unreachable because initClass will always
                    // return TRUE for primitive types

                    if (op1->gtOper == GT_LCL_VAR) 
                        lclNum = op1->gtLclVar.gtLclNum;
                    else 
                    {
                        lclNum = lvaGrabTemp();
                        impAssignTempGen(lclNum, op1, CHECK_SPILL_NONE);
                        op1 = gtNewLclvNode(lclNum, op1->TypeGet());
                    }
#ifdef DEBUG
                    // This local must never be converted to a double in stress mode,
                    // because we cannot take the address of a value returned from
                    // a helper call (i.e. the helper to convert from double to int)
                    lvaTable[lclNum].lvKeepType = 1;
#endif
                    op1 = gtNewOperNode(GT_ADDR, TYP_I_IMPL, op1);
                    op1->gtFlags |= GTF_ADDR_ONSTACK;
                    lvaTable[lclNum].lvVolatile = 1;
                }
                op2 = gtNewArgList(op2, op1);
                op1 = gtNewHelperCallNode(CORINFO_HELP_BOX, TYP_REF, 0, op2);
            }

            impPushOnStack(op1, tiRetVal);
            } break;

        case CEE_SIZEOF:
            /* Get the Class index */
            assertImp(sz == sizeof(unsigned));
            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf(" %08X", typeRef);
#endif           
            /* Pop the address and create the box helper call */

            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);

                // Review : why do we bother checking this?
            if (tiVerificationNeeded)
            {
                Verify(eeGetClassAttribs(clsHnd) & CORINFO_FLG_VALUECLASS, "value class expected");
                tiRetVal = typeInfo(TI_INT);
            }

            op1 = gtNewIconNode(eeGetClassSize(clsHnd));
            impPushOnStack(op1, tiRetVal); 
            break;

        case CEE_CASTCLASS:

            /* Get the Class index */

            assertImp(sz == sizeof(unsigned));
            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf(" %08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);

            if (tiVerificationNeeded)
            {
                Verify(impStackTop().seTypeInfo.IsObjRef(), "object ref expected");
                    // Even if this is a value class, we know it is boxed.  
                tiRetVal = typeInfo(TI_REF, clsHnd);
            }
            
                op1 = impPopStack().val;

            /* Pop the address and create the 'checked cast' helper call */

            op2 = gtNewIconEmbClsHndNode(clsHnd, typeRef, info.compScopeHnd);
            op2 = gtNewArgList(op2, op1);

            op1 = gtNewHelperCallNode(eeGetChkCastHelper(clsHnd), TYP_REF, 0, op2);

            /* Push the result back on the stack */
            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_THROW:

            if (tiVerificationNeeded)
            {
                tiRetVal = impStackTop().seTypeInfo;
                Verify(tiRetVal.IsObjRef(), "object ref expected");
                if (verTrackObjCtorInitState && 
                    !verCurrentState.thisInitialized)
                Verify(!tiRetVal.IsThisPtr(), "throw uninitialised this");
            }

            block->bbSetRunRarely(); // any block with a throw is rare
            /* Pop the exception object and create the 'throw' helper call */

            op1 = gtNewHelperCallNode(CORINFO_HELP_THROW,
                                      TYP_VOID,
                                      GTF_EXCEPT,
                                      gtNewArgList(impPopStack().val));


EVAL_APPEND:
            if (verCurrentState.esStackDepth > 0)
                impEvalSideEffects();

            assert(verCurrentState.esStackDepth == 0);

            goto APPEND;

        case CEE_RETHROW:
            if (info.compXcptnsCount == 0)
                BADCODE("rethrow outside catch");

            if (tiVerificationNeeded)
            {
                Verify(block->hasHndIndex(), "rethrow outside catch");
                EHblkDsc *HBtab = compHndBBtab + block->getHndIndex();
                Verify(!(HBtab->ebdFlags & (CORINFO_EH_CLAUSE_FINALLY | CORINFO_EH_CLAUSE_FAULT)), 
                       "rethrow in finally or fault");
                if (HBtab->ebdFlags & CORINFO_EH_CLAUSE_FILTER) {
                    // we better be in the catch clause part, not the filter part
                    Verify (HBtab->ebdHndBeg->bbCodeOffs <= compCurBB->bbCodeOffs &&
                            compCurBB->bbCodeOffs < HBtab->ebdHndEnd->bbCodeOffs, "rethrow in filter");
                }
            }

            /* Create the 'rethrow' helper call */

            op1 = gtNewHelperCallNode(CORINFO_HELP_RETHROW, 
                                      TYP_VOID,
                                      GTF_EXCEPT);

            goto EVAL_APPEND;

        case CEE_INITOBJ:

            assertImp(sz == sizeof(unsigned));
            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);

#ifdef DEBUG
            if (verbose) printf(" %08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);

            if (tiVerificationNeeded)
            {   
                typeInfo tiTo = impStackTop().seTypeInfo;
                typeInfo tiInstr = verMakeTypeInfo(clsHnd);

                        // Review: we don't really need it to be a value class
                Verify(tiInstr.IsValueClass(), "bad initObj arg");
                Verify(tiInstr == tiTo.DereferenceByRef(), "bad initObj arg");
            }            

            op3 = gtNewIconNode(eeGetClassSize(clsHnd));  // Size
            op2 = gtNewIconNode(0);                       // Value
            goto  INITBLK_OR_INITOBJ;

        case CEE_INITBLK:
            if (tiVerificationNeeded)
                Verify(false, "bad opcode");

            op3 = impPopStack().val;        // Size
            op2 = impPopStack().val;        // Value
INITBLK_OR_INITOBJ:
            op1 = impPopStack().val;        // Dest
            op1 = impHandleBlockOp(GT_INITBLK, op1, op2, op3, (prefixFlags & PREFIX_VOLATILE) != 0);

            goto SPILL_APPEND;


        case CEE_CPBLK:
            if (tiVerificationNeeded)
                Verify(false, "bad opcode");
            op3 = impPopStack().val;        // Size
            op2 = impPopStack().val;        // Src
            op1 = impPopStack().val;        // Dest
            goto CPBLK_OR_CPOBJ;

        case CEE_CPOBJ:
            assertImp(sz == sizeof(unsigned));
            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf(" %08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);

            if (tiVerificationNeeded)
            {
                typeInfo tiFrom = impStackTop(1).seTypeInfo;
                typeInfo tiTo = impStackTop().seTypeInfo;
                typeInfo tiInstr = verMakeTypeInfo(clsHnd);
    
                Verify(tiInstr.IsValueClass(), "need value class");
                Verify(tiFrom == tiTo, "bad to arg");
                Verify(tiInstr == tiTo.DereferenceByRef(), "bad from arg");
            }            

                op2 = impPopStack().val;                   // Src
                op1 = impPopStack().val;                   // Dest
            op3 = impGetCpobjHandle(clsHnd);           // Size
            goto  CPBLK_OR_CPOBJ;

CPBLK_OR_CPOBJ:

            op1     = impHandleBlockOp(GT_COPYBLK, op1, op2, op3, (prefixFlags & PREFIX_VOLATILE) != 0);

            goto SPILL_APPEND;


        case CEE_STOBJ: {
            assertImp(sz == sizeof(unsigned));
            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf(" %08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);

            if (tiVerificationNeeded)
            {
                typeInfo ptrVal = verVerifySTIND(impStackTop(1).seTypeInfo, impStackTop(0).seTypeInfo, TYP_STRUCT);
                Verify(ptrVal == verMakeTypeInfo(clsHnd), "type does not agree with instr");
            }

            CorInfoType jitTyp = info.compCompHnd->asCorInfoType(clsHnd);
            if (CORINFO_TYPE_BOOL <= jitTyp && jitTyp <= CORINFO_TYPE_DOUBLE)
            {
                lclTyp = JITtype2varType(jitTyp);
                goto STIND_POST_VERIFY;
            }
 
            op2 = impPopStack().val;        // Value
            op1 = impPopStack().val;        // Ptr

            assertImp(op2->TypeGet() == TYP_STRUCT);


            op1 = impAssignStructPtr(op1, op2, clsHnd, CHECK_SPILL_ALL);
            goto SPILL_APPEND;
            }

        case CEE_MKREFANY:
            oper = GT_MKREFANY;
            assertImp(sz == sizeof(unsigned));
            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);

#ifdef DEBUG
            if (verbose) printf("%08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);

            if (tiVerificationNeeded)
            {
                typeInfo tiPtr = impStackTop().seTypeInfo;
                typeInfo tiInstr = verMakeTypeInfo(clsHnd);

                Verify(!verIsByRefLike(tiInstr), "mkrefany of byref-like class");
                Verify(tiPtr.DereferenceByRef() == tiInstr, "type mismatch");
            }

                op1 = impPopStack().val;

            assertImp(op1->TypeGet() == TYP_BYREF || op1->TypeGet() == TYP_I_IMPL);

            // MKREFANY returns a struct
            op1 = gtNewOperNode(oper, TYP_STRUCT, op1);

            // Set the class token
            op1->gtOp.gtOp2 = gtNewIconEmbClsHndNode(clsHnd);

            impPushOnStack(op1, verMakeTypeInfo(impGetRefAnyClass()));
            break;


        case CEE_LDOBJ: {
            oper = GT_LDOBJ;
            assertImp(sz == sizeof(unsigned));
            typeRef = _impGetToken(codeAddr, info.compScopeHnd, tiVerificationNeeded);
#ifdef DEBUG
            if (verbose) printf("%08X", typeRef);
#endif
            clsHnd = eeFindClass(typeRef, info.compScopeHnd, info.compMethodHnd);

            tiRetVal =  verMakeTypeInfo(clsHnd);
            if (tiVerificationNeeded)
            {
                typeInfo tiPtr = impStackTop().seTypeInfo;
                typeInfo tiPtrVal = verVerifyLDIND(tiPtr, TYP_STRUCT);
                Verify(tiRetVal == tiPtrVal, "type does not agree with instr");
                tiRetVal.NormaliseForStack();
            }
            op1 = impPopStack().val;

            assertImp(op1->TypeGet() == TYP_BYREF || op1->TypeGet() == TYP_I_IMPL);

            // LDOBJ returns a struct
            op1 = gtNewOperNode(oper, TYP_STRUCT, op1);

            op1->gtFlags |= (GTF_EXCEPT | GTF_GLOB_REF);

            // and an inline argument which is the class token of the loaded obj
            op1->gtLdObj.gtClass = clsHnd;

            CorInfoType jitTyp = info.compCompHnd->asCorInfoType(clsHnd);
            if (CORINFO_TYPE_BOOL <= jitTyp && jitTyp <= CORINFO_TYPE_DOUBLE)
            {
                // GT_IND is a large node, but its OK if GTF_IND_RNGCHK is not set
                assert(!(op1->gtFlags & GTF_IND_RNGCHK));
                op1->ChangeOperUnchecked(GT_IND);

                // ldobj could point anywhere, example a boxed class static int
                op1->gtFlags |= GTF_IND_TGTANYWHERE;

                op1->gtType = JITtype2varType(jitTyp);
                op1->gtOp.gtOp2 = 0;            // must be zero for tree walkers
                assertImp(varTypeIsArithmetic(op1->gtType));
            }
            
            impPushOnStack(op1, tiRetVal);
            break;
            }

        case CEE_LDLEN:
            if (tiVerificationNeeded)
            {
                typeInfo tiArray = impStackTop().seTypeInfo;
                Verify(verIsSDArray(tiArray), "bad array");
                tiRetVal = typeInfo(TI_INT);
            }

                op1 = impPopStack().val;
            if  (!opts.compMinOptim && !opts.compDbgCode)
            {
                /* Use GT_ARR_LENGTH operator so rng check opts see this */
                op1 = gtNewOperNode(GT_ARR_LENGTH, TYP_INT, op1);
                op1->gtSetArrLenOffset(offsetof(CORINFO_Array, length));
            }
            else
            {
                /* Create the expression "*(array_addr + ArrLenOffs)" */
                op1 = gtNewOperNode(GT_ADD, TYP_REF, op1,
                                    gtNewIconNode(offsetof(CORINFO_Array, length), TYP_INT));
                op1 = gtNewOperNode(GT_IND, TYP_INT, op1);
            }

            /* An indirection will cause a GPF if the address is null */
            op1->gtFlags |= GTF_EXCEPT;

            /* Push the result back on the stack */
            impPushOnStack(op1, tiRetVal);
            break;

        case CEE_BREAK:
            op1 = gtNewHelperCallNode(CORINFO_HELP_USER_BREAKPOINT, TYP_VOID);
            goto SPILL_APPEND;

        case CEE_NOP:
            if (opts.compDbgCode)
            {
                op1 = gtNewOperNode(GT_NO_OP, TYP_VOID);
                goto SPILL_APPEND;
            }
            break;

        /******************************** NYI *******************************/

        case CEE_ILLEGAL:
        case CEE_MACRO_END:

        default:
            BADCODE("unknown opcode");
        }

#undef assertImp

        codeAddr += sz;

        prefixFlags = 0;
        assert(!insertLdloc || opcode == CEE_DUP);
    }

    assert(!insertLdloc);

    return;

#undef _impGetToken
}

/*****************************************************************************
 *  Mark the block as unimported.
 *  Note that the caller is responsible for calling impImportBlockPending(),
 *  with the appropriate stack-state
 */

inline
void            Compiler::impReimportMarkBlock(BasicBlock * block)
{
#ifdef DEBUG
    if (verbose && (block->bbFlags & BBF_IMPORTED))
        printf("\nBB%2d will be reimported\n", block->bbNum);
#endif

    block->bbFlags &= ~BBF_IMPORTED;
}

/*****************************************************************************
 *  Mark the successors of the given block as unimported.
 *  Note that the caller is responsible for calling impImportBlockPending()
 *  for all the successors, with the appropriate stack-state.
 */

void            Compiler::impReimportMarkSuccessors(BasicBlock * block)
{
    switch (block->bbJumpKind)
    {
    case BBJ_RET:
    case BBJ_THROW:
    case BBJ_RETURN:
        break;

    case BBJ_COND:
        impReimportMarkBlock(block->bbNext);
        impReimportMarkBlock(block->bbJumpDest);
        break;

    case BBJ_ALWAYS:
    case BBJ_CALL:

        impReimportMarkBlock(block->bbJumpDest);
        break;

    case BBJ_NONE:

        impReimportMarkBlock(block->bbNext);
        break;

    case BBJ_SWITCH:

        BasicBlock * *  jmpTab;
        unsigned        jmpCnt;

        jmpCnt = block->bbJumpSwt->bbsCount;
        jmpTab = block->bbJumpSwt->bbsDstTab;

        do
        {
            impReimportMarkBlock(*jmpTab);
        }
        while (++jmpTab, --jmpCnt);

        break;

#ifdef DEBUG
    default: assert(!"Unexpected bbJumpKind in impReimportMarkSuccessors()");
#endif
    }
}

/*****************************************************************************
 *
 *  Import the instr for the given basic block (and any blocks reachable
 *  from it).
 */

void                Compiler::impImportBlock(BasicBlock *block)
{
    SavedStack      blockState;
    bool            reImport; 
    bool            markImport;

AGAIN:
    reImport = false;

    assert(block);
    assert(!(block->bbFlags & BBF_INTERNAL));

    /* Make the block globaly available */

    compCurBB = block;

#ifdef DEBUG
    /* Initialize the debug variables */
    impCurOpcName = "unknown";
    impCurOpcOffs = block->bbCodeOffs;
#endif

    /* If the block hasn't already been imported, bail */

    if  (block->bbFlags & BBF_IMPORTED)
    {
        /* The stack should have the same height on entry to the block from
           all its predecessors */

        if (tiVerificationNeeded)
        {
            // if we've already failed verification or the entry state hasn't changed, do nothing
            if ((block->bbFlags & BBF_FAILED_VERIFICATION) || verEntryStateMatches(block))
                return;
                
            // entry state did not match, we need to merge the entry states
            block->bbFlags &= ~BBF_IMPORTED;
            
            if (!verMergeEntryStates(block))
            {
                verConvertBBToThrowVerificationException(block DEBUGARG(true));
                impEndTreeList(block);
                return;
            }
            
            /* The verifier needs us to re-import this block */
            reImport = true;
            goto CONTINUE_IMPORT;
        }

        if (block->bbStkDepth != verCurrentState.esStackDepth)
        {
#ifdef DEBUG
            char buffer[400];
            _snprintf(buffer, 400, "Block at offset %4.4x to %4.4x in %s entered with different stack depths.\n"
                            "Previous depth was %d, current depth is %d",
                            block->bbCodeOffs, block->bbCodeOffs+block->bbCodeSize, info.compFullName,
                            block->bbStkDepth, verCurrentState.esStackDepth);
            buffer[400-1] = 0;
            NO_WAY(buffer);
#else
            NO_WAY("Block entered with different stack depths");
#endif
        }

#ifdef DEBUG
        if (compStressCompile(STRESS_CHK_REIMPORT, 15) && !(block->bbFlags & BBF_VISITED))
        {
            block->bbFlags &= ~BBF_IMPORTED;
            block->bbFlags |= BBF_VISITED; // Set BBF_VISITED so that we reimport blocks just once

            reImport = true;
            goto CONTINUE_IMPORT;
        }
#endif
        return;
    }

    // if got here, this is the first time the block is being added to the worker-list
    // initialize the BB's entry state

    verInitBBEntryState(block, &verCurrentState);

CONTINUE_IMPORT:

    /* Remember whether the stack is non-empty on entry */

    block->bbStkDepth = verCurrentState.esStackDepth;

    /* @VERIFICATION : For now, the only state propagation from try
       to it's handler is "thisInit" state (stack is empty at start of try). 
       So adding the state before try to it's handlers is sufficient. In case 
       we add new states to track, make sure that code is added so that when
       ever a state changes inside any basic block in a try block, that state
       needs to get added on to the handler states. This is not necessary for
       'thisInit' state because thisInit can only get "better" inside a try
       block, and Merge("better", "bad") always gives "bad" state. */

    /* Is this block the start of a try ? If so, then we need to
       process its exception handlers */

    if  (block->bbFlags & BBF_TRY_BEG)
    {
        assert(block->bbFlags & BBF_HAS_HANDLER);

        /* Save the stack contents, we'll need to restore it later */

        // Stack must be 0
        if (block->bbStkDepth != 0)
            BADCODE("Conditional jumps to catch handler unsupported");        
        
        impSaveStackState(&blockState, false);

        unsigned        XTnum;
        EHblkDsc *      HBtab;

        for (XTnum = 0, HBtab = compHndBBtab;
             XTnum < info.compXcptnsCount;
             XTnum++  , HBtab++)
        {
            if  (HBtab->ebdTryBeg != block)
                continue;

            /* Recursively process the handler block */
            verCurrentState.esStackDepth = 0;

            BasicBlock * hndBegBB = HBtab->ebdHndBeg;
            GenTreePtr   arg;

            if (hndBegBB->bbCatchTyp &&
                handlerGetsXcptnObj(hndBegBB->bbCatchTyp))
            {
                /* Push the exception address value on the stack */
                GenTreePtr  arg = gtNewOperNode(GT_CATCH_ARG, TYP_REF);

                /* Mark the node as having a side-effect - i.e. cannot be
                 * moved around since it is tied to a fixed location (EAX) */
                arg->gtFlags |= GTF_OTHER_SIDEEFF;

                CORINFO_CLASS_HANDLE clsHnd;

                if ((HBtab->ebdFlags & CORINFO_EH_CLAUSE_FILTER) != 0)
                    clsHnd = impGetObjectClass();
                else
                {
                    if (tiVerificationNeeded)
                        Verify(info.compCompHnd->isValidToken(info.compScopeHnd, HBtab->ebdTyp), "bad token");
                    clsHnd = eeFindClass(HBtab->ebdTyp, info.compScopeHnd, info.compMethodHnd);
                }

                    // Even if it is a value class, we know it is an object reference                                    
                impPushOnStack(arg, typeInfo(TI_REF, clsHnd));
            }

            // Queue up the handler for importing

            impImportBlockPending(hndBegBB, false);

            if (HBtab->ebdFlags & CORINFO_EH_CLAUSE_FILTER)
            {
                /* @VERIFICATION : Ideally the end of filter state should get
                   propagated to the catch handler, this is an incompleteness,
                   but is not a security/compliance issue, since the only
                   interesting state is the 'thisInit' state.
                 */
                verCurrentState.esStackDepth = 0;
                arg = gtNewOperNode(GT_CATCH_ARG, TYP_REF);
                arg->gtFlags |= GTF_OTHER_SIDEEFF;
                impPushOnStack(arg, typeInfo(TI_REF, impGetObjectClass()));

                impImportBlockPending(HBtab->ebdFilter, false);
            }
        }

        /* Restore the stack contents */

        impRestoreStackState(&blockState);
    }

    /* Now walk the code an import the IL into GenTrees */

    __try 
    {
        impImportBlockCode(block);
    }
    __except (GetExceptionCode() == SEH_VERIFICATION_EXCEPTION ? EXCEPTION_EXECUTE_HANDLER : EXCEPTION_CONTINUE_SEARCH)
    {
        verHandleVerificationFailure(block DEBUGARG(false));
    }


    markImport = false;

SPILLSTACK:

    /* If max. optimizations enabled, check for an "?:" expression */

    if  ((opts.compFlags & CLFLG_QMARK) && !(block->bbFlags & BBF_HAS_HANDLER)
         // @TODO [CONSIDER] [04/16/01] []: Remove this check if 
         // this is a perf issue for code that needs to be verified
         && !(tiVerificationNeeded || impCanReimport))
    {
        if  (block->bbJumpKind == BBJ_COND && impCheckForQmarkColon(block))
            return;
    }

    GenTreePtr  qcx = NULL;
    typeInfo    tiQcx;
    unsigned    baseTmp;

    /* If the stack is non-empty, we might have to spill its contents */

    if  (verCurrentState.esStackDepth)
    {
        impBoxTemp = BAD_VAR_NUM;       // if a box temp is used in a block that leaves something
                                        // on the stack, its lifetime is hard to determine, simply
                                        // don't reuse such temps.  

        GenTreePtr      addStmt = 0;

        /* Special case: a block that computes one part of a _?: value */

        if  (isBBF_BB_COLON(block->bbFlags))
        {
            /* Pop one value from the top of the stack */

            GenTreePtr      val;
            val = impPopStack(tiQcx);

            var_types       typ = genActualType(val->gtType);

            /* Append a GT_BB_COLON node */

            impAppendTree(gtNewOperNode(GT_BB_COLON, typ, val), CHECK_SPILL_NONE, impCurStmtOffs);

            assert(verCurrentState.esStackDepth == 0);

            /* Create the "(?)" node for the 'result' block */

            qcx = gtNewOperNode(GT_BB_QMARK, typ);
            qcx->gtFlags |= GTF_OTHER_SIDEEFF;
            goto EMPTY_STK;
        }

        /* Special case: a block that computes one part of a ?: value */

        if  (isBBF_COLON(block->bbFlags))
        {
            /* Pop one value from the top of the stack and append it to the
               stmt list. The rsltBlk will pick it up from there. */

            impAppendTree(impPopStack().val, CHECK_SPILL_NONE, impCurStmtOffs);

            /* We are done here */

            impEndTreeList(block);

            return;
        }

        // Do any of the blocks that follow have input temps assigned ?

        baseTmp = NO_BASE_TMP;

        /* Do the successors of 'block' have any other predecessors ?
           We do not want to do some of the optimizations related to multiRef
           if we can reimport blocks */

        unsigned    multRef = impCanReimport ? unsigned(~0) : 0;

        switch (block->bbJumpKind)
        {
        case BBJ_COND:

            /* Temporarily remove the 'jtrue' from the end of the tree list */

            assert(impTreeLast);
            assert(impTreeLast                   ->gtOper == GT_STMT );
            assert(impTreeLast->gtStmt.gtStmtExpr->gtOper == GT_JTRUE);

            addStmt = impTreeLast;
                      impTreeLast = impTreeLast->gtPrev;

            /* Note if the next block has more than one ancestor */

            multRef |= block->bbNext->bbRefs;

            /* Does the next block have temps assigned? */

            baseTmp = block->bbNext->bbStkTemps;
            if  (baseTmp != NO_BASE_TMP)
            {
                break;
            }

            /* Try the target of the jump then */

            multRef |= block->bbJumpDest->bbRefs;
            baseTmp  = block->bbJumpDest->bbStkTemps;

            /* The catch handler expects the stack expr to be a GT_CATCH_ARG
               while other BBs expect them temps.  To support this, we would
               have to reconcile these */

            if (block->bbNext->bbCatchTyp)
                BADCODE("Conditional jumps to catch handler unsupported");
            break;

        case BBJ_ALWAYS:
            multRef |= block->bbJumpDest->bbRefs;
            baseTmp  = block->bbJumpDest->bbStkTemps;

            assert(!block->bbJumpDest->bbCatchTyp);  // can't jump to catch handler
            break;

        case BBJ_NONE:
            multRef |= block->bbNext    ->bbRefs;
            baseTmp  = block->bbNext    ->bbStkTemps;

            // We dont allow falling into a handler
            assert(!block->bbNext->bbCatchTyp);
            break;

        case BBJ_CALL:
            NO_WAY("ISSUE: 'leaveCall' with non-empty stack - do we have to handle this?");

        case BBJ_RETURN:
        case BBJ_RET:
        case BBJ_THROW:
            // @TODO [CONSIDER] [04/16/01] []: add code to evaluate side effects
            NO_WAY("can't have 'unreached' end of BB with non-empty stack");
            break;

        case BBJ_SWITCH:

            BasicBlock * *  jmpTab;
            unsigned        jmpCnt;

            /* Temporarily remove the GT_SWITCH from the end of the tree list */

            assert(impTreeLast);
            assert(impTreeLast                   ->gtOper == GT_STMT );
            assert(impTreeLast->gtStmt.gtStmtExpr->gtOper == GT_SWITCH);

            addStmt = impTreeLast;
                      impTreeLast = impTreeLast->gtPrev;

            jmpCnt = block->bbJumpSwt->bbsCount;
            jmpTab = block->bbJumpSwt->bbsDstTab;

            do
            {
                BasicBlock * tgtBlk = (*jmpTab);

                multRef |= tgtBlk->bbRefs;

                baseTmp  = tgtBlk->bbStkTemps;
                if  (baseTmp != NO_BASE_TMP)
                {
                    assert(multRef > 1);
                    break;
                }

                if (tgtBlk->bbCatchTyp)
                    BADCODE("Conditional jumps to catch handler unsupported");
            }
            while (++jmpTab, --jmpCnt);

            break;
        }

        assert(multRef >= 1);

        /* Do we have a base temp number? */

        bool newTemps; newTemps = (baseTmp == NO_BASE_TMP);

        if  (newTemps)
        {
            /* Grab enough temps for the whole stack */
            baseTmp = lvaGrabTemps(verCurrentState.esStackDepth);
        }
        
        /* Spill all stack entries into temps */        
        unsigned level, tempNum;

        for (level = 0, tempNum = baseTmp; level < verCurrentState.esStackDepth; level++, tempNum++)
        {
            GenTreePtr      tree = verCurrentState.esStack[level].val;

            /* TYP_BYREF and TYP_I_IMPL merge to TYP_BYREF. However, the branch
               which leaves the TYP_I_IMPL on the stack is imported first, the
               successor would be imported assuming there was a TYP_I_IMPL on
               the stack. Thus the value would not get GC-tracked. Hence,
               bash the temp to TYP_BYREF and reimport the successors */

            if (tree->gtType == TYP_BYREF && lvaTable[tempNum].lvType != TYP_UNDEF
                                          && lvaTable[tempNum].lvType != TYP_BYREF)
            {
                if (lvaTable[tempNum].lvType != TYP_I_IMPL)
                    BADCODE("Merging byref and non-int on stack");

                lvaTable[tempNum].lvType = TYP_BYREF;
                impReimportMarkSuccessors(block);
                markImport = true;
            }

            /* If there aren't multiple ancestors, we may not spill the 'easy' values.
               We dont do this for tiVerificationNeeded since can forcibly copy
               the stack before importing the successors. */

            if  (multRef == 1 && impValidSpilledStackEntry(tree) && !tiVerificationNeeded)
            {
                lvaTable[tempNum].lvType = genActualType(tree->gtType);
                continue;
            }

            /* We have to use higher precision as the other predecessors
               might be spilling TYP_DOUBLE. */

            if (multRef > 1 && tree->gtType == TYP_FLOAT)
            {
                verCurrentState.esStack[level].val = gtNewCastNode(TYP_DOUBLE, tree, TYP_DOUBLE);
            }

            /* If addStmt has a reference to tempNum (can only happen if we
               are spilling to the temps already used by a previous block),
               we need to spill addStmt */

            if (addStmt && !newTemps &&
                gtHasRef(addStmt->gtOp.gtOp1, tempNum, false))
            {
                GenTreePtr addTree = addStmt->gtStmt.gtStmtExpr;

                if (addTree->gtOper == GT_JTRUE)
                {
                    GenTreePtr relOp = addTree->gtOp.gtOp1;
                    assert(relOp->OperIsCompare());

                    var_types type = genActualType(relOp->gtOp.gtOp1->TypeGet());

                    if (gtHasRef(relOp->gtOp.gtOp1, tempNum, false))
                    {
                        unsigned temp = lvaGrabTemp();
                        impAssignTempGen(temp, relOp->gtOp.gtOp1, level);
                        relOp->gtOp.gtOp1 = gtNewLclvNode(temp, type);
                    }

                    if (gtHasRef(relOp->gtOp.gtOp2, tempNum, false))
                    {
                        unsigned temp = lvaGrabTemp();
                        impAssignTempGen(temp, relOp->gtOp.gtOp2, level);
                        relOp->gtOp.gtOp2 = gtNewLclvNode(temp, type);
                    }
                }
                else
                {
                    assert(addTree->gtOper == GT_SWITCH &&
                           genActualType(addTree->gtOp.gtOp1->gtType) == TYP_INT);

                    unsigned temp = lvaGrabTemp();
                    impAssignTempGen(temp, addTree->gtOp.gtOp1, level);
                    addTree->gtOp.gtOp1 = gtNewLclvNode(temp, TYP_INT);
                }
            }

            /* Spill the stack entry, and replace with the temp */

            if (!impSpillStackEntry(level, tempNum))
            {
                if (markImport)
                    BADCODE("bad stack state");

                // Oops. Something went wrong when spilling. Bad code.
                verHandleVerificationFailure(block DEBUGARG(true));

                goto SPILLSTACK;                
            }
        }

        /* Put back the 'jtrue'/'switch' if we removed it earlier */

        if  (addStmt)
            impAppendStmt(addStmt, CHECK_SPILL_NONE);
    }

EMPTY_STK:

    // Some of the append/spill logic works on compCurBB

    assert(compCurBB == block);

    /* Save the tree list in the block */

    impEndTreeList(block);

    /* Does this block jump to any other blocks? */

    switch (block->bbJumpKind)
    {
    case BBJ_RET:
    case BBJ_THROW:
    case BBJ_RETURN:
        break;

    case BBJ_COND:

        if  (!verCurrentState.esStackDepth)
        {
            /* Queue up the next block for importing */

            impImportBlockPending(block->bbNext, false);

            /* Continue with the target of the conditional jump */

            block = block->bbJumpDest;
            goto AGAIN;
        }

        /* Does the next block have a different input temp set? */

        assert(block->bbNext->bbStkTemps == NO_BASE_TMP ||
               block->bbNext->bbStkTemps == baseTmp);

        if (block->bbNext->bbStkTemps == NO_BASE_TMP)
        {
            /* Tell the block where it's getting its input from */

            block->bbNext->bbStkTemps = baseTmp;
        }

        /* Queue up the next block for importing */
        impImportBlockPending(block->bbNext, true);

        /* Fall through, the jump target is also reachable */

    case BBJ_ALWAYS:

        if  (verCurrentState.esStackDepth)
        {
            /* Does the jump target have a different input temp set? */

            if  (block->bbJumpDest->bbStkTemps != NO_BASE_TMP)
            {
                assert(baseTmp != NO_BASE_TMP);

                if  (block->bbJumpDest->bbStkTemps != baseTmp)
                {
                    /* Ouch -- we'll have to move the temps around */

                    block->bbJumpDest = impMoveTemps(block, block->bbJumpDest, baseTmp);

                    /* The new block will inherit this block's weight */

                    block->bbJumpDest->bbWeight = block->bbWeight;
                }
            }
            else
            {
                /* Tell the block where it's getting its input from */

                block->bbJumpDest->bbStkTemps = baseTmp;
            }
        }

        if (qcx)
        {
            assert(isBBF_BB_COLON(block->bbFlags));

            /* Push a GT_BB_QMARK node on the stack */

            // assert(!"Verification NYI");
            impPushOnStack(qcx, tiQcx);
        }

        block = block->bbJumpDest;

        /* If we had to add a block to move temps around then  */
        /* then we don't import that block                     */

        if (block->bbFlags & BBF_INTERNAL)
            block = block->bbJumpDest;

        goto AGAIN;

    case BBJ_CALL:

        assert(verCurrentState.esStackDepth == 0);
        break;

    case BBJ_NONE:

        if  (verCurrentState.esStackDepth)
        {
            /* Does the next block have a different input temp set? */

            if  (block->bbNext->bbStkTemps != NO_BASE_TMP)
            {
                assert(baseTmp != NO_BASE_TMP);

                if  (block->bbNext->bbStkTemps != baseTmp)
                {
                    /* Ouch -- we'll have to move the temps around */

                    assert(!"UNDONE: transfer temps between blocks");
                }
            }
            else
            {
                /* Tell the block where it's getting its input from */

                block->bbNext->bbStkTemps = baseTmp;
            }
        }

        if (qcx)
        {
            assert(isBBF_BB_COLON(block->bbFlags));

            /* Push a GT_BB_QMARK node on the stack */
            impPushOnStack(qcx, tiQcx);
        }

        block = block->bbNext;
        goto AGAIN;

    case BBJ_SWITCH:

        BasicBlock * *  jmpTab;
        unsigned        jmpCnt;

        jmpCnt = block->bbJumpSwt->bbsCount;
        jmpTab = block->bbJumpSwt->bbsDstTab;

        do
        {
            BasicBlock * tgtBlk = (*jmpTab);
            if  (verCurrentState.esStackDepth)
            {
                /* Does the jump target have a different input temp set? */

                if  (tgtBlk->bbStkTemps != NO_BASE_TMP)
                {
                    assert(baseTmp != NO_BASE_TMP);

                    if  (tgtBlk->bbStkTemps != baseTmp)
                    {
                        /* Ouch -- we'll have to move the temps around */

                        (*jmpTab) = impMoveTemps(block, tgtBlk, baseTmp);

                        /* The new block will inherit this block's weight */

                        (*jmpTab)->bbWeight = block->bbWeight;
                    }
                }
                else
                {
                    /* Tell the block where it's getting its input from */

                    tgtBlk->bbStkTemps = baseTmp;
                }
            }

            /* Add the target case label to the pending list. */

            impImportBlockPending(tgtBlk, true);
        }
        while (++jmpTab, --jmpCnt);

        break;
    }
}

/*****************************************************************************
 *
 *  Adds 'block' to the list of BBs waiting to be imported. ie. it appends
 *  to the worker-list.
 */

void                Compiler::impImportBlockPending(BasicBlock * block,
                                                    bool         copyStkState)
{
    // BBF_COLON blocks are imported directly as they have to be processed
    // before the GT_QMARK to get to the expressions evaluated by these blocks.
    assert(!isBBF_COLON(block->bbFlags));

    if (block->bbFlags & BBF_IMPORTED)
    {
        // if the entry state changed, add the block to the pending-list...
        if ((tiVerificationNeeded == FALSE) ||
            (block->bbFlags & BBF_FAILED_VERIFICATION) || 
            verEntryStateMatches(block))
        {
        // Under DEBUG, add the block to the pending-list anyway as some
        // additional checks will get done on the block. For non-DEBUG, do nothing.
#ifndef DEBUG
            return;
#endif
        }
        else
        {
            // entry state did not match, we need to merge the entry states
            if (!verMergeEntryStates(block))
            {
                block->bbFlags |= BBF_FAILED_VERIFICATION; 
            }
            else // entry states matched, but we need to reimport this block
            {

                block->bbFlags &= ~BBF_IMPORTED;                    
            }

            
        }

#ifdef DEBUG
        if ((compStressCompile(STRESS_CHK_REIMPORT, 15) && !(block->bbFlags & BBF_VISITED)))
        {
            block->bbFlags &= ~BBF_IMPORTED;
            block->bbFlags |= BBF_VISITED; // Set BBF_VISITED so that we reimport blocks just once                        
        }
#endif
    }
    else 
    {
        // if got here, this is the first time the block is being added to the worker-list
        // initialize the BB's entry state
        
        verInitBBEntryState(block, &verCurrentState);
    }

    // Get an entry to add to the pending list

    PendingDsc * dsc;

    if (impPendingFree)
    {
        // We can reuse one of the freed up dscs.
        dsc = impPendingFree;
        impPendingFree = dsc->pdNext;
    }
    else
    {
        // We have to create a new dsc
        dsc = (PendingDsc *)compGetMem(sizeof(*dsc));
    }

    dsc->pdBB           = block;
    dsc->pdSavedStack.ssDepth = verCurrentState.esStackDepth;
    dsc->pdThisPtrInit  = verCurrentState.thisInitialized;

    // Save the stack trees for later

    if (verCurrentState.esStackDepth)
        impSaveStackState(&dsc->pdSavedStack, copyStkState);

    // Add the entry to the pending list

    dsc->pdNext         = impPendingList;
    impPendingList      = dsc;

#ifdef DEBUG
    if (verbose&&0) printf("Added PendingDsc - %08X for BB#%03d\n",
                           dsc, block->bbNum);
#endif
}


/*
 * Create an entry state from the current state which is tracked in 
 */
void Compiler::verInitBBEntryState(BasicBlock* block,
                                   EntryState* srcState)
{
    if (srcState->esLocVarLiveness == NULL &&
        srcState->esStackDepth == 0 &&
        srcState->esValuetypeFieldInitialized == 0 &&
        srcState->thisInitialized)
    {
        assert(verNumBytesLocVarLiveness == 0);
        assert(verNumBytesValuetypeFieldInitialized == 0);
        block->bbEntryState = NULL;
        return;
    }

    block->bbEntryState = (EntryState*) compGetMemA(sizeof(EntryState));

    if (verNumBytesLocVarLiveness != 0)
    {
        BYTE* bitMap = (BYTE*) compGetMemA(verNumBytesLocVarLiveness);
        memcpy(bitMap,
               srcState->esLocVarLiveness, 
               verNumBytesLocVarLiveness);
        block->bbSetLocVarLiveness(bitMap);
    }

    if (verNumBytesValuetypeFieldInitialized)
    {
        BYTE* bitMap = (BYTE*) compGetMemA(verNumBytesValuetypeFieldInitialized);
        
        block->bbSetValuetypeFieldInitialized(bitMap);  
        
        memcpy(bitMap,
               srcState->esValuetypeFieldInitialized,
               verNumBytesValuetypeFieldInitialized);
    }

    //block->bbEntryState.esRefcount = 1;

    block->bbEntryState->esStackDepth = srcState->esStackDepth;
    
    if (srcState->esStackDepth > 0)
    {
        block->bbSetStack(compGetMemArrayA(srcState->esStackDepth, sizeof(StackEntry)));
        unsigned stackSize = srcState->esStackDepth * sizeof(StackEntry);

        memcpy(block->bbEntryState->esStack, 
               srcState->esStack,
               stackSize);
    }

    verSetThisInit(block, srcState->thisInitialized);

    return;
}

void Compiler::verSetThisInit(BasicBlock* block, BOOL init)
{
    if (!block->bbSetThisOnEntry(init))
    {
        block->bbEntryState = (EntryState*) compGetMemA(sizeof(EntryState));
        memset(block->bbEntryState, 0, sizeof(EntryState));
        block->bbSetThisOnEntry(init);

        assert(block->bbThisOnEntry() == init);
    }
}

/*
 * Resets the current state to the state at the start of the basic block 
 */
void Compiler::verResetCurrentState(BasicBlock* block,
                                    EntryState* destState)
{

    if (block->bbEntryState == NULL)
    {
        destState->esLocVarLiveness             = NULL;
        destState->esStackDepth                 = 0;
        destState->esValuetypeFieldInitialized  = 0;
        destState->thisInitialized              = TRUE;
        return;
    }
    if (verNumBytesLocVarLiveness > 0)
    {
        memcpy(destState->esLocVarLiveness,
               block->bbLocVarLivenessBitmapOnEntry(),
               verNumBytesLocVarLiveness);
    }
    if (verNumBytesValuetypeFieldInitialized > 0)
    {
        memcpy(destState->esValuetypeFieldInitialized,
               block->bbValuetypeFieldBitmapOnEntry(),
               verNumBytesValuetypeFieldInitialized);
    }

    destState->esStackDepth =  block->bbEntryState->esStackDepth;

    if (destState->esStackDepth > 0)
    {
        unsigned stackSize = destState->esStackDepth * sizeof(StackEntry);


        memcpy(destState->esStack,
               block->bbStackOnEntry(),  
               stackSize);
    }

    destState->thisInitialized = block->bbThisOnEntry();

    return;
}



BOOL                BasicBlock::bbThisOnEntry()
{
    return bbEntryState ? (BOOL) bbEntryState->thisInitialized : TRUE;
}

BOOL                BasicBlock::bbSetThisOnEntry(BOOL val)
{
    if (bbEntryState) 
        bbEntryState->thisInitialized = (BYTE) val;
    else
    {
        if (val != 0)
            return FALSE;   // Memory needs to be allocated for bbEntryState.
    }

    return TRUE;
}

void                BasicBlock::bbSetLocVarLiveness(BYTE* bitmap)  
{
    assert(bbEntryState);
    assert(bitmap);
    bbEntryState->esLocVarLiveness = bitmap;
}

BYTE*               BasicBlock::bbLocVarLivenessBitmapOnEntry()
{
    assert(bbEntryState);
    return bbEntryState->esLocVarLiveness;
}

void                BasicBlock::bbSetValuetypeFieldInitialized(BYTE* bitmap)  
{
    assert(bbEntryState);
    assert(bitmap);
    bbEntryState->esValuetypeFieldInitialized = bitmap;
}

BYTE*               BasicBlock::bbValuetypeFieldBitmapOnEntry()
{
    assert(bbEntryState);
    return bbEntryState->esValuetypeFieldInitialized;
}

unsigned            BasicBlock::bbStackDepthOnEntry()
{
    return (bbEntryState ? bbEntryState->esStackDepth : 0);

}

void                BasicBlock::bbSetStack(void* stackBuffer)
{
    assert(bbEntryState);
    assert(stackBuffer);
    bbEntryState->esStack = (StackEntry*) stackBuffer;
}

StackEntry*           BasicBlock::bbStackOnEntry()
{
    assert(bbEntryState);
    return bbEntryState->esStack;
}



void                Compiler::verInitCurrentState()
{
    verTrackObjCtorInitState = FALSE;
    verCurrentState.thisInitialized = TRUE;

    if (tiVerificationNeeded)
    {
        // Track this ptr initialization
        if (!info.compIsStatic &&
            (info.compFlags & CORINFO_FLG_CONSTRUCTOR) &&
            lvaTable[0].lvVerTypeInfo.IsObjRef())
        {
            verTrackObjCtorInitState = TRUE;
            verCurrentState.thisInitialized = FALSE;
        }
    }

    // initialize loc var liveness info
    if (!tiVerificationNeeded && !info.compInitMem && (info.compMethodInfo->locals.numArgs != 0))
    {
        verNumBytesLocVarLiveness = (info.compMethodInfo->locals.numArgs + 7)/8;
        verCurrentState.esLocVarLiveness = (BYTE*) compGetMemA(verNumBytesLocVarLiveness);
        memset(verCurrentState.esLocVarLiveness, 0, verNumBytesLocVarLiveness);
    }
    else
    {
        verNumBytesLocVarLiveness = 0;  
        verCurrentState.esLocVarLiveness = NULL;
    }

    // initialize valuetype field initialization info
    verNumValuetypeFields = 0;
    if (!info.compIsStatic)
    {
        if (info.compFlags & CORINFO_FLG_CONSTRUCTOR)
        {
            if (tiVerificationNeeded && lvaTable[0].lvVerTypeInfo.IsByRef())
            {
                assert(DereferenceByRef(lvaTable[0].lvVerTypeInfo).IsValueClass());
                verNumValuetypeFields = eeGetClassNumInstanceFields(info.compClassHnd);
            }
        }
    }
    
    verNumBytesValuetypeFieldInitialized = (verNumValuetypeFields + 7)/8;
    if (verNumBytesValuetypeFieldInitialized > 0)
    {
        verCurrentState.esValuetypeFieldInitialized = (BYTE*) compGetMemA(verNumBytesValuetypeFieldInitialized);
        memset(verCurrentState.esValuetypeFieldInitialized, 0, verNumBytesValuetypeFieldInitialized);
    }
    else
    {
        verCurrentState.esValuetypeFieldInitialized = NULL;
    }

    // initialize stack info

    verCurrentState.esStackDepth = 0;
    assert(verCurrentState.esStack != NULL);

    // copy current state to entry state of first BB
    verInitBBEntryState(fgFirstBB,&verCurrentState);
}

/*****************************************************************************
 *
 *  Convert the instrs ("import") into our internal format (trees). The
 *  basic flowgraph has already been constructed and is passed in.
 */

void                Compiler::impImport(BasicBlock *method)
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In impImport() for %s\n", info.compFullName);
#endif
    /* Allocate the stack contents */

#if INLINING
    if  (info.compMaxStack <= sizeof(impSmallStack)/sizeof(impSmallStack[0]))
    {
        /* Use local variable, don't waste time allocating on the heap */

        impStkSize = sizeof(impSmallStack)/sizeof(impSmallStack[0]);
        verCurrentState.esStack     = impSmallStack;
    }
    else
#endif
    {
        impStkSize = info.compMaxStack;
        verCurrentState.esStack     = (StackEntry *)compGetMemArray(impStkSize, sizeof(*verCurrentState.esStack));
    }

    // initialize the entry state at start of method
    verInitCurrentState();

#if TGT_RISC
    genReturnCnt = 0;
#endif

#ifdef  DEBUG
    impLastILoffsStmt = NULL;
#endif
    impBoxTemp = BAD_VAR_NUM;

    impPendingList = impPendingFree = NULL;

    /* Add the entry-point to the worker-list */

    if (compStressCompile(STRESS_CHK_REIMPORT, 15))
        impImportBlockPending(method, true);

    impImportBlockPending(method, false);

    /* Import blocks in the worker-list until there are no more */

    while(impPendingList)
    {
        /* Remove the entry at the front of the list */

        PendingDsc * dsc = impPendingList;
        impPendingList   = impPendingList->pdNext;

        /* Restore the stack state */

        verCurrentState.thisInitialized  = dsc->pdThisPtrInit;
        verCurrentState.esStackDepth = dsc->pdSavedStack.ssDepth;
        if (verCurrentState.esStackDepth)
            impRestoreStackState(&dsc->pdSavedStack);

        /* Add the entry to the free list for reuse */

        dsc->pdNext = impPendingFree;
        impPendingFree = dsc;

        /* Now import the block */

        if (dsc->pdBB->bbFlags & BBF_FAILED_VERIFICATION)
        {
            verConvertBBToThrowVerificationException(dsc->pdBB DEBUGARG(true));
            impEndTreeList(dsc->pdBB);
        }
        else
        {
            impImportBlock(dsc->pdBB);
        }
    }

#ifdef DEBUG
    if (verbose && info.compXcptnsCount)
    {
        printf("\nAfter impImport() added block for try,catch,finally");
        fgDispBasicBlocks();
        printf("\n");
    }

    // Used in impImportBlockPending() for STRESS_CHK_REIMPORT
    for (BasicBlock * block = fgFirstBB; block; block = block->bbNext)
        block->bbFlags &= ~(BBF_VISITED|BBF_MARKED);
#endif
}

/*****************************************************************************/
#if INLINING
/*****************************************************************************
 *
 *  The inliner version of spilling side effects from the stack
 *  Doesn't need to handle value types.
 */

inline
void                Compiler::impInlineSpillStackEntry(unsigned   level)
{
    GenTreePtr      tree   = verCurrentState.esStack[level].val;
    var_types       lclTyp = genActualType(tree->gtType);

    /* Allocate a temp */

    unsigned tnum = lvaGrabTemp();

    /* The inliner doesn't handle value types on the stack */

    assert(lclTyp != TYP_STRUCT);

    /* Assign the spilled entry to the temp */

    GenTreePtr asg = gtNewTempAssign(tnum, tree);

    /* Append to the "statement" list */

    impInlineExpr = impConcatExprs(impInlineExpr, asg);

    /* Replace the stack entry with the temp */

    verCurrentState.esStack[level].val = gtNewLclvNode(tnum, lclTyp);

    JITLOG((LL_INFO1000000, "INLINER WARNING: Spilled side effect from stack! - "
            "caller is %s\n", info.compFullName));
}

inline
void                Compiler::impInlineSpillGlobEffects()
{
    for (unsigned level = 0; level < verCurrentState.esStackDepth; level++)
    {
        if  (verCurrentState.esStack[level].val->gtFlags & GTF_GLOB_EFFECT)
            impInlineSpillStackEntry(level);
    }
}


void                Compiler::impInlineSpillLclRefs(int lclNum)
{
    for (unsigned level = 0; level < verCurrentState.esStackDepth; level++)
    {
        GenTreePtr      tree = verCurrentState.esStack[level].val;

        /* Skip the tree if it doesn't have an affected reference */

        if  (gtHasRef(tree, lclNum, false))
        {
            impInlineSpillStackEntry(level);
        }
    }
}
/*****************************************************************************
 *
 *  Return an expression that contains both arguments; either of the arguments
 *  may be zero.
 */

GenTreePtr          Compiler::impConcatExprs(GenTreePtr exp1, GenTreePtr exp2)
{
    if  (exp1)
    {
        if  (exp2)
        {
            /* The first expression better be useful for something */

            assert(exp1->gtFlags & GTF_SIDE_EFFECT);

            /* The second expresion should not be a NOP */

            assert(exp2->gtOper != GT_NOP);

            /* Link the two expressions through a comma operator */

            var_types type2 = exp2->gtType;
            if (exp2->gtOper == GT_ASG)
            {
                type2 = TYP_VOID;
            }

            return gtNewCommaNode  (exp1,
                                    exp2);
        }
        else
            return  exp1;
    }
    else
        return  exp2;
}

/*****************************************************************************
 *
 *  Extract side effects from a single expression.
 */

GenTreePtr          Compiler::impExtractSideEffect(GenTreePtr val, GenTreePtr *lstPtr)
{
    GenTreePtr      addx;

    assert(val && val->gtType != TYP_VOID && (val->gtFlags & GTF_SIDE_EFFECT));

    /* Special case: comma expression */

    if  (val->gtOper == GT_COMMA && !(val->gtOp.gtOp2->gtFlags & GTF_SIDE_EFFECT))
    {
        addx = val->gtOp.gtOp1;
        val  = val->gtOp.gtOp2;
    }
    else
    {
        /* Allocate a temp and assign the value to it */

        unsigned        tnum = lvaGrabTemp();

        addx = gtNewTempAssign(tnum, val);

        /* Use the value of the temp */

        val  = gtNewLclvNode(tnum, genActualType(val->gtType));
    }

    /* Add the side effect expression to the list */

    *lstPtr = impConcatExprs(*lstPtr, addx);

    return  val;
}


/*****************************************************************************/

const int   MAX_INL_ARGS =      6;      // does not include obj pointer
const int   MAX_INL_LCLS =      8;

/*****************************************************************************
 */

CorInfoInline  Compiler::impCanInline1(CORINFO_METHOD_HANDLE fncHandle,
                                       unsigned              methAttr,
                                       CORINFO_CLASS_HANDLE  clsHandle,
                                       unsigned              clsAttr)
    {
    /* Do not inline functions inside <clinit> */

    if ((info.compFlags & FLG_CCTOR) == FLG_CCTOR)
    {
        JITLOG((LL_EVERYTHING, "INLINER FAILED: Do not inline method inside <clinit>:"
                " %s called by %s\n",
                eeGetMethodFullName(fncHandle), info.compFullName));

        /* Return but do not mark the method as not inlinable */
        return INLINE_FAIL;
    }

    /* Check if we tried to inline this method before */

    if (methAttr & CORINFO_FLG_DONT_INLINE)
    {
        JITLOG((LL_EVERYTHING, "INLINER FAILED: Method marked as not inline: "
                "%s called by %s\n",
                eeGetMethodFullName(fncHandle), info.compFullName));

        return INLINE_FAIL;
    }

    /* Do not inline if caller or callee need security checks */

    if (methAttr & CORINFO_FLG_SECURITYCHECK)
    {
        JITLOG((LL_EVERYTHING, "INLINER FAILED: Callee needs security check: "
                "%s called by %s\n",
                eeGetMethodFullName(fncHandle), info.compFullName));

        return INLINE_NEVER;
    }

    /* In the caller case do not mark as not inlinable */

    if (opts.compNeedSecurityCheck)
    {
        JITLOG((LL_EVERYTHING, "INLINER FAILED: Caller needs security check: "
                "%s called by %s\n",
                eeGetMethodFullName(fncHandle), info.compFullName));

        return INLINE_FAIL;
    }

    /* Cannot inline the method if it's class has not been initialized
       as we dont want inlining to force loading of extra classes */

    /* methods marked RUN_CCTOR will be OK since the inliner puts in the
       clinit call, also non-statics - non-constructors are OK because to have a 
       'this' pointer you had to have an instance (which implies that cctor is run */

    if (clsHandle && 
        !((clsAttr & CORINFO_FLG_INITIALIZED)
          || !(clsAttr & CORINFO_FLG_NEEDS_INIT)
          || info.compCompHnd->initClass(clsHandle, info.compMethodHnd, TRUE)) &&  
        clsHandle != info.compClassHnd           &&
        !(methAttr & CORINFO_FLG_RUN_CCTOR)      && 
        ((methAttr & CORINFO_FLG_STATIC)      ||
         (methAttr & CORINFO_FLG_CONSTRUCTOR) || 
         ( clsAttr & CORINFO_FLG_VALUECLASS)))
    {
        JITLOG((LL_EVERYTHING, "INLINER FAILED: Method class is not initialized: "
                "%s called by %s\n",
                eeGetMethodFullName(fncHandle), info.compFullName));

        /* Return but do not mark the method as not inlinable */
        return INLINE_FAIL;
    }

    return INLINE_PASS;
}

/*****************************************************************************
 */

CorInfoInline  Compiler::impCanInline2(CORINFO_METHOD_HANDLE    fncHandle,
                                       unsigned                 methAttr,
                                       CORINFO_METHOD_INFO *    methInfo)
{
    unsigned    codeSize = methInfo->ILCodeSize;

    if (methInfo->EHcount || (methInfo->ILCode == 0) || (codeSize == 0))
        return INLINE_NEVER;

    /* For now we don't inline varargs (import code can't handle it) */

    if (methInfo->args.isVarArg())
        return INLINE_NEVER;

    /* Reject if it has too many locals */

    if (methInfo->locals.numArgs > MAX_INL_LCLS)
    {
        JITLOG((LL_EVERYTHING, "INLINER FAILED: Method has %u locals: "
                "%s called by %s\n",
                methInfo->locals.numArgs, eeGetMethodFullName(fncHandle),
                info.compFullName));

        return INLINE_NEVER;
    }

    /* Make sure there aren't too many arguments */

    if  (methInfo->args.numArgs > MAX_INL_ARGS)
    {
        JITLOG((LL_EVERYTHING, "INLINER FAILED: Method has %u arguments: "
                "%s called by %s\n",
                methInfo->args.numArgs, eeGetMethodFullName(fncHandle),
                info.compFullName));

        return INLINE_NEVER;
    }

    unsigned threshold = impInlineSize;


    // Are we in a rarely executed block?  If so, don't inline too much

    if (compCurBB->isRunRarely())
    {
        // Basically only allow inlining that will shrink the
        // X86 code size, so things like empty methods, field accessors
        // rebound calls, literal constants, ...
        threshold = MAX_NONEXPANDING_INLINE_SIZE;
    }

    /* Bump up the threshold if there are arguments, as we would avoid
       the expense of setting them up */

    threshold += 3 * methInfo->args.totalILArgs();

    if  (codeSize > threshold)
    {
        JITLOG((LL_EVERYTHING, "INLINER FAILED: Method is too big threshold "
                "%d codesize %d: %s called by %s\n",
                threshold, codeSize, eeGetMethodFullName(fncHandle),
                info.compFullName));

        return INLINE_FAIL;
    }

    JITLOG((LL_EVERYTHING, "INLINER: Considering %u instrs of %s called by %s\n",
            codeSize, eeGetMethodFullName(fncHandle), info.compFullName));

    /* Make sure maxstack it's not too big */

    if  (methInfo->maxStack > impStkSize)
    {
        // Make sure that we are using the small stack. Without the small stack,
        // we will silently stop inlining a bunch of methods.
        assert(impStkSize >= sizeof(impSmallStack)/sizeof(impSmallStack[0]));

        JITLOG((LL_EVERYTHING, "INLINER FAILED: Method has %u MaxStack "
                "bigger than callee stack %u: %s called by %s\n",
                methInfo->maxStack, info.compMaxStack,
                eeGetMethodFullName(fncHandle), info.compFullName));

        return INLINE_FAIL;
    }

    /* For now, fail if the function returns a struct */

    if (methInfo->args.retType == CORINFO_TYPE_VALUECLASS || methInfo->args.retType == CORINFO_TYPE_REFANY)
    {
        JITLOG((LL_INFO100000, "INLINER FAILED: Method %s returns a value class "
                "called from %s\n",
                eeGetMethodFullName(fncHandle), info.compFullName));

        return INLINE_NEVER;
    }

    return INLINE_PASS;
}

GenTreePtr  Compiler::impInlineFetchArg(unsigned lclNum, InlArgInfo *inlArgInfo, InlLclVarInfo*   lclVarInfo)
{
    /* Get the argument type */
    var_types lclTyp  = lclVarInfo[lclNum].lclTypeInfo;
    assert(lclTyp != TYP_STRUCT);

    GenTreePtr op1 = NULL;

    if (inlArgInfo[lclNum].argIsConst)
    {
        /* Clone the constant. Note that we cannot directly use argNode
        in the trees even if inlArgInfo[lclNum].argIsUsed==0 as this
        would introduce aliasing between inlArgInfo[].argNode and
        impInlineExpr. Then gtFoldExpr() could bash it, causing further
        references to the argument working off of the bashed copy. */
        
        op1 = gtCloneExpr(inlArgInfo[lclNum].argNode);
        inlArgInfo[lclNum].argTmpNum = -1;      // illegal temp
    }
    else if (inlArgInfo[lclNum].argIsLclVar)
    {
        /* Argument is a local variable (of the caller)
         * Can we re-use the passed argument node? */
        
        op1 = inlArgInfo[lclNum].argNode;
        inlArgInfo[lclNum].argTmpNum = op1->gtLclVar.gtLclNum;
        
        if (inlArgInfo[lclNum].argIsUsed)
        {
            assert(op1->gtOper == GT_LCL_VAR);
            assert(lclNum == op1->gtLclVar.gtLclILoffs);
            
            if (!lvaTable[op1->gtLclVar.gtLclNum].lvNormalizeOnLoad())
                lclTyp = genActualType(lclTyp);
            
            /* Create a new lcl var node - remember the argument lclNum */
            op1 = gtNewLclvNode(op1->gtLclVar.gtLclNum, lclTyp, op1->gtLclVar.gtLclILoffs);
        }
    }
    else
    {
        /* Argument is a complex expression - has it been eval to a temp */
        
        if (inlArgInfo[lclNum].argHasTmp)
        {
            assert(inlArgInfo[lclNum].argIsUsed);
            assert(inlArgInfo[lclNum].argTmpNum < lvaCount);
            
            /* Create a new lcl var node - remember the argument lclNum */
            op1 = gtNewLclvNode(inlArgInfo[lclNum].argTmpNum, genActualType(lclTyp));
            
            /* This is the second or later use of the this argument,
            so we have to use the temp (instead of the actual arg) */
            inlArgInfo[lclNum].argBashTmpNode = NULL;
        }
        else
        {
            /* First time use */
            assert(inlArgInfo[lclNum].argIsUsed == false);
            
            /* Reserve a temp for the expression.
            * Use a large size node as we may bash it later */
            
            unsigned tmpNum = lvaGrabTemp();
            
            lvaTable[tmpNum].lvType = lclTyp;
            lvaTable[tmpNum].lvAddrTaken = 0;
            
            inlArgInfo[lclNum].argHasTmp = true;
            inlArgInfo[lclNum].argTmpNum = tmpNum;
            
            /* If we require strict exception order then, arguments must
            be evaulated in sequence before the body of the inlined
            method. So we need to evaluate them to a temp.
            Also, if arguments have global references, we need to
            evaluate them to a temp before the inlined body as the
            inlined body may be modifying the global ref.
            */
            
            if ((!inlArgInfo[lclNum].argHasSideEff || info.compLooseExceptions) &&
                (!inlArgInfo[lclNum].argHasGlobRef))
            {
                /* Get a *LARGE* LCL_VAR node */
                op1 = gtNewLclLNode(tmpNum, genActualType(lclTyp), lclNum);
                
                /* Record op1 as the very first use of this argument.
                If there are no further uses of the arg, we may be
                able to use the actual arg node instead of the temp.
                If we do see any further uses, we will clear this. */
                inlArgInfo[lclNum].argBashTmpNode = op1;
            }
            else
            {
                /* Get a small LCL_VAR node */
                op1 = gtNewLclvNode(tmpNum, genActualType(lclTyp));
                /* No bashing of this argument */
                inlArgInfo[lclNum].argBashTmpNode = NULL;
            }
        }
    }
    
    /* Mark the argument as used */
    
    inlArgInfo[lclNum].argIsUsed = true;

    return op1;
}

/*****************************************************************************
 *
 */

CorInfoInline  Compiler::impInlineInitVars(GenTreePtr          call,
                                                    CORINFO_METHOD_HANDLE       fncHandle,
                                                    CORINFO_CLASS_HANDLE    clsHandle,
                                                    CORINFO_METHOD_INFO *   methInfo,
                                                    unsigned            clsAttr,
                                                    InlArgInfo      *   inlArgInfo,
                                                    InlLclVarInfo   *   lclVarInfo)
{

    /* init the argument stuct */

    memset(inlArgInfo, 0, (MAX_INL_ARGS + 1) * sizeof(inlArgInfo[0]));

    /* Get hold of the 'this' pointer and the argument list proper */

    GenTreePtr  thisArg = call->gtCall.gtCallObjp;
    GenTreePtr  argList = call->gtCall.gtCallArgs;

    /* Count the arguments */

    unsigned    argCnt = 0;

    if  (thisArg)
    {
        inlArgInfo[0].argNode = thisArg;

        if (thisArg->gtFlags & GTF_OTHER_SIDEEFF)
        {
            // Right now impInlineSpillLclRefs and impInlineSpillGlobEffects dont take
            // into account special side effects, so we disallow them during inlining.
            JITLOG((LL_INFO100000, "INLINER FAILED: argument has other side effect: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
            return INLINE_FAIL;
        }

        if (thisArg->gtFlags & GTF_GLOB_EFFECT)
        {
            inlArgInfo[0].argHasGlobRef = (thisArg->gtFlags & GTF_GLOB_REF   ) != 0;
            inlArgInfo[0].argHasSideEff = (thisArg->gtFlags & GTF_SIDE_EFFECT) != 0;
        }
        else if (thisArg->OperKind() & GTK_CONST)
        {
            if (thisArg->gtOper == GT_CNS_INT && thisArg->gtIntCon.gtIconVal == 0)
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: Null this pointer: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));

                /* Abort, but do not mark as not inlinable */
                return INLINE_FAIL;
            }

            inlArgInfo[0].argIsConst = true;
        }
        else if (thisArg->gtOper == GT_LCL_VAR)
        {
            assert(!lvaTable[thisArg->gtLclVar.gtLclNum].lvAddrTaken); // or GTF_GLOB_REF should have been set
            inlArgInfo[0].argIsLclVar = true;

            /* Remember the "original" argument number */
            thisArg->gtLclVar.gtLclILoffs = 0;
        }

        argCnt++;
    }

    /* Record all possible data about the arguments */

    for (GenTreePtr argTmp = argList; argTmp; argTmp = argTmp->gtOp.gtOp2)
    {
        GenTreePtr      argVal;

        assert(argTmp->gtOper == GT_LIST);
        argVal = argTmp->gtOp.gtOp1;

        inlArgInfo[argCnt].argNode = argVal;

        if (argVal->gtFlags & GTF_OTHER_SIDEEFF)
        {
            // Right now impInlineSpillLclRefs and impInlineSpillGlobEffects dont take
            // into account special side effects, so we disallow them during inlining.
            JITLOG((LL_INFO100000, "INLINER FAILED: argument has other side effect: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
            return INLINE_FAIL;
        }

        if (argVal->gtFlags & GTF_GLOB_EFFECT)
        {
            inlArgInfo[argCnt].argHasGlobRef = (argVal->gtFlags & GTF_GLOB_REF   ) != 0;
            inlArgInfo[argCnt].argHasSideEff = (argVal->gtFlags & GTF_SIDE_EFFECT) != 0;
        }
        else if (argVal->gtOper == GT_LCL_VAR)
        {
            assert(!lvaTable[argVal->gtLclVar.gtLclNum].lvAddrTaken); // or GTF_GLOB_REF should have been set

            inlArgInfo[argCnt].argIsLclVar = true;

            /* Remember the "original" argument number */
            argVal->gtLclVar.gtLclILoffs = argCnt;
        }
        else if (argVal->OperKind() & GTK_CONST)
        {
            inlArgInfo[argCnt].argIsConst = true;
        }

#ifdef DEBUG
        if (verbose)
        {
            printf("\nArgument #%u:", argCnt);
            if  (inlArgInfo[argCnt].argIsLclVar)
                printf(" is a local var");
            if  (inlArgInfo[argCnt].argIsConst)
                printf(" is a constant");
            if  (inlArgInfo[argCnt].argHasGlobRef)
                printf(" has global refs");
            if  (inlArgInfo[argCnt].argHasSideEff)
                printf(" has side effects");

            printf("\n");
            gtDispTree(argVal);
            printf("\n");
        }
#endif

        /* Count this argument */

        argCnt++;
    }

    /* Make sure we got the arg number right */
    assert(argCnt == methInfo->args.totalILArgs());

    /* We have typeless opcodes, get type information from the signature */

    if (thisArg)
    {
        var_types   sigType;

        if (clsAttr & CORINFO_FLG_VALUECLASS)
                sigType = TYP_BYREF;
        else
            sigType = TYP_REF;
        lclVarInfo[0].lclTypeInfo = sigType;

        assert(varTypeIsGC(thisArg->gtType) ||      // "this" is managed
               (thisArg->gtType == TYP_I_IMPL &&    // "this" is unmgd but the method's class doesnt care                
                (clsAttr & CORINFO_FLG_VALUECLASS)));

        if (genActualType(thisArg->gtType) != genActualType(sigType))
        {
            /* This can only happen with byrefs <-> ints/shorts */

            assert(genActualType(sigType) == TYP_INT || sigType == TYP_BYREF);
            assert(genActualType(thisArg->gtType) == TYP_INT  ||
                                 thisArg->gtType  == TYP_BYREF );

            if (sigType == TYP_BYREF)
            {
                /* Arguments 'byref <- int' not currently supported */
                JITLOG((LL_INFO100000, "INLINER FAILED: Arguments 'byref <- int' "
                        "not currently supported: %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));

                return INLINE_FAIL;
            }
            else if (thisArg->gtType == TYP_BYREF)
            {
                assert(sigType == TYP_I_IMPL);

                /* If possible bash the BYREF to an int */
                if (thisArg->IsVarAddr())
                {
                    thisArg->gtType = TYP_INT;
                    lclVarInfo[0].lclVerTypeInfo = typeInfo(TI_INT);
                }
                else
                {
                    /* Arguments 'int <- byref' cannot be bashed */
                    JITLOG((LL_INFO100000, "INLINER FAILED: Arguments 'int <- byref' "
                            "cannot be bashed: %s called by %s\n",
                            eeGetMethodFullName(fncHandle), info.compFullName));

                    return INLINE_FAIL;
                }
            }
        }

    }

    /* Init the types of the arguments and make sure the types
     * from the trees match the types in the signature */

    CORINFO_ARG_LIST_HANDLE     argLst = methInfo->args.args;

    for(unsigned i = (thisArg ? 1 : 0); i < argCnt; i++, argLst = eeGetArgNext(argLst))
    {
        var_types sigType = (var_types) eeGetArgType(argLst, &methInfo->args);

        /* For now do not handle structs */

        if (sigType == TYP_STRUCT)
        {
            JITLOG((LL_INFO100000, "INLINER FAILED: No TYP_STRUCT arguments allowed: "
                    "%s called by %s\n",
                    eeGetMethodFullName(fncHandle), info.compFullName));

            return INLINE_NEVER;
        }

        lclVarInfo[i].lclVerTypeInfo = verParseArgSigToTypeInfo(&methInfo->args, argLst);
        lclVarInfo[i].lclTypeInfo = sigType;

        /* Does the tree type match the signature type? */

        GenTreePtr inlArgNode = inlArgInfo[i].argNode;

        if (sigType != inlArgNode->gtType)
        {
            /* This can only happen for short integer types or byrefs <-> ints */

            assert(genActualType(sigType) == TYP_INT || sigType == TYP_BYREF);
            assert(genActualType(inlArgNode->gtType) == TYP_INT  ||
                                 inlArgNode->gtType  == TYP_BYREF );

            /* Is it a narrowing or widening cast?
             * Widening casts are ok since the value computed is already
             * normalized to an int (on the IL stack) */

            if (genTypeSize(inlArgNode->gtType) >= genTypeSize(sigType))
            {
                if (sigType == TYP_BYREF)
                {
                    /* Arguments 'byref <- int' not currently supported */
                    JITLOG((LL_INFO100000, "INLINER FAILED: Arguments 'byref <- int' "
                            "not currently supported: %s called by %s\n",
                            eeGetMethodFullName(fncHandle), info.compFullName));

                    return INLINE_FAIL;
                }
                else if (inlArgNode->gtType == TYP_BYREF)
                {
                    assert(sigType == TYP_I_IMPL);

                    /* If possible bash the BYREF to an int */
                    if (inlArgNode->IsVarAddr())
                    {
                        inlArgNode->gtType = TYP_INT;
                        lclVarInfo[i].lclVerTypeInfo = typeInfo(TI_INT);
                    }
                    else
                    {
                        /* Arguments 'int <- byref' cannot be bashed */
                        JITLOG((LL_INFO100000, "INLINER FAILED: Arguments 'int <- byref' "
                                "cannot be bashed: %s called by %s\n",
                                eeGetMethodFullName(fncHandle), info.compFullName));

                        return INLINE_FAIL;
                    }
                }
                else if (genTypeSize(sigType) < 4 /* && sigType != TYP_BOOL @TODO [REVISIT] [04/16/01] [vancem]*/)
                {
                    /* Narrowing cast */

                    assert(genTypeSize(sigType) == 2 || genTypeSize(sigType) == 1);

                    if (inlArgNode->gtOper == GT_LCL_VAR &&
                        !lvaTable[inlArgNode->gtLclVar.gtLclNum].lvNormalizeOnLoad() &&
                        sigType == lvaGetRealType(inlArgNode->gtLclVar.gtLclNum))
                    {
                        /* We dont need to insert a cast here as the variable
                           was assigned a normalized value of the right type */

                        continue;
                    }

                    inlArgNode            =
                    inlArgInfo[i].argNode = gtNewCastNode (TYP_INT,
                                                           inlArgNode, 
                                                           sigType);

                    inlArgInfo[i].argIsLclVar = false;

                    /* Try to fold the node in case we have constant arguments */

                    if (inlArgInfo[i].argIsConst)
                    {
                        inlArgNode            =
                        inlArgInfo[i].argNode = gtFoldExprConst(inlArgNode);
                        assert(inlArgNode->OperIsConst());
                    }
                }
            }
        }
    }

    /* Init the types of the local variables */

    CORINFO_ARG_LIST_HANDLE     localsSig = methInfo->locals.args;

    for(i = 0; i < methInfo->locals.numArgs; i++)
    {
        bool isPinned;
        var_types type = (var_types) eeGetArgType(localsSig, &methInfo->locals, &isPinned);

        lclVarInfo[i + argCnt].lclTypeInfo = type;

        if (isPinned)
        {
            JITLOG((LL_INFO100000, "INLINER FAILED:pinned locals allowed: "
                    "%s called by %s\n",
                    eeGetMethodFullName(fncHandle), info.compFullName));
            return INLINE_NEVER;
        }


        /* For now do not handle structs */

        if (lclVarInfo[i + argCnt].lclTypeInfo == TYP_STRUCT)
        {
            JITLOG((LL_INFO100000, "INLINER FAILED: No TYP_STRUCT locals allowed: "
                    "%s called by %s\n",
                    eeGetMethodFullName(fncHandle), info.compFullName));

            return INLINE_NEVER;
        }

        lclVarInfo[i + argCnt].lclVerTypeInfo = verParseArgSigToTypeInfo(&methInfo->locals, localsSig);
        localsSig = eeGetArgNext(localsSig);
    }

    return INLINE_PASS;
}

/*****************************************************************************
 *
 *  See if the given method and argument list can be expanded inline.
 *
 *  NOTE: Use the following logging levels for inlining info
 *        LL_INFO10000:  Use when reporting successful inline of a method
 *        LL_INFO100000:  Use when reporting NYI stuff about the inliner
 *        LL_INFO1000000:  Use when reporting UNUSUAL situations why inlining FAILED
 *        LL_EVERYTHING:  Use when WARNING about incoming flags that prevent inlining
 *        LL_EVERYTHING: Verbose info including NORMAL inlining failures
 */

CorInfoInline      Compiler::impExpandInline(GenTreePtr      tree,
                                                      CORINFO_METHOD_HANDLE   fncHandle,
                                                      GenTreePtr  *   pInlinedTree)

{
    GenTreePtr      bodyExp = 0;

    InlArgInfo      inlArgInfo [MAX_INL_ARGS + 1];
    int             lclTmpNum  [MAX_INL_LCLS];    // map local# -> temp# (-1 if unused)
    InlLclVarInfo   lclVarInfo[MAX_INL_LCLS + MAX_INL_ARGS + 1];  // type information from local sig

    bool            inlineeHasRangeChks = false;
    bool            inlineeHasNewArray  = false;
    bool            dupOfLclVar         = false;
    bool            staticAccessedFirst = false;        // static accessed before any side effect
    bool            didSideEffect       = false;        // did a side effect
    bool            thisAccessedFirst   = false;        // this fetched before any side effect (no null check needed)

#define INLINE_CONDITIONALS 1

#if     INLINE_CONDITIONALS

    GenTreePtr      stmList;                // pre-condition statement list
    GenTreePtr      ifStmts;                // contents of 'if' when in 'else'
    GenTreePtr      ifCondx;                // condition of 'if' statement
    bool            ifNvoid = false;        // does 'if' yield non-void value?
    const   BYTE *  jmpAddr = NULL;         // current pending jump address
    bool            inElse  = false;        // are we in an 'else' part?

    bool            hasCondReturn = false;  // do we have ret from a conditional
    unsigned        retLclNum;              // the return lcl var #

#endif

#ifdef DEBUG
    bool            hasFOC = false;         // has flow-of-control
#endif

    assert(fgGlobalMorph && fgExpandInline);

    /* This assert only indicates that HOIST_THIS_FLDS doesnt work for a
       method with calls, even if those calls are inlined away as
       optHoistTFRhasCall() eagerly sets optThisFldDont.
       @TODO [CONSIDER] [04/16/01] []: Perform inlining and then notice if any 
       calls are left (which would interfere with HOIST_THIS_FLDS */

    assert(optThisFldDont);

    /* Get the method properties */

    CORINFO_CLASS_HANDLE    clsHandle   = eeGetMethodClass(fncHandle);

    unsigned        methAttr    = eeGetMethodAttribs(fncHandle);
    unsigned        clsAttr     = clsHandle ? eeGetClassAttribs(clsHandle) : 0;
    GenTreePtr      callObj;

    /* Does the method look acceptable ? */

    CorInfoInline   result = impCanInline1(fncHandle, methAttr, clsHandle, clsAttr);
    if (dontInline(result))             // inlining failed
        return result;

    /* Try to get the code address/size for the method */

    CORINFO_METHOD_INFO methInfo;

    if (!eeGetMethodInfo(fncHandle, &methInfo))
        return INLINE_NEVER;

    /* Reject the method if it has exceptions or looks weird */

    result = impCanInline2(fncHandle, methAttr, &methInfo);
    if (dontInline(result))
        return result;

    /* Get the return type */

    var_types       fncRetType = tree->TypeGet();

    assert(genActualType(JITtype2varType(methInfo.args.retType)) ==
           genActualType(fncRetType));

    /* How many locals before we start grabbing inliner temps ? */

    unsigned    startVars = lvaCount;

    result = impInlineInitVars(tree, fncHandle, clsHandle, &methInfo, clsAttr, inlArgInfo, lclVarInfo);
    if (dontInline(result))
        return result;

    // Given the EE the final say in whether to inline or not.  
    // This shoudl be last since for veriable code, this can be expensive

    CORINFO_ACCESS_FLAGS  aflags = CORINFO_ACCESS_ANY;
    GenTreePtr      thisArg     = tree->gtCall.gtCallObjp;
    if (impIsThis(thisArg))
        aflags = CORINFO_ACCESS_THIS;

    /* Cannot inline across assemblies also insures that the method is verifiable if needed */
    result = eeCanInline(info.compMethodHnd, fncHandle, aflags);
    if (dontInline(result))
    {
        JITLOG((LL_INFO1000000, "INLINER FAILED: Inline rejected the inline : "
                "%s called by %s\n",
                eeGetMethodFullName(fncHandle), info.compFullName));
        return result;
    }

    // if we have to resepect the call boundry, we will only succeed if there are no calls
    // so bias ourselves toward only trying smaller routines.
    if (result == INLINE_RESPECT_BOUNDARY)
    {
            // @TODO [REVISIT] [04/16/01] []: for now we will be conservative, if we 
            // can be certain that we don't waste much time here we can rase
            // this to 1/2 way or even leaving the threshold unchanged
        if (methInfo.ILCodeSize > MAX_NONEXPANDING_INLINE_SIZE + 3) 
        {
            JITLOG((LL_EVERYTHING, "INLINER FAILED: Method is too big for cross assembly inline "
                    "codesize %d: %s called by %s\n",
                    methInfo.ILCodeSize, eeGetMethodFullName(fncHandle),
                    info.compFullName));
            return INLINE_FAIL;
        }
    }

    /* Clear the temp table */

    memset(lclTmpNum, -1, sizeof(lclTmpNum));

    CORINFO_MODULE_HANDLE    scpHandle = methInfo.scope;

    /* The stack is empty */

    verCurrentState.esStackDepth = 0;

    /* Initialize the inlined "statement" list */

    impInlineExpr = 0;

    /* Get the method IL */

    const BYTE *    codeAddr = methInfo.ILCode;
    size_t          codeSize = methInfo.ILCodeSize;

    const   BYTE *  codeBegp = methInfo.ILCode;
    const   BYTE *  codeEndp = codeBegp + codeSize;;

    unsigned        argCnt   = methInfo.args.totalILArgs();
    unsigned        lclCnt   = methInfo.locals.numArgs;
    bool            volatil  = false;

#ifdef DEBUG
    if (verbose) 
        printf("Inliner considering %2u instrs of %s:\n",
               codeSize, eeGetMethodFullName(fncHandle));
#endif

    /* Convert the opcodes of the method into an expression tree */

    while (codeAddr <= codeEndp)
    {
        int         sz      = 0;
        OPCODE      opcode;

#ifdef DEBUG
        callObj     = (GenTreePtr) 0xDEADBEEF;
#endif

#if INLINE_CONDITIONALS

        /* Have we reached the target of a pending if/else statement? */

        if  (jmpAddr == codeAddr)
        {
            GenTreePtr      fulStmt;
            GenTreePtr      noStmts = NULL;

            /* An end of 'if' (without 'else') or end of 'else' */

            if  (inElse)
            {
                /* The 'else' part is the current statement list */

                noStmts = impInlineExpr;

                /* The end of 'if/else' - does the 'else' yield a value? */

                if  (verCurrentState.esStackDepth)
                {
                    /* We return a non void value */

                    if  (ifNvoid == false)
                    {
                        JITLOG((LL_INFO100000, "INLINER FAILED: If returns a value, "
                                "else doesn't: %s called by %s\n",
                                eeGetMethodFullName(fncHandle), info.compFullName));
                        goto ABORT;
                    }

                    /* We must have an 'if' part */

                    assert(ifStmts);

                    if  (verCurrentState.esStackDepth > 1)
                    {
                        JITLOG((LL_INFO100000, "INLINER FAILED: More than one return "
                                "value in else: %s called by %s\n",
                                eeGetMethodFullName(fncHandle), info.compFullName));
                        goto ABORT;
                    }

                    /* Both the 'if' and 'else' yield one value */

                    noStmts = impConcatExprs(noStmts, impPopStack().val);
                    assert(noStmts);

                    /* Make sure both parts have matching types */

                    if (genActualType(ifStmts->gtType) != genActualType(noStmts->gtType)) {
                        JITLOG((LL_INFO100000, "INLINER FAILED: If then differ in type: %s called by %s\n",
                                eeGetMethodFullName(fncHandle), info.compFullName));
                        goto ABORT;
                    }
               }
                else
                {
                    assert(ifNvoid == false);
                }
            }
            else
            {
                /* This is a conditional with no 'else' part
                 * The 'if' part is the current statement list */

                ifStmts = impInlineExpr;

                /* Did the 'if' part yield a value? */

                if  (verCurrentState.esStackDepth)
                {
                    if  (verCurrentState.esStackDepth > 1)
                    {
                        JITLOG((LL_INFO100000, "INLINER FAILED: More than one "
                                "return value in if: %s called by %s\n",
                                eeGetMethodFullName(fncHandle), info.compFullName));
                        goto ABORT;
                    }

                    ifStmts = impConcatExprs(ifStmts, impPopStack().val);
                    assert(ifStmts);
                }
            }

            /* Check for empty 'if' or 'else' part */

            if (ifStmts == NULL)
            {
                if (noStmts == NULL)
                {
                    /* Both 'if' and 'else' are empty - useless conditional*/

                    assert(ifCondx->OperKind() & GTK_RELOP);

                    /* Restore the original statement list */

                    impInlineExpr = stmList;

                    /* Append the side effects from the conditional
                     * @TODO [CONSIDER] [04/16/01] []: After you optimize impConcatExprs to
                     * truly extract the side effects can reduce the following to one call */

                    if  (ifCondx->gtOp.gtOp1->gtFlags & GTF_SIDE_EFFECT)
                        impInlineExpr = impConcatExprs(impInlineExpr, ifCondx->gtOp.gtOp1);

                    if  (ifCondx->gtOp.gtOp2->gtFlags & GTF_SIDE_EFFECT)
                        impInlineExpr = impConcatExprs(impInlineExpr, ifCondx->gtOp.gtOp2);

                    goto DONE_QMARK;
                }
                else
                {
                    /* Empty 'if', have 'else' - Swap the operands */

                    ifStmts = noStmts;
                    noStmts = gtNewNothingNode();

                    assert(!ifStmts->IsNothingNode());

                    /* Reverse the sense of the condition */

                    ifCondx->SetOper(GenTree::ReverseRelop(ifCondx->OperGet()));
                }
            }
            else
            {
                /* 'if' is non empty */

                if (noStmts == NULL)
                {
                    /* The 'else' is empty */
                    noStmts = gtNewNothingNode();
                }
            }

            /* At this point 'ifStmt/noStmts' are the 'if/else' parts */

            assert(ifStmts);
            assert(!ifStmts->IsNothingNode());
            assert(noStmts);

            var_types   typ;

            /* Set the type to void if there is no value */

            typ = ifNvoid ? ifStmts->TypeGet() : TYP_VOID;

            /* FP values are not handled currently */

            assert(!varTypeIsFloating(typ));

            /* Do not inline longs at this time */

            if (typ == TYP_LONG)
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: Inlining of conditionals "
                        "that return LONG NYI: %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }

            if (USE_GT_LOG)
            {
                int         sense;

                long        val1;
                long        val2;

                /* Check for the case "(val ==/!= 0) ? 0/1 : 1/0" */

                if      (ifCondx->gtOper == GT_EQ)
                {
                    sense = 0;
                }
                else if (ifCondx->gtOper == GT_NE)
                {
                    sense = 1;
                }
                else
                    goto NOT_LOG;

                if  (ifCondx->gtOp.gtOp2->gtOper != GT_CNS_INT)
                    goto NOT_LOG;
                if  (ifCondx->gtOp.gtOp2->gtIntCon.gtIconVal != 0)
                    goto NOT_LOG;

                /* The simplest case is "cond ? 1/0 : 0/1" */

                if  (ifStmts->gtOper == GT_CNS_INT &&
                     noStmts->gtOper == GT_CNS_INT)
                {
                    // UNDONE: Do the rest of this thing ....

                }

                /* Now see if we have "dest = cond ? 1/0 : 0/1" */

                if  (ifStmts->gtOper != GT_ASG)
                    goto NOT_LOG;
                if  (ifStmts->gtOp.gtOp2->gtOper != GT_CNS_INT)
                    goto NOT_LOG;
                val1 = ifStmts->gtOp.gtOp2->gtIntCon.gtIconVal;
                if  (val1 != 0 && val1 != 1)
                    goto NOT_LOG;

                if  (noStmts->gtOper != GT_ASG)
                    goto NOT_LOG;
                if  (noStmts->gtOp.gtOp2->gtOper != GT_CNS_INT)
                    goto NOT_LOG;
                val2 = noStmts->gtOp.gtOp2->gtIntCon.gtIconVal;
                if  (val2 != (val1 ^ 1))
                    goto NOT_LOG;

                /* Make sure the assignment targets are the same */

                if  (!GenTree::Compare(ifStmts->gtOp.gtOp1,
                                       noStmts->gtOp.gtOp1))
                    goto NOT_LOG;

                /* We have the right thing, it would appear */

                fulStmt = ifStmts;
                fulStmt->gtOp.gtOp2 = gtNewOperNode((sense ^ val1) ? GT_LOG0
                                                                   : GT_LOG1,
                                                    TYP_INT,
                                                    ifCondx->gtOp.gtOp1);

                goto DONE_IF;
            }

NOT_LOG:

            /* Create the "?:" expression */

            fulStmt = gtNewOperNode(GT_COLON, typ, ifStmts, noStmts);
            fulStmt = gtNewOperNode(GT_QMARK, typ, ifCondx, fulStmt);

            /* Mark the condition of the ?: node */

            ifCondx->gtFlags |= GTF_RELOP_QMARK;

DONE_IF:

            /* Restore the original expression */

            impInlineExpr = stmList;

            /* Does the ?: expression yield a non-void value? */

            if  (ifNvoid)
            {
                /* Push the entire statement on the stack */

                // @TODO [CONSIDER] [04/16/01] []: extract any comma prefixes and append them

                assert(fulStmt->gtType != TYP_VOID);

                
                impPushOnStackNoType(fulStmt);
            }
            else
            {
                /* 'if' yielded no value, just append the ?: */

                assert(fulStmt->gtType == TYP_VOID);
                impInlineExpr = impConcatExprs(impInlineExpr, fulStmt);
            }

            if (inElse && hasCondReturn)
            {
                assert(jmpAddr == codeEndp);
                assert(ifNvoid == false);
                assert(fncRetType != TYP_VOID);

                /* The return value is the return local variable */
                bodyExp = gtNewLclvNode(retLclNum, fncRetType);
            }

DONE_QMARK:
            /* We're no longer in an 'if' statement */

            jmpAddr = NULL;
        }

#endif  // INLINE_CONDITIONALS

        /* Done importing the instr */

        if (codeAddr == codeEndp)
            goto DONE;

        /* Get the next opcode and the size of its parameters */

        opcode = (OPCODE) getU1LittleEndian(codeAddr);
        codeAddr += sizeof(__int8);

#ifdef  DEBUG
        impCurOpcOffs   = codeAddr - codeBegp - 1;

        if  (verbose)
            printf("\n[%2u] %3u (0x%03x)",
                   verCurrentState.esStackDepth, impCurOpcOffs, impCurOpcOffs);
#endif

DECODE_OPCODE:

        /* Get the size of additional parameters */

        sz = opcodeSizes[opcode];

#ifdef  DEBUG

        impCurOpcOffs   = codeAddr - codeBegp - 1;
        impCurOpcName   = opcodeNames[opcode];

        if (verbose && (controlFlow[opcode] != META))
            printf(" %s", impCurOpcName);

#endif

        /* See what kind of an opcode we have, then */

        switch (opcode)
        {
            unsigned        lclNum, tmpNum;
            //unsigned        initLclNum;
            var_types       lclTyp, type, callTyp;

            genTreeOps      oper;
            GenTreePtr      op1, op2, tmp;
            GenTreePtr      thisPtr, arr;

            int             memberRef;
            int             typeRef;
            int             val;

            CORINFO_CLASS_HANDLE    clsHnd;
            CORINFO_METHOD_HANDLE   methHnd;
            CORINFO_FIELD_HANDLE    fldHnd;

            CORINFO_SIG_INFO    sig;

#if INLINE_CONDITIONALS
            signed          jmpDist;
            bool            unordered;
#endif

            unsigned        clsFlags;
            unsigned        flags, mflags;

          //unsigned        offs;
            bool            ovfl, uns;
            bool            callNode;

            union
            {
                long            intVal;
                float           fltVal;
                __int64         lngVal;
                double          dblVal;
            }
                            cval;

            case CEE_PREFIX1:
                opcode = OPCODE(getU1LittleEndian(codeAddr) + 256);
                codeAddr += sizeof(__int8);
                goto DECODE_OPCODE;

        case CEE_LDNULL:
            impPushNullObjRefOnStack();
            break;

        case CEE_LDC_I4_M1 :
        case CEE_LDC_I4_0 :
        case CEE_LDC_I4_1 :
        case CEE_LDC_I4_2 :
        case CEE_LDC_I4_3 :
        case CEE_LDC_I4_4 :
        case CEE_LDC_I4_5 :
        case CEE_LDC_I4_6 :
        case CEE_LDC_I4_7 :
        case CEE_LDC_I4_8 :
            cval.intVal = (opcode - CEE_LDC_I4_0);
            assert(-1 <= cval.intVal && cval.intVal <= 8);
            goto PUSH_I4CON;

        case CEE_LDC_I4_S: cval.intVal = getI1LittleEndian(codeAddr); goto PUSH_I4CON;
        case CEE_LDC_I4:   cval.intVal = getI4LittleEndian(codeAddr); goto PUSH_I4CON;
        PUSH_I4CON:
            impPushOnStackNoType(gtNewIconNode(cval.intVal));
            break;

        case CEE_LDC_I8:
            cval.lngVal = getI8LittleEndian(codeAddr);
            impPushOnStackNoType(gtNewLconNode(cval.lngVal));
            break;

        case CEE_LDC_R8:
            cval.dblVal = getR8LittleEndian(codeAddr);
            impPushOnStackNoType(gtNewDconNode(cval.dblVal));
            break;

        case CEE_LDC_R4:
            cval.dblVal = getR4LittleEndian(codeAddr);
            impPushOnStackNoType(gtNewDconNode(cval.dblVal));
            break;

        case CEE_LDSTR:
            val = getU4LittleEndian(codeAddr);
            impPushOnStackNoType(gtNewSconNode(val, scpHandle));
            break;

        case CEE_LDARG_0:
        case CEE_LDARG_1:
        case CEE_LDARG_2:
        case CEE_LDARG_3:
                lclNum = (opcode - CEE_LDARG_0);
                assert(lclNum >= 0 && lclNum < 4);
                goto LOAD_ARG;

        case CEE_LDARG_S:
            lclNum = getU1LittleEndian(codeAddr);
            goto LOAD_ARG;

        case CEE_LDARG:
            lclNum = getU2LittleEndian(codeAddr);

        LOAD_ARG:
            assert(lclNum < argCnt);

            /* Push the argument value on the stack */
            op1 = impInlineFetchArg(lclNum, inlArgInfo, lclVarInfo);
            impPushOnStackNoType(op1);
            break;

        case CEE_LDLOC:
            lclNum = getU2LittleEndian(codeAddr);
            goto LOAD_LCL_VAR;

        case CEE_LDLOC_0:
        case CEE_LDLOC_1:
        case CEE_LDLOC_2:
        case CEE_LDLOC_3:
            lclNum = (opcode - CEE_LDLOC_0);
            assert(lclNum >= 0 && lclNum < 4);
            goto LOAD_LCL_VAR;

        case CEE_LDLOC_S:
            lclNum = getU1LittleEndian(codeAddr);

        LOAD_LCL_VAR:

            // lclCnt would be zero for 65536 variables because it's
            // stored in a 16-bit counter.

            if (lclNum >= lclCnt)
            {
                IMPL_LIMITATION("Bad IL or unsupported 65536 local variables");
            }

            // Get the local type

            lclTyp = lclVarInfo[lclNum + argCnt].lclTypeInfo;

            assert(lclTyp != TYP_STRUCT);

            /* Have we allocated a temp for this local? */

            if  (lclTmpNum[lclNum] == -1)
            {
                /* Use before def - the method may be CORINFO_OPT_INIT_LOCALS or
                   there must be a goto or something later on. Use "0" for
                   now. We will filter out backward jumps later. */

                op1 = gtNewZeroConNode(genActualType(lclTyp));
                //impPushOnStackNoType(op1);
            }
            else
            {
                /* Get the temp lcl number */

                unsigned newLclNum; newLclNum = lclTmpNum[lclNum];

                // All vars of inlined methods should be !lvNormalizeOnLoad()

                assert(!lvaTable[newLclNum].lvNormalizeOnLoad());
                lclTyp = genActualType(lclTyp);

                op1 = gtNewLclvNode(newLclNum, lclTyp);
            }

            /* Push the local variable value on the stack */
            impPushOnStackNoType(op1);
            break;

        /* STORES */

        case CEE_STARG_S:
        case CEE_STARG:
            /* Storing into arguments not allowed */

            JITLOG((LL_INFO100000, "INLINER FAILED: Storing into arguments not "
                    "allowed: %s called by %s\n",
                    eeGetMethodFullName(fncHandle), info.compFullName));

            goto ABORT;

        case CEE_STLOC:
            lclNum = getU2LittleEndian(codeAddr);
            goto LCL_STORE;

        case CEE_STLOC_0:
        case CEE_STLOC_1:
        case CEE_STLOC_2:
        case CEE_STLOC_3:
                lclNum = (opcode - CEE_STLOC_0);
                assert(lclNum >= 0 && lclNum < 4);
                goto LCL_STORE;

        case CEE_STLOC_S:
            lclNum = getU1LittleEndian(codeAddr);

        LCL_STORE:

            // lclCnt would be zero for 65536 variables because it's
            // stored in a 16-bit counter.

            if (lclNum >= lclCnt)
            {
                IMPL_LIMITATION("Bad IL or unsupported 65536 local variables");
            }


            /* Pop the value being assigned */

            op1 = impPopStack().val;

            // We had better assign it a value of the correct type
            // All locals of inlined methods should be !lvNormalizeOnLoad()

            lclTyp = lclVarInfo[lclNum + argCnt].lclTypeInfo;

            assert(genActualType(lclTyp) == genActualType(op1->gtType) ||
                   genActualType(lclTyp) == TYP_I_IMPL && op1->IsVarAddr() ||
                   (genActualType(lclTyp) == TYP_INT && op1->gtType == TYP_BYREF)||
                   (genActualType(op1->gtType) == TYP_INT && lclTyp == TYP_BYREF)||
                    varTypeIsFloating(lclTyp) && varTypeIsFloating(op1->gtType));

            /* If op1 is "&var" then its type is the transient "*" and it can
               be used either as TYP_BYREF or TYP_I_IMPL */

            if (op1->IsVarAddr())
            {
                assert(genActualType(lclTyp) == TYP_I_IMPL || lclTyp == TYP_BYREF);

                /* When "&var" is created, we assume it is a byref. If it is
                   being assigned to a TYP_I_IMPL var, bash the type to
                   prevent unnecessary GC info */

                if (genActualType(lclTyp) == TYP_I_IMPL)
                    op1->gtType = TYP_I_IMPL;
            }

            /* Have we allocated a temp for this local? */

            tmpNum = lclTmpNum[lclNum];

            if  (tmpNum == -1)
            {
                 lclTmpNum[lclNum] = tmpNum = lvaGrabTemp();

                 lvaTable[tmpNum].lvType = lclTyp;
                 lvaTable[tmpNum].lvAddrTaken = 0;
            }

            /* Record this use of the variable slot */

            //lvaTypeRefs[tmpNum] |= Compiler::lvaTypeRefMask(op1->TypeGet());

            impInlineSpillLclRefs(tmpNum);

            /* Create the assignment node */

            // ISSUE: the code generator generates GC tracking information
            // based on the RHS of the assignment.  Later the LHS (which is
            // is a BYREF) gets used and the emitter checks that that variable 
            // is being tracked.  It is not (since the RHS was an int and did
            // not need tracking).  To keep this assert happy, we bash the RHS
            if (lclTyp == TYP_BYREF)
                op1->gtType = TYP_BYREF;

            op1 = gtNewAssignNode(gtNewLclvNode(tmpNum, genActualType(lclTyp)), op1);


INLINE_APPEND:

            /* @TODO [CONSIDER] [04/16/01] []: Spilling indiscriminantelly is too conservative */

            impInlineSpillGlobEffects();

            /* The value better have side effects */

            assert(op1->gtFlags & GTF_SIDE_EFFECT);

            /* Append to the 'init' expression */

            impInlineExpr = impConcatExprs(impInlineExpr, op1);

            break;

        case CEE_ENDFINALLY:
        case CEE_ENDFILTER:
            assert(!"Shouldn't have exception handlers in the inliner!");
            goto ABORT;

        case CEE_RET:
            if (fncRetType != TYP_VOID)
            {
                {
                bodyExp = impPopStack().val;
                }

                if (genActualType(bodyExp->TypeGet()) != fncRetType)
                {
                    JITLOG((LL_INFO100000, "INLINER FAILED: Return types are not "
                            "matching in %s called by %s\n",
                            eeGetMethodFullName(fncHandle), info.compFullName));
                    goto ABORT;
                }
            }

#if INLINE_CONDITIONALS

            /* Are we in an if/else statement? */

            if  (jmpAddr)
            {
                /* For now ignore void returns inside if/else */

                if (fncRetType == TYP_VOID)
                {
                    JITLOG((LL_INFO100000, "INLINER FAILED: void return from within "
                            "a conditional in %s called by %s\n",
                            eeGetMethodFullName(fncHandle), info.compFullName));
                    goto ABORT;
                }

                /* We don't process cases that have two branches and only one return in
                   the conditional eg. if() {... no ret} else { ... return } ... return */

                assert(verCurrentState.esStackDepth == 0);
                
                assert(ifNvoid == false);

                if (inElse)
                {
                    /* The 'if' part must have a return */

                    if (!hasCondReturn)
                    {
                        JITLOG((LL_INFO100000, "INLINER FAILED: Cannot have "
                                "'if(c){no_ret}else{ret} ret' in %s called by %s\n",
                                eeGetMethodFullName(fncHandle), info.compFullName));
                        goto ABORT;
                    }

                    /* This must be the last instruction - i.e. cannot have code after else */

                    if (codeAddr + sz != codeEndp)
                    {
                        JITLOG((LL_INFO100000, "INLINER FAILED: Cannot have code "
                                "following else in %s called by %s\n",
                                eeGetMethodFullName(fncHandle), info.compFullName));
                        goto ABORT;
                    }
                }
                else
                {
                    assert(!hasCondReturn);
                    hasCondReturn = true;

                    /* Grab a temp for the return local variable */
                    retLclNum = lvaGrabTemp();
                    lvaTable[retLclNum].lvType = genActualType(fncRetType);
                }

                /* Assign the return value to the return local variable */
                op1 = gtNewAssignNode(
                         gtNewLclvNode(retLclNum, fncRetType),
                         bodyExp);

                /* Append the assignment to the current body of the branch */
                impInlineExpr = impConcatExprs(impInlineExpr, op1);

                if (!inElse)
                {
                    /* Remember the 'if' statement part */
                    ifStmts = impInlineExpr;
                              impInlineExpr = NULL;

                    /* The next instruction will start at 'jmpAddr' (skip junk after ret) */
                    codeAddr = jmpAddr;

                    /* The rest of the code is the else part - so its end is the end of the code */
                    jmpAddr = codeEndp;
                    inElse  = true;
                }
                break;
            }
#endif
            goto DONE;


        case CEE_LDELEMA :
            assert(sz == sizeof(unsigned));
            typeRef = getU4LittleEndian(codeAddr);
            clsHnd = eeFindClass(typeRef, scpHandle, fncHandle, false);
            clsFlags = eeGetClassAttribs(clsHnd);

            if (!clsHnd)
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: Cannot get class handle: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT_THIS_INLINE_ONLY;
            }

            if (clsFlags & CORINFO_FLG_VALUECLASS)
                lclTyp = TYP_STRUCT;
            else
            {
                op1 = gtNewIconEmbClsHndNode(clsHnd, typeRef, info.compScopeHnd);
                op1 = gtNewOperNode(GT_LIST, TYP_VOID, op1);                // Type
                op1 = gtNewOperNode(GT_LIST, TYP_VOID, impPopStack().val, op1); // index
                
                op1 = gtNewOperNode(GT_LIST, TYP_VOID, impPopStack().val, op1); // array 
                op1 = gtNewHelperCallNode(CORINFO_HELP_LDELEMA_REF, TYP_BYREF, GTF_EXCEPT, op1);

                impPushOnStackNoType(op1);
                break;
            }
            goto ARR_LD;

        case CEE_LDELEM_I1 : lclTyp = TYP_BYTE  ; goto ARR_LD;
        case CEE_LDELEM_I2 : lclTyp = TYP_SHORT ; goto ARR_LD;
        case CEE_LDELEM_I  :
        case CEE_LDELEM_U4 :
        case CEE_LDELEM_I4 : lclTyp = TYP_INT   ; goto ARR_LD;
        case CEE_LDELEM_I8 : lclTyp = TYP_LONG  ; goto ARR_LD;
        case CEE_LDELEM_REF: lclTyp = TYP_REF   ; goto ARR_LD;
        case CEE_LDELEM_R4 : lclTyp = TYP_FLOAT ; goto ARR_LD;
        case CEE_LDELEM_R8 : lclTyp = TYP_DOUBLE; goto ARR_LD;
        case CEE_LDELEM_U1 : lclTyp = TYP_UBYTE ; goto ARR_LD;
        case CEE_LDELEM_U2 : lclTyp = TYP_CHAR  ; goto ARR_LD;

        ARR_LD:

#if CSELENGTH
            fgHasRangeChks = true;
#endif

            /* Pull the index value and array address */
            {
            op2 = impPopStack().val;
            op1 = impPopStack().val;   assert (op1->gtType == TYP_REF);
            }

            /* Check for null pointer - in the inliner case we simply abort */

            if (op1->gtOper == GT_CNS_INT)
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: NULL pointer for LDELEM in "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }

            /* Create the index node and push it on the stack */

            op1 = gtNewIndexRef(lclTyp, op1, op2);

            if (opcode == CEE_LDELEMA)
            {
                    // rememer the element size
                if (lclTyp == TYP_REF)
                    op1->gtIndex.gtIndElemSize = sizeof(void*);
                else
                    op1->gtIndex.gtIndElemSize = eeGetClassSize(clsHnd);

                // wrap it in a &
                lclTyp = TYP_BYREF;

                op1 = gtNewOperNode(GT_ADDR, lclTyp, op1);
            }

            impPushOnStackNoType(op1);
            break;


        case CEE_STELEM_REF:

            // @TODO [CONSIDER] [04/16/01] []: Check for assignment of null and generate inline code

            /* Call a helper function to do the assignment */

            op1 = gtNewHelperCallNode(CORINFO_HELP_ARRADDR_ST,
                                      TYP_VOID, 0,
                                      impPopList(3, &flags, 0));

            goto INLINE_APPEND;


        case CEE_STELEM_I1: lclTyp = TYP_BYTE  ; goto ARR_ST;
        case CEE_STELEM_I2: lclTyp = TYP_SHORT ; goto ARR_ST;
        case CEE_STELEM_I:
        case CEE_STELEM_I4: lclTyp = TYP_INT   ; goto ARR_ST;
        case CEE_STELEM_I8: lclTyp = TYP_LONG  ; goto ARR_ST;
        case CEE_STELEM_R4: lclTyp = TYP_FLOAT ; goto ARR_ST;
        case CEE_STELEM_R8: lclTyp = TYP_DOUBLE; goto ARR_ST;

        ARR_ST:

            if (!info.compLooseExceptions &&
                (impStackTop().val->gtFlags & GTF_SIDE_EFFECT) )
            {
                impInlineSpillGlobEffects();
            }

#if CSELENGTH
            inlineeHasRangeChks = true;
#endif

            /* Pull the new value from the stack */
            {
            op2 = impPopStack().val;

            /* Pull the index value */

            op1 = impPopStack().val;

            /* Pull the array address */

                arr = impPopStack().val;   
            }
            assert (arr->gtType == TYP_REF);

            if (op2->IsVarAddr())
                op2->gtType = TYP_I_IMPL;

            arr = impCheckForNullPointer(arr);

            /* Create the index node */

            op1 = gtNewIndexRef(lclTyp, arr, op1);

            /* Create the assignment node and append it */

            op1 = gtNewAssignNode(op1, op2);

            goto INLINE_APPEND;

        case CEE_DUP:

            if (jmpAddr)
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: DUP inside of conditional in "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }

            /* Spill any side effects from the stack */

            impInlineSpillGlobEffects();

            /* Pull the top value from the stack */

            op1 = impPopStack().val;
            assert(op1->TypeGet() != TYP_STRUCT);       // inliner does not support structs

            /* @TODO [CONSIDER] [04/16/01] []: We allow only cloning of simple nodes. That way
               it is easy for us to track the dup of a local var.
               @TODO [CONSIDER] [04/16/01] []: Don't globally disable the bashing 
               of the temp node as soon as any local var has been duplicated! Unfortunately
               the local var node doesn't give us the index into inlArgInfo,
               i.e. we would have to linearly scan the array in order to reset
               argBashTmpNode.
               @TODO [CONSIDER] [04/16/01] []: Make gtClone() detect and return 
               presence of vars. Then we could allow complicated expressions to be cloned as well.
            */

            /* Is the value simple enough to be cloneable? */

            op2 = gtClone(op1, false);

            if  (op2)
            {
                if (op2->gtOper == GT_LCL_VAR)
                    dupOfLclVar = true;

                /* Cool - we can stuff two copies of the value back */
                impPushOnStackNoType(op1);
                impPushOnStackNoType(op2); 
                break;
            }

            /* expression too complicated */

            JITLOG((LL_INFO100000, "INLINER FAILED: DUP of complex expression in "
                    "%s called by %s\n",
                    eeGetMethodFullName(fncHandle), info.compFullName));
            goto ABORT;


        case CEE_ADD:           oper = GT_ADD;      goto MATH_OP2;

        case CEE_ADD_OVF:       lclTyp = TYP_UNKNOWN; uns = false;  goto ADD_OVF;

        case CEE_ADD_OVF_UN:    lclTyp = TYP_UNKNOWN; uns = true; goto ADD_OVF;

ADD_OVF:

            ovfl = true;        callNode = false;
            oper = GT_ADD;      goto MATH_OP2_FLAGS;

        case CEE_SUB:           oper = GT_SUB;      goto MATH_OP2;

        case CEE_SUB_OVF:       lclTyp = TYP_UNKNOWN; uns = false;  goto SUB_OVF;

        case CEE_SUB_OVF_UN:    lclTyp = TYP_UNKNOWN; uns = true;   goto SUB_OVF;

SUB_OVF:
            ovfl = true;
            callNode = false;
            oper = GT_SUB;
            goto MATH_OP2_FLAGS;

        case CEE_MUL:           oper = GT_MUL;      goto MATH_CALL_ON_LNG;

        case CEE_MUL_OVF:       lclTyp = TYP_UNKNOWN; uns = false;  goto MUL_OVF;

        case CEE_MUL_OVF_UN:    lclTyp = TYP_UNKNOWN; uns = true; goto MUL_OVF;

        MUL_OVF:
                                ovfl = true;        callNode = false;
                                oper = GT_MUL;      goto MATH_CALL_ON_LNG_OVF;

        // Other binary math operations

        case CEE_DIV :          oper = GT_DIV;  goto MATH_CALL_ON_LNG;

        case CEE_DIV_UN :       oper = GT_UDIV;  goto MATH_CALL_ON_LNG;

        case CEE_REM:
            oper = GT_MOD;
            ovfl = false;
            callNode = true;
                // can use small node for INT case
            if (impStackTop().val->gtType == TYP_INT)
                callNode = false;
            goto MATH_OP2_FLAGS;

        case CEE_REM_UN :       oper = GT_UMOD;  goto MATH_CALL_ON_LNG;

        MATH_CALL_ON_LNG:
            ovfl = false;
        MATH_CALL_ON_LNG_OVF:
            callNode = false;
            if (impStackTop().val->gtType == TYP_LONG)
                callNode = true;
            goto MATH_OP2_FLAGS;

        case CEE_AND:        oper = GT_AND;  goto MATH_OP2;
        case CEE_OR:         oper = GT_OR ;  goto MATH_OP2;
        case CEE_XOR:        oper = GT_XOR;  goto MATH_OP2;

        MATH_OP2:       // For default values of 'ovfl' and 'callNode'

            ovfl        = false;
            callNode    = false;

        MATH_OP2_FLAGS: // If 'ovfl' and 'callNode' have already been set

            /* Pull two values and push back the result */

            {
            op2 = impPopStack().val;
            op1 = impPopStack().val;
            }

#if!CPU_HAS_FP_SUPPORT
            if (varTypeIsFloating(op1->gtType))
            {
                callNode    = true;
            }
#endif
            /* Cant do arithmetic with references */
            assert(genActualType(op1->TypeGet()) != TYP_REF &&
                   genActualType(op2->TypeGet()) != TYP_REF);

            // Arithemetic operations are generally only allowed with
            // primitive types, but certain operations are allowed
            // with byrefs

            if ((oper == GT_SUB) &&
                (genActualType(op1->TypeGet()) == TYP_BYREF ||
                 genActualType(op2->TypeGet()) == TYP_BYREF))
            {
                // byref1-byref2 => gives an int
                // byref - int   => gives a byref

                if ((genActualType(op1->TypeGet()) == TYP_BYREF) &&
                    (genActualType(op2->TypeGet()) == TYP_BYREF))
                {
                    // byref1-byref2 => gives an int
                    type = TYP_I_IMPL;
                    impBashVarAddrsToI(op1, op2);
                }
                else
                {
                    // byref - int => gives a byref
                    // (but if &var, then dont need to report to GC)

                    assert(genActualType(op1->TypeGet()) == TYP_I_IMPL ||
                           genActualType(op2->TypeGet()) == TYP_I_IMPL);

                    impBashVarAddrsToI(op1, op2);

                    if (genActualType(op1->TypeGet()) == TYP_BYREF ||
                        genActualType(op2->TypeGet()) == TYP_BYREF)
                        type = TYP_BYREF;
                    else
                        type = TYP_I_IMPL;
                }
            }
            else if ((oper == GT_ADD) &&
                     (genActualType(op1->TypeGet()) == TYP_BYREF ||
                      genActualType(op2->TypeGet()) == TYP_BYREF))
            {
                // only one can be a byref : byref+byref not allowed
                assert(genActualType(op1->TypeGet()) != TYP_BYREF ||
                       genActualType(op2->TypeGet()) != TYP_BYREF);
                assert(genActualType(op1->TypeGet()) == TYP_I_IMPL ||
                       genActualType(op2->TypeGet()) == TYP_I_IMPL);

                // byref + int => gives a byref
                // (but if &var, then dont need to report to GC)

                impBashVarAddrsToI(op1, op2);

                if (genActualType(op1->TypeGet()) == TYP_BYREF ||
                    genActualType(op2->TypeGet()) == TYP_BYREF)
                    type = TYP_BYREF;
                else
                    type = TYP_I_IMPL;
            }
            else
            {
                assert(genActualType(op1->TypeGet()) != TYP_BYREF &&
                       genActualType(op2->TypeGet()) != TYP_BYREF);

                assert(genActualType(op1->TypeGet()) == genActualType(op2->TypeGet()) || 
                       varTypeIsFloating(op1->gtType) && varTypeIsFloating(op2->gtType));

                type = genActualType(op1->gtType);

                if (type == TYP_FLOAT)          // spill intermediate expressions as double
                    type = TYP_DOUBLE;          
            }

            /* Special case: "int+0", "int-0", "int*1", "int/1" */

            if  (op2->gtOper == GT_CNS_INT)
            {
                if  (((op2->gtIntCon.gtIconVal == 0) && (oper == GT_ADD || oper == GT_SUB)) ||
                     ((op2->gtIntCon.gtIconVal == 1) && (oper == GT_MUL || oper == GT_DIV)))

                {
                    impPushOnStackNoType(op1);
                    break;
                }
            }

            /* Special case: "int+0", "int-0", "int*1", "int/1" */

            if  (op2->gtOper == GT_CNS_INT)
            {
                if  (((op2->gtIntCon.gtIconVal == 0) && (oper == GT_ADD || oper == GT_SUB)) ||
                     ((op2->gtIntCon.gtIconVal == 1) && (oper == GT_MUL || oper == GT_DIV)))

                {
                    impPushOnStackNoType(op1);
                    break;
                }
            }

#if SMALL_TREE_NODES
            if (callNode)
            {
                /* These operators later get transformed into 'GT_CALL' */

                assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_MUL]);
                assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_DIV]);
                assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_UDIV]);
                assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_MOD]);
                assert(GenTree::s_gtNodeSizes[GT_CALL] > GenTree::s_gtNodeSizes[GT_UMOD]);

                op1 = gtNewOperNode(GT_CALL, type, op1, op2);
                op1->SetOper(oper);
            }
            else
#endif
            {
                op1 = gtNewOperNode(oper,    type, op1, op2);
            }

            /* Special case: integer/long division may throw an exception */

            if  (varTypeIsIntegral(op1->TypeGet()) && op1->OperMayThrow())
            {
                op1->gtFlags |=  GTF_EXCEPT;
            }

            if  (ovfl)
            {
                assert(oper==GT_ADD || oper==GT_SUB || oper==GT_MUL);
                if (lclTyp != TYP_UNKNOWN)
                    op1->gtType   = lclTyp;
                op1->gtFlags |= (GTF_EXCEPT | GTF_OVERFLOW);
                if (uns)
                    op1->gtFlags |= GTF_UNSIGNED;
            }

            
            /* See if we can actually fold the expression */

            op1 = gtFoldExpr(op1);

            impPushOnStackNoType(op1);
            break;

        case CEE_SHL:        oper = GT_LSH;  goto CEE_SH_OP2;

        case CEE_SHR:        oper = GT_RSH;  goto CEE_SH_OP2;
        case CEE_SHR_UN:     oper = GT_RSZ;  goto CEE_SH_OP2;

        CEE_SH_OP2:

            op2     = impPopStack().val;
            op1     = impPopStack().val;    // operand to be shifted
            impBashVarAddrsToI(op1, op2);

            // The shiftAmount is a U4.
            assert(genActualType(op2->TypeGet()) == TYP_INT);

            type    = genActualType(op1->TypeGet());
            op1     = gtNewOperNode(oper, type, op1, op2);
            op1 = gtFoldExpr(op1);
            impPushOnStackNoType(op1);
            break;

        case CEE_NOT:

            op1 = impPopStack().val;
            impBashVarAddrsToI(op1, NULL);

            type = op1->TypeGet();

            op1 = gtNewOperNode(GT_NOT, type, op1);
            op1 = gtFoldExpr(op1);
            impPushOnStackNoType(op1);
            break;

        case CEE_CKFINITE:

            op1 = impPopStack().val;
            type = op1->TypeGet();

            op1 = gtNewOperNode(GT_CKFINITE, type, op1);
            op1->gtFlags |= GTF_EXCEPT;

            impPushOnStackNoType(op1);
            break;


        /************************** Casting OPCODES ***************************/

        case CEE_CONV_OVF_I1:      lclTyp = TYP_BYTE ;    goto CONV_OVF;
        case CEE_CONV_OVF_I2:      lclTyp = TYP_SHORT;    goto CONV_OVF;
        case CEE_CONV_OVF_I :
        case CEE_CONV_OVF_I4:      lclTyp = TYP_INT  ;    goto CONV_OVF;
        case CEE_CONV_OVF_I8:      lclTyp = TYP_LONG ;    goto CONV_OVF;

        case CEE_CONV_OVF_U1:      lclTyp = TYP_UBYTE;    goto CONV_OVF;
        case CEE_CONV_OVF_U2:      lclTyp = TYP_CHAR ;    goto CONV_OVF;
        case CEE_CONV_OVF_U :
        case CEE_CONV_OVF_U4:      lclTyp = TYP_UINT ;    goto CONV_OVF;
        case CEE_CONV_OVF_U8:      lclTyp = TYP_ULONG;    goto CONV_OVF;

        case CEE_CONV_OVF_I1_UN:   lclTyp = TYP_BYTE ;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_I2_UN:   lclTyp = TYP_SHORT;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_I_UN :
        case CEE_CONV_OVF_I4_UN:   lclTyp = TYP_INT  ;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_I8_UN:   lclTyp = TYP_LONG ;    goto CONV_OVF_UN;

        case CEE_CONV_OVF_U1_UN:   lclTyp = TYP_UBYTE;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_U2_UN:   lclTyp = TYP_CHAR ;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_U_UN :
        case CEE_CONV_OVF_U4_UN:   lclTyp = TYP_UINT ;    goto CONV_OVF_UN;
        case CEE_CONV_OVF_U8_UN:   lclTyp = TYP_ULONG;    goto CONV_OVF_UN;

CONV_OVF_UN:
            uns      = true;    goto CONV_OVF_COMMON;
CONV_OVF:
            uns      = false;   goto CONV_OVF_COMMON;

CONV_OVF_COMMON:
            ovfl     = true;
            goto _CONV;

        case CEE_CONV_I1:       lclTyp = TYP_BYTE  ;    goto CONV;
        case CEE_CONV_I2:       lclTyp = TYP_SHORT ;    goto CONV;
        case CEE_CONV_I:
        case CEE_CONV_I4:       lclTyp = TYP_INT   ;    goto CONV;
        case CEE_CONV_I8:       lclTyp = TYP_LONG  ;    goto CONV;

        case CEE_CONV_U1:       lclTyp = TYP_UBYTE ;    goto CONV;
        case CEE_CONV_U2:       lclTyp = TYP_CHAR  ;    goto CONV;
        case CEE_CONV_U:
        case CEE_CONV_U4:       lclTyp = TYP_UINT  ;    goto CONV;
        case CEE_CONV_U8:       lclTyp = TYP_ULONG ;    goto CONV_UN;

        case CEE_CONV_R4:       lclTyp = TYP_FLOAT;     goto CONV;
        case CEE_CONV_R8:       lclTyp = TYP_DOUBLE;    goto CONV;

        case CEE_CONV_R_UN :    lclTyp = TYP_DOUBLE;    goto CONV_UN;
    
CONV_UN:
            uns    = true; 
            ovfl   = false;
            goto _CONV;

CONV:
            uns      = false;
            ovfl     = false;
            goto _CONV;

_CONV:
            // only converts from FLOAT or DOUBLE to integer type 
            //  and converts from UINT or ULONG   to DOUBLE get morphed to calls

            if (varTypeIsFloating(lclTyp))
            {
                callNode = uns;
            }
            else
            {
                callNode = varTypeIsFloating(impStackTop().val->TypeGet());
            }
            
            // At this point uns, ovf, callNode all set
            op1  = impPopStack().val;
            impBashVarAddrsToI(op1);

            /* Check for a worthless cast, such as "(byte)(int & 32)" */
            /* @TODO [REVISIT] [04/16/01] []: this should be in the morpher */

            if  (varTypeIsSmall(lclTyp) && !ovfl &&
                 op1->gtType == TYP_INT && op1->gtOper == GT_AND)
            {
                op2 = op1->gtOp.gtOp2;

                if  (op2->gtOper == GT_CNS_INT)
                {
                    int         ival = op2->gtIntCon.gtIconVal;
                    int         mask, umask;

                    switch (lclTyp)
                    {
                    case TYP_BYTE :
                    case TYP_UBYTE: mask = 0x00FF; umask = 0x007F; break;
                    case TYP_CHAR :
                    case TYP_SHORT: mask = 0xFFFF; umask = 0x7FFF; break;

                    default:
                        assert(!"unexpected type");
                    }

                    if  (((ival & umask) == ival) ||
                         ((ival &  mask) == ival && uns))
                    {
                        /* Toss the cast, it's a waste of time */

                        impPushOnStackNoType(op1);
                        break;
                    }
                }
            }

            /*  The 'op2' sub-operand of a cast is the 'real' type number,
                since the result of a cast to one of the 'small' integer
                types is an integer.
             */

            type = genActualType(lclTyp);

#if SMALL_TREE_NODES
            if (callNode)
                op1 = gtNewCastNodeL(type, op1, lclTyp);
            else
#endif
                op1 = gtNewCastNode (type, op1, lclTyp);

            if (ovfl)
                op1->gtFlags |= (GTF_OVERFLOW | GTF_EXCEPT);
            if (uns)
                op1->gtFlags |= GTF_UNSIGNED;

            op1 = gtFoldExpr(op1);
            impPushOnStackNoType(op1);
            break;

        case CEE_NEG:

            op1 = impPopStack().val;
            impBashVarAddrsToI(op1, NULL);

            op1 = gtNewOperNode(GT_NEG, genActualType(op1->gtType), op1);
            op1 = gtFoldExpr(op1);
            impPushOnStackNoType(op1);
            break;

        case CEE_POP:

            /* Pull the top value from the stack */

            op1 = impPopStack().val;

            /* Does the value have any side effects? */

            if  (op1->gtFlags & GTF_SIDE_EFFECT)
            {
                /* Create an unused node (a cast to void) which means
                 * we only need to evaluate the side effects */

                op1 = gtUnusedValNode(op1);

                goto INLINE_APPEND;
            }

            /* No side effects - just throw the <BEEP> thing away */

            break;

        case CEE_STIND_I1:      lclTyp  = TYP_BYTE;     goto STIND;
        case CEE_STIND_I2:      lclTyp  = TYP_SHORT;    goto STIND;
        case CEE_STIND_I4:      lclTyp  = TYP_INT;      goto STIND;
        case CEE_STIND_I8:      lclTyp  = TYP_LONG;     goto STIND;
        case CEE_STIND_I:       lclTyp  = TYP_I_IMPL;   goto STIND;
        case CEE_STIND_REF:     lclTyp  = TYP_REF;      goto STIND;
        case CEE_STIND_R4:      lclTyp  = TYP_FLOAT;    goto STIND;
        case CEE_STIND_R8:      lclTyp  = TYP_DOUBLE;   goto STIND;
STIND:

            op2 = impPopStack().val;    // value to store
            op1 = impPopStack().val;    // address to store to

            // you can indirect off of a TYP_I_IMPL (if we are in C) or a BYREF
            assert(genActualType(op1->gtType) == TYP_I_IMPL ||
                                 op1->gtType  == TYP_BYREF);

            impBashVarAddrsToI(op1, op2);

            if (opcode == CEE_STIND_REF)
            {
                // STIND_REF can be used to store TYP_I_IMPL, TYP_REF, or TYP_BYREF
                assert(op2->gtType == TYP_I_IMPL || varTypeIsGC(op2->TypeGet()));
                lclTyp = genActualType(op2->TypeGet());
            }

            // Check target type.
#ifdef DEBUG
            if (op2->gtType == TYP_BYREF || lclTyp == TYP_BYREF)
            {
                if (op2->gtType == TYP_BYREF)
                    assert(lclTyp == TYP_BYREF || lclTyp == TYP_I_IMPL);
                else if (lclTyp == TYP_BYREF)
                    assert(op2->gtType == TYP_BYREF ||op2->gtType == TYP_I_IMPL);
            }
            else
#endif
                assert(genActualType(op2->gtType) == genActualType(lclTyp) ||
                       varTypeIsFloating(op2->gtType) && varTypeIsFloating(lclTyp));

            op1 = gtNewOperNode(GT_IND, lclTyp, op1);

            // stind could point anywhere, example a boxed class static int
            op1->gtFlags |= GTF_IND_TGTANYWHERE;

            if (volatil)
            {
                op1->gtFlags |= GTF_DONT_CSE;
            }

            op1 = gtNewAssignNode(op1, op2);
            op1->gtFlags |= GTF_EXCEPT | GTF_GLOB_REF;

            goto INLINE_APPEND;


        case CEE_LDIND_I1:      lclTyp  = TYP_BYTE;     goto LDIND;
        case CEE_LDIND_I2:      lclTyp  = TYP_SHORT;    goto LDIND;
        case CEE_LDIND_U4:
        case CEE_LDIND_I4:      lclTyp  = TYP_INT;      goto LDIND;
        case CEE_LDIND_I8:      lclTyp  = TYP_LONG;     goto LDIND;
        case CEE_LDIND_REF:     lclTyp  = TYP_REF;      goto LDIND;
        case CEE_LDIND_I:       lclTyp  = TYP_I_IMPL;   goto LDIND;
        case CEE_LDIND_R4:      lclTyp  = TYP_FLOAT;    goto LDIND;
        case CEE_LDIND_R8:      lclTyp  = TYP_DOUBLE;   goto LDIND;
        case CEE_LDIND_U1:      lclTyp  = TYP_UBYTE;    goto LDIND;
        case CEE_LDIND_U2:      lclTyp  = TYP_CHAR;     goto LDIND;
LDIND:
            {
            op1 = impPopStack().val;    // address to load from
            }

            impBashVarAddrsToI(op1);

            assert(genActualType(op1->gtType) == TYP_I_IMPL ||
                                 op1->gtType  == TYP_BYREF);

            op1 = gtNewOperNode(GT_IND, lclTyp, op1);

            // ldind could point anywhere, example a boxed class static int
            op1->gtFlags |= (GTF_EXCEPT | GTF_GLOB_REF | GTF_IND_TGTANYWHERE);

            if (volatil)
            {
                op1->gtFlags |= GTF_DONT_CSE;
            }

            impPushOnStackNoType(op1);
            break;

        case CEE_VOLATILE:
            volatil = true;
            assert(sz == 0);
            continue;

        case CEE_LDFTN:

            memberRef = getU4LittleEndian(codeAddr);
                                // Note need to do this here to perform the access check.
            methHnd   = eeFindMethod(memberRef, scpHandle, fncHandle, false);

            if (!methHnd)
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: Cannot get method handle: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT_THIS_INLINE_ONLY;
            }

            op1 = gtNewIconHandleNode(memberRef, GTF_ICON_FTN_ADDR, (unsigned)scpHandle);
            op1->gtVal.gtVal2 = (unsigned)scpHandle;
            op1->SetOper(GT_FTN_ADDR);
            op1->gtType = TYP_I_IMPL;
            impPushOnStackNoType(op1);
            break;

        case CEE_LDVIRTFTN:

            /* Get the method token */

            memberRef = getU4LittleEndian(codeAddr);
            methHnd   = eeFindMethod(memberRef, scpHandle, fncHandle, false);

            if (!methHnd)
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: Cannot get method handle: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT_THIS_INLINE_ONLY;
            }

            mflags = eeGetMethodAttribs(methHnd);
            if (mflags & (CORINFO_FLG_FINAL|CORINFO_FLG_STATIC) || !(mflags & CORINFO_FLG_VIRTUAL))
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: ldvirtFtn on a non-virtual "
                        "function %s in %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT_THIS_INLINE_ONLY;
            }

            /* Get the object-ref */

            {
                op1 = impPopStack().val;
            }
            assert(op1->gtType == TYP_REF);

            clsFlags = eeGetClassAttribs(eeGetMethodClass(methHnd));

            /* Get the vtable-ptr from the object */

            op1 = gtNewOperNode(GT_IND, TYP_I_IMPL, op1);

            op1 = gtNewOperNode(GT_VIRT_FTN, TYP_I_IMPL, op1);
            op1->gtVal.gtVal2 = unsigned(methHnd);

            op1->gtFlags |= GTF_EXCEPT; // Null-pointer exception

            /* @TODO [REVISIT] [04/16/01] []: this shouldn't be marked as a call anymore */

            if (clsFlags & CORINFO_FLG_INTERFACE)
                op1->gtFlags |= GTF_CALL_INTF | GTF_CALL;

            impPushOnStackNoType(op1);
            break;

        case CEE_LDFLD:
        case CEE_LDSFLD:
        case CEE_LDFLDA:
        case CEE_LDSFLDA: {
            BOOL isStaticOpcode = (opcode == CEE_LDSFLD || opcode == CEE_LDSFLDA);
            BOOL isLoadAddress  = (opcode == CEE_LDFLDA || opcode == CEE_LDSFLDA);

            /* Get the CP_Fieldref index */
            assert(sz == sizeof(unsigned));
            memberRef = getU4LittleEndian(codeAddr);
            fldHnd = eeFindField(memberRef, scpHandle, fncHandle, false);

            if (!fldHnd)
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: Cannot get field handle: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT_THIS_INLINE_ONLY;
            }

            /* Figure out the type of the member */
            lclTyp = eeGetFieldType(fldHnd, &clsHnd);

            if (lclTyp == TYP_STRUCT && (opcode == CEE_LDFLD || opcode == CEE_LDSFLD))
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: ldfld of valueclass\n"));
                goto ABORT;
            }


            /* Preserve 'small' int types */
            if  (lclTyp > TYP_INT) lclTyp = genActualType(lclTyp);

            /* Get hold of the flags for the member */

            flags = eeGetFieldAttribs(fldHnd);


            /* Is this a 'special' (COM) field? or a TLS ref static field? */

            if  (flags & (CORINFO_FLG_HELPER | CORINFO_FLG_TLS))
                goto ABORT;

            assert((lclTyp != TYP_BYREF) && "Illegal to have an field of TYP_BYREF");

            /* Create the data member node */

            op1 = gtNewFieldRef(lclTyp, fldHnd);

            if (volatil)
            {
                op1->gtFlags |= GTF_DONT_CSE;
            }

            tmp = 0;
            StackEntry se; 

            /* Pull the object's address if opcode says it is non-static */

            if  (!isStaticOpcode)
            {
                {
                    tmp = impPopStack().val;
                }

                /* Check for null pointer - in the inliner case we simply abort */

                if (tmp->gtOper == GT_CNS_INT && tmp->gtIntCon.gtIconVal == 0)
                {
                    JITLOG((LL_INFO100000, "INLINER FAILED: NULL pointer for LDFLD"
                            " in %s called by %s\n",
                            eeGetMethodFullName(fncHandle), info.compFullName));
                    goto ABORT;
                }
            }


            if  (flags & CORINFO_FLG_STATIC)
            {
                CORINFO_CLASS_HANDLE fldClass   = eeGetFieldClass(fldHnd);
                DWORD                fldClsAttr = eeGetClassAttribs(fldClass);

                if (flags & CORINFO_FLG_SHARED_HELPER) 
                {
                    op1->gtFlags |= GTF_IND_SHARED;
                    
                    // if we haven't hit a branch or a side effect, then statics were accessed first,
                    // which means we don't need an explicit cctor check in the inlined method
                    if (fldClass == clsHandle && jmpAddr == NULL && !didSideEffect)
                        staticAccessedFirst = true;
                }
                else if (!(fldClsAttr & CORINFO_FLG_INITIALIZED)    && 
                         (fldClsAttr & CORINFO_FLG_NEEDS_INIT)      && 
                         !info.compCompHnd->initClass(fldClass, info.compMethodHnd, FALSE) &&
                         fldClass != clsHandle)
                {
                    JITLOG((LL_INFO1000000, "INLINER FAILED: Field class is "
                            "not initialized: %s called by %s\n",
                            eeGetMethodFullName(fncHandle), info.compFullName));

                    /* We refuse to inline this class, but don't mark it as not inlinable */
                    goto ABORT_THIS_INLINE_ONLY;
                }

                    // The EE gives us the 
                    // handle to the boxed object. We then access the unboxed object from it.
                    // Remove when our story for static value classes changes.
                if (flags & CORINFO_FLG_STATIC_IN_HEAP)
                {
                    assert(!(flags & CORINFO_FLG_UNMANAGED));
                    op1 = gtNewFieldRef(TYP_REF, fldHnd); // Boxed object
                    op2 = gtNewIconNode(sizeof(void*), TYP_I_IMPL);
                    op1 = gtNewOperNode(GT_ADD, TYP_REF, op1, op2);
                    op1 = gtNewOperNode(GT_IND, lclTyp, op1);
                }

                if (isLoadAddress) // LDFLDA or LDSFLDA 
                {
                    INDEBUG(clsHnd = BAD_CLASS_HANDLE;)
                    if  (flags & CORINFO_FLG_UNMANAGED)
                        lclTyp = TYP_I_IMPL;
                    else
                        lclTyp = TYP_BYREF;

                    op1 = gtNewOperNode(GT_ADDR, lclTyp, op1);

                    // &clsVar doesnt need GTF_GLOB_REF, though clsVar does
                    if (flags & CORINFO_FLG_STATIC)
                        op1->gtFlags &= ~GTF_GLOB_REF;
                }
            }
            else
            {
                // it is an instance field, so verify that we are doing ldfld/ldflda
                if (tmp == 0)
                    BADCODE("LDSFLD done on an instance field.  No obj pointer available");

                op1->gtField.gtFldObj = tmp;

                op1->gtFlags |= (tmp->gtFlags & GTF_GLOB_EFFECT) | GTF_EXCEPT;

                // If gtFldObj is a BYREF then our target is a value class and
                // it could point anywhere, example a boxed class static int
                if (tmp->gtType == TYP_BYREF)
                    op1->gtFlags |= GTF_IND_TGTANYWHERE;

                // wrap it in a address of operator if necessary
                if (opcode == CEE_LDFLDA)
                {
                    op1 = gtNewOperNode(GT_ADDR, varTypeIsGC(tmp->TypeGet()) ?
                                                 TYP_BYREF : TYP_I_IMPL, op1);
                }
                else
                {
                        // if we haven't hit a branch or a side effect, and we are fetching
                        // from 'this' then we should be able to avoid a null pointer check
                    if ((tree->gtFlags & GTF_CALL_VIRT_RES) && jmpAddr == NULL && !didSideEffect && 
                        tmp->gtOper == GT_LCL_VAR && tmp->gtLclVar.gtLclNum == inlArgInfo[0].argTmpNum)
                        thisAccessedFirst = true;
                }

            }

            lclTyp = eeGetFieldType(fldHnd, &clsHnd);
            impPushOnStackNoType(op1);
            break;
        }

        case CEE_STFLD:
        case CEE_STSFLD:

            /* Get the CP_Fieldref index */

            assert(sz == sizeof(unsigned));
            memberRef = getU4LittleEndian(codeAddr);
            fldHnd    = eeFindField(memberRef, scpHandle, fncHandle, false);

            if (!fldHnd)
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: Cannot get field handle: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT_THIS_INLINE_ONLY;
            }

            flags   = eeGetFieldAttribs(fldHnd);

            /* Figure out the type of the member */

            lclTyp  = eeGetFieldType   (fldHnd, &clsHnd);

            /* Check field access */

            assert(eeCanPutField(fldHnd, flags, 0, fncHandle));

            /* Preserve 'small' int types */

            if  (lclTyp > TYP_INT) lclTyp = genActualType(lclTyp);

            tmp = 0;

            /* Pull the value from the stack */

            op2 = impPopStack().val;

            if (opcode == CEE_STFLD)
                tmp = impPopStack().val;

            /* Spill any refs to the same member from the stack */

            impInlineSpillLclRefs(int(fldHnd));

            /* Is this a 'special' (COM) field? or a TLS ref static field?, field stored int GC heap? */

            if  (flags & (CORINFO_FLG_HELPER | CORINFO_FLG_TLS | CORINFO_FLG_STATIC_IN_HEAP))
                goto ABORT;

            assert((lclTyp != TYP_BYREF) && "Illegal to have an field of TYP_BYREF");

            /* Create the data member node */

            op1 = gtNewFieldRef(lclTyp, fldHnd);

            /* Pull the object's address if opcode say it is non-static */
            if  (opcode == CEE_STFLD)
            {
                /* Check for null pointer - in the inliner case we simply abort */

                if (tmp->gtOper == GT_CNS_INT)
                {
                    JITLOG((LL_INFO100000, "INLINER FAILED: NULL pointer for LDFLD "
                            "in %s called by %s\n",
                            eeGetMethodFullName(fncHandle), info.compFullName));
                    goto ABORT;
                }
            }

            if  (flags & CORINFO_FLG_STATIC)
            {
                /* Don't inline if the field class is not initialized */
                CORINFO_CLASS_HANDLE fldClass   = eeGetFieldClass(fldHnd);
                DWORD                fldClsAttr = eeGetClassAttribs(fldClass);

                if (flags & CORINFO_FLG_SHARED_HELPER) 
                {
                    op1->gtFlags |= GTF_IND_SHARED;
                    
                    // if we haven't hit a branch or a side effect, then statics were accessed first,
                    // which means we don't need an explicit cctor check in the inlined method
                    if (fldClass == clsHandle && jmpAddr == NULL && !didSideEffect)
                        staticAccessedFirst = true;
                }
                else if (!(fldClsAttr & CORINFO_FLG_INITIALIZED)    && 
                         (fldClsAttr & CORINFO_FLG_NEEDS_INIT)      && 
                         !info.compCompHnd->initClass(fldClass, info.compMethodHnd, FALSE) &&
                         fldClass != clsHandle)
                {
                    JITLOG((LL_EVERYTHING, "INLINER FAILED: Field class is "
                            "not initialized: %s called by %s\n",
                            eeGetMethodFullName(fncHandle), info.compFullName));

                    /* We refuse to inline this class, but don't mark it as not inlinable */

                    goto ABORT_THIS_INLINE_ONLY;
                }
            }
            else 
            {
                assert(tmp);
                op1->gtField.gtFldObj = tmp;
                op1->gtFlags |= (tmp->gtFlags & GTF_GLOB_EFFECT) | GTF_EXCEPT;

                // If gtFldObj is a BYREF then our target is a value class and
                // it could point anywhere, example a boxed class static int
                if (tmp->gtType == TYP_BYREF)
                    op1->gtFlags |= GTF_IND_TGTANYWHERE;

                // if we haven't hit a branch or a side effect, and we are fetching
                // from 'this' then we should be able to avoid a null pointer check
                if ((tree->gtFlags & GTF_CALL_VIRT_RES) && jmpAddr == NULL && !didSideEffect && 
                    tmp->gtOper == GT_LCL_VAR && tmp->gtLclVar.gtLclNum == inlArgInfo[0].argTmpNum)
                    thisAccessedFirst = true;
            }

            /* Create the member assignment */

             op1 = gtNewAssignNode(op1, op2);

            goto INLINE_APPEND;


        case CEE_NEWOBJ:

            memberRef = getU4LittleEndian(codeAddr);
            methHnd = eeFindMethod(memberRef, scpHandle, fncHandle, false);

            if (!methHnd)
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: Cannot get method handle: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT_THIS_INLINE_ONLY;
            }

            assert((eeGetMethodAttribs(methHnd) & CORINFO_FLG_STATIC) == 0);  // constructors are not static

            clsHnd = eeGetMethodClass(methHnd);
                // There are three different cases for new
                // Object size is variable (depends on arguments)
                //      1) Object is an array (arrays treated specially by the EE)
                //      2) Object is some other variable sized object (e.g. String)
                // 3) Class Size can be determined beforehand (normal case
                // In the first case, we need to call a NEWOBJ helper (multinewarray)
                // in the second case we call the constructor with a '0' this pointer
                // In the third case we alloc the memory, then call the constuctor

            clsFlags = eeGetClassAttribs(clsHnd);

                // We don't handle this in the inliner
            if (clsFlags & CORINFO_FLG_VALUECLASS)
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: NEWOBJ of a value class\n"));
                goto ABORT_THIS_INLINE_ONLY;
            }

            if (clsFlags & CORINFO_FLG_ARRAY)
            {
                // Arrays need to call the NEWOBJ helper.
                assert(clsFlags & CORINFO_FLG_VAROBJSIZE);

                /* The varargs helper needs the scope and method token as last
                   and  last-1 param (this is a cdecl call, so args will be
                   pushed in reverse order on the CPU stack) */

                op1 = gtNewIconEmbScpHndNode(scpHandle);
                op1 = gtNewOperNode(GT_LIST, TYP_VOID, op1);

                op2 = gtNewIconNode(memberRef, TYP_INT);
                op2 = gtNewOperNode(GT_LIST, TYP_VOID, op2, op1);

                eeGetMethodSig(methHnd, &sig, false);
                assert(sig.numArgs);
                if (varTypeIsComposite(JITtype2varType(sig.retType)) && sig.retTypeClass == NULL)
                    goto ABORT;

                flags = 0;
                op2 = impPopList(sig.numArgs, &flags, &sig, op2);

                op1 = gtNewHelperCallNode(  CORINFO_HELP_NEWOBJ,
                                            TYP_REF, 0,
                                            op2 );

                // varargs, so we pop the arguments
                op1->gtFlags |= GTF_CALL_POP_ARGS;

#ifdef DEBUG
                // At the present time we don't track Caller pop arguments
                // that have GC references in them
                GenTreePtr temp = op2;
                while(temp != 0)
                {
                    assert(temp->gtOp.gtOp1->gtType != TYP_REF);
                    temp = temp->gtOp.gtOp2;
                }
#endif
                op1->gtFlags |= op2->gtFlags & GTF_GLOB_EFFECT;

                clsHnd = info.compCompHnd->findMethodClass(scpHandle, memberRef);

                impPushOnStackNoType(op1);
                break;
            }
            else if (clsFlags & CORINFO_FLG_VAROBJSIZE)
            {
                // This is the case for varible sized objects that are not
                // arrays.  In this case, call the constructor with a null 'this'
                // pointer
                thisPtr = gtNewIconNode(0, TYP_REF);
            }
            else
            {
                // This is the normal case where the size of the object is
                // fixed.  Allocate the memory and call the constructor.

                op1 = gtNewIconEmbClsHndNode(clsHnd,
                                            -1, // Note if we persist the code this will need to be fixed
                                            scpHandle);

                op1 = gtNewHelperCallNode(  eeGetNewHelper (clsHnd, fncHandle),
                                            TYP_REF, 0,
                                            gtNewArgList(op1));

                /* We will append a call to the stmt list
                 * Must spill side effects from the stack */

                impInlineSpillGlobEffects();

                /* get a temporary for the new object */

                tmpNum = lvaGrabTemp();
                lvaTable[tmpNum].lvType = TYP_REF;

                /* Create the assignment node */

                op1 = gtNewAssignNode(
                         gtNewLclvNode(tmpNum, TYP_REF),
                         op1);

                /* Append the assignment to the statement list so far */

                impInlineExpr = impConcatExprs(impInlineExpr, op1);

                /* Create the 'this' ptr for the call below */

                thisPtr = gtNewLclvNode(tmpNum, TYP_REF);
            }

            goto CALL_GOT_METHODHND;
        
        case CEE_CALLVIRT:
        case CEE_CALL:
            memberRef = getU4LittleEndian(codeAddr);
            methHnd = eeFindMethod(memberRef, scpHandle, fncHandle, false);

            if (!methHnd)
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: Cannot get method handle: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT_THIS_INLINE_ONLY;
            }

CALL_GOT_METHODHND:

            eeGetMethodSig(methHnd, &sig, false);
            callTyp = genActualType(JITtype2varType(sig.retType));
            if (varTypeIsComposite(callTyp) && sig.retTypeClass == NULL)
                goto ABORT;

            mflags = eeGetMethodAttribs(methHnd);

            /* Does the inlinee need a security check token on the frame */

            if (mflags & CORINFO_FLG_SECURITYCHECK)
            {
                JITLOG((LL_EVERYTHING, "INLINER FAILED: Inlinee needs own frame "
                        "for security object\n"));
                goto ABORT;
            }

            /* For now ignore delegate invoke */

            if (mflags & CORINFO_FLG_DELEGATE_INVOKE)
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: DELEGATE_INVOKE not supported\n"));
                goto ABORT;
            }

            /* For now ignore varargs */

            if  ((sig.callConv & CORINFO_CALLCONV_MASK) == CORINFO_CALLCONV_VARARG)
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: VarArgs not supported\n"));
                goto ABORT;
            }

            if  (sig.callConv & CORINFO_CALLCONV_PARAMTYPE)
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: paramtype funtions not suppored \n"));
                goto ABORT;
            }

            if (mflags & CORINFO_FLG_INTRINSIC)
            {
                // We are going to assume that all intrinsics 'touch' their
                // first argument and thus can avoid the null check.\
                // This is important for the string fetch and length intrinsics.
                callObj = 0;
                int firstArg = sig.totalILArgs()-1;
                if (firstArg >= 0 && opcode != CEE_NEWOBJ)
                    callObj = impStackTop(firstArg).val;
                clsHnd = eeGetMethodClass(methHnd);
                op1 = impIntrinsic(clsHnd, methHnd, &sig, memberRef);
                if (op1 != 0)
                    goto DONE_CALL;
                callObj = 0;
            }

            /* Create the function call node */
            op1 = gtNewCallNode(CT_USER_FUNC, methHnd, callTyp, NULL); 

            if (mflags & CORINFO_FLG_NOGCCHECK)
                op1->gtCall.gtCallMoreFlags |= GTF_CALL_M_NOGCCHECK;

            if (opcode == CEE_CALLVIRT)
            {
                assert(!(mflags & CORINFO_FLG_STATIC));     // can't call a static method

                /* Get the method class flags */

                clsFlags = eeGetClassAttribs(eeGetMethodClass(methHnd));

                /* Cannot call virtual on a value class method */

                assert(!(clsFlags & CORINFO_FLG_VALUECLASS));

                /* Set the correct flags - virtual, interface, etc...
                 * If the method is final or private mark it VIRT_RES
                 * which indicates that we should check for a null this pointer */

                if (clsFlags & CORINFO_FLG_INTERFACE)
                    op1->gtFlags |= GTF_CALL_INTF | GTF_CALL_VIRT;
                else if (!(mflags & CORINFO_FLG_VIRTUAL) || (mflags & CORINFO_FLG_FINAL))
                    op1->gtFlags |= GTF_CALL_VIRT_RES;
                else
                    op1->gtFlags |= GTF_CALL_VIRT;
            }

            // 'op1'      should be a GT_CALL node.
            // 'sig'      the signature for the call
            // 'mflags'   should be set
            if (callTyp == TYP_STRUCT)
            {
                JITLOG((LL_INFO100000, "INLINER FAILED call returned a valueclass\n"));
                goto ABORT;
            }

            if (result == INLINE_RESPECT_BOUNDARY)
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: for cross assemby call: "
                        "%d il bytes %s called by %s\n",
                        methInfo.ILCodeSize, eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT_THIS_INLINE_ONLY;
            }


            assert(op1->OperGet() == GT_CALL);

            op2   = 0;
            flags = 0;

            /* Create the argument list
             * Special case - for varargs we have an implicit last argument */

            assert((sig.callConv & CORINFO_CALLCONV_MASK) != CORINFO_CALLCONV_VARARG);

            /* Now pop the arguments */
            op2 = op1->gtCall.gtCallArgs = impPopList(sig.numArgs, &flags, &sig, 0);
            if  (op2)
                op1->gtFlags |= op2->gtFlags & GTF_GLOB_EFFECT;

            /* Are we supposed to have a 'this' pointer? */
            if (!(mflags & CORINFO_FLG_STATIC) || opcode == CEE_NEWOBJ)
            {
                GenTreePtr  obj;

                /* Pop the 'this' value from the stack */

                if (opcode == CEE_NEWOBJ)
                    obj = thisPtr;
                else
                    obj = impPopStack().val;

#ifdef DEBUG
                clsFlags = eeGetClassAttribs(eeGetMethodClass(methHnd));
                assert(varTypeIsGC(obj->gtType) ||      // "this" is managed ptr
                       (obj->TypeGet() == TYP_I_IMPL && // "this" in ummgd but it doesnt matter
                        (clsFlags & CORINFO_FLG_VALUECLASS)));
#endif

                /* Is this a virtual or interface call? */

                if  (op1->gtFlags & (GTF_CALL_VIRT | GTF_CALL_INTF | GTF_CALL_VIRT_RES))
                {
                    /* We cannot have objptr of TYP_BYREF - value classes cannot be virtual */

                    assert(obj->gtType == TYP_REF);
                }

                /* Store the "this" value in the call */

                op1->gtFlags          |= obj->gtFlags & GTF_GLOB_EFFECT;
                op1->gtCall.gtCallObjp = obj;
            }

            if (opcode == CEE_NEWOBJ)
            {
                if (clsFlags & CORINFO_FLG_VAROBJSIZE)
                {
                    assert(!(clsFlags & CORINFO_FLG_ARRAY));    // arrays handled separately
                    // This is a 'new' of a variable sized object, wher
                    // the constructor is to return the object.  In this case
                    // the constructor claims to return VOID but we know it
                    // actually returns the new object
                    assert(callTyp == TYP_VOID);
                    callTyp = TYP_REF;
                    op1->gtType = TYP_REF;
                    impPushOnStackNoType(op1);
                }
                else
                {
                    // This is a 'new' of a non-variable sized object.
                    // append the new node (op1) to the statement list,
                    // and then push the local holding the value of this
                    // new instruction on the stack.

                    impInlineExpr = impConcatExprs(impInlineExpr, op1);
                    impPushOnStackNoType(gtNewLclvNode(tmpNum, TYP_REF));
                    break;
                }
            }

            assert(op1->gtOper == GT_CALL);
            callObj = 0;
            if (opcode == CEE_CALLVIRT) 
                callObj = op1->gtCall.gtCallObjp;

            /* Push or append the result of the call */
DONE_CALL:
                // if we haven't hit a branch or a side effect, and we are fetching
                // from 'this' then we should be able to avoid a null pointer check
            if ((tree->gtFlags & GTF_CALL_VIRT_RES) && jmpAddr == NULL && !didSideEffect && callObj != 0)
            {
                if (callObj->gtOper == GT_LCL_VAR && callObj->gtLclVar.gtLclNum == inlArgInfo[0].argTmpNum)
                    thisAccessedFirst = true;
            }


            didSideEffect = true;
            if  (callTyp == TYP_VOID)
                goto INLINE_APPEND;

            if (opcode != CEE_NEWOBJ)
            {
                impPushOnStackNoType(op1);
            }

            break;


        case CEE_NEWARR:

            /* Get the class type index operand */

            typeRef = getU4LittleEndian(codeAddr);
            clsHnd = eeFindClass(typeRef, scpHandle, fncHandle, false);

            if (!clsHnd)
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: Cannot get class handle: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT_THIS_INLINE_ONLY;
            }

            clsHnd = info.compCompHnd->getSDArrayForClass(clsHnd);
            if (!clsHnd)
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: Cannot get SDArrayClass "
                        "handle: %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT_THIS_INLINE_ONLY;
            }

            /* Form the arglist: array class handle, size */
            op2 = impPopStack().val;
            op2 = gtNewOperNode(GT_LIST, TYP_VOID,           op2, 0);
            op1 = gtNewIconEmbClsHndNode(clsHnd,
                                         typeRef,
                                         info.compScopeHnd);
            op2 = gtNewOperNode(GT_LIST, TYP_VOID, op1, op2);

            /* Create a call to 'new' */

            op1 = gtNewHelperCallNode(info.compCompHnd->getNewArrHelper(clsHnd, info.compMethodHnd),
                                      TYP_REF, 0, op2);

            /* Remember that this basic block contains 'new' of an array */

            inlineeHasNewArray = true;

            /* Push the result of the call on the stack */

            impPushOnStackNoType(op1);
            break;


        case CEE_THROW:

            if (jmpAddr == NULL)        // this routine unconditionally throws, don't bother inlining
            {
                JITLOG((LL_INFO1000000, "INLINER FAILED: Reaching 'throw' unconditionally: %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }
            
            /* Do we have just the exception on the stack ?*/

            if (verCurrentState.esStackDepth != 1)
            {
                /* if not, just don't inline the method */

                JITLOG((LL_INFO1000000, "INLINER FAILED: Reaching 'throw' with "
                        "too many things on stack: %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }

            /* Don't inline non-void conditionals that have a throw in one of the branches */

            /* NOTE: If we do allow this, note that we cant simply do a
              checkLiveness() to match the liveness at the end of the "then"
              and "else" branches of the GT_COLON. The branch with the throw
              will keep nothing live, so we should use the liveness at the
              end of the non-throw branch. */

            if  (jmpAddr && (fncRetType != TYP_VOID))
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: No inlining for THROW "
                        "within a non-void conditional in %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }

            /* Pop the exception object and create the 'throw' helper call */

            op1 = gtNewHelperCallNode(CORINFO_HELP_THROW,
                                      TYP_VOID,
                                      GTF_EXCEPT,
                                      gtNewArgList(impPopStack().val));

            goto INLINE_APPEND;


        case CEE_LDLEN:
            {
            op1 = impPopStack().val;
            }

            /* If the value is constant abort - This shouldn't happen if we
             * eliminate dead branches - have the assert only for debug, it aborts in retail */

            if (op1->gtOper == GT_CNS_INT)
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: Inliner has null object "
                        "in ldlen in %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }

            if  (!opts.compMinOptim && !opts.compDbgCode)
            {
                /* Use GT_ARR_LENGTH operator so rng check opts see this */
                op1 = gtNewOperNode(GT_ARR_LENGTH, TYP_INT, op1);
                op1->gtSetArrLenOffset(offsetof(CORINFO_Array, length));
            }
            else
            {
                /* Create the expression "*(array_addr + ArrLenOffs)" */
                op1 = gtNewOperNode(GT_ADD, TYP_REF, op1, gtNewIconNode(offsetof(CORINFO_Array, length),
                                                                        TYP_INT));

                op1 = gtNewOperNode(GT_IND, TYP_INT, op1);
            }

            /* An indirection will cause a GPF if the address is null */
            op1->gtFlags |= GTF_EXCEPT;

            /* Push the result back on the stack */
            impPushOnStackNoType(op1);
            break;


#if INLINE_CONDITIONALS

        case CEE_BR:
        case CEE_BR_S:

            assert(sz == 1 || sz == 4);

#ifdef DEBUG
            hasFOC = true;
#endif
            /* The jump must be a forward one (we don't allow loops) */

            jmpDist = (sz==1) ? getI1LittleEndian(codeAddr)
                              : getI4LittleEndian(codeAddr);
            if (jmpDist == 0)
                break;        /* NOP */

            if  (jmpDist < 0)
            {
                JITLOG((LL_EVERYTHING, "INLINER FAILED: Cannot inline backward jumps "
                        "in %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }

            
            /* Check if this is part of an 'if' */

            if (!jmpAddr)
            {
                /* Unconditional branch, the if part has probably been folded
                 * Skip the dead code and continue */

#ifdef DEBUG
                if (verbose)
                    printf("\nUnconditional branch without 'if' - Skipping %d bytes\n", sz + jmpDist);
#endif
                    codeAddr += jmpDist;
                    break;
            }

            /* Is the stack empty? */

            if  (verCurrentState.esStackDepth)
            {
                /* We allow the 'if' part to yield one value */

                if  (verCurrentState.esStackDepth > 1)
                {
                    JITLOG((LL_EVERYTHING, "INLINER FAILED: More than one value "
                            "on stack for 'if' in %s called by %s\n",
                            eeGetMethodFullName(fncHandle), info.compFullName));
                    goto ABORT;
                }

                impInlineExpr = impConcatExprs(impInlineExpr, impPopStack().val);

                ifNvoid = true;
            }
            else
                ifNvoid = false;

            /* We better be in an 'if' statement */

            if  ((jmpAddr != codeAddr + sz) || inElse)
            {
                JITLOG((LL_EVERYTHING, "INLINER FAILED: Not in an 'if' statment "
                        "in %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }

            /* Remember the 'if' statement part */

            ifStmts = impInlineExpr;
                      impInlineExpr = NULL;

            /* Record the jump target (where the 'else' part will end) */

            jmpAddr = codeBegp + (codeAddr - codeBegp) + sz + jmpDist;

            inElse  = true;
            break;


        case CEE_BRTRUE:
        case CEE_BRTRUE_S:
        case CEE_BRFALSE:
        case CEE_BRFALSE_S:

            /* Pop the comparand (now there's a neat term) from the stack */

            op1 = impPopStack().val;

            /* We'll compare against an equally-sized 0 */

            op2 = gtNewZeroConNode(genActualType(op1->gtType));

            /* Create the comparison operator and try to fold it */

            oper = (opcode==CEE_BRTRUE || opcode==CEE_BRTRUE_S) ? GT_NE
                                                                : GT_EQ;
            op1 = gtNewOperNode(oper, TYP_INT , op1, op2);

            // fall through

        COND_JUMP:

            if (lclCnt != 0 && (methInfo.options & CORINFO_OPT_INIT_LOCALS))
            {
                /* A zeroinit local may be explicitly set in only one branch of
                   the "if". If we dont consider the implicit initialization,
                   we will get incorrect lifetime for such locals. So for now,
                   just bail on such methods, unless all locals have already
                   been initialized
                   @TODO [REVISIT] [04/16/01] []: Detect such locals and handle correctly
                 */

                for (unsigned lcl = 0; lcl < lclCnt; lcl++)
                {
                    if (lclTmpNum[lcl] == -1)
                    {
                        JITLOG((LL_EVERYTHING, "INLINER FAILED: Cannot inline zeroinit "
                                "locals and conditional branch for %s called by %s\n",
                                eeGetMethodFullName(fncHandle), info.compFullName));
                        goto ABORT;
                    }
                }
            }

#ifdef DEBUG
            hasFOC = true;
#endif

            assert(op1->OperKind() & GTK_RELOP);
            assert(sz == 1 || sz == 4);

            /* The jump must be a forward one (we don't allow loops) */

            jmpDist = (sz==1) ? getI1LittleEndian(codeAddr)
                              : getI4LittleEndian(codeAddr);

            if (jmpDist == 0)
                break;          /* NOP */

            if  (jmpDist < 0)
            {
                JITLOG((LL_EVERYTHING, "INLINER FAILED: Cannot inline backward "
                        "jumps in %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }

            /* Make sure the stack is empty */

            if  (verCurrentState.esStackDepth)
            {
                JITLOG((LL_EVERYTHING, "INLINER FAILED: Non empty stack entering "
                        "if statement in %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }

            /* Currently we disallow nested if statements */

            if  (jmpAddr != NULL)
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: Cannot inline nested if "
                        "statements in %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto ABORT;
            }

            /* Try to fold the conditional */

            op1 = gtFoldExpr(op1);

            /* Have we folded the condition? */

            if  (op1->gtOper == GT_CNS_INT)
            {
                /* If unconditional jump, skip the dead code and continue
                 * If fall through, continue normally and the corresponding
                 * else part will be taken care later
                 * UNDONE!!! - revisit for nested if /else */

#ifdef DEBUG
                if (verbose)
                    printf("\nConditional folded - result = %d\n", op1->gtIntCon.gtIconVal);
#endif

                assert(op1->gtIntCon.gtIconVal == 1 || op1->gtIntCon.gtIconVal == 0);

                jmpAddr = NULL;

                if (op1->gtIntCon.gtIconVal)
                {
                    /* Skip the dead code */
#ifdef DEBUG
                    if (verbose)
                        printf("\nSkipping dead code %d bytes\n", sz + jmpDist);
#endif

                    codeAddr += jmpDist;
                }
                break;
            }

            /* Record the condition and save the current statement list */

            ifCondx = op1;
            stmList = impInlineExpr;
                      impInlineExpr = NULL;

            /* Record the jump target (where the 'if' part will end) */

            jmpAddr = codeBegp + (codeAddr - codeBegp) + sz + jmpDist;

            ifNvoid = false;
            inElse  = false;
            break;


        case CEE_CEQ:       oper = GT_EQ; goto CMP_2_OPs;

        case CEE_CGT_UN:
        case CEE_CGT:       oper = GT_GT; goto CMP_2_OPs;

        case CEE_CLT_UN:
        case CEE_CLT:       oper = GT_LT; goto CMP_2_OPs;

CMP_2_OPs:
            /* Pull two values */

            op2 = impPopStack().val;
            op1 = impPopStack().val;

            {
            assert(genActualType(op1->TypeGet()) == genActualType(op2->TypeGet()) ||
                   varTypeIsI(op1->TypeGet()) && varTypeIsI(op2->TypeGet()) ||
                   varTypeIsFloating(op1->gtType) && varTypeIsFloating(op2->gtType));
            }
            /* Create the comparison node */

            op1 = gtNewOperNode(oper, TYP_INT, op1, op2);

            /* REVIEW: I am settng both flags when only one is approprate */
            if (opcode==CEE_CGT_UN || opcode==CEE_CLT_UN)
                op1->gtFlags |= GTF_RELOP_NAN_UN | GTF_UNSIGNED;

#ifdef DEBUG
            hasFOC = true;
#endif

            /* Definetely try to fold this one */

            op1 = gtFoldExpr(op1);

            // @ISSUE :  The next opcode will almost always be a conditional
            // branch. Should we try to look ahead for it here ?

            impPushOnStackNoType(op1);
            break;


        case CEE_BEQ_S:
        case CEE_BEQ:       oper = GT_EQ; goto CMP_2_OPs_AND_BR;

        case CEE_BGE_S:
        case CEE_BGE:       oper = GT_GE; goto CMP_2_OPs_AND_BR;

        case CEE_BGE_UN_S:
        case CEE_BGE_UN:    oper = GT_GE; goto CMP_2_OPs_AND_BR_UN;

        case CEE_BGT_S:
        case CEE_BGT:       oper = GT_GT; goto CMP_2_OPs_AND_BR;

        case CEE_BGT_UN_S:
        case CEE_BGT_UN:    oper = GT_GT; goto CMP_2_OPs_AND_BR_UN;

        case CEE_BLE_S:
        case CEE_BLE:       oper = GT_LE; goto CMP_2_OPs_AND_BR;

        case CEE_BLE_UN_S:
        case CEE_BLE_UN:    oper = GT_LE; goto CMP_2_OPs_AND_BR_UN;

        case CEE_BLT_S:
        case CEE_BLT:       oper = GT_LT; goto CMP_2_OPs_AND_BR;

        case CEE_BLT_UN_S:
        case CEE_BLT_UN:    oper = GT_LT; goto CMP_2_OPs_AND_BR_UN;

        case CEE_BNE_UN_S:
        case CEE_BNE_UN:    oper = GT_NE; goto CMP_2_OPs_AND_BR_UN;


CMP_2_OPs_AND_BR_UN:
            uns = true;  unordered = true; goto CMP_2_OPs_AND_BR_ALL;

CMP_2_OPs_AND_BR:
            uns = false; unordered = false; goto CMP_2_OPs_AND_BR_ALL;

CMP_2_OPs_AND_BR_ALL:

            /* Pull two values */

            op2 = impPopStack().val;
            op1 = impPopStack().val;

            {
            assert(genActualType(op1->TypeGet()) == genActualType(op2->TypeGet()) ||
                   varTypeIsI(op1->TypeGet()) && varTypeIsI(op2->TypeGet()) ||
                   varTypeIsFloating(op1->gtType) && varTypeIsFloating(op2->gtType));
            }

            /* Create and append the operator */

            op1 = gtNewOperNode(oper, TYP_INT , op1, op2);

            if (uns)
                op1->gtFlags |= GTF_UNSIGNED;

            if (unordered)
                op1->gtFlags |= GTF_RELOP_NAN_UN;

            goto COND_JUMP;

#endif //#if INLINE_CONDITIONALS


        case CEE_BREAK:
            op1 = gtNewHelperCallNode(CORINFO_HELP_USER_BREAKPOINT, TYP_VOID);
            goto INLINE_APPEND;

        case CEE_NOP:
            break;

        case CEE_TAILCALL:
            /* If the method is inlined we can ignore the tail prefix */
            break;

         // OptIL annotations. Just skip

        case CEE_LDLOCA_S:
        case CEE_LDLOCA:
        case CEE_LDARGA_S:
        case CEE_LDARGA:
            /* @MIHAII - If you decide to implement these disalow taking the address of arguments */
                        /* currently the class library uses LDLOCA as a way of inhibiting inlining, so
                           if you implement watch out */
ABORT:
        default:
            JITLOG((LL_INFO100000, "INLINER FAILED due to opcode OP_%s\n",
                    impCurOpcName));

#ifdef  DEBUG
            if (verbose || 0)
                printf("\n\nInline expansion aborted due to opcode [%02u] OP_%s\n",
                       impCurOpcOffs, impCurOpcName);
#endif

            goto INLINING_FAILED;
        }

        codeAddr += sz;

        volatil = false;

#if INLINE_CONDITIONALS
        /* Currently FP enregistering doesn't know about QMARK - Colon
         * so we need to disable inlining of conditionals if we have
         * floats in the COLON branches */

        if (jmpAddr && verCurrentState.esStackDepth)
        {
            if (varTypeIsFloating(impStackTop().val->TypeGet()))
            {
                /* Abort inlining */

                JITLOG((LL_INFO100000, "INLINER FAILED: Inlining of conditionals "
                        "with FP not supported: %s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));

                goto INLINING_FAILED;
            }
        }
#endif
    }

DONE:

    assert(verCurrentState.esStackDepth == 0);

#if INLINE_CONDITIONALS
    assert(jmpAddr == NULL);
#endif

    /* Prepend any initialization / side effects to the return expression */

    bodyExp = impConcatExprs(impInlineExpr, bodyExp);

    // add a null check for the 'this' pointer if necessary
    if (tree->gtFlags & GTF_CALL_VIRT_RES && !thisAccessedFirst)
    {
            // we don't need to do the check if it is the 'this' pointer
            // since that is guarenteed to be non-null
        GenTreePtr obj = tree->gtCall.gtCallObjp;
        if (!(!info.compIsStatic && !optThisPtrModified && 
            obj->gtOper == GT_LCL_VAR && obj->gtLclVar.gtLclNum == 0))
        {
            GenTreePtr nullCheck = gtNewOperNode(GT_IND, TYP_INT, 
                                       impInlineFetchArg(0, inlArgInfo, lclVarInfo));
            nullCheck->gtFlags |= GTF_EXCEPT;
            bodyExp = impConcatExprs(nullCheck, bodyExp);
        }
    }

    /* Treat arguments that had to be assigned to temps */
    if (argCnt)
    {
        GenTreePtr      initArg = 0;

        for (unsigned argNum = 0; argNum < argCnt; argNum++)
        {
            if (inlArgInfo[argNum].argHasTmp)
            {
                assert(inlArgInfo[argNum].argIsUsed);

                /* For 'single' use arguments, we would have created a temp
                   node as a place holder. Can we directly use the actual
                   argument by bashing the temp node?
                   If the flag dupOfLclVars is set we globally disable
                   the bashing of 'single' use arguments.
                 */

                if (!dupOfLclVar && inlArgInfo[argNum].argBashTmpNode)
                {
                    /* Bash the temp in-place to the actual argument */

                    inlArgInfo[argNum].argBashTmpNode->CopyFrom(inlArgInfo[argNum].argNode);
                    continue;
                }
                else
                {
                    /* Create the temp assignment for this argument and append it to 'initArg' */

                    initArg = impConcatExprs(initArg,
                                             gtNewTempAssign(inlArgInfo[argNum].argTmpNum,
                                                             inlArgInfo[argNum].argNode  ));
                }
            }
            else
            {
                /* The argument is either not used or a const or lcl var */

                assert(!inlArgInfo[argNum].argIsUsed  ||
                        inlArgInfo[argNum].argIsConst ||
                        inlArgInfo[argNum].argIsLclVar );

                /* Make sure we didnt bash argNode's along the way, or else
                   subsequent uses of the arg would have worked with the bashed value */
                assert((inlArgInfo[argNum].argIsConst  == 0) == (inlArgInfo[argNum].argNode->OperIsConst() == 0));
                assert((inlArgInfo[argNum].argIsLclVar == 0) == (inlArgInfo[argNum].argNode->gtOper != GT_LCL_VAR ||
                                                                (inlArgInfo[argNum].argNode->gtFlags & GTF_GLOB_REF)));

                /* If the argument has side effects, append it to 'initArg' */

                if (inlArgInfo[argNum].argHasSideEff)
                {
                    assert(inlArgInfo[argNum].argIsUsed == false);
                    initArg = impConcatExprs(initArg, gtUnusedValNode(inlArgInfo[argNum].argNode));
                }
            }
        }

        /* Prepend any arg initialization to the body */

        bodyExp = impConcatExprs(initArg, bodyExp);
    }

        // Add the CCTOR check if asked for and can't be optimized away
    if ((methAttr & CORINFO_FLG_RUN_CCTOR) && 
        clsHandle != info.compClassHnd &&
        !staticAccessedFirst)
    {
        bodyExp = impConcatExprs(fgGetStaticsBlock(clsHandle), bodyExp);
    }

    /* Make sure we have something to return to the caller */

    if  (!bodyExp)
    {
        bodyExp = gtNewNothingNode();
    }
    else
    {
        /* Make sure the type matches the original call */

        if  (fncRetType != genActualType(bodyExp->gtType))
        {
            if  (fncRetType == TYP_VOID)
            {
                if (bodyExp->gtOper == GT_COMMA)
                {
                    /* Simply bash the comma operator type */
                    bodyExp->gtType = fncRetType;
                }
            }
            else
            {
                    /* Note that this *SHOULD* never happen unless the IL is bad */
                JITLOG((LL_INFO100000, "INLINER FAILED: Return type mismatch: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));

                goto INLINING_FAILED;
            }
        }
    }

    /* Produce our class initialization side effect if necessary */

    if (clsHandle && 
        clsHandle != info.compClassHnd               &&
        !(methAttr & CORINFO_FLG_RUN_CCTOR)          && 
        ((methAttr & CORINFO_FLG_STATIC)      ||
         (methAttr & CORINFO_FLG_CONSTRUCTOR) ||
         (methAttr & CORINFO_FLG_VALUECLASS)))
    {
        if (!(clsAttr & CORINFO_FLG_INITIALIZED)     &&
            (clsAttr & CORINFO_FLG_NEEDS_INIT))
        {
            /* We need to run the cctor (this will also handle the restore case) */
            if (!info.compCompHnd->initClass(clsHandle, info.compMethodHnd, FALSE))
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: Could not complete class init side effect: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto INLINING_FAILED;
            }
        }
        else if ((methAttr & CORINFO_FLG_STATIC)      ||
                 (methAttr & CORINFO_FLG_CONSTRUCTOR))
        {
            /* We need to restore the class */
            if (!info.compCompHnd->loadClass(clsHandle, info.compMethodHnd, FALSE))
            {
                JITLOG((LL_INFO100000, "INLINER FAILED: Could not complete class restore side effect: "
                        "%s called by %s\n",
                        eeGetMethodFullName(fncHandle), info.compFullName));
                goto INLINING_FAILED;
            }
        }
    }


#ifdef  DEBUG

    JITLOG((LL_INFO10000, "Jit Inlined %s%s called by %s\n",
            hasFOC ? "FOC " : "",
            eeGetMethodFullName(fncHandle),
            info.compFullName));

    if (verbose || 0)
    {
        printf("\n\nInlined %s called by %s:\n", eeGetMethodFullName(fncHandle), info.compFullName);

        //gtDispTree(bodyExp);
    }


    if  (verbose||0)
    {
        printf("Call before inlining:\n");
        gtDispTree(tree);
        printf("\n");

        printf("Call  after inlining:\n");
        if  (bodyExp)
            gtDispTree(bodyExp);
        else
            printf("<NOP>\n");

        printf("\n");
        printf("");         // null string means flush
    }

#endif

    /* Success we have inlined the method - set all the global cached flags */

    if (inlineeHasRangeChks)
        fgHasRangeChks = true;

    if (inlineeHasNewArray)
        compCurBB->bbFlags |= BBF_NEW_ARRAY;

    /* Return the inlined function as a chain of GT_COMMA "statements" */

    *pInlinedTree = bodyExp;

    // @TODO [REVISIT] [04/16/01] []: remove this
    if (result != INLINE_PASS) {
        JITLOG((LL_EVERYTHING, "INLINER SUCCEEDED for cross assemby call: "
                "%d il bytes %s called by %s\n",
                methInfo.ILCodeSize, eeGetMethodFullName(fncHandle), info.compFullName));
    }
    return INLINE_PASS;

INLINING_FAILED:

    /* We couldn't inline the function, but we may
     * already have allocated temps so cleanup */

    lvaCount = startVars;

    return INLINE_NEVER;

ABORT_THIS_INLINE_ONLY:

    /* We couldn't inline the function, but we may
     * already have allocated temps so cleanup */

    lvaCount = startVars;

    return INLINE_FAIL;
}

/*****************************************************************************/
#endif//INLINING
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\gtlist.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*****************************************************************************/
#ifndef GTNODE
#error  Define GTNODE before including this file.
#endif
/*****************************************************************************/
//
//    Node enum
//                   , "Node name"
//                                  ,commutative
//                                    ,operKind

GTNODE(GT_NONE       , "<none>"     ,0,GTK_SPECIAL)

//-----------------------------------------------------------------------------
//  Leaf nodes (i.e. these nodes have no sub-operands):
//-----------------------------------------------------------------------------

GTNODE(GT_LCL_VAR    , "lclVar"     ,0,GTK_LEAF)    // function variable
GTNODE(GT_LCL_FLD    , "lclFld"     ,0,GTK_LEAF)    // Field in a non-primitive variable
GTNODE(GT_RET_ADDR   , "retAddr"    ,0,GTK_LEAF)    // Return-address (for sub-procedures)
GTNODE(GT_CATCH_ARG  , "catchArg"   ,0,GTK_LEAF)    // Exception object in a catch block
GTNODE(GT_LABEL      , "codeLabel"  ,0,GTK_LEAF)    // Jump-target
GTNODE(GT_POP        , "pop"        ,0,GTK_LEAF)    // Use value sitting on stack (for tail-recursion)
GTNODE(GT_FTN_ADDR   , "ftnAddr"    ,0,GTK_LEAF)    // Address of a function

GTNODE(GT_BB_QMARK   , "bb qmark"   ,0,GTK_LEAF)    // Use of a value resulting from conditionally-executing BasicBlocks
GTNODE(GT_BB_COLON   , "bb_colon"   ,0,GTK_UNOP)    // Yielding of a value by conditionally-executing BasicBlocks

//-----------------------------------------------------------------------------
//  Constant nodes:
//-----------------------------------------------------------------------------

GTNODE(GT_CNS_INT    , "const"      ,0,GTK_LEAF|GTK_CONST)
GTNODE(GT_CNS_LNG    , "const"      ,0,GTK_LEAF|GTK_CONST)
GTNODE(GT_CNS_DBL    , "const"      ,0,GTK_LEAF|GTK_CONST)
GTNODE(GT_CNS_STR    , "const"      ,0,GTK_LEAF|GTK_CONST)

//-----------------------------------------------------------------------------
//  Unary  operators (1 operand):
//-----------------------------------------------------------------------------

GTNODE(GT_NOT        , "~"          ,0,GTK_UNOP)
GTNODE(GT_NOP        , "nop"        ,0,GTK_UNOP)
GTNODE(GT_NEG        , "unary -"    ,0,GTK_UNOP)
GTNODE(GT_CHS        , "flipsign"   ,0,GTK_BINOP|GTK_ASGOP) // it's unary

GTNODE(GT_LOG0       , "log 0"      ,0,GTK_UNOP)    // (op1==0) ? 1 : 0
GTNODE(GT_LOG1       , "log 1"      ,0,GTK_UNOP)    // (op1==0) ? 0 : 1

GTNODE(GT_ARR_LENGTH , "arrLen"     ,0,GTK_UNOP)    // array-length
#if     CSELENGTH
GTNODE(GT_ARR_LENREF , "arrLenRef"  ,0,GTK_SPECIAL) // use of array-length for range-check
#endif

#if     INLINE_MATH
GTNODE(GT_MATH       , "mathFN"     ,0,GTK_UNOP)    // Math functions/operators/intrinsics
#endif

GTNODE(GT_CAST       , "cast"       ,0,GTK_UNOP)    // conversion to another type
GTNODE(GT_CKFINITE   , "ckfinite"   ,0,GTK_UNOP)    // Check for NaN
GTNODE(GT_LCLHEAP    , "lclHeap"    ,0,GTK_UNOP)    // alloca()
GTNODE(GT_VIRT_FTN   , "virtFtn"    ,0,GTK_UNOP)    // virtual-function pointer
GTNODE(GT_JMP        , "jump"       ,0,GTK_LEAF)    // Jump to another function
GTNODE(GT_JMPI       , "jumpi"      ,0,GTK_UNOP)    // Indirect jump to another function


GTNODE(GT_ADDR       , "addr"       ,0,GTK_UNOP)    // address of
GTNODE(GT_IND        , "indir"      ,0,GTK_UNOP)    // indirection
GTNODE(GT_LDOBJ      , "ldobj"      ,0,GTK_UNOP)

//-----------------------------------------------------------------------------
//  Binary operators (2 operands):
//-----------------------------------------------------------------------------

GTNODE(GT_ADD        , "+"          ,1,GTK_BINOP)
GTNODE(GT_SUB        , "-"          ,0,GTK_BINOP)
GTNODE(GT_MUL        , "*"          ,1,GTK_BINOP)
GTNODE(GT_DIV        , "/"          ,0,GTK_BINOP)
GTNODE(GT_MOD        , "%"          ,0,GTK_BINOP)

GTNODE(GT_UDIV       , "/"          ,0,GTK_BINOP)
GTNODE(GT_UMOD       , "%"          ,0,GTK_BINOP)

GTNODE(GT_OR         , "|"          ,1,GTK_BINOP|GTK_LOGOP)
GTNODE(GT_XOR        , "^"          ,1,GTK_BINOP|GTK_LOGOP)
GTNODE(GT_AND        , "&"          ,1,GTK_BINOP|GTK_LOGOP)

GTNODE(GT_LSH        , "<<"         ,0,GTK_BINOP)
GTNODE(GT_RSH        , ">>"         ,0,GTK_BINOP)
GTNODE(GT_RSZ        , ">>>"        ,0,GTK_BINOP)

GTNODE(GT_ASG        , "="          ,0,GTK_BINOP|GTK_ASGOP)
GTNODE(GT_ASG_ADD    , "+="         ,0,GTK_BINOP|GTK_ASGOP)
GTNODE(GT_ASG_SUB    , "-="         ,0,GTK_BINOP|GTK_ASGOP)
GTNODE(GT_ASG_MUL    , "*="         ,0,GTK_BINOP|GTK_ASGOP)
GTNODE(GT_ASG_DIV    , "/="         ,0,GTK_BINOP|GTK_ASGOP)
GTNODE(GT_ASG_MOD    , "%="         ,0,GTK_BINOP|GTK_ASGOP)

GTNODE(GT_ASG_UDIV   , "/="         ,0,GTK_BINOP|GTK_ASGOP)
GTNODE(GT_ASG_UMOD   , "%="         ,0,GTK_BINOP|GTK_ASGOP)

GTNODE(GT_ASG_OR     , "|="         ,0,GTK_BINOP|GTK_ASGOP)
GTNODE(GT_ASG_XOR    , "^="         ,0,GTK_BINOP|GTK_ASGOP)
GTNODE(GT_ASG_AND    , "&="         ,0,GTK_BINOP|GTK_ASGOP)
GTNODE(GT_ASG_LSH    , "<<="        ,0,GTK_BINOP|GTK_ASGOP)
GTNODE(GT_ASG_RSH    , ">>="        ,0,GTK_BINOP|GTK_ASGOP)
GTNODE(GT_ASG_RSZ    , ">>>="       ,0,GTK_BINOP|GTK_ASGOP)

GTNODE(GT_EQ         , "=="         ,0,GTK_BINOP|GTK_RELOP)
GTNODE(GT_NE         , "!="         ,0,GTK_BINOP|GTK_RELOP)
GTNODE(GT_LT         , "<"          ,0,GTK_BINOP|GTK_RELOP)
GTNODE(GT_LE         , "<="         ,0,GTK_BINOP|GTK_RELOP)
GTNODE(GT_GE         , ">="         ,0,GTK_BINOP|GTK_RELOP)
GTNODE(GT_GT         , ">"          ,0,GTK_BINOP|GTK_RELOP)

GTNODE(GT_COMMA      , "comma"      ,0,GTK_BINOP)

GTNODE(GT_QMARK      , "qmark"      ,0,GTK_BINOP)
GTNODE(GT_COLON      , "colon"      ,0,GTK_BINOP)

GTNODE(GT_INSTOF     , "instanceof" ,0,GTK_BINOP)

GTNODE(GT_INDEX      , "[]"         ,0,GTK_BINOP)   // SZ-array-element

GTNODE(GT_MKREFANY   , "mkrefany"   ,0,GTK_BINOP)

//-----------------------------------------------------------------------------
//  Other nodes that look like unary/binary operators:
//-----------------------------------------------------------------------------

GTNODE(GT_JTRUE      , "jmpTrue"    ,0,GTK_UNOP)

GTNODE(GT_LIST       , "<list>"     ,0,GTK_BINOP)

//-----------------------------------------------------------------------------
//  Other nodes that have special structure:
//-----------------------------------------------------------------------------

GTNODE(GT_FIELD      , "field"      ,0,GTK_SPECIAL) // Member-field
GTNODE(GT_ARR_ELEM   , "arrMD&"     ,0,GTK_SPECIAL) // Multi-dimensional array-element address
GTNODE(GT_CALL       , "call()"     ,0,GTK_SPECIAL)

//-----------------------------------------------------------------------------
//  Statement operator nodes:
//-----------------------------------------------------------------------------

GTNODE(GT_BEG_STMTS  , "begStmts"   ,0,GTK_SPECIAL) // used only temporarily in importer by impBegin/EndTreeList()
GTNODE(GT_STMT       , "stmtExpr"   ,0,GTK_SPECIAL) // top-level list nodes in bbTreeList

GTNODE(GT_RET        , "ret"        ,0,GTK_UNOP)    // return from sub-routine
GTNODE(GT_RETURN     , "return"     ,0,GTK_UNOP)    // return from current function
GTNODE(GT_SWITCH     , "switch"     ,0,GTK_UNOP)    // switch

GTNODE(GT_BREAK      , "break"      ,0,GTK_LEAF)    // used by debuggers
GTNODE(GT_NO_OP      , "no_op"      ,0,GTK_LEAF)    // nop!

GTNODE(GT_RETFILT    , "retfilt",    0,GTK_UNOP)    // end filter with TYP_I_IMPL return value
GTNODE(GT_END_LFIN   , "endLFin"    ,0,GTK_LEAF)    // end locally-invoked finally

GTNODE(GT_INITBLK    , "initBlk"    ,0,GTK_BINOP)
GTNODE(GT_COPYBLK    , "copyBlk"    ,0,GTK_BINOP)

//-----------------------------------------------------------------------------
//  Nodes used only within the code generator:
//-----------------------------------------------------------------------------

GTNODE(GT_REG_VAR    , "regVar"     ,0,GTK_LEAF)      // register variable
GTNODE(GT_CLS_VAR    , "clsVar"     ,0,GTK_LEAF)      // static data member

/*****************************************************************************/
#undef  GTNODE
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\instrs.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*****************************************************************************
 *  x86 instructions for [Opt]JIT compiler
 *
 *          id      -- the enum name for the instruction
 *          nm      -- textual name (for assembly dipslay)
 *          fp      -- floating point instruction
 *          um      -- update mode, see IUM_xx enum (rd, wr, or rw)
 *          rf      -- reads flags
 *          wf      -- writes flags
 *          ss      -- needs special scheduler handling (has implicit operands, etc.)
 *          mr      -- base encoding for R/M[reg] addressing mode
 *          mi      -- base encoding for R/M,icon addressing mode
 *          rm      -- base encoding for reg,R/M  addressing mode
 *          a4      -- base encoding for eax,i32  addressing mode
 *          rr      -- base encoding for register addressing mode
 *
******************************************************************************/
#ifndef INST1
#error  At least INST1 must be defined before including this file.
#endif
/*****************************************************************************/
#ifndef INST0
#define INST0(id, nm, fp, um, rf, wf, ss, mr                )
#endif
#ifndef INST2
#define INST2(id, nm, fp, um, rf, wf, ss, mr, mi            )
#endif
#ifndef INST3
#define INST3(id, nm, fp, um, rf, wf, ss, mr, mi, rm        )
#endif
#ifndef INST4
#define INST4(id, nm, fp, um, rf, wf, ss, mr, mi, rm, a4    )
#endif
#ifndef INST5
#define INST5(id, nm, fp, um, rf, wf, ss, mr, mi, rm, a4, rr)
#endif
/*****************************************************************************/
/*               The following is somewhat x86-specific                      */
/*****************************************************************************/

//    enum     name            FP  updmode rf wf ss R/M[reg]  R/M,icon  reg,R/M   eax,i32   register

INST5(push   , "push"         , 0, IUM_RD, 0, 0, 1, 0x0030FE, 0x000068, BAD_CODE, BAD_CODE, 0x000050)
INST5(pop    , "pop"          , 0, IUM_WR, 0, 0, 1, 0x00008E, BAD_CODE, BAD_CODE, BAD_CODE, 0x000058)
// Does not affect the stack tracking in the emitter
INST5(push_hide, "push"       , 0, IUM_RD, 0, 0, 1, 0x0030FE, 0x000068, BAD_CODE, BAD_CODE, 0x000050)
INST5(pop_hide,  "pop"        , 0, IUM_WR, 0, 0, 1, 0x00008E, BAD_CODE, BAD_CODE, BAD_CODE, 0x000058)

INST5(inc    , "inc"          , 0, IUM_RW, 0, 1, 0, 0x0000FE, BAD_CODE, BAD_CODE, BAD_CODE, 0x000040)
INST5(inc_l  , "inc"          , 0, IUM_RW, 0, 1, 0, 0x0000FE, BAD_CODE, BAD_CODE, BAD_CODE, 0x00C0FE)
INST5(dec    , "dec"          , 0, IUM_RW, 0, 1, 0, 0x0008FE, BAD_CODE, BAD_CODE, BAD_CODE, 0x000048)
INST5(dec_l  , "dec"          , 0, IUM_RW, 0, 1, 0, 0x0008FE, BAD_CODE, BAD_CODE, BAD_CODE, 0x00C8FE)

//    enum     name            FP  updmode rf wf ss R/M,R/M[reg] R/M,icon  reg,R/M   eax,i32

INST4(add    , "add"          , 0, IUM_RW, 0, 1, 1, 0x000000, 0x000080, 0x000002, 0x000004)
INST4(or     , "or"           , 0, IUM_RW, 0, 1, 0, 0x000008, 0x000880, 0x00000A, 0x00000C)
INST4(adc    , "adc"          , 0, IUM_RW, 1, 1, 0, 0x000010, 0x001080, 0x000012, 0x000014)
INST4(sbb    , "sbb"          , 0, IUM_RW, 1, 1, 0, 0x000018, 0x001880, 0x00001A, 0x00001C)
INST4(and    , "and"          , 0, IUM_RW, 0, 1, 0, 0x000020, 0x002080, 0x000022, 0x000024)
INST4(sub    , "sub"          , 0, IUM_RW, 0, 1, 1, 0x000028, 0x002880, 0x00002A, 0x00002C)
INST4(xor    , "xor"          , 0, IUM_RW, 0, 1, 1, 0x000030, 0x003080, 0x000032, 0x000034)
INST4(cmp    , "cmp"          , 0, IUM_RD, 0, 1, 0, 0x000038, 0x003880, 0x00003A, 0x00003C)
INST4(test   , "test"         , 0, IUM_RD, 0, 1, 0, 0x000084, 0x0000F6, 0x000084, 0x0000A8)
INST4(mov    , "mov"          , 0, IUM_WR, 0, 0, 0, 0x000088, 0x0000C6, 0x00008A, 0x0000B0)

INST4(lea    , "lea"          , 0, IUM_WR, 0, 0, 0, BAD_CODE, BAD_CODE, 0x00008D, BAD_CODE)

//    enum     name            FP  updmode rf wf ss R/M,R/M[reg]  R/M,icon  reg,R/M

INST3(movsx  , "movsx"        , 0, IUM_WR, 0, 0, 0, BAD_CODE, BAD_CODE, 0x0F00BE)
INST3(movzx  , "movzx"        , 0, IUM_WR, 0, 0, 0, BAD_CODE, BAD_CODE, 0x0F00B6)

INST3(cmovo  , "cmovo"        , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F0040)
INST3(cmovno , "cmovno"       , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F0041)
INST3(cmovb  , "cmovb"        , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F0042)
INST3(cmovae , "cmovae"       , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F0043)
INST3(cmove  , "cmove"        , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F0044)
INST3(cmovne , "cmovne"       , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F0045)
INST3(cmovbe , "cmovbe"       , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F0046)
INST3(cmova  , "cmova"        , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F0047)
INST3(cmovs  , "cmovs"        , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F0048)
INST3(cmovns , "cmovns"       , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F0049)
INST3(cmovpe , "cmovpe"       , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F004A)
INST3(cmovpo , "cmovpo"       , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F004B)
INST3(cmovl  , "cmovl"        , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F004C)
INST3(cmovge , "cmovge"       , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F004D)
INST3(cmovle , "cmovle"       , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F004E)
INST3(cmovg  , "cmovg"        , 0, IUM_WR, 1, 0, 0, BAD_CODE, BAD_CODE, 0x0F004F)

INST3(xchg   , "xchg"         , 0, IUM_RW, 0, 0, 0, 0x000084, BAD_CODE, 0x000084)
INST3(imul   , "imul"         , 0, IUM_RW, 0, 1, 0, 0x0F00AC, BAD_CODE, 0x0F00AF) // op1 *= op2

//    enum     name            FP  updmode rf wf ss R/M,R/M[reg]  R/M,icon

INST2(ret    , "ret"          , 0, IUM_RD, 0, 0, 0, 0x0000C3, 0x0000C2)
INST2(loop   , "loop"         , 0, IUM_RD, 0, 0, 0, BAD_CODE, 0x0000E2)
INST2(call   , "call"         , 0, IUM_RD, 0, 1, 0, 0x0010FF, 0x0000E8)

INST2(rcl    , "rcl"          , 0, IUM_RW, 1, 1, 1, 0x0010D2, BAD_CODE)
INST2(rcl_1  , "rcl"          , 0, IUM_RW, 1, 1, 0, 0x0010D0, 0x0010D0)
INST2(rcl_N  , "rcl"          , 0, IUM_RW, 1, 1, 0, 0x0010C0, 0x0010C0)
INST2(rcr    , "rcr"          , 0, IUM_RW, 1, 1, 1, 0x0018D2, BAD_CODE)
INST2(rcr_1  , "rcr"          , 0, IUM_RW, 1, 1, 0, 0x0018D0, 0x0018D0)
INST2(rcr_N  , "rcr"          , 0, IUM_RW, 1, 1, 0, 0x0018C0, 0x0018C0)
INST2(shl    , "shl"          , 0, IUM_RW, 0, 1, 1, 0x0020D2, BAD_CODE)
INST2(shl_1  , "shl"          , 0, IUM_RW, 0, 1, 0, 0x0020D0, 0x0020D0)
INST2(shl_N  , "shl"          , 0, IUM_RW, 0, 1, 0, 0x0020C0, 0x0020C0)
INST2(shr    , "shr"          , 0, IUM_RW, 0, 1, 1, 0x0028D2, BAD_CODE)
INST2(shr_1  , "shr"          , 0, IUM_RW, 0, 1, 0, 0x0028D0, 0x0028D0)
INST2(shr_N  , "shr"          , 0, IUM_RW, 0, 1, 0, 0x0028C0, 0x0028C0)
INST2(sar    , "sar"          , 0, IUM_RW, 0, 1, 1, 0x0038D2, BAD_CODE)
INST2(sar_1  , "sar"          , 0, IUM_RW, 0, 1, 0, 0x0038D0, 0x0038D0)
INST2(sar_N  , "sar"          , 0, IUM_RW, 0, 1, 0, 0x0038C0, 0x0038C0)

// Instead of encoding these as 3-operand instructions, we encode them
// as 2-operand instructions with the target register being implicit
// implicit_reg = op1*op2_icon
INST2(imul_AX, "imul    EAX, ", 0, IUM_RD, 0, 1, 1, BAD_CODE, 0x000068)
INST2(imul_CX, "imul    ECX, ", 0, IUM_RD, 0, 1, 1, BAD_CODE, 0x000868)
INST2(imul_DX, "imul    EDX, ", 0, IUM_RD, 0, 1, 1, BAD_CODE, 0x001068)
INST2(imul_BX, "imul    EBX, ", 0, IUM_RD, 0, 1, 1, BAD_CODE, 0x001868)

INST2(imul_SP, "imul    ESP, ", 0, IUM_RD, 0, 1, 1, BAD_CODE, BAD_CODE)
INST2(imul_BP, "imul    EBP, ", 0, IUM_RD, 0, 1, 1, BAD_CODE, 0x002868)
INST2(imul_SI, "imul    ESI, ", 0, IUM_RD, 0, 1, 1, BAD_CODE, 0x003068)
INST2(imul_DI, "imul    EDI, ", 0, IUM_RD, 0, 1, 1, BAD_CODE, 0x003868)

#ifndef instrIs3opImul
#define instrIs3opImul(ins) ((ins) >= INS_imul_AX && (ins) <= INS_imul_DI)
#endif

//    enum     name            FP  updmode rf wf ss R/M,R/M[reg]

INST1(r_movsb, "rep     movsb", 0, IUM_RD, 0, 0, 1, 0x00A4F3)
INST1(r_movsd, "rep     movsd", 0, IUM_RD, 0, 0, 1, 0x00A5F3)
INST1(movsb  , "movsb"        , 0, IUM_RD, 0, 0, 1, 0x0000A4)
INST1(movsd  , "movsd"        , 0, IUM_RD, 0, 0, 1, 0x0000A5)

INST1(r_stosb, "rep     stosb", 0, IUM_RD, 0, 0, 1, 0x00AAF3)
INST1(r_stosd, "rep     stosd", 0, IUM_RD, 0, 0, 1, 0x00ABF3)
INST1(stosb,   "stosb"        , 0, IUM_RD, 0, 0, 1, 0x0000AA)
INST1(stosd,   "stosd"        , 0, IUM_RD, 0, 0, 1, 0x0000AB)

INST1(int3   , "int3"         , 0, IUM_RD, 0, 0, 0, 0x0000CC)
INST1(nop    , "nop"          , 0, IUM_RD, 0, 0, 0, 0x000090)
INST1(leave  , "leave"        , 0, IUM_RD, 0, 0, 0, 0x0000C9)


INST1(neg    , "neg"          , 0, IUM_RW, 0, 1, 0, 0x0018F6)
INST1(not    , "not"          , 0, IUM_RW, 0, 1, 0, 0x0010F6)

INST1(cdq    , "cdq"          , 0, IUM_RD, 0, 1, 1, 0x000099)
INST1(idiv   , "idiv"         , 0, IUM_RD, 0, 1, 1, 0x0038F6)
INST1(imulEAX, "imul"         , 0, IUM_RD, 0, 1, 1, 0x0028F6) // edx:eax = eax*op1
INST1(div    , "div"          , 0, IUM_RD, 0, 1, 1, 0x0030F6)
INST1(mulEAX , "mul"          , 0, IUM_RD, 0, 1, 1, 0x0020F6)

INST1(sahf   , "sahf"         , 0, IUM_RD, 0, 1, 1, 0x00009E)

INST1(xadd   , "xadd"         , 0, IUM_RW, 0, 1, 0, 0x0F00C0)

INST1(shld   , "shld"         , 0, IUM_RW, 0, 1, 0, 0x0F00A4)
INST1(shrd   , "shrd"         , 0, IUM_RW, 0, 1, 0, 0x0F00AC)

INST1(fnstsw , "fnstsw"       , 1, IUM_WR, 1, 0, 0, 0x0020DF)
INST1(fcom   , "fcom"         , 1, IUM_RD, 0, 1, 0, 0x00D0D8)
INST1(fcomp  , "fcomp"        , 1, IUM_RD, 0, 1, 0, 0x0018D8)
INST1(fcompp , "fcompp"       , 1, IUM_RD, 0, 1, 0, 0x00D9DE)
INST1(fcomi  , "fcomi"        , 1, IUM_RD, 0, 1, 0, 0x00F0DB)
INST1(fcomip , "fcomip"       , 1, IUM_RD, 0, 1, 0, 0x00F0DF)

INST1(fchs   , "fchs"         , 1, IUM_RW, 0, 1, 0, 0x00E0D9)
#if INLINE_MATH
INST1(fabs   , "fabs"         , 1, IUM_RW, 0, 1, 0, 0x00E1D9)
INST1(fsin   , "fsin"         , 1, IUM_RW, 0, 1, 0, 0x00FED9)
INST1(fcos   , "fcos"         , 1, IUM_RW, 0, 1, 0, 0x00FFD9)
INST1(fsqrt  , "fsqrt"        , 1, IUM_RW, 0, 1, 0, 0x00FAD9)
INST1(fldl2e , "fldl2e"       , 1, IUM_RW, 0, 1, 0, 0x00EAD9)
INST1(frndint, "frndint"      , 1, IUM_RW, 0, 1, 0, 0x00FCD9)
INST1(f2xm1  , "f2xm1"        , 1, IUM_RW, 0, 1, 0, 0x00F0D9)
INST1(fscale , "fscale"       , 1, IUM_RW, 0, 1, 0, 0x00FDD9)
#endif

INST1(fld    , "fld"          , 1, IUM_WR, 0, 0, 0, 0x0000D9)
INST1(fld1   , "fld1"         , 1, IUM_WR, 0, 0, 0, 0x00E8D9)
INST1(fldz   , "fldz"         , 1, IUM_WR, 0, 0, 0, 0x00EED9)
INST1(fstp   , "fstp"         , 1, IUM_WR, 0, 0, 1, 0x0018D9)
INST1(fst    , "fst"          , 1, IUM_WR, 0, 0, 0, 0x0010D9)

INST1(fadd   , "fadd"         , 1, IUM_RW, 0, 0, 0, 0x0000D8)
INST1(faddp  , "faddp"        , 1, IUM_RW, 0, 0, 0, 0x0000DA)
INST1(fsub   , "fsub"         , 1, IUM_RW, 0, 0, 0, 0x0020D8)
INST1(fsubp  , "fsubp"        , 1, IUM_RW, 0, 0, 0, 0x0028DA)
INST1(fsubr  , "fsubr"        , 1, IUM_RW, 0, 0, 0, 0x0028D8)
INST1(fsubrp , "fsubrp"       , 1, IUM_RW, 0, 0, 0, 0x0020DA)
INST1(fmul   , "fmul"         , 1, IUM_RW, 0, 0, 0, 0x0008D8)
INST1(fmulp  , "fmulp"        , 1, IUM_RW, 0, 0, 0, 0x0008DA)
INST1(fdiv   , "fdiv"         , 1, IUM_RW, 0, 0, 0, 0x0030D8)
INST1(fdivp  , "fdivp"        , 1, IUM_RW, 0, 0, 0, 0x0038DA)
INST1(fdivr  , "fdivr"        , 1, IUM_RW, 0, 0, 0, 0x0038D8)
INST1(fdivrp , "fdivrp"       , 1, IUM_RW, 0, 0, 0, 0x0030DA)

INST1(fxch   , "fxch"         , 1, IUM_RW, 0, 0, 0, 0x00C8D9)
INST1(fprem  , "fprem"        , 0, IUM_RW, 0, 1, 0, 0x00F8D9)

INST1(fild   , "fild"         , 1, IUM_RD, 0, 0, 0, 0x0000DB)
INST1(fildl  , "fild"         , 1, IUM_RD, 0, 0, 0, 0x0028DB)
INST1(fistp  , "fistp"        , 1, IUM_WR, 0, 0, 0, 0x0018DB)
INST1(fistpl , "fistp"        , 1, IUM_WR, 0, 0, 0, 0x0038DB)

INST1(fldcw  , "fldcw"        , 1, IUM_RD, 0, 0, 0, 0x0028D9)
INST1(fnstcw , "fnstcw"       , 1, IUM_WR, 0, 0, 0, 0x0038D9)

INST1(seto   , "seto"         , 0, IUM_WR, 1, 0, 0, 0x0F0090)
INST1(setno  , "setno"        , 0, IUM_WR, 1, 0, 0, 0x0F0091)
INST1(setb   , "setb"         , 0, IUM_WR, 1, 0, 0, 0x0F0092)
INST1(setae  , "setae"        , 0, IUM_WR, 1, 0, 0, 0x0F0093)
INST1(sete   , "sete"         , 0, IUM_WR, 1, 0, 0, 0x0F0094)
INST1(setne  , "setne"        , 0, IUM_WR, 1, 0, 0, 0x0F0095)
INST1(setbe  , "setbe"        , 0, IUM_WR, 1, 0, 0, 0x0F0096)
INST1(seta   , "seta"         , 0, IUM_WR, 1, 0, 0, 0x0F0097)
INST1(sets   , "sets"         , 0, IUM_WR, 1, 0, 0, 0x0F0098)
INST1(setns  , "setns"        , 0, IUM_WR, 1, 0, 0, 0x0F0099)
INST1(setpe  , "setpe"        , 0, IUM_WR, 1, 0, 0, 0x0F009A)
INST1(setpo  , "setpo"        , 0, IUM_WR, 1, 0, 0, 0x0F009B)
INST1(setl   , "setl"         , 0, IUM_WR, 1, 0, 0, 0x0F009C)
INST1(setge  , "setge"        , 0, IUM_WR, 1, 0, 0, 0x0F009D)
INST1(setle  , "setle"        , 0, IUM_WR, 1, 0, 0, 0x0F009E)
INST1(setg   , "setg"         , 0, IUM_WR, 1, 0, 0, 0x0F009F)

INST1(i_jmp  , "jmp"          , 0, IUM_RD, 0, 0, 0, 0x0020FE)

INST0(jmp    , "jmp"          , 0, IUM_RD, 0, 0, 0, 0x0000EB)
INST0(jo     , "jo"           , 0, IUM_RD, 1, 0, 0, 0x000070)
INST0(jno    , "jno"          , 0, IUM_RD, 1, 0, 0, 0x000071)
INST0(jb     , "jb"           , 0, IUM_RD, 1, 0, 0, 0x000072)
INST0(jae    , "jae"          , 0, IUM_RD, 1, 0, 1, 0x000073)
INST0(je     , "je"           , 0, IUM_RD, 1, 0, 0, 0x000074)
INST0(jne    , "jne"          , 0, IUM_RD, 1, 0, 0, 0x000075)
INST0(jbe    , "jbe"          , 0, IUM_RD, 1, 0, 1, 0x000076)
INST0(ja     , "ja"           , 0, IUM_RD, 1, 0, 0, 0x000077)
INST0(js     , "js"           , 0, IUM_RD, 1, 0, 0, 0x000078)
INST0(jns    , "jns"          , 0, IUM_RD, 1, 0, 0, 0x000079)
INST0(jpe    , "jpe"          , 0, IUM_RD, 1, 0, 0, 0x00007A)
INST0(jpo    , "jpo"          , 0, IUM_RD, 1, 0, 0, 0x00007B)
INST0(jl     , "jl"           , 0, IUM_RD, 1, 0, 0, 0x00007C)
INST0(jge    , "jge"          , 0, IUM_RD, 1, 0, 0, 0x00007D)
INST0(jle    , "jle"          , 0, IUM_RD, 1, 0, 0, 0x00007E)
INST0(jg     , "jg"           , 0, IUM_RD, 1, 0, 0, 0x00007F)

INST0(l_jmp  , "jmp"          , 0, IUM_RD, 0, 0, 0, 0x0000E9)
INST0(l_jo   , "jo"           , 0, IUM_RD, 1, 0, 0, 0x00800F)
INST0(l_jno  , "jno"          , 0, IUM_RD, 1, 0, 0, 0x00810F)
INST0(l_jb   , "jb"           , 0, IUM_RD, 1, 0, 0, 0x00820F)
INST0(l_jae  , "jae"          , 0, IUM_RD, 1, 0, 0, 0x00830F)
INST0(l_je   , "je"           , 0, IUM_RD, 1, 0, 0, 0x00840F)
INST0(l_jne  , "jne"          , 0, IUM_RD, 1, 0, 0, 0x00850F)
INST0(l_jbe  , "jbe"          , 0, IUM_RD, 1, 0, 0, 0x00860F)
INST0(l_ja   , "ja"           , 0, IUM_RD, 1, 0, 0, 0x00870F)
INST0(l_js   , "js"           , 0, IUM_RD, 1, 0, 0, 0x00880F)
INST0(l_jns  , "jns"          , 0, IUM_RD, 1, 0, 0, 0x00890F)
INST0(l_jpe  , "jpe"          , 0, IUM_RD, 1, 0, 0, 0x008A0F)
INST0(l_jpo  , "jpo"          , 0, IUM_RD, 1, 0, 0, 0x008B0F)
INST0(l_jl   , "jl"           , 0, IUM_RD, 1, 0, 0, 0x008C0F)
INST0(l_jge  , "jge"          , 0, IUM_RD, 1, 0, 0, 0x008D0F)
INST0(l_jle  , "jle"          , 0, IUM_RD, 1, 0, 0, 0x008E0F)
INST0(l_jg   , "jg"           , 0, IUM_RD, 1, 0, 0, 0x008F0F)

INST0(align  , "align"        , 0, IUM_RD, 0, 0, 0, BAD_CODE)

#if SCHEDULER
INST0(noSched, "noSched"      , 0, IUM_RD, 0, 1, 0, BAD_CODE)
#endif

/*****************************************************************************/
#undef  INST0
#undef  INST1
#undef  INST2
#undef  INST3
#undef  INST4
#undef  INST5
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\jitpch.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
#ifndef OPT_IL_JIT

#include <windows.h>

#ifdef  UNDER_CE_GUI
#include "jitWCE.h"
#else
#include <assert.h>
#include <stdio.h>
#include <stddef.h>
#include <fcntl.h>
#include <io.h>
#define  sprintfA sprintf
#endif

#include <stdlib.h>
#include <limits.h>
#include <string.h>
#include <excpt.h>
#include <float.h>

#include "jit.h"
#include "Compiler.h"
#include "block.h"

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\lclvars.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                           LclVarsInfo                                     XX
XX                                                                           XX
XX   The variables to be used by the code generator.                         XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

#include "jitpch.h"
#pragma hdrstop
#include "emit.h"

/*****************************************************************************/

#if DOUBLE_ALIGN
#ifdef DEBUG
/* static */
unsigned            Compiler::s_lvaDoubleAlignedProcsCount = 0;
#endif
#endif

/*****************************************************************************/

void                Compiler::lvaInit()
{
    /* We haven't allocated stack variables yet */

    lvaDoneFrameLayout = 0;
}

/*****************************************************************************/

void                Compiler::lvaInitTypeRef()
{
    /* Set compArgsCount and compLocalsCount */

    info.compArgsCount      = info.compMethodInfo->args.numArgs;

    /* Is there a 'this' pointer */

    if (!info.compIsStatic)
        info.compArgsCount++;

    info.compILargsCount    = info.compArgsCount;

    /* Is there a hidden pointer to the return value ? */

    info.compRetBuffArg     = -1;   // indicates not present

    if (info.compMethodInfo->args.hasRetBuffArg())
    {
        info.compRetBuffArg = info.compIsStatic ? 0:1;
        info.compArgsCount++;
    }

    /* There is a 'hidden' cookie pushed last when the
       calling convention is varargs */

    if (info.compIsVarArgs)
        info.compArgsCount++;

    lvaCount                =
    info.compLocalsCount    = info.compArgsCount +
                              info.compMethodInfo->locals.numArgs;

    info.compILlocalsCount  = info.compILargsCount +
                              info.compMethodInfo->locals.numArgs;

    /* Now allocate the variable descriptor table */

    lvaTableCnt = lvaCount * 2;

    if (lvaTableCnt < 16)
        lvaTableCnt = 16;

    lvaTable = (LclVarDsc*)compGetMemArray(lvaTableCnt, sizeof(*lvaTable));
    size_t tableSize = lvaTableCnt * sizeof(*lvaTable);
    memset(lvaTable, 0, tableSize);

    //-------------------------------------------------------------------------
    // Count the arguments and initialize the resp. lvaTable[] entries
    //
    // First the implicit arguments
    //-------------------------------------------------------------------------

    LclVarDsc * varDsc    = lvaTable;
    unsigned varNum       = 0;
    unsigned regArgNum    = 0;

    compArgSize           = 0;

    /* Is there a "this" pointer ? */

    if  (!info.compIsStatic)
    {
        varDsc->lvIsParam   = 1;
#if ASSERTION_PROP
        varDsc->lvSingleDef = 1;
#endif

        DWORD clsFlags = eeGetClassAttribs(info.compClassHnd);
        if (clsFlags & CORINFO_FLG_VALUECLASS)
            varDsc->lvType = TYP_BYREF;
        else
            varDsc->lvType  = TYP_REF;
        
        if (tiVerificationNeeded) 
        {
            varDsc->lvVerTypeInfo = verMakeTypeInfo(info.compClassHnd);        
            if (varDsc->lvVerTypeInfo.IsValueClass())
                varDsc->lvVerTypeInfo.MakeByRef();
        }
        else
            varDsc->lvVerTypeInfo = typeInfo();

        
            // Mark the 'this' pointer for the method
        varDsc->lvVerTypeInfo.SetIsThisPtr();

        varDsc->lvIsRegArg = 1;
        assert(regArgNum == 0);
        varDsc->lvArgReg   = (regNumberSmall) genRegArgNum(0);
        varDsc->setPrefReg(varDsc->lvArgReg, this);

        regArgNum++;
#ifdef  DEBUG
        if  (verbose&&0) printf("'this'        passed in register\n");
#endif
        compArgSize       += sizeof(void *);
        varNum++;
        varDsc++;
    }

#if RET_64BIT_AS_STRUCTS

    /* Figure out whether we need to add a secret "retval addr" argument */

    fgRetArgUse = false;
    fgRetArgNum = 0xFFFF;

    if  (genTypeStSz(info.compDeclRetType) > 1)
    {
        /* Yes, we have to add the retvaladdr argument */

        fgRetArgUse = true;

        /* "this" needs to stay as argument 0, if present */

        fgRetArgNum = info.compIsStatic ? 0 : 1;

        varDsc->lvType      = TYP_REF;
        varDsc->lvIsParam   = 1;
        varDsc->lvIsRegArg  = 1;
#if ASSERTION_PROP
        varDsc->lvSingleDef = 1;
#endif
        assert(regArgNum < maxRegArg);
        varDsc->lvArgReg    = (regNumberSmall)genRegArgNum(regArgNum);
        varDsc->setPrefReg(varDsc->lvArgReg, this);

        regArgNum++;
        lvaCount++;

        /* Update the total argument size, count and varDsc */

        compArgSize += sizeof(void *);
        varNum++;
        varDsc++;
    }

#endif

    /* If we have a hidden return-buffer parameter, that comes here */

    if (info.compRetBuffArg >= 0)
    {
        assert(regArgNum < MAX_REG_ARG);

        varDsc->lvType      = TYP_BYREF;
        varDsc->lvIsParam   = 1;
        varDsc->lvIsRegArg  = 1;
#if ASSERTION_PROP
        varDsc->lvSingleDef = 1;
#endif
        varDsc->lvArgReg    = (regNumberSmall)genRegArgNum(regArgNum);
        varDsc->setPrefReg(varDsc->lvArgReg, this);

        regArgNum++;
#ifdef  DEBUG
        if  (verbose&&0) printf("'__retBuf'    passed in register\n");
#endif

        /* Update the total argument size, count and varDsc */

        compArgSize += sizeof(void*);
        varNum++;
        varDsc++;
    }

    //-------------------------------------------------------------------------
    // Now walk the function signature for the explicit arguments
    //-------------------------------------------------------------------------

    // Only (some of) the implicit args are enregistered for varargs
    unsigned maxRegArg = info.compIsVarArgs ? regArgNum : MAX_REG_ARG;

    CORINFO_ARG_LIST_HANDLE argLst  = info.compMethodInfo->args.args;
    unsigned argSigLen      = info.compMethodInfo->args.numArgs;

    unsigned i;
    for (  i = 0; 
           i < argSigLen; 
           i++, varNum++, varDsc++, argLst = eeGetArgNext(argLst))
    {
        CORINFO_CLASS_HANDLE typeHnd;
        var_types type = JITtype2varType(strip(info.compCompHnd->getArgType(&info.compMethodInfo->args, argLst, &typeHnd)));
                    
        varDsc->lvIsParam   = 1;
#if ASSERTION_PROP
        varDsc->lvSingleDef = 1;
#endif

        lvaInitVarDsc(varDsc, varNum, type, typeHnd, argLst, &info.compMethodInfo->args);

        if (regArgNum < maxRegArg && isRegParamType(varDsc->TypeGet()))
        {
            /* Another register argument */

            varDsc->lvIsRegArg = 1;
            varDsc->lvArgReg   = (regNumberSmall) genRegArgNum(regArgNum);
            varDsc->setPrefReg(varDsc->lvArgReg, this);

            regArgNum++;

#ifdef  DEBUG
            if  (verbose&&0) printf("Arg   #%3u    passed in register\n", varNum);
#endif
        }

        compArgSize += eeGetArgSize(argLst, &info.compMethodInfo->args);

        if (info.compIsVarArgs)
            varDsc->lvStkOffs       = compArgSize;
    }

    /* If the method is varargs, process the varargs cookie */

    if (info.compIsVarArgs)
    {
        varDsc->lvType      = TYP_I_IMPL;
        varDsc->lvIsParam   = 1;
        varDsc->lvAddrTaken = 1;
#if ASSERTION_PROP
        varDsc->lvSingleDef = 1;
#endif

        /* Update the total argument size, count and varDsc */

        compArgSize += sizeof(void*);
        varNum++;
        varDsc++;

        // Allocate a temp to point at the beginning of the args

        unsigned lclNum = lvaGrabTemp();
        assert(lclNum == lvaVarargsBaseOfStkArgs);
        lvaTable[lclNum].lvType = TYP_I_IMPL;
    }

    /* We set info.compArgsCount in compCompile() */

    assert(varNum == info.compArgsCount);

    assert(regArgNum <= MAX_REG_ARG);
    rsCalleeRegArgNum = regArgNum;

    /* The total argument size must be aligned. */

    assert((compArgSize % sizeof(int)) == 0);

#if TGT_x86
    /* We cant pass more than 2^16 dwords as arguments as the "ret"
       instruction can only pop 2^16 arguments. Could be handled correctly
       but it will be very difficult for fully interruptible code */

    if (compArgSize != (size_t)(unsigned short)compArgSize)
        NO_WAY("Too many arguments for the \"ret\" instruction to pop");
#endif

    /* Does it fit into LclVarDsc.lvStkOffs (which is a signed short) */

    if (compArgSize != (size_t)(signed short)compArgSize)
        NO_WAY("Arguments are too big for the jit to handle");

    //-------------------------------------------------------------------------
    // Finally the local variables
    //-------------------------------------------------------------------------

    CORINFO_ARG_LIST_HANDLE     localsSig = info.compMethodInfo->locals.args;

    for(  i = 0; 
          i < info.compMethodInfo->locals.numArgs; 
          i++, varNum++, varDsc++, localsSig = eeGetArgNext(localsSig))
    {
        CORINFO_CLASS_HANDLE        typeHnd;
        CorInfoTypeWithMod corInfoType = info.compCompHnd->getArgType(&info.compMethodInfo->locals, localsSig, &typeHnd);
        varDsc->lvPinned = ((corInfoType & CORINFO_TYPE_MOD_PINNED) != 0);

        lvaInitVarDsc(varDsc, varNum, JITtype2varType(strip(corInfoType)), typeHnd, localsSig, &info.compMethodInfo->locals);
    }
#ifdef DEBUG
    if (verbose)
        lvaTableDump(true);
#endif
}

/*****************************************************************************/
void                Compiler::lvaInitVarDsc(LclVarDsc *              varDsc,
                                            unsigned                 varNum,
                                            var_types                type,
                                            CORINFO_CLASS_HANDLE     typeHnd,
                                            CORINFO_ARG_LIST_HANDLE  varList, 
                                            CORINFO_SIG_INFO *       varSig)
{
    assert(varDsc == &lvaTable[varNum]);

    varDsc->lvType   = type;
    if (varTypeIsGC(type)) 
        varDsc->lvStructGcCount = 1;

    if (type == TYP_STRUCT) 
        lvaSetStruct(varNum, typeHnd);
    else if (tiVerificationNeeded) 
    {
        varDsc->lvVerTypeInfo = verParseArgSigToTypeInfo(varSig, varList);
        
        // Disallow byrefs to byref like objects (ArgTypeHandle)
        // techncally we could get away with just not setting them
        if (varDsc->lvVerTypeInfo.IsByRef() && verIsByRefLike(DereferenceByRef(varDsc->lvVerTypeInfo)))
            varDsc->lvVerTypeInfo = typeInfo();
    }

#if OPT_BOOL_OPS
    if  (type == TYP_BOOL)
        varDsc->lvIsBoolean = true;
#endif
}

/*****************************************************************************
 * Returns our internal varNum for a given IL variable.
 * asserts assume it is called after lvaTable[] has been set up.
 * Returns -1 if it cant be mapped.
 */

unsigned                Compiler::compMapILvarNum(unsigned ILvarNum)
{
    assert(ILvarNum < info.compILlocalsCount ||
           ILvarNum == ICorDebugInfo::VARARGS_HANDLE);

    unsigned varNum;

    if (ILvarNum == ICorDebugInfo::VARARGS_HANDLE)
    {
        // The varargs cookie is the last argument in lvaTable[]
        assert(info.compIsVarArgs);

        varNum = lvaVarargsHandleArg;
        assert(lvaTable[varNum].lvIsParam);
    }
    else if (ILvarNum == RETBUF_ILNUM)
    {
        assert(info.compRetBuffArg >= 0);
        varNum = unsigned(info.compRetBuffArg);
    }
    else if (ILvarNum >= info.compLocalsCount)
    {
        varNum = -1;
    }
    else if (ILvarNum < info.compILargsCount)
    {
        // Parameter
        varNum = compMapILargNum(ILvarNum);
        assert(lvaTable[varNum].lvIsParam);
    }
    else
    {
        // Local variable
        unsigned lclNum = ILvarNum - info.compILargsCount;
        varNum = info.compArgsCount + lclNum;
        assert(!lvaTable[varNum].lvIsParam);
    }

    assert(varNum < info.compLocalsCount);
    return varNum;
}


/*****************************************************************************
 * Returns the IL variable number given our internal varNum.
 * Special return values are
 *       VARG_ILNUM (-1)
 *     RETBUF_ILNUM (-2)
 *    UNKNOWN_ILNUM (-3)
 *
 * Returns UNKNOWN_ILNUM if it can't be mapped.
 */

unsigned                Compiler::compMap2ILvarNum(unsigned varNum)
{
    assert(varNum < lvaCount);
    assert(ICorDebugInfo::VARARGS_HANDLE == VARG_ILNUM);

    if (varNum == (unsigned) info.compRetBuffArg)
        return RETBUF_ILNUM;

    // Is this a varargs function?

    if (info.compIsVarArgs)
    {
        // We create an extra argument for the varargs handle at the
        // end of the other arguements in lvaTable[].

        if (varNum == lvaVarargsHandleArg)
            return VARG_ILNUM;
        else if (varNum > lvaVarargsHandleArg)
            varNum--;
    }
    
    if (varNum >= info.compLocalsCount)
        return UNKNOWN_ILNUM;  // Cannot be mapped

    /* Is there a hidden argument for the return buffer.
       Note that this code works because if the retBuf is not present, 
       compRetBuffArg will be negative, which when cast to an unsigned
       is larger than any possible varNum */

    if (varNum > (unsigned) info.compRetBuffArg)
        varNum--;

    return varNum;
}


/*****************************************************************************
 * Returns true if "ldloca" was used on the variable
 */

bool                Compiler::lvaVarAddrTaken(unsigned varNum)
{
    assert(varNum < lvaCount);

    return lvaTable[varNum].lvAddrTaken;
}

/*****************************************************************************
 * Returns the handle to the class of the local variable lclNum
 */

CORINFO_CLASS_HANDLE        Compiler::lvaGetStruct(unsigned varNum)
{
    return lvaTable[varNum].lvVerTypeInfo.GetClassHandleForValueClass();
}

/*****************************************************************************
 * Set the lvClass for a local variable of TYP_STRUCT */

void   Compiler::lvaSetStruct(unsigned varNum, CORINFO_CLASS_HANDLE typeHnd)
{
    assert(varNum < lvaCount);

    LclVarDsc *  varDsc = &lvaTable[varNum];
    varDsc->lvType     = TYP_STRUCT;

    if (tiVerificationNeeded) {
        varDsc->lvVerTypeInfo = verMakeTypeInfo(CORINFO_TYPE_VALUECLASS, typeHnd);
            // If we have a bad sig, treat it as dead, but don't call back to the EE
            // to get its particulars, since the EE will assert.
        if (varDsc->lvVerTypeInfo.IsDead())
        {
            varDsc->lvType = TYP_VOID;
            return;
        }
    }
    else
        varDsc->lvVerTypeInfo = typeInfo(TI_STRUCT, typeHnd);

    varDsc->lvSize     = roundUp(info.compCompHnd->getClassSize(typeHnd),  sizeof(void*));
    varDsc->lvGcLayout = (BYTE*) compGetMemA(varDsc->lvSize / sizeof(void*) * sizeof(BYTE));
    unsigned numGCVars = eeGetClassGClayout(typeHnd, varDsc->lvGcLayout);
    if (numGCVars >= 8)
        numGCVars = 7;
    varDsc->lvStructGcCount = numGCVars;
}

/*****************************************************************************
 * Returns the array of BYTEs containing the GC layout information
 */

BYTE *             Compiler::lvaGetGcLayout(unsigned varNum)
{
    assert(lvaTable[varNum].lvType == TYP_STRUCT);

    return lvaTable[varNum].lvGcLayout;
}

/*****************************************************************************
 * Return the number of bytes needed for a local varable
 */

size_t              Compiler::lvaLclSize(unsigned varNum)
{
    var_types   varType = lvaTable[varNum].TypeGet();

    switch(varType)
    {
    case TYP_STRUCT:
    case TYP_BLK:
        return lvaTable[varNum].lvSize;

    case TYP_LCLBLK:
        assert(lvaScratchMem > 0);
        assert(varNum == lvaScratchMemVar);
        return lvaScratchMem;

    default:    // This is a primitve var. Fall out of switch statement
        break;
    }

    return genTypeStSz(varType)*sizeof(int);
}

/*****************************************************************************
 *
 *  Callback used by the tree walker to call lvaDecRefCnts
 */
Compiler::fgWalkResult      Compiler::lvaDecRefCntsCB(GenTreePtr tree, void *p)
{
    assert(p);
    ((Compiler *)p)->lvaDecRefCnts(tree);
    return WALK_CONTINUE;
}


/*****************************************************************************
 *
 *  Helper passed to the tree walker to decrement the refCnts for
 *  all local variables in an expression
 */
void               Compiler::lvaDecRefCnts(GenTreePtr tree)
{
    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    if ((tree->gtOper == GT_CALL) && (tree->gtFlags & GTF_CALL_UNMANAGED))
    {
        /* Get the special variable descriptor */

        lclNum = info.compLvFrameListRoot;
            
        assert(lclNum <= lvaCount);
        varDsc = lvaTable + lclNum;
            
        /* Decrement the reference counts twice */

        varDsc->decRefCnts(compCurBB->bbWeight, this);  
        varDsc->decRefCnts(compCurBB->bbWeight, this);
    }
    else
    {
        /* This must be a local variable */

        assert(tree->gtOper == GT_LCL_VAR || tree->gtOper == GT_LCL_FLD);

        /* Get the variable descriptor */

        lclNum = tree->gtLclVar.gtLclNum;

        assert(lclNum < lvaCount);
        varDsc = lvaTable + lclNum;

        /* Decrement its lvRefCnt and lvRefCntWtd */

        varDsc->decRefCnts(compCurBB->bbWeight, this);
    }
}

/*****************************************************************************
 *
 *  Callback used by the tree walker to call lvaIncRefCnts
 */
Compiler::fgWalkResult      Compiler::lvaIncRefCntsCB(GenTreePtr tree, void *p)
{
    assert(p);
    ((Compiler *)p)->lvaIncRefCnts(tree);
    return WALK_CONTINUE;
}

/*****************************************************************************
 *
 *  Helper passed to the tree walker to increment the refCnts for
 *  all local variables in an expression
 */
void               Compiler::lvaIncRefCnts(GenTreePtr tree)
{
    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    if ((tree->gtOper == GT_CALL) && (tree->gtFlags & GTF_CALL_UNMANAGED))
    {
        /* Get the special variable descriptor */

        lclNum = info.compLvFrameListRoot;
            
        assert(lclNum <= lvaCount);
        varDsc = lvaTable + lclNum;
            
        /* Increment the reference counts twice */

        varDsc->incRefCnts(compCurBB->bbWeight, this);  
        varDsc->incRefCnts(compCurBB->bbWeight, this);
    }
    else
    {
        /* This must be a local variable */

        assert(tree->gtOper == GT_LCL_VAR || tree->gtOper == GT_LCL_FLD);

        /* Get the variable descriptor */

        lclNum = tree->gtLclVar.gtLclNum;

        assert(lclNum < lvaCount);
        varDsc = lvaTable + lclNum;

        /* Increment its lvRefCnt and lvRefCntWtd */

        varDsc->incRefCnts(compCurBB->bbWeight, this);
    }
}


/*****************************************************************************
 *
 *  Compare function passed to qsort() by Compiler::lclVars.lvaSortByRefCount().
 *  when generating SMALL_CODE.
 *    Return positive if dsc2 has a higher ref count
 *    Return negative if dsc1 has a higher ref count
 *    Return zero     if the ref counts are the same
 *    lvPrefReg is only used to break ties
 */

/* static */
int __cdecl         Compiler::RefCntCmp(const void *op1, const void *op2)
{
    LclVarDsc *     dsc1 = *(LclVarDsc * *)op1;
    LclVarDsc *     dsc2 = *(LclVarDsc * *)op2;

    /* Make sure we preference tracked variables over untracked variables */

    if  (dsc1->lvTracked != dsc2->lvTracked)
    {
        return (dsc2->lvTracked) ? +1 : -1;
    }


    unsigned weight1 = dsc1->lvRefCnt;
    unsigned weight2 = dsc2->lvRefCnt;

    /* Make sure we preference int/long/ptr over floating point types */

#if TGT_x86
    if  (dsc1->lvType != dsc2->lvType)
    {
        if (weight2 && isFloatRegType(dsc1->lvType))
            return +1;
        if (weight1 && isFloatRegType(dsc2->lvType))
            return -1;
    }
#endif

    int diff = weight2 - weight1;

    if  (diff != 0)
       return diff;

    /* The unweighted ref counts were the same */
    /* If the weighted ref counts are different then use their difference */
    diff = dsc2->lvRefCntWtd - dsc1->lvRefCntWtd;

    if  (diff != 0)
       return diff;

    /* We have equal ref counts and weighted ref counts */

    /* Break the tie by: */
    /* Increasing the weight by 2   if we have exactly one bit set in lvPrefReg   */
    /* Increasing the weight by 1   if we have more than one bit set in lvPrefReg */
    /* Increasing the weight by 0.5 if we were enregistered in the previous pass  */

    if (weight1)
    {
        if (dsc1->lvPrefReg)
    {
        if ( (dsc1->lvPrefReg & ~RBM_BYTE_REG_FLAG) && genMaxOneBit((unsigned)dsc1->lvPrefReg))
            weight1 += 2 * BB_UNITY_WEIGHT;
        else
            weight1 += 1 * BB_UNITY_WEIGHT;
    }
    if (dsc1->lvRegister)
        weight1 += BB_UNITY_WEIGHT / 2;
    }

    if (weight2)
    {
        if (dsc2->lvPrefReg)
    {
        if ( (dsc2->lvPrefReg & ~RBM_BYTE_REG_FLAG) && genMaxOneBit((unsigned)dsc2->lvPrefReg))
            weight2 += 2 * BB_UNITY_WEIGHT;
        else
            weight2 += 1 * BB_UNITY_WEIGHT;
    }
    if (dsc2->lvRegister)
        weight2 += BB_UNITY_WEIGHT / 2;
    }

    diff = weight2 - weight1;

    return diff;
}

/*****************************************************************************
 *
 *  Compare function passed to qsort() by Compiler::lclVars.lvaSortByRefCount().
 *  when not generating SMALL_CODE.
 *    Return positive if dsc2 has a higher weighted ref count
 *    Return negative if dsc1 has a higher weighted ref count
 *    Return zero     if the ref counts are the same
 */

/* static */
int __cdecl         Compiler::WtdRefCntCmp(const void *op1, const void *op2)
{
    LclVarDsc *     dsc1 = *(LclVarDsc * *)op1;
    LclVarDsc *     dsc2 = *(LclVarDsc * *)op2;

    /* Make sure we preference tracked variables over untracked variables */

    if  (dsc1->lvTracked != dsc2->lvTracked)
    {
        return (dsc2->lvTracked) ? +1 : -1;
    }

    unsigned weight1 = dsc1->lvRefCntWtd;
    unsigned weight2 = dsc2->lvRefCntWtd;
    
    /* Make sure we preference int/long/ptr over float/double */

#if TGT_x86

    if  (dsc1->lvType != dsc2->lvType)
    {
        if (weight2 && isFloatRegType(dsc1->lvType))
            return +1;
        if (weight1 && isFloatRegType(dsc2->lvType))
            return -1;
    }

#endif

    /* Increase the weight by 2 if we have exactly one bit set in lvPrefReg */
    /* Increase the weight by 1 if we have more than one bit set in lvPrefReg */

    if (weight1 && dsc1->lvPrefReg)
    {
        if ( (dsc1->lvPrefReg & ~RBM_BYTE_REG_FLAG) && genMaxOneBit((unsigned)dsc1->lvPrefReg))
            weight1 += 2 * BB_UNITY_WEIGHT;
        else
            weight1 += 1 * BB_UNITY_WEIGHT;
    }

    if (weight2 && dsc2->lvPrefReg)
    {
        if ( (dsc2->lvPrefReg & ~RBM_BYTE_REG_FLAG) && genMaxOneBit((unsigned)dsc2->lvPrefReg))
            weight2 += 2 * BB_UNITY_WEIGHT;
        else
            weight2 += 1 * BB_UNITY_WEIGHT;
    }

    int diff = weight2 - weight1;

    if (diff != 0)
        return diff;

    /* We have equal weighted ref counts */

    /* If the unweighted ref counts are different then use their difference */
    diff = dsc2->lvRefCnt - dsc1->lvRefCnt;

    if  (diff != 0)
       return diff;

    /* If one was enregistered in the previous pass then it wins */
    if (dsc1->lvRegister != dsc2->lvRegister)
    {
        if (dsc1->lvRegister)
        diff = -1;
    else
        diff = +1;
    }

    return diff;
}


/*****************************************************************************
 *
 *  Sort the local variable table by refcount and assign tracking indices.
 */

void                Compiler::lvaSortOnly()
{
    /* Now sort the variable table by ref-count */

    qsort(lvaRefSorted, lvaCount, sizeof(*lvaRefSorted),
          (compCodeOpt() == SMALL_CODE) ? RefCntCmp
                                           : WtdRefCntCmp);

    lvaSortAgain = false;

#ifdef  DEBUG

    if  (verbose && lvaCount)
    {
        printf("refCnt table for '%s':\n", info.compMethodName);

        for (unsigned lclNum = 0; lclNum < lvaCount; lclNum++)
        {
            if  (!lvaRefSorted[lclNum]->lvRefCnt)
                break;

            printf("   ");
            gtDispLclVar(lvaRefSorted[lclNum] - lvaTable);
            printf(" [%6s]: refCnt = %4u, refCntWtd = %6u",
                   varTypeName(lvaRefSorted[lclNum]->TypeGet()),
                   lvaRefSorted[lclNum]->lvRefCnt,
                   lvaRefSorted[lclNum]->lvRefCntWtd);
            unsigned pref = lvaRefSorted[lclNum]->lvPrefReg;
            if (pref)
            {
                printf(" pref ");
                if (pref & 0x01)  printf("EAX ");
                if (pref & 0x02)  printf("ECX ");
                if (pref & 0x04)  printf("EDX ");
                if (pref & 0x08)  printf("EBX ");
                if (pref & 0x20)  printf("EBP ");
                if (pref & 0x40)  printf("ESI ");
                if (pref & 0x80)  printf("EDI ");
                if (pref & 0x10)  printf("byteable ");
            }
            printf("\n");
        }

        printf("\n");
    }

#endif
}

/*****************************************************************************
 *
 *  Sort the local variable table by refcount and assign tracking indices.
 */

void                Compiler::lvaSortByRefCount()
{
    lvaTrackedCount = 0;
    lvaTrackedVars  = 0;

    if (lvaCount == 0)
        return;

    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    LclVarDsc * *   refTab;

    /* We'll sort the variables by ref count - allocate the sorted table */

    lvaRefSorted = refTab = (LclVarDsc **) compGetMemArray(lvaCount, sizeof(*refTab));

    /* Fill in the table used for sorting */

    for (lclNum = 0, varDsc = lvaTable;
         lclNum < lvaCount;
         lclNum++  , varDsc++)
    {
        /* Append this variable to the table for sorting */

        *refTab++ = varDsc;

        /* If we have JMP or JMPI, all arguments must have a location
         * even if we don't use them inside the method */

        if  (varDsc->lvIsParam && compJmpOpUsed)
        {
            /* ...except when we have varargs and the argument is
              passed on the stack.  In that case, it's important
              for the ref count to be zero, so that we don't attempt
              to track them for GC info (which is not possible since we
              don't know their offset in the stack).  See the assert at the
              end of raMarkStkVars and bug #28949 for more info. */

            if (!raIsVarargsStackArg(lclNum))
            {
                varDsc->incRefCnts(1, this);
            }
        }

        /* For now assume we'll be able to track all locals */

        varDsc->lvTracked = 1;

        /* If the ref count is zero */
        if  (varDsc->lvRefCnt == 0)
        {
            /* Zero ref count, make this untracked */
            varDsc->lvTracked   = 0;
            varDsc->lvRefCntWtd = 0;
        }

        // If the address of the local var was taken, mark it as volatile
        // All structures are assumed to have their address taken
        // Also pinned locals must be untracked
        // All untracked locals later get lvMustInit set as well
        //
        if  (varDsc->lvAddrTaken          ||
             varDsc->lvType == TYP_STRUCT ||
             varDsc->lvPinned)
        {
            varDsc->lvVolatile = 1;
            varDsc->lvTracked  = 0;
        }

        //  Are we not optimizing and we have exception handlers?
        //   if so mark all args and locals as volatile, so that they
        //   won't ever get enregistered.
        //
        if  (opts.compMinOptim && info.compXcptnsCount)
        {
            varDsc->lvVolatile = 1;
            continue;
        }

        var_types type = genActualType(varDsc->TypeGet());

        switch (type)
        {
#if CPU_HAS_FP_SUPPORT
        case TYP_FLOAT:
        case TYP_DOUBLE:
#endif
        case TYP_INT:
        case TYP_LONG:
        case TYP_REF:
        case TYP_BYREF:
            break;

        case TYP_UNDEF:
        case TYP_UNKNOWN:
            assert(!"lvType not set correctly");
            varDsc->lvType = TYP_INT;

            /* Fall through */
        default:
            varDsc->lvTracked = 0;
        }
    }

    /* Now sort the variable table by ref-count */

    lvaSortOnly();

    /* Decide which variables will be worth tracking */

    if  (lvaCount > lclMAX_TRACKED)
    {
        /* Mark all variables past the first 'lclMAX_TRACKED' as untracked */

        for (lclNum = lclMAX_TRACKED; lclNum < lvaCount; lclNum++)
        {
            lvaRefSorted[lclNum]->lvTracked = 0;
        }
    }

#ifdef DEBUG
    // Re-Initialize to -1 for safety in debug build.
    memset(lvaTrackedToVarNum, -1, sizeof(lvaTrackedToVarNum));
#endif

    /* Assign indices to all the variables we've decided to track */

    for (lclNum = 0, varDsc = lvaTable;
         lclNum < lvaCount;
         lclNum++  , varDsc++)
    {
        if  (varDsc->lvTracked)
        {
            assert(varDsc->lvRefCnt > 0);

            /* This variable will be tracked - assign it an index */

            lvaTrackedVars |= genVarIndexToBit(lvaTrackedCount);
            
            lvaTrackedToVarNum[lvaTrackedCount] = lclNum;
            
            varDsc->lvVarIndex = lvaTrackedCount++;
        }
    }
}

/*****************************************************************************
 *
 *  This is called by lvaMarkLclRefsCallback() to do variable ref marking
 */

void                Compiler::lvaMarkLclRefs(GenTreePtr tree)
{
#if INLINE_NDIRECT
    /* Is this a call to unmanaged code ? */
    if (tree->gtOper == GT_CALL && tree->gtFlags & GTF_CALL_UNMANAGED) 
    {
        /* Get the special variable descriptor */

        unsigned lclNum = info.compLvFrameListRoot;
            
        assert(lclNum <= lvaCount);
        LclVarDsc * varDsc = lvaTable + lclNum;

        /* Increment the ref counts twice */
        varDsc->incRefCnts(lvaMarkRefsWeight, this);
        varDsc->incRefCnts(lvaMarkRefsWeight, this);
    }
#endif
        
    /* Is this an assigment? */

    if (tree->OperKind() & GTK_ASGOP)
    {
        GenTreePtr      op1 = tree->gtOp.gtOp1;
        GenTreePtr      op2 = tree->gtOp.gtOp2;

#if TGT_x86

        /* Set target register for RHS local if assignment is of a "small" type */

        if (varTypeIsByte(tree->gtType))
        {
            unsigned      lclNum;
            LclVarDsc *   varDsc = NULL;

            /* GT_CHS is special it doesn't have a valid op2 */
            if (tree->gtOper == GT_CHS) 
            {
                if  (op1->gtOper == GT_LCL_VAR)
                {      
                    lclNum = op1->gtLclVar.gtLclNum;
                    assert(lclNum < lvaCount);
                    varDsc = &lvaTable[lclNum];
                }
            }
            else 
            {
                if  (op2->gtOper == GT_LCL_VAR)
                {
                    lclNum = op2->gtLclVar.gtLclNum;
                    assert(lclNum < lvaCount);
                    varDsc = &lvaTable[lclNum];
                }
            }

            if (varDsc)
                varDsc->addPrefReg(RBM_BYTE_REG_FLAG, this);
        }
#endif

#if OPT_BOOL_OPS

        /* Is this an assignment to a local variable? */

        if  (op1->gtOper == GT_LCL_VAR && op2->gtType != TYP_BOOL)
        {
            /* Only simple assignments allowed for booleans */

            if  (tree->gtOper != GT_ASG)
                goto NOT_BOOL;

            /* Is the RHS clearly a boolean value? */

            switch (op2->gtOper)
            {
                unsigned        lclNum;

            case GT_LOG0:
            case GT_LOG1:

                /* The result of log0/1 is always a true boolean */

                break;
#if 0
            // @TODO [CONSIDER] [04/16/01] []: Allow assignments of other lvIsBoolean variables.
            // We have to find the closure of such variables.
            case GT_LCL_VAR:
                lclNum = op2->gtLclVar.gtLclNum;
                if (lvaTable[lclNum].lvIsBoolean)
                    break;
                else
                    goto NOT_BOOL;
#endif

            case GT_CNS_INT:

                if  (op2->gtIntCon.gtIconVal == 0)
                    break;
                if  (op2->gtIntCon.gtIconVal == 1)
                    break;

                // Not 0 or 1, fall through ....

            default:

                if (op2->OperIsCompare())
                    break;

            NOT_BOOL:

                lclNum = op1->gtLclVar.gtLclNum;
                assert(lclNum < lvaCount);

                lvaTable[lclNum].lvIsBoolean = false;
                break;
            }
        }
#endif
    }

#if FANCY_ARRAY_OPT

    /* Special case: assignment node */

    if  (tree->gtOper == GT_ASG)
    {
        if  (tree->gtType == TYP_INT)
        {
            unsigned        lclNum1;
            LclVarDsc   *   varDsc1;

            GenTreePtr      op1 = tree->gtOp.gtOp1;

            if  (op1->gtOper != GT_LCL_VAR)
                return;

            lclNum1 = op1->gtLclVar.gtLclNum;
            assert(lclNum1 < lvaCount);
            varDsc1 = lvaTable + lclNum1;

            if  (varDsc1->lvAssignOne)
                varDsc1->lvAssignTwo = true;
            else
                varDsc1->lvAssignOne = true;
        }

        return;
    }

#endif

#if TGT_x86

    /* Special case: integer shift node by a variable amount */

    if  (tree->gtOper == GT_LSH ||
         tree->gtOper == GT_RSH ||
         tree->gtOper == GT_RSZ)
    {
        if  (tree->gtType == TYP_INT)
        {
            GenTreePtr      op2 = tree->gtOp.gtOp2;

            if  (op2->gtOper == GT_LCL_VAR)
            {
                unsigned lclNum = op2->gtLclVar.gtLclNum;
                assert(lclNum < lvaCount);
                lvaTable[lclNum].setPrefReg(REG_ECX, this);
            }
        }

        return;
    }

#endif

#if TGT_SH3

    if  (tree->gtOper == GT_IND)
    {
        /* Indexed address modes work well with r0 */

        int             rev;
        unsigned        mul;
        unsigned        cns;

        GenTreePtr      adr;
        GenTreePtr      idx;

        if  (genCreateAddrMode(tree->gtOp.gtOp1,    // address
                               0,                   // mode
                               false,               // fold
                               0,                   // reg mask
#if!LEA_AVAILABLE
                               tree->TypeGet(),     // operand type
#endif
                               &rev,                // reverse ops
                               &adr,                // base addr
                               &idx,                // index val
#if SCALED_ADDR_MODES
                               &mul,                // scaling
#endif
                               &cns,                // displacement
                               true))               // don't generate code
        {
            unsigned        varNum;
            LclVarDsc   *   varDsc;

            if  (!adr || !idx)
                goto NO_R0;
            if  (idx->gtOper == GT_CNS_INT)
                goto NO_R0;

            if      (adr->gtOper == GT_LCL_VAR)
                varNum = adr->gtLclVar.gtLclNum;
            else if (idx->gtOper == GT_LCL_VAR)
                varNum = idx->gtLclVar.gtLclNum;
            else
                goto NO_R0;

            assert(varNum < lvaCount);
            varDsc = lvaTable + varNum;

            varDsc->lvPrefReg |= RBM_r00;
        }

    NO_R0:;

    }

#endif

    if  ((tree->gtOper != GT_LCL_VAR) && (tree->gtOper != GT_LCL_FLD))
        return;

    /* This must be a local variable reference */

    assert((tree->gtOper == GT_LCL_VAR) || (tree->gtOper == GT_LCL_FLD));
    unsigned lclNum = tree->gtLclVar.gtLclNum;

    assert(lclNum < lvaCount);
    LclVarDsc * varDsc = lvaTable + lclNum;

    /* Increment the reference counts */

    varDsc->incRefCnts(lvaMarkRefsWeight, this);

    if (lvaVarAddrTaken(lclNum))
        varDsc->lvIsBoolean = false;

    if  (tree->gtOper == GT_LCL_FLD)
        return;

#if ASSERTION_PROP
    /* Exclude the normal entry block */
    if (fgDomsComputed  && fgEnterBlks    && 
        (lvaMarkRefsCurBlock->bbNum != 1) &&
        (lvaMarkRefsCurBlock->bbDom != NULL))
    {
        /* Check if fgEntryBlk dominates compCurBB */
        
        for (unsigned i=0; i < fgPerBlock; i++) 
        {
            unsigned domMask = lvaMarkRefsCurBlock->bbDom[i] & fgEnterBlks[i];
            if (i == 0)
                domMask &= ~1;  // Exclude the normal entry block
            if (domMask)
                varDsc->lvVolatileHint = 1;
        }
    }

    /* Record if the variable has a single def or not */
    if (!varDsc->lvDisqualify)
    {
        if  (tree->gtFlags & GTF_VAR_DEF)
        {
            /* This is a def of our var */
            if (varDsc->lvSingleDef || (tree->gtFlags & GTF_COLON_COND))
            {
                // We can't have multiple definitions or 
                //  definitions that occur inside QMARK-COLON trees
                goto DISQUALIFY_LCL_VAR;
            }
            else 
            {
                varDsc->lvSingleDef   = true;
                varDsc->lvDefStmt     = lvaMarkRefsCurStmt;
            }
            if (tree->gtFlags & GTF_VAR_USEASG)
                goto REF_OF_LCL_VAR;
        }
        else  // This is a ref of our variable
        {
REF_OF_LCL_VAR:
          unsigned blkNum = lvaMarkRefsCurBlock->bbNum;
          if (blkNum < 32)
          {
              varDsc->lvRefBlks |= (1 << (blkNum-1));
          }
          else
          {
DISQUALIFY_LCL_VAR:
              varDsc->lvDisqualify  = true;
              varDsc->lvSingleDef   = false;
              varDsc->lvDefStmt     = NULL;
          }
        }
    }
#endif // ASSERTION_PROP

    /* Variables must be used as the same type throughout the method */
    assert(tiVerificationNeeded ||
           varDsc->lvType == TYP_UNDEF   || tree->gtType   == TYP_UNKNOWN ||
           genActualType(varDsc->TypeGet()) == genActualType(tree->gtType) ||
           (tree->gtType == TYP_BYREF && varDsc->TypeGet() == TYP_I_IMPL)  ||
           (tree->gtType == TYP_I_IMPL && varDsc->TypeGet() == TYP_BYREF)  ||
           (tree->gtFlags & GTF_VAR_CAST) ||
           varTypeIsFloating(varDsc->TypeGet()) && varTypeIsFloating(tree->gtType));

    /* Remember the type of the reference */

    if (tree->gtType == TYP_UNKNOWN || varDsc->lvType == TYP_UNDEF)
    {
        varDsc->lvType = tree->gtType;
        assert(genActualType(varDsc->TypeGet()) == tree->gtType); // no truncation
    }

#ifdef DEBUG
    if  (tree->gtFlags & GTF_VAR_CAST)
    {
        // it should never be bigger than the variable slot
        assert(genTypeSize(tree->TypeGet()) <= genTypeSize(varDsc->TypeGet()));
    }
#endif
}


/*****************************************************************************
 *
 *  Helper passed to Compiler::fgWalkTreePre() to do variable ref marking.
 */

/* static */
Compiler::fgWalkResult  Compiler::lvaMarkLclRefsCallback(GenTreePtr tree, void *p)
{
    assert(p);

    ((Compiler*)p)->lvaMarkLclRefs(tree);

    return WALK_CONTINUE;
}

/*****************************************************************************
 *
 *  Update the local variable reference counts for one basic block
 */

void                Compiler::lvaMarkLocalVars(BasicBlock * block)
{
#if ASSERTION_PROP
    lvaMarkRefsCurBlock = block;
#endif
    lvaMarkRefsWeight   = block->bbWeight; 

#ifdef DEBUG
    if (verbose)
        printf("*** marking local variables in block BB%02d (weight=%d)\n",
               block->bbNum, lvaMarkRefsWeight);
#endif

    for (GenTreePtr tree = block->bbTreeList; tree; tree = tree->gtNext)
    {
        assert(tree->gtOper == GT_STMT);
        
#if ASSERTION_PROP
        lvaMarkRefsCurStmt = tree;
#endif

#ifdef DEBUG
        if (verbose)
            gtDispTree(tree);
#endif

    fgWalkTreePre(tree->gtStmt.gtStmtExpr, 
                      Compiler::lvaMarkLclRefsCallback, 
                      (void *) this, 
                      false);
    }
}

/*****************************************************************************
 *
 *  Create the local variable table and compute local variable reference
 *  counts.
 */

void                Compiler::lvaMarkLocalVars()
{

#ifdef DEBUG
    if (verbose)
        printf("*************** In lvaMarkLocalVars()\n");
#endif

#if INLINE_NDIRECT

    /* If there is a call to an unmanaged target, we already grabbed a
       local slot for the current thread control block.
     */

    if (info.compCallUnmanaged != 0)
    {
        assert(info.compLvFrameListRoot >= info.compLocalsCount &&
               info.compLvFrameListRoot <  lvaCount);

        lvaTable[info.compLvFrameListRoot].lvType       = TYP_I_IMPL;

        /* Set the refCnt, it is used in the prolog and return block(s) */

        lvaTable[info.compLvFrameListRoot].lvRefCnt     = 2 * BB_UNITY_WEIGHT;
        lvaTable[info.compLvFrameListRoot].lvRefCntWtd  = 2 * BB_UNITY_WEIGHT;

        info.compNDFrameOffset = lvaScratchMem;

        /* make room for the inlined frame and some spill area */
        /* return value */
        lvaScratchMem += eeGetEEInfo()->sizeOfFrame + (2*sizeof(int));
    }
#endif

    /* If there is a locspace region, we would have already grabbed a slot for
       the dummy var for the locspace region. Set its lvType in the lvaTable[].
     */

    if (lvaScratchMem)
    {
        assert(lvaScratchMemVar >= info.compLocalsCount &&
               lvaScratchMemVar <  lvaCount);

        lvaTable[lvaScratchMemVar].lvType = TYP_LCLBLK;
    }

    BasicBlock *    block;

#if OPT_BOOL_OPS

    if  (USE_GT_LOG && fgMultipleNots && (opts.compFlags & CLFLG_TREETRANS))
    {
        for (block = fgFirstBB; block; block = block->bbNext)
        {
            GenTreePtr      tree;

            for (tree = block->bbTreeList; tree; tree = tree->gtNext)
            {
                GenTreePtr      expr;

                int             logVar;
                int             nxtVar;

                assert(tree->gtOper == GT_STMT); expr = tree->gtStmt.gtStmtExpr;

                /* Check for "lclVar = log0(lclVar);" */

                logVar = expr->IsNotAssign();
                if  (logVar != -1)
                {
                    GenTreePtr      next;
                    GenTreePtr      temp;

                    /* Look for any consecutive assignments */

                    for (next = tree->gtNext; next; next = next->gtNext)
                    {
                        assert(next->gtOper == GT_STMT); temp = next->gtStmt.gtStmtExpr;

                        /* If we don't have another assignment to a local, bail */

                        nxtVar = temp->IsNotAssign();

                        if  (nxtVar == -1)
                        {
                            /* It could be a "nothing" node we put in earlier */

                            if  (temp->IsNothingNode())
                                continue;
                            else
                                break;
                        }

                        /* Do we have an assignment to the same local? */

                        if  (nxtVar == logVar)
                        {
                            LclVarDsc   *   varDsc;
                            unsigned        lclNum;

                            assert(tree->gtOper == GT_STMT);
                            assert(next->gtOper == GT_STMT);

                            /* Change the first "log0" to "log1" */

                            assert(expr->                        gtOper == GT_ASG);
                            assert(expr->gtOp.gtOp2->            gtOper == GT_LOG0);
                            assert(expr->gtOp.gtOp2->gtOp.gtOp1->gtOper == GT_LCL_VAR);

                            expr->gtOp.gtOp2->SetOper(GT_LOG1);

                            /* Special case: is the variable a boolean? */

                            lclNum = expr->gtOp.gtOp1->gtLclVar.gtLclNum;
                            assert(lclNum < lvaCount);
                            varDsc = lvaTable + lclNum;

                            /* If the variable is boolean, toss the assignment */

                            if  (varDsc->lvIsBoolean)
                                tree->gtStmt.gtStmtExpr = gtNewNothingNode();

                            /* Get rid of the second "log0" assignment */

                            next->gtStmt.gtStmtExpr = gtNewNothingNode();
                            break;
                        }
                    }
                }
            }
        }
    }

#endif

#if defined(DEBUGGING_SUPPORT) || defined(DEBUG)

// Assign slot numbers to all variables
// If compiler generated local variables, slot numbers will be
// invalid (out of range of info.compLocalVars)

// Also have to check if variable was not reallocated to another
// slot in which case we have to register the original slot #

// We dont need to do this for IL, but this keeps lvSlotNum consistent

#ifndef DEBUG
    if (opts.compScopeInfo && info.compLocalVarsCount>0)
#endif
    {
        unsigned        lclNum;
        LclVarDsc *     varDsc;

        for (lclNum = 0, varDsc = lvaTable;
             lclNum < lvaCount;
             lclNum++  , varDsc++)
        {
            varDsc->lvSlotNum = lclNum;
        }
    }

#endif

    /* Mark all local variable references */

    for (block = fgFirstBB;
         block;
         block = block->bbNext)
    {
        lvaMarkLocalVars(block);
    }

    /*  For incoming register arguments, if there are reference in the body
     *  then we will have to copy them to the final home in the prolog
     *  This counts as an extra reference with a weight of 2
     */
    if (rsCalleeRegArgNum > 0)
    {
        unsigned        lclNum;
        LclVarDsc *     varDsc;

        for (lclNum = 0, varDsc = lvaTable;
             lclNum < lvaCount;
             lclNum++  , varDsc++)
        {
            if (lclNum >= info.compArgsCount)
                break;  // early exit for loop

            if ((varDsc->lvIsRegArg) && (varDsc->lvRefCnt > 0))
            {
                varDsc->lvRefCnt    += 1 * BB_UNITY_WEIGHT;
                varDsc->lvRefCntWtd += 2 * BB_UNITY_WEIGHT;
            }
        }
    }

#if ASSERTION_PROP
    if  (!opts.compMinOptim && !opts.compDbgCode)
        optAddCopies();
#endif

    lvaSortByRefCount();
}

/*****************************************************************************
 *
 *  Compute stack frame offsets for arguments, locals and optionally temps.
 *
 *  The frame is laid out as follows :
 *
 *              ESP frames                              EBP frames
 *
 *      |                       |               |                       |
 *      |-----------------------|               |-----------------------|
 *      |       incoming        |               |       incoming        |
 *      |       arguments       |               |       arguments       |
 *      +=======================+               +=======================+
 *      |       Temps           |               |    incoming EBP       |
 *      |-----------------------|     EBP ----->|-----------------------|
 *      |       locspace        |               |   security object     |
 *      |-----------------------|               |-----------------------|
 *      |                       |               |                       |
 *      |       Variables       |               |       Variables       |
 *      |                       |               |                       |
 *      |-----------------------|               |-----------------------|
 *      |Callee saved registers |               |       locspace        |
 *      |-----------------------|               |-----------------------|
 *      |   Arguments for the   |               |       Temps           |
 *      ~    next function      ~ <----- ESP    |-----------------------|
 *      |                       |               |Callee saved registers |
 *      |       |               |               |-----------------------|
 *      |       | Stack grows   |               |       localloc        |
 *              | downward                      |-----------------------|
 *              V                               |   Arguments for the   |
 *                                              ~    next function      ~
 *                                              |                       |
 *                                              |       |               |
 *                                              |       | Stack grows   |
 *                                                      | downward
 *                                                      V
 */

void                Compiler::lvaAssignFrameOffsets(bool final)
{
    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    /* For RISC targets we assign offsets in order of ref count */

#if     TGT_RISC
#define ASSIGN_FRAME_OFFSETS_BY_REFCNT  1
#else
#define ASSIGN_FRAME_OFFSETS_BY_REFCNT  0
#endif

#if     ASSIGN_FRAME_OFFSETS_BY_REFCNT
    LclVarDsc * *   refTab;
    unsigned        refNum;
#endif

    unsigned        hasThis;
    CORINFO_ARG_LIST_HANDLE argLst;
    int             argOffs, firstStkArgOffs;

#ifdef  DEBUG

    const   char *  fprName;
    const   char *  sprName;

#if     TGT_x86
    fprName = "EBP";
    sprName = "ESP";
#elif   TGT_SH3
    fprName = "R14";
    sprName = " sp";
#else
#error  Unexpected target
#endif

#endif

#if TGT_RISC
    /* For RISC targets we asign frame offsets exactly once */
    assert(final);
#endif

    assert(lvaDoneFrameLayout < 2);
           lvaDoneFrameLayout = 1+final;

    /*-------------------------------------------------------------------------
     *
     * First process the arguments.
     * For frameless methods, the argument offsets will need to be patched up
     *  after we know how many locals/temps are on the stack.
     *
     *-------------------------------------------------------------------------
     */

#if TGT_x86

    /* Figure out the base frame offset */

    if  (!DOUBLE_ALIGN_NEED_EBPFRAME)
    {
        /*
            Assume all callee-saved registers will be pushed. Why, you
            may ask? Well, if we're not conservative wrt stack offsets,
            we may end up generating a byte-displacement opcode and
            later discover that because we need to push more registers
            the larger offset doesn't fit in a byte. So what we do is
            assume the worst (largest offsets) and if we end up not
            pushing all registers we'll go back and reduce all the
            offsets by the appropriate amount.

            In addition to the pushed callee-saved registers we also
            need to count the return address on the stack in order to
            get to the first argument.
         */

        assert(compCalleeRegsPushed * sizeof(int) <= CALLEE_SAVED_REG_MAXSZ);

        firstStkArgOffs = (compCalleeRegsPushed * sizeof(int)) + sizeof(int);
    }
    else
    {
        firstStkArgOffs = FIRST_ARG_STACK_OFFS;
    }

#else // not TGT_x86

    // RISC target

    regMaskTP       regUse;

    /*
        The argument frame offset will depend on how many callee-saved
        registers we use. This is kind of hard to predict, though, so
        we use the following approach:

            If there are no arguments that are not enregistered and
            are used within the body of the method, it doesn't matter
            how many callee-saved registers we use.

            If we do have arguments that are on the stack and are also
            used within the method, we'll use the set of callee-saved
            registers holding register variables as our estimate of
            which callee-saved registers we'll use. We'll assign frame
            offsets based on this estimate and prevent any temps from
            being stored in any other callee-saved registers.
     */

    genEstRegUse = regUse = genEstRegUse & RBM_CALLEE_SAVED;

    /* See if we have any used stack-based arguments */

    for (lclNum = 0, varDsc = lvaTable;
         lclNum < lvaCount;
         lclNum++  , varDsc++)
    {
        /* Is this an argument that lives on the frame and is used? */

        if  (varDsc->lvIsParam && varDsc->lvOnFrame)
        {
            /* We'll commit to a callee-saved reg area size right now */

            genFixedArgBase = true;

            /* Figure out the base frame offset */

            firstStkArgOffs = FIRST_ARG_STACK_OFFS;

            /* Each bit in the "regUse" mask represents a saved register */

            while (regUse)
            {
                regUse          -= genFindLowestBit(regUse);
                firstStkArgOffs += sizeof(int);
            }

            /* If this is a non-leaf method we'll have to save the retaddr */

            if  (genNonLeaf)
                firstStkArgOffs += sizeof(int);

//          printf("NOTE: Callee-saved area size fixed at %u bytes\n", firstStkArgOffs);

            goto DONE_CSR;
        }
    }

    /* We have found no used stack-based args */

//  printf("NOTE: Callee-saved area size not predicted\n");

    firstStkArgOffs = FIRST_ARG_STACK_OFFS;
    genEstRegUse    = ~(regMaskTP)0;
    genFixedArgBase = false;

DONE_CSR:

#endif // not TGT_x86 (RISC target)

    /*
        Assign stack offsets to arguments (in reverse order of passing).

        This means that if we pass arguments left->right, we start at
        the end of the list and work backwards, for right->left we start
        with the first argument and move forward.
     */

#if ARG_ORDER_R2L
    argOffs  = firstStkArgOffs;
#else
    argOffs  = firstStkArgOffs + compArgSize;
#endif

#if !STK_FASTCALL

    /* Update the argOffs to reflect arguments that are passed in registers */

    assert(rsCalleeRegArgNum <= MAX_REG_ARG);
    assert(compArgSize >= rsCalleeRegArgNum * sizeof(void *));

    argOffs -= rsCalleeRegArgNum * sizeof(void *);

#endif

    /* Is there a "this" argument? */

    hasThis  = 0;

    if  (!info.compIsStatic)
    {
        hasThis++;
    }

    lclNum = hasThis;

#if RET_64BIT_AS_STRUCTS

    /* Are we adding a secret "retval addr" argument? */

    if  (fgRetArgUse)
    {
        assert(fgRetArgNum == lclNum); lclNum++;
    }

#endif

    argLst              = info.compMethodInfo->args.args;
    unsigned argSigLen  = info.compMethodInfo->args.numArgs;

    /* if we have a hidden buffer parameter, that comes here */

    if (info.compRetBuffArg >= 0 )
    {
#if     ARG_ORDER_R2L
        assert(!"Did not implement hidden param for R2L case");
#endif
        assert(lclNum < info.compArgsCount);                // this param better be there
        assert(lclNum == (unsigned) info.compRetBuffArg);   // and be where I expect
        assert(lvaTable[lclNum].lvIsRegArg);
        lclNum++;
    }

    for(unsigned i = 0; i < argSigLen; i++)
    {
#if     ARG_ORDER_L2R
        assert(eeGetArgSize(argLst, &info.compMethodInfo->args));
        argOffs -= eeGetArgSize(argLst, &info.compMethodInfo->args);
#endif

        varDsc = lvaTable + lclNum;
        assert(varDsc->lvIsParam);

#if !STK_FASTCALL
        if (varDsc->lvIsRegArg)
        {
            /* Argument is passed in a register, don't count it
             * when updating the current offset on the stack */

            assert(eeGetArgSize(argLst, &info.compMethodInfo->args) == sizeof(void *));
            argOffs += sizeof(void *);
        }
        else
#endif
            varDsc->lvStkOffs = argOffs;

#if     ARG_ORDER_R2L
        assert(eeGetArgSize(argLst, &info.compMethodInfo->args));
        argOffs += eeGetArgSize(argLst, &info.compMethodInfo->args);
#endif

        assert(lclNum < info.compArgsCount);
        lclNum += 1;

        argLst = eeGetArgNext(argLst);
    }

    if (info.compIsVarArgs)
    {
        argOffs -= sizeof(void*);
        lvaTable[lclNum].lvStkOffs = argOffs;
    }

#if RET_64BIT_AS_STRUCTS

    /* Are we adding a secret "retval addr" argument? */

    if  (fgRetArgUse)
    {
#if     ARG_ORDER_L2R
        argOffs -= sizeof(void *);
#endif

        lvaTable[fgRetArgNum].lvStkOffs = argOffs;

#if     ARG_ORDER_R2L
        argOffs += sizeof(void *);
#endif
    }

#endif  // RET_64BIT_AS_STRUCTS

    if  (hasThis)
    {

#if     !STK_FASTCALL

        /* The "this" pointer is always passed in a register */

        assert(lvaTable[0].lvIsRegArg);
        assert(lvaTable[0].lvArgReg == genRegArgNum(0));

#else
        /* The "this" pointer is pushed last (smallest offset) */

#if     ARG_ORDER_L2R
        argOffs -= sizeof(void *);
#endif

        lvaTable[0].lvStkOffs = argOffs;

#if     ARG_ORDER_R2L
        argOffs += sizeof(void *);
#endif

#endif  // !STK_FASTCALL

    }

#if ARG_ORDER_R2L
    assert(argOffs == firstStkArgOffs + (int)compArgSize);
#else
    assert(argOffs == firstStkArgOffs);
#endif

    /*-------------------------------------------------------------------------
     *
     * Now compute stack offsets for any variables that don't live in registers
     *
     *-------------------------------------------------------------------------
     */

#if TGT_x86

    size_t calleeSavedRegsSize = 0;

    if (!genFPused)
    {
        // If FP is not used, the locals live beyond the callee-saved
        // registers. Need to add that size while accessing locals
        // relative to SP

        calleeSavedRegsSize = compCalleeRegsPushed * sizeof(int);

        assert(calleeSavedRegsSize <= CALLEE_SAVED_REG_MAXSZ);
    }

    compLclFrameSize = 0;

#else

    /* Make sure we leave enough room for outgoing arguments */

    if  (genNonLeaf && genMaxCallArgs < MIN_OUT_ARG_RESERVE)
                       genMaxCallArgs = MIN_OUT_ARG_RESERVE;

    compLclFrameSize = genMaxCallArgs;

#endif

    /* If we need space for a security token, reserve it now */

    if  (opts.compNeedSecurityCheck)
    {
        /* This can't work without an explicit frame, so make sure */

        assert(genFPused);

        /* Reserve space on the stack by bumping the frame size */

        compLclFrameSize += sizeof(void *);
    }

    /* If we need space for slots for shadow SP, reserve it now */

    if (info.compXcptnsCount || compLocallocUsed)
    {
        assert(genFPused); // else offsets of locals of frameless methods will be incorrect

        if (compLocallocUsed)
            compLclFrameSize += sizeof(void *);

        if (info.compXcptnsCount)
            compLclFrameSize += sizeof(void *); // end-of-last-executed-filter (lvaLastFilterOffs())

        compLclFrameSize     += sizeof(void *);
        lvaShadowSPfirstOffs  = compLclFrameSize;

        // plus 1 for zero-termination 
        if (info.compXcptnsCount)
            compLclFrameSize += (info.compXcptnsCount + 1) * sizeof(void*);
    }

    /*
        If we're supposed to track lifetimes of pointer temps, we'll
        assign frame offsets in the following order:

            non-pointer local variables (also untracked pointer variables)
                pointer local variables
                pointer temps
            non-pointer temps
     */

    bool    assignDone = false; // false in first pass, true in second
    bool    assignNptr = true;  // First pass,  assign offsets to non-ptr
    bool    assignPtrs = false; // Second pass, assign offsets to tracked ptrs
    bool    assignMore = false; // Are there any tracked ptrs (else 2nd pass not needed)

    /* We will use just one pass, and assign offsets to all variables */

    if  (opts.compDbgEnC)
        assignPtrs = true;

AGAIN1:

#if ASSIGN_FRAME_OFFSETS_BY_REFCNT
    for (refNum = 0, refTab = lvaRefSorted;
         refNum < lvaCount;
         refNum++  , refTab++)
    {
        assert(!opts.compDbgEnC); // For EnC, vars have to be assigned as they appear in the locals-sig
        varDsc = *refTab;
#else
    for (lclNum = 0, varDsc = lvaTable;
         lclNum < lvaCount;
         lclNum++  , varDsc++)
    {
#endif

        /* Ignore variables that are not on the stack frame */

        if  (!varDsc->lvOnFrame)
        {
            /* For EnC, all variables have to be allocated space on the
               stack, even though they may actually be enregistered. This
               way, the frame layout can be directly inferred from the
               locals-sig.
             */

            if(!opts.compDbgEnC)
                continue;
            else if (lclNum >= info.compLocalsCount) // ignore temps for EnC
                continue;
        }

        if  (varDsc->lvIsParam)
        {
            /*  A register argument that is not enregistred ends up as
                a local variable which will need stack frame space,
             */

            if  (!varDsc->lvIsRegArg)
                continue;
        }

        /* Make sure the type is appropriate */

        if  (varTypeIsGC(varDsc->TypeGet()) && varDsc->lvTracked)
        {
            if  (!assignPtrs)
            {
                assignMore = true;
                continue;
            }
        }
        else
        {
            if  (!assignNptr)
            {
                assignMore = true;
                continue;
            }
        }

#if TGT_x86

        if  (!genFPused)
        {

#if DOUBLE_ALIGN

            /* Need to align the offset? */

            if (genDoubleAlign && varDsc->lvType == TYP_DOUBLE)
            {
                assert((compLclFrameSize & 3) == 0);

                /* This makes the offset of the double divisible by 8 */

                compLclFrameSize += (compLclFrameSize & 4);
            }
#endif

            /* The stack offset is positive relative to ESP */

            varDsc->lvStkOffs = +(int)compLclFrameSize + calleeSavedRegsSize;
        }

#else // not TGT_x86

        // RISC target

        /* Just save the offset, we'll figure out the rest later */

        varDsc->lvStkOffs = compLclFrameSize;

#endif // not TGT_x86

        /* Reserve the stack space for this variable */

        compLclFrameSize += lvaLclSize(lclNum);
        assert(compLclFrameSize % sizeof(int) == 0);

#if TGT_x86

        /* Record the stack offset */

        if  (genFPused)
        {
            /* The stack offset is negative relative to EBP */

            varDsc->lvStkOffs = -(int)compLclFrameSize;
        }

#endif // TGT_x86

    }

    /* If we've only assigned one type, go back and do the others now */

    if  (!assignDone && assignMore)
    {
        assignNptr = !assignNptr;
        assignPtrs = !assignPtrs;
        assignDone = true;

        goto AGAIN1;
    }

    /*-------------------------------------------------------------------------
     *
     * Now the temps
     *
     *-------------------------------------------------------------------------
     */

#if TGT_RISC
    assert(!"temp allocation NYI for RISC");
#endif

    size_t  tempsSize = 0;  // total size of all the temps

    /* Allocate temps */

    assignPtrs = true;

    if  (TRACK_GC_TEMP_LIFETIMES)
    {
         /* first pointers, then non-pointers in second pass */
        assignNptr = false;
        assignDone = false;
    }
    else
    {
        /* Pointers and non-pointers together in single pass */
        assignNptr = true;
        assignDone = true;
    }

AGAIN2:

    for (TempDsc * temp = tmpListBeg();
         temp;
         temp = tmpListNxt(temp))
    {
        size_t          size;

        /* Make sure the type is appropriate */

        if  (!assignPtrs &&  varTypeIsGC(temp->tdTempType()))
            continue;
        if  (!assignNptr && !varTypeIsGC(temp->tdTempType()))
            continue;

        size = temp->tdTempSize();

        tempsSize += size;

        /* Figure out and record the stack offset of the temp */

        if  (genFPused)
        {
            /* The stack offset is negative relative to EBP */

                        compLclFrameSize += size;
            temp->tdOffs = -(int)compLclFrameSize;
            }
        else
        {
            /* The stack offset is positive relative to ESP */

            temp->tdOffs      = compLclFrameSize + calleeSavedRegsSize;
            compLclFrameSize += size;
        }
    }

    /* If we've only assigned some temps, go back and do the rest now */

    if  (!assignDone)
    {
        assignNptr = !assignNptr;
        assignPtrs = !assignPtrs;
        assignDone = true;

        goto AGAIN2;
    }

    /*-------------------------------------------------------------------------
     *
     * For frameless methods, patch the argument offsets
     *
     *-------------------------------------------------------------------------
     */

#if TGT_x86
    if (compLclFrameSize && !DOUBLE_ALIGN_NEED_EBPFRAME)
#else
#if DOUBLE_ALIGN
    if (compLclFrameSize && !genDoubleAlign)
#endif
#endif
    {
        /* Adjust the argument offsets by the size of the locals/temps */

        for (lclNum = 0, varDsc = lvaTable;
             lclNum < lvaCount;
             lclNum++  , varDsc++)
        {
            if  (varDsc->lvIsParam)
            {
                if  (varDsc->lvIsRegArg)
                    continue;
                varDsc->lvStkOffs += (int)compLclFrameSize;
            }
        }
    }

    /*-------------------------------------------------------------------------
     *
     * Now do some final stuff
     *
     *-------------------------------------------------------------------------
     */

#if TGT_RISC

    /* If we have to set up an FP frame, the FP->SP distance isn't known */

    assert(genFPused == false || genFPtoSP == 0);

    /*
        We can now figure out what base register (frame vs. stack
        pointer) each variable will be addressed off of, and what
        the final offset will be.

        We'll count how many variables are beyond "direct" reach
        from the stack pointer, and if there are many we'll try
        to set up an FP register even if we don't have to.
     */

    if  (!genFPused && !genFPcant)
    {
        /* We haven't decided to use FP but the option is still open */

        unsigned        varOffs;
        unsigned        minOffs = 0xFFFF;
        unsigned        loffCnt = 0;

        /* Count the references that are too far from SP */

        for (lclNum = 0, varDsc = lvaTable;
             lclNum < lvaCount;
             lclNum++  , varDsc++)
        {
            /* Ignore variables that are not on the stack frame */

            if  (!varDsc->lvOnFrame)
                continue;

            assert(varDsc->lvFPbased == false);

            /* Get hold of the SP offset of this variable */

            varOffs = varDsc->lvStkOffs; assert((int)varOffs >= 0);

            /* Is the offset of this variable very large? */

            if  (varOffs > MAX_SPBASE_OFFS)
            {
                loffCnt += varDsc->lvRefCnt;

                /* Keep track of the closest variable above the limit */

                if  (minOffs > varOffs)
                     minOffs = varOffs;
            }
        }

        if  (loffCnt > 8)       // arbitrary heuristic
        {
            /*
                The variables whose offset is less than or equal to
                MAX_SPBASE_OFFS will be addressed off of SP, others
                will be addressed off of FP (which will be set to
                the value "SP + minOffs".
             */

            genFPused = true;

            /* The value "minOffs" becomes the distance from SP to FP */

            genFPtoSP = minOffs;

            assert(minOffs < compLclFrameSize);
            assert(minOffs > MAX_SPBASE_OFFS);

            /* Mark all variables with high offsets to be FP-relative */

            for (lclNum = 0, varDsc = lvaTable;
                 lclNum < lvaCount;
                 lclNum++  , varDsc++)
            {
                /* Ignore variables that are not on the stack frame */

                if  (!varDsc->lvOnFrame)
                    continue;

                assert(varDsc->lvFPbased == false);

                /* Get hold of the SP offset of this variable */

                varOffs = varDsc->lvStkOffs; assert((int)varOffs >= 0);

                /* Is the offset of this variable very large? */

                if  (varOffs > MAX_SPBASE_OFFS)
                {
                    assert(varOffs >= minOffs);

                    /* This variable will live off of SP */

                    varDsc->lvFPbased = true;
                    varDsc->lvStkOffs = varOffs - minOffs;
                }
            }
        }
    }
#endif // TGT_RISC
}

#ifdef DEBUG
/*****************************************************************************
 *
 *  dump the lvaTable
 */

void   Compiler::lvaTableDump(bool early)
{
    printf("; %s local variable assignments\n;",
           early ? "Initial" : "Final");

    unsigned        lclNum;
    LclVarDsc *     varDsc;

    for (lclNum = 0, varDsc = lvaTable;
         lclNum < lvaCount;
         lclNum++  , varDsc++)
    {
        var_types type = varDsc->TypeGet();

        if (early)
        {
            printf("\n;  ");
            gtDispLclVar(lclNum);

            printf(" %7s ", varTypeName(type));
        }
        else
        {
            if (varDsc->lvRefCnt == 0)
                continue;

            printf("\n;  ");
            gtDispLclVar(lclNum);

            printf("[V%02u", lclNum);
            if (varDsc->lvTracked)      printf(",T%02u]", varDsc->lvVarIndex);
            else                        printf("    ]");

            printf(" (%3u,%4u%s)",
                   varDsc->lvRefCnt,
                   varDsc->lvRefCntWtd/2,
                   (varDsc->lvRefCntWtd & 1)?".5":"  ");

            printf(" %7s -> ", varTypeName(type));

            if (varDsc->lvRegister)
            {
                if (varTypeIsFloating(type))
                {
                    printf("fpu stack ");
                }
                else if (isRegPairType(type))
                {
                    assert(varDsc->lvRegNum != REG_STK);
                    if (varDsc->lvOtherReg != REG_STK)
                    {
                        /* Fully enregistered long */
                        printf("%3s:%3s   ",
                               getRegName(varDsc->lvOtherReg),  // hi32
                               getRegName(varDsc->lvRegNum));   // lo32
                    }
                    else
                    {
                        /* Partially enregistered long */
                        int  offset  = varDsc->lvStkOffs+4;
                        printf("[%1s%02XH]:%3s",
                               (offset < 0 ? "-"     : "+"),
                               (offset < 0 ? -offset : offset),
                               getRegName(varDsc->lvRegNum));    // lo32
                    }
                }
                else
                {
                    printf("%3s       ", getRegName(varDsc->lvRegNum));
                }
            }
            else
            {
                int  offset  = varDsc->lvStkOffs;
                printf("[%3s%1s%02XH] ",
                       (varDsc->lvFPbased     ? "EBP" : "ESP"),
                       (offset < 0 ? "-"     : "+"),
                       (offset < 0 ? -offset : offset));
            }
        }

        if (varDsc->lvVerTypeInfo.IsThisPtr())   printf(" this");
        if (varDsc->lvPinned)                    printf(" pinned");
        if (varDsc->lvVolatile)                  printf(" volatile");
        if (varDsc->lvRefAssign)                 printf(" ref-asgn");
        if (varDsc->lvAddrTaken)                 printf(" addr-taken");
        if (varDsc->lvMustInit)                  printf(" must-init");
    }
    if (lvaCount > 0)
        printf("\n");
}
#endif

/*****************************************************************************
 *
 *  Conservatively estimate the layout of the stack frame.
 */

size_t              Compiler::lvaFrameSize()
{
    size_t result;

#if TGT_x86

    /* Layout the stack frame conservatively.
       Assume all callee-saved registers are spilled to stack */

    compCalleeRegsPushed = CALLEE_SAVED_REG_MAXSZ/sizeof(int);

    lvaAssignFrameOffsets(false);

    result = compLclFrameSize + CALLEE_SAVED_REG_MAXSZ;

#else

    lvaAssignFrameOffsets(true);

    result = compLclFrameSize;

#endif

    return result;
}

/*****************************************************************************/
#if 0
/*****************************************************************************
 *
 *  Based on variable interference levels computed earlier, adjust reference
 *  counts for all variables. The idea is that any variable that interferes
 *  with lots of other variables will cost more to enregister, and this way
 *  variables with e.g. short lifetimes (such as compiler temps) will have
 *  precedence over long-lived variables.
 */

inline
int                 genAdjRefCnt(unsigned refCnt, unsigned refLo,
                                                  unsigned refHi,
                                 unsigned intCnt, unsigned intLo,
                                                  unsigned intHi)
{
    /*
        multiplier        total size

           0.0             803334
           0.1             803825
           0.2             803908
           0.3             803959
           0.5             804205
           0.7             804736
           1.0             806632
     */

#if DEBUG
    if (verbose && 0)
    {
        printf("ref=%4u [%04u..%04u] , int=%04u [%04u..%04u]",
                refCnt, refLo, refHi, intCnt, intLo, intHi);
        printf(" ratio=%lf , log = %lf\n", intCnt/(double)intHi,
                                           log(1+intCnt/(double)intHi));
    }
#endif

    return  (int)(refCnt * (1 - 0.1 * log(1 + intCnt / (double)intHi)  ));
}

/*****************************************************************************/

void                Compiler::lvaAdjustRefCnts()
{
    LclVarDsc   *   v1Dsc;
    unsigned        v1Num;

    LclVarDsc   *   v2Dsc;
    unsigned        v2Num;

    unsigned        refHi;
    unsigned        refLo;

    unsigned        intHi;
    unsigned        intLo;

    if  ((opts.compFlags & CLFLG_MAXOPT) != CLFLG_MAXOPT)
        return;

    /* Compute interference counts for all live variables */

    for (v1Num = 0, v1Dsc = lvaTable, refHi = 0, refLo = UINT_MAX;
         v1Num < lvaCount;
         v1Num++  , v1Dsc++)
    {
        VARSET_TP   intf;

        if  (!v1Dsc->lvTracked)
            continue;

        /* Figure out the range of ref counts */

        if  (refHi < v1Dsc->lvRefCntWtd)
             refHi = v1Dsc->lvRefCntWtd;
        if  (refLo > v1Dsc->lvRefCntWtd)
             refLo = v1Dsc->lvRefCntWtd;

        /* Now see what variables we interfere with */

        intf = lvaVarIntf[v1Dsc->lvVarIndex];

        for (v2Num = 0, v2Dsc = lvaTable;
             v2Num < v1Num;
             v2Num++  , v2Dsc++)
        {
            if  (!v2Dsc->lvTracked)
                continue;

            if  (intf & genVarIndexToBit(v2Dsc->lvVarIndex))
                v1Dsc->lvIntCnt += v2Dsc->lvRefCntWtd;
        }
    }

    refHi -= refLo;

    /* Figure out the range of int counts */

    for (v1Num = 0, v1Dsc = lvaTable, intHi = 0, intLo = UINT_MAX;
         v1Num < lvaCount;
         v1Num++  , v1Dsc++)
    {
        if  (v1Dsc->lvTracked)
        {
            if  (intHi < v1Dsc->lvIntCnt)
                 intHi = v1Dsc->lvIntCnt;
            if  (intLo > v1Dsc->lvIntCnt)
                 intLo = v1Dsc->lvIntCnt;
        }
    }

    /* Now compute the adjusted ref counts */

    for (v1Num = 0, v1Dsc = lvaTable;
         v1Num < lvaCount;
         v1Num++  , v1Dsc++)
    {
        if  (v1Dsc->lvTracked)
        {
            long  refs = genAdjRefCnt(v1Dsc->lvRefCntWtd,
                                      refLo,
                                      refHi,
                                      v1Dsc->lvIntCnt,
                                      intLo,
                                      intHi);

            if  (refs <= 0)
                refs = 1;

#ifdef DEBUG
            if  (verbose)
            {
                printf("Var #%02u ref=%4u [%04u..%04u] , int=%04u [%04u..%04u] ==> %4u\n",
                        v1Num,
                        v1Dsc->lvRefCntWtd,
                        refLo,
                        refHi,
                        v1Dsc->lvIntCnt,
                        intLo,
                        intHi,
                        refs);
            }
#endif

            v1Dsc->lvRefCntWtd = refs;
        }
    }

    /* Re-sort the variable table by ref-count */

    lvaSortByRefCount()
}

#endif // 0

/*****************************************************************************/
#ifdef DEBUG
/*****************************************************************************
 *  Pick a padding size at "random" for the local.
 *  0 means that it should not be converted to a GT_LCL_FLD
 */

static
unsigned            LCL_FLD_PADDING(unsigned lclNum)
{
    // Convert every 2nd variable
    if (lclNum % 2)
        return 0;

    // Pick a padding size at "random"
    unsigned    size = lclNum % 7;

    return size;
}


/*****************************************************************************
 *
 *  Callback for fgWalkAllTreesPre()
 *  Convert as many GT_LCL_VAR's to GT_LCL_FLD's
 */

/* static */
Compiler::fgWalkResult      Compiler::lvaStressLclFldCB(GenTreePtr tree, void * p)
{
    genTreeOps  oper    = tree->OperGet();
    GenTreePtr  lcl;

    switch(oper)
    {
    case GT_LCL_VAR:
        lcl = tree;

        break;

    case GT_ADDR:
        if (tree->gtOp.gtOp1->gtOper != GT_LCL_VAR)
            return WALK_CONTINUE;
        lcl = tree->gtOp.gtOp1;
        break;

    default:
        return WALK_CONTINUE;
    }

    Compiler *  pComp   = (Compiler*)p;
    assert(lcl->gtOper == GT_LCL_VAR);
    unsigned    lclNum  = lcl->gtLclVar.gtLclNum;
    var_types   type    = lcl->TypeGet();
    LclVarDsc * varDsc  = &pComp->lvaTable[lclNum];

    // Ignore arguments and temps
    if (varDsc->lvIsParam || lclNum >= pComp->info.compLocalsCount)
        return WALK_SKIP_SUBTREES;

#ifdef DEBUG
    // Fix for lcl_fld stress mode
    if (varDsc->lvKeepType)
    {
        return WALK_SKIP_SUBTREES;
    }
#endif

    // Cant have GC ptrs in TYP_BLK. 
    // @TODO [CONSIDER] [04/16/01] []: making up a class to hold these
    // Also, we will weed out non-primitives
    if (!varTypeIsArithmetic(type))
        return WALK_SKIP_SUBTREES;

    // Weed out "small" types like TYP_BYTE as we dont mark the GT_LCL_VAR
    // node with the accurate small type. If we bash lvaTable[].lvType,
    // then there will be no indication that it was ever a small type
    var_types varType = varDsc->TypeGet();
    if (varType != TYP_BLK &&
        genTypeSize(varType) != genTypeSize(genActualType(varType)))
        return WALK_SKIP_SUBTREES;

    assert(varDsc->lvType == lcl->gtType || varDsc->lvType == TYP_BLK);

    // Offset some of the local variable by a "random" non-zero amount
    unsigned padding = LCL_FLD_PADDING(lclNum);
    if (padding == 0)
        return WALK_SKIP_SUBTREES;

    // Bash the variable to a TYP_BLK
    if (varType != TYP_BLK)
    {
        varDsc->lvType      = TYP_BLK;
        varDsc->lvSize      = roundUp(padding + genTypeSize(varType));
        varDsc->lvAddrTaken = 1;
    }

    tree->gtFlags |= GTF_GLOB_REF;

    /* Now morph the tree appropriately */

    if (oper == GT_LCL_VAR)
    {
        /* Change lclVar(lclNum) to lclFld(lclNum,padding) */

        tree->ChangeOper(GT_LCL_FLD);
        tree->gtLclFld.gtLclOffs = padding;
    }
    else
    {
        /* Change addr(lclVar) to addr(lclVar)+padding */

        assert(oper == GT_ADDR);
        GenTreePtr  newAddr = pComp->gtNewNode(GT_NONE, TYP_UNKNOWN);
        newAddr->CopyFrom(tree);

        tree->ChangeOper(GT_ADD);
        tree->gtOp.gtOp1 = newAddr;
        tree->gtOp.gtOp2 = pComp->gtNewIconNode(padding);

        lcl->gtType = TYP_BLK;
    }

    return WALK_SKIP_SUBTREES;
}

/*****************************************************************************/

void                Compiler::lvaStressLclFld()
{
    if (opts.compDbgInfo) // Since we need to bash lvaTable[].lvType
        return;

    if (!compStressCompile(STRESS_LCL_FLDS, 5))
        return;

    fgWalkAllTreesPre(lvaStressLclFldCB, (void*)this);
}

/*****************************************************************************
 *
 *  Callback for fgWalkAllTreesPre()
 *  Convert as many TYP_INT locals to TYP_DOUBLE. Hopefully they will get
 *   enregistered on the FP stack.
 */

/* static */
Compiler::fgWalkResult      Compiler::lvaStressFloatLclsCB(GenTreePtr tree, void * p)
{
    Compiler *  pComp   = (Compiler*)p;
    genTreeOps  oper    = tree->OperGet();
    GenTreePtr  lcl;

    switch(oper)
    {
    case GT_LCL_VAR:
        if (tree->gtFlags & GTF_VAR_DEF)
            return WALK_CONTINUE;

        lcl = tree;
        break;

    case GT_ASG:
        if (tree->gtOp.gtOp1->gtOper != GT_LCL_VAR)
            return WALK_CONTINUE;
        lcl = tree->gtOp.gtOp1;
        assert(lcl->gtFlags & GTF_VAR_DEF);
        break;

    default:
        return WALK_CONTINUE;
    }

    assert(tree == lcl || (lcl->gtFlags & GTF_VAR_DEF));

    unsigned    lclNum  = lcl->gtLclVar.gtLclNum;
    LclVarDsc * varDsc  = &pComp->lvaTable[lclNum];

    if (varDsc->lvIsParam ||
        varDsc->lvType != TYP_INT ||
        varDsc->lvAddrTaken ||
        varDsc->lvKeepType)
    {
        return WALK_CONTINUE;
    }

    // Leave some TYP_INTs unconverted for variety
    if ((lclNum % 4) == 0)
        return WALK_CONTINUE;

    // Mark it

    varDsc->lvDblWasInt = true;

    if (tree == lcl)
    {
        tree->ChangeOper(GT_COMMA);
        tree->gtOp.gtOp1 = pComp->gtNewNothingNode();
        tree->gtOp.gtOp2 = pComp->gtNewCastNodeL(TYP_INT,
                                pComp->gtNewLclvNode(lclNum, TYP_DOUBLE),
                                TYP_INT);

        return WALK_SKIP_SUBTREES;
    }
    else
    {
        assert(oper == GT_ASG);
        assert(genActualType(tree->gtOp.gtOp2->gtType) == TYP_INT ||
               genActualType(tree->gtOp.gtOp2->gtType) == TYP_BYREF);
        tree->gtOp.gtOp2 = pComp->gtNewCastNode(TYP_DOUBLE,
                                                tree->gtOp.gtOp2,
                                                TYP_DOUBLE);
        tree->gtType    =
        lcl->gtType     = TYP_DOUBLE;

        return WALK_CONTINUE;
    }

}

/*****************************************************************************/

void                Compiler::lvaStressFloatLcls()
{
    if (opts.compDbgInfo) // Since we need to bash lvaTable[].lvType
        return;

    if (!compStressCompile(STRESS_ENREG_FP, 15))
        return;
        

    // Change the types of all the TYP_INT local variable nodes

    fgWalkAllTreesPre(lvaStressFloatLclsCB, (void*)this);

    // Also, change lvaTable accordingly

    for (unsigned lcl = 0; lcl < lvaCount; lcl++)
    {
        LclVarDsc * varDsc = &lvaTable[lcl];

        if (varDsc->lvIsParam ||
            varDsc->lvType != TYP_INT ||
            varDsc->lvAddrTaken)
        {
            assert(!varDsc->lvDblWasInt);
            continue;
        }

        if (varDsc->lvDblWasInt)
            varDsc->lvType = TYP_DOUBLE;
    }
}

/*****************************************************************************/
#endif // DEBUG
/*****************************************************************************
 *
 *  A little routine that displays a local variable bitset.
 *  'set' is mask of variables that have to be displayed
 *  'allVars' is the complete set of interesting variables (blank space is
 *    inserted if it corresponding bit is not in 'set').
 */

#ifdef  DEBUG

void                Compiler::lvaDispVarSet(VARSET_TP set, VARSET_TP allVars)
{
    printf("{");

    for (unsigned index = 0; index < VARSET_SZ; index++)
    {
        if  (set & genVarIndexToBit(index))
        {
            unsigned        lclNum;
            LclVarDsc   *   varDsc;

            /* Look for the matching variable */

            for (lclNum = 0, varDsc = lvaTable;
                 lclNum < lvaCount;
                 lclNum++  , varDsc++)
            {
                if  ((varDsc->lvVarIndex == index) && varDsc->lvTracked)
                    break;
            }

            printf("V%02u ", lclNum);
        }
        else if (allVars & genVarIndexToBit(index))
        {
            printf("    ");
        }
    }
    printf("}");
}

#endif
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\jit.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*****************************************************************************/
#ifndef _JIT_H_
#define _JIT_H_
/*****************************************************************************/

    // These don't seem useful, so turning them off is no big deal
#pragma warning(disable:4201)   // nameless struct/union
#pragma warning(disable:4510)   // can't generate default constructor
#pragma warning(disable:4511)   // can't generate copy constructor
#pragma warning(disable:4512)   // can't generate assignment constructor
#pragma warning(disable:4610)   // user defined constructor required 
#pragma warning(disable:4211)   // nonstandard extention used (char name[0] in structs)
#pragma warning(disable:4127)	// conditional expression constant

    // Depending on the code base, you may want to not disable these
#pragma warning(disable:4245)   // assigning signed / unsigned
#pragma warning(disable:4146)   // unary minus applied to unsigned
#pragma warning(disable:4244)   // loss of data int -> char ..

#ifndef DEBUG
#pragma warning(disable:4189)   // local variable initialized but not used
#endif

    // @TODO [CONSIDER] [04/16/01] []: put these back in
#pragma warning(disable:4063)   // bad switch value for enum (only in Disasm.cpp)
#pragma warning(disable:4100)	// unreferenced formal parameter
#pragma warning(disable:4291)	// new operator without delete (only in emitX86.cpp)

    // @TODO [CONSIDER] [04/16/01] []: we really probably need this one put back in!!!
#pragma warning(disable:4701)   // local variable may be used without being initialized 


#include "corhdr.h"
#define __OPERATOR_NEW_INLINE 1         // indicate that I will define these

#include "utilcode.h"

#ifdef DEBUG
#define INDEBUG(x)  x
#else 
#define INDEBUG(x)
#endif 

#define TGT_RISC_CNT (TGT_SH3+TGT_ARM+TGT_PPC+TGT_MIPS16+TGT_MIPS32)

#if     TGT_RISC_CNT != 0
#if     TGT_RISC_CNT != 1 || defined(TGT_x86)
#error  Exactly one target CPU must be specified.
#endif
#define TGT_RISC    1
#else
#define TGT_RISC    0
#undef  TGT_x86
#define TGT_x86     1
#endif

#ifndef TRACK_GC_REFS
#if     TGT_RISC
#define TRACK_GC_REFS   0           // GC ref tracking is NYI on RISC
#else
#define TRACK_GC_REFS   1
#endif
#endif

#ifdef TRACK_GC_REFS
#define REGEN_SHORTCUTS 0
#define REGEN_CALLPAT   0
#endif

#define NEW_EMIT_ATTR   TRACK_GC_REFS

#define THIS_CLASS_CP_IDX   0   // a special CP index code for current class

/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                          jit.h                                            XX
XX                                                                           XX
XX   Interface of the JIT with jit.cpp or                                    XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

/*****************************************************************************/
#if defined(DEBUG)
#include "log.h"

#define INFO6       LL_INFO10000            // Did Jit or Inline succeeded?
#define INFO7       LL_INFO100000           // NYI stuff
#define INFO8       LL_INFO1000000          // Weird failures
#define INFO9       LL_EVERYTHING           // Info about incoming settings
#define INFO10      LL_EVERYTHING           // Totally verbose

#define JITLOG(x) logf x
#else
#define JITLOG(x)
#endif

#define INJITDLL   // Defined if we export the functions in corjit.h/vm2jit.h



#include "corjit.h"

typedef class ICorJitInfo*    COMP_HANDLE;

#ifdef DEBUG
        // The value is better for debugging, because the stack is inited to this
const CORINFO_CLASS_HANDLE  BAD_CLASS_HANDLE    = (CORINFO_CLASS_HANDLE) 0xCCCCCCCC;
#else 
const CORINFO_CLASS_HANDLE  BAD_CLASS_HANDLE    = (CORINFO_CLASS_HANDLE) -1;
#endif

#include "Utils.h"

/*****************************************************************************/

typedef unsigned    IL_OFFSET;
const IL_OFFSET     BAD_IL_OFFSET   = UINT_MAX;

typedef unsigned    IL_OFFSETX; // IL_OFFSET with stack-empty bit
const IL_OFFSETX    IL_OFFSETX_STKBIT = 0x80000000;
IL_OFFSET           jitGetILoffs   (IL_OFFSETX offsx);
bool                jitIsStackEmpty(IL_OFFSETX offx);

const unsigned      BAD_LINE_NUMBER = UINT_MAX;
const unsigned      BAD_VAR_NUM     = UINT_MAX;

typedef size_t      NATIVE_IP;
typedef ptrdiff_t   NATIVE_OFFSET;

// For the following specially handled FIELD_HANDLES we need
//   values that are both even and in (0 > val > -8)
// See eeFindJitDataOffs and eeGetHitDataOffs in Compiler.hpp
//   for the gory details
#define FLD_GLOBAL_DS   ((CORINFO_FIELD_HANDLE) -2 )
#define FLD_GLOBAL_FS   ((CORINFO_FIELD_HANDLE) -4 )

/*****************************************************************************/

#include "host.h"
#include "vartype.h"

/*****************************************************************************/

// Debugging support is ON by default. Can be turned OFF by
// adding /DDEBUGGING_SUPPORT=0 on the command line.

#ifndef   DEBUGGING_SUPPORT
# define  DEBUGGING_SUPPORT
#elif    !DEBUGGING_SUPPORT
# undef   DEBUGGING_SUPPORT
#endif

/*****************************************************************************/

// Late disassembly is OFF by default. Can be turned ON by
// adding /DLATE_DISASM=1 on the command line.
// Always OFF in the non-debug version

#ifdef  DEBUG
    #if defined(LATE_DISASM) && (LATE_DISASM == 0)
    #undef  LATE_DISASM
    #endif
#else // DEBUG
    #undef  LATE_DISASM
#endif

/*****************************************************************************/


/*****************************************************************************/

#define RNGCHK_OPT          1       // enable global range check optimizer

#ifndef SCHEDULER
#if     TGT_x86
#define SCHEDULER           1       // scheduler defaults to on  for x86
#else
#define SCHEDULER           0       // scheduler defaults to off for RISC
#endif
#endif

#define CSE                 1       // enable CSE logic
#define CSELENGTH           1       // expose length use in range check for CSE
#define MORECSES            1       // CSE other expressions besides indirs
#define CODE_MOTION         1       // enable loop code motion etc.

#define SPECIAL_DOUBLE_ASG  0       // special handling for double assignments

#define CAN_DISABLE_DFA     1       // completely disable data flow (doesn't work!)
#define ALLOW_MIN_OPT       1       // allow "dumb" compilation mode

#define OPTIMIZE_RECURSION  1       // convert recursive methods into iterations
#define OPTIMIZE_INC_RNG    0       // combine multiple increments of index variables

#define LARGE_EXPSET        1       // Track 64 or 32 assertions/copies/consts/rangechecks
#define ASSERTION_PROP      1       // Enable value/assertion propagation

#define LOCAL_ASSERTION_PROP  ASSERTION_PROP  // Enable local assertion propagation

//=============================================================================

#define FANCY_ARRAY_OPT     0       // optimize more complex index checks

//=============================================================================

#define LONG_ASG_OPS        0       // implementation isn't complete yet

//=============================================================================

#define OPT_MULT_ADDSUB     1       // optimize consecutive "lclVar += or -= icon"
#define OPT_BOOL_OPS        1       // optimize boolean operations

#define OPTIMIZE_TAIL_REC   0       // UNDONE: no tail recursion for __fastcall

//=============================================================================

#define REDUNDANT_LOAD      1       // track locals in regs, suppress loads
#define MORE_REDUNDANT_LOAD 0       // track statics and aliased locals, suppress loads
#define INLINING            1       // inline calls to small methods
#define HOIST_THIS_FLDS     1       // hoist "this.fld" out of loops etc.
#define INLINE_NDIRECT      TGT_x86 // try to inline N/Direct stubs
#define PROFILER_SUPPORT    TGT_x86
#define GEN_SHAREABLE_CODE  0       // access static data members via helper
#define USE_GT_LOG          0       // Is it worth it now that we have GT_QMARKs?
#define USE_SET_FOR_LOGOPS  1       // enable this only for P6's
#define ROUND_FLOAT         TGT_x86 // round intermed float expression results
#define LONG_MATH_REGPARAM  0       // args to long mul/div passed in registers
#define FPU_DEFEREDDEATH    0       // if 1 we will be able to defer any fpu enregistered variables deaths

/*****************************************************************************/

#define VPTR_OFFS           0       // offset of vtable pointer from obj ptr

#define ARR_DIMCNT_OFFS(type) (varTypeIsGC(type) ? offsetof(CORINFO_RefArray, refElems) \
                                                 : offsetof(CORINFO_Array, u1Elems))

/*****************************************************************************/

#define INDIRECT_CALLS      1

/*****************************************************************************/

#if     COUNT_CYCLES
#endif

#define VERBOSE_SIZES       0
#define COUNT_BASIC_BLOCKS  0
#define INLINER_STATS       0
#define DUMP_INFOHDR        DEBUG
#define DUMP_GC_TABLES      DEBUG
#define GEN_COUNT_CODE      0       // enable *only* for debugging of crashes and such
#define GEN_COUNT_CALLS     0
#define GEN_COUNT_CALL_TYPES 0
#define GEN_COUNT_PTRASG    0
#define MEASURE_NODE_SIZE   0
#define MEASURE_NODE_HIST   0
#define MEASURE_BLOCK_SIZE  0
#define MEASURE_MEM_ALLOC   0
#define VERIFY_GC_TABLES    0
#define REARRANGE_ADDS      1
#define COUNT_OPCODES       0

/*****************************************************************************/
/*****************************************************************************/

#define DISPLAY_SIZES       0
#define COUNT_RANGECHECKS   0
#define INTERFACE_STATS     0

/*****************************************************************************/
#ifdef  DEBUG
/*****************************************************************************/

#define DUMPER

#else // !DEBUG

#if     DUMP_GC_TABLES
#pragma message("NOTE: this non-debug build has GC ptr table dumping always enabled!")
const   bool        dspGCtbls = true;
#endif

/*****************************************************************************/
#endif // !DEBUG
/*****************************************************************************
 *
 * Double alignment. This aligns ESP to 0 mod 8 in function prolog, then uses ESP
 * to reference locals, EBP to reference parameters.
 * It only makes sense if frameless method support is on.
 * (frameless method support is now always on)
 */


#if     TGT_x86
#define DOUBLE_ALIGN        1       // align ESP in prolog, align double local offsets
#endif

/*****************************************************************************/
#ifdef  DEBUG
extern  void _cdecl debugStop(const char *why, ...);
#endif
/*****************************************************************************/

extern  unsigned    warnLvl;

extern  const char* methodName;
extern  const char* className;
extern  unsigned    testMask;

#ifdef DEBUG
extern  const char* srcPath;
extern  bool        dumpTrees;
extern  bool        verbose;
extern  bool        verboseTrees;
#endif

extern  bool        genOrder;
extern  bool        genClinit;
extern  unsigned    genMinSz;
extern  bool        genAllSz;
extern  bool        native;
extern  bool        maxOpts;
extern  bool        genFPopt;
extern  bool        goSpeed;
extern  bool        savCode;
extern  bool        runCode;
extern  unsigned    repCode;
extern  bool        vmSdk3_0;
extern  bool        disAsm;
extern  bool        disAsm2;
extern  bool        riscCode;
#ifdef  DEBUG
extern  bool        dspInstrs;
extern  bool        dspEmit;
extern  bool        dspCode;
extern  bool        dspLines;
extern  bool        dmpHex;
extern  bool        varNames;
extern  bool        asmFile;
extern  double      CGknob;
#endif
#if     DUMP_INFOHDR
extern  bool        dspInfoHdr;
#endif
#if     DUMP_GC_TABLES
extern  bool        dspGCtbls;
extern  bool        dspGCoffs;
#endif
extern  bool        genGcChk;
#ifdef  DEBUGGING_SUPPORT
extern  bool        debugInfo;
extern  bool        debuggableCode;
extern  bool        debugEnC;
#endif

#ifdef  DUMPER
extern  bool        dmpClass;
extern  bool        dmp4diff;
extern  bool        dmpPCofs;
extern  bool        dmpCodes;
extern  bool        dmpSort;
#endif // DUMPER

extern  bool        nothing;

/*****************************************************************************/

enum accessLevel
{
    ACL_NONE,
    ACL_PRIVATE,
    ACL_DEFAULT,
    ACL_PROTECTED,
    ACL_PUBLIC,
};

/*****************************************************************************/

#define castto(var,typ) (*(typ *)&var)

#define sizeto(typ,mem) (offsetof(typ, mem) + sizeof(((typ*)0)->mem))

/*****************************************************************************/

#ifdef  NO_MISALIGNED_ACCESS

#define MISALIGNED_RD_I2(src)                   \
    (*castto(src  , char  *) |                  \
     *castto(src+1, char  *) << 8)

#define MISALIGNED_RD_U2(src)                   \
    (*castto(src  , char  *) |                  \
     *castto(src+1, char  *) << 8)

#define MISALIGNED_WR_I2(dst, val)              \
    *castto(dst  , char  *) = val;              \
    *castto(dst+1, char  *) = val >> 8;

#define MISALIGNED_WR_I4(dst, val)              \
    *castto(dst  , char  *) = val;              \
    *castto(dst+1, char  *) = val >> 8;         \
    *castto(dst+2, char  *) = val >> 16;        \
    *castto(dst+3, char  *) = val >> 24;

#else

#define MISALIGNED_RD_I2(src)                   \
    (*castto(src  ,          short *))
#define MISALIGNED_RD_U2(src)                   \
    (*castto(src  , unsigned short *))

#define MISALIGNED_WR_I2(dst, val)              \
    *castto(dst  ,           short *) = val;
#define MISALIGNED_WR_I4(dst, val)              \
    *castto(dst  ,           long  *) = val;

#endif

/*****************************************************************************/

#if     COUNT_CYCLES

extern  void            cycleCounterInit  ();
extern  void            cycleCounterBeg   ();
extern  void            cycleCounterPause ();
extern  void            cycleCounterResume();
extern  void            cycleCounterEnd   ();

#else

inline  void            cycleCounterInit  (){}
inline  void            cycleCounterBeg   (){}
inline  void            cycleCounterPause (){}
inline  void            cycleCounterResume(){}
inline  void            cycleCounterEnd   (){}

#endif

/*****************************************************************************/

inline
size_t              roundUp(size_t size, size_t mult = sizeof(int))
{
    assert(mult && ((mult & (mult-1)) == 0));   // power of two test

    return  (size + (mult - 1)) & ~(mult - 1);
}

inline
size_t              roundDn(size_t size, size_t mult = sizeof(int))
{
    assert(mult && ((mult & (mult-1)) == 0));   // power of two test

    return  (size             ) & ~(mult - 1);
}

/*****************************************************************************/

#if defined(DEBUG)

struct  histo
{
                    histo(unsigned * sizeTab, unsigned sizeCnt = 0);
                   ~histo();

    void            histoClr();
    void            histoDsp();
    void            histoRec(unsigned siz, unsigned cnt);

private:

    unsigned        histoSizCnt;
    unsigned    *   histoSizTab;

    unsigned    *   histoCounts;
};

#endif

/*****************************************************************************/
#if    !_WIN32_WCE
/*****************************************************************************/
#ifdef  ICECAP
#include "icapexp.h"
#include "icapctrl.h"
#endif
/*****************************************************************************/
#endif//!_WIN32_WCE
/*****************************************************************************/

#if defined(LATE_DISASM) && defined(JIT_AS_COMPILER)
#error "LATE_DISASM and JIT_AS_COMPILER should not be defined together"
#endif

/*****************************************************************************/

#ifndef FASTCALL
#define FASTCALL    __fastcall
#endif

/*****************************************************************************/

extern  unsigned    genCPU;

/*****************************************************************************/

#define SECURITY_CHECK          1
#define VERIFY_IMPORTER         1

/*****************************************************************************/

#if !defined(RELOC_SUPPORT)
#define RELOC_SUPPORT          1
#endif

/*****************************************************************************/

#include "error.h"
#include "alloc.h"
#include "target.h"

/*****************************************************************************/

#ifndef INLINE_MATH
#if     CPU_HAS_FP_SUPPORT
#define INLINE_MATH         1       //  enable inline math intrinsics
#else
#define INLINE_MATH         0       // disable inline math intrinsics
#endif
#endif

/*****************************************************************************/

#define CLFLG_CODESIZE        0x00001
#define CLFLG_CODESPEED       0x00002
#define CLFLG_CSE             0x00004
#define CLFLG_REGVAR          0x00008
#define CLFLG_RNGCHKOPT       0x00010
#define CLFLG_DEADASGN        0x00020
#define CLFLG_CODEMOTION      0x00040
#define CLFLG_QMARK           0x00080
#define CLFLG_TREETRANS       0x00100


#define CLFLG_MAXOPT         (CLFLG_CSE        | \
                              CLFLG_REGVAR     | \
                              CLFLG_RNGCHKOPT  | \
                              CLFLG_DEADASGN   | \
                              CLFLG_CODEMOTION | \
                              CLFLG_QMARK      | \
                              CLFLG_TREETRANS   )

#define CLFLG_MINOPT         (CLFLG_REGVAR     | \
                              CLFLG_TREETRANS   )


/*****************************************************************************/

extern  unsigned                dumpSingleInstr(const BYTE * codeAddr,
                                                IL_OFFSET    offs,
                                                const char * prefix = NULL );
/*****************************************************************************/




extern  int         FASTCALL    jitNativeCode(CORINFO_METHOD_HANDLE methodHnd,
                                              CORINFO_MODULE_HANDLE classHnd,
                                              COMP_HANDLE           compHnd,
                                              CORINFO_METHOD_INFO * methodInfo,
                                              void *          * methodCodePtr,
                                              SIZE_T          * methodCodeSize,
                                              void *          * methodConsPtr,
                                              void *          * methodDataPtr,
                                              void *          * methodInfoPtr,
                                              unsigned          compileFlags);




/*****************************************************************************/
#endif //_JIT_H_
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\instrsh3.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*****************************************************************************
 *
 *  Microsoft confidential
 *
 *  SH-3 opcodes for [Opt]JIT
 *
 *  How to use this header file:
 *
 *       define INST1(id, nm, bd, um, rf, wf, rm, wm, i1) \
 *          id      -- the enum name for the instruction
 *          nm      -- textual name (for assembly dipslay)
 *          bd      -- branch-delayed execution [bit0=BD,bit1=conditional]
 *          um      -- update mode, see IUM_xx enum
 *          rf      -- read flags (1 => T, 2 => S)
 *          wf      -- write flags (1 => T, 2 => S)
 *          rx      -- read  extra register(s)
 *          wx      -- write extra register(s)
 *          br      -- branch/call/return instruction?
 *          i1      -- instruction encoding
 *
 *      _M_N    -- M is source, N is destination
 *
 *****************************************************************************/

#ifndef         SCHED_XDEP_DEF
#define         SCHED_XDEP_DEF

// Define extra dependency flag values for the table below

#define         SCHED_XDEP_ALL  0x0F    // assume we'll need <= 4 flags

#define         SCHED_XDEP_PR   0x01
#define         SCHED_XDEP_MAC  0x02

// Define short-cuts to make the table a bit more readable

#define XPR     SCHED_XDEP_PR
#define XMAC    SCHED_XDEP_MAC

#endif

//     enum     name      BD  updmode rf wf   rx    wx  br _I8_N   _I16_N  _I32_N
INST3(movi,    "movi",    0,  IUM_WR, 0, 0,    0,    0, 0, 0xE000, 0x9000, 0xD000)

//     enum     name      BD  updmode rf wf   rx    wx  br _M_N    _I_0    _I_G0
INST3(and,     "and",     0,  IUM_RW, 0, 0,    0,    0, 0, 0x2009, 0xC900, 0xCD00)
INST3(cmpeq,   "cmpeq",   0,  IUM_RD, 0, 1,    0,    0, 0, 0x3000, 0x8800, BAD_CODE)
INST3(or,      "or",      0,  IUM_RW, 0, 0,    0,    0, 0, 0x200B, 0xCB00, 0xCF00)
INST3(tst,     "tst",     0,  IUM_RD, 0, 1,    0,    0, 0, 0x2008, 0xC800, 0xCC00)
INST3(xor,     "xor",     0,  IUM_RW, 0, 0,    0,    0, 0, 0x200A, 0xCA00, 0xCE00)

//     enum     name      BD  updmode rf wf   rx    wx  br _M_N
INST1(addc,    "addc",    0,  IUM_RW, 1, 1,    0,    0, 0, 0x300E)
INST1(addv,    "addv",    0,  IUM_RW, 0, 1,    0,    0, 0, 0x300F)
INST1(cmpEQ,   "cmp/eq",  0,  IUM_RD, 0, 1,    0,    0, 0, 0x3000)
INST1(cmpGE,   "cmp/ge",  0,  IUM_RD, 0, 1,    0,    0, 0, 0x3003)
INST1(cmpGT,   "cmp/gt",  0,  IUM_RD, 0, 1,    0,    0, 0, 0x3007)
INST1(cmpHI,   "cmp/hi",  0,  IUM_RD, 0, 1,    0,    0, 0, 0x3006)
INST1(cmpHS,   "cmp/hs",  0,  IUM_RD, 0, 1,    0,    0, 0, 0x3002)
INST1(cmpSTR,  "cmp/str", 0,  IUM_RD, 0, 1,    0,    0, 0, 0x200C)
INST1(div0s,   "div0s",   0,  IUM_RD, 0, 1,    0,    0, 0, 0x2007)
INST1(div1,    "div1",    0,  IUM_RW, 0, 1,    0,    0, 0, 0x3004)
INST1(dmuls,   "dmuls",   0,  IUM_RD, 0, 0,    0, XMAC, 0, 0x300D)
INST1(dmulu,   "dmulu",   0,  IUM_RD, 0, 0,    0, XMAC, 0, 0x3005)
INST1(extsb,   "exts",    0,  IUM_RW, 0, 0,    0,    0, 0, 0x600E)
INST1(extsw,   "exts",    0,  IUM_RW, 0, 0,    0,    0, 0, 0x600F)
INST1(extub,   "extu",    0,  IUM_RW, 0, 0,    0,    0, 0, 0x600C)
INST1(extuw,   "extu",    0,  IUM_RW, 0, 0,    0,    0, 0, 0x600D)
INST1(mul,     "mul",     0,  IUM_RD, 0, 0,    0, XMAC, 0, 0x0007)
INST1(mulsw,   "mulsw",   0,  IUM_RD, 0, 0,    0, XMAC, 0, 0x200F)
INST1(muluw,   "muluw",   0,  IUM_RD, 0, 0,    0, XMAC, 0, 0x200E)
INST1(neg,     "neg",     0,  IUM_RW, 0, 0,    0,    0, 0, 0x600B)
INST1(negc,    "negc",    0,  IUM_RW, 1, 1,    0,    0, 0, 0x600A)
INST1(not,     "not",     0,  IUM_RW, 0, 0,    0,    0, 0, 0x6007)
INST1(shad,    "shad",    0,  IUM_RW, 0, 0,    0,    0, 0, 0x400C)
INST1(shld,    "shld",    0,  IUM_RW, 0, 0,    0,    0, 0, 0x400D)
INST1(sub,     "sub",     0,  IUM_RW, 0, 0,    0,    0, 0, 0x3008)
INST1(subc,    "subc",    0,  IUM_RW, 1, 1,    0,    0, 0, 0x300A)
INST1(subv,    "subv",    0,  IUM_RW, 0, 1,    0,    0, 0, 0x300B)
INST1(swapb,   "swapb",   0,  IUM_RW, 0, 0,    0,    0, 0, 0x6008)
INST1(swapw,   "swapw",   0,  IUM_RW, 0, 0,    0,    0, 0, 0x6009)
INST1(xtrct,   "xtrct",   0,  IUM_RW, 0, 0,    0,    0, 0, 0x200D)

//     enum     name      BD  updmode rf wf   rx    wx  br _I8_0
INST1(mova,    "mova",    0,  IUM_WR, 0, 0,    0,    0, 0, 0xC700)

//     enum     name      BD  updmode rf wf   rx    wx  br _N
INST1(cmpPL,   "cmp/pl",  0,  IUM_RD, 0, 1,    0,    0, 0, 0x4015)
INST1(cmpPZ,   "cmp/pz",  0,  IUM_RD, 0, 1,    0,    0, 0, 0x4011)
INST1(jmp,     "jmp",     0,  IUM_RD, 0, 0,    0,    0, 1, 0x402B)
INST1(jsr,     "jsr",     1,  IUM_RD, 0, 0,    0, XPR , 1, 0x400B)
INST1(braf,    "braf",    0,  IUM_RD, 0, 0,    0,    0, 1, 0x0023)
INST1(bsrf,    "bsrf",    0,  IUM_WR, 0, 0,    0, XPR , 1, 0x0003)
INST1(dt,      "dt",      0,  IUM_RW, 0, 1,    0,    0, 0, 0x4010)
INST1(movt,    "movt",    0,  IUM_WR, 1, 0,    0,    0, 0, 0x0029)
INST1(pref,    "pref",    0,  IUM_RD, 0, 0,    0,    0, 0, 0x0083)
INST1(rotcl,   "rotcl",   0,  IUM_RW, 1, 1,    0,    0, 0, 0x4024)
INST1(rotcr,   "rotcr",   0,  IUM_RW, 1, 1,    0,    0, 0, 0x4025)
INST1(rotl,    "rotl",    0,  IUM_RW, 0, 1,    0,    0, 0, 0x4004)
INST1(rotr,    "rotr",    0,  IUM_RW, 0, 1,    0,    0, 0, 0x4005)
INST1(shal,    "shal",    0,  IUM_RW, 0, 1,    0,    0, 0, 0x4020)
INST1(shar,    "shar",    0,  IUM_RW, 0, 1,    0,    0, 0, 0x4021)
INST1(shll,    "shll",    0,  IUM_RW, 0, 1,    0,    0, 0, 0x4000)
INST1(shll2,   "shll2",   0,  IUM_RW, 0, 0,    0,    0, 0, 0x4008)
INST1(shll8,   "shll8",   0,  IUM_RW, 0, 0,    0,    0, 0, 0x4018)
INST1(shll16,  "shll16",  0,  IUM_RW, 0, 0,    0,    0, 0, 0x4028)
INST1(shlr,    "shlr",    0,  IUM_RW, 0, 1,    0,    0, 0, 0x4001)
INST1(shlr2,   "shlr2",   0,  IUM_RW, 0, 0,    0,    0, 0, 0x4009)
INST1(shlr8,   "shlr8",   0,  IUM_RW, 0, 0,    0,    0, 0, 0x4019)
INST1(shlr16,  "shlr16",  0,  IUM_RW, 0, 0,    0,    0, 0, 0x4029)

//     enum     name      BD  updmode rf wf   rx    wx  br _
INST1(clrmac,  "clrmac",  0,  IUM_RD, 0, 0,    0, XMAC, 0, 0x0028)
INST1(clrs,    "clrs",    0,  IUM_RD, 0, 2,    0,    0, 0, 0x0048)
INST1(clrt,    "clrt",    0,  IUM_RD, 0, 1,    0,    0, 0, 0x0008)
INST1(div0u,   "div0u",   0,  IUM_RD, 0, 1,    0,    0, 0, 0x0019)
INST1(nop,     "nop",     0,  IUM_RD, 0, 0,    0,    0, 0, 0x0009)
INST1(rts,     "rts",     1,  IUM_RD, 0, 0, XPR ,    0, 1, 0x000B)
INST1(sets,    "sets",    0,  IUM_RD, 0, 2,    0,    0, 0, 0x0058)
INST1(sett,    "sett",    0,  IUM_RD, 0, 1,    0,    0, 0, 0x0018)

//     enum     name      BD  updmode rf wf   rx    wx  br _@M+_@N+
INST1(macw,    "macw",    0,  IUM_RD, 2, 0, XMAC, XMAC, 0, 0x400F)
INST1(mac,     "mac",     0,  IUM_RD, 2, 0, XMAC, XMAC, 0, 0x000F)

//     enum     name      BD  updmode rf wf   rx    wx  br _D8
INST1(bf,      "bf",      0,  IUM_RD, 1, 0,    0,    0, 1, 0x8B00)
INST1(bfs,     "bf/s",    3,  IUM_RD, 1, 0,    0,    0, 1, 0x8F00)
INST1(bt,      "bt",      0,  IUM_RD, 1, 0,    0,    0, 1, 0x8900)
INST1(bts,     "bt/s",    3,  IUM_RD, 1, 0,    0,    0, 1, 0x8D00)

//     enum     name      BD  updmode rf wf   rx    wx  br _D12
INST1(bra,     "bra",     1,  IUM_RD, 0, 0,    0,    0, 1, 0xA000)
INST1(bsr,     "bsr",     1,  IUM_RD, 0, 0,    0, XPR , 1, 0xB000)

//     enum     name      BD  updmode rf wf   rx    wx  brreg/imm

INST1(mov,     "mov",     0,  IUM_WR, 0, 0,    0,    0, 0, 0x6003)
INST1(mov_imm, "mov",     0,  IUM_WR, 0, 0,    0,    0, 0, 0xE000)
INST1(add,     "add",     0,  IUM_RW, 0, 0,    0,    0, 0, 0x300C)
INST1(add_imm, "add",     0,  IUM_RW, 0, 0,    0,    0, 0, 0x7000)
INST1(xor_imm, "xor",     0,  IUM_RW, 0, 0,    0,    0, 0, 0xCA00)

//     enum     name      BD  updmode rf wf   rx    wx  br addr

INST1(mov_ind, "mov",     0,  IUM_RW, 0, 0,    0,    0, 0, 0x2000)

//     enum     name      BD  updmode rf wf   rx    wx  br _ND4

INST1(mov_dsp, "mov",     0,  IUM_WR, 0, 0,    0,    0, 0, 0x1000)

//     enum     name      BD  updmode rf wf   rx    wx  brpcdisp

INST1(mov_PC,  "mov",     0,  IUM_WR, 0, 0,    0,    0, 0, 0x9000)

//     enum     name      BD  updmode rf wf   rx    wx  br _GD8_

INST1(mov_GBR, "mov",     0,  IUM_RW, 0, 0,    0,    0, 0, 0xC000)

//     enum     name      BD  updmode rf wf   rx    wx  br_@R0_RM

INST1(mov_ix0, "mov",     0,  IUM_RW, 0, 0,    0,    0, 0, 0x0004)
INST1(movl_ix0, "mov",     0,  IUM_RW, 0, 0,    0,    0, 0, 0x000C)

//     enum     name      BD  updmode rf wf   rx    wx  br _@M+_SR
INST1(ldcgbr,  "ldc",     0,  IUM_WR, 0, 0,    0, XMAC, 0, 0x4017)
INST1(ldsmach, "lds",     0,  IUM_WR, 0, 0,    0, XMAC, 0, 0x4006)
INST1(ldsmacl, "lds",     0,  IUM_WR, 0, 0,    0, XMAC, 0, 0x4016)
INST1(ldspr,   "lds",     0,  IUM_WR, 0, 0,    0, XPR , 0, 0x4026)

//     enum     name      BD  updmode rf wf   rx    wx  br _SR_@-N
INST1(stcgbr,  "stc",     0,  IUM_RD, 0, 0, XMAC,    0, 0, 0x4013)
INST1(stsmach, "sts",     0,  IUM_RD, 0, 0, XMAC,    0, 0, 0x4002)
INST1(stsmacl, "sts",     0,  IUM_RD, 0, 0, XMAC,    0, 0, 0x4012)
INST1(stspr,   "sts",     0,  IUM_RD, 0, 0, XPR ,    0, 0, 0x4022)

//     enum     name      BD  updmode rf wf   rx    wx  br _@M+_SR
INST1(ldcgbr_reg,  "ldc",     0,  IUM_RW, 0, 0,    0, XMAC, 0, 0x401E)
INST1(ldsmach_reg, "lds",     0,  IUM_RW, 0, 0,    0, XMAC, 0, 0x400A)
INST1(ldsmacl_reg, "lds",     0,  IUM_RW, 0, 0,    0, XMAC, 0, 0x401A)
INST1(ldspr_reg,   "lds",     0,  IUM_RW, 0, 0,    0, XPR , 0, 0x402A)

//     enum     name      BD  updmode rf wf   rx    wx  br _SR_@-N
INST1(stcgbr_reg,  "stc",     0,  IUM_RW, 0, 0, XMAC,    0, 0, 0x0012)
INST1(stsmach_reg, "sts",     0,  IUM_RW, 0, 0, XMAC,    0, 0, 0x000A)
INST1(stsmacl_reg, "sts",     0,  IUM_RW, 0, 0, XMAC,    0, 0, 0x001A)
INST1(stspr_reg,   "sts",     0,  IUM_RW, 0, 0, XPR ,    0, 0, 0x002A)

//     enum     name      BD  updmode rf wf   rx    wx  br _SR_@-N
INST1(lod_gbr,  "mov",     0,  IUM_RW, 0, 0, XMAC,    0, 0, 0xC400)
INST1(sto_gbr,  "mov",     0,  IUM_RW, 0, 0, XMAC,    0, 0, 0xC000)

//     enum     name      BD  updmode rf wf   rx    wx  br  N/A
INST1(ignore,   "ignore",  0,  IUM_RD, 0, 0,    0,    0, 0, 0x0000)
#if SCHEDULER
INST1(noSched,  "noSched", 0,  IUM_RD, 0, 0,    0,    0, 0, 0x0000)
#endif

// FP
INST1(fdiv,     "fdiv",    0,  IUM_RW, 0, 0,    0,    0, 0, 0xf003)
INST1(fldi0,    "fldi0",   0,  IUM_RW, 0, 0,    0,    0, 0, 0x0000)
INST1(fldi1,    "fldi1",   0,  IUM_RW, 0, 0,    0,    0, 0, 0xf09d)
INST1(flds,     "flds",    0,  IUM_RW, 0, 0,    0,    0, 0, 0xf01d)
INST1(float,    "float",   0,  IUM_RW, 0, 0,    0,    0, 0, 0xf02d)
INST1(fmac,     "fmac",    0,  IUM_RW, 0, 0,    0,    0, 0, 0xf00e)
INST1(fmul,     "fmul",    0,  IUM_RW, 0, 0,    0,    0, 0, 0xf002)
INST1(fneg,     "fneg",    0,  IUM_RW, 0, 0,    0,    0, 0, 0xf04d)
INST1(fschg,    "fschg",   0,  IUM_RW, 0, 0,    0,    0, 0, 0xf3fd)
INST1(fsqrt,    "fsqrt",   0,  IUM_RW, 0, 0,    0,    0, 0, 0xf06d)
INST1(fsts,     "fsts",    0,  IUM_RW, 0, 0,    0,    0, 0, 0xf00d)
INST1(fsub,     "fsub",    0,  IUM_RW, 0, 0,    0,    0, 0, 0xf001)
INST1(fcmpEQ,   "fcmp/eq", 0,  IUM_RW, 0, 0,    0,    0, 0, 0xf004)
INST1(fcmpGT,   "fcmp/gt", 0,  IUM_RW, 0, 0,    0,    0, 0, 0xf005)
INST1(ftrc,     "ftrc",    0,  IUM_RW, 0, 0,    0,    0, 0, 0xf03d)
INST1(fabs,     "fabs",    0,  IUM_RW, 0, 0,    0,    0, 0, 0xf09d)
INST1(fadd,     "fadd",    0,  IUM_RW, 0, 0,    0,    0, 0, 0xf000)
INST1(fmov,     "fmov",    0,  IUM_WR, 0, 0,    0,    0, 0, 0xf00c)
INST1(fmov_ind, "fmov",    0,  IUM_RW, 0, 0,    0,    0, 0, 0xf008)
INST1(ldsfpul,  "ldsfpul", 0,  IUM_RW, 0, 0,    0,    0, 0, 0x405a)
INST1(stsfpul,  "stsfpul", 0,  IUM_RW, 0, 0,    0,    0, 0, 0x005a)
INST1(ldsfpscr, "ldsfpscr",0,  IUM_RW, 0, 0,    0,    0, 0, 0x406a)
INST1(stsfpscr, "stsfpscr",0,  IUM_RW, 0, 0,    0,    0, 0, 0x006a)
INST1(fcnvds,   "fcnvds"  ,0,  IUM_RW, 0, 0,    0,    0, 0, 0xf0bd)
INST1(fcnvsd,   "fcnvsd"  ,0,  IUM_RW, 0, 0,    0,    0, 0, 0xf0ad)

/*****************************************************************************/

#undef  INST1
#undef  INST2
#undef  INST3

/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\opcode.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                             opcodes.h                                     XX  
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

/*****************************************************************************/
#ifndef _OPCODE_H_
#define _OPCODE_H_

#define OLD_OPCODE_FORMAT 0         // Please remove after 7/1/99

#include "openum.h"

extern const signed char    opcodeSizes     [];


#if COUNT_OPCODES || defined(DEBUG)
extern const char * const   opcodeNames     [];
#endif


#ifdef DUMPER
extern const BYTE           opcodeArgKinds  [];
#endif


/*****************************************************************************/
#endif // _OPCODE_H_
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\regpair.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*****************************************************************************/
#ifndef PAIRDEF
#error  Must define PAIRDEF macro before including this file
#endif
/*****************************************************************************/

#ifndef PAIRBEG
#define PAIRBEG(reg)
#endif

#ifndef PAIRSTK
#define PAIRSTK(r1,r2) PAIRDEF(r1,r2)
#endif

/*****************************************************************************/
#if     TGT_x86
/*****************************************************************************/
/*                  The following is for x86                                 */
/*****************************************************************************/

//      rlo rhi

PAIRBEG(EAX    )
PAIRDEF(EAX,ECX)
PAIRDEF(EAX,EDX)
PAIRDEF(EAX,EBX)
PAIRDEF(EAX,EBP)
PAIRDEF(EAX,ESI)
PAIRDEF(EAX,EDI)
PAIRSTK(EAX,STK)

PAIRBEG(ECX    )
PAIRDEF(ECX,EAX)
PAIRDEF(ECX,EDX)
PAIRDEF(ECX,EBX)
PAIRDEF(ECX,EBP)
PAIRDEF(ECX,ESI)
PAIRDEF(ECX,EDI)
PAIRSTK(ECX,STK)

PAIRBEG(EDX    )
PAIRDEF(EDX,EAX)
PAIRDEF(EDX,ECX)
PAIRDEF(EDX,EBX)
PAIRDEF(EDX,EBP)
PAIRDEF(EDX,ESI)
PAIRDEF(EDX,EDI)
PAIRSTK(EDX,STK)

PAIRBEG(EBX    )
PAIRDEF(EBX,EAX)
PAIRDEF(EBX,EDX)
PAIRDEF(EBX,ECX)
PAIRDEF(EBX,EBP)
PAIRDEF(EBX,ESI)
PAIRDEF(EBX,EDI)
PAIRSTK(EBX,STK)

PAIRBEG(EBP    )
PAIRDEF(EBP,EAX)
PAIRDEF(EBP,EDX)
PAIRDEF(EBP,ECX)
PAIRDEF(EBP,EBX)
PAIRDEF(EBP,ESI)
PAIRDEF(EBP,EDI)
PAIRSTK(EBP,STK)

PAIRBEG(ESI    )
PAIRDEF(ESI,EAX)
PAIRDEF(ESI,EDX)
PAIRDEF(ESI,ECX)
PAIRDEF(ESI,EBX)
PAIRDEF(ESI,EBP)
PAIRDEF(ESI,EDI)
PAIRSTK(ESI,STK)

PAIRBEG(EDI    )
PAIRDEF(EDI,EAX)
PAIRDEF(EDI,EDX)
PAIRDEF(EDI,ECX)
PAIRDEF(EDI,EBX)
PAIRDEF(EDI,EBP)
PAIRDEF(EDI,ESI)
PAIRSTK(EDI,STK)

PAIRBEG(STK    )
PAIRSTK(STK,EAX)
PAIRSTK(STK,EDX)
PAIRSTK(STK,ECX)
PAIRSTK(STK,EBX)
PAIRSTK(STK,EBP)
PAIRSTK(STK,ESI)
PAIRSTK(STK,EDI)

/*****************************************************************************/
#endif//TGT_x86
/*****************************************************************************/
#if     TGT_SH3
/*****************************************************************************/
/*                  The following is for SH3                                 */
/*****************************************************************************/

//      rlo rhi

PAIRBEG(r00    )
PAIRDEF(r00,r01)
PAIRDEF(r00,r02)
PAIRDEF(r00,r03)
PAIRDEF(r00,r04)
PAIRDEF(r00,r05)
PAIRDEF(r00,r06)
PAIRDEF(r00,r07)
PAIRDEF(r00,r08)
PAIRDEF(r00,r09)
PAIRDEF(r00,r10)
PAIRDEF(r00,r11)
PAIRDEF(r00,r12)
PAIRDEF(r00,r13)
PAIRDEF(r00,r14)
PAIRSTK(r00,STK)

PAIRBEG(r01    )
PAIRDEF(r01,r02)
PAIRDEF(r01,r03)
PAIRDEF(r01,r04)
PAIRDEF(r01,r05)
PAIRDEF(r01,r06)
PAIRDEF(r01,r07)
PAIRDEF(r01,r08)
PAIRDEF(r01,r09)
PAIRDEF(r01,r10)
PAIRDEF(r01,r11)
PAIRDEF(r01,r12)
PAIRDEF(r01,r13)
PAIRDEF(r01,r14)
PAIRSTK(r01,STK)

PAIRBEG(r02    )
PAIRDEF(r02,r00)
PAIRDEF(r02,r01)
PAIRDEF(r02,r03)
PAIRDEF(r02,r04)
PAIRDEF(r02,r05)
PAIRDEF(r02,r06)
PAIRDEF(r02,r07)
PAIRDEF(r02,r08)
PAIRDEF(r02,r09)
PAIRDEF(r02,r10)
PAIRDEF(r02,r11)
PAIRDEF(r02,r12)
PAIRDEF(r02,r13)
PAIRDEF(r02,r14)
PAIRSTK(r02,STK)

PAIRBEG(r03    )
PAIRDEF(r03,r00)
PAIRDEF(r03,r01)
PAIRDEF(r03,r02)
PAIRDEF(r03,r04)
PAIRDEF(r03,r05)
PAIRDEF(r03,r06)
PAIRDEF(r03,r07)
PAIRDEF(r03,r08)
PAIRDEF(r03,r09)
PAIRDEF(r03,r10)
PAIRDEF(r03,r11)
PAIRDEF(r03,r12)
PAIRDEF(r03,r13)
PAIRDEF(r03,r14)
PAIRSTK(r03,STK)

PAIRBEG(r04    )
PAIRDEF(r04,r00)
PAIRDEF(r04,r01)
PAIRDEF(r04,r02)
PAIRDEF(r04,r03)
PAIRDEF(r04,r05)
PAIRDEF(r04,r06)
PAIRDEF(r04,r07)
PAIRDEF(r04,r08)
PAIRDEF(r04,r09)
PAIRDEF(r04,r10)
PAIRDEF(r04,r11)
PAIRDEF(r04,r12)
PAIRDEF(r04,r13)
PAIRDEF(r04,r14)
PAIRSTK(r04,STK)

PAIRBEG(r05    )
PAIRDEF(r05,r00)
PAIRDEF(r05,r01)
PAIRDEF(r05,r02)
PAIRDEF(r05,r03)
PAIRDEF(r05,r04)
PAIRDEF(r05,r06)
PAIRDEF(r05,r07)
PAIRDEF(r05,r08)
PAIRDEF(r05,r09)
PAIRDEF(r05,r10)
PAIRDEF(r05,r11)
PAIRDEF(r05,r12)
PAIRDEF(r05,r13)
PAIRDEF(r05,r14)
PAIRSTK(r05,STK)

PAIRBEG(r06    )
PAIRDEF(r06,r00)
PAIRDEF(r06,r01)
PAIRDEF(r06,r02)
PAIRDEF(r06,r03)
PAIRDEF(r06,r04)
PAIRDEF(r06,r05)
PAIRDEF(r06,r07)
PAIRDEF(r06,r08)
PAIRDEF(r06,r09)
PAIRDEF(r06,r10)
PAIRDEF(r06,r11)
PAIRDEF(r06,r12)
PAIRDEF(r06,r13)
PAIRDEF(r06,r14)
PAIRSTK(r06,STK)

PAIRBEG(r07    )
PAIRDEF(r07,r00)
PAIRDEF(r07,r01)
PAIRDEF(r07,r02)
PAIRDEF(r07,r03)
PAIRDEF(r07,r04)
PAIRDEF(r07,r05)
PAIRDEF(r07,r06)
PAIRDEF(r07,r08)
PAIRDEF(r07,r09)
PAIRDEF(r07,r10)
PAIRDEF(r07,r11)
PAIRDEF(r07,r12)
PAIRDEF(r07,r13)
PAIRDEF(r07,r14)
PAIRSTK(r07,STK)

PAIRBEG(r08    )
PAIRDEF(r08,r00)
PAIRDEF(r08,r01)
PAIRDEF(r08,r02)
PAIRDEF(r08,r03)
PAIRDEF(r08,r04)
PAIRDEF(r08,r05)
PAIRDEF(r08,r06)
PAIRDEF(r08,r07)
PAIRDEF(r08,r09)
PAIRDEF(r08,r10)
PAIRDEF(r08,r11)
PAIRDEF(r08,r12)
PAIRDEF(r08,r13)
PAIRDEF(r08,r14)
PAIRSTK(r08,STK)

PAIRBEG(r09    )
PAIRDEF(r09,r00)
PAIRDEF(r09,r01)
PAIRDEF(r09,r02)
PAIRDEF(r09,r03)
PAIRDEF(r09,r04)
PAIRDEF(r09,r05)
PAIRDEF(r09,r06)
PAIRDEF(r09,r07)
PAIRDEF(r09,r08)
PAIRDEF(r09,r10)
PAIRDEF(r09,r11)
PAIRDEF(r09,r12)
PAIRDEF(r09,r13)
PAIRDEF(r09,r14)
PAIRSTK(r09,STK)

PAIRBEG(r10    )
PAIRDEF(r10,r00)
PAIRDEF(r10,r01)
PAIRDEF(r10,r02)
PAIRDEF(r10,r03)
PAIRDEF(r10,r04)
PAIRDEF(r10,r05)
PAIRDEF(r10,r06)
PAIRDEF(r10,r07)
PAIRDEF(r10,r08)
PAIRDEF(r10,r09)
PAIRDEF(r10,r11)
PAIRDEF(r10,r12)
PAIRDEF(r10,r13)
PAIRDEF(r10,r14)
PAIRSTK(r10,STK)

PAIRBEG(r11    )
PAIRDEF(r11,r00)
PAIRDEF(r11,r01)
PAIRDEF(r11,r02)
PAIRDEF(r11,r03)
PAIRDEF(r11,r04)
PAIRDEF(r11,r05)
PAIRDEF(r11,r06)
PAIRDEF(r11,r07)
PAIRDEF(r11,r08)
PAIRDEF(r11,r09)
PAIRDEF(r11,r10)
PAIRDEF(r11,r12)
PAIRDEF(r11,r13)
PAIRDEF(r11,r14)
PAIRSTK(r11,STK)

PAIRBEG(r12    )
PAIRDEF(r12,r00)
PAIRDEF(r12,r01)
PAIRDEF(r12,r02)
PAIRDEF(r12,r03)
PAIRDEF(r12,r04)
PAIRDEF(r12,r05)
PAIRDEF(r12,r06)
PAIRDEF(r12,r07)
PAIRDEF(r12,r08)
PAIRDEF(r12,r09)
PAIRDEF(r12,r10)
PAIRDEF(r12,r11)
PAIRDEF(r12,r13)
PAIRDEF(r12,r14)
PAIRSTK(r12,STK)

PAIRBEG(r13    )
PAIRDEF(r13,r00)
PAIRDEF(r13,r01)
PAIRDEF(r13,r02)
PAIRDEF(r13,r03)
PAIRDEF(r13,r04)
PAIRDEF(r13,r05)
PAIRDEF(r13,r06)
PAIRDEF(r13,r07)
PAIRDEF(r13,r08)
PAIRDEF(r13,r09)
PAIRDEF(r13,r10)
PAIRDEF(r13,r11)
PAIRDEF(r13,r12)
PAIRDEF(r13,r14)
PAIRSTK(r13,STK)

PAIRBEG(r14    )
PAIRDEF(r14,r00)
PAIRDEF(r14,r01)
PAIRDEF(r14,r02)
PAIRDEF(r14,r03)
PAIRDEF(r14,r04)
PAIRDEF(r14,r05)
PAIRDEF(r14,r06)
PAIRDEF(r14,r07)
PAIRDEF(r14,r08)
PAIRDEF(r14,r09)
PAIRDEF(r14,r10)
PAIRDEF(r14,r11)
PAIRDEF(r14,r12)
PAIRDEF(r14,r13)
PAIRSTK(r14,STK)

PAIRBEG(STK    )
PAIRSTK(STK,r00)
PAIRSTK(STK,r01)
PAIRSTK(STK,r02)
PAIRSTK(STK,r03)
PAIRSTK(STK,r04)
PAIRSTK(STK,r05)
PAIRSTK(STK,r06)
PAIRSTK(STK,r07)
PAIRSTK(STK,r08)
PAIRSTK(STK,r09)
PAIRSTK(STK,r10)
PAIRSTK(STK,r11)
PAIRSTK(STK,r12)
PAIRSTK(STK,r13)
PAIRSTK(STK,r14)

/*****************************************************************************/
#endif//TGT_SH3
/*****************************************************************************/

#undef PAIRSTK

/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\optimizer.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                              Optimizer                                    XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

#include "jitpch.h"
#pragma hdrstop

/*****************************************************************************/

/* static */
const size_t            Compiler::s_optCSEhashSize  = EXPSET_SZ*2;
/* static */
const size_t            Compiler::optRngChkHashSize = RNGSET_SZ*2;

#if COUNT_RANGECHECKS
/* static */
unsigned                Compiler::optRangeChkRmv = 0;
/* static */
unsigned                Compiler::optRangeChkAll = 0;
#endif

/*****************************************************************************/

void                Compiler::optInit()
{
    optArrayInits       = false;
    optLoopsMarked      = false;
    
    /* Initialize the # of tracked loops to 0 */
    optLoopCount        = 0;

#ifdef DEBUG            
    optCSEstart         = UINT_MAX;
#endif
}

/*****************************************************************************
 *
 */

void                Compiler::optSetBlockWeights()
{
#if DEBUG
    bool changed = false;
#endif

    BasicBlock  *   block;

    for (block = fgFirstBB; block->bbNext; block = block->bbNext)
    {
        /* Blocks that can't be reached via the first block are rarely executed */
        if (!fgReachable(fgFirstBB, block))
            block->bbSetRunRarely();

        if (block->bbWeight != 0)
        {
            // Calculate our bbWeight:
            //
            //  o BB_UNITY_WEIGHT if we dominate all BBJ_RETURN blocks
            //  o otherwise BB_UNITY_WEIGHT / 2
            //
            bool domsRets = true;     // Assume that we will dominate
            
            for (flowList *  rets  = fgReturnBlocks; 
                 rets != NULL; 
                 rets  = rets->flNext)
            {
                if (!fgDominate(block, rets->flBlock))
                {
                    domsRets = false;
                    break;
                }
            }
            
            if (!domsRets)
            {
#if DEBUG
                changed = true;
#endif
                block->bbWeight /= 2;
                assert(block->bbWeight);
            }
        }
    }

#if DEBUG
    if  (changed && verbose)
    {
        printf("\nAfter optSetBlockWeights:\n");
        fgDispBasicBlocks();
        printf("\n");
    }

    /* Check that the flowgraph data (bbNums, bbRefs, bbPreds) is up-to-date */
    fgDebugCheckBBlist();
#endif
}

/*****************************************************************************
 *
 *  Marks the blocks between 'begBlk' and 'endBlk' as part of a loop.
 */

void        Compiler::optMarkLoopBlocks(BasicBlock *begBlk,
                                        BasicBlock *endBlk,
                                        bool       excludeEndBlk)
{
    /* Calculate the 'loopWeight',
       this is the amount to increase each block in the loop
       Our heuristic is that loops are weighted six times more
       than straight line code.
       Thus we increase each block by 7 times the weight of
       the loop header block,
       if the loops are all properly formed gives us:
       (assuming that BB_LOOP_WEIGHT is 8)

          1 -- non loop basic block
          8 -- single loop nesting
         64 -- double loop nesting
        512 -- triple loop nesting

    */

    assert(begBlk->bbNum <= endBlk->bbNum);
    assert(begBlk->isLoopHead());
    assert(fgReachable(begBlk, endBlk));

#ifdef  DEBUG
    if (verbose)
        printf("\nMarking loop L%02u", begBlk->bbLoopNum);
#endif

    /* Build list of backedges for block begBlk */
    flowList *  backedgeList = NULL;

    for (flowList* pred  = begBlk->bbPreds; 
                   pred != NULL; 
                   pred  = pred->flNext)
    {
        /* Is this a backedge? */
        if (pred->flBlock->bbNum >= begBlk->bbNum)
        {
            flowList *  flow = (flowList *)compGetMem(sizeof(*flow));

            flow->flNext  = backedgeList;
            flow->flBlock = pred->flBlock;
            backedgeList   = flow;
        }
    }

    /* At least one backedge must have been found (the one from endBlk) */
    assert(backedgeList);
    
    BasicBlock * curBlk = begBlk;

    while (true)
    {
        assert(curBlk);

        /* If this block reaches any of the backedge blocks set reachable   */
        /* If this block dominates any of the backedge blocks set dominates */
        bool          reachable = false;
        bool          dominates = false;

        for (flowList* tmp  = backedgeList; 
                       tmp != NULL; 
                       tmp  = tmp->flNext)
        {
            BasicBlock *  backedge = tmp->flBlock;

            if (!curBlk->isRunRarely())
            {
                reachable |= fgReachable(curBlk, backedge);
                dominates |= fgDominate (curBlk, backedge);

                if (dominates && reachable)
                    break;
            }
        }

        if (reachable)
        {
            assert(curBlk->bbWeight > 0);

            unsigned weight = curBlk->bbWeight * BB_LOOP_WEIGHT;
                
            if (!dominates)
                weight /= 2;

            if (weight > BB_MAX_LOOP_WEIGHT)
                weight = BB_MAX_LOOP_WEIGHT;

            curBlk->bbWeight = weight;

#ifdef  DEBUG
            if (verbose)
                printf("%s BB%02u(wt=%u%s)", 
                       (curBlk == begBlk) ? ":" : ",",
                       curBlk->bbNum,
                       weight/2, (weight&1)?".5":"");
#endif
        }

        /* Stop if we've reached the last block in the loop */
        
        if  (curBlk == endBlk)
            break;
        
        curBlk = curBlk->bbNext;

        /* If we are excluding the endBlk then stop if we've reached endBlk */
        
        if  (excludeEndBlk && (curBlk == endBlk))
            break;
    }
}

/*****************************************************************************
 *
 *   Unmark the blocks between 'begBlk' and 'endBlk' as part of a loop.
 */

void        Compiler::optUnmarkLoopBlocks(BasicBlock *begBlk,
                                          BasicBlock *endBlk)
{
    /* A set of blocks that were previously marked as a loop are now
       to be unmarked, since we have decided that for some reason this
       loop no longer exists.
       Basically we are just reseting the blocks bbWeight to their
       previous values.
    */
    
    assert(begBlk->bbNum <= endBlk->bbNum);
    assert(begBlk->isLoopHead());

    BasicBlock *  curBlk;
    unsigned      backEdgeCount = 0;

    for (flowList * pred  = begBlk->bbPreds; 
                    pred != NULL;
                    pred  = pred->flNext)
    {
        curBlk = pred->flBlock;

        /* is this a backward edge? (from curBlk to begBlk) */

        if (begBlk->bbNum > curBlk->bbNum)
            continue;

        /* We only consider back-edges that are BBJ_COND or BBJ_ALWAYS for loops */

        if ((curBlk->bbJumpKind != BBJ_COND)   &&
            (curBlk->bbJumpKind != BBJ_ALWAYS)    )
          continue;

        backEdgeCount++;
    }

    /* Only unmark the loop blocks if we have exactly one loop back edge */
    if (backEdgeCount != 1)
    {
#ifdef  DEBUG
        if (verbose)
        {
            if (backEdgeCount > 0)
            {
                printf("\nNot removing loop L%02u, due to an additional back edge",
                       begBlk->bbLoopNum);
            }
            else if (backEdgeCount == 0)
            {
                printf("\nNot removing loop L%02u, due to no back edge",
                       begBlk->bbLoopNum);
            }
        }
#endif
        return;
    }
    assert(backEdgeCount == 1);
    assert(fgReachable(begBlk, endBlk));

#ifdef  DEBUG
    if (verbose)
        printf("\nRemoving loop L%02u", begBlk->bbLoopNum);
#endif

    curBlk = begBlk;

    while (true)
    {        
        assert(curBlk);

        if (!curBlk->isRunRarely() && 
            fgReachable(curBlk, endBlk))
        {
            unsigned weight = curBlk->bbWeight;

            /* Don't unmark blocks that are set to BB_MAX_LOOP_WEIGHT */
            if (weight != BB_MAX_LOOP_WEIGHT)
            {
                if (!fgDominate(curBlk, endBlk))
                {
                    weight *= 2;
                }
                else
                {
                    /* Merging of blocks can disturb the Dominates
                       information (see RAID #46649) */
                    if (weight < BB_LOOP_WEIGHT)
                        weight *= 2;
                }

                assert (weight >= BB_LOOP_WEIGHT);
                
                weight /= BB_LOOP_WEIGHT;
                
                curBlk->bbWeight = weight;
            }

#ifdef  DEBUG
            if (verbose)
                printf("%s BB%02u(wt=%d%s)", 
                       (curBlk == begBlk) ? ":" : ",",
                       curBlk->bbNum,
                       weight/2,
                       (weight&1) ? ".5" : "");
#endif
        }
        /* Stop if we've reached the last block in the loop */
        
        if  (curBlk == endBlk)
            break;

        curBlk = curBlk->bbNext;

        /* Stop if we go past the last block in the loop, as it may have been deleted */
        if (curBlk->bbNum > endBlk->bbNum)
            break;
    }
}

/*****************************************************************************************************
 *
 *  Function called to update the loop table and bbWeight before removing a block
 */

void                Compiler::optUpdateLoopsBeforeRemoveBlock(BasicBlock * block, 
                                                              BasicBlock * bPrev,
                                                              bool         skipUnmarkLoop)
{
    if (!optLoopsMarked)
        return;

    bool removeLoop = false;

    /* If an unreacheable block was part of a loop entry or bottom then the loop is unreacheable */
    /* Special case: the block was the head of a loop - or pointing to a loop entry */

    for (unsigned loopNum = 0; loopNum < optLoopCount; loopNum++)
    {
        /* Some loops may have been already removed by
         * loop unrolling or conditional folding */

        if (optLoopTable[loopNum].lpFlags & LPFLG_REMOVED)
            continue;

        if (block == optLoopTable[loopNum].lpEntry ||
            block == optLoopTable[loopNum].lpEnd    )
        {
            optLoopTable[loopNum].lpFlags |= LPFLG_REMOVED;
            continue;
        }

        /* If the loop is still in the table
         * any block in the loop must be reachable !!! */

        assert(optLoopTable[loopNum].lpEntry != block);
        assert(optLoopTable[loopNum].lpEnd   != block);
        assert(optLoopTable[loopNum].lpExit  != block);

        /* If this points to the actual entry in the loop
         * then the whole loop may become unreachable */

        switch (block->bbJumpKind)
        {
            unsigned        jumpCnt;
            BasicBlock * *  jumpTab;

        case BBJ_NONE:
        case BBJ_COND:
            if (block->bbNext == optLoopTable[loopNum].lpEntry)
            {
                removeLoop = true;
                break;
            }
            if (block->bbJumpKind == BBJ_NONE)
                break;

            // fall through
        case BBJ_ALWAYS:
            assert(block->bbJumpDest);
            if (block->bbJumpDest == optLoopTable[loopNum].lpEntry)
            {
              removeLoop = true;
            }
            break;

        case BBJ_SWITCH:
            jumpCnt = block->bbJumpSwt->bbsCount;
            jumpTab = block->bbJumpSwt->bbsDstTab;

            do {
                assert(*jumpTab);
                if ((*jumpTab) == optLoopTable[loopNum].lpEntry)
                {
                    removeLoop = true;
                }
            } while (++jumpTab, --jumpCnt);
            break;
        }

        if  (removeLoop)
        {
            /* Check if the entry has other predecessors outside the loop
             * UNDONE: Replace this when predecessors are available */

            BasicBlock      *       auxBlock;
            for (auxBlock = fgFirstBB; auxBlock; auxBlock = auxBlock->bbNext)
            {
                /* Ignore blocks in the loop */

                if  (auxBlock->bbNum >  optLoopTable[loopNum].lpHead->bbNum &&
                     auxBlock->bbNum <= optLoopTable[loopNum].lpEnd ->bbNum    )
                    continue;

                switch (auxBlock->bbJumpKind)
                {
                    unsigned        jumpCnt;
                    BasicBlock * *  jumpTab;

                case BBJ_NONE:
                case BBJ_COND:
                    if (auxBlock->bbNext == optLoopTable[loopNum].lpEntry)
                    {
                        removeLoop = false;
                        break;
                    }
                    if (auxBlock->bbJumpKind == BBJ_NONE)
                        break;

                    // fall through
                case BBJ_ALWAYS:
                    assert(auxBlock->bbJumpDest);
                    if (auxBlock->bbJumpDest == optLoopTable[loopNum].lpEntry)
                    {
                        removeLoop = false;
                    }
                    break;

                case BBJ_SWITCH:
                    jumpCnt = auxBlock->bbJumpSwt->bbsCount;
                    jumpTab = auxBlock->bbJumpSwt->bbsDstTab;

                    do {
                        assert(*jumpTab);
                          if ((*jumpTab) == optLoopTable[loopNum].lpEntry)
                          {
                              removeLoop = false;
                          }
                    } while (++jumpTab, --jumpCnt);
                    break;
                }
            }

            if (removeLoop)
            {
                optLoopTable[loopNum].lpFlags |= LPFLG_REMOVED;
            }
        }
        else if (optLoopTable[loopNum].lpHead == block)
        {
            /* The loop has a new head - Just update the loop table */
            optLoopTable[loopNum].lpHead = bPrev;
        }
    }

    if ((skipUnmarkLoop == false)                                              &&
        ((block->bbJumpKind == BBJ_ALWAYS) || (block->bbJumpKind == BBJ_COND)) &&
        (block->bbJumpDest->isLoopHead())                                      &&
        (block->bbJumpDest->bbNum <= block->bbNum)                             &&
        fgReachable(block->bbJumpDest, block))
    {
        optUnmarkLoopBlocks(block->bbJumpDest, block);
    }
}

/*****************************************************************************
 *
 *  Record the loop in the loop table.
 */

void                Compiler::optRecordLoop(BasicBlock *    head,
                                            BasicBlock *    bottom,
                                            BasicBlock *    entry,
                                            BasicBlock *    exit,
                                            unsigned char   exitCnt)
{
    /* record this loop in the table */

    if (optLoopCount < MAX_LOOP_NUM)
    {
        optLoopTable[optLoopCount].lpHead      = head;
        optLoopTable[optLoopCount].lpEnd       = bottom;
        optLoopTable[optLoopCount].lpEntry     = entry;
        optLoopTable[optLoopCount].lpExit      = exit;
        optLoopTable[optLoopCount].lpExitCnt   = exitCnt;

        optLoopTable[optLoopCount].lpFlags     = 0;

        /* if DO-WHILE loop mark it as such */

        if (head->bbNext == entry)
            optLoopTable[optLoopCount].lpFlags |= LPFLG_DO_WHILE;

        /* if single exit loop mark it as such */

        if (exitCnt == 1)
        {
            assert(exit);
            optLoopTable[optLoopCount].lpFlags |= LPFLG_ONE_EXIT;
        }

        /* @TODO [CONSIDER] [04/16/01] []: also mark infinite loops */


        /* Try to find loops that have an iterator (i.e. for-like loops) "for (init; test; incr){ ... }"
         * We have the following restrictions:
         *     1. The loop condition must be a simple one i.e. only one JTRUE node
         *     2. There must be a loop iterator (a local var) that is
         *        incremented (decremented, etc) with a constant value
         *     3. The iterator is incremented exactly once
         *     4. The loop condition must use the iterator */

        if  (bottom->bbJumpKind == BBJ_COND)
        {
            BasicBlock   *  block;

            GenTree *       test;               // holds the test node
            GenTree *       incr;               // holds the incrementor node
            GenTree *       phdr;
            GenTree *       init;               // holds the initialization node

            GenTree *       opr1;
            GenTree *       opr2;

            unsigned        iterVar;            // the local var # of the iterator
            long            iterConst;          // the constant with which we increment the iterator (i.e. i+=const)

            long            constInit;          // constant to which iterator is initialized
            unsigned short  varInit;            // local var # to which iterator is initialized

            long            constLimit;         // constant limit of the iterator
            unsigned short  varLimit;           // local var # limit of the iterator


            /* Find the last two statements in the loop body
             * Those have to be the "increment" of the iterator
             * and the loop condition */

            assert(bottom->bbTreeList);
            test = bottom->bbTreeList->gtPrev;
            assert(test && test->gtNext == 0);

            incr = test->gtPrev;
            if  (!incr)
                goto DONE_LOOP;

            /* Special case: incr and test may be in separate BB's
             * for "while" loops because we first jump to the condition */

            if  ((incr == test) && (head->bbNext != bottom))
            {
                block = head;

                do
                {
                    block = block->bbNext;
                }
                while  (block->bbNext != bottom);

                incr = block->bbTreeList;
                if  (!incr)
                    goto DONE_LOOP;

                incr = incr->gtPrev; assert(incr && (incr->gtNext == 0));
            }

            /* Find the last statement in the loop pre-header
             * which we expect to be the initialization of
             * the loop iterator */

            phdr = head->bbTreeList;
            if  (!phdr)
                goto DONE_LOOP;

            init = phdr->gtPrev; assert(init && (init->gtNext == 0));

            /* if it is a duplicated loop condition, skip it */

            if  (init->gtFlags & GTF_STMT_CMPADD)
            {
                /* Must be a duplicated loop condition */
                assert(init->gtStmt.gtStmtExpr->gtOper == GT_JTRUE);

                init = init->gtPrev; assert(init);
            }

            /* Get hold of the expression trees */

            assert(init->gtOper == GT_STMT); init = init->gtStmt.gtStmtExpr;
            assert(test->gtOper == GT_STMT); test = test->gtStmt.gtStmtExpr;
            assert(incr->gtOper == GT_STMT); incr = incr->gtStmt.gtStmtExpr;

//          printf("Constant loop candidate:\n\n");
//          printf("init:\n"); gtDispTree(init);
//          printf("incr:\n"); gtDispTree(incr);
//          printf("test:\n"); gtDispTree(test);

            /* The increment statement must be "lclVar <op>= const;" */

            switch (incr->gtOper)
            {
            case GT_ASG_ADD:
            case GT_ASG_SUB:
            case GT_ASG_MUL:
            case GT_ASG_DIV:
            case GT_ASG_RSH:
            case GT_ASG_LSH:
            case GT_ASG_UDIV:
                break;

            default:
                goto DONE_LOOP;
            }

            opr1 = incr->gtOp.gtOp1;
            opr2 = incr->gtOp.gtOp2;

            if  (opr1->gtOper != GT_LCL_VAR)
                goto DONE_LOOP;
            iterVar = opr1->gtLclVar.gtLclNum;

            if  (opr2->gtOper != GT_CNS_INT)
                goto DONE_LOOP;
            iterConst = opr2->gtIntCon.gtIconVal;

            /* Make sure "iterVar" is not assigned in the loop (besides where we increment it) */

            if  (optIsVarAssigned(head->bbNext, bottom, incr, iterVar))
                goto DONE_LOOP;

            /* Make sure the "iterVar" initialization is never skipped, i.e. HEAD dominates the ENTRY */

            if (!fgDominate(head, entry))
                goto DONE_LOOP;

            /* Make sure the block before the loop ends with "iterVar = icon"
             * or "iterVar = other_lvar" */

            if  (init->gtOper != GT_ASG)
                goto DONE_LOOP;

            opr1 = init->gtOp.gtOp1;
            opr2 = init->gtOp.gtOp2;

            if  (opr1->gtOper != GT_LCL_VAR)
                goto DONE_LOOP;
            if  (opr1->gtLclVar.gtLclNum != iterVar)
                goto DONE_LOOP;

            if  (opr2->gtOper == GT_CNS_INT)
            {
                constInit = opr2->gtIntCon.gtIconVal;
            }
            else if (opr2->gtOper == GT_LCL_VAR)
            {
                varInit = opr2->gtLclVar.gtLclNum;
            }
            else
                goto DONE_LOOP;

            /* check that the iterator is used in the loop condition */

            assert(test->gtOper == GT_JTRUE);
            assert(test->gtOp.gtOp1->OperKind() & GTK_RELOP);
            assert(bottom->bbTreeList->gtPrev->gtStmt.gtStmtExpr == test);

            opr1 = test->gtOp.gtOp1->gtOp.gtOp1;

            if  (opr1->gtOper != GT_LCL_VAR)
                goto DONE_LOOP;
            if  (opr1->gtLclVar.gtLclNum != iterVar)
                goto DONE_LOOP;

            /* We know the loop has an iterator at this point ->flag it as LPFLG_ITER
             * Record the iterator, the pointer to the test node
             * and the initial value of the iterator (constant or local var) */

            optLoopTable[optLoopCount].lpFlags    |= LPFLG_ITER;

            /* record iterator */

            optLoopTable[optLoopCount].lpIterTree  = incr;

            /* save the initial value of the iterator - can be lclVar or constant
             * Flag the loop accordingly */

            if (opr2->gtOper == GT_CNS_INT)
            {
                /* initializer is a constant */

                optLoopTable[optLoopCount].lpConstInit  = constInit;
                optLoopTable[optLoopCount].lpFlags     |= LPFLG_CONST_INIT;
            }
            else
            {
                /* initializer is a local variable */

                assert (opr2->gtOper == GT_LCL_VAR);
                optLoopTable[optLoopCount].lpVarInit    = varInit;
                optLoopTable[optLoopCount].lpFlags     |= LPFLG_VAR_INIT;
            }

#if COUNT_LOOPS
            iterLoopCount++;
#endif

            /* Now check if a simple condition loop (i.e. "iter REL_OP icon or lclVar"
             * @TODO [REVISIT] [04/16/01] []: Consider also instanceVar */

            //
            // UNSIGNED_ISSUE : Extend this to work with unsigned operators
            //

            assert(test->gtOper == GT_JTRUE);
            test = test->gtOp.gtOp1;
            assert(test->OperKind() & GTK_RELOP);

            opr1 = test->gtOp.gtOp1;
            opr2 = test->gtOp.gtOp2;

            if  (opr1->gtType != TYP_INT)
                goto DONE_LOOP;

            /* opr1 has to be the iterator */

            if  (opr1->gtOper != GT_LCL_VAR)
                goto DONE_LOOP;
            if  (opr1->gtLclVar.gtLclNum != iterVar)
                goto DONE_LOOP;

            /* opr2 has to be constant or lclVar */

            if  (opr2->gtOper == GT_CNS_INT)
            {
                constLimit = opr2->gtIntCon.gtIconVal;
            }
            else if (opr2->gtOper == GT_LCL_VAR)
            {
                varLimit  = opr2->gtLclVar.gtLclNum;
            }
            else
            {
                goto DONE_LOOP;
            }

            /* Record the fact that this is a SIMPLE_TEST iteration loop */

            optLoopTable[optLoopCount].lpFlags         |= LPFLG_SIMPLE_TEST;

            /* save the type of the comparisson between the iterator and the limit */

            optLoopTable[optLoopCount].lpTestTree       = test;

            /* save the limit of the iterator - flag the loop accordingly */

            if (opr2->gtOper == GT_CNS_INT)
            {
                /* iterator limit is a constant */

                optLoopTable[optLoopCount].lpFlags      |= LPFLG_CONST_LIMIT;
            }
            else
            {
                /* iterator limit is a local variable */

                assert (opr2->gtOper == GT_LCL_VAR);
                optLoopTable[optLoopCount].lpFlags      |= LPFLG_VAR_LIMIT;
            }

#if COUNT_LOOPS
            simpleTestLoopCount++;
#endif

            /* check if a constant iteration loop */

            if ((optLoopTable[optLoopCount].lpFlags & LPFLG_CONST_INIT) &&
                (optLoopTable[optLoopCount].lpFlags & LPFLG_CONST_LIMIT)  )
            {
                /* this is a constant loop */

                optLoopTable[optLoopCount].lpFlags      |= LPFLG_CONST;
#if COUNT_LOOPS
                constIterLoopCount++;
#endif
            }

#ifdef  DEBUG
            if (verbose&&0)
            {
                printf("\nConstant loop initializer:\n");
                gtDispTree(init);

                printf("\nConstant loop body:\n");

                block = head;
                do
                {
                    GenTree *       stmt;
                    GenTree *       expr;

                    block = block->bbNext;
                    stmt  = block->bbTreeList;

                    while (stmt)
                    {
                        assert(stmt);

                        expr = stmt->gtStmt.gtStmtExpr;
                        if  (expr == incr)
                            break;

                        printf("\n");
                        gtDispTree(expr);

                        stmt = stmt->gtNext;
                    }
                }
                while (block != bottom);
            }
#endif

        }


    DONE_LOOP:

#ifdef  DEBUG

        if (verbose)
        {
            printf("Recorded loop L%02u, from BB%02u to BB%02u", optLoopCount,
                                                             head->bbNext->bbNum,
                                                             bottom      ->bbNum);

            /* if an iterator loop print the iterator and the initialization */

            if  (optLoopTable[optLoopCount].lpFlags & LPFLG_ITER)
            {
                printf(" [over V%02u", optLoopTable[optLoopCount].lpIterVar());

                switch (optLoopTable[optLoopCount].lpIterOper())
                {
                    case GT_ASG_ADD:
                        printf(" ( += ");
                        break;

                    case GT_ASG_SUB:
                        printf(" ( -= ");
                        break;

                    case GT_ASG_MUL:
                        printf(" ( *= ");
                        break;

                    case GT_ASG_DIV:
                        printf(" ( /= ");
                        break;

                    case GT_ASG_UDIV:
                        printf(" ( /= ");
                        break;

                    case GT_ASG_RSH:
                        printf(" ( >>= ");
                        break;

                    case GT_ASG_LSH:
                        printf(" ( <<= ");
                        break;

                    default:
                        assert(!"Unknown operator for loop iterator");
                }

                printf("%d )", optLoopTable[optLoopCount].lpIterConst());

                if  (optLoopTable[optLoopCount].lpFlags & LPFLG_CONST_INIT)
                    printf(" from %d", optLoopTable[optLoopCount].lpConstInit);

                if  (optLoopTable[optLoopCount].lpFlags & LPFLG_VAR_INIT)
                    printf(" from V%02u", optLoopTable[optLoopCount].lpVarInit);

                /* if a simple test condition print operator and the limits */

                if  (optLoopTable[optLoopCount].lpFlags & LPFLG_SIMPLE_TEST)
                {
                    switch (optLoopTable[optLoopCount].lpTestOper())
                    {
                        case GT_EQ:
                            printf(" == ");
                            break;

                        case GT_NE:
                            printf(" != ");
                            break;

                        case GT_LT:
                            printf(" < ");
                            break;

                        case GT_LE:
                            printf(" <= ");
                            break;

                        case GT_GT:
                            printf(" > ");
                            break;

                        case GT_GE:
                            printf(" >= ");
                            break;

                        default:
                            assert(!"Unknown operator for loop condition");
                    }

                    if  (optLoopTable[optLoopCount].lpFlags & LPFLG_CONST_LIMIT)
                        printf("%d ", optLoopTable[optLoopCount].lpConstLimit());

                    if  (optLoopTable[optLoopCount].lpFlags & LPFLG_VAR_LIMIT)
                        printf("V%02u ", optLoopTable[optLoopCount].lpVarLimit());
                }

                printf("]");
            }

            printf("\n");
        }
#endif

        optLoopCount++;
    }
}


#ifdef DEBUG

void                Compiler::optCheckPreds()
{
    BasicBlock   *  block;
    BasicBlock   *  blockPred;
    flowList     *  pred;

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        for (pred = block->bbPreds; pred; pred = pred->flNext)
        {
            // make sure this pred is part of the BB list
            for (blockPred = fgFirstBB; blockPred; blockPred = blockPred->bbNext)
            {
                if (blockPred == pred->flBlock)
                    break;
            }
            assert(blockPred);
            switch (blockPred->bbJumpKind)
            {
            case BBJ_COND:
                if (blockPred->bbJumpDest == block)
                    break;
                // otherwise fall through
            case BBJ_NONE:
                assert(blockPred->bbNext == block);
                break;
            case BBJ_RET:
                if(!(blockPred->bbFlags & BBF_ENDFILTER))
                    break;
                // otherwise fall through
            case BBJ_ALWAYS:
                assert(blockPred->bbJumpDest == block);
                break;
            default:
                break;
            }
        }
    }
}

#endif

/*****************************************************************************
 * Find the natural loops, using dominators. Note that the test for
 * a loop is slightly different from the standard one, because we have
 * not done a depth first reordering of the basic blocks.
 */

void                Compiler::optFindNaturalLoops()
{
    flowList    *   pred;
    flowList    *   predTop;
    flowList    *   predEntry;

    assert(fgDomsComputed);

#if COUNT_LOOPS
    hasMethodLoops  = false;
    loopsThisMethod = 0;
#endif

    /* We will use the following terminology:
     * HEAD    - the that flows int entry the loop (Currently MUST be lexically before entry)
                    @TODO [REVISIT] [04/16/01] []: remove the need for the head to be lexically before the entry.  
     * TOP     - the lexically first basic block in the loop (i.e. the head of the backward edge)
     * BOTTOM  - the lexically last block in the loop (i.e. the block from which we jump to the top)
     * EXIT    - the loop exit or the block right after the bottom
     * ENTRY   - the entry in the loop (not necessarly the TOP), but there must be only one entry

            |
            v
          head 
            |
            |    top/beg <--+
            |       |       |
            |      ...      |
            |       |       |
            |       v       |
            +---> entry     |
                    |       |
                   ...      |
                    |       |
                    v       |
             +-- exit/tail  |
             |      |       |
             |     ...      |
             |      |       |
             |      v       |
             |    bottom ---+
             |      
             +------+      
                    |
                    v

     */

    BasicBlock   *  head;
    BasicBlock   *  top;
    BasicBlock   *  bottom;
    BasicBlock   *  entry;
    BasicBlock   *  exit;
    unsigned char   exitCount;


    for (head = fgFirstBB; head->bbNext; head = head->bbNext)
    {
        top       = head->bbNext;
        exit      = NULL;
        exitCount = 0;

        //  Blocks that are rarely run have a zero bbWeight and should
        //  never be optimized here

        if (top->bbWeight == 0)
            continue;

        for (pred = top->bbPreds; pred; pred = pred->flNext)
        {
            /* Is this a loop candidate? - We look for "back edges", i.e. an edge from BOTTOM
             * to TOP (note that this is an abuse of notation since this is not necesarly a back edge
             * as the definition says, but merely an indication that we have a loop there)
             * Thus, we have to be very careful and after entry discovery check that it is indeed
             * the only place we enter the loop (especially for non-reducible flow graphs) */

            bottom    = pred->flBlock;
            exitCount = 0;

            if (top->bbNum <= bottom->bbNum)    // is this a backward edge? (from BOTTOM to TOP)
            {
                if ((bottom->bbJumpKind == BBJ_RET)    ||
                    (bottom->bbJumpKind == BBJ_CALL  ) ||
                    (bottom->bbJumpKind == BBJ_SWITCH)  )
                {
                    /* RET and CALL can never form a loop
                     * SWITCH that has a backward jump appears only for labeled break */
                    goto NO_LOOP;
                }

                BasicBlock   *   loopBlock;

                /* The presence of a "back edge" is an indication that a loop might be present here
                 *
                 * LOOP:
                 *        1. A collection of STRONGLY CONNECTED nodes i.e. there is a path from any
                 *           node in the loop to any other node in the loop (wholly within the loop)
                 *        2. The loop has a unique ENTRY, i.e. there is only one way to reach a node
                 *           in the loop from outside the loop, and that is through the ENTRY
                 */

                /* Let's find the loop ENTRY */

                if (head->bbJumpKind == BBJ_ALWAYS)
                {
                    if (head->bbJumpDest->bbNum <= bottom->bbNum &&
                        head->bbJumpDest->bbNum >= top->bbNum  )
                    {
                        /* OK - we enter somewhere within the loop */
                        entry = head->bbJumpDest;

                        /* some useful asserts
                         * Cannot enter at the top - should have being caught by redundant jumps */

                        assert (entry != top);
                    }
                    else
                    {
                        /* special case - don't consider now */
                        //assert (!"Loop entered in weird way!");
                        goto NO_LOOP;
                    }
                }       /// can we fall through into the loop
                else if (head->bbJumpKind == BBJ_NONE  || head->bbJumpKind == BBJ_COND)
                {
                    /* The ENTRY is at the TOP (a do-while loop) */
                    entry = top;
                }
                else 
                    goto NO_LOOP;       // head does not flow into the loop bail for now
 

                /* Make sure ENTRY dominates all blocks in the loop
                 * This is necessary to ensure condition 2. above
                 * At the same time check if the loop has a single exit
                 * point - those loops are easier to optimize */

                for (loopBlock = top; loopBlock != bottom->bbNext;
                     loopBlock = loopBlock->bbNext)
                {
                    if (!fgDominate(entry, loopBlock))
                    {
                        goto NO_LOOP;
                    }

                    if (loopBlock == bottom)
                    {
                        if (bottom->bbJumpKind != BBJ_ALWAYS)
                        {
                            /* there is an exit at the bottom */

                            assert(bottom->bbJumpDest == top);
                            exit = bottom;
                            exitCount++;
                            continue;
                        }
                    }

                    BasicBlock  * exitPoint;

                    switch (loopBlock->bbJumpKind)
                    {
                    case BBJ_COND:
                    case BBJ_CALL:
                    case BBJ_ALWAYS:
                        assert (loopBlock->bbJumpDest);
                        exitPoint = loopBlock->bbJumpDest;

                        if (exitPoint->bbNum < top->bbNum     ||
                            exitPoint->bbNum > bottom->bbNum   )
                        {
                            /* exit from a block other than BOTTOM */
                            exit = loopBlock;
                            exitCount++;
                        }
                        break;

                    case BBJ_NONE:
                        break;

                    case BBJ_RET:
                        /* The "try" associated with this "finally" must be in the
                         * same loop, so the finally block will return control inside the loop */
                        break;

                    case BBJ_THROW:
                    case BBJ_RETURN:
                        /* those are exits from the loop */
                        exit = loopBlock;
                        exitCount++;
                        break;

                    case BBJ_SWITCH:

                        unsigned        jumpCnt = loopBlock->bbJumpSwt->bbsCount;
                        BasicBlock * *  jumpTab = loopBlock->bbJumpSwt->bbsDstTab;

                        do
                        {
                            assert(*jumpTab);
                            exitPoint = *jumpTab;

                            if (exitPoint->bbNum < top->bbNum     ||
                                exitPoint->bbNum > bottom->bbNum   )
                            {
                                exit = loopBlock;
                                exitCount++;
                            }
                        }
                        while (++jumpTab, --jumpCnt);
                        break;
                    }
                }

                /* Make sure we can iterate the loop (i.e. there is a way back to ENTRY)
                 * This is to ensure condition 1. above which prevents marking fake loops
                 *
                 * Below is an example:
                 *          for(....)
                 *          {
                 *            ...
                 *              computations
                 *            ...
                 *            break;
                 *          }
                 * The example above is not a loop since we bail after the first iteration
                 *
                 * The condition we have to check for is
                 *  1. ENTRY must have at least one predecessor inside the loop. Since we know that that block is reacheable,
                 *     it can only be reached through ENTRY, therefore we have a way back to ENTRY
                 *
                 *  2. If we have a GOTO (BBJ_ALWAYS) outside of the loop and that block dominates the
                 *     loop bottom then we cannot iterate
                 *
                 * NOTE that this doesn't entirely satisfy condition 1. since "break" statements are not
                 * part of the loop nodes (as per definition they are loop exits executed only once),
                 * but we have no choice but to include them because we consider all blocks within TOP-BOTTOM */


                for (loopBlock = top; loopBlock != bottom; loopBlock = loopBlock->bbNext)
                {
                    switch(loopBlock->bbJumpKind)
                    {
                    case BBJ_ALWAYS:
                    case BBJ_THROW:
                    case BBJ_RETURN:
                        if  (fgDominate(loopBlock, bottom))
                            goto NO_LOOP;
                    }
                }

                bool canIterateLoop = false;

                for (predEntry = entry->bbPreds; predEntry; predEntry = predEntry->flNext)
                {
                    if (predEntry->flBlock->bbNum >= top->bbNum    &&
                        predEntry->flBlock->bbNum <= bottom->bbNum  )
                    {
                        canIterateLoop = true;
                        break;
                    }
                }

                if (!canIterateLoop)
                    goto NO_LOOP;

                /* Double check - make sure that all loop blocks except ENTRY
                 * have no predecessors outside the loop - this ensures only one loop entry and prevents
                 * us from considering non-loops due to incorrectly assuming that we had a back edge
                 *
                 * OBSERVATION:
                 *    Loops of the form "while (a || b)" will be treated as 2 nested loops (with the same header)
                 */

                for (loopBlock = top; loopBlock != bottom->bbNext;
                     loopBlock = loopBlock->bbNext)
                {
                    if (loopBlock == entry)
                        continue;

                    for (predTop = loopBlock->bbPreds; predTop;
                         predTop = predTop->flNext)
                    {
                        if (predTop->flBlock->bbNum < top->bbNum    ||
                            predTop->flBlock->bbNum > bottom->bbNum  )
                        {
                            // @TODO [CONSIDER] [04/16/01] []: if the predecessor is a jsr-ret, 
                            // it can be outside the loop 

                            //assert(!"Found loop with multiple entries");
                            goto NO_LOOP;
                        }
                    }
                 }

                /* At this point we have a loop - record it in the loop table
                 * If we found only one exit, record it in the table too
                 * (otherwise an exit = 0 in the loop table means multiple exits) */

                assert (pred);
                if (exitCount > 1)
                {
                    exit = 0;
                }
                optRecordLoop(head, bottom, entry, exit, exitCount);

#if COUNT_LOOPS
                if (!hasMethodLoops)
                {
                    /* mark the method as containing natural loops */
                    totalLoopMethods++;
                    hasMethodLoops = true;
                }

                /* increment total number of loops found */
                totalLoopCount++;
                loopsThisMethod++;

                /* keep track of the number of exits */
                if (exitCount <= 6)
                {
                    exitLoopCond[exitCount]++;
                }
                else
                {
                    exitLoopCond[7]++;
                }
#endif
            }

            /* current predecessor not good for a loop - continue with another one, if any */
NO_LOOP: ;
        }
    }

#if COUNT_LOOPS
    if (maxLoopsPerMethod < loopsThisMethod)
    {
        maxLoopsPerMethod = loopsThisMethod;
    }
#endif

}

/*****************************************************************************
 * If the : i += const" will cause an overflow exception for the small types.
 */

bool                jitIterSmallOverflow(long iterAtExit, var_types incrType)
{
    long            type_MAX;

    switch(incrType)
    {
    case TYP_BYTE:  type_MAX = SCHAR_MAX;   break;
    case TYP_UBYTE: type_MAX = UCHAR_MAX;   break;
    case TYP_SHORT: type_MAX =  SHRT_MAX;   break;
    case TYP_CHAR:  type_MAX = USHRT_MAX;   break;

    case TYP_UINT:                  // Detected by checking for 32bit ....
    case TYP_INT:   return false;   // ... overflow same as done for TYP_INT

    default:        NO_WAY_RET("Bad type", bool);
    }

    if (iterAtExit > type_MAX)
        return true;
    else
        return false;
}

/*****************************************************************************
 * If the "i -= const" will cause an underflow exception for the small types
 */

bool                jitIterSmallUnderflow(long iterAtExit, var_types decrType)
{
    long            type_MIN;

    switch(decrType)
    {
    case TYP_BYTE:  type_MIN = SCHAR_MIN;   break;
    case TYP_SHORT: type_MIN =  SHRT_MIN;   break;
    case TYP_UBYTE: type_MIN =         0;   break;
    case TYP_CHAR:  type_MIN =         0;   break;

    case TYP_UINT:                  // Detected by checking for 32bit ....
    case TYP_INT:   return false;   // ... underflow same as done for TYP_INT

    default:        NO_WAY_RET("Bad type", bool);
    }

    if (iterAtExit < type_MIN)
        return true;
    else
        return false;
}

/*****************************************************************************
 *
 *  Helper for unroll loops - Computes the number of repetitions
 *  in a constant loop. If it cannot prove the number is constant returns false
 */

bool                Compiler::optComputeLoopRep(long            constInit,
                                                long            constLimit,
                                                long            iterInc,
                                                genTreeOps      iterOper,
                                                var_types       iterOperType,
                                                genTreeOps      testOper,
                                                bool            unsTest,
                                                bool            dupCond,
                                                unsigned *      iterCount)
{
    assert(genActualType(iterOperType) == TYP_INT);

    __int64         constInitX;
    __int64         constLimitX;

    unsigned        loopCount;
    int             iterSign;

    // Using this, we can just do a signed comparison with other 32 bit values.
    if (unsTest)    constLimitX = (unsigned long)constLimit;
    else            constLimitX = (  signed long)constLimit;

    switch(iterOperType)
    {
        // For small types, the iteration operator will narrow these values if big

#define INIT_ITER_BY_TYPE(type)      constInitX = (type)constInit; iterInc = (type)iterInc;

    case TYP_BYTE:  INIT_ITER_BY_TYPE(  signed char );  break;
    case TYP_UBYTE: INIT_ITER_BY_TYPE(unsigned char );  break;
    case TYP_SHORT: INIT_ITER_BY_TYPE(  signed short);  break;
    case TYP_CHAR:  INIT_ITER_BY_TYPE(unsigned short);  break;

        // For the big types, 32 bit arithmetic is performed

    case TYP_INT:
    case TYP_UINT:  if (unsTest)    constInitX = (unsigned long)constInit;
                    else            constInitX = (  signed long)constInit;
                    break;

    default:        
        assert(!"Bad type");
        NO_WAY("Bad type");
    }

    /* If iterInc is zero we have an infinite loop */

    if (iterInc == 0)
        return false;

    /* Set iterSign to +1 for positive iterInc and -1 for negative iterInc */
    iterSign  = (iterInc > 0) ? +1 : -1;

    /* Initialize loopCount to zero */
    loopCount = 0;

    // If dupCond is true then the loop head contains a test which skips
    // this loop, if the constInit does not pass the loop test
    // Such a loop can execute zero times.
    // If supCond is false then we have a true do-while loop which we
    // always execute the loop once before performing the loop test
    if (!dupCond)
    {
        loopCount  += 1;
        constInitX += iterInc;       
    }

    /* Compute the number of repetitions */

    switch (testOper)
    {
        __int64     iterAtExitX;

    case GT_EQ:
        /* something like "for(i=init; i == lim; i++)" doesn't make any sense */
        return false;

    case GT_NE:
        /*  "for(i=init; i != lim; i+=const)" - this is tricky since it may 
         *  have a constant number of iterations or loop forever - 
         *  we have to compute (lim-init) mod iterInc to see if it is zero.
         * If mod iterInc is not zero then the limit test will miss an a wrap will occur
         * which is probably not what the end user wanted, but it is legal.
         */

        if (iterInc > 0)
        {
            /* Stepping by one, i.e. Mod with 1 is always zero */
            if (iterInc != 1)
            {
                if (((constLimitX - constInitX) % iterInc) != 0)
                    return false;
            }
        }
        else
        {
            assert(iterInc < 0);
            /* Stepping by -1, i.e. Mod with 1 is always zero */
            if (iterInc != -1)
            {
                if (((constInitX - constLimitX) % (-iterInc)) != 0)
                    return false;
            }
        }

        switch (iterOper)
        {
        case GT_ASG_SUB:
            iterInc = -iterInc;
            // FALL THROUGH

        case GT_ASG_ADD:
            if (constInitX != constLimitX)
                loopCount += (unsigned) ((constLimitX - constInitX - iterSign) / iterInc) + 1;

            iterAtExitX = (long)(constInitX + iterInc * (long)loopCount);
                
            if (unsTest)
                iterAtExitX = (unsigned)iterAtExitX;
                 
            // Check if iteration incr will cause overflow for small types
            if (jitIterSmallOverflow((long)iterAtExitX, iterOperType))
                return false;
                 
            // iterator with 32bit overflow. Bad for TYP_(U)INT
            if (iterAtExitX < constLimitX)
                return false;

            *iterCount = loopCount;
            return true;

        case GT_ASG_MUL:
        case GT_ASG_DIV:
        case GT_ASG_RSH:
        case GT_ASG_LSH:
        case GT_ASG_UDIV:
            return false;

        default:
            assert(!"Unknown operator for loop iterator");
            return false;
        }

    case GT_LT:
        switch (iterOper)
        {
        case GT_ASG_SUB:
            iterInc = -iterInc;
            // FALL THROUGH

        case GT_ASG_ADD:
            if (constInitX < constLimitX)
                loopCount += (unsigned) ((constLimitX - constInitX - iterSign) / iterInc) + 1;

            iterAtExitX = (long)(constInitX + iterInc * (long)loopCount);
                
            if (unsTest)
                iterAtExitX = (unsigned)iterAtExitX;
                
            // Check if iteration incr will cause overflow for small types
            if (jitIterSmallOverflow((long)iterAtExitX, iterOperType))
                return false;
                
            // iterator with 32bit overflow. Bad for TYP_(U)INT
            if (iterAtExitX < constLimitX)
                return false;
            
            *iterCount = loopCount;
            return true;

        case GT_ASG_MUL:
        case GT_ASG_DIV:
        case GT_ASG_RSH:
        case GT_ASG_LSH:
        case GT_ASG_UDIV:
            return false;

        default:
            assert(!"Unknown operator for loop iterator");
            return false;
        }

    case GT_LE:
        switch (iterOper)
        {
        case GT_ASG_SUB:
            iterInc = -iterInc;
            // FALL THROUGH

        case GT_ASG_ADD:
            if (constInitX <= constLimitX)
                loopCount += (unsigned) ((constLimitX - constInitX) / iterInc) + 1;
                
            iterAtExitX = (long)(constInitX + iterInc * (long)loopCount);
                
            if (unsTest)
                iterAtExitX = (unsigned)iterAtExitX;
                
            // Check if iteration incr will cause overflow for small types
            if (jitIterSmallOverflow((long)iterAtExitX, iterOperType))
                return false;
                
            // iterator with 32bit overflow. Bad for TYP_(U)INT
            if (iterAtExitX <= constLimitX)
                return false;

            *iterCount = loopCount;
            return true;

        case GT_ASG_MUL:
        case GT_ASG_DIV:
        case GT_ASG_RSH:
        case GT_ASG_LSH:
        case GT_ASG_UDIV:
            return false;

        default:
            assert(!"Unknown operator for loop iterator");
            return false;
        }

    case GT_GT:
        switch (iterOper)
        {
        case GT_ASG_SUB:
            iterInc = -iterInc;
            // FALL THROUGH

        case GT_ASG_ADD:
            if (constInitX > constLimitX)
                loopCount += (unsigned) ((constLimitX - constInitX - iterSign) / iterInc) + 1;

            iterAtExitX = (long)(constInitX + iterInc * (long)loopCount);
                
            if (unsTest)
                iterAtExitX = (unsigned)iterAtExitX;
                
            // Check if small types will underflow
            if (jitIterSmallUnderflow((long)iterAtExitX, iterOperType))
                return false;

            // iterator with 32bit underflow. Bad for TYP_INT and unsigneds
            if (iterAtExitX > constLimitX)
                return false;

            *iterCount = loopCount;
            return true;

        case GT_ASG_MUL:
        case GT_ASG_DIV:
        case GT_ASG_RSH:
        case GT_ASG_LSH:
        case GT_ASG_UDIV:
            return false;

        default:
            assert(!"Unknown operator for loop iterator");
            return false;
        }

    case GT_GE:
        switch (iterOper)
        {
        case GT_ASG_SUB:
            iterInc = -iterInc;
            // FALL THROUGH

        case GT_ASG_ADD:
            if (constInitX >= constLimitX)
                loopCount += (unsigned) ((constLimitX - constInitX) / iterInc) + 1;
                
            iterAtExitX = (long)(constInitX + iterInc * (long)loopCount);
            
            if (unsTest)
                iterAtExitX = (unsigned)iterAtExitX;
            
            // Check if small types will underflow
            if (jitIterSmallUnderflow((long)iterAtExitX, iterOperType))
                return false;
            
            // iterator with 32bit underflow. Bad for TYP_INT and unsigneds
            if (iterAtExitX >= constLimitX)
                return false;

            *iterCount = loopCount;
            return true;

        case GT_ASG_MUL:
        case GT_ASG_DIV:
        case GT_ASG_RSH:
        case GT_ASG_LSH:
        case GT_ASG_UDIV:
            return false;

        default:
            assert(!"Unknown operator for loop iterator");
            return false;
        }

    default:
        assert(!"Unknown operator for loop condition");
    }

    return false;
}


/*****************************************************************************
 *
 *  Look for loop unrolling candidates and unroll them
 */

void                Compiler::optUnrollLoops()
{
    if (optLoopCount == 0)
        return;

#ifdef DEBUG
            static ConfigDWORD fJitNoUnroll(L"JitNoUnroll", 0);
            if (fJitNoUnroll.val())
                return;
#endif

#ifdef DEBUG
    if  (verbose) 
        printf("*************** In optUnrollLoops()\n");
#endif
    /* Look for loop unrolling candidates */

    /*  Double loop so that after unrolling an inner loop we set change to true
     *  and we then go back over all of the loop candidates and try to unroll
     *  the next outer loop, until we don't unroll any loops, 
     *  then change will be false and we are done.
     */
    for (;;)
    {
        bool        change = false;

        for (unsigned lnum = 0; lnum < optLoopCount; lnum++)
        {
            BasicBlock *    block;
            BasicBlock *    head;
            BasicBlock *    bottom;

            GenTree *       loop;
            GenTree *       test;
            GenTree *       incr;
            GenTree *       phdr;
            GenTree *       init;

            bool            dupCond;
            long            lval;
            long            lbeg;               // initial value for iterator
            long            llim;               // limit value for iterator
            unsigned        lvar;               // iterator lclVar #
            long            iterInc;            // value to increment the iterator
            genTreeOps      iterOper;           // type of iterator increment (i.e. ASG_ADD, ASG_SUB, etc.)
            var_types       iterOperType;       // type result of the oper (for overflow instrs)
            genTreeOps      testOper;           // type of loop test (i.e. GT_LE, GT_GE, etc.)
            bool            unsTest;            // Is the comparison u/int

            unsigned        totalIter;          // total number of iterations in the constant loop
            unsigned        loopCostSz;         // Cost is size of one iteration
            unsigned        loopFlags;          // actual lpFlags
            unsigned        requiredFlags;      // required lpFlags

            GenTree *       loopList;           // new stmt list of the unrolled loop
            GenTree *       loopLast;

            static const ITER_LIMIT[COUNT_OPT_CODE + 1] =
            {
                10, // BLENDED_CODE
                0,  // SMALL_CODE
                20, // FAST_CODE
                0   // COUNT_OPT_CODE
            };

            assert(ITER_LIMIT[    SMALL_CODE] == 0);
            assert(ITER_LIMIT[COUNT_OPT_CODE] == 0);

            unsigned iterLimit = (unsigned)ITER_LIMIT[compCodeOpt()];

#ifdef DEBUG
            if (compStressCompile(STRESS_UNROLL_LOOPS, 50))
                iterLimit *= 10;
#endif

            static const UNROLL_LIMIT_SZ[COUNT_OPT_CODE + 1] =
            {
                30, // BLENDED_CODE
                0,  // SMALL_CODE
                60, // FAST_CODE
                0   // COUNT_OPT_CODE
            };

            assert(UNROLL_LIMIT_SZ[    SMALL_CODE] == 0);
            assert(UNROLL_LIMIT_SZ[COUNT_OPT_CODE] == 0);

            int unrollLimitSz = (unsigned)UNROLL_LIMIT_SZ[compCodeOpt()];

#ifdef DEBUG
            if (compStressCompile(STRESS_UNROLL_LOOPS, 50))
                unrollLimitSz *= 10;
#endif

            loopFlags     = optLoopTable[lnum].lpFlags;
            requiredFlags = LPFLG_DO_WHILE | LPFLG_ONE_EXIT | LPFLG_CONST;


            /* Ignore the loop if we don't have a do-while with a single exit
               that has a constant number of iterations */

            if  ((loopFlags & requiredFlags) != requiredFlags)
                continue;

            /* ignore if removed or marked as not unrollable */

            if  (optLoopTable[lnum].lpFlags & (LPFLG_DONT_UNROLL | LPFLG_REMOVED))
                continue;

            head   = optLoopTable[lnum].lpHead; assert(head);
            bottom = optLoopTable[lnum].lpEnd;  assert(bottom);

            /* The single exit must be at the bottom of the loop */
            assert(optLoopTable[lnum].lpExit);
            if  (optLoopTable[lnum].lpExit != bottom)
                continue;

            /* Unrolling loops with jumps in them is not worth the headache
             * Later we might consider unrolling loops after un-switching */

            block = head;
            do
            {
                block = block->bbNext; assert(block);

                if  (block->bbJumpKind != BBJ_NONE)
                {
                    if  (block != bottom)
                        goto DONE_LOOP;
                }
            }
            while (block != bottom);

            /* Get the loop data:
                - initial constant
                - limit constant
                - iterator
                - iterator increment
                - increment operation type (i.e. ASG_ADD, ASG_SUB, etc...)
                - loop test type (i.e. GT_GE, GT_LT, etc...)
             */

            lbeg        = optLoopTable[lnum].lpConstInit;
            llim        = optLoopTable[lnum].lpConstLimit();
            testOper    = optLoopTable[lnum].lpTestOper();

            lvar        = optLoopTable[lnum].lpIterVar();
            iterInc     = optLoopTable[lnum].lpIterConst();
            iterOper    = optLoopTable[lnum].lpIterOper();

            iterOperType= optLoopTable[lnum].lpIterOperType();
            unsTest     =(optLoopTable[lnum].lpTestTree->gtFlags & GTF_UNSIGNED) != 0;
            if (lvaTable[lvar].lvAddrTaken)
                continue;

            /* Locate the pre-header and initialization and increment/test statements */

            phdr = head->bbTreeList; assert(phdr);
            loop = bottom->bbTreeList; assert(loop);

            init = phdr->gtPrev; assert(init && (init->gtNext == 0));
            test = loop->gtPrev; assert(test && (test->gtNext == 0));
            incr = test->gtPrev; assert(incr);

            if  (init->gtFlags & GTF_STMT_CMPADD)
            {
                /* Must be a duplicated loop condition */
                assert(init->gtStmt.gtStmtExpr->gtOper == GT_JTRUE);

                dupCond = true;
                init    = init->gtPrev; assert(init);
            }
            else
                dupCond = false;

            /* Find the number of iterations - the function returns false if not a constant number */

            if (!optComputeLoopRep(lbeg, llim,
                                   iterInc, iterOper, iterOperType,
                                   testOper, unsTest, dupCond,
                                   &totalIter))
                continue;

            /* Forget it if there are too many repetitions or not a constant loop */

            if  (totalIter > iterLimit)
                continue;

            assert(init->gtOper == GT_STMT); init = init->gtStmt.gtStmtExpr;
            assert(test->gtOper == GT_STMT); test = test->gtStmt.gtStmtExpr;
            assert(incr->gtOper == GT_STMT); incr = incr->gtStmt.gtStmtExpr;

            assert(init->gtOper             == GT_ASG);
            assert(incr->gtOp.gtOp1->gtOper == GT_LCL_VAR);
            assert(incr->gtOp.gtOp2->gtOper == GT_CNS_INT);
            assert(test->gtOper             == GT_JTRUE);

            /* heuristic - Estimated cost in code size of the unrolled loop */

            loopCostSz = 0;

            block = head;

            do
            {
                block = block->bbNext;

                /* Visit all the statements in the block */

                for (GenTreePtr stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
                {
                    /* Get the expression and stop if end reached */

                    GenTreePtr expr = stmt->gtStmt.gtStmtExpr;
                    if  (expr == incr)
                        break;

                    /* Calculate gtCostSz */
                    gtSetStmtInfo(stmt);

                    /* Update loopCostSz */
                    loopCostSz += stmt->gtCostSz;
                }
            }
            while (block != bottom);

            /* Compute the estimated increase in code size for the unrolled loop */

            const unsigned int fixedLoopCostSz = 8;

            const int unrollCostSz  = (loopCostSz * totalIter) - (loopCostSz + fixedLoopCostSz);

            /* Don't unroll if too much code duplication would result. */

            if  (unrollCostSz > unrollLimitSz)
            {
                /* prevent this loop from being revisited */
                optLoopTable[lnum].lpFlags |= LPFLG_DONT_UNROLL;
                goto DONE_LOOP;
            }

            /* Looks like a good idea to unroll this loop, let's do it! */

#ifdef  DEBUG
            if (verbose)
            {
                printf("\nUnrolling loop BB%02u", head->bbNext->bbNum);
                if (head->bbNext->bbNum != bottom->bbNum)
                    printf("..BB%02u", bottom->bbNum);
                printf(" over V%02u from %u to %u", lvar, lbeg, llim);
                printf(" unrollCostSz = %d\n", unrollCostSz);
                printf("\n");
            }
#endif

            /* Make sure everything looks ok */

            assert(init->gtOper                         == GT_ASG);
            assert(init->gtOp.gtOp1->gtOper             == GT_LCL_VAR);
            assert(init->gtOp.gtOp1->gtLclVar.gtLclNum  == lvar);
            assert(init->gtOp.gtOp2->gtOper             == GT_CNS_INT);
            assert(init->gtOp.gtOp2->gtIntCon.gtIconVal == lbeg);

            /* Create the unrolled loop statement list */

            loopList =
            loopLast = 0;

            for (lval = lbeg; totalIter; totalIter--)
            {
                block = head;

                do
                {
                    GenTree *       stmt;
                    GenTree *       expr;

                    block = block->bbNext; assert(block);

                    /* Visit all the statements in the block */

                    for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
                    {
                        /* Stop if we've reached the end of the loop */

                        if  (stmt->gtStmt.gtStmtExpr == incr)
                            break;

                        /* Clone/substitute the expression */

                        expr = gtCloneExpr(stmt, 0, lvar, lval);

                        // HACK: cloneExpr doesn't handle everything

                        if  (!expr)
                        {
                            optLoopTable[lnum].lpFlags |= LPFLG_DONT_UNROLL;
                            goto DONE_LOOP;
                        }

                        /* Append the expression to our list */

                        if  (loopList)
                            loopLast->gtNext = expr;
                        else
                            loopList         = expr;

                        expr->gtPrev = loopLast;
                                       loopLast = expr;
                    }
                }
                while (block != bottom);

                /* update the new value for the unrolled iterator */

                switch (iterOper)
                {
                    case GT_ASG_ADD:
                        lval += iterInc;
                        break;

                    case GT_ASG_SUB:
                        lval -= iterInc;
                        break;

                    case GT_ASG_RSH:
                    case GT_ASG_LSH:
                        assert(!"Unrolling not implemented for this loop iterator");
                        goto DONE_LOOP;
                    default:
                        assert(!"Unknown operator for constant loop iterator");
                        goto DONE_LOOP;
                }
            }

            /* Finish the linked list */

            if (loopList)
            {
                loopList->gtPrev = loopLast;
                loopLast->gtNext = 0;
            }

            /* Replace the body with the unrolled one */

            block = head;

            do
            {
                block             = block->bbNext; assert(block);
                block->bbTreeList = 0;
                block->bbJumpKind = BBJ_NONE;
            }
            while (block != bottom);

            bottom->bbJumpKind  = BBJ_NONE;
            bottom->bbTreeList  = loopList;
            bottom->bbWeight   /= BB_LOOP_WEIGHT;

            bool        dummy;

            fgMorphStmts(bottom, &dummy, &dummy, &dummy);

            /* Update bbRefs and bbPreds */
            /* Here head->bbNext is bottom !!! - Replace it */

            assert(head->bbNext->bbRefs);
            head->bbNext->bbRefs--;

            fgRemovePred(head->bbNext, bottom);

            /* Now change the initialization statement in the HEAD to "lvar = lval;"
             * (the last value of the iterator in the loop)
             * and drop the jump condition since the unrolled loop will always execute */

            assert(init->gtOper                         == GT_ASG);
            assert(init->gtOp.gtOp1->gtOper             == GT_LCL_VAR);
            assert(init->gtOp.gtOp1->gtLclVar.gtLclNum  == lvar);
            assert(init->gtOp.gtOp2->gtOper             == GT_CNS_INT);
            assert(init->gtOp.gtOp2->gtIntCon.gtIconVal == lbeg);

            init->gtOp.gtOp2->gtIntCon.gtIconVal =  lval;

            /* if the HEAD is a BBJ_COND drop the condition (and make HEAD a BBJ_NONE block) */

            if (head->bbJumpKind == BBJ_COND)
            {
                phdr = head->bbTreeList; assert(phdr);
                test = phdr->gtPrev;

                assert(test && (test->gtNext == 0));
                assert(test->gtOper == GT_STMT);
                assert(test->gtStmt.gtStmtExpr->gtOper == GT_JTRUE);

                init = test->gtPrev; assert(init && (init->gtNext == test));
                assert(init->gtOper == GT_STMT);

                init->gtNext = 0;
                phdr->gtPrev = init;
                head->bbJumpKind = BBJ_NONE;

                /* Update bbRefs and bbPreds */

                assert(head->bbJumpDest->bbRefs);
                head->bbJumpDest->bbRefs--;

                fgRemovePred(head->bbJumpDest, head);
            }
            else
            {
                /* the loop must execute */
                assert(head->bbJumpKind == BBJ_NONE);
            }

#ifdef  DEBUG
            if (verbose)
            {
                printf("Whole unrolled loop:\n");

                GenTreePtr s = loopList;

                while (s)
                {
                    assert(s->gtOper == GT_STMT);
                    gtDispTree(s);
                    s = s->gtNext;
                }
                printf("\n");

                gtDispTree(init);
                printf("\n");
            }
#endif

            /* Remember that something has changed */

            change = true;

            /* Make sure to update loop table */

            /* Use the LPFLG_REMOVED flag and update the bbLoopMask acordingly
             * (also make head and bottom NULL - to hit an assert or GPF) */

            optLoopTable[lnum].lpFlags |= LPFLG_REMOVED;
            optLoopTable[lnum].lpHead   =
            optLoopTable[lnum].lpEnd    = 0;

        DONE_LOOP:;
        }

        if  (!change)
            break;
    }

#ifdef  DEBUG
    fgDebugCheckBBlist();
#endif
}

/*****************************************************************************
 *
 *  Return non-zero if there is a code path from 'srcBB' to 'dstBB' that will
 *  not execute a method call.
 */

bool                Compiler::optReachWithoutCall(BasicBlock *srcBB,
                                                  BasicBlock *dstBB)
{
    /*  @TODO [CONSIDER] [04/16/01] []: Currently BBF_GC_SAFE_POINT is not set 
     *  for helper calls, as some helper calls are neither interruptible nor hijackable.
     *  If we can determine this, then we can set BBF_GC_SAFE_POINT for 
     *  some helpers too.
     */

    assert(srcBB->bbNum <= dstBB->bbNum);

    /* Are dominator sets available? */

    if  (!fgDomsComputed)
    {
        /* All we can check is the src/dst blocks */

        if ((srcBB->bbFlags|dstBB->bbFlags) & BBF_GC_SAFE_POINT)
            return false;
        else
            return true;
    }

    for (;;)
    {
        assert(srcBB);

        /*  If we added a loop pre-header block then we will
            have a bbNum greater than fgLastBB, and we won't have
            any dominator information about this block, so skip it.
         */
        if  (srcBB->bbNum <= fgLastBB->bbNum)
        {
            assert(srcBB->bbNum <= dstBB->bbNum);

            /* Does this block contain a call? */

            if  (srcBB->bbFlags & BBF_GC_SAFE_POINT)
            {
                /* Will this block always execute on the way to dstBB ? */

                if  (srcBB == dstBB || fgDominate(srcBB, dstBB))
                    return  false;
            }
            else
            {
                /* If we've reached the destination block, we're done */

                if  (srcBB == dstBB)
                    break;
            }
        }

        srcBB = srcBB->bbNext;
    }

    return  true;
}

/*****************************************************************************
 *
 * Check if the termination test at the bottom of the loop
 * is of the form we want. We require that the first operand of the
 * compare is a leaf. The caller checks the second operand.
 */

static
GenTreePtr          genLoopTermTest(BasicBlock *top,
                                    BasicBlock *bottom, bool bigOK = false)
{
    GenTreePtr      testt;
    GenTreePtr      condt;
    GenTreePtr      op1;
    GenTreePtr      op2;

    testt = bottom->bbTreeList;
    assert(testt && testt->gtOper == GT_STMT);

        // @TODO NOW 7/12/01 lets use gtPrev instead of looping - vancem
        // (did not do it now only because we are in low churn mode right now)
    while (testt->gtNext)
        testt = testt->gtNext;

    condt = testt->gtStmt.gtStmtExpr;
    assert(condt->gtOper == GT_JTRUE);
    condt = condt->gtOp.gtOp1;

    /* For now, let's only allow "int-leaf <relop> int-leaf" */

    if  (!condt->OperIsCompare())
        return NULL;

    op1 = condt->gtOp.gtOp1;
    op2 = condt->gtOp.gtOp2;

    if  (!op1->OperIsLeaf())
    {
        if  (!bigOK)
            return NULL;

        /* Allow "leaf + leaf" as well */

        if  (op1->gtOper != GT_ADD)
            return NULL;

        op2 = op1->gtOp.gtOp2;
        op1 = op1->gtOp.gtOp1;

        if  (!op1->OperIsLeaf())
            return NULL;
        if  (!op2->OperIsLeaf())
            return NULL;
    }

    /* Make sure the comparands have handy size */

    if  (condt->gtOp.gtOp1->gtType != TYP_INT)
        return NULL;

    return testt;
}

/*****************************************************************************
 * Optimize "jmp C; do{} C:while(cond);" loops to "if (cond){ do{}while(cond}; }"
 */

void                Compiler::fgOptWhileLoop(BasicBlock * block)
{
    /*
        Optimize while-like loops to not always jump to the test at the bottom
        of the loop initially. Specifically, we're looking for the following
        case:
                ...
                ...
                jmp test
        loop:
                ...
                ...
        test:
                cond
                jtrue   loop

        If we find this, and the condition is a simple one, we change
        the loop to the following:

                ...
                ...

                cond
                jfalse done
        loop:
                ...
                ...
        test:
                cond
                jtrue   loop
        done:

     */

    BasicBlock *    testb;
    GenTreePtr      testt;
    GenTreePtr      conds;
    GenTreePtr      condt;

    /* Does the BB end with an unconditional jump? */

    if  (block->bbJumpKind != BBJ_ALWAYS)
        return;

    /* Get hold of the jump target */

    testb = block->bbJumpDest; 
    
    /* It has to be a forward jump */

    if (testb->bbNum <= block->bbNum)
        return;

    /* Does the block consist of 'jtrue(cond) block' ? */

    if  (testb->bbJumpKind != BBJ_COND)
        return;
    if  (testb->bbJumpDest != block->bbNext)
        return;

    assert(testb->bbNext);
    conds = genLoopTermTest(block, testb, true);

    /* If test not found or not right, keep going */

    if  (conds == NULL)
        return;

    /* testb must only contain only a jtrue with no other stmts, we will only clone
       conds, so any other statements will not get cloned */

    if (testb->bbTreeList != conds)
        return;

    /* Get to the condition node from the statement tree */

    assert(conds->gtOper == GT_STMT);
    
    condt = conds->gtStmt.gtStmtExpr;
    assert(condt->gtOper == GT_JTRUE);
    
    condt = condt->gtOp.gtOp1;
    assert(condt->OperIsCompare());
    
    /* We call gtSetEvalOrder to measure the cost of duplicating this tree */
    genFPstkLevel = 0;
    gtSetEvalOrder(condt);

    unsigned maxDupCostSz = 20;

    if (compCodeOpt() == FAST_CODE)
        maxDupCostSz *= 4;

    /* If the compare has too high cost then we don't want to dup */

    if  ((condt->gtCostSz > maxDupCostSz))
        return;

    /* Looks good - duplicate the condition test */

    condt = gtCloneExpr(condt);
    condt->SetOper(GenTree::ReverseRelop(condt->OperGet()));

    condt = gtNewOperNode(GT_JTRUE, TYP_VOID, condt, 0);

    /* Create a statement entry out of the condition */

    testt = gtNewStmt(condt); 
    testt->gtFlags |= GTF_STMT_CMPADD;

#ifdef DEBUGGING_SUPPORT
    if  (opts.compDbgInfo)
        testt->gtStmtILoffsx = conds->gtStmtILoffsx;
#endif

    /* Append the condition test at the end of 'block' */

    fgInsertStmtAtEnd(block, testt);

    /* Change the block to end with a conditional jump */

    block->bbJumpKind = BBJ_COND;
    block->bbJumpDest = testb->bbNext;

    /* Mark the jump dest block as being a jump target */
    block->bbJumpDest->bbFlags |= BBF_JMP_TARGET|BBF_HAS_LABEL;

    /* Update bbRefs and bbPreds for 'block->bbNext' 'testb' and 'testb->bbNext' */

    fgAddRefPred(block->bbNext, block);

    assert(testb->bbRefs);
    testb->bbRefs--;
    fgRemovePred(testb, block);
    fgAddRefPred(testb->bbNext, block);

#ifdef  DEBUG
    if  (verbose)
    {
        printf("\nDuplicating loop condition in BB%02u for loop (BB%02u - BB%02u)\n",
               block->bbNum, block->bbNext->bbNum, testb->bbNum);
        gtDispTree(testt);
    }

#endif
}

/*****************************************************************************
 *
 *  Perform loop inversion, find and classify natural loops
 */

void                Compiler::optOptimizeLoops()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In optOptimizeLoops()\n");
#endif

    BasicBlock *    block;

#ifdef  DEBUG
    /* Check that the flowgraph data (bbNums, bbRefs, bbPreds) is up-to-date */
    fgDebugCheckBBlist();
#endif

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        /* Make sure the appropriate fields are initialized */

        if (block->bbWeight == 0)
        {
            /* Zero weighted block can't have a LOOP_HEAD flag */
            assert(block->isLoopHead() == false);
            continue;
        }

        assert(block->bbWeight  == BB_UNITY_WEIGHT);
        assert(block->bbLoopNum  == 0);
//      assert(block->bbLoopMask == 0);

        if (compCodeOpt() != SMALL_CODE)
        {
            /* Optimize "while(cond){}" loops to "cond; do{}while(cond};" */

            fgOptWhileLoop(block);
        }
    }

    fgUpdateFlowGraph();

    fgReorderBlocks();

    bool hasLoops = fgComputeDoms();

#ifdef DEBUG
    if  (verbose)
    {
        printf("\nAfter computing the dominators:\n");
        fgDispBasicBlocks(verboseTrees);
        printf("\n");

        fgDispDoms();
        fgDispReach();
    }
#endif

    optSetBlockWeights();

    /* Were there any loops in the flow graph? */

    if  (hasLoops)
    {
        /* now that we have dominator information we can find loops */

        optFindNaturalLoops();

        unsigned        loopNum = 0;

        /* Iterate over the flow graph, marking all loops */

        /* We will use the following terminology:
         * top        - the first basic block in the loop (i.e. the head of the backward edge)
         * bottom     - the last block in the loop (i.e. the block from which we jump to the top)
         * lastBottom - used when we have multiple back-edges to the same top
         */

        flowList *      pred;

        BasicBlock *    top;

        for (top = fgFirstBB; top; top = top->bbNext)
        {
            BasicBlock * foundBottom = NULL;

            for (pred = top->bbPreds; pred; pred = pred->flNext)
            {
                /* Is this a loop candidate? - We look for "back edges" */

                BasicBlock * bottom = pred->flBlock;

                /* is this a backward edge? (from BOTTOM to TOP) */

                if (top->bbNum > bottom->bbNum)
                    continue;

                /* 'top' also must have the BBF_LOOP_HEAD flag set */

                if (top->isLoopHead() == false)
                    continue;

                /* We only consider back-edges that are BBJ_COND or BBJ_ALWAYS for loops */

                if ((bottom->bbJumpKind != BBJ_COND)   &&
                    (bottom->bbJumpKind != BBJ_ALWAYS)    )
                    continue;

                /* the top block must be able to reach the bottom block */
                if (!fgReachable(top, bottom))
                    continue;

                /* Found a new loop, record the longest backedge in foundBottom */

                if ((foundBottom == NULL) || (bottom->bbNum > foundBottom->bbNum))
                {
                    foundBottom = bottom;
                }
            }

            if (foundBottom)
            {
                loopNum++;
                
                /* Mark the loop header as such */
                
                top->bbLoopNum = loopNum;

                /* Mark all blocks between 'top' and 'bottom' */
                
                optMarkLoopBlocks(top, foundBottom, false);
            }
        }

#ifdef  DEBUG
        if  (verbose)
        {
            if  (loopNum > 0)
            {
                printf("\nAfter loop weight marking:\n");
                fgDispBasicBlocks();
                printf("\n");
            }
        }
#endif
        optLoopsMarked = true;
    }
}

/*****************************************************************************
 * If the tree is a tracked local variable, return its LclVarDsc ptr.
 */

inline
Compiler::LclVarDsc *   Compiler::optIsTrackedLocal(GenTreePtr tree)
{
    LclVarDsc   *   varDsc;
    unsigned        lclNum;

    if (tree->gtOper != GT_LCL_VAR)
        return NULL;

    lclNum = tree->gtLclVar.gtLclNum;

    assert(lclNum < lvaCount);
    varDsc = lvaTable + lclNum;

    /* if variable not tracked, return NULL */
    if  (!varDsc->lvTracked)
        return NULL;

    return varDsc;
}


void                Compiler::optMorphTree(BasicBlock * block, 
                                           GenTreePtr   stmt
                                  DEBUGARG(const char * msg)  )
{
    assert(stmt->gtOper == GT_STMT);

    GenTreePtr morph  = fgMorphTree(stmt->gtStmt.gtStmtExpr);

    /* Check for morph as a GT_COMMA with an unconditional throw */
    if (fgIsCommaThrow(morph, true))
    {
       /* Use the call as the new stmt */
        morph = morph->gtOp.gtOp1;
        assert(morph->gtOper == GT_CALL);
    }

    /* we can get a throw as a statement root*/
    if (fgIsThrow(morph))
    {
        assert((morph->gtFlags & GTF_COLON_COND) == 0);
        fgRemoveRestOfBlock = true;                    
    }

    stmt->gtStmt.gtStmtExpr = morph;

    /* Can the entire tree be removed ? */
    /* or this is the last statement of a conditional branch that was just folded */

    if (fgCheckRemoveStmt(block, stmt) ||
       ((stmt->gtNext == NULL)    && 
        !fgRemoveRestOfBlock      &&
         fgFoldConditional(block) &&
        (block->bbJumpKind != BBJ_THROW)))
    {
#ifdef DEBUG
        if (verbose)
        {
            printf("\n%s removed the tree:\n", msg);
            gtDispTree(morph);
        }
#endif
    }
    else
    {
#ifdef DEBUG
        if (verbose)
        {
            printf("\n%s morphed tree:\n", msg);
            gtDispTree(morph);
        }
#endif
    
        /* Have to re-do the evaluation order since for example
         * some later code does not expect constants as op1 */
        gtSetStmtInfo(stmt);

        /* Have to re-link the nodes for this statement */
        fgSetStmtSeq(stmt);
    }

    if (fgRemoveRestOfBlock)
    {
        /* Remove the rest of the stmts in the block */

        while (stmt->gtNext)
        {
            stmt = stmt->gtNext;
            assert(stmt->gtOper == GT_STMT);

            fgRemoveStmt(block, stmt);
        }

        // The rest of block has been removed 
        // and we will always throw an exception 

        // Update succesors of block
        fgRemoveBlockAsPred(block);

        // Convert block to a throw bb
        fgConvertBBToThrowBB(block);    

#ifdef  DEBUG
        if (verbose)
        {
            printf("\n%s Block BB%02u becomes a throw block.\n", msg, block->bbNum);                            
        }            
#endif
    }
}

/*****************************************************************************/
#if CSE
/*****************************************************************************/

void                Compiler::optRngChkInit()
{
    optRngIndPtr   =
    optRngIndScl        =
    optRngGlbRef        =
    optRngAddrTakenVar  = 0;
    optRngChkCount = 0;

    /* Allocate and clear the hash bucket table */

    size_t          byteSize = optRngChkHashSize * sizeof(*optRngChkHash);

    optRngChkHash = (RngChkDsc **)compGetMem(byteSize);
    memset(optRngChkHash, 0, byteSize);
}

/*****************************************************************************
 *
 *  Return the bit corresponding to a range check with the given index.
 */

inline
RNGSET_TP           genRngnum2bit(unsigned index)
{
    assert(index != -1 && index <= RNGSET_SZ);

    return  ((RNGSET_TP)1 << index);
}

/*****************************************************************************/

int                 Compiler::optRngChkIndex(GenTreePtr tree)
{
    unsigned        hash;
    unsigned        hval;

    RngChkDsc *     hashDsc;

    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    unsigned        index;
    RNGSET_TP       mask;

    assert(tree->gtOper == GT_IND);

    /* Compute the dependency mask of the pointer, and make sure
       the expression is acceptable */

    varRefKinds     refMask = VR_NONE;
    VARSET_TP       depMask = lvaLclVarRefs(tree->gtInd.gtIndOp1, NULL, &refMask);
    if  (depMask == VARSET_NOT_ACCEPTABLE)
        return  -1;

    /* Compute the hash value for the expression */

    hash = gtHashValue(tree);
    hval = hash % optRngChkHashSize;

    /* Look for a matching index in the hash table */

    for (hashDsc = optRngChkHash[hval];
         hashDsc;
         hashDsc = hashDsc->rcdNextInBucket)
    {
        if  (hashDsc->rcdHashValue == hash)
        {
            if  (GenTree::Compare(hashDsc->rcdTree, tree, true))
            {
                optDoRngChk = true; // Found a duplicate range-check tree

                return  hashDsc->rcdIndex;
            }
        }
    }

    /* Not found, create a new entry (unless we have too many already) */

    if  (optRngChkCount == RNGSET_SZ)
        return  -1;

    hashDsc = (RngChkDsc *)compGetMem(sizeof(*hashDsc));

    hashDsc->rcdHashValue = hash;
    hashDsc->rcdTree      = tree;
    hashDsc->rcdIndex     = index = optRngChkCount++;
    mask                  = genRngnum2bit(index);

    /* Append the entry to the hash bucket */

    hashDsc->rcdNextInBucket = optRngChkHash[hval];
                               optRngChkHash[hval] = hashDsc;

#ifdef  DEBUG

    if  (verbose)
    {
        printf("\nRange check #%02u [depMask = %s , refMask = ", index, genVS2str(depMask));
        if  (refMask & VR_IND_PTR) printf("Ptr");
        if  (refMask & VR_IND_SCL) printf("Scl");
        if  (refMask & VR_GLB_VAR)    printf("Glb");
        printf("]:\n");
        gtDispTree(tree);
    }

#endif

    /* Mark all variables this index depends on */

    for (lclNum = 0, varDsc = lvaTable;
         lclNum < lvaCount;
         lclNum++  , varDsc++)
    {
        VARSET_TP       lclBit;

        if  (!varDsc->lvTracked)
            continue;

        lclBit = genVarIndexToBit(varDsc->lvVarIndex);

        if  (depMask & lclBit)
        {
            varDsc->lvRngDep    |= mask;

            if  (varDsc->lvAddrTaken)
                optRngAddrTakenVar  |= mask;

            depMask &= ~lclBit;
            if  (!depMask)
                break;
        }
    }

    /* Remember whether the index expression contains an indirection/global ref */

    if  (refMask & VR_IND_PTR) optRngIndPtr |= mask;
    if  (refMask & VR_IND_SCL) optRngIndScl |= mask;
    if  (refMask & VR_GLB_VAR)    optRngGlbRef |= mask;

    return  index;
}

/*****************************************************************************
 *
 *  The following is the upper limit on how many expressions we'll keep track
 *  of for the CSE analysis.
 */

const unsigned MAX_CSE_CNT = EXPSET_SZ;

/*****************************************************************************
 *
 *  The following determines whether the given expression is a worthy CSE
 *  candidate.
 */

inline
bool                Compiler::optIsCSEcandidate(GenTreePtr tree)
{
    /* No good if the expression contains side effects 
       or if it was marked as DONT CSE */

    if  (tree->gtFlags & (GTF_ASG|GTF_DONT_CSE))
        return  false;
    
    /* If GTF_CALL is set, return false unless we at the shared static call node */
    if  ((tree->gtFlags & GTF_CALL) &&
         ((tree->gtOper != GT_CALL) ||
          (tree->gtCall.gtCallType != CT_HELPER)  ||
          (eeGetHelperNum(tree->gtCall.gtCallMethHnd) != CORINFO_HELP_GETSHAREDSTATICBASE)))
    {
        return  false;
    }

    /* The only reason a TYP_STRUCT tree might occur is as an argument to
       GT_ADDR. It will never be actually materialized. So ignore them.
       Also TYP_VOIDs */

    unsigned    cost;
    var_types   type = tree->TypeGet();

    if  (type == TYP_STRUCT || type == TYP_VOID)
        return false;
    
    if (compCodeOpt() == SMALL_CODE)
        cost = tree->gtCostSz;
    else
        cost = tree->gtCostEx;

#if TGT_x86
    /* Don't bother if the potential savings are very low */
    if  (cost < 3)
        return  false;
#endif

    genTreeOps  oper = tree->OperGet();

    if (!MORECSES)
    {
        return    (oper == GT_IND && tree->gtInd.gtIndOp1->gtOper != GT_ARR_ELEM)
               || (oper == GT_ARR_ELEM)
#if CSELENGTH
               || (oper == GT_ARR_LENREF)
               || (oper == GT_ARR_LENGTH)
#endif
              ;
    }
    else // MORECSES
    {
        /* Don't bother with constants and assignments */

        if  (tree->OperKind() & (GTK_CONST|GTK_ASGOP))
            return  false;

        /* Check for some special cases */

        switch (oper)
        {
        case GT_CALL:
            if ((tree->gtCall.gtCallType == CT_HELPER)  &&
                (eeGetHelperNum(tree->gtCall.gtCallMethHnd) == CORINFO_HELP_GETSHAREDSTATICBASE))
            {
                return  true;
            }
            return false;

        case GT_IND:
            return (tree->gtInd.gtIndOp1->gtOper != GT_ARR_ELEM);

            /* We try to cse GT_ARR_ELEM nodes instead of GT_IND(GT_ARR_ELEM).
               Doing the first allows cse to also kick in for code like
               "GT_IND(GT_ARR_ELEM) = GT_IND(GT_ARR_ELEM) + xyz", whereas doing
               the second would not allow it */

        case GT_ARR_ELEM:
#if CSELENGTH
        case GT_ARR_LENREF:
        case GT_ARR_LENGTH:
#endif
        case GT_CLS_VAR:
        case GT_LCL_FLD:
            return true;

        case GT_ADD:
        case GT_SUB:
        case GT_DIV:
        case GT_MUL:
        case GT_MOD:
        case GT_UDIV:
        case GT_UMOD:
        case GT_OR:
        case GT_AND:
        case GT_XOR:
        case GT_LSH:
        case GT_RSH:
        case GT_RSZ:
            return  true;

        case GT_NOP:
        case GT_RET:
        case GT_JTRUE:
        case GT_RETURN:
        case GT_SWITCH:
        case GT_RETFILT:
            return  false;

        }

        if (compStressCompile(STRESS_MAKE_CSE, 30) && cost > 5)
            return true;
        else
            return  false;
    }
}

/*****************************************************************************
 *
 *  Return the bit corresponding to a CSE with the given index.
 */

inline
EXPSET_TP           genCSEnum2bit(unsigned index)
{
    assert(index && index <= EXPSET_SZ);

    return  ((EXPSET_TP)1 << (index-1));
}

/*****************************************************************************
 *
 *  Initialize the CSE tracking logic.
 */

void                Compiler::optCSEinit()
{
    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    for (lclNum = 0, varDsc = lvaTable;
         lclNum < lvaCount;
         lclNum++  , varDsc++)
    {
        varDsc->lvRngDep =
        varDsc->lvExpDep = 0;
    }

    optCSEindPtr        = 0;
    optCSEindScl        = 0;
    optCSEglbRef        = 0;
    optCSEaddrTakenVar  = 0;
    optCSEneverKilled   = 0;
    optCSEcount         = 0;
#ifdef DEBUG            
    optCSEtab           = NULL;
    optCSEstart         = lvaCount;
    optUnmarkCSEtree    = NULL;
#endif

    /* Allocate and clear the hash bucket table */

    size_t          byteSize = s_optCSEhashSize * sizeof(*optCSEhash);

    optCSEhash = (CSEdsc **)compGetMem(byteSize);
    memset(optCSEhash, 0, byteSize);
}

/*****************************************************************************
 *
 *  We've found all the candidates, build the index for easy access.
 */

void                Compiler::optCSEstop()
{
    if (optCSEcount == 0)
        return;

    CSEdsc   *      dsc;
    CSEdsc   *   *  ptr;
    unsigned        cnt;

    optCSEtab = (CSEdsc **)compGetMemArray(optCSEcount, sizeof(*optCSEtab));

    memset(optCSEtab, 0, optCSEcount * sizeof(*optCSEtab));

    for (cnt = s_optCSEhashSize, ptr = optCSEhash;
         cnt;
         cnt--            , ptr++)
    {
        for (dsc = *ptr; dsc; dsc = dsc->csdNextInBucket)
        {
            if (dsc->csdIndex)
            {
            assert(dsc->csdIndex <= optCSEcount);
                if (optCSEtab[dsc->csdIndex-1] == 0)
            optCSEtab[dsc->csdIndex-1] = dsc;
        }
    }
}

#ifdef DEBUG
    for (cnt = 0; cnt < optCSEcount; cnt++)
        assert(optCSEtab[cnt]);
#endif
}

/*****************************************************************************
 *
 *  Return the descriptor for the CSE with the given index.
 */

inline
Compiler::CSEdsc   *   Compiler::optCSEfindDsc(unsigned index)
{
    assert(index);
    assert(index <= optCSEcount);
    assert(optCSEtab[index-1]);

    return  optCSEtab[index-1];
}

/*****************************************************************************
 *
 *  Assign an index to the given expression (adding it to the lookup table,
 *  if necessary). Returns the index or 0 if the expression can't be a CSE.
 */

int                 Compiler::optCSEindex(GenTreePtr tree, GenTreePtr stmt)
{
    unsigned        hash;
    unsigned        hval;

    CSEdsc *        hashDsc;

    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    unsigned        CSEindex;
    EXPSET_TP       CSEmask;
    VARSET_TP       bit;

    /* Compute the dependency mask of the CSE, and make sure
       the expression is acceptable */

    varRefKinds     refMask = VR_NONE;
    VARSET_TP       depMask = lvaLclVarRefs(tree, NULL, &refMask);
    if  (depMask == VARSET_NOT_ACCEPTABLE)
        return  0;

    /* Compute the hash value for the expression */

    hash = gtHashValue(tree);
    hval = hash % s_optCSEhashSize;

    /* Look for a matching index in the hash table */

    for (hashDsc = optCSEhash[hval];
         hashDsc;
         hashDsc = hashDsc->csdNextInBucket)
    {
        if  (hashDsc->csdHashValue == hash)
        {
            if  (GenTree::Compare(hashDsc->csdTree, tree))
            {
                treeStmtLstPtr  list;

                /* Have we started the list of matching nodes? */

                if  (hashDsc->csdTreeList == 0)
                {
                    /* Start the list with the first CSE candidate
                     * recorded - matching CSE of itself */

                    hashDsc->csdTreeList =
                    hashDsc->csdTreeLast =
                    list                 = (treeStmtLstPtr)compGetMem(sizeof(*list));

                    list->tslTree  = hashDsc->csdTree;
                    list->tslStmt  = hashDsc->csdStmt;
                    list->tslBlock = hashDsc->csdBlock;

                    list->tslNext = 0;
                }
                assert(hashDsc->csdTreeList);

                /* Append this expression to the end of the list */

                list = (treeStmtLstPtr)compGetMem(sizeof(*list));

                list->tslTree  = tree;
                list->tslStmt  = stmt;
                list->tslBlock = compCurBB;
                list->tslNext  = 0;

                hashDsc->csdTreeLast->tslNext = list;
                hashDsc->csdTreeLast          = list;

                optDoCSE = true;  // Found a duplicate CSE tree

                /* Have we assigned a CSE index? */
                if (hashDsc->csdIndex == 0)
                    goto FOUND_NEW_CSE;

                return  hashDsc->csdIndex;
            }
        }
    }

    /* Not found, create a new entry (unless we have too many already) */

    if  (optCSEcount < EXPSET_SZ)
    {
        hashDsc = (CSEdsc *)compGetMem(sizeof(*hashDsc));

        hashDsc->csdHashValue = hash;
        hashDsc->csdIndex     = 0;
        hashDsc->csdDefCount  =
        hashDsc->csdUseCount  =
        hashDsc->csdDefWtCnt  =
        hashDsc->csdUseWtCnt  = 0;
        
        hashDsc->csdTree      = tree;
        hashDsc->csdStmt      = stmt;
        hashDsc->csdBlock     = compCurBB;
        hashDsc->csdTreeList  = 0;

        /* Append the entry to the hash bucket */
        
        hashDsc->csdNextInBucket = optCSEhash[hval];
                                   optCSEhash[hval] = hashDsc;
    }
    return 0;

FOUND_NEW_CSE:
    /* We get here only after finding a matching CSE */

    /* Create a new cse (unless we have the maximum already) */

    if  (optCSEcount == EXPSET_SZ)
        return  0;

    CSEindex = ++optCSEcount;
    CSEmask               = genCSEnum2bit(CSEindex);

    /* Record the new cse index in the hashDsc */
    hashDsc->csdIndex                       = CSEindex;

    /* Update the gtCSEnum field in the original tree */
    assert(hashDsc->csdTreeList->tslTree->gtCSEnum == 0);
    hashDsc->csdTreeList->tslTree->gtCSEnum = CSEindex;

#ifdef  DEBUG
    if  (verbose)
    {
        printf("\nCSE candidate #%02u [cost = %d,%d depMask = %s, refMask = ", 
               CSEindex, tree->gtCostEx, tree->gtCostSz, genVS2str(depMask));
        if  (refMask & VR_IND_PTR) printf("Ptr");
        if  (refMask & VR_IND_SCL) printf("Scl");
        if  (refMask  & VR_GLB_VAR)    printf("Glb");
        if  (refMask == VR_INVARIANT)  printf("Inv");
        printf("]:\n");
        gtDispTree(tree);
    }
#endif

    /* Mark all variables this CSE depends on */

    for (bit = 1; (bit <= depMask) && (bit != 0); bit <<= 1)
    {
        if ((bit & depMask) == 0)
            continue;

        lclNum = lvaTrackedToVarNum[genVarBitToIndex(bit)];
        assert(lclNum < lvaCount);
        varDsc = &lvaTable[lclNum];
        assert(varDsc->lvTracked);

        varDsc->lvExpDep |= CSEmask;
        
        if  (varDsc->lvAddrTaken)
            optCSEaddrTakenVar |= CSEmask;
    }

    /* Remember whether the CSE expression contains an indirection/global ref */

    if  (refMask & VR_IND_PTR) optCSEindPtr |= CSEmask;
    if  (refMask & VR_IND_SCL) optCSEindScl |= CSEmask;
    if  (refMask  & VR_GLB_VAR)    optCSEglbRef      |= CSEmask;
    if  (refMask == VR_INVARIANT)  optCSEneverKilled |= CSEmask;

    return  CSEindex;
}

/*****************************************************************************
 *
 *  For a previously marked CSE, decrement the use counts and unmark it
 */

void                Compiler::optUnmarkCSE(GenTreePtr tree)
{
    assert(IS_CSE_INDEX(tree->gtCSEnum));

    unsigned CSEnum = GET_CSE_INDEX(tree->gtCSEnum);
    CSEdsc * desc;

    assert(optCSEweight <= BB_MAX_LOOP_WEIGHT); // make sure it's been initialized

     /* Is this a CSE use? */
    if  (IS_CSE_USE(tree->gtCSEnum))
    {
        desc   = optCSEfindDsc(CSEnum);

#ifdef  DEBUG
        if  (verbose)
            printf("Unmark CSE use #%02d at %08X: %3d -> %3d\n",
                   CSEnum, tree, desc->csdUseCount, desc->csdUseCount - 1);
#endif

        /* Reduce the nested CSE's 'use' count */

        assert(desc->csdUseCount > 0);

        if  (desc->csdUseCount > 0)
        {
            desc->csdUseCount -= 1;

            if (desc->csdUseWtCnt < optCSEweight)
                desc->csdUseWtCnt  = 0;
            else
                desc->csdUseWtCnt -= optCSEweight;
        }
    }
    else
    {
        desc = optCSEfindDsc(CSEnum);

#ifdef  DEBUG
        if  (verbose)
            printf("Unmark CSE def #%02d at %08X: %3d -> %3d\n",
                   CSEnum, tree, desc->csdDefCount, desc->csdDefCount - 1);
#endif

        /* Reduce the nested CSE's 'def' count */

        assert(desc->csdDefCount > 0);

        if  (desc->csdDefCount > 0)
        {
            desc->csdDefCount -= 1;

            if (desc->csdDefWtCnt < optCSEweight)
                desc->csdDefWtCnt  = 0;
            else
                desc->csdDefWtCnt -= optCSEweight;
        }
    }

    tree->gtCSEnum = NO_CSE;
}

/*****************************************************************************
 *
 *  Helper passed to Compiler::fgWalkAllTreesPre() to unmark nested CSE's.
 */

/* static */
Compiler::fgWalkResult      Compiler::optUnmarkCSEs(GenTreePtr tree, void *p)
{
    Compiler *      comp = (Compiler *)p; assert(comp);

    if (tree == comp->optUnmarkCSEtree)
        return WALK_CONTINUE;

    tree->gtFlags |= GTF_DEAD;

    if  (IS_CSE_INDEX(tree->gtCSEnum))
        comp->optUnmarkCSE(tree);

    /* Look for any local variable references */

    if  (tree->gtOper == GT_LCL_VAR)
    {
        unsigned        lclNum;
        LclVarDsc   *   varDsc;

        /* This variable ref is going away, decrease its ref counts */

        lclNum = tree->gtLclVar.gtLclNum;
        assert(lclNum < comp->lvaCount);
        varDsc = comp->lvaTable + lclNum;

        assert(comp->optCSEweight <= BB_MAX_LOOP_WEIGHT); // make sure it's been initialized

        /* Decrement its lvRefCnt and lvRefCntWtd */

        varDsc->decRefCnts(comp->optCSEweight, comp);
    }

    return  WALK_CONTINUE;
}

/*****************************************************************************
 *
 *  Compare function passed to qsort() by optOptimizeCSEs().
 */

/* static */
int __cdecl         Compiler::optCSEcostCmpEx(const void *op1, const void *op2)
{
    CSEdsc *        dsc1 = *(CSEdsc * *)op1;
    CSEdsc *        dsc2 = *(CSEdsc * *)op2;

    GenTreePtr      exp1 = dsc1->csdTree;
    GenTreePtr      exp2 = dsc2->csdTree;

    return  exp2->gtCostEx - exp1->gtCostEx;
}

/*****************************************************************************
 *
 *  Compare function passed to qsort() by optOptimizeCSEs().
 */

/* static */
int __cdecl         Compiler::optCSEcostCmpSz(const void *op1, const void *op2)
{
    CSEdsc *        dsc1 = *(CSEdsc * *)op1;
    CSEdsc *        dsc2 = *(CSEdsc * *)op2;

    GenTreePtr      exp1 = dsc1->csdTree;
    GenTreePtr      exp2 = dsc2->csdTree;

    return  exp2->gtCostSz - exp1->gtCostSz;
}

/*****************************************************************************
 *
 *  Determine the kind of interference for the call.
 */

/* static */ inline
Compiler::callInterf    Compiler::optCallInterf(GenTreePtr call)
{
    assert(call->gtOper == GT_CALL);

    // if not a helper, kills everything
    if  (call->gtCall.gtCallType != CT_HELPER)
        return CALLINT_ALL;

    // setfield and array address store kill all indirections
    switch (eeGetHelperNum(call->gtCall.gtCallMethHnd))
    {
    case CORINFO_HELP_ARRADDR_ST:
    case CORINFO_HELP_SETFIELD32:
    case CORINFO_HELP_SETFIELD64:
    case CORINFO_HELP_SETFIELD32OBJ:
    case CORINFO_HELP_SETFIELDSTRUCT:
        return CALLINT_INDIRS;
    }

    // other helpers kill nothing
    return CALLINT_NONE;
}

/*****************************************************************************
 *
 *  Perform common sub-expression elimination.
 */

void                Compiler::optOptimizeCSEs()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In optOptimizeCSEs()\n");
#endif
    BasicBlock *    block;

    CSEdsc   *   *  ptr;
    unsigned        cnt;

    CSEdsc   *   *  sortTab;
    size_t          sortSiz;

    /* Initialize the expression tracking logic (i.e. the lookup table) */

    optCSEinit();

    /* Initialize the range check tracking logic (i.e. the lookup table) */

    optRngChkInit();

    optDoCSE    = false;    // Stays false until we find duplicate CSE tree
    optDoRngChk = false;    // Stays false until we find duplicate range-check tree

    /* Locate interesting expressions, and range checks and assign indices
     * to them */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        GenTreePtr      stmt;
        GenTreePtr      tree;

        /* Make the block publicly available */

        compCurBB = block;

        /* Walk the statement trees in this basic block */

        for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

#ifdef DEBUG
            if (verbose && 0)
                gtDispTree(stmt);

            /* We check that the nodes are properly marked with the CANT_CSE markers      */
            for (tree = stmt->gtStmt.gtStmtExpr; tree; tree = tree->gtPrev)
            {
                assert(tree->gtCSEnum == NO_CSE);

                if (tree->OperKind() & GTK_ASGOP)
                {
                    /* Targets of assignments are never CSE candidates */
                    assert(tree->gtOp.gtOp1->gtFlags & GTF_DONT_CSE);
                }
                else if (tree->OperGet() == GT_ADDR)
                {
                    /* We can't CSE something hanging below GT_ADDR */
                    assert(tree->gtOp.gtOp1->gtFlags & GTF_DONT_CSE);
                }
                else if (tree->OperGet() == GT_COLON)
                {
                    /* Check that conditionally executed node are marked */
                    fgWalkTreePre(tree, gtAssertColonCond);
                }
            }
#endif

            /* We walk the tree in the forwards direction (bottom up) */
            for (tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
            {
                if  (optIsCSEcandidate(tree))
                {
                    /* Assign an index to this expression */

                    tree->gtCSEnum = optCSEindex(tree, stmt);
                }

                if  (tree->gtOper == GT_IND && (tree->gtFlags & GTF_IND_RNGCHK))
                {
                    /* Assign an index to this range check */

                    tree->gtInd.gtRngChkIndex = optRngChkIndex(tree);
                }
            }
        }
    }

    /* We're done if there were no interesting expressions */

    if  (optDoCSE == false && !optDoRngChk)
        return;

    // We will use this to record if we have found any removal candidates
    bool bRngChkCandidates = false;

    /* We're finished building the expression lookup table */

    optCSEstop();

#ifdef DEBUG
    fgDebugCheckLinks();
#endif

    /* Compute 'gen' and 'kill' sets for all blocks */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        GenTreePtr      stmt;
        GenTreePtr      tree;

        RNGSET_TP       rngGen  = 0;
        RNGSET_TP       rngKill = 0;

        EXPSET_TP       cseGen  = 0;
        EXPSET_TP       cseKill = 0;

        /* Walk the statement trees in this basic block */

        for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

            for (tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
            {
                unsigned treeFlags = tree->gtFlags;

                // Don't create definitions inside of QMARK-COLON trees
                if ((treeFlags & GTF_COLON_COND) == 0)
                {
                    if  (IS_CSE_INDEX(tree->gtCSEnum))
                    {
                        /* An interesting expression is computed here */
                        cseGen |= genCSEnum2bit(tree->gtCSEnum);
                    }

                    if  ((treeFlags                 &  GTF_IND_RNGCHK) && 
                         (tree->gtOper              == GT_IND)         && 
                         (tree->gtInd.gtRngChkIndex != -1))
                    {
                        /* A range check is generated here */
                        rngGen |= genRngnum2bit(tree->gtInd.gtRngChkIndex);
                    }
                }

                if (tree->OperKind() & GTK_ASGOP)
                {
                    /* What is the target of the assignment? */

                    GenTreePtr target = tree->gtOp.gtOp1;

                    switch (target->gtOper)
                    {
                        var_types typ;

                    case GT_CATCH_ARG:
                        break;

                    case GT_LCL_VAR:
                    {
                        /* Assignment to a local variable */

                        unsigned        lclNum = target->gtLclVar.gtLclNum;
                        LclVarDsc   *   varDsc = lvaTable + lclNum;
                        assert(lclNum < lvaCount);

                        /* All dependent exprs are killed here */

                        cseKill |=  varDsc->lvExpDep;
                        cseGen  &= ~varDsc->lvExpDep;

                        /* All dependent range checks are killed here */

                        rngKill |=  varDsc->lvRngDep;
                        rngGen  &= ~varDsc->lvRngDep;

                        /* If the var is aliased, then it could may be
                           accessed indirectly. Kill all indirect accesses */

                        if  (varDsc->lvAddrTaken)
                        {
                            if  (varTypeIsGC(varDsc->TypeGet()))
                            {
                                cseKill |=  optCSEindPtr;
                                cseGen  &= ~optCSEindPtr;

                                rngKill |=  optRngIndPtr;
                                rngGen  &= ~optRngIndPtr;
                            }
                            else
                            {
                                cseKill |=  optCSEindScl;
                                cseGen  &= ~optCSEindScl;

                                rngKill |=  optRngIndScl;
                                rngGen  &= ~optRngIndScl;
                            }
                        }
                        break;
                    }

                    case GT_IND:
                        // If the indirection has the flag GTF_IND_TGTANYWHERE
                        // we could be modifying an aliased local or a global variable

                        if (target->gtFlags & GTF_IND_TGTANYWHERE)
                        {
                            // What type of pointer was used for this indirection?
                            // There are three legal types: TYP_REF, TYP_BYREF or TYP_I_IMPL
                            
                            typ = target->gtInd.gtIndOp1->gtType;

                            // TYP_REF addresses should never have GTF_IND_TGTANYWHERE set
                            assert((typ == TYP_BYREF) || (typ == TYP_I_IMPL));

                            cseKill |=  optCSEaddrTakenVar;
                            cseGen  &= ~optCSEaddrTakenVar;

                            rngKill |=  optRngAddrTakenVar;
                            rngGen  &= ~optRngAddrTakenVar;

                            cseKill |=  optCSEglbRef;
                            cseGen  &= ~optCSEglbRef;
                            
                            rngKill |=  optRngGlbRef;
                            rngGen  &= ~optRngGlbRef;
                        }
                        // FALL THROUGH

                    case GT_LCL_FLD:

                        /* Indirect assignment - kill set is based on type */

                        if  (varTypeIsGC(tree->TypeGet()))
                        {
                            cseKill |=  optCSEindPtr;
                            cseGen  &= ~optCSEindPtr;

                            rngKill |=  optRngIndPtr;
                            rngGen  &= ~optRngIndPtr;
                        }
                        else
                        {
                            cseKill |=  optCSEindScl;
                            cseGen  &= ~optCSEindScl;

                            rngKill |=  optRngIndScl;
                            rngGen  &= ~optRngIndScl;
                        }

                        break;

                    default:

                        /* Must be a static data member (global) assignment */

                        assert(target->gtOper == GT_CLS_VAR);

                        /* This is a global assignment */

                        cseKill |=  optCSEglbRef;
                        cseGen  &= ~optCSEglbRef;

                        rngKill |=  optRngGlbRef;
                        rngGen  &= ~optRngGlbRef;

                        break;
                    }
                }
                else if (tree->gtOper == GT_CALL)
                {
                    switch (optCallInterf(tree))
                    {
                    case CALLINT_ALL:

                        /* Play it safe: method calls kill all normal expressions */

                        cseKill = ~optCSEneverKilled;
                        cseGen  = 0;

                        /* Play it safe: method calls kill most range checks. */

                        rngKill |=  (optRngAddrTakenVar|optRngIndScl|optRngIndPtr|optRngGlbRef);
                        rngGen  &= ~(optRngAddrTakenVar|optRngIndScl|optRngIndPtr|optRngGlbRef);
                        break;

                    case CALLINT_INDIRS:

                        /* Array elem assignment kills all pointer indirections */

                        cseKill |=  optCSEindPtr;
                        cseGen  &= ~optCSEindPtr;

                        rngKill |=  optRngIndPtr;
                        rngGen  &= ~optRngIndPtr;
                        break;

                    case CALLINT_NONE:

                        /* Other helpers kill nothing */

                        break;

                    }
                }
                else if (tree->gtOper == GT_COPYBLK ||
                         tree->gtOper == GT_INITBLK)
                {
                    /* Kill all pointer indirections */

                    if  (tree->gtType == TYP_REF)
                    {
                        cseKill |=  optCSEindPtr;
                        cseGen  &= ~optCSEindPtr;

                        rngKill |=  optRngIndPtr;
                        rngGen  &= ~optRngIndPtr;
                    }
                    else
                    {
                        cseKill |=  optCSEindScl;
                        cseGen  &= ~optCSEindScl;

                        rngKill |=  optRngIndScl;
                        rngGen  &= ~optRngIndScl;
                    }
                }
            }
        }

#ifdef  DEBUG

        if  (verbose)
        {
            if  (!(block->bbFlags & BBF_INTERNAL))
            {
                printf("\nBB%02u", block->bbNum);
                printf(" expGen = %08X", cseGen );
                printf(" expKill= %08X", cseKill);
                printf(" rngGen = %08X", rngGen );
                printf(" rngKill= %08X", rngKill);
            }
        }

#endif

        block->bbExpGen  = cseGen;
        block->bbExpKill = cseKill;

        block->bbExpIn   = 0;

        block->bbRngIn   = 0;

        block->bbRngGen  = rngGen;
        block->bbRngKill = rngKill;
    }

    /* Compute the data flow values for all tracked expressions */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        block->bbExpOut = (EXPSET_TP)((EXPSET_TP)0 - 1);
        block->bbRngOut = (RNGSET_TP)((RNGSET_TP)0 - 1);
    }

    /* Nothing is available on entry to the method */

    fgFirstBB->bbExpOut = fgFirstBB->bbExpGen;
    fgFirstBB->bbRngOut = fgFirstBB->bbRngGen;

    // @TODO [CONSIDER] [04/16/01] []: This should be combined with live variable analysis
    //                                    and/or range check data flow analysis.

    for (;;)
    {
        bool        change = false;

#if DATAFLOW_ITER
        CSEiterCount++;
#endif

        /* Set 'in' to {ALL} in preparation for and'ing all predecessors */

        for (block = fgFirstBB->bbNext; block; block = block->bbNext)
        {
            if (block->bbRefs)
            {
                block->bbExpIn = (EXPSET_TP)((EXPSET_TP)0 - 1);
                block->bbRngIn = (RNGSET_TP)((RNGSET_TP)0 - 1);
            }
            else
            {
                block->bbExpIn = block->bbRngIn = 0;
            }
        }

        /* Visit all blocks and compute new data flow values */

        for (block = fgFirstBB; block; block = block->bbNext)
        {
            EXPSET_TP       cseOut = block->bbExpOut;
            RNGSET_TP       rngOut = block->bbRngOut;

            switch (block->bbJumpKind)
            {
                BasicBlock * *  jmpTab;
                unsigned        jmpCnt;

            case BBJ_THROW:
            case BBJ_RETURN:
                break;

            case BBJ_COND:
                block->bbNext    ->bbExpIn &= cseOut;
                block->bbJumpDest->bbExpIn &= cseOut;

                block->bbNext    ->bbRngIn &= rngOut;
                block->bbJumpDest->bbRngIn &= rngOut;
                break;

            case BBJ_ALWAYS:
                block->bbJumpDest->bbExpIn &= cseOut;
                block->bbJumpDest->bbRngIn &= rngOut;
                break;

            case BBJ_NONE:
                block->bbNext    ->bbExpIn &= cseOut;
                block->bbNext    ->bbRngIn &= rngOut;
                break;

            case BBJ_SWITCH:

                jmpCnt = block->bbJumpSwt->bbsCount;
                jmpTab = block->bbJumpSwt->bbsDstTab;

                do
                {
                    (*jmpTab)->bbExpIn &= cseOut;
                    (*jmpTab)->bbRngIn &= rngOut;
                }
                while (++jmpTab, --jmpCnt);

                break;

            case BBJ_CALL:
                /* Since the finally is conditionally executed, it wont
                   have any useful liveness anyway */

                if (!(block->bbFlags & BBF_RETLESS_CALL))
                {
                    // Only for BBJ_ALWAYS that are associated with the BBJ_CALL
                    block->bbNext    ->bbExpIn &= 0;
                    block->bbNext    ->bbRngIn &= 0;
                }
                break;

            case BBJ_RET:
                break;
            }

        }

        /* Clear everything on entry to filters or handlers */

        for (unsigned XTnum = 0; XTnum < info.compXcptnsCount; XTnum++)
        {
            EHblkDsc * ehDsc = compHndBBtab + XTnum;

            if (ehDsc->ebdFlags & CORINFO_EH_CLAUSE_FILTER)
            {
                ehDsc->ebdFilter->bbExpIn = 0;
                ehDsc->ebdFilter->bbRngIn = 0;
            }
            ehDsc->ebdHndBeg->bbExpIn = 0;
            ehDsc->ebdHndBeg->bbRngIn = 0;
        }

        /* Compute the new 'in' values and see if anything changed */

        for (block = fgFirstBB; block; block = block->bbNext)
        {
            EXPSET_TP       newExpOut;
            RNGSET_TP       newRngOut;

            /* Compute new 'out' exp value for this block */

            newExpOut = block->bbExpOut & ((block->bbExpIn & ~block->bbExpKill) | block->bbExpGen);

            /* Has the 'out' set changed? */

            if  (block->bbExpOut != newExpOut)
            {
                /* Yes - record the new value and loop again */

//              printf("Change exp out of BB%02u from %08X to %08X\n", block->bbNum, (int)block->bbExpOut, (int)newExpOut);

                 block->bbExpOut  = newExpOut;
                 change = true;
            }

            /* Compute new 'out' exp value for this block */

            newRngOut = block->bbRngOut & ((block->bbRngIn & ~block->bbRngKill) | block->bbRngGen);

            /* Has the 'out' set changed? */

            if  (block->bbRngOut != newRngOut)
            {
                /* Yes - record the new value and loop again */

                block->bbRngOut  = newRngOut;
                change = true;
            }
        }

        if  (!change)
            break;
    }

#ifdef  DEBUG

    if  (verbose)
    {
        printf("\n");

        for (block = fgFirstBB; block; block = block->bbNext)
        {
            if  (!(block->bbFlags & BBF_INTERNAL))
            {
                printf("BB%02u", block->bbNum);
                printf(" expIn  = %08X", block->bbExpIn );
                printf(" expOut = %08X", block->bbExpOut);
                printf(" rngIn  = %08X", block->bbRngIn );
                printf(" rngOut = %08X", block->bbRngOut);
                printf("\n");
            }
        }

        printf("\n");

        printf("Pointer indir: rng = %s, exp = %s\n", genVS2str(optRngIndPtr), genVS2str(optCSEindPtr));
        printf("Scalar  indir: rng = %s, exp = %s\n", genVS2str(optRngIndScl), genVS2str(optCSEindScl));
        printf("Global    ref: rng = %s, exp = %s\n", genVS2str(optRngGlbRef), genVS2str(optCSEglbRef));

        printf("\n");
    }

#endif

    /* Now mark any interesting CSE's as such, and
     *     mark any redundant range checks as such
     */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        GenTreePtr      stmt;
        GenTreePtr      tree;

        EXPSET_TP       exp = block->bbExpIn & ~block->bbExpKill;
        RNGSET_TP       rng = block->bbRngIn & ~block->bbRngKill;

        /* Make sure we update the weighted ref count correctly */
        optCSEweight = block->bbWeight;

        /* Insure that the BBF_MARKED flag is clear */
        /* Everyone who uses this flag is required to clear afterwards */
        assert((block->bbFlags & BBF_MARKED ) == 0);

        /* Walk the statement trees in this basic block */

        for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

//          gtDispTree(stmt);

            for (tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
            {
                if  ((tree->gtFlags & GTF_IND_RNGCHK) && tree->gtOper == GT_IND)
                {
                    /* Is this range check redundant? */

#if COUNT_RANGECHECKS
                    optRangeChkAll++;
#endif

                    if  (tree->gtInd.gtRngChkIndex != -1)
                    {
                        RNGSET_TP mask = genRngnum2bit(tree->gtInd.gtRngChkIndex);

                        if  (rng & mask)
                        {
                            /* This range check may be redundant.But we will have to make sure that
                               the array and the index are locals before we remove it (if not, we can
                               get screwed by a malicious race condition). Therefore, we wait until the
                               CSEs get done to see if they made it as locals.
                            */
#ifdef  DEBUG
                            if  (verbose)
                            {
                                printf("Marked redundant range check:\n");
                                gtDispTree(tree);
                                printf("\n");
                            }
#endif
                            // Mark as a candidate for removal
                            assert( (tree->gtFlags & GTF_REDINDEX_CHECK) == 0);
                            tree->gtFlags |= GTF_REDINDEX_CHECK;                            
                            bRngChkCandidates = true;
                        }
                        else
                        {
                            rng |= mask;
                        }
                    }
                }
                
                if  (IS_CSE_INDEX(tree->gtCSEnum))
                {
                    EXPSET_TP   mask;
                    CSEdsc   *  desc;
                    unsigned    stmw = block->bbWeight;

                    /* Is this expression available here? */

                    mask = genCSEnum2bit(tree->gtCSEnum);
                    desc = optCSEfindDsc(tree->gtCSEnum);

                    /* Is this expression available here? */

                    if  (exp & mask)
                    {
                        /* This is a CSE use */

                        desc->csdUseCount += 1;
                        desc->csdUseWtCnt += stmw;

                    }
                    else
                    {
                        if (tree->gtFlags & GTF_COLON_COND)
                        {
                            // We can't use definitions that occur inside QMARK-COLON trees
                            tree->gtCSEnum = NO_CSE;
                        }
                        else
                        {
                            /* This is a CSE def */

                            desc->csdDefCount += 1;
                            desc->csdDefWtCnt += stmw;
                            
                            /* This CSE will be available after this def */
                            
                            exp |= mask;
                            
                            /* Mark the node as a CSE definition */
                            
                            tree->gtCSEnum = TO_CSE_DEF(tree->gtCSEnum);
                        }
                    }
#ifdef DEBUG
                    if (verbose && IS_CSE_INDEX(tree->gtCSEnum))
                    {
                        printf("[%08X] %s of CSE #%u [weight=%2u%s]\n",
                               tree,
                               IS_CSE_USE(tree->gtCSEnum) ? "Use" : "Def",
                               GET_CSE_INDEX(tree->gtCSEnum), 
                               stmw/BB_UNITY_WEIGHT, (stmw & 1) ? ".5" : "");
                    }
#endif
                }

                if (tree->OperKind() & GTK_ASGOP)
                {
                    /* What is the target of the assignment? */

                    GenTreePtr target = tree->gtOp.gtOp1;

                    switch (target->gtOper)
                    {
                        var_types typ;

                    case GT_CATCH_ARG:
                        break;

                    case GT_LCL_VAR:
                    {
                        /* Assignment to a local variable */

                        unsigned        lclNum = target->gtLclVar.gtLclNum;
                        LclVarDsc   *   varDsc = lvaTable + lclNum;
                        assert(lclNum < lvaCount);

                        /* All dependent expressions and range checks are killed here */

                        exp &= ~varDsc->lvExpDep;
                        rng &= ~varDsc->lvRngDep;

                        /* If the var is aliased, then it could may be
                           accessed indirectly. Kill all indirect accesses */

                        if  (varDsc->lvAddrTaken)
                        {
                            if  (varTypeIsGC(varDsc->TypeGet()))
                            {
                                exp &= ~optCSEindPtr;
                                rng &= ~optRngIndPtr;
                            }
                            else
                            {
                                exp &= ~optCSEindScl;
                                rng &= ~optRngIndScl;
                            }
                        }
                        break;
                    }

                    case GT_IND:
                        // If the indirection has the flag GTF_IND_TGTANYWHERE
                        // we could be modifying an aliased local or a global variable

                        if (target->gtFlags & GTF_IND_TGTANYWHERE)
                        {
                            // What type of pointer was used for this indirection?
                            // There are three legal types: TYP_REF, TYP_BYREF or TYP_I_IMPL
                            
                            typ = target->gtInd.gtIndOp1->gtType;

                            // TYP_REF addresses should never have GTF_IND_TGTANYWHERE set
                            assert((typ == TYP_BYREF) || (typ == TYP_I_IMPL));

                            exp &= ~optCSEaddrTakenVar;
                            rng &= ~optRngAddrTakenVar;

                            exp &= ~optCSEglbRef;
                            rng &= ~optRngGlbRef;
                        }
                        // FALL THROUGH

                    case GT_LCL_FLD:

                        /* Indirect assignment - kill set is based on type */

                        if  (varTypeIsGC(tree->TypeGet()))
                        {
                            exp &= ~optCSEindPtr;
                            rng &= ~optRngIndPtr;
                        }
                        else
                        {
                            exp &= ~optCSEindScl;
                            rng &= ~optRngIndScl;
                        }
                        break;

                    default:

                        /* Must be a static data member (global) assignment */

                        assert(target->gtOper == GT_CLS_VAR);

                        /* This is a global assignment */

                        exp &= ~optCSEglbRef;
                        rng &= ~optRngGlbRef;

                        break;
                    }
                }
                else if (tree->gtOper == GT_CALL)
                {
                    switch (optCallInterf(tree))
                    {
                    case CALLINT_ALL:

                        /* All exprs, range checks are killed here */

                        exp &= 0;
                        rng &= ~(optRngAddrTakenVar|optRngIndScl|optRngIndPtr|optRngGlbRef);
                        break;

                    case CALLINT_INDIRS:

                        /* Array elem assignment kills all indirect exprs */

                        exp &= ~optCSEindPtr;
                        rng &= ~optRngIndPtr;
                        break;

                    case CALLINT_NONE:

                        /* other helpers kill nothing */

                        break;
                    }
                }
                else if (tree->gtOper == GT_COPYBLK ||
                         tree->gtOper == GT_INITBLK)
                {
                    // Due to aliasing, assume all indirect exprs as killed

                    exp &= ~(optCSEindPtr | optCSEindScl);
                    rng &= ~(optRngIndPtr | optRngIndScl);
                }
            }
        }
    }

    if (optCSEcount)
    {
        /* Create an expression table sorted by decreasing cost */    
        sortTab = (CSEdsc **)compGetMemArray(optCSEcount, sizeof(*sortTab));
        sortSiz = optCSEcount * sizeof(*sortTab);
        memcpy(sortTab, optCSEtab, sortSiz);

        if (compCodeOpt() == SMALL_CODE)
            qsort(sortTab, optCSEcount, sizeof(*sortTab), optCSEcostCmpSz);
        else
            qsort(sortTab, optCSEcount, sizeof(*sortTab), optCSEcostCmpEx);

        unsigned  addCSEcount = 0;  /* Count of the number of CSE's added */

        /* Consider each CSE candidate, in order of decreasing cost */
        for (cnt = optCSEcount, ptr = sortTab;
            cnt;
            cnt--            , ptr++)
        {
            CSEdsc   *     dsc  = *ptr;
            GenTreePtr     expr = dsc->csdTree;
            unsigned       def  = dsc->csdDefWtCnt; // weighted def count
            unsigned       use  = dsc->csdUseWtCnt; // weighted use count (excluding the implicit uses at defs)
            unsigned       cost;

        if (compCodeOpt() == SMALL_CODE)
            cost = expr->gtCostSz;
        else
            cost = expr->gtCostEx;

    #ifdef  DEBUG
            if  (verbose)
            {
                printf("CSE #%02u [def=%2d, use=%2d, cost=%2u]:\n", 
                    dsc->csdIndex, def, use, cost);
            }
    #endif

            /* Assume we won't make this candidate into a CSE */
            dsc->csdVarNum = 0xFFFF;

            if (use == 0)
                continue;

    #ifdef DEBUG
            static ConfigDWORD fJitNoCSE(L"JitNoCSE", 0);
            if (fJitNoCSE.val())
                continue;
    #endif

            if ((dsc->csdDefCount <= 0) || (dsc->csdUseCount == 0))        
            {
                /* If this assert fires then the CSE def was incorrectly unmarked! */
                /* or the block with this use is unreachable!                      */
                assert(!"CSE def was incorrectly unmarked!");
            
                // We shouldn't ever reach this. #72161 hits it.It was an extreme corner
                // case, but would have generated bad code in retail. Therefore in addition to the assert
                // we will bail out if something seems wrong and at least not generate bad code.
                continue;
            }
            
            /* Our calculation is based on the following cost estimate formula

            Existing costs are:
                
            (def + use) * cost

            If we introduce a CSE temp are each definition and
            replace the use with a CSE temp then our cost is:

            (def * (cost + cse-def-cost)) + (use * cse-use-cost)

            We must estimate the values to use for cse-def-cost and cse-use-cost

            If we are able to enregister the CSE then the cse-use-cost is one 
            and cse-def-cost is either zero or one.  Zero in the case where 
            we needed to evaluate the def into a register an we can use that
            register as the CSE temp as well.

            If we are unable to enregister the CSE then the cse-use-cost is IND_COST
            and the cse-def-cost is also IND_COST.

            If we want to be pessimistic we could use IND_COST as the the value
            for both cse-def-cost and cse-use-cost and then we would never introduce
            a CSE that could pessimize the execution time of the method.

            However that is more pessimistic that the CSE rules that we were previously using.

            Since we typically can enregister the CSE, but we don't want to be too
            aggressive here, I have choosen to use:

            (IND_COST_EX + 1) / 2 as the values for both cse-def-cost and cse-use-cost.

            */

            unsigned cse_def_cost;
            unsigned cse_use_cost;

            if (compCodeOpt() == SMALL_CODE)
            {
                /* The following formula is good choice when optimizing CSE for SMALL_CODE */
                cse_def_cost = 3;  // mov disp[EBP],reg
                cse_use_cost = 2;  //     disp[EBP]
            }
            else
            {
                cse_def_cost = 2;  // (IND_COST_EX + 1) / 2;
                cse_use_cost = 2;  // (IND_COST_EX + 1) / 2;
            }

            /* no_cse_cost  is the cost estimate when we decide not to make a CSE */
            /* yes_cse_cost is the cost estimate when we decide to make a CSE     */
            
            unsigned no_cse_cost  = use * cost;
            unsigned yes_cse_cost = (def * cse_def_cost) + (use * cse_use_cost);

            /* Does it cost us more to make this expression a CSE? */
            if  (yes_cse_cost > no_cse_cost)
            {
                /* In stress mode we will make some extra CSEs */
                if (no_cse_cost > 0)
                {
                    int  percentage  = (no_cse_cost * 100) / yes_cse_cost;

                    if (compStressCompile(STRESS_MAKE_CSE, percentage))
                    {
                        goto YES_CSE;
                    }
                }

                goto NOT_CSE;
            }

    #if CSELENGTH
            if  (!(expr->gtFlags & GTF_MAKE_CSE))
            {
                /* Is this an array length expression? */

                if  (expr->gtOper == GT_ARR_LENREF)
                {
                    /* There better be good use for this one */
                    
                    /* @TODO [CONSIDER] [04/16/01] []: using yes_cse_cost > no_cse_cost*3 */
                    if  (use < def*3)
                        goto NOT_CSE;
                }
            }
    #endif

    YES_CSE:
            /* We'll introduce a new temp for the CSE */
            unsigned   tmp       = lvaGrabTemp(false);
            var_types  tmpTyp    = genActualType(expr->TypeGet());

            lvaTable[tmp].lvType = tmpTyp;
            dsc->csdVarNum       = tmp;
            addCSEcount++;

    #ifdef  DEBUG
            if  (verbose)
            {
                printf("Promoting CSE [temp=%u]:\n", tmp);
                gtDispTree(expr);
                printf("\n");
            }
    #endif

            /*  Walk all references to this CSE, adding an assignment
                to the CSE temp to all defs and changing all refs to
                a simple use of the CSE temp.
                
                We also unmark nested CSE's for all uses.
            */

            treeStmtLstPtr lst = dsc->csdTreeList; assert(lst);
            
            do
            {
                /* Process the next node in the list */
                GenTreePtr    exp = lst->tslTree;
                GenTreePtr    stm = lst->tslStmt; assert(stm->gtOper == GT_STMT);
                BasicBlock *  blk = lst->tslBlock;

                /* Advance to the next node in the list */
                lst = lst->tslNext;
                
                /* Ignore the node if it's part of a removed CSE */
                if  (exp->gtFlags & GTF_DEAD)
                    continue;
                
                /* Ignore the node if it's not been marked as a CSE */
                
                if  (!IS_CSE_INDEX(exp->gtCSEnum))
                    continue;
                
                /* Make sure we update the weighted ref count correctly */
                optCSEweight = blk->bbWeight;

                /* Set the BBF_MARKED flag */
                blk->bbFlags |= BBF_MARKED;

                /* Set the GTF_STMT_HAS_CSE */
                stm->gtFlags |= GTF_STMT_HAS_CSE;

                /* Figure out the actual type of the value */
                var_types typ = genActualType(exp->TypeGet());
                assert(typ == tmpTyp);
                
                if  (IS_CSE_USE(exp->gtCSEnum))
                {
                    /* This is a use of the CSE */
    #ifdef  DEBUG
                    if  (verbose)
                        printf("CSE #%02u use at %08X replaced with temp use.\n", 
                            exp->gtCSEnum, exp);
    #endif

    #if CSELENGTH
                    /* Array length use CSE's are handled differently */
                    
                    if  (exp->gtOper == GT_ARR_LENREF)
                    {
                        /* Store the CSE use under the arrlen node */
                        GenTreePtr ref = gtNewLclvNode(tmp, typ);
                        ref->gtLclVar.gtLclILoffs   = BAD_IL_OFFSET;
                        
                        exp->gtArrLen.gtArrLenCse   = ref;
                    }
                    else
    #endif
                    {
                        /* Unmark any nested CSE's in the sub-operands */
                        
                        assert(!optUnmarkCSEtree);
                        optUnmarkCSEtree = exp;
       
                        fgWalkTreePre(exp, optUnmarkCSEs, (void*)this);

    #ifdef DEBUG
                        optUnmarkCSEtree = NULL;
    #endif

                        /* Replace the ref with a simple use of the temp */

                        exp->SetOperResetFlags(GT_LCL_VAR);
    #ifdef DEBUG
                        exp->gtFlags               |= GTFD_VAR_CSE_REF;
    #endif
                        exp->gtType                 = typ;
                        exp->gtLclVar.gtLclNum      = tmp;
                        exp->gtLclVar.gtLclILoffs   = BAD_IL_OFFSET;
                        
                    }

                    // Increment ref count
                    lvaTable[tmp].incRefCnts(blk->bbWeight, this);
                }
                else
                {
                    /* This is a def of the CSE */
    #ifdef  DEBUG
                    if  (verbose)
                        printf("CSE #%02u def at %08X replaced with def of V%02u\n",
                            GET_CSE_INDEX(exp->gtCSEnum), exp, tmp);
    #endif
                    /* Make a copy of the expression */
                    GenTreePtr  val;
    #if CSELENGTH
                    if  (exp->gtOper == GT_ARR_LENREF)
                    {
                        /* Use a "nothing" node to prevent cycles */
                        
                        val          = gtNewNothingNode();
                        val->gtType  = exp->TypeGet();
                    }
                    else
    #endif
                    {
                        val = gtNewNode(exp->OperGet(), typ);
                        val->CopyFrom(exp);

                        // We dont want to duplicate the def flag, as it can
                        // cause grief while removing nested CSEs
                        val->gtCSEnum = NO_CSE;

                    }

                    /* Create an assignment of the value to the temp */
                    GenTreePtr  asg = gtNewTempAssign(tmp, val);

                    assert(asg->gtOp.gtOp1->gtOper == GT_LCL_VAR);
                    assert(asg->gtOp.gtOp2         == val);
                    
                    /* Create a reference to the CSE temp */
                    GenTreePtr  ref = gtNewLclvNode(tmp, typ);

                    if  (exp->gtPrev == NULL)
                    {
                        assert(stm->gtStmt.gtStmtList == exp);
                        stm->gtStmt.gtStmtList = val;
                    }

    #if CSELENGTH
                    if  (exp->gtOper == GT_ARR_LENREF)
                    {
                        /* Create a comma node for the CSE assignment */
                        GenTreePtr cse = gtNewOperNode(GT_COMMA, typ, asg, ref);

                        /* Record the CSE expression in the array length node */
                        exp->gtArrLen.gtArrLenCse = cse;
                    }
                    else
    #endif
                    {
                        /* Change the expression to "(tmp=val),tmp" */
                        
                        exp->SetOper(GT_COMMA);
                        exp->gtFlags   &= ~GTF_REVERSE_OPS;
                        exp->gtType     = typ;
                        exp->gtOp.gtOp1 = asg;
                        exp->gtOp.gtOp2 = ref;
                        
                    }

                    // Increment ref count for both sides of the comma
                    lvaTable[tmp].incRefCnts(blk->bbWeight, this);
                    lvaTable[tmp].incRefCnts(blk->bbWeight, this);
                }

    #ifdef  DEBUG
                if  (verbose)
                {
                    printf("CSE transformed:\n");
                    gtDispTree(exp);
                    printf("\n");
                }
    #endif
            }
            while (lst);

            continue;

        NOT_CSE:

            /*
                Last-ditch effort to get some benefit out of this CSE. Consider
                the following code:

                    int cse(int [] a, int i)
                    {
                        if  (i > 0)
                        {
                            if  (a[i] < 0)  // def of CSE
                                i = a[i];   // use of CSE
                        }
                        else
                        {
                            if  (a[i] > 0)  // def of CSE
                                i = 0;
                        }

                        return  i;
                    }

                We will see 2 defs and only 1 use of the CSE a[i] in the method
                but it's still a good idea to make the first def and use into a
                CSE.
            */

            /* @TODO [BROKEN] [04/16/01] []: The logic here is broken. It tries to salvage a CSE 
            selectively without following the control flow when a CSE is considered worthless. It does
            not seem to have any noticeable effects on benchmark perfs and so the 
            safest thing is to turn it off for now (bug # 38977). 
            */
            if (FALSE && dsc->csdTreeList && expr->gtCostEx > 3)
            {
                bool            fnd = false;
                treeStmtLstPtr  lst;

                /* Look for a definition followed by a use "nearby" */

                lst = dsc->csdTreeList;

    //          fgDispBasicBlocks( true);
    //          fgDispBasicBlocks(false);

                do
                {
                    GenTreePtr      stm;
                    treeStmtLstPtr  tmp;
                    BasicBlock  *   beg;
                    int             got;

                    /* Get the next node in the list */

                    expr = lst->tslTree; assert(expr);

                    /* Ignore the node if it's part of a removed CSE */

                    if  (expr->gtFlags & GTF_DEAD)
                        goto NEXT_NCSE;

                    /* Is this a CSE definition? */

                    if  (IS_CSE_DEF(expr->gtCSEnum))
                    {
                        /* Disable this CSE, it looks hopeless */

                        expr->gtCSEnum = NO_CSE;
                    }

                    /* If this CSE has been disabled then skip to next */

                    if (!IS_CSE_INDEX(expr->gtCSEnum))
                        goto NEXT_NCSE;

                    /* Now look for any uses that immediately follow */

                    stm = lst->tslStmt; assert(stm->gtOper == GT_STMT);
                    beg = lst->tslBlock;

                    /* If the block doesn't flow into its successor, give up */

                    if  (beg->bbJumpKind != BBJ_NONE &&
                        beg->bbJumpKind != BBJ_COND)
                    {
                        /* Disable this CSE, it looks hopeless */

                        expr->gtCSEnum = NO_CSE;
                        goto NEXT_NCSE;
                    }

    //              printf("CSE def %08X (cost=%2u) in stmt %08X of BB%02u\n", expr, expr->gtCostEx, stm, beg->bbNum);

                    got = -expr->gtCostEx;

                    for (tmp = lst->tslNext; tmp; tmp = tmp->tslNext)
                    {
                        GenTreePtr      nxt;
                        BasicBlock  *   blk;
                        unsigned        ben;

                        nxt = tmp->tslTree;

                        /* Ignore it if it's part of a removed CSE */

                        if  (nxt->gtFlags & GTF_DEAD)
                            continue;

                        /* Skip if this a CSE def? */

                        if  (IS_CSE_DEF(nxt->gtCSEnum))
                            break;

                        /* We'll be computing the benefit of the CSE */

                        ben = nxt->gtCostEx;

                        /* Is this CSE in the same block as the def? */

                        blk = tmp->tslBlock;

                        if  (blk != beg)
                        {
                            unsigned        lnum;
                            LoopDsc     *   ldsc;

                            /* Does the use immediately follow the def ? */

                            if  (beg->bbNext != blk)
                                break;

                            if  (blk->isLoopHead())
                            {
                                /* Is 'beg' right in front of a loop? */

                                for (lnum = 0, ldsc = optLoopTable;
                                    lnum < optLoopCount;
                                    lnum++  , ldsc++)
                                {
                                    if  (beg == ldsc->lpHead)
                                    {
                                        // UNDONE: Make sure no other defs within loop

                                        ben *= 4;
                                        goto CSE_HDR;
                                    }
                                }

                                break;
                            }
                            else
                            {
                                /* Does any other block jump to the use ? */

                                if  (blk->bbRefs > 1)
                                    break;
                            }
                        }

                    CSE_HDR:

    //                  printf("CSE use %08X (ben =%2u) in stmt %08X of BB%02u\n", nxt, ben, tmp->tslStmt, blk->bbNum);

                        /* This CSE use is reached only by the above def */

                        got += ben;
                    }

    //              printf("Estimated benefit of CSE = %u\n", got);

                    /* Did we find enough CSE's worth keeping? */

                    if  (got > 0)
                    {
                        /* Skip to the first acceptable CSE */

                        lst = tmp;

                        /* Remember that we've found something worth salvaging */

                        fnd = true;
                    }
                    else
                    {
                        /* Disable all the worthless CSE's we've just seen */

                        do
                        {
                            lst->tslTree->gtCSEnum = NO_CSE;
                        }
                        while ((lst = lst->tslNext) != tmp);
                    }

                    continue;

                NEXT_NCSE:

                    lst = lst->tslNext;
                }
                while (lst);

                /* If we've kept any of the CSE defs/uses, go process them */

                if  (fnd)
                    goto YES_CSE;
            }
        }

        /* Did we end up creating any CSE's ? */
        if  (addCSEcount > 0)
        {
            for (block = fgFirstBB; block; block = block->bbNext)
            {
                /* If the BBF_MARKED flag isn't set then skip this block   */
                /* If the block has any CSE subsitutions we set BBF_MARKED */
                if ((block->bbFlags & BBF_MARKED) == 0)
                    continue;

                /* Clear the BBF_MARKED flag */
                block->bbFlags &= ~BBF_MARKED;

                fgRemoveRestOfBlock = false;

                for (GenTreePtr stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
                {
                    assert(stmt->gtOper == GT_STMT);

                    /* If the GTF_STMT_HAS_CSE flag isn't set then skip this stmt   */
                    /* If the stmt has any CSE subsitutions we set GTF_STMT_HAS_CSE */
                    if (stmt->gtFlags & GTF_STMT_HAS_CSE)
                    {
                        /* re-morph the statement */
                        optMorphTree(block, stmt DEBUGARG("optOptimizeCSEs"));
                    }
                }
            }

    #ifdef DEBUG
            if (verbose)
                printf("\n");
    #endif        

            /* We've added new local variables to the lvaTable so recreate the sorted table */
            lvaSortByRefCount();
        }
    }

    // Remove reduntant range checks
    if (bRngChkCandidates)
    {
        #ifdef  DEBUG
        if  (verbose)
        {
            printf("\nChecking redundant range checks:\n");
        }
        #endif

        for (block = fgFirstBB; block; block = block->bbNext)
        {
            GenTreePtr      stmt;
            GenTreePtr      tree;

            /* Make the block publicly available */

            compCurBB = block;

            /* Walk the statement trees in this basic block */
            for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
            {
                assert(stmt->gtOper == GT_STMT);

                /* We walk the tree in the forwards direction (bottom up) */
                for (tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
                {                            
                    if  (tree->gtFlags & GTF_REDINDEX_CHECK)
                    {
                        // Remove range check flag (GT_IND could have been morphed to a comma in CSEs)
                        tree->gtFlags &= ~GTF_REDINDEX_CHECK;

                        // If it's still a range check, see if we can remove it
                        if  ((tree->gtFlags & GTF_IND_RNGCHK) && tree->gtOper == GT_IND)
                        {
                            if (optIsRangeCheckRemovable(tree))
                            {
                                optRemoveRangeCheck(tree, stmt, true);

                                #if COUNT_RANGECHECKS
                                    optRangeChkRmv++;
                                #endif

                                #ifdef  DEBUG
                                if  (verbose)
                                {
                                    printf("\nAfter eliminating redundant range check:\n");
                                    gtDispTree(tree);
                                    printf("\n");
                                }
                                #endif
                            }
                            else
                            {
                                #ifdef  DEBUG
                                if  (verbose)
                                {
                                    printf("\nCouldn't eliminate range check because the index or the array aren't locals:\n");
                                    gtDispTree(tree);
                                    printf("\n");
                                }
                                #endif
                            }                        
                        }
                    }
                    assert ((tree->gtFlags & GTF_REDINDEX_CHECK)==0);
                }
            }
        }
    }
}

/*****************************************************************************/
#endif  // CSE
/*****************************************************************************/

#if ASSERTION_PROP

/*****************************************************************************
 *
 *  Initialize the assertion prop tracking logic.
 */

void                Compiler::optAssertionInit()
{
    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    for (lclNum = 0, varDsc = lvaTable;
         lclNum < lvaCount;
         lclNum++  , varDsc++)
    {
        varDsc->lvAssertionDep = 0;
    }

    optAssertionCount      = 0;
    optAssertionPropagated = false;

}


/*****************************************************************************
 *
 *  Helper passed to Compiler::fgWalkTreePre() to find the Asgn node for optAddCopies()
 */

/* static */
Compiler::fgWalkResult  Compiler::optAddCopiesCallback(GenTreePtr tree, void *p)
{
    assert(p);

    if (tree->OperKind() & GTK_ASGOP)
    {
        GenTreePtr op1  = tree->gtOp.gtOp1;
        Compiler * comp = (Compiler*) p;

        if ((op1->gtOper == GT_LCL_VAR) &&
            (op1->gtLclVar.gtLclNum == comp->optAddCopyLclNum))
        {
            comp->optAddCopyAsgnNode = tree;
            return WALK_ABORT;
        }
    }
    return WALK_CONTINUE;
}

                
/*****************************************************************************
 *
 *  Add new copies before Assertion Prop.
 */

void                Compiler::optAddCopies()
{
    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    for (lclNum = 0, varDsc = lvaTable;
         lclNum < lvaCount;
         lclNum++  , varDsc++)
    {
        var_types   typ = varDsc->TypeGet();

        // We only add copies for non temp local variables 
        // that have a single def and that can possibly be enregistered

        if (varDsc->lvIsTemp || !varDsc->lvSingleDef || !varTypeCanReg(typ))
            continue;

        /* For lvNormalizeOnLoad(), we need to add a cast to the copy-assg
           like "copyLclNum = int(varDsc)" and optAssertionAdd() only
           tracks simple assignments. The same goes for lvNormalizedOnStore as
           the cast is generated in fgMorphSmpOpAsg. This boils down to not having
           a copy until optAssertionAdd handles this*/

        if (varDsc->lvNormalizeOnLoad() || varDsc->lvNormalizeOnStore() || typ == TYP_BOOL)
            continue;
            
        if (varTypeIsSmall(varDsc->TypeGet()) || typ == TYP_BOOL)
            continue;

        const unsigned significant  = (BB_LOOP_WEIGHT * BB_UNITY_WEIGHT / 2);
        bool           isFloatParam = varDsc->lvIsParam && varTypeIsFloating(typ);

        // We require that the weighted ref count be significant 
        // and that we have a lvVolatile variable or floating point parameter
        
        if ((varDsc->lvRefCntWtd > significant) &&
            (isFloatParam || varDsc->lvVolatileHint))
        {
            // We must have a ref in a block that is dominated only by the entry block
            unsigned useWtd      = varDsc->lvRefCntWtd / (varDsc->lvRefCnt * 2); 
            unsigned useDom      = -1;      // Initial value is all blocks
            unsigned blkNum      = 1;
            unsigned blkMask     = 1;
            unsigned refMask     = varDsc->lvRefBlks;
            bool     isCandidate = false;

            while (refMask)
            {
                if (refMask & blkMask)
                {
                    /* Find the block 'blkNum' */
                    BasicBlock * block = fgFirstBB;
                    while (block && (block->bbNum != blkNum))
                    {
                        block = block->bbNext;
                    }
                    assert(block && (block->bbNum == blkNum));

                    if (block->bbDom != NULL)
                    {
                        /* Is this block dominated by fgFirstBB? */
                        if (block->bbDom[0] & 1)
                            isCandidate = true;
                    }

                    if (varDsc->lvIsParam && (block->bbWeight > useWtd) && (block->bbDom != NULL))
                    {
                        /* Keep track of the blocks which dominate uses of this variable */
                        useDom &= block->bbDom[0]; // Clear blocks that do not dominate
                    }

                    /* remove this blkMask from refMask */
                    refMask -= blkMask;
                }
                /* Setup for the next block */
                blkNum   += 1;
                blkMask <<= 1;
            }

            assert(!varDsc->lvIsParam || (useDom != -1));

            // We require that we have a lvVolatile variable or floating point parameter

            if (isFloatParam || (isCandidate && varDsc->lvVolatileHint))
            {
                GenTreePtr stmt;
                unsigned copyLclNum = lvaGrabTemp(false);

					// Because lvaGrabTemp may have reallocated the lvaTable, insure varDsc
					// is still in sync with lvaTable[lclNum];
				varDsc = &lvaTable[lclNum];
            
                // Set lvType on the new Temp Lcl Var
                lvaTable[copyLclNum].lvType = typ;

                if (varDsc->lvIsParam)
                {
                    assert(varDsc->lvDefStmt == NULL);

                    // Create a new copy assignment tree
                    GenTreePtr copyAsgn = gtNewTempAssign(copyLclNum, gtNewLclvNode(lclNum, typ));

                    /* create the new assignment stmt */
                    stmt = gtNewStmt(copyAsgn);

                    /* Find the best block to insert the new assignment    */
                    /* We will choose the lowest weighted block and within */
                    /* those block the highest numbered block which        */
                    /* dominates all the uses of the local variable        */

                    /* Our default is to use the first block */
                    BasicBlock * bestBlock  = fgFirstBB;
                    unsigned     bestWeight = bestBlock->bbWeight;
                    BasicBlock * block      = bestBlock;

                    blkNum  = 2;
                    blkMask = 2;

                    /* We have already calculated useDom above. The two comparisons in the 'while'
                       condition are needed to handle the useDom=0x80...00 case */

                    while (blkMask != 0 && useDom >= blkMask)
                    {
                        /* Is this block one that dominate the useDom? */
                        if (useDom & blkMask)
                        {
                            /* Advance block to point to 'blkNum' */
                            while (block && (block->bbNum != blkNum))
                            {
                                block = block->bbNext;
                            }
                            assert(block && (block->bbNum == blkNum));

                            /* Does this block have a good bbWeight value? */

                            if (block->bbWeight <= bestWeight)
                            {
                                bestBlock  = block;
                                bestWeight = block->bbWeight;
                            }
                        }
                        blkNum   += 1;
                        blkMask <<= 1;
                    }
                    blkMask = 1 << (bestBlock->bbNum-1);

                    /* If there is a use of the variable in this block */
                    /* then we insert the assignment at the beginning  */
                    /* otherwise we insert the statement at the end    */

                    if ((useDom == 0) || (blkMask & varDsc->lvRefBlks))
                        fgInsertStmtAtBeg(bestBlock, stmt);
                    else
                        fgInsertStmtNearEnd(bestBlock, stmt);

                    /* Increment its lvRefCnt and lvRefCntWtd */
                    lvaTable[lclNum].incRefCnts(fgFirstBB->bbWeight, this);

                    /* Increment its lvRefCnt and lvRefCntWtd */
                    lvaTable[copyLclNum].incRefCnts(fgFirstBB->bbWeight, this);
                }
                else
                {
                    assert(varDsc->lvDefStmt != NULL);

                    /* Locate the assignment in the lvDefStmt */
                    stmt = varDsc->lvDefStmt;
                    assert(stmt->gtOper == GT_STMT);

                    optAddCopyLclNum   = lclNum; // in
                    optAddCopyAsgnNode = NULL;   // out
                
                    fgWalkTreePre(stmt->gtStmt.gtStmtExpr, 
                                  Compiler::optAddCopiesCallback, 
                                  (void *) this, 
                                  false);

                    assert(optAddCopyAsgnNode);

                    GenTreePtr tree = optAddCopyAsgnNode;
                    GenTreePtr op1  = tree->gtOp.gtOp1;

                    assert( tree && op1                   &&
                           (tree->OperKind() & GTK_ASGOP) &&
                           (op1->gtOper == GT_LCL_VAR)    &&
                           (op1->gtLclVar.gtLclNum == lclNum));

                    /* Bug: BB_UNITY_WEIGHT really shopuld be the block's weight */
                    unsigned   blockWeight = BB_UNITY_WEIGHT;

                    /* Increment its lvRefCnt and lvRefCntWtd twice */
                    lvaTable[copyLclNum].incRefCnts(blockWeight, this);
                    lvaTable[copyLclNum].incRefCnts(blockWeight, this);

                    /* Assign the old expression into the new temp */

                    GenTreePtr newAsgn  = gtNewTempAssign(copyLclNum, tree->gtOp.gtOp2);

                    /* Copy the new temp to op1 */

                    GenTreePtr copyAsgn = gtNewAssignNode(op1, gtNewLclvNode(copyLclNum, typ));

                    /* Bash the tree to a GT_COMMA with the two assignments as child nodes */

                    tree->gtBashToNOP();
                    tree->ChangeOper(GT_COMMA);

                    tree->gtOp.gtOp1  = newAsgn;
                    tree->gtOp.gtOp2  = copyAsgn;

                    tree->gtFlags    |= ( newAsgn->gtFlags & GTF_GLOB_EFFECT);
                    tree->gtFlags    |= (copyAsgn->gtFlags & GTF_GLOB_EFFECT);
                }
#ifdef DEBUG
                if  (verbose)
                {
                    printf("Introducing a new copy for V%02u\n", lclNum);
                    gtDispTree(stmt->gtStmt.gtStmtExpr);
                    printf("\n");
                }
#endif
            }
        }
    }
}


/*****************************************************************************
 *
 *  If this statement creates a value assignment or assertion
 *  then assign an index to the given value assignment by adding
 *  it to the lookup table, if necessary. 
 */

void                Compiler::optAssertionAdd(GenTreePtr tree, 
                                              bool       localProp)
{
    unsigned     index;
    GenTreePtr   op1;
    GenTreePtr   op2;
    unsigned     op1LclNum;
    LclVarDsc *  op1LclVar;
    unsigned     op2LclNum;
    LclVarDsc *  op2LclVar;
    bool         addCns;

#ifdef DEBUG
    index = 0xbaadf00d;
#endif

    if (tree->gtOper == GT_ASG)
    {
        op1 = tree->gtOp.gtOp1;
        op2 = tree->gtOp.gtOp2;

        /* If we are not assigning to a local variable then bail */
        if (op1->gtOper != GT_LCL_VAR)
            goto SKIP_ASG;

        op1LclNum = op1->gtLclVar.gtLclNum; assert(op1LclNum  < lvaCount);
        op1LclVar = &lvaTable[op1LclNum];

        /* If the local variable has its address taken then bail */
        if (op1LclVar->lvAddrTaken)
            goto SKIP_ASG;

        switch (op2->gtOper)
        {
            var_types toType;
            int       loVal;
            int       hiVal;

        case GT_CNS_INT:
        case GT_CNS_LNG:
        case GT_CNS_DBL:

            /* Assignment of a constant */

            if (op1->gtType != op2->gtType)
                goto NO_ASSERTION;

#if !PROP_ICON_FLAGS
            if ((op2->gtOper == GT_CNS_INT) && (op2->gtFlags & GTF_ICON_HDL_MASK))
                goto NO_ASSERTION;
#endif

            /* Check to see if the assertion is already recorded in the table */

            for (index=0; index < optAssertionCount; index++)
            {
                if ((optAssertionTab[index].assertion  == OA_EQUAL)   &&
                    (optAssertionTab[index].op1.lclNum == op1LclNum)  &&
                    (optAssertionTab[index].op2.type   == op2->gtOper))
                {
                    switch (op2->gtOper)
                    {
                    case GT_CNS_INT:
                        if (optAssertionTab[index].op2.iconVal == op2->gtIntCon.gtIconVal)
                            goto DONE_OLD;
                        break;

                    case GT_CNS_LNG:
                        if (optAssertionTab[index].op2.lconVal == op2->gtLngCon.gtLconVal)
                            goto DONE_OLD;
                        break;

                    case GT_CNS_DBL:
                        /* we have to special case for positive and negative zero */
                        if ((optAssertionTab[index].op2.dconVal == op2->gtDblCon.gtDconVal) && 
                            (_fpclass(optAssertionTab[index].op2.dconVal) == _fpclass(op2->gtDblCon.gtDconVal)))
                            goto DONE_OLD;
                        break;
                    }
                }
            }
            assert(index == optAssertionCount);

            /* Not found, add a new entry (unless we reached the maximum) */
                
            if (index == EXPSET_SZ)
                goto FOUND_MAX;
                
            optAssertionTab[index].assertion  = OA_EQUAL;
            optAssertionTab[index].op1.lclNum = op1LclNum;
            optAssertionTab[index].op2.type   = op2->gtOper;
            optAssertionTab[index].op2.lconVal = 0;
            
            switch (op2->gtOper)
            {
            default:
                /* unsuported type, exit optAssertionCount is unchanged */
                goto NO_ASSERTION;

            case GT_CNS_INT:
                optAssertionTab[index].op2.iconVal   = op2->gtIntCon.gtIconVal;
#if PROP_ICON_FLAGS
                /* iconFlags should only contain bits in GTF_ICON_HDL_MASK */
                optAssertionTab[index].op2.iconFlags = (op2->gtFlags & GTF_ICON_HDL_MASK);
                /* @TODO [REVISIT] [04/16/01] []: Need to add handle1 and handle2 arguments if LATE_DISASM is on */
#endif
                break;

            case GT_CNS_LNG:
                optAssertionTab[index].op2.lconVal = op2->gtLngCon.gtLconVal;
                break;

            case GT_CNS_DBL:
                /* If we have an NaN value then don't record it */
              if  (_isnan(op2->gtDblCon.gtDconVal))
                  goto NO_ASSERTION;
              optAssertionTab[index].op2.dconVal = op2->gtDblCon.gtDconVal;
              break;
            }

            optAssertionCount++;

#ifdef  DEBUG
            if  (verbose)
            {
                printf("\nNew %s constant assignment V%02u, index #%02u:\n", 
                       localProp ? "local" : "", op1LclNum, index+1);
                gtDispTree(tree);
            }
#endif
            goto DONE_NEW;


        case GT_LCL_VAR:

            /* Copy assignment?  (i.e. LCL_VAR_A = LCL_VAR_B) */

            op2LclNum = op2->gtLclVar.gtLclNum; assert(op2LclNum < lvaCount);
            op2LclVar = &lvaTable[op2LclNum];
 
            /* If the types are different or op2 has its address taken then bail */
            if ((op1LclVar->lvType != op2LclVar->lvType) || op2LclVar->lvAddrTaken)
                goto NO_ASSERTION;

            /* Check to see if the assignment is not already recorded in the table */

            for (index=0; index < optAssertionCount; index++)
            {
                if ((optAssertionTab[index].assertion  == OA_EQUAL)   &&
                    (optAssertionTab[index].op2.type   == GT_LCL_VAR) &&
                    (optAssertionTab[index].op2.lclNum == op2LclNum)  &&
                    (optAssertionTab[index].op1.lclNum == op1LclNum))
                {
                    /* we have a match - set the tree asg num */
                    goto DONE_OLD;
                }
            }
            assert(index == optAssertionCount);

            /* Not found, add a new entry (unless we reached the maximum) */
            
            if (index == EXPSET_SZ)
                goto FOUND_MAX;
                
            /* If op2 is not address taken, and either */
            /*    op2 is a volatile variable and op1 is not then swap them */
            /* or op2 is a parameter         and op1 is not then swap them */
            
            if (!op2LclVar->lvAddrTaken &&
                ((op2LclVar->lvVolatileHint && !op1LclVar->lvVolatileHint) ||
                 (op2LclVar->lvIsParam      && !op1LclVar->lvIsParam)))
            {
                unsigned tmpLclNum = op1LclNum;
                         op1LclNum = op2LclNum;
                         op2LclNum = tmpLclNum;
            }

            optAssertionTab[index].assertion  = OA_EQUAL;
            optAssertionTab[index].op1.lclNum = op1LclNum;
            optAssertionTab[index].op2.lconVal = 0;
            
            optAssertionTab[index].op2.type   = GT_LCL_VAR;
            optAssertionTab[index].op2.lclNum = op2LclNum;
            
            optAssertionCount++;
            
#ifdef  DEBUG
            if  (verbose)
            {
                printf("\nNew %s copy assignment V%02u, index #%02u:\n", 
                       localProp ? "local " : "", op1LclNum, index+1);
                gtDispTree(tree);
            }
#endif
            /* Mark the variables this index depends on */
            lvaTable[op2LclNum].lvAssertionDep |= genCSEnum2bit(index+1);

            goto DONE_NEW;
                

        case GT_EQ:
        case GT_NE:
        case GT_LT:
        case GT_LE:
        case GT_GT:
        case GT_GE:

            /* Assigning the result of a RELOP, see if we can add a boolean subrange assertion */

            goto IS_BOOL_ASGN;


        case GT_CLS_VAR:
        case GT_ARR_ELEM:
        case GT_LCL_FLD:
        case GT_IND:

            /* Assigning the result of an indirection into a LCL_VAR, see if we can add a subrange assertion */

            toType = op2->gtType;

            goto CHK_SUBRANGE;
            

        case GT_CAST:

            /* Assigning the result of a cast to a LCL_VAR, see if we can add a subrange assertion */

            toType = op2->gtCast.gtCastType;

CHK_SUBRANGE:            
            switch (toType)
            {
            case TYP_BOOL:
IS_BOOL_ASGN:
                loVal = 0;
                hiVal = 1;
                break;

            case TYP_BYTE:
                loVal = -0x80;
                hiVal = +0x7f;
                break;

            case TYP_UBYTE:
                loVal = 0;
                hiVal = 0xff;
                break;

            case TYP_SHORT:
                loVal = -0x8000;
                hiVal = +0x7fff;
                break;

            case TYP_USHORT:
            case TYP_CHAR:
                loVal = 0;
                hiVal = 0xffff;
                break;

            default:
                goto SKIP_ASG;
            }

            /* Check to see if the assertion is already recorded in the table */
            
            for (index=0; index < optAssertionCount; index++)
            {
                if ((optAssertionTab[index].assertion   == OA_SUBRANGE) &&
                    (optAssertionTab[index].op1.lclNum  == op1LclNum)   &&
                    (optAssertionTab[index].op2.type    == GT_CAST)     &&
                    (optAssertionTab[index].op2.loBound == loVal)       &&
                    (optAssertionTab[index].op2.hiBound == hiVal))
                {
                    goto DONE_OLD;
                }
            }

            assert(index == optAssertionCount);
            
            /* Not found, add a new entry (unless we reached the maximum) */
            
            if (index == EXPSET_SZ)
                goto FOUND_MAX;

            optAssertionTab[index].assertion   = OA_SUBRANGE;
            optAssertionTab[index].op1.lclNum  = op1LclNum;
            optAssertionTab[index].op2.type    = GT_CAST;
            optAssertionTab[index].op2.loBound = loVal;
            optAssertionTab[index].op2.hiBound = hiVal;
            
            optAssertionCount++;

#ifdef  DEBUG
            if  (verbose)
            {
                printf("\nNew %s subrange assertion V%02u in [%d..%d], index #%02u:\n", 
                       localProp ? "local" : "", op1LclNum, loVal, hiVal, index+1);
                gtDispTree(tree);
            }
#endif
            goto DONE_NEW;

        case GT_CALL:
            
            if (op2->gtCall.gtCallType != CT_HELPER)
                goto NO_ASSERTION;

            /* We are assigning the result of a helper call */

            switch (eeGetHelperNum(op2->gtCall.gtCallMethHnd))
            {
            case CORINFO_HELP_GETSHAREDSTATICBASE:
            case CORINFO_HELP_NEW_DIRECT:
            case CORINFO_HELP_NEW_CROSSCONTEXT:
            case CORINFO_HELP_NEWFAST:
            case CORINFO_HELP_NEWSFAST:
            case CORINFO_HELP_NEWSFAST_ALIGN8:
            case CORINFO_HELP_NEW_SPECIALDIRECT:
            case CORINFO_HELP_NEWOBJ:
            case CORINFO_HELP_NEWARR_1_DIRECT:
            case CORINFO_HELP_NEWARR_1_OBJ:
            case CORINFO_HELP_NEWARR_1_VC:
            case CORINFO_HELP_NEWARR_1_ALIGN8:
            case CORINFO_HELP_STRCNS:
            case CORINFO_HELP_BOX:
                // All of these are sure to return a non-Null reference
                goto NON_NULL_LCL_VAR_OP1;

            default:
                goto NO_ASSERTION;
            }

        } // end of switch (op2->gtOper)

    }  // end of if(tree->gtOper == GT_ASG)

 SKIP_ASG:;

    // Since clearing the GTF_EXCEPT flag can allow the op1 and op2 nodes
    // of a binary op to be reordered we don't add the non-null assertion
    // on the GT_IND node itself, instead we add the assertion on the 
    // parent node of the GT_IND.

    // Now we examine the current node for any child nodes that are GT_IND's */

    unsigned  kind = tree->OperKind();

    /* If this is a constant node or leaf node then bail */

    if  (kind & (GTK_CONST|GTK_LEAF))
        goto NO_ASSERTION;

    /* If this is a 'simple' unary/binary operator */
    /* then see if we have an GT_IND node child    */
    
    if  (kind & GTK_SMPOP)
    {
        op1 = tree->gtOp.gtOp1;
        op2 = tree->gtGetOp2();
        goto CHECK_FOR_IND;
    }

    /* See what kind of a special operator we have here */
    
    switch  (tree->OperGet())
    {
    case GT_FIELD:
        op1 = tree->gtField.gtFldObj;
        op2 = NULL;
        goto CHECK_FOR_IND;
        
    case GT_CALL:
        op1 = NULL;
        if (tree->gtCall.gtCallObjp)
            op1 = tree->gtCall.gtCallObjp;
        op2 = NULL;
        if (tree->gtCall.gtCallType == CT_INDIRECT)
            op2 = tree->gtCall.gtCallAddr;
        goto CHECK_FOR_IND;
        
    case GT_ARR_LENREF:
        op1 = tree->gtArrLen.gtArrLenAdr;
        op2 = NULL;
        if (tree->gtArrLen.gtArrLenCse)
            op2 = tree->gtArrLen.gtArrLenCse;
        goto CHECK_FOR_IND;
        
    case GT_ARR_ELEM:
        op1 = tree->gtArrElem.gtArrObj;
        /* @TODO [REVISIT] [04/16/01] []: We set op2 to the first GT_IND node in gtArrElem[] */
        op2 = NULL;
        unsigned dim;
        for(dim = 0; dim < tree->gtArrElem.gtArrRank; dim++)
        {
            if (tree->gtArrElem.gtArrInds[dim]->gtOper == GT_IND)
            {
                op2 = tree->gtArrElem.gtArrInds[dim];
                break;
            }
        }
        goto CHECK_FOR_IND;

    default:
#ifdef  DEBUG
        gtDispTree(tree);
#endif
        assert(!"unexpected operator");
    }

    assert(!"Shouldn't get here in optAssertionAdd()");
    goto NO_ASSERTION;

CHECK_FOR_IND:

    GenTreePtr op2save;

#ifdef DEBUG
    op2save = NULL;
#endif

    /* Is op2 non-null and a GT_IND node ? */

    if ((op2 != NULL) && (op2->gtOper == GT_IND))
    {
        op2save = op2;

        /* Set op2 to child of GT_IND */
        op2 = op2->gtOp.gtOp1;

        /* Check for add of a constant */
        if ((op2->gtOper             == GT_ADD)    && 
            (op2->gtOp.gtOp2->gtOper == GT_CNS_INT)  )
        {
            /* Set op2 to the non-constant child */
            op2 = op2->gtOp.gtOp1;
            addCns = true;
        }
        else
        {
            addCns = false;
        }

        /* If op2 is not a GT_LCL_VAR then bail */
        if (op2->gtOper != GT_LCL_VAR)
            goto CHECK_OP1;

        op2LclNum = op2->gtLclVar.gtLclNum; assert(op2LclNum  < lvaCount);
        op2LclVar = &lvaTable[op2LclNum];

        /* Can't have the address taken flag */
        /* and if we added an constant we must have a GC type */
        if ( op2LclVar->lvAddrTaken ||
            !(addCns && !varTypeIsGC(op2LclVar->TypeGet())))
            goto CHECK_OP1;

        /* Check for an assignment to op2LclNum */
        if ((tree->OperKind()       &  GTK_ASGOP ) &&
            (op1->gtOper            == GT_LCL_VAR) &&
            (op1->gtLclVar.gtLclNum == op2LclNum )   )
            goto NO_ASSERTION;

        // Here we have an indirection of a LCL_VAR 
        // We can add a assertion that it is now non-NULL

        /* Check to see if the assertion is already recorded in the table */
            
        for (index=0; index < optAssertionCount; index++)
        {
            if ((optAssertionTab[index].assertion   == OA_NOT_EQUAL)   &&
                (optAssertionTab[index].op1.lclNum  == op2LclNum)      &&
                (optAssertionTab[index].op2.type    == GT_CNS_INT)     &&
                (optAssertionTab[index].op2.iconVal == 0))
            {
                goto DONE_OP2;
            }
        }

        assert(index == optAssertionCount);
            
        /* Not found, add a new entry (unless we reached the maximum) */
            
        if (index == EXPSET_SZ)
            goto FOUND_MAX;
                
        optAssertionTab[index].assertion   = OA_NOT_EQUAL;
        optAssertionTab[index].op1.lclNum  = op2LclNum;
        optAssertionTab[index].op2.type    = GT_CNS_INT;
        optAssertionTab[index].op2.lconVal = 0;

        optAssertionCount++;

#ifdef  DEBUG
        if  (verbose)
        {
            printf("\nNew %s non-null assertion V%02u, index #%02u:\n", 
                   localProp ? "local" : "", op2LclNum, index+1);
            gtDispTree(tree->gtOp.gtOp2);
        }
#endif
        assert(index+1 == optAssertionCount);

        /* Mark the variables this index depends on */
        lvaTable[op2LclNum].lvAssertionDep |= genCSEnum2bit(index+1);

DONE_OP2:

        if (!localProp)
        {
            assert(tree->gtAssertionNum == 0);

            /* Set the tree asg num field */
            tree->gtAssertionNum = index+1;
        }
    }

CHECK_OP1:

    /* Is op1 non-null and a GT_IND node ? */

    if ((op1 != NULL) && (op1->gtOper == GT_IND))
    {
        /* Set op1 to child of GT_IND */
        op1 = op1->gtOp.gtOp1;

        /* Check for add of a constant */
        if ((op1->gtOper             == GT_ADD)    && 
            (op1->gtOp.gtOp2->gtOper == GT_CNS_INT)  )
        {
            /* Set op1 to the non-constant child */
            op1 = op1->gtOp.gtOp1;        
            addCns = true;
        }
        else
        {
            addCns = false;
        }

        /* If op1 is not a GT_LCL_VAR then bail */
        if (op1->gtOper != GT_LCL_VAR)
            goto NO_ASSERTION;

NON_NULL_LCL_VAR_OP1:

        op1LclNum = op1->gtLclVar.gtLclNum; assert(op1LclNum  < lvaCount);
        op1LclVar = &lvaTable[op1LclNum];

        /* Can't have the address taken flag */
        /* and if we added an constant we must have a GC type */
        if ( op1LclVar->lvAddrTaken ||
            !(addCns && !varTypeIsGC(op1LclVar->TypeGet())))
            goto NO_ASSERTION;

        // Here we have an indirection of a LCL_VAR 
        // We can add a assertion that it is now non-NULL

        /* Check to see if the assertion is already recorded in the table */
            
        for (index=0; index < optAssertionCount; index++)
        {
            if ((optAssertionTab[index].assertion   == OA_NOT_EQUAL)   &&
                (optAssertionTab[index].op1.lclNum  == op1LclNum)      &&
                (optAssertionTab[index].op2.type    == GT_CNS_INT)     &&
                (optAssertionTab[index].op2.iconVal == 0))
            {
                if (!localProp && (tree->gtAssertionNum != 0))
                {
                    /* We must have had two active assertions at this node */
                    assert(op2save != NULL);
                    /* Move the op2 assertion onto the op2 child tree */
                    assert(op2save->gtAssertionNum == 0);
                    op2save->gtAssertionNum = tree->gtAssertionNum;
                    tree->gtAssertionNum    = 0;
                }
                goto DONE_OLD;
            }
        }

        assert(index == optAssertionCount);
            
        /* Not found, add a new entry (unless we reached the maximum) */
            
        if (index == EXPSET_SZ)
            goto FOUND_MAX;
                
        optAssertionTab[index].assertion   = OA_NOT_EQUAL;
        optAssertionTab[index].op1.lclNum  = op1LclNum;
        optAssertionTab[index].op2.type    = GT_CNS_INT;
        optAssertionTab[index].op2.lconVal = 0;

        optAssertionCount++;

#ifdef  DEBUG
        if  (verbose)
        {
            printf("\nNew %s non-null assertion V%02u, index #%02u:\n", 
                   localProp ? "local" : "", op1LclNum, index+1);
            gtDispTree(tree->gtOp.gtOp1);
        }
#endif
        if (!localProp && (tree->gtAssertionNum != 0))
        {
            /* We must have had two active assertions at this node */
            assert(op2save != NULL);
            /* Move the op2 assertion onto the op2 child tree */
            assert(op2save->gtAssertionNum == 0);
            op2save->gtAssertionNum = tree->gtAssertionNum;
            tree->gtAssertionNum    = 0;
        }
        goto DONE_NEW;
    }

NO_ASSERTION:

    return;

FOUND_MAX:

    assert(index == EXPSET_SZ);

    return;

DONE_NEW:

    assert(index+1 == optAssertionCount);

    /* Mark the variables this index depends on */
    lvaTable[op1LclNum].lvAssertionDep |= genCSEnum2bit(index+1);

DONE_OLD:
    
    if (!localProp)
    {
        assert(tree->gtAssertionNum == 0);

        /* Set the tree asg num field */
        tree->gtAssertionNum = index+1;
    }
}

/*****************************************************************************
 *
 *  Given a lclNum and a toType, return true if it is known that the 
 *  variable's value is always a valid subrange of toType.
 *  Thus we can discard or omit a cast to toType
 */

bool                Compiler::optAssertionIsSubrange(unsigned   lclNum,
                                                     var_types  toType,
                                                     EXPSET_TP  assertions, 
                                                     bool       localProp
                                                     DEBUGARG(unsigned *pIndex))
{
    unsigned    index;
    EXPSET_TP   mask;

    /* Check each assertion to see if it can be applied here */

    for (index=0, mask=1; index < optAssertionCount; index++, mask<<=1)
    {
        assert(mask == genCSEnum2bit(index+1));

        /* See we have a subrange assertion about this variable */

        if  ((localProp || (assertions & mask))                 && 
             (optAssertionTab[index].assertion  == OA_SUBRANGE) &&
             (optAssertionTab[index].op1.lclNum == lclNum)        )
        {
            /* See if we can discard the cast */

            switch (toType)
            {
            default:
                continue;        // Keep the cast

            case TYP_BYTE:
                if ((optAssertionTab[index].op2.loBound < -0x80)   ||
                    (optAssertionTab[index].op2.hiBound > +0x7f))
                {
                    continue;    // Keep the cast
                }
                break;
            case TYP_UBYTE:
                if ((optAssertionTab[index].op2.loBound < 0)       ||
                    (optAssertionTab[index].op2.hiBound > 0xff))
                {
                    continue;    // Keep the cast
                }
                break;
            case TYP_SHORT:
                if ((optAssertionTab[index].op2.loBound < -0x8000) ||
                    (optAssertionTab[index].op2.hiBound > +0x7fff))
                {
                    continue;    // Keep the cast
                }
                break;
            case TYP_USHORT:
            case TYP_CHAR:
                if ((optAssertionTab[index].op2.loBound < 0)       ||
                    (optAssertionTab[index].op2.hiBound > 0xffff))
                {
                    continue;    // Keep the cast
                }
                break;
            case TYP_UINT:
                if (optAssertionTab[index].op2.loBound < 0)
                {
                    continue;    // Keep the cast
                }
                break;
            case TYP_INT:
                break;
            }
#ifdef DEBUG
            if (pIndex)
                *pIndex = index;
#endif
            return true;
        }
    }
    return false;
}


/*****************************************************************************
 *
 *  Given a tree and a set of available assertions
 *  we try to propagate an assertion and modify 'tree' if we can.
 *  Returns the modified tree, or NULL if no assertion prop took place
 */

GenTreePtr          Compiler::optAssertionProp(EXPSET_TP  assertions, 
                                               GenTreePtr tree,
                                               bool       localProp)
{
    unsigned  index;
#if DEBUG
    index = 0xbaadf00d;
#endif

    if (assertions == 0)
        return NULL;

    /* Is this node the left-side of an assignment or the child of a GT_ADDR? */
    if (tree->gtFlags & GTF_DONT_CSE)
        return NULL;

    switch (tree->gtOper)
    {
        unsigned    lclNum;
        EXPSET_TP   mask;
        GenTreePtr  op1;
        GenTreePtr  op2;
        var_types   toType;

    case GT_LCL_VAR:

        /* If we have a var definition then bail */

        if (tree->gtFlags & GTF_VAR_DEF)
            return NULL;

        lclNum = tree->gtLclVar.gtLclNum; assert(lclNum < lvaCount);

        /* Check each assertion to see if it can be applied here */
        
        for (index=0, mask=1; index < optAssertionCount; index++, mask<<=1)
        {
            assert(mask == genCSEnum2bit(index+1));

            /* See if the variable is equal to a constant or another variable */

            if  ((localProp || (assertions & mask))  && 
                 (optAssertionTab[index].assertion  == OA_EQUAL) &&
                 (optAssertionTab[index].op1.lclNum == lclNum))
            {
                /* hurah, our variable is in the assertion table */
#ifdef  DEBUG
                if  (verbose)
                {
                    printf("\n%s prop for index #%02u in BB%02u:\n", 
                           optAssertionTab[index].op2.type == GT_LCL_VAR ? "Copy" : "Constant",
                           index+1, compCurBB->bbNum);
                    gtDispTree(tree);
                }
#endif
                if (!localProp)
                {
                    /* Decrement lclNum lvRefCnt and lvRefCntWtd */
                    lvaTable[lclNum].decRefCnts(compCurBB->bbWeight, this);
                }

                /* Replace 'tree' with the new value from our table */
                switch (optAssertionTab[index].op2.type)
                {
                    unsigned  newLclNum;

                case GT_LCL_VAR:
                    assert(optAssertionTab[index].op2.lclNum < lvaCount);
                    if (!localProp)
                    {
                        /* Increment its lvRefCnt and lvRefCntWtd */
                        lvaTable[optAssertionTab[index].op2.lclNum].incRefCnts(compCurBB->bbWeight, this);
                    }
                    newLclNum = optAssertionTab[index].op2.lclNum;    assert(newLclNum < lvaCount);
                    tree->gtLclVar.gtLclNum = newLclNum;
                    break;

                case GT_CNS_LNG:
                    if (tree->gtType == TYP_LONG)
                    {
                        tree->ChangeOperConst(GT_CNS_LNG);
                        tree->gtLngCon.gtLconVal = optAssertionTab[index].op2.lconVal;
                    }
                    else
                    {
                        tree->ChangeOperConst(GT_CNS_INT);
                        tree->gtIntCon.gtIconVal = (int) optAssertionTab[index].op2.lconVal;                        

                        tree->gtType=TYP_INT;
                    }
                    break;

                case GT_CNS_INT:
#if PROP_ICON_FLAGS
                    if (optAssertionTab[index].op2.iconFlags)
                    {
                        /* iconFlags should only contain bits in GTF_ICON_HDL_MASK */
                        assert((optAssertionTab[index].op2.iconFlags & ~GTF_ICON_HDL_MASK) == 0);
                        /* @TODO [REVISIT] [04/16/01] []: Need to add handle1 and handle2 
                           arguments if LATE_DISASM is on */
                        GenTreePtr newTree = gtNewIconHandleNode(optAssertionTab[index].op2.iconVal, 
                                                                 optAssertionTab[index].op2.iconFlags);
                        if (!localProp)
                        {
                            /* Update the next and prev links */
                            newTree->gtNext = tree->gtNext;
                            if (newTree->gtNext)
                                newTree->gtNext->gtPrev = newTree;

                            newTree->gtPrev = tree->gtPrev;
                            if (newTree->gtPrev)
                                newTree->gtPrev->gtNext = newTree;
                        }
                        tree = newTree;
                    }
                    else
#endif
                    {
                        tree->ChangeOperConst(GT_CNS_INT);
                        tree->gtIntCon.gtIconVal = optAssertionTab[index].op2.iconVal;

                        /* Clear the GTF_ICON_HDL_MASK in the gtFlags for tree */
                        tree->gtFlags           &= ~GTF_ICON_HDL_MASK;                        
                    }

                    // constant ints are of type TYP_INT, not any of the short forms.
                    if (varTypeIsIntegral(tree->TypeGet()))
                    {
                        assert(tree->gtType!=TYP_REF && tree->gtType!=TYP_LONG);
                        tree->gtType = TYP_INT;
                    }
                    break;

                case GT_CNS_DBL:
                    tree->ChangeOperConst(GT_CNS_DBL);
                    tree->gtDblCon.gtDconVal = optAssertionTab[index].op2.dconVal;
                    break;
                }

                if (!localProp && (tree->gtOper == GT_LCL_VAR))
                {
                    /* Check for cascaded assertion props */
                    assertions &= ~mask;

                    if (assertions)
                    {
                        GenTreePtr newTree = optAssertionProp(assertions, tree, localProp);
                        if ((newTree != NULL) && (newTree != tree))
                            tree = newTree;
                    }
                }
                goto DID_ASSERTION_PROP;
            }
        }
        break;

    case GT_EQ:
    case GT_NE:

        op1 = tree->gtOp.gtOp1;
        op2 = tree->gtOp.gtOp2;

        /* If op1 is not a LCL_VAR then bail */
        if (op1->gtOper != GT_LCL_VAR)
            return NULL;

        /* If op2 is not a CNS_INT 0 then bail */
        if ((op2->gtOper != GT_CNS_INT) || (op2->gtIntCon.gtIconVal != 0))
            return NULL;

        lclNum = op1->gtLclVar.gtLclNum; assert(lclNum < lvaCount);
            
        /* Check each assertion to see if it can be applied here */

        for (index=0, mask=1; index < optAssertionCount; index++, mask<<=1)
        {
            assert(mask == genCSEnum2bit(index+1));

            /* See if the variable is know to be non-null */

            if  ((localProp || (assertions & mask))  && 
                 (optAssertionTab[index].assertion   == OA_NOT_EQUAL) &&
                 (optAssertionTab[index].op1.lclNum  == lclNum)       &&
                 (optAssertionTab[index].op2.type    == GT_CNS_INT)   &&
                 (optAssertionTab[index].op2.iconVal == 0))
            {
                /* hurah, our variable is in the assertion table */
#ifdef  DEBUG
                if  (verbose)
                {
                    printf("\nNon-null prop for index #%02u in BB%02u:\n",
                           index+1, compCurBB->bbNum); 
                }
#endif
                if (localProp)
                {
                    /* return either CNS_INT 0 or CNS_INT 1 */
                    if (tree->gtOper == GT_NE)
                        op2->gtIntCon.gtIconVal = 1;

                    tree = op2;
                    tree->gtType = TYP_INT;
                }
                else
                {
                    /* Decrement lclNum lvRefCnt and lvRefCntWtd */
                    lvaTable[lclNum].decRefCnts(compCurBB->bbWeight, this);

                    /* We just bash the LCL_VAR into a zero and reverse the condition */
                    tree->SetOper(GenTree::ReverseRelop(tree->OperGet()));
                    op1->ChangeOperConst(GT_CNS_INT);
                    op1->gtIntCon.gtIconVal = 0;

                    /* fgMorphTree will now fold this tree */
                }
                goto DID_ASSERTION_PROP;
            }
        }
        break;

    case GT_CAST:

        toType = tree->gtCast.gtCastType;

        /* If we don't have an integer cast then bail */

        if (varTypeIsFloating(toType))
            return NULL;

        op1 = tree->gtCast.gtCastOp;

        /* Skip over a GT_COMMA node, if necessary */
        if (op1->gtOper == GT_COMMA)
            op1 = op1->gtOp.gtOp2;

        /* If we don't have a cast of a LCL_VAR then bail */
        if (op1->gtOper != GT_LCL_VAR)
            return NULL;

        lclNum = op1->gtLclVar.gtLclNum; assert(lclNum < lvaCount);

        if (optAssertionIsSubrange(lclNum, toType, assertions, localProp DEBUGARG(&index)))
        {
            /* Reset op1 in case we skiped over a GT_COMMA node */
            op1 = tree->gtCast.gtCastOp;

#ifdef  DEBUG
            if  (verbose)
            {
                printf("\nSubrange prop for index #%02u in BB%02u:\n", 
                       index+1, compCurBB->bbNum); 
               gtDispTree(tree);
            }
#endif
            /* Ok we can discard this cast */
            tree = op1;

            goto DID_ASSERTION_PROP;
        }
        break;

    case GT_IND:

        /* If the Exception flags is not set then bail */
        if (!(tree->gtFlags & GTF_EXCEPT))
            return NULL;

        op1 = tree->gtOp.gtOp1;

        /* Check for add of a constant */
        if ((op1->gtOper             == GT_ADD) && 
            (op1->gtOp.gtOp2->gtOper == GT_CNS_INT))
        {
            op1 = op1->gtOp.gtOp1;        
        }

        /* If we don't have an indirection of a LCL_VAR then bail */

        if (op1->gtOper != GT_LCL_VAR)
            return NULL;

        lclNum = op1->gtLclVar.gtLclNum; assert(lclNum < lvaCount);
            
        /* Check each assertion to see if it can be applied here */

        for (index=0, mask=1; index < optAssertionCount; index++, mask<<=1)
        {
            assert(mask == genCSEnum2bit(index+1));

            /* See if the variable is know to be non-null */

            if  ((localProp || (assertions & mask))  && 
                 (optAssertionTab[index].assertion   == OA_NOT_EQUAL) &&
                 (optAssertionTab[index].op1.lclNum  == lclNum)       &&
                 (optAssertionTab[index].op2.type    == GT_CNS_INT)   &&
                 (optAssertionTab[index].op2.iconVal == 0))
            {
                /* hurah, our variable is in the assertion table */
#ifdef  DEBUG
                if  (verbose)
                {
                    printf("\nNon-null prop for index #%02u in BB%02u:\n",
                           index+1, compCurBB->bbNum); 
                }
#endif
                /* Ok we can clear the exception flag */
                tree->gtFlags &= ~GTF_EXCEPT;

                goto DID_ASSERTION_PROP;
            }
        }
        break;

    } // end switch (tree->gtOper)

    /* Node is not an assertion prop candidate */

    return NULL;


DID_ASSERTION_PROP:

    /* record the fact that we propagated a value */
    optAssertionPropagated = true;

#ifdef  DEBUG
    if  (verbose)
    {
        printf("\nNew node for index #%02u:\n", index+1);
        gtDispTree(tree);
    }
#endif

    return tree;
}


/*****************************************************************************
 *
 *   The entry point for assertion propagation
 */

void                Compiler::optAssertionPropMain()
{
#ifdef DEBUG
    if (verbose) 
        printf("*************** In optAssertionPropMain()\n");
#endif

    /* initialize the value assignments tracking logic */

    optAssertionInit();

    /* first discover all value assignments and record them in the table */

    for (BasicBlock * block = fgFirstBB; block; block = block->bbNext)
    {
        /* Walk the statement trees in this basic block */

        for (GenTreePtr stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

            for (GenTreePtr tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
            {
                tree->gtAssertionNum = 0;

                /* No assertions can be generated in a COLON */

                if (tree->gtFlags & GTF_COLON_COND)
                    continue;

                optAssertionAdd(tree, false);
            }
        }
    }

    /* We're done if there were no value assignments */

    if  (!optAssertionCount)
        return;

#ifdef DEBUG
    fgDebugCheckLinks();
#endif

    /* Compute 'gen' and 'kill' sets for all blocks
     * This is a classic available expressions forward
     * dataflow analysis */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        EXPSET_TP       valueGen  = 0;
        EXPSET_TP       valueKill = 0;

        /* Walk the statement trees in this basic block */

        for (GenTreePtr stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

            for (GenTreePtr tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
            {
                if (tree->OperKind() & GTK_ASGOP)
                {
                    /* What is the target of the assignment? */

                    if  (tree->gtOp.gtOp1->gtOper == GT_LCL_VAR)
                    {
                        /* Assignment to a local variable */

                        unsigned    lclNum = tree->gtOp.gtOp1->gtLclVar.gtLclNum;  assert(lclNum < lvaCount);
                        LclVarDsc * varDsc = lvaTable + lclNum;

                        /* All dependent value assignments are killed here */

                        valueKill |=  varDsc->lvAssertionDep;
                        valueGen  &= ~varDsc->lvAssertionDep;

                        /* Is this a tracked value assignment */

                        if  (tree->gtAssertionNum)
                        {
                            /* A new value assignment is generated here */
                            valueGen  |=  genCSEnum2bit(tree->gtAssertionNum);
                            valueKill &= ~genCSEnum2bit(tree->gtAssertionNum);
                        }
                    }
                }
            }
        }

#ifdef  DEBUG

        if  (verbose)
        {
            printf("\nBB%02u", block->bbNum);
            printf(" valueGen = %08X", valueGen );
            printf(" valueKill= %08X", valueKill);
        }

#endif

        block->bbAssertionGen  = valueGen;
        block->bbAssertionKill = valueKill;

        block->bbFlags |= BBF_CHANGED;      // out set need to be propaged to in set
    }

    /* Compute the data flow values for all tracked expressions
     * IN and OUT never change for the initial basic block B1 */

    fgFirstBB->bbAssertionIn  = 0;
    fgFirstBB->bbAssertionOut = fgFirstBB->bbAssertionGen;

    /* Initially estimate the OUT sets to everything except killed expressions
     * Also set the IN sets to 1, so that we can perform the intersection */

    for (block = fgFirstBB->bbNext; block; block = block->bbNext)
    {
        block->bbAssertionOut   = ((EXPSET_TP) -1 ) & ~block->bbAssertionKill;
        block->bbAssertionIn    = ((EXPSET_TP) -1 );
    }

#if 1

    /* Modified dataflow algorithm for available expressions */

    // int i = 0;
    for (;;)
    {
        bool        change = false;
        // printf("DataFlow loop %d\n", i++);

#if DATAFLOW_ITER
        CFiterCount++;
#endif

        /* Visit all blocks and compute new data flow values */

        for (block = fgFirstBB; block; block = block->bbNext)
        {
                // @TODO [CONSIDER] [04/16/01] []: make a work list instead of a bit, 
                // so we don't have to visit all the blocks to find out if they have changed.  

            if (!(block->bbFlags & BBF_CHANGED))       // skip if out set has not changed.
                continue;

            EXPSET_TP       valueOut = block->bbAssertionOut;

            switch (block->bbJumpKind)
            {
                BasicBlock * *  jmpTab;
                unsigned        jmpCnt;

            case BBJ_THROW:
            case BBJ_RETURN:
                break;

            case BBJ_COND:
                block->bbNext    ->bbAssertionIn &= valueOut;
                block->bbJumpDest->bbAssertionIn &= valueOut;
                break;

            case BBJ_ALWAYS:
                block->bbJumpDest->bbAssertionIn &= valueOut;
                break;

            case BBJ_NONE:
                block->bbNext    ->bbAssertionIn &= valueOut;
                break;

            case BBJ_SWITCH:

                jmpCnt = block->bbJumpSwt->bbsCount;
                jmpTab = block->bbJumpSwt->bbsDstTab;

                do
                {
                    (*jmpTab)->bbAssertionIn &= valueOut;
                }
                while (++jmpTab, --jmpCnt);

                break;

            case BBJ_CALL:
                /* Since the finally is conditionally executed, it wont
                   have any useful liveness anyway */

                if (!(block->bbFlags & BBF_RETLESS_CALL))
                {
                    // Only do this if the next block is not an associated BBJ_ALWAYS
                    block->bbNext    ->bbAssertionIn &= 0;
                }
                break;

            case BBJ_RET:
                break;
            }
        }

        /* Clear everything on entry to filters or handlers */

        for (unsigned XTnum = 0; XTnum < info.compXcptnsCount; XTnum++)
        {
            EHblkDsc * ehDsc = compHndBBtab + XTnum;

            if (ehDsc->ebdFlags & CORINFO_EH_CLAUSE_FILTER)
                ehDsc->ebdFilter->bbAssertionIn = 0;

            ehDsc->ebdHndBeg->bbAssertionIn = 0;
        }

        /* Compute the new 'in' values and see if anything changed */

        for (block = fgFirstBB->bbNext; block; block = block->bbNext)
        {
            EXPSET_TP       newAssertionOut;

            /* Compute new 'out' exp value for this block */

            newAssertionOut = block->bbAssertionOut & ((block->bbAssertionIn & ~block->bbAssertionKill) | block->bbAssertionGen);

            /* Has the 'out' set changed? */

            block->bbFlags &= ~BBF_CHANGED;
            if  (block->bbAssertionOut != newAssertionOut)
            {
                /* Yes - record the new value and loop again */

              //printf("Change value out of BB%02u from %08X to %08X\n", block->bbNum, (int)block->bbAssertionOut, (int)newAssertionOut);

                 block->bbAssertionOut  = newAssertionOut;
                 change = true;
                 block->bbFlags |= BBF_CHANGED;      // out set need to be propaged to in set
            }
        }

        if  (!change)
            break;
    }

#else

    /* classic algorithm for available expressions */

    for (;;)
    {
        bool        change = false;

#if DATAFLOW_ITER
        CFiterCount++;
#endif

        /* Visit all blocks and compute new data flow values */

        for (block = fgFirstBB->bbNext; block; block = block->bbNext)
        {
            /* compute the IN set: IN[B] = intersect OUT[P}, for all P = predecessor of B */
            /* special case - this is a BBJ_RET block - cannot figure out which blocks may call it */

            for (BasicBlock * predB = fgFirstBB; predB; predB = predB->bbNext)
            {
                EXPSET_TP       valueOut = predB->bbAssertionOut;

                if  (predB->bbNext == block)
                {
                    /* we have a "direct" predecessor */

                    assert(predB->bbNum + 1 == block->bbNum);
                    block->bbAssertionIn &= valueOut;
                    continue;
                }

                switch (predB->bbJumpKind)
                {
                    BasicBlock * *  jmpTab;
                    unsigned        jmpCnt;

                case BBJ_NONE:
                    /* the only interesting case - when this is a predecessor - was treated above */
                    break;

                case BBJ_THROW:
                    /* THROW is an internal block and lets everything go through it - catched above */
                case BBJ_RETURN:
                    /* RETURN cannot have a successor */
                    break;

                case BBJ_COND:
                case BBJ_ALWAYS:

                    if  (predB->bbJumpDest == block)
                    {
                        block->bbAssertionIn &= valueOut;
                    }
                    break;

                case BBJ_SWITCH:

                    jmpCnt = predB->bbJumpSwt->bbsCount;
                    jmpTab = predB->bbJumpSwt->bbsDstTab;

                    do
                    {
                        if  ((*jmpTab) == block)
                        {
                            block->bbAssertionIn &= valueOut;
                        }
                    }
                    while (++jmpTab, --jmpCnt);

                    break;

                case BBJ_CALL:
                case BBJ_RET:
                    block->bbAssertionIn &= 0;
                    break;
                }
            }

            EXPSET_TP       valueOldOut = block->bbAssertionOut;

            /* compute the new OUT set */

            block->bbAssertionOut = (block->bbAssertionIn & ~block->bbAssertionKill) |
                                    block->bbAssertionGen;

            if  (valueOldOut != block->bbAssertionOut)
            {
                change = true;
            }
        }

        if  (!change)
            break;
    }

#endif

#ifdef  DEBUG
    if  (verbose)
    {
        printf("\n");

        for (block = fgFirstBB; block; block = block->bbNext)
        {
            printf("BB%02u", block->bbNum);
            printf(" valueIn  = %08X", block->bbAssertionIn );
            printf(" valueOut = %08X", block->bbAssertionOut);
            printf("\n");
        }

        printf("\n");
    }
#endif

    /* Perform assertion propagation (and constant folding) */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        GenTreePtr      stmt;
        GenTreePtr      tree;
        EXPSET_TP       assertions = block->bbAssertionIn;

        /* If IN = 0 and GEN = 0, there's nothing to do */

        if ((assertions == 0) && !block->bbAssertionGen)
             continue;

        /* Make the current basic block address available globally */

        compCurBB = block;
        fgRemoveRestOfBlock = false;

        /* Walk the statement trees in this basic block */

        for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

            /* - Propagate any values
             * - Look for anything that can kill an available expression
             *   i.e assignments to local variables
             */

            if (fgRemoveRestOfBlock)
            {
                fgRemoveStmt(block, stmt);
                continue;
            }

            bool updateStmt = false;  // set to true if a assertion propagation took place
                                      // and thus we must morph, set order, re-link

            for (tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
            {
                if (assertions)
                {
                    /* Try to propagate the assertions */
                    GenTreePtr newTree = optAssertionProp(assertions, tree, false);

                    if (newTree)
                    {
                        if (tree->OperKind() & GTK_CONST)
                            updateStmt = true;

                        /* @TODO [CONSIDER] [04/16/01] []: having optAssertionProp perform 
                           the following update */
                        if (tree != newTree)
                        {
                            updateStmt = true;
                            /* If tree was a GT_CAST node then we need to unlink it as it is dead */
                            if (tree->gtOper == GT_CAST) 
                            {
                                /* Check that newTree is the child of tree */
                                assert(tree->gtCast.gtCastOp == newTree);
                            
                                /* Unlink the dead node via the gtNext and gtPrev links */
                        
                                // TODO: write a generic routine to find the parent of a tree

                                newTree->gtNext      = tree->gtNext;
                                if (tree->gtNext)
                                {
                                    tree->gtNext->gtPrev = newTree;
                            
                                    /* Unlink the dead node via the op1/op2 link from it's parent */

                                    GenTreePtr parent;
                                    for (parent = tree->gtNext; parent; parent = parent->gtNext)
                                    {
                                        /* QMARK nodes are special because the op2 under the GT_COLON
                                           is threaded all the way back to the GT_QMARK */

                                        if (parent->OperGet() == GT_QMARK)
                                        {
                                            assert(parent->gtOp.gtOp2->OperGet() == GT_COLON);
                                            if (parent->gtOp.gtOp2->gtOp.gtOp2 == tree)
                                            {
                                                parent = parent->gtOp.gtOp2;
                                            }
                                        }

                                        // BUGBUG: without checking for the operation type, gtOp1 and gtOp2 may be invalid or contain data with different meaning from another part of the union
                                        if (parent->gtOp.gtOp1 == tree)
                                        {
                                            parent->gtOp.gtOp1 = newTree;
                                            break;
                                        }
                                        if (parent->gtOp.gtOp2 == tree)
                                        {
                                            parent->gtOp.gtOp2 = newTree;
                                            break;
                                        }
                                    }
//                                    assert(parent);   performance assert (missed optimization)
                                }
                                else
                                {
                                    stmt->gtStmt.gtStmtExpr = tree;
                                }
                            }
                        }
                    }
                }

                /* Is this an assignment to a local variable */

                if ((tree->OperKind() & GTK_ASGOP) &&
                    (tree->gtOp.gtOp1->gtOper == GT_LCL_VAR))
                {
                    /* Assignment to a local variable */
                    unsigned lclNum = tree->gtOp.gtOp1->gtLclVar.gtLclNum; assert(lclNum < lvaCount);

                    /* All dependent assertions are killed here */
                    assertions &= ~lvaTable[lclNum].lvAssertionDep;
                }

                /* If this tree makes an assertion - make it available */
                if  (tree->gtAssertionNum)
                {
                    assertions |= genCSEnum2bit(tree->gtAssertionNum);
                }
            }

            if  (updateStmt)
            {
                /* re-morph the statement */
                optMorphTree(block, stmt DEBUGARG("optAssertionPropMain"));
            }
        }

    }

    
#ifdef DEBUG
    fgDebugCheckBBlist();
    fgDebugCheckLinks();
#endif

    // Assertion propagation may have changed the reference counts 
    // We need to resort the variable table

    if (optAssertionPropagated)
    {
        lvaSortAgain = true;
    }
}

#endif // !ASSERTION_PROP

/*****************************************************************************/
#if CSE
/*****************************************************************************
 *
 *  Take a morphed array index expression (i.e. an GT_IND node) and break it
 *  apart into its components. Returns 0 if the expression looks weird.
 */

GenTreePtr          Compiler::gtCrackIndexExpr(GenTreePtr   tree,
                                               GenTreePtr * indxPtr,
                                               long       * indvPtr,
                                               long       * basvPtr,
                                               bool       * mvarPtr,
                                               long       * offsPtr,
                                               unsigned   * multPtr)
{
    GenTreePtr      ind;
    GenTreePtr      op1;
    GenTreePtr      op2;
    unsigned        ofs;

    assert(tree->gtOper == GT_IND);
    assert(tree->gtInd.gtRngChkOffs == 4 || tree->gtInd.gtRngChkOffs == 8);

    /* Skip over the "ind" node to the operand */

    ind = tree->gtOp.gtOp1;

    /* Skip past the "+ offs" node, if present */

    ofs = 0;

    if  (ind->gtOper             == GT_ADD     &&
         ind->gtOp.gtOp2->gtOper == GT_CNS_INT)
    {
        ofs = ind->gtOp.gtOp2->gtIntCon.gtIconVal;
        ind = ind->gtOp.gtOp1;
    }

    /* We should have "array_base + [ size * ] index" */

    if  (ind->gtOper != GT_ADD)
        return 0;

    op1 = ind->gtOp.gtOp1;
    op2 = ind->gtOp.gtOp2;

    /* The index value may be scaled, of course */

    *multPtr = 1;

    if  (op2->gtOper == GT_LSH)
    {
        long        shf;

        if  (op2->gtOp.gtOp2->gtOper != GT_CNS_INT)
            return  0;

        shf = op2->gtOp.gtOp2->gtIntCon.gtIconVal;

        if  (shf < 1 || shf > 3)
            return  0;

        *multPtr <<= shf;

        op2 = op2->gtOp.gtOp1;
    }

    /* There might be a nop node on top of the index value */

    if  (op2->gtOper == GT_NOP)
        op2 = op2->gtOp.gtOp1;

    /* Report the index expression to the caller */

    *indxPtr = op2;

    /* Figure out the index offset */

    /* hack we assume data is after length (unless OBJARRAY is on) */
    *offsPtr = 0;
    unsigned elemOffs = tree->gtInd.gtRngChkOffs + sizeof(void*);   
    if (tree->gtFlags & GTF_IND_OBJARRAY)
        elemOffs += sizeof(void*);   

    if  (ofs)
        *offsPtr = (ofs - elemOffs) / *multPtr;

    /* Is the index a simple local ? */

    if  (op2->gtOper != GT_LCL_VAR)
    {
        /* Allow "local + icon" */

        if  (op2->gtOper == GT_ADD && op2->gtOp.gtOp1->gtOper == GT_LCL_VAR
                                   && op2->gtOp.gtOp2->gtOper == GT_CNS_INT)
        {
            *offsPtr += op2->gtOp.gtOp2->gtIntCon.gtIconVal;

            op2 = op2->gtOp.gtOp1;
        }
    }

    /* If the address/index values are local vars, report them */

    if  (op1->gtOper == GT_LCL_VAR)
    {
        if  (op2->gtOper == GT_LCL_VAR)
        {
            *indvPtr = op2->gtLclVar.gtLclNum;
            *basvPtr = op1->gtLclVar.gtLclNum;
            *mvarPtr = false;

            return  op1;
        }

        if  (op2->gtOper == GT_CNS_INT)
        {
            *indvPtr = -1;
            *basvPtr = op1->gtLclVar.gtLclNum;
            *mvarPtr = false;

            return  op1;
        }
    }

    *basvPtr =
    *indvPtr = -1;
    *mvarPtr = true;

    return  op1;
}

/*****************************************************************************
 *
 *  See if the given tree can be computed in the given precision (which must
 *  be smaller than the type of the tree for this to make sense). If 'doit'
 *  is false, we merely check to see whether narrowing is possible; if we
 *  get called with 'doit' being true, we actually perform the narrowing.
 */

bool                Compiler::optNarrowTree(GenTreePtr     tree,
                                            var_types      srct,
                                            var_types      dstt,
                                            bool           doit)
{
    genTreeOps      oper;
    unsigned        kind;

    assert(tree);
    assert(genActualType(tree->gtType) == genActualType(srct));

    /* Assume we're only handling integer types */
    assert(varTypeIsIntegral(srct));
    assert(varTypeIsIntegral(dstt));

    unsigned srcSize      = genTypeSize(srct);
    unsigned dstSize      = genTypeSize(dstt);

    /* dstt must be smaller than srct to narrow */
    if (dstSize >= srcSize)
        return false;

    /* Figure out what kind of a node we have */
    oper = tree->OperGet();
    kind = tree->OperKind();

    if  (kind & GTK_ASGOP)
    {
        assert(doit == false);
        return  false;
    }

    if  (kind & GTK_LEAF)
    {
        switch (oper)
        {
        /* Constants can usually be narrowed by changing their value */
        case GT_CNS_LNG:

            __int64  lval;  lval  = tree->gtLngCon.gtLconVal;
            __int64  lmask; lmask = 0;

            switch (dstt)
            {
            case TYP_BYTE : lmask = 0x0000007F; break;
            case TYP_UBYTE: lmask = 0x000000FF; break;
            case TYP_SHORT: lmask = 0x00007FFF; break;
            case TYP_CHAR : lmask = 0x0000FFFF; break;
            case TYP_INT  : lmask = 0x7FFFFFFF; break;
            case TYP_UINT : lmask = 0xFFFFFFFF; break;
            case TYP_ULONG:
            case TYP_LONG : return false; 
            }

            if  ((lval & lmask) != lval)
                return false;

            if  (doit)
            {
                tree->ChangeOperConst     (GT_CNS_INT);
                tree->gtType             = TYP_INT;
                tree->gtIntCon.gtIconVal = (int) lval;
            }

            return  true;

        case GT_CNS_INT:

            long  ival;  ival  = tree->gtIntCon.gtIconVal;
            long  imask; imask = 0;

            switch (dstt)
            {
            case TYP_BYTE : imask = 0x0000007F; break;
            case TYP_UBYTE: imask = 0x000000FF; break;
            case TYP_SHORT: imask = 0x00007FFF; break;
            case TYP_CHAR : imask = 0x0000FFFF; break;
            case TYP_UINT :
            case TYP_INT  : return false; 
            }

            if  ((ival & imask) != ival)
                return false;

            return  true;

        /* Operands that are in memory can usually be narrowed 
           simply by changing their gtType */

        case GT_LCL_VAR:
            /* We only allow narrowing long -> int for a GT_LCL_VAR */
            if (dstSize == sizeof(void *))
                goto NARROW_IND;
            break;

        case GT_CLS_VAR:
        case GT_LCL_FLD:
            goto NARROW_IND;
        }

        assert(doit == false);
        return  false;

    }

    if (kind & (GTK_BINOP|GTK_UNOP))
    {
        GenTreePtr      op1 = tree->gtOp.gtOp1;
        GenTreePtr      op2 = tree->gtOp.gtOp2;

        switch(tree->gtOper)
        {
        case GT_AND:
            assert(genActualType(tree->gtType) == genActualType(op2->gtType));

            if ((op2->gtOper == GT_CNS_INT) && optNarrowTree(op2, srct, dstt, doit)) 
            {               
                /* Simply bash the type of the tree */

                if  (doit)
                    tree->gtType = genActualType(dstt);
                return true;
            }
            
            goto COMMON_BINOP;

        case GT_ADD:
        case GT_MUL:

            if (tree->gtOverflow() || varTypeIsSmall(dstt))
            {
                assert(doit == false);
                return false;
            }
            /* Fall through */

        case GT_OR:
        case GT_XOR:
COMMON_BINOP:
            assert(genActualType(tree->gtType) == genActualType(op1->gtType));
            assert(genActualType(tree->gtType) == genActualType(op2->gtType));

            if  (!optNarrowTree(op1, srct, dstt, doit) ||
                 !optNarrowTree(op2, srct, dstt, doit))
            {
                assert(doit == false);
                return  false;
            }

            /* Simply bash the type of the tree */

            if  (doit)
            {
                if  (tree->gtOper == GT_MUL && (tree->gtFlags & GTF_MUL_64RSLT))
                    tree->gtFlags &= ~GTF_MUL_64RSLT;

                tree->gtType = genActualType(dstt);
            }

            return true;

        case GT_IND:

NARROW_IND:
            /* Simply bash the type of the tree */

            if  (doit && (dstSize <= genTypeSize(tree->gtType)))
            {
                tree->gtType = genSignedType(dstt);

                /* Make sure we don't mess up the variable type */
                if  ((oper == GT_LCL_VAR) || (oper == GT_LCL_FLD))
                    tree->gtFlags |= GTF_VAR_CAST;
            }

            return  true;

        case GT_EQ:
        case GT_NE:
        case GT_LT:
        case GT_LE:
        case GT_GT:
        case GT_GE:

            /* These can alwaus be narrowed since they only represent 0 or 1 */
            return  true;

        case GT_CAST:
            {
                var_types       cast    = tree->gtCast.gtCastType;
                var_types       oprt    = op1->TypeGet();
                unsigned        oprSize = genTypeSize(oprt);

                if (cast != srct)
                    return false;

                if (tree->gtOverflow())
                    return false;

                /* Is this a cast from the type we're narrowing to or a smaller one? */

                if  (oprSize <= dstSize)
                {
                    /* Bash the target type of the cast */

                    if  (doit)
                    {
                        dstt = genSignedType(dstt);

                        if  (oprSize == dstSize)
                        {
                            // Same size: Bash the CAST into a NOP
                            tree->ChangeOper         (GT_NOP);
                            tree->gtType            = dstt;
                            tree->gtOp.gtOp2        = NULL;
                        }
                        else
                        {
                            // oprt smaller: Bash the target type in the CAST
                            tree->gtCast.gtCastType =
                            tree->gtType            = dstt;
                        }
                    }

                    return  true;
                }
            }
            return  false;

        case GT_COMMA:
            if (optNarrowTree(op2, srct, dstt, doit)) 
            {               
                /* Simply bash the type of the tree */

                if  (doit)
                    tree->gtType = genActualType(dstt);
                return true;
            }
            return false;

        default:
            assert(doit == false);
            return  false;
        }

    }

    return  false;
}

/*****************************************************************************
 *
 *  Callback (for fgWalkTreePre) used by the loop-based range check optimization
 *  code.
 */

struct loopRngOptDsc
{
    Compiler    *       lpoComp;

    unsigned short      lpoCandidateCnt;    // count of variable candidates

    unsigned short      lpoIndexVar;        // phase2: index variable
      signed short      lpoAaddrVar;        // phase2: array address or -1
    unsigned short      lpoIndexHigh;       // phase2: highest index offs
    unsigned            lpoIndexOff;        // phase2: current offset
    GenTreePtr          lpoStmt;            // phase2: containing statement

    var_types           lpoElemType :8;     // phase2: element type

    unsigned char       lpoCheckRmvd:1;     // phase2: any range checks removed?
    unsigned char       lpoDomExit  :1;     // current BB dominates loop exit?
    unsigned char       lpoPhase2   :1;     // the second phase in progress

#ifdef DEBUG
    void    *           lpoSelf;
    unsigned            lpoVarCount;        // total number of locals
#endif
    Compiler::LclVarDsc*lpoVarTable;        // variable descriptor table

    unsigned char       lpoHadSideEffect:1; // we've found a side effect
};

Compiler::fgWalkResult      Compiler::optFindRangeOpsCB(GenTreePtr tree, void *p)
{
    LclVarDsc   *   varDsc;

    GenTreePtr      op1;
    GenTreePtr      op2;

    loopRngOptDsc * dsc = (loopRngOptDsc*)p;
    Compiler * pComp = dsc->lpoComp;

    assert(dsc && dsc->lpoSelf == dsc);

    /* Do we have an assignment node? */

    if  (tree->OperKind() & GTK_ASGOP)
    {
        unsigned        lclNum;

        op1 = tree->gtOp.gtOp1;
        op2 = tree->gtOp.gtOp2;

        /* What is the target of the assignment? */

        if  (op1->gtOper != GT_LCL_VAR)
        {
            /* Indirect/global assignment - bad news! */

            dsc->lpoHadSideEffect = true;
            return WALK_ABORT;
        }

        /* Get hold of the variable descriptor */

        lclNum = op1->gtLclVar.gtLclNum;
        assert(lclNum < dsc->lpoVarCount);
        varDsc = dsc->lpoVarTable + lclNum;

        /* After a side effect is found, all is hopeless */

        if  (dsc->lpoHadSideEffect)
        {
            varDsc->lvLoopAsg = true;
            return  WALK_CONTINUE;
        }

        /* Is this "i += icon" ? */

        if  (tree->gtOper             == GT_ASG_ADD &&
             tree->gtOp.gtOp2->gtOper == GT_CNS_INT)
        {
            if  (dsc->lpoDomExit)
            {
                /* Are we in phase2 ? */

                if  (dsc->lpoPhase2)
                {
                    /* Is this the variable we're interested in? */

                    if  (dsc->lpoIndexVar != lclNum)
                        return  WALK_CONTINUE;

                    /* Update the current offset of the index */

                    dsc->lpoIndexOff += tree->gtOp.gtOp2->gtIntCon.gtIconVal;

                    return  WALK_CONTINUE;
                }

//              printf("Found increment of variable %u at %08X\n", lclNum, tree);
            }

            if  (varDsc->lvLoopInc == false)
            {
                varDsc->lvLoopInc = true;

                if  (varDsc->lvArrIndx)
                    dsc->lpoCandidateCnt++;
            }
        }
        else
        {
            varDsc->lvLoopAsg = true;
        }

        return WALK_CONTINUE;
    }

    /* After a side effect is found, all is hopeless */

    if  (dsc->lpoHadSideEffect)
        return  WALK_CONTINUE;

    /* Look for array index expressions */

    if  (tree->gtOper == GT_IND && (tree->gtFlags & GTF_IND_RNGCHK))
    {
        GenTreePtr      base;
        long            basv;
        GenTreePtr      indx;
        long            indv;
        bool            mval;
        long            offs;
        unsigned        mult;

        /* Does the current block dominate the loop exit? */

        if  (!dsc->lpoDomExit)
            return WALK_CONTINUE;

        /* Break apart the index expression */

        base = pComp->gtCrackIndexExpr(tree, &indx, &indv, &basv, &mval, &offs, &mult);
        if  (!base)
            return WALK_CONTINUE;

        /* The index value must be a simple local, possibly with "+ positive offset" */

        if  (indv == -1)
            return WALK_CONTINUE;
        if  (offs < 0)
            return WALK_CONTINUE;

        /* For now the array address must be a simple local */

        if  (basv == -1)
            return  WALK_CONTINUE;

        /* Get hold of the index variable's descriptor */

        assert((unsigned)indv < dsc->lpoVarCount);
        varDsc = dsc->lpoVarTable + indv;

        /* Are we in phase2 ? */

        if  (dsc->lpoPhase2)
        {
            LclVarDsc   *   arrDsc;

            /* Is this the index variable we're interested in? */

            if  (dsc->lpoIndexVar != indv)
            {
                dsc->lpoHadSideEffect = true;
                return  WALK_CONTINUE;
            }

            /* Is the array base reassigned within the loop? */

            assert((unsigned)basv < dsc->lpoVarCount);
            arrDsc = dsc->lpoVarTable + basv;

            if  (arrDsc->lvLoopAsg)
            {
                dsc->lpoHadSideEffect = true;
                return  WALK_CONTINUE;
            }

            /* Is this the array we're looking for? */

            if  (dsc->lpoAaddrVar != basv)
            {
                /* Do we know which array we're looking for? */

                if  (dsc->lpoAaddrVar != -1)
                    return  WALK_CONTINUE;

                dsc->lpoAaddrVar = (SHORT)basv;
            }

            /* Calculate the actual index offset */

            offs += dsc->lpoIndexOff; assert(offs >= 0);

            /* Is this statement guaranteed to be executed? */

            if  (varDsc->lvArrIndxDom)
            {
                /* Is this higher than the highest known offset? */

                if  (dsc->lpoIndexHigh < offs)
                     dsc->lpoIndexHigh = (unsigned short)offs;
            }
            else
            {
                /* The offset may not exceed the max. found thus far */

                if  (dsc->lpoIndexHigh < offs)
                    return  WALK_CONTINUE;
            }

                /* we are going to just bail on structs for now */
            if (tree->gtType == TYP_STRUCT)
                return WALK_CONTINUE;

            dsc->lpoCheckRmvd = true;
            dsc->lpoElemType  = tree->TypeGet();

//          printf("Remove index (at offset %u):\n", off); pComp->gtDispTree(tree); printf("\n\n");

            pComp->optRemoveRangeCheck(tree, dsc->lpoStmt, false);

            return  WALK_CONTINUE;
        }

        /* Mark the index variable as being used as an array index */

        if  (varDsc->lvLoopInc || offs)
        {
            if  (varDsc->lvArrIndxOff == false)
            {
                 varDsc->lvArrIndxOff = true;
                 dsc->lpoCandidateCnt++;
            }
        }
        else
        {
            if  (varDsc->lvArrIndx    == false)
            {
                 varDsc->lvArrIndx    = true;
                 dsc->lpoCandidateCnt++;
            }
        }

        varDsc->lvArrIndxDom = true;

        return WALK_CONTINUE;
    }

    if  (pComp->gtHasSideEffects(tree))
    {
        dsc->lpoHadSideEffect = true;
        return WALK_ABORT;
    }

    return  WALK_CONTINUE;
}

/*****************************************************************************
 *
 *  Look for opportunities to remove range checks based on natural loop and
 *  constant propagation info.
 */

void                Compiler::optRemoveRangeChecks()
{
    /* @TODO [REVISIT] [04/16/01] []: The following optimization has been disabled for now */
    if (true)
        return;

#ifdef DEBUG
    if  (verbose) 
        printf("*************** In optRemoveRangeChecks()\n");
#endif
    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    unsigned        lnum;
    LoopDsc     *   ldsc;

    // UNDONE: The following needs to be done to enable this logic:
    //
    //          Fix the dominator business
    //          Detect inner loops
    //          Set a flag during morph that says whether it's worth
    //              our while to look for these things, since this
    //              optimization is very expensive (several tree
    //              walks).

    /*
        Look for loops that contain array index expressions of the form

            a[i] and a[i+1] or a[i-1]

        Note that the equivalent thing we look for is as follows:

            a[i++] and a[i++] and ...

        In both cases, if there are no calls or assignments to global
        data, and we can prove that the index value is not negative,
        we can replace both/all the range checks with one (for the
        highest index value).

        In all the cases, we first look for array index expressions
        of the appropriate form that will execute every time around
        the loop - if we can find a non-negative initializer for the
        index variable, we can thus prove that the index values will
        never be negative.
     */

    for (lnum = 0, ldsc = optLoopTable;
         lnum < optLoopCount;
         lnum++  , ldsc++)
    {
        BasicBlock *    block;
        BasicBlock *    head;
        BasicBlock *    lbeg;
        BasicBlock *    tail;

        loopRngOptDsc   desc;

        /* Get hold of the beg and end blocks of the loop */

        head = ldsc->lpHead;

        /* Get hold of the top and bottom of the loop */

        tail = ldsc->lpEnd;
        lbeg = head->bbNext;

        // UNDONE: Need to walk backwards and look for a constant
        // UNDONE: initializer for index variables as we don't
        // UNDONE: have the constant propagation info available
        // UNDONE: at this stage of the compilation process.


        for (lclNum = 0, varDsc = lvaTable;
             lclNum < lvaCount;
             lclNum++  , varDsc++)
        {
            // HACK: Pretend all variables have constant positive initializers

            varDsc->lvRngOptDone = false;

            assert(varDsc->lvLoopInc    == false);
            assert(varDsc->lvLoopAsg    == false);
            assert(varDsc->lvArrIndx    == false);
            assert(varDsc->lvArrIndxOff == false);
        }

        /* Initialize the struct that holds the state of the optimization */

        desc.lpoComp          = this;
        desc.lpoCandidateCnt  = 0;
        desc.lpoPhase2        = false;
        desc.lpoHadSideEffect = false;
#ifdef DEBUG
        desc.lpoSelf          = &desc;
        desc.lpoVarCount      = lvaCount;
#endif
        desc.lpoVarTable      = lvaTable;
        desc.lpoDomExit       = true;

        /* Walk the trees of the loop, looking for indices and increments */

        block = head;
        do
        {
            GenTree *       stmt;

            block = block->bbNext;
            stmt  = block->bbTreeList;

            /* Make sure the loop is not in a try block */

            if  (block->bbFlags & BBF_HAS_HANDLER)
                goto NEXT_LOOP;

            /* Does the current block dominate the loop exit? */

            desc.lpoDomExit = (fgDominate(block, tail) != 0);

            /* Walk all the statements in this basic block */

            while (stmt)
            {
                assert(stmt && stmt->gtOper == GT_STMT);

                fgWalkTreePre(stmt->gtStmt.gtStmtExpr, optFindRangeOpsCB, &desc);

                stmt = stmt->gtNext;
            }
        }
        while (block != tail);

        /* Did we find any candidates? */

        if  (desc.lpoCandidateCnt)
        {
            /* Visit each variable marked as a candidate */

            for (lclNum = 0, varDsc = lvaTable;
                 lclNum < lvaCount;
                 lclNum++  , varDsc++)
            {
                if  (varDsc->lvRngOptDone == true)
                    continue;
                if  (varDsc->lvLoopInc    == false)
                    continue;
                if  (varDsc->lvLoopAsg    == true )
                    continue;
                if  (varDsc->lvArrIndx    == false)
                    continue;
                if  (varDsc->lvArrIndxOff == false)
                    continue;
                if  (varDsc->lvArrIndxDom == false)
                    continue;

//              printf("Candidate variable %u\n", lclNum);

                /*
                    Find the highest offset that is added to the variable
                    to index into a given array. This index expression has
                    to dominate the exit of the loop, since otherwise it
                    might be skipped. Also, it must not be preceded by any
                    side effects. The array must not be modified within
                    the loop.
                 */

                desc.lpoPhase2        = true;
                desc.lpoHadSideEffect = false;
                desc.lpoDomExit       = true;
                desc.lpoIndexVar      = lclNum;
                desc.lpoAaddrVar      = -1;
                desc.lpoIndexOff      = 0;
                desc.lpoIndexHigh     = 0;
                desc.lpoCheckRmvd     = false;

                // UNDONE: If the index variable is incremented several
                // UNDONE: times in the loop, its only use is to index
                // UNDONE: into arrays, and all these index operations
                // UNDONE: have their range checks removed, remove the
                // UNDONE: increments, substitute a simple "i += icon"
                // UNDONE: and change the index to be "i + 1", etc.

                /* Walk the trees of the loop, looking for indices and increments */

                block = head;
                do
                {
                    GenTree *       stmt;

                    block = block->bbNext;
                    stmt  = block->bbTreeList;

                    assert(!(block->bbFlags & BBF_HAS_HANDLER));

                    // UNDONE: Same issue as the corresponding code above

                    if  (block != lbeg)
                    {
                        flowList   *    flow;
                        unsigned        pcnt;

                        for (flow = block->bbPreds, pcnt = 0;
                             flow;
                             flow = flow->flNext  , pcnt++)
                        {
                            if  (flow->flBlock         != tail &&
                                 flow->flBlock->bbNext != block)
                            {
                                /* Looks like a nested loop or something */

                                desc.lpoDomExit = false;
                                break;
                            }
                        }
                    }

                    /* Walk all the statements in this basic block */

                    while (stmt)
                    {
                        desc.lpoStmt = stmt;

                        assert(stmt && stmt->gtOper == GT_STMT);

                        fgWalkTreePre(stmt->gtStmt.gtStmtExpr, optFindRangeOpsCB, &desc);

                        stmt = stmt->gtNext;
                    }
                }
                while (block != tail);

                /* Did we remove any range checks? */

                if  (desc.lpoCheckRmvd)
                {
                    GenTreePtr  chks;
                    GenTreePtr  loop;
                    GenTreePtr  ends;
                    GenTreePtr  temp;

                    assert(desc.lpoIndexHigh);      // ISSUE: Could this actually happen?

                    /* The following needed for gtNewRngChkNode() */

                    compCurBB      = lbeg;
                    fgPtrArgCntCur = 0;

                    /* Create the combined range check */

                    chks = gtNewLclvNode(desc.lpoIndexVar , TYP_INT);

                    temp = gtNewIconNode(desc.lpoIndexHigh, TYP_INT);

                    chks = gtNewOperNode(GT_ADD, TYP_INT, chks, temp);

                    temp = gtNewLclvNode(desc.lpoAaddrVar, TYP_REF);

                    assert(desc.lpoElemType != TYP_STRUCT);     // We don't handle structs for now
                    chks = gtNewRngChkNode(NULL,
                                           temp,
                                           chks,
                                           desc.lpoElemType, genTypeSize(desc.lpoElemType));

                    chks->gtFlags |= GTF_DONT_CSE;
                    chks = gtNewStmt(chks);
                    chks->gtFlags |= GTF_STMT_CMPADD;

//                  printf("Insert combined range check [0 .. %u] for %u[%u]:\n", desc.lpoIndexHigh, desc.lpoAaddrVar, lclNum);
//                  gtDispTree(chks);

                    /* Insert the range check at the loop head */

                    loop = lbeg->bbTreeList; assert(loop);
                    ends = loop->gtPrev;

                    lbeg->bbTreeList = chks;

                    chks->gtNext = loop;
                    chks->gtPrev = ends;

                    loop->gtPrev = chks;
                }
            }
        }

    NEXT_LOOP:

        /* Clear all the flags for the next round */

        for (lclNum = 0, varDsc = lvaTable;
             lclNum < lvaCount;
             lclNum++  , varDsc++)
        {
            varDsc->lvRngOptDone =
            varDsc->lvLoopInc    =
            varDsc->lvLoopAsg    =
            varDsc->lvArrIndx    =
            varDsc->lvArrIndxOff = false;
        }
    }
}

/*****************************************************************************
 *
 *  The following logic figures out whether the given variable is assigned
 *  somewhere in a list of basic blocks (or in an entire loop).
 */

struct  isVarAssgDsc
{
    GenTreePtr          ivaSkip;
#ifdef DEBUG
    void    *           ivaSelf;
#endif
    unsigned            ivaVar;         // Variable we are interested in, or -1
    VARSET_TP           ivaMaskVal;     // Set of variables assigned to (if !ivaMaskBad)
    bool                ivaMaskBad;     // Is ivaMaskVal valid?
    varRefKinds         ivaMaskInd : 8; // What kind of indirect assignments are there?
    BYTE                ivaMaskCall;    // What kind of calls are there?
};


Compiler::fgWalkResult      Compiler::optIsVarAssgCB(GenTreePtr tree, void *p)
{
    if  (tree->OperKind() & GTK_ASGOP)
    {
        GenTreePtr      dest     = tree->gtOp.gtOp1;
        genTreeOps      destOper = dest->OperGet();

        isVarAssgDsc *  desc = (isVarAssgDsc*)p;
        assert(desc && desc->ivaSelf == desc);

        if  (destOper == GT_LCL_VAR)
        {
            unsigned        tvar = dest->gtLclVar.gtLclNum;

            if  (tvar < VARSET_SZ)
                desc->ivaMaskVal |= genVarIndexToBit(tvar);
            else
                desc->ivaMaskBad  = true;

            if  (tvar == desc->ivaVar)
            {
                if  (tree != desc->ivaSkip)
                    return  WALK_ABORT;
            }
        }
        else if (destOper == GT_LCL_FLD)
        {
            /* We cant track every field of every var. Moreover, indirections
               may access different parts of the var as different (but
               overlapping) fields. So just treat them as indirect accesses */

         // unsigned    lclNum = dest->gtLclFld.gtLclNum;
         // assert(lvaTable[lclNum].lvAddrTaken);

            varRefKinds     refs = varTypeIsGC(tree->TypeGet()) ? VR_IND_PTR
                                                                : VR_IND_SCL;
            desc->ivaMaskInd = varRefKinds(desc->ivaMaskInd | refs);
        }
        else if (destOper == GT_CLS_VAR)
        {
            desc->ivaMaskInd = varRefKinds(desc->ivaMaskInd | VR_GLB_VAR);
        }
        else if (destOper == GT_IND)
        {
            /* Set the proper indirection bits */

            varRefKinds refs = varTypeIsGC(tree->TypeGet()) ? VR_IND_PTR
                                                            : VR_IND_SCL;
            desc->ivaMaskInd = varRefKinds(desc->ivaMaskInd | refs);
        }
    }
    else if (tree->gtOper == GT_CALL)
    {
        isVarAssgDsc *  desc = (isVarAssgDsc*)p;
        assert(desc && desc->ivaSelf == desc);

        desc->ivaMaskCall = optCallInterf(tree);
    }

    return  WALK_CONTINUE;
}


/*****************************************************************************/

bool                Compiler::optIsVarAssigned(BasicBlock *   beg,
                                               BasicBlock *   end,
                                               GenTreePtr     skip,
                                               long           var)
{
    bool            result;
    isVarAssgDsc    desc;

    desc.ivaSkip     = skip;
#ifdef DEBUG
    desc.ivaSelf     = &desc;
#endif
    desc.ivaVar      = var;
    desc.ivaMaskCall = CALLINT_NONE;

    fgWalkTreePreReEnter();

    for (;;)
    {
        GenTreePtr      stmt;

        assert(beg);

        for (stmt = beg->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

            if  (fgWalkTreePre(stmt->gtStmt.gtStmtExpr, optIsVarAssgCB, &desc))
            {
                result = true;
                goto DONE;
            }
        }

        if  (beg == end)
            break;

        beg = beg->bbNext;
    }

    result = false;

DONE:

    fgWalkTreePreRestore();

    return  result;
}


/*****************************************************************************/

int                 Compiler::optIsSetAssgLoop(unsigned     lnum,
                                               VARSET_TP    vars,
                                               varRefKinds  inds)
{
    LoopDsc *       loop;

    /* Get hold of the loop descriptor */

    assert(lnum < optLoopCount);
    loop = optLoopTable + lnum;

    /* Do we already know what variables are assigned within this loop? */

    if  (!(loop->lpFlags & LPFLG_ASGVARS_YES))
    {
        isVarAssgDsc    desc;

        BasicBlock  *   beg;
        BasicBlock  *   end;

        /* Prepare the descriptor used by the tree walker call-back */

        desc.ivaVar     = -1;
        desc.ivaSkip    = NULL;
#ifdef DEBUG
        desc.ivaSelf    = &desc;
#endif
        desc.ivaMaskVal = 0;
        desc.ivaMaskInd = VR_NONE;
        desc.ivaMaskCall= CALLINT_NONE;
        desc.ivaMaskBad = false;

        /* Now walk all the statements of the loop */

        fgWalkTreePreReEnter();

        beg = loop->lpHead->bbNext;
        end = loop->lpEnd;

        for (/**/; /**/; beg = beg->bbNext)
        {
            GenTreePtr      stmt;

            assert(beg);

            for (stmt = beg->bbTreeList; stmt; stmt = stmt->gtNext)
            {
                assert(stmt->gtOper == GT_STMT);

                fgWalkTreePre(stmt->gtStmt.gtStmtExpr, optIsVarAssgCB, &desc);

                if  (desc.ivaMaskBad)
                {
                    loop->lpFlags |= LPFLG_ASGVARS_BAD;
                    return  -1;
                }
            }

            if  (beg == end)
                break;
        }

        fgWalkTreePreRestore();

        loop->lpAsgVars = desc.ivaMaskVal;
        loop->lpAsgInds = desc.ivaMaskInd;
        loop->lpAsgCall = desc.ivaMaskCall;

        /* Now we know what variables are assigned in the loop */

        loop->lpFlags |= LPFLG_ASGVARS_YES;
    }

    /* If we know we can't compute the mask, bail */

    if  (loop->lpFlags & LPFLG_ASGVARS_BAD)
        return  -1;

    /* Now we can finally test the caller's mask against the loop's */

    if  ((loop->lpAsgVars & vars) ||
         (loop->lpAsgInds & inds))
    {
        return  1;
    }

    switch (loop->lpAsgCall)
    {
    case CALLINT_ALL:

        /* All exprs are killed */

        return  1;

    case CALLINT_INDIRS:

        /* Object array elem assignment kills all pointer indirections */

        if  (inds & VR_IND_PTR)
            return  1;

        break;

    case CALLINT_NONE:

        /* Other helpers kill nothing */

        break;
    }

    return  0;
}

/*****************************************************************************
 *
 *  Callback (for fgWalkTreePre) used by the loop code hoisting logic.
 */

struct  codeHoistDsc
{
    Compiler    *       chComp;
#ifdef DEBUG
    void        *       chSelf;
#endif

    GenTreePtr          chHoistExpr;    // the hoisting candidate
    unsigned short      chLoopNum;      // number of the loop we're working on
    bool                chSideEffect;   // have we encountered side effects?
};

Compiler::fgWalkResult      Compiler::optHoistLoopCodeCB(GenTreePtr tree,
                                                         void *     p,
                                                         bool       prefix)
{
    /* Get hold of the descriptor */

    codeHoistDsc  * desc = (codeHoistDsc*)p;
    assert(desc && desc->chSelf == desc);

    /* After we find a side effect, we just give up */

    if  (desc->chSideEffect)
        return  WALK_ABORT;

    /* Has this tree already been marked for hoisting ? */

    if (tree->gtFlags & GTF_MAKE_CSE)
    {
        // Skip this tree and sub-trees. We can hoist subsequent trees as
        // this tree with side-effects has already been hoisted

        desc->chHoistExpr = NULL;
        return WALK_CONTINUE;
    }

    /* Is this an assignment? */

    if  (tree->OperKind() & GTK_ASGOP)
    {
        /* Is the target a simple local variable? */

        if  (tree->gtOp.gtOp1->gtOper != GT_LCL_VAR)
        {
            desc->chSideEffect = true;
            return  WALK_ABORT;
        }

        /* Assignment to a local variable, ignore it */

        return  WALK_CONTINUE;
    }

    genTreeOps      oper = tree->OperGet();

    if  (oper == GT_QMARK && tree->gtOp.gtOp1)
    {
        // UNDONE: Need to handle ?: correctly; for now just bail

        desc->chSideEffect = true;
        return  WALK_ABORT;
    }

    GenTreePtr      depx;

#if CSELENGTH
    /* An array length value depends on the array address */

    if      (oper == GT_ARR_LENREF || oper == GT_ARR_LENGTH)
    {
        depx = tree->gtArrLen.gtArrLenAdr;
    }
    else
#endif
    if      (oper == GT_ADDR && tree->gtOp.gtOp1->gtOper == GT_IND)
    {
         // For ldelema, we use GT_ADDR(GT_IND). They have to be kept together
        assert(tree->gtOp.gtOp1->gtFlags & (GTF_IND_RNGCHK | GTF_IND_FIELD));

        /* The GT_ADDR can be hoisted (along with the GT_IND, not just the
           GT_IND itself. This works as we are doing a post-walk */

        if (desc->chHoistExpr == tree->gtOp.gtOp1)
            desc->chHoistExpr =  tree;

        return WALK_CONTINUE;
    }
    else if (oper != GT_IND)
    {
        /* Not an indirection, is this a side effect? */

        if  (desc->chComp->gtHasSideEffects(tree))
        {
            desc->chSideEffect = true;
            return  WALK_ABORT;
        }

        return  WALK_CONTINUE;
    }
    else
    {
        assert(oper == GT_IND);

        depx = tree;

        /* Special case: instance variable reference */

        GenTreePtr      addr = tree->gtOp.gtOp1;

        if  (addr->gtOper == GT_ADD)
        {
            GenTreePtr      add1 = addr->gtOp.gtOp1;
            GenTreePtr      add2 = addr->gtOp.gtOp2;

            if  (add1->gtOper == GT_LCL_VAR &&
                 add2->gtOper == GT_CNS_INT)
            {
                /* Special case: "this" is almost always non-null */

                if  (add1->gtLclVar.gtLclNum == 0 && desc->chComp->optThisPtrModified)
                {
                    /* Do we already have a hoisting candidate? */

                    if  (desc->chHoistExpr)
                        return  WALK_CONTINUE;
                }
            }
        }
    }

#ifdef DEBUG
    if (verbose && 0)
    {
        printf("Considering loop hoisting candidate [cur=%08X]:\n", tree);
        desc->chComp->gtDispTree(tree);
        printf("\n");

        if  (oper == GT_ARR_LENREF)
        {
            desc->chComp->gtDispTree(depx);
            printf("\n");
        }
    }
#endif

    /* Find out what variables the expression depends on */

    varRefKinds     refs = VR_NONE;
    GenTreePtr      oldx = desc->chHoistExpr;
    VARSET_TP       deps = desc->chComp->lvaLclVarRefs(depx, &oldx, &refs);

    if  (deps == VARSET_NOT_ACCEPTABLE)
        return  WALK_CONTINUE;

    if  (oldx)
    {
        /*
            We already have a candidate and the current expression
            doesn't contain it as a sub-operand, so we'll just
            ignore the new expression and stick with the old one.
         */

        return  WALK_CONTINUE;
    }

    /* Make sure the expression is loop-invariant */

    if  (desc->chComp->optIsSetAssgLoop(desc->chLoopNum, deps, refs))
    {
        /* Can't hoist something that changes within the loop! */

        return  WALK_ABORT;
    }

    /* We have found a hoisting candidate. Keep walking to see if we can
       find another candidate which includes this tree as a subtree */

    desc->chHoistExpr = tree;

    return  WALK_CONTINUE;
}

/*****************************************************************************
 *
 *  Looks for a hoisting candidate starting at the given basic block. The idea
 *  is that we explore each path through the loop and make sure that on every
 *  trip we will encounter the same expression before any other side effects.
 *
 *  Returns -1 if a side effect is encountered, 0 if nothing interesting at
 *  all is found, and +1 if a hoist candidate is found (the candidate tree
 *  must either match "*hoistxPtr" if non-zero, or "*hoistxPtr" will be set
 *  to the hoist candidate).
 *
 *  NOTE: compCurBB is set to the main basic block (from where recursion
 *  starts). We try to find a hoisting candidate even in the presence of
 *  control flow using recursion. However, finding multiple candidates is
 *  more difficult (but possible since we have dominator info). So we only
 *  look for multiple candidates in the initial block ie. compCurBB.
 */

int                 Compiler::optFindHoistCandidate(unsigned    lnum,
                                                    unsigned    lbeg,
                                                    unsigned    lend,
                                                    BasicBlock *block,
                                                    GenTreePtr *hoistxPtr)
{
    GenTree *       stmt;
    codeHoistDsc    desc;

    int             res1;
    int             res2;

    /* Is this block outside of the loop? */

    if  (block->bbNum < lbeg)
        return  -1;
    if  (block->bbNum > lend)
        return  -1;

    /* For now, we don't try to hoist out of catch blocks */

    if  (block->bbCatchTyp)
        return  -1;

    /* Does this block have a handler? */

    if  (block->bbFlags & BBF_TRY_BEG)
    {
        /* Is this the first block in the loop? */

        if  (optLoopTable[lnum].lpEntry != block)
        {
            /*
                Not the same try block as loop (or loop isn't in one),
                don't hoist out of it.
             */

            return  -1;
        }
    }

    /* Have we visited this block before? */

    if  (block->bbFlags & BBF_VISITED)
    {
        if  (block->bbFlags & BBF_MARKED)
            return  1;
        else
            return  0;
    }

    /* Remember that we've visited this block */

    block->bbFlags |= BBF_VISITED;

    /* Look for any loop hoisting candidates in the block */

    desc.chComp         = this;
#ifdef DEBUG          
    desc.chSelf         = &desc;
#endif                  
    desc.chLoopNum      = lnum;
    desc.chSideEffect   = false;
    desc.chHoistExpr    = NULL;

    for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
    {
        assert(stmt->gtOper == GT_STMT);

//      printf("Walking     loop hoisting candidate:\n"); gtDispTree(stmt->gtStmt.gtStmtExpr); printf("\n");

    AGAIN:

        fgWalkTreePost(stmt->gtStmt.gtStmtExpr, optHoistLoopCodeCB, &desc);

        if  (desc.chHoistExpr)
        {
            /* Have we found a candidate in another block already? */

            if  (*hoistxPtr)
            {
                /* The two candidate expressions must be identical */

                if  (!GenTree::Compare(desc.chHoistExpr, *hoistxPtr))
                    return  -1;
            }
            else
            {
                /* Is this the inital block?, If so, there is no control
                   flow yet, and hence no recursion. So we can look for
                   multiple candidates */

                if (block == compCurBB)
                {
                    optHoistCandidateFound(lnum, desc.chHoistExpr);

                    /* Reset and keep looking for more candidates. Start with
                       the same statement as there may be other hoistable
                       expressions. */
                    
                    /* Old candidates will be marked with GTF_MAKE_CSE, and
                       hence we will skip them during the tree walk.
                       NOTE: Even though we skip previously hoisted expression
                       trees, we wont skip similar expressions. So CONSIDER
                       keeping a list of hoisted expressions and using
                       GenTree::Compare() against them.
                       */

                    assert(GTF_MAKE_CSE & desc.chHoistExpr->gtFlags);
                    desc.chHoistExpr = NULL;

                    goto AGAIN;
                }

                *hoistxPtr = desc.chHoistExpr;
            }

            /* Remember that this block has a hoistable expression */

            block->bbFlags |= BBF_MARKED;

            return  +1;
        }
        else if (desc.chSideEffect)
        {
            return -1;
        }
    }

    /* Nothing interesting found in this block, consider its successors */

    switch (block->bbJumpKind)
    {
    case BBJ_COND:
        res1 = optFindHoistCandidate(lnum,
                                     lbeg,
                                     lend,
                                     block->bbJumpDest,
                                     hoistxPtr);

        if  (res1 == -1)
            return  -1;

        block = block->bbNext;
        break;

    case BBJ_ALWAYS:
        block = block->bbJumpDest;
        res1 = 1;
        break;

    case BBJ_NONE:
        block = block->bbNext;
        res1  = 1;
        break;

    case BBJ_RET:
    case BBJ_CALL:
    case BBJ_THROW:
    case BBJ_RETURN:
        return  -1;

    case BBJ_SWITCH:
        // @TODO [CONSIDER] [04/16/01] []: Don't be lazy and add support for switches
        return  -1;
    }

    /* Here we have BBJ_NONE/BBJ_COND/BBJ_ALWAYS */

    res2 = optFindHoistCandidate(lnum, lbeg, lend, block, hoistxPtr);
    if  (res2 == -1)
        return  res2;

    return res1 & res2;
}

/*****************************************************************************
 *
 */

void                    Compiler::optHoistCandidateFound(unsigned   lnum,
                                                         GenTreePtr hoist)
{
#ifdef DEBUG
    GenTreePtr      orig = hoist;
#endif
    /* Create a copy of the expression and mark it for CSE's */

    hoist->gtFlags |= GTF_MAKE_CSE;

#if CSELENGTH
    if  (hoist->gtOper == GT_ARR_LENREF)
    {
        GenTreePtr      oldhx;

        /* Make sure we clone the address expression */

        oldhx = hoist;
        oldhx->gtFlags |= GTF_ALN_CSEVAL;
        hoist = gtCloneExpr(oldhx, GTF_MAKE_CSE);
        oldhx->gtFlags &= ~GTF_ALN_CSEVAL;
    }
    else
#endif
        hoist = gtCloneExpr(hoist, GTF_MAKE_CSE);

    hoist->gtFlags |= GTF_MAKE_CSE;

    /* The value of the expression isn't used */

    hoist = gtUnusedValNode(hoist);

    hoist = fgMorphTree(hoist);

    hoist = gtNewStmt(hoist);
    hoist->gtFlags |= GTF_STMT_CMPADD;

    /* Put the statement in the preheader */

    fgCreateLoopPreHeader(lnum);

    BasicBlock *  preHead = optLoopTable[lnum].lpHead;
    assert (preHead->bbJumpKind == BBJ_NONE);

    /* simply append the statement at the end of the preHead's list */

    GenTreePtr treeList = preHead->bbTreeList;

    if (treeList)
    {
        /* append after last statement */

        GenTreePtr  last = treeList->gtPrev;
        assert (last->gtNext == 0);

        last->gtNext        = hoist;
        hoist->gtPrev       = last;
        treeList->gtPrev    = hoist;
    }
    else
    {
        /* Empty pre-header - store the single statement in the block */

        preHead->bbTreeList = hoist;
        hoist->gtPrev       = hoist;
    }

    hoist->gtNext       = 0;

#ifdef DEBUG
    if (verbose)
    {
//      printf("Copying expression to hoist:\n");
//      gtDispTree(orig);
        printf("Hoisted copy of %08X for loop <%u..%u>:\n",
                orig, preHead->bbNext->bbNum, optLoopTable[lnum].lpEnd->bbNum);
        gtDispTree(hoist->gtStmt.gtStmtExpr->gtOp.gtOp1);
//      printf("\n");
//      fgDispBasicBlocks(false);
        printf("\n");
    }
#endif
}

/*****************************************************************************
 *
 *  Look for expressions to hoist out of loops.
 */

void                    Compiler::optHoistLoopCode()
{
#ifdef DEBUG
    if  (verbose)
        printf("*************** In optHoistLoopCode()\n");
#endif

    for (unsigned lnum = 0; lnum < optLoopCount; lnum++)
    {
        /* If loop was removed continue */

        if  (optLoopTable[lnum].lpFlags & LPFLG_REMOVED)
            continue;

        /* Get the head and tail of the loop */

        BasicBlock *    head = optLoopTable[lnum].lpHead;
        BasicBlock *    tail = optLoopTable[lnum].lpEnd;
        BasicBlock *    lbeg = optLoopTable[lnum].lpEntry;
        BasicBlock *    block;

        /* Make sure we have a do-while loop such that the 
           loop-head always executes before we enter the loop.
           The loop-head must dominate the loop-entry */

        if ( ((optLoopTable[lnum].lpFlags & LPFLG_DO_WHILE) == 0) ||
             (head->bbDom == NULL)                                ||
             !fgDominate(head, lbeg)                 )
        {
            continue;
        }

        /* For now, we don't try to hoist out of catch blocks */

        if  (lbeg->bbCatchTyp)
            continue;

        unsigned        begn = lbeg->bbNum;
        unsigned        endn = tail->bbNum;

//      fgDispBasicBlocks(false);

        /* Make sure the "visited" bit is cleared for all blocks */

#ifdef DEBUG
        block = head;
        do
        {
            block = block->bbNext;

            assert(block && (block->bbFlags & (BBF_VISITED|BBF_MARKED)) == 0);

        }
        while (block != tail);
#endif

        /* Recursively look for a hoisting candidate */

        GenTree *   hoist = 0;

        compCurBB = lbeg;

        bool found = (1 == optFindHoistCandidate(lnum, begn, endn, lbeg, &hoist));

        assert(!found || hoist);

        if (found)
            optHoistCandidateFound(lnum, hoist);

        /* Now clear all the "visited" bits on all the blocks */

        block = head;
        do
        {
            block = block->bbNext; assert(block);
            block->bbFlags &= ~(BBF_VISITED|BBF_MARKED);
        }
        while (block != tail);

        /* This stuff is OK, but disabled until we fix the problem of
         * keeping the dominators in synch and remove the limitation
         * on the number of BB */

        if (true)
            continue;

        /* Look for loop invariant statements
         * For now consider only single exit loops since we will
         * have to show that the invariant dominates all exits */

        if (!optLoopTable[lnum].lpExit)
            continue;

        assert (optLoopTable[lnum].lpFlags & LPFLG_ONE_EXIT);

        /* Conditions to hoist an invariant statement s (of the form "x = something"):
         *    1. The statement has to dominate all loop exits
         *    2. x is never assigned in the loop again
         *    3. Any use of x is used by this definition of x (i.e the block dominates all uses of x)
         *    4. The are no side effects on any path from the ENTRY to s
         */

        /* For now consider only the first BB since it will automatically satisfy 1 and 3 above */

        GenTreePtr  stmt;
        GenTreePtr  tree;

        for (stmt = lbeg->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert (stmt->gtOper == GT_STMT);

            tree = stmt->gtStmt.gtStmtExpr;
            assert(tree);

            /* if any side effect encountered bail - satisfy condition 4 */

            if (tree->gtFlags & (GTF_SIDE_EFFECT & ~GTF_ASG))
                break;

            /* interested only in assignments */

            if (tree->gtOper != GT_ASG)
                continue;

            /* has to be an assignment to a local var */

            GenTreePtr  op1 = tree->gtOp.gtOp1;
            GenTreePtr  op2 = tree->gtOp.gtOp2;

            if (op1->gtOper != GT_LCL_VAR)
                continue;

            if (!optIsTreeLoopInvariant(lnum, lbeg, tail, op2))
                continue;

            /* Great - the RHS is loop invariant, now we have to make sure
             * the local var in LSH is never re-defined in the loop */

            assert (op1->gtOper == GT_LCL_VAR);

            if (optIsVarAssigned(lbeg, tail, tree, op1->gtLclVar.gtLclNum))
                continue;

            /* Yupee - we have an invariant statement - Remove it from the
             * current block and put it in the preheader */

#ifdef  DEBUG
            if  (verbose)
            {
                printf("Hoisting invariant statement from loop L%02u (BB%02u - BB%02u)\n",
                       lnum, lbeg->bbNum, tail->bbNum);
                gtDispTree(stmt);
                printf("\n");
            }
#endif

            /* remove the invariant statement from the loop (remember is in the first block) */

            assert (lbeg == optLoopTable[lnum].lpHead->bbNext);
            assert (lbeg == optLoopTable[lnum].lpEntry);
            fgRemoveStmt(lbeg, stmt);

            /* put the invariant statement in the pre-header */

            BasicBlock  * preHead;
            fgCreateLoopPreHeader(lnum);

            assert (optLoopTable[lnum].lpHead->bbJumpKind == BBJ_NONE);
            preHead = optLoopTable[lnum].lpHead;

            assert (preHead->bbJumpKind == BBJ_NONE);
            assert (preHead->bbNext == optLoopTable[lnum].lpEntry);

            /* simply append the statement at the end of the preHead's list */

            tree = preHead->bbTreeList;

            if (tree)
            {
                /* append after last statement */

                GenTreePtr  last = tree->gtPrev;
                assert (last->gtNext == 0);

                last->gtNext        = stmt;
                stmt->gtNext        = 0;
                stmt->gtPrev        = last;
                tree->gtPrev        = stmt;
            }
            else
            {
                /* Empty pre-header - store the single statement in the block */

                preHead->bbTreeList = stmt;
                stmt->gtNext        = 0;
                stmt->gtPrev        = stmt;
            }
        }

        assert(!"This has been disabled for now");

        /* Look for loop "iterative" invariants and put them in "post-blocks" */
    }
}


/*****************************************************************************
 *
 *  Creates a pre-header block for the given loop - a preheader is a BBJ_NONE
 *  header. The pre-header will replace the current lpHead in the loop table.
 *  The loop has to be a do-while loop. Thus, all blocks dominated by lpHead
 *  will also be dominated by the loop-top, lpHead->bbNext. 
 *
 */

void                 Compiler::fgCreateLoopPreHeader(unsigned   lnum)
{
    /* This loop has to be a "do-while" loop */

    assert (optLoopTable[lnum].lpFlags & LPFLG_DO_WHILE);
    
    /* Have we already created a loop-preheader block? */

    if (optLoopTable[lnum].lpFlags & LPFLG_HAS_PREHEAD)
        return;

    BasicBlock   *   head = optLoopTable[lnum].lpHead;
    BasicBlock   *   top = head->bbNext;

    // Insure that lpHead always dominates lpEntry

    assert((head->bbDom != NULL) && 
            fgDominate(head, optLoopTable[lnum].lpEntry));
    
    /* Get hold of the first block of the loop body */

    assert (top == optLoopTable[lnum].lpEntry);

    /* Allocate a new basic block */

    BasicBlock * preHead = bbNewBasicBlock(BBJ_NONE);
    preHead->bbFlags    |= BBF_INTERNAL | BBF_LOOP_PREHEADER;
    preHead->bbNext      = top;
    head->bbNext         = preHead;

#ifdef  DEBUG
    if  (verbose)
        printf("Creating PreHeader (BB%02u) for loop L%02u (BB%02u - BB%02u)\n\n",
               preHead->bbNum, lnum, top->bbNum, optLoopTable[lnum].lpEnd->bbNum);
#endif

    // Must set IL code offset
    preHead->bbCodeOffs  = top->bbCodeOffs;

    // top should be weighted like a loop
    assert(top->bbWeight > BB_UNITY_WEIGHT);

    // Set the preHead weight
    preHead->bbWeight    = (top->bbWeight + BB_UNITY_WEIGHT) / BB_LOOP_WEIGHT;
    // Increase the weight of the loop pre-header block
    preHead->bbWeight   *= 2;

    if (top->bbFlags & BBF_HAS_HANDLER)
    {
        assert(top->hasTryIndex());
        preHead->bbFlags    |= BBF_HAS_HANDLER;
        preHead->bbTryIndex  = top->bbTryIndex;
    }

    // TODO: set dominators for this block, to allow loop optimizations requiring them (e.g: hoisting expression in a loop with the same 'head' as this one)

    /* Update the loop entry */

    optLoopTable[lnum].lpHead   = preHead;
    optLoopTable[lnum].lpFlags |= LPFLG_HAS_PREHEAD;

    /* The new block becomes the 'head' of the loop - update bbRefs and bbPreds
       All predecessors of 'beg', (which is the entry in the loop)
       now have to jump to 'preHead', unless they are dominated by 'head' */

    preHead->bbRefs = 0;
    fgAddRefPred(preHead, head);
    bool checkNestedLoops = false;

    for (flowList * pred = top->bbPreds; pred; pred = pred->flNext)
    {
        BasicBlock  *   predBlock = pred->flBlock;

        if(fgDominate(top, predBlock))
        {   // note: if 'top' dominates predBlock, 'head' dominates predBlock too
            // (we know that 'head' dominates 'top'), but using 'top' instead of
            // 'head' in the test allows us to not enter here if 'predBlock == head'

            if (predBlock != optLoopTable[lnum].lpEnd)
            {
                assert(predBlock != head);
                checkNestedLoops = true;
            }
            continue;
        }

        switch(predBlock->bbJumpKind)
        {
        case BBJ_NONE:
            assert(predBlock == head);
            break;

        case BBJ_COND:
            if  (predBlock == head)
            {
                assert(predBlock->bbJumpDest != top);
                break;
            }

            /* Fall through for the jump case */

        case BBJ_ALWAYS:
            assert(predBlock->bbJumpDest == top);
            predBlock->bbJumpDest = preHead;

            assert(top->bbRefs);
            top->bbRefs--;
            fgRemovePred(top, predBlock);
            fgAddRefPred(preHead, predBlock);
            preHead->bbFlags |= BBF_JMP_TARGET|BBF_HAS_LABEL;
            break;

        case BBJ_SWITCH:
            unsigned        jumpCnt = predBlock->bbJumpSwt->bbsCount;
            BasicBlock * *  jumpTab = predBlock->bbJumpSwt->bbsDstTab;

            do
            {
                assert (*jumpTab);
                if ((*jumpTab) == top)
                {
                    (*jumpTab) = preHead;

                    assert(top->bbRefs);
                    top->bbRefs--;
                    fgRemovePred(top, predBlock);
                    fgAddRefPred(preHead, predBlock);
                    preHead->bbFlags |= BBF_JMP_TARGET|BBF_HAS_LABEL;
                }
            }
            while (++jumpTab, --jumpCnt);
        }
    }

    assert(!fgIsPredForBlock(top, preHead));
    assert(top->bbRefs);
    top->bbRefs--;
    fgRemovePred(top, head);
    fgAddRefPred(top, preHead);

    /* 
        If we found at least one back-edge in the flowgraph pointing to the top/entry of the loop
        (other than the back-edge of the loop we are considering) then we likely have nested
        do-while loops with the same entry block and inserting the preheader block changes the head
        of all the nested loops. Now we will update this piece of information in the loop table, and
        mark all nested loops as having a preheader (the preheader block can be shared among all nested
        do-while loops with the same entry block).
    */
    if (checkNestedLoops)
    {
        for (unsigned l = 0; l < optLoopCount; l++)
        {
            if (optLoopTable[l].lpHead == head)
            {
                assert(l != lnum);  // optLoopTable[lnum].lpHead was already changed from 'head' to 'preHead'
                assert(optLoopTable[l].lpEntry == top);
                optLoopTable[l].lpHead = preHead;
                optLoopTable[l].lpFlags |= LPFLG_HAS_PREHEAD;
#ifdef  DEBUG
                if  (verbose)
                    printf("Same PreHeader (BB%02u) can be used for loop L%02u (BB%02u - BB%02u)\n\n",
                           preHead->bbNum, l, top->bbNum, optLoopTable[l].lpEnd->bbNum);
#endif
            }
        }
    }
    
    /*
        If necessary update the EH table to make the hoisted block
        part of the loop's EH block.
    */

    unsigned        XTnum;
    EHblkDsc *      HBtab;

    for (XTnum = 0, HBtab = compHndBBtab;
         XTnum < info.compXcptnsCount;
         XTnum++  , HBtab++)
    {
        /* If try/catch began at the loop, it now begins at hoist block */
        if  (HBtab->ebdTryBeg == top)
        {
            HBtab->ebdTryBeg->bbFlags &= ~(BBF_TRY_BEG | BBF_DONT_REMOVE);
            HBtab->ebdTryBeg           =  preHead;
            HBtab->ebdTryBeg->bbFlags |=  BBF_TRY_BEG | BBF_DONT_REMOVE | BBF_HAS_LABEL;
        }
        /* If try/catch ended at the loop, it now ends at hoist block */
        else if  (HBtab->ebdTryEnd == top)
        {
            HBtab->ebdTryEnd->bbFlags &= ~(BBF_TRY_HND_END | BBF_DONT_REMOVE);
            HBtab->ebdTryEnd           =  preHead;
            HBtab->ebdTryEnd->bbFlags |=  BBF_TRY_HND_END | BBF_DONT_REMOVE | BBF_HAS_LABEL;
        }
        /* If exception handler began at the loop, it now begins at hoist block */
        else if  (HBtab->ebdHndBeg == top)
        {
            HBtab->ebdHndBeg->bbFlags &= ~BBF_DONT_REMOVE;
            HBtab->ebdHndBeg->bbRefs--;
            HBtab->ebdHndBeg           =  preHead;
            HBtab->ebdHndBeg->bbFlags |=  BBF_DONT_REMOVE | BBF_HAS_LABEL;
            HBtab->ebdHndBeg->bbRefs++;
        }
        /* If exception handler ended at the loop, it now ends at hoist block */
        else if  (HBtab->ebdHndEnd == top)
        {
            HBtab->ebdHndEnd->bbFlags &= ~(BBF_TRY_HND_END | BBF_DONT_REMOVE);
            HBtab->ebdHndEnd           =  preHead;
            HBtab->ebdHndEnd->bbFlags |=  BBF_TRY_HND_END | BBF_DONT_REMOVE | BBF_HAS_LABEL;
        }
        /* If the filter began at the loop, it now begins at hoist block */
        else if ((HBtab->ebdFlags & CORINFO_EH_CLAUSE_FILTER) &&
                 (HBtab->ebdFilter == top))
        {
            HBtab->ebdFilter->bbFlags &= ~BBF_DONT_REMOVE;
            HBtab->ebdFilter->bbRefs--;
            HBtab->ebdFilter           =  preHead;
            HBtab->ebdFilter->bbFlags |=  BBF_DONT_REMOVE | BBF_HAS_LABEL;
            HBtab->ebdFilter->bbRefs++;
        }
    }
}


/*****************************************************************************
 *
 *  Given a loop and a tree, checks if the tree is loop invariant
 *  i.e. has no side effects and all variables being part of it are
 *  never assigned in the loop
 */

bool                 Compiler::optIsTreeLoopInvariant(unsigned        lnum,
                                                      BasicBlock  *   top,
                                                      BasicBlock  *   bottom,
                                                      GenTreePtr      tree)
{
    assert (optLoopTable[lnum].lpEnd   == bottom);
    assert (optLoopTable[lnum].lpEntry == top);

    assert (!(tree->gtFlags & GTF_SIDE_EFFECT));

    switch (tree->gtOper)
    {
    case GT_CNS_INT:
    case GT_CNS_LNG:
    case GT_CNS_DBL:
        return true;

    case GT_LCL_VAR:
        return !optIsVarAssigned(top, bottom, 0, tree->gtLclVar.gtLclNum);

    case GT_ADD:
    case GT_SUB:
    case GT_MUL:
    case GT_DIV:
    case GT_MOD:

    case GT_OR:
    case GT_XOR:
    case GT_AND:

    case GT_LSH:
    case GT_RSH:
    case GT_RSZ:

        assert(tree->OperKind() & GTK_BINOP);

        GenTreePtr  op1 = tree->gtOp.gtOp1;
        GenTreePtr  op2 = tree->gtOp.gtOp2;

        assert(op1 && op2);

        return (optIsTreeLoopInvariant(lnum, top, bottom, op1) &&
                optIsTreeLoopInvariant(lnum, top, bottom, op2));
    }

    return false;
}

/*****************************************************************************/
#endif  // CSE
/*****************************************************************************
 *
 *  Callback (for fgWalkTreePre) used by the increment / range check optimization
 *  code.
 */

struct optIncRngDsc
{
    // Fields common to all phases:

    Compiler    *       oirComp;
    unsigned short      oirPhase;    // which pass are we performing?

    var_types           oirElemType;
    bool                oirSideEffect;

    unsigned char       oirFoundX:1;// have we found an array/index pair?
    unsigned char       oirExpVar:1;// have we just expanded an index value?

    // Debugging fields:

#ifdef DEBUG
    void    *           oirSelf;
#endif

    BasicBlock  *       oirBlock;
    GenTreePtr          oirStmt;

    unsigned short      oirArrVar;  // # of index variable
    unsigned short      oirInxVar;  // # of index variable

    unsigned short      oirInxCnt;  // # of uses of index variable as index
    unsigned short      oirInxUse;  // # of uses of index variable, overall

    unsigned short      oirInxOff;  // how many times index incremented?
};

Compiler::fgWalkResult      Compiler::optIncRngCB(GenTreePtr tree, void *p)
{
    optIncRngDsc*   desc;
    GenTreePtr      expr;

    /* Get hold of the descriptor */

    desc = (optIncRngDsc*)p; assert(desc && desc->oirSelf == desc);

    /* After we find a side effect, we just give up */

    if  (desc->oirSideEffect)
        return  WALK_ABORT;

    /* Is this an assignment? */

    if  (tree->OperKind() & GTK_ASGOP)
    {
        /* Is the target a simple local variable? */

        expr = tree->gtOp.gtOp1;
        if  (expr->gtOper != GT_LCL_VAR)
            goto SIDE_EFFECT;

        /* Is this either the array or the index variable? */

        if  (expr->gtLclVar.gtLclNum == desc->oirInxVar ||
             expr->gtLclVar.gtLclNum == desc->oirArrVar)
        {
            /* Variable is modified, consider this a side effect */

            goto SIDE_EFFECT;
        }

        /* Assignment to a boring local variable, ignore it */

        return  WALK_CONTINUE;
    }

    /* Is this an index expression? */

    if  (tree->gtOper == GT_INDEX)
    {
        int         arrx;
        int         indx;

        /* Is the array address a simple local variable? */

        expr = tree->gtOp.gtOp1;
        if  (expr->gtOper != GT_LCL_VAR)
            goto SIDE_EFFECT;

        arrx = expr->gtLclVar.gtLclNum;

        /* Is the index value   a simple local variable? */

        expr = tree->gtOp.gtOp2;
        if  (expr->gtOper != GT_LCL_VAR)
            goto SIDE_EFFECT;

        indx = expr->gtLclVar.gtLclNum;

        /* Have we decided which array and index to track? */

        if  (arrx != desc->oirArrVar ||
             indx != desc->oirInxVar)
        {
            /* If we have decided, these must be the wrong variables */

            if  (desc->oirFoundX)
                goto SIDE_EFFECT;

            /* Looks like we're deciding now */

            desc->oirArrVar = arrx;
            desc->oirInxVar = indx;
            desc->oirFoundX = true;
        }

        /* Are we in the second phase? */

        if  (desc->oirPhase == 2)
        {
            /* This range check is eliminated */

            tree->gtFlags &= ~GTF_INX_RNGCHK;

            /* Record the element type while we're at it */

            desc->oirElemType = tree->TypeGet();
        }

        /* Count this as a use as array index */

        desc->oirInxCnt++;

        return  WALK_CONTINUE;
    }

    /* Is this a use of the index variable? */

    if  (tree->gtOper == GT_LCL_VAR)
    {
        /* Is this a use of the index variable? */

        if  (tree->gtLclVar.gtLclNum == desc->oirInxVar)
        {
            /* Count this as a use as array index */

            desc->oirInxUse++;

            /* Are we in the second phase? */

            if  (desc->oirPhase == 2)
            {
                /* Add the appropriate offset to the variable value */

                if  (desc->oirInxOff)
                {
                    /* Avoid recursive death */

                    if  (desc->oirExpVar)
                    {
                        desc->oirExpVar = false;
                    }
                    else
                    {
                        tree->ChangeOper  (GT_ADD);
                        tree->gtOp.gtOp1 = desc->oirComp->gtNewLclvNode(desc->oirInxVar, TYP_INT);
                        tree->gtOp.gtOp2 = desc->oirComp->gtNewIconNode(desc->oirInxOff, TYP_INT);

                        desc->oirExpVar = true;
                    }
                }
            }
        }
    }

    /* Are any other side effects here ? */

    if  (!desc->oirComp->gtHasSideEffects(tree))
        return  WALK_CONTINUE;

SIDE_EFFECT:

//  printf("Side effect:\n"); desc->oirComp->gtDispTree(tree); printf("\n\n");

    desc->oirSideEffect = true;
    return  WALK_ABORT;
}

/*****************************************************************************
 *
 *  Try to optimize series of increments and array index expressions.
 */

void                Compiler::optOptimizeIncRng()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In optOptimizeIncRng()\n");
#endif
    
    BasicBlock  *   block;
    optIncRngDsc    desc;

    /* Did we find enough increments to make this worth while? */

    if  (fgIncrCount < 10)
        return;

    /* First pass: look for increments followed by index expressions */

    desc.oirComp  = this;
#ifdef DEBUG
    desc.oirSelf  = &desc;
#endif

    /*
        Walk all the basic blocks, and look for blocks that are known
        to contain both index and increment expressions. For all such
        blocks, walk their trees and do the following:

            Remember the last index expression found. Also remember
            where the last side effect was encountered.

            When an increment is found, see if the variable matches
            the index of the last index expression recorded above,
            if it does this will determine the array and index we
            will try to optimize.

        The values in the following loop are as follows:


                ..... stuff ......
                ..... stuff ......

            sideEff:

                ..... last stmt with unrelated side effects .....

            arrStmt:

                ..... stmt 1 with a matching array expr  .....

                ..... increment 1 .....

                ..... stmt 2 with a matching array expr  .....

                ..... increment 2 .....

                ...
                ... above two repeated N times
                ...

            arrLast:

                ..... stmt <N> with a matching array expr  .....

            incLast:

                ..... increment <N> .....

     */

    for (block = fgFirstBB; block; block = block->bbNext)
    {
        GenTreePtr      sideEff;        // stmt with last side-effect

        GenTreePtr      arrStmt;        // stmt with first array ref
        GenTreePtr      arrLast;        // stmt with last  array ref
        unsigned        arrLstx;        // index of above
        GenTreePtr      incLast;        // stmt with last  increment
        unsigned        incLstx;        // index of above

        unsigned        arrXcnt;        // total number of array exprs

        GenTreePtr      stmt;
        unsigned        snum;

        if  ((block->bbFlags & BBF_HAS_HANDLER) != 0)
            continue;
        if  ((block->bbFlags & BBF_HAS_INC    ) == 0)
            continue;
        if  ((block->bbFlags & BBF_HAS_INDX   ) == 0)
            continue;

        /* This basic block shows potential, let's process it */

        desc.oirBlock  = compCurBB = block;
        desc.oirFoundX = false;
        desc.oirInxOff = 0;

        /* Remember the statements with interesting things in them */

        sideEff =
        arrStmt = 0;
        arrXcnt = 0;

        /* Phase 1 of our algorithm is now beginning */

        desc.oirPhase = 1;

        for (stmt = block->bbTreeList, snum = 0;
             stmt;
             stmt = stmt->gtNext     , snum++)
        {
            GenTreePtr      expr;

            assert(stmt->gtOper == GT_STMT); expr = stmt->gtStmt.gtStmtExpr;

            /* Is this an integer increment statement? */

            if  (expr->gtOper == GT_ASG_ADD && desc.oirFoundX)
            {
                GenTreePtr      op1 = expr->gtOp.gtOp1;
                GenTreePtr      op2 = expr->gtOp.gtOp2;

                if  (op1->gtOper != GT_LCL_VAR)
                    goto NOT_INC1;
                if  (op1->gtLclVar.gtLclNum != desc.oirInxVar)
                    goto NOT_INC1;

                if  (op2->gtOper != GT_CNS_INT)
                    goto NOT_INC1;
                if  (op2->gtIntCon.gtIconVal != 1)
                    goto NOT_INC1;

                /* We've found an increment of the index variable */

//              printf("Increment index [%u]:\n", desc.oirInxOff); gtDispTree(expr);

                desc.oirInxOff++;

                /* Remember the last increment statement */

                incLast = stmt;
                incLstx = snum;

                /* Continue if there are any more statements left */

                if  (stmt->gtNext)
                    continue;

                /* We've reached the end of the basic block */

                goto END_LST;
            }

        NOT_INC1:

            /* Recursively process this tree */

            desc.oirStmt       = stmt;
            desc.oirSideEffect = false;
            desc.oirInxUse     =
            desc.oirInxCnt     = 0;

            fgWalkTreePre(expr, optIncRngCB, &desc);

            /* Was there a side effect in the expression? */

            if  (desc.oirSideEffect)
                goto END_LST;

            /* Was the index variable used after being incremented? */

            if  (desc.oirInxUse > desc.oirInxCnt)
            {
                /* For now, we simply give up */

                goto END_LST;
            }

            /* Did the expression contain interesting array exprs? */

            if  (desc.oirInxCnt)
            {
                assert(desc.oirFoundX);

                /* Record the last array statement and the total count */

                arrXcnt += desc.oirInxCnt;
                arrLast  = stmt;
                arrLstx  = snum;
            }

            /* Is this the last statement of this basic block? */

            if  (stmt->gtNext == NULL)
                goto END_LST;

            /* Have we found an incremented index/array pair yet? */

            if  (desc.oirFoundX && desc.oirInxOff == 0)
            {
                /* Remember the first stmt with an index in it */

                arrStmt = stmt;
            }

            continue;

        END_LST:

            /* Have we decided on an array expression yet? */

            if  (desc.oirFoundX)
            {
                /* End of list; did we find enough interesting expressions? */

                if  (desc.oirInxOff >= 3 && arrXcnt >= 3)
                {
                    GenTreePtr      list;
                    GenTreePtr      ends;

                    GenTreePtr      rng1;
                    GenTreePtr      rng2;

                    GenTreePtr      next;

                    /* Process statements from "arrStmt" to incLast */

                    list = arrStmt; assert(list);
                    ends = incLast; assert(ends && ends != list);

                    /* Begin phase 2 of tree walking */

                    desc.oirInxOff = 0;
                    desc.oirPhase  = 2;

                    for (;;)
                    {
                        GenTreePtr      expr;

                    AGAIN:

                        assert(list->gtOper == GT_STMT); expr = list->gtStmt.gtStmtExpr;

                        /* Is this an integer increment statement? */

                        if  (expr->gtOper == GT_ASG_ADD)
                        {
                            GenTreePtr      prev;
                            GenTreePtr      next;

                            GenTreePtr      op1 = expr->gtOp.gtOp1;
                            GenTreePtr      op2 = expr->gtOp.gtOp2;

                            if  (op1->gtOper != GT_LCL_VAR)
                                goto NOT_INC2;
                            if  (op1->gtLclVar.gtLclNum != desc.oirInxVar)
                                goto NOT_INC2;

                            if  (op2->gtOper != GT_CNS_INT)
                                goto NOT_INC2;
                            if  (op2->gtIntCon.gtIconVal != 1)
                                goto NOT_INC2;

                            /* Keep track of how many times incremented */

                            desc.oirInxOff++;

                            /* Is this the last increment? */

                            if  (list == ends)
                            {
                                /* Replace with "+= total_count" */

                                op2->gtIntCon.gtIconVal = desc.oirInxOff;

                                /* We're done now */

                                break;
                            }

                            /* Remove this increment statement */

                            prev = list->gtPrev; assert(prev);
                            next = list->gtNext; assert(next);

                            assert(prev->gtNext == list);
                            assert(next->gtPrev == list);

                            prev->gtNext = next;
                            next->gtPrev = prev;

                            /* Don't follow the "next" link, block is now gone */

                            list = next;

                            goto AGAIN;
                        }

                    NOT_INC2:

                        assert(list != ends);

                        /* Recursively process this tree */

                        desc.oirStmt       = list;
                        desc.oirSideEffect = false;
                        desc.oirInxUse     =
                        desc.oirInxCnt     = 0;

                        fgWalkTreePre(expr, optIncRngCB, &desc);

                        if  (list == ends)
                            break;

                        list = list->gtNext;
                    }

                    /* Create the combined range checks */

                    rng1 = gtNewIndexRef(desc.oirElemType,
                                         gtNewLclvNode(desc.oirArrVar  , TYP_REF),
                                         gtNewLclvNode(desc.oirInxVar  , TYP_INT));

                    rng1->gtFlags |= GTF_DONT_CSE;
                    rng1 = gtNewStmt(rng1);
                    rng1->gtFlags |= GTF_STMT_CMPADD;

                    rng2 = gtNewOperNode(GT_ADD,
                                         TYP_INT,
                                         gtNewLclvNode(desc.oirInxVar  , TYP_INT),
                                         gtNewIconNode(desc.oirInxOff-1, TYP_INT));

                    rng2 = gtNewIndexRef(desc.oirElemType,
                                         gtNewLclvNode(desc.oirArrVar  , TYP_REF),
                                         rng2);

                    rng2->gtFlags |= GTF_DONT_CSE;
                    rng2 = gtNewStmt(rng2);
                    rng2->gtFlags |= GTF_STMT_CMPADD;

                    rng1->gtNext = rng2;
                    rng2->gtPrev = rng1;

                    /* Insert the combined range checks */

                    list = sideEff;
                    if  (list)
                    {
                        next = list->gtNext;

                        list->gtNext = rng1;
                        rng1->gtPrev = list;
                    }
                    else
                    {
                        next = block->bbTreeList;
                               block->bbTreeList = rng1;

                        rng1->gtPrev = next->gtPrev;
                    }

                    next->gtPrev = rng2;
                    rng2->gtNext = next;
                }

                /* Clear everything, we're starting over */

                desc.oirInxVar = -1;
                desc.oirArrVar = -1;
                desc.oirInxOff = 0;
            }
            else
            {
                /* Remember this as the last side-effect statement */

                sideEff = stmt;

                /* We have not found any matching array expressions */

                arrXcnt = 0;
            }
        }
    }
}

/*****************************************************************************
 *
 *  Given an array index node, mark it as not needing a range check.
 */

void                Compiler::optRemoveRangeCheck(GenTreePtr tree,
                                                  GenTreePtr stmt,
                                                  bool       updateCSEcounts)
{
    GenTreePtr      add1;
    GenTreePtr  *   addp;

    GenTreePtr      nop1;
    GenTreePtr  *   nopp;

    GenTreePtr      icon;
    GenTreePtr      mult;

    GenTreePtr      temp;
    GenTreePtr      base;

    long            ival;

#if !REARRANGE_ADDS
    assert(!"can't remove range checks without REARRANGE_ADDS right now");
#endif

    assert(stmt->gtOper     == GT_STMT);
    assert(tree->gtOper     == GT_IND);
    assert(tree->gtOp.gtOp2 == 0);
    assert(tree->gtFlags & GTF_IND_RNGCHK);
    assert(optIsRangeCheckRemovable(tree));
    
    /* Unmark the topmost node */

    tree->gtFlags &= ~GTF_IND_RNGCHK;
#ifdef DEBUG
    if (verbose) 
        printf("Eliminating range check for [%08X]\n", tree);
#endif

#if CSELENGTH

    /* Is there an array length expression? */

    if  (tree->gtInd.gtIndLen)
    {
        GenTreePtr      len = tree->gtInd.gtIndLen;

        assert(len->gtOper == GT_ARR_LENREF);

        if (updateCSEcounts)
        {
            /* Unmark if it is a CSE */
            if (IS_CSE_INDEX(len->gtCSEnum))
                optUnmarkCSE(len);
        }

        tree->gtInd.gtIndLen = NULL;
    }

#endif

    /* Locate the 'nop' node so that we can remove it */

    addp = &tree->gtOp.gtOp1;
    add1 = *addp; assert(add1->gtOper == GT_ADD);

    /* Is "+icon" present? */

    icon = 0;

    if  ((add1->gtOp.gtOp2->gtOper == GT_CNS_INT) &&
         (add1->gtOp.gtOp2->gtType != TYP_REF))
    {
        icon =  add1->gtOp.gtOp2;
        addp = &add1->gtOp.gtOp1;
        add1 = *addp;
    }

    /* 'addp' points to where 'add1' came from, 'add1' must be a '+' */

    assert(*addp == add1); assert(add1->gtOper == GT_ADD);

    /*  Figure out which is the array address and which is the index;
        the index value is always a 'NOP' node possibly wrapped with
        a multiplication (left-shift) operator.
     */

    temp = add1->gtOp.gtOp1;
    base = add1->gtOp.gtOp2;

    if      (temp->gtOper == GT_NOP)
    {
        /* 'op1' is the index value, and it's not multiplied */

        mult = 0;

        nopp = &add1->gtOp.gtOp1;
        nop1 =  add1->gtOp.gtOp1;

        assert(base->gtType == TYP_REF);
    }
    else if ((temp->gtOper == GT_LSH || temp->gtOper == GT_MUL) && temp->gtOp.gtOp1->gtOper == GT_NOP)
    {
        /* 'op1' is the index value, and it *is*  multiplied */

        mult =  temp;

        nopp = &temp->gtOp.gtOp1;
        nop1 =  temp->gtOp.gtOp1;

        assert(base->gtType == TYP_REF);
    }
    else
    {
        base = temp;
        temp = add1->gtOp.gtOp2;
        assert(base->gtType == TYP_REF);

        if  (temp->gtOper == GT_NOP)
        {
            /* 'op2' is the index value, and it's not multiplied */

            mult = 0;

            nopp = &add1->gtOp.gtOp2;
            nop1 =  add1->gtOp.gtOp2;
        }
        else
        {
            /* 'op2' is the index value, and it *is*  multiplied */

            assert((temp->gtOper == GT_LSH || temp->gtOper == GT_MUL) && temp->gtOp.gtOp1->gtOper == GT_NOP);
            mult =  temp;

            nopp = &temp->gtOp.gtOp1;
            nop1 =  temp->gtOp.gtOp1;
        }
    }

    /* 'addp' points to where 'add1' came from, 'add1' is the NOP node */

    assert(*nopp == nop1 && nop1->gtOper == GT_NOP);

    /* Get rid of the NOP node */

    assert(nop1->gtOp.gtOp2 == NULL);

    nop1 = *nopp = nop1->gtOp.gtOp1;

    /* Can we hoist "+icon" out of the index computation? */

    if  (nop1->gtOper == GT_ADD && nop1->gtOp.gtOp2->gtOper == GT_CNS_INT)
    {
        if (updateCSEcounts)
        {
            /* Unmark if it is a CSE */
            if (IS_CSE_INDEX(nop1->gtCSEnum))
                optUnmarkCSE(nop1);
        }
        addp = nopp;
        add1 = nop1;
        base = add1->gtOp.gtOp1;

        ival = add1->gtOp.gtOp2->gtIntCon.gtIconVal;
    }
    else if (nop1->gtOper == GT_CNS_INT)
    {
        /* in this case the index itself is a constant */

        ival = nop1->gtIntCon.gtIconVal;
    }
    else
        goto DONE;

    /* 'addp' points to where 'add1' came from, 'add1' must be a '+' */

    assert(*addp == add1); assert(add1->gtOper == GT_ADD);

    /* remove the added constant */

    *addp = base;

    /* Multiply the constant if the index is scaled */

    if  (mult)
    {
        assert(mult->gtOper == GT_LSH || mult->gtOper == GT_MUL);
        assert(mult->gtOp.gtOp2->gtOper == GT_CNS_INT);

        if (mult->gtOper == GT_MUL)
            ival  *= mult->gtOp.gtOp2->gtIntCon.gtIconVal;
        else
            ival <<= mult->gtOp.gtOp2->gtIntCon.gtIconVal;
    }

    /* Was there a constant added to the offset? */

    assert(icon);
    assert(icon->gtOper == GT_CNS_INT);

    icon->gtIntCon.gtIconVal += ival;

DONE:

    /* Re-thread the nodes if necessary */

    if (fgStmtListThreaded)
        fgSetStmtSeq(stmt);
}

/*****************************************************************************
 * parse the array reference, return
 *    pointer to nop (which is above index)
 *    pointer to scaling multiply/shift (or NULL if no multiply/shift)
 *    pointer to the address of the array
 * Since this is called before and after reordering, we have to be distinguish
 * between the array address and the index expression.
 */

GenTreePtr    *           Compiler::optParseArrayRef(GenTreePtr tree,
                                                     GenTreePtr *pmul,
                                                     GenTreePtr *parrayAddr)
{
    GenTreePtr     mul;
    GenTreePtr     index;
    GenTreePtr   * ptr;

    assert(tree->gtOper == GT_ADD);

#if REARRANGE_ADDS
    /* ignore constant offset added to array */
    if  (tree->gtOp.gtOp2->gtOper == GT_CNS_INT)
        tree = tree->gtOp.gtOp1;
#endif

    /* the array address is TYP_REF */

    if (tree->gtOp.gtOp1->gtType == TYP_REF)
    {
        /* set the return value for the array address */
        if (parrayAddr)
        {
            *parrayAddr = tree->gtOp.gtOp1;
        }

        /* Op2 must be the index expression */
        ptr = &tree->gtOp.gtOp2; index = *ptr;
    }
    else
    {
        assert(tree->gtOp.gtOp2->gtType == TYP_REF);

        /* set the return value for the array address */
        if (parrayAddr)
        {
            *parrayAddr = tree->gtOp.gtOp2;
        }

        /* Op1 must be the index expression */
        ptr = &tree->gtOp.gtOp1; index = *ptr;
    }

    /* Get rid of the base element address, if present */

    if  (index->gtOper == GT_ADD)
    {
        ptr = &index->gtOp.gtOp1; index = *ptr;
    }

    /* Get rid of a scaling operator, if present */

    mul = 0;

    if  (index->gtOper == GT_MUL ||
         index->gtOper == GT_LSH)
    {
        mul = index;
        ptr = &index->gtOp.gtOp1; index = *ptr;
    }

    /* We should have the index value at this point */

    assert(index == *ptr);
    assert(index->gtOper == GT_NOP);
    assert(index->gtFlags & GTF_NOP_RNGCHK);

    if (pmul)
    {
        *pmul = mul;
    }

    return ptr;
}

/*****************************************************************************
 * find the last assignment to of the local variable in the block. return
 * RHS or NULL. If any local variable in the RHS has been killed in
 * intervening code, return NULL.
 *
 */

GenTreePtr       Compiler::optFindLocalInit(BasicBlock *block, GenTreePtr local)
{

    GenTreePtr      rhs;
    GenTreePtr      list;
    GenTreePtr      stmt;
    GenTreePtr      tree;
    unsigned        LclNum;
    VARSET_TP       killedLocals = 0;
    LclVarDsc   *   varDsc;

    rhs = NULL;
    list = block->bbTreeList;

    if  (!list)
        return NULL;

    LclNum = local->gtLclVar.gtLclNum;

    stmt = list;
    do
    {

        stmt = stmt->gtPrev;
        if  (!stmt)
            break;

        tree = stmt->gtStmt.gtStmtExpr;
        if (tree->gtOper == GT_ASG && tree->gtOp.gtOp1->gtOper == GT_LCL_VAR)
        {
            if (tree->gtOp.gtOp1->gtLclVar.gtLclNum == LclNum)
            {
                rhs = tree->gtOp.gtOp2;
                break;
            }
            varDsc = optIsTrackedLocal(tree->gtOp.gtOp1);

            if (varDsc == NULL)
                return NULL;

            killedLocals |= genVarIndexToBit(varDsc->lvVarIndex);

        }

    }
    while (stmt != list);

    if (rhs == NULL)
        return NULL;

    /* if any local in the RHS is killed in intervening code, or RHS has an
     * in indirection, return NULL
     */
    varRefKinds     rhsRefs   = VR_NONE;
    VARSET_TP       rhsLocals = lvaLclVarRefs(rhs, NULL, &rhsRefs);

    if ((rhsLocals & killedLocals) || rhsRefs)
        return NULL;

    return rhs;
}

/*****************************************************************************
 *
 *  Return true if "op1" is guaranteed to be less then or equal to "op2".
 */

#if FANCY_ARRAY_OPT

bool                Compiler::optIsNoMore(GenTreePtr op1, GenTreePtr op2,
                                          int add1, int add2)
{
    if  (op1->gtOper == GT_CNS_INT &&
         op2->gtOper == GT_CNS_INT)
    {
        add1 += op1->gtIntCon.gtIconVal;
        add2 += op2->gtIntCon.gtIconVal;
    }
    else
    {
        /* Check for +/- constant on either operand */

        if  (op1->gtOper == GT_ADD && op1->gtOp.gtOp2->gtOper == GT_CNS_INT)
        {
            add1 += op1->gtOp.gtOp2->gtIntCon.gtIconVal;
            op1   = op1->gtOp.gtOp1;
        }

        if  (op2->gtOper == GT_ADD && op2->gtOp.gtOp2->gtOper == GT_CNS_INT)
        {
            add2 += op2->gtOp.gtOp2->gtIntCon.gtIconVal;
            op2   = op2->gtOp.gtOp1;
        }

        /* We only allow local variable references */

        if  (op1->gtOper != GT_LCL_VAR)
            return false;
        if  (op2->gtOper != GT_LCL_VAR)
            return false;
        if  (op1->gtLclVar.gtLclNum != op2->gtLclVar.gtLclNum)
            return false;

        /* NOTE: Caller ensures that this variable has only one def */

//      printf("limit [%d]:\n", add1); gtDispTree(op1);
//      printf("size  [%d]:\n", add2); gtDispTree(op2);
//      printf("\n");

    }

    return  (bool)(add1 <= add2);
}

#endif

/*****************************************************************************
 *
 * Delete range checks in a loop if can prove that the index expression is
 * in range.
 */

void                Compiler::optOptimizeInducIndexChecks(unsigned loopNum)
{
    assert(loopNum < optLoopCount);
    LoopDsc *       loop = optLoopTable + loopNum;

    /* Get the iterator variable */

    if (!(loop->lpFlags & LPFLG_ITER))
        return;

    unsigned        ivLclNum = loop->lpIterVar();

    if (lvaVarAddrTaken(ivLclNum))
    {
        return;
    }


    /* Any non-negative constant is a good initial value */

    if (!(loop->lpFlags & LPFLG_CONST_INIT))
        return;

    long            posBias = loop->lpConstInit;

    if (posBias < 0)
        return;

    /* We require the loop to add by exactly one.
       The reason for this requirement is that we don't have any guarantees 
       on the length of the array.  For example, if the array length is 0x7fffffff 
       and the increment is by 2, then it's possible for the loop test to overflow
       to 0x80000000 and hence succeed.  In that case it's not legal for us to
       remove the bounds check, because it is expected to fail.
       It is possible to relax this constraint to allow for iterators <= i if we
       can guarantee that arrays will have no more than 0x7fffffff-i+1 elements.
       This requires corresponding logic in the EE that can get out of date with
       the codes here.  We will consider implementing that if this constraint
       causes performance issues. */

    if ((loop->lpIterOper() != GT_ASG_ADD) || (loop->lpIterConst() != 1))
        return;

    BasicBlock *    head = loop->lpHead;
    BasicBlock *    end  = loop->lpEnd;
    BasicBlock *    beg  = head->bbNext;

    /* Find the loop termination test. if can't, give up */

    if (end->bbJumpKind != BBJ_COND)
        return;

    /* conditional branch must go back to top of loop */
    if  (end->bbJumpDest != beg)
        return;

    GenTreePtr      conds = genLoopTermTest(beg, end);

    if  (conds == NULL)
        return;

    /* Get to the condition node from the statement tree */

    assert(conds->gtOper == GT_STMT);

    GenTreePtr      condt = conds->gtStmt.gtStmtExpr;
    assert(condt->gtOper == GT_JTRUE);

    condt = condt->gtOp.gtOp1;
    assert(condt->OperIsCompare());

    /* if test isn't less than, forget it */

    if (condt->gtOper != GT_LT)
    {
#if FANCY_ARRAY_OPT
        if (condt->gtOper != GT_LE)
#endif
            return;
    }

    /* condt is the loop termination test */
    GenTreePtr op2 = condt->gtOp.gtOp2;

    /* Is second operand a region constant? We could allow some expressions
     * here like "arr_length - 1"
     */
    GenTreePtr      rc = op2;
    long            negBias = 0;

AGAIN:
    switch (rc->gtOper)
    {
    case GT_ADD:
        /* we allow length + negconst */
        if (rc->gtOp.gtOp2->gtOper == GT_CNS_INT
            && rc->gtOp.gtOp2->gtIntCon.gtIconVal < 0
            && rc->gtOp.gtOp1->gtOper == GT_ARR_LENGTH)
        {
            negBias = -rc->gtOp.gtOp2->gtIntCon.gtIconVal;
            op2 = rc = rc->gtOp.gtOp1;
            goto AGAIN;
        }
        break;

    case GT_SUB:
        /* we allow length - posconst */
        if (rc->gtOp.gtOp2->gtOper == GT_CNS_INT
            && rc->gtOp.gtOp2->gtIntCon.gtIconVal > 0
            && rc->gtOp.gtOp1->gtOper == GT_ARR_LENGTH)
        {
            negBias = rc->gtOp.gtOp2->gtIntCon.gtIconVal;
            op2 = rc = rc->gtOp.gtOp1;
            goto AGAIN;
        }
        break;

    case GT_ARR_LENGTH:
        /* recurse to check if operand is RC */
        rc = rc->gtOp.gtOp1;
        goto AGAIN;

    case GT_LCL_VAR:
        LclVarDsc * varDscRc;
        varDscRc = optIsTrackedLocal(rc);

        /* if variable not tracked, quit */
        if  (!varDscRc)
            return;

        /* If address taken quit */
        if (varDscRc->lvAddrTaken)
        {
            return;
        }

        /* if altered, then quit */
        if (optIsVarAssgLoop(loopNum, rc->gtLclVar.gtLclNum))
            return;

        break;

    default:
        return;
    }

    if (op2->gtOper  == GT_LCL_VAR)
        op2 = optFindLocalInit(head, op2);

    unsigned    arrayLclNum;

#if FANCY_ARRAY_OPT
    GenTreePtr  loopLim;
    const unsigned  CONST_ARRAY_LEN = ((unsigned)-1);
    arrayLclNum = CONST_ARRAY_LEN
#endif

    /* only thing we want is array length (note we update op2 above
     * to allow "arr_length - posconst")
     */
    if (op2 && op2->gtOper == GT_ARR_LENGTH &&
        op2->gtArrLen.gtArrLenAdr->gtOper == GT_LCL_VAR)
    {
#if FANCY_ARRAY_OPT
        if (condt->gtOper == GT_LT)
#endif
            arrayLclNum = op2->gtArrLen.gtArrLenAdr->gtLclVar.gtLclNum;
    }
    else
    {
#if FANCY_ARRAY_OPT
        loopLim = rc;
#else
        return;
#endif
    }

#if FANCY_ARRAY_OPT
    if  (arrayLclNum != CONST_ARRAY_LEN)
#endif
    {
        LclVarDsc * varDscArr = optIsTrackedLocal(op2->gtOp.gtOp1);

        /* If array local not tracked, quit.
           If the array has been altered in the loop, forget it */

        if  (!varDscArr ||
             optIsVarAssgLoop(loopNum, varDscArr - lvaTable))
        {
#if FANCY_ARRAY_OPT
            arrayLclNum = CONST_ARRAY_LEN;
#else
            return;
#endif
        }
    }

    /* Now scan for range checks on the induction variable */

    for (BasicBlock * block = beg; /**/; block = block->bbNext)
    {
        /* Make sure we update the weighted ref count correctly */
        optCSEweight = block->bbWeight;

        /* Walk the statement trees in this basic block */

        for (GenTreePtr stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

            for (GenTreePtr tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
            {
                /* no more can be done if we see the increment of induc var */
                if (tree->OperKind() & GTK_ASGOP)
                {
                    if (tree->gtOp.gtOp1->gtOper == GT_LCL_VAR
                        && tree->gtOp.gtOp1->gtLclVar.gtLclNum == ivLclNum)
                        return;
                }

                if  ((tree->gtFlags & GTF_IND_RNGCHK) && tree->gtOper == GT_IND)
                {
                    GenTreePtr  arr, mul, index;
                    GenTreePtr * pnop;

                    pnop = optParseArrayRef(tree->gtOp.gtOp1, &mul, &arr);

                    /* does the array ref match our known array */
                    if (arr->gtOper != GT_LCL_VAR)
                        continue;

                    if  (arr->gtLclVar.gtLclNum != arrayLclNum)
                    {
#if FANCY_ARRAY_OPT
                        if  (arrayLclNum == CONST_ARRAY_LEN)
                        {
                            LclVarDsc   *   arrayDsc;

                            assert(arr->gtLclVar.gtLclNum < lvaCount);
                            arrayDsc = lvaTable + arr->gtLclVar.gtLclNum;

                            if  (arrayDsc->lvKnownDim)
                            {
                                if  (optIsNoMore(loopLim, arrayDsc->lvKnownDim, (condt->gtOper == GT_LE)))
                                {
                                    index = (*pnop)->gtOp.gtOp1;

                                    // UNDONE: Allow "i+1" and things like that

                                    goto RMV;
                                }
                            }
                        }
#endif
                        continue;
                    }

                    index = (*pnop)->gtOp.gtOp1;

                    /* allow sub of non-neg constant from induction variable
                     * if we had bigger initial value
                     */
                    if (index->gtOper == GT_SUB
                        && index->gtOp.gtOp2->gtOper == GT_CNS_INT)
                    {
                        long ival = index->gtOp.gtOp2->gtIntCon.gtIconVal;
                        if (ival >= 0 && ival <= posBias)
                            index = index->gtOp.gtOp1;
                    }

                    /* allow add of constant to induction variable
                     * if we had a sub from length
                     */
                    if (index->gtOper == GT_ADD
                        && index->gtOp.gtOp2->gtOper == GT_CNS_INT)
                    {
                        long ival = index->gtOp.gtOp2->gtIntCon.gtIconVal;
                        if (ival >= 0 && ival <= negBias)
                            index = index->gtOp.gtOp1;
                    }

#if FANCY_ARRAY_OPT
                RMV:
#endif
                    /* is index our induction var? */
                    if (!(index->gtOper == GT_LCL_VAR
                        && index->gtLclVar.gtLclNum == ivLclNum))
                        continue;

                    /* no need for range check */
                    optRemoveRangeCheck(tree, stmt, false);

#if COUNT_RANGECHECKS
                    optRangeChkRmv++;
#endif
                }
            }
        }

        if  (block == end)
            break;
    }
}


struct optRangeCheckDsc
{
    Compiler* pCompiler;
    bool      bValidIndex;
};
/*
    Walk to make sure that only locals and constants are contained in the index
    for a range check
*/
Compiler::fgWalkResult      Compiler::optValidRangeCheckIndex(GenTreePtr  tree, void    *   pCallBackData)
{
    optRangeCheckDsc* pData= (optRangeCheckDsc*) pCallBackData;

    if (tree->gtOper == GT_IND || tree->gtOper == GT_CLS_VAR || 
		tree->gtOper == GT_FIELD || tree->gtOper == GT_LCL_FLD)
    {
        pData->bValidIndex = false;
        return WALK_ABORT;
    }

    if (tree->gtOper == GT_LCL_VAR)
    {
        if (pData->pCompiler->lvaTable[tree->gtLclVar.gtLclNum].lvAddrTaken)
        {
            pData->bValidIndex = false;
            return WALK_ABORT;
        }
    }

    return WALK_CONTINUE;
}

/*
    returns true if a range check can legally be removed (for the moment it checks
    that the array is a local array (non subject to racing conditions) and that the
    index is either a constant or a local
*/
bool Compiler::optIsRangeCheckRemovable(GenTreePtr tree)
{
    GenTreePtr pIndex, pArray;

    assert(tree->gtOper             == GT_IND &&
           tree->gtOp.gtOp1->gtOper == GT_ADD);
    
      
    // Obtain the index and the array
    pIndex = * optParseArrayRef(tree->gtOp.gtOp1,
                                NULL,
                                &pArray);
    pIndex = pIndex->gtOp.gtOp1;

    
    if ( pArray->gtOper != GT_LCL_VAR )
    {        
        // The array reference must be a local. Otherwise we can be targetted
        // by malicious races.
        #ifdef DEBUG
        if (verbose)
        {
            printf("Can't remove range check if the array isn't referenced with a local\n");
            gtDispTree(pArray);
        }
        #endif
        return false;
    }
    else
    {
        assert(pArray->gtType == TYP_REF);    
        assert(pArray->gtLclVar.gtLclNum < lvaCount);

        if (lvaTable[pArray->gtLclVar.gtLclNum].lvAddrTaken)
        {
            // If the array address has been taken, don't do the optimization
            // (this restriction can be lowered a bit, but i don't think it's worth it)
            #ifdef DEBUG
            if (verbose)
            {
                printf("Can't remove range check if the array has its address taken\n");
                gtDispTree(pArray);
            }
            #endif

            return false;
        }
    }
    
    
    optRangeCheckDsc Data;
    Data.pCompiler  =this;
    Data.bValidIndex=true;

    fgWalkTreePre(pIndex, optValidRangeCheckIndex, &Data);
        
    if (!Data.bValidIndex)
    {
        #ifdef DEBUG
        if (verbose)
        {
            printf("Can't remove range check with this index");
            gtDispTree(pIndex);
        }
        #endif

        return false;
    }
    

    return true;
}

/*****************************************************************************
 *
 *  Try to optimize away as many array index range checks as possible.
 */

void                Compiler::optOptimizeIndexChecks()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In optOptimizeIndexChecks()\n");
#endif

	unsigned        arrayVar;
    long            arrayDim;
#if FANCY_ARRAY_OPT
    LclVarDsc   *   arrayDsc;
#endif

    unsigned
    const           NO_ARR_VAR = (unsigned)-1;

    /* Walk all the basic blocks in the function. We dont need to worry about
       flow control as we already know if the array-local is lvAssignTwo or
       not. Also, the local may be accessed before it is assigned the newarr
       but we can still eliminate the range-check, as a null-ptr exception
       will be caused as desired. */

    for (BasicBlock * block = fgFirstBB; block; block = block->bbNext)
    {
        GenTreePtr      stmt;
        GenTreePtr      tree;

        /* Ignore the block if it doesn't contain 'new' of an array */

		if  (!(block->bbFlags & BBF_NEW_ARRAY))
            continue;
		
        /* We have not noticed any array allocations yet */

        arrayVar = NO_ARR_VAR;

        /* Walk the statement trees in this basic block */

        for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

            for (tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
            {
                switch (tree->gtOper)
                {
                    GenTreePtr      op1;
                    GenTreePtr      op2;

                case GT_ASG:

                    op1 = tree->gtOp.gtOp1;
                    op2 = tree->gtOp.gtOp2;

                    /* We are only interested in assignments to locals */					
                    if  (op1->gtOper != GT_LCL_VAR)
                        break;

                    /* If the variable has its address taken, we cannot remove
                       the range check */
                    if (lvaTable[op1->gtLclVar.gtLclNum].lvAddrTaken)
                    {
                        break;
                    }
                    

                    /* Are we trashing the variable that holds the array addr? */
                    if  (arrayVar == op1->gtLclVar.gtLclNum)
                    {
                        /* The variable no longer holds the array it had */

                        arrayVar = NO_ARR_VAR;
                    }

                    /* Is this an assignment of 'new array' ? */

                    if  (op2->gtOper            != GT_CALL   ||
                         op2->gtCall.gtCallType != CT_HELPER)
                    {
                         break;
                    }

                    if (op2->gtCall.gtCallMethHnd != eeFindHelper(CORINFO_HELP_NEWARR_1_DIRECT) &&
                        op2->gtCall.gtCallMethHnd != eeFindHelper(CORINFO_HELP_NEWARR_1_OBJ)    &&
                        op2->gtCall.gtCallMethHnd != eeFindHelper(CORINFO_HELP_NEWARR_1_VC)     &&
                        op2->gtCall.gtCallMethHnd != eeFindHelper(CORINFO_HELP_NEWARR_1_ALIGN8))
                    {
                        break;
                    }

                    /* Extract the array dimension from the helper call */

                    op2 = op2->gtCall.gtCallRegArgs;
                    assert(op2->gtOper == GT_LIST);
                    op2 = op2->gtOp.gtOp1;

                    if  (op2->gtOper == GT_CNS_INT)
                    {
                        /* We have a constant-sized array */

                        arrayVar = op1->gtLclVar.gtLclNum;
                        arrayDim = op2->gtIntCon.gtIconVal;
                    }
#if FANCY_ARRAY_OPT
                    else
                    {
                        /* Make sure the value looks promising */

                        GenTreePtr  tmp = op2;
                        if  (tmp->gtOper == GT_ADD &&
                             tmp->gtOp.gtOp2->gtOper == GT_CNS_INT)
                            tmp = tmp->gtOp.gtOp1;

                        if  (tmp->gtOper != GT_LCL_VAR)
                            break;

                        assert(tmp->gtLclVar.gtLclNum < lvaCount);
                        arrayDsc = lvaTable + tmp->gtLclVar.gtLclNum;

                        if  (arrayDsc->lvAssignTwo)
                            break;
                        if  (arrayDsc->lvAssignOne && arrayDsc->lvIsParam)
                            break;
                    }

                    /* Is there one assignment to the array? */

                    assert(op1->gtLclVar.gtLclNum < lvaCount);
                    arrayDsc = lvaTable + op1->gtLclVar.gtLclNum;

                    if  (arrayDsc->lvAssignTwo)
                        break;

                    /* Record the array size for later */

                    arrayDsc->lvKnownDim = op2;
#endif
                    break;


                case GT_IND:

#if FANCY_ARRAY_OPT
                    if  ((tree->gtFlags & GTF_IND_RNGCHK))
#else
                    if  ((tree->gtFlags & GTF_IND_RNGCHK) && arrayVar != NO_ARR_VAR)
#endif
                    {
                        GenTreePtr      mul;
                        GenTreePtr *    pnop = optParseArrayRef(tree->gtOp.gtOp1, &mul, &op1);

                        /* Is the address of the array a simple variable? */

                        if  (op1->gtOper != GT_LCL_VAR)
                            break;

                        /* Is the index value a constant? */

                        op2 = (*pnop)->gtOp.gtOp1;

                        if  (op2->gtOper != GT_CNS_INT)
                            break;

                        /* Do we know the size of the array? */

                        long    size;

                        if  (op1->gtLclVar.gtLclNum == arrayVar)
                        {
                            size = arrayDim;
                        }
                        else
                        {
#if FANCY_ARRAY_OPT
                            assert(op1->gtLclVar.gtLclNum < lvaCount);
                            arrayDsc = lvaTable + op1->gtLclVar.gtLclNum;

                            GenTreePtr  dimx = arrayDsc->lvKnownDim;
                            if  (!dimx)
                                break;
                            size = dimx->gtIntCon.gtIconVal;
#else
                            break;
#endif
                        }

                        /* Is the index value within the correct range? */

                        if  (op2->gtIntCon.gtIconVal < 0)
                            break;
                        if  (op2->gtIntCon.gtIconVal >= size)
                            break;

                        /* no need for range check */
                        optRemoveRangeCheck(tree, stmt, true);

                        /* Get rid of the range check */
                        // *pnop = op2; tree->gtFlags &= ~GTF_IND_RNGCHK;

                        /* Remember that we have array initializer(s) */

                        optArrayInits = true;

                        /* Is the index value scaled? */

                        if  (mul && mul->gtOp.gtOp2->gtOper == GT_CNS_INT)
                        {
                            long        index =             op2->gtIntCon.gtIconVal;
                            long        scale = mul->gtOp.gtOp2->gtIntCon.gtIconVal;

                            assert(mul->gtOp.gtOp1 == op2);

                            if  (op2->gtOper == GT_MUL)
                                index  *= scale;
                            else
                                index <<= scale;

                            mul->ChangeOperConst(GT_CNS_INT);
                            mul->gtIntCon.gtIconVal = index;

                            /* Was there an additional offset? */

                            if  (false && // this is not finished
                                 tree->gtOp.gtOp2            ->gtOper == GT_ADD &&
                                 tree->gtOp.gtOp2->gtOp.gtOp2->gtOper == GT_CNS_INT)
                            {
                                mul->ChangeOperConst(GT_CNS_INT);
                                mul->gtIntCon.gtIconVal = index + tree->gtOp.gtOp2->gtOp.gtOp2->gtIntCon.gtIconVal;
                            }
                        }
                    }

                    break;
                }
            }
        }
    }

    /* Optimize range checks on induction variables.
       @TODO [NOW] [04/16/01] []: Since we set lvKnownDim without checking flow-control, 
       we may incorrectly hoist a null-ref array-access out of a loop. */

    for (unsigned i=0; i < optLoopCount; i++)
    {
        /* Beware, some loops may be thrown away by unrolling or loop removal */

        if (!(optLoopTable[i].lpFlags & LPFLG_REMOVED))
           optOptimizeInducIndexChecks(i);
    }
}

/*****************************************************************************
 *
 *  Optimize array initializers.
 */

void                Compiler::optOptimizeArrayInits()
{
#ifndef DEBUG
    // @TODO [REVISIT] [04/16/01] []: Not implemented yet
    if (true)
        return;
#endif

    if  (!optArrayInits)
        return;

    /* Until interior pointers are allowed, we can't generate "rep movs" */

#ifdef  DEBUG
    genIntrptibleUse = true;
#endif

//  if  (genInterruptible)
//      return;

    /* Walk the basic block list, looking for promising initializers */

    for (BasicBlock * block = fgFirstBB; block; block = block->bbNext)
    {
        GenTreePtr      stmt;
        GenTreePtr      tree;

        /* Ignore the block if it doesn't contain 'new' of an array */

        if  (!(block->bbFlags & BBF_NEW_ARRAY))
            continue;

        /* Walk the statement trees in this basic block */

        for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            assert(stmt->gtOper == GT_STMT);

            for (tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
            {
                switch (tree->gtOper)
                {
                    GenTreePtr      op1;
                    GenTreePtr      op2;
                    long            off;

                case GT_ASG:

                    op2  = tree->gtOp.gtOp2;
                    if  (!(op2->OperKind() & GTK_CONST))
                        break;

                    op1  = tree->gtOp.gtOp1;
                    if  (op1->gtOper != GT_IND || op1->gtOp.gtOp2 != NULL)
                        break;

                    op1  = op1->gtOp.gtOp1;
                    if  (op1->gtOper != GT_ADD || op1->gtOp.gtOp2->gtOper != GT_CNS_INT)
                        break;

                    off  = op1->gtOp.gtOp2->gtIntCon.gtIconVal;
                    op1  = op1->gtOp.gtOp1;
                    if  (op1->gtOper != GT_ADD || op1->gtOp.gtOp2->gtOper != GT_CNS_INT)
                        break;

                    off += op1->gtOp.gtOp2->gtIntCon.gtIconVal;
                    op1  = op1->gtOp.gtOp1;
                    if  (op1->gtOper != GT_LCL_VAR)
                        break;
#ifdef DEBUG
                    if (verbose)
                    {
                        printf("Array init candidate: offset = %d\n", off);
                        gtDispTree(op2);
                        printf("\n");
                    }
#endif
                    break;
                }
            }
        }
    }
}


/*****************************************************************************/
#if     OPTIMIZE_RECURSION
/*****************************************************************************
 *
 *  A little helper to form the expression "arg * (arg + 1) / 2".
 */

/* static */
GenTreePtr          Compiler::gtNewArithSeries(unsigned argNum, var_types argTyp)
{
    GenTreePtr      tree;

    tree = gtNewOperNode(GT_ADD, argTyp, gtNewLclvNode(argNum, argTyp),
                                         gtNewIconNode(1, argTyp));
    tree = gtNewOperNode(GT_MUL, argTyp, gtNewLclvNode(argNum, argTyp),
                                         tree);
    tree = gtNewOperNode(GT_RSH, argTyp, tree,
                                         gtNewIconNode(1, argTyp));

    return  tree;
}

/*****************************************************************************
 *
 *  Converts recursive methods into iterative ones whenever possible.
 */

void                Compiler::optOptimizeRecursion()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In optOptimizeRecursion()\n");
#endif

    BasicBlock  *   blk0;
    BasicBlock  *   blk1;
    BasicBlock  *   blk2;
    BasicBlock  *   blk3;

    unsigned        argNum;
    var_types       argTyp;

    GenTreePtr      expTmp;
    GenTreePtr      asgTmp;

    GenTreePtr      tstIni;
    GenTreePtr      cnsIni;
    GenTreePtr      tstExp;
    GenTreePtr      cnsLim;
    GenTreePtr      expRet;
    GenTreePtr      expAdd;
    GenTreePtr      argAdd;
    GenTreePtr      cnsAdd;

    union
    {
        long            intVal;
        float           fltVal;
        __int64         lngVal;
        double          dblVal;
    }
                    iniVal, limVal, addVal;

    unsigned        resTmp;
    bool            isArith;
    genTreeOps      expOp;

    /* Check for a flow graph that looks promising */

    if  (fgBBcount != 3)
        return;

    blk1 = fgFirstBB;
    blk2 = blk1->bbNext;
    blk3 = blk2->bbNext; assert(blk3->bbNext == NULL);

    if  (blk1->bbJumpKind != BBJ_COND  ) return;
    if  (blk1->bbJumpDest != blk3      ) return;
    if  (blk2->bbJumpKind != BBJ_RETURN) return;
    if  (blk3->bbJumpKind != BBJ_RETURN) return;

    /* Check argument count and figure out index of first real arg */

    argNum = 0;
    if (!info.compIsStatic)
        argNum++;

    if  (info.compArgsCount < argNum+1)
        return;

    // @TODO [CONSIDER] [04/16/01] []: Allow the second and third blocks to be swapped.

    /* Second block must be "return cnsIni" */

    tstIni = blk2->bbTreeList; assert(tstIni->gtOper == GT_STMT);
    tstIni = tstIni->gtStmt.gtStmtExpr;
    if  (tstIni->gtOper != GT_RETURN)
        return;
    cnsIni = tstIni->gtOp.gtOp1;
    if  (!cnsIni || !(cnsIni->OperKind() & GTK_CONST))
        return;

    /* First block must be "if (arg1 <relop> cnsLim)" */

    tstExp = blk1->bbTreeList; assert(tstExp->gtOper == GT_STMT);
    tstExp = tstExp->gtStmt.gtStmtExpr;
    if  (tstExp->gtOper != GT_JTRUE)
        return;
    tstExp = tstExp->gtOp.gtOp1;
    if  (!(tstExp->OperKind() & GTK_RELOP))
        return;

    expTmp = tstExp->gtOp.gtOp1;
    if  (expTmp->gtOper != GT_LCL_VAR)
        return;
    if  (expTmp->gtLclVar.gtLclNum != argNum)
        return;

    cnsLim = tstExp->gtOp.gtOp2;
    if  (!(cnsLim->OperKind() & GTK_CONST))
        return;

    /* Third block must be "return arg1 <add/mul> f(arg1 +/- cnsAdd)" */

    expRet = blk3->bbTreeList; assert(expRet->gtOper == GT_STMT);
    expRet = expRet->gtStmt.gtStmtExpr;
    if  (expRet->gtOper != GT_RETURN)
        return;
    expAdd = expRet->gtOp.gtOp1;

    /* Inspect the return expression */

    switch (expAdd->gtOper)
    {
    case GT_ADD:
        expOp = GT_ASG_ADD;
        break;
    case GT_MUL:
        expOp = GT_ASG_MUL;
        break;

    default:
        return;
    }

    /* Look for "arg1" on either side of the operation */

    expTmp = expAdd->gtOp.gtOp1;
    cnsAdd = expAdd->gtOp.gtOp2;

    if  (expTmp->gtOper != GT_LCL_VAR)
    {
        /* Try it the other way around */

        expTmp = expAdd->gtOp.gtOp2;
        cnsAdd = expAdd->gtOp.gtOp1;

        if  (expTmp->gtOper != GT_LCL_VAR)
            return;
    }

    if  (expTmp->gtLclVar.gtLclNum != argNum)
        return;

    /* The other operand must be a directly recursive call */

    if  (cnsAdd->gtOper != GT_CALL)
        return;
    if  (cnsAdd->gtFlags & (GTF_CALL_VIRT|GTF_CALL_INTF))
        return;

    gtCallTypes callType = cnsAdd->gtCall.gtCallType;
    if  (callType == CT_HELPER || !eeIsOurMethod(cnsAdd->gtCall.gtCallMethHnd))
        return;

    /* If the method is not static, check the 'this' value */

    if  (cnsAdd->gtCall.gtCallObjp)
    {
        if  (cnsAdd->gtCall.gtCallObjp->gtOper != GT_LCL_VAR)   return;
        if  (cnsAdd->gtCall.gtCallObjp->gtLclVar.gtLclNum != 0) return;
    }

    /* There must be at least one argument */

    argAdd = cnsAdd->gtCall.gtCallArgs; assert(argAdd && argAdd->gtOper == GT_LIST);
    argAdd = argAdd->gtOp.gtOp1;

    /* Inspect the argument value */

    switch (argAdd->gtOper)
    {
    case GT_ADD:
    case GT_SUB:
        break;

    default:
        return;
    }

    /* Look for "arg1" on either side of the operation */

    expTmp = argAdd->gtOp.gtOp1;
    cnsAdd = argAdd->gtOp.gtOp2;

    if  (expTmp->gtOper != GT_LCL_VAR)
    {
        if  (argAdd->gtOper != GT_ADD)
            return;

        /* Try it the other way around */

        expTmp = argAdd->gtOp.gtOp2;
        cnsAdd = argAdd->gtOp.gtOp1;

        if  (expTmp->gtOper != GT_LCL_VAR)
            return;
    }

    if  (expTmp->gtLclVar.gtLclNum != argNum)
        return;

    /* Get hold of the adjustment constant */

    if  (!(cnsAdd->OperKind() & GTK_CONST))
        return;

    /* Make sure all the constants have the same type */

    argTyp = cnsAdd->TypeGet();

    if  (argTyp != cnsLim->gtType)
        return;
    if  (argTyp != cnsIni->gtType)
        return;

    switch (argTyp)
    {
    case TYP_INT:

        iniVal.intVal = cnsIni->gtIntCon.gtIconVal;
        limVal.intVal = cnsLim->gtIntCon.gtIconVal;
        addVal.intVal = cnsAdd->gtIntCon.gtIconVal;

        if  (argAdd->gtOper == GT_SUB)
            addVal.intVal = -addVal.intVal;

        break;

    default:
        // @TODO [CONSIDER] [04/16/01] []: Allow types other than 'int'
        return;
    }

#ifdef  DEBUG

    if  (verbose)
    {
        fgDispBasicBlocks(true);

        printf("\n");
        printf("Unrecursing method '%s':\n", info.compMethodName);
        printf("    Init  value = %d\n", iniVal.intVal);
        printf("    Limit value = %d\n", limVal.intVal);
        printf("    Incr. value = %d\n", addVal.intVal);

        printf("\n");
    }

#endif

    /*
        We have a method with the following definition:

            int     rec(int arg, ....)
            {
                if  (arg == limVal)
                    return  iniVal;
                else
                    return  arg + rec(arg + addVal, ...);
            }

        We'll change the above into the following:

            int     rec(int arg, ....)
            {
                int     res = iniVal;

                while (arg != limVal)
                {
                    res += arg;
                    arg += addVal;
                }

                return  res;
            }

        But first, let's check for the following special case:

            int     rec(int arg)
            {
                if  (arg <= 0)
                    return  0;
                else
                    return  arg + rec(arg - 1);
            }

        The above can be transformed into the following:

            int     rec(int arg)
            {
                if  (arg <= 0)
                    return  0;
                else
                    return  (arg * (arg + 1)) / 2;
            }

        We check for this special case first.
     */

    isArith = false;

    if  (argTyp == TYP_INT && iniVal.intVal == 0
                           && limVal.intVal == 0
                           && addVal.intVal == -1)
    {
        if  (tstExp->gtOper != GT_LE)
        {
            if  (tstExp->gtOper == GT_NE)
                isArith = true;

            goto NOT_ARITH;
        }

        /* Simply change the final return statement and we're done */

        assert(expRet->gtOper == GT_RETURN);

        expRet->gtOp.gtOp1 = gtNewArithSeries(argNum, argTyp);

        return;
    }

NOT_ARITH:

    /* Create an initialization block with "tmp = iniVal" */

    resTmp = lvaGrabTemp();
    expTmp = gtNewTempAssign(resTmp, gtNewIconNode(iniVal.intVal, argTyp));

    /* Prepend a block with the tree to our method */

    blk0 = fgPrependBB(expTmp);

    /* Flip the condition on the first block */

    assert(tstExp->OperKind() & GTK_RELOP);
    tstExp->SetOper(GenTree::ReverseRelop(tstExp->OperGet()));

    /* Now replace block 2 with "res += arg ; arg += addVal" */

    if (expOp == GT_ASG_ADD)
    {
        expTmp = gtNewAssignNode(gtNewLclvNode(resTmp, argTyp),
                                 gtNewLclvNode(argNum, argTyp));
        expTmp->SetOper(GT_ASG_ADD);
    }
    else
    {
        // @UNDONE: Unfortunately the codegenerator cannot digest
        //          GT_ASG_MUL, so we have to generate a bigger tree.

        GenTreePtr op1;
        op1    = gtNewOperNode(GT_MUL, argTyp, gtNewLclvNode(argNum, argTyp),
                                               gtNewLclvNode(resTmp, argTyp));
        expTmp = gtNewAssignNode(gtNewLclvNode(resTmp, argTyp), op1);
    }
    expTmp = gtNewStmt(expTmp);

    asgTmp = gtNewAssignNode(gtNewLclvNode(argNum, argTyp),
                             gtNewIconNode(addVal.intVal, argTyp));
    asgTmp->SetOper(GT_ASG_ADD);

    asgTmp = gtNewStmt(asgTmp);

    /* Store the two trees in the second block */

    blk2->bbTreeList = expTmp;

    asgTmp->gtPrev = expTmp;
    expTmp->gtNext = asgTmp;
    expTmp->gtPrev = asgTmp;

    /* Make the second block jump back to the top */

    blk2->bbJumpKind = BBJ_ALWAYS;
    blk2->bbJumpDest = blk1;

    /* Finally, change the return value of the third block to the temp */

    expRet->gtOp.gtOp1 = gtNewLclvNode(resTmp, argTyp);

    /* Special case: arithmetic series with non-negative check */

    if  (isArith)
    {
        BasicBlock  *   retBlk;
        BasicBlock  *   tstBlk;

        /* Create the "easy" return expression */

        expTmp = gtNewOperNode(GT_RETURN, argTyp, gtNewArithSeries(argNum, argTyp));

        /* Prepend the "easy" return expression to the method */

        retBlk = fgPrependBB(expTmp);
        retBlk->bbJumpKind = BBJ_RETURN;

        /* Create the test expression */

        expTmp = gtNewOperNode(GT_AND  ,  argTyp, gtNewLclvNode(argNum, argTyp),
                                                  gtNewIconNode(0xFFFF8000, argTyp));
        expTmp = gtNewOperNode(GT_NE   , TYP_INT, expTmp,
                                                  gtNewIconNode(0, TYP_INT));
        expTmp = gtNewOperNode(GT_JTRUE, TYP_INT, expTmp);

        /* Prepend the test to the method */

        tstBlk = fgPrependBB(expTmp);
        tstBlk->bbJumpKind = BBJ_COND;
        tstBlk->bbJumpDest = blk0;
    }
}

/*****************************************************************************/
#endif//OPTIMIZE_RECURSION
/*****************************************************************************/



/*****************************************************************************/
#if CODE_MOTION
/*****************************************************************************
 *
 *  For now, we only remove entire worthless loops.
 */

#define RMV_ENTIRE_LOOPS_ONLY    1

/*****************************************************************************
 *
 *  Remove the blocks from 'head' (inclusive) to 'tail' (exclusive) from
 *  the flow graph.
 */

void                genRemoveBBsection(BasicBlock *head, BasicBlock *tail)
{
    BasicBlock *    block;

    VARSET_TP       liveExit = tail->bbLiveIn;

    for (block = head; block != tail; block = block->bbNext)
    {
        block->bbLiveIn   =
        block->bbLiveOut  = liveExit;

        block->bbTreeList = 0;
        block->bbJumpKind = BBJ_NONE;
        block->bbFlags   |= BBF_REMOVED;
    }
}

/*****************************************************************************
 *
 *  Tree walker used by loop code motion. Returns non-zero if the expression
 *  is not acceptable for some reason.
 */

bool                Compiler::optFindLiveRefs(GenTreePtr tree, bool used, bool cond)
{
    genTreeOps      oper;
    unsigned        kind;

AGAIN:

    assert(tree);
    assert(tree->gtOper != GT_STMT);

    /* Figure out what kind of a node we have */

    oper = tree->OperGet();
    kind = tree->OperKind();

    /* Is this a constant or leaf node? */

    if  (kind & (GTK_CONST|GTK_LEAF))
    {
        if  (oper == GT_LCL_VAR)
        {
            unsigned        lclNum;
            LclVarDsc   *   varDsc;

            assert(tree->gtOper == GT_LCL_VAR);
            lclNum = tree->gtLclVar.gtLclNum;

            assert(lclNum < lvaCount);
            varDsc = lvaTable + lclNum;

            /* Give up if volatile or untracked variable */

            if  (varDsc->lvVolatile || !varDsc->lvTracked)
                return  true;

            /* Mark the use of this variable, if appropriate */

#if !RMV_ENTIRE_LOOPS_ONLY
            if  (used) optLoopLiveExit |= genVarIndexToBit(varDsc->lvVarIndex);
            if  (cond) optLoopCondTest |= genVarIndexToBit(varDsc->lvVarIndex);
#endif
        }
//      else if (oper == GT_CLS_VAR)
//      {
//          return  true;
//      }

        return  false;
    }

    /* Is it a 'simple' unary/binary operator? */

    if  (kind & GTK_SMPOP)
    {
        if  (tree->gtGetOp2())
        {
            /* It's a binary operator; is it an assignment? */

            if  (kind & GTK_ASGOP)
            {
                unsigned        lclNum;
                LclVarDsc   *   varDsc;
                VARSET_TP       varBit;

                GenTreePtr      dest = tree->gtOp.gtOp1;

                /* The target better be a variable */

                if  (dest->gtOper != GT_LCL_VAR)
                    return  true;

                /* Is the target variable in the 'live exit' set? */

                assert(dest->gtOper == GT_LCL_VAR);
                lclNum = dest->gtLclVar.gtLclNum;

                assert(lclNum < lvaCount);
                varDsc = lvaTable + lclNum;

                /* Give up if volatile or untracked variable */

                if  (varDsc->lvVolatile || !varDsc->lvTracked)
                    return  true;

                varBit = genVarIndexToBit(varDsc->lvVarIndex);

                /* Keep track of all assigned variables */

                optLoopAssign |= varBit;

                /* Is the variable live on exit? */

                if  (optLoopLiveExit & varBit)
                {
#if !RMV_ENTIRE_LOOPS_ONLY
                    /* The value assigned to this variable is useful */

                    used = true;

                    /* This assignment could depend on a condition */

                    optLoopLiveExit |= optLoopCondTest;
#else
                    /* Assignment is useful - loop is not worthless */

                    return  true;
#endif
                }
            }
            else
            {
                if  (optFindLiveRefs(tree->gtOp.gtOp1, used, cond))
                    return  true;
            }

            tree = tree->gtOp.gtOp2; assert(tree);
            goto AGAIN;
        }
        else
        {
            /* It's a unary (or nilary) operator */

            tree = tree->gtOp.gtOp1;
            if  (tree)
                goto AGAIN;

            return  false;
        }
    }

    /* We don't allow any 'special' operators */

    return  true;
}


/*****************************************************************************
 *
 *  Perform loop code motion / worthless code removal.
 */

void                Compiler::optLoopCodeMotion()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In optLoopCodeMotion()\n");
#endif
    
    unsigned        loopNum;
    unsigned        loopCnt;
    unsigned        loopSkp;

    LOOP_MASK_TP    loopRmv;
    LOOP_MASK_TP    loopBit;

    bool            repeat;

    /* Process all loops, looking for worthless code that can be removed */

    loopRmv = 0;
    loopCnt = 0;
    loopSkp = 1;

AGAIN:

    do
    {
        repeat = false;

        for (loopNum = 0, loopBit = 1;
             loopNum < optLoopCount;
             loopNum++  , loopBit <<= 1)
        {
            BasicBlock *    block;
            GenTreePtr      tree;

#if !RMV_ENTIRE_LOOPS_ONLY
            VARSET_TP       liveExit;
            VARSET_TP       loopCond;
#endif

            /* Some loops may have been already removed by loop unrolling */

            if (optLoopTable[loopNum].lpFlags & LPFLG_REMOVED)
                continue;

            BasicBlock *    head   = optLoopTable[loopNum].lpHead->bbNext;
            BasicBlock *    bottom = optLoopTable[loopNum].lpEnd;
            BasicBlock *    tail   = bottom->bbNext;

            /* Skip the loop if it's already been removed or
             * if it's at the end of the method (while("true"){};)
             * or if it's a nested loop and the outer loop was
             * removed first - can happen in stupid benchmarks like LoopMark */

            if  ((loopRmv & loopBit)     ||
                 tail == 0               ||
                 head->bbTreeList == 0    )
            {
                continue;
            }

            /* get the loop condition - if not a conditional jump bail */

            if (bottom->bbJumpKind != BBJ_COND)
                continue;

            GenTreePtr      cond   = bottom->bbTreeList->gtPrev->gtStmt.gtStmtExpr;
            assert (cond->gtOper == GT_JTRUE);

            /* check if a simple termination condition - operands have to be leaves */

            GenTreePtr         op1 = cond->gtOp.gtOp1->gtOp.gtOp1;
            GenTreePtr         op2 = cond->gtOp.gtOp1->gtOp.gtOp2;

            if ( !(op1->OperIsLeaf() || op2->OperIsLeaf()) )
                continue;

            /* Make sure no side effects other than assignments are present */

            for (block = head; block != tail; block = block->bbNext)
            {
                for (tree = block->bbTreeList; tree; tree = tree->gtNext)
                {
                    GenTreePtr      stmt = tree->gtStmt.gtStmtExpr;

                    /* We must not remove return or side-effect statements */

                    if  (stmt->gtOper != GT_RETURN                        &&
                         !(stmt->gtFlags & (GTF_SIDE_EFFECT & ~GTF_ASG))  )
                    {
                        /* If a statement is a comparisson marked GLOBAL
                         * it must be the loop condition */

                        if (stmt->gtOper == GT_JTRUE)
                        {
                            if (stmt->gtFlags & GTF_GLOB_REF)
                            {
                                /* must be the loop condition */

                                if (stmt != cond)
                                {
                                    loopRmv |= loopBit;
                                    goto NEXT_LOOP;
                                }
                            }
                        }

                        /* Does the statement contain an assignment? */

                        if  (!(stmt->gtFlags & GTF_ASG))
                            continue;

                        /* Don't remove assignments to globals */

                        if  (!(stmt->gtFlags & GTF_GLOB_REF))
                            continue;

                        /* It's OK if the global is only in the RHS */

                        if  (stmt->OperKind() & GTK_ASGOP)
                        {
                            GenTreePtr  dst = stmt->gtOp.gtOp1;
                            GenTreePtr  src = stmt->gtOp.gtOp2;

                            /* The RHS must not have another assignment */

                            if  (!(dst->gtFlags & GTF_GLOB_REF) &&
                                 !(src->gtFlags & GTF_ASG))
                            {
                                continue;
                            }
                        }
                    }

                    /* Don't waste time with this loop any more */

                    loopRmv |= loopBit;
                    goto NEXT_LOOP;
                }
            }

            /* We have a candidate loop */

#ifdef  DEBUG

            if  (verbose)
            {
                printf("Candidate loop for worthless code removal:\n");

                for (block = head; block != tail; block = block->bbNext)
                {
                    printf("BB%02u:\n", block->bbNum);

                    for (tree = block->bbTreeList; tree; tree = tree->gtNext)
                    {
                        gtDispTree(tree->gtStmt.gtStmtExpr, 0);
                        printf("\n");
                    }
                    printf("\n");
                }
                printf("This is currently busted because the dominators are out of synch - Skip it!\n"
                       "The whole thing should be combined with loop invariants\n");
            }

#endif

            /* This is currently busted because the dominators are out of synch
             * The whole thing should be combined with loop invariants */

            goto NEXT_LOOP;

            // @TODO [BROKEN] [04/16/01] []: either fix this or remove it 
#if 0 
            GenTreePtr keepStmtLast = 0;
            GenTreePtr keepStmtList = 0;                    // list with statements we will keep

            /* Get hold of the set of variables live on exit from the loop */

            optLoopLiveExit = tail->bbLiveIn;

            /* Keep track of what variables are being assigned in the loop */

            optLoopAssign   = 0;

            /* Keep track of what variables are being assigned in the loop */

#if !RMV_ENTIRE_LOOPS_ONLY
            optLoopCondTest = 0;
#endif

            /*
                Find variables assigned in the loop that are used to compute
                any values that are live on exit. We repeat this until we
                don't find any more variables to add to the set.
             */

#if !RMV_ENTIRE_LOOPS_ONLY
            do
            {
                liveExit = optLoopLiveExit;
                loopCond = optLoopCondTest;
#endif

                for (block = head; block != tail; block = block->bbNext)
                {
                    /* Make sure the block is of an acceptable kind */

                    switch (block->bbJumpKind)
                    {
                    case BBJ_ALWAYS:
                    case BBJ_COND:

                        /* Since we are considering only loops with a single loop condition
                         * the only backward edge allowed is the loop jump (from bottom to top) */

                        if (block->bbJumpDest->bbNum <= block->bbNum)
                        {
                            /* we have a backward edge */

                            if ((block != bottom) && (block->bbJumpDest != head))
                                goto NEXT_LOOP;
                        }

                        /* fall through */

                    case BBJ_NONE:
                    case BBJ_THROW:
                    case BBJ_RETURN:
                        break;

                    case BBJ_RET:
                    case BBJ_CALL:
                    case BBJ_SWITCH:
                        goto NEXT_LOOP;
                    }

                    /* Check all statements in the block */

                    for (tree = block->bbTreeList; tree; tree = tree->gtNext)
                    {
                        GenTreePtr      stmt = tree->gtStmt.gtStmtExpr;

#if !RMV_ENTIRE_LOOPS_ONLY
                        if (stmt->gtOper == GT_JTRUE)
                        {
                            /* The condition might affect live variables */

                            if  (optFindLiveRefs(stmt, false,  true))
                            {
                                /* An unacceptable tree was detected; give up */

                                goto NEXT_LOOP;
                            }
                        }
                        else
#endif
                        if  (stmt->gtFlags & GTF_ASG)
                        {
                            /* There is an assignment - look for more live refs */

                            if  (optFindLiveRefs(stmt, false, false))
                            {
                                /* An unacceptable tree was detected; give up */

                                goto NEXT_LOOP;
                            }
                        }
                    }
                }
#if !RMV_ENTIRE_LOOPS_ONLY
            }
            while (liveExit != optLoopLiveExit || loopCond != optLoopCondTest);
#endif

#ifdef  DEBUG
            if  (verbose)
            {
                VARSET_TP allVars = (optLoopLiveExit | optLoopAssign);
                printf("Loop [BB%02u..BB%02u]", head->bbNum, tail->bbNum - 1);
                printf(  " exit="); lvaDispVarSet(optLoopLiveExit, allVars);
                printf("\n assg="); lvaDispVarSet(optLoopAssign  , allVars);
                printf("\n\n");
            }
#endif
            /* The entire loop seems totally worthless but we can only throw 
             * away the loop body since at this point we cannot guarantee the 
             * loop is not infinite */

            /* @TODO [CONSIDER] [04/16/01] []:
             * So far we only consider while-do loops with only one 
             * condition - Expand the logic to allow multiple 
             * conditions, but mark the one condition loop as special
             * because we can do more  optimizations with them */

            /* the last statement in the loop has to be the conditional jump */

            assert (bottom->bbJumpKind == BBJ_COND);
            assert (cond->gtOper == GT_JTRUE);

            unsigned        lclNum;
            LclVarDsc   *   varDsc;
            unsigned        varIndex;
            VARSET_TP       bitMask;

            unsigned        rhsLclNum;
            VARSET_TP       rhsBitMask;

            /* find who's who - the loop condition has to be a simple comparisson
             * between locals and/or constants */

            if (op2->OperKind() & GTK_CONST)
            {
                /* op1 must be a local var, otherwise we bail */

                if (op1->gtOper != GT_LCL_VAR)
                    goto NEXT_LOOP;

                lclNum = op1->gtLclVar.gtLclNum;

                /* UNDONE: this is a special loop that iterates a KNOWN constant
                 * UNDONE: number of times (provided we can later tell if the iterator
                 * UNDONE: is i++ (or similar) - treat this case separately */
            }
            else
            {
                /* if op2 not a local var quit */

                if (op2->gtOper != GT_LCL_VAR)
                    goto NEXT_LOOP;

                /* op1 has to be either constant or local var
                 * if constant things are simple */

                if (op1->OperKind() & GTK_CONST)
                {
                    /* here is our iterator */
                    lclNum = op2->gtLclVar.gtLclNum;
                }
                else if (op1->gtOper == GT_LCL_VAR)
                {
                    /* special case - both are local vars
                     * check if one of them is not assigned in the loop
                     * then the other one is the iterator */

                    lclNum = op1->gtLclVar.gtLclNum;
                    assert(lclNum < lvaCount);
                    varDsc = lvaTable + lclNum;
                    varIndex = varDsc->lvVarIndex;
                    assert(varIndex < lvaTrackedCount);
                    bitMask  = genVarIndexToBit(varIndex);

                    rhsLclNum = op2->gtLclVar.gtLclNum;
                    assert(rhsLclNum < lvaCount);
                    varDsc = lvaTable + rhsLclNum;
                    varIndex = varDsc->lvVarIndex;
                    assert(varIndex < lvaTrackedCount);
                    rhsBitMask  = genVarIndexToBit(varIndex);

                    if (optLoopAssign & bitMask)
                    {
                        /* op1 is assigned in the loop */

                        if (optLoopAssign & rhsBitMask)
                        {
                            /* both are assigned in the loop - bail */
                            goto NEXT_LOOP;
                        }

                        /* op1 is our iterator - already catched by lclNum */
                    }
                    else
                    {
                        /* op2 must be the iterator  - check that is asigned in the loop
                         * otherwise we have a loop that is probably infinite */

                        if (optLoopAssign & rhsBitMask)
                        {
                            lclNum = rhsLclNum;
                        }
                        else
                        {
                            /* none is assigned in the loop !!!
                             * so they are both "constants" - better not worry about this loop */
                            goto NEXT_LOOP;
                        }
                    }
                }
                else
                    goto NEXT_LOOP;
            }

            /* we have the loop iterator - it has to be a tracked and
               non volatile variable (checked by optFindLiveRefs) */

            assert(lclNum < lvaCount);
            varDsc = lvaTable + lclNum;
            assert ((varDsc->lvTracked && !varDsc->lvVolatile));

            varIndex = varDsc->lvVarIndex;
            assert(varIndex < lvaTrackedCount);
            bitMask  = genVarIndexToBit(varIndex);

            /* we can remove the whole body of the loop except the
             * statements that control the iterator and the loop test */

            /* We'll create a list to hold these statements and attach it
             * to the last BB in the list and remove the other BBs */

            for (block = head; block != tail; block = block->bbNext)
            {
                /* Check all statements in the block */

                for (GenTreePtr stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
                {
                    assert (stmt->gtOper == GT_STMT);

                    /* look for assignments */

                    if ((stmt->gtStmt.gtStmtExpr->gtFlags & GTF_ASG) == 0)
                        continue;

                    for (tree = stmt->gtStmt.gtStmtList; tree; tree = tree->gtNext)
                    {
                        /* we only look for assignments that are at the top node
                         * if an assignment to the iterator is made in a subtree we bail */

                        if  (tree->OperKind() & GTK_ASGOP)
                        {
                            /* Look for assignments to the iterator */

                            GenTreePtr      iterVar = tree->gtOp.gtOp1;

                            if (iterVar->gtOper == GT_LCL_VAR)
                            {
                                /* check if this is the iterator */

                                if (iterVar->gtLclVar.gtLclNum == lclNum)
                                {
                                    /* make sure we are at the top of the tree */
                                    /* also require that the iterator is a GTF_VAR_USEASG */

                                    if ((tree->gtNext != 0) || ((iterVar->gtFlags & GTF_VAR_USEASG) == 0))
                                        goto NEXT_LOOP;

                                    /* this is the iterator - make sure it is in a block
                                     * that dominates the loop bottom */

                                    if ( !fgDominate(block, bottom) )
                                    {
                                        /* iterator is conditionally updated - too complicated to track */
                                        goto NEXT_LOOP;
                                    }

                                    /* require that the RHS is either a constant or
                                       a local var not assigned in the loop */

                                    if (tree->gtOp.gtOp2->OperKind() & GTK_CONST)
                                        goto ITER_STMT;

                                    if (tree->gtOp.gtOp2->gtOper == GT_LCL_VAR)
                                    {
                                        rhsLclNum = tree->gtOp.gtOp2->gtLclVar.gtLclNum;

                                        assert(rhsLclNum < lvaCount);
                                        varDsc = lvaTable + rhsLclNum;

                                        varIndex = varDsc->lvVarIndex;
                                        assert(varIndex < lvaTrackedCount);
                                        rhsBitMask  = genVarIndexToBit(varIndex);

                                        if (optLoopAssign & rhsBitMask)
                                        {
                                            /* variable is assigned in the loop - bail */

                                            goto NEXT_LOOP;
                                        }

ITER_STMT:
                                        /* everything OK - add this statement to the list of
                                         * statements we won't throw away */

                                        assert(stmt->gtOper == GT_STMT);

                                        if (keepStmtList)
                                        {
                                            /* we already have statements in the list - append the new statement */

                                            assert(keepStmtLast);

                                            /* Point 'prev' at the previous node, so that we can walk backwards */

                                            stmt->gtPrev = keepStmtLast;

                                            /* Append the expression statement to the list */

                                            keepStmtLast->gtNext = stmt;
                                            keepStmtLast         = stmt;
                                        }
                                        else
                                        {
                                            /* first statement in the list */

                                            assert(keepStmtLast == 0);
                                            keepStmtList = keepStmtLast = stmt;
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }

            /* check if we found any valid iterators in the loop */

            if (keepStmtList)
            {
                /* append the termination condition */

                GenTreePtr condStmt = bottom->bbTreeList->gtPrev;
                assert(condStmt->gtOper == GT_STMT);

                assert(keepStmtLast);
                condStmt->gtPrev = keepStmtLast;
                keepStmtLast->gtNext = condStmt;

                /* Make the list circular, so that we can easily walk it backwards */

                keepStmtList->gtPrev =  condStmt;
            }
            else
            {
                /* bail */

                goto NEXT_LOOP;
            }

            /* the loop will now consist of only the last BB with the new list of statements */

            genRemoveBBsection(head, bottom);

            /* bottom is the last and only block in the loop - store the new tree list */

            bottom->bbTreeList = keepStmtList;

            /* make it jump to itself */

            assert (bottom->bbJumpKind == BBJ_COND);
            bottom->bbJumpDest = bottom;

#ifdef  DEBUG
            if  (verbose)
            {
                printf("Partially worthless loop found [BB%02u..BB%02u]\n", head->bbNum, tail->bbNum - 1);
                printf("Removing the body of the loop and keeping the loop condition:\n");

                printf("New loop condition BB%02u:\n", block->bbNum);

                for (tree = bottom->bbTreeList; tree; tree = tree->gtNext)
                {
                    gtDispTree(tree->gtStmt.gtStmtExpr, 0);
                    printf("\n");
                }

                printf("\n");
            }
#endif

            /* Mark the loop as removed and force another pass */

            loopRmv |= loopBit;
            repeat   = true;

#endif // 0 
        NEXT_LOOP:;

        }
    }
    while (repeat);

    /* What is the world is this code here trying to do??? */
    if  (optLoopCount == 16 && loopSkp == 1 && loopCnt == 14)
    {
        loopSkp = -1;
        goto AGAIN;
    }
}

/*****************************************************************************/
#endif // CODE_MOTION
/*****************************************************************************/
#if HOIST_THIS_FLDS
/*****************************************************************************
  HOIST_THIS_FLDS is a similar to optOptimizeCSEs() but customized for
  accesses of fields of the "this" pointer (for non-static methods).
  CSE is coarse-grained in tracking interference. An indirect-write kills
  all candidates which use indirections, and would kill all field-accesses.
  HOIST_THIS_FLDS tracks individual fields, but only for the "this" object.
 *****************************************************************************
 */

void                Compiler::optHoistTFRinit()
{
    optThisFldLst       = 0;
    optThisFldCnt       = 0;
    optThisFldLoop      = false;
    optThisFldDont      = true;
    optThisPtrModified  = false;

    if  (opts.compMinOptim)
        return;

    if (info.compIsStatic)
        return;

    /* Since "this" can be NULL on entry to a method, we can only do this
       optimization for field-references which are dominated by a
       de-referencing of the "this" pointer, or if loose exceptions are OK. */

    if (!info.compLooseExceptions)
        return;

    optThisFldDont = false;
}


Compiler::thisFldPtr      Compiler::optHoistTFRlookup(CORINFO_FIELD_HANDLE hnd)
{
    thisFldPtr      fld;

    for (fld = optThisFldLst; fld; fld = fld->tfrNext)
    {
        if  (fld->tfrField == hnd)
            return  fld;
    }

    fld = (thisFldPtr)compGetMem(sizeof(*fld));

    fld->tfrField   = hnd;
    fld->tfrIndex   = ++optThisFldCnt;

    fld->tfrUseCnt  = 0;
    fld->tfrDef     = 0;
    fld->tfrTempNum = 0;

#ifdef DEBUG
    fld->optTFRHoisted = false;
#endif

    fld->tfrNext    = optThisFldLst;
                      optThisFldLst = fld;

    return  fld;
}


/*****************************************************************************
 *  Replace occurences of "this->field" with a substitue temp variable.
 */

void                Compiler::optHoistTFRoptimize()
{
    thisFldPtr      fld;
    BasicBlock *    blk;
    GenTreePtr      lst;
    GenTreePtr      beg;

    assert(fgFirstBB);

    if  (optThisFldLoop == false && !compStressCompile(STRESS_GENERIC_VARN, 30))
        optThisFldDont = true;

    if  (optThisFldDont)
        return;

    for (fld = optThisFldLst, blk = NULL; fld; fld = fld->tfrNext)
    {
        assert(fld->optTFRHoisted == false);
        assert(fld->tfrTree->gtOper == GT_FIELD);
#if INLINING
        assert(eeGetFieldClass(fld->tfrTree->gtField.gtFldHnd) == info.compClassHnd);
#endif

//      printf("optHoist candidate [handle=%08X,refcnt=%02u]\n", fld->tfrField, fld->tfrUseCnt);

        /* If this field has been assigned, forget it */

        if  (fld->tfrDef)
            continue;

        /* If the use count is not high enough, forget it */

        if  (fld->tfrUseCnt < 1)
        {
            /* Mark the field as off limits for later logic */

            fld->tfrDef = true;
            continue;
        }

#ifdef DEBUG
        fld->optTFRHoisted = true;
#endif

        /* Make sure we've allocated the initialization block */

        if  (!blk)
        {
            /* Allocate the block descriptor */

            blk = bbNewBasicBlock(BBJ_NONE);

            /* Make sure the block doesn't get thrown away! */

            blk->bbFlags |= (BBF_IMPORTED | BBF_INTERNAL);

            /* Prepend the block to the global basic block list */

            blk->bbNext = fgFirstBB;
                          fgFirstBB = blk;

            /* We don't have any trees yet */

            lst = 0;
        }

        /* Grab a temp for this field */

        unsigned        tmp = lvaGrabTemp();

        fld->tfrTempNum = tmp;

        GenTreePtr      val = fld->tfrTree;

        assert(val->gtOper == GT_FIELD);
        assert(val->gtOp.gtOp1->gtOper == GT_LCL_VAR &&
               val->gtOp.gtOp1->gtLclVar.gtLclNum == 0);

        /* Create an assignment to the temp */

        GenTreePtr      asg = gtNewStmt();
        asg->gtStmt.gtStmtExpr = gtNewTempAssign(tmp, val);

        /* make sure the right flags are passed on to the temp */

        // asg->gtStmt.gtStmtExpr->gtOp.gtOp1->gtFlags |= val->gtFlags & GTF_GLOB_EFFECT;

#ifdef  DEBUG
        if  (verbose)
        {
            printf("\nHoisted field ref [handle=%08X,refcnt=%02u]\n", fld->tfrField, fld->tfrUseCnt);
            gtDispTree(asg);
        }
#endif

        /* Prepend the assignment to the list */

        asg->gtPrev = lst;
        asg->gtNext = 0;

        if  (lst)
            lst->gtNext = asg;
        else
            beg         = asg;

        lst = asg;
    }

    /* Have we added a basic block? */

    if  (blk)
    {
        /* Store the assignment statement list in the block */

        blk->bbTreeList = beg;

        /* Point the "prev" field of first entry to the last one */

        beg->gtPrev = lst;
    }
    else
    {
        /* We didn't hoist anything, so pretend nothing ever happened */

        optThisFldDont = true;
    }
}


/*****************************************************************************/
#endif//HOIST_THIS_FLDS
/******************************************************************************
 * Function used by folding of boolean conditionals
 * Given a GT_JTRUE node, checks that it is a boolean comparision of the form
 *    "if (boolVal ==/!=  0/1)". This is translated into a GT_EQ node with "op1"
 *    being a boolean lclVar and "op2" the const 0/1.
 * On success, the comparand (ie. boolVal) is returned.   Else NULL.
 * compPtr returns the compare node (i.e. GT_EQ or GT_NE node)
 * boolPtr returns whether the comparand is a boolean value (must be 0 or 1).
 * When return boolPtr equal to true, if the comparision was against a 1 (i.e true)
 * value then we morph the tree by reversing the GT_EQ/GT_NE and change the 1 to 0.
 */

GenTree *           Compiler::optIsBoolCond(GenTree *   condBranch,
                                            GenTree * * compPtr,
                                            bool      * boolPtr)
{
    bool isBool = false;

    assert(condBranch->gtOper == GT_JTRUE);
    GenTree *   cond = condBranch->gtOp.gtOp1;

    /* The condition must be "!= 0" or "== 0" */

    if (cond->gtOper != GT_EQ && cond->gtOper != GT_NE)
        return NULL;

    /* Return the compare node to the caller */

    *compPtr = cond;

    /* Get hold of the comparands */

    GenTree *   opr1 = cond->gtOp.gtOp1;
    GenTree *   opr2 = cond->gtOp.gtOp2;

    if  (opr2->gtOper != GT_CNS_INT)
        return  NULL;

    if  (((unsigned) opr2->gtIntCon.gtIconVal) > 1)
        return NULL;

    /* Is the value a boolean?
     * We can either have a boolean expression (marked GTF_BOOLEAN) or
     * a local variable that is marked as being boolean (lvIsBoolean) */

    if  (opr1->gtFlags & GTF_BOOLEAN)
    {
        isBool = true;
    }
    else if (opr1->gtOper == GT_CNS_INT)
    {
        if (((unsigned) opr1->gtIntCon.gtIconVal) <= 1)
            isBool = true;
    }
    else if (opr1->gtOper == GT_LCL_VAR)
    {
        /* is it a boolean local variable */

        unsigned    lclNum = opr1->gtLclVar.gtLclNum;
        assert(lclNum < lvaCount);

        if (lvaTable[lclNum].lvIsBoolean)
            isBool = true;
    }

    /* Was our comparison against the constant 1 (i.e. true) */
    if  (opr2->gtIntCon.gtIconVal == 1)
    {
        // If this is a boolean expression tree we can reverse the relop 
        // and change the true to false.
        if (isBool)
        {
            cond->SetOper(GenTree::ReverseRelop(cond->OperGet()));
            opr2->gtIntCon.gtIconVal = 0;
        }
        else
            return NULL;
    }

    *boolPtr = isBool;
    return opr1;
}


void                Compiler::optOptimizeBools()
{
#ifdef DEBUG
    if  (verbose) 
        printf("*************** In optOptimizeBools()\n");
#endif
    bool            change;
    bool            condFolded = false;

#ifdef  DEBUG
    fgDebugCheckBBlist();
#endif

    do
    {
        change = false;

        for (BasicBlock * b1 = fgFirstBB; b1; b1 = b1->bbNext)
        {
            /* We're only interested in conditional jumps here */

            if  (b1->bbJumpKind != BBJ_COND)
                continue;

            /* If there is no next block, we're done */

            BasicBlock * b2 = b1->bbNext;
            if  (!b2)
                break;

            /* The next block must not be marked as BBF_DONT_REMOVE */
            if  (b2->bbFlags & BBF_DONT_REMOVE)
                continue;

            /* The next block also needs to be a condition */

            if  (b2->bbJumpKind != BBJ_COND)
                continue;

            bool    sameTarget; // Do b1 and b2 have the same bbJumpDest?

            if      (b1->bbJumpDest == b2->bbJumpDest)
            {
                /* Given the following sequence of blocks :
                        B1: brtrue(t1, BX)
                        B2: brtrue(t2, BX)
                        B3:
                   we wil try to fold it to :
                        B1: brtrue(t1|t2, BX)
                        B3:
                */

                sameTarget = true;
            }
            else if (b1->bbJumpDest == b2->bbNext)  /*b1->bbJumpDest->bbNum == n1+2*/
            {
                /* Given the following sequence of blocks :
                        B1: brtrue(t1, B3)
                        B2: brtrue(t2, BX)
                        B3:
                   we will try to fold it to :
                        B1: brtrue((!t1)&&t2, B3)
                        B3:
                */

                sameTarget = false;
            }
            else
            {
                continue;
            }

            /* The second block must contain a single statement */

            GenTreePtr s2 = b2->bbTreeList;
            if  (s2->gtPrev != s2)
                continue;

            assert(s2->gtOper == GT_STMT);
            GenTreePtr  t2 = s2->gtStmt.gtStmtExpr;
            assert(t2->gtOper == GT_JTRUE);

            /* Find the condition for the first block */

            GenTreePtr  s1 = b1->bbTreeList->gtPrev;

            assert(s1->gtOper == GT_STMT);
            GenTreePtr  t1 = s1->gtStmt.gtStmtExpr;
            assert(t1->gtOper == GT_JTRUE);

            /* UNDONE: make sure nobody else jumps to "b2" */

            if  (b2->bbRefs > 1)
                continue;

            /* Find the branch conditions of b1 and b2 */

            bool        bool1, bool2;

            GenTreePtr  c1 = optIsBoolCond(t1, &t1, &bool1);
            if (!c1) continue;

            GenTreePtr  c2 = optIsBoolCond(t2, &t2, &bool2);
            if (!c2) continue;

            assert(t1->gtOper == GT_EQ || t1->gtOper == GT_NE && t1->gtOp.gtOp1 == c1);
            assert(t2->gtOper == GT_EQ || t2->gtOper == GT_NE && t2->gtOp.gtOp1 == c2);

            /* The second condition must not contain side effects */

            if  (c2->gtFlags & GTF_GLOB_EFFECT)
                continue;

            /* The second condition must not be too expensive */
            // @TODO [CONSIDER] [04/16/01] []: smarter heuristics

            if  (!c2->OperIsLeaf())
                continue;

            genTreeOps      foldOp;
            genTreeOps      cmpOp;

            if (sameTarget)
            {
                /* Both conditions must be the same */

                if (t1->gtOper != t2->gtOper)
                    continue;

                if (t1->gtOper == GT_EQ)
                {
                    /* t1:c1==0 t2:c2==0 ==> Branch to BX if either value is 0
                       So we will branch to BX if (c1&c2)==0 */

                    foldOp = GT_AND;
                    cmpOp  = GT_EQ;
                }
                else
                {
                    /* t1:c1!=0 t2:c2!=0 ==> Branch to BX if either value is non-0
                       So we will branch to BX if (c1|c2)!=0 */

                    foldOp = GT_OR;
                    cmpOp  = GT_NE;
                }
            }
            else
            {
                /* The b1 condition must be the reverse of the b2 condition */

                if (t1->gtOper == t2->gtOper)
                    continue;

                if (t1->gtOper == GT_EQ)
                {
                    /* t1:c1==0 t2:c2!=0 ==> Branch to BX if both values are non-0
                       So we will branch to BX if (c1&c2)!=0 */

                    foldOp = GT_AND;
                    cmpOp  = GT_NE;
                }
                else
                {
                    /* t1:c1!=0 t2:c2==0 ==> Branch to BX if both values are 0
                       So we will branch to BX if (c1|c2)==0 */

                    foldOp = GT_OR;
                    cmpOp  = GT_EQ;
                }
            }

            // Anding requires both values to be 0 or 1

            if ((foldOp == GT_AND) && (!bool1 || !bool2))
                continue;

            t1->SetOper(cmpOp);
            t1->gtOp.gtOp1 = t2 = gtNewOperNode(foldOp, TYP_INT, c1, c2);

            if (bool1 && bool2)
            {
                /* When we 'OR'/'AND' two booleans, the result is boolean as well */
                t2->gtFlags |= GTF_BOOLEAN;
            }

            if (!sameTarget)
            {
                /* Modify the target of the conditional jump and update bbRefs and bbPreds */

                b1->bbJumpDest->bbRefs--;
                fgRemovePred(b1->bbJumpDest, b1);

                b1->bbJumpDest = b2->bbJumpDest;

                fgAddRefPred(b2->bbJumpDest, b1);
            }

            /* Get rid of the second block (which is a BBJ_COND) */

            assert(b1->bbJumpKind == BBJ_COND);
            assert(b2->bbJumpKind == BBJ_COND);
            assert(b1->bbJumpDest == b2->bbJumpDest);
            assert(b1->bbNext == b2); assert(b2->bbNext);           

            b1->bbNext = b2->bbNext;

            /* Update bbRefs and bbPreds */

            /* Replace pred 'b2' for 'b2->bbNext' with 'b1'
             * Remove  pred 'b2' for 'b2->bbJumpDest' */

            fgReplacePred(b2->bbNext, b2, b1);

            b2->bbJumpDest->bbRefs--;
            fgRemovePred(b2->bbJumpDest, b2);

            /* Update the block numbers and try again */

            change = true;
            condFolded = true;
/*
            do
            {
                b2->bbNum = ++n1;
                b2 = b2->bbNext;
            }
            while (b2);
*/

            // Update loop table
            fgUpdateLoopsAfterCompacting(b1, b2);
            
#ifdef DEBUG
            if  (verbose)
            {
                printf("Folded boolean conditions of BB%02u and BB%02u to :\n",
                       b1->bbNum, b2->bbNum);
                gtDispTree(s1); printf("\n");
            }
#endif
        }
    }
    while (change);
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\peloader.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                            PELoader.cpp                                   XX
XX                                                                           XX
XX   This file has been grabbed out of VM\ceeload.cpp                        XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/


#include "jitpch.h"
#pragma hdrstop

#define JIT_OR_NATIVE_SUPPORTED 1
#include "corhandle.cpp"

#undef memcpy

/*************************************************************************************/
// Constructor and destructor!
/*************************************************************************************/

PELoader::PELoader()
{
    m_hFile = NULL;
    m_pNT = NULL;
}

PELoader::~PELoader()
{

    // If we have an hFile then we opened this file ourselves!
    if (m_hFile)
        this->close();
    // Unload the dll so that refcounting of EE will be done correctly
    if (m_pNT && (m_pNT->FileHeader.Characteristics & IMAGE_FILE_DLL)) {
        m_hMod.ReleaseResources(TRUE);
    }
    m_pNT = NULL;
}

/*************************************************************************************/
/*************************************************************************************/
void PELoader::close()
{

    _ASSERTE(m_hFile != NULL);
    if (m_hFile)
    {
        CloseHandle(m_hFile);
        m_hFile = NULL;
    }
}


// We will use an overridden open method to take either an LPCSTR or an HMODULE!
/*************************************************************************************/
/*************************************************************************************/
HMODULE PELoader::open(LPCSTR moduleName)
{
  HMODULE newhMod = NULL;

  _ASSERTE(moduleName);
  if (!moduleName)
    return FALSE;
  newhMod = LoadLibraryA(moduleName);
  return newhMod;
}

/*************************************************************************************/
HRESULT PELoader::open(HMODULE hMod)
{

    IMAGE_DOS_HEADER* pdosHeader;

    _ASSERTE(hMod);
    m_hMod.SetHandle(hMod);

    // get the dos header...
    pdosHeader = (IMAGE_DOS_HEADER*) m_hMod.ToHandle();


    if ((pdosHeader->e_magic == IMAGE_DOS_SIGNATURE) &&
        (pdosHeader->e_lfanew != 0))
    {
        m_pNT = (IMAGE_NT_HEADERS*) (pdosHeader->e_lfanew + (DWORD) m_hMod.ToHandle());

        if ((m_pNT->Signature != IMAGE_NT_SIGNATURE) ||
            (m_pNT->FileHeader.SizeOfOptionalHeader != IMAGE_SIZEOF_NT_OPTIONAL_HEADER) ||
            (m_pNT->OptionalHeader.Magic != IMAGE_NT_OPTIONAL_HDR_MAGIC))
        {
            // @TODO [REVISIT] [04/16/01] []: add some SetLastError info? Not sure that 
            // in this case this could happen...But!
            // Make this appear uninitalized because for some reason this file is toast...
            // Not sure that this could ever happen because this file has already been loaded
            // bye the system loader unless someone gave us garbage as the hmod
            m_pNT = NULL;
            return HRESULT_FROM_WIN32(ERROR_BAD_FORMAT);
        }
    }
    else
    {
    // @TODO [REVISIT] [04/16/01] []: add some SetLastError info? 
    // Not sure that in this case this could happen...But!
        return HRESULT_FROM_WIN32(ERROR_BAD_FORMAT);
    }

    return S_OK;
}

IMAGE_COR20_HEADER *PELoader::getCOMHeader(HMODULE hMod, 
										   IMAGE_NT_HEADERS *pNT)
{
	if (pNT == NULL)
		pNT = getNTHeader(hMod);

	IMAGE_DATA_DIRECTORY *entry 
	  = &pNT->OptionalHeader.DataDirectory[IMAGE_DIRECTORY_ENTRY_COMHEADER];
	
    if (entry->VirtualAddress == 0 || entry->Size == 0) 
        return NULL;

	if (entry->Size < sizeof(IMAGE_COR20_HEADER))
		return NULL;

    //verify RVA and size of the COM+ header
    HRESULT hr = verifyDirectory(pNT, entry);
    if(FAILED(hr))
		return NULL;

	return (IMAGE_COR20_HEADER *) (entry->VirtualAddress + (DWORD) hMod);
}

IMAGE_NT_HEADERS *PELoader::getNTHeader(HMODULE hMod)
{
	IMAGE_DOS_HEADER *pDOS = (IMAGE_DOS_HEADER*) hMod;
	IMAGE_NT_HEADERS *pNT;
    
    if ((pDOS->e_magic == IMAGE_DOS_SIGNATURE) &&
        (pDOS->e_lfanew != 0))
    {
        pNT = (IMAGE_NT_HEADERS*) (pDOS->e_lfanew + (DWORD) hMod);

        if ((pNT->Signature == IMAGE_NT_SIGNATURE) ||
            (pNT->FileHeader.SizeOfOptionalHeader == IMAGE_SIZEOF_NT_OPTIONAL_HEADER) ||
            (pNT->OptionalHeader.Magic == IMAGE_NT_OPTIONAL_HDR_MAGIC))
        {
			return pNT;
        }
    }

	return NULL;
}

HRESULT PELoader::verifyDirectory(IMAGE_NT_HEADERS *pNT,
								  IMAGE_DATA_DIRECTORY *dir) 
{
	// Under CE, we have no NT header.
	if (pNT == NULL)
		return S_OK;

    int section_num = 1;
    int max_section = pNT->FileHeader.NumberOfSections;
    IMAGE_SECTION_HEADER* pCurrSection;     //pointer to current section header
    IMAGE_SECTION_HEADER* prevSection = NULL;       //pointer to previous section

    //initally pCurrSectionRVA points to the first section in the PE file
    pCurrSection = IMAGE_FIRST_SECTION32(pNT);  // @TODO [REVISIT] [04/16/01] []: need to use 64 bit version??

    //check if both RVA and size equal zero
    if(dir->VirtualAddress == NULL && dir->Size == NULL)
        return S_OK;

    //find which section the (input) RVA belongs to
    while(dir->VirtualAddress >= pCurrSection->VirtualAddress && section_num <= max_section)
    {
        section_num++;
        prevSection = pCurrSection;
        pCurrSection++;     //pCurrSection now points to next section header
    }
    //check if (input) size fits within section size
    if(prevSection != NULL)     
    {
        if(dir->VirtualAddress <= prevSection->VirtualAddress + prevSection->Misc.VirtualSize)
        {
            if(dir->VirtualAddress + dir->Size <= prevSection->VirtualAddress + prevSection->Misc.VirtualSize)
                return S_OK;
        }
    }   
    return HRESULT_FROM_WIN32(ERROR_BAD_FORMAT);
}

void SectionInfo::Init(PELoader *pPELoader, IMAGE_DATA_DIRECTORY *dir)
{
    _ASSERTE(dir);
    m_dwSectionOffset = dir->VirtualAddress;
    if (m_dwSectionOffset != 0)
        m_pSection = pPELoader->base() + m_dwSectionOffset;
    else
        m_pSection = 0;
    m_dwSectionSize = dir->Size;
}
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\schedsh3.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                             schedSH3.cpp                                  XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

#include "jitpch.h"
#pragma hdrstop

#include "alloc.h"
#include "instr.h"
#include "emit.h"
#include "target.h"

/*****************************************************************************/
#if     SCHEDULER && TGT_SH3
/*****************************************************************************
 *
 *  We store two values in each entry of the "extra dependency" table, with
 *  the "write" value shifted by the following amount.
 */

const
unsigned        SCHED_XDEP_SHF = 4;

/*****************************************************************************
 *
 *  Records any "extra" target-dependent scheduling dependencies.
 */

emitRegs            emitter::scSpecInsDep(instrDesc   * id,
                                          scDagNode   * dagDsc,
                                          scExtraInfo * xptr)
{
    unsigned        extra;
    unsigned        extraRD;
    unsigned        extraWR;

    /* Make sure the bits in our table don't overlap */

    assert(SCHED_XDEP_ALL < 1 << SCHED_XDEP_SHF);

    static
    BYTE            extraDep[] =
    {
        #define INST1(id, nm, bd, um, rf, wf, rx, wx, br, i1         ) (wx)<<SCHED_XDEP_SHF|(rx),
        #define INST2(id, nm, bd, um, rf, wf, rx, wx, br, i1, i2     ) (wx)<<SCHED_XDEP_SHF|(rx),
        #define INST3(id, nm, bd, um, rf, wf, rx, wx, br, i1, i2, i3 ) (wx)<<SCHED_XDEP_SHF|(rx),
        #include "instrSH3.h"
        #undef  INST1
        #undef  INST2
        #undef  INST3
    };

    /* Get hold of the "extra dependency" bitset for our instruction */

    assert(id->idIns < sizeof(extraDep)/sizeof(extraDep[0]));

    extra   = extraDep[id->idIns];

    extraRD = extra &  SCHED_XDEP_ALL;
    extraWR = extra >> SCHED_XDEP_SHF;

//  printf("%10s %02X[XR=%1X,XW=%1X]\n", emitComp->genInsName(id->idIns), extra, extraRD, extraWR);

    /* Process any read and write dependencies */

    if  (extraRD)
    {
        if  (extraRD & SCHED_XDEP_PR ) scDepUse(dagDsc, "MAC", scMACdef, scMACuse);
        if  (extraRD & SCHED_XDEP_MAC) scDepUse(dagDsc, "PR" , scPRRdef, scPRRuse);
    }

    if  (extraWR)
    {
        if  (extraWR & SCHED_XDEP_PR ) scDepDef(dagDsc, "MAC", scMACdef, scMACuse);
        if  (extraWR & SCHED_XDEP_MAC) scDepDef(dagDsc, "PR" , scPRRdef, scPRRuse);
    }

    return SR_NA;
}

/*****************************************************************************
 *
 *  Updates any "extra" target-dependent scheduling dependencies.
 */

void                emitter::scSpecInsUpd(instrDesc   * id,
                                          scDagNode   * dagDsc,
                                          scExtraInfo * xptr)
{
    unsigned        extra;
    unsigned        extraRD;
    unsigned        extraWR;

    /* Get hold of the "extra dependency" bitset for our instruction */

    extra   = xptr->scxDeps;

    extraRD = extra &  SCHED_XDEP_ALL;
    extraWR = extra >> SCHED_XDEP_SHF;

    /* Process any read and write dependencies */

    if  (extraRD)
    {
        if  (extraRD & SCHED_XDEP_PR ) scUpdUse(dagDsc, &scMACdef, &scMACuse);
        if  (extraRD & SCHED_XDEP_MAC) scUpdUse(dagDsc, &scPRRdef, &scPRRuse);
    }

    if  (extraWR)
    {
        if  (extraWR & SCHED_XDEP_PR ) scUpdDef(dagDsc, &scMACdef, &scMACuse);
        if  (extraWR & SCHED_XDEP_MAC) scUpdDef(dagDsc, &scPRRdef, &scPRRuse);
    }
}

/*****************************************************************************/
#endif//SCHEDULER && TGT_SH3
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\register.h ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*****************************************************************************/
#if  !  TGT_x86
#error  This file can only be used when targetting x86!
#endif
/*****************************************************************************/
#ifndef REGDEF
#error  Must define REGDEF macro before including this file
#endif
/*****************************************************************************/
/*                  The following is x86 specific                            */
/*****************************************************************************/
/*
REGDEF(name, rnum,  mask, byteable) */
REGDEF(EAX,     0,  0x01,    1    )
REGDEF(ECX,     1,  0x02,    1    )
REGDEF(EDX,     2,  0x04,    1    )
REGDEF(EBX,     3,  0x08,    1    )
REGDEF(ESP,     4,  0x10,    0    )
REGDEF(EBP,     5,  0x20,    0    )
REGDEF(ESI,     6,  0x40,    0    )
REGDEF(EDI,     7,  0x80,    0    )

REGDEF(STK,     8,  0x00,    0    )

/*****************************************************************************/
#undef  REGDEF
/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\regset.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                           RegSet                                          XX
XX                                                                           XX
XX  Represents the register set, and their states during code generation     XX
XX  Can select an unused register, keeps track of the contents of the        XX
XX  registers, and can spill registers                                       XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

#include "jitpch.h"
#pragma hdrstop

#include "emit.h"

/*****************************************************************************/

const regMaskSmall      regMasks[] =
{
    #if     TGT_x86
    #define REGDEF(name, rnum, mask, byte) mask,
    #include "register.h"
    #undef  REGDEF
    #endif

    #if     TGT_SH3
    #define REGDEF(name, strn, rnum, mask) mask,
    #include "regSH3.h"
    #undef  REGDEF
    #endif
};

/*****************************************************************************/
#if SCHEDULER || USE_SET_FOR_LOGOPS

/* Round robin register allocation. When scheduling, the following table is
 * cycled through for register allocation, to create more scheduling opportunities.
 * Also, when a redundant load is performed (not a true register allocation) we
 * still reset the global index to the table, so our next selection can be scheduled
 * with respect to the reused register

 * @TODO [REVISIT] [04/16/01] []: Using round-robin selection of registers 
 * increases code-size in small functions as we end up using callee-saved registers
 * which we will need to save and restore in the prolog and epilog.
 * Consider only using registers in rsMaskModf, or having the register-
 * predictor predict which registers will be touched anyway.
 */

#if TGT_x86

const regMaskSmall  rsREG_MASK_ORDER[] =
{
    RBM_EAX,
    RBM_EDX,
    RBM_ECX,
    RBM_EBX,
    RBM_EBP,
    RBM_ESI,
    RBM_EDI,
};

#endif

#if TGT_SH3

const regMaskSmall  rsREG_MASK_ORDER[] =
{
    RBM_r00,
    RBM_r01,
    RBM_r02,
    RBM_r03,
    RBM_r04,
    RBM_r05,
    RBM_r06,
    RBM_r07,
    RBM_r14,
    RBM_r08,
    RBM_r09,
    RBM_r10,
    RBM_r11,
    RBM_r12,
    RBM_r13,
};

#endif

inline
unsigned            Compiler::rsREGORDER_SIZE()
{
    return (sizeof(rsREG_MASK_ORDER)/sizeof(*rsREG_MASK_ORDER));
}

/*****************************************************************************
 *
 *  Given a regmask, find the best regPairNo that can be formed
 *  or return REG_PAIR_NONE if no register pair can be formed
 */

regPairNo    Compiler::rsFindRegPairNo   (regMaskTP  regMask)
{
    regPairNo       regPair;

    /* Check if regMask has zero or one bits set */
    if ((regMask & (regMask-1)) == 0)
    {
        /* If so we won't be able to find a reg pair */
        return REG_PAIR_NONE;
    }

#if TGT_x86
    if  (regMask & RBM_EAX)
    {
        /* EAX is available, see if we can pair it with another reg */

        if  (regMask & RBM_EDX) { regPair = REG_PAIR_EAXEDX; goto RET; }
        if  (regMask & RBM_ECX) { regPair = REG_PAIR_EAXECX; goto RET; }
        if  (regMask & RBM_EBX) { regPair = REG_PAIR_EAXEBX; goto RET; }
        if  (regMask & RBM_ESI) { regPair = REG_PAIR_EAXESI; goto RET; }
        if  (regMask & RBM_EDI) { regPair = REG_PAIR_EAXEDI; goto RET; }
        if  (regMask & RBM_EBP) { regPair = REG_PAIR_EAXEBP; goto RET; }
    }

    if  (regMask & RBM_ECX)
    {
        /* ECX is available, see if we can pair it with another reg */

        if  (regMask & RBM_EDX) { regPair = REG_PAIR_ECXEDX; goto RET; }
        if  (regMask & RBM_EBX) { regPair = REG_PAIR_ECXEBX; goto RET; }
        if  (regMask & RBM_ESI) { regPair = REG_PAIR_ECXESI; goto RET; }
        if  (regMask & RBM_EDI) { regPair = REG_PAIR_ECXEDI; goto RET; }
        if  (regMask & RBM_EBP) { regPair = REG_PAIR_ECXEBP; goto RET; }
    }

    if  (regMask & RBM_EDX)
    {
        /* EDX is available, see if we can pair it with another reg */

        if  (regMask & RBM_EBX) { regPair = REG_PAIR_EDXEBX; goto RET; }
        if  (regMask & RBM_ESI) { regPair = REG_PAIR_EDXESI; goto RET; }
        if  (regMask & RBM_EDI) { regPair = REG_PAIR_EDXEDI; goto RET; }
        if  (regMask & RBM_EBP) { regPair = REG_PAIR_EDXEBP; goto RET; }
    }

    if  (regMask & RBM_EBX)
    {
        /* EBX is available, see if we can pair it with another reg */

        if  (regMask & RBM_ESI) { regPair = REG_PAIR_EBXESI; goto RET; }
        if  (regMask & RBM_EDI) { regPair = REG_PAIR_EBXEDI; goto RET; }
        if  (regMask & RBM_EBP) { regPair = REG_PAIR_EBXEBP; goto RET; }
    }

    if  (regMask & RBM_ESI)
    {
        /* ESI is available, see if we can pair it with another reg */

        if  (regMask & RBM_EDI) { regPair = REG_PAIR_ESIEDI; goto RET; }
        if  (regMask & RBM_EBP) { regPair = REG_PAIR_EBPESI; goto RET; }
    }

    if  (regMask & RBM_EDI)
    {
        /* EDI is available, see if we can pair it with another reg */

        if  (regMask & RBM_EBP) { regPair = REG_PAIR_EBPEDI; goto RET; }
    }

    assert(!"Unreachable code");
    regPair = REG_PAIR_NONE;
    
#else

    /* Get hold of the two lowest bits in the avaialble mask */

    if  (regMask)
    {
        regPairMaskTP   regMaskTemp;

        regPairMaskTP   regMask1;
        regPairMaskTP   regMask2;

        /* Get the lowest bit */

        regMask1 = genFindLowestBit(regMask);

        /* Remove the lowest bit and look for another one */

        regMaskTemp = regMask - regMask1;

        if  (regMaskTemp)
        {
            /* We know we have two bits available at this point */

            regMask2 = genFindLowestBit(regMaskTemp);

            /* Convert the masks to register numbers */

            regPair  = gen2regs2pair(genRegNumFromMask(regMask1),
                                     genRegNumFromMask(regMask2));
        }
    }

#endif

RET:

    return  regPair;
}

/*****************************************************************************
 *
 *  This function is called when we reuse a register. It updates
 *  rsNextPickRegIndex as if we had just allocated that register,
 *  so that the next allocation will be some other register.
 */

void                Compiler::rsUpdateRegOrderIndex(regNumber reg)
{
    for (unsigned i = 0; i < rsREGORDER_SIZE(); i++)
    {
        if (rsREG_MASK_ORDER[i] & genRegMask(reg))
        {
            rsNextPickRegIndex = (i + 1) % rsREGORDER_SIZE();
            return;
        }
    }

    assert(!"bad reg passed to genRegOrderIndex");
}

/*****************************************************************************/
#endif//SCHEDULER || USE_SET_FOR_LOGOPS
/*****************************************************************************/

void                Compiler::rsInit()
{
    /* Initialize the spill logic */

    rsSpillInit();

    /* Initialize this to 0 so method compiles are for sure reproducible */

#if SCHEDULER || USE_SET_FOR_LOGOPS
    rsNextPickRegIndex = 0;
#endif

    /* Initialize the argument register count */
    rsCurRegArg = 0;
}

/*****************************************************************************
 *
 *  Marks the register that holds the given operand value as 'used'. If 'addr'
 *  is non-zero, the register is part of a complex address mode that needs to
 *  be marked if the register is ever spilled.
 */

void                Compiler::rsMarkRegUsed(GenTreePtr tree, GenTreePtr addr)
{
    regNumber       regNum;
    regMaskTP       regMask;

    /* The value must be sitting in a register */

    assert(tree);
    assert(tree->gtFlags & GTF_REG_VAL);
#if CPU_HAS_FP_SUPPORT
    assert(genActualType(tree->gtType) == TYP_INT ||
           genActualType(tree->gtType) == TYP_REF ||
                         tree->gtType  == TYP_BYREF);
#else
    assert(genTypeSize(genActualType(tree->TypeGet())) == genTypeSize(TYP_INT));
#endif

    regNum  = tree->gtRegNum;
    regMask = genRegMask(regNum);

#ifdef  DEBUG
    if  (verbose) printf("The register %s currently holds [%08X/%08X]\n", compRegVarName(regNum), tree, addr);
#endif

    /* Remember whether the register holds a pointer */

    gcMarkRegPtrVal(regNum, tree->TypeGet());

    /* No locked register may ever be marked as free */

    assert((rsMaskLock & rsRegMaskFree()) == 0);

    /* Is the register used by two different values simultaneously? */

    if  (regMask & rsMaskUsed)
    {
        /* Save the preceding use information */

        rsRecMultiReg(regNum);
    }

    /* Set the register's bit in the 'used' bitset */

    rsMaskUsed |= regMask;

    /* Remember what values are in what registers, in case we have to spill */

    assert(rsUsedTree[regNum] == 0); rsUsedTree[regNum] = tree;
    assert(rsUsedAddr[regNum] == 0); rsUsedAddr[regNum] = addr;
}

/*****************************************************************************
 *
 *  Marks the register pair that holds the given operand value as 'used'.
 */

void                Compiler::rsMarkRegPairUsed(GenTreePtr tree)
{
    regNumber       regLo;
    regNumber       regHi;
    regPairNo       regPair;
    regMaskTP       regMask;

    /* The value must be sitting in a register */

    assert(tree);
#if SPECIAL_DOUBLE_ASG
    assert(genTypeSize(tree->TypeGet()) == genTypeSize(TYP_LONG));
#else
#if CPU_HAS_FP_SUPPORT
    assert(tree->gtType == TYP_LONG);
#else
    assert(tree->gtType == TYP_LONG || tree->gtType == TYP_DOUBLE);
#endif
#endif
    assert(tree->gtFlags & GTF_REG_VAL);

    regPair = tree->gtRegPair;
    regMask = genRegPairMask(regPair);

    regLo   = genRegPairLo(regPair);
    regHi   = genRegPairHi(regPair);

    /* Neither register obviously holds a pointer value */

    gcMarkRegSetNpt(regMask);

    /* No locked register may ever be marked as free */

    assert((rsMaskLock & rsRegMaskFree()) == 0);

    /* Are the registers used by two different values simultaneously? */

    if  (rsMaskUsed & genRegMask(regLo))
    {
        /* Save the preceding use information */

        rsRecMultiReg(regLo);
    }

    if  (rsMaskUsed & genRegMask(regHi))
    {
        /* Save the preceding use information */

        rsRecMultiReg(regHi);
    }

    /* Can't mark a register pair more than once as used */

//    assert((regMask & rsMaskUsed) == 0);

    /* Mark the registers as 'used' */

    rsMaskUsed |= regMask;

    /* Remember what values are in what registers, in case we have to spill */

    if  (regLo != REG_STK)
    {
        assert(rsUsedTree[regLo] == 0);
        rsUsedTree[regLo] = tree;
    }

    if  (regHi != REG_STK)
    {
        assert(rsUsedTree[regHi] == 0);
        rsUsedTree[regHi] = tree;
    }
}

/*****************************************************************************
 *
 *  Returns true is the given tree is currently held in reg.
 *  Note that reg may by used by multiple trees, in which case we have
 *  to search rsMultiDesc[reg].
 */

bool                Compiler::rsIsTreeInReg(regNumber reg, GenTreePtr tree)
{
    /* First do the trivial check */

    if (rsUsedTree[reg] == tree)
        return true;

    /* If the register is used by mutliple trees, we have to search the list
       in rsMultiDesc[reg] */

    if (genRegMask(reg) & rsMaskMult)
    {
        SpillDsc * multiDesc = rsMultiDesc[reg];
        assert(multiDesc);

        for (/**/; multiDesc; multiDesc = multiDesc->spillNext)
        {
            if (multiDesc->spillTree == tree)
                return true;

            assert((!multiDesc->spillNext) == (!multiDesc->spillMoreMultis));
        }
    }

    /* Not found. It must be spilled */

    return false;
}

/*****************************************************************************
 *
 *  Finds the SpillDsc correponding to tree assuming it was spilled from reg.
 */

Compiler::SpillDsc *        Compiler::rsGetSpillInfo(GenTreePtr tree,
                                                     regNumber  reg,
                                                     SpillDsc **pPrevDsc,
                                                     SpillDsc **pMultiDsc)
{
    /* Normally, trees are unspilled in the order of being spilled due to
       the post-order walking of trees during code-gen. However, this will
       not be true for something like a GT_ARR_ELEM node */

    SpillDsc * prev = NULL, * multi = rsSpillDesc[reg];

    for (SpillDsc * dsc = rsSpillDesc[reg]; dsc; prev = dsc, dsc = dsc->spillNext)
    {
        if (prev && !prev->spillMoreMultis)
            multi = dsc;

        if (dsc->spillTree == tree)
            break;
    }

    if (pPrevDsc)  *pPrevDsc  = prev;
    if (pMultiDsc) *pMultiDsc = multi;

    return dsc;
}

/*****************************************************************************
 *
 *  Mark the register set given by the register mask as not used.
 */

void                Compiler::rsMarkRegFree(regMaskTP regMask)
{
    /* Are we freeing any multi-use registers? */

    if  (regMask & rsMaskMult)
    {
        rsMultRegFree(regMask);
        return;
    }

    gcMarkRegSetNpt(regMask);

#ifdef DEBUG
    unsigned    regBit = 1;

    for (regNumber regNum = REG_FIRST; regNum < REG_COUNT; regNum = REG_NEXT(regNum), regBit <<= 1)
    {
        if  (regBit & regMask)
        {
            if  (verbose) printf("The register %s no longer holds [%08X/%08X]\n",
                compRegVarName(regNum), rsUsedTree[regNum], rsUsedAddr[regNum]);

            assert(rsUsedTree[regNum] != 0);
                   rsUsedTree[regNum] = 0;
                   rsUsedAddr[regNum] = 0;
        }
    }
#endif
    
    /* Remove the register set from the 'used' set */

    assert((regMask & rsMaskUsed) == regMask); rsMaskUsed -= regMask;

    /* No locked register may ever be marked as free */

    assert((rsMaskLock & rsRegMaskFree()) == 0);
}

/*****************************************************************************
 *
 *  Free the register from the given tree. If the register holds other tree,
 *  it will still be marked as used, else it will be completely free.
 */

void                Compiler::rsMarkRegFree(regNumber reg, GenTreePtr tree)
{
    assert(rsIsTreeInReg(reg, tree));
    regMaskTP regMask = genRegMask(reg);

    /* If the register is not multi-used, its easy. Just do the default work */

    if (!(regMask & rsMaskMult))
    {
        rsMarkRegFree(regMask);
        return;
    }

    /* The tree is multi-used. We just have to free it off the given tree but
       leave other trees which use the register as they are. The register may
       not be multi-used after freeing it from the given tree */
       
    /* Is the tree in rsUsedTree[] or in rsMultiDesc[]?
       If it is in rsUsedTree[], update rsUsedTree[] */

    if (rsUsedTree[reg] == tree)
    {
        rsRmvMultiReg(reg);
        return;
    }

    /* The tree is in rsMultiDesc[] instead of in rsUsedTree[]. Find the desc
       corresponding to the tree and just remove it from there */
    
    for (SpillDsc * multiDesc = rsMultiDesc[reg], *prevDesc = NULL;
         multiDesc; 
         prevDesc = multiDesc, multiDesc = multiDesc->spillNext)
    {
        /* If we find the descriptor with the tree we are looking for,
           discard it */

        if (multiDesc->spillTree != tree)
            continue;

        if (prevDesc == NULL)
        {
            /* The very first desc in rsMultiDesc[] matched. If there are
               no further descs, then the register is no more multi-used */

            if (!multiDesc->spillMoreMultis)
                rsMaskMult -= regMask;

            rsMultiDesc[reg] = multiDesc->spillNext;
        }
        else
        {
            /* There are a couple of other descs before the match. So the
               register is still multi-used. However, we may have to
               update spillMoreMultis for the previous desc. */

            if (!multiDesc->spillMoreMultis)
                prevDesc->spillMoreMultis = false;

            prevDesc->spillNext = multiDesc->spillNext;
        }

        /* Move the dsc to the free list */

        multiDesc->spillNext = rsSpillFree;
                               rsSpillFree = multiDesc;
#ifdef  DEBUG
        if  (verbose) 
            printf("Register %s multi-use dec for %08X - now [%08X/%08X] multMask=%08X\n",
                   compRegVarName(reg), tree, rsUsedTree[reg], rsUsedAddr[reg], rsMaskMult);
#endif

        return;
    }

    assert(!"Didnt find the spilled tree in rsMultiDesc[]");
}


/*****************************************************************************
 *
 *  Mark the register set given by the register mask as not used; there may
 *  be some 'multiple-use' registers in the set.
 */

void                Compiler::rsMultRegFree(regMaskTP regMask)
{
    regMaskTP       mulMask;

    /* Free any multiple-use registers first */

    mulMask = regMask & rsMaskMult;

    if  (mulMask)
    {
        regNumber   regNum;
        unsigned    regBit;

        for (regNum = REG_FIRST       , regBit = 1;
             regNum < REG_COUNT;
             regNum = REG_NEXT(regNum), regBit <<= 1)
        {
            if  (regBit & mulMask)
            {
                /* Free the multi-use register 'regNum' */

                rsRmvMultiReg(regNum);

                /* This register has been taken care of */

                regMask -= regBit;
            }
        }
    }

    /* If there are any single-use registers, free them */

    if  (regMask)
        rsMarkRegFree(regMask);
}

/*****************************************************************************
 *
 *  Returns the number of registers that are currently free which appear in needReg.
 */

unsigned            Compiler::rsFreeNeededRegCount(regMaskTP needReg)
{
    regMaskTP       regNeededFree = rsRegMaskFree() & needReg;
    unsigned        cntFree = 0;

    /* While some registers are free ... */

    while (regNeededFree)
    {
        /* Remove the next register bit and bump the count */

        regNeededFree -= genFindLowestBit(regNeededFree);
        cntFree += 1;
    }

    return cntFree;
}

/*****************************************************************************
 *
 *  Record the fact that the given register now contains the given local
 *  variable. Pointers are handled specially since reusing the register
 *  will extend the lifetime of a pointer register which is not a register
 *  variable.
 */

void                Compiler::rsTrackRegLclVar(regNumber reg, unsigned var)
{
    LclVarDsc *     varDsc = &lvaTable[var];
    assert(reg != REG_STK);
#if CPU_HAS_FP_SUPPORT
    assert(varTypeIsFloating(varDsc->TypeGet()) == false);
#endif
		// Kill the register before doing anything in case we take a
		// shortcut out of here
    rsRegValues[reg].rvdKind      = RV_TRASH;

    rsRegValues[reg].rvdKind = RV_TRASH;

    if (!MORE_REDUNDANT_LOAD && lvaTable[var].lvAddrTaken)
        return;

    /* Keep track of which registers we ever touch */

    regMaskTP      regMask = genRegMask(reg);

    rsMaskModf |= regMask;

#if REDUNDANT_LOAD

    /* Is the variable a pointer? */

    if (varTypeIsGC(varDsc->TypeGet()))
    {
        /* Don't track pointer register vars */

        if (varDsc->lvRegister)
            return;

        if (!rsCanTrackGCreg(regMask))
            return;
    }
    else if (varDsc->lvNormalizeOnLoad())
    {
            return;
    }

#endif

    /* Record the new value for the register. ptr var needed for
     * lifetime extension
     */

#ifdef  DEBUG
    if  (verbose) 
        printf("The register %s now holds V%02u\n", compRegVarName(reg), var);
#endif

    rsRegValues[reg].rvdKind      = RV_LCL_VAR;

    // If this is a cast of a 64 bit int, then we must have the low 32 bits.
    if (genActualType(varDsc->TypeGet()) == TYP_LONG)
    {
        rsRegValues[reg].rvdKind = RV_LCL_VAR_LNG_LO;
    }

    rsRegValues[reg].rvdLclVarNum = var;
}

/*****************************************************************************/

void                Compiler::rsTrackRegSwap(regNumber reg1, regNumber reg2)
{
    RegValDsc       tmp;

    tmp = rsRegValues[reg1];
          rsRegValues[reg1] = rsRegValues[reg2];
                              rsRegValues[reg2] = tmp;
}

void                Compiler::rsTrackRegCopy(regNumber reg1, regNumber reg2)
{
    /* Keep track of which registers we ever touch */

    rsMaskModf |= genRegMask(reg1);

    rsRegValues[reg1] = rsRegValues[reg2];
}


/*****************************************************************************
 *  One of the operands of this complex address mode has been spilled
 */

void                rsAddrSpillOper(GenTreePtr addr)
{
    if  (addr)
    {
        assert (addr->gtOper == GT_IND || addr->gtOper == GT_ARR_ELEM);

        // GTF_SPILLED_OP2 says "both operands have been spilled"
        assert((addr->gtFlags & GTF_SPILLED_OP2) == 0);

        if ((addr->gtFlags & GTF_SPILLED_OPER) == 0)
            addr->gtFlags |= GTF_SPILLED_OPER;
        else
            addr->gtFlags |= GTF_SPILLED_OP2;
    }
}

void            rsAddrUnspillOper(GenTreePtr addr)
{
    if (addr)
    {
        assert (addr->gtOper == GT_IND || addr->gtOper == GT_ARR_ELEM);

        assert((addr->gtFlags &       GTF_SPILLED_OPER) != 0);

        // Both operands spilled? */
        if    ((addr->gtFlags &       GTF_SPILLED_OP2 ) != 0)
            addr->gtFlags         &= ~GTF_SPILLED_OP2 ;
        else
            addr->gtFlags         &= ~GTF_SPILLED_OPER;
    }
}

/*****************************************************************************
 *
 *  Spill the given register (which we assume to be currently marked as used).
 */

void                Compiler::rsSpillReg(regNumber reg)
{
    SpillDsc   *    spill;
    TempDsc    *    temp;
    GenTreePtr      tree;
    var_types       type;

    regMaskTP       mask = genRegMask(reg);

    /* The register we're spilling must be used but not locked
       or an enregistered variable. */

    assert((mask & rsMaskUsed) != 0);
    assert((mask & rsMaskLock) == 0);
    assert((mask & rsMaskVars) == 0);

    /* Is this a 'multi-use' register? */

    /* We must know the value in the register that we are spilling */

    tree = rsUsedTree[reg]; assert(tree);

    /* Are we spilling a part of a register pair? */

    if  (tree->gtType == TYP_LONG)
    {
        assert(genRegPairLo(tree->gtRegPair) == reg ||
               genRegPairHi(tree->gtRegPair) == reg);
    }
    else
    {
        assert(tree->gtFlags & GTF_REG_VAL);
        assert(tree->gtRegNum == reg);
        assert(genActualType(tree->gtType) == TYP_INT ||
               genActualType(tree->gtType) == TYP_REF ||
                             tree->gtType  == TYP_BYREF);
    }

    /* Are any registers free for spillage? */

//  if  (rsRegMaskFree())
    {
        // @TODO [CONSIDER] [04/16/01] []:
        // Spill to another register: this should only be done
        // under controlled circumstances (such as when a call
        // is spilling registers) since it's relatively likely
        // that the other register may end up being spilled as
        // well, thus generating an extra move.
    }

    /* Allocate/reuse a spill descriptor */

    spill = rsSpillFree;
    if  (spill)
    {
        rsSpillFree = spill->spillNext;
    }
    else
    {
        spill = (SpillDsc *)compGetMem(sizeof(*spill));
    }

    /* Figure out the type of the spill temp */

    type = tree->TypeGet();

    if  (genActualType(type) == TYP_LONG)
        type = TYP_INT;

    /* Grab a temp to store the spilled value */

    spill->spillTemp = temp = tmpGetTemp(type);

    /* Remember what it is we have spilled */

    spill->spillTree = tree;
    spill->spillAddr = rsUsedAddr[reg];

#ifdef  DEBUG
    if  (verbose) 
        printf("The register %s spilled with    [%08X/%08X]",
               compRegVarName(reg), spill->spillTree, spill->spillAddr);
#endif

    /* Is the register part of a complex address mode? */

    rsAddrSpillOper(rsUsedAddr[reg]);

    /* 'lastDsc' is 'spill' for simple cases, and will point to the last
       multi-use descriptor if 'reg' is being multi-used */

    SpillDsc *  lastDsc = spill;

    if  ((rsMaskMult & mask) == 0)
    {
        spill->spillMoreMultis = false;
    }
    else
    {
        /* The register is being multi-used and will have entries in
           rsMultiDesc[reg]. Spill all of them (ie. move them to
           rsSpillDesc[reg]).
           When we unspill the reg, they will all be moved back to
           rsMultiDesc[].
         */

        spill->spillMoreMultis = true;

        SpillDsc * nextDsc = rsMultiDesc[reg];

        do
        {
            assert(nextDsc);

            /* Is this multi-use part of a complex address mode? */

            rsAddrSpillOper(nextDsc->spillAddr);

            /* Mark the tree node as having been spilled */

            rsMarkSpill(nextDsc->spillTree, reg);

            /* lastDsc points to the last of the multi-spill decrs for 'reg' */

            nextDsc->spillTemp = temp;

#ifdef  DEBUG
            if  (verbose) 
                printf(", [%08X/%08X]", nextDsc->spillTree, nextDsc->spillAddr);
#endif

            lastDsc->spillNext = nextDsc;
            lastDsc = nextDsc;

            nextDsc = nextDsc->spillNext;
        }
        while (lastDsc->spillMoreMultis);

        rsMultiDesc[reg] = nextDsc;

        /* 'reg' is no longer considered to be multi-used. We will set this
           mask again when this value gets unspilled */

        rsMaskMult &= ~mask;
    }

    /* Insert the spill descriptor(s) in the list */

    lastDsc->spillNext = rsSpillDesc[reg];
                         rsSpillDesc[reg] = spill;

#ifdef  DEBUG
    if  (verbose)     printf("\n");
#endif

     /* Generate the code to spill the register */

#if TGT_x86

    inst_ST_RV(INS_mov, temp, 0, reg, tree->TypeGet());

    genTmpAccessCnt++;

#else

#ifdef  DEBUG
    printf("Need to spill %s\n", getRegName(reg));
#endif
    assert(!"need non-x86 code for spilling");

#endif

    /* Mark the tree node as having been spilled */

    rsMarkSpill(tree, reg);

    /* The register is now free */

    rsMarkRegFree(mask);

    /* The register no longer holds its original value */

    rsUsedTree[reg] = 0;
}

/*****************************************************************************
 *
 *  Spill all registers in 'regMask' that are currently marked as used.
 */

void                Compiler::rsSpillRegs(regMaskTP regMask)
{
    /* The registers we're spilling must not be locked,
       or enregistered variables */

    assert((regMask & rsMaskLock) == 0);
    assert((regMask & rsMaskVars) == 0);

    /* Only spill what's currently marked as used */

    regMask &= rsMaskUsed; assert(regMask);

    regNumber       regNum;
    unsigned        regBit;

    for (regNum = REG_FIRST, regBit = 1; regNum < REG_COUNT; regNum = REG_NEXT(regNum), regBit <<= 1)
    {
        if  (regMask & regBit)
        {
            rsSpillReg(regNum);

            regMask -= regBit;
            if  (!regMask)
                break;
        }
    }
}

/*****************************************************************************
 *
 *  The following shows the code sizes that correspond to the order in which
 *  registers are picked in the routines that follow:
 *
 *
 *          EDX, EAX, ECX   [338662 VM, 688808/609151 x86 203%]
 *          ECX, EDX, EAX   [338662 VM, 688715/609029 x86 203%]
 *          EAX, ECX, EDX   [338662 VM, 687988/608337 x86 203%]
 *          EAX, EDX, ECX   [338662 VM, 687945/608314 x86 203%]
 */

/*****************************************************************************
 *
 *  Choose a register from the given set; if no registers in the set are
 *  currently free, one of them will have to be spilled (even if other
 *  registers - not in the set - are currently free).
 */

regNumber           Compiler::rsGrabReg(regMaskTP regMask)
{
    regMaskTP       OKmask;
    regNumber       regNum;
    unsigned        regBit;

    assert(regMask);
    regMask &= ~rsMaskLock;
    assert(regMask);

    /* See if one of the desired registers happens to be free */

    OKmask = regMask & rsRegMaskFree();

#if TGT_x86

    if  (OKmask & RBM_EAX) { regNum = REG_EAX; goto RET; }
    if  (OKmask & RBM_EDX) { regNum = REG_EDX; goto RET; }
    if  (OKmask & RBM_ECX) { regNum = REG_ECX; goto RET; }
    if  (OKmask & RBM_EBX) { regNum = REG_EBX; goto RET; }
    if  (OKmask & RBM_EBP) { regNum = REG_EBP; goto RET; }
    if  (OKmask & RBM_ESI) { regNum = REG_ESI; goto RET; }
    if  (OKmask & RBM_EDI) { regNum = REG_EDI; goto RET; }

#else

    /* Grab the lowest-numbered available register */

    if  (OKmask)
    {
        regNum = genRegNumFromMask(genFindLowestBit(OKmask));
        goto RET;
    }

#endif

    /* We'll have to spill one of the registers in 'regMask' */

    OKmask = rsRegMaskCanGrab() & regMask; assert(OKmask);

    // @TODO [CONSIDER] [04/16/01] []: Rotate the registers we consider for spilling, so that
    //           we avoid repeatedly spilling the same register; maybe.

	for(regNum = REG_FIRST, regBit = 1; 
		(regBit & OKmask) == 0; 
		regNum = REG_NEXT(regNum), regBit <<= 1)
    {
         if (regNum >= REG_COUNT) {
			assert(!"no register to grab!");
			NO_WAY_RET("Could not grab a register, Predictor should have prevented this!", regNumber);
			}
    }

	/* This will be the victim -- spill the sucker */
	rsSpillReg(regNum);

    /* Make sure we did find a register to spill */
    assert(genIsValidReg(regNum));
RET:
    /* Keep track of which registers we ever touch */
    rsMaskModf |= genRegMask(regNum);
    return  regNum;
}

/*****************************************************************************
 *
 *  Choose a register from the given set if possible (i.e. if any register
 *  in the set is currently free, choose it), otherwise pick any other reg
 *  that is currently free.
 *
 *  In other words, 'regMask' (if non-zero) is purely a recommendation and
 *  can be safely ignored (with likely loss of code quality, of course).
 */

regNumber           Compiler::rsPickReg(regMaskTP regMask,
                                        regMaskTP regBest,
                                        var_types regType)
{
    regNumber       reg;

#ifdef DEBUG
    if (rsStressRegs() >= 1 )
    {
        /* 'regMask' is purely a recommendation, and callers should be
           able to handle the case where it is not satisfied.
           The logic here tries to return ~regMask to check that all callers
           are prepared to handle such a case */

        regMaskTP   badRegs = rsMaskMult & rsRegMaskCanGrab();

        badRegs = rsUseIfZero(badRegs, rsMaskUsed & rsRegMaskCanGrab());
        badRegs = rsUseIfZero(badRegs, rsRegMaskCanGrab());
        badRegs = rsExcludeHint(badRegs, regMask);

        assert(badRegs != RBM_NONE);
        
        return rsGrabReg(badRegs);
    }
    
#endif

AGAIN:

    /* By default we'd prefer to accept all available registers */

    regMaskTP       OKmask = rsRegMaskFree();

    // OKmask = rsNarrowHint(OKmask, rsUselessRegs());

    /* Is there a 'best' register set? */

    if  (regBest)
    {
        OKmask &= regBest;
        if  (OKmask)
            goto TRY_REG;
        else
            goto TRY_ALL;
    }

    /* Was a register set recommended by the caller? */

    if  (regMask)
    {
        OKmask &= regMask;
        if  (!OKmask)
            goto TRY_ALL;
    }

TRY_REG:

    /* Any takers in the recommended/free set? */

#if SCHEDULER || USE_SET_FOR_LOGOPS

    unsigned    i;
    unsigned    count;

#if SCHEDULER
    if (!varTypeIsGC(regType) && rsRiscify(TYP_INT, OKmask))
#else
    if (!varTypeIsGC(regType))
#endif
    {
        i = rsNextPickRegIndex;
    }
    else
    {
        i = 0;
    }

    for (count = 0; count < rsREGORDER_SIZE(); count++)
    {
        if (OKmask & rsREG_MASK_ORDER[i])
        {
            reg = genRegNumFromMask(rsREG_MASK_ORDER[i]);
            goto RET;
        }

        i = (i + 1) % rsREGORDER_SIZE();
    }

#else

#if TGT_x86

    if  (OKmask & RBM_EAX) { reg = REG_EAX; goto RET; }
    if  (OKmask & RBM_EDX) { reg = REG_EDX; goto RET; }
    if  (OKmask & RBM_ECX) { reg = REG_ECX; goto RET; }
    if  (OKmask & RBM_EBX) { reg = REG_EBX; goto RET; }
    if  (OKmask & RBM_EBP) { reg = REG_EBP; goto RET; }
    if  (OKmask & RBM_ESI) { reg = REG_ESI; goto RET; }
    if  (OKmask & RBM_EDI) { reg = REG_EDI; goto RET; }

#else

    assert(!"need non-x86 code");

#endif

#endif

TRY_ALL:

    /* Were we considering 'regBest' ? */

    if  (regBest)
    {
        /* 'regBest' is no good -- ignore it and try 'regMask' instead */

        regBest = 0;
        goto AGAIN;
    }

    regMaskTP   freeMask;
    regMaskTP   spillMask;

    /* Now let's consider all available registers */

    freeMask = rsRegMaskFree();

    /* Were we limited in out consideration? */

    if  (!regMask)
    {
        /* We need to spill one of the free registers */

        spillMask = freeMask;
    }
    else
    {
        /* Did we not consider all free registers? */

        if  ((regMask & freeMask) != freeMask)
        {
            /* The recommended regset didn't work, so try all available regs */

#if SCHEDULER || USE_SET_FOR_LOGOPS

#if SCHEDULER
            if (!varTypeIsGC(regType) && rsRiscify(TYP_INT, freeMask))
#else
            if (!varTypeIsGC(regType))
#endif
            {
                i = rsNextPickRegIndex;
            }
            else
            {
                i = 0;
            }

            for (count = 0; count < rsREGORDER_SIZE(); count++)
            {
                if (freeMask & rsREG_MASK_ORDER[i])
                {
                    reg = genRegNumFromMask(rsREG_MASK_ORDER[i]);
                    goto RET;
                }

                i = (i + 1) % rsREGORDER_SIZE();
            }

#else

#if TGT_x86

            if  (freeMask & RBM_EAX) { reg = REG_EAX; goto RET; }
            if  (freeMask & RBM_EDX) { reg = REG_EDX; goto RET; }
            if  (freeMask & RBM_ECX) { reg = REG_ECX; goto RET; }
            if  (freeMask & RBM_EBX) { reg = REG_EBX; goto RET; }
            if  (freeMask & RBM_EBP) { reg = REG_EBP; goto RET; }
            if  (freeMask & RBM_ESI) { reg = REG_ESI; goto RET; }
            if  (freeMask & RBM_EDI) { reg = REG_EDI; goto RET; }

#else

            assert(!"need non-x86 code");

#endif

#endif

        }

        /* If we're going to spill, might as well go for the right one */

        spillMask = regMask;
    }

    /* Make sure we can spill some register. */

    if  ((spillMask & rsRegMaskCanGrab()) == 0)
        spillMask = rsRegMaskCanGrab();

    assert(spillMask);

    /* We have no choice but to spill one of the regs */

    return  rsGrabReg(spillMask);

RET:

#if SCHEDULER || USE_SET_FOR_LOGOPS
    rsNextPickRegIndex = (i + 1) % rsREGORDER_SIZE();
#endif

    rsMaskModf |= genRegMask(reg);
    return  reg;
}

/*****************************************************************************
 *
 *  Get the temp that was spilled from the given register (and free its
 *  spill descriptor while we're at it). Returns the temp (i.e. local var)
 */

Compiler::TempDsc *    Compiler::rsGetSpillTempWord(regNumber   reg,
                                                    SpillDsc *  dsc,
                                                    SpillDsc *  prevDsc)
{
    assert(!prevDsc || prevDsc->spillNext == dsc);

    /* Is dsc the last of a set of multi-used values */

    if (prevDsc && prevDsc->spillMoreMultis && !dsc->spillMoreMultis)
        prevDsc->spillMoreMultis = false;

    /* Remove this spill entry from the register's list */

    (prevDsc ? prevDsc->spillNext : rsSpillDesc[reg]) = dsc->spillNext;

    /* Remember which temp the value is in */

    TempDsc    *   temp = dsc->spillTemp;

    /* Add the entry to the free list */

    dsc->spillNext = rsSpillFree;
                     rsSpillFree = dsc;

    /* return the temp variable */

    return temp;
}

/*****************************************************************************
 *
 *  Reload the value that was spilled from the given register (and free its
 *  spill descriptor while we're at it). Returns the new register (which will
 *  be a member of 'needReg' if that value is non-zero).
 *
 *  'willKeepNewReg' indicates if the caller intends to mark newReg as used.
 *      If not, then we cant unspill the other multi-used descriptor (if any).
 *      Instead, we will just hold on to the temp and unspill them
 *      again as needed.
 */

regNumber           Compiler::rsUnspillOneReg(GenTreePtr    tree,
                                              regNumber     oldReg,
                                              KeepReg       willKeepNewReg,
                                              regMaskTP     needReg)
{
    /* Was oldReg multi-used when it was spilled? */

    SpillDsc *  prevDsc, * multiDsc;
    SpillDsc *  spillDsc = rsGetSpillInfo(tree, oldReg, &prevDsc, &multiDsc);
    assert(spillDsc);
    assert(multiDsc);

    bool        multiUsed       = multiDsc->spillMoreMultis;

    /* We will use multiDsc to walk the rest of the spill list (if it's
       multiUsed). As we're going to remove spillDsc from the multiDsc
       list in the rsGetSpillTempWord() call we have to take care of the 
       case where multiDsc==spillDsc. We will set multiDsc as spillDsc->spillNext */
    if (multiUsed && multiDsc==spillDsc)
    {
        assert(spillDsc->spillNext);
        multiDsc=spillDsc->spillNext;
    }


    /* Get the temp and free the spill-descriptor */

    TempDsc  *  temp = rsGetSpillTempWord(oldReg, spillDsc, prevDsc);

    /*  Pick a new home for the value (this must be a register matching
        the 'needReg' mask, if non-zero); note that the rsGrabReg call
        below may cause the chosen register to be spilled.
     */

    regNumber   newReg = rsGrabReg(rsUseIfZero(needReg, RBM_ALL));

    /* UNDONE: the following isn't right, but it's good enough for v1 */

    rsTrackRegTrash(newReg);

    /* Reload the value from the saved location into the new register */

#if TGT_x86

    inst_RV_ST(INS_mov, newReg, temp, 0, temp->tdTempType());

    genTmpAccessCnt++;

#else

    assert(!"need non-x86 code");

#endif

    if (!multiUsed)
    {
        /* Free the temp, it's no longer used */

        tmpRlsTemp(temp);
    }
    else if (willKeepNewReg == KEEP_REG)
    {
        /* We will unspill all the other multi-use trees if the register
           is going to be marked as used. If it is not gonig to be marked
           as used, we will have a problem if the new regiser gets spilled
           again.
         */

        /* We dont do the extra unspilling for complex address modes,
           since someone up the call chain may have a different idea about
           what registers are used to form the complex address mode (the
           addrReg return value from genMakeAddressable).
         
           Also, it is not safe to unspill all the multi-uses with a TYP_LONG.
	   
           Finally, it is not safe to unspill into a different register, because
           the caller of genMakeAddressable caches the addrReg return value
           (register mask), but when unspilling into a different register it's
           not possible to inform the caller that addrReg is now different.
           See bug #89946 for an example of this.  There is an assert for this
           in rsMarkRegFree via genDoneAddressable.
         */

        for (SpillDsc * dsc = multiDsc; /**/; dsc = dsc->spillNext)
        {
            if ((oldReg != newReg) ||
                (dsc->spillAddr != NULL) ||
                (dsc->spillTree->gtType == TYP_LONG))
            {
                return newReg;
            }

            if (!dsc->spillMoreMultis)
            {
                /* All the remaining multi-uses are fine. We will now
                   unspill them all */
                break;
            }
        }

        
        bool bFound=false;
        SpillDsc*  pDsc;
        SpillDsc** ppPrev;

        for (pDsc=rsSpillDesc[oldReg], ppPrev=&rsSpillDesc[oldReg] ; ; pDsc=pDsc->spillNext)
        {
            if (pDsc==multiDsc)
            {
                // We've found the sequence we were searching for
                bFound=true;
            }

            if (bFound)
            {   
                rsAddrUnspillOper(pDsc->spillAddr);

                // Mark the tree node as having been unspilled into newReg 
                rsMarkUnspill(pDsc->spillTree, newReg);
            }

            if (!pDsc->spillMoreMultis)
            {
                if (bFound)
                {
                    // End of sequence

                    // We link remaining sides of list
                    *ppPrev=pDsc->spillNext;

                    // Exit walk
                    break;
                }
                else
                {
                    ppPrev=&(pDsc->spillNext);
                }
            }
        }


        /* pDsc points to the last multi-used descriptor from the spill-list
           for the current value (pDsc->spillMoreMultis == false) */

        pDsc->spillNext = rsMultiDesc[newReg];
                          rsMultiDesc[newReg] = multiDsc;

        rsMaskMult |= genRegMask(newReg);

        /* Free the temp, it's no longer used */

        tmpRlsTemp(temp);
    }

    return newReg;
}

/*****************************************************************************
 *
 *  The given tree operand has been spilled; just mark it as unspilled so
 *  that we can use it as "normal" local.
 *  The spill temp is freed if freeTemp==true, else it needs to be done
 *  by the caller.
 */

Compiler::TempDsc *     Compiler::rsUnspillInPlace(GenTreePtr    tree,
                                                   bool          freeTemp)
{
    /* Get the tree's SpillDsc */

    assert(!isRegPairType(tree->gtType));
    regNumber   oldReg = tree->gtRegNum;

    SpillDsc *  prevDsc, * multiDsc;
    SpillDsc *  spillDsc = rsGetSpillInfo(tree, oldReg, &prevDsc, &multiDsc);
    assert(spillDsc);

    /* Get the temp */

    TempDsc *   temp = rsGetSpillTempWord(oldReg, spillDsc, prevDsc);

    /* The value is now unspilled */

    tree->gtFlags &= ~GTF_SPILLED;

#ifdef  DEBUG
    if  (verbose) printf("Tree-Node marked unspilled from  [%08X]\n", tree);
#endif

    /* Free the temp, it's no longer used */

    if (freeTemp)
        tmpRlsTemp(temp);

    return temp;
}

/*****************************************************************************
 *
 *  The given tree operand has been spilled; reload it into a register that
 *  is in 'regMask' (if 'regMask' is 0, any register will do). If 'keepReg'
 *  is non-zero, we'll mark the new register as used.
 */

void                Compiler::rsUnspillReg(GenTreePtr tree, 
                                           regMaskTP  needReg,
                                           KeepReg    keepReg)
{
    assert(!isRegPairType(tree->gtType)); // use rsUnspillRegPair()
    regNumber   oldReg = tree->gtRegNum;

    /* Get the SpillDsc for the tree */

    SpillDsc * spillDsc = rsGetSpillInfo(tree, oldReg);
    assert(spillDsc);

    /* Before spillDsc is stomped on by rsUnspillOneReg(), note whether
     * the reg was part of an address mode
     */

    GenTreePtr  unspillAddr = spillDsc->spillAddr;

    /* Pick a new home for the value */

    regNumber   newReg = rsUnspillOneReg(tree, oldReg, keepReg, needReg);

    /* Mark the tree node as having been unspilled into newReg */

    rsMarkUnspill(tree, newReg);

    // If this reg was part of a complex address mode, need to clear this flag which
    // tells address mode building that a component has been spilled

    rsAddrUnspillOper(unspillAddr);

#ifdef  DEBUG
    if  (verbose) 
        printf("The register %s unspilled from  [%08X]\n", compRegVarName(newReg), tree);
#endif

    /* Mark the new value as used, if the caller desires so */

    if  (keepReg == KEEP_REG)
        rsMarkRegUsed(tree);
}

void                Compiler::rsMarkSpill(GenTreePtr tree, regNumber reg)
{
    tree->gtFlags  &= ~GTF_REG_VAL;
    tree->gtFlags  |=  GTF_SPILLED;
}

void                Compiler::rsMarkUnspill(GenTreePtr tree, regNumber reg)
{
    assert(tree->gtType != TYP_LONG);

    tree->gtFlags  |=  GTF_REG_VAL;
    tree->gtFlags  &= ~GTF_SPILLED;
    tree->gtRegNum  = reg;
}

/*****************************************************************************
 *
 *  Choose a register pair from the given set (note: only registers in the
 *  given set will be considered).
 */

regPairNo           Compiler::rsGrabRegPair(regMaskTP regMask)
{
    regPairNo       regPair;
    regMaskTP       OKmask;
    regNumber       reg1;
    regNumber       reg2;

    assert(regMask);
    regMask &= ~rsMaskLock;
    assert(regMask);

    /* We'd prefer to choose a free register pair if possible */

    OKmask = regMask & rsRegMaskFree();

    /* Any takers in the recommended/free set? */

    regPair = rsFindRegPairNo(OKmask);
    
    if (regPair != REG_PAIR_NONE)
    {
        // The normal early exit

        /* Keep track of which registers we ever touch */
        rsMaskModf |= genRegPairMask(regPair);

        return regPair;
    }

    /* We have no choice but to spill one or two used regs */

    if  (OKmask)
    {
        /* One (and only one) register is free and acceptable - grab it */

        assert(genMaxOneBit(OKmask));

        for (reg1 = REG_FIRST; reg1 < REG_LAST; reg1 = REG_NEXT(reg1))
        {
            if  (OKmask & genRegMask(reg1))
                break;
        }
        assert(OKmask & genRegMask(reg1));
    }
    else
    {
        /* No register is free and acceptable - we'll have to spill two */

        reg1 = rsGrabReg(regMask);
    }

    /* Temporarily lock the first register so it doesn't go away */

    rsLockReg(genRegMask(reg1));

    /* Now grab another register */

    reg2 = rsGrabReg(regMask);

    /* We can unlock the first register now */

    rsUnlockReg(genRegMask(reg1));

    /* Convert the two register numbers into a pair */

     if  (reg1 < reg2)
        regPair = gen2regs2pair(reg1, reg2);
     else
        regPair = gen2regs2pair(reg2, reg1);

     return  regPair;
}

/*****************************************************************************
 *
 *  Choose a register pair from the given set (if non-zero) or from the set of
 *  currently available registers (if 'regMask' is zero).
 */

regPairNo           Compiler::rsPickRegPair(regMaskTP regMask)
{
    regPairMaskTP   OKmask;
    regPairNo       regPair;

    int             repeat = 0;

    /* By default we'd prefer to accept all available registers */

    OKmask = rsRegMaskFree();

    if  (regMask)
    {
        /* A register set was recommended by the caller */

        OKmask &= regMask;
    }

AGAIN:

    regPair = rsFindRegPairNo(OKmask);
    
    if (regPair != REG_PAIR_NONE)
    {
        return regPair;         // Normal early exit
    }

    regMaskTP freeMask;
    regMaskTP spillMask;

    /* Now let's consider all available registers */

    freeMask = rsRegMaskFree();

    /* Were we limited in our consideration? */

    if  (!regMask)
    {
        /* We need to spill two of the free registers */

        spillMask = freeMask;
    }
    else
    {
        /* Did we not consider all free registers? */

        if  ((regMask & freeMask) != freeMask && repeat == 0)
        {
            /* The recommended regset didn't work, so try all available regs */

            OKmask = freeMask;
            repeat++;
            goto AGAIN;
        }

        /* If we're going to spill, might as well go for the right one */

        spillMask = regMask;
    }

    /* Make sure that we have at least two bits set */

    if (genMaxOneBit(spillMask & rsRegMaskCanGrab()))
        spillMask = rsRegMaskCanGrab();

    assert(!genMaxOneBit(spillMask));

    /* We have no choice but to spill 1/2 of the regs */

    return  rsGrabRegPair(spillMask);
}

/*****************************************************************************
 *
 *  The given tree operand has been spilled; reload it into a register pair
 *  that is in 'regMask' (if 'regMask' is 0, any register pair will do). If
 *  'keepReg' is non-zero, we'll mark the new register pair as used. It is
 *  assumed that the current register pair has been marked as used (modulo
 *  any spillage, of course).
 */

void                Compiler::rsUnspillRegPair(GenTreePtr tree, 
                                               regMaskTP  needReg,
                                               KeepReg    keepReg)
{
    assert(isRegPairType(tree->gtType));

    regPairNo       regPair = tree->gtRegPair;
    regNumber       regLo   = genRegPairLo(regPair);
    regNumber       regHi   = genRegPairHi(regPair);

    /* Has the register holding the lower half been spilled? */

    if  (!rsIsTreeInReg(regLo, tree))
    {
        /* Is the upper half already in the right place? */

        if  (rsIsTreeInReg(regHi, tree))
        {
            /* Temporarily lock the high part */

            rsLockUsedReg(genRegMask(regHi));

            /* Pick a new home for the lower half */

            regLo = rsUnspillOneReg(tree, regLo, keepReg, needReg);

            /* We can unlock the high part now */

            rsUnlockUsedReg(genRegMask(regHi));
        }
        else
        {
            /* Pick a new home for the lower half */

            regLo = rsUnspillOneReg(tree, regLo, keepReg, needReg);
        }
    }
    else
    {
        /* Free the register holding the lower half */

        rsMarkRegFree(genRegMask(regLo));
    }

    /* Has the register holding the upper half been spilled? */

    if  (!rsIsTreeInReg(regHi, tree))
    {
        regMaskTP   regLoUsed;

        /* Temporarily lock the low part so it doesnt get spilled */

        rsLockReg(genRegMask(regLo), &regLoUsed);

        /* Pick a new home for the upper half */

        regHi = rsUnspillOneReg(tree, regHi, keepReg, needReg);

        /* We can unlock the low register now */

        rsUnlockReg(genRegMask(regLo), regLoUsed);
    }
    else
    {
        /* Free the register holding the upper half */

        rsMarkRegFree(genRegMask(regHi));
    }

    /* The value is now residing in the new register */

    tree->gtFlags    |=  GTF_REG_VAL;
    tree->gtFlags    &= ~GTF_SPILLED;
    tree->gtRegPair   = gen2regs2pair(regLo, regHi);

    /* Mark the new value as used, if the caller desires so */

    if  (keepReg == KEEP_REG)
        rsMarkRegPairUsed(tree);
}

/*****************************************************************************
 *
 *  The given register is being used by multiple trees (all of which represent
 *  the same logical value). Happens mainly because of REDUNDANT_LOAD;
 *  We dont want to really spill the register as it actually holds the
 *  value we want. But the multiple trees may be part of different
 *  addressing modes.
 *  Save the previous 'use' info so that when we return the register will
 *  appear unused.
 */

void                Compiler::rsRecMultiReg(regNumber reg)
{
    SpillDsc   *   spill;

    regMaskTP       regMask = genRegMask(reg);

#ifdef  DEBUG
    if  (verbose) 
        printf("Register %s multi-use inc for   [%08X/%08X] multMask=%08X->%08X\n",
               compRegVarName(reg), rsUsedTree[reg], rsUsedAddr[reg], rsMaskMult, rsMaskMult | regMask);
#endif

    /* The register is supposed to be already used */

    assert(regMask & rsMaskUsed);
    assert(rsUsedTree[reg]);

    /* Allocate/reuse a spill descriptor */

    spill = rsSpillFree;
    if  (spill)
    {
        rsSpillFree = spill->spillNext;
    }
    else
    {
        spill = (SpillDsc *)compGetMem(sizeof(*spill));
    }

    /* Record the current 'use' info in the spill descriptor */

    spill->spillTree = rsUsedTree[reg]; rsUsedTree[reg] = 0;
    spill->spillAddr = rsUsedAddr[reg]; rsUsedAddr[reg] = 0;

    /* Remember whether the register is already 'multi-use' */

    spill->spillMoreMultis = ((rsMaskMult & regMask) != 0);

    /* Insert the new multi-use record in the list for the register */

    spill->spillNext = rsMultiDesc[reg];
                       rsMultiDesc[reg] = spill;

    /* This register is now 'multi-use' */

    rsMaskMult |= regMask;
}

/*****************************************************************************
 *
 *  Free the given register, which is known to have multiple uses.
 */

void                Compiler::rsRmvMultiReg(regNumber reg)
{
    SpillDsc   *   dsc;

    assert(rsMaskMult & genRegMask(reg));

#ifdef  DEBUG
    if  (verbose) 
        printf("Register %s multi-use dec for   [%08X/%08X] multMask=%08X\n",
               compRegVarName(reg), rsUsedTree[reg], rsUsedAddr[reg], rsMaskMult);
#endif

    /* Get hold of the spill descriptor for the register */

    dsc = rsMultiDesc[reg]; assert(dsc);
          rsMultiDesc[reg] = dsc->spillNext;

    /* Copy the previous 'use' info from the descriptor */

    rsUsedTree[reg] = dsc->spillTree;
    rsUsedAddr[reg] = dsc->spillAddr;

    if (!(dsc->spillTree->gtFlags & GTF_SPILLED))
        gcMarkRegPtrVal(reg, dsc->spillTree->TypeGet());

    /* Is only one use of the register left? */

    if  (!dsc->spillMoreMultis)
        rsMaskMult -= genRegMask(reg);

#ifdef  DEBUG
    if  (verbose) 
        printf("Register %s multi-use dec - now [%08X/%08X] multMask=%08X\n",
               compRegVarName(reg), rsUsedTree[reg], rsUsedAddr[reg], rsMaskMult);
#endif

    /* Put the spill entry on the free list */

    dsc->spillNext = rsSpillFree;
                     rsSpillFree = dsc;
}

/*****************************************************************************/
#if REDUNDANT_LOAD
/*****************************************************************************
 *
 *  Search for a register which contains the given constant value.
 *  Return success/failure and set the register if success.
 *  If the closeDelta argument is non-NULL then register values that
 *  are within +127..-128 are also returned and *closeDelta is set to
 *  the difference that needs to be added to the register returned.
 *  In Codegen an lea instruction is used to set the target register
 *  using the register that contains the close integer constant.
 */

regNumber           Compiler::rsIconIsInReg(long   val,
                                            long * closeDelta /* = NULL */)
{
    regNumber  closeReg = REG_NA;

    if  (opts.compMinOptim || opts.compDbgCode)
        return REG_NA;

    for (regNumber reg = REG_FIRST; reg < REG_COUNT; reg = REG_NEXT(reg))
    { 
        if (rsRegValues[reg].rvdKind == RV_INT_CNS)
        {
            long regCnsVal = rsRegValues[reg].rvdIntCnsVal;
            if (regCnsVal == val)
            {
#if SCHEDULER
                rsUpdateRegOrderIndex(reg);
#endif
                if (closeDelta)
                    *closeDelta = 0;
                return reg;
            }
            if (closeDelta && (closeReg == REG_NA))
            {
                long regCnsDelta = val - regCnsVal;
                /* Does delta fit inside a byte [-128..127] */
                if (regCnsDelta == (signed char)regCnsDelta)
                {
                    closeReg    = reg;
                    *closeDelta = regCnsDelta;
                }
            }
        }
    }

    /* There was not an exact match */

    return closeReg;  /* will always be REG_NA when closeDelta is NULL */
}

/*****************************************************************************
 *
 *  Assume all non-integer registers contain garbage (this is called when
 *  we encounter a code label that isn't jumped by any block; we need to
 *  clear pointer values our of the table lest the GC pointer tables get
 *  out of whack).
 */

void                Compiler::rsTrackRegClrPtr()
{
    for (regNumber reg = REG_FIRST; reg < REG_COUNT; reg = REG_NEXT(reg))
    {
        /* Preserve constant values */

        if  (rsRegValues[reg].rvdKind == RV_INT_CNS)
        {
            /* Make sure we don't preserve NULL (it's a pointer) */

            if  (rsRegValues[reg].rvdIntCnsVal != NULL)
                continue;
        }

        /* Preserve variables known to not be pointers */

        if  (rsRegValues[reg].rvdKind == RV_LCL_VAR)
        {
            if  (!varTypeIsGC(lvaTable[rsRegValues[reg].rvdLclVarNum].TypeGet()))
                continue;
        }

        rsRegValues[reg].rvdKind = RV_TRASH;
    }
}

/*****************************************************************************
 *
 *  Search for a register which contains the given local var.
 *  Return success/failure and set the register if success.
 *  Return FALSE on register variables, because otherwise their lifetimes
 *  can get screwed up with respect to pointer tracking.
 */

regNumber           Compiler::rsLclIsInReg(unsigned var)
{
#ifdef  DEBUG
    genIntrptibleUse = true;
#endif

    assert(var < lvaCount);

    if  (opts.compMinOptim || opts.compDbgCode)
        return REG_NA;

    /* return false if register var so genMarkLclVar can do its job */

    if (lvaTable[var].lvRegister)
        return REG_NA;

    for (regNumber reg = REG_FIRST; reg < REG_COUNT; reg = REG_NEXT(reg))
    {
        if (rsRegValues[reg].rvdLclVarNum == var &&
            rsRegValues[reg].rvdKind == RV_LCL_VAR)
        {
#if SCHEDULER
            rsUpdateRegOrderIndex(reg);
#endif
            return reg;
        }
    }

    return REG_NA;
}

/*****************************************************************************/

regPairNo           Compiler::rsLclIsInRegPair(unsigned var)
{
    assert(var < lvaCount);

    if  (opts.compMinOptim || opts.compDbgCode)
        return REG_PAIR_NONE;

    regValKind  rvKind = RV_TRASH;
    regNumber   regNo;

    for (regNumber reg = REG_FIRST; reg < REG_COUNT; reg = REG_NEXT(reg))
    {
        if  (rvKind != rsRegValues[reg].rvdKind &&
             rsTrackIsLclVarLng(rsRegValues[reg].rvdKind) &&
             rsRegValues[reg].rvdLclVarNum == var)
        {
            /* first occurrence of this variable ? */

            if  (rvKind == RV_TRASH)
            {
                regNo = reg;
                rvKind = rsRegValues[reg].rvdKind;
            }
            else if (rvKind == RV_LCL_VAR_LNG_HI)
            {
                /* We found the lower half of the long */

                return gen2regs2pair(reg, regNo);
            }
            else
            {
                /* We found the upper half of the long */

                assert(rvKind == RV_LCL_VAR_LNG_LO);
                return gen2regs2pair(regNo, reg);
            }
        }
    }

    return REG_PAIR_NONE;
}

/*****************************************************************************/

regNumber           Compiler::rsClsVarIsInReg(CORINFO_FIELD_HANDLE fldHnd)
{
    if (!MORE_REDUNDANT_LOAD)
        return REG_NA;

#ifdef  DEBUG
    genIntrptibleUse = true;
#endif

    if  (opts.compMinOptim || opts.compDbgCode)
        return REG_NA;

    for (regNumber reg = REG_FIRST; reg < REG_COUNT; reg = REG_NEXT(reg))
    {
        if (rsRegValues[reg].rvdKind == RV_CLS_VAR &&
            rsRegValues[reg].rvdClsVarHnd == fldHnd)
        {
#if SCHEDULER
            rsUpdateRegOrderIndex(reg);
#endif
            return reg;
        }
    }
    return REG_NA;
}

/*****************************************************************************/

void                Compiler::rsTrashLclLong(unsigned var)
{
    if  (opts.compMinOptim || opts.compDbgCode)
        return;

    for  (regNumber reg = REG_FIRST; reg < REG_COUNT; reg = REG_NEXT(reg))
    {
        if  (rsTrackIsLclVarLng(rsRegValues[reg].rvdKind) &&
             rsRegValues[reg].rvdLclVarNum == var)
        {
            rsRegValues[reg].rvdKind = RV_TRASH;
        }
     }
}

/*****************************************************************************
 *
 *  Local's value has changed, mark all regs which contained it as trash.
 */

void                Compiler::rsTrashLcl(unsigned var)
{
    if  (opts.compMinOptim || opts.compDbgCode)
        return;

    for (regNumber reg = REG_FIRST; reg < REG_COUNT; reg = REG_NEXT(reg))
    {
        if (rsRegValues[reg].rvdKind == RV_LCL_VAR &&
            rsRegValues[reg].rvdLclVarNum == var)
        {
            rsRegValues[reg].rvdKind = RV_TRASH;
        }
    }
}

/*****************************************************************************
 *
 *  Class-variable's value has changed, mark all regs which contained it as trash.
 */

void                Compiler::rsTrashClsVar(CORINFO_FIELD_HANDLE fldHnd)
{
    if (!MORE_REDUNDANT_LOAD)
        return;

    if  (opts.compMinOptim || opts.compDbgCode)
        return;

    for (regNumber reg = REG_FIRST; reg < REG_COUNT; reg = REG_NEXT(reg))
    {
        if (rsRegValues[reg].rvdKind == RV_CLS_VAR &&
            rsRegValues[reg].rvdClsVarHnd == fldHnd)
        {
            rsRegValues[reg].rvdKind = RV_TRASH;
        }
    }
}

/*****************************************************************************
 *
 *  A little helper to trash the given set of registers.
 *  Usually used after a call has been generated.
 */

void                Compiler::rsTrashRegSet(regMaskTP regMask)
{
    if  (opts.compMinOptim || opts.compDbgCode)
        return;

#if     TGT_x86

    assert((regMask & ~(RBM_EAX|RBM_ECX|RBM_EDX)) == 0);

    if  (regMask & RBM_EAX) rsTrackRegTrash(REG_EAX);
    if  (regMask & RBM_ECX) rsTrackRegTrash(REG_ECX);
    if  (regMask & RBM_EDX) rsTrackRegTrash(REG_EDX);

#else

    while (regMask)
    {
        regMaskTP   regTemp;

        /* Get the next bit in the mask */

        regTemp  = genFindLowestBit(regMask);

        /* Trash the corresponding register */

        rsTrackRegTrash(genRegNumFromMask(regTemp));

        /* Clear the bit and continue if any more left */

        regMask -= regTemp;
    }

#endif

}

/*****************************************************************************
 asg - tree being assigned to. If NULL, no specific tree is being assigned.
*/

void                Compiler::rsTrashAliasedValues(GenTreePtr asg)
{
    if (!MORE_REDUNDANT_LOAD)
        return;

    if  (opts.compMinOptim || opts.compDbgCode)
        return;

    if (asg)
    {
        assert(asg->gtOper != GT_LCL_VAR); // Use rsTrashLcl()/rsTrashLclLong()

        if (asg->gtOper != GT_IND)
            return;

        if (asg->gtFlags & (GTF_IND_RNGCHK|GTF_IND_FIELD))
        {
            /* We know that writes into arrays and to member fields wont
               interfere with any aliased tracked values */
            return;
        }
    }

    for  (regNumber reg = REG_FIRST; reg < REG_COUNT; reg = REG_NEXT(reg))
    {
        switch(rsRegValues[reg].rvdKind)
        {
        case RV_LCL_VAR:
        case RV_LCL_VAR_LNG_LO:
        case RV_LCL_VAR_LNG_HI:
            if (!lvaTable[rsRegValues[reg].rvdLclVarNum].lvAddrTaken)
                break;

            // Fall through ...

        case RV_CLS_VAR:

            rsRegValues[reg].rvdKind = RV_TRASH;
        }
    }
}

/*****************************************************************************
 *
 *  Return a mask of registers that hold no useful value.
 */

regMaskTP           Compiler::rsUselessRegs()
{
    if  (opts.compMinOptim || opts.compDbgCode)
        return  RBM_ALL;

    unsigned    mask = 0;
    for (regNumber reg = REG_FIRST; reg < REG_COUNT; reg = REG_NEXT(reg))
    {
        if  (rsRegValues[reg].rvdKind == RV_TRASH)
            mask |= genRegMask(reg);
    }

    return  mask;
}

/*****************************************************************************/
#endif//REDUNDANT_LOAD
/*****************************************************************************/




/*
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                           TempsInfo                                       XX
XX                                                                           XX
XX  The temporary lclVars allocated by the compiler for code generation      XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/


void                Compiler::tmpInit()
{
    tmpDoubleSpillMax = 0;
    tmpIntSpillMax    = 0;

#ifdef DEBUG
    if (rsStressRegs())
        tmpIntSpillMax = (SCHAR_MAX/sizeof(int));
#endif

    tmpCount    = 0;
    tmpSize     = 0;
#ifdef DEBUG
    tmpGetCount = 0;
#endif

    memset(tmpFree, 0, sizeof(tmpFree));
}

/*****************************************************************************
 *
 *  Allocate a temp of the given size (and type, if tracking pointers for
 *  the garbage collector).
 */

Compiler::TempDsc * Compiler::tmpGetTemp(var_types type)
{
    size_t          size = roundUp(genTypeSize(type));

    /* We only care whether a temp is pointer or not */

    if  (!varTypeIsGC(type))
    {
        switch (genTypeStSz(type))
        {
        case 1: type = TYP_INT   ; break;
        case 2: type = TYP_DOUBLE; break;
        default: assert(!"unexpected type");
        }
    }

    /* Find the slot to search for a free temp of the right size */

    unsigned    slot = tmpFreeSlot(size);

    /* Look for a temp with a matching type */

    TempDsc * * last = &tmpFree[slot];
    TempDsc *   temp;

    for (temp = *last; temp; last = &temp->tdNext, temp = *last)
    {
        /* Does the type match? */

        if  (temp->tdType == type)
        {
            /* We have a match -- remove it from the free list */

            *last = temp->tdNext;
            break;
        }
    }

    /* Do we need to allocate a new temp */

    if  (!temp)
    {
        tmpCount++;
        tmpSize += size;
        genEmitter->emitTmpSizeChanged(tmpSize);

        temp = (TempDsc *)compGetMem(sizeof(*temp));
        temp->tdType = type;
        temp->tdSize = size;

        /* For now, only assign an index to the temp */

#ifdef DEBUG
        temp->tdOffs = BAD_TEMP_OFFSET;
#endif
        temp->tdNum  = -tmpCount;
    }

#ifdef  DEBUG
    if  (verbose) printf("get temp[%u->%u] at [EBP-%04X]\n",
                         temp->tdSize, slot, -temp->tdOffs);
    tmpGetCount++;
#endif

    return temp;
}

/*****************************************************************************
 *
 *  Release the given temp.
 */

void                Compiler::tmpRlsTemp(TempDsc *temp)
{
    unsigned        slot;

    /* Add the temp to the 'free' list */

    slot = tmpFreeSlot(temp->tdSize);

#ifdef  DEBUG
    if  (verbose) printf("rls temp[%u->%u] at [EBP-%04X]\n",
                         temp->tdSize, slot, -temp->tdOffs);
    assert(tmpGetCount);
    tmpGetCount--;
#endif

    temp->tdNext = tmpFree[slot];
                   tmpFree[slot] = temp;
}

/*****************************************************************************
 *
 *  Given a temp number, find the corresponding temp.
 */

Compiler::TempDsc * Compiler::tmpFindNum(int tnum)
{
    /* @TODO [CONSIDER] [04/16/01] []: Do something smarter, a linear search is dumb!!!! */

    for (TempDsc * temp = tmpListBeg(); temp; temp = tmpListNxt(temp))
    {
        if  (temp->tdTempNum() == tnum)
            return  temp;
    }

    return  NULL;
}

/*****************************************************************************
 * Used with tmpListBeg() to iterate over the list of temps.
 */

Compiler::TempDsc *     Compiler::tmpListNxt(TempDsc * curTemp)
{
    TempDsc     *       temp;

    assert(curTemp);

    temp = curTemp->tdNext;

    if (temp == 0)
    {
        size_t size = curTemp->tdSize;

        // If there are no more temps in the list, check if there are more
        // slots (for bigger sized temps) to walk

        if (size < TEMP_MAX_SIZE)
        {
            // Using "if" above instead of "while" assumes only 2 sizes
            assert(size + sizeof(int) == TEMP_MAX_SIZE);

            unsigned slot = tmpFreeSlot(size + sizeof(int));
            temp = tmpFree[slot];

            assert(temp == 0 || temp->tdSize == size + sizeof(int));
        }
    }

    return temp;
}

/*****************************************************************************
 *
 *  Returns whether regPair is a combination of two x86 registers or
 *  contains a pseudo register.
 *  In debug it also asserts that reg1 and reg2 are not the same.
 */

BOOL                genIsProperRegPair(regPairNo regPair)
{
    regNumber rlo = genRegPairLo(regPair);
    regNumber rhi = genRegPairHi(regPair);

    assert(regPair >= REG_PAIR_FIRST &&
           regPair <= REG_PAIR_LAST);

    if  (rlo == rhi)
        return false;

    if  (rlo == REG_L_STK || rhi == REG_L_STK)
        return false;

    if  (rlo >= REG_COUNT || rhi >= REG_COUNT)
        return false;

    return (rlo != REG_STK && rhi != REG_STK);
}

/*****************************************************************************/
=== C:/Users/treeman/Desktop/windows nt source code\Source\Win2K3\NT\com\netfx\src\clr\jit\il\regalloc.cpp ===
// ==++==
// 
//   Copyright (c) Microsoft Corporation.  All rights reserved.
// 
// ==--==
/*XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XX                                                                           XX
XX                           RegAlloc                                        XX
XX                                                                           XX
XX  Does the register allocation and puts the remaining lclVars on the stack XX
XX                                                                           XX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
*/

#include "jitpch.h"
#pragma hdrstop

enum CanDoubleAlign
{
    CANT_DOUBLE_ALIGN,
    CAN_DOUBLE_ALIGN,
    MUST_DOUBLE_ALIGN,
    COUNT_DOUBLE_ALIGN,

    DEFAULT_DOUBLE_ALIGN = CAN_DOUBLE_ALIGN
};

enum FrameType
{
    FT_NOT_SET,
    FT_ESP_FRAME,
    FT_EBP_FRAME,
    FT_DOUBLE_ALIGN_FRAME,
};


#ifdef DEBUG
static ConfigDWORD fJitDoubleAlign(L"JitDoubleAlign", DEFAULT_DOUBLE_ALIGN);
static const int s_canDoubleAlign = fJitDoubleAlign.val();
#else 
static const int s_canDoubleAlign = DEFAULT_DOUBLE_ALIGN;
#endif

void                Compiler::raInit()
{
    // If opts.compMinOptim, then we dont dont raPredictRegUse(). We simply
    // only use RBM_MIN_OPT_LCLVAR_REGS for register allocation

#if ALLOW_MIN_OPT
    raMinOptLclVarRegs = RBM_MIN_OPT_LCLVAR_REGS;
#endif

    /* We have not assigned any FP variables to registers yet */

#if TGT_x86
    optAllFPregVars = 0;
#endif

    rpReverseEBPenreg = false;
    rpAsgVarNum       = -1;
    rpPassesMax       = 6;
    rpPassesPessimize = rpPassesMax - 4;
    if (opts.compDbgCode)
        rpPassesMax++;
    rpFrameType       = FT_NOT_SET;
    rpLostEnreg       = false;
}

/*****************************************************************************
 *
 *  The following table determines the order in which registers are considered
 *  for variables to live in
 */

static const regNumber  raRegVarOrder[]   = { REG_VAR_LIST };
const unsigned          raRegVarOrderSize = sizeof(raRegVarOrder)/sizeof(raRegVarOrder[0]);


#ifdef  DEBUG
static ConfigDWORD fJitNoFPRegLoc(L"JitNoFPRegLoc");

/*****************************************************************************
 *
 *  Dump out the variable interference graph
 *
 */

void                Compiler::raDumpVarIntf()
{
    unsigned        lclNum;
    LclVarDsc   *   varDsc;

    printf("Var. interference graph for %s\n", info.compFullName);

    for (lclNum = 0, varDsc = lvaTable;
         lclNum < lvaCount;
         lclNum++  , varDsc++)
    {
        /* Ignore the variable if it's not tracked */

        if  (!varDsc->lvTracked)
            continue;

        /* Get hold of the index and the interference mask for the variable */
        unsigned   varIndex = varDsc->lvVarIndex;

        printf("  V%02u,T%02u and ", lclNum, varIndex);

        unsigned        refIndex;
        VARSET_TP       refBit;

        for (refIndex = 0, refBit = 1;
             refIndex < lvaTrackedCount;
             refIndex++  , refBit <<= 1)
        {
            if  (lvaVarIntf[varIndex] & refBit)
                printf("T%02u ", refIndex);
            else
                printf("    ");
        }

        printf("\n");
    }

    printf("\n");
}

/*****************************************************************************
 *
 *  Dump out the register interference graph
 *
 */
void                Compiler::raDumpRegIntf()
{
    printf("Reg. interference graph for %s\n", info.compFullName);

    unsigned    lclNum;
    LclVarDsc * varDsc;

    for (lclNum = 0, varDsc = lvaTable;
         lclNum < lvaCount;
         lclNum++  , varDsc++)
    {
        unsigned        varNum;
        VARSET_TP       varBit;

        /* Ignore the variable if it's not tracked */

        if  (!varDsc->lvTracked)
            continue;

        /* Get hold of the index and the interference mask for the variable */

        varNum = varDsc->lvVarIndex;
        varBit = genVarIndexToBit(varNum);

        printf("  V%02u,T%02u and ", lclNum, varNum);

        if  (isFloatRegType(varDsc->lvType))
        {
#if TGT_x86
            for (unsigned regNum = 0; regNum < FP_STK_SIZE; regNum++)
            {
                if  (raFPlvlLife[regNum] & varBit)
                {
                    printf("ST(%u) ", regNum);
                }
            }
#endif
        }
        else
        {
            for (regNumber regNum = REG_FIRST; regNum < REG_COUNT; regNum = REG_NEXT(regNum))
            {
                if  (raLclRegIntf[regNum] & varBit)
                    printf("%3s ", compRegVarName(regNum));
                else
                    printf("    ");
            }
        }

        printf("\n");
    }

    printf("\n");
}
#endif

/*****************************************************************************
 *
 * We'll adjust the ref counts based on interference
 *
 */

void                Compiler::raAdjustVarIntf()
{
    if (true) // @Disabled
        return;

    unsigned        lclNum;
    LclVarDsc * *   cntTab;

    for (lclNum = 0, cntTab = lvaRefSorted;
         lclNum < lvaCount;
         lclNum++  , cntTab++)
    {
        /* Get hold of the variable descriptor */

        LclVarDsc *     varDsc = *cntTab;

        /* Skip the variable if it's not tracked */

        if  (!varDsc->lvTracked)
            continue;

        /* Skip the variable if it's already enregistered */

        if  (varDsc->lvRegister)
            continue;

        /* Skip the variable if it's marked as 'volatile' */

        if  (varDsc->lvVolatile)
            continue;

        /* Stop if we've reached the point of no use */

        if  (varDsc->lvRefCnt < 1)
            break;

        /* See how many registers this variable interferes with */

        unsigned        varIndex = varDsc->lvVarIndex;
        VARSET_TP       regIntf  = raLclRegIntf[varIndex];

        unsigned        reg;
        unsigned        regCnt;

        for (reg = regCnt = 0;
             reg < raRegVarOrderSize;
             reg++)
        {
            regNumber  regNum = raRegVarOrder[reg];
            regMaskTP  regBit = genRegMask(regNum);

            if  (regIntf & regBit)
                regCnt++;
        }

        printf("V%02u interferes with %u registers\n", varDsc-lvaTable, regCnt);
    }
}

/*****************************************************************************/
#if TGT_x86
/*****************************************************************************/
/* Determine register mask for a call/return from type.
 */

inline
regMaskTP               genTypeToReturnReg(var_types type)
{
    const  static
    regMaskTP returnMap[TYP_COUNT] =
    {   
        RBM_ILLEGAL, // TYP_UNDEF,
        RBM_NONE,    // TYP_VOID,
        RBM_INTRET,  // TYP_BOOL,
        RBM_INTRET,  // TYP_CHAR,
        RBM_INTRET,  // TYP_BYTE,
        RBM_INTRET,  // TYP_UBYTE,
        RBM_INTRET,  // TYP_SHORT,
        RBM_INTRET,  // TYP_USHORT,
        RBM_INTRET,  // TYP_INT,
        RBM_INTRET,  // TYP_UINT,
        RBM_LNGRET,  // TYP_LONG,
        RBM_LNGRET,  // TYP_ULONG,
        RBM_NONE,    // TYP_FLOAT,
        RBM_NONE,    // TYP_DOUBLE,
        RBM_INTRET,  // TYP_REF,
        RBM_INTRET,  // TYP_BYREF,
        RBM_INTRET,  // TYP_ARRAY,
        RBM_ILLEGAL, // TYP_STRUCT,
        RBM_ILLEGAL, // TYP_BLK,
        RBM_ILLEGAL, // TYP_LCLBLK,
        RBM_ILLEGAL, // TYP_PTR,
        RBM_ILLEGAL, // TYP_FNC,
        RBM_ILLEGAL, // TYP_UNKNOWN,
    };

    assert(type < sizeof(returnMap)/sizeof(returnMap[0]));
    assert(returnMap[TYP_LONG]   == RBM_LNGRET);
    assert(returnMap[TYP_DOUBLE] == RBM_NONE);
    assert(returnMap[TYP_REF]    == RBM_INTRET);
    assert(returnMap[TYP_STRUCT] == RBM_ILLEGAL);

    regMaskTP result = returnMap[type];
    assert(result != RBM_ILLEGAL);
    return result;
}


/*****************************************************************************/
#else //not TGT_x86
/*****************************************************************************
 *
 *  Predict the temporary register needs of a list of expressions (typically,
 *  an argument list).
 */

unsigned            Compiler::raPredictListRegUse(GenTreePtr list)
{
    unsigned        count = 0;

    do
    {
        assert(list && list->gtOper == GT_LIST);

        count = max(count, raPredictTreeRegUse(list->gtOp.gtOp1));

        list  = list->gtOp.gtOp2;
    }
    while (list);

    return  count;
}

/*****************************************************************************/
#endif//not TGT_x86


/****************************************************************************/

#ifdef  DEBUG

static
void                dispLifeSet(Compiler *comp, VARSET_TP mask, VARSET_TP life)
{
    unsigned                lclNum;
    Compiler::LclVarDsc *   varDsc;

    for (lclNum = 0, varDsc = comp->lvaTable;
         lclNum < comp->lvaCount;
         lclNum++  , varDsc++)
    {
        VARSET_TP       vbit;

        if  (!varDsc->lvTracked)
            continue;

        vbit = genVarIndexToBit(varDsc->lvVarIndex);

        if  (!(vbit & mask))
            continue;

        if  (life & vbit)
            printf("V%02u ", lclNum);
    }
}

#endif

/*****************************************************************************/
#ifdef  DEBUG
/*****************************************************************************
 *
 *  Debugging helpers - display variables liveness info.
 */

void                dispFPvarsInBBlist(BasicBlock * beg,
                                       BasicBlock * end,
                                       VARSET_TP    mask,
                                       Compiler   * comp)
{
    do
    {
        printf("BB%02u: ", beg->bbNum);

        printf(" in  = [ ");
        dispLifeSet(comp, mask, beg->bbLiveIn );
        printf("] ,");

        printf(" out = [ ");
        dispLifeSet(comp, mask, beg->bbLiveOut);
        printf("]");

        if  (beg->bbFlags & BBF_VISITED)
            printf(" inner=%u", beg->bbFPinVars);

        printf("\n");

        beg = beg->bbNext;
        if  (!beg)
            return;
    }
    while (beg != end);
}

void                Compiler::raDispFPlifeInfo()
{
    BasicBlock  *   block;

    for (block = fgFirstBB;
         block;
         block = block->bbNext)
    {
        GenTreePtr      stmt;

        printf("BB%02u: in  = [ ", block->bbNum);
        dispLifeSet(this, optAllFloatVars, block->bbLiveIn);
        printf("]\n\n");

        for (stmt = block->bbTreeList; stmt; stmt = stmt->gtNext)
        {
            GenTreePtr      tree;

            assert(stmt->gtOper == GT_STMT);

            for (tree = stmt->gtStmt.gtStmtList;
                 tree;
                 tree = tree->gtNext)
            {
                VARSET_TP       life = tree->gtLiveSet;

                dispLifeSet(this, optAllFloatVars, life);
                printf("   ");
                gtDispTree(tree, 0, NULL, true);
            }

            printf("\n");
        }

        printf("BB%02u: out = [ ", block->bbNum);
        dispLifeSet(this, optAllFloatVars, block->bbLiveOut);
        printf("]\n\n");
    }
}

/*****************************************************************************/
#endif//DEBUG
/*****************************************************************************/
#if     TGT_x86
/*****************************************************************************
 *
 *  Upon a transfer of control from 'srcBlk' to '*dstPtr', the given FP
 *  register variable dies. We need to arrange for its value to be popped
 *  from the FP stack when we land on the destination block.
 */

void                Compiler::raInsertFPregVarPop(BasicBlock *  srcBlk,
                                                  BasicBlock * *dstPtr,
                                                  unsigned      varNum)
{
    BasicBlock  *   dstBlk = *dstPtr;

    LclVarDsc   *   varDsc;
    VARSET_TP       varBit;

    VARSET_TP       newLife;

    flowList    *   predList;

    GenTreePtr      rvar;
    GenTreePtr      kill;
    GenTreePtr      stmt;

    /* Get hold of the variable's lifeset bit */

    assert(varNum < lvaCount);
    varDsc = lvaTable + varNum;
    assert(varDsc->lvTracked);
    varBit = genVarIndexToBit(varDsc->lvVarIndex);

    /*
        Check all predecessors of the target block; if all of them jump
        to the block with our variable live, we can simply prepend the
        killing statement to the target block since all the paths to
        the block need to kill our variable. If there is at least one
        path where death doesn't occur we'll have to insert a killing
        basic block into those paths that need the death.
     */

#ifdef DEBUG
    fgDebugCheckBBlist();

    if (verbose)
    {
        printf("Adding an FP register pop for V%02u on edge BB%02u -> BB%02u\n",
               varNum, srcBlk->bbNum, (*dstPtr)->bbNum);
    }
#endif

    bool            addBlk = false;

    for (predList = dstBlk->bbPreds; predList; predList = predList->flNext)
    {
        BasicBlock  *   pred = predList->flBlock;

        if  (!(pred->bbLiveOut & varBit))
        {
            /* No death along this particular edge, we'll have to add a block */

            addBlk = true;
        }
    }

    /* Do we need to add a "killing block" ? */

    if  (addBlk)
    {
        /* Allocate a new basic block */
        raNewBlocks         = true;
      
        BasicBlock * tmpBlk = bbNewBasicBlock(BBJ_NONE);
        tmpBlk->bbFlags    |= BBF_INTERNAL | BBF_JMP_TARGET | BBF_HAS_LABEL;
        tmpBlk->bbTreeList  = NULL;
        tmpBlk->bbRefs      = 0;

        tmpBlk->bbLiveIn    = dstBlk->bbLiveIn | varBit;
        tmpBlk->bbLiveOut   = dstBlk->bbLiveIn;

        tmpBlk->bbVarUse    = dstBlk->bbVarUse | varBit;
        tmpBlk->bbFPoutVars = dstBlk->bbFPoutVars;
        tmpBlk->bbVarTmp    = dstBlk->bbVarTmp;

        tmpBlk->bbWeight    = dstBlk->bbWeight;
        tmpBlk->bbFlags    |= dstBlk->bbFlags & BBF_RUN_RARELY;

#ifdef  DEBUG
        if  (verbose)
            printf("Added new FP regvar killing basic block BB%02u for V%02u [bit=%08X]\n",
                   tmpBlk->bbNum, varNum, varBit);
#endif

        bool            addBlkAtEnd = true;

        for (predList = dstBlk->bbPreds; predList; predList = predList->flNext)
        {
            BasicBlock  *   pred = predList->flBlock;

#ifdef  DEBUG
            if  (verbose && 0)
            {
                printf("BB%02u: out = %08X [ ",   pred->bbNum,   pred->bbLiveOut);
                dispLifeSet(this, optAllFloatVars,   pred->bbLiveOut);
                printf("]\n");

                printf("BB%02u: in  = %08X [ ", dstBlk->bbNum, dstBlk->bbLiveIn );
                dispLifeSet(this, optAllFloatVars, dstBlk->bbLiveIn );
                printf("]\n\n");
            }
#endif

            /* Ignore this block if it doesn't need the kill */

            if  (!(pred->bbLiveOut & varBit))
                continue;

            /* Need to update the links to point to the new block */

            switch (pred->bbJumpKind)
            {
                BasicBlock * *  jmpTab;
                unsigned        jmpCnt;

            case BBJ_COND:

                if  (pred->bbJumpDest == dstBlk)
                {
                    pred->bbJumpDest = tmpBlk;

                    /* Update bbPreds and bbRefs */

                    fgReplacePred(dstBlk, pred, tmpBlk);
                    fgAddRefPred (tmpBlk, pred);
                }

                // Fall through ...

            case BBJ_NONE:

                if  (pred->bbNext     == dstBlk)
                {
                    /* Insert the kill block right after this fall-through predecessor */

                    pred->bbNext   = tmpBlk;

                    /* Update bbPreds and bbRefs */

                    fgReplacePred(dstBlk, pred, tmpBlk);
                    fgAddRefPred (tmpBlk, pred);

                    /* Remember that we've inserted the target block */

                    addBlkAtEnd = false;
                    tmpBlk->bbNext = dstBlk;  // set tmpBlk->bbNext only when addBlkAtEnd is false
                }
                break;

            case BBJ_ALWAYS:

                if  (pred->bbJumpDest == dstBlk)
                {
                     pred->bbJumpDest =  tmpBlk;

                    /* Update bbPreds and bbRefs */

                    fgReplacePred(dstBlk, pred, tmpBlk);
                    fgAddRefPred (tmpBlk, pred);
                }

                break;

            case BBJ_SWITCH:

                jmpCnt = pred->bbJumpSwt->bbsCount;
                jmpTab = pred->bbJumpSwt->bbsDstTab;

                do
                {
                    if  (*jmpTab == dstBlk)
                    {
                         *jmpTab =  tmpBlk;

                        /* Update bbPreds and bbRefs */

                        fgReplacePred(dstBlk, pred, tmpBlk);
                        fgAddRefPred (tmpBlk, pred);
                    }
                }
                while (++jmpTab, --jmpCnt);

                break;

            default:
                assert(!"unexpected jump kind");
            }
        }

        if  (addBlkAtEnd)
        {
            /* Append the kill block at the end of the method */

            fgLastBB->bbNext = tmpBlk;
            fgLastBB         = tmpBlk;

            /* We have to jump from the kill block to the target block */

            tmpBlk->bbJumpKind  = BBJ_ALWAYS;
            tmpBlk->bbJumpDest  = dstBlk;
        }

        *dstPtr = dstBlk = tmpBlk;
    }

    /*
        At this point we know that all paths to 'dstBlk' involve the death
        of our variable. Create the expression that will kill it.
     */

    rvar = gtNewOperNode(GT_REG_VAR, TYP_DOUBLE);
    rvar->gtRegNum             =
    rvar->gtRegVar.gtRegNum    = (regNumber)0;
    rvar->gtRegVar.gtRegVar    = varNum;
    rvar->gtFlags             |= GTF_REG_DEATH;

    kill = gtNewOperNode(GT_NOP, TYP_DOUBLE, rvar);
    kill->gtFlags |= GTF_NOP_DEATH;

    /* Create a statement entry out of the nop/kill expression */

    stmt = gtNewStmt(kill);
    stmt->gtFlags |= GTF_STMT_CMPADD;

    /* Create the linked list of tree nodes for the statement */

    stmt->gtStmt.gtStmtList = rvar;
    stmt->gtStmtFPrvcOut    = genCountBits(dstBlk->bbLiveIn & optAllFPregVars);

    rvar->gtPrev            = 0;
    rvar->gtNext            = kill;

    kill->gtPrev            = rvar;
    kill->gtNext            = 0;

    gtSetStmtInfo(stmt);

    /*  If any nested FP register variables are killed on entry to this block,
        we need to insert the new kill node after the ones for the inner vars.
     */

    if  (dstBlk->bbFPinVars)
    {
        GenTreePtr      next;
        GenTreePtr      list = dstBlk->bbTreeList;
        unsigned        kcnt = dstBlk->bbFPinVars;

        /* Update the number of live FP regvars after our statement */

        stmt->gtStmtFPrvcOut -= kcnt;

        /* Skip over any "inner" kill statements */

        for (;;)
        {
            assert(list);
            assert(list->gtOper == GT_STMT);
            assert(list->gtFlags & GTF_STMT_CMPADD);
            assert(list->gtStmt.gtStmtExpr->gtOper == GT_NOP);
            assert(list->gtStmt.gtStmtExpr->gtOp.gtOp1->gtOper == GT_REG_VAR);
            assert(list->gtStmt.gtStmtExpr->gtOp.gtOp1->gtFlags & GTF_REG_DEATH);

            /* Remember the liveness at the preceding statement */

            newLife = list->gtStmt.gtStmtExpr->gtLiveSet;

            /* Our variable is still live at this (innner) kill block */

            //list                               ->gtLiveSet |= varBit;
            list->gtStmt.gtStmtExpr            ->gtLiveSet |= varBit;
            list->gtStmt.gtStmtExpr->gtOp.gtOp1->gtLiveSet |= varBit;

            /* Have we skipped enough kill statements? */

            if  (--kcnt == 0)
                break;

            /* Get the next kill and continue */

            list = list->gtNext;
        }

        /* Insert the new statement into the list */

        next = list->gtNext; assert(next && next->gtPrev == list);

        list->gtNext = stmt;
        stmt->gtPrev = list;
        stmt->gtNext = next;
        next->gtPrev = stmt;

#ifdef DEBUG
        if (verbose)
        {
            printf("Added FP register pop of V%02u after %d inner FP kills in BB%02u\n",
                   varNum, dstBlk->bbFPinVars, (*dstPtr)->bbNum);
        }
#endif
    }
    else
    {
        /* Append the kill statement at the beginning of the target block */

        fgInsertStmtAtBeg(dstBlk, stmt);

        /* Use the liveness on entry to the block */

        newLife = dstBlk->bbLiveIn;

#ifdef DEBUG
        if (verbose)
        {
            printf("Added FP register pop of V%02u at the start of BB%02u\n",
                   varNum, (*dstPtr)->bbNum);
        }
#endif
    }

    /* Set the appropriate liveness values */

    rvar->gtLiveSet =
    kill->gtLiveSet = newLife & ~varBit;

    /* Now our variable is live on entry to the target block */

    dstBlk->bbLiveIn    |= varBit;
    dstBlk->bbFPoutVars |= varBit;
}

/*****************************************************************************
 *
 *  While looking for FP variables to be enregistered, we've reached the end
 *  of a basic block which has a control path to the given target block.
 *
 *  Returns true if there is an unresolvable conflict, false upon success.
 */

bool                Compiler::raMarkFPblock(BasicBlock *srcBlk,
                                            BasicBlock *dstBlk,
                                            unsigned    icnt,
                                            VARSET_TP   life,
                                            VARSET_TP   lifeOuter,
                                            VARSET_TP   varBit,
                                            VARSET_TP   intVars,
                                            bool    *    deathPtr,
                                            bool    *   repeatPtr)
{
    *deathPtr = false;

    /* Has we seen this block already? */

    if  (dstBlk->bbFlags & BBF_VISITED)
    {
        /* Our variable may die, but otherwise the life set must match */

        if  (lifeOuter == dstBlk->bbVarTmp)
        {
            if  (life ==  dstBlk->bbFPoutVars)
            {
                /* If our variable is alive then the "inner" count better match */

                assert(((life & varBit) == 0) || (icnt == dstBlk->bbFPinVars));

                return  false;
            }

            if  (life == (dstBlk->bbFPoutVars|varBit))
            {
                *deathPtr = true;
                return  false;
            }
        }

#ifdef  DEBUG

        if  (verbose)
        {
            printf("Incompatible edge from BB%02u to BB%02u: ",
                   srcBlk->bbNum, dstBlk->bbNum);

            VARSET_TP diffLife = lifeOuter ^ dstBlk->bbVarTmp;
            if (!diffLife)
            {
                diffLife = lifeOuter ^ dstBlk->bbFPoutVars;
                if (diffLife & varBit)
                    diffLife &= ~varBit;
            }
            assert(diffLife);
            diffLife = genFindLowestBit(diffLife);
            unsigned varNum = genLog2(diffLife);
            unsigned lclNum = lvaTrackedToVarNum[varNum];
            printf("Incompatible outer life for V%02u,T%02u\n",
                   lclNum, varNum);
        }

#endif

        return  true;
    }
    else
    {
        VARSET_TP       dstl = dstBlk->bbLiveIn & intVars;

        /* This is the first time we've encountered this block */

        /* Is anything dying upon reaching the target block? */

        if  (dstl != life)
        {
            /* The only change here should be the death of our variable */

            assert((dstl | varBit) == life);
            assert((life - varBit) == dstl);

            *deathPtr = true;
        }

        dstBlk->bbFlags    |= BBF_VISITED;

        /* Store the values from the predecessor block */

        dstBlk->bbFPoutVars = dstl;
        dstBlk->bbFPinVars  = icnt;
        dstBlk->bbVarTmp    = lifeOuter;

//      printf("Set vardef of BB%02u to %08X at %s(%u)\n", dstBlk->bbNum, (int)dstBlk->bbFPoutVars, __FILE__, __LINE__);

        /* Have we already skipped past this block? */

        if  (srcBlk->bbNum > dstBlk->bbNum)
            *repeatPtr = true;

        return  false;
    }
}

/*****************************************************************************
 *
 *  Check the variable's lifetime for any conflicts. Basically,
 *  we make sure the following are all true for the variable:
 *
 *      1.  Its lifetime is properly nested within or wholly
 *          contain any other enregistered FP variable (i.e.
 *          the lifetimes nest within each other and don't
 *          "cross over".
 *
 *      2.  Wherever the variable dies
 *              a.  If we want to defer the death, we must make
 *                  sure no other fpu enregistrated var becomes live or 
 *                  dead for the rest of the statement. (NOTE that this code is 
 *                  actually commented out as we have ifdefed out deferring deaths
 *
 *      3.  Whenever a basic block boundary is crossed, one of
 *         